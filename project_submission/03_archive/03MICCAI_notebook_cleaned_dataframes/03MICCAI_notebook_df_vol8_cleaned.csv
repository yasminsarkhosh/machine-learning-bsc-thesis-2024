Paper Title,Header Number,Header Title,Text
LUCYD: A Feature-Driven Richardson-Lucy Deconvolution Network,1,Introduction,"Microscopy is one of the most widely used imaging techniques that allows life scientists to analyse cells, tissues and subcellular structures with a high level of detail. However, microscopy images often suffer from degradation such as blur, noise and other artefacts, which may result in an inaccurate quantification and hinder downstream analysis. Therefore, deconvolution techniques are necessary to restore the images to improve their quality, thus increasing the accuracy of downstream tasks [7,12,16]. Image deconvolution is a well-studied task in computer vision and imaging sciences that aims to recover a sharp and clear object out of a degraded input. The mathematical representation of image corruption can be expressed as:where * represents convolution, y denotes the resulting image of an object x, which has been blurred with a point spread function (PSF) K, and degraded by noise n. Two classic image deconvolution methods widely used in microscopy and medical imaging are Wiener filter [18] and Richardson-Lucy algorithm (RL) [9,11]. The Wiener filter is a linear filter that is applied to the frequency domain representation of the blurred image. It assumes the Gaussian noise distribution and thus minimises the mean squared error between the restored image and the original one. The RL method, on the other hand, is an iterative algorithm that works in the spatial domain, usually leading to better reconstruction than Wiener filter. It assumes Poisson noise distribution and seeks to estimate the corresponding sharp image x in a fixed number of iterations or until a convergence criterion is met [5]. While being simple and effective, the main limitations of both methods are their susceptibility to noise amplification [3,13] and the assumption that the accurate PSF is known. In practice, however, PSF is challenging to obtain and is often unknown or varies across the image, which leads to inaccurate reconstructions of the sharp image. Moreover, as an iterative method, RL is computationally costly for three-dimensional (3D) data [4].In the computer vision field, numerous deep learning models have been trained on large datasets with the objective to learn a direct mapping between input and output domains [1,6,10,14,15]. Some of these models have also been adapted for use in microscopy, such as the U-Net-based content-aware image restoration networks (CARE) [17]. These methods have exhibited exceptional performance in tasks such as super-resolution and denoising. However, the interpretability of these methods is limited, and given their data-driven nature, the quantity and quality of training data can be a restricting factor, particularly in biomedical applications where data pairs are often scarce or not available.Inspired by the RL algorithm, the Richardson-Lucy Network (RLN) [8] was recently designed to overcome the problem of data-driven models by embedding the RL formula for iterative image restoration into a neural network and substituting convolutions of the measured PSF kernel with learnable convolutional layers. Although being more compact than U-Net, the low capacity of RLN makes it insufficiently robust to different blur intensities and noise levels, requiring the network to be re-trained whenever there is a shift in the input image domain. This reduces the efficacy of the method.To address the limitations of existing methods, we propose a novel lightweight model called LUCYD, which integrates the RL deconvolution formula and a Ushaped network. The main contributions of this paper can be summarised as:1. LUCYD is a lightweight deconvolution method that embeds the RL deconvolution formula into a deep convolutional network that leverages the features extracted by a U-shaped module while maintaining low computational costs for processing 3D microscopy images and a high level of interpretability. 2. The proposed method outperforms existing deconvolution methods on both real and synthetic datasets, based on qualitative assessment and quantitative evaluation metrics, respectively. 3. We show that LUCYD has strong resistance to noise and can generalise well to new and unseen data. This ability makes it a valuable tool for practical applications in microscopy imaging fields where image quality is critical for downstream tasks yet training data are often scarce or unavailable."
LUCYD: A Feature-Driven Richardson-Lucy Deconvolution Network,2,Method,"The overall architecture of the proposed model is illustrated in Fig. 1 and it comprises three main components: a correction module, an update module, and a bottleneck that is shared between the two modules. The data flow in the model is based on the following iterative RL formula:x estimatewhich aims to recover x in k steps. We bypass the requirement of k-1 preceding iterations with the correction module that generates a mask M to form an intermediate sharp image estimation through a single forward pass, allowing to rapidly process 3D data, as follows:Next, inspired by Li et al. [8], we adopt the three-step approach to decompose the RL update term from Eq. 2 in the update module:Specifically, we replace convolutions with a known PSF in steps (a) and (c) with forward projector f and backward projector b, which consist of sets of learnable convolutional layers. The produced update term u allows us to recondition the estimate z from the correction module into a sharp image through multiplication, i.e. the last step of image formation in the RL formula: x = z • u. The whole network can then be expressed as follows,By adhering to the image formation steps as prescribed by the RL formula, we maintain a high degree of interpretability, critical for real-world scenarios, where the accuracy and reliability of the generated results are of utmost importance.  "
LUCYD: A Feature-Driven Richardson-Lucy Deconvolution Network,2.1,Correction Module and Bottleneck,"The proposed correction module and bottleneck architectures consist of encoder blocks (EBs), decoder blocks (DBs), and multi-scale feature fusion blocks to facilitate efficient information exchange across different scales within the model.Feature Encoding. The features of the volumetric input image y ∈ R C×D×H×W are obtained through the first encoder block EB 1 in the correction module, and then encoded by a convolutional layer with a stride 2. Subsequently, the downsampled features are concatenated with the encoded features of the forward projection f from the update module and then fed to the bottleneck encoder EB 2 to integrate the information from both modules.Feature Fusion Block. Similarly to Cho et al. [2], we enhance the connections between encoders and decoders and allow information flow from different scales within the network through Feature Fusion Blocks (FFBlocks). The features from EB 1 and EB 2 are refined as follows,where up-sampling (↑) and down-sampling (↓) is applied to allow for feature concatenation. The multi-scale features are then combined and processed by 1 × 1 and 3 × 3 convolutional layers, respectively, to allow the decoder blocks DB 1 and DB 2 to utilise information obtained on different scales. The structure of the blocks is shown in Fig. 2a.Feature Decoding. Initially, the refined features are decoded in the bottleneck using a convolutional layer and residual block within the DB 2 . Next, these features are expanded with a convolutional layer to match the dimensions in both the correction and update modules. The resulting features are then concatenated with the output of FFBlock 1 and subsequently fed into decoder DB 2 within the correction module. The features are then mapped to the image dimensions resulting in mask M , which is summed with y to form z."
LUCYD: A Feature-Driven Richardson-Lucy Deconvolution Network,2.2,Update Module,"Inspired by the forward and backward projector functions [8], we substitute the PSF convolution operations from Richardson-Lucy algorithm with learnable convolutional layers and residual blocks.During forward projection (FP), shallow features are initially extracted by a single convolutional layer and then refined by a residual block. The output of f is then passed to Richardson-Lucy Division Block (RLDiv) which embeds the division of the raw image y by the channel-wise mean of the refined FP features. Next, we project the division result to a feature map to extract more information about the image. The visualisation of the process is in Fig. 2b. These features are then concatenated with the features extracted by the bottleneck and combined by a convolutional layer which initiates the backward projection with b. The output is then summed with the output of RLDiv, forming a skipconnection, and passed through a residual block. The features are then refined by a convolutional layer and their channel-wise mean is taken to be the ""update term"" u, which is used to obtain the final model output x through multiplication with z (denoted as RLMul)."
LUCYD: A Feature-Driven Richardson-Lucy Deconvolution Network,2.3,Loss Function,The entire model is trained end-to-end with a single loss function that combines the Mean Squared Error (MSE) and the Structural Similarity Index Measure (SSIM) as follows:where x is the ground truth sharp image and x is the model estimation of x (Table 1).
LUCYD: A Feature-Driven Richardson-Lucy Deconvolution Network,3.1,Setup,"Datasets. We assess the performance of LUCYD on both simulated phantom objects and real microscopy images. To achieve this, we use five sets of 3D grayscale volumes generated by Li et al. [8], consisting of dots, solid spheres, and ellipsoidal surfaces, which are provided along with their sharp ground truth volumes of dimensions 128 × 128 × 128 (one exemplary image shown in Fig. 3e).To test the generalization capabilities of our method, we also include two blurry and noisy versions of the dataset, D nuc and D act , which utilize different image degradation processes for embryonic nuclei and membrane data. Additionally, we generate a mixed dataset by applying permutations of three Gaussian blur intensities (σ b = [1.0, 1.2, 1.5]) and three levels of additive Gaussian noise (σ n = [0, 15, 30]) to the ground truth volumes, and then test the ability of the model to generalize to volumes blurred with Gaussian kernels (σ b = [0.5, 2.0]) and additive Gaussian noise (σ n = [20, 50, 70, 100]) levels outside of the training dataset. The model is trained on patches of dimensions 32 × 64 × 64 that are randomly sampled from the training datasets. Moreover, we evaluate the model trained using synthetic phantom shapes on a real 3D light-sheet image of a starfish (private data) and widefield microscopy image of U2OS cell (from the dataset of [8]), to explore the generalisation capabilities. Baseline and Metrics. We employ one classic U-Net-based fluorescence image restoration model CARE [17] and one RL-based convolutional model RLN [8] as baselines. We quantitatively evaluate the deconvolution performance on simulated data using two metrics: Structural Similarity Index Measure (SSIM) and Peak Signal-to-Noise Ratio (PSNR).  ). LUCYD exhibits superior performance in recovering fine details and structures as compared to CARE and RLN, while simultaneously maintaining low levels of noise and haze surrounding the objects."
LUCYD: A Feature-Driven Richardson-Lucy Deconvolution Network,3.2,Results,"In Table 2, we present the quantitative results of all three methods on simulated phantom objects degraded with blur and noise levels that were not present in the training dataset. LUCYD achieves the best performance even in cases where the amount of additive noise exceeds the maximum level included in the training dataset. This is in contrast to CARE and RLN, which did not demonstrate such exceptional generalisation capabilities and noise resistance. We further examine LUCYD's performance on datasets simulating widefield microscopy imaging of embryo nuclei and membrane data. As shown in Table 3, LUCYD outperforms CARE and RLN in both in-domain and cross-domain assessments, further supporting the model's capabilities in cross-domain applications. We finally apply LUCYD on two real microscopy test samples, as illustrated in Fig. 4. On the 3D light-sheet image of starfish, LUCYD recovers more details and structures than RLN while maintaining low levels of noise and haze surrounding the object in both lateral and axial projections. On the other test sample of a fixed U2OS cell acquired by widefield microscopy, LUCYD also suppresses noise and haze to a higher degree compared to RLN and CARE and retrieves finer and sharper details."
LUCYD: A Feature-Driven Richardson-Lucy Deconvolution Network,4,Conclusion,"In this paper, we introduce LUCYD, an innovative technique for deconvolving volumetric microscopy images that combines a classic image deconvolution formula with a U-shaped network. LUCYD takes advantages of both approaches, resulting in a highly efficient method capable of processing 3D data with high efficacy. We have demonstrated through experiments on both synthetic and real microscopy datasets that LUCYD exhibits strong generalization capabilities, as well as robustness to noise. These qualities make it an excellent tool for crossdomain applications in various domains, such as biology and medical imaging. Additionally, the lightweight nature of LUCYD makes it computationally feasible for real-time applications, which can be crucial in various settings."
Dynamic Structural Brain Network Construction by Hierarchical Prototype Embedding GCN Using T1-MRI,2.1,Backbone,"In this study, we utilize a Convmixer-like [24] block as the backbone to achieve primary discriminative brain regions localization, which could provide a large enough channel dimension for subsequent channel clustering with relatively low complexity. Specifically, depicted in Fig. 1(A), the backbone consists of a patch embedding layer followed by several full-convolution blocks. Patch embedding comprises a 5 × 5 × 5 convolution, and the full convolution block comprises a 5 × 5 × 5 depthwise convolution (grouped convolution with groups equal to the number of channels) and a pointwise convolution (kernel size is 1 × 1 × 1) with 2048 channels. By the backbone, features of discriminative regions are finally extracted as F b ∈ R C×D×H×W , where D, H, W and C indicate depth, height, width and the number of channels, respectively."
Dynamic Structural Brain Network Construction by Hierarchical Prototype Embedding GCN Using T1-MRI,2.2,Dynamic Hierarchical Prototype Learning,"Prototypes Definition. In this study, we regard feature maps of each channel as corresponding to the response of distinct brain regions relevant to tasks, and cluster spatially-correlated subtle patterns as compact and discriminative parts from a group of channels whose peak responses appear in neighboring location following [30]. Intuitively, we utilize the location of each peak response as channel information, which can be represented as a position vector whose elements are coordinates from peak responses over all training images. The position vector of the same channel of all training images are combined as the candidate prototype, which can be obtained as following:whererepresents the peak response coordinate of the i-th image and Ω represents the number of images in the training set. K-means [17] is used to achieve prototypes initialization. Specifically, vectors of all channels are clustered to obtain N sets of clusters K = {k n } N n=1 , and prototypes are defined as clustering centers Γ = {γ n } N n=1 which are taken as N critical regions for the discriminative localization (i.e., ROIs). F h ∈ R N ×D×H×W represents features of clustering centers.Dynamic Hierarchical Prototype Exploring. Inter-regional spatial connectivity is fixed, but the correlation between them is dynamic with disease progression. We argue that there are structural correlations between different regions, just as the complex hierarchical functional connectome in rich-clubs [25] organization with fMRI. We therefore explore the hierarchical semantic structure of critical brain regions by the hierarchical prototype clustering method. Specifically, we start by using the initial prototypes as the first hierarchy clustering prototypes, denoted as Γ 0 = {γ 0 n } N0 n=1 . Then, K-means is applied iteratively to obtain parent prototypes of the lower-hierarchy prototypes, where i represents the i-th hierarchy and N i represents the number of clusters at i-th hierarchy, corresponding to the clusterIn this paper, i is set as 2. The number of prototypes in the first, second and third hierarchy is set as 16, 8 and 4, respectively.To facilitate optimal clustering of the network during training, we use two fully convolutional layers with two contrastive learning loss functions L node and L edge to approximate the clustering process. With L node , each channel clustering is enforced to become more compact inside and have significant inter-class differences with other clusterings, enabling all prototypes to be well separated:where L is the total number of layers, and N l is the number of clusters in the l-th layer. K l n , γ l n , and φ l n denote the set of all elements, the cluster center (prototype), and the estimation of concentration of the n-th cluster in the l-th layer, respectively. α is a smoothing parameter to prevent small clusters from having overly-large φ.The cluster concentration φ measures the closeness of elements in a cluster. A larger φ indicates more elements in the cluster or smaller total average distance between all elements and the cluster center. Ultimately, L node compels all elements u in K l n to be close to their cluster center γ l n and away from other cluster center at the same level.Similarly, L edge aims to embed the hierarchical correlation between clustering prototypes, which can be expressed as:P arent(γ l n ) represents the parent prototype of the prototype γ l n , and τ is a temperature hyper-parameter. L edge forces all prototypes γ l in the l-th layer to be close to their parent prototype and away from other prototypes within the same level."
Dynamic Structural Brain Network Construction by Hierarchical Prototype Embedding GCN Using T1-MRI,2.3,Brain Network Graph Construction and Classification,"Through Sect. 2.2, critical brain regions are clustered in a hierarchical semantic latent space. We hereby employ the prototypes regions as nodes and correlations between them as edges to construct structural brain network graphs.We first apply a self-attention mechanism [26] to compute inter-region correlations to generate edges of the brain network. Then, the features F h is input to three separate fully connected layers to obtain three vectors: query, key and value, which are used to compute attention scores A ∈ R N ×N between each pair of prototypes, followed by being used to weight the value vector and obtain the output of the self-attention layer as following operation:wheredenote query, key, and value, respectively. d k represents the dimension of Q, K. N represents the number of critical regions, which is set as 16 in this paper.We then employ GCN to capture the topological interaction in the brain network graph and update features of nodes by performing the following operation:where Â = A+I is the adjacency matrix with inserted self-loops and I denotes an identity matrix. Dii = j=0 Âij is the diagonal degree matrix, and Θ represents learned weights. To prevent the network overfitting, we just use two GCN layers as the encoder to obtain the final graph feature F g ∈ R N ×D×H×W . To achieve the classification, we perform channel squeezing on the backbone feature F b to obtain global features F se ∈ R 1×D×H×W , concatenate it with F g and input them into the classification layer.To this end, the information of critical brain regions are fully learned. Notably, as prototypes are dynamic, constructed brain network graphs are also dynamic, rather than predefined and fixed. This allows DH-ProGCN to model and explore the individual hierarchical information, providing a more personalise brain network representation for every subject."
Dynamic Structural Brain Network Construction by Hierarchical Prototype Embedding GCN Using T1-MRI,3.1,Dataset,"The data we used are from two public databases: ADNI-1 (http://www.adniinfo.org) and ADNI-2 [20]. The demographic information of the subjects and preprocessing steps are shown in the supplemental material. The images are finally resized to 91 × 109 × 91 voxels. Through the quality checking, 305 images are left from ADNI-1 (197 for sMCI, 108 for pMCI), and 350 images are left from ADNI-2 (251 for sMCI, 99 for pMCI). Note that we only keep the earliest images for those who have more than two images at different times. Following [15], we train DH-ProGCN on ADNI-1 and perform independent testing on ADNI-2."
Dynamic Structural Brain Network Construction by Hierarchical Prototype Embedding GCN Using T1-MRI,3.2,Implementation Details,"We first train backbone with 2048 channels in all layers to extract the output features F b with the cross-entropy loss L cls1 . Then the clustering results are initialized by K-means and further optimized by CCL with L node and L edge . Finally, The cross-entropy loss L cls2 is used for the final classification. The overall loss function is defined as:where L node and L edge are explained in Sect. 2.2. Smooth parameter α = 10 and temperature parameter τ = 0.2 following [3]. All blocks are trained by SGD optimizer with a momentum of 0.9 and weight decay of 0.001. The model is trained for 300 epochs with an initial learning rate of 0.01 that is decreased by a factor of 10 every 100 epochs. Four metrics, namely accuracy (ACC), sensitivity (SEN), specificity (SPE), and area under the curve (AUC), are used to evaluate the performance of the proposed model. We use Python based on the PyTorch package and run the network on a single NVIDIA GeForce 3090 24 GB GPU. Source code is available at https://github.com/Leng-10/DH-ProGCN."
Dynamic Structural Brain Network Construction by Hierarchical Prototype Embedding GCN Using T1-MRI,4.1,Comparing with SOTA Methods,Six SOTA methods are used for comparison: 1) LDMIL [16] captured both local information conveyed by patches and global information; 2) H-FCN [15] implemented three levels of networks to obtain multi-scale feature representations Table 1. Comparsion of our method with current SOTA methods for MCI conversion prediction on ADNI-2 obtained by the models trained on ADNI-1. which are fused for the construction of hierarchical classifiers; 3) HybNet [14] assigned the subject-level label to patches for local feature learning by iterative network pruning; 4) AD 2 A [10] located discriminative disease-related regions by an attention modules; 5) DSNet [19] provided disease-image specificity to an image synthesis network; 6) MSA3D [2] implemented a slice-level attention and a 3D CNN to capture subject-level structural changes.
Dynamic Structural Brain Network Construction by Hierarchical Prototype Embedding GCN Using T1-MRI,,Method,"Results in Table 1 show the superiority of DH-ProGCN over SOTA approaches for MCI conversion prediction. Specifically, DH-ProGCN achieves ACC of 0.849 and AUC of 0.845 tested on ADNI-2 by models trained on ADNI-1. It is worth noting that our method: 1) needs no predefined manual landmarks, but achieves better diagnostic results than existing deep-learning-based MCI diagnosis methods; 2) needs no pretrain network parameters from other tasks like AD diagnosis; 3) introduces hierarchical distribution structure to connect regions and form region-based specificity brain structure networks, rather than generalizing the correlations between regions with global information."
Dynamic Structural Brain Network Construction by Hierarchical Prototype Embedding GCN Using T1-MRI,4.2,Ablation Study,"Effect of Dynamic Prototype Learning. To verify the effect of dynamic prototype clustering, we compare 1) ROI-based approach [29], 2) backbone without channel clustering (BL), 3) backbone with dynamic prototypes clustering (BL+L node ). As shown in Fig. 2, results indicate that dynamic prototype clus-  [27] is used to generate these figures with the peak response coordinates of cluster centers and the correlation matrix between them mathbf A as node and edge features. The size of node increases with its hierarchy, and nodes with same color are clustered into the same parent prototype. Lower-hierarchy prototypes within cluster are closer to its parent prototypes, and higher-hierarchy prototypes between different clusters are closer than lower-hierarchy prototypes tering outperforms the ROI-based and backbone on MCI conversion, and could generate better feature distributions for downstream brain images analysis tasks.Effect of Hierarchical Prototype Learning. To evaluate the impact of hierarchical prototype learning, we compare backbone with flattened prototypes clustering (BL+L node ), and hierarchical clustering (BL+L node +L edge ). The results are presented in Fig. 2. With the constraint strengthened on the distribution of regions, the results are progressively improved. This implies that it makes sense to introduce hierarchical semantics into the construction of structure brain networks.Effect of Dynamic Brain Network Construction. To verify whether our constructed dynamic brain network capability outperforms the fixed architecture, we obtained the fixed brain network graph by directly connecting all critical regions after obtaining hierarchical features and feeding them into the GCN for classification. The results are shown in Fig. 2, where the dynamic brain network structure performs better, suggesting that the correlation between regions needs to be measured dynamically to construct a better brain network.In addition, we visualize the sagittal, coronal and axial views of hierarchical critical regions and their connectome in Fig. 3, which is based on graphs before GNN. The thickness of edges represents the correlation coefficient between nodes, i.e., the connected strength between brain regions. Localized regions are roughly distributed in anatomically defined parahippocampal gyrus, superior frontal gyrus, and cingulate gyrus for different sMCI subjects, lingual gyrus right, and superior longitudinal fasciculus for different pMCI subjects, which agree with previous studies. [5,7,9]. In general, critical regions and correlations are varied for different subjects, indicating the proposed network is a subject-wise dynamic model that processes each subject using data-dependent architectures and parameters."
Dynamic Structural Brain Network Construction by Hierarchical Prototype Embedding GCN Using T1-MRI,5,Conclusion,"In this paper, we propose a novel dynamic structural brain network construction method named DH-ProGCN. DH-ProGCN could dynamically cluster critical brain regions by the prototype learning, implicitly encode the hierarchical semantic structure of the brain into the latent space by hierarchical prototypes embedding, dynamically construct brain networks by self-attention and extract topology features in the brain network by GCN. Experimental results show that DH-ProGCN outperforms SOTA methods on the MCI conversion task. Essentially, DH-ProGCN has the potential to model hierarchical topological structures in other kinds of medical images. In our future work, we will apply this framework to other kinds of modalities and neurological disorders."
Dynamic Structural Brain Network Construction by Hierarchical Prototype Embedding GCN Using T1-MRI,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3_12.
Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,1,Introduction,"Machine learning (ML), specifically deep learning (DL), algorithms have shown exceptional performance on numerous medical image analysis tasks [2]. Never-theless, comprehensive reviews highlight major issues of generalizability, robustness, and reproducibility in medical imaging AI/ML [9,15]. For a generalizability assessment, reporting only aggregate performance measures is not sufficient. Due to model complexity and limited training data, ML performance often varies across data subgroups or domains, such as different patient subpopulations or varied data acquisition scenarios. Aggregate performance measures (e.g., sensitivity, specificity, ROC AUC) can be dominated by the larger subgroups, masking the poor ML model performance on smaller but clinically important subgroups [11]. Thus, achieving (through training) and demonstrating (as part of testing) satisfactory ML model performance across relevant subgroups is crucial before the real-world clinical deployment of a medical ML system [13].However, a challenging situation arises when relevant subgroups are unrecognized. One solution to this issue is to apply a clustering algorithm to the data, with the goal of identifying the unannotated subgroups. The main objective of unsupervised clustering is to group data points into distinct classes of similar traits. However, due to the complexity and high dimensionality of the medical imaging data and the resulting difficulty in establishing a concrete notion of similarity, extracting low-dimensional characteristics becomes the key to establishing the best criteria for grouping. Unsupervised generative clustering aims to simultaneously address both domain identification and dimensionality reduction. Deep unsupervised clustering algorithms could map the medical imaging data back to their causal factors or underlying domains, such as image acquisition equipment, patient subpopulations, or other meaningful data subgroups. However, there is a practical need to be able to guide the deep clustering model towards the identification of grouping structures in a given dataset that have not been already annotated. To that end, we propose a mechanism that is intended to constrain the model towards identifying clusters in the data that are not associated with given variables of choice (already known class labels or subgroup structures). The resulting algorithmic cluster assignments could then be used to improve ML algorithm training, or for generalizability and robustness evaluation."
Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,2,Methods,"We provide a PyTorch-based implementation of all deep clustering algorithms described below (VaDE, CDVaDE, and DEC) in the open source Python package DomId that is publicly available under https://github.com/DIDSR/DomId."
Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,2.1,Variational Deep Embedding (VaDE),"Variational Deep Embedding (VaDE) [6] is an unsupervised generative clustering approach based on Variational Autoencoders [10]. In our study, VaDE is deployed as a deep clustering model using Convolutional Neural Network (CNN) architectures for the encoder g(x; φ) and the decoder f (z; θ). The encoder learns to compress the high-dimensional input images x into lower-dimensional latent representations z. Using a Mixture-of-Gaussians (MOG) prior distribution for the latent representations z, we examine subgroups or domains within the dataset, revealed by the individual Gaussians within the learned latent space, and how z affects the generation of x. The model can be used to perform inference, where observed images x are mapped to corresponding latent variables z and their cluster/domain assignments c. We denote the latent space dimensionality by d (i.e., z ∈ R d ), and the number of clusters by D (i.e., c ∈ {1, 2, . . . , D}). The trained decoder CNN can also be used to generate synthetic images from the algorithmically identified subgroups.VaDE is optimized using Stochastic Gradient Variational Bayes [10] to maximize a statistical measure called the Evidence Lower Bound (ELBO). We denote the true data distribution by p(z, x, c) and the variational posterior distribution by q(z, c|x). The ELBO of VaDE can be written aswhere p(x|z) is modeled by the decoder CNN, and q(z|x) is modeled by the encoder CNN g(x; φ) asFinally, the cluster assignments can be determined viawhere the probability distributions p(c) and p(z|c) come from the MOG prior of the latent space, with the respective distributional parameters π, μ c , σ 2 c (for c ∈ {1, 2, . . . , D}) optimized by maximizing the ELBO of Eq. ( 1). Note that Eq. ( 2) follows from the observation that in order to maximize the ELBO in Eq. 1, the KL Divergence between q(c|x) and p(c|z) needs to be equal to 0. We refer to [6] for details.In all our experiments, we apply VaDE with CNN architectures for the encoder and decoder. The CNN encoder consists of convolution layers with 32, 64, 128 filters, respectively, followed by a fully-connected layer. Respectively, the CNN decoder consists of a fully-connected layer followed by transposed convolution layers with the number of input/output channels decreasing as 128, 64, 32, 3. Batch normalization and the leaky ReLU activation functions are used. "
Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,2.2,Conditionally Decoded Variational Deep Embedding (CDVaDE),"We propose the Conditionally Decoded Variational Deep Embedding (CDVaDE) model as an extension to VaDE as shown in Fig. 1. The generative process of CDVaDE differs from VaDE in that it concatenates additional variables y to the latent representation z. For example, y may contain the available class labels or already known subgroup structures, which do not need to be discovered. It is assumed that these additional variables y are available at training and test time. Specifically, the generative process of CDVaDE takes the formSince our goal is to find clusters c that are unassociated with the available variables y of choice and to learn latent representations z that do not contain information about y, the generative process of CDVaDE assumes that z, c are jointly independent of y.The changes compared to the generative process of VaDE can also be regarded as imposing a structure on the model, where the encoder learns hidden representations of the image x conditioned to the additional variables y (i.e., q(z|x, y)), but acts as an identity function with respect to y (i.e., y can be regarded as being simply concatenated to the latent space representations z). The decoder then translates this data representation in the form of (z, y) to the input space (i.e., p(x|z, y)). Given that the underlying VAE architecture seeks to efficiently compress the input data x into a learned representation, this incentivizes the model to exclude information about y from the learned variables z and c.The ELBO of CDVaDE can be derived as follows,where we use the fact that by the generative process of CDVaDE it holds that p(x, z, c|y) = p(x|z, y)p(z|c, y)p(c|y) = p(x|z, y)p(z|c)p(c),and we adopt from VaDE the assumption that q(z, c|x) = q(z|x)q(c|x) holds. Hence, once the base VaDE decoder CNN is replaced by its modified version f (z, y; θ) in CDVaDE, there are no further differences between the ELBO loss function of Eq. ( 8) compared to Eq. ( 1).While in this work we present our conditioning mechanism as an extension to VaDE, it can be combined with any deep clustering algorithm that follows an encoder-decoder architecture. In all our experiments, we use the same CNN architectures for the encoder and decoder as in VaDE (see Sect. 2.1)."
Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,2.3,Deep Embedding Clustering (DEC),"Deep Embedding Clustering (DEC) [14] is a popular state-of-the-art clustering approach that combines a deep embedding model with k-means clustering. In this study, we include comparisons of VaDE and the proposed CDVaDE to DEC, because it is a model that belongs to a different family of deep clustering algorithms which are not based on variational inference. In our DEC experiments, we use the same autoencoder architecture and the same initialization as for the VaDE."
Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,2.4,Related Works in Medical Imaging,"A number of studies have been conducted with several approaches of deep clustering for medical imaging data. Typically, clustering is performed on top of features extracted with the use of an encoder neural network, and the cluster assignments are determined by using conventional clustering algorithms, such as k-means, on top of the learned latent representations [1,5,7,12]. In contrast, this work investigates models which enforce a clustering structure in the latent space through the use of a MOG prior distribution, as well as guidance of the clustering model via the proposed conditioning mechanism."
Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,3.1,Colored MNIST,"The Colored MNIST is an extension to the classic MNIST dataset [3], which contains binary images of handwritten digits. The Colored MNIST includes colored images of the same digits, where each number and background have a color assignment. We present results of the experiments with five distinct colors and five digits of MNIST (0-4). To enhance computational efficiency and expedite experiments, we utilized only 1% of the MNIST images, which were sampled at random. This simple dataset can be used to investigate whether a given clustering algorithm will categorize the images by color or by the digit label and whether the proposed conditioning mechanism of CDVaDE can successfully guide the clustering away from the categorization we want to avoid (e.g., condition the model to avoid clustering by color, in order to distinguish the digits in an unsupervised fashion). We compare CDVaDE to the deep clustering models VaDE and DEC that do not incorporate such conditioning. We use latent space dimensionality d = 20 for all models. In Fig. 2 a summary of the results for the experiments on the colored MNIST dataset is presented. The results demonstrate that by allowing for the incorporation of additional information, particularly color labels, the proposed CDVaDE model is more sensitized to learning other underlying features, which allows for distinguishing between the different digits in this particular example. Notably, both VaDE and DEC end up clustering the data by color, as it is the most striking distinguishing characteristic of these images. On the other hand, the predicted domains of CDVaDE have no association with color, and the data are separated by the shapes in the images, distinguishing some of the digit labels (albeit imperfectly). This example serves as a proof of concept for the proposed conditioning mechanism of CDVaDE."
Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,3.2,Application to a Digital Pathology Dataset,"HER2 Dataset. Human epidermal growth factor receptor 2 (HER2 or HER2/neu) is a protein involved in normal cell growth, which plays an important role in the diagnosis and treatment of breast cancer [8]. The dataset consists of 241 patches extracted from 64 digitized slides of breast cancer tissue which were stained with HER2 antibody. Each tissue slide has been digitized at three different sites using three different whole slide imaging systems, evaluated by 7 pathologists on a 0-100 scale, and following clinical practice labeled as HER2 Class 1, 2, or 3 (based on mean pathologists' scores with cut-points at 33 and 66). We use a subset of this dataset consisting of 672 images (the remainder is held out for future research). Because the intended purpose is finding subgroups in the given dataset only, a separate test set is not used. The dimensions of the images vary from 600 to 826 pixels, and we scale all data to a uniform size of 128 × 128 pixels before further processing. We refer to [4,8] for more details about this dataset. This retrospective human subject dataset has been made available to us by the authors of the prior studies [4,8], who are not associated with this paper. Appropriate ethical approval for the use of this material in research has been obtained.Deep Clustering Models Applied to the HER2 Dataset. We evaluate the performance and behavior of the DEC, VaDE, and CDVaDE models on the HER2 dataset. We investigate whether the models will learn to distinguish the HER2 class labels, the scanner labels, or other potentially meaningful data subgroups in a fully unsupervised fashion. To investigate the clustering abilities of CDVaDE on the HER2 dataset, we inject the HER2 class labels into the latent embedding space. We hypothesize that this will disincentivize the encoder network from including information related to the HER2 class labels in the latent representations z. Thus, with CDVaDE we aim to guide the clustering towards identifying subgroup structures that are not associated with the HER2 classes, and potentially were not previously recognized. The dimensionality of the latent embedding space was set to d = 500 for all three models.  As illustrated by the bar graphs in Fig. 3, there is an association between HER2 class 2 and predicted domain 2, as well as between HER2 class 3 and predicted domain 3. Similarly to the VaDE model, the DEC model has also shown the ability to separate between HER2 class 2 and HER2 class 3. To investigate these observations further, we look at the distribution of the ground truth HER2/neu scores within each of the predicted domains. The boxplots in Fig. 4 show that both the VaDE and DEC models tend to separate high HER2/neu scores from the lower ones. The Pearson's correlation coefficient between the clustering assignments c of VaDE and the HER2/neu scores is 0.46. The correlation coefficient between the DEC clusters and the HER2/neu scores is 0.71. However, neither VaDE nor DEC clusters are associated to the scanner labels. We investigate the proposed CDVaDE model with the goal of identifying meaningful data subgroups which are not associated with the already known HER2 class labels. As visualized in Fig. 3, the predicted domains are again clearly visually disparate. However, as intended, there is a weaker association with the HER2 class labels and a stronger association with the scanner labels, compared to the results of VaDE and DEC. In Fig. 4, HER2/neu median scores of the three clusters move closer together, illustrating the decrease of association with HER2 class labels, as intended by the formulation of the CDVaDE model. The correlation coefficient between the CDVaDE cluster assignments and the HER2/neu scores is 0.39. While the CDVaDE model does not achieve full independence between the identified clusters and the HER2 labels, it decreases this association compared to VaDE and DEC. Moreover, the clusters identified by CDVaDE are distinctly different from those of VaDE, with a 0.43 proportion of agreement between the two algorithms (after matching the two sets of cluster assignments using the Hungarian algorithm)."
Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,4,Conclusion,"We investigated deep clustering models for the identification of meaningful subgroups within medical image datasets. The proposed CDVaDE model incorporates a conditioning mechanism that is capable of guiding the clustering model away from subgroup structures that have already been annotated and towards the identification of yet unrecognized image subgroups/domains. Our experimental findings on the HER2 digital pathology dataset surmise that VaDE and DEC are capable of finding, in an unsupervised fashion, image subgroups related to the HER2 class labels, while CDVaDE (conditioned on the HER2 labels) identifies visually distinct subgroups that have a weaker association to the HER2 labels. Because the CDVaDE clusters do not clearly correspond to the scanner labels either, future work involves a review by a pathologist to see whether these subgroups capture meaningful but unannotated characteristics in the images. While CDVaDE can be used as an exploratory tool to unveil unknown subgroups in a given dataset, developing specialized quantitative evaluation metrics for this unsupervised task is inherently difficult and will also be a focus in our future work."
Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3 64.
Relaxation-Diffusion Spectrum Imaging for Probing Tissue Microarchitecture,1,Introduction,"Recent advances in diffusion MRI (dMRI) and diffusion signal modeling equip brain researchers with an in vivo probe into microscopic tissue compositions [15,21]. Signal differences between water molecules in restricted, hindered, and free compartments can be characterized by higher-order diffusion models for estimating the relative proportions of cell bodies, axonal fibers, and interstitial fluids within an imaging voxel. This allows for the detection of tissue compositional changes driven by development, degeneration, and disorders [13,22]. However, accurate characterization of tissue composition is not only affected by compartment-specific diffusivities but also transverse relaxation rates [4,27]. Several studies have shown that explicit consideration of the relaxation-diffusion coupling may improve the characterization of tissue microstructure [6,16,25].Multi-compartment models are typically used to characterize signals from, for example, intra-and extra-neurite compartments [18,29]. However, due to the multitude of possible compartments and fiber configurations, solving for these models can be challenging. The problem can be simplified by considering per-axon diffusion models [8,10,28], which typically factor out orientation information and hence involve less parameters. However, existing models are typically constrained to data acquired with a single TE (STE) and do not account for compartment-specific T 2 relaxation. Several studies have shown that multi-TE (MTE) data can account better for intravoxel architectures and fiber orientation distribution functions (fODFs) [1,6,16,17,19].Here, we propose a unified strategy to estimate using MTE diffusion data (i) compartment specific T 2 relaxation times; (ii) non-T 2 -weighted (non-T 2 w) parameters of multi-scale microstructure; and (iii) non-T 2 w multi-scale fODFs. Our method, called relaxation-diffusion spectrum imaging (RDSI), allows for the direct estimation of non-T 2 w volume fractions and T 2 relaxation times of tissue compartments. We evaluate RDSI using both ex vivo monkey and in vivo human brain MTE data, acquired with fixed diffusion times across multiple b-values. Using RDSI, we demonstrate the TE dependence of T 2 w fODFs. Furthermore, we show the diagnostic potential of RDSI in differentiating tumors and normal tissues."
Relaxation-Diffusion Spectrum Imaging for Probing Tissue Microarchitecture,2.1,Multi-compartment Model,"The diffusion-attenuated signal S(τ, b, g) acquired with TE τ , diffusion gradient vector g, and gradient strength b can be modeled aswhich can be expanded to a multi-compartment model:to account for signals S r (b, g), S h (b, g), and S f (b) and T 2 values of restricted, hindered, and free compartments. The apparent relaxation rates at different b-values, r(b) = 1/T 2 (b), can be estimated using single-shell data acquired with two or more TEs [14]. This model can be expressed using spherical deconvolution [9]:where the compartment-specific response functions R(b, g, D r ), R(b, g, D h ), and R(b, D f ) are associated with apparent diffusion coefficients D r , D h , and D f , yielding compartment-specific multi-scale fODFs f (D r ), f (D h ), and f (D f ). Operator A relates the spherical harmonics coefficients to fODF amplitudes."
Relaxation-Diffusion Spectrum Imaging for Probing Tissue Microarchitecture,2.2,Model Simplification via Spherical Mean,"The spherical mean technique (SMT) [10] focuses on the direction-averaged signal to factor out the effects of the fiber orientation distribution. Taking the spherical mean, (3) can be written aswhere w(D r ), w(D h ), and w(D f ) are volume fractions and k(b, D r ), k(b, D h ), and k(b, D f ) are spherical means of response functions R(b, g, D r ), R(b, g, D h ), and R(b, D f ), respectively. Based on [8,10], spherical means can be written as:where D r , D h , and D f are parameterized by parallel diffusivity λ and perpendicular diffusivity λ ⊥ for the restricted (Λ r ), hindered (Λ h ) and free (Λ f ) compartments. φ is the geometric tortuosity [28]. The spherical mean signal can thus be seen as the weighted combination of the spherical mean signals of spin packets. Similar to [8,28], (4) allows us to probe the relaxation-diffusion coupling across a spectrum of diffusion scales. Anisotropic diffusion can be further separated as restricted or hindered."
Relaxation-Diffusion Spectrum Imaging for Probing Tissue Microarchitecture,2.3,Estimation of Relaxation and Diffusion Parameters,"We first solve for the relaxation modulated spherical mean coefficients in (4). Next, we disentangle the relaxation terms from the spherical mean coefficients and solve for the relaxation rates. Finally, we estimate the fODFs using (3). Details are provided below:(i) Relaxation modulated spherical mean coefficients. We rewrite (4) in matrix form aswhere the mean signal S is expressed as the product of the response function spherical mean matrix K and the Kronecker product (•) of relaxation matrix E and volume fraction matrix W. We can solve for X in ( 6) via an augmented problem with the OSQP solver 1 :(ii) Relaxation times. With X solved, E and W can be determined by minimizing a constrained non-linear multivariate problem:which can be solved using a gradient based optimizer. Relaxation times can be determined based on E.(iii) fODFs. With E determined, (3) can be rewritten as a strictly convex quadratic programming (QP) problem:which can be solved using the OSQP solver."
Relaxation-Diffusion Spectrum Imaging for Probing Tissue Microarchitecture,2.4,Microstructure Indices,"Based on (3) and ( 4), various microstructure indices can be derived:-Microscopic fractional anisotropy [20], per-axon axial and radial diffusivity [2], and free and restricted isotropic diffusivity. -Axonal morphology indices derived based on [17,26] to compute the mean neurite radius (Mean NR), its internal deviation (Std. NR), and relative neurite radius (Cov. NR):, where 0 is a pulse scale that only depends on the pulse width δ and diffusion time Δ of the diffusion gradients.), which is independent on ."
Relaxation-Diffusion Spectrum Imaging for Probing Tissue Microarchitecture,2.5,Data Acquisition and Processing,"Ex Vivo Data. We used an ex vivo monkey dMRI dataset2 collected with a 7T MRI scanner [19]. Implementation. To cover the whole diffusion spectrum, we set the diffusivity from 0 s/mm 2 (no diffusion) to 3 × 10 -3 s/mm 2 (free diffusion). For the anisotropic compartment, λ was set from 1.5 × 10 -3 mm 2 /s to 2 × 10 -3 mm 2 /s. Radial diffusivity λ ⊥ was set to satisfy λ /λ ⊥ ≥ 1.1 as in [8,28]. For the isotropic compartment, we set the diffusivity λ = λ ⊥ from 0 mm 2 /s to 3×10 -3 mm 2 /s with step size 0.1×10 -3 mm 2 /s. "
Relaxation-Diffusion Spectrum Imaging for Probing Tissue Microarchitecture,3.1,Ex Vivo Data: Compartment-Specific Parameters,"Figure 1(a) shows the estimated maps of T 2 -independent parameters given by a baseline comparison method, called REDIM [16], and RDSI. We observe that the two methods yield similar intracellular volume fraction (ICVF) estimates. However, REDIM overestimates the anisotropic volume fraction (AVF) compared to RDSI, resulting in blurred boundaries between the gray matter and superficial white matter. RDSI yields consistent distribution between ICVF and μFA maps. Figure 1(b) shows the RDSI T 2 relaxation maps of restricted, hindered, and free diffusion across b-values. As the b-value increases, the relaxation time increases for the restricted component but decreases for the hindered and free components. At lower b-values, the relaxation time for the extra-neurite compartment is substantially higher than that of the intra-neurite compartment."
Relaxation-Diffusion Spectrum Imaging for Probing Tissue Microarchitecture,3.2,In Vivo Data: Compartment-Specific Parameters,"Figure 2 shows the RDSI T 2 relaxation maps of restricted, hindered, and free diffusion across b-values. The values are consistent between healthy and glioma subjects. The estimated relaxation times are in general in line with previous reports [6,11]. RDSI shows substantial differences between tumor and normal tissues in the relaxation maps (Fig. 2(b)).Figure 3 shows the voxel distributions with respect to relaxation times and b-values. It is apparent that at higher b-values, a greater fraction of voxels in the restricted compartment have relaxation times within 100 to 200 ms, particularly for higher-grade gliomas. This might be related to prolonged transverse relaxation time due to increased water content within the tumor [5,7,24]. This property is useful in the visualization of peritumoral edema, an area containing infiltrating tumor cells and increased extracellular water due to plasma fluid leakage from aberrant tumor capillaries that surrounds the tumor core in higher-grade gliomas. "
Relaxation-Diffusion Spectrum Imaging for Probing Tissue Microarchitecture,3.3,In Vivo Data: Neurite Morphology,"Figure 4(a) shows the relaxation times of the restricted compartment in white matter lesions, indicating that relaxation times are longer in gliomas than normal white matter tissue. The higher T 2 in grade 4 glioma is associated with changes in metabolite compositions, resulting in remarkable changes in neurite morphology in lesioned tissues (Fig. 4(c-d)), consistent with previous observations [12,23]. The rate of longitudinal relaxation time has been shown to be positively correlated with myelin content. Our results indicate that MTE dMRI is more sensitive to neurite morphology than STE dMRI (Fig. 4(b)).Figures 4(c-d) show that the estimated Mean NR in the gray matter is approximately in the range of 10 µm, which is in good agreement with the sizes of somas in human brains, i.e., 11 ± 7 µm [26]. RDSI improves the detection of small metastases, delineation of tumor extent, and characterization of the intratumoral microenvironment when compared to conventional microstructure models (Fig. 4(c)). Our studies suggest that RDSI provides useful information on microvascularity and necrosis helpful for facilitating early stratification of patients with gliomas (Fig. 4(d)). "
Relaxation-Diffusion Spectrum Imaging for Probing Tissue Microarchitecture,3.4,Relation Between Relaxation and Diffusivity,"Figure 5 shows the relaxation-diffusivity distributions of white matter (WM), cortical gray matter (GM), and subcortical gray matter (SGM). The 2D plots show the contours of the joint distributions of the relaxation and diffusivity values across all voxels. The average diffusivity and relaxation in these regions indicate the existence of a single homogeneous region in WM and SGM. For GM, however, we observe a small peak for the relaxation rate arange 1e-3 to 1.5e-3."
Relaxation-Diffusion Spectrum Imaging for Probing Tissue Microarchitecture,3.5,fODFs,"Figure 6 shows that the reconstructed fODFs are consistent with the expected WM arrangement of the healthy human brain. We provide a visual comparison of the fODFs estimated with and without the explicit consideration of relaxation. The two cases yield different fODFs. As expected, fiber populations are associated with different relaxation times, in line with [3,16]. Our studies suggest that this difference could be caused by the spatially heterogeneous tissue microstructure, since fiber bundles with slower relaxation times contribute less to diffusion signals acquired with a longer TE. Explicitly taking into account relaxation in our model results in noteworthy contrast improvement in spatially heterogeneous superficial WM.   "
Relaxation-Diffusion Spectrum Imaging for Probing Tissue Microarchitecture,4,Conclusion,"RDSI provides a unified strategy for direct estimation of relaxation-independent volume fractions and compartment-specific relaxation times. Using MTE data, we demonstrated that RDSI can delineate heterogeneous tissue microstructure elusive to STE data. We also showed that RDSI provides information that is conducive to characterizing tissue abnormalities."
Weakly-Supervised Drug Efficiency Estimation with Confidence Score: Application to COVID-19 Drug Discovery,1,Introduction,"COVID-19 is an infectious disease caused by Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2); that is a novel virus from the genus of Betacoronovirus of coronavirus genera. In extreme cases, host cells activate intensive immune responses for defense, leading to Acute respiratory distress syndrome (ARDS) and multiple organ failure [2,18]. Though modern vaccines against this disease effectively prevent severity, hospitalization, and death, the SARS-CoV-2 virus has continued to mutate, and there is still a chance of showing severe symptoms, especially for those with chronic diseases [2]. Yet, we have a few medications that affect this tough disease. Hence search for treatment has never stopped.Designing a new drug is time-consuming, while drug repurposing can be a real game-changer when it comes to a pandemic. Every newly designed drug candidate should go through a filter of preclinical research, clinical trials, and FDA reviews to gain safety permission to be on the market. Many drugs filter out before reaching the market, while this journey takes about 7-12 years for those few that last [19]. Drug repurposing accelerates this process by reusing approved chemical compounds for a new target. Because drug safety and efficiency have initially been tested in another clinical study, getting approval for a new purpose is faster for approved medicines [12,14,23].High Throughput Screening (HTS) has proven its worth in facilitating drug repurposing [8]. HTS methodology relies on the fact that chemical compounds change the morphology of cells. In particular, chemical compounds with similar molecular structures are expected to induce similar cellular morphological changes [3,15]. These changes can be tracked and analyzed to identify the compound hits capable of reversing the cell morphologies impacted by the disease.RxRx19a is an extensive HTS experiment conducted by the Recursion biotechnology company to investigate potential therapeutics of approved drugs for COVID-19 treatment [7]. To our knowledge, RxRx19a is the biggest public fluorescent microscopy dataset exploring approved drugs' effects on SARS-CoV-2 virus. Based on the substances that are added to the cultured cells, samples in this study can be categorized into negative controls (mock and UV-irradiated SARS-CoV-2), positive controls (active SARS-CoV-2), and treatments (contaminated drugged).Previous studies for hit discovery on this dataset used positive and negative control samples to train a model to discriminate healthy from infected cell image representations [7,11,16]. The trained model was then used to estimate treatment scores. Although adequate evidence ensures that positive and negative controls are separate in embedding space, the top hit-score drugs published by these works are rarely overlapped. Besides, previous algorithms falsely predicted many treatments as effective, including toxic doses. We argue that this phenomenon is due to the fact that although cellular morphological changes caused by drugs are not visible to human eyes, they are so drastic that can confuse the model. In other words, treatment samples that have never been seen in the training of models are out-of-distribution samples to the trained model distribution, and we need a measure of confidence to rely on model judgment on them [5,10,22].Here, we present a novel weakly supervised confident hit prediction pipeline that estimates disease scores regardless of drug side effects and provides a confi-dence score for its prediction. We applied our model on both CellProfiler features [17], and deep neural network embeddings and showed that consensus between discovered hits increases. Our contributions are:-We are the first to address the hit-discovery methodology's out-of-distribution problem and solve it with a novel weakly-supervised confident hit prediction pipeline. -We exhaustively evaluate our pipeline and show that our results are robust, stable, and sensitive to the cell death and drug toxicity. -We provide the CellProfiler features, deep learning embeddings, and their related pipelines for nowadays the biggest fluorescent microscopy dataset to explore drug efficiency."
Weakly-Supervised Drug Efficiency Estimation with Confidence Score: Application to COVID-19 Drug Discovery,2,Dataset,"Our work uses RxRx19a dataset, which consists of 305, 520 site-level 1024×1024 pixels images captured by fluorescent microscopes in multiple high throughput screening experiments. These experiments were conducted on two tissue types, HRCE and VERO, to explore the efficiency of 1672 FDA or EMA-approved drugs with 6 to 8 different doses against SARS-CoV-2. A drug with a specific dosage was added post-seeding for preparing treatment samples, and then samples were contaminated with the active SARS-CoV-2 virus. Then, samples were fixed and stained with 5 fluorescent colors to indicate DNA, RNA, actin cytoskeleton, Golgi apparatus, and endoplasmic reticulum organelles in cells. RxRx19a is publicly available and licensed under the Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/)."
Weakly-Supervised Drug Efficiency Estimation with Confidence Score: Application to COVID-19 Drug Discovery,3,Methods,"Assume X is a set of all single-cell images in our dataset. We partitioned X based on information of wells into three classes X ctrl+ , X ctrl-, and X t . These are sets of all single-cell images from positive control wells (active SARS-CoV-2), negative control wells (mock and UV-irradiated SARS-CoV-2), and contaminated drugged wells (treatment), respectively. We also define X ctrl = X ctrl+ ∪ X ctrl-as a set of all control samples. The function f (x i ), f : X → H, maps each image x i to its embedding h i . Same as partitions in X space, we consider H ctrl+ , H ctrl-, and H t as:A function d : H → [0, 1] returns a disease score for a given embedding, which is a score number indicating adversity of the disease. The lower and closer to 0, the healthier the cell is, and vice versa. For the sake of simplicity, we ignore experimental errors and consider all positive controls cells infected ∀h + ∈ H ctrl+ , d(h + ) = 1, and with the same logic, all negative cells as healthy ∀h -∈ H ctrl-, d(h -) = 0. Yet, the disease score for ∀h t ∈ H t is unknown. If a drug was effective, somehow, it could hinder the virus's activities in the cell after being contaminated. It could be through a direct effect on the virus, entry channels, or cells that make them more resistant to the virus. As a result, with a successful drug, the host cell's morphological features were close to the morphological characteristics of a healthy cell. That is, d(h t ) is close to 0 for a successful drug. In this study, we aim to estimate d (Fig. 1). In image preprocessing, we correct field-of-view illumination in images, followed by the cell segmentation. The embedding function f (.) calculates features for two random single-cell images from positive and negative control wells. A random convex combination of these two embeddings generates a transformed embedding h. We then calculate a disease score and a confidence score for h, which are evaluated with the weak label l( h) during training."
Weakly-Supervised Drug Efficiency Estimation with Confidence Score: Application to COVID-19 Drug Discovery,3.1,Image Preprocessing and Embedding,"We used a retrospective multi-image method to correct field-of-view illumination and calculate illumination functions for each channel across plates [3]. For the cell segmentation, we used CellProfiler to calculate cell locations and boxes [17]. After this process, artifacts and highly saturated pixels were removed. We deployed two methods of rule-based feature extraction and representation learning to extract embeddings for the single-cell images. For the rule-based feature extraction pipeline, we used CellProfiler. Plate-level normalization and Typical Variation Normalization (TVN) were applied to reduce the batch effects [1]. For the representation learning, we adopted a pre-trained ResNet18 [6]. The model was pre-trained on the ImageNet dataset. Nevertheless, both methods embed single-cell images into feature vectors; throughout the paper, f (.) could refer to either."
Weakly-Supervised Drug Efficiency Estimation with Confidence Score: Application to COVID-19 Drug Discovery,3.2,Data Augmentation with Weak Labels,"We propose a novel data augmentation based on [4] at our embedding level that generates simulated drugged samples with weak labels. Not only is our augmentation method a game-changer in network evaluation, but it also helps the model learn meaningful disease-related characteristics. A transformed sample h is calculated by convex combinations of positive and negative controls and then perturbed by random noise to mimic both a drug's efficiency and side effects.Our data transformation can be formulated as follows:hi,j,α = αhwhere α ∼ U(0, 1) is a random number between [0, 1], and h + i ∼ H ctrl+ and h - j ∼ H ctrl-are arbitrary samples from positive and negative controls. A normally distributed noise n ∼ N (μ H ctrl , σ H ctrl ) is calculated for each dimension independently based on the mean and standard deviation of control profiles. This noise is orthogonal to v = Hctrl+ -Hctrland is added to the representation to simulate drug-related morphological changes. v is a vector between positive and negative samples' embedding averages, and the final noise n is in a hyperplane perpendicular to v. And γ is a hyperparameter. Random noise n is not expected to affect the transformed data label because it is orthogonal to the positive and negative samples axis. We consider the weak label for hi,j,α only depended on α, l(h + i ) and l(h - j ):It is remarkable that with this method, we can augment our dataset indefinitely, and overcome the poor generalization issues that are caused by the limited control data. Further, this augmented dataset H which includes control samples H ctrl is used to train and evaluate our confident hit predictor."
Weakly-Supervised Drug Efficiency Estimation with Confidence Score: Application to COVID-19 Drug Discovery,3.3,Confident Hit Predictor,"Machine learning models' predictions about samples from out of their distribution cannot be trusted [5,10]. This issue is crucial when it comes to safetycritical applications like drug discovery. False negatives cause missing effective treatment, while false positives result in wasting money and time in follow-up experiments. That being said, the worst drawback is that we cannot trust our model judgments on unknown perturbations anymore, as they could act as outof-distribution samples. In our study, unfortunately, there are not any ground truth that can help us to quantify treatment efficiency against SARS-CoV-2 invitro. Although the model can learn deep phenotypic characteristics for healthy and infected cells during training, drug-related morphological features of drugged cells are unknown. To lean on model disease score estimation for a treatment, we need to ask about the model's confidence in the prediction.We offer a first-in-field confident hit predictor that utilizes the idea of hint and confidence [5] to calculate a measure of certainty about its hit-score estimations.During training, our model receives hints to predict disease scores. Larger hints lead to lower confidence scores. Using this method, we adjust the prediction probabilities using interpolation between the predicted disease score and the weak label probability distribution. The adjusted prediction is then subjected to MSE loss calculation as follows:where h is an augmented transformed hidden features and l( hi ) is the weak label, and d( hi ) is predicted diseas score. A function c(.) ∈ [0, 1] calculates the confidence score. The adjusted prediction c( h)d( h) + (1c( h))l( h) is closer to the hint score (l( hi )) when the confidence score is low, and vice versa. To prevent the model from always asking for hints and getting stuck in a trivial solution where c always returns 0, a confidence loss,log(c( h)) is added to the loss."
Weakly-Supervised Drug Efficiency Estimation with Confidence Score: Application to COVID-19 Drug Discovery,4.1,Experimental Settings,"We developed our CellProfiler pipeline with Luigi workflow manager, which was, in our experience, faster and more maintainable in locally hosted systems. For TVN, we use PCA whitening that embeds our representation into 1024 dimensions. Multilayer perceptrons (MLP) are used to estimate the confidence and disease scores. To train our model, we used the zero-shot learning method to test our model's reliability. For this purpose, we left out inactive UV-irradiated SARS-CoV-2 samples in the training process. The model should estimate inactive virus samples as healthy, while inactive viruses can still slightly change the cellular morphology."
Weakly-Supervised Drug Efficiency Estimation with Confidence Score: Application to COVID-19 Drug Discovery,4.2,Representation Quality Assurance,"Replicate Reproducibility Test: It is expected to see replicates of the same biological perturbation are significantly similar to each other than a random set of profiles [3]. We used the Pearson correlation to calculate the similarity of two representations. The distribution of the mean Pearson correlation between all replicates of a biological perturbation is compared against a null distribution. We observed that the replicate correlation of 98.99% of drugs are significant using this criterion (see Fig. 2a,S1)."
Weakly-Supervised Drug Efficiency Estimation with Confidence Score: Application to COVID-19 Drug Discovery,,Stability of Disease Scores:,"To test the model's stability, we randomly omitted a portion of the control samples before creating a new augmented dataset and training a new model. Then, we compared the correlation between prime and new scores and repeated the test five times. Our model showed enhanced stability due to weak label data augmentation, as opposed to the unstable on-disease score algorithm used in RxRx19a (see Fig. 2b)."
Weakly-Supervised Drug Efficiency Estimation with Confidence Score: Application to COVID-19 Drug Discovery,,Robustness of Disease Scores:,"We show the robustness of our method for computing disease scores by accurately predicting scores for new samples in a zero-shot learning setup [20]. The model was trained on an augmented dataset and tested on unseen ultraviolet-irradiated SARS-CoV-2 samples. The model predicted scores of this group near zero, and these scores' distribution is similar to the other negative controls (see Fig. 2c,S2)."
Weakly-Supervised Drug Efficiency Estimation with Confidence Score: Application to COVID-19 Drug Discovery,4.4,Confidence Scores Quality Assurance,Calibration of Confidence Scores: We have noticed that the entropy of disease score predictions rises as the samples are further away from being entirely healthy or diseased. This reveals that the model demonstrates measurable uncertainty on OOD samples (see Fig. 2d).
Weakly-Supervised Drug Efficiency Estimation with Confidence Score: Application to COVID-19 Drug Discovery,4.5,Evaluation,"We hypothesize that using confidence will reduce false positives in the hit discovery. Because ground truth labels for drugs in our dataset are not accessible, we cannot directly show false positive rate reduction. Instead, we can show that when we applied the confidence method consensus among top scores, the overlap between discovered hits based on different representations increased. We run our pipeline in both with and without confidence setups for three representations: our CellProfiler features, our single-cell image embedding, and the original proprietary RxRx19a embedding. When we used the confidence score, the mean Jaccard similarity between the top 10 drugs increased from 0.13 to 0.2 (see Table 1). Top score drugs predicted by our pipeline are Remdesivir [9,13] (d = 0.74, c = 0.66), Aloxistatin [23] (d = 0.72, c = 0.71), GS-441524 [21] (d = 0.67, c = 0.63), Albendazole (d = 0.61, c = 0.69), and Cinnarizine (d = 0.60, c = 0.66) (see Tables S2,S3). As the disease score drops, the model confidence increases. That model confidence for the known therapeutic, Remdesivir, is 0.66, while the model is pretty sure (c > 0.98) that Fluvoxamine and Hydroxychloroquine are ineffective. We suspect that effective drugs can cause a sequence of cellular morphological changes unknown to the model. However, when we have ineffective drugs, the well characterized morphological changes that are caused by the SARS-CoV-2 infection dominates. That is why the model is more sure about ineffective drugs' disease scores than effective ones."
Weakly-Supervised Drug Efficiency Estimation with Confidence Score: Application to COVID-19 Drug Discovery,5,Conclusion,"To safely conclude about hit discovery scores based on cellular morphological features, we need to be concerned about inevitable out-of-distribution phenotypes.We explored one possible solution and proposed a confidence-based weaklysupervised drug efficiency estimation pipeline that was trained to be unsure about out-of-distribution samples. Further, because truth values for drug efficiency scores are unknown, we indirectly defined a metric to calculate false positive rate reduction and showed that this metric improves in the confidence-based setup. We also enhanced our drug efficiency estimation pipeline with a unique weakly-supervised data transformation, simulating contaminated drugged cell phenotypes. Finally, we assessed our pipeline's robustness and stability."
Weakly-Supervised Drug Efficiency Estimation with Confidence Score: Application to COVID-19 Drug Discovery,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3 65.
AUA-dE: An Adaptive Uncertainty Guided Attention for Diffusion MRI Models Estimation,1,Introduction,"Diffusion MRI (dMRI) is a powerful medical imaging tool for probing microstructural information based on the restricted diffusion assumption of the water molecules in biological tissues [7]. The conventional dMRI uses the apparent diffusion coefficient (ADC) to measure the diffusivity change, but it's not specific to microstructural properties. Recently, a series of advanced dMRI models, such as time-dependent dMRI (TD-dMRI) models [5,8,9], have been proposed to sample diffusion at different effective diffusion-times (so-called t-space), and related biophysical models have been developed to resolve the cellular microstructures.Advanced dMRI models are typically multi-compartment models with highly non-linear and complex formulations. Accurate parameter estimation from such models requires dense q-space and/or t-space sampling. Deep learning techniques have been proposed to improve estimation accuracy from downsampled q-space data [2,13,15], by training networks to learn the relationship between downsampled diffusion signals and microstructural metrics from fully sampled q-space. However, TD-dMRI models require dense sampling in both q-space and t-space, which is challenging for clinical usage. To our best knowledge, previous works have not investigated downsampling models in t-space or joint q-t space.The previous q-space learning networks [2,13,15], simply learned the mapping relationship between undersampled diffusion signals in q-space and diffusion model parameters. They neglected the ""noisy label"" problem, and thus, may suffer from training degradation due to failure to obtain valid information. Different from the annotated label, which is called ground-truth in the natural image, in the dMRI model estimation area, we used the parameters estimated from fully sampled q-space as the ""gold standard"" that actually suffer from estimation error due to noise in the data acquisition and also the limited number of samples in q-space. These two problems are particularly worth noting in TD-dMRI, which is known to have low SNR, and the errors could accumulate in the joint qt space models. This imposed a vital problem for end-to-end t-space learning in practice. To address the challenge of learning from noisy data, we proposed an adaptive uncertainty guided attention for diffusion MRI models estimation (AUA-dE) based upon the previous AEME network [15] for estimating general dMRI model parameters.In this work, we proposed a reweighting strategy to reduce the negative effects of noisy label based on uncertainty. Our contributions can be summarized below:1. We brought up an important problem of the noisy label in dMRI model estimation which was not addressed before. 2. We proposed an attention-based sparse encoder to make the network focus on the key diffusion signal out of many q-space or t-space signals. 3. We developed an uncertainty-based reweighting strategy considering the uncertainty in both dMRI channels and spatial domain for microstructural estimation. 4. We proposed an end-to-end estimation strategy in both q-space and t-space with downsampled q-t space data.In our work, we firstly demonstrated the effectiveness of our attention-based reweighting strategy on a simulation dataset, and then we evaluated our work with three different downsampling strategies (q-space, t-space, and q-t space) on a TD-dMRI dataset of normal and injured rat brains. We tested a TD-dMRI model which estimates the transmembrane water exchange time based on timedependent diffusion kurtosis [10], here we named tDKI."
AUA-dE: An Adaptive Uncertainty Guided Attention for Diffusion MRI Models Estimation,2.1,q-t Space Sparsity,"In this study, we extended the q-space learning model AEME [15] into the q-t space, and the signal can be represented as follows:where is the diffusion signal normalized by b0 at different t d , and X ∈ R NΓ×NΥ is the matrix of the mixed sparse dictionary coefficients. Γ ∈ R K×NΓ and Υ ∈ R V ×NΥ are decomposed dictionaries that encode the information in the mixed q-t domain and the spatial domain. H is the noise corresponding to X. Then the sparse encoder can be formulated using the extragradient-based method similar to [15]:where, A 1 denotes a scalar matrix, AU A(•) is the adaptive uncertainty attention function, and H M denotes a nonlinear operator corresponding to the threshold layer in Fig. 1(b):So far, we can obtain the sparse representation of the signal."
AUA-dE: An Adaptive Uncertainty Guided Attention for Diffusion MRI Models Estimation,2.2,Adaptive Uncertainty Attention Modelling,"Uncertainty Attention Module. Inspired by the uncertainty modeling mechanism of Bayesian neural networks [6], we defined uncertainty attention (UA) to address the noisy label problem. For simplicity, we used Monte Carlo dropout to obtain the posterior distribution, other uncertainty quantification methods would also work. The basic attention module is adapted from CBAM [12], which includes channel and spatial attention. To better capture feature information, we employ mean and standard deviation pooling. Then the UA module is formulated according to the gray shaded box in Fig. 1(b). The top branch is the channel uncertainty attention (CUA) module, and the bottom branch is the spatial uncertainty attention (SUA) module. Through the CUA module, the uncertainty reweighted sparse representation of the original signal X can be obtained: X CUA = CU A(X). CU A(•) is the channel-wise uncertainty attention function, which is used to model the uncertainty in the diffusion channels. The CUA of X can be obtained after the stochastic forward with dropout:CUA is a cluster of different sparse representations of X after dropout in the CUA module, and k is the number of stochastic forwards.Similarly, the SUA module can be defined as: X SUA = SU A(X CUA ). SU A(•) is the spatial-wise uncertainty attention, which is used to model the uncertainty of the spatial-wise information. The SUA of X CUA can be estimated as follows:. U S is the uncertainty of the spatial-wise information, X k SUA is a cluster of different sparse representations of X CUA after dropout in the SUA module.For simplicity and efficiency, in practice, we combined these two kinds of uncertainty together to reweight the whole sparse representation X as: U = V ar(X k ). U is the uncertainty of the sparse representation, X k is a cluster of different sparse representations of X after dropout in both CUA and SUA modules.Adaptive Reweight Mechanism. In this work, we proposed an adaptive reweighting strategy by lowering the loss weight for a patch that may be corrupted by the noise. After the uncertainty U is approximated, We can set a weight tenor U w as below:Then, the impact of noise can be mitigated by an adaptive weight matrix R:where, t is a trainable parameter in the network, which can be modified adaptively. Then X will be reweighted by the R in the loss function as:where, denotes the element-wise multiplication, M (•) is a mapping function corresponding to the mapping networks, P is the observed label of the estimated dMRI model parameters.Network Construction. Following the q-t space sparsity analysis and the adaptive uncertainty mechanism mentioned above, we can incorporate historical information [15] into Eq. 2 and Eq. 3 to formulate the adaptive uncertainty attention sparse encoder (AUA-SE). where, , and G n+1 and they can be defined as [15]. Further fundamental details on formulas and network architecture can be found in [14].The overall network can be constructed by repeating the AUA-SE unit n times, and the output will be sent to the mapping networks for mapping the microstructure parameters, which consist of three fully connected layers of the feed-forward networks [15]. The overall structure is illustrated in Fig. 1 (a)."
AUA-dE: An Adaptive Uncertainty Guided Attention for Diffusion MRI Models Estimation,2.3,Dataset and Training,"The tDKI model is defined as below [10]:where, K(t) is the kurtosis at different t d , K 0 is the kurtosis at t d = 0, t is the t d and τ m is the water mixing time. K(t) at individual t d is obtained according to the DKE method [11].Simulation Dataset was formed following the method of Barbieri et al., [1], where we plugged the varying parameters (K 0 , τ m ) into Eq. 14 to obtain the kurtosis signal at different t d (50, 100, and 200 ms). Parameter values were sampled uniformly from the following ranges according to the fitted values observed in rat brain data: K 0 between 0 and 3, τ m between 2 and 200 ms, and a total of 409600 signals were generated. And 60% of them were used for training, 10% for validation, and 30% for testing.In order to replicate the noisy label problem, we varied the noise level from SNR=10 to 30 in the t-space signal. In the training data, we used a Bayesian method modified from Gustafsson et al. [4] to obtain the label, and in the test data, we used ""gold standard"" label set from simulation.Rat Brain Dataset was collected on a 7T Bruker scanner from 3 normal rats and 10 rats that underwent a model of ischemic injury by transient Middle Cerebral Artery Occlusion (MCAO). Diffusion gradients were applied in 18 directions per b-value at 3 b-values of 0.8, 1.5, and 2.5 ms/ μ m 2 and 5 t d (50, 80, 100, 150, and 200 ms) with the following acquisition parameters: repetition time/echo time = 2207/18 ms, in-plane resolution = 0.3 × 0.3 mm 2 , 10 slices with a slice thickness of 1 mm.In order to get the gold standard, the DKE toolbox [11] was used to calculate the kurtosis at different t d with the fully sampled q-space, and then used the Bayesian method mentioned above to estimate K 0 and τ m with the fully sampled t-space. The dataset was downsampled with randomly selected 9 gradients at b = 0.8 and 1.5 ms/ μ m 2 in q-space and 3 t d (50, 100, and 200 ms) in t-space. We mixed the 2 normal and 8 injured rats together for training (90%) and validation (10%), and 1 normal with 2 injured rats for testing.Training. In this work, the dictionary size of our AUA-dE was set at 301, and the hidden size of the fully connected layer was 75. We used an early stopping strategy and a reducing learning rate with an initial learning rate of 1 × 10 -4 . AdamW was selected as the optimizer with a batch size of 256. The dropout in the AUA-SE was 0.2 for 50 forward processes."
AUA-dE: An Adaptive Uncertainty Guided Attention for Diffusion MRI Models Estimation,3.1,Ablation Study,"We compared four different methods, including AEME [15] (baseline of the current network but without the attention or uncertainty mechanism), AEME with attention (the baseline with the attention), AEME with UA (the baseline with UA but without adaptive mechanism), and AEME with AUA (our proposed AUA-dE) to evaluate the effectiveness of UA and AUA in mitigating the negative effect of the noisy label.Here, we used the relative error (percentage of the gold standard) to compare different algorithms. Figure 2 showed that network structures with UA (AEME with UA and AUA-dE) achieved lower errors compared with other methods, especially in the lowest SNR environment. AUA-dE achieved the lowest estimation error because the threshold of uncertainty is a trainable parameter that does not need to be manually defined. Meanwhile, we performed paired t-tests for all comparative results, and AUA-dE showed significantly lower errors than all other methods. "
AUA-dE: An Adaptive Uncertainty Guided Attention for Diffusion MRI Models Estimation,3.2,Performance Test,"q -Space Downsampling. We used our AUA-dE to estimate kurtosis at different t d using the downsampled q-space, in comparison with an optimization based method DKE [11], a common q-space baseline q-DL [3], and the latest optimization based learning structure AEME [15]. Table 1 shows that our proposed AUA-dE achieved the lowest mean squared error (MSE) when compared to other methods for all t d in both the normal and injured rat brain regions.t-Space Downsampling. In the t-space downsampling performance experiment, we used our AUA-dE to estimate the K 0 and τ m based on K(t) at varying t d . We compared our method with the Bayesian method [4], q-DL, and AEME.From Table 2, it can be found our proposed AUA-dE achieved the lowest MSE compared with other methods in both normal and injured brain regions. Compared with previous methods, our error was only about 20% of the q-DL error in normal tissues and 40% in the injured regions. q-t Space Downsampling. In q-t space downsampling, we used our proposed AUA-dE to estimate the K 0 and τ m with jointly downsampled q-t space data with 5 times acceleration (3 folds in q-space and 1.7 folds in t-space). Figure 3 showed the estimated K 0 and τ m maps of an injured rat brain, using DKE+Bayesian, q-DL, AEME, and AUA-dE. Only AUA-dE was capable of capturing the abnormal rise in the injured cortex in the τ m map (denoted by the red arrow), indicating the clinical potential of this method for diagnosis of ischemic brain injury. "
AUA-dE: An Adaptive Uncertainty Guided Attention for Diffusion MRI Models Estimation,4,Conclusion,"In this work, we proposed an adaptive uncertainty guided attention for diffusion MRI models estimation (AUA-dE) to address the noisy label problem in the estimation of TD-dMRI-based microstructural models. We tested our proposed network and module in a rat brain dataset and a simulation dataset. The proposed method showed the highest estimation accuracy in all of these datasets. Meanwhile, we demonstrated its performance on jointly downsampled q-t space data, for which previous algorithms did not work well with the highly accelerated setup (270/54). In the future, we will further investigate our proposed AUA module as a plug-in in different dMRI model estimation networks and also different dMRI models to test its generalizability and robustness."
BigFUSE: Global Context-Aware Image Fusion in Dual-View Light-Sheet Fluorescence Microscopy with Image Formation Prior,1,Introduction,"Light-sheet fluorescence microscopy (LSFM), characterized by orthogonal illumination with respect to detection, provides higher imaging speeds than other light microscopies, e.g., confocal microscopy, via gentle optical sectioning [5,14,15], which makes it well-suited for whole-organism studies [11]. At macroscopic scales, however, light scattering degrade image quality. It leads to images from deeper layers of the sample being of worse quality than from tissues close to the illumination source [6,17]. To overcome the negative effect of photon propagation, dual-view LSFM is introduced, in which the sample is sequentially illuminated from opposing directions, and thus portions of the specimen with inferior quality in one view will be better in the other [16] (Fig. 1a). Thus, image fusion methods that combine information from opposite views into one volume are needed.To realize dual-view LSFM fusion, recent pipelines adapt image fusers for natural image fusion to weigh between views by comparing the local clarity of images [16,17]. For example, one line of research estimates focus measures in a transformed domain, e.g., wavelet [7] or contourlet [18], such that details with various scales can be considered independently [10]. However, the composite result often exhibits global artifacts [8]. Another line of studies conducts fusion in the image space, with pixel-level focus measures decided via local block-based representational engineering such as multi-scale weighted multi-scale weighted gradient [20] and SIFT [9]. Unfortunately, spatially inconsistent focus measures are commonly derived for LSFM, considering the sparse structures of the biological sample involved by the limited field-of-view (FOV).Apart from limited FOV, another obstacle that hinders the adoption of natural image fusion methods into dual-view LSFM is the inability to distinguish sample structures from structural artifacts [1]. For example, ghost artifacts, surrounding the sample as a result of scattered illumination light [3], can be observed, as it only appears in regions far from the light source after light travels through scattering tissues. Yet, when ghosts appear in one view, the same region in the opposite view would be background, i.e., no signal. Thus, ghosts will be erroneously transferred to the result by conventional fusion studies, as they are considered as owning richer information than its counterpart in the other view.Here, we propose BigFUSE to realize spatially consistent image fusion and exclude ghost artifacts. Main contributions are summarized as follows:• BigFUSE is the first effort to think of dual-view LSFM fusion using Bayes, which maximizes the conditional probability of fused volume regarding image clarity, given the image formation prior of opposing illumination directions. • The overall focus measure along illumination is modeled as a joint consideration of both global light scattering and local neighboring image qualities in the contourlet domain, which, together with the smoothness of focus-defocus, can be maximized as Likelihood and Prior in Bayesian. • Aided by a reliable initialization, BigFUSE can be efficiently optimized by utilizing expectation-maximum (EM) algorithm."
BigFUSE: Global Context-Aware Image Fusion in Dual-View Light-Sheet Fluorescence Microscopy with Image Formation Prior,2,Methods,"An illustration of BigFUSE for dual-view LSFM fusion is given in Fig. 1. First, pixel-level focus measures are derived for two opposing views separately, using nonsubsampled contourlet transform (NSCT) (Fig. 1b). Pixel-wise photon propagation maps in tissue are then determined along light sheet via segmentation (Fig. 1c). The overall focus measures are thus modeled as the inverse of integrated photon scattering along illumination conditioned on the focus-defocus change, i.e., Likelihood, whereas the smoothness of focus-defocus is ensured via Prior. Finally, the focus-defocus boundary is optimized via EM (Fig. 1d). "
BigFUSE: Global Context-Aware Image Fusion in Dual-View Light-Sheet Fluorescence Microscopy with Image Formation Prior,2.1,Revisiting Dual-View LSFM Fusion Using Bayes,"BigFUSE first rethinks dual-view LSFM fusion from a Bayesian perspective, that is, the conditional probability of fused volume in terms of ""in-focusness"", is given on not only a pair of image inputs, but also prior knowledge that these two images are illiminated from opposing orientations respectively:where Y ∈ R M ×N is our predicted fusion with minimal light scattering effect. X a and X b are two image views illuminated by light source a and b, respectively. We choose Y ∈ {X a , X b } depending on their competitive image clarity at each pixel. Priors P denote our empirical favor of X a against X b at each pixel if photons travel through fewer scattering tissues from source a than b, and vice versa. Due to the non-positive light scattering effect along illumination path, there is only one focus-defocus change per column for dual-view LSFM in Fig. 1a. Thus, fusion is equivalent to estimating a focus-defocus boundary ω defined as a function associating focus-defocus changes to column indexes:which can be further reformulated by logarithm:where ω i denotes the focus-defocus changeover at i -th column, X :,i is the i -th column of X. Next, estimating ω is decomposed into: (i ) define the column-wise image clarity, i.e., log-likelihood log(p((X a :,i , X b :,i )|ω i )); (ii ) consider the belief on a spatially smooth focus-defocus boundary, namely log-prior log(p(ω))."
BigFUSE: Global Context-Aware Image Fusion in Dual-View Light-Sheet Fluorescence Microscopy with Image Formation Prior,2.2,Image Clarity Characterization with Image Formation Prior,"In LSFM, log-likelihood log(p((X a :,i , X b :,i )|ω i )) can be interpreted as the probability of observing (X a :,i , X b :,i ) given the hypothesis that the focus-defocus change is determined as ω i in the i -th column:where ⊕ is a concatenation, c(•) is the column-wise image clarity to be defined."
BigFUSE: Global Context-Aware Image Fusion in Dual-View Light-Sheet Fluorescence Microscopy with Image Formation Prior,,Estimating Pixel-Level Image Clarity in NSCT.,"To define c(•), BigFUSE first uses NSCT, a shift-invariant image representation technique, to estimate pixel-level focus measures by characterizing salient image structures [18]. Specifically, NSCT coefficients S a and S b are derived for two opposing LSFM views, where S = {S i0 , S i,l |(1 ≤ i ≤ i 0 , 1 ≤ l ≤ 2 li )}, S i0 is the lowpass coefficient at the coarsest scale, S i,l is the bandpass directional coefficient at i -th scale and l -th direction. Local image clarity is then projected from S a and S b [18]:where R i,l is local directional band-limited image contrast and Si0 is the smoothed image baseline, whereas Dσ i highlights image features that are distributed only on a few directions, which is helpful to exclude noise [18]. As a result, pixel-level image clarity F = j0 j=1 2ˆlj l=1 F j,l is quantified for respective LSFM view.Reweighting Image Clarity Measures by Photon Traveling Path. Pixelindependent focus measures may be adversely sensitive to noise, due to the limited receptive field when characterizing local image clarities. Thus, BigFUSE proposes to integrate pixel-independent image clarity measures along columns by taking into consideration the photon propagation in depth. Specifically, given a pair of pixels (X a m,n , X b m,n ), X a m,n is empirically more in-focus than X a m,n , if photons travel through fewer light-scattering tissues from illumination objective a than from b to get to position (m, n), and vice versa. Therefore, BigFUSE defines column-level image clarity measures as:where A :,i is to model the image deterioration due to light scattering. To visualize photon traveling path, BigFUSE uses OTSU thresholding for foreground segmentation (followed by AlphaShape to generalize bounding polygons), and thus obtains sample boundary, i.e., incident points of light sheet, which we refer to as p u and p l for opposing views a and b respectively. Since the derivative of A :,i implicitly refers to the spatially varying index of refraction within the sample, which is nearly impossible to accurately measure from the physics perspective, we model it using a piecewise linear model, without loss of generality:As a result, log(p((X a :,i , X b :,i )|ω i )) is obtained as summed pixel-level image clarity measures with integral factors conditioned on photon propagation in depth."
BigFUSE: Global Context-Aware Image Fusion in Dual-View Light-Sheet Fluorescence Microscopy with Image Formation Prior,2.3,Least Squares Smoothness of Focus-Defocus Boundary,"With log-likelihood log(p((X a :,i , X b :,i )|ω i )) considering the focus-defocus consistency along illuminations using image formation prior in LSFM, BigFUSE then ensures consistency across columns in p(ω). Specifically, the smoothness of ω is characterized as a window-based polynomial fitness using linear least squares:, c i is the parameters to be estimated, the sliding window is with a size of 2s + 1."
BigFUSE: Global Context-Aware Image Fusion in Dual-View Light-Sheet Fluorescence Microscopy with Image Formation Prior,2.4,Focus-Defocus Boundary Inference via EM,"Finally, in order to estimate the ω together with the fitting parameter c, Big-FUSE reformulates the posterior distribution in Eq. ( 2) as follows:where λ is the trade-off parameter. Here, BigFUSE alternates the estimations of ω, and c, and iterates until the method converges. Specifically, given c (n) for the n-th iteration, ω (n+1) i is estimated by maximizing (E-step):which can be solved by iterating over {i|1 ≤ i ≤ M }. BigFUSE then updates c (n+1) i based on least squares estimation: Additionally, A n+1 is updated based on Eq. ( 7) subject to ω (n+1) (M-step). BigFUSE proposes to initialize ω based on F a and F b :where |Φ| denotes the total number of elements in Φ."
BigFUSE: Global Context-Aware Image Fusion in Dual-View Light-Sheet Fluorescence Microscopy with Image Formation Prior,2.5,Competitive Methods,"We compare BigFUSE to four baseline methods: (i ) DWT [16]: a multi-resolution image fusion method using discrete wavelet transform (DWT); (ii ) NSCT [18]: another multi-scale image fuser but in the NSCT domain; (iii ) dSIFT [9]: a dense SIFT-based focus estimator in the image space; (iv ) BF [19]: a focusdefocus boundary detection method that considers region consistency in focus; and two BigFUSE variations: (v ) S(•): built by disabling smooth constraint;(vi ) P(•): formulated by replacing the weighted summation of pixel-level clarity measures for overall characterization, by a simple average. To access the blind image fusion performance, we adopt three fusion quality metrics, Q mi [4], Q g [12] and Q s [13]. Specifically, Q mi , Q g and Q s use mutual information, gradient or image quality index to quantify how well the information or features of the inputs are transferred to the result, respectively. In the simulation studies where ground truth is available, mean square error (EMSE) and structural similarity index (SSIM) are used for quantification."
BigFUSE: Global Context-Aware Image Fusion in Dual-View Light-Sheet Fluorescence Microscopy with Image Formation Prior,3.1,Evaluation on LSFM Images with Synthetic Blur,"We first evaluate BigFUSE in fusing dual-view LSFM blurred by simulation. Here, we blur a mouse brain sample collected in [2] with spatially varying Gaussian filter for thirty times, which is chemically well-cleared and thus provides an almost optimal ground truth for simulation, perform image fusion, and compare the results to the ground truth. BigFUSE achieves the best EMSE and SSIM, statistically surpassing other approaches (Table 1, p < 0.05 using Wilcoxon signed-rank test). Only BigFUSE and BF realize information fusing without damaging original images with top two EMSE. In comparison, DWT, NSCT and dSIFT could distort the original signal when fusing (green box in Fig. 2).   "
BigFUSE: Global Context-Aware Image Fusion in Dual-View Light-Sheet Fluorescence Microscopy with Image Formation Prior,3.2,Evaluation on LSFM Images with Real Blur,"BigFUSE is then evaluated against baseline methods on real dual-view LSFM. A large sample volume, zebrafish embryo (282 × 2048 × 2048 for each view), is imaged using a Flamingo Light Sheet Microscope. BigFUSE takes roughly nine minutes to process this zebrafish embryo, using a T4 GPU with 25 GB system RAM and 15 GB GPU RAM. In Fig. 4, inconsistent boundary is detected by BF, while methods like DWT and NSCT generate structures that do not exist in either input. Moreover, only BigFUSE can exclude ghost artifact from the result (red box), as BigFUSE is the only pipeline that considers image formation prior. Additionally, we demonstrate the impact of bigFUSE on a specific downstream task in Fig. 5, i.e., segmentation by Cellpose. Only the fusion result provided by bigFUSE allows reasonable predicted cell pose, given that ghosting artifacts are excluded dramatically. This explains why the Q s in Fig. 3 is suboptimal, since BigFUSE do not allow for the transmission of structural ghosts to the output."
BigFUSE: Global Context-Aware Image Fusion in Dual-View Light-Sheet Fluorescence Microscopy with Image Formation Prior,4,Conclusion,"In this paper, we propose BigFUSE, a image fusion pipline with image formation prior. Specifically, image fusion in dual-view LSFM is revisited as inferring a focus-defocus boundary using Bayes, which is essential to exclude ghost artifacts. Furthermore, focus measures are determined based on not only pure image representational engineering in NSCT domain, but also the empirical effects of photon propagation in depth embeded in the opposite illumination directions in dualview LSFM. BigFUSE can be efficiently optimized using EM. Both qualitative and quantitative evaluations show that BigFUSE surpasses other state-of-the-art LSFM fusers by a large margin. BigFUSE will be made accessible."
Microstructure Fingerprinting for Heterogeneously Oriented Tissue Microenvironments,1,Introduction,"In diffusion MRI, biophysical models offer a non-invasive means of probing the tissue micro-architecture of the human brain. Most models rely on closed-form formulas derived with simplifying assumptions such as short gradient pulse, Gaussian phase distribution, and the absence of compartmental exchange. The reliability and interpretability of these models diminish with deviation from these assumptions.Monte Carlo (MC) simulations [1,2] and solving the Bloch-Torrey partial differential equation (BT-PDE) [3,4] are common methods for generating realistic signals associated with different gradient profiles and cellular geometries. However, the applications of these simulation techniques have been mostly limited to the validation of diffusion models rather than the estimation of microstructural properties.Microstructure fingerprinting (MF) [5,6] exploits the representation accuracy and physical interpretability of simulated models to quantify microstructural properties. Diffusion signals are first simulated for a large collection of microstructural geometries, giving ""fingerprints"" of, for example, axons, somas, and the extra-cellular matrix. Tissue properties are inferred based on the relative contribution of each fingerprint to the voxel signal.In this paper, we introduce a novel MF technique with the following key features:1. MC simulation [5,6] is computationally expensive [1], limiting its ability in constructing a sufficient large dictionary of fingerprints for accurate tissue quantification. We will use SpinDoctor [3] to simulate diffusion signal by solving the BT-PDE. The significant speed-up of SpinDoctor over MC simulations allows fast construction of a comprehensive dictionary of fingerprints. 2. Inspired by [7], we will include fingerprints associated with different levels of membrane permeability to account for inter-cellular exchange. The utilization of simulated models has been shown to remove estimation bias associated with short exchange times [7] and to yield marked improvement and higher reproducibility over the widely used Kärger model [6]. 3. Spherical Mean Spectrum Imaging (SMSI) [8] is used to eliminate confounding factors such as extra-axonal water and axonal orientation dispersion to improve sensitivity of the diffusion-weighted signal to axon radii [9]. 4. MR-derived statistics are often biased toward large axons due to their dominant signal [10]. We introduce a method to correct for this bias based on signal fingerprints, offering radius and permeability measurements that are more biologically realistic.Our method is able to efficiently and accurately probe microstructural properties such as cell size and membrane permeability without relying on assumptions associated with closed-form formulas."
Microstructure Fingerprinting for Heterogeneously Oriented Tissue Microenvironments,2.1,Fingerprint Dictionary,"The diffusion MRI signal at each voxel S is a combination of signals from multiple micro-environments, each represented by a signal fingerprint S i :where f [i] is the volume fraction of the i-th fingerprint, J i is a set of parameters characterizing the geometry of the corresponding micro-environment, and P is a set of acquisition parameters (e.g., pulse sequence, pulse duration, etc.). For brevity, we omit P as it is the same for all i's. In [8,11], J i is based on a tensor model defined by longitudinal diffusivity λ [i] and radial diffusivity λ ⊥ [i], yielding only basic tissue properties such as diffusivity or anisotropy. Inferring geometrical properties such as the radius from diffusivity is not straight-forward [6]. To effectively quantify cell size and membrane permeability, we use Spin-Doctor [3,4] to generate realistic signal fingerprints for various microstructural geometries (called 'atoms') representing axons and somas with a range of radii and different levels of permeability in an extra-cellular matrix. Specifically, the dictionary of fingerprints covers four diffusion patterns:1. Intra-axonal diffusion represented by packed cylinders with radii r ∈ {0.5, 2, 2.5, 3.0, 3.5, 4.0} µm and permeabilities κ's from 0 (impermeable) to 50× 10 -6 µm µs -1 . For b ≤ 3000s mm -2 (typical in most datasets), the diffusion signals of axons with radius from 0 to 2 µm are numerically indistinguishable. The atom for r = 0.5 µm summarizes the distribution in r ∈ [0, 2] µm and the atoms for r ∈ [2,4] µm capture the tail of the distribution as in [12].Cylinders are placed in an extra-cellular space to mimic realistic configurations. 2. Extra-axonal diffusion represented using a tensor model with λ λ ⊥ < τ 2 , with geometric tortuosity τ = 2.6. We choose 1.53. Intra-soma diffusion represented using impermeable spheres with radii from 0 to 20 µm. 4. Free-water diffusion represented using a tensor model with λ = λ ⊥ > λ soma-max , with λ soma-max = 1.0 × 10 -3 mm 2 s -1 is the maximum apparent intra-soma diffusivity.Parameters are chosen according to previous studies, covering the spectrum of biologically possible values in the human brain [6,8,[13][14][15][16][17][18][19][20]."
Microstructure Fingerprinting for Heterogeneously Oriented Tissue Microenvironments,2.2,Solving the Bloch-Torrey Partial Differential Equation (BT-PDE),"We employ SpinDoctor [3] to numerically simulate the diffusion signal from a known geometry (Fig. 1) via solving the BT-PDE. SpinDoctor is typically 50 times faster than MC simulations and does not require GPUs. Our parallelized implementation generates a dictionary of fingerprints for Human Connectome Project (HCP) data with 48 intra-axonal fingerprints (6 radii and 8 permeabilities) and 20 intra-soma (20 radii) in 5 min. Note that the dictionary was generated once for the study, stored, and then resampled to match each subjects' gradient table as in [21]."
Microstructure Fingerprinting for Heterogeneously Oriented Tissue Microenvironments,2.3,Solving for Volume Fractions,"From [8,11], the normalized spherical mean signal at each voxel Ē is a linear combination of the normalized spherical mean signals Ēi from multiple atoms: where A is a matrix containing Ēi and ν is a vector consisting of ν[i]. Note that Ēi is a function of J i but not the direction [22]. The direction-dependent signal S can be represented using rotational spherical harmonics (SHs) R(J i ), the SHs Y L of even orders up to L, and the SH coefficients ϕ i of the fODF corresponding to the i-th fingerprint:From Φ, the volume fraction ν[i] of atom i is the 0-th order SH coefficient in ϕ i [18,23].Following [8], to remove fiber dispersion and degeneracy confounding effect, we solve for ν[i] by using both the spherical mean and the full signal:1. Solve the full signal (FS) problem:and estimate the FS problem volume fraction ν FS from Φ. 2. Solve the mean signal (MS) problem:3. Iterative reweighing until convergence:wherewith ξ being a constant and ν 0 the geometric mean of ν FS and ν MS .We select the regularization parameters γ's using the Akaike information criterion (AIC) to balance the goodness of fit and complexity of the model. There is an empirical lower bound on the soma radius that can be detected via a mixture of somas (isotropic) and axons (anisotropic) when using the mean signal alone [24]. By using the full gradient-sensitized signal in Step 1, our method can distinguish between soma (isotropic signal) and axon (anisotropic signal). Our approach with both FS and MS allows for modeling a spectrum of diffusion from fine to coarse scales without fixing the number of compartments, e.g., one in [22], two (intra-cellular and extra-cellular) in [25], or three (intra-cellular, extra-cellular, and free-water) in [26]."
Microstructure Fingerprinting for Heterogeneously Oriented Tissue Microenvironments,2.4,Radius Bias Correction,"The average axon radius for each voxel can be calculated by averaging the radii of the respective fingerprints, weighted by the volume fractions. Since the volume fractions ν are estimated from the normalized signal model [26], they are actually 'signal fractions'. Weighted averaging using signal fractions yields the axon radius index [17]. Since each axon's contribution to the voxel signal is approximately proportional to the square of its radius [27], using the signal fraction will create a bias toward axons with large radius [10]. This explains why the radius index is often in the range of 3 to 6 µm [13,17] while most axons have actual radii of 0.05 to 1.5 µm [12,28]. We correct for this bias by examining the microstructure model and the actual signal. The spherical mean signal S(b) for diffusion weighting b is S(b)where ν[i] is the signal fraction and Si (b) is the spherical mean signal of the i-th atom. The unbiased volume fraction f [i] is given byallowing us to derive f [i] from ν[i]:Computing f requires the non-diffusion-weighted signal of each compartment Si (0). In our case, Si (0), scaled by an arbitrary factor, is known from the Spin-Doctor simulation. We define weight which is not affected by the scaling factor and therefore can be used to compute the unbiased weighted-average radius."
Microstructure Fingerprinting for Heterogeneously Oriented Tissue Microenvironments,3,Experiments,"We validate our technique, called microstructure fingerprinting SMSI (MF-SMSI), using both in-silico and in-vivo data. The dictionary and synthetic data were generated with HCP-like parameters: 3 b-shells of 1000, 2000, 3000 s mm -2 , 90 directions per shell, diffusion time Δ = 43.1 ms, and pulse width δ = 10.6 ms [29]."
Microstructure Fingerprinting for Heterogeneously Oriented Tissue Microenvironments,3.1,Volume Fraction,"We evaluate the accuracy of MF-SMSI in volume fraction estimation by generating synthetic data following the model in [8,26]:where ν and S are used to denote free-water (FW), intra-soma (IS), intra-axonal (IA), and extra-cellular (EC) volume fractions and signals. We set the ground truth volume fractions to ν FW = 0.1, ν IS = 0.2, and ν IA = 0.5. We generated 1000 instances of the signal with SNR = ∞, 50, 30, and 15. MF-SMSI estimates accurately the volume fraction of each compartment (Fig. 2)."
Microstructure Fingerprinting for Heterogeneously Oriented Tissue Microenvironments,3.2,Cell Size and Membrane Permeability,"To investigate the efficacy of our bias correction, we performed three experiments:1. Axonal radius -We simulated signals for two impermeable axons with radii 2 µm and 4 µm, each with volume fraction 0.5. In Fig. 3 (left panel), the axon radius is biased toward the axon with higher baseline signal, giving a large average radius, similar to the observation in [26]. Our unbiased estimate of the radius is markedly closer to the ground truth of 3 µm. 2. Membrane permeability -We simulated the signals for two axons with permeabilities 4 and 6 (×1 × 10 -6 µm µs -1 ), common radius 3 µm, and equal volume fraction 0.5. In Fig. 3 (middle panel), the biased permeability estimate is biased toward the axon with lower permeability and therefore higher baseline signal. MF-SMSI is able to estimate the correct permeability value matching the ground truth of 5 × 1 × 10 -6 µm µs -1 . 3. Soma radius -We simulated signals for two somas with radii 4 µm and 6 µm with equal volume fraction 0.5. From Fig. 3 (right panel), the bias toward the soma with higher radius (higher baseline signal) is not severe. MF-SMSI yields estimate of the soma radius that is closer to the ground truth."
Microstructure Fingerprinting for Heterogeneously Oriented Tissue Microenvironments,3.3,In-vivo Data,"We compare our estimates with the axonal radius index r index axon from ActiveAx [27]. Figure 4 presents the averaged MF-SMSI and ActiveAx maps of 35 HCP subjects. Intra-axonal (ν IA ), extra-cellular (ν EC ), free-water (ν FW ), and intrasoma (ν IS ) volume fractions are in great agreement with previous studies [8,19,20,25]. Briefly, ν IA is higher in white matter, ν FW is high in CSF, and ν IS is high in the cortical ribbon.Axonal radius and permeability are lower in deep white matter, especially at the body of the corpus callosum and the forceps major, where axons are myelinated and densely packed [8]. Axonal radius r axon ranges from 1.4 µm to 1.7 µm in most white matter areas with a small increase toward the cortex, similar to observations reported in histological studies [12,30,31]. Axonal radii are slightly lower in the anterior than the posterior part of the brain, in line with [6,13]. ActiveAx r index axon is overestimated with values ranging from 3.5 to 7 µm with abrupt spatial changes: almost doubled from the corpus callosum to the cortex, which is unrealistic as the spatial variation of axonal radii was reported to be small [12,15,32].The soma radii r soma in the cortical ribbon have a mean value of 11 µm, similar to what was reported in [19]. "
Microstructure Fingerprinting for Heterogeneously Oriented Tissue Microenvironments,3.4,Histological Corroboration,"We compare our axonal radius estimate r axon , ActiveAx axonal radius index r index axon , and the effective axon radius r eff from [9], with histological samples reported in [12,32]. Histological values were multiplied by 1.2 to correct for ex-vivo shrinkage [9]. Figure 5(a) shows the mean axonal radii estimated with MF-SMSI and ActiveAx for different areas of the corpus callosum from HCP subjects. Overall, there is a low-high-low trend going from the genu (anterior) through the midbody to the splenium (posterior) of the corpus callosum. Ex-vivo values (after shrinkage correction) fall almost entirely in the range calculated with MF-SMSI, indicating strong agreement between our results and histological measurements.Figure 5(b) shows the distribution of mean axonal radius in the corpus callosum from MF-SMSI, ActiveAx, and r eff from [9]. Unlike Active Ax r index axon and r eff , which are indicators of axon radii, our estimates of r axon are more realistic measurements of axonal radii, closer to ex-vivo samples."
Microstructure Fingerprinting for Heterogeneously Oriented Tissue Microenvironments,4,Conclusion,"We have presented a microstructure fingerprinting method that can provide accurate and reliable measurements of tissue properties beyond diffusivity and anisotropy, allowing quantification of cell size and permeability associated with axons and somas."
Brain Anatomy-Guided MRI Analysis for Assessing Clinical Progression of Cognitive Impairment with Structural MRI,1,Introduction,"Brain magnetic resonance imaging (MRI) has been increasingly used to assess future progression of cognitive impairment (CI) in various clinical and research fields by providing structural brain anatomy [1][2][3][4][5][6]. Many learning-based methods have been developed for automated MRI analysis and brain disorder prognosis, which usually heavily rely on labeled training data [7][8][9][10]. However, it is generally time-consuming and tedious to collect category labels for brain MRIs in practice, resulting in a limited number of labeled MRIs [11].Fig. 1. Illustration of brain anatomy-guided representation (BAR) learning framework for assessing the clinical progression of cognitive impairment. The BAR consists of a pretext model and a downstream model, with a shared brain anatomy-guided encoder for MRI feature extraction. The pretext model also contains a decoder for brain tissue segmentation, while the downstream model relies on a predictor for prediction. The pretext model is trained on the large-scale ADNI [12] with 9,544 T1-weighted MRIs, yielding a generalizable encoder. With this learned encoder frozen, the downstream model is then fine-tuned on target MRIs for prediction tasks.Even without task-specific category label information, brain anatomical structures provided by auxiliary MRIs can be employed as a prior to boost disease progression prediction performance. Considering that there are a large number of unlabeled MRIs in existing large-scale datasets [12,13], several deep learning methods propose to extract brain anatomical features from MRI without requiring specific category labels. For instance, Song et al. [14] suggest that the anatomy prior can be utilized to segment brain tumors, while Yamanakkanavar et al. [15] discuss how brain MRI segmentation improves disease diagnosis. Unfortunately, there are few studies that try to utilize such brain anatomy prior for assessing the clinical progression of cognitive impairment with structural MRIs.To this end, we propose a brain anatomy-guided representation (BAR) learning framework for cognitive impairment prognosis with T1-weighted MRIs, incorporated with brain anatomy prior provided by a brain tissue segmentation task. As shown in Fig. 1, the BAR consists of a pretext model and a downstream model, with a shared anatomy-guided encoder for MRI feature extraction. These two models also use a decoder and a predictor for brain tissue segmentation and disease progression prediction, respectively. The pretext model is trained on 9,544 MRI scans from the public Alzheimer's Disease Neuroimaging Initiative (ADNI) [12] without any category label information, yielding a generalizable encoder. The downstream model is further fine-tuned on target MRIs for CI progression prediction. Experiments are performed on two CI-related studies with 391 subjects, with results suggesting the efficacy of BAR compared with state-ofthe-art (SOTA) methods. The pretext model can also be used for brain tissue segmentation in other MRI-based studies. To the best of our knowledge, this is the first work that utilizes anatomy prior derived from large-scale T1-weighted MRIs for automated cognitive decline analysis. To promote reproducible research, the source code and trained models have been made publicly available to the research community (see https://github.com/goodaycoder/BAR)."
Brain Anatomy-Guided MRI Analysis for Assessing Clinical Progression of Cognitive Impairment with Structural MRI,2,Materials and Proposed Method,"Data and Preprocessing. The pretext model is trained via a tissue segmentation task on auxiliary MRIs (without category label) from ADNI. A total of 9,544 T1-weighted MRIs from 2,370 ADNI subjects with multiple scans are used in this work. To provide accurate brain anatomy, we perform image preprocessing and brain tissue segmentation for these MRIs to generate ground-truth segmentation of three tissues, i.e., white matter (WM), gray matter (GM) and cerebrospinal fluid (CSF), using an in-house toolbox iBEAT [16] with manual verification.The downstream model is trained on 1) a late-life depression (LLD) study with 309 subjects from two sites [17,18], and 2) a type 2 diabetes mellitus (DM) study with 82 subjects from the First Affiliated Hospital of Guangzhou University of Chinese Medicine. Subjects in LLD are categorized into three groups: 1) 89 non-depressed cognitively normal (CN), 2) 179 depressed but cognitively normal (CND), 3) 41 depressed subjects (called CI) who developed cognitive impairment or even dementia in the follow-up years. Category labels in the LLD study are determined based on subjects' 5-year follow-up diagnostic information, while MRIs are acquired at baseline time. The DM contains 1) 45 health control (HC) subjects and 2) 37 diabetes mellitus patients with mild CI (MCI). Detailed image acquisition protocols are given in Table SI of Supplementary Materials. All MRIs are preprocessed via the following pipeline: 1) bias field correction, 2) skull stripping, 3) affine registration to the MNI space, 4) resampling to 1 × 1 × 1 mm 3 , 5) deformable registration to AAL3 [19] with SyN [20], and 6) warping 166 regions-of-interest (ROIs) of AAL3 back to MRI volumes.Proposed Method. While it is often challenging to annotate MRIs in practice, there are a large number of MRIs (without task-specific category labels) in existing large-scale datasets. Even without category labels, previous studies propose to extract anatomical features (e.g., ROI volumes of GM segmentation maps) to characterize brain anatomy [21,22]. Such brain anatomy prior learned via tissue segmentation can be employed to boost learning performance intuitively. Accordingly, we propose a brain anatomy-guided representation (BAR) learning framework for progression prediction of cognitive impairment, incorporated with brain anatomy prior provided by brain tissue segmentation. As shown in Fig. 1, the BAR consists of a pretext model for brain tissue segmentation and a downstream model for disease progression prediction, both equipped with brain anatomy-guided encoders (shared weights) for MRI feature learning.(1) Pretext Model for Segmentation. To learn brain anatomical features from MRIs in a data-driven manner, we propose to employ a segmentation task for pretext model training. As shown in the top of Fig. 1, the pretext model consists of 1) a brain anatomy-guided encoder for MRI feature extraction and 2) a decoder for segmentation. The brain anatomy-guided encoder takes large-scale auxiliary 3D MRIs without category labels as input, and outputs 512 feature maps. It contains 8 convolution blocks, with each block containing a convolution layer (kernel size: 3 × 3 × 3), followed by instance normalization and Parametric Rectified Linear Unit (PReLU) activation. The first 4 blocks downsample the input with a stride of 2 × 2 × 2. The channel numbers of the eight blocks are [64, 128, 256, 512, 512, 512, 512, 512], respectively. A skip connection is applied to sum the input and output of every two of the last 4 blocks for residual learning.The decoder takes the 512 feature maps as input and outputs segmentation maps of three tissues (i.e., WM, GM, and CSF), thus guiding the encoder to learn brain anatomical features. The decoder contains four deconvolution blocks with 256, 128, 64 and 4 channels, respectively. Each deconvolution block shares the same architecture as the convolution block in the encoder. The output of the decoder is then fed into a SoftMax layer to get four probability maps that indicate the probability of a voxel belonging to a specific tissue (i.e., background, WM, GM, and CSF). Besides, the reconstruction task can be used to train the pretext model instead of segmentation when lacking ground-truth segmentation maps. For problems without ground-truth segmentation maps, we can resort to an MRI reconstruction task to train the pretext model in an unsupervised manner.(2) Downstream Model for Prediction. As shown in the bottom panel of Fig. 1, the downstream model takes target MRIs as input and outputs probabilities of category labels. It consists of 1) a brain anatomy-guided encoder and 2) a predictor for prognosis. This encoder shares the same architecture and parameters as that of the pre-trained pretext model. With the encoder frozen, predictor parameters will be updated when the downstream model is trained on target MRIs. The predictor has two convolution blocks (kernel size: 3 × 3 × 3, stride: 2 × 2 × 2, channel: 256) with a skip connection, followed by a flatten layer, an FC layer, and a SoftMax layer. The architecture of the predictor can be flexibly adjusted according to the requirements of different downstream tasks.(3) Implementation. The proposed BAR is trained via two steps. 1) The pretext model is first trained on 9,544 MRIs from ADNI, with ground-truth segmentation as supervision. The Adam optimizer [23] with a learning rate of 10 -4 and dice loss are used for training (batch size: 4, epoch: 30). 2) We then share  SII of Supplementary Materials. Such partition is repeated five times independently to avoid any bias introduced by random partition, and the mean and standard deviation results are recorded. The training data is duplicated and augmented using random affine transform. Five evaluation metrics are used, including area under ROC curve (AUC), accuracy (ACC), sensitivity (SEN), specificity (SPE), and F1-Score (F1s). Besides, we perform tissue segmentation by directly applying the trained pretext model to target MRIs from LLD and DM studies and visually compare the results of our BAR with those of FSL [24].Competing Methods. We compare our BAR with two classic machine learning methods and five SOTA deep learning approaches, including (1) support vector machine (SVM) [25] with a radial basis function kernel (regularization: 1.0), ( 2) XGBoost (XGB) [26] (estimators: 300, tree depth: 4, learning rate: 0.2), (3) ResNetx with x convolution layers, (4) Med3Dx [27] with x convolution layers, (5) SEResNet [28] that is an improved model by adding squeeze and excitation blocks to ResNet, (6) EfficientNet [29], and (7) MobileNet [30] that is an efficient lightweight CNN model. For SVM and XGB, we extract ROIbased WM and GM volumes of each MRI as input. All competing deep learning methods (with default architectures) take whole 3D MRIs as input and share the same training strategy as that used in the downstream model of the BAR. An early-stop training strategy (epoch: 90) is used in all deep learning models. Results of Depression and CI Identification. In this task, we aim to recognize cognitively normal subjects with depression with a higher risk of progressing to CI than healthy subjects. The results of fourteen methods on the LLD study are reported in Table 1, where '*' denotes that the results of BAR and a competing method are statistically significantly different (p < 0.05 via paired t-test).From the left of Table 1, we have the following observations on CND vs. CN classification. First, our BAR model generally outperforms thirteen competing methods in most cases. For instance, the BAR yields the results of AUC = 70.5% and SEN = 77.3%, which are 4.1% and 10.0% higher than those of the secondbest methods (i.e., SEResNet and ResNet50), respectively. This implies that the brain anatomical MRI features learned by our pretext model on large-scale datasets would be more discriminative, compared with those used in the competing methods. Second, among 10 deep models, our BAR produces the lowest standard deviation in most cases (especially on SEN and SPE), suggesting its robustness to bias introduced by random data partition in the downstream task. This could be due to the strong generalizability of the feature encoder guided by brain anatomy prior (derived from the auxiliary tissue segmentation task). In addition, our BAR significantly outperforms four machine learning methods and two lightweight deep models (i.e., EfficientNet and MobileNet) with p < 0.05.From the right of Table 1, we can see that the overall results of fourteen methods in CI vs. CND classification are usually worse than CND vs. CN classification. This suggests that the task of CI vs. CND classification is more challenging, which could be due to the more imbalanced training data in this task (as shown in Table SII of Supplementary Materials). On the other hand, the proposed BAR still performs best in terms of AUC=64.5% and SPE=67.0%, which are 2.8% and 2.0% higher than those of the second-best competing methods (i.e., XGB-WM and ResNet34), respectively. These results further demonstrate the superiority of the BAR in MRI-based depression recognition.Results of MCI Detection. The results of different methods in MCI detection (i.e., MCI vs. HC classification) on the DM study are reported in Table 2. There are a total of 42 subjects (i.e., 17 MCI and 25 HC) used for training in this task, which are fewer but more balanced than the two tasks in the LLD study (see Table SII). It can be observed from Tables 1 and2 that the proposed BAR yields relatively lower standard deviations in terms of AUC and ACC in MCI vs. HC classification, compared with the two tasks on the LLD study. These results imply that data imbalance may be an important issue affecting the performance of deep learning models when the number of training samples is limited.Segmentation Results. The pre-trained pretext model can also be used for brain tissue segmentation in downstream studies. Thus, we visualize brain segmentation maps generated by FSL and our BAR for target MRIs in both LLD and DM studies in Fig. 2. Note that T1-weighted MRIs in the LLD study are collected from 2 sites and have more inconsistent image quality when compared to those from DM. From Fig. 2, we have several interesting observations.First, the segmentation results generated by the proposed BAR are generally better than those of FSL in most cases, especially for those cortical surface areas on the two studies. For instance, the WM region in segmentation maps generated by our BAR is much cleaner than that of FSL, indicating that our model is not sensitive to noise in MRI. Even for the LLD study with significant inter-site data heterogeneity, the boundary of WM and GM produced by BAR is more continuous and smoother, which is in line with the brain anatomy prior. Second, for MRIs with severe motion artifacts in the LLD study (IDs: 1240, 1334, and 1653), our method can produce high-quality segmentation maps, and the results are even comparable to those of MRIs without motion artifacts. This demonstrates that our model is robust to motion artifacts. The underlying reason could be that the pretext model is trained on large-scale MRIs, and thus, has good generalization ability when applied to MRIs with different image quality. In addition, both BAR and FSL often achieve better results in the DM study, since DM has relatively higher image quality than LLD. Still, the proposed BAR can achieve better segmentation results in many fine-grained brain regions, such as the putamen region (see HC001 and MCI003) and the vermis region (see HC004). These results demonstrate that our method has good adaptability when applied to classification and segmentation tasks in MRI-based studies.Ablation Study. To validate the effectiveness of the learned brain anatomical MRI features, we further compare the BAR with its two variants (called BAR-B and BAR-R) that use anatomy prior derived from different pretext tasks in CND vs. CN classification on LLD. Specifically, the BAR-B is trained from scratch as a baseline on target data without any pre-trained encoder. The BAR-R trains the pretext model through an MRI reconstruction task in an unsupervised learning manner. As shown in Fig. 3(a), the BAR consistently performs better than its variants in terms of all five metrics. This implies that the learned MRI features guided by the segmentation task help promote prediction performance. Also, BAR and BAR-R outperform BAR-B in most cases, implying that brain anatomy prior derived from tissue segmentation or MRI reconstruction can help improve discriminative ability of MRI features and boost prediction performance.Influence of Training Data Size. We also study the influence of training data size on BAR in CND vs. CN classification on LLD. With fixed test data, we randomly select a part of MRIs (i.e., [20%, 40%, • • • , 100%]) from target training data to fine-tune the downstream prediction model. It can be observed from Fig. 3(b) that the overall performance in terms of AUC and ACC of our BAR increases with the increase of training data, and it produces the best results when using all training data for model fine-tuning. This suggests that using more data for downstream model fine-tuning helps promote learning performance."
Brain Anatomy-Guided MRI Analysis for Assessing Clinical Progression of Cognitive Impairment with Structural MRI,4,Conclusion and Future Work,"In this paper, we develop a brain anatomy-guided representation (BAR) learning framework for MRI-based progression prediction of cognitive impairment, incorporated by brain anatomy prior (derived from an auxiliary tissue segmentation task). We validate the proposed BAR on two CI-related studies with T1-weighted MRIs, and the experimental results demonstrate its effectiveness compared with SOTA methods. Besides, the pretext model trained on 9,544 MRIs from ADNI can be well adapted to tissue segmentation in the two CI-related studies. There is significant intra-and inter-site data heterogeneity in LLD with two sites. It is interesting to reduce such heterogeneity using domain adaptation [31,32], which will be our future work. Aside from tissue segmentation, one can use other auxiliary tasks to model brain anatomy, such as brain parcellation and brain MRI to CT translation. Besides, it is meaningful to compare our method with other model pre-training strategies [33,34], which will also be our future work."
Brain Anatomy-Guided MRI Analysis for Assessing Clinical Progression of Cognitive Impairment with Structural MRI,,Table 1 .,"Experimental Setting. Three classification tasks are performed: (1) depression recognition (i.e., CND vs. CN classification) on LLD, (2) CI identification (i.e., CI vs. CND classification) on LLD, and (3) MCI detection (i.e., MCI vs. HC classification) on DM. The partition of training/test set is given in Table"
Prompt-MIL: Boosting Multi-instance Learning Schemes via Task-Specific Prompt Tuning,1,Introduction,"Whole slide image (WSI) classification is a critical task in computational pathology enabling disease diagnosis and subtyping using automatic tools. Owing to the paucity of patch-level annotations, multiple instance learning (MIL) [9,18,24] techniques have become a staple in WSI classification. Under an MIL scheme, WSIs are divided into tissue patches or instances, and a feature extractor is used to generate features for each instance. These features are then aggregated using different pooling or attention-based operators to provide a WSI-level prediction. ImageNet pretrained networks have been widely used as MIL feature extractors. More recently, self-supervised learning (SSL), using a large amount of unlabeled histopathology data, has become quite popular for WSI classification [5,13] as it outperforms ImageNet feature encoders.Most existing MIL methods do not fine-tune their feature extractor together with their classification task; this stems from the requirement for far larger GPU memory than is available currently due to the gigapixel nature of WSIs, e.g. training a WSI at 10x magnification may require more than 300 Gb of GPU memory. Recently, researchers have started to explore optimization methods to enable end-to-end training of the entire network and entire WSI within GPU memory [21,25,29]. These methods show better performance compared to conventional MIL; they suffer, however, from two limitations. First, they are ImageNet-pretrained and do not leverage the powerful learning capabilities of histology-trained SSL models. Second, these are mostly limited to convolutional architectures rather than more effective attention-based architectures such as vision transformers [7]."
Prompt-MIL: Boosting Multi-instance Learning Schemes via Task-Specific Prompt Tuning,,Motivation:,"To improve WSI-level analysis, we explore end-to-end training of the entire network using SSL pretrained ViTs. To achieve this, we use the patch batching and gradient retaining techniques in [25]. However, we find that conventional fine-tuning approaches, where the entire network is fine-tuned, achieve low performance. For example, on the BRIGHT dataset [2], the accuracy drops more than 5% compared to the conventional MIL approaches. The poor performance is probably caused by the large network over-fitted to the limited downstream training data, leading to suboptimal feature representation. Indeed, especially for weakly supervised WSI classification, where annotated data for downstream tasks is significantly less compared to natural image datasets, conventional finetuning schemes can prove to be quite challenging.To address the subpar performance of SSL-pretrained vision transformers, we utilize the prompt tuning techniques. Initially proposed in natural language processing, a prompt is a trainable or a pre-defined natural language statement that is provided as additional input to a transformer to guide the neural network towards learning a specific task or objective [3,12]. Using prompt tuning we fine-tune only the prompt and downstream network without re-training the large backbone (e.g. GPT-3 with 17B parameters). This approach is parameter efficient [12,15] and has been shown to better inject task-specific information and reduce the overfitting in downstream tasks, particularly in limited data scenarios [8,23]. Recently, prompts have also been adopted in computer vision and demonstrated superior performance compared to conventional fine-tuning methods [10]. Prompt tuning performs well even when only limited labeled data is available for training, making it particularly attractive in computational pathology. The process of prompt tuning thus involves providing a form of limited guidance during the training of downstream tasks, with the goal of minimizing the discrepancy between feature representations that are fully tuned to the task and those that are not task-specific.In this paper, we propose a novel framework, Prompt-MIL, which uses prompts for WSI-level classification tasks within an MIL paradigm. Our contributions are:-Fine-tuning: Unlike existing works in histopathology image analysis, Prompt-MIL is fine-tuned using prompts rather than conventional full finetuning methods. -Task-specific representation learning: Our framework employs an SSL pretrained ViT feature extractor with a trainable prompt that calibrates the representations making them task-specific. By doing so, only the prompt parameters together with the classifier, are optimized. This avoids potential overfitting while still injecting task-specific knowledge into the learned representations.Extensive experiments on three public WSI datasets, TCGA-BRCA, TCGA-CRC, and BRIGHT demonstrate the superiority of Prompt-MIL over conventional MIL methods, achieving a relative improvement of 1.49%-4.03% in accuracy and 0.25%-8.97% in AUROC by using only less than 0.3% additional parameters. Compared to the conventional full fine-tuning approach, we finetune less than 1.3% of the parameters, yet achieve a relative improvement of 1.29%-13.61% in accuracy and 3.22%-27.18% in AUROC. Moreover, compared to the full fine-tuning approach, our method reduces GPU memory consumption by 38%-45% and trains 21%-27% faster. To the best of our knowledge, this is the first work where prompts are explored for WSI classification. While our method is quite simple, it is versatile as it is agnostic to the MIL scheme and can be easily applied to different MIL methods. Our code is available at https://github.com/cvlab-stonybrook/PromptMIL."
Prompt-MIL: Boosting Multi-instance Learning Schemes via Task-Specific Prompt Tuning,2,Method,"Our Prompt-MIL framework consists of three components: a frozen feature model to extract features of tissue patches, a classifier that performs an MIL scheme of feature aggregation and classification of the WSIs, and a trainable prompt. Given a WSI and its label y, the image is tiled into n tissue patches/instances {x 1 , x 2 , . . . , x n } at a predefined magnification. As shown in Fig. 1, the feature model F (•) computes n feature representations from the corresponding n patches:where h i denotes the feature of the i th patch, h is the concatenation of all h i , and P = {p i , i = 1, 2, . . . , k} is the trainable prompt consisting of k trainable tokens.The classifier G(•) applies an MIL scheme to predict the label ŷ and calculate the loss L as:where the L cls is a classification loss."
Prompt-MIL: Boosting Multi-instance Learning Schemes via Task-Specific Prompt Tuning,2.1,Visual Prompt Tuning,"The visual prompt tuning is the key component of our framework. As shown in Fig.  where t 0 i is the embedding token of z i and T 0 z is the collection of such tokens. These tokens T 0 z are concatenated with a class token t 0 cls and a prompt P: The class token is used to aggregate information from all other tokens. The prompt consists of k trainable tokens P = {p i |i = 1, 2, . . . , k}. The concatenation is fed into l layers of the Transformer encoders:where p i j is the j th output prompt token of the i th Transformer encoder and T i P is the collection of all k such output prompt tokens, which are not trainable. The output feature of x i is defined as the last class token:"
Prompt-MIL: Boosting Multi-instance Learning Schemes via Task-Specific Prompt Tuning,2.2,Optimization,"Our overall loss function is defined aswhere only the parameters of the G(•) and the prompt P are optimized, while the feature extractor model F (•) is frozen.Training the entire pipeline in an end-to-end fashion on gigapixel images is infeasible using the current hardware. To address this issue, we utilize the patch batching and gradient retaining techniques from [25]. As shown in Fig. 1(a), to reduce the GPU memory consumption, the n tissue patches {x 1 , x 2 , . . . , x n } are grouped into m batches. The first step (step① in the figure) of our optimization is to sequentially feed m batches of tissue patches forward to the feature model to compute its respective features which are subsequently concatenated into the h matrix. In this step, we just conduct a forward pass like the inference stage, without storing the memory-intensive computational graph for back-propagation.In the second step (step②), we feed h into the classifier G(•) to calculate the loss L and update the parameters of G(•) by back-propagate the loss. The back-propagated gradients g = ∂L/∂h on h are retained for the next step.Finally (step③), we feed the input batches into the feature model F (•) again and use the output h and the retained gradients g from the last step to update the trainable prompt tokens. In particular, the gradients on the j th prompt token p j are calculated as:where g i is the gradient calculated with respect to h i .To sum up, in each step, we only update either F or G given the current batch, which avoid storing the gradients of the whole framework for all the input patches. This patch batching and gradient retaining techniques make the end-to-end training feasible.In this study, we use DSMIL [13] as the classifier and binary cross entropy as the classification loss L cls when the task is a tumor sub-type classification or cross entropy otherwise."
Prompt-MIL: Boosting Multi-instance Learning Schemes via Task-Specific Prompt Tuning,3.1,Datasets,"We assessed Prompt-MIL using three histopathological WSI datasets: TCGA-BRCA [14], TCGA-CRC [19], and BRIGHT [2]. These datasets were utilized for both the self-supervised feature extractor pretraining and the end-to-end finetuning (with or without prompts), including the MIL component. Note that the testing data were not used in the SSL pretraining. TCGA-BRCA contains 1034 diagnostic digital slides of two breast cancer subtypes: invasive ductal carcinoma (IDC) and invasive lobular carcinoma (ILC). We used the same training, validation, and test split as that in the first fold cross validation in [5]. The cropped patches (790K training, 90K test) were extracted at 5× magnification. TCGA-CRC contains 430 diagnostic digital slides of colorectal cancer for a binary classification task: chromosomal instability (CIN) or genome stable (GS). Following the common 4-fold data split [1,16], we used the first three folds for training (236 GS, 89 CIN), and the fourth for testing (77 GS, 28 CIN). We further split 20% (65 slides) training data as a validation set. The cropped patches (1.07M training, 370K test) were extracted at 10× magnification. BRIGHT contains 503 diagnostic slides of breast tissues. We used the official training (423 WSIs) and test (80 WSIs) splits. "
Prompt-MIL: Boosting Multi-instance Learning Schemes via Task-Specific Prompt Tuning,3.2,Implementation Details,"We cropped non-overlapping 224 × 224 sized patches in all our experiments and used ViT-Tiny (ViT-T/16) [7] for feature extraction. For SSL pretraining, we leveraged the DINO framework [4] with the default hyperparameters, but adjusted the batch size to 256 and employed the global average pooling for token aggregation. We pretrained separate ViT models on the TCGA-CRC datasets for 50 epochs, on the BRIGHT dataset for 50 epochs, and on the BRCA dataset for 30 epochs. For TCGA-BRCA, we used the AdamW [17] optimizer with a learning rate of 1e -4, 1e -2 weight decay, and trained for 40 epochs. For TCGA-CRC, we also used the AdamW optimizer with a learning rate of 5e -4 and trained for 40 epochs. For Bright, we used the Adam [11] optimizer with a learning rate of 1e -4, 5e -2 weight decay and trained for 40 epochs. We applied a cosine annealing learning rate decay policy in all our experiments. For the MIL baselines, we employed the same hyperparameters as above. For all full fine-tuning experiments, we used the learning rate in the corresponding prompt experiment as the base learning rate. For parameters in the feature model F (•), which are SSL pretrained, we use 1/10 of the base learning rate. For parameters in the Classifier G(•), which are randomly initialized, we use the base learning rate. We train the full tuning model for 10 more epochs than our prompt training to allow full convergence. This training strategy is optimized using the validation datasets. All model implementations were in PyTorch [20] on a NVIDIA Tesla V100 or a Nvidia Quadro RTX 8000."
Prompt-MIL: Boosting Multi-instance Learning Schemes via Task-Specific Prompt Tuning,3.3,Results,"We chose overall accuracy and Area Under Receiver Operating Characteristic curve (AUROC) as the evaluation metrics.Evaluation of Prompt Tuning Performance: We compared the proposed Prompt-MIL with two baselines: 1) a conventional MIL model with a frozen feature extractor [13], 2) fine-tuning all parameters in the feature model (full fine-tuning). Table 1 highlights that our Prompt-MIL consistently outperformed both. Compared to the conventional MIL method, Prompt-MIL added negligible parameters (192, less than 0.3% of the total parameters), achieving a relative improvement of 1.49% in accuracy and 0.25% in AUROC on TCGA-BRCA, 3.36% in accuracy and 8.97% in AUROC on TCGA-CRC, and 4.03% in accuracy and 0.43% in AUROC on BRIGHT. The observed improvement can be attributed to a more optimal alignment between the feature representation learned during the SSL pretraining and the downstream task, i.e., the prompt explicitly calibrated the features toward the downstream task.The computationally intensive full fine-tuning method under-performed conventional MIL and Prompt-MIL. Compared to the full fine-tuning method, our method achieved a relative improvement of 1.29% to 13.61% in accuracy and 3.22% to 27.18% in AUROC on the three datasets. Due to the relatively small amount of slide-level labels (few hundred to a few thousands) fully fine tuning 5M parameters in the feature model might suffer from overfitting. In contrast, our method contained less than 1.3% of parameters compared to full fine-tuning, leading to robust training.  Evaluation of Time and GPU Memory Efficiency: Prompt-MIL is an efficient method requiring less GPU memory to train and running much faster than full fine-tuning methods. We evaluated the training speed and memory consumption of our method and compared to the full fine-tuning baseline on four different sized WSIs in the BRIGHT dataset. As shown in Table 2, our method consumed around 38% to 45% less GPU memory compared to full finetuning and was 21% to 27% faster. As we scaled up the WSI size (i.e. WSIs with more number of patches), the memory cost difference between Prompt-MIL and full fine-tuning further widened.Evaluation on the Pathological Foundation Models: We demonstrated our Prompt-MIL also had a better performance when used with the pathological foundation model. Foundational models refer to those trained on large-scale pathology datasets (e.g. the entire TCGA Pan-cancer dataset [28]). We utilized the publicly available [26,27] ViT-Small network pretrained using MoCo v3 [6] on all the slides from TCGA [28] and PAIP [22]. In Table 3, we showed that our method robustly boosted the performance on both TCGA (the same domain as the foundation model trained on) and BRIGHT (a different domain). The improvement is more prominent in BRIGHT, which further confirmed that Prompt-MIL aligns the feature extractor to be more task-specific. Ablation Study: An ablation was performed to study the effect of the number of trainable prompt tokens on downstream tasks. Table 4 shows the accuracy and AUROC of our Prompt-MIL model with 1, 2 and 3 trainable prompt tokens (k = 1, 2, 3) on the TCGA-BRCA and the BRIGHT datasets. On the TCGA-BRCA dataset, our Prompt-MIL model with 1 to 3 prompt tokens reported similar performance. On the BRIGHT dataset, the performance of our model dropped with the increased number of prompt tokens. Empirically, this ablation study shows that for classification tasks, one prompt token is sufficient to boost the performance of conventional MIL methods."
Prompt-MIL: Boosting Multi-instance Learning Schemes via Task-Specific Prompt Tuning,4,Conclusion,"In this work, we introduced a new framework, Prompt-MIL, which combines the use of Multiple Instance Learning (MIL) with prompts to improve the performance of WSI classification. Prompt-MIL adopts a prompt tuning mechanism rather than a conventional full fine-tuning of the entire feature representation.In such a scheme, only a small fraction of parameters calibrates the pretrained representations to encode task-specific information, so the entire training can be performed in an end-to-end manner. We applied our proposed method to three publicly available datasets. Extensive experiments demonstrated the superiority of Prompt-MIL over the conventional MIL as well as the conventional fully finetuning methods. Moreover, by fine-tuning much fewer parameters compared to fully fine-tuning, our method is GPU memory efficient and fast. Our proposed approach also showed promising potentials in transferring foundation models. We will further explore the task-specific features that are captured by our prompt toward explainability of these models."
Multimodal Brain Age Estimation Using Interpretable Adaptive Population-Graph Learning,1,Introduction,"Healthy brain aging follows specific patterns [2]. However, various neurological diseases, such as Alzheimer's disease [8], Parkinson's disease [23], and schizophrenia [18], are accompanied by an abnormal accelerated aging of the human brain. Thus, the difference between the biological brain age of a person and their chronological age can show the deviation from the healthy aging trajectory, and may prove to be an important biomarker for neurodegenerative diseases [6,9].Recently, graph-based methods have been explored for brain age estimation as graphs can inherently combine multimodal information by integrating the subjects' neuroimaging information as node features and, through a similarity metric, the associations among subjects through as edges that connect these nodes [21]. However, in medical applications, the construction of a population-graph is not always simple as there are various ways subjects could be considered similar.GCNs [16] have been extensively [1] used in the medical domain for node classification tasks, such as Alzheimer's prediction [15,21] and Autism classification [4]. They take graphs as input and, in most cases, the graph structure is predefined and static. GCN performance is highly dependent on the graph structure. This has been correlated in related literature with the heterophily of graphs in the semi-supervised node classification tasks, which refers to the case when the nodes of a graph are connected to nodes that have dissimilar features and different class labels [29]. It has been shown that if the homophily ratio is very low, a simple Multi-Layer Perceptron (MLP) that completely ignores the structure, can outperform a GCN [30].A way to address this problem is through adaptive graph learning [27], which learns the graph structure through training. In the medical domain, there is little ongoing research on the topic [13,28]. However, the adaptive graphs in [7,13,14] Fig. 1. Proposed methodology: The non-imaging features q i and a subset of the imaging features si ⊆ xi per subject i are used as input to a MLP, which produces attention weights for each one of these features. Based on these, the edges of the graph are stochastically sampled using the Gumbel-Top-k trick. The constructed graph, which uses the imaging features X as node features is used to train a GCN for brain age estimation tasks.are connected based on the imaging features and do not take advantage of the associations of the non-imaging information for the edges. In [11], non-imaging information is used for graph connectivity, but the edges are already pre-pruned similar to [21], and only the weights of the edges can be modified. In [26], even though the graph is dynamic, it is not being learnt. While the extracted graphs in these studies are optimized for the task at hand, there is no explanation for the node connections that were proposed. Given that interpretability is essential in the medical domain [24], since the tools need to be trusted by the clinicians, an adaptive graph learnt during the training, whose connections are also interpretable and clear can prove important. Additionally, most existing works focus on brain age classification in four bins, usually classifying the subjects per decade [7,13,14]. Age regression, which is a more challenging task, has not been extensively explored with published results not reaching sufficient levels of performance [25].Contributions. This paper has the following contributions: 1) We combine imaging and non-imaging information in an attention-based framework to learn adaptively an optimized graph structure for brain age estimation. 2) We propose a novel graph loss that enables end-to-end training for the task of brain age regression. 3) Our framework is inherently interpretable as the attention mechanism allows us to rank all imaging and non-imaging phenotypes according to their significance for the task. 4) We evaluate our method on the UK Biobank (UKBB) and achieve state-of-the-art results on the tasks of brain age regression and classification. The code can be found on GitHub at: https://github.com/ bintsi/adaptive-graph-learning."
Multimodal Brain Age Estimation Using Interpretable Adaptive Population-Graph Learning,2,Methods,"Given a set of N subjects with M features X = [x 1 , ..., x N ] ∈ R N ×M and labels y ∈ R N , a population graph is defined as G = {V, E}, where V is a set of nodes, one per subject, and E is a set of paired nodes that specifies the connectivity of the graph, meaning the edges of the graph. To create an optimized set of edges for the task of brain age estimation, we leverage a set of non-imaging phenotypes q i ∈ R Q and a set of imaging phenotypes s i ∈ R S per subject i through an attention-based framework. The imaging phenotypes are a subset of the imaging features s i ⊆ x i . The phenotypes are selected according to [5]. The resulting graph is used as input in a GCN that performs node-level prediction tasks. Here, we give a detailed description of the proposed architecture, in which the connectivity of the graph E, is learnt through end-to-end training in order to find the optimal graph structure for the task of brain age estimation. An outline of the proposed pipeline may be found in Fig. 1."
Multimodal Brain Age Estimation Using Interpretable Adaptive Population-Graph Learning,,Attention Weights Extraction.,"Based on the assumption that not all phenotypes are equally important for the construction of the graph, we train a MLP g θ , with parameters θ, which takes as input both the non-imaging features q i and the imaging features s i for every subject i and outputs an attention weight vector a ∈ R Q+S , where every element of a corresponds to a specific phenotype. Intuitively, we expect that the features that are relevant to brain age estimation will get attention weights close to 1, and close to 0 otherwise. Since we are interested in the overall relevance of the phenotypes for the task, the output weights need to be global and apply to all of the nodes. To do so, we average the attention weights across subjects and normalize them between 0 and 1.Edge Extraction. The weighted phenotypes for each subject i are calculated as f i = a (q i s i ), where f i ∈ R Q+S , a are the attention weights produced by the MLP g θ , (• •) denotes the concatenation function between two vectors and denotes the Hadamard product. We further define the probability p ij (f i , f j ; θ, t) of a pair of nodes (i, j) ∈ V to be connected in Eq. ( 1):where t is a learnable parameter and d is a distance function that calculates the distance between the weighted phenotypes of two nodes. To keep the memory cost low, we create a sparse k-degree graph. We use the Gumbel-Top-k trick [17], which acts as a stochastic relaxation of the kNN rule, in order to sample k edges for every node according to the probability matrix P ∈ R N ×N . Since there is stochasticity in the sampling scheme, multiple runs are performed at inference time and the predictions are averaged.Optimization. The extracted graph is used as input, along with the imaging features X, which are used as node features, to a GCN g ψ , with parameters ψ, which comprises of a number of graph convolutional layers, followed by fully connected layers. The pipeline is trained end-to-end with a loss function L that consists of two components and is defined as in Eq. ( 2).The first component, L GCN , is optimizing the GCN, g ψ . For regression we use the Huber loss [12], while for classification we use the Cross Entropy loss function.The second component, L graph , optimizes the MLP g θ , whose output are the phenotypes' attention weights. However, the graph is sparse with discrete edges and hence the network cannot be trained with backpropagation as is. To alleviate this issue, we formulate our graph loss in a way that rewards edges that lead to correct predictions and penalize edges that lead to wrong predictions. Inspired by [13], where a similar approach was used for classification, the proposed graph loss function is designed for regression instead and is defined in Eq. ( 3):where ρ(•, •) is the reward function which is defined in Eq. (4):Here ε is the null model's prediction (i.e. the average brain age of the training set). Intuitively, when the prediction error |y ig ψ (x i )| is smaller than the null model's prediction then the reward function will be negative. In turn, this will encourage the maximization of p ij so that L graph is minimized."
Multimodal Brain Age Estimation Using Interpretable Adaptive Population-Graph Learning,3,Experiments,"Dataset. The proposed framework is evaluated on the UKBB [3], which provides not only a wide collection of images of vital organs, including brain scans, but also various non-imaging information, such as demographics, and biomedical, lifestyle, and cognitive performance measurements. Hence, it is perfectly suitable for brain age estimation tasks that incorporate the integration of imaging and non-imaging information. Here, we use 68 neuroimaging phenotypes and 20 non-imaging phenotypes proposed by [5] as the ones most relevant to brain age in UKBB. The neuroimaging features are provided by UKBB, and include measurements extracted from structural MRI and diffusion weighted MRI. All phenotypes are normalized from 0 to 1. The age range of the subjects is 47-81 years. We only keep the subjects that have available the necessary phenotypes ending up with about 6500 subjects. We split the dataset into 75% for the training set, 5% for the validation set, and 20% for the test set. Our pipeline has been primarily designed to tackle the challenging regression task and therefore the main experiment is brain age regression. We also evaluate our framework on the 4-class classification task that has been used in other related papers.Baselines. Given that a GCN trained on a meaningless graph can perform even worse than a simple regressor/classifier, our first baseline is a Linear/Logistic Regression model. For the second baseline, we construct a static graph based on a similarity metric, more specifically cosine similarity, of a set of features (either node features or non-imaging and imaging phenotypes) using the kNN rule with k = 5 and train a GCN on that graph. Using euclidean distance as the similarity metric leads to worse performance and is therefore not explored for the baselines. We also compare our method with DGM, which is the state-ofthe-art on graph learning for medical applications [13]. Since this work is only applicable for classification tasks, we extend it to regression, implementing the graph loss function we used for our pipeline as well.Implementation Details. The GCN architecture uses ReLU activations and consists of one graph convolutional layer with 512 units and one fully connected layer with 128 units before the regression/classification layer. The number and the dimensions of the layers are determined through hyperparameter search based on validation performance. The networks are trained with the AdamW optimizer [20], with a learning rate of 0.005 for 300 epochs and utilize an early stopping scheme. The average brain age of the training set, which is used in Eq.(3), is ε = 6. We use PyTorch [22] and a Titan RTX GPU. The reported results are computed by averaging 10 runs with different initializations."
Multimodal Brain Age Estimation Using Interpretable Adaptive Population-Graph Learning,3.1,Results,"Regression. For the evaluation of the pipeline in the regression task, we use Mean Absolute Error (MAE), and the Pearson Correlation Coefficient (r score). A summary of the performance of the proposed and competing methods is available at Table 1. Linear regression (MAE = 3.82) outperforms GCNs trained on static graphs whether these are based on the node features (MAE = 3.87) or leverage the phenotypes (MAE = 3.98). The DGM outperforms the baselines (MAE = 3.72). The proposed method outperforms all others (MAE = 3.61).Classification. For the classification task, we divide the subjects into four balanced classes. The metrics used for the classification task to evaluate the performance of the model are accuracy, the area under the ROC curve (AUC), which is defined as the average of the AUC curve of every class, and the Macro F1-score (Table 1). A similar trend to the regression task appears here, with the proposed method reaching top performance with 58% accuracy. Our hypothesis that the construction of a pre-defined graph structure is suboptimal, and might even hurt performance, is confirmed. Both for the classification and regression tasks, GCNs trained on static graphs do not outperform a simple linear model that ignores completely the structure of the graph. The proposed approach proves that not all phenotypes are equally important for the construction of the graph, and that giving attention weights accordingly increases the performance for both tasks."
Multimodal Brain Age Estimation Using Interpretable Adaptive Population-Graph Learning,3.2,Ablation Studies,"Number of Phenotypes. We perform an ablation test to investigate the effect of the number of features used for the extraction of the edges. We used only non-imaging phenotypes, only imaging phenotypes, or a combination of both (Table 2). The combination of imaging and non-imaging phenotypes (MAE = 3.61) performs better than using either one of them, while adding more imaging phenotypes does not necessarily improve performance.Distance Metrics. Moreover, we explore how different distance metrics affect performance. Here, euclidean, cosine, and hyperbolic [19] distances are explored. We also include a random edge selection to examine whether using the phenotypic information improves the results. The results (Table 2) indicate that euclidean distance performs the best, closely followed by cosine similarity. Hyperbolic performed a bit worse (MAE = 4 years), while random edges gave a MAE of 5.59. Regardless of the distance metric, phenotypes do incorporate valuable information as performance is considerably better than using random edges."
Multimodal Brain Age Estimation Using Interpretable Adaptive Population-Graph Learning,3.3,Interpretability,"A very important advantage of the pipeline is that the graph extracted through the training is interpretable, in terms of why two nodes are connected or not. Visualizing the attention weights given to the phenotypes, we get an understanding of the features that are the most relevant to brain aging. The regression problem is clinically more important, thus we will focus on this in this section. A similar trend was presented for the classification task as well.The imaging and non-imaging phenotypes that were given the highest attention scores in the construction of the graph can be seen in Fig. 2. We color the non-imaging phenotypes in pink, and the imaging ones in blue. A detailed list of the names of the phenotypes along with the attention weights given by the pipeline can be found in the Supplementary Material. The non-imaging phenotypes that were given the highest attention weights, were two cognitive tasks, the numeric and the alphanumeric trail making tasks. Systolic blood pressure, and stroke, were the next most important non-imaging phenotypes, even though they were not as important as some of the imaging features. Various neuroimaging phenotypes were considered important, such as information regarding the tract anterior thalamic radiaton, the volume of white matter hyperintensities, gray matter volumes, as well as measurements extracted from the FA (fractional anisotropy, a measure of the connectivity of the brain) skeleton. Our findings are in agreement with the relevant literature [5].Apart from the attention weights, we also visualize the population graph that was used as the static graph of the baseline (Fig. 3(left)). This population graph is constructed based on the cosine similarity of the phenotypes, with all the phenotypes playing an equally important role for the connectivity. In addition, we visualize the population graph using the cosine similarity of the weighted phenotypes (Fig. 3(right)), with the attention weights provided by our trained pipeline. In both of the graph visualizations, the color of each node corresponds to the subject's age. It is clear that learning the graph through our pipeline results in a population graph where subjects with similar ages end up in more compact clusters, whereas the static graph does not demonstrate any form of organization. Since GCNs are affected by a graph construction with low homophily, it is reasonable that the static graphs perform worse than a simple machine learning method, and why the proposed approach manages to produce state-of-the-art results. Using a different GNN architecture that is not as dependent on the constructed graph's homophily could prove beneficial [10,29]."
Multimodal Brain Age Estimation Using Interpretable Adaptive Population-Graph Learning,4,Conclusion,"In this paper, we propose an end-to-end pipeline for adaptive population-graph learning that is optimized for brain age estimation. We integrate multimodal information and use attention scores for the construction of the graph, while also increasing interpretability. The graph is sparse, which minimizes the computational costs. We implement the approach both for node regression and node classification and show that we outperform both the static graph-based and state-of-the-art approaches. We finally provide an insight into the most important phenotypes for graph construction, which are in agreement with related neurobiological literature.In future work, we plan to extract node features from the latent space of a CNN. Training end-to-end will focus on latent imaging features that are also important for the GCN. Such features would potentially be more expressive and improve the overall performance. Finally, we leverage the UKBB because of the wide variety of multimodal data it contains, which makes it a perfect fit for brain age estimation but as a next step we also plan to evaluate our framework on different tasks and datasets."
Multimodal Brain Age Estimation Using Interpretable Adaptive Population-Graph Learning,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3 19.
Joint Representation of Functional and Structural Profiles for Identifying Common and Consistent 3-Hinge Gyral Folding Landmark,1,Introduction,"Cortical folds have been shown to be related to brain function, cognition, and behavior. Based on the research of the past decades, the cortex can be further decomposed into fine-grained basic morphological patterns, such as gyri and sulci. Gyri are more potential to be functional connection centers, which are responsible for exchanging information among remote gyri and nearby sulci; on the contrary, sulci exchange information directly with their nearby gyri [1].Recent studies have shown that gyri can be further separated by the number of hinges it comprises, both for anatomical analysis and for functional timing analysis. Thus, the 3-hinge cyclotron fold pattern was gradually introduced and determined. It has been demonstrated that the 3-hinge regions have a thicker cortex [2], a stronger pattern of fiber connections [3], and a greater diversity of structural connections [4]. These studies revealed the salient features and potential value of the 3-hinge region. In a recent study, it was claimed that 3-hinges have been found as ""connector"" hubs in the brain [5].Although these works have achieved great success, there remain several obstacles to studying and comprehending the role of 3-hinge gyral folding patterns. One of the most significant challenges is identifying common and consistent 3-hinge gyrus folding patterns across subjects, which has yet to be resolved and has impeded group-level 3-hinges analysis. The morphology of cortical folds varies greatly between individuals, so identifying stable 3-hinge regions between individuals is difficult. In 2017 Li et al. addressed this issue by manually labeling 3-hinges across subjects and species (macaques, chimpanzees, and humans), demonstrating that six 3-hinges have functional correspondence across subjects and even across species [4]. In 2020, Zhang et al. proposed a semiautomatic approach that combines the fold morphology of the cerebral cortex and the characteristics of white matter nerve fibers to estimate the correspondence of the 3-hinge regions across subjects [6]. In a recent study, by transferring Dense Individualized and Common Connectivity-Based Cortical Landmarks (DICCCOL) stability across subjects to the 3-hinge regions, a DICCCOL-based K-nearest landmark detection method was proposed, which automatically identified 79 consistent 3-hinge regions [7]. These studies have shown the possibility of automatically identifying consistent 3-hinge regions via data-driven approach. However, those identified consistent 3-hinge regions were still studied by single modality.To identify the three-hinge regions with multi-modal stability via data-driven approach, we present a joint representation of functional and structural profiles for identifying consistent 3-hinges in this paper. We use functional network representation and fiber connectivity pattern of the DICCCOL system to obtain functional and structural consistency, respectively. And combine these two consistencies to identify 38 3-hinge regions that are consistent in both function and structure. We further compare these results with those based solely on structural data, which deepens our understanding of the 3-hinge region. Our work provides a basis for further inter-group analysis of the 3-hinge gyrus."
Joint Representation of Functional and Structural Profiles for Identifying Common and Consistent 3-Hinge Gyral Folding Landmark,2.1,Dataset and Preprocessing,"We used the Q1 release of Human Connectome Project (HCP) [8] consortium and randomly selected 50 human brains from it in this study. The acquisition parameters of functional magnetic resonance imaging (fMRI) data are as follows: 90 × 104 matrix, 220 mm FOV, 72 slices, TR = 0.72 s, TE = 33.1 ms, flip angle = 52°, BW = 2290 Hz/Px, in-plane FOV = 208 × 180 mm, 2.0 mm isotropic voxels. For fMRI images, the preprocessing pipelines included skull removal, motion correction, slice time correction, spatial smoothing, global drift removal. All of these steps are implemented by FMRIB Software Library (FSL) FEAT [9]. We use resting state fMRI and task fMRI data. Among them, task fMRI data contains a total of seven tasks, which are EMOTION, GAMBLING, LANGUAGE, MOTOR, RELATION, SOCIAL and WM.For diffusion weighted imaging (DWI) data, the parameters are as follows: Spinecho EPI, TR = 5520 ms, TE = 89.5 ms, flip angle = 78°, refocusing flip angle = 160°, FOV 210 × 180 (RO × PE), matrix 168 × 144 (RO × PE), slice thickness 1.25 mm, 111 slices, 1.25 mm isotropic voxels, Multiband factor = 3, and Echo spacing = 0.78 ms. Fiber tracking and cortical surface can be reconstructed from DWI dataset. Please refer to [10,11] for more pre-processing details. One subject is randomly selected as template and 10 subjects are randomly selected as referential subjects. The remaining subjects are used as predictive subjects to predict consistent 3-hinges."
Joint Representation of Functional and Structural Profiles for Identifying Common and Consistent 3-Hinge Gyral Folding Landmark,2.2,Joint Representation of Functional and Structural Profiles,"We introduce a joint representation of functional and structural profiles for identifying common and consistent 3-hinges. The method contains four steps, as shown in Fig. 1. First step is to obtain the functional network. The 4-dimensional data of the resting state and the fMRI of 7 tasks are stretched into 2-dimensional data, which are input into dictionary learning and sparse representation algorithm [12]. 400 functional networks were obtained on the data of each modality. Based on prior knowledge, we select 10 functional networks from the resting state data and 23 functional networks from the task state data for this work. In the second step, we randomly select a subject as a template and use it as a bridge to study the consistency of 3-hinges across subjects. The second step is to obtain the corresponding of 3-hinges between the template and subjects using structural data. We improved the DICCCOL-based K-nearest landmark detection method [7] and retained the 3-hinges with the same spatial position and the same nearest DICCCOLs as the candidate matching set D s . The third step is to obtain the corresponding of 3-hinges between the template and subjects using functional network. We register the functional network to all subjects, denote activation intensity vector of functional network with 3-hinge i on the template as A t i , and on other subjects as A s i . The candidate set D f j on subjects match 3-hinge j on the template based on functional network is given by formula 1:where P A t j , A s i represents the Pearson correlation coefficient of A t j and A s i , and the threshold is set to 0.1.The fourth step combines the results of the second step and the third step to obtain stable 3-hinges across subjects. Firstly, the overlap in D s and D f j is found as a matching result of 3-hinges between the template and subject. If multiple ones are found, the one with higher functional similarity is selected. Secondly, 10 subjects are selected as referential subjects. The stable sequence is obtained on the template by statistical matching information between the referential subject and the template. According to the stable sequence and matching information, the consistent 3-hinge regions cross subjects are identified. And it can be used to predict consistent 3-hinge regions on new subjects."
Joint Representation of Functional and Structural Profiles for Identifying Common and Consistent 3-Hinge Gyral Folding Landmark,2.3,"Consistency Analysis from Anatomical, Structural and Functional Perspective","From the previous section, a group of the consistent 3-hinges were identified. Then, it is important to come up with the evaluation operations and check the consistency of these 3-hinges. In this section, the evaluation operations are designed from structural, functional, and anatomical perspectives.For the anatomical perspective, we calculate the voxel-wise distance of 3-hinges across subjects to measure their consistency. We register all subjects into the Montreal Neurological Institute (MNI) standard space. After obtaining the coordinates of 3-hinges, we calculated the voxel-level distance of 3-hinges between different subjects.For the structural perspective, we use the similarity of fiber connection pattern passing through 3-hinges to evaluate the consistency of them. In detail, we count the nerve fibers passing through each 3-hinge region, and then used the trace-map method [13] to convert the nerve fiber bundles into vectors that could be quantified, and evaluated the consistency of the 3-hinges among groups by the Pearson correlation coefficient between these vectors. The trace-map is a computational model that transforms the directional information in the trajectory of an arbitrary bundle to a standard spherical surface, for quantitative comparison of structural connectivity patterns.For the functional perspective, we count the activation intensity of the 3-hinge regions among different functional networks as vector and used the Pearson correlation coefficient of these vectors across different subjects to quantify the functional consistency of 3-hinge regions."
Joint Representation of Functional and Structural Profiles for Identifying Common and Consistent 3-Hinge Gyral Folding Landmark,2.4,Comparative Analysis of Consistent 3-hinges for Structural Data and Multimodal Data,"Both joint representation of functional and structural profiles and DICCCOL-based Knearest landmark detection method are methods that identify consistent 3-hinges by using templates as bridges. We compare the results of the two methods and study the distribution and corresponding functions of the two groups of 3-hinges on the cortical surface. We register consistent 3-hinges on all subjects to the MNI standard space and calculate the distribution of consistent 3-hinges on the Automated anatomical labelling (AAL) template [14]. Since the 90 brain regions of the AAL template are symmetric between the left and right brain, we combine the left and right brain regions. For statistics, we count the distribution of consistent 3-hinges over 45 AAL brain regions on each subject and average across all subjects. We then investigated the functions with consistent 3-hinges distributions according to the functions corresponding to AAL regions."
Joint Representation of Functional and Structural Profiles for Identifying Common and Consistent 3-Hinge Gyral Folding Landmark,3.1,Visualization of the Identified Consistent 3-hinges,The consistent 3-hinges of subjects are shown in Fig. 2.
Joint Representation of Functional and Structural Profiles for Identifying Common and Consistent 3-Hinge Gyral Folding Landmark,,Fig. 2. Visualization of the distribution of identified 3-hinges in the cerebral cortex,"Through adopting a joint representation of functional and structural profiles, stable sequences of length 65 are determined on the template and average 38 consistent 3hinges can be successfully identified on subjects. Figure 2 shows the consistent 3-hinges of 10 referential subjects and the template subject. And we can see that those identified common and consistent 3-hinges are indeed consistent across the subjects.In order to show the consistency of the identified 3-hinges among different subjects subjectively, in Fig. 3, we select 11 subjects (including a template, five referential subjects, and five predictive subjects) and mark the identified 3-hinges with the same color. As shown in Fig. 3, most of the 3-hinges can be found in the corresponding 3-hinges regions between different subjects, and these consistent 3-hinges regions are relatively close. We will quantitatively analyze the consistency of the 3-hinges identified in the following section. "
Joint Representation of Functional and Structural Profiles for Identifying Common and Consistent 3-Hinge Gyral Folding Landmark,3.2,Effectiveness of the Proposed Consistent 3-hinges,"After the identification of 3-hinges, it is important to evaluate whether they are common and consistent. To evaluate the consistency of these identified 3-hinges, we conduct quantitative experiments from three perspectives of anatomy, structure, and function to evaluate the performance mentioned in Sect. 2.3.From the anatomical perspective, all the subjects are registered into the MNI standard space via a linear algorithm. Then, for each corresponding 3-hinge, the voxel-level distances are calculated to measure the distance between the template and other subjects. As shown in Table 1, consistent 3-hinge regions based on multi-modal data have better inter-population stability than three-hinge regions based on structural data. From the structural perspective, the average Pearson correlation coefficient of fiber connection patterns of identified 3-hinges is as high as 0.44. As a comparison, the similarity is 0.37 for consistent 3-hinges based on structural data. This shows that the method of adding functional data to identify the consistent 3-hinges also has a great improvement in the structural stability of the identified 3-hinges. In order to better demonstrate the structural consistency of these stable 3-hinges, in Fig. 4, we visualized six consistent 3-hinges on 11 selected subjects and the neural fiber modes across these 3-hinges. The stable 3-hinges and the corresponding nerve fibers are marked with the same color, and the similarity of the nerve fibers across the 3-hinge regions between the template and subjects is also marked with the same color. As shown in Fig. 4, the corresponding similarity of the 3-hinges in dark purple is close to the average similarity, and the corresponding nerve fibers are also similar between populations. This verifies the structural consistency of the identified 3-hinges.From the functional perspective, the average similarity of activation intensity of consistent 3-hinges in a functional network based on multi-modal data between template and subject is 0.441, while the similarity based on structural data is 0.366. This indicates that the addition of functional data analysis does greatly improve the functional consistency of the identified 3-hinges.In general, in this section, we verify that the consistency of the identified 3-hinges is significantly stronger than that of the methods based on structural data.  "
Joint Representation of Functional and Structural Profiles for Identifying Common and Consistent 3-Hinge Gyral Folding Landmark,4,Conclusion,"In this work, we propose a joint representation of functional and structural profiles in a data-driven approach for identifying consistent 3-hinges. We use functional network representation and fiber connectivity pattern of the DICCCOL system to obtain functional and structural consistency, respectively. And combine these two consistencies to identify 38 functionally and structurally consistent 3-hinge regions. Compared with the single-modal method with DICCCOL only, the results obtained by our proposed multimodal method have a more consistent 3-hinge pattern across subjects. And we further analyze that consistent 3-hinge regions based on multimodal data are closer to visual functions, while 3-hinge regions based on structural data are closer to memory and motor function."
Single-subject Multi-contrast MRI Super-resolution via Implicit Neural Representations,1,Introduction,"In clinical practice, Magnetic Resonance Imaging (MRI) provides important information for diagnosing and monitoring patient conditions [4,16]. To capture the complex pathophysiological aspects during disease progression, multiparametric MRI (such as T1w, T2w, DIR, FLAIR) is routinely acquired. Image acquisition inherently poses a trade-off between scan time, resolution, and signalto-noise ratio (SNR) [19]. To maximize the source of information within a reasonable time budget, clinical protocol often combines anisotropic 2D scans of different contrasts in complementary viewing directions. Although acquired 2D scans offer an excellent in-plane resolution, they lack important details in the orthogonal out-of-plane. For a reliable pathological assessment, radiologists often resort to a second scan of a different contrast in the orthogonal viewing direction. Furthermore, poor out-of-plane resolution significantly affects the accuracy of volumetric downstream image analysis, such as radiomics and lesion volume estimation, which usually require isotropic 3D scans. As multi-parametric isotropic 3D scans are not always feasible to acquire due to time-constraints [19], motion [9], and patient's condition [10], super-resolution offers a convenient alternative to obtain the same from anisotropic 2D scans. Recently, it has been shown that acquiring three complementary 2D views of the same contrast may yield higher SNR at reduced scan time [19,29]. However, it remains under-explored if orthogonal anisotropic 2D views of different contrasts can benefit from each other based on the underlying anatomical consistency. Additionally, whether such strategies can further decrease scan times while preserving similar resolution and SNR remains unanswered. Moreover, unlike conventional super-resolution models trained on a cohort, a personalized model is of clinical relevance to avoid the danger of potential misdiagnosis caused by cohort-learned biases. In this work, we mitigate these gaps by proposing a novel multi-contrast super-resolution framework that only requires the patient-specific low-resolution MR scans of different sequences (and views) as supervision. As shown in various settings, our approach is not limited to specific contrasts or views but provides a generic framework for super-resolution. The contributions in this paper are three-fold: 1. To the best of our knowledge, our work is the first to enable subject-specific multi-contrast super-resolution from low-resolution scans without needing any high-resolution training data. We demonstrate that Implicit Neural Representations (INR) are good candidates to learn from complementary views of multi-parametric sequences and can efficiently fuse low-resolution images into anatomically faithful super-resolution. 2. We introduce Mutual Information (MI) [26] as an evaluation metric and find that our method preserves the MI between high-resolution ground truths in its predictions. Further observation of its convergence to the ground truth value during training motivates us to use MI as an early stopping criterion. 3. We extensively evaluate our method on multiple brain MRI datasets and show that it achieves high visual quality for different contrasts and views and preserves pathological details, highlighting its potential clinical usage.Related Work. Single-image super-resolution (SISR) aims at restoring a highresolution (HR) image from a low-resolution (LR) input from a single sequence and targets applications such as low-field MR upsampling or optimization of MRI acquisition [3]. Recent methods [3,8] incorporate priors learned from a training set [3], which is later combined with generative models [2]. On the other hand, multi-image super-resolution (MISR) relies on the information from complementary views of the same sequence [29] and is especially relevant to capturing temporal redundancy in motion-corrupted low-resolution MRI [9,27]. Multi-contrast Super-resolution (MCSR) targets using inter-contrast priors [20]. In conventional settings [15], an isotropic HR image of another contrast is used to guide the reconstruction of an anisotropic LR image. Zeng et al. [30] use a two-stage architecture for both SISR and MCSR. Utilizing a feature extraction network, Lyu et al. [14] learn multi-contrast information in a joint feature space. Later, multi-stage integration networks [6], separatable attention [7] and transformers [13] have been used to enhance joint feature space learning. However, all current MCSR approaches are limited by their need for a large training dataset. Consequently, this constrains their usage to specific resolutions and further harbors the danger of hallucination of features (e.g., lesions, artifacts) present in the training set and does not generalize well to unseen data.Originating from shape reconstruction [18] and multi-view scene representations [17], Implicit Neural Representations (INR) have achieved state-of-the-art results by modeling a continuous function on a space from discrete measurements. Key reasons behind INR's success can be attributed to overcoming the low-frequency bias of Multi-Layer Perceptrons (MLP) [21,24,25]. Although MRI is a discrete measurement, the underlying anatomy is a continuous space. We find INR to be a good fit to model a continuous intensity function on the anatomical space. Once learned, it can be sampled at an arbitrary resolution to obtain the super-resolved MRI. Following this spirit, INRs have recently been successfully employed in medical imaging applications ranging from k-space reconstruction [11] to SISR [29]. Unlike [22,29], which learn anatomical priors in single contrasts, and [1,28], which leverage INR with latent embeddings learned over a cohort, we focus on employing INR in subject-specific, multi-contrast settings."
Single-subject Multi-contrast MRI Super-resolution via Implicit Neural Representations,2,Methods,"In this section, we first formally introduce the problem of joint super-resolution of multi-contrast MRI from only one image per contrast per patient. Next, we describe strategies for embedding information from two contrasts in a shared space. Subsequently, we detail our model architecture and training configuration.Problem Statement. We denote the collection of all 3D coordinates of interest in this anatomical space as Ω = {(x, y, z)} with anatomical function q : Ω → A. The image intensities are a function of the underlying anatomical properties A. Two contrasts C 1 and C 2 can be scanned in a low-resolution subspace Ω 1 , Ω 2 ⊂ Ω. Let us consider g 1 , g 2 : A → R that map from anatomical properties to contrast intensities C 1 and C 2 , respectively. We obtain sparse observations, where f i is composition of g i and q. However, one can easily obtain the global anatomical space Ω by knowing Ω 1 and Ω 2 , e.g., by rigid registration between the two images. In this paper, we aim to estimate f 1 , f 2 : Ω → R given I 1 and I 2 .Joint Multi-contrast Modelling. Since both component-functions f 1 and f 2 operate on a subset of the same input space, we argue that it is beneficial to model them jointly as a single function f : Ω → R 2 and optimize it based on their estimation error incurred in their respective subsets. This will enable information transfer from one contrast to another, thus improving the estimation and preventing over-fitting in single contrasts, bringing consistency to the prediction.To this end, we propose to leverage INR to model a continuous multi-contrast function f from discretely sampled sparse observations I 1 and I 2 .MCSR Setup. Without loss of generalization, let us consider two LR input contrasts scanned in two orthogonal planes p 1 and p 2 , where p 1 , p 2 ∈ {axial, sagittal, coronal}. We assume they are aligned by rigid registration requiring no coordinate transformation. Their corresponding in-plane resolutions are (s 1 ×s 1 ) and (s 2 × s 2 ) and slice thickness is t 1 and t 2 , respectively. Note that s 1 < t 1 and s 2 < t 2 imply high in-plane and low out-of-plane resolution. In the end, we aim to sample an isotropic (s × s × s) grid for both contrasts where s ≤ s 1 , s 2 .Implicit Neural Representations for MCSR. We intend to project the information available in one contrast into another by embedding both in the shared weight space of a neural network. However, a high degree of weight sharing could hinder contrast-specific feature learning. Based on this reasoning, we aim to hit the sweet spot where maximum information exchange can be encouraged without impeding contrast-specific expressiveness. We propose a split-head architecture, as shown in Fig. 1, where the initial layers jointly learn the common anatomical features, and subsequently, two heads specialize in contrast-specific information. The model takes Fourier [25] Features v = [cos(2πBx), sin(2πBx)] T as input and predicts [ Î1 , Î2 ] = f (v), where x = (x, y, z) and B is sampled from a Gaussian distribution N (μ, σ 2 ). We use mean-squared error loss, L MSE , for training.where α and β are coefficients for the reconstruction loss of two contrasts. Note that for points {(x, y, z)} ∈ Ω 2 \ Ω 1 , there is no explicit supervision coming from low resolution C 1 . For these points, one can interpret learning C 1 from the loss in C 2 , and vice versa, to be a weakly supervised task."
Single-subject Multi-contrast MRI Super-resolution via Implicit Neural Representations,,Implementation and Training.,"Given the rigidly registered LR images, we compute Ω 1 , Ω 2 ∈ Ω in the scanner reference space using their affine matrices. Subsequently, we normalize Ω to the interval [-1, 1] 3 and independently normalize each contrast's intensities to [0, 1]. We use 512-dimensional Fourier Features in the input. Our model consists of a four-layer MLP with a hidden dimension of 1024 for the shared layers and two layers with a hidden dimension of 512 for the heads. We use Adam optimizer with a learning rate of 4e-4 and a Cosine annealing rate scheduler with a batch size of 1000. For the multi-contrast INR models, we use MI as in Eq. 2 for early stopping. Implemented in PyTorch, we train our model on a single A6000 GPU. Please refer to Table 3 in supplementary for an exhaustive hyper-parameter search.Model Selection and Inference. Since our model is trained on sparse sets of coordinates, it is prone to overfitting them and has little incentive to generalize in out-of-plane predictions for single contrast settings. A remedy to this is to hold random points as a validation set. However, this will reduce the number of training samples and hinder the reconstruction of fine details. For multi-contrast settings, one can exploit the agreement between the two predicted contrasts. Ideally, the network should reach an equilibrium between the contrasts over the training period, where both contrasts optimally benefit from each other. We empirically show that Mutual Information (MI) [26] is a good candidate to capture such an equilibrium point without the need for ground truth data in its computation. For two predicted contrasts Î1 and Î2 , MI can be expressed as:Compared to image registration, we do not use MI as a loss for aligning two images; instead, we use it as a quantitative assessment metric. Given two ground truth HR images for a subject, one can compute the optimum state of MI. We observe that the MI between our model predictions converges close to such an optimum state over the training period without any explicit knowledge about it, c.f. Fig. 3 in the supplementary. This observation motivates us to detect a plateau in MI between the predicted contrasts and use it as a stopping criterion for model selection in multi-contrast INR."
Single-subject Multi-contrast MRI Super-resolution via Implicit Neural Representations,3,Experiments and Results,"Datasets. To enable fair evaluation between our predictions and the reference HR ground truths, the in-plane SNR between the LR input scan and corresponding ground truth has to match. To synthetically create 2D LR images, it is necessary to downsample out-of-plane in the image domain anisotropically [32] while preserving in-plane resolution. Consequently, to mimic realistic 2D clinical protocol, which often has higher in-plane details than that of 3D scans, we use spline interpolation to model partial volume and downsampling. We demonstrate our network's modeling capabilities for different contrasts (T1w, T2w, FLAIR, DIR), views (axial, coronal, sagittal), and pathologies (MS, brain tumor). We conduct experiments on two public datasets, BraTS [16], and MSSEG [4], and an in-house clinical MS dataset (cMS). In each dataset, we select 25 patients that fulfill the isotropic acquisition criteria for both ground truth HR scans. Note that we only use the ground truth HR for evaluation, not anywhere in training. We optimize separate INRs for each subject with supervision from only its two LR scans. If required, we employ skull-stripping [12] and rigid registration to the MNI152 (MSSEG, cMS) or SRI24 (BraTS) templates. For details, we refer to Table 2 in the supplementary.Metrics. We evaluate our results by employing common SR [5,14,29] quality metrics, namely PSNR and SSIM. To showcase perceptual image quality, we additionally compute the Learned Perceptual Image Patch Similarity (LPIPS)  [31] and measure the absolute error MI in mutual information of two upsampled images to their ground truth counterparts as follows:Baselines and Ablation. To the best of our knowledge, there are no prior data-driven methods that can perform MCSR on a single-subject basis. Hence, we provide single-subject baselines that operate solely on single contrast and demonstrate the benefit of information transfer from other contrasts with our proposed models.  Quantitative Analysis. Table 1 demonstrates that our proposed framework poses a trustworthy candidate for the task of MCSR. As observed in [32], LRTV struggles for anisotropic up-sampling while SMORE's overall performance is better than cubic-spline, but slightly worse to single-contrast INR. However, the benefit of single-contrast INR may be limited if not complemented by additional views as in [29]. For MCSR from single-subject scans, we achieve encouraging results across all metrics for all datasets, contrasts, and views. Since T1w and T2w both encode anatomical structures, the consistent improvement in BraTS for both sequences serves as a proof-of-concept for our approach. As FLAIR is the go-to-sequence for MS lesions, and T1w does not encode such information, the results are in line with the expectation that there could be a relatively higher transfer of anatomical information to pathologically more relevant FLAIR than vice-versa. Lastly, given their similar physical acquisition and lesion sensitivity, we note that DIR/FLAIR benefit to the same degree in the cMS dataset.Qualitative Analysis. Figure 2 shows the typical behavior of our models on cMS dataset, where one can qualitatively observe that the split-head INR pre-serves the lesions and anatomical structures shown in the yellow boxes, which other models fail to capture. While our reconstruction is not identical to the GT HR, the coronal view confirms anatomically faithful reconstructions despite not receiving any in-plane supervision from any contrast during training. We refer to Fig. 4 in the supplementary for similar observations on BraTS and MSSEG."
Single-subject Multi-contrast MRI Super-resolution via Implicit Neural Representations,4,Discussion and Conclusion,"Given the importance and abundance of large multi-parametric retrospective cohorts [4,16], our proposed approach will allow the upscaling of LR scans with the help of other sequences. Deployment of such a model in clinical routine would likely reduce acquisition time for multi-parametric MRI protocols maintaining an acceptable level of image fidelity. Importantly, our model exhibits trustworthiness in its clinical applicability being 1) subject-specific, and 2) as its gain in information via super-resolution is validated by MI preservation and is not prone to hallucinations that often occur in a typical generative model.In conclusion, we propose the first subject-specific deep learning solution for isotropic 3D super-resolution from anisotropic 2D scans of two different contrasts of complementary views. Our experiments provide evidence of inter-contrast information transfer with the help of INR. Given the supervision of only single subject data and trained within minutes on a single GPU, we believe our framework to be potentially suited for broad clinical applications. Future research will focus on prospectively acquired data, including other anatomies."
Single-subject Multi-contrast MRI Super-resolution via Implicit Neural Representations,,,Our proposed split-head INR: Single INR with two separate heads that jointly predicts the two contrast intensities (cf. Fig.1).
Single-subject Multi-contrast MRI Super-resolution via Implicit Neural Representations,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3_17.
DeepSOZ: A Robust Deep Model for Joint Temporal and Spatial Seizure Onset Localization from Multichannel EEG Data,1,Introduction,"Epilepsy is a debilitating neurological disorder characterized by spontaneous and recurring seizures [17]. Roughly 30% of epilepsy patients are drug resistant, meaning they do not positively respond to anti-seizure medications. In such cases, the best alternative treatment is to identify and surgically resect the brain region responsible for triggering the seizures, i.e., the seizure onset zone (SOZ). Scalp electroencephalography (EEG) is the first and foremost modality used to monitor epileptic activity. However, seizure detection and SOZ localization from scalp EEG are based on expert visual inspection, which is time consuming and heavily prone to the subjective biases of the clinicians [8].Computer-aided tools for scalp EEG almost exclusively focus on the task of (temporal) seizure detection. Early works approached the problem via feature engineering and explored spectral [24,25], entropy-based [9], and graphtheoretic [1] features for the task. In general, these methods extract features from short time windows and use a machine learning classifier to discriminate between window-wise seizure and baseline activity [1,25]. More recently, deep learning models have shown promise in extracting generalizable information from noisy and heterogeneous datasets. Deep learning applications to EEG include convolutional neural networks (CNNs) [4,12,22,23], graph convolutional networks(GCNs) [21], and a combination of attention-based feature extraction [10] and recurrent layers to capture evolving dynamics [4,15,20]. Transformers have also been used for seizure detection, both in combination with CNNs [14] and directly on the EEG signals and their derived features [11,18]. While these methods have greatly advanced the problem of seizure detection, they provide little information about the SOZ, which is ultimately the more important clinical question.A few works have explored the difficult task of localizing the SOZ via post hoc evaluations of deep networks trained for seizure detection. For example, the authors of [7,16] perform a cross-channel connectivity analysis of the learned representations to determine the SOZ. In contrast, the method of [2] identifies the SOZ by dropping out nodes of the trained GCN until the seizure detection performance degrades below a threshold. Finally, the SZTrack model of [6] jointly detects and tracks the spatio-temporal seizure spread by aggregating channelwise detectors; the predictions of this model are seen to correlate with the SOZ. While valuable, the post hoc nature of these unsupervised analyses means that the results may not generalize to unseen patients. The first supervised approach for SOZ localization was proposed by [3] and uses probabilistic graphical models for simultaneous detection and localization. The more recent SZLoc model [5] proposes an end-to-end deep architecture for SOZ localization along with a set of novel loss functions to weakly supervise the localization task from coarse inexact labels. While these two methods represent seminal contributions to the field, they are difficult to train and only report the localization performance on short (i.e., < 2 min) EEG recordings around the time of seizure onset.In this paper, we present DeepSOZ, a robust model for joint seizure detection and SOZ localization from multichannel scalp EEG. Our model consists of a spatial transformer encoder to combine cross-channel information and LSTM layers to capture dynamic activity for window-wise seizure detection. In parallel, we use a novel attention-weighted multi-instance pooling to supervise seizure-level SOZ localization at the single channel resolution. We curate a large evaluation dataset from the publicly available TUH seizure corpus by creating SOZ labels from the clinician notes for each patient. We perform extensive window-level, seizure-level, and patient-level evaluations of our model. Additionally, we analyze the consistency of predictions across seizure occurrences, which has not previously been reported for SOZ localization. Quantifying the error variance is the first step in establishing trust in DeepSOZ for clinical translation."
DeepSOZ: A Robust Deep Model for Joint Temporal and Spatial Seizure Onset Localization from Multichannel EEG Data,2,Methodology,"Figure 1 illustrates our DeepSOZ architecture. The inputs to DeepSOZ are multichannel EEG data for a single seizure recording segmented into one-second windows. The outputs are a temporal sequence of predicted seizure versus baseline activity (detection) and a channel-wise posterior distribution for the SOZ (localization). Formally, let x t i denote the EEG data for channel i and time window t. Clinical EEG is recorded in the 10-20 system, which consists of 19 channels distributed across the scalp. For training, let S t ∈ {0, 1} denote the seizure versus baseline activity label for time window t, and let y ∈ {0, 1} 19×1 be a vector representing the clinician annotated SOZ. Below, we describe each component of DeepSOZ, along with our training and validation strategy. "
DeepSOZ: A Robust Deep Model for Joint Temporal and Spatial Seizure Onset Localization from Multichannel EEG Data,2.1,The DeepSOZ Model,"where LN (•) denotes layer normalization, and F F (•) represents a learned two layer feed-forward network with ReLU activation.The MHA(•) operation uses parallel self attentions to map the input data into a set of projections, as guided by the other channels in the montage. Formally, let n index the attention head. The attention weights A t n ∈ R 20×20 captures global (1) and cross-channel (19) similarities via the key matrixXt as follows:where ξ(•) represents the softmax function, and d is our model dimension. The attention A t n is multiplied by the value matrixXt to generate the output for head n. These outputs are concatenated and fed into a linear layer to produce MHA(•). Finally, these MHA outputs are passed into a two layer feed forward neural network with ReLU activation, post residual connections and layer normalization to generate the hidden encoding H t ∈ R 20×200 .The matrices W Q n , W K n , and W V n are trained parameters of the encoder. For simplicity, we set the model dimension d to be the same as our input x t i (d = 200 in this work), and we specify 8 attention heads in the MHA operation."
DeepSOZ: A Robust Deep Model for Joint Temporal and Spatial Seizure Onset Localization from Multichannel EEG Data,,LSTM for Temporal Seizure Detection:,"We use a bidirectional LSTM to capture evolving patterns in the global encodings of the one-second EEG windows, i.e., {h t 0 } T t=1 . We use a single LSTM layer with 100 hidden units to process the global encodings and capture both long-term and short-term dependencies. The output of the LSTM is passed into a linear layer, followed by a softmax function, to generate window-wise predictions Ŝt ∈ [0, 1]. Here, Ŝt represents the posterior probability of seizure versus baseline activity at window t."
DeepSOZ: A Robust Deep Model for Joint Temporal and Spatial Seizure Onset Localization from Multichannel EEG Data,,Attention-Weighted Multi-Instance Pooling for SOZ Localization:,We treat the localization task as a multi-instance learning problem to predict a channel-wise posterior distribution for the SOZ vector {y i } 19  i=1 by computing a weighted average of the hidden representations from the transformer. We first map the channel-wise encodings {h t i } 19 i=1 ∈ R 200 to scalars ŷt i using the same linear layer across channels. We use the predicted seizure probability Ŝt as our attention to compute the final SOZ prediction as follows:where σ(•) is the sigmoid function. The final patient-level predictions are obtained by averaging ŷi across all seizure recordings for that patient.
DeepSOZ: A Robust Deep Model for Joint Temporal and Spatial Seizure Onset Localization from Multichannel EEG Data,2.2,Loss Function and Model Training,"We train DeepSOZ in two stages. First, the transformer and LSTM layers are trained for window-wise seizure detection using weighted cross entropy loss:where the weight δ = 0.8 is fixed based on the ratio of non-seizure to seizure activity in the dataset. DeepSOZ is then finetuned for SOZ localization. To avoid catastrophic forgetting of the detection task, we freeze the LSTM layers and provide a weak supervision for detection via the loss function:where the ||.|| 1 penalizes the L1 norm to encourage sparsity in predicted ŷ"
DeepSOZ: A Robust Deep Model for Joint Temporal and Spatial Seizure Onset Localization from Multichannel EEG Data,2.3,Model Validation,"We evaluate DeepSOZ using bootstrapped 5-fold nested cross validation. Within each training fold, we select the learning rate and seizure detection threshold through a grid search with a fixed dropout of 0.15. We use PyTorch v1.9.0 with Adam [13] for training with a batch size of one patient; early stopping is implemented using a validation set drawn from the training data. We re-sample the original 5-fold split three times and report the results across all 15 models1 .Seizure Detection: At the window level, we report sensitivity, specificity, and area under the receiver operating characteristic curve (AU-ROC). At the seizure level, we adopt the strategy of [4] and select a detection threshold that ensures no more than 2 min of false positive detections per hour in the validation dataset.To eliminate spikes, we smooth the output predictions using a 30 s window and count only the contiguous intervals beyond the calibrated detection threshold as seizure predictions. Following the standard of [4], we do not penalize post-ictal predictions. We report the false positive rate (FPR) per hour (min/hour), the sensitivity, and the latency (seconds) in seizure detection. "
DeepSOZ: A Robust Deep Model for Joint Temporal and Spatial Seizure Onset Localization from Multichannel EEG Data,,Baseline Comparisons:,"We compare the performance of DeepSOZ with one model ablation and four state-of-the-art methods from the literature. Our ablation replaces the attention-weighted multi-instance pooling in DeepSOZ with a standard maxpool operation within the prediction seizure window (DeepSOZmax). Our baselines consist of the CNN-BLSTM model for seizure detection developed by [4], the SZTrack model proposed by [6] that uses a convolutionalrecurrent architecture for each channel, the SZLoc model by [5] consisting of CNN-transformer-LSTM layers, and the Temporal Graph Convolutional Network (TGCN) developed by [2]. SZTrack and SZLoc are trained and evaluated for localization via the approach published by the authors which uses only 45 s of data around onset time. We modify the TGCN slightly to extract channel-wise prediction for localization task but evaluate it on the full 10-minute recordings like DeepSOZ. Finally, we note that the CNN-BLSTM can only be used for seizure detection, and SZLoc is only trained for SOZ localization."
DeepSOZ: A Robust Deep Model for Joint Temporal and Spatial Seizure Onset Localization from Multichannel EEG Data,,Data and Preprocessing:,"We validate DeepSOZ on 642 EEG recordings from 120 adult epilepsy patients in the publicly available Temple University Hospital (TUH) corpus [19] with a well characterized unifocal seizure onset. We use the clinical notes to localize the SOZ to a subset of the 19 EEG channels. Table 1 describes the seizure characteristics across patients in our curated subset. Following [4], we re-sample the raw EEG to 200 Hz for uniformity, filter the signals between 1.6-30 Hz, and clip them at two standard deviations from mean to remove high intensity artifacts. All signals are normalized to have zero mean and unit variance. We standardize the input lengths by cropping the signals to 10 min around the seizure interval, while ensuring that the onset times are uniformly distributed within this period. We segment the EEG into one second non-overlapping windows to obtain the model inputs x t i .Seizure Detection Performance: Table 2 reports the seizure detection performance averaged over the 15 bootstrapped testing folds. At the window level, both aggregation strategies for DeepSOZ (weighted posterior and max pooling) perform similarly and achieve higher AU-ROC values than the other baselines.The TGCN and CNN-BLSTM baselines achieve notably worse AU-ROC values, establishing the power of a transformer encoder in extracting more meaningful features. SZTrack is trained using the published strategy in [6] and fails to detect seizures effectively. The differences in AU-ROC between DeepSOZ and TGCN, SZTrack, and CNN-BLSTM are statistically significant per a De Long's test at p < 0.05. At the seizure level, DeepSOZ achieves a good balance between sensitivity (0.81) and FPR (0.44 min/h). The negative latency of 18 s contributes towards the slightly elevated FPR. The TGCN and SZTrack have a high sensitivity, which comes at the cost of much higher FPR, while the CNN-BLSTM has a low detection sensitivity but comparable FPR.SOZ Localization Performance: Table 3 summarizes the SOZ localization performance across models. DeepSOZ performs the best at both patient and seizure levels. In contrast, the SZTrack and TGCN baselines are confident in their predictions but more often incorrect, once again highlighting the value of a transformer encoder. While the SZLoc model performs the best of the baselines, we note that both it and SZTrack have an unfair advantage of being trained and evaluated on 45 s EEG recordings around the seizure onset time. In contrast, DeepSOZ processes full 10-minute recordings for both tasks. Figure 2 aggregates the final predictions of DeepSOZ across the 120 patients into quadrants. As seen, DeepSOZ is adept at differentiating right-and lefthemisphere onsets but struggles to differentiate anterior and posterior SOZs. We hypothesize that this trend is due to the skew towards temporal epilepsy patients in the TUH dataset. A similar trend can be observed at the finer lobewise predictions. Figure 3 illustrate sample DeepSOZ outputs for two patients in the testing fold. As seen, DeepSOZ accurately detects the seizure interval in all cases but has two false positive detections for Patient 1. Nonetheless, DeepSOZ correctly localizes the seizure to the left frontal area. The localization for Patient 2 is more varied, which correlates with the patient notes that specify a right-posterior onset but epileptogenic activity quickly spreading to the left hemisphere. Overall, DeepSOZ is more uncertain about this patient. "
DeepSOZ: A Robust Deep Model for Joint Temporal and Spatial Seizure Onset Localization from Multichannel EEG Data,4,Conclusion,"We have introduced DeepSOZ for joint seizure detection and SOZ localization from scalp EEG. DeepSOZ leverages a self-attention mechanism to generate informative global and channel-wise latent representations that strategically fuse multi-channel information. The subsequent recurrent layers and attentionweighted pooling allow DeepSOZ to generalize across a heterogeneous cohort. We validate DeepSOZ on data from 120 epilepsy patients and report improved detection and localization performance over numerous baselines. Finally, we quantify the prediction uncertainty as a first step towards building trust in the model."
DeepSOZ: A Robust Deep Model for Joint Temporal and Spatial Seizure Onset Localization from Multichannel EEG Data,,Architecture Spatial Transformer Encoder:,"i } 19 i=1 is passed to a transformer encoder consisting of multi-head attention (MHA) layers to generate both a global h t 0 and channel-wise {h t i } 19 i=1 encodings. Since the spatial orientation of these channels is crucial for tracking seizure activity, we add a positional embedding generated by a trainable linear layer W p , resulting in the modified input xt i = x t i + W T p 1(i), where 1(i) 1 . . . xt 19 ] as follows:"
DeepSOZ: A Robust Deep Model for Joint Temporal and Spatial Seizure Onset Localization from Multichannel EEG Data,,Table 1 .,"SOZ Localization: By construction, DeepSOZ processes each seizure recording separately to find the SOZ. Patient-level SOZ predictions are obtained by averaging across all seizure recordings for that patient. The SOZ is correctly localized if the maximum channel-wise probability lies in the neighborhood determined by the clinician. We quantify the prediction variance at the seizure level by generating Monte Carlo samples during test via active dropout. At the patient level, we compute the prediction variance across all seizures for that patient."
DeepSOZ: A Robust Deep Model for Joint Temporal and Spatial Seizure Onset Localization from Multichannel EEG Data,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3 18.
Development and Fast Transferring of General Connectivity-Based Diagnosis Model to New Brain Disorders with Adaptive Graph Meta-Learner,1,Introduction,"Brain disorders (BDs) pose severe challenges to public mental health in the global world. To understand the pathology, and for accurate diagnosis as well, functional MRI (fMRI), one of the MRI modalities, is widely studied for BDs. The fMRI provides assessments of the disease-induced changes in the brain functional connectivity networks (FCNs) among different brain regions of interest (ROIs). And a huge body of studies has successfully built effective classifiers for different BDs based on FCN and deep learning methods [6,14,15]. However, when facing new BDs, it is often needed to train a new classification model, which requires collections of large clinical data. During this process, the high costs in time, money, and labor prevent collecting sufficient data and thus the applications of deep learning models to new BDs with a small number of samples, especially for some rare BDs. Recently, there has been study [24] illustrating that BDs share significant commonness under the perspective of FCN alternations. Based on this knowledge, developing and transferring a general model to new BDs could be possible, which is promising to address the issues of building new classifiers for new BDs under data limitation.Meta-learning based algorithm is one of the advanced methods to develop a general model based on heterogeneous information from data in different domains or for different tasks. It aims to learn optimal initial parameters for the model (meta-learner) which can be quickly generalized to new tasks, directly or with a few new training data for fine-tuning. There have been extensive discussions in the literature on developing more general models utilizing meta-learning [12,18,22,23] in medical fields.Upon this evidence, it would be promising to develop a general BD diagnosis model based on FCN and further use meta-learning to solve the above-mentioned issues. However, there are still at least two challenges to achieve this goal. First, how to optimally extract the generalizable common knowledge (features) from the procedure of diagnosing various BDs? Previous methods focused on conventional FCNs (computed using simple linear correlations) and only treated FCN as a vector [12]. This manner of analysis neither properly explores topological features in the FCN, nor fully characterizes the complex, high-order functional interactions among brain regions, which are demonstrated to be associated with BDs [2]. Second, after developing a general model, how to optimally adapt the model to new datasets with different conditions? The best tuning of the model may exist only within a critical range of parameters, but previous studies blindly search for optimal parameters using ad hoc manual configurations, such as the adaption step size, which is easy to cause over-fitting and degrading the performance on small datasets. Theoretically, it would be beneficial to let the model adaptively configure the adaption step size and other parameters, according to the given dataset.In this paper, we develop a novel framework (illustrated in Fig. 1) to explore the aforementioned issues. First, we assemble a large amount of data from both public datasets and in-house datasets (i.e., a total of 6 datasets, 4,114 subjects) to develop a general BD diagnosis model with meta-learning. Second, during the meta-learning procedure, we propose an adaptive multi-view graph classifier to mine topological information of low-and high-order FCNs, as different views of brain dynamics for classification. The attention mechanism is implemented to dynamically weigh and fuse different views of the information under given tasks, which collaborate with meta-learning and helps the model to learn to adapt to diagnoses for different BDs. Third, we apply a meta-controller driven by reinforcement learning [25] to choose the optimal adaption step size for the general model, for properly adapting to new BDs with small datasets and alleviating the over-fitting issue."
Development and Fast Transferring of General Connectivity-Based Diagnosis Model to New Brain Disorders with Adaptive Graph Meta-Learner,2.1,Notation and Problem Formulation,"We define the entire set of BD diagnosis tasks using different datasets with T . For a specific task T j ∈ T (j = 1, 2, ..., 6), we have n pairs of FCN and labeled data, where F is the fMRI data and Y is the label set of all subjects. ROIs are defined in individual fMRI spaces based on the brain atlas [20] to extract ROI-based fMRI signals.We extract three features from individual fMRI data for the graph deep learning analysis. First, we compute the low-order FCN A low , where A low ∈ R NROI×NROI is a graph adjacency matrix containing the pair-wise correlations among different ROI signals. Second, we compute the high-order FCN A high based on the topological information in A low (described in detail in Sect. 2.3). These two features will be used as edge features. Third, we use the corresponding order adjacency matrix, together with the mean and standard deviation value of ROI signals, as the node features for both lowand high-order graphs [14].We have meta-training and meta-testing stages for our meta-learner. The corresponding tasks are named as meta-training task T train and meta-testing task T test . The meta-training stage mimics cross-task adaptations, aiming to make the model learning to adapt to new task T test with a small number of samples under initial parameters. To simulate the cross-task scenario, we utilized the episodic training mechanism [13], which samples small sets across all meta-training datasets. In detail, for each round the in meta-training stage, we randomly sample the non-overlapping support datai=1 across all meta-training datasets, based on which, the T train is constructed. With constructed T train , we have two loops to update the initial parameters of the meta-learner, which are inner and outer loops. During inner loops, initial parameters update gradients are first estimated using the back-propagation learned from the D train sup . Then, if initial parameters are updated by the first gradient, we further estimate the gradients based on D train que as the outer loop to finally update them. The two loops will be combined to update the initial meta-learner parameters for better fine-tuning on new tasks as detailed in Sect. 2.2. At the meta-testing stage, we adapt the initialized parameters from the meta-training stage to the new T test with a few data and test its performance. We first fine-tune the meta-learner on, and then report classification performance on. Our target is to make the accuracy on D test que as high as possible when the size of the support data s is only a small portion of T test . If s = K, we denote the setting as K-shot classification."
Development and Fast Transferring of General Connectivity-Based Diagnosis Model to New Brain Disorders with Adaptive Graph Meta-Learner,2.2,Meta-Learner Training Algorithm,"Our meta-learner consists of two modules, which are 1) multi-view classifier as detailed in Sect. 2.3 and 2) meta-controller as detailed in Sect. 2.4. The pseudo-codes for the meta-learner training algorithm are given in Algorithm 1. During the meta-training stage, our meta-learner algorithm has two iterative parameter update loops, which are inner (lines 3-9) and outer loops (lines 10-14) [5]. The inner loop aims to fast adapt the multi-view graph classifier parameterized with θ c to the new sampled T train under given initial parameters (θ 0 c , θ m ). During the inner loop, the meta-controller parameterized with θ m decides whether to stop at the adaption step t to avoid over-fitting. While the outer loop aims to improve the generality of the meta-learner by exploring the optimal initial parameters (θ 0 c , θ m ) according to the loss on D train que which can easily adapt to other tasks. The initial parameter updated in the outer loop will be set for the next sampled T train . At the meta-testing stage, the meta-learner will utilize the meta-training stage initialized (θ 0 c , θ m ), fixing θ m and fine-tuning θ 0 c on D test sup , finally predicting the label D test que ."
Development and Fast Transferring of General Connectivity-Based Diagnosis Model to New Brain Disorders with Adaptive Graph Meta-Learner,2.3,Multi-view Graph Classifier θ c,"As mentioned above, exploring the topological information of the FCN is necessary for BD diagnosis. However, a single low-order FCN view can only illustrate the simple pair-wise relation. So, we additionally construct the high-order FCN, which reflects the correlation among ROIs in terms of their own FC patterns, as calculated below:After constructing high-order adjacency matrix A high as a complementary view, we input them into the multi-view graph classifier parameterized with θ t c at the adaption step t. To extract the disease-related features of different-view graphs, we use convolution in GCN [11] to aggregate the neighboring node features.Then, we use the pooling operation to further extract the global topological features of different graph views. Here, we opt for gPOOL operation [7] for its parameter-saving and ability to extract global topological features. For each pooling stage, it acquires the importance of each node by calculating the inner product between the node feature vectors and a learnable vector. The top-k important nodes and their corresponding subgraph will be sampled for the following graph convolutions. By iteratively repeating the node feature aggregation and pooling illustrated in Fig. 1 (3), we can finally acquire the representation of the graph, denoted as H. Furthermore, to let the model adaptively choose the proper view for classification, we apply an attention-based mechanism to aggregate the learned embeddings by feeding the concatenation of the learned representations into an attention module as below:The attention mechanism allows the model to decide which view should rely on for specific tasks more adaptively. Then, we forward attention-weighted features into an MLP and acquire the final prediction label. Here, we use the cross-entropy loss as a constraint to the network."
Development and Fast Transferring of General Connectivity-Based Diagnosis Model to New Brain Disorders with Adaptive Graph Meta-Learner,2.4,Meta-Controller θ m,"In machine learning, over-fitting is one of the critical problems that restrict the performance of models, especially in small datasets. Previous works utilize the early stopping to alleviate this problem, but the hand-crafted early stopping parameter is hard to choose. Here, we utilize a neural network to learn the stop policy adaptively, which we call meta-controller parameterized with θ m depicted in Fig. 1 (4). The meta-controller uses a reinforcement learning based algorithm [17] to decide optimal adaption step sizes for cross-task adaptions. Considering the characteristics of the graph data, we let the model determine when to stop not only according to the classification loss, but also the graph embedding quality on the support data. For the embedding quality, we use the Average Node Information (ANI), denoted as Q, to measure it. It represents how a node can be measured by the neighboring nodes. A high ANI value indicates that the embedding module has learned the most information about the graph. If we keep aggregating the nodal features by graph convolution when the ANI is high, the over-smoothing will happen, making all nodes have similar features and thus degrading the performance. We define the ANI Q sup of the support data within T train with the L1 norm [9] as follows:where D i X L i represent the degree matrix of A i and the feature matrix in the last layer L, respectively; j denotes j-th node which is also the j-th row in those matrices, and• 1 denotes the L1-norm of row vector.For classification losses L sup and ANI values Q sup on D train sup across t adaption steps, we use them to compute the stop probability p t at step t with an LSTM [8] model by considering the temporal information as follows:where o t is the output of the LSTM model at step t and σ is the SoftMax function.Finally, we sample the choices c t by Bernoulli distribution to decide whether we should stop at step t.Since the relation between θ m and θ c is undifferentiable, it is impossible to take direct gradient descent on θ m . We use stochastic policy gradient to optimize θ m . Once we sample the stop choice at step T , we train the meta-controller according to loss changes on D train que across T steps. Given the parameter update trajectory of classifier {θ 0 c , θ 1 c , ...θ T c } during T steps, we calculate the corresponding loss change trajectory of D train que . Based on that, we further define the controller immediate rewards r at step t as the loss change on D train que (caused by parameter update of step t):Then, the accumulative reward R at step t iswhere T is the total number of steps and R t is the change of classification loss on D train que from step t to the end of adaption. Then we update our meta-controller by policy gradients, which is a typical method in Reinforcement learning [25]:where∇ θm is the gradients over θ m and β 2 is the learning rate."
Development and Fast Transferring of General Connectivity-Based Diagnosis Model to New Brain Disorders with Adaptive Graph Meta-Learner,3.1,Dataset,"We use fMRI meta-training data from five datasets including Alzheimer's Disease Neuroimaging Initiative (ADNI) [1,10], Open Access Series of Imaging Studies (OASIS) [19], and in-house dataset from Huashan Hospital (elder BDs datasets); ADHD-200 [3] and Autism Brain Imaging Data Exchange (ABIDE) [4] (youth BD datasets). For the meta-testing dataset, we use the in-house dataset from Zhongshan Hospital which is about vascular cognitive impairment (VCI). All datasets are shown in Table 1 The details of image acquisition parameters and processing procedures can be found in [16]. "
Development and Fast Transferring of General Connectivity-Based Diagnosis Model to New Brain Disorders with Adaptive Graph Meta-Learner,3.2,Settings,"To ensure a fair comparison, we use three graph convolutional layers, followed by corresponding pooling layers, for all GNN based methods. We use the ADAM optimizer with 1e-4 for learning rate and 5e-4 for weight decay, 100 for epochs, 0.001 for both β 1 and β 2 , respectively. For the inner loop fast adaption, we set the minimum and maximum steps by 4 and 16. In the meta-training stage, we sample D train sup and D train que from all training datasets; and, in the meta-testing stage, we only randomly sample D test sup and D test que from the meta-testing dataset. The size of support data for both meta-training and meta-testing stages is depicted in the first line of Table 2, and we set the size of the query data as 256 for two stages. For different datasets, we only diagnose whether they are BD or not. We randomly select the support and the query data five times to report the mean and standard deviation value of the performance."
Development and Fast Transferring of General Connectivity-Based Diagnosis Model to New Brain Disorders with Adaptive Graph Meta-Learner,3.3,Results and Discussions,"In Table 2, we compare our method with two SOTA meta-learning based methods [12,17] (in lines 3 and 4) under different K-shot configurations. It can be observed that, when the size of the support data increases, the performances of all methods increase and our proposed method outperforms the SOTA methods in terms of all performance metrics. We also validate the effectiveness of the use of multiple-BD datasets for training our model. When compared with the model without the meta-training stage (line 5) or reducing either elder or youth BD datasets (lines 8 and 9), we find that the performances all drop significantly. This can validate that not only the information from MCI/AD, which is more similar to VCI, is useful, but also the general BD commonness suggested by data of youth BDs is beneficial. For the ablation study, we test the performance without a multi-view graph classifier or meta-controller as shown in Table 2 (lines 6 and 7) and Fig. 2 (a).  Finally, we visualize the predictive importance of different resting-state networks, including visual network (VIS), somatomotor network (SM), dorsal attention network (DAN), salience network (SAL), limbic network (LIM), executive control network (ECN) and default mode network (DMN) when adapting to elder BD and younger BD as shown in Fig. 2 (d) by GradCAM [21]. In the results, the LIM consistently shows importance when diagnosing elder and younger BDs, which is in line with previous neuroscience studies [24]. This validates that our proposed method can properly detect meaningful common features among different BDs."
Development and Fast Transferring of General Connectivity-Based Diagnosis Model to New Brain Disorders with Adaptive Graph Meta-Learner,4,Conclusion,"In this work, we focus on the issues of developing a classifier on new BD datasets with small samples and propose a novel framework. A broad of datasets covering elder and youth BDs are used to train the model to estimate the common features among BDs. An adaptive multi-view graph classifier is proposed to enable the model efficiently extract features for different BD diagnosis tasks. In addition, to avoid over-fitting during the adaptions to new data, we utilize a novel meta-controller driven by RL. Extensive experiments demonstrate the effectiveness and generalization of our proposed method. It is expected that advanced graph embedding methods can be integrated into our framework to improve performance. Our work is also promising to be extended to neuroscience studies to reveal both common and unique characteristics of different BDs."
Development and Fast Transferring of General Connectivity-Based Diagnosis Model to New Brain Disorders with Adaptive Graph Meta-Learner,,Algorithm 1 :,13 θm ← θm + β2R t ∇ θm ln p(t) 14 end 15 end
Pick and Trace: Instance Segmentation for Filamentous Objects with a Recurrent Neural Network,1,Introduction,"Filamentous objects, such as microtubules, actin filaments, and blood vessels, play a fundamental role in biological systems. For example, microtubules (See Fig. 1) form part of cell cytoskeleton structures and are involved in various cellular activities such as movement, transportation, and key signaling events. To understand the mechanism of filamentous objects, quantifying their properties, including quantity, length, curvature, and distribution, is fundamental for biological research. Instance segmentation is often the first step for quantitative analysis. However, as filaments are very thin, non-rigid, and usually span over the image intersecting each other, extracting individual filaments is very challenging.Since the advent of deep learning, region-based instance segmentation methods [4,7,8,13,27], which segment instances within the detected bounding boxes, have shown remarkable performance on objects with well-defined centers and boundaries. However, filaments are non-rigid objects and span widely across the image. Each filament has a distinct shape varying in length and deformation, while segments of different filaments share a similar appearance. These properties make it extremely hard for region-based methods to detect the centers and bounding boxes for filaments and segment the target within the detected region.Region-free methods utilize learned embedding [1,2,5,14] or affinity graph [6,19] to separate instances. These methods rely on pixel-level prediction to group instances and do not directly extract the complete shape of instances. Since filaments may be densely clustered and overlapping, these methods have difficulties in disentangling filaments in complicated scenes. Liu et al. [17] address the overlapping challenge by proposing an orientation-aware network to disentangle filaments at intersections, but it requires a heuristic post-processing to form instances. Hirsch et al. [9] predict dense patches representing instances' shape for each pixel and assemble them to form instances with affinity graph. Effectively extracting longer filaments requires predicting a larger shape patch for each pixel, leading to a longer computational time.Lacking instance-level labels of filaments is another challenge. Most existing approaches for filaments extraction adopt traditional computer vision techniques such as morphological operations [29], template matching [28,29] and active contour [26]. These methods follow the segment-break-regroup strategy. They first obtain the binary segmentation, then break it at intersections and regroup the segments into filaments by geometric properties. The performance heavily relies on manual parameter tuning.When a human tries to manually extract filaments in Fig. 1, directly pointing out each filament can be challenging. Instead, a human would first identify each filament's tip and then trace each filament. Inspired by this human behavior, we introduce Deep Recurrent Instance Filament Tracer (DRIFT) for instance segmentation on filamentous objects. Figure 2 shows an example of how DRIFT mimics a human and sequentially extracts filaments. As shown in Fig. 3, the pick module in DRIFT first detects all tip points as candidates. The recurrent neural network (RNN) based tracing module will 'look' at the patch around tip points, segment the object within the patch, and predict the next location. The trace module sequentially segments the object until a stop flag is predicted. The RNN learns where 'it' comes from, where 'it' is, and where 'it' goes next.Our method is fundamentally different from [11,12,23,25]. Ren et al. [23] and Amaia [25] et al. use the RNN model to sequentially predict objects' bounding boxes, which are essentially region-based segmentation methods. Januszewski et al. [11,12] propose a flood-filling network to repeatedly perform segmentation within a set of manually defined patches to grow object masks. While the iterations in [11,12] are heuristic, our method learns to trace the filaments and sequentially segments the targets.The major contributions of this work are as follows: (1). To our knowledge, our method is the first method that converts the instance segmentation into a sequence modeling problem. Our proposed method mimics human tracing and extracting individual filament, tackling the challenges of extracting filamentous objects. (2). We propose a synthetic filament dataset for training and evaluation. Our model trained on synthetic datasets can be applied to various real filament datasets and thus alleviate the data shortage problem. (3). We collected a dataset of 15 microscopic images of microtubules with instance labels for evaluation. (4). Our method is evaluated on four different datasets and compared with several competing approaches. Our method shows better or comparable results regarding accuracy and computational time. "
Pick and Trace: Instance Segmentation for Filamentous Objects with a Recurrent Neural Network,2,Method,"Figure 3 shows the framework of our proposed method. The pick module detects tip points for all filaments. Then we crop the patches around tip points, and the tracing module will encode these patches into patch embeddings with a convolutional block. The tip point's patch embedding is used to initialize the hidden state of RNN. Then a decoder will decode the hidden state output and predict a stop flag and the object's mask within the current patch. The decoder also outputs an offset map, where each pixel predicts a vector pointing to the next center. We use the offset map to locate the next center via Hough voting. The model sequentially segment instances until the stop flag turns on."
Pick and Trace: Instance Segmentation for Filamentous Objects with a Recurrent Neural Network,2.1,Pick: Tip Points Detection Module,"We adapt the U-shaped structure from [3,15] to regress the tip points' heatmap and use a maximum filter to acquire coordinates of tip points. Network details are included in supplementary materials."
Pick and Trace: Instance Segmentation for Filamentous Objects with a Recurrent Neural Network,2.2,Trace: A Recurrent Network for Filament Tracing,"Network Description. After tip points are detected, the tracing module will trace and extract each instance. As shown in Fig. 4, we use patch size 64 as an example to describe our network design. Since we have converted the instance segmentation problem into a sequence learning problem, we use Long Short Term Memory (LSTM) [10] to encode the sequence of patches. We use one LSTM layer with a hidden size of 512 and an input size of 256. The tracing module takes patches as input, and encode each patch into a 256 embedding vector by 3 downsampling blocks followed by a dense layer. The input of LSTM layer is the encoded embedding vector. At each step, the decoder outputs a stop flag, offset maps, and mask. We use a dense layer with a sigmoid activation function to predict the stop flag, which takes the hidden unit as input. As stop flag prediction is a classification problem, it takes an independent branch. The other branch uses a dense layer and decodes the hidden unit to a vector size of 4096, which is reshaped to 256 × 4 × 4. The following layers include 3 conv3x3-bn-relu-upsampling blocks. The offset map prediction is a regression problem, and mask prediction is a binary classification problem. We split the current branch into two branches. The offset map prediction includes a conv3x3bn-relu-upsampling block and outputs the offset map with a size of 2 × 64 × 64. The offset map includes a horizontal offset channel and a vertical offset channel. The mask branch includes a conv3x3-bn-relu-upsampling-sigmoid block.Predicting the Next Points. At each step, the decoder regresses offset maps where each pixel predicts a vector pointing to the center of the next patch. We use Hough Voting to decide the exact coordinates of the next center. Each pixel casts a vote to the next point, generating a heatmap of the number of votes for each pixel. The highest response point will be selected as the next center.Loss Function. We use binary cross entropy (BCE) for the tip prediction. For the tracing module, we use BCE for stop flag and mask prediction and L 1 loss for offset prediction. The final loss for tracing module isT is the number of steps for tracing, and s, A, M stand for stop flag, binary mask, and offset maps. λ 1 , λ 2 , λ 3 are the balance parameters and set as one.Training and Inference. We use patches as input for training, and the labels are the corresponding offset map, binary mask, and stop flag. The offset map is generated by computing the distance vector between pixels in the current patch "
Pick and Trace: Instance Segmentation for Filamentous Objects with a Recurrent Neural Network,3,Experiments,"Our model is implemented with Pytorch [22] and trained on one RTX 2080 Ti GPU. We convert each instance into patch sequences with a step size of 30 pixels, and patch size of 64 × 64. We evaluate our approach on four datasets.  [8] 0.559 0.865 0.641 -Harmonic Emb. [14] 0.724 0.900 0.723 -PatchPerPix [9] 0.775 0.939 0.891 13 Ours 0.745 0.935 0.828 4"
Pick and Trace: Instance Segmentation for Filamentous Objects with a Recurrent Neural Network,3.1,Synthetic Dataset,"We first create eight synthetic filament datasets. The statistics of generated datasets are shown in Table 1, and samples are shown in Fig. 5. Each dataset contains 1000 images with random filaments varying in widths. We split each dataset into 700, 200, 100 images for training, validating, and testing. We train our network on A-D (image size 256 × 256) for 20 epochs and evaluate our model on the their test set. We also directly evaluate the models trained with dataset A-D on dataset E-H (image size 512 × 512) respectively. We report the average precision (AP) in COCO evaluation criteria [16]. As shown in Table 1, our model trained with smaller size images achieves better results on the unseen datasets (E, F, G, H) with larger images and longer filaments. This is because our model learns how to trace filaments, and longer filaments do not affect the performance of our model. Also, the lower density of filament in E-H making it easier for our model to trace filaments. In addition, thicker filaments create larger overlapping areas and make it harder to separate the filaments. Therefore, the performance decreases from dataset A to D and E to H. As shown in Table 1, MRCNN [8] achieves zero AP for filaments with a width below five, as segmentation is performed at strides of eight. Our approach outperforms Liu et al. [17] in all metrics except AP 0.75 of dataset C."
Pick and Trace: Instance Segmentation for Filamentous Objects with a Recurrent Neural Network,3.2,Microtubule Dataset,"We annotated 15 microscopic images of microtubules with a size of 1376 × 1504. The number of filaments per image is 631 ± 167, and the length of filaments is 104 ± 96. We use a modified U-net [18,24] to obtain the binary segmentation. As the average width of microtubules is five, we directly evaluate the microtubule dataset with model trained on synthetic dataset B (see table 1). We compare our approach against SOAX [26], SFINE [29], and deep learning method in [17].Figure 1 shows qualitative results in full size, and Fig. 6 presents qualitative comparison with detailed area. Our method and Liu et al. [17]'s approach can better extract long and crossing filaments. SOAX [26] and SFINE [29]'s breakregroup strategies struggle to regroup segments at intersections and create fragments. Table 2a presents the quantitative comparisons with AP. Our approach has shown a better performance than [17] regarding process time and accuracy.  "
Pick and Trace: Instance Segmentation for Filamentous Objects with a Recurrent Neural Network,3.3,P. Rubescens Dataset,"P. rubescens is a type of filamentous cyanobacteria, and Zeder et al. [28] provide a dataset (Fig. 7) of seven 5000 × 5000 microscopic images of P. rubescens. We apply our model trained with synthetic dataset A (see Table 1) to the binary predictions and follow the evaluation scheme in [28] by comparing the quantity per image. Table 2b shows our result is close to the manual count."
Pick and Trace: Instance Segmentation for Filamentous Objects with a Recurrent Neural Network,3.4,C. Elegans Dataset,"We further investigate our model's performance on the C. elegans roundworm dataset (Fig. 8) from the Broad Bioimage Benchmark Collection [20]. The dataset contains 100 696 × 520 images with an average of 30 roundworms per image. Different from P. rubescens and microtubules, roundworms are much thicker and shorter. We convert each instance in the training set into a sequence of points with a step size of 30 and generate the corresponding 64 × 64 patches for training. The network is trained and evaluated following the set up in [14,21].Table 2c shows the quantitative comparison between our approach and previous methods [8,9,14,21]. Our method achieves comparable AP 0.5 to SOTA, and our method runs 9 s faster than SOTA. Red circles in Fig. 8 show our approach handles complex crossover areas."
Pick and Trace: Instance Segmentation for Filamentous Objects with a Recurrent Neural Network,4,Conclusion,"We present a novel method for filament extraction by transforming the instance segmentation problem to a sequence modeling problem. Our method comprises of a sequential encoder-decoder framework to imitate humans extracting filaments and address the challenges brought by filaments' properties, including crossover, spanning and non-rigidity. The experiments show that our method can achieve better or comparable results on filament datasets from different domains. Our method can alleviate the data shortage problem as our models trained on synthetic dataset achieve a better performance on microtubules and P. rubescens dataset. We also train and evaluate our model on C. elegans dataset, achieving comparable results with thicker and shorter filaments. Our method exhibits limitations in tracing ""Y""-shaped junctions due to the limited directional information in 2D images. Future work will focus on extending the current method to 3D data."
Pick and Trace: Instance Segmentation for Filamentous Objects with a Recurrent Neural Network,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3 61.
CircleFormer: Circular Nuclei Detection in Whole Slide Images with Circle Queries and Attention,1,Introduction,"Nuclei detection is a highly challenging task and plays an important role in many biological applications such as cancer diagnosis and drug discovery. Rectangle object detection approaches that use CNN have made great progress in the    last decade [4,7,12,14,18]. These popular CNN models use boxes to represent objects that are not optimized for circular medical objects, such as detection of glomeruli in renal pathology. To address the problem, an anchor-free CNNbased circular object detection method CircleNet [16] is proposed for glomeruli detection. Different from CenterNet [18], CircleNet estimates the radius rather than the box size for circular objects. But it also suffers poor detection accuracy for overlapping objects and requires additional post-processing steps to obtain the final detection results.Recently, DETR [1], a Transformer-based object detection method reformulates object detection as a set-to-set prediction problem, and it removes both the hand-crafted anchors and the non-maximum suppression (NMS) postprocessing. Its variants ( [3,10,11,15,19]) demonstrate promising results compared with CNN-based methods and DETR by improving the design of queries for faster training convergence. Built upon Conditional-DETR, DAB-DETR [10] introduces an analytic study of how query design affects rectangle object detection. Specifically, it models object query as 4D dynamic anchor boxes (x, y, w, h) and iteratively refine them by a sequence of Transformer decoders. However, recent studies on Transformer-based detection methods are designed for rectangle object detection in computer vision, which are not specifically designed for circular objects in medical images.In this paper, we introduce CircleFormer, a Transformer-based circular object detection for medical image analysis. Inspired by DAB-DETR, we propose to use an anchor circle (x, y, r) as the query for circular object detection, where (x, y) is the center of the circle and r is the radius. We propose a novel circle cross attention module which enables us to apply circle center (x, y) to extract image features around a circle and make use of circle radius to modulate the cross attention map. In addition, a circle matching loss is adopted in the set-to-set prediction part to process circular predictions. In this way, our design of Circle-Former lends itself to circular object detection. We evaluate our CircleFormer on the public MoNuSeg dataset for nuclei detection in whole slide images. Experimental results show that our method outperforms both CNN-based methods for box detection and circular object detection. It also achieves superior results compared with recently Transformer-based box detection approaches. Meanwhile, we carry out ablation studies to demonstrate the effectiveness of each proposed component. To further study the generalization ability of our approach, we add a simple segmentation branch to CircleFormer following the recent query based instance segmentation models [2,17] and verify its performance on MoNuSeg as well."
CircleFormer: Circular Nuclei Detection in Whole Slide Images with Circle Queries and Attention,2.1,Overview,"Our CircleFormer (Fig. 1) consists of a CNN backbone, a Transformer encoder module, a Transformer decoder and a prediction head to generate circular object results. The detail of the Transformer decoder is illustrated in Fig. 2."
CircleFormer: Circular Nuclei Detection in Whole Slide Images with Circle Queries and Attention,2.2,Representing Query with Anchor Circle,"Inspired by DAB-DETR, we represent queries in Transformer-based circular object detection with anchor circles. We denote C i = (x i , y i , r i ) as the i-th anchor, x i , y i , r i ∈ R. Its corresponding content part and positional part are Z i ∈ R D and P i ∈ R D , respectively. The positional query P i is calculated by:where positional encoding (PE) generates embeddings from floating point numbers, and the parameters of the MLP are shared among all layers.In Transformer decoder, the self-attention and cross-attention are written as:Self-Attn :Cross-attn :where F x,y ∈ R D denote the image feature at position (x, y) and an MLP (csq) : R D → R D is used to obtain a scaled vector conditioned on content information for a query. By representing a circle query as (x, y, r), we can refine the circle query layer-by-layer in the Transformer decoder. Specifically, each Transformer decoder estimates relative circle information (Δx, Δy, Δr). In this way, the circle query representation is suitable for circular object detection and is able to accelerate the learning convergence via layer-by-layer refinement scheme."
CircleFormer: Circular Nuclei Detection in Whole Slide Images with Circle Queries and Attention,2.3,Circle Cross Attention,We propose circle-modulated attention and deformable circle cross attention to consider size information of circular object detection in cross attention module.Circle-modulated Attention. The circle radius modulated positional attention map provides benefits to extract image features of objects with different scales.
CircleFormer: Circular Nuclei Detection in Whole Slide Images with Circle Queries and Attention,,"MA((x, y), (x ref , y","where r i is the radius of the circle anchor A i , and r i,ref is the reference radius calculated byDeformable Circle Cross Attention. We modify standard deformable attention to deformable circle cross attention by applying radius information as constraint. Given an input feature map F ∈ R C×H×W , let i index a query element with content feature Z i and a reference point P i , the deformable circle cross attention feature is calculated by:where m indexes the attention head, k indexes the sampled keys. M and K are the number of multi-heads and the total sampled key number. W m ∈ R D×d , W m ∈ R d×D are the learnable weights and d = D/M . Attn mik denotes attention weight of the k th sampling point in the m th attention head. Δr mik and Δθ mik are radius offset and angle offset, r i,ref is the reference radius. In circle deformable attention, we transform the offset in polar coordinates to Cartesian coordinates so that the reference point ends up in the circle anchor. Rather than initialize the reference points by uniformly sampling within the rectangle as does Deformable DETR, we explore two ways to initialize the reference points within a circle, random sampling (CDA-r) and uniform sampling (CDA-c) (As in Fig. 3). Experiments show that CDA-c initialization of reference points outperforms others."
CircleFormer: Circular Nuclei Detection in Whole Slide Images with Circle Queries and Attention,2.4,Circle Regression,"A circle is predicted from a decoder embedding as ĉi = sigmoid(FFN(f i ) + [A i ]), where f is the decoder embedding. ĉi = (x, ŷ, r) consists of the circle center and circle radius. sigmoid is used to normalize the prediction ĉ to the range [0, 1]. FFN aims to predict the unnormalized box, A i is a circle anchor."
CircleFormer: Circular Nuclei Detection in Whole Slide Images with Circle Queries and Attention,2.5,Circle Instance Segmentation,"A mask is predicted from a decoder embedding by mi = FFN(FFN(f i ) + f i ), where f is the decoder embedding. mi ∈ R 28×28 is the predicted mask. We use dice and BCE as the segmentation loss:) between prediction mi and the groundtruth m i ."
CircleFormer: Circular Nuclei Detection in Whole Slide Images with Circle Queries and Attention,2.6,Generalized Circle IoU,"CircleNet extends intersection over union (IoU) of bounding boxes to circle IoU (cIoU) and shows that the cIOU is a valid overlap metric for detection of circular objects in medical images. To address the difficulty optimizing non-overlapping bounding boxes, generalized IoU (GIoU) [13] is introduced as a loss for rectangle object detection tasks. We propose a generalized circle IoU (gCIoU) to compute the similarity between two circles: gCIoU"
CircleFormer: Circular Nuclei Detection in Whole Slide Images with Circle Queries and Attention,,CC,", where C A and C B denotes two circles, and C C is the smallest circle containing these two circles. We show that gCIoU can bring consistent improvement on circular object detection. Figure 4 shows the different measurements between two rectangles and circles. Different from CircleNet that only uses cIoU in the evaluation, we incorporate gCIoU in the training step. Then, we define the circle loss as: L circle (c, ĉ) = λ gciou L gciou (c, ĉ) + λ c cĉ 1 , while L gciou is generalized circle IoU loss, • 1 is 1 loss, and λ gciou , λ c ∈ R are hyperparameters.Circle Training Loss. Following DETR, i-th each element of the groundtruth set is y i = (l i , c i ), where l i is the target class label (which may be ∅) and c i = (x, y, r). We define the matching cost between the predictions and the groundtruth set as: (6) where σ ∈ S N is a permutation of all prediction elements, ŷσ(i) = ( lσ(i) , ĉσ(i) ) is the prediction, λ focal ∈ R are hyperparameters, and L focal is focal loss [8].Finally, the overall loss is:where σ(i) is the index of prediction ŷ corresponding to the i-th ground truth y after completing the match. m i is the ground truth obtained by RoI Align [5] corresponding to mi ."
CircleFormer: Circular Nuclei Detection in Whole Slide Images with Circle Queries and Attention,3.1,Dataset and Evaluation,"MoNuSeg Dataset. MoNuSeg dataset is a public dataset from the 2018 Multi-Organ Nuclei Segmentation Challenge [6]. It contains 30 training/validataion tissue images sampled from a separate whole slide image of H&E stained tissue and 14 testing images of lung and brain tissue images. Following [16], we randomly sample 10 patches with size 512 ×512 from each image and create 200 training images, 100 validation images and 140 testing images.Evaluation Metrics. We use AP for nuclei detection evaluation metrics as in CircleNet [16], and AP m for the instance segmentation evaluation metrics. S and M are used to measure the performance of small scale with area less than 32 2 and median scale with area between 32 2 and 96 2 ."
CircleFormer: Circular Nuclei Detection in Whole Slide Images with Circle Queries and Attention,3.2,Implementation Details,"Two variants of our proposed method for nuclei detection, CircleFormer and CircleFormer-D are built with a circle cross attention module and a deformable circle cross attention module, respectively. CircleFormer-D-Joint (Ours) extends CircleFormer-D to include instance segmentation as additional output. All the models are with ResNet50 as backbone and the number of Transformer encoders and decoders is set to 6. The MLPs of the prediction heads share the same parameters. Since the maximum number of objects per image in the dataset is close to 1000, we set the number of queries to 1000. The parameter of focal loss for classification is set to α = 0.25, γ = 0.1. λ focal is set to 2.0 in the matching step and λ focal = 1.0 in the final circle loss. We use λ iou = 2.0, λ c = 5.0, λ dice = 8.0 and λ bce = 2.0 in the experiments. All the models are initialized with the COCO pre-trained model [9]."
CircleFormer: Circular Nuclei Detection in Whole Slide Images with Circle Queries and Attention,3.3,Main Results,"In Table jointly outputs detection and segmentation results additionally boosts the detection results of CircleFormer-D. Experiments of joint nuclei detection and segmentation are listed in Table 2. Our method outperforms QueryInst [2], a CNN-based instance segmentation method and SOIT [17], an Transformer-based instance segmentation approach. We extend Transformer-based box detection method to provide additional segmentation output inside the detection region, denoted as Deformable-DETR-Joint. Our method with circular query representation largely improves both detection and segmentation results.To summarize, our method with only detection head outperforms both CNNbased methods and Transformer based approaches in most evaluation metrics for circular nuclei detection task. Our CircleFormer-D-Joint provides superior results compared to CNN-based and Transformer-based instance segmentation methods. Also, our method with joint detection and segmentation outputs also improves the detection-only setting. We have provided additional visual analysis in the open source code repository."
CircleFormer: Circular Nuclei Detection in Whole Slide Images with Circle Queries and Attention,3.4,Ablation Studies,"We conduct ablation studies with CircleFormer on the nuclei detection task (Table 5).  Effects of the Proposed Components. For simplicity, we denote the two parts of Table 3 as P1 and P2.In CircleFormer, the proposed circle-Modulated attention (c-MA) improves the performance of box AP from 45.7% to 48.6% box AP (Row 1 and Row 2 in P1). We replaced circle IoU (CIoU) loss with generalized circle IoU (gCIoU) loss, the performance is further boosted by 2.2% (Row 2 and Row 3 in P1).We obtain similar observations of CircleFormer-D. When using standard deformable attention (SDA), learning cIoU loss gives a 1.2% improvement on box AP compared to using box IoU (Row 1 and Row 2 in P2). Replacing CIoU with gCIoU, the performances of SDA (Row 2 and Row 5 in P2), CDA-r (Row 3 and Row 6 in P2) and CDA-c (Row 4 and Row 7 in P2) are boostd by 0.3% box AP, 0.7% box AP and 1.8% box AP, respectively. Results show that the proposed gCIoU is a favorable loss for circular object detection.Two multi-head initialization methods, random sampling (CDA-r) and uniform sampling (CDA-c), achieve similar results (Row 3 and Row 4 in P2) and both surpass SDA by 0.3% box AP (Row 2 and Row 3 in P2). By using gCIoU, CDA-r and CDA-c initialization methods surpasses SDA 1.1% box AP (Row 5 and Row 6 in P2), and 1.8% box AP (Row 5 and Row 7 in P2), respectively.Numbers of Multi-head and Reference Points. We discuss how the number of Multi-heads in the Decoder affects the CircleFormer-D-DETR model. We vary the number of heads for multi-head attention and the performance of the model is shown in the Table 4. We find that the performance increases gradually as the number of heads increases up to 8. However, the performance drops when the number of head is 16. We assume increasing the number of heads brings too many parameters and makes the model difficult to converge. Similarly, we study the impact of the number of reference points in the cross attention module. We find that 4 reference points give the best performance. Therefore, we choose to use 8 attention heads of decoder and use 4 reference points in the cross attention module through all the experiments."
CircleFormer: Circular Nuclei Detection in Whole Slide Images with Circle Queries and Attention,4,Conclusion,"In this paper, we introduce CircleFormer, a Transformer-based circular medical object detection method. It formulates object queries as anchor circles and refines them layer-by-layer in Transformer decoders. In addition, we also present a circle cross attention module to compute the key-to-image similarity which can not only pool image features at the circle center but also leverage scale information of a circle object. We also extend CircleFormer to achieve instance segmentation with circle detection results. To this end, our CircleFormer is specifically designed for circular object analysis with DETR scheme."
CircleFormer: Circular Nuclei Detection in Whole Slide Images with Circle Queries and Attention,,Table 4 .,# AP↑ AP (50) ↑ AP (75) ↑ AP (S) ↑ AP (M ) ↑
CircleFormer: Circular Nuclei Detection in Whole Slide Images with Circle Queries and Attention,,Table 5 .,# AP↑ AP (50) ↑ AP (75) ↑ AP (S) ↑ AP (M ) ↑
Unified Surface and Volumetric Inference on Functional Imaging Data,1,Introduction,"In recent years, there has been increasing interest in performing surface-based analysis of magnetic resonance neuroimaging (MRI) data in the human cortex. A basic assumption that underpins this approach is that cortical activity will correlate better according to geodesic distance (along the surface) than geometric distance (straight-line). The benefits include improvements in the localisation of functional areas and the establishment of inter-subject correspondence [5]. Due to the fact that MRI data is typically acquired on a 3D voxel grid that does not directly correspond with a 2D surface, a necessary pre-processing step is to project the volumetric data onto the cortical surface, a complex operation for which there is no commonly-agreed upon solution [11,13]. Of particular relevance is the partial volume effect (PVE): because functional imaging voxels are of comparable size to the thickness of the human cortex, each is likely to contain a mixture of cortical grey matter (GM), white matter (WM) and cerebrospinal fluid (CSF). This mixing of signals from different tissues introduces confound into later analysis steps. Furthermore, because not all anatomies in the brain are amenable to surface analysis, data for the subcortex is usually processed in a volumetric manner. This leads to separate workflows for the different anatomies within the brain, as exemplified by the Human Connectome Project's (HCP) parallel fMRISurface and fMRIVolume pipelines [8].The objective of this work was to develop a framework for parameter inference that is able to operate in a surface and volumetric manner simultaneously. This would remove the need for separate workflows whilst ensuring that each anatomy of interest is treated in an optimal manner, namely surface-based for the cortex, volumetric for WM, and region-of-interest (ROI) for subcortical GM structures. This principle corresponds directly with the HCP's concept of grayordinates [8]. In order to achieve this, a novel algorithm that maps data between different representations has been embedded within a modality-specific generative model. Via the use variational Bayesian techniques, it is then possible to perform non-linear model-fitting directly on volumetric timeseries data without pre-processing. The advantages of this approach are demonstrated on simulated perfusion MRI data."
Unified Surface and Volumetric Inference on Functional Imaging Data,2,Methods,"Theory. The spatial locations within the brain at which parameter estimates are obtained are hereafter called nodes. A key step in performing inference is to define a mapping between nodes and the voxel data that has been acquired, which is then embedded within a generative model to yield a function that relates physiological parameters to the data they reconstruct. In the case of volumetric inference, this mapping is one-to-one: each node corresponds to exactly one voxel. The concept can be extended to include other types of node: alongside voxels for WM, surface vertices can represent the cortex and volumetric ROIs can represent subcortical GM structures. Collectively these are referred to as hybrid nodes, which correspond to greyordinates in the HCP terminology [8]. An illustration is given in Fig. 1.In this work, the mapping for hybrid nodes previously introduced in [10] was used. It is constructed by calculating the volume of intersection between individual voxels and the geometric primitives that construct the cortical ribbon, WM tracts, or individual ROIs. Importantly, because each node corresponds by definition to exactly one tissue type, the mapping will be many-to-one in voxels that contain multiple tissues which is an explicit representation of PVE. The mapping takes the form of a non-square sparse matrix (number of nodes > voxels), an example of which is illustrated in Fig. 2. Having defined and embedded the mapping within a generative model, one can turn to the process of parameter inference. We adopt a Bayesian approach because this permits the incorporation of prior knowledge and the quantification of parameter uncertainty. Both attributes are valuable given the challenging signal-to-noise ratio (SNR) of functional imaging data. Under this approach, the objective is to obtain a posterior distribution p for the parameters θ of a generative model M given observed data y, which may be expressed as:The choice of model M and corresponding physiological parameters θ is determined by the imaging modality in question. In practice, evaluating this expression for all but the most trivial configurations is infeasible due to the integrations entailed (notably the evidence term in the denominator). A number of numerical approaches have thus been developed, one of which is variational Bayes (VB) which approximates the posterior p with an arbitrary distribution q and uses the free energy F to assess the accuracy of approximation [1]. Omitting M from notation, F is defined as:VB thus turns parameter inference into an optimisation problem: what q best approximates p as measured by the free energy? One means of implementing this is to use stochastic optimisation techniques. A Monte Carlo approximation to the objective function F may be obtained using an average over L randomly drawn samples θ * l from q(θ):This strategy is referred to as stochastic variational Bayes (SVB). By constructing this expression as a computational graph, including the modalityspecific generative model M with its embedded mapping from hybrid nodes to voxels, automatic differentiation techniques may be used to maximise F and thus derive the optimal approximation q to the true posterior p. The volumetric implementation of SVB previously introduced in [4] has been extended in this manner and the end result is referred to as hybrid SVB, hSVB. Note that this approach does not involve any learning or training: on each dataset, the inference starts afresh and operates on all data until approximate convergence of the optimisation problem has been reached.A key advantage of the Bayesian approach is that it enables prior information to be incorporated into the inference. Such priors can either be distributional, for example an empirically-derived normal distribution; or spatial, which encode the belief that parameter values should correlate in adjacent regions of the brain [15]. A spatial prior is particularly useful as a means of mitigating the low SNR inherent to many functional imaging techniques because it applies regularisation in a principled manner that is determined solely by data quality as opposed to relying on user-set parameters. When applied to volumetric data, a drawback of the spatial prior is that it is unaware of underlying anatomy, and in particular PVE. This is because it is typically implemented using an isotropic Laplacian operator over the first-order neighbourhood of voxels, which is especially problematic at the cortical boundary where it will enforce a similarity constraint across the GM/WM boundary even though these tissues may have different parameter values. Under a hybrid approach, it is possible to restrict the spatial prior to operate only in anatomically contiguous regions. Specifically, one spatial prior is defined on the surface for the cortex, a second spatial prior is defined in the volume for non-cortical tissue, and hence regularisation no longer happens across tissue boundaries. In this work, the isotropic Laplacian on firstorder neighbours was used for the volumetric prior, and the discrete cotangent Laplacian was used for the surface prior [6].Evaluation. hSVB was evaluated using simulated mutli-delay arterial spin labelling (ASL) data, a perfusion modality which is sensitive to both cerebral blood flow (CBF) and arterial transit time (ATT). Inference on multi-delay ASL data requires fitting a non-linear model with two parameters which is more challenging than for single-delay ASL. A single subject's T1 MPRAGE anatomical image (1.5 mm isotropic resolution, TR 1.9 s, TE 3.74 ms, flip angle 8 • ) was processed to extract the left cortical hemisphere, which defined the ground truth anatomy from which ASL data was simulated. FreeSurfer was used to obtain mesh reconstructions of the white and pial cortical surfaces [7]; FSL FIRST was used to segment subcortcial GM structures [14]; FSL FAST was used to segment cerebrospinal fluid [16]; and finally Toblerone [9] was used to obtain WM PV estimates.A single-compartment well-mixed ASL model was used to represent term M in Eq. 1 and simulate data [2]. Pseudo-continuous ASL with six post-label delays of [0.25, 0.5, 0.75, 1.0, 1.25, 1.5] s and 1.8 s label duration was used. For the cortex, a ground truth CBF map with a mean value of 60 units and sinusoidal variation of 40 units peak-to-peak was used, illustrated in Fig. 3, and a constant ATT of 1.1 s was used. In WM and subcortical GM structures, constant reference CBF and ATT values were assumed. Data was simulated at spatial resolutions of [2,3,4,5] mm isotropic and SNR levels of x • 2 n , n ∈ [-1, -0.5, 0, 0.5, 1], i.e., from x/2 to 2x, where x represents typical SNR estimated from a single subject's mutli-delay data acquired in a previous study [12]. In the results, the value x is omitted from notation, so SNR = 2 0 means 'typical SNR'. This variety of resolutions and SNR levels reflects the diversity of acquisitions seen in clinical practice. In all cases, zero-mean additive white noise was used.hSVB has been implemented using TensorFlow version 2.9 running on Python 3.9.12. Optimisation was performed using RMSProp with a learning rate of 0.1, a decay factor of 0.97, and a sample size of 10. During training, a reversion to previous best state ('mean reversion') was performed when cost did not improve for 50 consecutive epochs; for all experiments, training was run until 20 such reversions had taken place which served as a proxy for convergence. Typically this implied training for around 2000 epochs. A folded normal distribution was used on all model parameters to restrict inference to positive values only. Runtime on a 6 core CPU was around 5 mins per inference, using around 4 GB of RAM.The comparator method was based on BASIL, a conventional volumetric ASL processing workflow [3]. Specifically, the oxford_asl pipeline with partial volume effect correction was run using the same PV estimates from which the ASL data was simulated, and then the GM-specific parameter maps were projected onto the cortical surface using the same method as embedded within hSVB [10]. This method is referred to as BASIL-projected, BP. Runtime (single-threaded) was around 5 min per inference, using around 1 GB of RAM.Performance was assessed by calculating the following metrics with respect to the ground truth cortical CBF map: sum of squared differences (SSD) of CBF; SSD of Z-transformed CBF, and Bhattacharyya distance of CBF distribution. The second and third metrics are included because they are sensitive to relative perfusion, whereas the first is sensitive to absolute CBF. A receiver-operator characteristic (ROC) analysis was performed using a binary classifier of varying threshold value t. Recalling that the ground truth CBF map had a mean value of 60 with extrema of ±20, t was set at values of 5 to 15 with an increment of 1. At each t, areas of hypoperfusion were classified with a threshold value of 60 -t and hyperperfusion with 60 + t. The area-under ROC (AUROC) for hypo and hyperperfusion was then calculated and the mean of the two taken. This yielded an AUROC score for each method at varying levels of threshold t. "
Unified Surface and Volumetric Inference on Functional Imaging Data,3,Results,"Figure 4 shows cortical CBF maps estimated by both methods on 3 mm data, flattened down onto a 2D plane for ease of inspection. For all SNR, hSVB's map retained more contrast than BP's: the bright spots were brighter, and vice-versa, particularly so at low SNR. At SNR = 2 -1 , BP's result displayed a positive bias in CBF which was readily observed by comparison to other SNR levels. Figure 5 shows SSD in CBF, SSD in Z-transformed CBF, and Bhattacharyya distance of CBF perfusion distribution, all for the cortex. For SSD, hSVB performed worse than BP at low SNR, and better for SNR ≥ 2 0 . By contrast, for SSD of Z-transformed CBF, hSVB performed better at almost all SNR. In Bhattacharyya distance, hSVB performed better at all voxel sizes and SNR. Of note in all panels was the consistency of hSVB's results across voxel sizes, whereas BP displayed greater variation (particularly for SSD of Z-transformed CBF, where the 2 mm result was substantially better than all others). Fig. 6. AUROC scores for binary classification of cortical hypo and hyper-perfusion at varying threshold values t. For all comparisons, hSVB obtained a higher score than BP.Figure 6 shows AUROC scores for binary classification of the estimated CBF maps returned by both methods. For all voxel sizes and SNR, the AUROC score for hSVB was higher than that of BP, particularly at high threshold values."
Unified Surface and Volumetric Inference on Functional Imaging Data,4,Discussion,"The results presented here demonstrate that a hybrid approach to parameter inference using SVB (hSVB) offers a compelling alternative for the surface-based analysis of functional neuroimaging data. hSVB can operate directly on volumetric data without pre-processing and is able to able to apply the spatial prior in an anatomically-informed manner that respects tissue boundaries.Applied to simulated ASL data, hSVB demonstrated a number of positive attributes in relation to a conventional volumetric workflow with post-projection (BP). Firstly, greater consistency in performance across voxel sizes was observed. As spatial resolution directly determines the extent of PVE within data, this implied that hSVB is more robust to PVE which is an important source of confound. Secondly, at higher levels of SNR, hSVB was able to deliver estimates that scored substantially better across a variety of metrics, which suggests it is well-placed to exploit future increases in SNR that result from advances in hardware and acquisition. Finally, hSVB was better able to discern relative perfusion differences, i.e., areas of abnormality. The trade-off for this was higher SSD errors in absolute perfusion values at low SNR.A key point of divergence between hSVB and BP observed in this work was in the effect of the spatial prior at low SNR levels. Bayesian inference can be understood as an updating process whereby the prior distribution is modified to the extent that the observed data supports this. For low SNR data, the data will support less deviation from the priors. Referring to Fig. 4, BP tended towards globally homogenous solutions with high smoothing and low detail, whereas hSVB did the opposite: increasingly heterogenous ('textured', in the language of image analysis) solutions with low smoothing and high detail. In this scenario, the SSD of CBF metric is asymmetric: as smoothing increases towards a perfectly homogenous solution, SSD will approach a finite asymptotic value, whereas in the opposite outcome of increasingly extreme minima and maxima that preserve overall detail, SSD will increase towards infinity. It is believed that this mechanism explains the high SSD errors obtained by hSVB at low SNR, and also explains why hSVB performed much better on a relative basis (SSD of Z-score CBF), where the ability to distinguish perfusion variation matters more than the absolute values.Two limitations of this work are that the ground truth cortical CBF map used to simulate data (Fig. 3) is clearly highly contrived, and the simulation did not include any of the imperfections normally seen in acquisition data, such as motion, intensity or geometric distortion artefacts. Though such problems are normally dealt with prior to performing inference via separate pre-processing operations, they can rarely be fully corrected, and thus some residual artefact may remain during inference which may degrade performance. The next step in the development of this work will be to try hSVB on human acquisition data to verify similar performance to the results presented here, which is currently in progress."
Towards Accurate Microstructure Estimation via 3D Hybrid Graph Transformer,1,Introduction,"Diffusion microstructure imaging has drawn increasing research attention in recent years. A number of powerful microstructure models have been proposed and shown great success in both clinical and research sides. Typical examples include diffusion kurtosis imaging (DKI) [15], neurite orientation dispersion and density imaging (NODDI) [23], and spherical mean technique (SMT) [16]. However, these models usually rely on diffusion MRI (dMRI) data densely sampled in q-space with a sufficient angular resolution and multiple shells, which are impractical in clinical settings.To resolve this problem, deep learning has been introduced to learn the mapping between high-quality microstructure indices and q-space undersampled dMRI data. For instance, Golkov et al. [12], for the first time, introduced deep learning to microstructure estimation from undersampled dMRI data with a multilayer perceptron (MLP). Gibbons et al. [11] trained a 2D CNN to generate NODDI and fractional anisotropy parameter maps from undersampled dMRI data. Tian et al. [18] proposed DeepDTI to predict high-fidelity diffusion tensor imaging metrics using a 10-layer 3D CNN. Ye et al. designed MEDN [21] and MEDN+ [22] based on a dictionary-inspired framework to learn the mapping between undersampled dMRI data and high-quality microstructure indices. Zheng et al. [24] proposed a three-stage microstructure estimation model that combines sparse encoding with a transformer. Chen et al. [4] introduced the graph convolutional neural network (GCNN) to learn q-space information for microstructure estimation, which considers the angular information in q-space and achieves promising performance.Recently, inspired by the fact that dMRI data live in a joint x-q space [3,14], a powerful hybrid model, called HGT [5], was proposed to jointly learn spatialangular information for accurate microstructure estimation. It achieves superior performance in comparison with various types of existing methods. However, a major limitation of HGT lies in the ignorance of the fact that the spatial domain of dMRI is a 3D space rather than a 2D one. A large number of studies have demonstrated that careful consideration of 3D space is the key to advancing the performance of different medical image analysis tasks [7,9].To this end, we propose 3D-HGT, an advanced microstructure estimation model capable of making full use of 3D x-space information and q-space information jointly. Specifically, we design an efficient q-space learning module based on simple graph convolution (SGC) [20], which runs with less memory at high speed and is able to alleviate the large computational burden associated with 3D spatial information learning. We further propose a 3D x-space learning module based on a U-shape transformer, which is able to capture the long-range relationships in 3D spatial space for improved performance. Finally, we train two modules end-to-end for predicting microstructure index maps. Our 3D-HGT exploits 3D spatial information and angular information in an efficient manner for more accurate microstructure estimation. We perform extensive experiments on data from the human connectome project (HCP) [19]. The experimental results demon-strate that our 3D-HGT outperforms cutting-edge methods, including HGT, both quantitatively and qualitatively.Fig. 1. Overview of 3D-HGT. (a) shows the overall architecture of the model, where the q-space learning module first learns the feature of dMRI data in q-space, and then the x-space learning module learns the 3D spatial information; (b) shows the architecture of the LSA/WSA component, which is a cascaded transformer group with two self-attention blocks; (c) shows the structure of the x-space learning module."
Towards Accurate Microstructure Estimation via 3D Hybrid Graph Transformer,2.1,Network Overview,"As shown in Fig. 1, our 3D-HGT consists of two key modules: an efficient qspace learning module and a 3D x-space learning module. The q-space learning module is built with SGC, which can extract q-space features from voxel-wise dMRI data more efficiently. Motivated by the nnFormer [25], the x-space learning module mainly consists of a U-shaped network composed of local volume-based multi-head self-attention (LSA) and wide volume-based multi-head self-attention (WSA) components.The input dMRI data X ∈ R L×W ×H×G has four dimensions, where L, W , H, and G denote the length, width, height, and number of gradient directions, respectively. In our model, the data is first reshaped into a two-dimensional tensor X ∈ R LW H×G and fed into the SGC network to learn the q-space features efficiently. The output X q is then reshaped to four dimensions Xq ∈ R L×W ×H×G and enters into the x-space learning module followed by convolutional layers. Finally, our 3D-HGT generates microstructure predictions Y ∈ R L×W ×H×M , where M denotes the number of microstructure indices."
Towards Accurate Microstructure Estimation via 3D Hybrid Graph Transformer,2.2,Efficient q-Space Learning Module,"According to [2,6], the geometric structure of the q-space of dMRI data can be encoded as a graph G determined by a binary affinity matrix A = {a i,j }. If the angle θ i,j between two sampling points i and j in q-space is less than an angle threshold θ, then a i,j = 1; otherwise, a i,j = 0. Following such processing, GCNN can be used for q-space learning.Different from HGT, 3D-HGT learns in q-space with SGC, which removes the non-linearity between the graph convolutional layers and collapses K graph convolutional layers into one. In this way, the complex computation of a multilayer network is reduced to a few matrix multiplications. Thus, we can use fast matrix multiplication to speed up computation, reduce network redundancy, and improve computational efficiency. Mathematically, the feature provided by our q-space learning module is as follows:where K denotes the number of SGC layers, Â is the normalized version of A with self-loop added, and Θ is the product of the weight matrices learned by the q-space learning module."
Towards Accurate Microstructure Estimation via 3D Hybrid Graph Transformer,2.3,3D x-Space Learning Module,"The overall design of the x-space learning module is a U-shaped network composed of three parts: an encoder, a bottleneck, and a decoder. The three components are made up of cascaded self-attention transformer blocks. The encoder and decoder are similar in structure, with two self-attention layers. However, there are two key differences between them. Firstly, the former is composed of two local volume-based layers, while the latter has two wide volumebased layers. Secondly, before entering the network, data from the encoder passes through an embedding layer, while data from decoders pass through an extension layer for feature integration before outputting results. The bottleneck layer comprises two LSAs and one WSA, which allows skip attention to integrate shallow and deep attention between the encoder and decoder layers. Notably, we omit the pooling or un-pooling operations in the encoder-decoder architecture since pooling can lead to the loss of features, especially when dealing with small-sized 3D patches.The Embedding Layer. The embedding layer divides the input Xq into highdimensional patches, which are then sent into the transformer block. Unlike the nnFormer, our embedding layer consists of two convolutional layers, a GELU [13] layer and a normalization layer [1], where both the convolutional kernel and step size are set to one.Local Volume-Based Multi-head Self-attention (LSA). Based on Swin Transformer [17], LSA is an offset window self-retaining module that defines the window as a local 3D volume block. The multi-head self-attention is computed in this local volume, which reduces the computational cost significantly.Let Q, K, V ∈ R S L ×S W ×S H ×D for LSA be the query, key, and value matrices, and B ∈ R S L ×S W ×S H be the relative position matrix, where {S L , S W , S H } denotes the size of the local volume and D is the dimension of query/key. The self-attention is then computed in each 3D local volume as follows:The LSA computational complexity can be expressed as follows:where N P is the size of a patch, and C is the length of the embedding sequence.Wide Volume-Based Multi-head Self-attention (WSA). Although LSA is efficient, its receptive field is limited. Thus, by increasing the size of the window to {m × S L , m × S W , m × S H } with m denoting a constant (two by default), we have WSA, which improves the global context awareness ability of LSA. The computational complexity of WSA is as follows:We extend the self-attention field of view in the bottleneck using three wide-field transformer blocks, where six WSA layers are used.Skip Attention. This component effectively combines shallow and deep attention. A single-layer neural network decomposes the output X l at layer l of the encoder into a key matrix K l and a value matrix V l , while the output of X l at layer l of the decoder is used as a query matrix Q l . Mathematically, the self-attention is defined as:where B l is the relative position encoding matrix, and D l is the dimension of query/key."
Towards Accurate Microstructure Estimation via 3D Hybrid Graph Transformer,3.1,Implementation Details,"Experimental Settings. We use the mean square error as the loss function, Adam as the optimizer, and an initial learning rate of 9e-4. We set the number of epochs to 100, batch size to 2, and iterations to 10. The angular threshold θ for constructing graph is set at 45 • , and the number of graph convolutional layers (i.e., K) is set to 2. In the x-space learning module, the embedding dimension is set to 192, and the window sizes of the LSA and WSA layers are set to 4 and 8, respectively. The model was implemented using PyTorch 1.11 and PyTorch-Geometric 2.1.0, and trained on a server equipped with an RTX 3090 GPU.Comparison Methods. We compare our 3D-HGT with various methods, including AMICO [8], MLP [12], GCNN [4], MEDN [21], MEDN+ [22], U-Net [10], U-Net++ [26], and HGT [5]. "
Towards Accurate Microstructure Estimation via 3D Hybrid Graph Transformer,3.2,Dataset and Evaluation Metrics,"Dataset. Following [5], we randomly select 21 subjects from the HCP to construct our dataset. For each dMRI scan, q-space undersampling is performed on a single shell with b=1000 s/mm 2 to extract 30 gradients uniformly. The undersampled data is then normalized by dividing by the average of b 0 images. Finally, we extract patches from the normalized data with a dimension of 32×32×32×30.The validation and test sets are also processed in the same way. In our experiments, the ratio between training, validation, and test sets is 10:1:10. We train our model to predict NODDI-derived indices, including intracellular volume fraction (ICVF), isotropic volume fraction (ISOVF), and orientation dispersion index (ODI). The gold standard microstructural indices are computed using the complete HCP data with 270 gradients using AMICO [8].Evaluation Metrics. We evaluate the quality of predicted NODDI-derived indices with the peak signal-to-noise ratio (PSNR) and structural similarity index measure (SSIM). "
Towards Accurate Microstructure Estimation via 3D Hybrid Graph Transformer,3.3,Experimental Results,"Table 1 compares our 3D-HGT model with cutting-edge models in control experiments under the same conditions using various evaluation metrics. By taking into account the 3D x-space data from the dMRI data, our model is able to learn rich features across a broader range of spatial domains. 3D-HGT outperforms HGT in terms of PSNR and SSIM. The most notable improvements among all indices can be observed in the PSNR of the ICVF, which has increased from 21.96 to 23.60 dB, and the SSIM of the ICVF, which has increased from 0.814 to 0.837. Furthermore, compared with the deep learning models (i.e., MLP to HGT) in Table 1, 3D-HGT improves the PSNR of ALL by 16.3%, 15.5%, 15.2%, 14.0%, 13.3%, 12.0%, and 4.1%, respectively. Although HGT takes into account x-space and q-space, it only makes use of the 2D spatial features of the data. In contrast, 3D-HGT explicitly considers the 3D spatial domain and improves the PSNR and SSIM of ALL by 4.1% and 1.1%, respectively. Figure 2 depicts the visual comparison of 3D-HGT and other models' microstructure predictions. In particular, the close-up views, shown in the bottom row of Fig. 2, indicate that our model provides the best result, which is much closer to the ground truth."
Towards Accurate Microstructure Estimation via 3D Hybrid Graph Transformer,3.4,Ablation Study,"To investigate the effectiveness of the proposed modules, we perform ablation experiments with different ablated versions. The ablation results are shown in Table 2. It should be noted that the ablated versions ""(A)"" and ""(E)"" correspond to HGT and the full version of 3D-HGT, respectively. Effectiveness of Efficient q-Space Learning Module. As shown in Table 2, ""(C)"" is the ablated version with only the 3D x-space learning module. It can be observed that ""(E)"" outperforms ""(C)"", demonstrating that our q-space learning module can effectively improve performance. Moreover, we perform additional experiments using the undersampled dMRI data with a higher angular resolution, i.e., 45 gradients. The results show that our q-space learning module provides a larger improvement when the dMRI data is with a higher angular resolution.The q-space learning with SGC also shows advantages over TAGCN. Both ""(B)"" and ""(E)"" improve the performance and computational efficiency in comparison with their corresponding ablated versions equipped with TAGCN, i.e., ""(A)"" and ""(D)"". In particular, compared with ""(D)"", the training time cost of ""(E)"" is reduced by nearly 30 s for one epoch, verifying the high efficiency of our q-space learning module.Effectiveness of 3D x-Space Learning Module. As shown in Table 2, two sets of comparisons, ""(A)"" vs. ""(D)"" and ""(B)"" vs. ""(E)"", indicate that modifying the x-space learning module to a 3D transformer improves performance. More specifically, compared with ""(A)"", ""(D)"" improves the PSNR of ICVF by 1.48 dB, ISOVF by 0.67 dB, ODI by 0.23 dB and ALL by 0.82 dB. Compared with ""(B)"", ""(E)"" improves the PSNR of ICVF by 1.63 dB, ISOVF by 0.77 dB, ODI by 0.10 dB and ALL by 0.82 dB. The improvement owes to the 3D x-space learning module, which is equipped with LSAs and WSAs, allowing the model to capture long-term dependencies with a large 3D receptive field."
Towards Accurate Microstructure Estimation via 3D Hybrid Graph Transformer,4,Conclusion,"In this paper, we proposed 3D-HGT, an improved microstructure estimation model that makes full use of 3D x-space information and q-space information. Our x-space learning is achieved with a 3D transformer architecture, allowing the model to thoroughly learn the long-term dependencies of features in the 3D spatial domain. To alleviate the large computational burden associated with 3D x-space learning, we further propose an efficient q-space learning module, which is built with a simplified graph learning architecture. Extensive experiments on the HCP demonstrate that, compared with HGT, 3D-HGT effectively improves the quality of microstructure estimation."
Vertex Correspondence in Cortical Surface Reconstruction,1,Introduction,"The reconstruction of cortical surfaces from brain MRI scans, a fundamental process in neuroimaging, involves extracting the pial surface (outer cerebral cortex layer) and the white matter surface (white-gray matter boundary). Various methods, including FreeSurfer [8] and CAT12 [5], have been widely employed for cortical surface reconstruction. While a single patient's surface can be used for computing metrics such as cortical thickness, curvature, and gyrification, one of the main objectives of reconstructing cortical surfaces is to perform group comparisons, which are essential for detecting differences in brain structures between patients and healthy control groups. To enable such comparisons, it is necessary to establish point-to-point correspondence between the vertices of a patient's cortical mesh and a group template. This allows for measures such as cortical thickness to be compared at the vertex level. In addition, vertex correspondence enables the mapping of an atlas parcellation from the template onto individual surfaces, which is useful for comparing measures at a regional level. This includes computing cortical thickness for specific parcels, which has wide applications for studying cortex maturation, as well as, aging-and disease-related cortical atrophy [14,17,20,21]. Currently, vertex correspondence is generated in a post-processing step, which is a time-consuming process that typically involves registering and remeshing a patient's surfaces to an atlas [7,9]. Therefore, directly generating surfaces with vertex correspondence would be valuable for fast, reliable, and accurate cortical surface comparison. Recently, several deep learning methods for cortex reconstruction have emerged, including DeepCSR [4], Vox2Cortex [3], CorticalFlow [13,19], CortexODE [15], and Topofit [11]. These methods can be divided into two categories: (i) implicit surface reconstruction methods, and (ii) explicit template deformation methods. Implicit surface reconstruction methods represent a 2D surface as a signed distance function and rely on mesh extraction and topology correction, which can be computationally demanding and result in geometric artifacts [3,13]. Explicit deformation approaches for cortical surface reconstruction take a template mesh as input to the deep learning model, where a vertex-wise deformation field is learned conditioned on 3D MRI scans. One advantage of mesh-based methods in cortical surface reconstruction is that the sphere-like topology of neural tissue boundaries can already be incorporated into the template mesh. As a result, there is no need for topology correction during the reconstruction process. The main challenge with these methods is generating smooth and watertight output meshes, i.e., ensuring a diffeomorphic mapping from the input to the output mesh. Researchers have addressed this issue through regularization losses [3,11] or numerical integration of a deformation-describing ODE [13,15,19]. The connectivity of the output mesh is mostly determined by the template mesh, with potential up-or down-sampling of the mesh resolution. Keeping the mesh resolution constant facilitates comparisons between reconstructed surfaces as the output mesh has the same number of vertices and vertex connectivity as the input mesh. Despite maintaining constant mesh resolution, Vox2Cortex [3] (V2C), Cor-ticalFlow++ [19] (CFPP), and Topofit [11] have not focused on optimizing or evaluating the accuracy of vertex correspondence. In this work, we propose a novel surface reconstruction approach that natively provides correspondence to a template without the need for spherical registration, see Fig. 1. We achieve this by training on meshes with vertex correspondence instead of meshes that vary in the number of vertices and vertex connectivity. For the network to learn these correspondences, we replace the commonly used Chamfer loss with the L1 loss, which has not yet been used for cortical surface reconstruction. We use V2C as our backbone network as it is fast to train and provides white and pial surfaces for both hemispheres with one network, but our approach is generic and can also be integrated in other surface reconstruction methods. We term our method Vox2Cortex with Correspondence (V2CC). We demonstrate that template deformation methods trained with the Chamfer loss, such as V2C, Topofit, and CFPP, provide vertex correspondences that are insufficient for mapping parcellations. Instead, our approach results in improved interand intra-subject vertex correspondence, making it suitable for direct group comparison and atlas-based parcellation. Top: Overview of existing cortical surface reconstruction approaches, that are dependent on a cumbersome spherical registration as post-processing to obtain vertex correspondence to a template. Bottom: Our approach directly yields surface predictions with correspondence to the input template and does not require any registration."
Vertex Correspondence in Cortical Surface Reconstruction,2,Methods,"Fig. 2. Overview of our V2CC method. The ground truth mesh is registered to the template mesh in a pre-processing step, allowing to compute the L1 loss on the vertex locations. We use V2C [3] as the surface reconstruction network. Vertex correspondence to the template enables direct mapping of an atlas parcellation at inference.In cortical surface reconstruction, template deformation methods transform a mesh template to match the neuroanatomy of the given patient. Let M x be the triangular mesh template, where, storing the indices of the respective vertices that make up the triangles, and r edges E ∈ R r×2 , storing the indices of two adjacent faces that share a common edge. The surface reconstruction algorithm computes the displacement f : R n×3 → R n×3 for the set of vertices V x . In V2C, this displacement is computed by a graph convolutional network, which is conditioned on image features from a convolutional neural network that takes the MRI scan as input. The two sub-networks are connected via feature-sampling modules that map features extracted by the CNN to vertex locations of the meshes. V2C addresses the issue of self-intersections in explicit surface reconstruction methods by incorporating multiple regularization terms into the loss function. Let M y = {V y , F y , E y } be the ground truth mesh, and M ŷ = {V ŷ , F ŷ , E ŷ } the predicted deformed mesh, where Vy ∈ R n×3 , and V y ∈ R m×3 . Note that n = m and therefore there exists no one-to-one mapping for vertex correspondence. The full loss function of V2C consists of a loss term for the CNN and a loss term for the mesh reconstruction, with further details in [3]. Here, we focus on the mesh reconstruction loss, which contains a geometric consistency loss and several regularization terms. The geometric consistency loss L C is a curvature weighted Chamfer loss, and is defined as:where P y ∈ R q×3 , and P ŷ ∈ R q×3 are point clouds sampled from the surfaces of M y and M ŷ respectively. For simplicity, we have omitted the curvature weights. In order to optimize for vertex correspondence, we propose to use a preprocessing step that registers the Mesh M y to the template mesh M x , resulting in a resampled ground truth mesh M y = {V y , F y , E y }, with V y ∈ R n×3 and F y ∈ R o×3 , where each index i ∈ 1, . . . , n represents the same anatomical location in both V x and V y . Instead of the Chamfer loss in Eq. ( 1), we propose the loss function of V2CC as L(M y , M y ) = L1(M y , M y ) + λL reg (M ŷ ), where L 1 is the mean absolute distance between corresponding vertices in M y and My , and L reg is the normal consistency regularization to avoid self-intersections in M ŷ . L 1 and L reg are defined as:where ni is the unit normal of the i-th face of M ŷ . Our method relies on only one regularization term, compared to three in V2C. The regularization factor λ needs to be tuned as a hyperparameter. Our proposed V2CC method and the pre-processing step are presented in Fig. 2."
Vertex Correspondence in Cortical Surface Reconstruction,3,Experiments and Results,"Evaluation Metrics: To assess the quality of reconstructed cortical surfaces, we employ four metrics. With the average symmetric Chamfer distance (cdist ) and the percentage of self-intersecting faces (%SIF ), we evaluate the reconstructed surfaces' quality. For evaluating vertex correspondence, we use two different approaches for intra-and inter-subject cases. In intra-subject cases, we measure whether the same template vertex moves to the same location when provided with different scans of the same subject. In this case, we use scans that were acquired within a brief period of time to avoid structural changes. We calculate the consistency of vertex locations using the root-mean-square deviation (RMSD) of vertex positions. In inter-subject cases, we assess the ability of our method to map pre-defined parcellation atlases, such as the Desikan-Killany-Tourville (DKT) atlas [1,12], onto cortical surfaces. This mapping allows for the assessment of morphological measurements, such as cortical thickness in cortical regions. To evaluate inter-subject vertex correspondence, we directly map vertex classes from the template atlas onto the predicted mesh and calculate the Dice overlap (Dice) to FreeSurfer's silver standard parcellation.Data: For evaluation, we used the ADNI dataset (available at http://adni.loni. usc.edu), which provides MRI T1 scans for subjects with Alzheimer's disease, mild cognitive impairment, and cognitively normal. After removing scans with processing artifacts, we split the data into training, validation, and testing sets, balanced according to diagnosis, age, and sex. As ADNI is a longitudinal study, only the initial (baseline) scan for each subject was used. Our ADNI subset contains 1,155 subjects for training, 169 for validation, and 323 for testing. We used the TRT dataset [16] to evaluate intra-subject correspondence, which contains 120 MRI T1 scans from 3 subjects, where each subject was scanned twice in 20 days. We further tested generalization to the Mindboggle-101 dataset [12] (101 scans) and the Japanese ADNI (J-ADNI, https://www.j-adni.org) (502 baseline scans). All three datasets contain scans from various scanner vendors, with different field strengths (1.5 and 3T).Implementation Details: To prepare for training, we pre-processed the data using FreeSurfer v7.2 [8], generating orig.mgz files and white and pial surfaces to use as silver standard ground truth surfaces. We use FreeSurfer's mri surf2surf tool to register surfaces to fsaverage6 (40,962 vertices) and fsaverage (163,842 vertices) template surfaces. We further followed the pipeline of [3,4], registering MRI scans to the MNI152 space. We used public implementations of baseline methods [2,6,18] and made adaptations so that all methods use the same template for training and testing. All models were trained on NVIDIA Titan-RTX or A100 GPUs. The hyperparameter λ was set to 0.003 for white matter surfaces and 0.007 for pial surfaces after grid search. Our code is publicly available1 ."
Vertex Correspondence in Cortical Surface Reconstruction,,Results and Discussion:,"We compare the proposed V2CC method to state-ofthe-art models V2C, CFPP, and Topofit on the ADNI dataset with FreeSurfer's fsaverage6 right hemisphere templates as an input mesh. All methods were trained using the resampled ground truth meshes. For baseline models we used hyperparameters proposed by the original method. We show the results in the top part of Table 1. Topofit achieves the highest surface reconstruction accuracy on the white matter surface, and CFPP has the lowest number of self-intersecting faces pial surfaces. V2C achieves lower surface accuracy compared to CFPP and Topofit and has a higher number of self-intersections. We believe this could be due to longer training time for Topofit and CFPP, 2600 and 1000 epochs, compared to 100 epochs for V2C. When replacing the loss function in V2C with L 1 , we interestingly observe an immense boost in surface accuracy, as well as improved inter-and intra-subject correspondence. The disadvantage of using the L 1 alone, is seen in an increase of self-intersecting faces, especially on pial surfaces. Self-intersections of pial surfaces can be reduced by introducing the normal consistency regularizer in Eq. ( 2). Next, we trained the baseline V2C model, Topofit, and our V2CC model on higher resolution templates (fsaverage) and images. We present the results on the right hemisphere in the lower section of Table 1. Results for the left hemisphere can be found in the supplementary material. We did not train CFPP on high resolution because of the long training process (about four weeks). We can observe that all models achieve lower surface reconstruction error (cdist) when trained with higher resolution, but also more self-intersections. We believe this is partly due to already existing self-intersections in the fsaverage templates and the resampled ground truth meshes. For the vertex correspondence metrics, we can observe that both baselines, V2C and Topofit, achieve higher parcellation scores than in the low-resolution experiment, but are still outperformed by V2CC. We have further trained a state-of-the-art parcellation model (Fast-Surfer [10]) on the same dataset, which yields a Dice score of 0.88 ± 0.022, so we can conclude that our atlas-based parcellation can even outperform dedicated parcellation models. We visualize the quality of intra-subject correspondence of V2CC and FreeSurfer in the top box of Fig. 3, where we display the per-vertex RMSD on each subject's white matter surface of the right hemisphere. We can observe that for all three subjects, our method leads to less variance of vertex positions than FreeSurfer. This is interesting, as FreeSurfer results were registered and resampled to obtain vertex correspondence and our predictions were not. Further, this shows that even though FreeSurfer surfaces have been used as ground truth to train our model, V2CC generalizes well and is more robust to subtle changes in the images. The bottom box in Fig. 3 visualizes the parcellation result of one example subject and the average parcellation error over the whole test set. We observe that parcellation errors occur mainly in boundary regions for all methods, but these boundary regions are much finer in V2CC. To test the generalization ability of our method, we tested V2CC and V2C on two unseen datasets J-ADNI and Mindboggle. We observe, that V2CC achieves better parcellation Dice scores, while the surface reconstruction accuracy is similar for both methods."
Vertex Correspondence in Cortical Surface Reconstruction,,Downstream Applications:,"We hypothesize that our meshes with vertex correspondence to the template can be directly used for downstream applications such as group comparisons or disease classification, without the need for postprocessing steps. We performed a group comparison of subjects with Alzheimer's disease (AD) and healthy controls of the ADNI test set, where we compare cortical thickness measures on a per-vertex level. We present a visualization of p-values in Fig. 4. We observe that meshes generated with V2CC highlight similar regions to FreeSurfer meshes. The visualization shows significant atrophy throughout the cortex, with stronger amount of thinning in the left hemisphere which matches findings from studies on cortical thinning in Alzheimer's disease [17,21].We further performed an AD classification study based on thickness measures on the ADNI test set. We computed mean thickness values per parcel (DKT atlas parcellation) for V2CC, FreeSurfer, and the V2C [3] baseline. We show the classification results for AD and controls using a gradient-boosted regression tree, trained on thickness measurements from the ADNI training set. The classifiers achieved 0.810 balanced accuracy (bacc) for Freesurfer, 0.804 bacc for V2CC, and 0.776 bacc, for V2C on the ADNI test set. Demonstrating that V2CC achieves comparable results to FS and outperforms V2C. "
Vertex Correspondence in Cortical Surface Reconstruction,4,Conclusion,"In this work, we proposed V2CC, a novel approach for cortical surface reconstruction that directly provides vertx correspondence. V2CC utilizes a pre-processing step, where ground truth meshes are registered to a template, and directly learns the correspondences by optimizing an L1 loss instead of the commonly used Chamfer loss. We evaluated V2CC on several datasets, including ADNI, TRT, Mindboggle-101, and J-ADNI, and compared it to state-of-the-art methods. Our experimental results show that V2CC achieves comparable performance to previous methods in terms of surface reconstruction accuracy. However, V2CC improves intra-and inter-subject correspondence and disease classification based on cortical thickness. We have evaluated our proposed pre-processing step and loss function with V2C as the backbone network, but the underlying concepts are generic and could also be integrated in other methods like Topofit or CFPP."
Vertex Correspondence in Cortical Surface Reconstruction,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3 31.
Domain-Agnostic Segmentation of Thalamic Nuclei from Joint Structural and Diffusion MRI,1,Introduction,"The human thalamus is a brain region with connections to the whole cortex [6,30]. It comprises dozens of nuclei that are involved in diverse functions like cognition, memory, sensory, motor, consciousness, language, and others [14,30,32]. Crucially, these nuclei are differently affected by diseases such as Parkinson's [17], Alzheimer's [9,10], or frontotemporal dementia [40]. Such differentiation has sparked interest in studying the thalamic nuclei in vivo with MRI. This requires automated segmentation methods at the subregion level, as opposed to the whole thalamus provided by neuromaging packages like FreeSurfer [15] or FSL [27], or by convolutional neural networks (CNNs) like DeepNAT [41] or SynthSeg [7].Different approaches have been used to segment thalamic nuclei. Some methods have attempted to register manually labelled histology to MRI [20,23,29], but accuracy is limited by the difficulty in registering two modalities with such different contrasts, resolutions, and artifacts. Diffusion MRI (dMRI) has been used to spatially cluster voxels into subregions, based on similarity in diffusion signal [5,25,31] or connectivity to cortical regions [6,21]. Clustering based on functional MRI connectivity has also been explored [42]. However, such clusters are not guaranteed to correspond to anatomically defined nuclei.Other methods have relied on specialised MRI sequences to highlight the anatomical boundaries of the thalamus, typically at 7T [24,36] or with advanced dMRI acquisitions [4]. A popular method within this category is ""THOMAS"", a labelled dataset of 7T white-matter-nulled scans that has been used to segment the thalamic nuclei with multi-atlas segmentation [35] and CNNs [38]. Its disadvantage is requiring such advanced acquisitions at test time, which precludes its application to legacy data or at sites without the required expertise or resources.One approach that supports training and test data of different modalities is Bayesian segmentation, which combines a probabilistic atlas (derived from one modality) with a likelihood model to compute adaptive segmentations on other modalities using Bayesian inference [3]. A probabilistic atlas of thalamic nuclei built from 3D reconstructed histology is available on FreeSurfer, along with a companion Bayesian segmentation method to segment the nuclei from structural scans [18]. We have recently released an improved version of this method that incorporates dMRI into the likelihood model [37], but it inherits the well-known problems of Bayesian segmentation with partial voluming (PV) [39]. While this tool works well with high-resolution dMRI data (like the Human Connectome Project, or HCP [33]), the lack of PV modelling is detrimental at resolutions much lower than ∼1 mm isotropic. This is the case of virtually every legacy dataset, and many modern datasets that use lower resolutions to keep acquisition time short or for consistency with older timepoints (e.g., ADNI [19]).Finally, there are also supervised discriminative methods that label the thalamus from dMRI. An early approach by Stough et al. [34] used a random forest to segment the thalamus into six groups of nuclei. As features, they used local measures like fractional anisotropy (FA) and the principal eigenvector (V1 ), and connectivity with cortical regions. A recent approach [13] segmented the whole thalamus using the six unique elements of the diffusion tensor image (DTI ) as inputs. While these supervised approaches can provide excellent performance on the training domain, they falter on datasets with different resolution. Here, we present the first CNN for joint dMRI and T1-weighted (T1w ) images that can segment the thalamic nuclei without retraining or fine-tuning. We use domain randomisation to model resolution during training, which enables the CNN to produce super-resolved 0.7 mm isotropic segmentations, independently of the resolution of the input images. Aggressive data augmentation is used to ensure robustness against variations in contrast, shape and artifacts. Finally, our CNN uses a parsimonious representation of the dMRI data (FA+V1), which makes our publicly available tool compatible with virtually every dMRI dataset."
Domain-Agnostic Segmentation of Thalamic Nuclei from Joint Structural and Diffusion MRI,2,Methods,"A summary of our method is shown in Fig. 1. We use our joint T1/DTI Bayesian method in FreeSurfer to segment the thalamic nuclei from a large number of modern, high-quality scans. These segmentations are used as silver standard to train a CNN, thus circumventing the need for manual segmentations. Our approach uses a hybrid domain randomisation and augmentation strategy that enables the network to generalise to virtually any diffusion dataset."
Domain-Agnostic Segmentation of Thalamic Nuclei from Joint Structural and Diffusion MRI,2.1,"Training Dataset, Preprocessing, and Data Representation","To make the CNN compatible with legacy datasets, we choose a simple representation based on the FA and V1 of the DTI fit at each voxel. DTI only requires 7 measurements and is thus compatible with even the oldest datasets. As in many DTI visualisation tools, we combine the FA and V1 into a single 3 × 1 red-greenblue vector at every voxel. This RGB vector has brightness proportional to the FA, and its colour encodes the direction of V1 as shown in Fig. 1a.To obtain accurate training segmentations from the Bayesian method in FreeSurfer [37] we require a high-resolution dataset with reduced PV artifacts. We choose the HCP, which includes 0.7 mm isotropic T1 and 1.25 mm isotropic dMRI with 90 directions and three b-values (1000, 2000, and 3000 s/mm 2 ). We use the HCP to generate our targets and then generate training images at a wide spectrum of resolutions by increasing the voxel size with a degradation model. We consider two RGB images per subject, derived from DTI fits of the b = 1000 and b = 2000 shells, respectively. For each of the two DTI fits, the Bayesian method yields three different sets of segmentations (corresponding to three available likelihood models). All six segmentations are defined on the .7 mm grid (Fig. 1a, right), and comprise 23 thalamic nuclei per hemisphere (46 total) [37]. We exclude the Pc, Pt and VM nuclei provided by the Bayesian segmentation as they are not labelled in every training example due to their small volumes (2-3 mm 3 )."
Domain-Agnostic Segmentation of Thalamic Nuclei from Joint Structural and Diffusion MRI,2.2,Domain Randomisation and Data Augmentation,"We employ domain randomisation and aggressive data augmentation for both our T1 and diffusion data in order to model: (i) the degradation in quality from HCP to more standard acquisition protocols, and (ii) the variability in appearance due to differences in acquisitions and scanners at test time. Domain Randomisation for Resolution: at the crux of our method is the domain randomisation of input resolutions. At every iteration, we randomly sample the voxel dimensions for the T1 and DTI (independently) in two steps. First, we sample a ""coarse"" scalar voxel size from a uniform distribution between 1 and 3 mm. Then, we sample the voxel side length in each direction from a normal distribution centred on this ""coarse"" mean with σ = 0.2 mm.Next, we resample the T1 and RGB channels to the sampled resolution. For the T1, we use a publicly available PV-aware degradation model [8], which accounts for variability in slice thickness and slice spacing. For the RGB, one should theoretically downsample the original diffusion-weighted images, and recompute the DTI at the target resolution. However, the exact characteristics of the blurring depend on the set of directions and b-values, which will not be the same for the training and test datasets. Moreover, recomputing the DTI is too slow for on-the-fly augmentation. Instead, we apply the degradation model to the RGB image directly, which can be done very efficiently. This is only an approximation to the actual degradation, but in practice, the domain randomisation strategy minimises the effects of the domain gap created by the approximation.Data Augmentations: we also apply a number of geometric and intensity augmentations, some standard, and some specific to our dMRI representation.-Global geometric augmentation: we use random uniform scaling (between 0.85 and 1.15) and rotations about each axis (between -15 and 15 • ). Rotations are applied to the images and also used to reorient the V1 vectors.-Local geometric augmentation: we deform the scans with a piecewise linear deformation field, obtained by linear interpolation on a 5 × 5 × 5 × 3 grid. V1 vectors are reoriented with the PPD method (""preservation of principle direction"" [1]). -Local orientation augmentation: we generate a smooth grid of random rotations between [-15 • , 15 • ] around each axis using piecewise linear interpolation on a 5 × 5 × 5 × 3 grid. These are applied to V1 to simulate noise in principle direction -DTI ""speckles"": To account for infeasible FA and V1 voxels generated by potentially unconstrained DTI fitting, we select random voxels in the low resolution images (with probability p = 1 × 10 -4 ), randomise their RGB values, and renormalise them so that their effective FA is between 0.5 and 1. -Noise, brightness, contrast, and gamma: we apply random Gaussian noise to both the T1 and FA; randomly stretch the contrast and modify the brightness of the T1; and apply a random gamma transform to the T1 and FA volumes.These augmentations are applied to the downsampled images. After that, the augmented images are upscaled back to 0.7 mm isotropic resolution. This ensures that all the channels (including the target segmentations) are defined on the same grid, independently of the intrinsic resolution of the inputs (Fig. 1b). At test time, this enables us to produce .7 mm segmentations for scans of any resolution (prior upscaling to the .7 mm grid)."
Domain-Agnostic Segmentation of Thalamic Nuclei from Joint Structural and Diffusion MRI,2.3,Loss,"We build on the soft dice loss [26]. Since some labels in the atlas are very small, we implemented a grouped soft dice by combining nuclei into 10 larger groupings [37]. We then combined this Dice with the average Dice of the individual nuclei and the Dice of the whole thalamus into the following composite loss:where, X l = {x l i } and Y l = {y l i } are the predicted and ground truth probability maps for label l ∈ [0, . . . , L]; G g is the set of label indices in nuclear group g ∈ [1, . . . , 10], label l = 0 corresponds to the background and SDC is the soft Dice"
Domain-Agnostic Segmentation of Thalamic Nuclei from Joint Structural and Diffusion MRI,2.4,Architecture and Implementation Details,"Our CNN is a 3D U-net [11,28] with 5 levels (2 layers each), 3 × 3 × 3 kernels and ELU activations [12]. The first level has 24 features, and every level has twice as many features as the previous one. The last layer has a softmax activation. The loss in Eq. 1 was optimised for 200,000 iterations with Adam [22]. A random crop of size 128 × 128 × 128 voxels (guaranteed to contain the thalami) was used at every iteration. The T1 scans are normalised by scaling the median white matter intensity to 0.75 and clipping at [0, 1]. The DTI volumes are upsampled to the 0.7 mm space of the T1s (using the log domain [2]) prior to the RGB computation. To generate a training target we combine all six segmentation candidates (three likelihood models times two shells) in a two step process: (i) averaging the one-hot encodings of each segmentation, and(ii) coarsely segmenting into 10 label groups and renormalising the soft target.For validation purposes, we used the Bayesian segmentations of 50 withheld HCP subjects and 14 withheld ADNI subjects. Even though the Bayesian segmentation of ADNI data is not reliable enough to be used as ground truth for evaluation (due to the PV problems described in the Introduction), it is still informative for validation purposes -particularly when combined with HCP data. The final model for each network is chosen based on the validation loss averaged between the HCP and ADNI validation sets."
Domain-Agnostic Segmentation of Thalamic Nuclei from Joint Structural and Diffusion MRI,3.1,MRI Data,"We trained on 600 HCP subjects (see Sect. 2.1). For evaluation, we used: HCP: 10 randomly selected subjects (not overlapping with the training data), with manual segmentations of 10 groups of labels (the same as in Sect. 2.3). LOCAL: 21 healthy subjects (9 males, ages 53-80), each with a 1.1 mm isotropic T1 and a test-retest pair of 2.5 mm isotropic dMRI (64 directions, b=1,000). ADNI: 90 subjects from the ADNI, 45 with with Alzheimer's disease (AD) and 45 healthy controls (73.8±7.7 years; 44 females), each with a T1w (1.2×1×1 mm, sagittal) and dMRI (1.35×1.35×2.7 mm, axial, 41 directions, b=1,000) scan."
Domain-Agnostic Segmentation of Thalamic Nuclei from Joint Structural and Diffusion MRI,3.2,Competing Methods and Ablations,"To the best of our knowledge, the only available tool that can segment T1/dMRI of any resolution is Freesurfer [37]. We therefore compare our method with this Table 1. Mean Dice for ground truth comparison (left columns) and test re-test (right columns). Dice is shown for labels grouped into: histological labels (""hist"", 23 labels), manual protocol (""manual"", 10 labels), nuclear groups [37] (""nuclear"", 5 labels), and whole thalamus. CNNs are sorted in descending order of average Dice across columns. algorithm, along with seven ablations of model options: using only the Dice loss; three variations on the way of merging the candidate Bayesian segmentations into a training target (average one-hot; majority voting; and selecting a segmentation at random); and three ablations on the augmentation (omitting the ""speckle"" DTI voxels, the random V1 rotations, and the piecewise linear deformation)."
Domain-Agnostic Segmentation of Thalamic Nuclei from Joint Structural and Diffusion MRI,3.3,Results,"Qualitative results are shown in Fig. 2. Our CNN successfully segments all scans at 0.7 mm resolution, despite the different input voxel sizes. Quantitative results are presented below for three experimental setups, one with each dataset."
Domain-Agnostic Segmentation of Thalamic Nuclei from Joint Structural and Diffusion MRI,,Direct Evaluation with Manual Ground truth Using HCP:,"We first evaluated all competing methods and ablations on the 10 manually labelled subjects. Table 1 (left columns) shows mean Dice scores at different levels of granularity. Thanks to the ground truth aggregation, domain randomisation and aggressive augmentation, most of the CNNs produce higher accuracy than the Bayesian method at every level of detail -despite having been trained on automated segmentations from the Bayesian tool. The only ablation with noticeable lower performance is the one using the Dice of only the fine histological labels (i.e., no Dice of groupings), which highlights the importance of our composite Dice loss.Test-Retest Using LOCAL: Table 1 (right columns) shows Dice scores between the segmentations of the two available sets of images for the LOCAL dataset, for different levels of granularity. All the networks are more stable than the Bayesian method, with considerably higher test-retest dice scores."
Domain-Agnostic Segmentation of Thalamic Nuclei from Joint Structural and Diffusion MRI,,Best-Performing CNN:,"Considering Table 1 as a whole, the CNN with the highest mean Dice is the one without local geometric augmentation. We hypothesise that this is because the benefit of this augmentation is negligible due to  the large number of training cases, and thus does not compensate for the loss of performance due to the approximations that are required to augment on the fly."
Domain-Agnostic Segmentation of Thalamic Nuclei from Joint Structural and Diffusion MRI,,Group Study Using ADNI:,"We segmented the ADNI subjects with the bestperforming CNN, and computed volumes of the thalamic nuclei normalised by the intracranial volume (estimated with FreeSurfer). We then computed ROC curves for AD discrimination using a threshold on: (i) the whole thalamic volume; and (ii) the likelihood ratio given by a linear discriminant analysis (LDA, [16]) on the volumes of the 23 nuclei (left-right averaged). The ROC curves are shown in Fig. 3(a). The area under the curve (AUC ) and accuracy at the elbow are shown in Table 2. The LDA from the CNN and Bayesian methods show no significant difference in overall discriminative power. However, the atrophy detected by the CNN shows greater significance with the AV and VA reaching significance after correction for multiple comparisons (p < 0.002 Wilcoxon ranksum). Additionally, there is a significant increase in the discriminative power of the whole thalamus from the CNN compared to the Bayesian method (p < 0.005 paired DeLong test). This indicates the external boundary of our method may be more useful than that provided by the Bayesian method and often corresponds to a reduction of oversegmentation into the pulvinar (Figs. 3b-e)."
Domain-Agnostic Segmentation of Thalamic Nuclei from Joint Structural and Diffusion MRI,4,Discussion and Conclusion,"We have presented the first method that can segment the thalamic nuclei from T1 and dMRI data obtained with virtually any acquisition, solving the problems posed by PV to Bayesian methods. Using Bayesian segmentations generated from multiple diffusion models while applying hybrid domain randomisation and augmentation methods, we remarkably improve upon both the accuracy and reliability of our source segmentations. Our tool is robust against misregistration from geometric distortion, which is generally more problematic in frontal and occipital regions. Nuclei volumes resulting from the tool show similar discriminative power to those provided by the Bayesian tool, while improving the utility of whole thalamus measurements and increasing segmentation resolution. Crucially, our use of the FA and V1 representation of dMRI data as input means that our tool is compatible with virtually every dMRI dataset. Publicly sharing this ready-to-use tool as part of FreeSurfer 7.4 will enable neuroimaging studies of the thalamic nuclei without requiring any expertise in neuroanatomy or machine learning, and without any specialised computational resources."
Domain-Agnostic Segmentation of Thalamic Nuclei from Joint Structural and Diffusion MRI,,Acknowledgments. Work primarily funded by ARUK (IRG2019A003,"). Additional support by the NIH (RF1MH123195, R01AG070988, P41EB015902, R01EB021265, R56MH121426, R01NS112161), EPSRC (EP/R006032/1), Wellcome Trust (221915/Z/20/Z), Alzheimer's Society (AS-JF-19a-004-517), Brain Research UK, Wolfson; UK NIHR (BRC149/NS/MH), UK MRC (MR/M008525/1), Marie Curie (765148), ERC (677697), and Miriam Marks Brain Research."
Prompt-Based Grouping Transformer for Nucleus Detection and Classification,1,Introduction,"Nucleus classification is to identify the cell types from digital pathology image, assisting pathologists in cancer diagnosis and prognosis [3,30]. For example, the involvement of tumor-infiltrating lymphocytes (TILs) is a critical prognostic variable for the evaluation of breast/lung cancer [4,29]. It is a challenge to infer the nucleus types due to the diversity and unbalanced distribution of nuclei. Thus, we aim to automatically classify cell nuclei in pathological images.A number of methods [7,10,14,[23][24][25]33,34] have been proposed for automatic nuclei segmentation and classification. Most of them use a U-shape model [28] for training to produce dense predictions with expensive pixel-level labels. In this paper, we aim to obtain the location and category of cells, which only needs affordable labels of centroids or bounding boxes. The task can be solved by generic object detector [17,26,27], but they are usually built for everyday objects whose positions and combinations are quite random. Differently, in pathological images, experts often identify nuclear communities via their relationships and spatial distribution. Some recent methods resort to the spatial contexts among nuclei. Abousamra et al. [1] adopt a spatial statistical function to model the local density of cells. Hassan et al. [11] build a location-based graph for nuclei classification. However, the semantics similarity and dissimilarity between nucleus instances as well as the category representations have not been fully exploited.Based on these observations, we develop a learnable Grouping Transformer based Classifier (GTC) that leverages the similarity between nuclei and their cluster representations to infer their types. Specifically, we define a number of nucleus clusters with learnable initial embeddings, and assign nucleus instances to their most correlated clusters by computing the correlations between clusters and nuclei. Next, the cluster embeddings are updated with their affiliated instances, and are further grouped into the categorical representations. Then, the cell types can be well estimated using the correlations between the nuclei and the categorical embeddings. We propose a novel fully transformer-based framework for nuclei detection and classification, by integrating a backbone, a centroid detector, and the grouping-based classifier. However, the transformer framework has a relatively large number of parameters, which could cause high costs in fine-tuning the whole model on large datasets. On the other hand, there exist domain gaps in the pathological images of different organs, staining, and institutions, which makes it necessary to fine-tune models to new applications. Thus, it is of great significance to tune our proposed transformer framework efficiently.Inspired by the prompt tuning methods [13,16,20] which train continuous prompts with frozen pretrained models for natural language processing tasks, we propose a grouping prompt based learning strategy for efficient tuning. We prepend the embeddings of nucleus clusters to the input space and freeze the entire pre-trained transformer backbone so that these group embeddings act as prompt information to help the backbone extract grouping-aware features. Our contributions are: (1) a prompt-based grouping transformer framework for end-to-end detection and classification of nuclei; (2) a novel grouping prompt learning mechanism that exploits nucleus clusters to guide feature learning with low tuning costs; (3) Experimental results show that our method achieves the state-of-the-art on three public benchmarks. "
Prompt-Based Grouping Transformer for Nucleus Detection and Classification,2,Methodology,"As shown in Fig. 1, We propose a novel framework, Prompt-based Grouping Transformer (PGT), which directly outputs the coordinates of nuclei centroids and leverages grouping prompts for cell-type prediction. In the architecture, the detection and classification parts are interdependent and can be trained together. The proposed framework consists of a transformer-based nucleus detector, a grouping transformer-based classifier, and a grouping prompt learning strategy, which are presented in the following."
Prompt-Based Grouping Transformer for Nucleus Detection and Classification,2.1,Transformer-Based Centroid Detector,"Backbone. We adopt Swin Transformer [21] as the backbone to learn deep features. The pixel-level feature maps output from Stage 2 to Stage 4 of the backbone are extracted. Then the Stage-4 feature map is downsampled with a 3 × 3 convolution of stride 2 to yield another lower-resolution feature map. We obtain four feature maps in total. The channel number of each feature map is aligned via a 1 × 1 convolution layer and a group normalization operator.Encoder and Decoder. The encoder and decoder have 3 deformable attention layers [35], respectively. The multi-scale feature maps output by the backbone are fed into the encoder in which the pixel-level feature vectors in all these feature maps are updated via deformable self-attention. After the attention layers, we send each feature vector into 2 fully connected (FC) layers separately to obtain the fine-grained categorical scores of each pixel. Only the Q feature vectors with the highest confidence are preserved as object embeddings and their position coordinates are recorded as reference points. Each decoder layer utilizes crossattention to enhance the object embeddings by taking them as queries/values and the updated feature maps as keys. The enhanced query embeddings are fed into 2 FC layers to regress position offsets which are added to and refine the reference points. The reference points output by the last decoder layer are the finally detected nucleus centroids. The last query embeddings from the decoder are sent to the proposed classifier for cell type prediction."
Prompt-Based Grouping Transformer for Nucleus Detection and Classification,2.2,Grouping Transformer Based Classifier,"In Fig. 2, we develop a Grouping Transformer based Classifier (GTC) that takes grouping prompts g ∈ R G×D and query embeddings q ∈ R Q×D as inputs, and yields categorical scores for each nucleus query. To divide the queries into primary groups, The similarity matrix S ∈ R G×Q between the query embeddings and the grouping prompts is built via inner product and Gumbel-Softmax [12] operation as Eq. ( 1):where W 1 q and W 1 k are the weights of learnable linear projections, γ ∈ R G×Q are i.i.d random samples drawn from the distribution Gumbel(0, 1) and τ denotes the Softmax temperature. Then we utilize the hard assignment strategy [31,32] and assign the query embedding to different groups as Eq. ( 2):where argmax(S) returns a 1 × Q vector, and one-hot(•) converts the vector to a binary G × Q matrix. sg is the stop gradient operator for better training of the one-hot function [31,32]. Then we merge the embeddings belonging to the same group into a primary group via Eq. (3):where g p denotes the embeddings of primary groups, W 1 v and W 1 o are learnable linear weights. To separate the primary groups into the cell categories, we measure the similar matrix between the primary groups g p and learnable class embeddings c e ∈ R C×D to yield advanced class embeddings c a ∈ R C×D , in the same way as Eq.( 1)-(3). To classify each centroid query, we measure the similarity between each query embedding and the advanced class embeddings. The category whose advanced embedding is most similar to a query, is assigned to the centroid query. The classification results c ∈ R C×Q are computed as:"
Prompt-Based Grouping Transformer for Nucleus Detection and Classification,2.3,Loss Function,"The proposed method outputs a set of centroid proposals {(x q , y q )|q ∈ {1, • • • , Q}} with a decoder layer, and their corresponding cell-type scores {c q |q ∈ {1, • • • , Q}} with our proposed classifier. To compute the loss with detected centroids, we use the Hungarian algorithm [15] to assign K target centroids (ground truth) to proposal centroids and get P positive (matched) samples and Q -P negative (unmatched) samples. The overall loss is defined as Eq. ( 4):where ω 1 , ω 2 , ω 3 are weight terms, (x i , y i ) is the i th matched centroid coordinates, (x i , ŷi ) is the target coordinates. c i and c j denote the categorical scores of matched and unmatched samples, respectively. As the target of unmatched samples, ĉj is set to an empty category. FL(•) is the Focal Loss [18] for training the proposed classifier. We adopt the deep supervision strategy [35]. In the training, each decoder layer produces the side outputs of centroids and query embeddings that are fed into a GTC for classifying nuclei. For the 3 decoder layers, they yield 3 sets of detection and classification results for the loss in Eq. (4)."
Prompt-Based Grouping Transformer for Nucleus Detection and Classification,2.4,Grouping Prompts Based Tuning,"To avoid the inefficient fine-tuning of the backbone, we propose a new and simple learning strategy based on grouping prompts, as shown in Fig. 1. We inject a set of prompt embeddings as extra input of the Swin-Transformer [21], and only tune the prompts instead of the backbone. To learn group-aware representations, we further propose to share the embeddings of prompts with those of initial groups in the proposed GTC. Such prompt embeddings are define as Grouping Prompts.For a typical Swin-Transformer backbone, an input pathological image I ∈ R H×W ×3 is divided into HW E 2 image patches of size E × E. We first embed each image patch into a D-dimensional latent space via a linear projection. Then we randomly initialize the grouping prompts g ∈ R G×D as learnable parameters, and concatenate them with the patch embeddings as input. Note that in the backbone, input patch embeddings are separated into different local windows and the grouping prompts are also inserted into each window, as shown in Fig. 3. Our proposed grouping prompt based learning consists of two phases, pre-tuning and prompt-tuning. In the pre-tuning phase, we adopt the Swin-b backbone pre-trained on ImageNet, replace the GTC head in our model (Fig. 1) with 2 FC layers, and train the overall framework without prompts and GTC. In the prompt-tuning phase, grouping prompts are added to the input of the backbone and GTC, while the backbone parameters are frozen.3 Experiments and Results"
Prompt-Based Grouping Transformer for Nucleus Detection and Classification,3.1,Datasets and Implementation Details,"CoNSeP 1 [10] is a colorectal nuclear dataset with three types, consisting of 41 H&E stained image tiles from 16 colorectal adenocarcinoma whole-slide images (WSIs). The WSIs are at 20× magnification and the size of the slides is 500 × 500. We split them following the official partition [1,10].is a breast cancer dataset with three types and consists of 120 image tiles from 113 patients. The WSIs are at 20× magnification and the size of the slides ranges from 465 × 465 to 504 × 504. We follow the work [1] to apply the SLIC [2] algorithm to generate superpixels as instances and split them into 80/10/30 slides for training/validation/testing.Lizard 3 [9] has 291 histology images of colon tissue from six datasets, containing nearly half a million labeled nuclei in H&E stained colon tissue. The WSIs are at 20× magnification with an average size of 1,016 × 917 pixels. Our implementation and the setting of hyper-parameters are based on MMDetection [5]. The number of grouping prompts G is 64. Random crop, flipping, and scaling are used for data augmentation. Our method is trained with PyTorch on a 48 GB GPU (NVIDIA A100) for 12-24 h (depending on the dataset size). More details are listed in the supplementary material."
Prompt-Based Grouping Transformer for Nucleus Detection and Classification,3.2,Comparison with the State-of-the-Art,"The proposed method is compared with the state-of-the-art models: the existing methods for detecting and classifying cells in pathological images, i.e., Hover-Net [10], MCSpatNet [1], SONNET [7], and the sate-of-the-art methods for object detection in natural images, i.e., DDOD [6], TOOD [8], DAB-DETR [19] and Uper-Net with ConvNeXt backbone [22]. As shown in Table 1, our method exceeds all 1 https://warwick.ac.uk/fac/cross_fac/tia/data/hovernet/. 2 https://github.com/TopoXLab/Dataset-BRCA-M2C/. the other methods on three benchmarks with both detection and classification metrics. Specifically, on the CoNSeP dataset, our approach achieves 1.6% higher F-score on the detection (F d ) and 1.8% higher F-score on the classification (F c ) than the second best methods MCSpatNet [1] and UperNet [22]. On BRCA-M2C dataset, our method has 0.5% higher F d and 3.9% higher F c , compared with the second best models MCSpatNet [1] and DAB-DETR [19]. Besides, on Lizard dataset, our method outperforms UperNet [22] by more than 1.5% and 6.4% on F d and F c , respectively. Meanwhile, we conduct t-test on CoNSeP dataset for statistical significance test. The details are listed in the supplementary material. The visual comparisons are shown in Fig. 4. With the context information from surrounding nuclei, our method effectively reduces the misclassification rate of the lymphocytes and neutrophil categories (Blue and Red)."
Prompt-Based Grouping Transformer for Nucleus Detection and Classification,3.3,Ablation Analysis,"The strengths of the grouping transformer based classifier and the grouping prompts are verified on CoNSeP dataset, as shown in Table 2.Prompt-based Grouping Transformer (PGT) is our proposed detection and classification architecture with grouping prompts and the GTC (in Fig. 1), while the  'Baseline' has no these two settings. PT means using naive prompt tuning. GTC means classifying nuclei with the grouping transformer. Our method achieves comparable results to the fully fine-tuning PGT with tuning only 15% parameters. Compared to the Baseline, our method yields 2.4% higher F d and 2.3% higher F c , respectively, which shows the effective combination of the grouping classifier and prompts. 'detached GTC & PT' means that group features and prompts are independent. Our method surpasses the detached setting by 2.4% in F d and 3.1% in F c , which suggests that sharing embeddings of groups and prompts is effective. With a frozen backbone, the performances of 'w/o PT' and 'w/o GTC' are both dropping, which verifies the strength of the prompt tuning and the GTC module, respectively. Table 3 shows the effect of different numbers of grouping prompts on CoNSeP dataset. When the number of groups is small, the classification result is inferior. When the group number is large than 64, the groups may contain too few nuclei to capture their common patterns. It is suggested to set the group number to a moderate value such as 64. "
Prompt-Based Grouping Transformer for Nucleus Detection and Classification,4,Conclusion,"We propose a new prompt-based grouping transformer framework that is fully transformer-based, and can achieve end-to-end nuclei detection and classification. In our framework, a grouping-based classifier groups nucleus features into cluster and category embeddings whose correlations with nuclei are used for identifying cell types. We further propose a novel learning scheme, which shares group embeddings with prompt tokens and extracts features guided by nuclei groups with less tuning costs. The results not only suggest that our method can obtain competitive performance on nuclei classification, but also indicate that the proposed prompt learning strategy can enhance the tuning efficiency."
Prompt-Based Grouping Transformer for Nucleus Detection and Classification,,Table 1 .,c
Prompt-Based Grouping Transformer for Nucleus Detection and Classification,,Table 2 .,c F Epi. c F Stro. c
Prompt-Based Grouping Transformer for Nucleus Detection and Classification,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3_55.
TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,1,Introduction,"Diffusion MRI (dMRI) tractography is the only non-invasive method capable of mapping the complex white matter (WM) connections within the brain [2]. Tractography parcellation [12,28,42] classifies the vast numbers of streamlines resulting from whole-brain tractography to enable visualization and quantification of the brain's WM connections. (Here a streamline is defined as a set of ordered points in 3D space resulting from tractography [45]). In recent years, deep-learning-based methods have been proposed for tractography parcellation [5,7,14,15,17,20,33,34,36,37,39,44], of which many methods are designed to classify streamlines [7,[15][16][17]37,39,44]. However, multiple challenges exist when using streamline data as deep network input. One well-known challenge is that streamlines can be equivalently represented in forward or reverse order [11,39], complicating their direct representation as vectors [7] or images [44]. Another challenge is that the geometric relationships between the streamlines in the brain have previously been ignored: existing parcellation methods [7,[15][16][17]37,39,44] train and classify each streamline independently. Finally, computational cost can pose a challenge for the parcellation of large tractography datasets that can include thousands of subjects with millions of streamlines per subject.In this work, we propose a novel point-cloud-based strategy that leverages neighboring and whole-brain streamline information to learn local-global streamline representations. Point clouds have been shown to be efficient and effective representations for streamlines [1,4,6,15,18,39] in applications such as tractography filtering [1], clustering [7], and parcellation [15,18,38,39]. One benefit of using point clouds is that streamlines with equivalent forward and reverse point orders (e.g., from cortex to brainstem or vice versa) can be represented equally. However, these existing methods focus on a single streamline (one point cloud) and ignore other streamlines (other point clouds) in the same brain that may provide important complementary information useful for tractography parcellation. In computer vision, point clouds are commonly used to describe scenes and objects (e.g., cars, tables, airplanes, etc.). However, point cloud segmentation methods from computer vision, which assign labels to points, cannot translate directly to the tractography field, where the task of interest is to label entire streamlines. Computer vision studies [21,26,32,35,40,41,46] have shown that point interactions within one point cloud can yield more effective features for downstream tasks. However, in tractography parcellation we are interested in the relationship between multiple point clouds (streamlines) in the brain. These other streamlines can provide detailed information about the local WM geometry surrounding the streamline to be classified, as well as global information about the location and pose of the brain that can reduce the need for image registration.Affine or even nonrigid registration is needed for current tractography parcellation methods [13,28,42]. Recently, registration-free techniques have been proposed for tractography parcellation to handle computational challenges resulting from large inter-subject variability and to increase robustness to image registration inaccuracies [19,29]. Avoiding image registration can also reduce computational time and cost when processing very large tractography datasets with thousands of subjects. While other registration-free tractography parcellation techniques require Freesurfer input [29] or work with rigidly MNI-aligned Human Connectome Project data [19], our method can directly parcellate tractography in individual subject space.In this study, we propose TractCloud, a registration-free tractography parcellation framework, as illustrated in Fig. 1. This paper has three main contributions. First, we propose a novel, learnable, local-global streamline representation that leverages information from neighboring and whole-brain streamlines to describe the local anatomy and global pose of the brain. Second, we leverage a training strategy using synthetic transformations of labeled tractography data to enable registration-free parcellation at the inference stage. Third, we implement our framework using two compared point cloud networks and demonstrate fast, registration-free, whole-brain tractography parcellation across the lifespan."
TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,2.1,Training and Testing Datasets,"We utilized a high-quality and large-scale dataset of 1 million labeled streamlines for model training and validation. The dataset was obtained from a WM tractography atlas [42] that was curated and annotated by a neuroanatomist. The atlas was derived from 100 registered tractography of young healthy adults in the Human Connectome Project (HCP) [30]. The training data includes 43 tract classes: 42 anatomically meaningful tracts from the whole brain and one tract category of ""other streamlines,"" including, most importantly, anatomically implausible outlier streamlines. On average, the 42 anatomical tracts have 2539 streamlines with a standard deviation of 2693 streamlines.For evaluation, we used a total of 120 subjects from four public datasets and one private dataset. These five datasets were independently acquired with different imaging protocols across ages and health conditions. (1) developing HCP (dHCP) [9]  S1. The twotensor Unscented Kalman Filter (UKF) [22,25,27] method, which is consistent across ages, health conditions, and image acquisitions [42], was utilized to create whole-brain tractography for all subjects across the datasets mentioned above."
TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,2.2,TractCloud Framework,"Synthetic Transform Data Augmentation. To enable tractography parcellation without registration, we augmented the training data by applying synthetic transform-based augmentation (STA) including rotation, scaling, and translations. These transformations have been used in voxel-based WM segmentation [34], but no study has applied these transformations to study tractography, to our knowledge. In detail, we applied 30 random transformations to each subject tractography in the training dataset to obtain 3000 transformed subjects and 30 million streamlines. Transformations included: rotation from -45 to 45 • C along the left-right axis, from -10 to 10 • C along the anterior-posterior axis, and from -10 to 10 • C along the superior-inferior axis; translation from -50 to 50 mm along all three axes; scaling from -45% to 5% along all three axes. These transformations were selected based on typical differences between subjects due to variability in brain anatomy and volume, head position, and image acquisition protocol. Many methods are capable of tractography parcellation after affine registration [12,42]; therefore, with STA applied to the training dataset, our framework has the potential for registration-free parcellation.Module for Local-Global Streamline Representation Learning. We propose a module (Fig. 2) to learn the proposed local-global representation, which benefits from information about the anatomy of the neighboring WM and the overall pose of the brain. We construct the input for the learning module by concatenating the coordinates of the original streamline (the one to be classified), its local neighbor streamlines, and global whole-brain streamlines. In detail, assume a brain has n streamlines, denoted by S = {s 1 , s 2 , . . . , s n }, s i ∈ R m×3 , where 3 is the dimensionality of the point coordinates and m is the number of points for each streamline (m = 15 as in [42,44]). For streamline s i , we obtain a set of k nearest streamlines, local(s i ) = {s j1 , s j2 , . . . , s jk }, using a pairwise streamline distance [11]. From the whole brain, we also randomly select a set of w streamlines, global(s i ) = {s q1 , s q2 , . . . , s qw }. Then s i , local(s i ), and global(s i ) are concatenated as shown in Fig. 2 to obtain the input of the module, t i ∈ R m×6× (k+w) . The proposed module begins with a shared fully connected (FC) layer with ReLU activation function (h Θ ): (k+w) , where h is the output dimension of h Θ (h=64 [3,26,32]). Finally, the local-global representation r i is obtained through max-pooling  [3,26,32,46]. Here, we explore two widely used networks: PointNet [3] and Dynamic Graph Convolutional Neural Network (DGCNN) [32]. PointNet (see Fig. S1 for network details) encodes point-wise features individually, but DGCNN (see Fig. S2 for network details) encodes point-wise features by interacting with other points on a streamline. Both PointNet and DGCNN then aggregate features of all points through pooling to get a single streamline descriptor, which is input into fully connected layers for classification."
TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,2.3,Implementation Details,"To learn r i , we used 20 local streamlines (selected from 10, 20, 50, 100) and 500 global streamlines (selected from 100, 300, 500, 1000). Our framework was trained with the Adam optimizer with a learning rate of 0.001 using cross-entropy loss. The epoch was 20, and the batch size was 1024. Training of our registrationfree framework (TractCloud reg-free ) with the large STA dataset took about 22 h and 10.9 GB GPU memory with Pytorch (v1.13) on an NVIDIA RTX A5000 3 Experiments and Results"
TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,3.1,Performance on the Labeled Atlas Dataset,"We evaluated our method on the original labeled training dataset (registered and aligned) and its synthetic transform augmented (STA) data (unregistered and unaligned). We divided both the original and STA data into train/validation/test sets with the distribution of 70%/10%/20% by subjects (such that all streamlines from an individual subject were placed into only one set, either train or validation or test). For experimental comparison, we included two deep-learning-based state-of-the-art (SOTA) tractography parcellation methods: DCNN++ [37] and DeepWMA [44]. They were both designed to perform deep WM parcellation using CNNs, with streamline spatial coordinate features as input. We trained the networks based on the recommended settings in their papers and code. Two widely used point-cloud-based networks (PointNet [3] and DGCNN [32]), with a single streamline as input, were included as baseline methods. To evaluate the effectiveness of the local-global representation in TractCloud, we performed experiments using only local neighbor features (PointNet +loc and DGCNN +loc ) and both local neighbor and whole-brain global features (PointNet +loc+glo and DGCNN +loc+glo ). For all methods, we report two metrics (accuracy and macro F1) that are widely used for tractography parcellation [19,24,37,39,44]. The accuracy is reported as the overall accuracy of streamline classification, and the macro F1 score is reported as the mean across 43 tract classes (Table 1). Table 1 shows that the TractCloud framework achieves the best performance on data with and without synthetic transformations (STA). Especially on STA data, TractCloud yields a large improvement in accuracy (up to 9.9%) and F1 (up to 13.8%), compared to PointNet and DGCNN baselines as well as SOTA methods. In addition, including local (PointNet +loc and DGCNN +loc ) and global (PointNet +loc+glo and DGCNN +loc+glo ) features both improve the performance compared to baselines (PointNet and DGCNN) with a single streamline as input. This demonstrates the effectiveness of our local-global representation."
TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,3.2,Performance on the Independently Acquired Testing Datasets,"We performed experiments on five independently acquired, unlabeled testing datasets (dHCP, ABCD, HCP, PPMI, BTP) to evaluate the robustness and generalization ability of our TractCloud reg-free framework on unseen and unregistered data. All compared SOTA methods (DeepWMA, DCNN++) and TractCloud regist were tested on registered tractography, and only TractCloud reg-free was tested on unregistered tractography. Tractography was registered to the space of the training atlas using an affine transform produced by registering the baseline (b = 0) image of each subject to the atlas population mean T2 image using 3D Slicer [10]. For each method, we quantified the tract identification rate (TIR) and calculated the tract-to-atlas distance (TAD), and statistical significance tests were performed for results of TIR and TAD (Table 2). TIR measures if the tract is identified successfully when labels are not available [7,42,44]. Here, we chose 50 as the minimum number of streamlines for a tract to be considered as identified (The threshold of 50 is more strict than 10 or 20 in [7,42,44]). As a complementary metric for TIR, TDA measures the geometric similarity between identified tracts and corresponding tracts from the training atlas. For each testing subject's tract, we calculated the streamline-specific minimum average direct-flip distance [7,11,42] to the atlas tract and then computed the average across subjects and tracts to obtain TDA. We also recorded the computation time for tractography parcellation for every method (Table 2). The computation time was tested on a Linux workstation with an NVIDIA RTX A4000 GPU using tractography (0.28 million streamlines) from a randomly selected subject. To evaluate if differences in result values between our registration-free method (TractCloud reg-free ) and other methods are significant, we implemented a repeated measure ANOVA test for all methods across subjects, and then we performed multiple paired Student's t-tests between TractCloud reg-free method and each compared method. In addition, in order to evaluate how well our framework can perform without registration, we converted identified tracts into volume space and calculated the spatial overlap (weighted Dice) [8,43] between results of TractCloud regist and TractCloud reg-free (Table 3). Furthermore, we also provide a visualization of identified tracts in an example individual subject for every dataset across methods (Fig. 2).As shown in Table 2, all methods achieve high TIRs on all datasets, and the TIR metric does not have significant differences across methods. This demonstrates that most tracts can be identified by all methods robustly. However, our registration-free framework (TractCloud reg-free ) obtains significantly lower TDA values (better quality of identified tracts) than all compared methods on ABCD, HCP, and PPMI datasets, where ages of test subjects are from 9 to 75 years old. On the very challenging dHCP (baby brain) dataset, TractCloud reg-free still significantly outperforms two SOTA methods. Note that TractCloud reg-free directly Table 2. Results of tract identification rate (TIR) and tract distance to atlas (TDA) on five independently acquired testing datasets as well as computation time on a randomly selected subject. TIR results show no significant differences across methods (ANOVA p > 0.05), while TDA results do (ANOVA p < 1 × 10 -10 ). Asterisks show that the difference between TractCloud reg-free and other methods is significant using a paired Student's t-test. ( * p < 0.05, * * p < 0.001). Abbreviations: TC -TractCloud.  The tract spatial overlap (wDice) is over 0.965 on all datasets, except for the challenging dHCP (wDice is 0.932) (Table 3). Overall, our registration-free framework is comparable to (or better than) our framework with registration.Figure 3 shows visualization results of example tracts. All methods can successfully identify these tracts across datasets. It is visually apparent that the TractCloud reg-free framework obtains results with fewer outlier streamlines, especially on the challenging dHCP dataset."
TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,4,Discussion and Conclusion,"We have demonstrated TractCloud, a registration-free tractography parcellation framework with a novel, learnable, local-global representation of streamlines. Experimental results show that TractCloud can achieve efficient and consistent tractography parcellation results across populations and dMRI acquisitions, with and without registration. The fast inference speed and robust ability to parcellate data in original subject space will allow TractCloud to be useful for analysis of large-scale tractography datasets. Future work can investigate additional data augmentation using local deformations to potentially increase robustness to pathology. Overall, TractCloud demonstrates the feasibility of registration-free tractography parcellation across the lifespan."
TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,,Table 3 .,"works on unregistered tractography from neonate brains (much smaller than adult brains). In the challenging BTP (tumor patients) dataset, TractCloud reg-free obtains significantly lower TDA values than SOTA methods and comparable performance to TractCloud regist . As shown in Table2, our registration-free framework is much faster than other compared methods."
TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3_40.
Cortical Analysis of Heterogeneous Clinical Brain MRI Scans for Large-Scale Neuroimaging Studies,1,Introduction,"Clinical MRI exams account for the overwhelming majority of brain MRI scans acquired worldwide every year [19]. These exams comprise several scans acquired during a session with different orientations (axial, coronal, sagittal), resolutions, and MRI contrasts. The acquisition hardware and pulse sequence parameters differ significantly across (and even within) centers, leading to highly heterogeneous data. Since cortical thickness is a robust biomarker in the study of normal aging [25] and many brain disorders and diseases [21,22,24], methods that can extract parcellations and thickness measurements from clinical scans (while registering to a reference spherical coordinate frame) are highly desirable. However, cortical analysis of clinical scans is complex due to large slice spacing (resulting in incomplete cortex geometry description) and heterogeneous acquisitions (hindering supervised approaches leveraging image intensity distributions).Existing neuroimaging research studies [12] rely on isotropic scans with good gray-white matter contrast (typically a 1 mm MPRAGE) and utilize prior information on tissue intensities. Classical cortical analysis pipelines like FreeSurfer [5,9] generate two triangle meshes per hemisphere, one for the white matter (WM) surface and one for the pial surface, while preventing selfintersections. The spherical topology of the surfaces enables mapping coordinates to a sphere, thus enabling computation of vertex-wise statistics across subjects in a common space.Over the last two years, machine learning approaches for cortical reconstruction on 1 mm MPRAGEs have emerged. Methods based on signed distance functions (SDF) like DeepCSR [4] or SegRecon [11] predict voxel-wise SDFs for the WM and pial surfaces. The final meshes are computed as the SDF isosurfaces and do not guarantee topological correctness. PialNN [18] uses an explicit representation to project the pial surface from the WM surface, which is assumed to be topologically correct. Approaches based on surface deformation like TopoFit [14] or Vox2Cortex [3] use image and graph convolutions to predict a deformation that maps a topologically correct template mesh to an input MRI, thus generating WM and pial surfaces. However, these approaches neither prevent self intersections nor guarantee topological correctness."
Cortical Analysis of Heterogeneous Clinical Brain MRI Scans for Large-Scale Neuroimaging Studies,,Contribution:,"Our proposed method allows cortical analysis of brain MRI scans of any orientation, resolution, and MRI contrast without retraining, making it possible to use it out of the box for straightforward analysis of large datasets ""in the wild"". The proposed method combines two modules: a convolutional neural network (CNN) that estimates SDFs of the WM and pial surfaces, and a classical geometry processing module that places the surfaces while satisfying geometric constraints (no self-intersections, spherical topology, regularity). The CNN capitalizes on recent advances in domain randomization to provide robustness against changes in acquisition -in contrast with existing learning approaches that can only process images acquired with the same resolution and MRI contrast as the scans they were trained on. Finally, our method's classical geometry processing gives us geometric guarantees and grants instant access to an array of existing methods for cortical thickness estimation, registration, and parcellation [9]."
Cortical Analysis of Heterogeneous Clinical Brain MRI Scans for Large-Scale Neuroimaging Studies,,Further Related Work:,"The parameterization of surfaces as SDFs has been combined with deep neural networks in several domains [20], including cortical reconstruction [4]. Our robustness to MRI contrast and resolution changes is achieved using ideas from the domain randomization literature [26], which involves training supervised CNNs with synthetic images generated from segmentations on the fly at every iteration. These techniques have been successfully applied to MRI analysis [1,13] and use random sampling of simulation param- eters such as orientation, contrast, and resolution from uniform distributions at every mini-batch, which results in unrealistic appearance, making the CNN agnostic to these features."
Cortical Analysis of Heterogeneous Clinical Brain MRI Scans for Large-Scale Neuroimaging Studies,2,Methods,Our proposed method (Fig. 1) has two distinct components: a learning module to estimate isotropic SDFs from anisotropic scans and a geometry processing module to place the WM and pial surfaces with topological constraints.
Cortical Analysis of Heterogeneous Clinical Brain MRI Scans for Large-Scale Neuroimaging Studies,2.1,Learning of SDFs,"This module estimates isotropic SDFs of the WM and pial surfaces of both hemispheres in a contrast-and resolution-independent fashion. It utilizes a domain randomization approach based on training a voxel-wise SDF regression CNN with synthetic data, which comprises volumetric segmentations and corresponding surfaces (real images are not used). Such training data can be obtained ""for free"" by running FreeSurfer on isotropic T1 scans (we used the HCP dataset [10]).Given a 3D segmentation and four surface meshes (WM and pial surfaces for each hemisphere; see Fig. 1a), we compute the following input/target pairs at every training iteration. As input, we simulate a synthetic MRI scan of random orientation, resolution, and contrast from the 3D segmentation. For this purpose, we use a Gaussian mixture model conditioned on the (spatially augmented) labels, combined with models of bias field, resolution, and noise similar to [1]. We use random sampling to determine the orientation (coronal, axial, sagittal, or isotropic), slice spacing (between 1 and 9 mm), and slice thickness (between 1 mm and the slice spacing). The thickness is simulated with a Gaussian kernel across slices. The final synthetic image is upscaled to 1 mm isotropic resolution, such that the CNN operates on input-output pairs of the same size and resolution.As regression targets, we use voxel-wise SDFs computed from the WM and pial meshes for both hemispheres. The computation of the SDFs would greatly slow down CNN training if performed on the fly. Instead, we precompute them before training and deform them nonlinearly (along with the 3D segmentation) for geometric augmentation during training. While this is only an approximation to the real SDF, it respects the zero-level-set that implicitly defines the surface, and we found it to work well in practice. An example of a synthetic scan and target SDFs used to train the CNN are shown in Fig. 1b.The regression CNN is trained by feeding the synthetic images to the network and optimizing the weights to minimize the L1 norm of the difference between the ground truth and predicted distance maps. In practice, we clip the SDFs at an absolute value of 5 mm to prevent the CNN from wasting capacity trying to model relatively small variations far away from the surfaces of interest. At test time, the input scan is upscaled to the isotropic resolution of the training data and pushed through the CNN to obtain the predicted SDFs."
Cortical Analysis of Heterogeneous Clinical Brain MRI Scans for Large-Scale Neuroimaging Studies,2.2,Geometry Processing for Surface Placement,"To process real clinical scans, we first feed them to the trained CNN to predict the SDFs for the pial and WM surfaces for both hemispheres (Fig. 1c). To avoid generating topologically incorrect surfaces from these SDFs, we capitalize on the extensive literature on the geometry processing of cortical meshes with classical techniques. For reconstructing WM surfaces, we run SynthSeg [1] on the input scan to obtain two binary masks corresponding to the left and right WM labels. From this point on, processing happens independently for each hemisphere. First, we fill in the holes in the hemisphere's mask and tessellate it to obtain the initial WM mesh. Then, we smooth the mesh and use automated manifold surgery [8] to guarantee spherical topology. Next, we iteratively deform the WM mesh by minimizing an objective function consisting of a fidelity term and a regularizer.Specifically: let M = (X, K) denote a triangle mesh, where X = [x 1 , . . . , x V ] represents the coordinates of its V vertices (x v ∈ R 3 ), and K represents the connectivity. Let D w (r) be the SDF for the WM surface estimated by our CNN, where r is the spatial location. The objective function (""energy"") is the following:The first term in Eq. 1 is the fidelity term, which encourages the SDF to be zero on the mesh vertices; we squash the SDF through a tanh function to prevent huge gradients far away from zero. The second and third terms are regularizers that endow the mesh with a spring-like behavior [5]: n v is the surface normal at vertex v; e 1v and e 2v define an orthonormal basis for the tangent plane at vertex v; N v is the neighborhood of v according to K; and λ 1 and λ 2 are relative weights, which we define according to [5] (λ 1 = 0.0006, λ 2 = 0.0002). Optimization is performed with gradient descent. At every iteration, self-intersections are monitored and eliminated by reducing the step size as needed [5].The pial surface is fitted with a very similar procedure, but using the predicted SDF of the pial surface. Figure 1d shows examples of reconstructed surfaces for the axial FLAIR scan from Fig. 1c. Given the fitted WM and pial surfaces, we use FreeSurfer to compute cortical thickness, parcellation, and registration to a common coordinate frame in spherical coordinates (Fig. 1e)."
Cortical Analysis of Heterogeneous Clinical Brain MRI Scans for Large-Scale Neuroimaging Studies,2.3,Implementation Details,"Our voxel-wise regression CNN is a 3D U-net [23] trained with synthetic pairs generated on the fly as explained in Sect. 2.1 above. The U-net has 5 levels with 2 layers each, uses 3 × 3× 3 convolutions and exponential linear activations. The layers have 24 l features, where l is the level number. The last layer uses linear activation functions to model the SDFs. The CNN weights were optimized with stochastic gradient descent using a fixed step size of 0.0001 and 300,000 iterations (enough to converge). At test time, the run time is dominated by the geometry processing (2-3 h, depending on the complexity of the manifold surgery)."
Cortical Analysis of Heterogeneous Clinical Brain MRI Scans for Large-Scale Neuroimaging Studies,3.1,Datasets,"-HCP: we used 150 randomly selected subjects (71 males, ages: 29.9±3.4 years) from HCP [10] to train the U-net. We ran FreeSurfer to obtain the segmentations and SDFs (images are discarded as they are not used in training). -ADNI: in our first experiment, we used 1 mm MPRAGE and corresponding 5 mm axial FLAIR scans of 200 randomly selected subjects from the ADNI dataset [17] (95 males, ages 74.5 ± 7.4 years). This setup enables us to directly compare the results from research-and clinical-grade scans. -Clinical: this dataset comprises 9,735 scans with a plethora of pulse sequence combinations and resolutions from 1,367 MRI sessions of distinct subjects with memory complaints (749 males, ages 18-90) from hospital [2]. Surfaces were successfully generated for 5,064 scans; the rest failed due to insufficient field of view. This dataset includes a wide range of MR contrasts and resolutions. We note that this dataset also includes 581 1 mm MPRAGE scans.The availability of 1 mm MPRAGEs for some of the subjects enables us to process them with FreeSurfer and use the result as ground truth [16] (Fig. 2a). "
Cortical Analysis of Heterogeneous Clinical Brain MRI Scans for Large-Scale Neuroimaging Studies,3.2,Competing Methods,"To the best of our knowledge, the only existing competing method for our proposed algorithm is SynthSR [15], which utilizes a synthetic data generator like ours to turn scans of any resolution and contrast into synthetic 1 mm MPRAGES -which can be subsequently processed with FreeSurfer to obtain surfaces (Fig. 2b). Compared with our proposed approach (Fig. 2c), this pipeline inherits the smoothness of the synthetic MPRAGE, leading to smoother surfaces that may miss larger folds. We also tried training TopoFit [14] on the synthetic images and predicted SDFs, but failed to produce neural networks with good generalization ability, as they led to small blobs on the surfaces at test time (Fig. 2d)."
Cortical Analysis of Heterogeneous Clinical Brain MRI Scans for Large-Scale Neuroimaging Studies,3.3,Results on the ADNI Dataset,"Figure 3 summarizes the results on the ADNI dataset. While previous machine learning approaches focus evaluation on distance errors, these can be difficult to interpret. Instead, we evaluate our method using the performance on the downstream tasks that one is ultimately interested in. First, we computed the accuracy of the Desikan-Killiany parcellation [6] produced by SynthSR and our proposed method. Figure 3a shows the results on the inflated surface of the fsaverage template. Since the parcellation is computed from the curvature of the WM surface, it is a relatively easy problem. The overlap between the ground truth parcellation and the two competing methods is very high. Dice scores over 0.90 are obtained for almost every region in both methods, and the average across regions is almost identical for both methods (0.95).We then used the obtained parcellations to study the effect of Alzheimer's disease (AD) on cortical thickness, using a group study between AD subjects and elderly controls. For this purpose, we first fit a general linear model (GLM) to the cortical thickness at every parcel, using age, gender, and AD status as covariates. We then used the model coefficients to correct the thickness estimates for age and gender, and compared the thicknesses of the two groups.Figure 3b shows the effect sizes (ES) for the reference 1 mm MPRAGEs and the competing methods. The 1 mm scans yield the expected AD cortical thinning pattern [7], with strong atrophy in the temporal lobe (ES >1.0) and, to a lesser extent, in parietal and middle frontal areas (ES∼1.0). The average ES across all regions is 0.64. As expected, the thickness estimates based on the FLAIR scans are less able to detect the differences between the two groups. SynthSR loses, on average, half of the ES (0.32 vs 0.64). Most worrying, it cannot detect the effect on the temporal areas (particularly middle temporal). Our method can detect these differences with ES>0.6 in all temporal regions. On average, our method recovers one third of the ES lost by SynthSR (0.42 vs 0.32).Finally, we studied the effect of aging on cortical thickness using the same GLM as above. Figure 3c shows maps of p-values computed with Student's t distribution, where we have transformed p * = log 10 (p) for easier visualization. Once more, the 1 mm MPRAGEs display the expected pattern [25], with strongest atrophy in superior-temporal and, to less extent, the central and medial frontal gyri. SynthSR fails to detect the superior-temporal effect in the left hemisphere and barely discerns it in the right hemisphere. Our approach, on the other hand, successfully detects these effects. We also note that SynthSR and our method display false positives in frontal areas of the right hemisphere; further analysis (possibly with manual quality control) will be needed to elucidate this result.  "
Cortical Analysis of Heterogeneous Clinical Brain MRI Scans for Large-Scale Neuroimaging Studies,3.4,Results on the Clinical Dataset,"The clinical dataset, despite not being clustered into well defined groups as ADNI, enables us to evaluate our method with the type of data that it is conceived for: a heterogeneous set of brain MRI scans acquired ""in the wild"". Samples of such scans and outputs produced by our method are shown in Fig. 4. In this experiment, we first used the 581 1 mm MPRAGEs to compute the Dice scores of the Desikan-Killiany parcellation on clinical acquisitions. The results are displayed in Fig. 5a, and show that our proposed method is able to sustain high accuracy in this task (the mean Dice is the same as for ADNI), despite the huge variability in the acquisition protocol of the input scans. As in the previous experiment, we also computed aging curves using all non-1 mm-MPRAGE scans (4,483 in total), while correcting for gender and slice spacing. The fitted curve for a representative region (the superior frontal area, which shows consistent effects in Fig. 3c) is shown in Fig. 5b. While the thinning trend exists, the data are rather noisy and the linear fit (ρ = -0.24) underestimates the effect of aging, i.e., the magnitude of the slope. This is apparent when comparing with the fit produced by the 581 MPRAGEs (Fig. 5c, ρ = -0.55)."
Cortical Analysis of Heterogeneous Clinical Brain MRI Scans for Large-Scale Neuroimaging Studies,3.5,Discussion and Conclusion,"We have presented a novel method for cortical analysis of clinical brain scans of any MRI contrast and resolution that does not require retraining. To the best of our knowledge, this is the first method seeking to solve this difficult problem. The method runs in 2-3 h but could be sped up by replacing some modules (e.g., the spherical registration) with faster learning methods.Our method provides accurate parcellation across the board, which is helpful in applications like diffusion MRI (e.g., for seeding or constraining tractography with surfaces and parcellations when a T1 scan is unavailable or is difficult to register due to geometric distortion of the diffusion-weighted images). However, we observed increased variability in cortical thickness when processing the highly heterogeneous clinical dataset. Future work will focus on improving the reliability of thickness measurements and provide a confidence for the quality of reconstruction and cortical thickness prediction for lower resolution scans. In such scenarios assessing and modeling geometric covariates (e.g., vertex-wise distance to the nearest slice or angle between surface and acquisition orientation) may help reduce such variability.Our method and the clinical dataset are publicly available, which enables researchers worldwide to capitalize on millions of retrospective clinical scans to perform cortical analysis currently unattainable in research studies, particularly for rare diseases and underrepresented populations."
atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,1,Introduction,"Diffusion-weighted MRI enables visualization of brain white matter structures. It can be used to generate tractography data consisting of millions of synthetic fibers or streamlines for a single subject stored in a tractogram that approximate groups of biological axons [1]. Many applications require streamlines to be segmented into individual tracts corresponding to known anatomy. Tract segmentations are used for a variety of tasks, including surgery planning or tract-specific analysis of psychiatric and neurodegenerative diseases [2,11,12,17].Automated methods built on supervised machine learning algorithms have attained the current state-of-the-art in segmenting tracts [3,18,21]. Those are trained using various features, either directly from diffusion data in voxel space or from tractography data. Models may output binary masks containing the target white matter tract, or perform a classification on streamline level. However, such algorithms are commonly trained on healthy subjects and have shown issues in processing cases with anatomical abnormalities, e.g. brain tumors [20]. Consequently, they are unsuitable for tasks such as preoperative planning of neurosurgical patients, as they may produce incomplete or false segmentations, which could have harmful consequences during surgery [19]. Additionally, supervised techniques are restricted to fixed sets of predetermined tracts and are trained on substantial volumes of hard-to-generate pre-annotated reference data.Manual methods are still frequently used for all cases not yet covered by automatic methods, such as certain populations like children, animal species, new acquisition schemes or special tracts of interests. Experts determine regions of interest (ROI) in areas where a particular tract is supposed to traverse or through which it must not pass, and segmentations can be accomplished either (1) by virtually excluding and maintaining streamlines from tractography according to the defined ROI or (2) by using these regions for tract-specific ROI-based tractography. Both approaches require a similar effort, although the latter is more commonly used. The correct definition of ROIs can be time-consuming and challenging, especially for inexperienced users. Despite these limitations, ROI-based techniques are currently without vivid alternatives for segmenting tracts that automated methods cannot handle.Methods to simplify tract segmentation have been proposed before. Clustering approaches were developed to reduce complexity of large amounts of streamlines in the input data [4,6]. Tractome is a tool that allows interactive segmentation of such clusters by representing them as single streamlines that can interactively be included or excluded from the target tract [14]. Although the approach has shown promise, it has not yet supplanted conventional ROIbased techniques.We propose a novel semi-automated tract segmentation method for efficient and intuitive identification of arbitrary white matter tracts. The method employs entropy-based active learning of a random forest classifier trained on features of the dissimilarity representation [13]. Active learning has been utilized for several cases in the medical domain, while it has never been applied in the context of tract segmentation [7,9,16]. It reduces manual efforts by iteratively identifying the most informative or ambiguous samples, here, streamlines, during classifier training, to be annotated by a human expert. The method is implemented as the tool atTRACTive in MITK Diffusion1 , enabling researchers to quickly and intuitively segment tracts in pathological datasets or other situations not covered by automatic techniques, simply by annotating a few but informative streamlines."
atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,2.1,Binary Classification for Tract Segmentation,"To create a segmentation of a white matter tract from an individual wholebrain tractogram T , streamlines which not belong to this tract must be excluded from the tractography data. This is formulated as a binary classification of a streamline s ∈ T , depending on whether it belongs to the target tract t (see Fig. 1 for a brief summary of the nomenclature of this work)To perform the classification, supervised models have been trained on various features representing the data. We choose the dissimilarity representation proposed by Olivetti to classify streamlines, which has shown well performance and can be computed quickly for arbitrary data [3,13]. A number of n streamlines, in this case, n = 100, are used as prototypes forming a reference system of the entire tractogram. A streamline is expressed through a feature vector relative to this reference system. Briefly, a single streamline s = [x 1 , ..., x m ], i.e. a polyline containing varying numbers of ordered 3D points..m, is described by its minimum average direct flip distance d MDF to each prototype [5]. The d MDF of a streamline s a to a prototype p a is defined aswherewith m being the number of 3D points of the streamlines andAdditionally to d MDF , the endpoint distance d END between a streamline and a prototype is calculated, which is equal to d MDF , besides, only the start points x 1 and endpoints x m of the streamline and prototype are respected for the calculation [3]. Hence, features for a single streamline are represented by a vector twice the number of prototypes. In order to calculate these, all streamlines must have the same number of 3D points and are thus resampled to m = 40 points."
atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,2.2,Active Learning for Tract Selection,"Commonly, for training classifiers, large amounts of annotated and potentially redundant data are used, leading to high annotation efforts and long training times. Active learning reduces both by training machine learning models with only small and iteratively updated labeled subsets of the originally unlabeled data. The proposed workflow is initialized, as shown in Fig. 2(a), by presenting a randomly selected subset S rand = [s 1 , ..., s n ] of n = streamlines from an individual whole-brain tractogram to an expert for annotation, where S rand ⊂ T . Subsequently, the dissimilarity representation is calculated using initially 100 prototypes (Fig. 2(b)), and a classifier is trained, in this case, a random forest. Within completing the training, which takes only a few seconds, the classifier predicts whether the remaining unlabeled streamlines belong to the target tract. Based on the predicted class labels, the target tract is presented (Fig. 2(c)). Furthermore, the class probabilities p(s) determined by the random forest are used to estimate its uncertainty with respect to each sample by calculating the entropy ENext, a subset S Emax of streamlines with the highest entropy or uncertainty is selected to be labeled by the expert and is added to the training data (Fig. 2 (c)) [8]. Additionally, these streamlines are utilized as adaptive prototypes until a threshold of n = 100 adaptive prototype streamlines is reached.Since the model selects ambiguous streamlines in the target tract region, utilizing them as supplementary prototypes improves feature expressiveness in this region of interest. This process is repeated iteratively until the expert accepts the prediction."
atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,3.1,Data,"The proposed technique was tested on a healthy-subject dataset and on a dataset containing tumor cases. The first comprises 21 subjects of the human connectome project (HCP) that were used for testing the automated methods TractSeg and Classifyber [3,18]. Visual examinations revealed false-negatives in the reference tracts, meaning that some streamlines that belong to the target tract were not included in the reference. These false-negatives did not affect the generation of accurate segmentation masks, since most false-negatives are occupied by true-positive streamlines, but negatively influenced our experiments. To reduce false-negatives, the reference segmentation mask as well as start-and end-region segmentations were used to reassign streamlines from the tractogram using two criteria: Streamlines must be inside the binary reference segmentation (1) and start and end in the assigned regions (2). As the initial size of ten million streamlines is computationally challenging and unsuitable for most tools, all tractograms were randomly down-sampled to one million streamlines. We focused on the left optic radiation (OR), the left cortico-spinal tract (CST), and the left arcuate-fasciculus (AF), representing a variety of established tracts.To test the proposed method on pathological data, we used an in-house dataset containing ten presurgical scans of patients with brain tumors. Tractography was performed using probabilistic streamline tractography in MITK Diffusion. To reduce computational costs, we retained one million streamlines that passed through a manually inserted ROI located in an area traversed by the OR [15]. Subjects have tumor appearance with varying sizes ((17.87±12.73 cm 3 )) in temporoloccipital, temporal, and occipital regions, that cause deformations around the OR and lead to deviations of the tract from the normative model."
atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,3.2,Experimental Setup,"To evaluate the proposed method, we conducted two types of experiments. Manual segmentation experiments using an interactive prototype of atTRACTive were initiated on the tumor data (holistic evaluation). Additionally, reproducible simulations on the freely available HCP and the internal tumor dataset were created (algorithmic evaluation). In order to mimic expert annotation during algorithmic evaluation, class labels were assigned to streamlines using previously generated references. The quality of the predictions was measured by calculating the dice score of the binary mask. The code used for these experiments is publicly available2 .For the algorithmic evaluation, the initial training dataset was created with 20 randomly selected streamlines from the whole-brain tractogram, which have been shown to be a decent number to start training. Since some tracts contain only a fraction of streamlines from the entire tractogram, it might be unlikely that the training dataset will contain any streamline belonging to the target tract. Therefore, two streamlines of the specific tract were further added to the training dataset, and class weights were used to compensate for the class unbalance. According to Fig. 2, the dissimilarity representation was determined, the random forest classifier was trained and the converged model was used to predict on the unlabeled streamlines and to calculate the entropy. In each iteration, the ten streamlines with the highest entropy are added to the training dataset, which has been determined to be a good trade-off between annotation effort and prediction improvement. The process was terminated after 20 iterations, increasing the size of the training data from 22 to 222 out of one million streamlines.The holistic evaluation was conducted with equal settings, except that the workflow was terminated when the prediction matched the expectation of the expert. To ensure that the initial dataset S rand contained streamlines from the target tract, the expert initiated the active learning workflow by defining a small ROI that included fibers of the tract. S rand was created by randomly sampling only those streamlines that pass through this ROI. To allow comparison between the proposed and traditional ROI-based techniques, the OR of subjects from the tumor dataset were segmented using both approaches by an expert familiar with the respective tool, and the time required was reported to measure efficiency.Note, in all experiments, the classifier is trained from scratch every iteration, prototypes are generated for each subject individually, and the classifier predicts on data from the same subject it is trained with, as it performs subject-individual tract segmentation and is not used as a fully automated method. To ensure a stable active learning setup that generalizes across different datasets, the whole method was developed on the HCP and applied with fixed settings to the tumor data [10]."
atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,3.3,Results,"In Table 1, the dice score of the active learning simulation on the HCP and tumor data after the fifth, tenth, and twentieth iterations are shown and compared with outcomes of Classifyber and TractSeg. Results for the HCP data were already on par with the benchmark of automatic methods between the fifth and tenth iterations. On the tumor data, the performance of the proposed method remains above 0.7 while the performance of TractSeg drops substantially. Furthermore, Classifyber does not support the OR and is therefore not listed in Table 1. Figure 3 depicts the quantitative gain of active learning on the three tracts of the HCP data and compares it to pure random sampling by displaying the dice score depending on annotated streamlines. While active learning leads to an increase in the metric until the predictions at around five to ten iterations show no meaningful improvements, the random selection does not improve overall. Qualitative results of the algorithmic evaluation of the AF of a randomly chosen subject of the HCP dataset are shown in Fig. 4(a). Initially, the randomly sampled streamlines in the training data are distributed throughout the brain, while entropy-based selected streamlines from subsequent iterations cluster around the AF. The prediction improves iteratively, as indicated by a rising dice score. When accessing qualitative results of the pathological dataset visual inspection revealed particularly poorly performance of TractSeg in cases where OR fibers were in close proximity to tumor tissue, leading to fragmented segmentations, while complete segmentations were reached with active learning even for these challenging tracts after a few iterations, as shown in Fig. 4(b).The initial manual experiments with atTRACTive were consistent with the simulations. The prediction aligned with the expectations of the expert at around five to seven iterations taking a mean of 4,5 min, while it took seven minutes on average to delineate the tract with ROI-based segmentation. During the iterations, streamlines around the target tract were suggested for labeling, and the prediction improved. Visual comparison yielded more false-positive streamlines with the ROI-based approach while atTRACTive created more compact tracts. "
atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,4,Discussion,"Active learning-based white matter tract segmentation enables the identification of arbitrary pathways and can be applied to cases where fully automated methods are unfeasible. In this work, algorithmic evaluation as well as the implementation of the technique into the GUI-based tool atTRACTive including further holistic manual experiments were conducted. The algorithmic evaluation yielded consistent results from the fifth to the tenth iterations on both the HCP and tumor datasets. As expected, outcomes obtained from the tumor dataset were not quite as good as those of the HCP dataset. This trend is generally observed in clinical datasets, which tend to exhibit lower performance levels compared to high-quality datasets, which could be responsible for the decline in the results. Preliminary manual experiments with atTRACTive indicated active learning to have shorter segmentation times compared to traditional ROI-based techniques. These experiments are in line with the simulations as the generated tracts matched the expectations of the expert after around five to seven iterations, meaning that less than a hundred out of million annotated streamlines are required to train the model. Enhancements to the usability of the prototype are expected to further improve efficiency. A current limitation of atTRACTive is the selection of the initial subset, based on randomly sampling streamlines passing through a manually inserted ROI. This approach does not guarantee that streamlines of the target tract are included in the subset. In that case, the ROI has to be replaced or S rand needs to be regenerated.Future analyses, evaluating the inter-and intra-rater variability compared to other interactive approaches, will be conducted on further tracts. For selected scenarios, the ability of the classifier to generalize by learning from previously annotated subjects will be investigated, which may even allow to train a fully automatic classifier for new tracts once enough data is annotated. To further optimize the method, the feature representation or sampling procedure could be improved. Uncertainty sampling may select redundant streamlines due to similar high entropy values. Instead, annotating samples with high entropy values being highly diverse or correcting false classifications could convey more information.By introducing active learning into tract segmentation, we provide an efficient and intuitive alternative compared to traditional ROI-based approaches.atTRACTive has the potential to interactively assist researchers in identifying arbitrary white matter tracts not captured by existing automated approaches."
Disentangling Site Effects with Cycle-Consistent Adversarial Autoencoder for Multi-site Cortical Data Harmonization,1,Introduction,"Modern large multi-site neuroimaging studies have shown increasing power to detect biological variability of interest and provided invaluable insights into the changes underlying neurodevelopmental and neurodegenerative disorders [8,12]. However, the aggregation of neuroimaging data across different sites and scanners typically introduces non-biological variability, also known as site effects [7]. Many harmonization methods are thus proposed to remove such unwanted site effects while preserving biological variability [14,20].Inspired by image-to-image translation techniques in the computer vision field [30], many methods harmonized neuroimaging data in the image domain by synthesizing brain images among different sites [2,4,11]. However, imagebased harmonization techniques cannot guarantee that the final derived structural or functional features are free of site effects, due to the huge complexity in neuroimaging data processing pipelines [23]. Alternatively, feature-based harmonization techniques have been proposed to directly mitigate the site effects in the final derived volumetric [19,21], structural [7,19,21,28], functional [26], or diffusion magnetic resonance imaging (dMRI) features [18,25]. Most of these methods are developed based on linear statistical models, e.g., Combat [7] and its variants [21], for harmonizing summarized cortical properties in each region of interest (ROI), e.g., cortical thickness [7] and surface area [20]. Although achieving promising results for long ROI-wise data (number of samples n > number of features p), statistical models have inherent problems when applied to wide vertex-wise data (n p) with spatially fine-grained cortical information [1,15]. This is because the growth in data dimension and possible associations in wide vertex-wise data makes the model more complex and consequently statistical inference becomes less tractable and precise [15]. Moreover, since the sources and underlying mechanisms of site effects are heterogeneous and not fully uncovered, linear models and their hypothesis on the model's parameter distribution may not sufficiently represent the complex non-linear mapping of vertex-wise data. Therefore, deep learning methods, that make minimal assumptions about the data-generating mechanisms and thus can automatically learn to fit the complex non-linear mappings, are more favored [15]. For example, Zhao et al. [28] proposed a surface-to-surface CycleGAN to harmonize cortical thickness maps between two sites, which, however, is inefficient and inconvenient in practice for harmonizing multi-site data, because a model needs to be re-trained between any two sites and ignores rich global information in the whole multi-site data.To address these issues, we develop a novel, flexible deep learning-based method to harmonize multi-site cortical surface maps in a vertex-wise manner free of parcellation scheme, thus preserving the spatially detailed cortical measure information after harmonization and enabling comprehensive vertexwise analysis in further studies. Our approach builds on a surface-based autoencoder and uses an adversarial strategy [5] to encourage the disentanglement of site-related and site-unrelated components. To learn a more controllable and meaningful generative model, we enforce the cycle consistency between forward and backward mappings, inspired by CycleGAN [30]. Our method also shares certain similarity with the adversarial autoencoder [17], domain-adversarial neural networks [9], and guided variational autoencoder [5], but significantly differs in the operation space, network structure, and most importantly, the aim and application, where we focus on a more meaningful data harmonization task in the neuroimaging field. To sum up, the main contributions of this paper are:1. We propose a novel method to fill the critical gap for multi-site vertex-wise cortical data harmonization, while there exist statistical models suitable for ROI-wise data harmonization and CycleGAN model for two-site data; 2. Taking advantage of disentanglement learning and adversarial strategies, we successfully learn a transparent, controllable, and meaningful generative model for efficiently mapping cortical surface data across different sites; 3. To the best of our knowledge, we performed the largest validation on infant cortical data harmonization with 2,342 scans from 4 sites and demonstrate the superior performance of our method compared to other methods."
Disentangling Site Effects with Cycle-Consistent Adversarial Autoencoder for Multi-site Cortical Data Harmonization,2.1,Vanilla Autoencoder (AE),"As shown in Fig. 1(a), we developed our method based on a vanilla autoencoder (AE) to enjoy its transparency and simplicity. Let X = (x 1 , ..., x n ) denote a set of input cortical surface property maps, e.g., cortical thickness map, where n is the number of samples. The encoder network E φ parameterized by φ extracts the latent features z : z = E φ (x), which is developed based on the fundamental operations of spherical CNN proposed in Spherical U-Net [27]. Spherical U-Net leverages the consistent and regular data structure of resampled spherical cortical surfaces to design the 1-ring filter on cortical surfaces and accordingly extends convolution, pooling, and upsampling operations to the spherical surface.Herein, E φ consists of 5 repeated spherical 1-ring-convolution+Batch Normalization (BN)+ReLU layers with 4 spherical mean pooling layers between them.The feature channel at each resolution is 8, 16, 32, 64, and 128, respectively. To enable more compact feature extraction, we add a flatten and a linear layer with 512 neurons to the end of the encoder and finally obtain the latent vector z with size 1×512. The decoder D θ first uses a linear layer with 20,736 neurons and then reshapes it into 162×128 to recover the feature map at lowest resolution. Then it gradually upsamples the features and finally reconstructs the input data with its original size, which can be formulated as: x = D θ (z). The training process of an AE thus tries to minimize the reconstruction loss:"
Disentangling Site Effects with Cycle-Consistent Adversarial Autoencoder for Multi-site Cortical Data Harmonization,2.2,Disentangled Autoencoder (DAE),"To detect and remove site effects from multi-site vertex-wise cortical measurements, we introduce a disentanglement learning strategy for its controllability and interpretability [5]. As shown in Fig. 1(b), suppose for training data X = (x 1 , . . . , x n ), there are M sites in total and the corresponding ground-truth site labels are T = (t 1 , ..., t n ). Let z = (z t , z i ), where z t defines a 1 × M vector representing the site-wise classification probability and z i represents remaining latent variables. We use the adversarial excitation and inhibition method [5] to encourage the disentanglement of the latent variables:where W t and W i refer to the site classifiers using latent variables z t and z i , respectively, t i is a one-hot vector encoding the ground-truth site label. The multi-class binary cross entropy loss in Eq. ( 2) thus encourages z t to be the same as correct site label. Conversely, Eq. ( 3) is an inhibition process, as we want the remaining variables z i to be site-unrelated. W t is a simple sigmoid layer for outputting probability, while W i consists of four linear layers with BN+ReLU+dropout layers between them and a sigmoid layer at the end. This is because z t should be directly correlated with the site-wise predictions, while z i with more features needs a stronger classifier to detect site-related information in it and adversarially train the encoder to extract site-unrelated features for z i ."
Disentangling Site Effects with Cycle-Consistent Adversarial Autoencoder for Multi-site Cortical Data Harmonization,2.3,Cycle-Consistent Disentangled Autoencoder (CDAE),"Previously, a DAE, after successful training, will be directly used for image synthesis [5] or MR image harmonization [31] by combining site-unrelated variables with the target site label. Such image generation process may introduce new and even unseen style patterns in computer vision applications, which, however, are artifacts that are not meaningful and acceptable in medical imaging field [16].To learn a more controllable and meaningful generative model, we propose to train the decoder with additional constraints on backward mapping. As shown in Fig. 1(c), after generating the surface map from source site a to target site b, denoted as x b i , we backward map it to the source site: (â,, and enforce the cycle-consistency loss to guarantee the generated surface map is meaningful to the original surface map:We also add the cycle-consistency loss to the latent site-unrelated variable z i to reinforce the correlation between the generated and original surface map:and an explicit correlation loss to further reduce the ambiguity of indirect cycleconsistency losses for better preserving global structural information:where cov denotes the covariance, σ is the standard deviation. Besides, we also use the losses in AE and DAE to train the backward mapping. This means we reuse the site classifier W t to adversarially train the decoder to generate fake surface maps at the target site that cannot be distinguished from the true maps, which is a standard GAN training process [10]. Finally, our model can automatically detect and disentangle the site-related feature from the input data using DAE losses, and generate more meaningful mappings across sites and better preserve individual variability using CDAE losses, thus fulfilling the requirement of data harmonization task. "
Disentangling Site Effects with Cycle-Consistent Adversarial Autoencoder for Multi-site Cortical Data Harmonization,3.1,Experimental Setting,"We evaluated our method using 4 large infant datasets (S1, S2, S3, and S4) acquired by different scanners and imaging protocols with resolutions ranging from isotropic 0.8 mm 3 (S1) to 1.25 × 1.25 × 1.95 mm 3 (S3). S1 [13] and S4 [12] are two public datasets with 609 and 779 scans, respectively. S2 [22] and S3 [24] are two in-house datasets containing 335 and 619 scans, respectively. To the best of our knowledge, this is the largest collection of infant MRI datasets for harmonization and joint analysis purposes. All images were preprocessed using the infant-dedicated pipeline iBEAT V2.0 (http://www.ibeat.cloud/) [23].Then the reconstructed cortical surfaces were mapped onto the sphere, nonlinearly aligned based on geometric features, and further resampled with 40,962 vertices [6]. Figure 2 shows typical reconstructed surfaces color-coded by cortical thickness in the first row. As can be seen, the site effects introduced by different acquisition methods largely dominate the data variance and will inevitably mislead the joint analysis of the four sites if without performing harmonization. We implemented our method using PyTorch and public Spherical U-Net code [27,29]. We trained the models in an easy-to-hard manner by gradually adding losses from Eq. (1) to Eq. ( 6) using Adam optimizer with a fixed learning rate 5e-4. All surfaces were randomly split into training and testing sets with the proportion of 7:3. The weights of different loss terms are empirically set as 1.0, 1.0, 0.5, 0.5, 4.0, 5.0 for L recon , L Excitation , L Inhibition , L cycle , L latent , and L cc , respectively. All the experiments were run on a PC with an Nvidia RTX 3080-Ti GPU and an Intel Core i7-9700K CPU. We adopted the popular statistical model, Combat [7], as the baseline method for comparison. We used its public code with age and sex as biological covariates for harmonizing the four sites' cortical thickness in a vertex-wise manner and ROI-wise manner, referred to Vertexwise Combat and ROI-wise Combat, respectively. Of note, Combat harmonizes multiple sites into one intermediate site, while our method maps less reliable sites (with low-quality images) to the more reliable site (with high-quality images), i.e., S1, in this work.  "
Disentangling Site Effects with Cycle-Consistent Adversarial Autoencoder for Multi-site Cortical Data Harmonization,3.2,Results,"Validation on Removing Site Effects. To validate if site effects are successfully removed, we first show the population-level developmental trajectory of average cortical thickness in Fig. 3. To fairly compare with statistical models that use all data and deliver more reliable comparison with larger sample size, we draw the figures based on the whole dataset. As shown, the developmental trajectories from different sites are not consistent and comparable before harmonization and after harmonization using Combat, but are well harmonized to a common space and enable joint analysis using our method. We also compare the ROI-level thickness values before and after harmonization as performed in [7,28]. To be less biased by the data distribution of ages, we show the average thickness of 1-year-old infants for each ROI in Desikan-Killiany parcellation [3]. As shown in Fig. 4, the differences of ROI-wise thickness of different sites are significant before harmonization (all p < 0.05) and still remain after harmonization by Combat, but are not significant after harmonization by our method (all p > 0.1). Of note, although our method deals with vertex-wise data, compared to ROI-wise Combat, Fig. 3 and Fig. 4 still show better performance of our method on population-level and region-level site effects removal. These results indicate that Combat and other linear models indeed suffer from limited modeling ability and thus are not able to capture the complex non-linear mapping in heterogeneous multi-site data, while our method based on deep learning can effectively solve this problem with the proposed loss functions. Moreover, we also perform the common practice to visualize the latent space of autoencoders, which is based on t-SNE embedding of the extracted latent features z. As shown in Fig. 5, the latent features extracted by AE demonstrate clear clusters of different sites, indicating that the data variance in the original data is dominated by site effects. However, after disentanglement learning using DAE, the site effects are successfully disentangled and removed, and CDAE further improves the results thanks to more constraints on backward mapping. Finally, to quantitatively evaluate site effects removal, we also attempted to predict the site from harmonized cortical thickness features [7]. Following the settings in [7], we used a support vector machine (SVM) model with radial basis kernel and the hyper-parameters of the model are selected using grid search based on 10-fold validation on the training set. After fitting on the training set, the SVM classification accuracies on the testing set are 74.3%, 92.2%, 51.4%, 40.2% for ROI-wise Combat, Vertex-wise Combat, DAE, and CDAE, respectively. A lower accuracy of DAE and CDAE indicates that our method successfully removes site effects in the cortical thickness measurements, while Combat still preserves certain site-identifiable information.Validation on Preserving Individual Variability. While it is important to validate if a harmonization method removes site effects, it is equally important to show that the method preserves biological variability after harmonization. Following [28], we computed the Euclidean distances between any two surface maps in the same site, thus forming a distance matrix, denoted as H n×n i,j = x ix j 2 . We computed the Pearson correlation coefficient (PCC) of the distance matrices before and after harmonization to estimate if relative distances of different scans in the same site are preserved after harmonization. The average PCC values across sites are 0.853, 0.921, 0.842, 0.959 for ROI-wise Combat, Vertex-wise Combat, DAE, and CDAE, respectively, suggesting that our method preserves the most individual differences. From Fig. 2, we can also see that individual variability is well preserved while the site-related information is successfully removed with our method.Validation on Downstream Task. We further investigate how different harmonization methods affect a downstream task. Same as in [7], we used a support vector regression (SVR) model with radial basis function to predict the scan age using cortical thickness features. After hyper-parameters selection and training, the mean absolute error on the testing set is 81.1, 59.6, 50.5, 56.3, and 45.1 days for no harmonization, ROI-wise Combat, Vertex-wise Combat, DAE and CDAE, respectively. The coefficient of determination, or R 2 , are 0.823, 0.865, 0.892, 0.884, 0.936 for no harmonization, ROI-wise Combat, Vertex-wise Combat, DAE, and CDAE, respectively. Note that Vertex-wise Combat improves the prediction accuracy of ROI-wise Combat possibly due to more spatial details preservation. All these results indicate that our CDAE model substantially increases the accuracy and trustworthiness of joint analysis of multi-site data compared to no harmonization and Combat-based harmonization methods."
Disentangling Site Effects with Cycle-Consistent Adversarial Autoencoder for Multi-site Cortical Data Harmonization,4,Conclusion,"In this paper, to address the important yet unsolved problem of multi-site vertexwise cortical data harmonization, we proposed a novel, flexible deep learningbased method, named Cycle-consistent Disentangled Autoencoder (CDAE). Our CDAE takes advantage of the simplicity and transparency of autoencoder, the controllability and interpretability of disentanglement learning, and reinforced meaningfulness of cycle-consistency to successfully learn the complex non-linear mapping among heterogeneous multi-site data, which is inherently difficult and unsuitable for existing methods. Both visual and quantitative results on four datasets with 2,342 scans show the effectiveness of our method on both site effects removal and biological variability preservation. Our method will not only facilitate multi-site vertex-wise neuroimaging data analysis but also inspire novel directions in learning-based data harmonization."
Mitosis Detection from Partial Annotation by Dataset Generation via Frame-Order Flipping,1,Introduction,"Fluorescent microscopy is widely used to capture cell nuclei behavior. Mitosis detection is the task of detecting the moment of cell division from time-lapse images (the dotted circles in Fig. 1). Mitosis detection from fluorescent sequences is important in biological research, medical diagnosis, and drug development.Conventionally tracking-based methods [1,19,19,21] and tracking-free methods [3,[5][6][7] have been proposed for mitosis detection. Recently, deep-learningbased mitosis-detection methods have achieved outstanding performance [8][9][10][11][12]18]. However, training deep-learning methods require a certain amount of annotation for each imaging condition, such as types of cells and microscopy and the density of cells. Collecting a sufficient number of labeled data covering the variability of cell type and cell density is time-consuming and labor-intensive.Unlike cell detection and segmentation, which aims to recognize objects from a single image, mitosis detection aims to identify events from time series of images. Thus, it is necessary to observe differences between multiple frames to make mitosis events annotation. Comprehensively annotating mitosis events is time-consuming, and annotators may be missed mitosis events. Thus, we must carefully review the annotations to ensure that they are comprehensive. Partial annotation has been used as a way to reduce the annotation costs of cell and object detection [2,15,22]. Figure 1 shows an example of partially annotated frames. Some mitosis events are annotated (a red-dotted circle), and others are not (light-blue-dotted circles). The annotation costs are low because the annotator only needs to plot a few mitotic positions. In addition, this style of annotation allows for missing annotations. Therefore, it would be effective for mitosis detection.Unlike supervised annotation, partial annotation can not treat unannotated areas as regions not containing mitosis events since the regions may contain mitosis events (Fig. 1). The regions naturally affect the training in the partial annotation setting. To avoid the effect of unlabeled objects in unlabeled regions, Qu et al. [15] proposed to use a Gaussian masked mean squared loss, which calculates the loss around the annotated regions. The loss function works in tasks in which foreground and background features have clearly different appearances, such as in cell detection. However, it does not work on mitosis detection since the appearance of several non-mitotic cells appears similar to mitosis cells; it produces many false positives.In this paper, we propose a cell-mitosis detection method for fluorescent time-lapse images by generating a fully labeled dataset from partially annotated sequences. We achieve mitosis detection training in a mitosis detection model with the generated dataset. To generate the fully labeled dataset, we should consider two problems: (1) no label indicating regions not containing mitosis cells and (2) few mitosis annotations. We can easily generate the regions not containing mitotic cells by using one image twice. However, such regions do not contribute to identifying mitotic cells and non-mitotic cells since the data do not show natural cell motions. For the training to be effective, the regions not containing mitotic cells should show the natural movements of cells. To generate such regions, we propose frame-order flipping which simply flips the frame order of a consecutive frame pair. As shown in the white rectangles in Fig. 2, we can convert a mitosis event to a cell fusion by flipping operation. Hence, the flipped pair is the region not containing mitosis cells. Even though we flipped the frame order, the non-mitotic cells still have natural time-series motion, as shown in the yellow rectangles in Fig. 2.In addition, we can make the most of a few partial annotations by using copy-and-paste-based techniques. Unlike regular copy-and-paste augmentation [4] for supervised augmentation of instance segmentations which have object mask annotations, we only have point-level annotations. Thus, we propose to use alpha-blending pasting techniques which naturally blend two images.Experiments conducted on four types of fluorescent sequences demonstrate that the proposed method outperforms other methods which use partial labels. Related Work. Some methods used partially labeled data to train model [2,15,22]. Qu [15] proposed a Gaussian masked mean squared loss, which calculates the loss around the annotated areas. To more accurately identify negative and positive samples, positive unlabeled learning has been used for object detection [2,22]. These methods have used positive unlabeled learning on candidates detected by using partial annotation to identify whether the candidates are labeled objects or backgrounds. However, since the candidates detected by partial annotation include many false positives, the positive unlabeled learning does not work on mitosis detection. the appearance of the mitosis event and backgrounds in the mitosis detection task, it is difficult to estimate positive prior. These methods could not work on mitosis detection. The positive unlabeled learning requires a positive prior."
Mitosis Detection from Partial Annotation by Dataset Generation via Frame-Order Flipping,2,Method: Mitosis Detection with Partial Labels,"Our method aims to detect coordinates and timing (t, x, y) of mitosis events from fluorescent sequences. For training, we use time-lapse images I = {I t } T t=1 and partial labels (a set of annotated mitosis cells). Here, I t denotes an image at frame t, and T is the total number of frames. Our method generates a fully labeled datasett=1 from time-lapse images I and partial labels and then trains a mitosis detection model f θ with the generated dataset. Here, I t is a generated image, and P t is a set of mitotic coordinates contained in (I t-1 , I t ). Since our method trains the network with partial labels, it can eliminate the costs of checking for missed annotations."
Mitosis Detection from Partial Annotation by Dataset Generation via Frame-Order Flipping,2.1,Labeled Dataset Generation,"Figure 3 shows an overview of our dataset generation. We randomly pick a pair of consecutive frames (I t-1 , I t ) from time-lapse images I. Since the pair may contain unannotated mitosis events, we forcibly convert the pair into a negative pair (i.e., a pair which does not contain mitosis events) by using frame-order flipping. Next, we paste mitosis events to a generated pair using alpha-blending pasting and obtain a generated pair (I t-1 , I t ). Since we know the pasted location, we can obtain the mitosis locations P t of the generated pair. Negative Pair Generation with Frame-Order Flipping: In this step, we generate a pair not containing mitotic cells by using a simple augmentation-based frame-order flipping. Figure 3 shows an example of the pair images (I t-1 , I t ). The pair may contain mitosis events. If we assume that the pair does not contain mitotic cells, it affects the training of the mitosis detection model f θ . To prevent the pair from containing mitosis events, we flip the frame order and treat the flipped pair (I t , I t-1 ) as a pair of negative.Since mitosis is the event that a cell divides into two daughter cells, the mitosis event is transformed into an event in which two cells fuse into one by flipping the order (Fig. 2). The flipped event can treat as a non-mitotic event. Note that the motivation behind using frame flipping is to be able to utilize pixels showing the motions of non-mitotic cells negatives by transforming mitosis into other events. Even if the order is flipped, the movements of the non-mitotic cell are still a non-mitotic cell feature, and we consider that these cells are effective for the training of the negative label.Mitosis Label Utilization with Alpha-Blending Pasting: Next, we paste mitosis events to the flipped pair by using copy-and-paste techniques in order to utilize the positive labels effectively. Copy and paste augmentation has been used for supervised augmentation of instance segmentation [4]. Unlike instance segmentation with object masks, we only have locations (t, x, y). A simple solution is to crop images around the mitosis position and copy and paste them to the target image, like in CutMix [23]. However, the cropped image naturally contains surrounding objects, and the generated image appears unnatural. Unnatural images cause the detection network to make biased predictions and reduce generalization performance. To avoid this problem, we propose alpha-blending pasting with a Gaussian blending mask. We blend two images by leaving the pixel value in the center and blurring the vicinity of the edge of the image. First, we crop the image around the positive annotations and obtain a set of cropped pair) and P t = {}. Here, N is the total number of partial annotations, while C i t-1 and C i t are images before and after the mitosis of the i-th annotation (Fig. 3). Define I t (l j ), I t-1 (l j ) as a cropped pair image at the j-th random spatial location l j . We crop each image centered at l j to a size that is the same as that of C i t . We update the randomly selected patch I t (l j ), I t-1 (l j ) by blending a randomly selected cropped pair (C i t-1 , C i t ) with the following formula: I t (l j ) = (1α) I t (l j ) + α C i t , where α is a Gaussian blending mask (Fig. 4). We generate the blending mask by blurring a binary mask around the annotation with a Gaussian filter. We use a random sigma value for the Gaussian filter. Then, we add the paste location l j to the set P t . We repeat this process random k times."
Mitosis Detection from Partial Annotation by Dataset Generation via Frame-Order Flipping,2.2,Mitosis Detection with Generated Dataset,"We modified a heatmap-based cell detection method [13] to work as a mitosis detection method. Figure 5 is an illustration of our mitosis detection model. Given two consecutive frames (I t-1 , I t ), the network output heatmap Ĥt . We treat the channel axis as the time axis for the input. The first channel is I t-1 , and the second is I t .First, we generate individual heatmaps H j t for each pasted coordinate l j = (l j x , l j y ). H j t is defined as H j t (p x , p y ) = exp -, where p x and p y are the coordinates of H j t and σ is a hyper parameter that controls the spread of the peak. The ground truth of the heatmap at t is generated by taking the maximum through the individual heatmaps, H t = max j (H j t ) (H t in Fig. 5). The network is trained with the mean square error loss between the ground truth H t and the output of the network Ĥt . We can find the mitosis position by finding a local maximum of the heatmap. "
Mitosis Detection from Partial Annotation by Dataset Generation via Frame-Order Flipping,3,Experiments,"Dataset: We evaluated our method on four datasets. The first set is HeLa [20], in which live cell images of HeLa cells expressing H2B-GFP were captured with 1100 × 700 resolution [20] 1 . Each sequence contains 92 fluorescent images with 141 mitosis events on average. The second set is ES, in which live cell images of mouse embryonic stem cells expressing H2B-mCherry were captured with 1024 × 1024 resolution. Each sequence contains 41 fluorescent images with 33 mitosis events on average. The third set is ES-D in which mouse embryonic stem cells expressing H2B-mCherry were induced to differentiate and used to capture live cell images. Each sequence contains 61 fluorescent images with 18 on average events on average. The fourth set is Fib, in which live cell images of mouse fibroblast cells expressing H2B-mCherry were captured with 1024 × 1024 resolution. Each sequence contains 42 fluorescent images with 11 mitosis events on average. Each dataset consists of four sequences of images. We performed four-fold cross-validation in which two sequences were used as training data, one as validation data, and one as test data. As shown in Fig. 6, the appearance and density are different depending on the dataset.Implementation Details: We implemented our method within the Pytorch framework [14] and used a UNet-based architecture [16] for the mitosis-detection network. The model was trained with the Adam optimizer with a learning rate of 1e-3. σ, which controls the spread of the heatmap, was 6. The cropping size of the positive annotations was 40 pixels. We randomly change the number of pasting operations k between 1 and 10. We used random flipping, random cropping, and brightness change for the augmentation. Evaluation Metrics: We evaluated our method using the F1 score [18], which is widely used in mitosis detection. Given ground-truth coordinates and detected coordinates, we performed one-by-one matching. If the distance of the matched pair was within spatially 15 pixels and temporally 6, we associated the closest coordinate pairs. We treated the matched pair as true positives (TP), unassociated coordinates as false positives (FP), and unassociated ground-truth coordinates as false negatives (FN).Comparisons: We conducted four comparisons that involved training the model with partially labeled data. For the first method, we trained the model by treating unlabeled pixels as non-mitosis ones (Baseline [13]). The second method used the Gaussian masked loss (GM [15]). The masked loss was calculated on the masked pixels around the positive-label pixels. Thus, the method ignored unlabeled pixels. The third method used positive unlabeled learning to identify mitosis from candidates obtained by the detection model trained with the masked loss (PU [22]). The fourth method generated pseudo-labels from the results of positive unlabeled learning and retrained the detection model with the pseudo-label (PU-I [2]).In Table 1, we compared our method with previous methods in one and fiveshot settings. We used N samples per sequence in the N-shot settings. For a robust comparison, we sampled one or five mitosis annotations under five seed conditions and took the average. Overall, our method outperformed all compared methods in F1 metric. GM [15], PU [22], and PU-I [2] are designed for detecting objects against simple backgrounds. Therefore, these methods are not suited to a mitosis detection task and are inferior to the baseline.The baseline [13] treats unlabeled pixels as non-mitosis cell pixels. In the partially labeled setting, unlabeled pixels contain unannotated mitosis events, and unannotated mitosis affects performance. Unlike cell detection, mitosis detection requires identifying mitosis events from various non-mitotic cell motions, including motions that appear mitotic appearances. Although GM [15] can ignore unlabeled mitosis pixels with the masked loss, it is difficult to identify such nonmitosis motions. Therefore, GM estimates produce many false positives. PU [22] uses positive unlabeled learning to eliminate false positives from candidates obtained from the detection results with partial labels. However, positive unla- beled learning requires a positive prior in the candidates and a certain amount of randomly sampled positive samples. Since the candidates contain many false positives, the positive prior is difficult to estimate. In addition, there is no guarantee that positive unlabeled learning can work correctly with the selected Nshot annotations. Moreover, since positive unlabeled learning does not work in the mitosis detection task, PU-I [2] can not select accurate pseudo labels. Unlike these methods, our method can estimate mitosis events accurately. Since our method generates a fully labeled dataset from a partial label, it effectively uses a few partial annotations."
Mitosis Detection from Partial Annotation by Dataset Generation via Frame-Order Flipping,,Effectiveness of Each Module:,"We performed an ablation study on the HeLa dataset to investigate the effectiveness of the proposed module. We used random augmentation (i.e., random elastic transformation [17], brightness change, and gaussian noise) instead of using frame-order flipping (FOF). We generated I aug t by augmenting I t and input the pair (I t , I aug t ) to the network. In the w/o ABP setting, we directly pasted cropped images on the target image as in CutMix [23]. Table 2 demonstrates that the proposed modules improve mitosis detection performance. Figure 8 shows examples of the estimation results for each condition. Without the FOF setting, the detection model estimates a high value for all moving cells, leading to over-detection. Without the ABP setting, the detection model overfits the directly pasted image. The directly pasted image tends to include unnatural boundaries on the edge, leading to missed detections in real images."
Mitosis Detection from Partial Annotation by Dataset Generation via Frame-Order Flipping,,Robustness Against Missing Annotations:,"We confirmed the robustness of the proposed method against missing annotations on the ES dataset. We changed the missing annotation rate from 0% to 30%. A comparison with the supervised method in terms of F1-score is shown in Fig. 7. The performance of the supervised method deteriorates as the percentage of missing labels increases, whereas the performance of the proposed method remains steady. Since our method flips the frame order, we can avoid the effects of missing annotations.Appearance of Generated Dataset: Fig. 9 shows an example of the generated image pair. The cropped mitosis image pairs were pasted on the red-dotted circle. It can be seen that the borders of the original image and the pasted image have been synthesized very naturally."
Mitosis Detection from Partial Annotation by Dataset Generation via Frame-Order Flipping,4,Conclusion,"We proposed a mitosis detection method using partially labeled sequences with frame-order flipping and alpha-blending pasting. Our frame-order flipping transforms unlabeled data into non-mitosis labeled data through a simple flipping operation. Moreover, we generate various positive labels with a few positive labels by using alpha-blending pasting. Unlike directly using copy-and-paste, our method generates a natural image. Experiments demonstrated that our method outperforms other methods that use partially annotated sequences on four fluorescent microscopy images."
Mitosis Detection from Partial Annotation by Dataset Generation via Frame-Order Flipping,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3 47.
Self-supervised Dense Representation Learning for Live-Cell Microscopy with Time Arrow Prediction,1,Introduction,"Live-cell microscopy is a fundamental tool to study the spatio-temporal dynamics of biological systems [4,24,26]. The resulting datasets can consist of terabytes of raw videos that require automatic methods for downstream tasks such as classification, segmentation, and tracking of objects (e.g. cells or nuclei). Current state-of-the-art methods rely on supervised learning using deep neural networks that are trained on large amounts of ground truth annotations [6,25,31]. The manual creation of these annotations, however, is laborious and often constitutes a practical bottleneck in the analysis of microscopy experiments [6]. Recently, self-supervised representation learning (SSL) has emerged as a promising approach to alleviate this problem [1,3]. In SSL one first defines a pretext task which can be formulated solely based on unlabeled images (e.g. inpainting [8], or rotation prediction [5]) and tasks a neural network to solve it, with the aim of generating latent representations that capture high-level image semantics. In a second step, these representations can then be either finetuned or used directly (e.g. via linear probing) for a downstream task (e.g. image classification) with available ground truth [7,10,18]. Importantly, a proper choice of the pretext task is crucial for the resulting representations to be beneficial for a specific downstream task.In this paper we investigate whether time arrow prediction, i.e. the prediction of the correct order of temporally shuffled image frames extracted from live-cell microscopy videos, can serve as a suitable pretext task to generate meaningful representations of microscopy images. We are motivated by the observation that for most biological systems the temporal dynamics of local image features are closely related to their semantic content: whereas static background regions are time-symmetric, processes such as cell divisions or cell death are inherently timeasymmetric (cf. Fig. 1a). Importantly, we are interested in dense representations of individual images as they are useful for both image-level (e.g. classification) or pixel-level (e.g. segmentation) downstream tasks. To that end, we propose a time arrow prediction pre-training scheme, which we call Tap, that uses a feature extractor operating on single images followed by a time arrow prediction head operating on the fused representations of consecutive time points. The use of time arrow prediction as a pretext task for natural (e.g. youtube) videos was introduced by Pickup et al . [19] and has since then seen numerous applications for image-level tasks, such as action recognition, video retrieval, and motion classification [2,11,14,15,22,30]. However, to the best of our knowledge, SSL via time arrow prediction has not yet been studied in the context of live-cell microscopy. Concretely our contributions are: i) We introduce the time arrow prediction pretext task to the domain of live-cell microscopy and propose the Tap pre-training scheme, which learns dense representations (in contrast to only image-level representations) from raw, unlabeled live-cell microscopy videos, ii) we propose a custom (permutation-equivariant) time arrow prediction head that enables robust training, iii) we show via attribution maps that the representations learned by Tap capture biologically relevant processes such as cell divisions, and finally iv) we demonstrate that Tap representations are beneficial for common image-level and pixel-level downstream tasks in live-cell microscopy, especially in the low training data regime."
Self-supervised Dense Representation Learning for Live-Cell Microscopy with Time Arrow Prediction,2,Method,"Our proposed Tap pre-training takes as input a set {I} of live-cell microscopy image sequences I ∈ R T ×H×W with the goal to produce a feature extractor f that generates c-dimensional dense representations z = f (x) ∈ R c×H×W from single images x ∈ R H×W (cf. Fig. 1b for an overview of Tap). To that end, we randomly sample from each sequence I pairs of smaller patches x 1 , x 2 ∈ R h×w from the same spatial location but consecutive time points x 1 ⊂ I t , x 2 ⊂ I t+1 . We next flip the order of each pair with equal probability p = 0.5, assign it the corresponding label y (forward or backward ) and compute dense representations z 1 = f (x 1 ) and z 2 = f (x 2 ) with z 1 , z 2 ∈ R c×h×w via a fully convolutional feature extractor f . The stacked representations z = [z 1 , z 2 ] ∈ R 2×c×h×w are fed to a time arrow prediction head h, which produces the classification logitsBoth f and h are trained jointly to minimize the losswhere L BCE denotes the standard softmax + binary cross-entropy loss between the ground truth label y and the logits ŷ = h(z), and L Decorr is a loss term that promotes z to be decorrelated across feature channels [12,33] via maximizing the diagonal of the softmax-normalized correlation matrix A ij :Here z ∈ R c×2hw denotes the stacked features z flattened across the non-channel dimensions, and τ is a temperature parameter. Throughout the experiments we use λ = 0.01 and τ = 0.2. Note that instead of creating image pairs from consecutive video frames we can as well choose a custom time step Δt ∈ N and sample x 1 ⊂ I t and x 2 ⊂ I t+Δt , which we empirically found to work better for datasets with high frame rate."
Self-supervised Dense Representation Learning for Live-Cell Microscopy with Time Arrow Prediction,,Permutation-equivariant Time Arrow Prediction Head:,"The time arrow prediction task has an inherent symmetry:In other words, h should be equivariant wrt. to permutations of the input. In contrast to common models (e.g. ResNet [9]) that lack this symmetry, we here directly incorporate this inductive bias via a permutation-equivariant head h that is a generalization of the set permutation-equivariant layer proposed in [32] to dense inputs. Specifically, we choose h = h 1 • . . . • h L as a chain of permutation-equivariant layers h l :with weight matrices L, G ∈ R c×c and a non-linear activation function σ. Note that L operates independently on each temporal axis and thus is trivially permutation equivariant, while G operates on the temporal sum and thus is permutation invariant. The last layer h L includes an additional global average pooling along the spatial dimensions to yield the final logits ŷ ∈ R 2 .Augmentations: To avoid overfitting on artificial image cues that could be discriminative of the temporal order (such as a globally consistent cell drift, or decay of image intensity due to photo-bleaching) we apply the following augmentations (with probability 0.5) to each image patch pair x 1 , x 2 : flips, arbitrary rotations and elastic transformations (jointly for x 1 and x 2 ), translations for x 1 and x 2 (independently), spatial scaling, additive Gaussian noise, and intensity shifting and scaling (jointly+independently)."
Self-supervised Dense Representation Learning for Live-Cell Microscopy with Time Arrow Prediction,3.1,Datasets,"To demonstrate the utility of Tap for a diverse set of specimen and microscopy modalities we use the following four different datasets:HeLa. Human cervical cancer cells expressing histone 2B-GFP imaged by fluorescence microscopy every 30 min [29] . The dataset consists of four videos with overall 368 frames of size 1100 × 700. We use Δt = 1 for Tap training.Mdck. Madin-Darby canine kidney epithelial cells expressing histone 2B-GFP (cf. Fig. 3b), imaged by fluorescence microscopy every 4 min [27,28]. The dataset consists of a single video with 1200 frames of size 1600×1200. We use Δt ∈ {4, 8}.Flywing. Drosphila melanogaster pupal wing expressing Ecad::GFP (cf. Fig. 3a), imaged by spinning disk confocal microscopy every 5 min [4,20]. The dataset consists of three videos with overall 410 frames of size 3900 × 1900.We use Δt = 1.Yeast. S. cerevisiae cells (cf. Fig. 3c) imaged by phase-contrast microscopy every 3 min [16,17]. The dataset consists of five videos with overall 600 frames of size 1024 × 1024. We use Δt ∈ {1, 2, 3}.For each dataset we heuristically choose Δt to roughly correspond to the time scale of observable biological processes (i.e. larger Δt for higher frame rates). "
Self-supervised Dense Representation Learning for Live-Cell Microscopy with Time Arrow Prediction,3.2,Implementation Details:,"For the feature extractor f we use a 2D U-Net [21] with depth 3 and c = 32 output features, batch normalization and leaky ReLU activation (approx. 2M params). The time arrow prediction head h consists of two permutationequivariant layers with batch normalization and leaky ReLU activation, followed by global average pooling and a final permutation-equivariant layer (approx. 5k params). We train all Tap models for 200 epochs and 10 5 samples per epoch, using the Adam optimizer [13] with a learning rate of 4 × 10 -4 with cyclic schedule, and batch size 256. Total training time for a single Tap model is roughly 8h on a single GPU. Tap is implemented in PyTorch."
Self-supervised Dense Representation Learning for Live-Cell Microscopy with Time Arrow Prediction,3.3,Time Arrow Prediction Pretraining,"We first study how well the time arrow prediction pretext task can be solved depending on different image structures and used data augmentations. To that end, we train Tap networks with an increasing number of augmentations on HeLa and compute the Tap classification accuracy for consecutive image patches x 1 , x 2 that contain either background, interphase (non-dividing) cells, or mitotic (dividing) cells. As shown in Fig. 2a, the accuracy on background regions is approx. 50% irrespective of the used augmentations, suggesting the absence of predictive cues in the background for this dataset. In contrast, on regions with cell divisions the accuracy reaches almost 100%, confirming that Tap is able to pick up on strong time-asymmetric image features. Interestingly, the accuracy for regions with non-dividing cells ranges from 68% to 80%, indicating the presence of weak visual cues such as global drift or cell growth. When using more data augmentations the accuracy decreases by roughly 12% points, suggesting that data augmentation is key to avoid overfitting on confounding cues.Next we investigate which regions in full-sized videos are most discriminative for Tap. To that end, we apply a trained Tap network on consecutive fullsized frames x 1 , x 2 and compute the dense attribution map of the classification logits y wrt. to the Tap representations z via Grad-CAM [23]. In Fig. 3 we show example attribution maps on top of single raw frames for three different datasets. Strikingly, the attribution maps highlight only a few distributed, yet highly localized image regions. When inspecting the top six most discriminative regions and their temporal context for a single image frame, we find that virtually all of them contain cell divisions (cf. Fig. 3). Moreover, when examining the attribution maps for full videos, we find that indeed most highlighted regions correspond to mitotic cells, underlining the strong potential of Tap to reveal time-asymmetric biological phenomena from raw microscopy videos alone (cf. Supplementary Video 1). Finally, we emphasize the positive effect of the permutation-equivariant time arrow prediction head on the training process. When we originally used a regular CNN-based head, we consistently observed that the Tap loss stagnated during the initial training epochs and decreased only slowly thereafter (cf. Fig. 2b). Using the permutation-equivariant head alleviated this problem and enabled a consistent loss decrease already from the beginning of training."
Self-supervised Dense Representation Learning for Live-Cell Microscopy with Time Arrow Prediction,3.4,Downstream Tasks,"We next investigate whether the learned Tap representations are useful for common supervised downstream tasks, where we especially focus on their utility in the low training data regime. First we test the learned representations on two image-level classification tasks, and later on two dense segmentation tasks. Mitosis Classification on Flywing: Since Tap attribution maps strongly highlight cell divisions, we consider predicting mitotic events an appropriate first downstream task to evaluate Tap. To that end, we generate a dataset of 97k crops of size 2 × 96 × 96 from Flywing and label them as mitotic/nonmitotic (16k/81k) based on available tracking data [20]. We train Tap networks on Flywing and use a small ResNet architecture (≈ 5M params) that is trained from scratch as a supervised baseline. In Fig. 4a   Mitosis Segmentation on Flywing: We now apply Tap on a pixel-level downstream task to fully exploit that the learned Tap representations are dense. We use the same dataset as for Flywing mitosis classification, but now densely label post-mitotic cells. We predict a pixel-wise probability map, threshold it at 0.5 and extract connected components as objects. To evaluate performance, we match a predicted/ground truth object if their intersection over union (IoU) is greater than 0.5, and report the F1 score after matching. The baseline model is a U-Net trained from scratch. Training a U-Net on fixed Tap representations always outperforms the baseline, and when only using 3% of the training data it reaches similar performance as the baseline trained on all available labels (0.67 vs. 0.68, Fig. 5a). Interestingly, fine-tuning Tap only slightly outperforms the supervised baseline for this task even for moderate amounts of training data, suggesting that fixed Tap representations generalize better for limited-size datasets.Emerging Bud Detection on Yeast: Finally, we test Tap on the challenging task of segmenting emerging buds in phase contrast images of yeast colonies. We train Tap networks on Yeast and generate a dataset of 1205 crops of size 5 × 192 × 192 where we densely label yeast buds in the central frame (defined as buds that appeared less than 13 frames ago) based on available segmentation data [17]. We evaluate all methods on held out test videos by interpreting the resulting 2D+time segmentations as 3D objects and computing the F1 score using an IoU threshold of 0.25. The baseline model is again a U-Net trained from scratch. Surprisingly, training with fixed Tap representations performs slightly worse than the baseline for this dataset (Fig. 5b), possibly due to cell density differences between Tap training and test videos. However, fine-tuning Tap features outperforms the baseline by a large margin (e.g. 0.64 vs. 0.39 for 120 frames) across the full training data regime, yielding already with 15% labels the same F1 score as the baseline using all labels."
Self-supervised Dense Representation Learning for Live-Cell Microscopy with Time Arrow Prediction,4,Discussion,"We have presented Tap, a self-supervised pretraining scheme that learns biologically meaningful representations from live-cell microscopy videos. We show that Tap uncovers sparse time-asymmetric biological processes and events in raw unlabeled recordings without any human supervision. Furthermore, we demonstrate on a variety of datasets that the learned features can substantially reduce the required amount of annotations for downstream tasks. Although in this work we focus on 2D+t image sequences, the principle of Tap should generalize to 3D+t datasets, for which dense ground truth creation is often prohibitively expensive and therefore the benefits of modern deep learning are not fully tapped into. We leave this to future work, together with the application of Tap to cell tracking algorithms, in which accurate mitosis detection is a crucial component."
Self-supervised Dense Representation Learning for Live-Cell Microscopy with Time Arrow Prediction,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3_52.
SurfFlow: A Flow-Based Approach for Rapid and Accurate Cortical Surface Reconstruction from Infant Brain MRI,1,Introduction,"The human brain undergoes dramatic changes in size, shape, and tissue architecture during the first postnatal year, driven by cellular processes [1] that lead to cortical folding and the formation of convoluted gyri and sulci on the cerebral surface. Understanding cortical development in these early postnatal years is crucial for comprehending later-stage functional development. Cortical surface reconstruction (CSR) is a step necessary for functional and anatomical visualization, brain morphology, and quantification of cortical growth. However, reconstructing the cortical surface from infant MRI poses significant challenges due to the low signal-to-noise ratio, pronounced motion artifacts, complex folding patterns, and rapid brain expansion [2].CSR methods developed so far include FreeSurfer [5], DeepCSR [4], Voxel2Mesh [13], PialNN [8], and CorticalFlow [7]. However, these methods are optimized primarily for adult brain MRI, typically with clear cortical gyral and sulcal patterns and good tissue contrasts. Their performance deteriorates significantly when applied to infant MRI due to challenges like narrow sulcal spaces, low tissue contrast, and partial volume effects. To address these issues, specific methods like the dHCP pipeline [9] and Infant FreeSurfer [14] have been developed. However, the dHCP pipeline is only effective for neonatal data and Infant FreeSurfer often fails to produce accurate cortical surfaces, especially for the first postnatal year when the brain undergoes dynamic changes in tissue contrasts and morphology. Additionally, the computational inefficiency of Infant FreeSurfer, which takes approximately 10 h to reconstruct cortical surfaces for a single subject, limits its applicability to large-scale studies.In this paper, we propose SurfFlow, a geometric deep learning model designed to reconstruct accurate cortical surfaces from infant brain MRI. Our model comprises three cascaded deformation blocks, each responsible for predicting a flow field and constructing a diffeomorphic mapping for each vertex through solving a flow ordinary differential equation (ODE). The flow fields deform template meshes progressively to finally produce accurate genus-zero cortical surfaces. Our work offers a threefold contribution. First, we propose an efficient dualmodal flow-based CSR method, enabling the creation of high-resolution and high-quality mesh representations for complex cortical surfaces. Second, we propose a novel loss function that effectively regularizes the lengths of mesh edges, leading to substantial improvements in mesh quality. Third, our method represents the first attempt in tackling the challenging task of directly reconstructing cortical surfaces from infant brain MRI. Our method outperforms state-of-theart methods by a significant margin, judging based on multiple surface evaluation metrics."
SurfFlow: A Flow-Based Approach for Rapid and Accurate Cortical Surface Reconstruction from Infant Brain MRI,2.1,Overview,"As depicted in Fig. 1a, SurfFlow consists of three deformation blocks, each consisting of a 3D Unet [3] and a diffeomorphic mesh deformation (DMD) module. The final mesh, either the pial surface or the white surface, is achieved by composing the three diffeomorphic deformations generated by the DMD modules. Each Unet takes a T1w-T2w image pair as input and, except the first network, also receives flow fields predicted by previous deformation blocks to predict a new flow field. Figure 1b provides a detailed illustration of the design of the two different 3D Unets used. Each DMD module employs the flow field predicted within the same block and computes a diffeomorphic mapping φ θ (t; x) for each vertex x in the mesh generated by the previous deformation block through integration, assuming a stationary flow field. Formally, the dynamics of mesh deformation are formulated as a stationary flow ODE specified by a 3D Unet:where θ denotes the parameters of the 3D Unet, x 0 is the initial mesh before deformation in each stage. SurfFlow is trained stage-wise: a deformation block is trained with all previous deformation blocks frozen. Following CorticalFlow++ [10], the Runge-Kutta method is used for numerical integration."
SurfFlow: A Flow-Based Approach for Rapid and Accurate Cortical Surface Reconstruction from Infant Brain MRI,2.2,Dual-Modal Input,"Infant MRI exhibits three distinct phases during the first year of life. In the infantile phase (≤5 months), gray matter (GM) shows higher signal intensity than white matter (WM) in T1w images. The isointense phase (5-8 months) corresponds to an increase in intensity of WM owing to myelination associated with brain maturation. This significantly lowers the contrast between GM and WM in T1w images and similarly T2w images. In the early adult-like phase (≥ 8 months), the GM intensity is lower than WM in T1w images, similar to the tissue contrast in adult MRI. We propose to use both T1w and T2w images for complementary information needed for accurate surface reconstruction."
SurfFlow: A Flow-Based Approach for Rapid and Accurate Cortical Surface Reconstruction from Infant Brain MRI,2.3,Loss Function,"The loss function used for training SurfFlow is a weighted sum of Chamfer distance (CD; L cd ) and a piecewise edge length loss (PELL; L e ), balanced by parameter λ:Chamfer Distance Loss. The Chamfer distance is commonly used as the loss function for surface reconstruction. It measures the distance from a vertex in one mesh P to the closest vertex in another mesh Q bidirectionally:where p and q are vertices in P and Q, respectively.Piecewise Edge Length Loss. We observed that vertices tend to be clustered in surfaces generated by CorticalFlow++ (Fig. 3). One possible cause is that the edge length loss used in CorticalFlow++ pushes edges to zero length. To address this problem, we propose a piecewise edge length loss that encourages edge lengths to lie within a suitable range. The proposed loss is formulated aswhere P denotes the predicted mesh, p is a vertex in P , N (p) consists of the neighbors of p, and and γ are two tunable hyper-parameters that control the range position and width, respectively."
SurfFlow: A Flow-Based Approach for Rapid and Accurate Cortical Surface Reconstruction from Infant Brain MRI,2.4,Deformation Computation in DMD Modules,"Following CorticalFlow++, we use the fourth-order Runge-Kutta method to solve the stationary flow ODE over time interval [0, 1] for both accuracy and stability. The solution x 1 is obtained iteratively withwhere2 ), and k 4 = V(x tn + k 3 h). V(x) represents the trilinear interpolation of the flow field at position x. h is the step size and is set to 1/30. t n is the time at n-th step, and t n+1 = t n + h."
SurfFlow: A Flow-Based Approach for Rapid and Accurate Cortical Surface Reconstruction from Infant Brain MRI,2.5,Implementation Details,Our network was trained stage-wise. We froze the parameters of one deformation block once trained and then start the training of the next block. Each deformation block was trained for 27k iterations. Adam optimizer was used with an initial learning rate of 0.0001. The parameter λ of the loss function was set to 3.0 to balance the two loss terms. We set for PELL based on the average edge length determined from the training set. We set the γ to 4.0 and to 5 × 10 -5 . Instance normalization (IN) [11] layers were added between convolutional and activation layers for faster convergence and improved performance.
SurfFlow: A Flow-Based Approach for Rapid and Accurate Cortical Surface Reconstruction from Infant Brain MRI,3.1,Data,"The dataset includes aligned T1w and T2w image pairs from 121 subjects, 2 weeks to 12 months of age, from the Baby Connectome Project (BCP) [6]. Among them, 90 cases were used for training, 12 for validation, and 19 for testing. Ground truth cortical surfaces were generated with iBEAT v2.0 [12].To obtain a smooth starting template, an average convex hull computed from the training dataset was re-meshed and triangularized. The Catmull-Clark subdivision algorithm was then applied to generate enough faces and vertices. Decimation was used to control the number of vertices. These steps were carried out in Blender1 ."
SurfFlow: A Flow-Based Approach for Rapid and Accurate Cortical Surface Reconstruction from Infant Brain MRI,3.2,Evaluation Metrics,"For performance evaluation and comparison between different methods, we utilized the following metrics: Chamfer distance (CD), average symmetric surface distance (ASSD), 90% Hausdorff distance (HD), and normal consistency (NC). Their definitions are as follows:where P and Q are respectively the predicted and ground truth (GT) meshes, n p and n q are the normals at p and q, n pq is the normal of the vertex in Q that is closest to p, and n qp is the normal of the vertex in P that is closest to q. "
SurfFlow: A Flow-Based Approach for Rapid and Accurate Cortical Surface Reconstruction from Infant Brain MRI,3.3,Results,"We compared SurfFlow with CorticalFlow++ and DeepCSR. To ensure a fair comparison, we modified both CorticalFlow++ and DeepCSR to use dual-modal inputs. Specifically, for DeepCSR, we utilized the finest possible configuration, generating a 512 3 3D grid to predict the signed distance field for surface reconstruction using the marching cube algorithm. As depicted in Table 1, our evaluation demonstrates that SurfFlow outperforms the other two methods in all metrics. SurfFlow stands out with an average Chamfer distance of less than 0.5 mm, demonstrating significantly smaller errors compared with CorticalFlow++ and DeepCSR, which result in 22% to 1100% larger errors. Similar improvements were observed for evaluations with the ASSD, HD, and NC metrics. Furthermore, during our evaluation, we noticed that DeepCSR yielded higher errors for pial surface reconstruction due to numerous mesh topological artifacts. The utilization of implicit representation in DeepCSR does not guarantee a genus zero manifold. Visual comparisons (Figs. 2 and3) further confirm that SurfFlow excels in reconstructing cortical gyri and sulci compared with CorticalFlow++ and DeepCSR. Moreover, SurfFlow demonstrates superior robustness with more consist results, as indicated lower standard deviations.SurfFlow utilizes PELL to ensure that mesh edge lengths are within the desired range. We observed that this optimization leads to smoother, more uniform, and accurate meshes. In contrast, CorticalFlow++ shows limited accuracy in certain mesh areas, primarily due to its inability to ensure mesh uniformity. This is evident in the zoomed-in areas depicted in Fig. 4, where the deficiency in mesh faces hinders accurate predictions. The superiority of SurfFlow in terms of mesh smoothness is further supported by the NC results.  "
SurfFlow: A Flow-Based Approach for Rapid and Accurate Cortical Surface Reconstruction from Infant Brain MRI,3.4,Ablation Study,"The results of an ablation study (Table 2) confirm that surface prediction performance improves (i) when both modalities are concurrently used; (ii) when  instance normalization is used, and (iii) when PELL is used as opposed to the edge length loss in CorticalFlow++."
SurfFlow: A Flow-Based Approach for Rapid and Accurate Cortical Surface Reconstruction from Infant Brain MRI,4,Conclusion,We presented SurfFlow-a flow-based deep-learning network to accurately reconstruct cortical surfaces. SurfFlow predicts a flow field to deform a template surface toward a target surface. It produces smooth and uniform surface meshes with sub-millimeter accuracy and outperforms CorticalFlow++ and DeepCSR in terms of surface accuracy and regularity.
SurfFlow: A Flow-Based Approach for Rapid and Accurate Cortical Surface Reconstruction from Infant Brain MRI,,Table 2 .,HD ↓ Pial L 1.86 (±0.20) 1.79 (±0.17) 2.15 (±0.24) 1.36 (±0.16) 1.69 (±0.60) 1.89 (±0.28) 0.98 (±0.08) R 2.04 (±0.21) 2.09 (±0.19) 1.55 (±0.12) 1.42 (±0.18) 1.41 (±0.31) 1.51 (±0.33) 0.80 (±0.10) White L 1.88 (±0.15) 1.92 (±0.22) 1.96 (±0.11) 1.57 (±0.12) 1.50 (±0.34) 1.62 (±0.26) 0.86 (±0.10)
Self-pruning Graph Neural Network for Predicting Inflammatory Disease Activity in Multiple Sclerosis from Brain MR Images,1,Introduction,"Multiple Sclerosis (MS) is a severe central nervous system disease with a highly nonlinear disease course where periodic relapses impair the patient's quality of life. Clinical studies show that relapses co-occur with the appearance of new inflammatory MS lesions in MR images [19,25], making MR imaging a central element for the clinical management of MS patients. Further, assessing new MS lesions is crucial for disease assessment and therapy monitoring [5,8]. Unfortunately, prevailing therapies often involve highly active immunomodulatory drugs with potentially severe side effects. Hence, it necessitates developing machine learning models capable of predicting the future disease activity of individual patients to select the best therapy.While recent approaches applied convolutional neural networks (CNN) to directly learn features from MR image space [3,4,21,26,28], there remain challenges to obtain an effective global representation to characterize disease status. We attribute the difficulty of MS inflammatory disease activity prediction to a set of distinct disease characteristics that are well observable in MR images. First, while lesions have a sufficient signal-to-noise profile in images, their variation in shape, size, and number of occurrences amongst patients make it challenging for existing CNN-based methods that process the whole MRI scan in one go. Second, with advanced age, small areas appearing in the MRI of healthy individuals may resemble MS. As such, it is crucial to not only predict MS but also to identify the lesions deemed consequential for the final prediction.To solve this problem, we use concepts from geometric deep learning. Specifically, we propose a two-stage pipeline. First, the lesions in the MRI scans are segmented using a state-of-the-art 3D segmentation algorithm [9], and their image features are extracted with a self-supervised method [14]. Second, the extracted lesions are converted into a patient graph. The lesions act as nodes of the graph, while the edge connectivity is determined using the spatial proximity of the lesions. By this representation, we can solve the MS inflammatory disease activity prediction task as a graph-level classification problem. We argue that formulating the MS inflammatory disease activity prediction in our two-stage pipeline has the following advantages: (1) Graph neural networks can easily handle different numbers of nodes (lesions) and efficiently incorporate their spatial locations. (2) Modern segmentation [9,16,23] and representation learning methods [2,11,14] are effective tools for lesion detection and allow us to extract pathology-specific features. (3) By operating at the lesion level, it is possible to discover the lesions that contribute most to the eventual prediction, making the decisions more interpretable. Thus, our proposed solution can be a viable methodology for MS inflammatory disease activity prediction to handle the associated challenges. We first detect lesions in the MRI scan using nn-Unet [9]. A crop centered at the detection is extracted and used to learn self-supervised lesion features. Next, we build a graph from these detected lesions, where each lesion becomes a node, with the connections (edges) between lesions (nodes) defined by spatial proximity. This graph is processed using a graph neural network to generate enriched lesion features. Next, our self-pruning module (SPM) processes these enriched lesion features to determine an importance score for each lesion. The least scoring lesions are pruned-off and the highest scoring lesions are passed to the readout layer to obtain a graph-level feature vector. This graph-level vector is used for the final prediction.Contributions. Our contribution is threefold: (1) we are the first to formulate the MS inflammatory disease activity prediction task as a graph classification problem, thereby bringing a new set of methods to a significant clinical problem.(2) We propose a two-stage pipeline that effectively captures inherent MS variations in MRI scans, thus generating an effective global representation. (3) We develop a self-pruning module, which assigns an importance score to each lesion and reduces the task complexity by prioritizing the critical lesions. Additionally, the assigned per-lesion importance score improves our model's explainability."
Self-pruning Graph Neural Network for Predicting Inflammatory Disease Activity in Multiple Sclerosis from Brain MR Images,2,Methodology,"Overview. The objective is to predict MS inflammatory disease activity, i.e., to classify if new or significantly enlarged inflammatory lesions appear in the follow-up after the initial MRI scans. We denote the dataset as D(X, y), where X is the set of lesion patches extracted from MR scans, and y ∈ {0, 1} is the inflammatory disease activity status. For patient i, multiple lesion patches {x i 1 , x i 2 , ...x i n } can exist, where n is the total number of lesions. We aim to learn a mapping function f :Please note that our formulation differs from existing methods [21,26,28], which aim to learn a direct mapping from the MR image to the inflammatory disease activity label. As shown in Fig. 1, our proposed method consists of four distinct components. We describe each component in the following sections.Lesion Detection and Feature Extraction. We focus on the individual lesions instead of processing whole-brain MRI scans. This is important because MS lesions typically comprise less than 1% of voxels in the MRI scan. With this strategy, the graph model can aggregate lesion-level features for an effective patient-specific representation. First, we detect the lesions using a state-of-theart nn-Unet [9] pre-trained with MR images and their consensus annotations from two neuro-radiologists. Then, for each detected lesion, we extract a small fixed-size patch centered at it. Finally, we learn self-supervised features z for the lesion using a recent Transformer-based approach [14].Lesion Graph Processor. In the second stage, we generate a patient-specific graph G(V, E, Z) from the detected lesions. The lesions act as vertices V of this graph and are initialized with the crop-derived features Z. The spatial location s of the lesions is used to determine their connectivity E using a k-Nearest Neighbor (kNN) graph method [27]. Furthermore, the connected edges are weighted based on their spatial proximity. Specifically, given two lesions v i and v j , with spatial locations s i and s j respectively, the edge weight). τ is a scalar that controls the contribution of distant lesions. Hence the final graph connectivity can be represented as:where N (v i ) are the nodes directly connected to the node v i . Constructing a graph from the lesions is instrumental in two aspects: (1) The framework allows us to work with varying numbers of lesions in different patients. Alternatively, sequential models could be employed. However, since the lesions lack a canonical ordering, such models would not achieve an effective global representation [12].(2) It is possible to incorporate meaningful lesion properties such as spatial proximity and the number of lesions. Please note that separate graphs are created for individual patients. Thus, MS inflammatory disease activity prediction is formulated as a graph-level classification task. The graph G(V, E, Z) can be processed using message-passing neural networks (such as GCN [10], GAT [1], EdgeConv [24], GraphSage [6], to name a few) to learn enriched lesion features Ẑ. These enriched features are passed through to the self-pruning module.Self-pruning Module. The number of lesions can vary substantially among the patients, including the possibility of false positives in the segmentation stage. As such, it is crucial to recognize the most relevant lesions for the final prediction. In addition, this will bring inherent explainability and make it easier for a doctor to validate model predictions. To accomplish this, the enriched lesion features Ẑ are passed through a self-pruning module (SPM). The SPM produces a binary mask M for each lesion to determine whether a lesion contributes to the classification. The SPM uses a learnable projection vector p to compute importance scores ( Ẑ p/|| p||) for the lesions. These scores are scaled with a sigmoid layer. We retain the high-scoring lesions and discard the rest, which is formulated as:where σ(x) = 1/(1 + e -x ) is the sigmoid function and top-r(•) is an operator which selects a fraction r of the lesions based on high importance score. r is a hyper-parameter in our setup. Since the masking process is part of the forward pass through the model during both training and inference stages and not a post hoc modification, we refer to it as self-pruning of nodes. Features of the remaining nodes ( Ẑ = Ẑ ⊗ M ) are passed to the classification head.It should be noted that the existence of multiple lesions is a typical characteristic of MS. Hence, a crucial aspect of MS management is that clinicians must identify signs of inflammatory disease activity in MR images to make treatment decisions [19]. Therefore, along with predicting inflammatory disease activity, interpreting the contribution of individual lesions to the prediction is essential. By assigning an importance score to each lesion, the SPM can provide explainability to clinicians at a lesion level, while existing CNN methods can not.Classification Head. The classification head consists of a readout layer aggregating all the remaining node's features to produce a single feature vector ẑ for the entire graph. This graph-level feature is passed through an MLP to obtain the final prediction ŷ. We train our model using a binary cross-entropy loss.where N is the number of patients, y i is the ground truth inflammatory disease activity information and ŷi is the model prediction."
Self-pruning Graph Neural Network for Predicting Inflammatory Disease Activity in Multiple Sclerosis from Brain MR Images,3,Experiments,"Datasets and Image Preprocessing. Our approach is evaluated on a cohort of 430 MS patients collected following approval from the local IRB [7]. Patients included in this analysis were diagnosed with relapsing-remitting MS, with a maximum disease duration of three years at the time of baseline scan. We collect the FLAIR and T1w MR scans for each patient. The scans have a uniform voxel size of 1 × 1 × 1 mm 3 , were rigidly co-registered to the MNI152 atlas and skull-stripped using HD-BET [17]. Three neuro-radiologists independently read longitudinal subtraction imaging, where FLAIR images from two time points were co-registered and subtracted. In this vein, new and significantly enlarged lesions are identified as positive inflammatory disease activity. The dataset contains MS inflammatory disease activity information for clinically relevant one-year and two-year intervals [20]. At the end of the first year, we have the inflammatory disease activity status of 430 patients, with 303 showing activity and 127 not. Similarly, at the end of two years, we have data available for 347 patients, with 287 showing activity and 60 not. Thus, the dataset shows a slight imbalance in favor of inflammatory disease activity. This imbalance is a typical property in the MS patients cohort that impairs algorithm development."
Self-pruning Graph Neural Network for Predicting Inflammatory Disease Activity in Multiple Sclerosis from Brain MR Images,,Feature Extraction and Training Configuration.,"We use an nn-Unet [9] for lesion segmentation and detection. Then a uniform crop of size 24 × 24 × 24 mm 3 is extracted centered at each lesion. The cropped patches are passed through a transformer-based masked autoencoder [14] to extract self-supervised lesion features. The encoder produces a 768-dimensional feature vector for each patch. We also append normalized lesion coordinates to the encoder output to get the final lesion features.The lesions are connected using a k-nearest neighbor algorithm with k = 5. Further, these connections (edges) are weighted using τ = 0.01 (Eq. 1). Two message-passing layers with hidden dimensions of 64 and 8, respectively, process the generated graph to enrich lesion features. Next, the enriched features are passed through the SPM. The SPM uses a learnable projection vector p ∈ R 8 and sigmoid activation to learn the importance score. Based on this importance score, a mask is produced to select r = 0.5 (i.e., 50%) of the highest-scoring lesions and discard the rest. Next, a sum aggregation is used as the readout function. These aggregated features are passed through 2 feed-forward layers with hidden dimensions of size 8. Finally, the features are passed to a sigmoid function to obtain the final prediction.The model is trained for 300 epochs using AdamW optimizer [13] with 0.0001 weight decay. The base learning rate is 1e-4. The batch size is set to 16. A dropout layer with p = 0.5 is used between different feed-forward blocks. Since the dataset is imbalanced in favor of patients experiencing inflammatory disease activity, we use a balanced batch sampler to load approximately the same number of positive and negative samples in each mini-batch.Evaluation Strategy, Classifier, and Metrics. We report our results on MS inflammatory disease activity prediction for the clinically relevant one and twoyear intervals [20]. The Area Under the Receiver Operating Characteristic Curve (AUC) is used as the evaluation metric. We use 80% of the samples as the training set and 10% as the validation. The validation set is used to select the best model which is then applied to the remaining 10% cases. This procedure is iterated until all cases have been assigned to a test set once (ten-fold cross-validation). The same folds are used for the proposed model and baseline algorithms."
Self-pruning Graph Neural Network for Predicting Inflammatory Disease Activity in Multiple Sclerosis from Brain MR Images,4,Results,"Quantitative Comparison. Table 1 shows the classification performance for the ten folds on one-year and two-year lesion inflammatory disease activity prediction. The ± indicates the corresponding standard deviations. We compare our method against two existing approaches for MS inflammatory disease activity prediction baselines, a 3D Res-Net [28], and a multi-resolution CNN architecture [21]. These methods learn a direct mapping from the MR image to the inflammatory disease activity label. Our graph model outperforms the baseline methods on one (0.67 vs. 0.61 AUC) and two-year inflammatory disease activity prediction (0.66 vs. 0.60 AUC). In the following, we analyze and discuss each component of our established framework. Ablation Study. In this section, we analyze the importance of different components of our proposed method. We defer the analysis of the lesion feature representation to the appendix owing to space constraints.The Effectiveness of Graph Structure. Since the lesion feature extractor generates rich lesion features, one may argue that the graph structure is unwarranted. There are two alternatives to using a graph, (i) completely discard the graph structure, use a feed-forward layer to enrich the lesion features further, and aggregate them to perform classification [15] (Since this formulation regards the input as a set, we call this Set-Proc model). (ii) Aggregate all the lesion features for a patient using a mean aggregation and process the aggregated feature by traditional machine learning algorithms such as random forest (RF), support vector machine (SVM) with the RBF-kernel, and logistic regression (LR). Table 2 compares our model's performance against these alternatives. Our proposed solution obtains better AUC than the alternatives, indicating that incorporating the graph structure is beneficial for eventual prediction.    The Importance of Encoding Spatial Proximity. The spatial proximity in our model is encoded at two levels. First, lesion connectivity is determined using a NN graph, and second, we weigh the edges based on their distance. Graph convolution layers such as EdgeConv, GCN, and GraphSAGE take the edge weights into account. (EdgeConv does it implicitly by taking a difference of lesion features that already contain spatial information).On the other hand, the GAT model learns an attention weight and ignores the pre-defined edge weights. However, it still computes these coefficients on only the connected nodes. We can go further, completely ignore the distances and instead use a fully connected graph. The TransformerConv [18] on such a graph is equivalent to applying the well-known transformer encoder [22] on the inputs. Table 3 shows that the methods that ignore spatial proximity (TransformerConv) or do not use distance-based weighting (GAT) struggle. On the other hand, EdgeConv, GCN, and GraphSAGE work better. We use GCN in our model owing to its superior performance.The Contribution of the Self-Pruning Module (SPM). The SPM selects a subset of lesions for the final prediction during the training and evaluation phases. However, the proposed classification method can work without it. In this case, none of the lesions is discarded during the readout operation. Table 4 shows the classification results with and without the SPM. We observe that including SPM leads to better outcomes across different message-passing networks. An explanation could be that the SPM can better handle patient variations (in terms of the total number of lesions) by operating on a subset of lesions (Fig. 3)."
Self-pruning Graph Neural Network for Predicting Inflammatory Disease Activity in Multiple Sclerosis from Brain MR Images,,Analysis of Hyperparameters.,"The retention ratio r and the number of neighbors k used for building the graph are the two critical hyperparameters in our proposed framework. We discuss the effect of the retention ratio r here and defer discussion about k to the appendix.Effect of Retention Ratio r. The retention ratio r ∈ (0, 1] controls the fraction of lesions retained after the self-pruning module. If we set its value to 1, all the lesions are retained for the final prediction and thus, bypassing the self-pruning module. Any other value implies that we ignore at least a few lesions in the readout layer. Since the number of lesions can vary across graphs, we retain (N.r) lesions after the self-pruning layer. To find the optimal r, we test our model with r between 0.1 and 1.0. The results are summarized in Fig. 2. We set r to 0.5 for both tasks."
Self-pruning Graph Neural Network for Predicting Inflammatory Disease Activity in Multiple Sclerosis from Brain MR Images,5,Conclusion,"Predicting MS inflammatory disease activity is a clinically relevant, albeit challenging task. In this work, we propose a two-stage graph-based pipeline that surpasses existing CNN-based methods by decoupling the tasks of detecting and learning rich semantic features for lesions. We also propose a self-pruning module that further improves model generalizability by handling variations in the number of lesions within patients. Most importantly, we frame the MS inflammatory disease activity prediction as a graph classification problem. We hope our work provides a new perspective and leads to cutting-edge research at the intersection of graph processing and MS inflammatory disease activity prediction."
Learning Large Margin Sparse Embeddings for Open Set Medical Diagnosis,1,Introduction and Related Work,"Deep learning achieves great success in image-based disease classification. However, the computer-aided diagnosis is far from being solved when considering various requirements in real-world applications. As an important one, open set recognition (OSR) specifies that diseases unseen in training could appear in testing [23]. It is practical in the medical field, caused by the difficulties of collecting a training dataset exhausting all diseases, and by the unpredictably appearing new or rare diseases. As a result, an OSR-informed model should not only accurately recognize known diseases but also detect unknowns and report them. Clinically, these models help construct trustworthy computer-aided systems. By forwarding unseen diseases to experts, not only the misdiagnosis of rare diseases could be avoided, but an early warning of a new disease outbreak could be raised.There are many fields related to OSR but are essentially different. In classification with reject options [7,9], samples with low confidence are rejected to avoid misclassification. However, since its closed set nature, unknown classes could still be misclassified confidently [8,23]. Anomaly detection, novelty detection, and one-class classification [21] aim at recognizing unknowns but ignore categorizing the known classes. In outlier detection or one-/few-show learning [27], samples of novel classes appear in training. In zero-shot learning [29], semantic information from novel classes could be accessed. Such as zebra, an unknown class, could be identified given the idea that they are stripped horses, and abundant samples of horse and stripe patterns. Differently, OSR knows nothing about novel classes and should have high classification accuracy of the known meanwhile recognize unknowns, as illustrated in Fig. 1a). Due to limited space, some reviews [8,22,34] are recommended for more comprehensive conceptual distinctions.Most OSR researches focus on natural images, while medical OSR is still in its infancy. In medical fields, representative work like T3PO [6] introduces an extra task to predict the input image augmentation, and samples with low probabilities are regarded as unknowns. CSL [32] uses generative adversarial neural networks (GAN) to generate proxy images and unknown anchors. As for natural images, a line of work tries to simulate unknowns using generated adversarial or counterfactual samples using GAN [14,20,28,33]. However, whether unknown patterns could be generated by learning from the known is unclear. Some works learn descriptive feature representations. They enhance better feature separation between unknowns and knowns or assume the known features following certain distributions so that samples away from distributional centers could be recognized as unknowns [3,5,10,17,18,24,35]. Differently, this work categorizes densely distributed known features and recognizes sparse embedding space as unknowns, regardless of the specific distribution.This work tackles OSR under the assumption that known features could be assembled compactly in feature embedding space, and remaining sparse regions could be recognized as unknowns. Inspired by this, the Open Margin Cosine Loss (OMCL) is proposed merging two components, Margin Loss with Adaptive Scale (MLAS) and Open-Space Suppression (OSS). The former enhances known feature compactness and the latter recognizes sparse feature space as unknown. Specifically, MLAS introduces the angular margin to the loss function, which reinforces the intra-class compactness and inter-class separability. Besides, a learnable scaling factor is proposed to enhance the generalization capacity. OSS generates feature space descriptors that scatter across a bounded feature space. By categorizing them as unknowns, it opens a classifier by recognizing sparse feature space as unknowns and suppressing the overconfidence of the known. An embedding space example is demonstrated in Fig. 1b), showing OMCL learns more descriptive features and more distinguishing known-unknown separation.Considering medical OSR is still a nascent field, besides OMCL, we also proposed two publicly available benchmark datasets. One is microscopic images of blood cells, and the other is optical coherence tomography (OCT) of the eye fundus. OMCL shows good adaptability to different image modalities.Our contributions are summarized as follows. Firstly, we propose a novel approach, OMCL for OSR in medical diagnosis. It reinforces intra-class compactness and inter-class separability, and meanwhile recognizes sparse feature space as unknowns. Secondly, an adaptive scaling factor is proposed to enhance the generalization capacity of OMCL. Thirdly, two benchmark datasets are proposed for OSR. Extensive ablation experiments and feature visualization demonstrate the effectiveness of each design. The superiority over state-of-the-art methods indicates the effectiveness of our method and the adaptability of OMCL on different image modalities."
Learning Large Margin Sparse Embeddings for Open Set Medical Diagnosis,2,Method,"In Sect. 2.1, the open set problem and the formation of cosine Softmax are introduced. The two mechanisms MLAS and OSS are sequentially elaborated in Sect. 2.2 and 2.3, followed by the overall formation of OMCL in Sect. 2.4.  Cosine Loss: The cosine Softmax is used as the basis of the OMCL. It transfers feature embeddings from the Euclidian space to a hyperspherical one, where feature differences depend merely on their angular separation rather than spatial distance. Given an image x i , its vectorized feature embedding z i , and its label y i , the derivation progress of the cosine Softmax is"
Learning Large Margin Sparse Embeddings for Open Set Medical Diagnosis,2.1,Preliminaries,"where W j denotes the weights of the last fully-connected layer (bias is set to 0 for simplicity). W j = 1 and z i = s are manually fixed to constant numbers 1 and s by L2 normalization. s is named the scaling factor. cos(θ j,i ) denotes the angle between W j and z i . By doing so, the direction of W j could be regarded as the prototypical direction of class j as shown in Fig. 2a). Samples with large angular differences from their corresponding prototype will be punished and meanwhile class-wise prototypes will be pushed apart in the angular space. Compared with Softmax, the cosine form has a more explicit geometric interpretation, promotes more stabilized weights updating, and learns more discriminative embeddings [15,16,26]. Moreover, the L2 normalization constrains features to a bounded feature space, which allows us to generate feature space descriptors for opening a classifier (will be further discussed in Sect. 2.3)."
Learning Large Margin Sparse Embeddings for Open Set Medical Diagnosis,2.2,Margin Loss with Adaptive Scale (MLAS),"MLAS serves three purposes. 1) By applying angular margin, the intra-class compactness and the inter-class separability are strengthened. 2) The threshold could represent the potential probability of the unknowns, which not only prepares for the open set but also learns more confident probabilities of the knowns.3) A trainable scaling factor is designed to strengthen the generalization capacity. MLAS is:m, t, and s respectively denote margin, threshold, and learnable scaling factor, with corresponding geometric interpretation demonstrated in Fig. 2b). By using the angular margin, the decision boundary could be more stringent. Without it, the decision boundary is cos(θ 1,i ) > cos(θ 2,i ) for the i-th sample of class 1. It becomes cos(θ 1,i ) > cos(θ 2,i ) + m when using the margin, which leads to stronger intra-class compactness. Moreover, the angular similarities with other classes are punished in the denominator to increase inter-class separability.The threshold t could be regarded as an extra dimension that prepares for unknown classes. Given the conventional input of Softmax as [q 1  i , q 2 i , ..., q C i ] ∈ R C , ours could be understood as [q 1  i , q 2 i , ..., q C i , t] ∈ R C+1 . Since t is added, the class-wise output q c i before Softmax is forced to have a higher value to avoid misclassification (at least larger than t). It reinforces more stringent learning and hence increases the feature compactness in the hyperspherical space.A large s makes the distribution more uniform, and a small s makes it collapses to a point mass. In this work, it is learnable, with a learning rate 0.1× the learning rate of the model. It theoretically offers stronger generalization capacity to various datasets and is experimentally observed to converge to different values in different data trails and could boost performances.LMCL [26] and NMCL [15] are the most similar arts to ours. Differently, from the task perspective, these designs are proposed for closed-world problems. From the method perspective, an OSS mechanism is designed to tackle OSR leveraging generate pseudo-unknown features for discriminative learning. Moreover, an adaptive scaling factor is introduced for increasing generalization."
Learning Large Margin Sparse Embeddings for Open Set Medical Diagnosis,2.3,Open-Space Suppression (OSS),"OSS generates feature space descriptors of bounded feature space. By categorizing them into an extra C + 1 class, samples in sparse feature space could be recognized as unknown and the overconfidence of the known is suppressed.OSS selects points scattered over the entire feature space, named descriptors, representing pseudo-unknown samples. Different from existing arts that generate pseudo-unknowns by learning from known samples, the OSS selects points scattered over the feature space. It guarantees all space could be possibly considered for simulating the potential unknowns. By competing with the known features, feature space with densely distributed samples is classified as the known, and the sparse space, represented by the descriptors, will be recognized as unknown.In this work, the corresponding descriptor set, with M samples, is s, s] denotes random continuous uniform distribution ranges between -s to s, and d is the dimension of feature embeddings. s is trainable and the descriptors are dynamically generated with the training. Figure 2c) demonstrates the geometric interpretation. During training, descriptors are concatenated with the training samples at the input of the last fully-connected layer, to equip the last layer with the discrimination capacity of known and unknown samples. The OSS is (θj,i)   (3) where t and s follow the same definition in MLAS.Most similar arts like AL [25] attempts to reduce misclassification by abandoning ambiguous training images. Differently, we focus on OSR and exploit a novel discriminative loss with feature-level descriptors for OSR."
Learning Large Margin Sparse Embeddings for Open Set Medical Diagnosis,2.4,Open Margin Cosine Loss (OMCL),"OMCL unifies MLAS and OSS into one formula, which isI i equals 1 if the i-th sample is training data, and equals 0 if it belongs to the feature space descriptors. λ is a weight factor. Since the output of the channel C +1 is fixed as t, no extra weights W C+1 are trained in the last fully-connected layer. As a result, OMCL does not increase the number of trainable weights in a neural network. During testing, just as in other works [2,3], the maximum probability of known classes is taken as the index of unknowns, where a lower known probability indicates a high possibility of unknowns."
Learning Large Margin Sparse Embeddings for Open Set Medical Diagnosis,3.1,"Datasets, Evaluation Metrics, and Implementation Details","Two datasets are adapted as new benchmarks for evaluating the OSR problem. Following protocols in natural images [2,4], half of the classes are selected as known and reminders as unknowns. Since the grouping affects the results, it is randomly repeated K times, leading to K independent data trials. The average results of K trials are used for evaluation. The specific groupings are listed in the supplementary material, so that future works could follow it for fair comparisons. BloodMnist contains 8 kinds of individual normal cells with 17,092 images [1]. Our setting is based on the closed set split and prepossessing from [31]. Classes are selected 5 rounds (K=5). In each trial, images belonging to 4 chosen classes are selected for training and closed-set evaluation. Images belonging to the other 4 classes in testing data are used for open set evaluation. OCTMnist has 109,309 optical coherence tomography (OCT) images [13], preprocessed following [31]. Among the 4 classes, 1 is healthy and the other 3 are retinal diseases. In data trail splitting, the healthy class is always in the known set, which is consistent with real circumstances, and trails equal to 3 (K=3). "
Learning Large Margin Sparse Embeddings for Open Set Medical Diagnosis,3.2,Comparison with State-of-the-Art Methods,"As demonstrated in Table 1, the proposed OMCL surpasses state-of-the-art models, including typical discriminative methods, baseline [12], GCPL [30], and RPL [3]; latest generative model DIAS [19]; and ARPL+CS [2] that hybrids both. All methods are implemented based on their official codes. Their best results after hyperparameter finetunes are reported. Results show the OMCL maintains the accuracy, meanwhile could effectively recognize unknowns.  "
Learning Large Margin Sparse Embeddings for Open Set Medical Diagnosis,3.3,Ablation Studies,"Effectiveness of MLAS and OSS : Table 2 demonstrates the respective contributions of MLAS and OSS in OMCL. Each of them enhances the performances and they could work complementarily to further improve performances.Ablation Study of Adaptive Scaling Factor : Fig. 3a) demonstrates the effectiveness of the adaptive scaling factor. Quantitatively, the adaptive design surpasses a fixed one. Moreover, Fig. 3b) displays the scaling factor will converge to different values in different training trials. Both results demonstrate the effectiveness and the generalization capacity of the adaptive design.Ablation Study of Hyperparameters t, m, and λ: Fig. 4a), b), and c) respectively show the influence on results when using different hyperparameters. t and m are the threshold and angular margin, presented in Eq. 2, and λ is the trade-off parameter in Eq. 4 .Ablation Study of M : Fig. 4d) illustrates the effect of the number of feature space descriptors upon results. The ratio 1:1 is experimentally validated as a proper ratio. Because a randomly generated descriptor could be extremely close to a known feature point, but classified as a novel category, which may disturb the training. If the number of descriptors is far more than that of the training samples (the 5 times shown in Fig. 4), the performance gets lower.Feature Visualization: Fig. 1b) visualizes the t-SNE results of features z of both known and unknown classes after dimension reduction. For each class, 200 samples are visualized and the perplexity of the t-SNE is set to 30. It shows that OMCL could learn better intra-class compactness and inter-class separability.Moreover, samples of unknown classes tend to be pushed away from known classes, incidcating the effectiveness of our designs."
Learning Large Margin Sparse Embeddings for Open Set Medical Diagnosis,4,Conclusion,"In this paper, two publicly available benchmark datasets are proposed for evaluating the OSR problem in medical fields. Besides, a novel method called OMCL is proposed, under the assumption that known features could be assembled compactly in feature space and the sparse regions could be recognized as unknowns.The OMCL unifies two mechanisms, MLAS and OSS, into a unified formula. The former reinforces intra-class compactness and inter-class separability of samples in the hyperspherical feature space, and an adaptive scaling factor is proposed to empower the generalization capability. The latter opens a classifier by categorizing sparse regions as unknown using feature space descriptors. Extensive ablation experiments and feature visualization demonstrate the effectiveness of each design. Compared to recent state-of-the-art methods, the proposed OMCL performs superior, measured by ACC, AUROC, and OSCR."
Learning Large Margin Sparse Embeddings for Open Set Medical Diagnosis,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3_53.
A Texture Neural Network to Predict the Abnormal Brachial Plexus from Routine Magnetic Resonance Imaging,1,Introduction,"Brachial plexopathy is a form of peripheral neuropathy [1]. It occurs when there is damage to the brachial plexus (BP) which is a complex nerve network under the skin of the shoulder. There is a wide range of disease that may cause a brachial plexopathy.Radiation fibrosis, primary and metastatic lung cancer, and metastatic breast cancer account for almost three-fourths of causes [2]. Brachial plexus syndrome occurs not infrequently in patients with malignant disease. It is due to compression or direct invasion of the nerves by tumor which will bring many serious symptoms [3]. Our research focuses on the brachial plexopathy caused by metastatic breast cancers.Magnetic resonance imaging (MRI) and ultrasound of the brachial plexus have become two reliable diagnostic tools for brachial plexopathy [4]. Automatic identification of the BP in MRI and ultrasound images has become a hot topic. Currently, most of relevant research in this field are focusing on Ultrasound modality [5][6][7][8]. Compared with ultrasound, MRI has become the primary imaging technique in the evaluation of brachial plexus pathology [9]. However, to our knowledge, radiomics related BP studies utilizing MRI have not been reported previously.Many radiomics studies have experimentally demonstrated that image texture has great potential for differentiation of different tissue types and pathologies [10]. In the past several decades, many state-of-the-art methods have been proposed to extract texture patterns [11,12]. However, how to most effectively combine texture features with deep learning, called deep texture, is still an open area of research. One prior approach, termed GLCM-CNN, was proposed to carry out a polyp differentiation task [13]. However, how to arrange these GLCMs to form the 3D volume to optimize the performance is a major challenge.With the goal of classifying normal from abnormal BP, we explored the approach of deep texture learning. This paper constructed a BP dataset with the most commonly used BP MRIs in our clinical practice. Considering the shortcoming of traditional patterns, triple point pattern (TPP) is proposed for the quantitative representation of the heterogeneity of abnormal BP's. In contrast to GLCM-CNN, TPPNet is designed to train models by feeding TPP matrices as the input with a huge number of channels. Finally, we analyze the model's performance in the experimental section. The major contributions of this study include 1) directed triangle construction idea for TPP, 2) huge number of TPP matrices as the heterogeneity representations of BP, 3) TPPNet with 15 layers and huge number of channels, 4) the BP dataset containing MR images and their corresponding ROI masks."
A Texture Neural Network to Predict the Abnormal Brachial Plexus from Routine Magnetic Resonance Imaging,2.1,Dataset Preparation and Preprocessing,"Following IRB approval for this study, we search for patients with metastatic breast cancer who had a breast cancer MRI performed between 2010 and 2020 and had morphologically positive BP on the MRI report from our electronic medical records (EMR) in * hospital. Totally, we collect approximate 807 series which include 274 T2, 254 T1 and 279 Post-gadolinium. Since some scans are seriously degraded due to motion artifacts. Therefore, each case underwent several essential image adjustments such as multi-series splitting, two-series merging, slice swapping, artifact checking and boundary corrections. To yield the ROI, firstly, we randomly sampled -40% of the sequences including both normal and abnormal ones that were manually segmented with ITK-snap by two skilled trainees [14,15]. Then, the manual segmentations were utilized to train a 3D nnUNet model which was utilized to train the model which was used to predict ROIs for the rest series [16]. The predicted segmentations were manually divided into three groups, i.e. good, fair and poor. Good cases were added to the training set. This process was repeated until no improvements in the predictions for the remaining sequences was seen. The final dataset for radiomic analysis was constructed by merging the datasets for each sequence type. Only patients that had all three sequences segmented (T2, T1 and Post-gadolinium) were included in the dataset. Table 1 shows a breakdown of the final dataset."
A Texture Neural Network to Predict the Abnormal Brachial Plexus from Routine Magnetic Resonance Imaging,2.2,Triple Point Pattern (TPP),"Theoretically, some texture pattern methods such as LBP, LTP, and GLDM, are based on single-variance pixel functions [17][18][19]. Therefore, they extract local texture features coded by the difference or difference counts between the concerned pixel and its neighboring ones. One obvious shortcoming is the absence of global properties which need other statistical methods as the aid to yield, such as histogram and invariants [20,21]. Meanwhile, some other texture methods are generally defined by two-variance functions that only focus on two-variance patterns, such as (pixel, pixel), (pixel, neighbor count) [22][23][24]. In general, image textures extracted by these methods contain both local texture properties and global texture information. Their shortcomings might come from pattern shapes which might lead to the overfitting risk while combining with deep learning since the yielded texture matrix might have slim shapes or adaptive columns. In summary, as the requirement of the image texture and deep learning, an excellent image texture pattern should have some essential features including 1) local properties to characterize the micro-unit of the image texture, 2) global properties to represent the macro-structure of the image texture, 3) uniform shapes under nonuniform-shape images, 4) invariant or robustness under some common geometric transforms such as rotation, scaling and so on.According above requirements, we developed a method to produce a serial of novel texture patterns by introducing a directed triangle idea with an adjacent triple pixel as a ternary group, called triple point pattern (TPP), to extract the local texture information. Then, a statistical method like histogram is employed to count the number of the same type of pixel-triplets within the ROI or throughout the whole image. Finally, a threedimensional (3D) TPP matrix is formed to characterize the image texture globally as the following:Two-dimensional image:where I is a MxN image, x, y, and z is the pixel triplet, x,y,z ∈ [0,L), L is its gray level, p c = (0,0) denotes the concerned pixel such as p 0 in Fig. 1, p i and p j are p c 's two adjacent pixels, i,j ∈ [1,H], H is the number of p c 's adjacent points. Three-dimensional image:where I is a three dimensional image with the shape of MxNxK, p c = (0,0,0), other parameters are similar to the Two-dimensional image.In statistics, a TPP matrix should a 3D distribution of directed triangle. As shown in Fig. 1, the TPP is formed by the concerned pixel and its two adjacent pixels in two-dimensional(2D) images. Similarly, the TPP in 3D images is constructed by one concerned voxel and its two neighboring voxels. More details could be found in the supplementary material. As the construction idea of TPP, there are four independent modes categorized by the concerned angle, i.e. 45°, 90°, 135°and 180°in 2D images, which produce 8 TPPs, 8 TPPs, 8 TPPs and 8 TPPs respectively. Analogously, the 3D image has twelve independent angle modes, i.e. 35.26°, 45°, 54.74°, 60°, 70.53°, 90°, 109.47°, 120°, 125.26°, 135°, 144.74°, and 180°. These angle modes could generate 24 TPPs, 24 TPPs, 24 TPPs, 24 TPPs, 12 TPPs, 96 TPPs, 12 TPPs, 24 TPPs, 24 TPPs, 24 TPPs, 24 TPPs and 13 TPPs respectively. Totally there are 32 TPPs in 2D images and 325 TPPs in 3D images and every TPP could produce one corresponding TPP matrix.By further analysis, we could find some TPP pairs have an isomorphism relationship since its TPP matrix could be generated by transposing or flipping another TPP matrix on some certain conditions when two triangles formed by the pixel triplet have the relevance of shifting or scaling. For an instance, the TPP matrix by pixel-triplet (p 1 , p 0 , p 2 ) in Fig. 1 (1), the TPP matrix by (p 4 , p 0 , p 5 ) in Fig. 1 (1) and the TPP matrix by (p 6 , p 0 , p 8 ) in Fig. 1(3) have the following relevance:where T denotes matrix transposing, F represents matrix flipping, * is the product operator. Since both T and F are continuous bijective mappings, these three TPPs are isomorphic.In our study, these isomorphic TPP matrices are not dropped from the TPP matrix set because they are equivalent to image rotations and re-scaling. Image scaling can result in the image pixels increasing. The normalization could almost remove the effect of pixel increase caused by scaling transformations. Moreover, TPPs generated by 135°c ould also be treated as affine transformations. Therefore, data augmentation could be omitted when we combine TPP with deep learning for this study. As its definition, the TPP matrix should be a cubic array with the shape of LxLxL where L is the gray level of the image."
A Texture Neural Network to Predict the Abnormal Brachial Plexus from Routine Magnetic Resonance Imaging,2.3,TPPNet,"The pipeline of the proposed method TPPNet is illustrated in Fig. 2. The TPP matrix calculation is the preprocessing module which feeds MRI and its ROI and yield TPP matrix set. The following step is the TPPNet architecture to yield training models. Based on the construction idea of TPP, the size of the TPP matrix depends on the gray level of the image. For the same image or ROI, the larger the gray level, the sparser the matrix will be. The sparse matrix would lead to overfitting while training the model. Therefore, the image requires a re-scaling step to lower its gray level to avoid the sparsity of the TPP matrix. Consequently, our proposed TPPNet only contains three convolution blocks consisting of 15 layers. Each block has two convolution, one normalization, one max-pooling and one dropout layer. It has four particular features as follows:1) Avoidance of image augmentation. Due to the stability of TPP matrix under rotation, scale and affine transformations, image augmentation could be omitted in the preprocessing step which can lead to image deformation. 2) Huge number of channels. TPPNet treats each TPP as an independent channel. For 2D images, there are at least 32 channels if more displacements of TPP is considered. Similarly, we could generate no less than 325 TPPs in 3D images. 3) Simple end-to-end architecture. We integrate the k-fold cross-validation, TPP generation and model training into one framework. Since the TPP matrix is always small, there are only 15 layers in TPP which could reduce the risk of overfitting issue met in deeper neural networks. 4) Free from the interference of multi-texture-pattern arrangements. Since each channel is corresponding with one TPP, it can solve the pattern arrangement issue occurred in GLCM-CNN. 3 Experiments"
A Texture Neural Network to Predict the Abnormal Brachial Plexus from Routine Magnetic Resonance Imaging,3.1,Preparations,"Some important specificities of our computing platform contain: one AMD EPYC 7352 24-Core Processor, 1 TB memory and four Nivida A100-SXM GPUs with 320 GB GPU memory. The whole dataset is divided into three subsets according to the MR sequence, i.e. T2, T1 and post-gad. For each subset, both normal cases and abnormal cases were randomly and evenly split into five subgroups. A five-fold cross-validation scheme was employed to generate five cohorts. Totally, 15 cohorts were produced. Each cohort consists of training set, validation set and testing set by the ratio of 6:2:2."
A Texture Neural Network to Predict the Abnormal Brachial Plexus from Routine Magnetic Resonance Imaging,3.2,Ablation Studies,"Since all images in our dataset are 3D images, therefore, the initial channel is set 325 which is equal to the TPP number. The loss functions in the following experiments shared categorical_crossentropy. Nadam is adopted as the optimizer with the learning rate of 0.0001 and batch size of 8 for 200 epochs. All performances listed in this section are the average of performances with 5-fold cross-validation."
A Texture Neural Network to Predict the Abnormal Brachial Plexus from Routine Magnetic Resonance Imaging,3.2.1,Impact of Gray Level,"The image gray level determines the shape of the TPP matrix. To avoid its sparsity, we compute the TPP matrix set via Eq. ( 2) with gray levels of 8, 12, 16, 20, 24. While rescaling the image intensity, an arc tangent approach is utilized to yield the new image.The models are trained and tested over 15 cohorts. Their performances evaluated by accuracies are listed in Table2 which tells us that T2 sequence yields the highest accuracy of 96.1% when the gray level is 12. T1 and post-gadolinium also get acceptable results with the accuracies of 93.5% and 93.6% respectively. Other performances could be read in supplementary materials. "
A Texture Neural Network to Predict the Abnormal Brachial Plexus from Routine Magnetic Resonance Imaging,,Acc,Loss Acc Loss Acc Loss 325 0.948±0.026 0.342±0.126 0.922±0.035 0.363±0.098 0.934±0.034 0.302±0. 124  1 0.863±0.027 0.274±0.091 0.810±0.145 0.458±0.105 0.811±0.047 0.590±0.107Input channel T2 T1 Pg
A Texture Neural Network to Predict the Abnormal Brachial Plexus from Routine Magnetic Resonance Imaging,3.2.2,Impact of Intensity Rescaling Approaches,"Rescaling approaches of image intensity could also bring impacts on the BP's differentiation while producing the TPP matrix. The commonly used methods include minmax-linear approach [25], arc tangent approach [26], and adaptive rescaling approach [27]. To test the performances fairly, we test above rescaling methods at the same gray level 12. The yielded performances evaluated by accuracies are shown in Table 3 where arc tangent method achieves the highest accuracy of 96.1% over the T2 sequence. Other performances are shown in supplementary materials."
A Texture Neural Network to Predict the Abnormal Brachial Plexus from Routine Magnetic Resonance Imaging,3.2.3,Multi-Channels vs Solo-Channel,"We carry out experiments to train the TPPNet model and make tests with arc tangent rescaling approach under gray level 16. As a comparison, we test single channel mode as well. By sharing every TPP matrix's label with the original case label, our TPPNet works well by assigning one channel for the initial input. Once the trained model with solo channel is generated, all TPP matrices of the testing set could be tested. Hereafter, we adopted a voting method to determine if the prediction is normal, if the predicted probability is less than 0.5, otherwise, it is considered abnormal. Performances with accuracies and loss are listed in Table 4. Other performances are listed in supplementary materials."
A Texture Neural Network to Predict the Abnormal Brachial Plexus from Routine Magnetic Resonance Imaging,3.3,Comparisons,"We evaluated our proposed TPPNet by comparing it to the recent state-of-the-art approaches over our BP dataset including VGG16 [28], InceptionNet [29], MobileNet [30], GLCM-CNN [13], ViT [31]. The GLCM size of 32 × 32 is used in GLCM_CNN. All approaches shared the same image shape of 128 × 128 × 64 with 1 channel. The patch shape of ViT is 8 × 8 × 8, projection dim is 64, attention head number is set 4 with 8 transformer layers. The intensity rescaling step adopts the arc tangent approach. Other parameters are similar to the ablation study. Their performances are "
A Texture Neural Network to Predict the Abnormal Brachial Plexus from Routine Magnetic Resonance Imaging,4,Conclusions,"In this paper, we develop an approach to carry out the pioneer study of differentiating abnormal BP from normal ones relevant to breast cancer. In particular, TPP is proposed to extract texture features as the representation of BP's heterogeneity from MRIs. Moreover, a TPPNet with huge number of initial channels is designed to train the model. To testify our proposed TPPNet, a BP dataset is constructed with 452 series including three most commonly used MR sequences in clinical practice, i.e. T2, T1 and Post-gadolinium. The best result is yielded when the gray level is 12, intensity rescaling method adopts arc tangent approach. Experimental outcomes also demonstrate that the proposed TPPNet not only exhibit more stable performances but also outperform six famous state-of-the-art approaches over three most commonly used BP MR sequences."
Flow-Based Geometric Interpolation of Fiber Orientation Distribution Functions,1,Introduction,"Diffusion MRI (dMRI) is the most widely used technique for studying human brain structural connectivity in vivo [1]. Significant improvements in imaging techniques dramatically increased the spatial and angular resolution of dMRI [2] and provided opportunities for advanced models such as fiber orientation distribution (FOD) [3], which facilitates the development of FOD-based fiber tracking for brain connectivity research. However, well-known challenges in current tractography methods generate large amounts of false positives and negatives [4]. While there have been considerable efforts in developing novel fiber tracking methods [5], a critical step in tractography, FOD interpolation, has received rare attention. In popular FOD-based tractography, linear interpolation is commonly adopted for numerical efficiency. Still, it often generates artificial directions and ignores rotations between neighboring FODs, as shown in Fig. 1. (B), which can lead to false positive streamlines. To enhance FOD interpolation, a Riemannian framework was proposed in [6]; under the square root reparameterization, the space of FOD functions can form the positive orthant of the unit Hilbert sphere. However, this framework is computationally expensive and sometimes fails to provide anatomically meaningful interpolations [7]. A rotation group action-based framework [7] was proposed that simultaneously averages the shape and rotation of FODs. A later work [8] proposed a rotation-induced Riemannian metric for FODs and introduced a weighted mean for FOD interpolation. However, since only one rotation is used for the whole FOD, these methods cannot handle more general situations where individual FOD peaks experience different rotations. More importantly, these methods have not been adopted in a tractography framework to advance fiber tracking performance due to their numerical complexity.In this work, we develop a novel framework to perform geometrically consistent interpolation of FODs and demonstrate its effectiveness in enhancing the performance of fiber tracking. We decompose each FOD function with multiple peak lobes into components, each with only one peak lobe. Then, we locally model neighboring voxels' single-peak components, consistent in direction, as a vector field flow fitted by polynomials. Each vector field locally represents the geometry of an underlying fiber bundle and continuously determines the direction of single-peak components within the support. Then, a closed-form solution is developed to account for rotations of FODs represented as spherical harmonics and realize the geometrically consistent interpolation of each FOD component, as shown in Fig. 1. (C). The interpolation of a complete FOD function with multiple peak lobes is obtained by merging the single-peak interpolations from all the covering vector fields. In our experiments, we use HCP data to quantify the accuracy of the proposed FOD interpolation algorithm and show that it achieves superior performance than the commonly used linear interpolation approach. Furthermore, we apply our interpolation method to perform upsampling of FOD fields and significantly improve the performance of FOD-based tractography both qualitatively and quantitatively. "
Flow-Based Geometric Interpolation of Fiber Orientation Distribution Functions,2.1,FOD Decomposition,"The fiber orientation distribution (FOD) is an advanced model representing the complicated crossing fiber's geometry [9]. However, the multiple peak lobes of the FOD function pose a challenge for image processing. Our solution is to decompose the FOD function into several independent components, each containing only one peak lobe. A FOD function is conventionally represented under the real spherical harmonics (SPHARMs) basis up to the order N:where Y m n is the m th (-n ≤ m ≤ n) real SPHARM basis at the order n (0 ≤ n ≤ N) and u m n is the coefficient for the corresponding basis, U is the vector that represents all the coefficients u m n , and θ and ϕ are the polar and azimuth angles of the spherical coordinates in R 3 . For any FOD function, we expand it using (1) on a unit sphere represented by a triangular mesh, search the peaks on the mesh, and accept the peaks whose value is higher than a threshold THD (e.g., 0.1). For a FOD function with K peak lobes, we solve the following optimization problem for its decomposition:where U k are the coefficients for the decomposed single-peak FOD components, and A k is the matrix that represents the values of SPHARMs at neighboring directions around the k th peak (vertices within two rings of each peak). The first term enforces the sum of the decomposed single-peak components to equal the original FOD; the second term enforces each component to equal the original FOD near the corresponding peak; the third term suppresses each component around other peaks. We show an example of a FOD function decomposition in Fig. 2, where a FOD function is decomposed into two single-peak components."
Flow-Based Geometric Interpolation of Fiber Orientation Distribution Functions,2.2,Modeling Single Peak FOD Components as Flow of Vector Fields,"For each single-peak FOD component, we model it with the flow of a smooth vector field, which supports geometrically consistent interpolations of FOD components. We represent the k th single-peak component of the FOD function at a voxel p 0 as F k p 0 . We choose the peak direction of F k p 0 as the seeding vector v p0 of the local supporting vector field. Then we compute a tube T k p 0 , centering at p 0 , along the direction v p0 with a radius r and a height h (Fig. 3. (A)). For each voxel p t within the tube T k p 0 , we choose the single-peak component F k p t (Fig. 3. (B)) whose peak direction v pt is closest to v p0 , and the peak direction v pt is a vector at p t (Fig. 3. (C)). We do not pick any vector for voxels without a valid peak direction whose angular difference is less than a threshold θ to the seeding vector v p0 . These peak vectors {v pt } form a vector field within this tube, and we use a second-order polynomial to fit each component of this vector field:where v d p t (1 ≤ d ≤ 3) represent d th component of the vector at voxel p t . The second term regulates the second-order coefficients for smoothness. The polynomials are used to model the vector field V k p 0 within the tube T k p 0 that represents the k th underlying fiber bundle locally around the voxel p 0 ."
Flow-Based Geometric Interpolation of Fiber Orientation Distribution Functions,2.3,Rotation Calculation for SPHARM-Based FODs,"For a target point q where we perform the interpolation, we choose the nearest voxel p 0 , which has been augmented with a set of tubes {T k p 0 } and vector fields {V k p 0 } through the computation of Sect. 2.2. For each vector field V k p 0 , we compute the vector v q at point q using its polynomial representation. Each of the corresponding k th single-peak FOD component F k p t from voxels within one voxel distance to q are used for interpolation. First, we rotate each single-peak component F k p t so that its peak direction is aligned with the vector v q . An easy way to compute the rotation is R t = exp([r] × ), where r is a vector with its direction determined by the crossing product between the peak vectors v pt and v q , and its length is the angle between v pt and v q ; [•] × is the cross-product matrix of a vector [8]. Since the rotated single-peak FOD components are now aligned in direction, we can compute the weighted mean of SPHARM coefficients, which is the interpolated FOD component corresponding to the k th peak around voxel p 0 . The weights can be inverse distance or linear interpolation weights. After interpolating all the FOD singlepeak components independently, we combine them into the complete interpolated FOD function at point q. The flowchart of the method is shown in Fig. 4. Our framework independently handles the single-peak components of different FODs and successfully obtains geometrically consistent interpolation of complicated crossing fiber geometry. An essential step for the interpolation above is to transform the FOD function by a rotation R. A straightforward numerical way is to expand the FOD function on a spherical triangular mesh and rotate the mesh to rotate the function and compute the inner products between the rotated FOD function and each of the SPHARM basis to obtain the coefficients. However, the numerical method is computationally expensive. Instead, we propose a closed-form solution to derive a matrix from the rotation R that can be applied to the coefficients of the SPHARMs. Let FOD R represent the FOD function after applying the rotation R. We have the following relation:where (θ r ,ϕ r ) is the coordinate acquired by rotating the coordinate (θ ,ϕ) with the inverse rotation R -1 . We represent (4) using the SPHARMs:where v m n and u m n are coefficients for FOD R and FOD, respectively. The key to computing coefficients v m n is representing the SPHARM function Y m n (θ r , ϕ r ) by a linear combination of Y m n (θ, ϕ); namely, finding the transformation of SPHARMs under a coordinate system rotation. For rotation R -1 , we follow Wigner's work [10] to decompose it as three successive rotations around three axes:where Z γ and Z α are the rotations around the current z-axis by angles γ and α, respectively; Y β is the rotation around the current y-axis by an angle β. We transform the real SPHARMs into complex SPHARMs for more straightforward computation. Based on a group symmetry argument [10], Wigner has proven that Wigner D-matrices can represent the transformation of the n th -order complex SPHARMs between two coordinate systems based on the decomposition in (6):where W n is a (2n + 1) complex vector that represents the n th -order complex SPHARMs; D n is a (2n + 1)-by-(2n + 1) matrix whose elements are represented as:where the first and third terms correspond to the rotations Z α and Z γ in ( 6), and the rotations around the z-axis are trivial since they only change the azimuth angle ϕ in the complex SPHARMs. The middle term is induced from the rotation Y β , which corresponds to a rotation around the y-axis, and is much more complicated:where P is the Jacobi polynomial, and other elements of this matrix can be induced by the symmetry property [11,12]. Combining Eqs. ( 5) and ( 7), we have: where v n and u n are real (2n + 1) vectors whose m th element is v m n and u m n in (5); U is the real to complex SPHARMs transformation matrix, and its inverse is U -1 . Now we can compute the n th -order coefficients of the rotated FOD R in (4) by formula (10). The computation achieved by the closed-form representation is efficient because it only involves small-size matrix operations. For example, the largest matrix in coefficients computation for a FOD function represented by up to 16 th -order SPHARMs is 33 × 33."
Flow-Based Geometric Interpolation of Fiber Orientation Distribution Functions,2.4,Evaluation Methods,"We compare the proposed FOD interpolation method with the linear interpolation of SPHARM coefficients, the most used method in FOD-based tractography. We measure the quality of the interpolated FOD functions based on down-sampling; we down-simple a ground truth FOD volume data to half the resolution, interpolate the down-sampled data to the original resolution, and measure the interpolated FOD functions against the ground truth data based on two metrics. The first metric is to measure the sharpness of the interpolated FOD functions, which indicates the specificity and accuracy of the FOD function. Inspired by the full width at half maximum (FWHM) in signal processing, we define the full area at half maximum (FAHM) of a FOD function f as FAHM (f ) = area({x : f (x) > max(f /2)})/4π . The metric FAHM is more sensitive to boating effects than entropy [7] and generalized fractional anisotropy [8]. Another metric is the relative error between the interpolated FOD function and the ground truth FOD function. The relative error is the L 2 distance of two FOD functions divided by the L 2 norm of the ground truth FOD function.We also evaluate the effectiveness of the proposed method on tractography. We upsample the FOD volume images to super-resolution images, including the cortical spinal tract (CST) area that connects the cortical surface to the internal capsule. Then, we run the popular tractography from the MRTrix [13] on the original and super-resolution data. We use an evaluation called Topographic Regularity, an essential property widely presented in motor and sensory pathways [14][15][16][17], to show the improvements of the tractography on up-sampled FOD data. We measure the topographic regularity using an intuitive metric proposed in [14], where the classical multidimensional scaling (MDS) is used to project both the beginning (cortical surface) and ending points (internal capsule) of the streamlines of each CST bundle to R 2 . Then, the Procrustes distance between the projected beginning and ending points is computed to characterize how well topographic regularity is preserved during fiber tracking."
Flow-Based Geometric Interpolation of Fiber Orientation Distribution Functions,3,Experiment Results,"We evaluated the FOD interpolation using 40 HCP subjects [18], including 20 females and 20 males. We reconstructed 16 th -order SPHARM-based FODs [9] from the HCP data with an isotropic resolution of 1.25mm. For parameters in our method, we set λ 1 , λ 2, and λ 3 in Eqs. ( 2) and (3) to be 1; the radius r, height h, and θ of the tubes to be three times of voxel size, five times of voxel size, and 10°.The HCP FOD data is used as the ground truth for down-sampling-based evaluation. We show the FODs from one interpolated slice of a subject in Fig. 5 and highlight the FODs from an ROI (red box) where several fiber bundles cross. Contrasting to the proposed method, FODs from linear interpolation tend to lose their sharpness. For each subject, we computed the FAHM and relative L 2 error for each interpolated FOD, which was then used to compute the mean FAHM and mean relative L 2 error among all interpolated FODs. We show the boxplots of these measures from the 40 HCP subjects in Fig. 6. (A) and (B). The FAHM measurement shows our approach avoids the bloating effects and preserves a similar level of sharpness to the ground truth FODs; the lower mean relative L 2 error to the ground truth from our method further shows the proposed interpolation achieved more accurate interpolation.We up-sampled the 40 HCP FOD volume images around the CST region to superresolution images with an isotropic resolution of 0.25 mm. Then, we ran FOD-based probabilistic tractography on the original and up-sampled FOD data using the iFOD1 algorithm of the MRtrix software [13]. In each run, 10K seed points are randomly selected, and the main parameters of iFOD1 are set as: step size = 0.02 mm, which is around 0.1 times the voxel size of the up-sampled image, and angle threshold = 7°. The same parameters were used for the original HCP dataset to avoid the bias of parameters. Three representative examples of the reconstructed CST bundle from the motor cortex to the internal capsule are shown in Fig. 7, where we can easily see that the tracts from the super-resolution FOD by our proposed interpolation method are much smoother and better reflect the somatotopic organizational principles of the CST from neuroanatomy than the baseline tracking results. In Fig. 6. (C), boxplots of the results from Procrustes analyses further confirm this observation and demonstrate that our geometric FOD interpolation algorithm can significantly enhance the anatomical consistency of fiber tracking results."
Flow-Based Geometric Interpolation of Fiber Orientation Distribution Functions,4,Conclusion,"We propose a novel interpolation method for FOD function with enhanced consistency of fiber geometry. The experiments show that our method provides a more accurate interpolation of FODs and can generate super-resolution FODs via upsampling to improve the tractography's performance significantly. In future work, we will integrate the proposed FOD interpolation with tractography algorithms and validate its performance in reducing false positives and negatives in challenging fiber bundles."
Neural Pre-processing: A Learning Framework for End-to-End Brain MRI Pre-processing,1,Introduction,"Brain magnetic resonance imaging (MRI) is widely-used in clinical practice and neuroscience. Many popular toolkits for pre-processing brain MRI scans exist, e.g., FreeSurfer [9], FSL [26], AFNI [5], and ANTs [3]. These toolkits divide up the pre-processing pipeline into sub-tasks, such as skull-stripping, intensity normalization, and spatial normalization/registration, which often rely on computationally-intensive optimization algorithms.Recent works have turned to machine learning-based methods to improve preprocessing efficiency. These methods, however, are designed to solve individual sub-tasks, such as SynthStrip [15] for skull-stripping and Voxelmorph [4] for registration. Learning-based methods have advantages in terms of inference time and performance. However, solving sub-tasks independently and serially has the drawback that each step's performance depends on the previous step. In this paper, we propose a neural network-based approach, which we term Neural Pre-Processing (NPP), to solve three basic tasks of pre-processing simultaneously.NPP first translates a head MRI scan into a skull-stripped and intensitynormalized brain using a translation module, and then spatially transforms to the standard coordinate space with a spatial transform module. As we demonstrate in our experiments, the design of the architecture is critical for solving these tasks together. Furthermore, NPP offers the flexibility to turn on/off different pre-processing steps at inference time. Our experiments demonstrate that NPP achieves state-of-the-art accuracy in all the sub-tasks we consider."
Neural Pre-processing: A Learning Framework for End-to-End Brain MRI Pre-processing,2.1,Model,"As shown in Fig. 1, our model contains two modules: a geometry-preserving translation module, and a spatial-transform module.Geometry-Preserving Translation Module. This module converts a brain MRI scan to a skull-stripped and intensity normalized brain. We implement it using a U-Net style [24] f θ architecture (see Fig. 1), where θ denotes the model weights. We operationalize skull stripping and intensity normalization as a pixelwise multiplication of the input image with a scalar multiplier field χ, which is the output of the U-Net f θ :where ⊗ denotes the element-wise (Hadamard) product. Such a parameterization allows us to impose constraints on χ. In this work, we penalize high-frequencies in χ, via the total variation loss described below. Another advantage of χ is that it can be computed at a lower resolution to boost both training and inference speed, and then up-sampled to the full resolution grid before being multiplied with the input image. This is possible because the multiplier χ is spatially smooth. In contrast, if we have f θ directly compute the output image, doing this at a lower resolution means we will inevitably lose high frequency information. In our experiments, we take advantage of thibass by having the model output the multiplicative field at a grid size that is 1/2 of the original input grid size along each dimension. The scalar field, which solves both skull stripping and intensity normalization, is not range restricted by design. The appropriate values will be learned from the data. In practice, we found that thresholding it at 0.2 yields a good brain mask."
Neural Pre-processing: A Learning Framework for End-to-End Brain MRI Pre-processing,,Spatial Transformation,"Module. Spatial normalization is implemented as a variant of the Spatial Transformer Network (STN) [17]; in our implementation, the STN outputs the 12 parameters of an affine matrix Φ af f . The STN takes  as input the bottleneck features from the image translation network f θ and feeds it through a multi-layer perceptron (MLP) that projects the features to a 12-dimensional vector encoding the affine transformation matrix. This affine transformation is in turn applied to the output of the image translation module T θ (x) via a differentiable resampling layer [17]."
Neural Pre-processing: A Learning Framework for End-to-End Brain MRI Pre-processing,2.2,Loss Function,"The objective to minimize is composed of two terms. The first term is a reconstruction loss L rec . In this paper, we use SSIM [31] for L rec . The second term penalizes T θ from making high-frequency intensity changes to the input image, encapsulating our prior knowledge that skull-stripping and MR bias field correction involve a pixel-wise product with a spatially smooth field. In this work, we use a total variation penalty [23] L T V on the multiplier field χ, which promotes sparsity of spatial gradients in χ. The final objective is:where x gt is the pre-processed ground truth images, λ ≥ 0 controls the trade-off between the two loss terms, and • denotes a spatial transformation.Conditioning on λ. Classically, hyperparameters like λ are tuned on a held-out validation set -a computationally-intensive task which requires training multiple models corresponding to different values of λ. To avoid this, we condition on λ in f θ by passing in λ as an input to a separate MLP h φ (λ) (see Fig. 1), which generates a λ-conditional scale and bias for each channel of the decoder layers. h φ can be interpreted as a hypernetwork [12,14,30] which generates a conditional scale and bias similar to adaptive instance normalization (AdaIN) [16]. Specifically, for a given decoder layer with C intermediate feature maps {z 1 , ..., z C }, h φ (λ) generates the parameters to scale and bias each channel z c such that the the channel values are computed as:for c ∈ {1, ..., C}. Here, α c and β c denote the scale and bias of channel c, conditioned on λ. This is repeated for every decoder layer, except the final layer."
Neural Pre-processing: A Learning Framework for End-to-End Brain MRI Pre-processing,3,Experiments,"Training Details. We created a large-scale dataset of 3D T1-weighted (T1w) brain MRI volumes by aggregating 7 datasets: GSP [13], ADNI [22], OASIS [20], ADHD [25], ABIDE [32], MCIC [11], and COBRE [1]. The whole training set contains 10,083 scans. As ground-truth target images, we used FreeSurfer generated skull-stripped, intensity-normalized and affine-registered (to so-called MNI atlas coordinates) images. We train NPP with ADAM [19] and a batch size of 2, for a maximum of 60 epochs. The initial learning rate is 1e-4 and then decreases by half after 30 epochs. We use random gamma transformation as a data augmentation technique, with parameter log gamma (-0.3, 0.3). We randomly sampled λ from a log-uniform distribution on (-3, 1) for each mini-batch.Architecture Details. f θ is a U-Net-style architecture containing an encoder and decoder with skip connections in between. The encoder and decoder have 5 levels and each level consists of 2 consecutive 3D convolutional layers. Specifically, each 3D convolutional layer is followed by an instance normalization layer and LeakyReLU (negative slope of 0.01). In the bottleneck, we use three transformer blocks to enhance the ability of capturing global information [29]. Each transformer block contains a self-attention layer and a MLP layer. For the transformer block, we use patch size 1 × 1 × 1, 8 attention heads, and an MLP expansion ratio of 1. We perform tokenization by flattening the 3D CNN feature maps into a 1D sequence. The hypernetwork, h φ , is a 3-layer MLP with hidden layers 512, 2048 and 496. The STN MLP is composed of a global average pooling layer and a 2-layer MLP with hidden layers of size 256 and 12. The 2-layer MLP contains: linear (256 channels); ReLU; linear (12 channels); Tanh. Note an identity matrix is added to the output affine matrix to make sure the initial transformation is close to identity. It's widely used in the affine registration literature to improve convergence and efficiency.Baselines. We chose three popular and widely-used tools, SynthStrip [15], C2F [21], FSL [26], and FreeSurfer [9], as baselines. SynthStrip (SS) is a learningbased skull-stripping method, while FSL and FreeSurfer (FS) is a cross-platform brain processing package containing multiple tools. FSL's Brain Extraction Tool (BET) and FMRIB's Automated Segmentation Tool are for skull stripping and MR bias field correction, respectively. FS uses a watershed method for skullstripping, a model-based tissue segmentation (N4biasfieldcorrection [28]) for intensity normalization and bias field correction."
Neural Pre-processing: A Learning Framework for End-to-End Brain MRI Pre-processing,3.1,Runtime Analyses,"The primary advantage of NPP is runtime. As shown in Table 1, for images with resolution 256 × 256 × 256, NPP requires less than 3 s on a GPU and less than 8 s on a CPU for all three pre-processing tasks. This is in part due to the fact that the multiplier field can be computed at a lower resolution (in our case, on a grid of 128 × 128 × 128). The output field is then up-sampled with trilinear interpolation before being multiplied with the input image. In contrast, SynthStrip needs 16.5 s on a GPU for skull stripping and C2F needs 5.6 on a GPU for spatial normalization. FSL's optimized implementation takes about 271.3 s per image for skull stripping and intensity normalization, whereas FreeSurfer needs more than 10 min."
Neural Pre-processing: A Learning Framework for End-to-End Brain MRI Pre-processing,3.2,Pre-processing Performance,"We empirically validate the performance of NPP for the three tasks we consider: skull-stripping, intensity normalization, and spatial transformation.Evaluation Datasets. For skull-stripping, we evaluate on the Neuralfeedback skull-stripped repository (NFSR) [7] dataset. NFSR contains 125 manually skullstripped T1w images from individuals aged 21 to 45, and are diagnosed with a wide range of psychiatric diseases. The definition of the brain mask used in NFSR follows that of FS. For intensity normalization, we evaluate on the test set (N = 856) from the Human Connectome Project (HCP). The HCP dataset includes T1w and T2w brain MRI scans which can be combined to obtain a high quality estimate of the bias field [10,27]. For spatial normalization, we use T1w MRI scans from the Parkinson's Progression Markers Initiative (PPMI). These images were automatically segmented using Freesurfer into anatomical regions of interest (ROIs)1  [6]."
Neural Pre-processing: A Learning Framework for End-to-End Brain MRI Pre-processing,,Metrics.,"For skull-stripping, we quantify performance using the Dice overlap coefficient (Dice), Sensitivity (Sens), Specificity (Spec), mean surface distance (MSD), residual mean surface distance (RMSD), and Hausdorff distance (HD), as defined elsewhere [18]. For intensity normalization, we evaluate the intensitynormalized reconstruction (Rec) and estimated bias image (Bias, which is equal to the multiplier field χ) to the ground truth images, using PSNR and SSIM. We quantify registration between ROIs in an individual MRI and the atlas ROI for the assessment of spatial normalization by calculating the Dice score between the spatially transformed segmentations (resampled using the estimated affine transformation) and the probabilistic labels of the target atlas.Results. Figure 2 and Fig. 3 shows skull-stripping performance for all models. We observe that the proposed method outperforms all traditional and learningbased baselines on Dice, Spec, and MSD/RMSD. Importantly, NPP achieved 93.8% accuracy and 2.7% improvement on Dice and 2.39mm MSD. Especially for MSD, NPP is 28% better than the second-best method, SynthStrip. We further observe that BET commonly fails, which has also been noted in the literature [8].Table 2 shows the quantitative results of FSL, FS and NPP(see visualization results in Supplementary S1). NPP outperforms the baselines on all metrics. From Table 2, we see that FreeSurfer's reconstruction is better than BET's, but the bias field estimates are relatively worse. We can appreciate this in the figure in Supplementary S1, as we observe that FS's bias field estimate (f) contains too much high-frequency anatomical detail.  on all ROIs measured. Figure 4(a) shows representative slices for spatial normalization."
Neural Pre-processing: A Learning Framework for End-to-End Brain MRI Pre-processing,3.3,Ablation,"As ablations, we compare the specialized architecture of NPP against a naive U-Net trained to solve all three tasks at once. Additionally, we implemented a different version of our model where the U-Net directly outputs the skullstripped and intensity-normalized image, which is in turn re-sampled with the STN. In this version, we did not have the scalar multiplication field and thus our loss function did not include the total variation term. We call this version U-Net+STN. As another alternative, we trained the U-Net+STN architecture via UMIRGPIT [2], which encourages the translation network (U-Net) to be geometry-preserving by alternating the order of the translation and registration. We note again that for all these baselines, we used the same architecture as f θ , but instead of computing the multiplier field χ, f θ computes the final intensity-normalized and spatially transformed image directly. The objective is the reconstruction loss L rec . All other implementation details were the same as NPP. For evaluation, we use the test images from the HCP dataset.Results: Tables 3 and4 lists the SSIM values for the estimated reconstruction and bias fields, for different ablations and NPP with a range of λ values. We observe that there is a sweet spot around λ = 0.1, which underscores the importance of considering different hyperparameter settings and affording the user to optimize this at test time. All ablation results are poor, supporting the importance of our architectural design. Figure 5 shows some representative results for a range of λ values."
Neural Pre-processing: A Learning Framework for End-to-End Brain MRI Pre-processing,4,Conclusion,"In this paper, we propose a novel neural network approach for brain MRI preprocessing. The proposed model, called NPP, disentangles geometry-preserving translation mapping (which includes skull stripping and bias field correction) and spatial transformation. Our experiments demonstrate that NPP can achieve state-of-the-art results for the major tasks of brain MRI pre-processing.Funding. Funding for this project was in part provided by the NIH grant R01AG053949, and the NSF CAREER 1748377 grant."
Neural Pre-processing: A Learning Framework for End-to-End Brain MRI Pre-processing,,Table 3 .,"NPP, λ = 0.1 99.25 ± 0.52 98.40 ± 0.40 NPP, λ = 0.01 99.24 ± 0.51 98.22 ±0.39 NPP, λ = 0.001 99.22 ± 0.52 98.13 ± 0.40"
Neural Pre-processing: A Learning Framework for End-to-End Brain MRI Pre-processing,,Table 4 .,"NPP, λ = 0.1 99.25 ± 0.52"
Neural Pre-processing: A Learning Framework for End-to-End Brain MRI Pre-processing,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3 25.
Exploring Brain Function-Structure Connectome Skeleton via Self-supervised Graph-Transformer Approach,1,Introduction,"Understanding the relationship between brain functional connectivity and structural connectivity is a key issue in studying the working mechanisms of the brain [1,2]. In the early stage, due to the dynamic variability nature of functional connectivity and the unique stability of structural connectivity, the two were often analyzed separately [3,4]. Later, attention was paid to their relationship, and a large number of studies were proposed to analyze this issue [5][6][7]. For example, Greicius et al. [5] found that higher structural connectivity tended to be accompanied by higher functional connectivity; MIŠI Ć et al. [6] used a multimodal approach to correlate structural and functional connectivity with each other using partial least squares analysis while searching for the best covariance pattern of both; Sarwa et al. [7] adopted a deep learning framework for structural connectivity to functional connectivity prediction.Despite the great success of the above methods, they are far from satisfactory for studying the relationship between brain functional and structural connectivity. Connections in brain regions make up networks, and it is crucial to identify key nodes and connections. Therefore, there are currently at least two difficulties. On the one hand, key brain regions, act as hubs for information transmission in the brain network, have not been completely identified in the joint analysis of brain functional and structural profiles. On the other hand, it has not been clearly studied for the connectome skeleton of brain, which plays a key role in both functional and structural connections of brain.To overcome the above limitations, we propose a transformer-based graph selfsupervised graph reconstruction framework. It can obtain the key connectome regions and skeleton of brain by combining brain function and structure. The method has two main characteristics. For one thing, graph neural networks (GNN) [8] is applied to integrate brain function and structure. We represent the brain as a graph, which its nodes are the regions of interest (ROIs) defined by the atlas. The edge information and node features of the graph come from the structural connectivity and the functional connectivity of the ROIs, respectively. The graph features are propagated through the self-attention mechanism [9] and graph convolution network (GCN) [10]. For the other thing, a self-supervised model is adopted to reconstruct the brain graph and obtain the contribution scores of ROIs for the reconstruction task. ROIs with higher scores are more important for reconstruction and are considered as key connectome regions of brain. We used several functional magnetic resonance imaging (fMRI) from the Human Connectome Project (HCP) 900 datasets [11], combined them with structure connectivity from diffusion-weighted MRI (dMRI) separately, and acquired the corresponding key connectome regions. Based on key regions, we obtained the connectome skeleton of the brain.Experimental results demonstrate the effectiveness of proposed method. First, we obtained low loss values, which indicated that the model achieved the reconstruction task. Second, we obtained the contribution scores of brain ROIs. ROIs with high scores play a role in the transmission of information in the network, are regarded as key connectome regions. Finally, we obtained the connectome skeleton of the brain, which expresses the most key connections of the brain both in functional and structural networks."
Exploring Brain Function-Structure Connectome Skeleton via Self-supervised Graph-Transformer Approach,2.1,Overview,"The pipeline of the transformer-based self-supervised graph reconstruction (TSGR) framework proposed in this paper is shown in Fig. 1. The method analyzes key connectome regions of brain in the joint analysis of brain functional and structural profiles.The brain is represented as a graph and used as input for the ScorePool-AE module, then the reconstructed graph and contribution scores of the nodes can be obtained. The framework consists of three main parts, i.e., graph generation, ScorePool-AE module, and reconstruction target. In the graph generation part, we adopt Destrieux Atlas to initialize the brain surface into 148 ROIs and use them as nodes of the graph. Based on the ROIs, functional and structural information are utilized to generate the node features and edge features of the graph, respectively. In the ScorePool-AE module, we design ScorePool to get the contribution score of each node in the graph. In the reconstruction target part, the mean square error (MSE) between the node features of the input graph and the reconstructed graph is applied as the loss function. "
Exploring Brain Function-Structure Connectome Skeleton via Self-supervised Graph-Transformer Approach,2.2,Data and Preprocess,"We used the HCP 900 dataset and randomly selected 98 subjects from it. T1-weighted MRI data were used to reconstruct the brain cortical surface, dMRI were utilized to reconstruct the fiber bundles from white matter, resting-state fMRI (rs-fMRI) and task fMRI showed the functional changes of the brain. Among them, task fMRI data contains a total of seven tasks, which are EMOTION, GAMBLING, LANGUAGE, MOTOR, RELATION, SOCIAL and WM.Standard Freesurfer pipeline including tissue-segmentation and white matter surface (inner surface) reconstruction [12] is proposed to preprocess the T1-weighted MRI. For rs-fMRI, we adopted the graycoordinate system [13] as the platform to extract rs-fMRI time sequence for each surface vertex. For dMRI, we followed the method in Van den Heuvel et al. [14] to use deterministic tractography to derive white matter fibers, and reconstructed 5 × 10 4 fiber tracts for each subject.To jointly use these three modalities, we aligned them into the same space. A linear image registration method (FLIRT) [15] and a nonlinear one (FNIRT) [16] were cascaded to transform and warp T1-weighted MRI to the dMRI space. The preprocessed rs-fMRI uses graycoordinate system, which is located at the same space as T1-weighted MRI, and the vertex-wise correspondence between them can be directly established."
Exploring Brain Function-Structure Connectome Skeleton via Self-supervised Graph-Transformer Approach,2.3,TSGR Framework,"Graph Generation. We define the brain as an undirected graph G = {V , E}. V = {v i |i ∈ 1, 2, . . . , n} represents the set of nodes of the graph. F ∈ R n×D represents the feature matrix of graph nodes. E ∈ R n×n represents the adjacency matrix of the graph.Graph Nodes with Generated Features. The brain surface is divided into 148 ROIs and used as nodes of the graph. fMRI signals are selected to express node features. We compute the Pearson correlation coefficients of the signals among all ROIs to obtain the functional similarity matrix and use it as the feature matrix F of the graph nodes. The length of each node feature D is 148.Graph Edge. For each pair of ROIs, we calculate the total number of fibers directly connected of them and then divide it by the geometric mean of their areas, thus obtaining the structural connectivity matrix S. We set the threshold t s for S to make it sparse and do the binarization to obtain E. E i,j = 1 means that nodes i and j are connected, otherwise E i,j = 0."
Exploring Brain Function-Structure Connectome Skeleton via Self-supervised Graph-Transformer Approach,,ScorePool-AE Module.,"In ScorePool-AE module, we adopt the encoder-decoder structure to implement the graph reconstruction task. The encoder is applied to extract node representations of the graph, then the ScorePool is used to obtain the contribution scores of nodes, and finally the decoder is used to reconstruct the node features of the graph.The encoder is based on the Ugformer [17], which implements the self-attention mechanism on the nodes of the graph via transformer. For one of the layers of UGformer, the self-attention mechanism is adopted on all nodes of the graph rather than on neighboring nodes, then GCNs is applied to exploit the structural information of the graph. The process is shown as Eq. ( 1) and (2).(1)where H (k) is the node representations of the graph at the k-th layer of UGformer. V is the set of all nodes of the graph.Inspired by the pooling operation of the GNN [18], we design the ScorePool to get the contribution score of each node in the graph for the reconstruction task. ScorePool comes after encoder and before decoder. The output of encoder is the graph node representations X ∈ R n×d , which is used as the input of the ScorePool. After an MLP layer, X is mapped to Y ∈ R n×1 . Y is seen as the scores of graph nodes. Finally, we do the element-wise product operation of X and Y , and use it as the input of the decoder. It is calculated as Eqs. ( 3) and ( 4). The decoder has the same structure as the encoder and is utilized to reconstruct the node features.where denotes the element-wise product and X denotes the input of the decoder.Reconstruction Target. Our TSGR framework reconstructs the graph by predicting the nodes features of the graph. The loss function is MSE between the feature matrix F of reconstructed graph nodes and the feature matrix F of input graph nodes."
Exploring Brain Function-Structure Connectome Skeleton via Self-supervised Graph-Transformer Approach,2.4,Analyzing Brain Key Connectome ROIs and Hierarchical Networks,"In this work, we propose a method that identifies key connectome ROIs and can be applied to the hierarchical analysis of brain networks. First, we obtained the average contribution scores of brain ROIs for all individuals. Second, we verify whether the ROIs with high scores are key connectome ROIs in the functional and structural networks. The functional network is obtained by averaging F over all individuals and setting a threshold. The structural network is the same operation done for S. We use three network centrality metrics, i.e., degree centrality, closeness centrality and PageRank centrality, to measure key connectome ROIs. In addition, we calculate the number of all ROIs participating in the functional network, where the functional networks are obtained by dictionary learning [19]. Finally, we divide all brain ROIs into 3 scales and analyze the characteristics of networks consisting of ROIs at different scales, namely, the connectivity strength and global efficiency of the networks."
Exploring Brain Function-Structure Connectome Skeleton via Self-supervised Graph-Transformer Approach,2.5,Exploring the Connectome Skeleton of Brain ROIs,"This study explores the connectome skeleton of the brain based on important ROIs. The connectome skeleton plays the role in common connections in the brain network and is a core connectivity pattern. First, we obtain the key functional connections of eight (one resting-state and seven tasks) functional networks. Specifically, we calculate the eight functional network connections consisting of Scale-1 and Scale-2 ROIs, and a connection is selected if it exists simultaneously in six and more functional networks. Then, we obtain the intersection between the key functional connections and the structural network connection as the connectome skeleton. Finally, we analyze the connectome skeleton, namely, counting the strength of connectivity between brain regions and the length of fibers in the connectome skeleton.  "
Exploring Brain Function-Structure Connectome Skeleton via Self-supervised Graph-Transformer Approach,3.2,Analysis of Key Brain ROIs and Network Hierarchy,"For each set of experiments, the contribution scores of brain ROIs are obtained by averaging the scores of all individuals, as shown in Fig. 3. According to the scores from high to low, we divide the ROIs into 3 scales. Each scale contains 20%, 30%, and 50% of the ROIs, and the corresponding number of ROIs is 30, 44, and 74 respectively. Scale-1 ROIs are considered as key connectome ROIs. Scale-2 and Scale-3 are in descending order of importance in brain connectivity. Figure 3(B) shows Scale-1 ROIs in eight sets of experiments, which are basically distributed in the superior frontal, parietal, and occipital of the lateral brain, with little distribution in the medial and bottom parts of the brain. These regions have important biological significance, for example, Heuvel et al. confirmed that the parietal and prefrontal cortex contain multiple hubs in almost all species [21]. Figure 3(D) lists the Scale-1 specific ROIs for all experiments. Some ROIs exist in the Scale-1 of most experiments, such as the F_mid, Precentral, and O_mid, indicating that these ROIs are involved in the functional network of multiple tasks; some ROIs are unique to one experiment, such as the fronto-margin only in the SOCIAL, indicating that this ROI is more important in this task. Figure 3(C) shows the Scale-1 IoU in all experiments, and it can be seen that EMOTION and LANGUAGE have the highest value, indicating that the functional activation of these two tasks is similar. To verify whether the ROIs with high scores are key connectome ROIs, we calculate the centrality of all ROIs in the functional network (orange) and structural network (green) separately, and the horizontal axis indicates the ROIs ranked from lowest to highest scores, as shown in Fig. 4(A). Here we use three centrality metrics, namely degree centrality, closeness centrality, and PageRank centrality. The results show that the metrics are positively correlated with scores of ROIs. We also calculate the number of all ROIs participating in the functional network (purple), and the results are positively correlated with scores of ROIs. This indicates that ROIs with high scores are located in key positions at the network and participate in more functional networks.In addition, we also investigate the networks composed among the scales. As shown in Fig. 4(B), the functional and structural connectivity strength is relatively similar overall, with decreasing strength of connections in the Scale-1, Scale-2 and Scale-3.We also analyze the global efficiency of the network and the coupling between the functional and structural networks, as shown in Fig. 4(C). Among the functional and structural networks, S1_1 has the highest global efficiency for most of the experiments, followed by S1_2 and S2_2. It can be seen at the bottom of Fig. 4(C), S1_1 has the highest functional-structural coupling. Therefore, we consider the Scale-1 ROIs as key connectome ROIs, and the network they form assumes the main information transfer function in the functional and structural network. "
Exploring Brain Function-Structure Connectome Skeleton via Self-supervised Graph-Transformer Approach,3.3,Analysis of Connectome Skeleton in Brain Networks,"We obtain the key connection for the functional network of all eight functional networks, where the functional network consists of connections within the ROIs including Scale-1 and Scale-2. Then we combine the key functional connections and the structural connections to obtain the connectome skeleton, as shown in Fig. 5(A). It shows that the connections are distributed more evenly over the whole brain rather than concentrated in a particular region. From Fig. 5(B), it can be concluded that the intra-occipital and intra-central connections are the most numerous, while the frontal and parietal connections are more evenly distributed. This indicates that the occipital region plays an important role in the connectome skeleton. We count the fiber lengths in the connectome skeleton, as shown in the upper of Fig. 5(C). The fiber lengths are mostly concentrated around 50 mm, and long fibers (fiber lengths greater than 80 mm) account for about 20%, which are more distributed in the frontal region. The high percentage of short fibers improves communication efficiency. The bottom of Fig. 5(C) counts the fiber lengths of the connectome skeleton in the Gyrus-Sulcus connection, in which the fibers of the G-G connection are the longest, followed by the G-S and finally the S-S, which indicates that the connectome skeleton still follows the traditional Gyrus-Sulcus connection pattern [22,23]."
Exploring Brain Function-Structure Connectome Skeleton via Self-supervised Graph-Transformer Approach,4,Conclusion,"We propose a new transformer-based self-supervised graph reconstruction framework to identify key brain connectome ROIs and connectome skeleton in the joint analysis of brain functional connectivity and structural connectivity. The main contribution of the approach is the use of GNN to fuse brain function and structure and the use of a self-supervised model to identify the ROIs that important for graph reconstruction. The experimental results validate the effectiveness of the method. First, we obtain the scores of ROIs. Second, we verify that the ROIs with high scores are key connectome ROIs. Finally, we obtain the connectome skeleton of the brain. It provides a new approach for analyzing the relationship between functional and structural connectivity, and further analysis will be settled in future work."
Robust and Generalisable Segmentation of Subtle Epilepsy-Causing Lesions: A Graph Convolutional Approach,1,Introduction,"Structural cerebral abnormalities commonly cause drug-resistant focal epilepsy, which may be cured with surgery. Focal cortical dysplasias (FCDs) are the most common pathology in children and the third most common pathology in adults undergoing epilepsy surgery [1]. However, 16-43% of FCDs are not identified on routine visual inspection of MRI data by radiologists [10]. Identification of these lesions on MRI is integral for presurgical planning. Furthermore, accurate identification of lesions assists with complete resection of the structural abnormality, which is associated with improved post-surgical seizure freedom rates [11].There has been significant work seeking to automate the detection of FCDs, with the aim of identifying subtle structural abnormalities in patients with lesions not identified by visual inspection, termed ""MRI negative"" [12]. These algorithms are increasingly being evaluated prospectively on patients who are ""MRI negative"" with suspected FCD, where radiologists review algorithm outputs and evaluate all putative lesions. However, previous methods operate locally or semi-locally: using multilayer perceptrons (MLPs) which consider voxels or points on the cortical surface (vertices) individually [2,10], or convolutional neural networks which have to date typically been trained on patches of cortex [6]. One widely-available algorithm using such an approach was able to detect 63% of MRI negative examples, with an AUC of 0.64 [10]. Overall, although these algorithms show significant promise in finding subtle and previously unidentified lesions, they are commonly associated with high false positive rates which hampers clinical utility [12]. Detecting FCDs is particularly challenging due to small dataset sizes, high inter-annotator variability in manual lesion masks, and the large class imbalance, as FCDs typically only cover around 1% of the total cortex. Nevertheless the urgent clinical need to identify more of these subtle lesions motivates the development of methods to address these challenges.Contributions. We propose a robust surface-based semantic segmentation approach to address the particular challenges of identifying FCDs (Fig. 1). Our three main contributions to address these challenges are: 1) Adapting nnU-Net [8], a state-of-the-art U-Net architecture, to a Graph Convolutional Network (GCN) for segmenting cortical surfaces. This creates a novel method for cortical segmentation in general and for FCD segmentation in particular, in which the model is able to learn spatial relationships between brain regions. 2) Inclusion of a distance loss to help reduce false positives, and 3) Inclusion of a hemisphere classification loss to act as form of weak supervision, mitigating the impact of imperfect lesion masks. We directly evaluate the added value of each contribution on performance in comparison to a previously published MLP [10]. We hypothesised that the proposed GCN to segment FCDs would improve overall performance (AUC), in particular reducing the number of false positives (improved specificity) while retaining sensitivity. This improvement in classifier performance would facilitate clinical translation of automated FCD detection into clinical practice. All code to reproduce these results can be found at github.com/MELDProject/meld_graph."
Robust and Generalisable Segmentation of Subtle Epilepsy-Causing Lesions: A Graph Convolutional Approach,2.1,Graph Convolutional Network (GCN) for Surface-Based Lesion Segmentation,"We consider the lesion detection problem as a surface-based segmentation task. For this purpose, cortical surface-based features (intensity, curvature, etc.; see Sect. 3.1) are extracted from each brain hemisphere and registered using FreeSurfer [4] to a symmetrical template. This template was generated by successively upsampling an icosahedral icosphere, S 1 , with 42 vertices and 80 triangular faces. Icospheres S i , with i the resolution level of the icosphere, are triangulated spherical meshes, where S i+1 is generated from S i by adding vertices at every edge. As input to our model we use icosphere S 7 (163842 vertices).U-Net Architecture. To segment lesions on the icosphere, we created a graphbased re-implementation of nnU-Net [8,9]. Unlike typical imaging data represented on rectangular grids, surface-based data require customised convolutions, downsampling and upsampling steps. Here, we used spiral convolutions [7] which translates standard 2D convolutions to irregular meshes by defining the filter by an outward spiral. This ends up capturing a ring of information around the current node, similar to how a 2d filter captures a ring of information around the current pixel. We use a spiral length of 7, representing the central and adjacent 6 neighbours on a hexagonal mesh, roughly equivalent to a 3 × 3 2D kernel. For downsampling from S i+1 to S i in the U-Net encoder, a similar translation of 2D max pooling is carried out by aggregating over all neighbours of the vertex at the higher-resolution S i+1 . Upsampling from S i to S i+1 in the decoder is implemented via assigning the mean of each vertex in S i to all neighbours at level S i+1 . In total, the U-Net contains seven levels (mirroring the seven icospheres S 7 -S 1 ), and every level consists of three convolutional layers using spiral convolutions and leaky Relu as activation function (Fig. 1).Loss Functions. Following best practices for U-Net segmentation models [8], we use both cross-entropy and dice as loss functions for the segmentation, where y is true labels, ŷ is predicted, n is the number of vertices:Distance Loss. To encourage the network to learn whole-brain context thereby reducing the number of false positives, we added an additional distance regression task. We train the model to predict the normalised geodesic distance d to the lesion boundary for every vertex, by applying an additional loss L dist to the non-lesional prediction, ŷi,0 of the segmentation output for vertex i. We use the mean absolute error loss, weighted by the distance so as not to overly penalise small errors in predicting large distances from the lesion:Classification Loss. To mitigate uncertainty in the correspondence between lesion masks and lesions, we used a weakly-supervised classification loss L class .For the ground truth c, examples were labelled as positive, if any of their vertices were annotated as positive. To predict this sample-level classification, we added a classification head to the deepest level (level 1) of the U-Net. The classification head contained a fully connected layer aggregating over all filters, followed by a fully-connected layer aggregating over all vertices, resulting in the classification output ĉ. This output was trained using cross-entropy:Deep Supervision. To encourage the flow of gradients through the entire U-Net, we use deep supervision at levelsdist be the cross-entropy, dice and distance losses applied to outputs at level i, respectively. The model is trained on a weighted sum of all the losses, with w i ds the loss weight at level i:"
Robust and Generalisable Segmentation of Subtle Epilepsy-Causing Lesions: A Graph Convolutional Approach,2.2,Data Augmentation,"Data augmentations consisted of spatial augmentations and intensity augmentations (Fig. 1), following recommendations outlined in nnU-Net. Spatial augmentation included rotation, inversion and non-linear deformations of the surfacebased data [3]. Intensity-based augmentations included adding a Gaussian noise to the features intensity, adjusting the contrast, scaling the brightness by a uniform factor, and adding a gamma intensity transform.3 Experiments and Results"
Robust and Generalisable Segmentation of Subtle Epilepsy-Causing Lesions: A Graph Convolutional Approach,3.1,Dataset and Implementation Details,"Dataset. For the following experiments, we used a dataset of post-processed surface-based features and manual lesion masks from 618 patients with FCD and 397 controls [10]. This is a heterogeneous, clinically-acquired dataset, collated from 22 international epilepsy surgery centres, including paediatric and adult participants scanned on either 1.5T or 3T MRI scanners. Each centre received local ethical approval from their institutional review board (IRB) or ethics committee (EC) to retrieve and anonymise retrospective, routinely available clinical data. For each participant, MR images were processed using FreeSurfer [4] and 11 surface-based features (cortical thickness, grey-white matter intensity contrast, intrinsic curvature, sulcal depth, curvature and FLAIR intensity sampled as 6 intra-and sub-cortical depths) were extracted. FCDs were manually drawn by neuroradiologists to create 3D regions of interest (ROI) on T1 or fluidattenuated inversion recovery (FLAIR) images. The ROIs were projected onto individual FreeSurfer surfaces and then the features and ROIs were registered to a bilaterally symmetrical template, fsaverage_sym, using folding-based registration. Post-processing included 10 mm full width at half-maximum surface-based smoothing of the per-vertex features, harmonisation of the data using Combat [5] (to account for scanners differences), inter-and intra-individual z-scoring to account for inter-regional differences and demographic differences, and computation of the asymmetry index of each feature. The final surface-based feature set consisted of the original, z-scored and asymmetry features, resulting in 33 input features.In order to compare performance, the train/validation and test datasets were kept identical to those in the previously published vertex-wise classifier [10]. The train/validation cohort comprised 50% of the dataset and 5-fold cross validation was used to evaluate the models. The remaining 50% was withheld for final evaluation and comparison of models. Data from two independent sites (35 patients and 18 controls) were used to test the generalisability of the full model. Hardware: High-performance cluster with Single NVIDIA A100 GPU, 1000 GiB RAM; Software: PyTorch 1.10.0+cu11.1, PyTorch Geometric 2.0.4, Python 3.9.13. Combined memory footprint of model and dataset while training is 49 GB."
Robust and Generalisable Segmentation of Subtle Epilepsy-Causing Lesions: A Graph Convolutional Approach,,Implementation,"Experiments. Using our graph-based adaptation of nnU-Net (GC-nnU-Net) and the previous MLP model as baseline, we ran an ablation study to measure the impact of the proposed auxiliary losses (Table 1). Each model was trained using the train/val cohort 5 times, withholding 20% of the cohort for validation and stopping criteria. Final test performance was computed by ensembling predictions across the 5-fold trained models, with uncertainty estimates calculated through bootstrapping. An additional experiment was carried out subsampling the training cohort at fixed fractions of 0.1, 0.2, 0.3, 0.4, 0.6 and 0.8 using the GC-nnU-Net+dc model (Fig. S2).Evaluations. Model performances were compared according to their Area Under the Curve (AUC), which was calculated by computing the sensitivity and specificity at a range of prediction thresholds. For sensitivity calculations, due to uncertainty in the lesion masks, a lesion was considered detected if the prediction was within 20 mm of the original mask, as this corresponds with the inter-observer variability measured across annotators [10]. Specificity was defined by the presence of false positives in non-lesional examples. As an additional measure of model specificity, the number of false positive clusters in both patients and controls were calculated. Model AUCs were statistically compared using t-tests, with correction for multiple comparisons using the Holm-Sidak method."
Robust and Generalisable Segmentation of Subtle Epilepsy-Causing Lesions: A Graph Convolutional Approach,3.2,Results,"Table 2 compares model performances on the withheld test set. GC-nnU-Net+dc, the graph-based implementation of nnU-Net with additional distance and classification losses, outperformed all other models. Examples of individual predictions using the MLP and GC-nnU-Net+dc model, as well as examples of the predicted geodesic distance from the lesion are presented in Fig. 2A,B. Figure 2C visualises the reduction in number of false positive clusters when using GC-nnU-Net+dc which is reflected in the significantly improved specificity. GC-nnU-Net+dc showed similarly improved specificity relative to the MLP on independent test sites, demonstrating good model generalisability (Table 3). In experiments varying the size of the training cohort, performance increased with sample size until around 220 subjects above which gains were negligible (Fig. S2).  "
Robust and Generalisable Segmentation of Subtle Epilepsy-Causing Lesions: A Graph Convolutional Approach,4,Conclusions and Future Work,"This paper presents a robust and generalisable graph convolutional approach for segmenting focal cortical dysplasias using surface-based cortical data. This approach outperforms specificity baselines by 22-27%, which is driven by three newly-proposed components. First, treating the hemispheric surface as a single connected graph allows the network to model spatial context. Second, a classi-fication loss mitigates the impact of imprecise lesion masks by simplifying the task to predicting whether or not a lesion is present in every hemisphere. Third, a distance-from-lesion prediction task penalises false positives and encourages the network to consider the entire hemisphere. The results show a significant increase in specificity, both in terms of presence of any false positive predictions in non-lesional hemispheres and a reduced number of additional clusters in lesional hemispheres. From a translational perspective, this improvement in performance will increase clinical confidence in applying these tools to cases of suspected FCD, while additionally minimising the number of putative lesions an expert neuroradiologist would need to review. Future work will include systematic prospective evaluation of the tool in suspected FCDs and expansion of these approaches to multiple causes of focal epilepsy."
Exploring Unsupervised Cell Recognition with Prior Self-activation Maps,1,Introduction,"Cell recognition serves a key role in exploiting pathological images for disease diagnosis. Clear and accurate cell shapes provide rich details: nucleus structure, cell counts, and cell density of distribution. Hence, pathologists are able to conduct a reliable diagnosis according to the information from the segmented cell, which also improves their experience of routine pathology workflow [5,14].In recent years, the advancement of deep learning has facilitated significant success in medical images [17,18,20]. However, the supervised training requires massive manual labels, especially when labeling cells in histopathology images. A large number of cells are required to be labeled, which results in inefficient and expensive annotating processes. It is also difficult to achieve accurate labeling because of the large variations among different cells and the variability of reading experiences among pathologists.Work has been devoted to reducing dependency on manual annotations recently. Qu et al. use points as supervision [19]. It is still a labor-intensive task due to the large number of objects contained in a pathological image. With regard to unsupervised cell recognition, traditional methods can segment the nuclei by clustering or morphological processing. But these methods suffer from worse performance than deep learning methods. Among AI-based methods, some works use domain adaptation to realize unsupervised instance segmentation [2,9,12], which transfers the source domain containing annotations to the unlabeled target. However, their satisfactory performance depends on the appropriate annotated source dataset. Hou et al. [10] proposed to synthesize training samples with GAN. It relies on predefined nuclei texture and color. Feng et al. [6] achieved unsupervised detection and segmentation by a mutual-complementing network. It combines the advantage of correlation filters and deep learning but needs iterative training and finetuning.CNNs with inductive biases have priority over local features of the nuclei with dense distribution and semi-regular shape. In this paper, we proposed a simple but effective framework for unsupervised cell recognition. Inspired by the strong representation capability of self-supervised learning, we devised the prior self-activation maps (PSM) as the supervision for downstream cell recognition tasks. Firstly, the activation network is initially trained with self-supervised learning like predicting instance-level contrastiveness. Gradient information accumulated in the shallow layers of the activation network is then calculated and aggregated with the raw input information. These features extracted from the activation network are then clustered to generate pseudo masks which are used for downstream cell recognition tasks. In the inferring stage, the networks which are supervised by pseudo masks are directly applied for cell detection or segmentation. To evaluate the effectiveness of PSM, we evaluated our method on two datasets. Our framework achieved comparable performance on cell detection and segmentation on par with supervised methods. Code is available at https://github.com/cpystan/PSM."
Exploring Unsupervised Cell Recognition with Prior Self-activation Maps,2,Method,"The structure of our proposed method is demonstrated in Fig. 1. Firstly, an activation network U ss is trained with self-supervised learning. After the backpropagation of gradients, gradient-weighted features are exploited to generate the self-activation maps (PSM). Next is semantic clustering where the PSM is combined with the raw input to generate pseudo masks. These pseudo masks can be used as supervision for downstream tasks. Related details are discussed in the following."
Exploring Unsupervised Cell Recognition with Prior Self-activation Maps,2.1,Proxy Task,"We introduce self-supervised learning to encourage the network to focus on the local features in the image. And our experiments show that neural networks are capable of adaptively recognizing nuclei with dense distribution and semi-regular shape. Here, we have experimented with several basic proxy tasks below."
Exploring Unsupervised Cell Recognition with Prior Self-activation Maps,,ImageNet Pre-training:,"It is straightforward to exploit the models pre-trained on natural images. In this strategy, we directly extract the gradient-weighted feature map in the ImageNet pre-trained network and generate prior self-activation maps.Contrastiveness: Following the contrastive learning [3] methods, the network is encouraged to distinguish between different patches. For each image, its augmented view will be regarded as the positive sample, and the other image sampled in the training set is defined as the negative sample. The network is trained to minimize the distance between positive samples. It also maximizes the distance between the negative sample and the input image. The optimization goal can be denoted as:where L dis is the loss function. Z l , Z r , and Z n are representations of the input sample, the positive sample, and the negative sample, respectively. In addition, dif f (•) is a function that measures the difference of embeddings.Similarity: LeCun et al. [4] proposed a Siamese network to train the model with a similarity metric. We also adopted a weight-shared network to learn the similarity discrimination task. In specific, the pair of samples (each input and its augmented view) will be fed to the network, and then embedded as highdimensional vectors Z l and Z r in the high-dimensional space, respectively. Based on the similarity measure sim(•), L dis is introduced to reduce the distance, which is denoted as,Here, maximizing the similarity of two embeddings is equal to minimizing their difference."
Exploring Unsupervised Cell Recognition with Prior Self-activation Maps,2.2,Prior Self-activation Map,"The self-supervised model U ss is constructed by sequential blocks which contain several convolutional layers, batch normalization layers, and activation layers.The self-activation map of a certain block can be obtained by nonlinearly mapping the weighted feature maps A k :where I am is the prior self-activation map. A k indicates the k-th feature map in the selected layer. α k is the weight of each feature map, which is defined by global-average-pooling the gradients of output z with regard to A k :where i, j denote the height and width of output, respectively, and N indicates the input size. The obtained features are visualized in the format of the heat map which is later transformed to pseudo masks by clustering.Semantic Clustering. We construct a semantic clustering module (SCM) which converts prior self-activation maps to pseudo masks. In SCM, the original information is included to strengthen the detailed features. It is defined as:where I f denotes the fused semantic map, I raw is the raw input, β is the weight of I raw .To generate semantic labels, an unsupervised clustering method K-Means is selected to directly split all pixels into several clusters and obtain foreground and background pixels. Given the semantic map I f and its N pixel featuresThe goal is to find S to reach the minimization of withinclass variances as follows:where c i denotes the centroid of each cluster S i . After clustering, the pseudo mask I sg can be obtained."
Exploring Unsupervised Cell Recognition with Prior Self-activation Maps,2.3,Downstream Tasks,"In this section, we introduce the training and inferring of cell recognition models.Cell Detection. For the task of cell detection, a detection network is trained under the supervision of pseudo mask I sg . In the inferring stage, the output of the detection network is a score map. Then, it is post-processed to obtain the detection result.The coordinates of cells can be got by searching local extremums in the score map, which is described below:where T (m,n) denotes the predicted label at location of (m, n), p is the value of the score map and D (m,n) indicates the neighborhood of point (m, n). T (m,n) is exactly the detection result.Cell Segmentation. Due to the lack of instance-level supervision, the model does not perform well in distinguishing adjacent objects in the segmentation.To further reduce errors and uncertainties, the Voronoi map I vor which can be transformed from I sg is utilized to encourage the model to focus on instance-wise features. In the Voronoi map, the edges are labeled as background and the seed points are denoted as foreground. Other pixels are ignored.We train the segmentation model with these two types of labels. The training loss function can be formulated as below,where λ is the partition enhancement coefficient. In our experiment, we discovered that false positives hamper the effectiveness of segmentation due to the ambiguity of cell boundaries. Since that, only the background of I sg will be concerned to eliminate the influence of false positives in instance identification."
Exploring Unsupervised Cell Recognition with Prior Self-activation Maps,3.1,Implementation Details,"Dataset. We validated the proposed method on the public dataset of Multi-Organ Nuclei Segmentation (MoNuSeg) [13] and Breast tumor Cell Dataset (BCData) [11]. MoNuSeg consists of 44 images of size 1000 × 1000 with around 29,000 nuclei boundary annotations. BCData is a public large-scale breast tumor dataset containing 1338 immunohistochemically Ki-67 stained images of size 640 × 640. Evaluation Metrics. In our experiments on MoNuSeg, F1-score and IOU are employed to evaluate the segmentation performance. Denote T P , F P , and F N as the number of true positives, false positives, and false negatives. Then F1score and IOU can be defined as: F 1 = 2T P/(2T P + F P + F N), IOU = T P/(T P + F P + F N). In addition, common object-level indicators such as Dice coefficient and Aggregated Jaccard Index (AJI) [13] are also considered to assess the segmentation performance.In the experiment on BCData, precision (P), recall (R), and F1-score are used to evaluate the detection performance. Predicted points will be matched to ground-truth points one by one. And those unmatched points are regarded as false positives. Precision and recall are: P = T P/(T P + F P ), and R = T P/(T P + F N). In addition, we introduce MP and MN to evaluate the cell counting results. 'MP' and 'MN' denote the mean average error of positive and negative cell numbers.Hyperparameters. Res2Net101 [7] is adopted as the activation network U ss with random initialization of parameters. The positive sample is augmented by rotation. The weights β are set to 2.5 and 4 for training in MoNuSeg and BCData, respectively. The weight λ is 0.5. The analysis for β and λ is included in the supplementary. Pixels of the fused semantic map will be decoupled into three piles by K-Means. The following segmentation and detection are constructed with ResNet-34. They are optimized using CrossEntropy loss by the Adam optimizer for 100 epochs with the initial learning rate of 1e -4 . The function dif f (•) is instantiated as the measurement of Manhattan distance.  "
Exploring Unsupervised Cell Recognition with Prior Self-activation Maps,3.2,Result,"This section includes the discussion of results which are visualized in Fig. 2 Segmentation. In MoNuSeg Dataset, four fully-supervised methods Unet [20], MedT [24], CDNet [8], and the competition winner [13] are adopted to estimate the upper limit as shown in the first four rows of Table 1. Two weakly-supervised models trained with only point annotations are also adopted as the comparison.Compared with the method [22] fully exploiting localization information, ours can achieve better performance without any annotations in object-level metrics (AJI). In addition, two unsupervised methods using traditional image processing tools [1,21] and two unsupervised methods [9,10] with deep learning are compared. Our framework has achieved promising performance because robust low-level features are exploited to generate high-quality pseudo masks.Detection. Following the benchmark of BCData, metrics of detection and counting are adopted to evaluate the performance as shown in Table 2. The first three methods are fully supervised methods which predict probability maps to achieve detection. Furthermore, TransCrowd [16] with the backbone of Swin-Transformer is employed as the weaker supervision trained by cell counts regression. By con-  trast, even without any annotation supervision, compared to CSRNet [15], NP-CNN [23] and U-CSRNet [11], our proposed method still achieved comparable performance. Especially in terms of MP, our model surpasses all the baselines. It is challenging to realize multi-class recognition in an unsupervised framework. Our method still achieves not bad counting results on negative cells.Ablation Study. Ablation experiments are built on MoNuSeg. In our pipeline, the activation network can be divided into four layers which consists of multiple basic units including ReLU, BacthNorm, and Convolution. We exploit the prior self-activation maps generated from different depths in the model after training with the same proxy tasks. As shown in Fig. 3, the performance goes down and up with we extracting features from deeper layers. Due to the relatively small receptive field, the shallowest layer in the activation network is the most capable to translate local descriptions We have also experimented with different types of proxy tasks in a selfsupervised manner, as shown in Table 3. We can see that relying on the pretrained models with external data can not improve the results of subsequent segmentation. The model achieves similar pixel-level performance (F1) when learning similarity or contrastiveness. But similarity learning makes the model performs better in object-level metrics (AJI) than contrastive learning. The high intra-domain similarity hinders the comparison between constructed image pairs. Unlike natural image datasets containing diverse samples, the minor inter class differences in biomedical images may not fully exploit the superiority of contrastive learning."
Exploring Unsupervised Cell Recognition with Prior Self-activation Maps,4,Conclusion,"In this paper, we proposed the prior self-activation map (PSM) based framework for unsupervised cell segmentation and multi-class detection. The framework is composed of an activation network, a semantic clustering module (SCM), and the networks for cell recognition. The proposed PSM has a strong capability of learning low-level representations to highlight the area of interest without the need for manual labels. SCM is designed to serve as a pipeline between representations from the activation network and the downstream task. And our segmentation and detection network are supervised by the pseudo masks. In the whole training process, no manual annotation is needed. Our unsupervised method was evaluated on two publicly available datasets and obtained competitive results compared to the methods with annotations. In the future, we will apply our PSM to other types of medical images to further release the dependency on annotations."
Exploring Unsupervised Cell Recognition with Prior Self-activation Maps,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3 54.
A Motion Transformer for Single Particle Tracking in Fluorescence Microscopy Images,1,Introduction,"A commonly used method to observe the dynamics of subcellular structures, such as microtubule tips, receptors, and vesicles, is to label them with fluorescent probes and then collect their videos using a fluorescence microscope. Since these subcellular structures are often smaller than the diffraction limit of visible light, they often appear as individual particles with Airy disk-like patterns in fluorescence microscopy images, as shown e.g., in Fig. 1. To quantitatively study the dynamic behavior of these structures in live cells, these trajectories need to be recovered using single particle tracking techniques [14].Most single particle tracking methods follow a two-step paradigm: particle detection and particle linking. Specifically, particles are detected first in each frame of the image sequence. The detected particles are then linked between consecutive frames to recover their complete trajectories. The contributions of this paper focus on particle linking. Classical particle linking methods [5,9,14] are usually based on joint probability data association (JPDA) [10,20], multiple hypothesis tracking (MHT) [16,19], etc. Many classical methods have been developed and evaluated in the 2012 International Symposium on Biomedical Imaging (ISBI) Particle Tracking Challenge [6]. However, classical methods require manual tuning of many model parameters and are usually designed for a specific type of dynamics, making it difficult to apply to complex dynamics. In addition, the performance of these methods tends to degrade when tracking dense particles. Deep learning provides a technique for automatically learning feature patterns and has been bringing performance improvements to many tasks. Recently, many deep learning-based single particle tracking methods have been developed. Many methods [21,25,26,30] use long short-term memory (LSTM) [13] modules to learn particle behavior. However, in [30], the matching probabilities between each tracklet and its multiple candidates are calculated independently, and there is no information exchange between multiple candidates. In [26,30], only detections in the next frame are used as candidates, which contain fewer motion features compared to hypothetical future tracklets. In [21,25], the number of their subnetworks grows exponentially with the depth of the hypothesis tree, making the network huge. And the trajectories will be disconnected due to missing detections. In addition, the source codes of most deep learning-based single particle tracking methods are not available, making them difficult to use for non-experts.Cell tracking is closely related to particle tracking. There are different classes of cell tracking methods. An important category is tracking-by-evolution [7], which assumes spatiotemporal overlap between corresponding cells. It is not suitable for tracking particles because they generally do not overlap between frames. Another important category is tracking-by-detection. Some methods [18,29] in this category assume coherence in motion of adjacent cells, which is not suitable for tracking particles that move independently from each other. There are also cell tracking methods [2] that rely on appearance features, which are not suitable for tracking particles because they lack appearance features.Transformer [27] was originally proposed for modeling word sequences in machine translation tasks and has been used in various applications [3,4]. Recently, there have been many Transformer-based methods for motion forecasting [11,17,23], which improve the performance of motion forecasting in natural scenes (e.g., pedestrians, cars.). Compared to LSTM, Transformer shows advantages in sequence modeling by using the attention mechanism instead of sequence memory. However, to the best of our knowledge, Transformer has not been used for single particle tracking in fluorescence microscopy images.In this paper, we propose a Transformer-based single particle tracking method MoTT, which is effective for different motion modes and different density levels of subcellular structures. The main contributions of our work are as follows: (1) We have developed a novel Transformer-based single particle tracking method MoTT. The attention mechanism of the Transformer is used to model complex particle behaviors from past and hypothetical future tracklets. To the best of our knowledge, we are the first to introduce Transformer-based networks to single particle tracking in fluorescence microscopy images; (2) We have designed an effective relinking strategy for those disconnected trajectories due to missed detections. Experiments have confirmed that the relinking strategy substantially alleviates the impact of missed detections and enhances the robustness of our tracking method; (3) Our method substantially outperforms competing state-of-the-art methods on the ISBI Particle Tracking Challenge dataset [6]. It provides a powerful tool for studying the complex spatiotemporal behavior of subcellular structures."
A Motion Transformer for Single Particle Tracking in Fluorescence Microscopy Images,2,Method,"Our particle tracking approach follows the two-step paradigm: particle detection and particle linking. We first use the detector DeepBlink [8] to detect particles at each frame. The detections of the first frame are initialized as the live tracklets. On each subsequent frame, we execute our particle linking method in four steps as follows. First (Sect. 2.1), for each live tracklet, we construct a hypothesis tree to generate its multiple hypothesis tracklets. Second (Sect. 2.2), all tracklets are  preprocessed and then fed into the proposed MoTT network to predict matching probabilities between each live tracklet and its multiple hypothesis tracklets, as well as the existence probability and position of each live tracklet in the next frame. Third (Sect. 2.3), we formulate a discrete optimization model to find the overall best matching for all live tracklets by maximizing the sum of the matching probabilities. Finally (Sect. 2.4), we design a track management scheme for trajectory initialization, updating, termination, and relinking."
A Motion Transformer for Single Particle Tracking in Fluorescence Microscopy Images,2.1,Hypothesis Tree Construction,"Assuming that the particle linking has been processed up to frame t. In order to find correspondence between the current live tracklets and the detections of frame t + 1, we will build a hypothesis tree of depth d for each live tracklet, with its detection at frame t as the root node. To build the tree beyond the root node, we select m (real) detections of the next frame nearest to the current node as well as another null detection that represents a missing detection as children of the current node. If the current node is null, m (real) detections of the next frame nearest to the parent of the current node are selected. From the hypothesis tree, (m + 1) d hypothesis tracklets will be obtained. Figure 2 shows an example of the hypothesis tree construction with m = 2 and d = 2."
A Motion Transformer for Single Particle Tracking in Fluorescence Microscopy Images,2.2,MoTT Network,"As shown in Fig. 3, We have designed a Transformer-based network, which contains a Transformer and two prediction head modules: classification head and regression head. Compared to the original Transformer, both the query masking and the positional encoding on the decoder are removed, since the input of the decoder is an unordered tracklet set. The classification head and regression head are constructed by fully connected layers.For the generated tracklets from the previous step, the preprocessing is performed to make the length of all live tracklets equal to Δt, to convert position sequence to velocity sequence, and to add the existence flag making the coordinate dimension n+1. See supplementary material for the details of preprocessing. Then the preprocessed live tracklet is fed into the Transformer encoder, while the (m + 1) d preprocessed hypothesis tracklets are fed into the Transformer decoder. The self-attention modules in the encoder and decoder are used to extract features of live tracklets and hypothesis tracklets, respectively. The cross-attention module is used to calculate the affinity between the live tracklet and its multiple candidate tracklets. The classification head outputs the predicted matching probabilities between the live tracklet and (m + 1) d hypothesis tracklets. The regression head outputs the predicted existence probability and velocity of each live tracklet in the next frame. The existence probability represents the probability of the live tracklet existence in the next frame. The predicted velocity can be easily converted to the predicted position.Training. We train the MoTT network in a supervised way, using the crossentropy (CE) loss to supervise the output of the classification head and the mean square error (MSE) loss to supervise the output of the regression head. The target of classification head output is a class index in the range [0, (m + 1) d ) where (m + 1) d is the number of hypothesis tracklets. The target of regression head output is the ground truth of the concatenation of normalized velocity and the existence flag.Inference. In inference, we add a 1D max-pooling layer following the classification head to select the highest probability of the hypothesis tracklets with the same detection at frame t + 1 as the matching probabilities between the live tracklet and the candidate detection at frame t + 1. Then the (m + 1) predicted matching probabilities are normalized by softmax. The matching probabilities between the live tracklet and other detections besides the m + 1 candidate detections are set to zero."
A Motion Transformer for Single Particle Tracking in Fluorescence Microscopy Images,2.3,Modeling Discrete Optimization Problem,"To find a one-to-one correspondence solution, we construct a discrete optimization formulation as (1), where p ij is the predicted match probabilities between the live tracklet i and the detection j, and a ij ∈ {0, 1} is the indicator variable. In particular, j = 0 represents the null detection. The objective function aims at maximizing the sum of matching probabilities under the constraints that each live tracklet is matched to only one detection (real or null), and each real detection is matched by at most one tracklet. This optimization problem is solved by using Gurobi (a solver for mathematical programming) [12] to obtain a one-to-one correspondence solution."
A Motion Transformer for Single Particle Tracking in Fluorescence Microscopy Images,2.4,Track Management,"The one-to-one correspondence solution generally includes three situations. For each tracklet matched to a real detection, we add the matched real detection to the end of the live tracklet for updating. For each tracklet matched to a null detection, if the predicted existence probability is greater than a threshold p the predicted position is used to substitute for the null detection, else the live tracklet is terminated. In this way, the disconnected tracklets due to missing detections will be kept and be relinked when their detections emerge. For each detection that is not matched to any of the tracklets, a new live tracklet is initialized with this detection. After finishing particle linking on a whole movie, we remove the trajectories of length one, because they are considered false positive detections. See supplementary material for the details of track management."
A Motion Transformer for Single Particle Tracking in Fluorescence Microscopy Images,3,Experimental Results,"Datasets. The performance of our method is evaluated on ISBI Particle Tracking Challenge datasets (ISBI PTC, http://bioimageanalysis.org/track/) [6], which consist of movies of biological particles of four subcellular structures: microtubule tips, vesicles, receptors, and viruses. These movies cover three different particle motion modes, four different SNR levels, three different particle density levels, and two different coordinate dimensions. For each movie in the training set, we use the first 70% frames for training and the last 30% frames for validation.Metrics. Metrics α, β, JSC θ , JSC are used to evaluate the method performance [6]. Metric α ∈ [0, 1] quantifies the matching degree of ground truth and estimated tracks, while β ∈ [0, α] is penalized by false positive tracks additionally compared to α. JSC θ ∈ [0, 1] and JSC ∈ [0, 1] are the Jaccard similarity coefficients for entire tracks and track points, respectively. Higher values of the four metrics indicate better performance.Table 1. Comparison with SOTA methods on microtubule movies of ISBI PTC datasets. Method 5, Method 1, and Method 2 are the overall top-three approaches in the 2012 ISBI Particle Tracking Challenge. See [6] for details of these three methods. ""-"" denotes that results are not reported in the papers. Bold represents the best performance. Trackpy [1], SORT [28], Bytetrack [31]  Implementation Details. In the following experiments, we set the length of live tracklets Δt + 1 = 7, the extension number m = 4, the depth of hypothesis tree d = 2, and the existence probability threshold p equals the mean of predicted existence probabilities of all live tracklets of current frame. See supplementary material for the ablation study on hyperparameters. We retrained the deepBlink network using simulated data generated by ISBI Challenge Track Generator. The MoTT model is implemented using PyTorch 1.8 and is trained on 1 NVIDIA GEFORCE RTX 2080 Ti with a batch size of 64 and an optimizer of Adam with an initial learning rate lr = 10 -3 , as well as betas = (0.9, 0.98) and eps = 10 -9 ."
A Motion Transformer for Single Particle Tracking in Fluorescence Microscopy Images,3.1,Quantitative Performance,"Comparison with the SOTA Methods. We compared our single particle tracking method with other SOTA methods, and the quantitative results on the microtubule scenario are shown in Table 1. Generally, our method outperforms other methods. Example visualization of tracking results can be found in Fig. 1.Comparison Under the Same Ground Truth Detections. Under the ground truth detections, we compare our particle linking method with LAP [14] and KF (Kalman filter) [15]. The results in Table 2 show that our method generally outperforms other methods in both medium-density and high-density cases.Effectiveness for All Scenarios. We perform our particle linking method using ground truth detections on the four scenarios with three density levels in the ISBI PTC dataset. The results (see the supplementary material) demonstrate the effectiveness of our method for both 2D and 3D single particle tracking."
A Motion Transformer for Single Particle Tracking in Fluorescence Microscopy Images,3.2,Robustness Analysis,"There are false positives (FPs) and false negatives (FNs) in actual detection results. Early study shows that FNs affect performance more than FPs [24]. We evaluated the robustness of our method under different FN levels. The receptor particle with medium density is used in this experiment. We randomly drop 5%, 10%, 15%, 20%, 30%, 40%, 50% detections from ground truth detections. As Fig. 4 shows, the tracking performance with the relinking strategy is better than that without the relinking strategy under different FN levels. Therefore, the proposed relinking strategy alleviates the impact of missed detections and enhances the robustness of our tracking method."
A Motion Transformer for Single Particle Tracking in Fluorescence Microscopy Images,4,Conclusion,"In this paper, we proposed a novel Transformer-based method for single particle tracking in fluorescence microscopy images. We exploited the attention mechanism to model complex particle behaviors from past and hypothetical future tracklets. We designed a relinking strategy to alleviate the impact of missed detections due to e.g., low SNRs, and to enhance the robustness of our tracking method. Our experimental results show that our method is effective for all subcellular structures of ISBI Particle Tracking Challenge datasets, which cover different motion modes and different density levels. And our method achieves state-of-the-art performance on the microtubule movies of ISBI PTC datasets.In the future, we will test our method on other live cell fluorescence microscopy image sequences."
Generating Realistic Brain MRIs via a Conditional Diffusion Probabilistic Model,1,Introduction,"The synthesis of medical images has great potential in aiding tasks like improving image quality, imputing missing modalities [30], performing counterfactual analysis [17], and modeling disease progression [9,10,29]. However, synthesizing brain MRIs is non-trivial as they are of high dimension, yet the training data are relatively small in size (compared to 2D natural images). High-quality synthetic MRIs have been produced by conditional models based on real MRI of the same subject acquired with different MRI sequences [2,16,21,27]. However, such models require large data sets (which are difficult to get) and fail to significantly improve data diversity [11,26], i.e., producing MRIs substantially deviating from those in the training data; data diversity is essential to the generalizability of large-scale models [26]. Unconditional models based on Generative Adversarial Networks (GANs) bypass this drawback by generating new, independent MRIs from random noise [1,7]. However, these models often produce lower quality MRIs as they currently can only be trained on lower resolution MRIs or 2D slices due to their computational needs [6,12]. Furthermore, GANbased models are known to be unstable during training and even can suffer from mode collapse [4]. An alternative is diffusion probabilistic models (DPMs) [8,22], which formulate the fine-grained mapping between data distribution and Gaussian noise as a gradual process modeled within a Markov chain. Due to their multi-step, fine-grained training strategy, DPMs tend to be more stable during training than GANs and therefore are more accurate for certain medical imaging applications, such as segmentation and anomaly detection [13,25]. However, DPMs tend to be computationally too expensive to synthesize brain MRI at full image resolution [3,18]. We address this issue by proposing a memory-efficient 2D conditional DPM (cDPM) that relies on learning the interdependencies between 2D slices to produce high-quality 3D MRI volumes. Unlike the sequence of 2D images defining a video, all 2D slices of an MRI are interconnected with each other as they define a 3D volume capturing brain anatomy. Our cDPM learns these interdependencies (even between distant slices) by training an attention network [24] on arbitrary combinations of condition and target slices. Once learned, the cDPM creates new samples while capturing brain anatomy in 3D. It does so by producing the first few slices from random noise and then using those slices to synthesize subsequent ones (see Fig. 1). We show that this computationally efficient conditional DPM can produce MRIs that are more realistic than those produced by GAN-based architectures. Furthermore, our experiments reveal that cDPM is able to generate synthetic MRIs, whose distribution matches that of the training data."
Generating Realistic Brain MRIs via a Conditional Diffusion Probabilistic Model,2,Methodology,"We first review the basic DPM framework for data generation (Sect. 2.1). Then, we introduce our efficient strategy for generating 3D MRI slices (Sect. 2.2) and finally describe the neural architecture of cDPMs (Sect. 2.3)."
Generating Realistic Brain MRIs via a Conditional Diffusion Probabilistic Model,2.1,Diffusion Probabilistic Model,"The Diffusion Probabilistic Model (DPM) [8,22] generates MRIs from random noise by iterating between mapping 1) data gradually to noise (a.k.a., Forward Diffusion Process) and 2) noise back to data (a.k.a., Reverse Diffusion Process).Forward Diffusion Process (FDP). Let real data X 0 ∼ q sampled from the (real data) distribution q be the input to the FDP. FDP then simulates the diffusion process that turns X 0 after T perturbations into Gaussian noise X T ∼ N (0, I ), where N is the Gaussian distribution with zero mean and the variance being the identity matrix I. This process is formulated as a Markov chain, whose transition kernel q(X t |X t-1 ) at time step t ∈ {0, . . . , T } is defined asThe weight β t ∈ (0, 1) is changed so that the chain gradually enforces drift, i.e., adds Gaussian noise to the data. Let α t := 1β t and ᾱt :=Given this closed-form solution, we can sample X t at any arbitrary time step t without needing to iterate through the entire Markov chain.Reverse Diffusion Process (RDP). The RDP aims to generate realistic data from random noise X T by approximating the posterior distribution p(X t-1 |X t ).It does so by going through the entire Markov chain from time step T to 0, i.e.,Defining the conditional distribution p θ (X t-1 |X t ) := N (X t-1 ; μ θ (X t , t), Σ) with fixed variance Σ, then (according to [8]) the mean can be rewritten aswith θ (•) being the estimate of a neural network defined by parameters θ. θ minimizes the reconstructing loss defined by the following expected value E X0∼q,t∈[0,...,T], ∼N (0,I) ||θ (X t , t)|| 2  2 , where || • || 2 is the L2 norm, and X t is inferred from Eq. ( 2) based on X 0 ."
Generating Realistic Brain MRIs via a Conditional Diffusion Probabilistic Model,2.2,Conditional Generation with DPM (cDPM),"To synthetically create high-resolution 3D MRI, we propose an efficient cDPM model that learns the interdependencies between 2D slices of an MRI so that it can generate slices based on another set of already synthesized ones (see Fig. 1).Specifically, given an MRI X ∈ R D×H×W , we randomly sample two sets of slice indexes: the condition set C and the target set P. Let len(•) be the number of slices in a set, then the 'condition' slices are defined as X C ∈ R len(C)×H×W and the 'target' slices as X P ∈ R len(P)×H×W with len(P) ≥ 1. Confining the FDP of Sect. 2.1 just to the target X P , the RDP now aims to reconstruct X P t for each time t = T, T -1, . . . , 0 starting from random noise at t = T and conditioned on X C . Let X t be the subvolume consisting of X P t and X C , then the joint distribution of the Markov chain defined by Eq. (3) now readsObserve that Eq. ( 5) is equal to Eq. ( 3) in case len(C) = 0.To estimate μ θ ( X t , t) as described in Eq. ( 4), we sample arbitrary index sets C and P so that len(C) + len(P) ≤ τ max , where τ max is the maximum number of slices based on the available resources. We then capture the dependencies across slices by feeding the index sets C and P and the corresponding slices (i.e., X C and X P t built from X 0 ∼ q) into an attention network [20]. The neural network aims to minimize the canonical loss function Loss(θ) := E X0∼q, ∼N (0,I),C+P≤τmax,t ||θ (X P t , X C , C, P, t)|| 2 2 .(6)Fig. 2. The architecture of cDPM is a U-shape neural network with skip connections and the input at step 't' are slice indexes {C, P}, condition sub-volume X C , and current target sub-volume X P t .As the neural network can now be trained on many different (arbitrary) slice combinations (defined by C and P), the cDPM only requires a relatively small number of MRIs for training. Furthermore, it will learn short-and longrange dependencies across slices as the spatial distance between slices from C and P varies. Learning these dependencies (after being trained for a sufficiently large number of iterations) enables cDPMs to produce 2D slices that, when put together, result in realistic looking, high-resolution 3D MRIs."
Generating Realistic Brain MRIs via a Conditional Diffusion Probabilistic Model,2.3,Network Architecture,"As done by [8], cDPMs are implemented as a U-Net [19] with a time embedding module (see Fig. 2). We add a multi-head self-attention mechanism [24] to model the relationship between slices. After training the cDPM as in Fig. 1, a 3D MRI is generated in N stages. Specifically, the cDPM produces the initial set of slices of that MRI volume from random noise (i.e., unconditioned). Conditioned on those synthetic slices, the cDPM then runs again to produce a new set of slices. The process of synthetically creating slices based on ones generated during prior stages is repeated until an entire 3D MRI is produced."
Generating Realistic Brain MRIs via a Conditional Diffusion Probabilistic Model,3.1,Data,"We use 1262 t1-weighted brain MRIs of subjects from three different datasets: the Alzheimer's Disease Neuroimaging Initiative (ADNI-1), UCSF (PI: V. Valcour), and SRI International (PI: E.V. Sullivan and A. Pfefferbaum) [28]. Processing includes denoising, bias field correction, skull stripping, and affine registration to a template, and normalizing intensity values between 0 and 1. In addition, we padded and resized the MRIs to have dimensions 128 × 128 × 128 resulting in a voxel resolution of 1.375 mm × 1.375 mm × 1.0 mm. Splitting the MRI along the axial direction results in 2D slices. Note, this could have also been done along the sagittal or coronal direction."
Generating Realistic Brain MRIs via a Conditional Diffusion Probabilistic Model,3.2,Implementation Details,"Our experiments are conducted on an NVIDIA A100 GPU using the PyTorch framework. The model is trained using 200,000 iterations with the AdamW optimizer adopting a learning rate of 10 -4 and a batch size of 3. τ max is set to 20. After the training, cDPM generates a synthetic MRI consisting of 128 slices by following the process outlined in Fig. 1 in N=13 stages. Each stage generates 10 slices starting with pure noise (X C = ∅) and (after the first stage) being conditioned on the 10 slices produced by the prior stage. After training on all real MRIs, we use the resulting conditional DPM to generate 500 synthetic MRIs.  "
Generating Realistic Brain MRIs via a Conditional Diffusion Probabilistic Model,3.3,Quantitative Comparison,"We evaluate the quality of synthetic MRIs based on 3 metrics: (i) computing the distance between synthetic and 500 randomly selected real MRIs via the Maximum-Mean Discrepancy (MMD) score [5], (ii) measuring the diversity of the synthetic MRIs via the pair-wise multi-scale Structure Similarity (MS-SSIM) [12], and (iii) comparing the distributions of synthetic to real MRIs with respect to the 3 views via the Frèchet Inception Distance (FID) [26] (a.k.a, FID-Axial, FID-Coronal, FID-Sagittal).We compare those scores to ones produced by six recently published methods: (i) 3D-DPM [3], (ii) 3D-VAE-GAN [14], (iii) 3D-GAN-GP [6], (iv) 3Dα-WGAN [12], (v) CCE-GAN [26], and (vi) HA-GAN [23]. We needed to reimplement the first 5 methods and used the open-source code available for HA-GAN. 3D-DPM was only able to generate 32 slices at a time (due to GPU limitations) so that we computed its quality metrics by also cropping the corresponding real MRI to those 32 slices. "
Generating Realistic Brain MRIs via a Conditional Diffusion Probabilistic Model,3.4,Results,"Qualitative Results. The center of the axial, coronal, and sagittal views of five MRIs generated by cDPM shown in Fig. 3 look realistic. Compared to the MRIs produced by the other approaches other than 3D-DPM (see Fig. 4), the MRIs of cDPM are sharper; specifically, the gray matter boundaries are more distinct and the scan provides greater anatomical details. As expected, 3D-DPM produced synthetic slices of similar quality as cDPM but failed to do so for the entire MRI.The synthetic MRIs of cDPM shown in Fig. 3 are also substantially different from each other, suggesting that our method could be used to create an augmented data set that is anatomically diverse. Figure 5 further substantiates the claim, which plots the t-SNE embedding [15] of 200 synthetic MRIs (blue) and their closest real counterpart (orange) according to MS-SSIM for each method. Note, matching of all 500 synthetic MRIs was computationally too expensive to perform (takes days to complete per method). Based on those plots, cDPM is the only approach able to generate MRIs, whose distribution resembled that of the real MRIs. This finding is somewhat surprising given that the MRI subvolumes generated by 3D-DPM looked real. Unlike the real data, however, their distributions are clustered around the average. Thus, 3D-DPM fails to diversify the data set even if (in the future) more computational resources would allow the method to generate a complete 3D MRI. Quantitative Results. Table 1 lists the average scores of MS-SSIM, MMD, and FID for each method. Among all models that generated complete MRI volumes, cDPM performed best. Only the absolute difference between the MS-SSIM score of 3D-DPM and the real MRIs was slightly lower (i.e., 0.005) than the absolute difference for cDPM (i.e., 0.006). This comparison, however, is not fair as the MS-SSIM score for 3D-DPM was only computed on 32 slices. Further supporting this argument is that FID-A (the only score computed for the same slice across all methods) was almost 5 times worse for 3D-DPM than cDPM."
Generating Realistic Brain MRIs via a Conditional Diffusion Probabilistic Model,4,Conclusion,"We propose a novel conditional DPM (cDPM) for efficiently generating 3D brain MRIs. Starting with random noise, our model can progressively generate MRI slices based on previously generated slices. This conditional scheme enables training the cDPM with limited computational resources and training data. Qualitative and quantitative results demonstrate that the model is able to produce high-fidelity 3D MRIs and outperform popular and recent generative models such as the CCE-GAN and 3D-DPM. Our framework can easily be extended to other imaging modalities and can potentially assist in training deep learning models on a small number of samples."
Prior-Driven Dynamic Brain Networks for Multi-modal Emotion Recognition,1,Introduction,"In healthcare, affective computing can help measure the psychological state of patients automatically, especially for those with cognitive deficits. For example, the emotional state of hospitalized patients contributes to the early diagnosis of Parkinson's Disease (PD) [11]. In addition, for patients with neurological diseases, since neurological diseases are degenerative in nature, resulting in unstable cognitive function. The patients may not notice the symptoms of their disease, such as changes in their mood. Recent clinical diagnosis standards rely on the patients' self reports of their feelings to emotional disorders, but it may not be very accurate and stable. Therefore, we need to develop data-driven emotion identification method to improve the diagnosis of these disorders.As EEG signals are directly related to high-level cognitive processes, EEGbased emotion recognition draws increasing attention in recent years [1]. Song et al. [15] proposed a dynamic graph convolutional network, which trained neural networks to dynamically learn the internal relationships between different EEG channels and extract more discriminative features. Zhang et al. [23] proposed a self-attention network to jointly model both local and global temporal information of EEG to reduce the effect of noise at the temporal level. These efforts do not take advantage of the complementary information between the modalities, which limits the performance of the model. Recently, a lot of works shown multi-modal data can provide complementary information to improve emotion recognition performance. Wang et al. [20] combined transformer encoders with attention based fusion to integrate EEG and eye movement data for emotion recognition. Ma et al. [10] designed a multi-modal residual long short-term memory network (MMResLSTM) to learn the correlation between EEG and peripheral physiological on multi-modal emotion recognition. However, the above work ignores correlations between EEG channels and fails to provide interpretable fusion model. Brain network analysis has been widely used in the field of disease diagnosis [8,22], which can describe the complex spatial relationships between brain regions of the brain. In recent years, researchers have migrated brain networks into emotion recognition. Wang et al. [21] implemented PageRank algorithm to rank the importance of brain network nodes, and screened important channels in emotion recognition according to the weight of channels. Huang et al. [6] proposed a novel neural decoding framework, which builds a bridge between emotions and brain regions, and captures their relationships by performing embedding propagation. However, the methods mentioned above regard the structure of brain network as static, ignoring that the variability of electrode channel connectivity over time. Since the multi-modal data is obtained from the synchronous stimulus in the same time period, this temporal level dynamic is particularly important in the multi-modal emotion recognition. In addition, integrating the heterogeneous data of EEG and facial expression also poses challenge to multi-modal emotion classification.To overcome the above limitations, we design a spatial-temporal feature extraction framework based on prior-driven dynamic brain networks and apply it to emotion recognition. Specifically, we treat each electrode of EEG as a node of brain network, and then the dynamic functional connectivity networks (DFCNs) is constructed by Pearson correlation coefficient under non-overlapping time window. Besides, we calculate the correlation between EEG and facial expression across modal channels by cross attention mechanism, as the prior knowledge of DFCNs, and then embed it to above model obtain the final DFCNs representation. Finally, we implemented residual blocks and non-local attention to construct STFENet, so as to extract complex spatial-temporal feature and preserve the long-range dependencies in the time series."
Prior-Driven Dynamic Brain Networks for Multi-modal Emotion Recognition,2,Method,"Figure 1 shows the framework of our approach, including four parts, i.e., the construction of dynamic brain networks, the representation and learning of crossmodal correlation between EEG and facial expression, the embedding of correlation into DFCNs as prior knowledge, and the extraction of spatial-temporal features of DFCNs for emotion recognition based on 3D convolutions.Dynamic Brain Networks Construction: Functional Connectivity Networks (FCNs) ignore the temporal changes of brain connectivity. In this paper, we construct Dynamic Functional Connectivity Networks (DFCNs) to solve the above problem. First, each subject's EEG data can be represented as X E ∈ R P ×T ×D , where P represents the number of channels, T is the number of time windows, and D represents the feature dimension. The t-th subsequence feature of P channels can be represented as a matrixwhere x i (t) ∈ R D represents the t-th subsequence feature extracted from the EEG time series of the i-th channel. According to the divided nonoverlapping sliding time window, we build a functional connectivity network (i.e., matrix) by computing Pearson correlation coefficient between EEG from a pair of channels within the t-th time window:where cov denotes the covariance between two vectors, σ xi(t) denotes the standard deviation of vector x i (t), x i (t) and x j (t) represent the EEG of a pair of channels i and j within the t-th time window, respectively. Thus, the original DFCNs of each subjectPrior Knowledge Embedding: Most of the existing multi-modal emotion recognition works aim to extract the features of different modalities respectively for fusion, which always lost the correlation between modalities. Existing studies have found there is high correlation between EEG and facial expression [5,[12][13][14], but it is still challenging to find an appropriate way to fuse them. Therefore, we calculate the correlation of different modality data as prior knowledge to embed the previously constructed DFCN. Specifically, for each subject, where W Q and W K are the parameter matrices used to generate query and key, which are updated through network back-propagation during model training.We determined correlation scores across channel-dimension between modalities based on cross attention, by treating one modality as query and the other as key:where, Cor(E, F ) and Cor(F, E) represents the correlation score between the cross-modality channels, d 1 , d 2 are normalized parameters equal to the dimension of K. It is worth noting that softmax is applied to the scoring weight of the equation Eq. 4. However, softmax proved to be overconfident in its results, which would result in the correlation scores of certain time windows being too high or too low, affecting the reliability of the prior knowledge. Therefore, we improve softmax to softplus to solve this problem while ensuring that the correlation matrix is non-negative. The calculated correlation matrix is as follows:At this point, we obtain the correlation between the cross-modal channels, and use it as the prior knowledge of DFCNs construction. We embed the modified prior knowledge into the previously constructed DFCNs by element-wise product:where represents element-wise product. By the embedding of prior knowledge, we obtain the discriminative DFCNs representations with prior knowledge.Spatial-Temporal Feature Extraction: Different from static brain networks, DFCNs can not only describe brain connectivity, but also contain the temporallevel volatility of brain connectivity. Most of the existing methods focus on extracting the temporal and spatial features of EEG separately, and concat them for feature fusion, which ignores the dynamic variations of electrode connectivity in the temporal dimension. 2D convolution has been widely used in the field of computer vision, but it is challenging to capture information at the temporal level. Previous studies has shown that 3D convolution operations can better model spatial information in continuous sequences [3,19]. So, we introduce 3D convolution to extract spatial-temporal feature of DFCNs simultaneously. Considering the DFCNs representation X ∈ R C×T ×P ×P of each subject, where C is the number of channels, T represents the number of time windows, and P represents the number of electrode channels, then the m-th feature representation of the location (T, P, P ) calculated by 3D convolution in space can be represented as:where σ is the assigned activation function, b m is the deviation, w T ,P ,P c ,m represents the weight of the convolution kernel connected by the c -th stacking channel to the feature representation of the position (T, P, P ), and v T +T ,P +P ,P +P c represents the characteristic value of the c -th stacking channel at the position (T, P, P ). To better capture the spatial-temporal topological structure in DFCNs, inspired by ResNet's remarkable success [4], we build a deeper network by stacking multiple residual blocks. A spatial-temporal feature extraction network (STFENet) is designed to extract spatial-temporal features of the DFCNs. The construction of STFENet is shown in the second half of Fig. 1. A residual block is used as the basic block, which includes two 3D convolutions, two activation functions and a residual connection. 3D Maxpooling is adopted between the multiple stacked residual block.Since the operation of convolution will eventually focus on local areas, longrange dependencies which describe luxuriant emotion-related information will be lost to some extent. To solve this problem, we further introduce non-local block [18] to preserve information after the maxpooling layer. For a given input, non-local attention performs two different transformations:where W θ and W φ is the weight to be learned, which is realized by 3D convolution in this paper. Then, non-local attention uses the self-attention term [17] to calculate the final features with the help of softmax:where g is implemented by 1×1×1 convolution in this paper. Then, the non-local block can be defined as:where ""+x"" denotes the residual connection, and W z represents the weight matrix. By the STFENet, we finally effectively extract the spatial-temporal emotion-related information in prior-driven DFCNs for the identification of emotions. "
Prior-Driven Dynamic Brain Networks for Multi-modal Emotion Recognition,3,Experiment Results,"Emotional Database: The DEAP dataset [9] collected EEG data from 32 healthy participants. The volunteers were asked to watch 40 one-minute videos and collect EEG signals from the subjects. The facial states of the first 22 subjects were recorded simultaneously. All participants rated each video on a 1-9 scale with the indicators, i.e. arousal, valence, dominance. We choose 18 subjects with both EEG signals and a complete facial video for the experiment. Same as many state-of-the-art studies [2], we turn the identification task into binary classification problem by setting the evaluation threshold of 5.Data Pre-processing: For the EEG data, The 32-channel EEG signal with a duration of 63 s is down-sampled to 128HZ, and remove the first 3 s pre-trial baseline. Power spectral density (PSD) features is extracted from 3 s time windows with non-overlap through the Welch method in EEG, and 5 frequency bands are adopted, i.e. theta (4-8 Hz), slow alpha (8-10 Hz), alpha (8-12 Hz), beta (12-30 Hz), and gamma (30+ Hz) [9]. For the facial video data, referring to [12], we utilize OPENFACE to extract expression features from facial videos, including 3 face positions relative to the camera, 3 head position, 6 eye gaze directions and 17 facial action units. Similar to EEG, the face sequences are divided according to the 3-second non-overlapping sliding time window and the average value of each feature is taken.Experiment Settings: In our experiment, we adopt the leave one subject out (LOSO) cross-validation strategy to verify the effectiveness of our method. Specifically, the samples are divided into 18 non-overlapping parts according to the subjects. The samples of one subject are selected as the test set while the remaining subjects are selected as the training set for each cross-validation. This process is repeated 18 times, and the average performance of the crossvalidation is taken as the final result. Identification performance is measured by accuracy (ACC) and F1-Score. The proposed method is based on the Pytorch implementation, and the model mentioned in this study is trained on a single GPU (NVIDIA GeForce RTX3080). Adam algorithm is used to optimize this method, and the learning rate and batch size are set to 0.001 and 40, respectively.Results and Discussion: We evaluate the performance of our method by calculating ACC and F1 on both valence and arousal. We also compare our method with many comparison methods, which can be divided into two categories: EEG based methods and multi-modal based methods. More specifically, EEG based methods are Support vector machine (SVM), GraphSleepNet (GSN) [7], Dynamical Graph-CNN (DGCNN) [15]. Multi-modal based methods include Multi-kernel learning (MKL), Deep-CCA (DCCA), MMResLSTM [10]. Emotion transformer fusion (ETF) [20]. For quantitative results in Table 1, firstly, most of the multi-modal based methods achieve higher performance than EEGbased methods, which shows the advantage of complementary information from multiple modalities. Secondly, our proposed method achieve the best emotion recognition performance. On valence and arousal, the average ACC and F1 of our method reached 67.36%, 69.17% and 68.47% and 74.68% respectively. The main reason for the superiority of our method is that we can not only use multimodal data as prior knowledge to guide the construction of DFCNs, but also extract discriminate spatial-temporal features. To evaluate the effectiveness of the different modules of our framework, we conduct several ablation experiments on DEAP dataset. Our method mainly contains two modules, Prior knowledge embedding (PKE) module and STFENet. Besides, we also evaluate the contribution of non-local block in STFENet. As can be seen in Table 2, every module used in our framework greatly improve the performance compared with baseline model, with an increase of 6.25%, 4.36% and 7.56%, 9.58% for ACC and F1 on valence and arousal, respectively. It can be seen that both non-local block and STFENet demonstrate the better performance of our proposed method. The reason lies in that STFENet is able to extract complex spatial-temporal feature, and non-local block of STFENet helps it preserve the long-range dependencies in the time series. Moreover, when we remove PKE module from our method, there comes a performance degradation. It suggested that the prior knowledge has vital guiding significance for the construction of DFCNs, so that it can better express emotion-related information. In addition, to further verify the feasibility of the prior knowledge embedded in our method, we visualize the facial expression of subject over several time windows and its channel correlation with EEG, as shown in Fig. 2. Firstly, from the channel correlation topographic map, the deeper the red of the brain area, the higher the correlation between EEG and facial expression. Conversely, the deeper the blue, the lower the correlation. On valence and arousal, high correlation areas focus on the binaural and prefrontal regions, which is in line with existing medical cognition [16]. As the stimulation method adopted by DEAP dataset is musical stimulation, the binaural region is highly activated. The prefrontal lobe plays a crucial role in emotional mobilization [2]. The experimental results show that our method can mine electrode channels that are highly correlated with emotion to provide prior knowledge to guide the construction of dynamic brain networks. Combined with the experimental results after removing PKE of our method in Table 2, it can be seen that embedding prior knowledge can achieve better emotion recognition performance. Therefore, the prior knowledge can better describe emotion-related information."
Prior-Driven Dynamic Brain Networks for Multi-modal Emotion Recognition,4,Conclusion,"In this paper, we develop a spatial-temporal feature extraction framework based on prior-driven DFCNs for multi-modal emotion recognition. In our approach, not only the connectivity between EEG channels but also the dynamics of connectivity over time are jointly learned. Besides, we also calculate the correlation across modalities via cross attention to guide the construction of DFCNs. In addition, we build STFENet based on 3D convolution to model the spatial-temporal features contained in DFCNs to extract emotion-related spatial-temporal information and preserve the long-range dependencies in the time series. Experimental results show that our method outperforms the state-of-the-art methods."
Maximum-Entropy Estimation of Joint Relaxation-Diffusion Distribution Using Multi-TE Diffusion MRI,1,Introduction,"Diffusion magnetic resonance imaging (dMRI) is sensitive to water diffusion in biological tissue. Analytical models of dMRI signals have played an essential role in quantifying tissue microstructure in clinical studies. Several methods have been developed to use dMRI signals measured with a single or multiple b-values to estimate compartment-specific diffusivity or diffusion propagators [1,20,24]. But these methods are developed using dMRI data acquired with a fixed echo time (TE). Several studies have shown that joint modeling of dMRI with multiple TE can probe TE-dependent diffusivity and [23], tissue-specific T 2 relaxation rate [14] and the joint relaxation diffusion distribution (RDD) in each voxel [2,3,12,19]. More specifically, RDD functions describe the multidimensional distribution of T 2 relaxation rate and diffusivity in each voxel, providing a general framework to characterize heterogeneous tissue microstructure. The RDD functions were first applied to measure the structure of porous media [4,6,8,11]. It was generalized in [2,3,12] to probe the microstructure of biological tissue using rdMRI. A standard approach for estimating RDD functions is to represent the measurement signal using basis functions of different diffusivity and relaxation rates which may lead to biased estimation results because of the strong coupling between basis signals.This work introduces the maximum entropy (ME) estimation method for more accurate estimation of RDD functions by adapting theories and techniques developed for the classical Hausdorff moment problems [10,13,15,18]. ME estimation is also a standard approach for high-resolution power spectral estimation of time series data which involves a similar trigonometric moment problem [7,21]. The ME power spectral estimation usually performs better than Fourier transform-based methods [21], which motivates this work to derive ME methods for estimating RDD functions and compare the results with basis function-based methods. To this end, we first show that the problem of estimation RDD function is equivalent to the multivariate Hausdorff moment problem by applying a change of variables. Three formulations of maximum entropy (ME) estimation problems are proposed to estimate ME-RDD functions in different parameter spaces. The performance of these methods is compared with results based on basis functions using simulations and in vivo data. "
Maximum-Entropy Estimation of Joint Relaxation-Diffusion Distribution Using Multi-TE Diffusion MRI,2.1,On the Hausdorff Moment Problem,"where x • θ denotes the inner product between x and θ. represents the set of all feasible indices. Next, let s k := s(x k ) which satisfies thatwhere p(θ) = e -xmin•θ p(θ) is a scaled RDD function adjusted based on the non-zero minimum TE.The Hausdorff moment problem focuses on the existence of distribution functions that satisfy a sequence of power moments [10,15]. To change s k to power moments as in the Hausdorff moment problem, we define γ := [e -δ b θ1 , e -δtθ2 ], which takes value in the interval Γ := [e -δ b D0 , 1] × [e -δtR0 , 1]. For a vector k, we define γ k = i γ ki i = e -x k •θ following the convention in [17]. Then Eq. ( 4) can be expressed using the new variables aswith). Thus, s k can be considered as the power moments of the density function f (γ) on the interval Γ. Therefore the problem of estimating RDD functions using finite rdMRI measurements is equivalent to a multivariate Hausdorff moment problem. We note that the unit interval is usually considered in Hausdorff moment problems. This can be obtained by changing the variable]. Thus γ takes the value on the unit interval I 2 whose moments can be computed using linear transforms of s k . To simplify notations, the analysis in the following subsection will be based on s k and the distribution of γ."
Maximum-Entropy Estimation of Joint Relaxation-Diffusion Distribution Using Multi-TE Diffusion MRI,2.2,Maximum-Entropy Estimation,"The RDD functions that satisfy the rdMRI data may not be unique. In moment problems, the maximum entropy (ME) method is a standard approach to estimate probability distributions and power spectral density functions [15]. Based on the three representations of rdMRI data in Eqs. (3), ( 4) and ( 5), three optimization problems are introduced to estimate ME-RDD functions below.The first ME problem is developed based on Eq. (3) as below:where the objective function is the Shannon differential entropy of p(θ). The solutions to ME problems have been extensively investigated in moment problems [15]. Using the Lagrangian method, one can derive that the optimal solution has the following formfor some coefficients λ k with k ∈ K. The optimal parameters λ k need to be solved to satisfy the constraints in Eq. ( 6).Based on Eq. ( 4), the second ME problem is introduced as below:where p(θ) is the pre-scaled RDD to adjust for the nonzero t min . The optimal solution to Eq. ( 8) has the following formIt is noted that Eq. ( 9) does not include the constant -1 as in Eq. ( 7) since it can be absorbed by the variable λ 0 . Then, pME2 (θ) can be scaled back to the original RDD function by, which has a different form than the solution in Eq. (7).The third ME-RDD is estimated based on the new variable γ as in Eq. ( 5) by solving the following problem:ΓBy changing the variable γ back to θ, Eq. ( 10) is transformed toIt is interesting to note that the above objective function is equal to minimizing the Kullback-Leibler divergence, i.e., the relative entropy, between p(θ) and δ b δ t e -δ •θ . The optimal solution has the following formThen, pme3 λ (θ) is scaled back to obtainIt is noted the difference between p me1 λ (θ) and p me2 λ (θ) is related to the nonzero offset x min . The two solutions are equal if the shortest TE is zero, but it is impossible in practice. The difference between p me2 λ (θ) and p me3 λ (θ) is related to the sampling rate δ b and δ t . The difference is less significant, with a higher sampling rate in the b-value and TEs."
Maximum-Entropy Estimation of Joint Relaxation-Diffusion Distribution Using Multi-TE Diffusion MRI,2.3,Dual Energy Minimization Problems,"The optimal values of λ k in Eqs. ( 7), ( 9) and ( 12) can be obtained by solving the dual formulation of the ME problems, which can be expressed as energy minimization problems based on the dual formulations.For the solution in Eq. ( 7), the corresponding energy function, i.e., the dual objective function, is given byIt can be shown thatThus, the Hessian matrix of Δ 1 (λ) positive definite, indicating that Δ 1 (λ) is a convex function. If Δ 1 (λ) has a finite minimizer, then the minimizer satisfies thatTherefore, the optimal parameters for p me1 λ (θ) can be obtained from the minimizer of Δ 1 (λ).The optimal λ for Eq. ( 9) and Eq. ( 12) can be obtained by minimizing the following to convex energy functions:In this paper, the energy minimization problem was solved using a customized Newton algorithm with the Armijo line-search method [5]. The code and data used in this work are available at https://github.com/LipengNing/ME-RDD."
Maximum-Entropy Estimation of Joint Relaxation-Diffusion Distribution Using Multi-TE Diffusion MRI,3.1,Synthetic Data,"The proposed algorithms were examined using synthetic rdMRI data with an RDD function consisting of three Gaussian components with the mean at (1.5 µm 2 /ms, 10 ms -1 ), (0.5 µm 2 /ms, 40 ms -1 ), and (1.5 µm 2 /ms, 40 ms -1 ) with the volume fraction being 0.2, 0.5 and 0.3, respectively. D and R were uncorrelated in each component, with the standard deviation being 0.01 µm 2 /ms and 5ms -1 . Simulated rdMRI signals had b-values at b = 0, 0.5 ms/μm 2 , . . . , 5 ms/µm 2 and TEs at t = 50 ms, 75 ms, . . . , 200 ms, which can be achieved using an advanced MRI scanner for in vivo human brains such as the connectom scanner [9]. Then, independently and identically distributed Gaussian noise was added to simulated rdMRI signals with an average signal-to-noise ratio (SNR) from 100 to 600, similar to the range of the SNR of direction-averaged dMRI signals of in vivo human brains that scale according to the square root of the number of directions and voxel size."
Maximum-Entropy Estimation of Joint Relaxation-Diffusion Distribution Using Multi-TE Diffusion MRI,3.2,Comparison Methods,"For comparison, we applied the basis-function representation method, similar to the methods in [12], by representing the signals aswhere θ n are a set of predefined points on a discrete grid in Θ. We solved a constrained L 2 minimization problem to find the optimal non-negative coefficient c n with minimum L 2 norm. To evaluate the performances, we computed the error of the center of mass (CE) of the estimated RDD functions in three regions defined using a watershed clustering approach; see the top left figure in Fig. 2.The CE in diffusivity and re Moreover, we also computed the volume-fraction error (VFE), i.e., VFE = K k=1 |F est (k)-F true (k)|, of the estimated RDD, where F est (k) and F true (k) denote the true and estimated volume fraction for each component."
Maximum-Entropy Estimation of Joint Relaxation-Diffusion Distribution Using Multi-TE Diffusion MRI,3.3,In Vivo rdMRI,"The proposed ME algorithms were applied to an in vivo rdMRI dataset acquired in our previous work [16]. The data was acquired from a healthy volunteer on a 3T Siemens Prisma scanner with the following parameters: voxel size = 2.5×2.5× 2.5mm 3 , matrix size = 96×96×54, TE = 71, 101, 131, 161 and 191 ms, TR=5.9 s, b = 700, 1400, 2100, 2800, 3500 s/mm 2 along 30 gradient directions together with 6 volumes at b = 0, simultaneous multi-slice (SMS) factor = 2, and iPAT = 2. The pulse width of the diffusion gradients and the diffusion time were fixed across scans. An additional pair of b=0 images with anterior-posterior (AP), and posterior-anterior (PA) phase encoding directions were acquired for distortion correction using FSL TOPUP/eddy. Then the data were further processed using the unring [22] tool."
Maximum-Entropy Estimation of Joint Relaxation-Diffusion Distribution Using Multi-TE Diffusion MRI,4,Results,"The first two figures in Fig. 1 show the error in the center of mass of D and R. The L2 approach based on basis functions had significantly overestimated D  and R, whereas the three ME methods had much lower estimation error and similar performance to each other. The L2 method also had a biased estimation of the volume fraction, as shown in the third figure. Figure 2 shows the true and estimated RDD functions. The L2-RDD has more spread and biased distributions compared to the true distribution. The three ME-RDD functions were more focal and less biased.  "
Maximum-Entropy Estimation of Joint Relaxation-Diffusion Distribution Using Multi-TE Diffusion MRI,5,Summary,"In summary, this work introduced a maximum-entropy framework for estimating the relaxation-diffusion distribution functions using rdMRI. To our knowledge, this is the first work showing that the estimation of multidimensional RDD functions is equivalent to the classical multivariate Hausdorff moment problem. Although this work focuses on the two dimensional RDD functions, the results generalize to the special cases for one dimensional relaxation or diffusion distribution functions. The contributions of this work also include the development of three algorithms to estimate RDD functions and the comparisons with the standard basis-function approach. The ME-RDD functions can be estimated using convex optimization algorithms. Experimental results have shown that the proposed methods provide more accurate parameters for each component and more accurate volume fractions compared to the standard basis function methods. Moreover, results based on in vivo data have shown that the proposed ME-RDD can resolve multiple components that cannot be distinguished by the basis function approach. The better performance ME-RDD functions compared to basis-function methods may relate to the superior performance of ME spectral estimation methods compared to Fourier transform-based methods [21]. We expect the improved spectral resolution will be useful in several clinical applications such as lesion and tumor detection. But further theoretical analysis on the performance of the ME methods is needed in future work. Moreover, further histological validations are needed to examine the biological basis of the RDD functions. Finally, we note a limitation of the proposed method is that the optimization algorithm may have a slow convergence speed because the Hessian matrices may not be well conditioned. Moreover, the results may be sensitive to measurement noise. Thus faster and more reliable computation algorithms will be developed in future work."
PAS-Net: Rapid Prediction of Antibiotic Susceptibility from Fluorescence Images of Bacterial Cells Using Parallel Dual-Branch Network,1,Introduction,"In recent years, the overuse and misuse of antibiotics have led to an increase in the rate of bacterial antibiotic resistance worldwide [1,2]. The increasing number of multidrug resistant strains not only poses a serious threat to human health, but also poses great difficulties in clinical anti-infection treatment [3]. To address this issue, clinicians rely on antibiotic susceptibility testing (AST) to determine bacterial susceptibility to antibiotics, thus guiding rational drug use. However, the traditional AST method requires overnight culture of the bacteria in the presence of antibiotics, which is time-consuming and laborious (usually 24-72 h). Such delays prevent physicians from determining effective antibiotic treatments promptly. Therefore, there is an urgent clinical need for a rapid AST method that allows physicians to prescribe appropriate antibiotics in an informed manner, which is essential to improve patient outcomes, shorten the treatment duration, and slow down the progression of bacterial resistance.In this paper, we take Pseudomonas aeruginosa (PA) as the research object and observe the difference in shape and distribution of bacterial aggregates formed by sensitive and multi-drug resistant bacteria through fluorescent images, so we want to use image recognition technology to distinguish these two types of bacteria for the purpose of rapid prediction of antibiotic susceptibility. However, we recognize that this classification task presents several challenges (as shown in Fig. 1). Firstly, in images of sensitive PA and multi-drug resistant Pseudomonas aeruginosa (MDRPA), inter-class variation is low, but intra-class variation is high. Secondly, some images have exposure problems due to the high intensity of bacterial aggregation. Lastly, there are low signal-to-noise ratio of the images, coupled with possible image artifacts resulting from inhomogeneous staining or inappropriate manipulation. In recent years, deep learning techniques have made a splash in the field of image recognition with their impressive performance and have provided powerful support for a wide range of applications in biomedical research and clinical practice. Notably, deep learning methods based on convolutional neural network (CNN, e.g., ResNet [4], ResNeXt [5], ResNeSt [6]) are widely used in microscopic image classification tasks. For instance, Waisman et al. [7] utilized transmission light microscopy images to train a CNN to distinguish pluripotent stem cells from early differentiated cells. Riasatian et al. [8] proposed a novel network based on DenseNet [9] and fine-tuned and trained it with various configurations of histopathology images. Recently, due to the successful application of ViT [10] to image classification tasks, many research efforts (e.g., DeiT [11], PVT [12], Swin Transformer [13]) have attempted to introduce the power of selfattention mechanism [14] into computer vision. For example, He et al. [15] applied a spatial pyramidal Transformer network to learn long-range contextual information for skin lesion analysis for skin disease classification.The above studies show that two deep learning frameworks, CNN and Transformer, are effective in microscopy image classification tasks. CNN is good at extracting local features, but its receptive field is limited by the size of the convolution kernel and cannot effectively capture the global information in the image. Meanwhile, in visual Transformer, its self-attention module is good at capturing feature dependencies over long distances, but ignores local feature information. However, these two kinds of feature information are very important for the classification of microscope images with complex features. To tackle this issue, this paper builds a hybrid model that maximizes the advantages of CNN and Transformer, thus enhancing the feature representation of the network. To achieve the complementary advantages of these two techniques, we propose a parallel dual-branch network named PAS-Net, specifically designed to enable rapid prediction of bacterial antibiotic susceptibility. The main contributions of this study are as follows:1) We develop a parallel dual-branch classification network to realize the interactive learning of features throughout the whole process through feature interaction unit (FIU), which can better integrate local features of CNN branch (C-branch) and global representations of Transformer branch (T-branch).2) We propose a more efficient hierarchical multi-head self-attention (HMSA) module, which utilizes a local-to-global attention mechanism to simulate the global information of an image, while effectively reducing the computational costs and memory consumption.To the best of our knowledge, this study represents the first attempt to use deep learning techniques to realize rapid AST based on PA fluorescence images, which provides a new perspective for predicting bacterial antibiotic susceptibility.  Feature dimension mismatch exists between feature map from C-branch and vector sequence from T-branch. Therefore, our network use FIU as a bridge to effectively combine the local features and the global representation in an interactive manner to eliminate the misalignment between the two features, as shown in Fig. 3."
PAS-Net: Rapid Prediction of Antibiotic Susceptibility from Fluorescence Images of Bacterial Cells Using Parallel Dual-Branch Network,,CNN→Transformer:,"The feature map is first aligned with the dimensions of the patch embedding by 1 × 1 convolution. Then, the feature resolution is adjusted using the downsampling module to complete the alignment of the spatial dimensions. Finally, the feature maps are summed with the patch embedding of the T-branch.Transformer→CNN: After going through the HMSA module and FFN, the patch embedding is fed back from the T-branch to the C-branch. An up-sampling module needs to be used first for the patch embedding to align the spatial scales. The patch embedding is then aligned to the number of channels of the feature map by 1 × 1 convolution, and added to the feature map of the C-branch."
PAS-Net: Rapid Prediction of Antibiotic Susceptibility from Fluorescence Images of Bacterial Cells Using Parallel Dual-Branch Network,1.3,Hierarchical Multi-head Self-attention,"Figure 4 shows the detailed structure of HMSA module. To be able to compute attention in a hierarchical manner, we reshape the input patch embedding E back to the patch map E p . Firstly, the patch map E p is divided into small grids of size G × G, i.e., each grid contains G × G (set G = 4 in this paper) pixel points. Then, a 1 × 1 pointwise convolution is performed on E p to obtain three matricesrespectively, where W Q , W K and W V are three learnable weight matrices with shared parameters that are updated together with the model parameters during training. After that, we compute local attention A 0 within each small grid using the self-attention mechanism, which can be defined as:Then Eq. ( 1) is applied once more on the basis of A 0 to obtain global attention A 1 . We reshape them back to the shape of the input E p . The final output of HMSA is(The original MSA module computes attention map over the entire input feature, and its computational complexity scale quadratically with spatial dimension N, which can be calculated as:(In contrast, our HMSA module computes attention map in a hierarchical manner so that A 0 and A 1 are computed within small G × G grids. The computational complexity of HMSA isWith this approach, only a limited number of image blocks need to be processed in each step, thus significantly reducing the computational effort of the module from O(N 2 ) to O(NG 2 ), where G 2 is much smaller than N. For example, the size of the input image is 224 × 224, if the patch is divided according to the size of 4 × 4, the division will get (224 / 4) 2 = 3136 patches, i.e., N = 3136. However, we set G to 4, so the computational complexity of the HMSA module is greatly reduced. "
PAS-Net: Rapid Prediction of Antibiotic Susceptibility from Fluorescence Images of Bacterial Cells Using Parallel Dual-Branch Network,2.1,Experimental Setup,"The fluorescent images of PA come from a local medical school. We screen out 12 multidrug resistant strains and 11 sensitive strains. Our dataset has 2625 fluorescent images of PA, 1233 images of sensitive PA and 1392 images of MDRPA. We randomly divide the data into a training set and a test set in a ratio of 9:1. To better train the network model and prevent overfitting, we perform five data enhancement operations on each image, including horizontal flip, vertical flip and rotation at different angles (90°, 180°, 270°). Finally, our data volume is expanded to 15,750 images, including 14,178 training images and 1,572 test images.To achieve comprehensive and objective assessment of the classification performance of the proposed method, we select eight classification evaluation metrics, including accuracy (Acc), precision (Pre), recall, specificity (Spec), F1-score (F1), Kappa, area under the receiver operating characteristic (ROC) curve (AUC). All experiments are implemented by configuring the PyTorch framework on NVIDIA GTX 2080Ti GPU with 11 GB of memory."
PAS-Net: Rapid Prediction of Antibiotic Susceptibility from Fluorescence Images of Bacterial Cells Using Parallel Dual-Branch Network,2.2,Results,"In this paper, we adopt Conformer [17] as the baseline of our network, and then make adjustments and improvements to optimize its performance on the PA fluorescence image dataset. Table 1 shows the results of the ablation experiments for different modules in the network. Among them, ""Baseline + CB"" indicates that the ResNet block in the original C-branch is replaced by the ConvNeXt block, reflecting the impact of the performanceenhanced C-branch on the classification performance. ""Baseline + CB + Stem"" replaces the convolutional module of the standard ResNet network in baseline with the Stem module on top of the modified C-branch. ""Baseline + CB + Stem + HMSA"" represents the replacement of the traditional MSA module in Baseline with the efficient HMSA module proposed in this paper on the basis of ""Baseline + CB + Stem"". The proposed HMSA module replaces the traditional MSA module in baseline, which achieves the improvement of network efficiency and classification performance.In order to evaluate the classification performance of the proposed method, we choose ten state-of-the-art image classification methods for comparison, including 5 CNN networks: ResNet50 [4], ResNeXt50 [5], ResNeSt50 [6], ConvNeXt-T [16] and DenseNet121 [9], and 5 Transformer-related networks: ViT-B/16 [10], DeiT-S [11], PVT-M [12], Swin-T [13] and CeiT-S [18]. The results of the comparative experiment are illustrated in Table 2. We can observe that our dual-branch network achieves the best performance on our dataset, and outperforms the CeiT-S by 7.08%, 6.7%, and 6.66% in accuracy, recall and F1-score, respectively.To further analyze and compare the computational complexity of different methods, we compare the number of model parameters (#Param) and the number of floating-point operations per second (FLOPs). In general, the higher the number of parameters and operations, the higher the performance of the model, but at the same time, the greater the computational and storage overhead. It can be seen that the accuracy of ViT is 5% lower than that of ResNet50, but its model complexity is about three times higher. The number of model parameters of PVT-M is similar to that of our PAS-Net, but the accuracy is much worse. The number of parameters of our proposed PAS-Net is 43.4M and FLOPs is 23.37G, indicating that the network achieves a good balance between the number of parameters, FLOPs, accuracy and classification consistency.  To verify the interpretability of the proposed PAS-Net and understand its classification effect more intuitively and effectively, we visualize the results using Grad-CAM, as shown in Fig. 5. From the second column vertically, we can see that the C-branch only focuses on local edge parts or incorrectly highlights some regions that are not relevant to the discrimination, as shown in the heat map in the first and second rows. From the third column, we can see that the T-branch can obtain the global attention map, but at the same time it produces some worthless and redundant features. A sideby-side comparison shows that the heat map in the fourth column can focus well on some discriminative regions with distinct features and reflect the correlation between local regions. For example, our network can effectively capture the bacterial aggregates with clear edges and largest area in the first image, and also establish the long-range feature dependencies among the three small bacterial aggregates near the lower right corner; for the second image, our dual-branch network corrects the error of focusing the C-branch to the exposure position because of the Transformer's ability to learn the global feature representation For the third and fourth images, the network nicely combines the discriminative regions focused on by the C-branch and T-branch, capturing both the local features of larger bacterial aggregates and learning the distributional dependencies among bacterial aggregates. This shows to some extent that our proposed model effectively exploits the advantages of CNN and Transformer and maximizes the retention of local features and global representation.We also use the t-SNE dimensionality reduction algorithm to map the feature vectors learned from the last feature extraction layer of different networks onto a twodimensional plane, as shown in Fig. 6. The visualization allows us to observe the clustering of the image features extracted by these networks. Compared with other models, the features extracted by our proposed PAS-Net can better distinguish the sensitive bacteria (blue) from the multi-drug resistant bacteria (pink)."
PAS-Net: Rapid Prediction of Antibiotic Susceptibility from Fluorescence Images of Bacterial Cells Using Parallel Dual-Branch Network,2.3,Robustness to HEp-2 Dataset,"To further verify the effectiveness of PAS-Net in fluorescent image classification tasks, we also apply our method to two HEp-2 cell public datasets, ICPR 2012 and I3A Task1. ICPR 2012 dataset uses average class accuracy (ACA) as the evaluation metric, which is the same concept as the accuracy mentioned above, while I3A Task1 uses mean class accuracy (MCA). We select four deep learning techniques for classification of HEp-2 cells for comparison, respectively, and the results are shown in Table 3. Without using pre-trained weights for migration learning and data augmentation, our network achieves 81.61% and 98.71% accuracy on ICPR 2012 dataset and I3A Task1 dataset, respectively, and the experimental results demonstrate the generalizability of the proposed PAS-Net for fluorescent image classification tasks. "
PAS-Net: Rapid Prediction of Antibiotic Susceptibility from Fluorescence Images of Bacterial Cells Using Parallel Dual-Branch Network,3,Conclusion,"In this paper, we develop a PAS-Net framework for rapid prediction of antibiotic susceptibility from bacterial fluorescence images only. PAS-Net is a parallel dual-branch feature interaction network. FIU is a connecting bridge to align and fuse the local features from the C-branch and the global representation from the T-branch, which enhances the feature representation ability of the network. We design a HMSA module with less computational overhead to improve the computational efficiency of the model. The experimental results demonstrate that our method is feasible and effective in PA fluorescence image classification task, and can assist clinicians in determining bacterial antibiotic susceptibility."
Learning Normal Asymmetry Representations for Homologous Brain Structures,1,Introduction,"(Sub)cortical brain structures are approximately symmetrical between the left and right hemispheres [24]. Although their appearance and size are similar, they usually present difficult-to-characterize morphometric differences that vary among healthy populations [27], e.g. due to natural ageing [3]. Moreover, it has been studied that some neurological conditions, including Alzheimer's disease [10,16] (AD), schizophrenia [6], and epilepsy [4,17], are associated to asymmetry of the hippocampus or the amygdala [26]. In regular medical practice, radiologists detect pathological changes in asymmetry by manually inspecting the structure using brain MRIs [4]. They rely on their own subjective experience and knowledge, which varies among observers and lacks reproducibility, or standard quantitative measurements, e.g. volume differences in segmentations [3,9,12,16,17,20], which fail to capture morphological asymmetries beyond differences in size [10,21]. No tools automate quantifying normal asymmetry patterns beyond volume [16], e.g. to detect deviations caused by a neurodegenerative disease. Some deep learning tools approximate this goal in a binary classification setting to differentiate one particular condition from normal cases [5,7]. However, this form is heavily specialized to discern asymmetry alterations associated with one specific disease, requiring retraining for every new condition [11,12].This paper introduces a novel framework for learning NORmal Asymmetries of Homologous cerebral structures (deep NORAH) based on anomaly detection and representation learning. Unlike previous methods that train Siamese neural networks with volume descriptors [12], our model takes 3D segmentations of left and right components from MRIs as inputs, and maps them into an embedding that summarizes their shape differences. Essentially, our Siamese architecture includes a shape characterization encoder that extracts morphological features directly from segmentations and an Asymmetry Projection Head (APH) that merges their differences to create a compact representation of asymmetries. To ensure this embedding learns the heterogeneity in normal individuals, our network is trained only with healthy samples, using a self-supervised pre-training stage based on a Contractive Autoencoder (CAE) and then fine-tuning using a Support Vector Data Description (SVDD) objective. Our experiments in the hippocampus show that our model can easily project new cases to the normal asymmetry space. Furthermore, we show that the distance between the embedding and the center of the normal space is a measure of deviation-from-normalasymmetry, as we empirically observed increased distance in pathological cases. Hence, deep NORAH can be used to diagnose, e.g. AD, hippocampal sclerosis and even mild cognitive impairment (MCI) by simply detecting the differences in asymmetry regarding the normal set, without needing diseased cases for training.In summary, our contributions are as follows: (i) ours is the first unsupervised deep learning model explicitly designed to learn normal asymmetries in homologous brain structures; (ii) although it is trained only with normal data, it can be used to detect diseased samples by quantifying the degree of deviation with respect to a healthy population , unlike existing methods that capture only disease-specific asymmetries [11]; and (iii) compared to other state-of-theart anomaly detection approaches, our method demonstrates consistently better results for discriminating both synthetic and diseased-related asymmetries."
Learning Normal Asymmetry Representations for Homologous Brain Structures,2,Methods,"Figure 1 depicts a flowchart of our method as applied in test time. Our goal is to automatically measure the asymmetry of a given homologous brain structure x = (x L , x R ), with x L and x R being the 3D segmentations of its left and right lateral elements, and learn if these differences are typical for a normal population. To do so, we propose to learn a Siamese deep neural network F θ (x) = z L-R with θ parameters using an anomaly detection objective and samples from healthy individuals. z L-R is a compact representation of the asymmetries in x, obtained by learning a hypersphere S with a center c and minimum radius. In test time, samples with normal asymmetries are projected to the vicinity of c, while those with unexpected differences fall far from this point. As a result, the distance d = F θ (x)c 2 2 can be used as a deviation-from-normal-asymmetry index. To train this model, we first learn a 3D shape encoder f θEN (x) as part of a CAE, using normal samples (Sect. 2.1). This network can take any single segmentation of a lateral element x (i) as input, and map it to a high dimensional shape representation h * (i) . We then add this encoder to a Siamese architecture by attaching it to an APH, which captures the differences in shape from h * L and h * R , and project them into the unique asymmetry embedding z L-R . To this end, both the pre-trained encoder and the APH are trained using a deep SVDD objective (Sect. 2.2). This second learning phase not only trains the APH from scratch but fine-tunes the shape encoder to capture those morphological characteristics that are the most common source of asymmetry in normal individuals."
Learning Normal Asymmetry Representations for Homologous Brain Structures,2.1,Pre-training the Shape Characterization Encoder as a CAE,"Our shape encoder f θEN indistinctly map an arbitrarily left or right segmentation x (i) of an homologous structure x, to a feature vector h * (i) that describes its shape. To this end, we apply a warm-up learning phase that trains f θEN as the encoding path of a CAE, using a self-supervised learning loss (Fig. 2a). Hence, the encoder is simultaneously trained with a decoder g θDE (i) using a reconstruction task. The encoder compresses the input into a lower-dimensional representation h * (i) by applying a series of convolutional and pooling operations, and the decoder tries to reconstruct it using upsampling operations and convolutions.Formally, let (g θDE •f θEN )(x (i) ) be a convolutional CAE with a decoding path g θDE (h * (i) ) with parameters θ DE that outputs a reconstruction x(i) of the input x (i) from its hidden representation h * (i) . This is achieved by minimizing a mean square error objective (Fig. 2a). After it, the decoder is discarded and the shape encoder is used in the Siamese setting of our anomaly detection network. "
Learning Normal Asymmetry Representations for Homologous Brain Structures,2.2,Learning Normal Asymmetries with a Siamese Network,"Asymmetry Projection Head. The purpose of our APH is to project the shape representation h * obtained by f θEN into a compact embedding z L-R (Fig. 2b) that better describes normal population asymmetry characteristics. In our implementation, this network is a multilayer perceptron (MLP) with two fully connected (FC) layers separated with a ReLU activation. The first FC layer is used in a Siamese setting by feeding it with the shape representations h * L and h * R of the left and right elements, respectively. Each of these inputs are projected into two new feature vectors h L and h R with a lower dimensionality. A merging operation (e.g. subtraction or concatenation) combines them into a joint representation h (L-R) , which is projected by the second FC layer into the asymmetry embedding z (L-R) . Notice that the main design choices to be made are the dimensionality of the outputs of each FC layer and the merging operation.One-Class Deep SVDD. In order to enforce z (L-R) to represent the asymmetry characteristics of normal individuals, we train the Siamese architecture F θ (x) in Fig. 2b using an anomaly detection objective. We adopted the one-class deep SVDD approach proposed in [22], which solves:The first term in Eq. 1 is a quadratic loss that penalizes the Euclidean distance of z (L-R) from the center c of a hypersphere S, that is implicitly determined by the distance itself in the representation space. The second term is a classic weight decay regularizer, controlled by λ. Notice that we do not contract S by explicitly penalizing its radius and samples lying outside its boundary, but by minimizing their mean Euclidean distance with respect to c [22]. To avoid convergence to a collapsed trivial solution with all zero weights, all layers in F θ do not use bias terms [22], and the center c was set to the average z (L-R) obtained by feeding F θ with all training samples before fine-tuning, as in [28]. At that stage, f is already pre-trained using the self-supervised strategy described in Sect. 2.1, but the APH has random weights. Nevertheless, we experimentally observed that this center c is already enough to avoid a collapsed S.Deviation-from-Normal-Asymmetry Index. The distance between the asymmetry embedding of an unseen sample x and the center of the learned hypersphere, s(x; c) = F θ (x)c 2 , can be used as a deviation-from-normalasymmetry index: when the input x is a normal sample, its asymmetry embedding is expected to lie in the vicinity of c, then associated to a small s value; on the other hand, if x is the segmentation of a subject with abnormal asymmetries, its associated z (L-R) will lie afar from c, reporting a higher s value."
Learning Normal Asymmetry Representations for Homologous Brain Structures,3,Experimental Setup,"We studied our method for hippocampal asymmetry characterization as a use case. First, we tested its ability to capture deviations in asymmetry using synthetically altered hippocampi with increased deformations, in a controlled setting. Then, we indirectly evaluated its performance as a diagnostic tool for neurodegenerative conditions, using s as a deviation-from-normal-asymmetry index.Finally, we performed an ablation study to understand the influence of design factors such as the shape encoder architecture, APH size, and merging operation.Materials. We used a total of 3243 3D T1 brain MRIs, including 2945 from normal control (NC) individuals, 71 from patients with MCI, 179 with AD, and 16 and 32 with right (HSR) and left (HSL) hippocampal sclerosis, respectively. Samples were retrospectively collected from OASIS [14] (NC = 2217, AD = 33), IXI [2] (NC = 539) and ADNI [1] (NC = 53, AD = 33, MCI = 71) public sets, and from two in-house databases, ROFFO (NC = 83) and HEC (NC = 53, HSL = 32, HSR = 16), (see supp. mat. for demographics characteristics). All images were integrated in a single set, that we split into training, validation and test.The training set was used to learn patterns of normal asymmetry, with NC from ROFFO (63), IXI (539), and 70% of the NC from OASIS. Ages ranged from 19 to 95 years old, to ensure capturing normal variations due to natural aging. 60 NC images were kept aside for validation (see below). The test sets, were used to evaluate the diagnostic ability of our method on different cohorts. TEST-ADNI and TEST-HEC sets include all subjects from ADNI and HEC sets, while TEST-OASIS includes all AD and the remaining 30% of NC from OASIS. Images from different devices were aligned and normalized to a standard reference MNI T1 template using SPM12 [19]. Hippocampal segmentations were obtained using HippMapp3r [8], a CNN model that is robust to atrophies and lesions. The resulting segmentations were cropped to create separate masks for each hippocampus, each with size 64 × 64 × 64 voxels. We created synthetic validation sets with abnormal hippocampal asymmetries to study the model's ability to identify asymmetries of different variations. We used 60 normal hippocampal segmentations (20 from ROFFO and 40 from OASIS) and applied elastic deformations [25] with σ ∈ {3, 5, 8} to one of the two hippocampi of 20 individuals, resulting in 60 simulated abnormal pairs (see supp. mat. for qualitative examples). Four validation sets S σ were created, with σ = 3, 5, 8 and all, including the original 60 normal pairs and the corresponding simulated cases in the first three and a mix of all of them in the last one. This allowed us to perform hyperparameter tuning and evaluate the model's performance. Implementation Details. We studied two backbones for our Siamese shape encoder: a LeNet-based architecture similar to the one in [22] (but adapted to 3D inputs), with 3 convolutional layers with 16, 32 and 64 5 × 5 × 5 filters, each followed by a 3D batch normalization (BN) layer and a ReLU operation; and a deeper CNN equal to the encoding path of the CAE in [15] to learn anatomical shape variations from 3D organs. The size of the FC layers in the APH were adjusted based on the validation set. For CAE pre-training, the LeNet based encoder was attached to a bottleneck FC layer, followed by a symmetric decoder with 3 trilinear interpolation upsamplings, each followed by a convolutional layer with 3D BN and ReLU. For the deeper encoder, on the other hand, we used the exact same CAE from [15]. Further details about the architectures are provided in the supp. mat. We pre-trained the networks using the CAE approach for 250 epochs, and then fine-tuned them with SVDD for another 250 epochs. In all cases, we used Adam optimization with a learning rate of 10 -4 , weight decay regularization with a factor of 10 -6 and a batch size of 12 hippocampi pairs. We used PyTorch 1.12 and SciKit Learn for our experiments.Baselines. We compared our model with respect to other multiple approaches. To account for the standard clinical methods, we included the absolute and normalized volume differences (AVD and NVD, respectively), used in [18] as scores for asymmetry. We also included shallow one-class support vector machines (OC-SVMs) [23] trained with the same NC subjects than ours but different feature sets. We used ShapeDNA [26] (ShapeDNA + OC-SVM), which was previously studied to characterize hippocampal asymmetries [21], and a combined large feature vector (LFV + OC-SVM) including volumetric differences, ShapeDNA and shape features obtained using PyRadiomics (sphericity, compactness, quadratic compactness, elongation, flatness, spherical disproportion, surface volume ratio, maximum 2D diameter, maximum 3D diameter and Major Axis). For standard deep practices in anomaly detection, we trained hybrid CAE + shallow OC-SVM using our LeNet (LeNet-CAE + OC-SVM) and deeper backbones (Oktay et al. [15] + OC-SVM). Finally, a binary network was trained to detect AD cases (AD classification), in order to have a supervised counterpart for comparison. We used a larger training set that, apart from the same NC subjects used for the anomaly detection models, included all samples in TEST-OASIS. The validation set had in this case the remaining NCs from OASIS and AD cases from ADNI.The same backbone architecture was used, but with an additional FC layer that had softmax activation for classification, as in [11]."
Learning Normal Asymmetry Representations for Homologous Brain Structures,4.1,Characterization of Normal and Disease Related Asymmetries,"To test our hypothesis that samples with abnormal hippocampal asymmetries deviate from the center of the normal hypersphere, we evaluated the distances s(x; c) between all samples in the validation and test sets and grouped them by disease category. The distribution of these distances is shown in Fig. 3 (left), with statistical significance assessed using Mann-Whitney-Wilcoxon rank-sum tests (α = 0.05) with Bonferroni correction applied for multiple comparisons. Figure 3 (right) represent the t-SNE projection of all representations. NC subjects are closely grouped in this plot, with the smallest distances to the center among all groups. These values are significantly lower than those obtained for synthetically altered samples (p < 0.004) and individuals with MCI (p < 0.017), AD (p < 0.0083), HSL (p < 0.017) and HSR (p < 0.017). Distances increase proportionally to σ for synthetic cases and conditions with unilateral hippocampus shrinkage such as HSL and HSR, which are located at the extremes in the t-SNE representation. Synthetic cases with σ = 8 group around one of the clusters, while those with σ = 5 and 3 are scattered closer but still far from the center. MCI subjects from ADNI are scattered similarly to NC samples from OASIS, which is consistent with their distances. This could be due to cognitive decline in MCI cases not necessarily associated with alterations in hippocampal asymmetry, which can resemble that of NC, but rather with changes in other brain areas such as amygdala or thalamus [13,26]. It is possible then that the MCI subjects in this study do not exhibit hippocampal asymmetry changes large enough to be distinguished from those in healthy controls. AD cases, on the other hand, are seen far from the hypersphere center as well, reporting distances higher than those from NC. Finally, when compared one another, we observed significant differences in the distances between NC from ADNI set (p < 0.0055), but not between OASIS and HEC sets (p < 0.1241)."
Learning Normal Asymmetry Representations for Homologous Brain Structures,4.2,Comparison with Other Approaches,"The evaluation results of different methods are displayed in Table 1. Volumetric based approaches only detected unilateral atrophies in synthetic and HSL/R cases and poor performance for MCI and AD. Feature-based methods performed slightly better for AD and synthetic but dropped for HSL/R and MCI, lacking the required robustness to abnormal asymmetry changes. Hybrid CAE + OC-SVM models had good performance on synthetic data but not on real data tasks. Finally, the AD binary classifier was only able to detect AD but failed in any other cases, which is consistent with the disease specialization hypothesis. Our method, on the contrary, outperformed all other methods for detecting both synthetically induced and pathological changes in hippocampal asymmetry. Notice,  1.00 (1.00-1.00) 1.00 (1.00-1.00) Deep NORAH (with CAE pretr.) 0.99 (0.99-1.00) 0.93 (0.87-0.97) 1.00 (0.99-1.00) 0.92 (0.86-0.96) 1.00 (1.00-1.00) 1.00 (1.00-1.00) however, that the AD classification method is not a state-of-the-art approach but a comparable model with approximately the same backbone than ours. Other alternatives such as [10,12] might achieve much higher AUC values.Ablation Analysis. Table 1 includes results with and without our CAE pretraining. This stage significantly improve performance in synthetic, MCI and AD (OASIS) cases, perhaps due to a better estimate of c. Figure 4 illustrates the variations in AUC in S σ=all when changing the merge operation and the size of h(•). Using differences seems to be much more efficient in terms of capacity usage, with almost the same AUC obtained without a FC layer for dimensionality reduction (0.977) and with a FC layer with 512 outputs (0.997). Nevertheless, when applied on real cases, adding this additional component aids to improve the discrimination performance (see Table 1). Finally, Table 2 shows results in S σ=all for different encoder settings. Using a Siamese approach with difference as merge operation was in all cases superior than the other alternatives. In terms of architecture, LeNet and deep backbones showed similar performance, with LeNet being slightly better. This might be due to a higher number of FC parameters, as the output of the shape encoder is a vector with >32k features.  "
Learning Normal Asymmetry Representations for Homologous Brain Structures,5,Conclusions,"We presented a novel anomaly detection-based method for automatically characterizing normal asymmetry in homologous brain structures. Supervised alternatives restrict the definition of normal individuals due to explicitly learning their differences with respect to subjects with a specific condition. This implies that they ignore the asymmetry in control subjects, capturing only the asymmetries induced by the analyzed disease [12], and requiring retraining to detect new conditions unseen during training. Conversely, our unsupervised alternative leverages a recently introduced one-class-based objective to learn the space of normal asymmetries. Hence, it can detect diseased samples by quantifying their distance to the control space. Our experiments on hippocampus data showed that our approach could effectively use symmetry information to characterize normal populations and then identify disease presence by contrast, even though only NC subjects are used for training. Our model can potentially be applied to other homologous brain structures and diverse cohorts to aid radiologists in quantifying asymmetries of a normal brain better. In its current form, the model inherits the limitations of the segmentation approach, although it showed to achieve good performance using the outputs of HippMapp3r. Furthermore, it has the burden of not offering qualitative feedback, so future work should focus on bringing interpretability to this tool, e.g., by means of occlusion analysis."
Learning Normal Asymmetry Representations for Homologous Brain Structures,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3 8.
Wasserstein Distance-Preserving Vector Space of Persistent Homology,1,Introduction,"Networks are ubiquitous representations for describing complex, highly interconnected systems that capture intricate patterns of relationships between nodes. [3]. Finding meaningful, computationally tractable characterizations of network structure is very difficult, especially for large and dense networks with node degrees ranging over multiple orders of magnitude [5].Persistent homology [10] is an emerging tool for understanding, characterizing and quantifying the topology of complex networks [19,21]. Connected components and cycles are the most dominant and fundamental topological features of real networks. For example, many networks naturally organize into modules or connected components [5]. Similarly, cycle structure is ubiquitous and is often interpreted in terms of information propagation, redundancy and feedback loops [14]. Topological features are represented in persistent homology using descriptors called persistence diagrams [10]. Effective use of such topological descriptors in machine learning requires a notion of proximity. Wasserstein distance is often used to quantify the distance between persistence diagrams, motivated by its central stability properties [18]. However, the integration of persistence diagrams and Wasserstein distance with standard learning methods from statistics and machine learning has been a challenging open problem due to the differences between Wasserstein distance and standard Euclidean-based metrics [8].Approaches that embed persistence diagrams into vector spaces [1] or Hilbert spaces [7,13] have recently been proposed to address this challenge. None of the embedding methods proposed thus far preserve Wasserstein distance in the original space of persistence diagrams [6]. Thus, these approaches do not inherit the stability properties of Wasserstein distance.Recently, it was shown that persistence diagrams are inherently 1dimensional if the topological features of networks are limited to connected components and cycles, and that the Wasserstein distance between these diagrams has a closed form expression [19]. Consequently, the work in [20] provides a computationally tractable, topological clustering approach for complex networks. However, significant limitations of the result in [19,20] are that it is unclear how this approach can be incorporated with standard Euclidean-based learning methods from statistics and machine learning, and that the approach is limited to evaluating networks with the identical number of nodes. There are many opportunities for applications of topological analysis of networks of different size, such as studies of the human brain when different subjects are sampled at different resolutions.In this work, we present a novel topological vector space (TopVS) that embeds 1-dimensional persistence diagrams representing connected components and cycles for networks of different sizes. Thus, TopVS enables topological machine learning with networks of different sizes and greatly expands the applicability of previous work. Importantly, TopVS preserves the Wasserstein distance in the original space of persistence diagrams. Preservation of the Wasserstein distance ensures the theoretical stability properties of persistence diagrams carry over to the proposed embedding. In addition to the robustness benefits, TopVS also enables the application of a wide variety of Euclidean metric-based learning methods to topological data analysis. Particularly, the utility of TopVS is demonstrated in topology-based classification problems using support vector machines. TopVS is illustrated by classifying measured functional brain networks based on data obtained from subjects with different numbers of electrodes. The results show that TopVS performs very well compared to other competing approaches."
Wasserstein Distance-Preserving Vector Space of Persistent Homology,2.1,One-Dimensional Persistence Diagrams,"Define a network as an undirected weighted graph G = (V, w) with a set of nodes V and a weighted adjacency matrix w = (w ij ). Define a binary graph G with the identical node set V by thresholding the edge weights so that an edge between nodes i and j exists if w ij > . The binary graph is viewed as a 1-skeleton [15]. As increases, more and more edges are removed from the network G. Thus, we have a graph filtration:Persistent homology keeps track of the birth and death of topological features over filtration values . A topological feature that is born at a filtration b i and persists up to a filtration d i , is represented by a point (b i , d i ) in a 2D plane. A set of all the points {(b i , d i )} is called persistence diagram [10]. In the 1-skeleton, the only non-trivial topological features are connected components and cycles [22]. As increases, the number of connected components β 0 (G ) and cycles β 1 (G ) are monotonically increasing and decreasing, respectively [19]. Thus, the representation of the connected components and cycles can be simplified to a collection of sorted birth values, respectively [19]. B(G) comprises edge weights in the maximum spanning tree (MST) of G. Once B(G) is identified, D(G) is given as the remaining edge weights that are not in the MST. Thus B(G) and D(G) are computed very efficiently in O(n log n) operations with n number of edges in networks."
Wasserstein Distance-Preserving Vector Space of Persistent Homology,2.2,Closed-Form Wasserstein Distance for Different-Size Networks,"The Wasserstein distance between the 1-dimensional persistence diagrams can be obtained using a closed-form solution. Let G 1 and G 2 be two given networks possibly with different node sizes, i.e., their birth and death sets may differ in size. Their underlying empirical distributions on the persistence diagrams for connected components are defined in the form of Dirac masses [23]:where δ(xb) is a Dirac delta centered at the point b. Then the empirical distribution functions are the integration of f G1,B and f G2,B asThen the empirical Wasserstein distance for connected components has a closed-form solution in terms of these pseudoinverses asSimilarly, the Wasserstein distance for cycles W p,D (G 1 , G 2 ) is defined in terms of empirical distributions for death sets D(G 1 ) and D(G 2 ). The empirical Wasserstein distances W p,B and W p,D are approximated by computing the Lebesgue integration in (1) numerically as follows.G1,D (n/n)} be pseudoinverses for network G 1 sampled with partitions of equal intervals. Let B(G 2 ) and D(G 2 ) be sampled pseudoinverses for network G 2 with the same partitions of m and n, respectively. Then the approximated Wasserstein distances are given byFor a special case when networks G 1 and G 2 have the same number of nodes, i.e., |B(Gthen exact computation of the Wasserstein distance is achieved using those birth and death sets, and setting m to the cardinality of the birth sets and n to that of the death sets."
Wasserstein Distance-Preserving Vector Space of Persistent Homology,2.3,Vector Representation of Persistence Diagrams,"A collection of 1-dimensional persistence diagrams together with the Wasserstein distance is a metric space. 1-dimensional persistence diagrams can be embedded into a vector space that preserves the Wasserstein metric on the original space of persistence diagrams as follows. Let G 1 , G 2 , ..., G N be N observed networks possibly with different node sizes. Let F -1  Gi,B be a pseudoinverse of network G i . The vector representation of a persistence diagram for connected components in network G i is defined as a vector of the pseudoinverse sampled at 1/m, 2/m, ..., m/m, i.e., v B,iThus, for p = 1 the proposed vector space describes Manhattan distance, p = 2 Euclidean distance, and p → ∞ the maximum metric, which in turn correspond to the earth mover's distance (W 1 ), 2-Wasserstein distance (W 2 ), and the bottleneck distance (W ∞ ), respectively, in the original space of persistence diagrams. Similarly, we can define a vector space of persistence diagrams for cycles M D = {v D,i } N i=1 with the p-norm metric d p,D . The normed vector space (M B , d p,B ) describes topological space of connected components in networks, while (M D , d p,D ) describes topological space of cycles in networks.The topology of a network viewed as a 1-skeleton is completely characterized by connected components and cycles. Thus, we can fully describe the network topology using both M B and M D as follows. Let where"
Wasserstein Distance-Preserving Vector Space of Persistent Homology,,2,". Thus, d p,× is a weighted combination of p-Wasserstein distances, and is simply the p-norm metric between vectors constructed by concatenating v B,i and v D,i . The normed vector space (M B × M D , d p,× ) is termed topological vector space (TopVS). Note the form of d p,× given in (4) results in an unnormalized mass after multiplying m and n by their reciprocals given in ( 2) and ( 3). This unnormalized variant of Wasserstein distance is widely used in both theory [8,18] and application [7,19,21] of persistent homology. A direct consequence of the equality given in ( 4) is that the mean of persistence diagrams under the approximated Wasserstein distance is equivalent to the sample mean vector in TopVS. In addition, the proposed vector representation is highly interpretable because persistence diagrams can be easily reconstructed from vectors by separating sorted births and deaths.For a special case in which networks G 1 , G 2 , ..., G N have the same number of nodes, the vectors v B,i and v D,i are simply the original birth set B(G i ) and death set D(G i ), respectively, and the p-norm metric d p,× is expressed in terms of exact Wasserstein distances as"
Wasserstein Distance-Preserving Vector Space of Persistent Homology,3,Application to Functional Brain Networks,"Dataset. We evaluate our method using functional brain networks from the anesthesia study reported by [2]. The brain networks are based on alpha band (8-12 Hz) weighted phase lag index applied to 10-second segments of resting state intracranial electroencephalography recordings. These recordings were made from eleven neurosurgical patients during administration of increasing doses of the general anesthetic propofol just prior to surgery. Each segment is labeled as one of the three arousal states: pre-drug wake (W), sedated but responsive to command (S), or unresponsive (U). The number of brain networks belonging to each subject varies from 71 to 119, resulting in the total of 977 networks from all the subjects. The network size varies from 89 to 199 nodes across subjects.Classification Performance Evaluation. We are interested in whether candidate methods 1) can differentiate arousal states within individual subjects, and 2) generalize their learned knowledge to unknown subjects afterwards. As a result, we consider two different nested cross validation (CV) tasks as follows.1. For the first task, we classify a collection of brain networks belonging to each subject separately. Specifically, we apply a nested CV comprising an outer loop of stratified 2-fold CV and an inner loop of stratified 3-fold CV. Since we may get a different split of data folds each time, we perform the nested CV for 100 trials and report an average accuracy score and standard deviation for each subject. We also average these individual accuracy scores across subjects (11 × 100 scores) to obtain an overall accuracy. 2. For the second task, we use a different nested CV comprising both outer and inner loops with a leave-one-subject-out scheme. That is, a classifier is trained using all but one test subject. The inner loop is used to determine optimal hyperparameters, while the outer loop is used to assess generalization capacity of the candidate methods to unknown subjects in the population.Method Comparison. Brain networks are used to compare the classification performance of the proposed TopVS relative to that of five state-of-the-art kernel methods and two well-established graph neural network methods. Three of these kernel methods are based on conventional 2-dimensional persistence diagrams for connected components and cycles: the persistence image (PI) vectorization [1], the sliced Wasserstein kernel (SWK) [7] and the persistence weighted Gaussian kernel (PWGK) [13]. The other two kernel methods are based on graph kernels: the propagation kernel (Prop) [16] and the GraphHopper kernel (GHK) [11]. The PI method embeds persistence diagrams into a vector space in which classification is performed using linear support vector machines (SVMs). The non-linear SWK, PWGK, Prop and GHK methods are combined with SVMs to perform classification. While nearly any classifier may be used with TopVS, here we illustrate results using the SVM with the linear kernel, which maximizes Wasserstein distance-based margin. When the TopVS method is applied to different-size networks, we upsample birth and death sets of smaller networks to match that of the largest network in size. Hyperparameters are tuned using grid search. SVMs have a regularization parameter C = {0.01, 1, 100}. Thus, a grid search trains TopVS and PI methods with each C ∈ C. The SWK and WGK methods have a bandwidth parameter Σ = {0.1, 1, 10}, and thus grid search trains both methods with each pair (C, σ) ∈ C × Σ. The Prop method has a maximum number of propagation iterations T max = {1, 5, 10}, and thus is trained with each pair (C, t max ) ∈ C × T max . GHK method uses the RBF kernel with a parameter Γ = {0.1, 1, 10} between node attributes, and thus is trained with each pair (C, γ) ∈ C × Γ .In addition, we also evaluate two well-established graph neural network methods including graph convolutional networks (GCN) [12] and graph isomorphism network (GIN) [25]. GCN and GIN are based on configurations and choices of hyperparameter values used in [25] as follows. Five graph neural network layers are applied, and the Adam optimizer with initial learning rate and weight decay of 0.01 are employed. We tune the following hyperparameters: the number of hidden units in {16, 32}, the batch size in {32, 128} and the dropout ratio in {0, 0.5}. The number of epochs is set to 100 to train both methods.Results. Results for the first task are summarized in Fig. 1, in which classification accuracy for individual subjects is shown. There is variability in individual subject performance because a different subject's network has a different number of electrodes, different electrode locations and different effective signal to noise ratio. So we expect these subjects to exhibit a diverse set of topological features across spatial resolutions. In most subjects all methods perform relatively well. The consistently poorer performance of PI, Prop and GIN is evident in the lower overall performance. On the other hand, our TopVS method is consistently among the best performing classifiers, resulting in the higher overall performance. For classification accuracy across subjects from the second task, we have 0.65±0.21 for TopVS, 0.58±0.22 for PI, 0.57±0.20 for SWK, 0.60±0.21 for WGK, 0.36 ± 0.12 for Prop, 0.43 ± 0.14 for GHK, 0.53 ± 0.20 for GCN and 0.48 ± 0.19 for GIN. TopVS is also among the best methods for classifying across subjects, while the performance of all the graph neural networks and graph kernels is significantly weaker. These results suggest that the use of computationally demanding and complex classification methods, such as GCN and GIN, does not result in significant increase in generalizability when classifying brain networks.In addition, we compute confusion matrices to gain insights into the acrosssubject predictions for the second task, as displayed in Fig. 2. The persistent homology based methods, including TopVS, PI, SWK and WGK, are generally effective for separating unresponsive (U) from the other two states, and the majority of classification errors are associated with the differentiation between wake (W) and sedated (S) states. Prior work [2] demonstrated that wake and sedated brains are expected to have a great deal of similarity in comparison to the less similar unresponsive brains. However, the work in [2] performed the analysis on each individual subject separately while the results presented here are based on the analysis across subjects. Thus, not only the results here are consistent with the previous work [2] but also suggest that such biological expectation carries over to brains across subjects and that topology based methods can potentially derive biomarkers of changes in arousal states in the population, which underlie transitions into and out of consciousness, informing our understanding of the neural correlates of consciousness in clinical settings. TopVS shows clear advantages over all other topological baseline methods for differentiating wake and sedated states, suggesting that the proposed vector representation is an effective choice for representing subtle topological structure in brain networks. Runtime Experiment. The kernel candidate methods are evaluated for a runtime experiment based on Intel Core i7 CPU with 16 GB of RAM. Figure 3 displays the runtime vs input size plot. The result clearly shows that all three persistent homology based kernels (PI, SWK and WGK) are limited to dense networks with a few hundred nodes, representing the current scaling limit of persistent homology embedding methods. On the other hand, TopVS is able to compute a kernel between 2000-node networks each with approx. two million edges in about one second. The computational practicality of TopVS extends its applicability to the large-scale analyses of brain networks that cannot be analyzed using prior methods based on conventional 2-dimensional persistence diagrams. Note that the time complexity of Prop is linear while TopVS has the slightly higher complexity as linearithmic. While Prop is the most efficient among all the methods, it has the lowest average accuracy when classifying the brain network data."
Wasserstein Distance-Preserving Vector Space of Persistent Homology,,Potential Impact and,"Limitation. An open problem in neuroscience is identifying an algorithm that reliably extracts a patient's level of consciousness from passively recorded brain signals (i.e., biomarkers) and is robust to inter-patient variability, including where the signals are recorded in the brain. Conveniently, the anesthesia dataset is labeled according to consciousness state, and electrode placement (node location) was dictated solely by clinical considerations and thus varied across patients. Importantly, the relatively robust performance across patients suggests there are reliable topological signatures of consciousness captured by TopVS. The distinction between Wake and Sedated states involves relatively nuanced differences in connectivity, yet TopVS exploits the subtle differences in topology that differentiate these states better than the com-Fig. 3. Runtime experiment. We measured the runtime as the average amount of time each algorithm takes to compute its kernel between two complete graphs starting from edge weights as a given input. The runtime is plotted with respect to network size in terms of both the number of nodes and edges.peting methods. Our results suggest that the neural correlates of consciousness can be captured in measurements of brain network topology, a longstanding problem of great significance. Additionally, TopVS is a principled framework that connects persistent homology theory with practical applications. Our versatile vector representation can be used with various vector-based statistical and machine learning models, expanding the potential for analyzing extensive and intricate networks beyond the scope of this paper. While TopVS is limited to representing connected components and cycles, assessment of higher-order topological features beyond cycles is of limited value due to their relative rarity and interpretive challenges, and consequent minimal discriminitive power [4,17,24]."
Path-Based Heterogeneous Brain Transformer Network for Resting-State Functional Connectivity Analysis,1,Introduction,"Brain functional network refers to the integrator of information exchange between different neurons, neuron clusters or brain regions. It can not only reveal the working mechanism and developmental changes of the brain [24], but also reflect anatomical connectivity of brain structure [12], making it a hot topic of neuroscience research in recent years. Current research [15,19,32] based on brain functional network mainly focus on two directions: brain physiological basis and brain diseases. Brain diseases are often associated with abnormal connections and have been shown to be related to physiological basis, especially age and sex [2,4,9]. In neuroscience, age and gender prediction based on brain functional network would be the basis for better studying brain diseases, and understanding and exploring the operating mechanisms of baby brains.As a powerful neuroimaging tool, the resting-sate functional Magnetic Resonance Imaging (rs-fMRI) constructs brain functional networks by capturing the changes of blood oxygen level-dependent (BOLD) signals and computing their correlation between different regions of interest (ROIs) [5]. Owing to the benefits of non-invasive and high-resolution of rs-fMRI, resting-state functional connectivity (rs-FC) derived from BOLD signals is increasingly used to analyze brain age and gender [6]. Up to now, rs-FC analysis methods have primarily included correlation-based methods and graph-based approaches [26]. Compared with correlation-based methods, representing rs-FC data as a graph can preserve the natural topological properties of brain networks, where the nodes are defined as ROIs by an atlas, and the edges are calculated as pairwise correlations between ROIs. However, the assumptions of most existing graph-based methods are still far from the reality of the human brain with the following limitations: Ignoring Path Importance. Some studies have indicated that connectivity is the core of the brain and no neuron is an island [1,21]. Similar as in the graph theory, these connections can be defined as paths in a brain graph. However, popular methods PR-GNN [18] and BrainGNN [17] mainly focused on node features, which ignored the significance of path features. Despite the fact that BrainNetCNN [14] built an edge-based brain network which can be viewed as a special path-based network, the edge-based description can only depict the direct connections, but cannot account for a variety of indirect connections between brain regions.Neglecting Heterogeneity . Although BC-GCN [19] formulated brain network as a path-based graph, the graph was homogeneous with only one type of path. In fact, brain functional network is heterogeneous, as proved by abundant studies [7,22,30].Overlooking Global Structures. Attention mechanism can help the model focus on crucial connections. Inspired by this, GAT model [25] was employed to analyze brain networks in [31], but limited by graph convolution, it only considered the local structures of neighboring nodes. According to emerging research, global and local structures provide different views for brain network analysis [8,28], and global brain information concerned by Graph Transformer can further improve predictive performance [3,13]. -We offer a new perspective on modeling the brain network as a heterogeneous graph with multiple types of path-based features under the prior knowledge of brain partitions, which takes into account the path significance and heterogeneity of the brain, and better simulates the brain network. -We develop a novel Graph Transformer Network, namely PH-BTN, which is unprecedently able to encode path-based heterogeneous brain network with Transformer to enforce neighbors while incorporating global information. The core component in PH-BTN, namely HP-GTC module, is combined with heterogeneous graph convolution and attention, which can aggregate rich and crucial path information to generate compact brain representation. Furthermore, the Squeeze-and-Excitation (SE) block [11] adopted in HP-GTC module can alleviate the over-smoothing problem of GNN. -We conduct extensive experiments on the Baby Connectome Project (BCP) dataset to verify the superiority of our proposed method compared with other state-of-the-art methods, and explore the age and gender relevance to the brain functional network."
Path-Based Heterogeneous Brain Transformer Network for Resting-State Functional Connectivity Analysis,2,Methodology,"An overview of the proposed PH-BTN is illustrated in Fig. 1. Below, we first introduce the construction of the path-based heterogeneous brain network. Then, we focus on the HP-GTC module and elaborate on the novel design and its two components with SE block, i.e., heterogeneous graph path convolution and transformer layer. Finally, we briefly describe the readout and prediction stages."
Path-Based Heterogeneous Brain Transformer Network for Resting-State Functional Connectivity Analysis,2.1,Path-Based Heterogeneous Graph Generation,"Path-Based Heterogeneous Brain Network. To retain the heterogeneity and path-based structure, we encode the brain functional network captured by rs-fMRI as a path-based heterogeneous graph G = (V, E, P), where V = {v i } N i=1 ∈ R N is the node set of size N defined by ROIs on a specific brain atlas, E = [e ij ] ∈ R N ×N ×D is the edge set constructed by using the Pearson's correlation coefficient (PCC) between a sub-series of BOLD signals between nodes, with each edge e ij initialized with D-dimensional edge features h ij , and P is the path set along with a path type mapping function ψ : P → R, where R denotes the graph path types, |R| ≥ 2.Graph Path. In graph theory, a graph path p is composed of a finite sequence of n edges, where n denotes that this path is a n-hop path p n . For example, a 2-hop path p 2 ikj between node v i and v j can be represented as p 2 ikj = {e ik , e kj }, where e ij is the edge between v i and v j and i = j = k. In particular, the 0-hop path indicates the self-loop of the node. According to ablation results of multihops [20], we can see that graph paths with finite hops contain enough effective information. To simplify, we limit the highest hop to 2. Then, the multi-hop paths between node v i and v j can be defined as, where P n ij means the set of all n-hop paths between node v i and v j , P 0 ij = {e ij |i = j} andParticularly, when combining P 0 ij and P 1 ij , we can gain the sequence {e ik , e kj }, where i = k = j or i = k = j. This conjunctive sequence of P 0 ij and P 1 ij can be regarded as a special 2-hop pathswhere k denotes the index of intermediate node. Thus, the multi-hop paths between node v i and v j can be recorded asHeterogeneous Graph Path. In order to obtain the type of paths, we introduce a brain partition (e.g., frontal, parietal, temporal, occipital and insular) as prior to define heterogeneous paths. It is intuitive to define the types of graph path by edges. However, with the increase of edges, the complexity of the path type definition will increase greatly. Thus, we directly regard the type of intermediate node v k as the type of graph path p ikj . For instance, if v k in the frontal lobe, we have ψ(p ikj ) = frontal. Considering the fact that the feature spaces of different types of paths are not completely irrelevant (as shown in Fig. 3-(a)), we have additionally defined all graph path as the base type, which can reflect the common information shared by different types of graph paths. For example, when using the above-mentioned partition as prior to define path type, we have |R| = |{base, f rontal, parietal, temporal, occipital, insular}| = 6."
Path-Based Heterogeneous Brain Transformer Network for Resting-State Functional Connectivity Analysis,2.2,HP-GTC: Heterogeneous Path Graph Transformer Convolution Module to Learn Compact Features,"After brain network construction, we present the HP-GTC module to extract and aggregate specific and common features from different types of graph paths.As illustrated in Fig. 2, the HP-GTC module consists of two layers, which are Heterogeneous Graph Path Convolution (HPConv) layer and Heterogeneous Graph Path Transformer (HPTrans) layer respectively. For the sake of simplicity and clarity, we here only show the above four paths' types as an example, which does not mean that we only use four types.HPConv. Considering that different heterogeneous paths have different distributions and contain different information, we design a novel graph convolution to learn the edge features of each path type independently. Under each path type, the brain network can be regarded as a homogeneous graph. Inspired by Li et al. [19], we utilize the same graph path convolution for each path type here. Within each path-based heterogeneous brain network G, we define the following simple propagation model as HPConv layer for the forward-pass update of an edge denoted by e ij under the graph type r ∈ R:where h under the graph path type r. Intuitively, the HPConv layer extracts edge features in specific and common path spaces, so as to ensure that the edge features learned in different specific feature spaces are independent and those learned in common spaces can retain global and shared information.HPTrans. Note that different types of graph paths would have different impacts on a specific edge, and the same type of graph paths may similarly affect the specific edge. To model these characteristics, we propose an attention mechanism to capture these heterogeneous and common information to learn more effective edge features. Inspired by Kan et al. [13], to further incorporate global information and inconsistent contributions from different types of paths, we design a novel Graph Transformer layer to further aggregate output featuresof HPConv layer for learning more compact edge features. The HPTrans layer is formulated as follows:where H denotes the output features, and the weight parameters2 linearly map edge features to different feature spaces, and then adopt L2-normalized operator norm(•) to generate the corresponding representations Q, K, V for subsequent attention matrix A calculation and feature aggregation. For simplicity of illustration, in this paper, we only consider the single-head self-attention and assume V = H . The extension to the multi-head attention is straightforward, and we omit bias terms for simplicity.To further enhance influential features and alleviate the over-smoothing problem of GNN, we adopt SE block in the HP-GTC module. Therefore, the final formulation of HP-GTC module can be represented as below:where H l is the edge feature matrix of l th HP-GTC module."
Path-Based Heterogeneous Brain Transformer Network for Resting-State Functional Connectivity Analysis,2.3,Readout and Prediction,"Lastly, inspired by Li et al. [19], a readout layer is adopted to transform the edge features learned by heterogeneous paths into the final graph embedding H Gm .Then H Gm is sent to a multi-layer perceptron (MLP) to give the final prediction ŷm , where G m denotes the m th graph sampled in subject s. Specifically, the final prediction of subject s is the average or weighted voting result of its' all samples during inferring."
Path-Based Heterogeneous Brain Transformer Network for Resting-State Functional Connectivity Analysis,3,Experimental Results,"Dataset and Implementation Details. We validated our PH-BTN on the Baby Connectome Project (BCP) dataset [10] including 612 longitudinal rs-fMRI scans from 248 subjects (ages 6-811days, 106 boys vs. 142 girls). We here adopted Harvard-Oxford atlas (N = 112 ROIs) [23] and the mainstream brain functional partition as prior, where |R| =  Visualization and Discussion. In this section, we utilized the gradient backtracking method [19] to visualize brain functional networks learned by the proposed PH-BTN. For visualizing age-related connections, we divided all scans into three typical groups, i.e., 0-1 year, 1-2 year and >2 year. All of our visual findings illustrated in Fig. 3(b)-(c) are consistent with those of many research [7,27,29], which have showed that frontal, parietal and occipital lobes are deeply related to brain age and gender, and well correspond to our statistical analysis (Fig. 3(a)) of different brain regions in the original data. We refer the readers to the Supplementary Materials for additional results of visualization."
Path-Based Heterogeneous Brain Transformer Network for Resting-State Functional Connectivity Analysis,4,Conclusion,"In this paper, we propose a novel network, namely PH-BTN, for encoding pathbased heterogeneous brain networks for analyzing brain functional connectivity.Different from most existing methods, our proposed model considers the path significance and heterogeneity by heterogeneous graph convolution, and incorporates global brain structure and key connections by Transformer mechanism. Experiments and visualization of age and gender prediction on the BCP dataset show the superiority and effectivity of our PH-BTN model. Moreover, the proposed PH-BTN offers a new way to understand neural development, explore sexual differences, and ultimately benefit neuroimaging research. In future work, we will extend and validate our methods on larger benchmark datasets."
Path-Based Heterogeneous Brain Transformer Network for Resting-State Functional Connectivity Analysis,,Table 1 .,"Ablation Studies. As shown in Supplementary Materials, we have done several ablation experiments to further evaluate the effectiveness of the proposed method, e.g., different path types by prior or random division, whether to use"
Path-Based Heterogeneous Brain Transformer Network for Resting-State Functional Connectivity Analysis,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3_32.
PMC-CLIP: Contrastive Language-Image Pre-training Using Biomedical Documents,1,Introduction,"In the recent literature, development of foundational models has been the main driving force in artificial intelligence, for example, large language models [2,8,27,30] trained with either autoregressive prediction or masked token inpainting, and computer vision models [19,29] trained by contrasting visuallanguage features. In contrast, development in the biomedical domain lags far behind due to limitations of data availability from two aspects, (i) the expertise required for annotation, (ii) privacy concerns. This paper presents our preliminary study for constructing a large-scale, high-quality, image-text biomedical dataset using publicly available scientific papers, with minimal manual efforts involved.In particular, we crawl figures and corresponding captions from scientific documents on PubMed Central, which is a free full-text archive of biomedical and life sciences journal literature at the U.S. National Institutes of Health's National Library of Medicine (NIH/NLM) [31]. This brings two benefits: (i) the contents in publications are generally well-annotated and examined by experts, (ii) the figures have been well-anonymized and de-identified. In the literature, we are clearly not the first to construct biomedical datasets in such manner, however, existing datasets [28,34,38] suffer from certain limitations in diversity or scale from today's standard. For example, as a pioneering work, ROCO [28] was constructed long time ago with only 81k radiology images. MedICAT [34] contains 217k images, but are mostly consisted of compound figures.In this work, we tackle the above-mentioned limitations by introducing an automatic pipeline to generate dataset with subfigure-subcaption correspondence from scientific documents, including three major stages: medical figure collection, subfigure separation, subcaption separation & alignment. The final dataset, PMC-OA, consisting of 1.65M image-text pairs (not including samples from ROCO), covers a wide scope of diagnostic procedures and diseases, as shown in Fig. 1 and Fig. 3. Along with the constructed dataset, we train a CLIP-style vision-language model for the biomedical domain, termed as PMC-CLIP. To achieve such a goal, the model is trained on PMC-OA with standard image-text contrastive (ITC) loss, and to encourage the joint interaction of image and text, masked language modeling (MLM) is also applied. We evaluate the pre-trained model on several downstream tasks, including medical image-text retrieval, medical image classification, and medical visual question answering (VQA). PMC-CLIP achieves state-of-the-art performance on various downstream tasks, surpassing previous methods significantly.Overall, in this paper, we make the following contributions: First, we propose an automatic pipeline to construct high-quality image-text biomedical datasets from scientific papers, and construct an image-caption dataset via the proposed pipeline, named PMC-OA, which is 8× larger than before. With the proposed pipeline, the dataset can be continuously updated. Second, we pre-train a vision-language model on the constructed image-caption dataset, termed as PMC-CLIP, to serve as a foundation model for biomedical domain. Third, we conduct thorough experiments on various downstream tasks (retrieval, classification, and VQA), and demonstrate state-of-the-art performance. The dataset and pre-trained model will be made available to the community. "
PMC-CLIP: Contrastive Language-Image Pre-training Using Biomedical Documents,2,The PMC-OA Dataset,"In this section, we start by describing the dataset collection procedure in Sect. 2.1, followed by a brief overview of PMC-OA in Sect. 2.2."
PMC-CLIP: Contrastive Language-Image Pre-training Using Biomedical Documents,2.1,Dataset Collection,"In this section, we detail the proposed pipeline to create PMC-OA, a large-scale dataset that contains 1.65M image-text pairs. The whole procedure consists of three major stages: (i) medical figure collection, (ii) subfigure separation, (iii) subcaption separation & alignment, as summarised in Fig. 2."
PMC-CLIP: Contrastive Language-Image Pre-training Using Biomedical Documents,,Medical Figure Collection (,"Step 1 and 2 in Fig. 2). We first extract figures and captions from PubMedCentral (till 2022-09-16) [31]. 2.4M papers are covered and 12M figure-caption pairs are extracted. To derive medical figures, inspired by MedICat [34], we first filter out the captions without any medical keywords1 and then use a classification network trained on DocFigure [14] to further pick out the medical figure, ending up with 381K medical figures.Subfigure Separation (Step 3 and 4 in Fig. 2). We randomly check around 300 figures from previous step, and find that around 80% of figures are compound, i.e. multiple pannels. We thus train a subfigure detector on MedICaT subfigure-subcaption subset [34] (MedICatSub) to break the compound figures into subfigures. After separation, to filter out non-medical subfigures missed in the former step, we apply the aforementioned classifier again on the derived subfigures, obtaining 1.6M subfigure-caption pairs. We termed this dataset as PMC-OA Beta version."
PMC-CLIP: Contrastive Language-Image Pre-training Using Biomedical Documents,,Subcaption Separation and Alignment (,"Step 5 and 6 in Fig. 2). To further align subfigure to its corresponding part within the full caption, i.e., subcaption, we need to break the captions into subcaptions first, we apply an off-shelf caption distributor [32]. We pretrain a CLIP-style model (termed as PMC-CLIP-Beta, training detail will be described in Sect. 3) on PMC-OA-Beta, then finetune it on MedICaTSub for subfigure-subcaption alignment, which achieves alignment accuracy=73% on test set. We finally align 1,003,911 subfigure-subcaption pairs, along with the remaining 642,681 subfigure-caption pairs, we termed this dataset as PMC-OA. Note that, we have explicitly removed duplication between our data and ROCO by identify each image-caption pair with paperID and image source link. We consequently pretrain the PMC-CLIP on it."
PMC-CLIP: Contrastive Language-Image Pre-training Using Biomedical Documents,2.2,Dataset Overview,"In this section, we provide a brief statistical overview of the collected dataset PMC-OA with UMLS parser [1] from three different perspectives, i.e., diagnostic procedure, diseases and findings, and fairness. First, PMC-OA covers a wide range of diagnostic procedures, spanning from common (CT, MRI) to rare ones (mitotic figure), which is more diverse than before (Fig. 3(a)). Second, PMC-OA contains various diseases and findings, and is more up-to-date, covering new emergent diseases like COVID-19 (Fig. 3(b)). And the wide disease coverage in our dataset supports learning the shared patterns of diseases, promoting accurate auto-diagnosis. Third, we also provide the sex-ratio across ages in Fig. 3(c), as we can see PMC-OA is approximately gender-balanced, with 54% males. The fairness on population ensures our dataset sightly suffers from patient characteristic bias, thus providing greater cross-center generalize ability.Discussion. Compared to pioneering works [28,34] for constructing dataset based on PubMedCentral, our proposed PMC-OA is of larger scale, diversity, and has more accurate alignment: First, PMC-OA covers a wider range of papers (2.4M) than ROCO [28](1.8M) and MedICaT [34](131K), and thus enlarge our dataset(1.6M). Second, unlike ROCO [28], we maintain the nonradiology images, which makes PMC-OA a more diverse biomedical dataset as shown in Fig. 3. Third, to the best of our knowledge, we are the first to integrate subfigures separation, subcaptions separation and the alignment into the data collection pipeline, which explicitly enlarges our dataset (8 times of MedICaT and 20 times of ROCO), while reducing the noise as much as possible."
PMC-CLIP: Contrastive Language-Image Pre-training Using Biomedical Documents,3,Visual-language Pre-training,"With our constructed image-caption dataset, we further train a visual-language model, termed as PMC-CLIP as shown in Fig. 2 (bottom). We describe the architecture first and then introduce the two training objectives separately.Architecture. Given N image-caption training pairs, i.e., D = {(I i , T i )| N i=1 }, where I i ∈ R H×W ×C represents images, H, W, C are height, width, channel, and T i represents the paired text. We aim to train a CLIP-style visual-language model with an image encoder Φ visual and a text encoder Φ text .In detail, given a specific image-caption pair (I, T ), we encode it separately with a ResNet-based Φ visual and a BERT-based Φ text , the embedding dimension is denoted as d and the text token length as l:where v represents the embedding for the whole image, T refers to the sentence embedding, and t denotes the embedding for [CLS] token."
PMC-CLIP: Contrastive Language-Image Pre-training Using Biomedical Documents,,Image-Text Contrastive Learning (ITC).,"We implement ITC loss following CLIP [29], that aims to match the corresponding visual and text representations from one sample. In detail, denoting batch size as b, we calculate the softmaxnormalized cross-modality dot product similarity between the current visual/text embedding (v / t) and all samples within the batch, termed as p i2t , p t2i ∈ R b , and the final ITC loss is:where y i2t , y t2i refer to one-hot matching labels, CE refers to InfoNCE loss [25]."
PMC-CLIP: Contrastive Language-Image Pre-training Using Biomedical Documents,,Masked Language Modeling (MLM).,"We implement MLM loss following BERT [7]. The network is trained to reconstruct the masked tokens from context contents and visual cues. We randomly mask the word in texts with a probability of 15% and replace it with a special token '[MASK]'. We concatenate the image embedding v with the text token embeddings T , input it into a self-attention transformer-based fusion module Φ fusion , and get the prediction for the masked token at the corresponding position in the output sequence, termed as p mask = Φ fusion (v, T ). Let y mask denote the ground truth, and the MLM loss is:Total Training Loss. The final loss is defined as L = L ITC + λL MLM , where λ is a hyper-parameter deciding the weight of L MLM , set as 0.5 by default.Disscussion. While we recognize a lot of progress in VLP methodology [13,35,36], PMC-CLIP is trained in an essential way to demonstrate the potential of the collected PMC-OA, and thus should be orthogonal to these works."
PMC-CLIP: Contrastive Language-Image Pre-training Using Biomedical Documents,4.1,Pre-training Datasets,"ROCO [28] is a image-caption dataset collected from PubMed [31]. It filters out all the compound or non-radiological images, and consists of 81K samples.MedICaT [34] extends ROCO to 217K samples (image-caption pairs), however, 75% of its figures are compound ones, i.e. one figure with multiple subfigures. [15] is the largest chest X-ray dataset, containing 377,110 samples (image-report pairs). Each image is paired with a clinical report describing findings from doctors."
PMC-CLIP: Contrastive Language-Image Pre-training Using Biomedical Documents,,MIMIC-CXR,"PMC-OA contains 1.65M image-text pairs, which we have explicitly conducted deduplication between ROCO."
PMC-CLIP: Contrastive Language-Image Pre-training Using Biomedical Documents,,Image-Text Retrieval (ITR). ITR contains both image-to-text(I2T,") and text-to-image(T2I) retrieval. We train PMC-CLIP on different datasets, and sample 2,000 image-text pairs from ROCO's testset for evaluation, following previous works [4,5,34]. Note that, as we have explicitly conducted deduplication, the results thus resemble zero-shot evaluation.Classification. We finetune the model for different downstream tasks that focus on image classification. Spcifically, MedMINIST [37] contains 12 tasks for 2D images, and it covers primary data modalities in biomedical images, including Colon Pathology, Dermatoscope, Retinal OCT, etc.Visual Question Answering (VQA). We evaluate on the official dataset split of SLAKE [22], and follow previous work's split [24] on VQA-RAD [18], where SLAKE is composed of 642 images and 14,028 questions and VQA-RAD contains 315 images and 3,515 questions. The questions in VQA-RAD and Slake are categorized as close-ended if answer choices are limited, otherwise openended. The image and text encoders are initialized from PMC-CLIP and finetuned, we refer the reader for more details in supplementary."
PMC-CLIP: Contrastive Language-Image Pre-training Using Biomedical Documents,4.3,Implementation Details,"For the visual and text encoders, we adopt ResNet50 [12] and PubmedBERT [11]. And we use 4 transformer layers for the fusion module. For input data, we resize each image to 224 × 224. During pre-training, our text encoder is initialized from PubmedBERT, while the vision encoder and fusion module are trained from scratch. We use AdamW [23] optimizer with lr = 1 × 10 -4 . We train on GeForce RTX 3090 GPUs with batch size 128 for 100 epochs. The first 10 epochs are set for warming up."
PMC-CLIP: Contrastive Language-Image Pre-training Using Biomedical Documents,5,Result,"We conduct experiments to validate our proposed dataset, and the effectiveness of model trained on it. In Sec. 5.1, we first compare with existing large-scale biomedical datasets on the image-text retrieval task to demonstrate the superiority of PMC-OA. In Sect. 5.2, we finetune the model (pre-trained on PMC-OA) across three different downstream tasks, namely, retrieval, classification, and visual question answering. And we also perform a thorough empirical study of the pretraining objectives and the model architectures in Sect. 5.3. Note that, for all experiments, we use the default setting: ResNet50 for image encoder, and pre-train with both ITC and MLM objectives, unless specified otherwise."
PMC-CLIP: Contrastive Language-Image Pre-training Using Biomedical Documents,5.1,PMC-OA surpasses SOTA large-scale biomedical dataset,"As shown in Table 1, we pre-train PMC-CLIP on different datasets and evaluate retrieval on ROCO test set. The performance can be largely improved by simply switching to our dataset, confirming the significance of it.Table 1. Ablation studies on pre-training dataset."
PMC-CLIP: Contrastive Language-Image Pre-training Using Biomedical Documents,,Methods,Pretrain Data DataSize I2T T2I R@1 R@5 R@10 R@1 R@5 R@10 
PMC-CLIP: Contrastive Language-Image Pre-training Using Biomedical Documents,5.2,PMC-CLIP achieves SOTA across downstream tasks,"To evaluate the learnt representation in PMC-CLIP, we compare it with several state-of-the-art approaches across various downstream tasks, including imagetext retrieval, image classification, and visual question answering.Image-Text Retrieval. As shown in Table 2, we report a state-of-the-art result on image-text retrieval. On I2T Rank@10, PMC-CLIP outperforms previous state-of-the-art by 8.1%. It is worth mentioning that, the training set of ROCO has been used during pretraining in M3AE [4], ARL [5]. While our dataset does not contain data from ROCO. "
PMC-CLIP: Contrastive Language-Image Pre-training Using Biomedical Documents,,Methods,"Pretrain Data DataSize I2T T2I R@1 R@5 R@10 R@1 R@5 R@10ViLT [16] COCO [20], VG [17], SBU, GCC Image Classification. To demonstrate the excellent transferability of PMC-CLIP, we validate it on MedMNIST and compare it with SOTA methods i.e., DWT-CV [6] and SADAE [10]. We present the results of 3 of 12 sub-tests here, and the full results can be found in the supplementary material. As shown in Table 3, PMC-CLIP obtains consistently higher results, and it is notable that finetuning from PMC-CLIP achieves significant performance gains compared with training from scratch with ResNet.Visual Question Answering. VQA requires model to learn finer grain visual and language representations. As Table 4 shows, we surpass SOTA method M3AE in 5 out of 6 results. "
PMC-CLIP: Contrastive Language-Image Pre-training Using Biomedical Documents,5.3,Ablation Study,"Training Objectives. We pre-train PMC-CLIP with different objectives (ITC, MLM) for ablation studies, and summarize the results in the supplementary material (Table 5 in the supplementary). Here, we present a summary of the observations: First, ITC objective is essential for pretraining, and contributes most of the performance. Second, MLM using only text context works as a regularization term. Third, With incorporation of visual features, the model learns finer grain correlation between image-caption pairs, and achieve the best results."
PMC-CLIP: Contrastive Language-Image Pre-training Using Biomedical Documents,,Data Collection Pipeline.,"To demonstrate the effectiveness of subfiguresubcaption alignment, we compare PMC-CLIP with the model pretrained on dataset w/o alignment (Table 6(1-3) in the supplementary). The result verify that subfigure-subcaption alignment reduces dataset's noise thus enhance the pretrained model.Visual Backbone. We have also explored different visual backbones, using the same setting as CLIP [29] (Table 6(4-7) in the supplementary). We observe that all ResNet variants have close performance with RN50, outperforming ViT-B/32, potentially due to the large patch size."
PMC-CLIP: Contrastive Language-Image Pre-training Using Biomedical Documents,6,Conclusion,"In this paper, we present a large-scale dataset in biomedical domain, named PMC-OA, by collecting image-caption pairs from abundant scientific docu-ments. We train a CLIP-style model on PMC-OA, termed as PMC-CLIP, it achieves SOTA performance across various downstream biomedical tasks, including image-text retrieval, image classification, visual question answering. With the automatic collection pipeline, the dataset can be further expanded, which can be beneficial to the research community, fostering development of foundation models in biomedical domain."
PMC-CLIP: Contrastive Language-Image Pre-training Using Biomedical Documents,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3 51.
Physics-Informed Conditional Autoencoder Approach for Robust Metabolic CEST MRI at 7T,1,Introduction,"Chemical exchange saturation transfer (CEST) is a novel metabolic magnetic resonance imaging (MRI) method that allows to detect molecules in tissue based on chemical exchange of their mobile protons with water protons [18]. CEST works by selectively saturating the magnetization of a specific pool of protons, such as those in metabolites or proteins, by applying narrow-band radiofrequency (RF) pulses at their respective Larmor frequency. Due to chemical exchange this saturation state is transferred to the water pool and a decrease in the detected water signal provides information about the concentration and exchange rate of the underlying molecules. This procedure is repeated for several RF frequencies to acquire the so-called CEST-spectrum in each voxel. CEST-MRI offers several promising contrasts that correlate with the diagnosis of diseases such as ischemic stroke [16], brain tumors [1], and neurodegenerative diseases [2,5]. The CESTspectrum contains effects of proton pools of various chemical components in the tissue, typically isolated by a Lorentzian model [14] that is derived form the underlying physics of the Bloch-McConnell equations [8]. In this conventional method, several Lorentzian distributions are fitted to the CEST-spectrum using nonlinear least squares method [10], and the amplitude of each fitted distribution represents a particular metabolic map. The number of Lorentzian functions utilized in this process depends on the expected number of exchanging proton pools present in the spectrum. Figure 1a shows an example of an acquired CEST-spectrum and the corresponding 5-pool Lorentzian fit. Increasing the static magnetic field B 0 (e.g., with B 0 = 7T), enhances spectral resolution, but leads to significant variations in the B 1 amplitude of the saturating RF field across the field of view (cf. Fig. 1c). This B 1 inhomogeneity is corrected by acquiring CEST-spectra at various RF field strengths (cf. Fig. 1b) and then interpolating between them at fixed B 1 to produce the B 1 -robust metabolic CEST contrast maps [14]. This B 1 correction increases the acquisition time at least twofold.Hunger et al. shown that supervised learning can be used to generate B 1robust CEST maps, coining the DeepCEST approach [3,4]. However, the previous work on generating the B 1 -robust CEST contrasts rely on valid target data and the underlying assumptions to generate it, and can only create CEST maps at one particular B 1 level.In this work, we developed a conditional autoencoder (CAE) [13] to generate B 1 -homogeneous CEST-spectra at arbitrary B 1 levels, and a physics-informed autoencoder (PIAE) to fit the 5-pool Lorentzian model to the B 1 corrected CEST-spectra. This inclusion of physical knowledge in the form of known operators in neural nets (NN) is expected to reduce the absolute error margin of the model [6,7] and to increase its interpretability. Both CAE and PIAE are trained in an unsupervised end-to-end method that eliminates the shortcomings of conventional Lorentzian curve fitting and produces robust CEST contrast at arbitrary B 1 levels without the need for an additional acquisition scan. We called the proposed method physics-informed conditional autoencoder (PICAE)."
Physics-Informed Conditional Autoencoder Approach for Robust Metabolic CEST MRI at 7T,2,Methods,"Data Measurements. CEST imaging was performed in seven subjects, including two glioblastoma patients, after written informed consent was obtained to investigate the dependence of CEST effects on B 1 in brain tissue. The local ethics committee approved the study. All volunteers were measured at three B 1 field strengths 0.72 μT, 1.0 μT, and 1.5 μT. A method as described by Mennecke et al. [9] was used to acquire CEST data on a 7T whole-body MRI system (MAGNE-TOM Terra, Siemens Healthcare GmbH, Erlangen, Germany). Saturated images were obtained for 54 non-equidistant frequency offsets ranging from -100 ppm to +100 ppm. The acquisition time per B 1 level was 6:42 min. The acquisition of the B 1 map required an additional 1:06 min.Conditional Autoencoder. We developed a conditional autoencoder (CAE) to solve the B 1 inhomogeneity problem, which is essential for the generation of metabolic CEST contrast maps at 7T. The left part of Fig. 2 describes the CAE. The encoding network of CAE took the raw CEST-spectrum and the corresponding effective B 1 value as input and generate a latent space that was concatenated once with the same B 1 input value and passed to the decoder that reconstruct the uncorrected B 1 CEST-spectrum, and another time the latent space was concatenated with the desired/specific effective B 1 value to reconstruct the CEST-spectrum at a specific B 1 saturation amplitude. Both decoders shared the weights (cf. Fig. 2). For the development of the CAE networks, we used the well-known fully concatenated (FC) layers with leaky ReLU activations except for the last layer of decoder, which had a linear activation. The encoder and decoder both consisted of 4 layers, where the layers of the encoder successively contain 128, 128, 64, 32 neurons, while the layers of the decoder successively contain 32, 64, 128, 128 neurons. The input, latent space and output layers had 55, 17 and 54 neurons respectively.Physics-Informed Autoencoder. The Lorentzian model and its B 1dispersion can be derived from the underlying spin physics described by the Bloch-McConnell equation system [8]. The physics-informed autoencoder (PIAE) utilized fully connected NN as encoder and Lorentzian distribution generator as a decoder to perform the pixel-wise 5-pool Lorentzian curve fit to the CEST-spectrum (water, amide, amine, NOE, MT) [14]. The 5-pool model was described aswhere L denotes the Lorentz function. The direct saturation pool (water) was defined as(The remaining other four pools were defined as) 2 , i ∈ amide, amine, rN OE, ssM T .(The right part of Fig. 2 describes the PIAE. The encoder of PIAE mapped the CEST-spectrum to the amplitudes A i , the full width half maximum (FWHM) τ i , and the water peak position δ DS of the 5-pool Lorentzian model. Its encoder consisted of four FC layers, each with 128 neurons with leaky ReLU activations. It had three so-called FC latent space layers with linear activation for position and exponential activations for FWHM and amplitudes of 5-pool Lorentzian model. The positions of amide, rNOE, ssMT, and amine were fixed at 3.5 ppm, -3.5 ppm, -3 ppm, and 2 ppm, respectively, and shifted with respect to the predicted position of the water peak. The decoder of PIAE consisted of a Lorentzian distribution generator (cf. Fig. 2). It generated samples of the 5-pool distributions exactly at the offsets Δω (i.e. between -100 ppm and 100 ppm) where the input CEST-spectrum was sampled, and combined them according to Eq. 1 to generate the input CEST spectrum with or without B 0 correction.Bound Loss. The peak positions δ i and widths τ i of the pools had to be within certain bounds so that certain neurons in the latent space layer of PIAE would not be exchanged and provide the same pool parameters for all samples. We developed a simple cost function along the lines of the hinge loss [12], called the bound loss. Mathematically, it is defined as followsThe bound loss increases linearly as the output of the latent space neurons of PIAE exceeds or recede from the boundaries. The lower and upper limits for positions and widths are given in Table 1 of the supplementary material.Training and Evaluation. Four healthy volunteers formed the training and validation sets. The test set consisted of the two tumor patients and one healthy subject. To ensure that the outcomes were exclusively based on the CEST-spectrum and not influenced by spatial position, the training was carried out voxel-by-voxel. Consequently, there were approximately one million CESTspectra for the training process. CAE was first trained with MSE loss. In this step, the CAE encoder was fed with the CEST-spectrum of a specific B 1 saturation amplitude, and it generated two CEST-spectra, one for the input B 1 saturation level and the other for the B 1 level injected into the latent space (cf. Fig. 2). Later, it was trained with a combination of MSE loss and perception loss (MSE loss between the latent space of the CEST-spectra at two different B 1 levels). To incorporate perception loss, we used two forward passes with two different B 1 CEST-spectra and used perception loss to generate a latent space that is independent of B 1 saturation amplitude. The following equation describes the loss of the second step.PIAE, on the other hand, was trained with a combination of MSE loss and bound loss. The PIAE loss was described as followsfor evaluation we input the uncorrected CEST-spectrum acquired at 1μT and generated corrected CEST-spectra at B 1 0.5, 0.72, 1.0, 1.3, 1.5 μT. PIAE encoder yielded the amplitudes of 5-pool for B 1 corrected CEST-spectrum. Its decoder reconstructed the B 1 B 0 fitted CEST-spectrum. The B 0 correction simply refers to the shift of the position of the water peak to 0 ppm.CEST Quantification. The multi-B 1 CEST-spectra allow quantification of CEST effects (amide, rNOE, amine) [14,15] down to the exchange rate and concentration. The amplitudes of the CEST contrasts were expressed according to the definition in [15] as followswhere f i , k i , and r 2i express the concentrations, exchange rates, and relaxation rates of the pools. Z ref defines the sum of all 5 distributions at the resonance frequency of the specific pool in B 1 B 0 corrected CEST-spectrum and w 1 is the frequency of the oscillating field.The amplitudes of CEST contrasts in the Lorentzian function have the B 1 dispersion function given by the labeling efficiency α (Eq. 7). The exchange rate occurs here separately from the concentration, which allows their quantification via the B 1 dispersion. Concentration and exchange rate were fitted as a product and denoted as Z 1 (quantified maps), and k(k+r 2 ) was also fitted with the single term Z 2 using trust-region reflective least squares [10]."
Physics-Informed Conditional Autoencoder Approach for Robust Metabolic CEST MRI at 7T,3,Results,"The comparison of PICAE with the conventional method [9,14] is shown in columns 1 and 2 of Fig. 3. The top image in column 3 shows the T 1 -weighted reference image enhanced with the exogenous contrast agent gadolinium (Gd-T 1 w), and the bottom image shows the B 1 -map. The tumor shows a typical so called gadolinium ring enhancement indicated by the arrow (a 15 ), which is also visible in the non-invasive and gadolinium-free CEST contrast maps (columns 1 and 2). The PICAE-CEST maps showed better visualization of this tumor feature compared to the conventional method. The proposed method yielded at least 25% increase in the structural similarity index (SSIM) with the Gd-T 1 w image for the ring enhancement region. The contrast maps also appear less noisy and more homogeneous over the whole brain compared to the Lorentzian fit on the interpolated-corrected B 1 CEST-spectra [14]. To further evaluate the performance of PIAE and CAE, we b1-corrected the data using CAE and fitted it with the least squares method (CAE-Lorentzian fit). The comparison of the CEST maps produced by the conventional Lorentzian fit, the CAE-Lorentzian fit, and PICAE is shown in Table 1 using SSIM and gradient cross correlation (GCC) [11] for the Tumor ring region. Both the CAE-Lorentzian fit and PICAE were better than the conventional method. CAE-Lorentzian fit even outperformed PICAE for rNOE metabolic map and has similar performance for amide, but it has much lower performance for amine. The ability of PICAE to produce B 1 -robust CEST maps at arbitrary levels is shown in Fig. 4, where different B 1 levels reveal different features of the heterogenous tumor. Quantification of chemical exchange rates and concentration, i.e., Z 1 = f•k, is shown in column 4. Z 1 (quantified maps) further improve the visualization of the ring enhancement area. Column 5 shows the Z 2 maps, which are combination of the exchange rate k and the relaxation rate r 2 . Quantified maps of amide, rNOE and amine for another tumor patient is shown in supplementary Fig. 1. The accuracy of the CAE to generate particular B 1 CESTspectra is depicted using absolute error for acquisition at different B 1 levels (see supplementary Fig. 2). The performance was lower for B 1 0.72 μT, and 1.5 μT compared to 1 μT."
Physics-Informed Conditional Autoencoder Approach for Robust Metabolic CEST MRI at 7T,4,Discussion,"In this work, we analyzed the use of an autoencoder approach to generate B 1robust CEST contrast maps at arbitrary B 1 levels, which requires multiple acquisitions in conventional methods [14]. The proposed method reduces the acquisition time by at least half when only two acquisitions are performed for B 1 correction. Supervised learning (DeepCEST) can generate CEST maps that are not susceptible to B 1 inhomogeneity at a particular B 1 , which already reduces acquisition time. However, DeepCEST was trained on data fitted using a conventional pipeline [9,14] which has suboptimal B 1 correction (cf. Fig. 3). Moreover, the different pools in the CEST spectrum are highlighted at different B 1 levels (cf. Fig. 4). An approach that can generate a B 1 -robust CEST-spectrum at multiple B 1 levels allows quantifying the exchange rate and concentration of the CEST pools [15]. The optimal B 1 can often only be selected at post-processing during the analysis of clinical data, as some clinically important features appear better at certain B 1 levels (cf. Fig. 4). The proposed PICAE approach combines B 1 correction and Lorentzian curve fitting in a single step. The B 1 correction  was performed with a CAE, while the Lorentzian line fitting was performed with a PIAE using NN as the encoder and Lorentzian distribution generator as the decoder. This allows interpretation of the model while overcoming the drawback of curve fitting, such as being prone to noise (cf. Fig. 3).The bound loss ensured that the positions of the pools were not interchanged. Quantification was still performed using the nonlinear least squares fit according to Eq. 7. The main reason for this was that it does not affect the acquisition time and it is affected by the Z ref .The training was performed voxel-wise to ensure that the results are based only on the CEST-spectrum. This also results in about 1 million CEST-spectra for the training. Figure 3 shows the superiority of PICAE over the standard method, and Fig. 4 shows that the results produced by PICAE are authentic because the quantification column Z 1 matches the amplitude images and follows Eq. 7. CAE-Lorentzian fitting showed comparable performance for amide and rNOE maps, but significantly lower performance for amine because it was still fitted using the least squares method, which is susceptible to noise in the input and takes up to 5 min to evaluate, compared to PICAE, which takes only a few seconds. The 1 μT acquisition performs better than 0.72 μT and 1.5 μT because it was trained for both lower and higher B 1 values compared to the other two acquisitions. The robustness of method for cyclic consistency [17] is displayed in supplementary Fig. 2, which also shows the interpretability of the method. The results of Fig. 3 and Fig. 4 also show the generalization capability of PICAE as it was trained without the tumor data."
Physics-Informed Conditional Autoencoder Approach for Robust Metabolic CEST MRI at 7T,5,Conclusion,"In this work, we propose a PICAE method for evaluating 7T-CEST MRI that accounts for B 1 inhomogeneity in the input and predicts homogeneous metabolic CEST contrasts at arbitrary B 1 levels. The proposed generative and interpretable method enables (i) a reduction of scan time by at least 50%, (ii) the generation of reliable 7T-CEST contrast maps robust to B 1 inhomogeneity at multiple B 1 levels, (iii) a clear physical interpretation of the B 1 correction of the CESTspectra and the fitting of the Lorentzian model to it, and (iv) the quantification of the CEST contrast maps."
Physics-Informed Conditional Autoencoder Approach for Robust Metabolic CEST MRI at 7T,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3_44.
DeepGraphDMD: Interpretable Spatio-Temporal Decomposition of Non-linear Functional Brain Network Dynamics,1,Introduction,"The human brain has evolved to support a set of complementary and temporally varying brain network organizations enabling parallel and higher-order information processing [16,20,24]. Decoupling these networks from a non-linear mixture of signals (such as functional MRI) and extracting their temporal characteristics in an interpretable manner has been a long-standing challenge in the neuroscience community.Conventional mode/component decomposition methods such as Principal Component Analysis (PCA) or Independent Component Analysis (ICA) assume the modes to be static [7,15,26] and thus sub-optimal for the functional networks generated by time-varying modes. Dynamic Mode Decomposition (DMD) can be treated as a dynamic extension of such component analysis methods since it allows its modes to oscillate over time with a fixed frequency [18]. This assumption is appropriate for the human brain as the functional brain organizations are supported by oscillatory network modes [2,8,11,23,27]. An extension of DMD for network data called GraphDMD [4] preserves the graph structure of the networks during the decomposition. In our work, we extend GraphDMD to a sequence of sliding window based dynamic functional connectivity (dNFC) networks to extract independent and oscillatory functional network modes.Under the hood, GraphDMD regards the network sequence as a linear dynamical system (LDS) where a linear operator shifts the current network state one time-point in the future. The LDS assumption, however, is not optimal for modeling functional brain networks that exhibit complex non-linearity such as rapid synchronization and desynchronization as well as transient events [6]. Articles [3,13] propose switching linear dynamical system (SLDS) to tackle the nonlinearity of spatiotemporal data by a piecewise linear approximation. While these models offer interpretability, their shallow architecture limits their generalizability to arbitrary nonlinear systems. On the other hand, the methods in [5,10,21] model the non-linearity with a deep neural network. While these models have more representation capabilities compared to SLDS, the latent states are not interpretable. More importantly, all of these methods consider the node-level dynamics instead of the network dynamics.Here, we propose a novel Deep Graph Dynamic Mode Decomposition (Deep-GraphDMD) algorithm that applies to arbitrary non-linear network dynamics while maintaining interpretability in the latent space. Our method uses Koopman operator theory to lift a non-linear dynamical system into a linear space through a set of Koopman eigenfunctions (Fig. 1a). There has been a growing line of work that learns these measurement functions using deep autoencoder architectures [14,22]. Training these autoencoders for network data, however, has two unique challenges -1. preserving the edge identity in the latent space so that the network modes are interpretable, 2. enforcing linearity in the latent space for the high dimensional network data. In DeepGraphDMD, we tackle the first challenge by indirectly computing the network embeddings by a novel node embedding scheme. For the second challenge, we introduce a sparse Koopman operator to reduce the complexity of the learning problem. We evaluate the effectiveness of our novel method in both simulated data and resting-state fMRI (rs-fMRI) data from Human Connectome Project."
DeepGraphDMD: Interpretable Spatio-Temporal Decomposition of Non-linear Functional Brain Network Dynamics,2,Methodology,"Let's assume X ∈ R n×t is a matrix containing the BOLD (blood-oxygen-leveldependent) signal of n brain regions (ROIs) in its rows at t time frames sampled at every kΔt time points, where Δt is the temporal resolution. To compute the dynamic connectivity matrix at time point kΔt, a snapshot X k = X :,k:k+s is taken in a sliding window of s time frames. A correlation matrix G k ∈ R n×n is then computed from X k by taking the pearson correlation between the rows of X k , i.e., G ij k = pearson(x i k , x j k ) where x i k , x j k are the i th and j th row of X k respectively. This yields a sequence of graphs) is a matrix containing g k in its columns. The goal is to decouple the overlapping spatiotemporal modes from the network sequence G using -1. Graph Dynamic Mode Decomposition algorithm, and 2. a novel Deep Learning-based Graph Dynamic Mode Decomposition algorithm."
DeepGraphDMD: Interpretable Spatio-Temporal Decomposition of Non-linear Functional Brain Network Dynamics,2.1,Graph Dynamic Mode Decomposition,"GraphDMD [4] assumes that g k follows an LDS:where A ∈ R n 2 ×n 2 is a linear operator that shifts the current state g k to the state at the next time frame g k+1 . To extract the low dimensional global network dynamics, GraphDMD projects A into a lower dimensional space Â using tensortrain decomposition, applies eigendecomposition of Â, and projects the eigenvectors back to the original space which we refer to as dynamic modes (DMs). GraphDMD uses tensor-train decomposition to maintain the network structure of g k and thus, the DMs from GraphDMD can be reshaped into n × n adjacency matrix forms. Let's assume these DMs are Φ 1 , Φ 2 , • • • , Φ r where Φ p ∈ C n×n and the corresponding eigenvalues are λ 1 , λ 2 , • • • , λ r where λ p ∈ C (Fig. 1b). Here, r is the total number of DMs. Φ p corresponds to the coherent spatial mode and λ p defines its temporal characteristics (growth/decay rate and frequencies). We can see this by unrolling Eq. 1 in time:where λ p = a p exp(ω p Δt), Φ † is the conjugate transpose of Φ, b p = vec(Φ † p )g 1 is the projection of the initial value onto the DMD modes, a p = ||λ p || is the growth/decay rate and ω p = Im(ln λ p )/Δt is the angular frequency of Φ p ."
DeepGraphDMD: Interpretable Spatio-Temporal Decomposition of Non-linear Functional Brain Network Dynamics,2.2,Adaptation of Graph-DMD for Nonlinear Graph Dynamics,"Since the dynamics of the functional networks are often non-linear, the linearity assumption of Eq. 1 is sub-optimal. In this regard, we resort to Koopman operator theory to transform the non-linear system into an LDS using a set of Koopman eigenfunctions ψ, i.e., ψ(g k+1 ) = Aψ(g k ) [9]. We learn ψ using a deep autoencoder-based architecture-DeepGraphDMD-where the encoder and the decoder are trained to approximate ψ and ψ -1 , respectively. We enforce ψ(g k ) to follow an LDS by applying Latent Koopman Invariant Loss [22] in the form:two matrices with columns stacked with ψ(g k ) and Y † is the right inverse of Y . After training, we reshape ψ(g k ) into a n × n network ψ(G k ) and generate the latent network sequence ψ(G 1 ), • • • , ψ(G t-s+1 ). We then apply GraphDMD (described in Sect. 2.1) on this latent and linearized network sequence to extract the DMs Φ p and their corresponding λ p .However, there are two unique challenges of learning network embeddings using the DeepGraphDMD model: 1. the edge identity and, thus, the interpretability will be lost in the latent space if we directly embed g k using ψ, and 2. Y † doesn't exist, and thus L lkis can't be computed because Y is low rank with the number of rows n(n-1)"
DeepGraphDMD: Interpretable Spatio-Temporal Decomposition of Non-linear Functional Brain Network Dynamics,,2,">> the number of columns t -s + 1. To solve the first problem, instead of learning ψ(g k ) directly, we embed the BOLD signal x i k of each ROI independently using the encoder to learn the latent embeddings z i k (Fig. 1a). We then compute the pearson correlation between the latent embeddings of the ROIs to get the Koopman eigenfunctions of g k i.e., ψ(g ij k ) = pearson(z i k , z j k ). The weights of the encoder and decoder are shared across the ROIs.The second problem arises because the Koopman operator A regresses the value of an edge at the next time-point as a linear combination of all the other edges at the current time-point, i.e., g ij k+1 =N p,q=1 w pq g pq k . This results in O(n2 ) covariates with t -s + 1 << O(n 2 ) samples making the regression ill-posed. We propose a sparse Koopman operator where each edge g ij k is regressed using only the edges that share a common end-point with it, i.e., g ij k+1 = n p=1,p =i,j w ip g ip k + n q=1,q =i,j w qj g qj k +w ij g ij k (Supplementary Fig. 1). Since there are only O(n) such edges, it solves the ill-posedness of the regression.Other than L lkis , we also train the autoencoder with a reconstruction loss L recon which is the mean-squared error (MSE) between x i k and the reconstructed output from the decoder xi k . Moreover, a regularizer L reg in the form of an MSE loss between g k and the latent ψ(g k ) is also added. The final loss is the following:where α and β are hyper-parameters. We choose α, β, and other hyperparameters using grid search on the validation set. The network architecture and the values of the hyper-parameters of DeepGraphDMD training are shown in Supplementary Fig. 1. The code is available in1 ."
DeepGraphDMD: Interpretable Spatio-Temporal Decomposition of Non-linear Functional Brain Network Dynamics,2.3,Window-Based GraphDMD,"We apply GraphDMD in a short window of size 64 time frames with a step size of 4 time frames instead of the whole sequence G because, in real-world fMRI data, both the frequency and the structure of the DMs can change over time.We then combine the DMs across different sliding windows using the following post-processing steps:Post-processing of the DMs: We first group the DMs within the frequency bins: 0-0.01 Hz, 0.01-0.04 Hz, 0.04-0.08 Hz, 0.08-0.12 Hz, and 0.12-0.16 Hz. We then cluster the DMs within each frequency bin using a clustering algorithm and select the cluster centroids as the representative DMs (except for the first bin where we average the DMs). We chose the optimal clustering algorithm to be Spherical KMeans [1] (among Gaussian Mixture Model, KMeans, Spherical KMeans, DBSCAN, and, KMedoids) and the optimal number of clusters to be 3 for every frequency bin based on silhouette analysis [17] (Supplementary Fig. 2). We use this frequency binning technique to allow for slight variations of ω of a DM over the scanning session. To align these representative DMs across subjects, we apply another round of spherical clustering on the DMs from all subjects and align them based on their cluster memberships."
DeepGraphDMD: Interpretable Spatio-Temporal Decomposition of Non-linear Functional Brain Network Dynamics,3.1,Dataset,"We use rs-fMRI for 840 subjects from the HCP Dense Connectome dataset 2 [25].Each fMRI image was acquired with a temporal resolution (Δt) of 0.72 s and a 2 mm isotropic spatial resolution using a 3T Siemens Skyra scanner. Individual subjects underwent four rs-fMRI runs of 14.4 min each (1200 frames per run). Group-ICA using FSL's MELODIC tool [7] was applied to parcellate the brain into 50 functional regions (ROIs). To find the correlation between cognition with the rs-fMRI data, we select two behavioral measures related to fluid intelligence: CogFluidComp, PMAT24 A CR and one measure related to crystallized intelligence: ReadEng, and, the normalized scores of the fluid and crystallized cognition measures: CogTotalComp. We regress out the confounding factors: age, gender, and head motion from these behavioral measures using ordinary least squares [12]."
DeepGraphDMD: Interpretable Spatio-Temporal Decomposition of Non-linear Functional Brain Network Dynamics,3.2,Baseline Methods,"We compare GraphDMD and DeepGraphDMD against three decomposition methods: Principal Component Analysis (PCA), Independent Component Analysis (ICA), and standard Dynamic Mode Decomposition (DMD) [18]. We use the sklearn decomposition library for PCA 3 and ICA4 and the pyDMD5 library for standard DMD. We apply PCA and ICA on g, and DMD directly on the bold signal X instead of g (for reasons described in Sect. 4.2). We choose the number of components (n_components) to be three for these decomposition methods, (Results for other n_components values are shown in Supplementary Table 1). The components are aligned across subjects using spherical clustering similar to the GraphDMD modes (Sect. 2.3). We also compare with static functional connectivity (sFC), which is the pairwise pearson correlation between brain regions across all time frames."
DeepGraphDMD: Interpretable Spatio-Temporal Decomposition of Non-linear Functional Brain Network Dynamics,3.3,Simulation Study,"We generate a sequence of dynamic adjacency matrices G using Eq. 2 from three time-varying modes Φ 1 , Φ 2 , Φ 3 with corresponding frequencies ω 1 ∼ N (0.1, 0.05), ω 2 ∼ N(1, 0.1), ω 3 ∼ N(2.5, 0.1) (Hz). Each Φ p is a 32 × 32 block diagonal matrices with block sizes 16, 8, and 4. We choose a 1 = 1.01, a 2 = 0.9, a 3 = 1.05 and b 1 = b 2 = b 3 = 1. We simulate the process for k = 1, • • • , 29 time-points yielding a sequence of 30 matrices of shape 32 × 32. We repeat the process ten times with different ω 1 , ω 2 , ω 3 and generate ten matrix sequences. We apply PCA, ICA, and GraphDMD (Sect. 2.1) on G to extract three components and compare them against the ground truth modes using pearson correlation."
DeepGraphDMD: Interpretable Spatio-Temporal Decomposition of Non-linear Functional Brain Network Dynamics,3.4,Application of GraphDMD and DeepGraphDMD in HCP Data,"Comparison of DMs with sFC: The ground truth DMs are unknown for the HCP dataset; however, we can use the sFC as a substitute for the ground truth DM with ω = 0 (static DM). sFC offsets the DMs with ω > 0 as they have both positive and negative cycles, and thus only retain the static DM. For comparison, we compute the pearson correlation between the DM within the frequency bin 0-0.01 Hz and sFC for both GraphDMD and DeepGraphDMD. For PCA and ICA, we take the maximum value of the correlation between sFC and the (PCA or ICA) components."
DeepGraphDMD: Interpretable Spatio-Temporal Decomposition of Non-linear Functional Brain Network Dynamics,,Regression Analysis of Behavioral Measures from HCP:,"In this experiment, we regress the behavioral measures with the DMs within each frequency bin (Sect. 2.3) using Elastic-net. As an input to the Elastic-net, we take the real part of the upper diagonal part of the DM and flatten it into a vector. We then train the Elastic-net in two ways-1. single-band: where we train the Elastic-net independently with the DMs in each frequency bin, and 2. multi-band: we concatenate two DMs in the frequency bins: 0-0.01 Hz and 0.08-0.12 Hz and regress using the concatenated vector. For evaluation, we compute the correlation coefficient r between the predicted and the true values of the measures."
DeepGraphDMD: Interpretable Spatio-Temporal Decomposition of Non-linear Functional Brain Network Dynamics,4.1,Simulation Study,"In Fig. 2a, we show the results after applying PCA, ICA, and, GraphDMD on the simulated data described in Sect. 3.3. Since the DMs in this data are oscillating, the data generated from this process are more likely to be overlapping compared to when the modes are static. As a result, methods that assume static modes, such as PCA and ICA, struggle to decouple the DMs and discover modes in overlapping high-density regions. For example, in Mode 2 of ICA, we can see the remnants of Mode 3 in the blue boxes and Mode 1 (negative cycle) in the orange boxes. We observe similar scenarios in the Mode 3 of ICA and Mode 1, Mode 2, and Mode 3 of PCA (the red boxes). On the other hand, the DMs from GraphDMD have fewer remnants from other modes and closely resemble the ground truth. To empirically compare, the mean (± std) pearson correlation for PCA, ICA, and, GraphDMD are 0.81(±0.04), 0.88(±0.03), and 0.98(±0.01)."
DeepGraphDMD: Interpretable Spatio-Temporal Decomposition of Non-linear Functional Brain Network Dynamics,4.2,Application of GraphDMD and DeepGraphDMD in HCP Data,"Comparison of DMs with sFC: The average pearson correlations with sFC across all the subjects are 0.6(±0.09), 0.6(±0.09), 0.84(±0.09), and 0.86(±0.05) for PCA, ICA, GraphDMD, and, DeepGraphDMD (Fig. 2b) respectively. This shows that the DMD-based methods can robustly decouple the static DM from time-varying DMs. In comparison, the corresponding PCA and ICA component has significantly lower correlation due to the overlap from the higher frequency components."
DeepGraphDMD: Interpretable Spatio-Temporal Decomposition of Non-linear Functional Brain Network Dynamics,,Regression Analysis of Behavioral Measures from HCP:,"We show the values of r across different methods in Table 1. We only show the results for two frequency bins 0-0.01 Hz and 0.08-0.12 Hz, as the DMs in the other bins are not significantly correlated (r < 0.2) with the behavioral measures (Supplementary Table 2). The table shows that multi-band training with the DMs from the DMD-based methods significantly improves the regression performance over the baseline methods. Compared to sFC, GraphDMD improves r by 22%, 6%, 0.7%, and, 3% for CogFluidComp, PMAT24 A CR, ReadEng, CogTotalComp, respectively and DeepGraphDMD further improves the performance by 5%, 2.2%, 0.7%, and, 1.5%, respectively. Significant performance improvement for CogFluidComp can be explained by the DM within the bin 0.08-0.12 Hz. This DM provides additional information related to fluid intelligence (r = 0.227 for GraphDMD) to which the sFC doesn't have access. By considering non-linearity, DeepGraphDMD extracts more robust and less noisy DMs (Fig. 2b-c), and hence, it improves the regression performance by 8% compared to GraphDMD in this frequency bin. By contrast, the standard DMD algorithm yields unstable modes with a p << 1 when applied to the network sequence G. These modes have no correspondence across subjects and thus can't be used for regression. We instead apply DMD on the BOLD signal X, but the DMD modes show little  Graph DMD 0-0.01 0.254 ± 0.003 0.289 ± 0.004 0.402 ± 0.004 0.438 ± 0.003 0.08-0.12 0.227 ± 0.004 0.193 ± 0.004 0.145 ± 0.004 0.248 ± 0.004 0-0.01, 0.08-0.12 0.308 ± 0.004 0.312 ± 0.004 0.410 ± 0.003 0.454 ± 0.004 Deep Graph DMD 0-0.01 0.259 ± 0.003 0.290 ± 0.002 0.404 ± 0.002 0.439 ± 0.002 0.08-0.12 0.245 ± 0.002 0.201 ± 0.004 0.144 ± 0.003 0.251 ± 0.004 0-0.01, 0.08-0.12 0.325 ± 0.003 0.319 ± 0.003 0.413 ± 0.002 0.461 ± 0.003 correlation with the behavioral measures. PCA and ICA perform significantly worse than the baseline sFC method for all behavioral measures.Traditional dynamical functional connectivity analysis methods (such as sliding window-based techniques) consider a sequence of network states. However, our results show that these states can be further decomposed into more atomic network modes. The importance of decoupling these network modes from nonlinearly mixed fMRI signals using DeepGraphDMD has been shown in regressing behavioral measures from HCP data."
DeepGraphDMD: Interpretable Spatio-Temporal Decomposition of Non-linear Functional Brain Network Dynamics,5,Conclusion,"In this paper, we proposed a novel algorithm-DeepGraphDMD-to decouple spatiotemporal network modes in dynamic functional brain networks. Unlike other decomposition methods, DeepGraphDMD accounts for both the non-linear and the time-varying nature of the functional modes. As a result, these functional modes from DeepGraphDMD are more robust compared to their linear counterpart in GraphDMD and are shown to be correlated with fluid and crystallized intelligence measures."
DeepGraphDMD: Interpretable Spatio-Temporal Decomposition of Non-linear Functional Brain Network Dynamics,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3 35.
BrainUSL: Unsupervised Graph Structure Learning for Functional Brain Network Analysis,1,Introduction,"Recent studies have shown that rs-fMRI based analysis for brain functional connectivity (FC) is effective in helping understand the pathology of brain diseases [8,17,25]. The functional connectivity in the brain network can be modeled as the graph where nodes denote the brain regions and the edges represent the correlations between those regions [6]. Hence, the brain disease identification can be seen as the graph classification with the refined graph structures [28].The representation learning of brain network heavily relies on the graph structure quality. The existing brain network construction methods [16,23] are often noisy or incomplete due to the inevitably error-prone data measurement or collection. The noisy or incomplete graphs often lead to unsatisfactory representations and prevent us from fully understanding the mechanism underlying the disease. In pursuit of an optimal graph structure for graph classification, recent studies have sparked an effort around the central theme of Graph Structure Learning (GSL), which aims to learn an optimized graph structure and corresponding graph representations. However, most works for GSL rely on the human annotation, which plays an important role in providing supervision signals for structure improvement. Since the fMRI data is expensive and limited, unsupervised graph structure learning is urgently required [1,4,14]. Moreover, disease interpretability is essential as it can help decision-making during diagnosis.Considering the above issues, we aim to discover useful graph structures via a learnable graph structure from the BOLD signals instead of measuring the associations between brain regions by a similarity estimation. In this paper, we propose an end-to-end unsupervised graph structure learning framework for functional brain network analysis (BrainUSL) directly from the BOLD signals.The unsupervised graph structure learning consists of a graph generation module and the topology-aware encoder. We propose three loss functions to constrain the graph structure learning, including the sparsity-inducing norm, the view consistency regularization and the correlation-guided contrastive loss. Finally, the generated graph structures are used for the graph classification. We evaluate our model on two real medical clinical applications: Bipolar Disorder diagnosis and Major Depressive Disorder diagnosis. The results demonstrate that our BrainUSL achieves remarkable improvements and outperforms state-of-the-art methods. The main contributions of this paper are summarized below:-We propose an end-to-end unsupervised graph structure learning method for functional brain network analysis. -We propose the correlation-guided contrastive loss to model the correlations between graphs by defining the sample correlation estimation matrix. -Our method provides a perspective for disease interpretable analysis and association analysis between BD and MDD. -The experimental results demonstrate the advantage of the proposed method in brain disorder diagnosis."
BrainUSL: Unsupervised Graph Structure Learning for Functional Brain Network Analysis,2,Method,"The constructed graph structure of brain network in existing works are often noisy or incomplete. To address this issue, we propose a novel unsupervised graph structure learning method, including the graph generation module for generating optimized sparsity-induced graphs and the topology-aware encoder for capturing the topological information in graphs, as illustrated in Fig. 1. Then, we propose a new objective function for unsupervised graph structure learning from the perspectives of constraining structure sparsity as well as view consistency and preserving discriminative patterns at the same time."
BrainUSL: Unsupervised Graph Structure Learning for Functional Brain Network Analysis,2.1,Graph Generation Module,"To exploit the information in fMRI signals for generating the optimized sparsityinduced graph structure, we propose a graph generation module, which containing a graph generation module contains BOLD signal feature aggregation (E f ) with a stack of convolutional layers [7] for learning the low-dimensional BOLD signal features. The feature is learned as follows:where K (l) is a convolutional kernel of l-th layer with a kernel size of U and u denotes the BOLD signal element in a brain region of the input x. With the feature learned by E f , we generate the optimized graph A G by calculating the correlation among the nodes."
BrainUSL: Unsupervised Graph Structure Learning for Functional Brain Network Analysis,2.2,Topology-Aware Encoder,"The graph topological information is crucial for graph embedding learning. Motivated by BrainNetCNN [5], we propose a Topology-aware Encoder for exploiting the spatial locality in the graph structure through a hierarchical local (edge)-toglobal (node) strategy by aggregating the embeddings of the connections associated with the nodes at the two ends of each edge. The topology-aware encoder involves an operator of edge aggregation ( E g ) with multiple cross-shaped filters for capturing the spatial locality in the graph and node aggregation ( E n ) for aggregating the associated edge. The cross-shaped filters in edge aggregation involve a combination of 1 × M and M × 1 basis filters with horizontal and vertical orientations, which are defined as:where w r ∈ R 1×M and w h ∈ R M ×1 denote the learned vectors of the horizontal and vertical convolution kernel, M denotes the number of ROIs, A and H g denote the adjacency matrix and the edge embeddings. With the learned edge embeddings, we further learn the node embeddings by aggregating the associated edges with the nodes with a learnable layer. More specifically, the node aggregation takes the edge embedding as the inputs and obtains the node embedding from a node-wise view by a 1D convolutional filter. The node aggregation is defined as:where H n ∈ R M ×d is the node embedding, and w n ∈ R 1×M is the learned vector of the filter, and d is the dimensionality of the node embeddings."
BrainUSL: Unsupervised Graph Structure Learning for Functional Brain Network Analysis,2.3,Objective Functions,"To better exploit the graph structure, we design three loss functions including a sparsity-inducing norm, a view consistency regularization and a correlationguided contrastive loss. We assume that the sparsity of the generated graphs allows for the preservation of the important edges and removal of noise. To achieve this, we utilize an l 1 norm to remove the irrelevant connections and preserve the sparsity of the generated graphs. Furthermore, we introduce a view consistency regularization to ensure the consistency of two views by maximizing the agreement between the node embeddings learned from the fixed graph structure A P and learnable graph structure A G . The view consistency regularization is defined as L vc = N i=1 sim(e i , êi ), where sim(•, •) is a cosine similarity measure, N denotes the number of samples, êi and e i represent the i-th graph embeddings from A P and A G .The motivation of contrastive learning is to capture the graph embeddings by modeling the correlations between graphs [24]. However, it produces bias depending on the simple data augmentations, which can degrade performance on downstream tasks. Hence, we introduce the graph correlation estimation by graph kernel [27] to construct the correlation matrix S ∈ N × N . Then, we binarize the matrix by a simple thresholding with a threshold value θ. If S ij ≥ θ, we set S ij to 1, which indicates that the i-th and j-th samples are regarded as a positive sample pair. Otherwise, S ij is set to 0, indicating they are considered as a negative sample pair. With the estimated positive and negative pairs, the correlation-guided contrastive loss is defined as:where 1(•) = {0, 1} is an indicator function, and τ is a temperature factor to control the desired attractiveness strength.The final objective function is formulated as:where α and β are the trade-off hyper-parameters. Finally, based on the generated graphs and pre-trained topology-aware encoder, we leverage the multi-layer perceptron (MLP) with the cross-entropy loss for the graph classification.3 Experiments and Results"
BrainUSL: Unsupervised Graph Structure Learning for Functional Brain Network Analysis,3.1,Dataset and Experimental Details,We evaluated our BrainUSL on a private dataset constructed from Nanjing Medical University (NMU) for BD and MDD diagnosis by repeating the 5-fold crossvalidation 5 times with different random seeds. We deal with the original fMRI data by dpabi [20]  
BrainUSL: Unsupervised Graph Structure Learning for Functional Brain Network Analysis,3.2,Classification Results,"We compare our BrainUSL with state-of-the-art models in terms of Accuracy (ACC), Area Under the Curve (AUC), Sensitive (SEN) and Specificity (SPEC). The comparable methods can be grouped into two categories: traditional methods including FC+SVM/RF [21] and deep learning methods including GroupINN [22], ASD-DiagNet [2], MVS-GCN [18], ST-GCN [3] and Brain-NetCNN [5].Comparison with SOTA. Compared with state-of-the-art methods, the proposed BrainUSL generally achieves the best performance on MDD and BD identification, the results are shown in Table 1. Specifically, the results show that our BrainUSL yields the best ACC and AUC results on MDD (ACC = 76.7% and AUC = 75.3%) and BD (ACC = 77.3% and AUC = 74.4%), compared to the existing brain disease diagnosis approaches. Moreover, Fig. 2 illustrates the influence of the pre-training epochs of BrainUSL on the classification performance. It can be found that the performance improves with the pre-training epochs increasing until 40/60 epochs for BD/MDD, which demonstrates that more pretraining epochs help capture accurate structural representation. In addition, the similar sparsity patterns are observed for both the diagnosis of two disorders.The results demonstrate that our generated graphs are more discriminative than the graphs constructed by pearson correlation coefficient, which confirms that the quality of the graph structure is critical for functional brain network representation learning, and noisy or redundant connections in brain network impede understanding of disease mechanisms.Ablation Study. There are three parts in our final objective function. Next, we perform a sequence of ablation studies on the three parts of our model. As  shown the third part in Table 1, all the proposed loss functions obviously improve the classification performance, showing the crucial role of the each component and the complementary one another. Therefore, our results demonstrate that the graph structure constructed in an unsupervised manner can provide the potential correlations and discriminative information between brain regions precisely."
BrainUSL: Unsupervised Graph Structure Learning for Functional Brain Network Analysis,3.3,Functional Connectivity Analysis,"We use BrainNetViewer [19] to illustrate the discriminative top-10 connections identified by our method for brain disease diagnosis in Fig. 3. Neuroimaging studies have demonstrated that the subnetworks of SN, CEM, and DMN are often co-activated or deactivated during emotional expression task. We find that some identified connections in MDD and BD such as Frontal-Mid-R between Frontal-Sup-Medial-L and Angular-L between Thalamus-L are the key connections in DMN, CEN and SN, which demonstrates that our model can generate the discriminative brain structure and facilitate the identification of biomarkers [10][11][12]. Furthermore, as shown in Fig. 4, by comparing the graphs constructed by PCC and BrainUSL, it can be observed that our method produces a sparser structure for the brain network, indicating that only a small portion of the functional connections are relevant to the outcome. The results indicate that the generated graph structures via unsupervised learning can effectively reflect the intrinsic connections between brain regions caused by brain disorders."
BrainUSL: Unsupervised Graph Structure Learning for Functional Brain Network Analysis,3.4,Association of Brain Diseases,"A number of studies [9,15] have demonstrated there exists associations between different psychiatric disorders [13], patients with one psychiatric disorder are more susceptible to other psychiatric disorders. We evaluate the association between the disorders and the transfer learning ability of our model. Specifically, We pre-train our model on one dataset, then fine-tune and evaluate it on the other dataset. The results are illustrated in the last part of Table 1. We observed that the transfer learning between two different brain disorder disease also achieve a better results compared with other methods. The result indicates that the two diseases are correlated, which is consistent with the existing study results [26]. Moreover, the results also indicate that our model learns more transferable representations and provides a perspective for the study of disease associations through transfer learning on the functional brain network analysis."
BrainUSL: Unsupervised Graph Structure Learning for Functional Brain Network Analysis,4,Conclusion,"Due to the inevitably error-prone data measurement or collection, the functional brain networks constructed by existing works are often noisy and incomplete.To address this issue, we propose the unsupervised graph structure learning framework for functional brain network analysis (BrainUSL), which generates the sparse and discriminative graph structures according to the characteristics of the graph data itself. We conducted extensive experiments on the NMU dataset which indicate that our BrainUSL achieves promising performance with the SOTA methods. In addition, we discuss the interpretability of our model and find discriminative correlations in functional brain networks for diagnosis."
BrainUSL: Unsupervised Graph Structure Learning for Functional Brain Network Analysis,,Table 1 .,"(90 females and 37 males, age range 17-34 years) and 102 BDs (76 females and 26 males, age range 16-32 years), who were scanned at a single site with identical inclusion and exclusion criteria."
BrainUSL: Unsupervised Graph Structure Learning for Functional Brain Network Analysis,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3_20.
FE-STGNN: Spatio-Temporal Graph Neural Network with Functional and Effective Connectivity Fusion for MCI Diagnosis,1,Introduction,"Mild cognitive impairment (MCI) is a prodromal stage of memory loss or other cognitive loss (e.g., language, visual and spatial perception) in individuals that may progress to Alzheimer's disease (AD), which is a neurological brain disease that leaves individuals without the ability to live independently with daily activities [1]. Diagnosis of MCI would allow on-time intervention to delay or prevent the progress of the disease, which is crucial for advanced AD without effective treatment.Resting-state functional magnetic resonance (rs-fMRI) as a non-invasive tool has demonstrated its potential to evaluate brain activity by measuring the blood oxygenation level-dependent (BOLD) signals over time [22]. Based on this, researchers model brain networks to analyze brain function and realize the diagnosis of brain diseases [13]. For example, compared with healthy controls, subjects with AD and MCI have shown changes in brain networks based on rs-fMRI data [14], manifested explicitly in the weakened connection between the right hippocampus and default mode network (DMN, a set of brain regions), and the enhanced connection between the left hippocampus with right lateral prefrontal lobe brain region, etc.A brain network can be represented as a graph structure naturally, which consists of brain regions as nodes and connections among brain regions as edges [20]. Considering the characteristics of functional integration and separation in the brain, brain connectivity is typically modeled in three different patterns: namely, structural connectivity (SC) [18], functional connectivity (FC) [21], and effective connectivity (EC) [12]. Structural connectivity refers to a network of physical or anatomical connections linking sets of neurons, which characterize the associated structural biophysical attributes extracted from diffusion-weighted imaging (DWI) data. Differently, both functional and effective connectivity can be generated from fMRI data. FC describes brain activity from the perspective of statistical dependencies between brain regions, while EC emphasizes directional causal interactions of one brain region over another.Currently, with the development of deep learning, various methods have been proposed to diagnose brain disease using rs-fMRI modeled with brain connectivity [7,11]. Among them, graph neural network-based (GNN) methods stand out from the others for their ability to capture network topology information [6]. However, there are three critical disadvantages to existing GNN-based methods for brain disease diagnosis. 1) Existing studies generally use a single brain connectivity pattern to model brain activities, which would restrict the following analysis to a single space of brain network characteristics. 2) Most of the GNNbased methods applied to brain diseases focus on extracting static topological information neglecting the dynamic nature of brain activity. 3) Many approaches seek to enhance the model's ability for disease diagnosis, while the underlying mechanism remains a black box, making it challenging to provide an explainable basis corresponding to brain neural activity.Therefore, we propose Spatio-Temporal Graph Neural Network with Functional and Effective Connectivity Fusion (FE-STGNN), which mainly focuses on the fusion of local spatial structure information from FC and temporal causal evolution information from EC, for MCI diagnosis using rs-fMRI. As FC focuses on describing the strength of brain connections in each time slice, while EC for characterizing the dynamic information flow. Thus, our motivation is to utilize the causal linkage of EC during time evolution to guide the fusion of FC networks at discrete time slices. We summarize the novelty of the proposed method in three aspects:1) We propose to model brain connectivity using FC and EC simultaneously, comprehensively considering the structural characteristics of the brain network and time-evolving properties of causal effects between brain regions. 2) Considering that FC networks focus on describing the strength of brain connections, while ECs are more detailed about the directional information flow among brain regions, we design a novel graph fusion framework based on a cross-attention mechanism that leads FCs to aggregate temporal structure information under EC guidance. 3) We track the model in the MCI diagnostic task and evaluate the importance of different brain regions for the impact of disease, which gives an explainable basis in combination with the background of biomedical knowledge."
FE-STGNN: Spatio-Temporal Graph Neural Network with Functional and Effective Connectivity Fusion for MCI Diagnosis,2,Method,"Figure 1 illustrates the framework of the proposed spatio-temporal graph neural network with functional and effective connectivity fusion, which consists of three main sequential components: 1) Dynamic FC and EC graph matrices construction; 2) Extraction module for local spatial structural features and short-term temporal evolution characteristics using spatial graph convolutional neural network (GCN); 3) Spatio-temporal fusion positional transformer. Specifically, we first construct dynamic FC networks by calculating the correlations between BOLD signals within sliding windows and build dynamic EC networks based on the directional information transmission between two brain regions at adjacent moments. Then, the graph features of the directed and undirected brain networks are extracted respectively to obtain the spatial structural correlation and short-term temporal evolution characteristics. Finally, a fusion framework based on the attention mechanism is designed to obtain the spatio-temporal features of the brain network under the guidance of EC networks, and the FE-STGNN model is used for MCI diagnosis."
FE-STGNN: Spatio-Temporal Graph Neural Network with Functional and Effective Connectivity Fusion for MCI Diagnosis,2.1,Local Spatial Structural Features and Short-Term Temporal Characteristics Extraction,"Bi-Graph Construction. To construct dynamic FC and EC networks captured by rs-fMRI, we first partition the T length BOLD signals into multiple time segments using a sliding window with window-length w and stride s. The total number of segments is)} be a set of undirected FC graphs, where V is a finite set of nodes |V | = N corresponding to each brain Regions of Interests (ROI), t varies from 1 to K indicating the time segment, and the adjacency matrix A t F C ∈ R N ×N consisting of edge element e vu can be obtained by:where ρ(v t , u t ) measuring the Pearson Correlation [3] between v-th and u-th brain ROIs in t-th time segment. Meanwhile, for directed EC networks we adopt Transfer Entropy [16] as a directional information-theoretic measure due to its advantages in detecting nonlinear interactions between neurons. Therefore, we have EC graphs for each t-th segment with {G|G t EC = (V, A t EC )}, where the edge element e vu can be computed through ζ(v t → u t+1 ), which measures the directed information of Transfer Entropy between two ROIs:Spatial Graph Convolution. The existing graph convolution methods are mainly divided into spectral methods and spatial methods. Here, we choose the spatial graph convolution for two reasons. First, the spatial method aggregates information based on the topological structure of the graph and pays close attention to the important spatial structural information in brain networks. Second, the spectral method cannot be applied to directed graphs because of the symmetry requirements of the spectral normalized Laplace matrix.From the perspective of each node, spatial graph convolution can be described as a process of message passing and node updating [10], i.e., the aggregation of hidden features of local neighborhoods combined with transformations. Therefore, the spatial graph convolution for each layer can be obtained by two steps:where N (v) denotes the neighbors of v in the graph, h l v and h l u are hidden representations of node v and u in l-th layer respectively. e vu as the edge weights are elements of the adjacent matrix, and m l+1 v is the message aggregated from the target node v combined with local neighborhoods. The message functions M l and node update functions U l are all learned differentiable functions. We further aggregate the representations of all nodes in the graph G from the final convolution layer (i.e., L-th layer) to obtain FC and EC graph embeddings} in each time segment respectively, where d H is the feature dimension of the graph embedding.Here, the readout function R can be a simple permutation invariant function such as graph-level pooling function."
FE-STGNN: Spatio-Temporal Graph Neural Network with Functional and Effective Connectivity Fusion for MCI Diagnosis,2.2,Spatio-Temporal Fusion with Dynamic FC and EC,"Fusion Positional Transformer. After obtaining the graph representations of FC and EC in each time segment, we further design a graph fusion network based on attention mechanism, using the causal linkage of EC in time to guide the fusion of FC networks. Considering that the attention itself does not have position awareness, it is treated equally in attention at any time segment. We first introduce a learnable temporal positional encoding P ∈ R K×dH to encode the time sequence properties of the brain network, which is randomly initialized and added to FC embeddings before attention.The attention mechanism can be described as mapping a query and a set of key-value pairs to an output which is computed as a weighted sum of the values. In practice, we first conduct linear transforms on dynamic FCs, which are computed in the form of a matrixcombined with the temporal positional encoding. The dynamic EC are also linear transformed in the form of matrixTherefore, both dynamic FC and EC are encoded into high-dimensional latent subspaces, including the query subspace Q ∈ R K×d k , the value subspace V ∈ R K×d k from FC, and the key subspace K ∈ R N ×d k from EC simultaneously, where d k is the dimension of latent subspace. The conducting progress can be formulated as:where W Q , W K , and W V are the weight matrices for Q, K, and V , respectively. We further calculate the spatio-temporal dependencies of the brain network with the dot product of FCs' query and ECs' key. Therefore, the attention of FC and EC fusion can be obtained by:Here, the softmax is used to normalize the spatio-temporal dependencies and the scale √ d k prevents the saturation led by softmax function. To ensure the stable training, we adopt the residual connection formulated as M = M + (H F C +P ). Furthermore, a feed-forward neural network with nonlinear activation combined with another residual connection layer are applied to further improve the prediction conditioned on the embeddings M . As a result, the output of the fusion positional transformer is Û = ReLU MW 0 + b 0 + M , where W 0 and b 0 are learnable weight and bias.Prediction Diagnosis. In the end, the prediction layer leverages a fully connected network to map the output embeddings of the fusion positional transformer into the corresponding label space, therefore we can obtain the prediction result Y = ReLU ÛW 1 + b 1 with W 1 and b 1 are learnable weight and bias. In the training process, we design the loss function as shown in Eq. 7. The first term is used to minimize the error between the real diagnosis result Ŷ and the prediction Y . The second term L reg is the L2 regularization term that helps to avoid an overfitting problem with λ as a hyper-parameter.3 Experiments"
FE-STGNN: Spatio-Temporal Graph Neural Network with Functional and Effective Connectivity Fusion for MCI Diagnosis,3.1,Dataset and Experimental Settings,"In order to verify the performance of our method on Alzheimer's Disease Neuroimaging Initiative (ADNI) data [17], we adopt the individual rs-fMRI images including T = 137 time points consisting of 60 normal controls (NC) and 54 MCI subjects, following the standard pre-processing pipeline same to [4]. Considering that medical imaging data in the real world is often insufficient for deep learning, we randomly selected balanced data with small samples for experiments. We perform 10-fold cross-validation 10 times for all the experiments, a sliding window of w = 37 and stride s = 10 is used to construct dynamic brain networks for all the dynamic FC-based methods. We train our model with parameters: λ = 0.01 weight of the regularization term, 1e -3 learning rate, and a maximum number of 800 epoches. We performed a grid search to determine these hyper-parameters. "
FE-STGNN: Spatio-Temporal Graph Neural Network with Functional and Effective Connectivity Fusion for MCI Diagnosis,3.2,Ablation Studies,"Here we conduct ablation studies to verify the effectiveness of 1) the fusion of dynamic FC and EC networks and 2) each module in the proposed FE-STGNN. Specifically, we first replace graph convolutional neural network (GCN) with graph attention network (GAT) in the spatial graph convolution module (Fig. 1(b)). Both of them are a kind of graph neural network methods that have been widely used in brain network analysis [5]. Then, we replace the proposed fusion positional transformer with LSTM in the spatio-temporal fusion module (Fig. 1(c)). Finally, we compare the models with different modules: only FCs, only ECs, and fusion FCs and ECs. It is worth noting that, in order to fuse FC and EC in LSTM, we specifically design the high-dimensional feature vector product of FC and EC in the LSTM cell. The result of ablation studies is presented in Table 1. These highlight modules (e.g. GCN, Transformer) are the modules constituting the proposed model. Comparing GAT and GCN methods, we find that GCN performs better than GAT in almost all models. The finding may support that GCN utilizes the Laplace matrix based on the adjacent matrix to aggregate structural information, while GAT uses attention coefficients based on node features, which are less important in brain network analysis. The Transformer also performs better than the LSTM method, which may be because compared with LSTM, the Transformer can not only extract the information of a period of time before the current time segment but also associate with the subsequent time.To compare FC and FC+EC models, we further draw the ROC curves of comparison models in Fig. 2(a). It is shown that the ROC curves of fusion methods (i.e., green line and red line) almost wrap the curves (i.e., blue line and yellow line) of the models that only depend on the FC network. The result validate the merit of fusing FC and EC in processing and identifying brain networks."
FE-STGNN: Spatio-Temporal Graph Neural Network with Functional and Effective Connectivity Fusion for MCI Diagnosis,3.3,Comparison with Other Methods,"We also compare the proposed model against baseline algorithms including three static methods: (1) Support Vector Machine (SVM) [19]; (2) Two layers of the   [15], and four dynamic methods: (4) Classical LSTM [8]; (5) Dynamic Functional Connectivity Matrix Graph Embedding Method (DFC) [4]; (6) MS-GCN with same normalized adjacency matrix (MS-GCN) [23]; (7) Spatio-Temporal Graph Convolution [9].From Table 2 and Fig. 2(b), one can have the following observations. First, compared with three static models (i.e., SVM, DCNN, and PRGNN), almost all the methods based on dynamic FC networks obtain better performance. For example, in terms of ACC value, FE-STGNN has achieved at least 6% improvement, which indicates that dynamic functional network modeling is more effective in brain network analysis than static methods. Furthermore, our FE-STGNN yields significantly higher ACC and AUC compared to the other seven competing methods. These results validate the effectiveness of FE-STGNN in identifying MCI patients based on the fusion of FC and EC networks."
FE-STGNN: Spatio-Temporal Graph Neural Network with Functional and Effective Connectivity Fusion for MCI Diagnosis,4,Conclusion,"In this paper, we present a novel FE-STGNN framework for modeling spatiotemporal connectivity patterns of brain networks for MCI diagnosis. To our knowledge, it is one of the earliest studies to use the fusion of FC and EC in a deep-learning fashion to model both spatial and temporal patterns of brain networks at the same time. The ablation studies examine the efficacy of each module of the proposed method as well as the significant benefits of combining FC and EC. The proposed model's superiority is also demonstrated when compared to other methods. We plan to extend the proposed FE-STGNN to diagnose other brain functional diseases in the future."
FE-STGNN: Spatio-Temporal Graph Neural Network with Functional and Effective Connectivity Fusion for MCI Diagnosis,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3_7.
Community-Aware Transformer for Autism Prediction in fMRI Connectome,1,Introduction,"Autism spectrum disorder (ASD) is a developmental disorder that affects communication, social interaction, and behaviour [18]. ASD diagnosis is challenging as there are no definitive medical tests such as a blood tests, and clinicians rely on behavioural and developmental assessments to accurately diagnose ASD. As a more objective alternative measurement, neuroimaging tools, such as fMRI has been widely used to derive and validate biomarkers associated with ASD [9]. Conventionally, fMRI data is modelled as a functional connectivity (FC) matrix, e.g., by calculating Pearson correlation coefficients of pairwise brain ROIs to depict neural connections in the brain. FC-based brain connectome analysis is a powerful tool to study connectivity between ROIs and their impact on cognitive processes, facilitating diagnosis and treatment of neurological disorders including ASD [14,28].Deep learning (DL) models have led to significant advances in various fields including fMRI-based brain connectome analysis. For instance, BrainNetCNN [17] proposes a unique convolutional neural network (CNN) architecture with special edge-to-edge, edge-to-node, and node-to-graph convolutional filters. Considering brain network's non-euclidean nature, graph neural networks (GNNs) [30] have emerged as a promising method for analyzing and modelling brain connectome by constructing graph representation from FC matrices [4,16,19,30]. More recently, transformers [29] have been utilized for brain connectome analysis as they can learn long-range interaction between ROIs without a predefined graph structure. Brain Network Transformer (BNT) [15] has transformer encoders that learn node embeddings using a pearson correlation matrix and a readout layer that learns brain clusters. BNT [15] shows that transformer-based methods outperform CNN and GNN models for fMRI-based classification.It is worth noting that our brain is comprised of functional communities [12] (often referred to as functional networks), which are groups of ROIs that perform similar functions as an integrated network [25,26]. These communities are highly reproducible across different studies [3,5] and are essential in understanding the functional organization of the brain [11]. Their importance extends beyond basic neuroscience, as functional communities have been shown to be relevant in understanding cognitive behaviours [12], mental states [8], and neurological and psychiatric disorders [2,24]. For example, numerous studies have found that individuals with ASD exhibit abnormalities in default mode network (DMN) [22], which is characterized by a combination of hypo-and hyperconnectivity with other functional networks. ASD is also associated with lifelong impairments in attention across multiple domains, and significant alterations in the dorsal attention network (DAN) and DMN have been linked to these deficits [7]. Such prior knowledge of functional relationships between functional communities, can be highly beneficial in predicting ASD. However, existing DL models often overlook this information, resulting in sub-optimal brain network analysis.To address this limitation, we propose a novel community-aware transformer for brain network analysis, dubbed Com-BrainTF, which integrates functional community information into the transformer encoder. Com-BrainTF consists of a hierarchical transformer with a local transformer that learns community-specific embeddings, and a global transformer that fuses the whole brain information. The local transformer takes FC matrices as input with its parameters shared across all communities, while, personalized prompt tokens are learnt to differentiate the local transformer embedding functions. The local transformer's output class tokens and node embeddings are passed to the global transformer, and a pooling layer summarizes the final prediction. Our approach enhances the accuracy of fMRI brain connectome analysis and improves understanding of the brain's functional organization. Our key contributions are:1. We propose a novel local-global hierarchical transformer architecture to efficiently learn and integrate community-aware ROI embeddings for brain connectome analysis by utilizing both ROI-level and community-level information. 2. We avoid over-parameterization by sharing the local transformer parameters and design personalized learnable prompt tokens for each community. 3. We prove the efficacy of Com-BrainTF with quantitative and qualitative experiment results. Our visualization demonstrates the ability of our model to capture functional community patterns that are crucial for ASD vs. Healthy Control (HC) classification."
Community-Aware Transformer for Autism Prediction in fMRI Connectome,2.1,Overview,"Problem Definition. In brain connectome analysis, we first parcellate the brain into N ROIs based on a given atlas. FC matrix is constructed by calculating the Pearson correlation coefficients between pairs of brain ROIs based on the strength of their fMRI activations. Formally, given a brain graph with N number of nodes, we have a symmetric FC matrix X ∈ R N ×N . Node feature vector of ROI j is defined as the j th row or column of this matrix. Given K functional communities and the membership of ROIs, we rearrange the rows and columns of the FC matrix, resulting in). This process helps in grouping together regions with similar functional connectivity patterns, facilitating the analysis of inter-community and intra-community connections. Com-BrainTF inputs X k to the community kspecific local transformer and outputs N dimensional tokensfollowed by a pooling layer and multi-layer perceptrons (MLPs) to predict the output.Overview of Our Pipeline. Human brain connectome is a hierarchical structure with ROIs in the same community having greater similarities compared to inter-community similarities. Therefore, we designed a local-global transformer architecture(Fig. 1(2)) that mimics this hierarchy and efficiently leverages community labels to learn community-specific node embeddings. This approach allows the model to effectively capture both local and global information. Our model has three main components: one local transformer, one global transformer and a pooling layer details of which are discussed in the following subsections. "
Community-Aware Transformer for Autism Prediction in fMRI Connectome,,ASD vs HC,2) 
Community-Aware Transformer for Autism Prediction in fMRI Connectome,2.2,Local-Global Transformer Encoder,"The transformer encoder [29] (Fig. 1(3)) is a crucial component of both local and global transformers. The transformer encoder takes as input an FC matrix (no position encodings) and has a multi-head-attention module to capture interdependencies between nodes using attention mechanism. The learnt node feature matrix H i is given byLocal Transformer: For optimal analysis of fMRI brain connectomes, it is important to incorporate both functional community information and node features. Therefore, we introduce the knowledge of community labels to the network by grouping node features based on community labels producing inputs {X 1 , X 2 , . . . , X K }. However, separate local transformers for each input would result in a significant increase in model parameters. Hence, we use the same local transformer for all inputs, but introduce unique learnable personalized 1D prompt tokens {p 1 , p 2 , . . . , p K }, where p i ∈ R 1×N , that learn to distinguish between node feature matrices of each community and therefore avoid overparameterization of the model. Previous transformer-based models like BNT [15] do not have any prompt tokens and only use the FC matrix as input. Using attention mechanism, pairwise connection strength between ROIs is learnt, producing community-specific node embeddings H i and prompt tokens p i ,Global Transformer: On obtaining community-specific node embeddings and prompt tokens from the local transformer, it is essential to combine this information and design a module to learn the brain network at a global level. Therefore, we introduce a global transformer encoder to learn inter-community dependencies. Input to the global transformer is the concatenated, learnt node feature matrices from the local transformer and a prompt token (Fig. 1(2)). Prompt tokens learnt by the local transformer contain valuable information to distinguish different communities and thus are concatenated and passed through an MLP to obtain a prompt token input for the global transformer as follows:The resulting attention-enhanced, node embedding matrix Z L is then passed to a pooling layer for further coarsening of the graph. Extensive ablation studies are presented in Sect. 3.3 to justify the choice of inputs and output."
Community-Aware Transformer for Autism Prediction in fMRI Connectome,2.3,Graph Readout Layer,"The final step involves aggregating global-level node embeddings to obtain a highlevel representation of the brain graph. We use OCRead layer [15] for aggregating the learnt node embeddings. OCRead initializes orthonormal cluster centers E ∈ R K×N and softly assigns nodes to these centers. Graph level embedding Z G is then obtained by Z G = A Z L , where A ∈ R K×N is a learnable assignment matrix computed by OCRead. Z G is then flattened and passed to an MLP for a graph-level prediction. The whole process is supervised with Cross-Entropy (CE) loss."
Community-Aware Transformer for Autism Prediction in fMRI Connectome,3.1,Datasets and Experimental Settings,ABIDE is an open-source collection of resting-state functional MRI (rs-fMRI) data from 17 international sites [1] with Configurable Pipeline for the Analysis of 
Community-Aware Transformer for Autism Prediction in fMRI Connectome,3.2,Quantitative and Qualitative Results,"Comparison with Baselines (Quantitative Results). Following the comparison method mentioned in BNT [15], we compare the performance of our model with three types of baselines (i) Comparison with transformer-based models -BNT (ii) Comparison with neural network model on fixed brain network -BrainNetCNN [17] (iii) Comparison with neural network models on learnable brain network -FBNETGEN [16]. Note that the selected baselines are reported to be the best among other alternatives in the same category on ABIDE dataset by BNT [15]. As seen in Table 1, our model outperforms all the other architectures on three evaluation metrics. In contrast to other models, Com-BrainTF is hierarchical, similar to the brain's functional organization and therefore is capable of learning relationships within and between communities. "
Community-Aware Transformer for Autism Prediction in fMRI Connectome,,Interpretibility of Com-BrainTF (Qualitative Results,"). We perform extensive experiments to analyze the interpretability of Com-BrainTF. The following results are discussed in detail:1. Interpretation of the learned attention matrix: Fig. 2(1) and Fig. 2(2) show the averaged attention matrices of Com-BrainTF and BNT [15] respectively. These attention matrices are obtained by averaging attention scores over correctly classified test data. In comparison to BNT, our learned attention scores clearly highlight the communities important for ASD prediction. Specifically, attention scores in the SMN region (green arrow) are high indicating that within SMN, connectivity plays an essential role in ASD prediction. This aligns with prior studies on autism which have shown reduced FC within SMN [13], reflecting altered sensory and motor processing [10,20,21]. Additionally, attention scores in DMN, DAN, and FPN regions were found to be high (magenta arrows) suggesting that the functional connectivity between DMN and DAN and between DMN and FPN are also crucial for ASD prediction which is in line with previous studies that report abnormalities in DMN [22] and its functional connectivity with other functional networks including DAN and FPN. These connections can be better visualized from the chord plot in Fig. 2(3). To summarize, Com-BrainTF is able to correctly identify functional communities associated with ASD and the results are consistent with neuroscience findings."
Community-Aware Transformer for Autism Prediction in fMRI Connectome,2.,Ability of the prompt's attention vector to differentiate between ASD and HC subjects:,"We investigate the first row of the learned attention matrix, namely the attention of ROIs corresponding to the prompt. The ROI- wise normalized attention scores shown in Fig. 2(4) are generated using averaged prompt vectors over correctly classified test data, where red to blue indicate attention scores from high to low (dark red:1 to dark blue: 0). As evident from the figure, clear differences can be seen in the important brain regions among ASD and HC subjects. Additionally, most differences in the prompt vector embeddings are seen in DMN and SMN (supporting figures in the appendix), consistent with previous neuroscience literature.3. Meta-analysis of the important functional networks that influence ASD prediction: DMN and SMN networks have been found to be crucial for ASD prediction based on Fig. 2(4). Using the difference of the prompt vector values in those regions between ASD and HC subjects, Fig. 2(5) was generated based on Meta-analysis using Neurosynth1 database [27]. Regions in DMN were found to be associated with trait keywords such as empathy, painful, stimulation, orienting and spatial, whereas regions in the SMN showed higher correlations with eye movement, ventral premotor, somatosensory and speech production."
Community-Aware Transformer for Autism Prediction in fMRI Connectome,3.3,Ablation Studies,"We conduct ablation studies to justify the designs of input and output pairing of the global transformer that facilitates effective learning of node embeddings, consequently resulting in superior performance. The input global transformer prompt token is intentionally retained in all of the experiments because of (i) its capability to identify important communities for the prediction task (Sect. 3.2) and (ii) its capability to learn relationships within and between communities (Sect. 3.2 Fig. 2 (3)).Input: Node Features vs. Prompt Tokens of Local Transformers. We evaluated two input possibilities for the global transformer: (i) use only prompt tokens from the local transformer (ii) incorporate both prompt tokens and updated node features. Table 2 reveals that prompt tokens alone are unable to capture the complete intra-functional connectivity and negatively affect performance.Output: Cross Entropy Loss on the Learned Node Features vs. Prompt Token. In comparison to vision models like ViT [6], which only use the class token (similar to the prompt token in our structure) for classification, we use output node embeddings from the global transformer for further processing steps. We justify this design by conducting a performance comparison experiment: (i) use learnt node features as the global transformer output (ii) use only the prompt token as output. The results (Table 2) demonstrated a significant decrease in performance when prompt token alone was used. This is expected since brain networks are non-euclidean structures and the learnt node features capture more information about the underlying graph, making them essential for accurately capturing the inter-community dependencies."
Community-Aware Transformer for Autism Prediction in fMRI Connectome,4,Conclusion,"In this work, we introduce Com-BrainTF, a hierarchical, community-aware localglobal transformer architecture for brain network analysis. Our model learns intra-and inter-community aware node embeddings for ASD prediction tasks. With built-in interpretability, Com-BrainTF not only outperforms SOTA on the ABIDE dataset but also detects salient functional networks associated with ASD. We believe that this is the first work leveraging functional community information for brain network analysis using transformer architecture. Our framework is generalizable for the analysis of other neuroimaging modalities, ultimately benefiting neuroimaging research. Our future work includes investigating alternate variants for choosing different atlases and community network parcellations."
Community-Aware Transformer for Autism Prediction in fMRI Connectome,,Table 1 .,"Experimental Settings: All models are implemented in PyTorch and trained on V100D-8C (C-Series Virtual GPU for Tesla V100) with 8GB memory. For both local and global transformers, the number of attention heads is equal to the number of communities. We used Adam optimizer with an initial learning rate of 10 -4 and a weight decay of 10 -4 . The train/validation/test data spilt ratio is 70:10:20, and the batch size is 64. Prediction of ASD vs. HC is the binary classification task. We use AUROC, accuracy, sensitivity, and specificity on the test set to evaluate performance. Models are trained for 50 epochs with an average runtime of around 1 minute and we use an early stopping strategy. The mean and standard deviation values were obtained from five random runs with five different seeds. The epoch with the highest AUROC performance on the validation set is used for performance comparison on the test set."
Community-Aware Transformer for Autism Prediction in fMRI Connectome,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3 28.
3D Mitochondria Instance Segmentation with Spatio-Temporal Transformers,1,Introduction,"Mitochondria are membrane-bound organelles that generate the primary energy required to power the cell activities, thereby crucial for metabolism. Mitochondrial dysfunction, which occurs when mitochondria are not functioning properly has been witnessed as a major factor in numerous diseases, including noncommunicable chronic diseases (e.g, cardiovascular and cancer), metabolic (e.g, obesity) and neurodegenerative (e.g, Alzheimer and Parkinson) disorders [23,25]. Electron microscopy (EM) images are typically utilized to reveal the corresponding 3D geometry and size of mitochondria at a nanometer scale, thereby facilitating basic biological research at finer scales. Therefore, automatic instance segmentation of mitochondria is desired, since manually segmenting from a large amount of data is particularly laborious and demanding. However, automatic 3D mitochondria instance segmentation is a challenging task, since complete shape of mitochondria can be sophisticated and multiple instances can also experience entanglement with each other resulting in unclear boundaries. Here, we look into the problem of accurate 3D mitochondria instance segmentation.Earlier works on mitochondria segmentation employ standard image processing and machine learning methods [20,21,33]. Recent approaches address [4,15,26] this problem by leveraging either 2D or 3D deep convolutional neural network (CNNs) architectures. These existing CNN-based approaches can be roughly categorized [36] into bottom-up [3,4,14,15,28] and top-down [12]. In case of bottom-up mitochondria instance segmentation approaches, a binary segmentation mask, an affinity map or a binary mask with boundary instances is computed typically using a 3D U-Net [5], followed by a post-processing step to distinguish the different instances. On the other hand, top-down methods typically rely on techniques such as Mask R-CNN [7] for segmentation. However, Mask R-CNN based approaches struggle due to undefined bounding-box scale in EM data volume.When designing a attention-based framework for 3D mitochondria instance segmentation, a straightforward way is to compute joint spatio-temporal selfattention where all pairwise interactions are modelled between all spatiotemporal tokens. However, such a joint spatio-temporal attention computation is computation and memory intensive as the number of tokens increases linearly with the number of input slices in the volume. In this work, we look into an alternative way to compute spatio-temporal attention that captures long-range global contextual relationships without significantly increasing the computational complexity. Our contributions are as follows:-We propose a hybrid CNN-transformers based encoder-decoder framework, named STT-UNET. The focus of our design is the introduction of a split spatio-temporal attention (SST) module that captures long-range dependencies within the cubic volume of human and rat mitochondria samples. The SST module independently computes spatial and temporal self-attentions in parallel, which are then later fused through a deformable convolution. -To accurately delineate the region of mitochondria instances from the cluttered background, we further introduce a semantic foreground-background (FG-BG) adversarial loss during the training that aids in learning improved instance-level features. -We conduct experiments on three commonly used benchmarks: Lucchi [20],MitoEM-R [36] and MitoEM-H [36]. Our STT-UNET achieves state-of-the-art Fig. 1. Qualitative 3D instance segmentation comparison between the recent Res-UNET [16] and our proposed STT-UNET approach on the example input regions from MitoEM-H and MitoEM-R validation sets. Here, we present the corresponding segmentation predictions of the baseline and our approach along with the ground truth.Our STT-UNET approach achieves superior segmentation performance by accurately segmenting 16% more cell instances in these examples, compared to Res-UNET-R.segmentation performance on all three datasets. On Lucchi test set, our STT-UNET outperforms the recent [4] with an absolute gain of 3.0% in terms of Jaccard-index coefficient. On MitoEM-H val. set, STT-UNET achieves AP-75 score of 0.842 and outperforms the recent 3D Res-UNET [16] by 3.0%.Figure 1 shows a qualitative comparison between our STT-UNET and 3D Res-UNET [16] on examples from MitoEM-R and MitoEM-H datasets."
3D Mitochondria Instance Segmentation with Spatio-Temporal Transformers,2,Related Work,"Most recent approaches for 3D mitochondria instance segmentation utilize convolution based designs within the ""U-shaped"" 3D encoder-decoder architecture.In such an architecture, the encoder aims to generate a low-dimensional representation of the 3D data by gradually performing the downsampling of the extracted features. On the other hand, the decoder performs upsampling of these extracted feature representations to the input resolution for segmentation prediction. Although such a CNN-based designs [11,16,34] has achieved promising segmentation results compared to traditional methods, they struggle to effectively capture long-range dependencies due to their limited local receptive field. Inspired from success in natural language processing [32], recently vision transformers (ViTs) [6,13,19,30,31] have been successfully utilized in different computer vision problems due to their capabilities at modelling long-range dependencies and enabling the model to attend to all the elements in the input sequence. The core component in ViTs is the self-attention mechanism that that learns the relationships between sequence elements by performing relevance estimation of one item to other items. The other attention such as [1,8,10,29,35] have demonstrated remarkable efficacy in effectively managing volumetric data.Inspired by ViTs [10,19] and based on the observation that attention-based vision transformers architectures are an intuitive design choice for modelling long-range global contextual relationships in volume data, we investigate designing a CNNtransformers based framework for the task of 3D mitochondria instance segmentation.3 Method"
3D Mitochondria Instance Segmentation with Spatio-Temporal Transformers,3.1,Baseline Framework,"We base our approach on the recent Res-UNET [16], which utilizes encoderdecoder structure of 3D UNET [34] with skip-connections between encoder and decoder. Here, 3D input patch of mitochondria volume (32 × 320 × 320) is taken from the entire volume of (400 × 4096 × 4096). The input volume is denoised using an interpolation network adapted for medical images [9]. The denoised volume is then processed utilizing an encoder-decoder structure containing residual anisotropic convolution blocks (ACB). The ACB contains three layers of 3D convolutions with kernels (1having skip connections between first and third layers. The decoder outputs semantic mask and instance boundary, which are then post-processed using connected component labelling to generate final instance masks. We refer to [16] for more details.Limitations: As discussed above, the recent Res-UNET approach utilizes 3D convolutions to handle the volumetric input data. However, 3D convolutions are designed to encode short-range spatio-temporal feature information and struggle to model global contextual dependencies that extend beyond the designated receptive field. In contrast, the self-attention mechanism within the vision transformers possesses the capabilities to effectively encode both local and global long-range dependencies by directly performing a comparison of feature activations at all the space-time locations. In this way, self-attention mechanism goes much beyond the receptive field of the conventional convolutional filters. While self-attention has been shown to be beneficial when combined with convolutional layers for different medical imaging tasks, to the best of our knowledge, no previous attempt to design spatio-temporal self-attention as an exclusive building block for the problem of 3D mitochondria instance segmentation exists in literature. Next, we present our approach that effectively utilizes an efficient spatiotemporal attention mechanism for 3D mitochondria instance segmentation.  with split spatio-temporal attention and an instance segmentation block. The denoising module alleviates the segmentation faults caused by anomalies in the EM images, as in the baseline. The denoising is performed by convolving the current frame with two adjacent frames using predicted kernels, thereby generating the resultant frame by adding the convolution outputs. The resulting denoised output is then processed by our transformer based encoder-decoder with split spatio-temporal attention to generate the semantic masks. Consequently, these semantic masks are post-processed by an instance segmentation module using a connected component labelling scheme, thereby generating the final instancelevel segmentation output prediction. To further enhance the semantic segmentation quality with cluttered background we introduced semantic adversarial loss which leads to improved semantic segmentation in noisy background.Split Spatio-Temporal Attention based Encoder-Decoder: Our STT-UNET framework comprises four encoder and three decoder layers. Within each layer, we introduce a split spatio-temporal attention-based (SST) module, Fig. 2(b), that strives to capture long-range dependencies within the cubic volume of human and rat samples. Instead of the memory expensive joint spatiotemporal representation, our SST module splits the attention computation into a spatial and a temporal parallel stream. The spatial attention refines the instance level features from input features along the spatial dimensions, whereas the temporal attention effectively learns the inter-dependencies between the input volume. where, X s is spatial attention map, X t is temporal attention map and d k is dimension of Q s and K s . To fuse spatial and temporal attention maps, X s and X t , we employ deformable convolution. The deformable convolution generates offsets according to temporal attention map X t and by using these offsets the spatial attention map X s is aligned. The deformable fusion is given as,where, C is no of channels, X is spatially aligned attention map with respect to X t . W is the weight matrix of kernels, X s is spatial attention map, k 0 is starting position of kernel, k n is enumerating along all the positions in kernel size of R and ΔK n is the offset sampled from temporal attention map X t . We empirically observe that fusing spatial and temporal features through a deformable convolution, instead of concatenation through a conv. layer or addition, leads to better performance. The resulting spatio-temporal features of decoder are then input to instance segmentation block to generate final instance masks, as in baseline.Semantic FG-BG Adversarial Loss: As discussed earlier, a common challenge in mitochondria instance segmentation is to accurately delineate the region of mitochondria instances from the cluttered background. To address this, we introduce a semantic foreground-background (FG-BG) adversarial loss during the training to enhance the FG-BG separability. Here, we introduce the auxiliary discriminator network D with two layers of 3D convolutions with stride 2 during the training as shown in Fig. 2(c). The discriminator takes the input volume I along with the corresponding mask as an input. Here, the mask M is obtained either from the ground truth or predictions, such that all mitochondria instances within a frame are marked as foreground. While the discriminator D attempts to distinguish between ground truth and predicted masks (M gt and M pred , respectively), the model Ψ learns to output semantic mask such that the predicted masks M pred are close to ground truth M gt . Let F gt = CONCAT(I, M gt ) and F pr = CONCAT(I, M pred ) denote the real and fake input, respectively, to the discriminator D. Similar to [11], the adversarial loss is then given by,Consequently, the overall loss for training is:Where, L BCE is BCE loss, λ = 0.5 and L fg-bg is semantic adversarial loss. "
3D Mitochondria Instance Segmentation with Spatio-Temporal Transformers,4,Experiments,"Dataset: We evaluate our approach on three datasets: MitoEM-R [36], MitoEM-H [36] and Lucchi [22]. The MitoEM [36] is a dense mitochondria instance segmentation dataset from ISBI 2021 challenge. The dataset consists of 2 EM image volumes (30 μm 3 ) of resolution of 8 × 8 × 30 nm, from rat tissues (MitoEM-R) and human tissue (MitoEM-H) samples, respectively. Each volume has 1000 grayscale images of resolution (4096 × 4096) of mitochondria, out of which train set has 400, validation set contains 100 and test set has 500 images. Lucchi [22] is a sparse mitochondria semantic segmentation dataset with training and test volume size of 165 × 1024 × 768.Implementation Details: We implement our approach using Pytorch1.9 [27] (rcom env) and models are trained using 2 AMD MI250X GPUs. During training of MitoEM, for the fair comparison, we adopt same data augmentation technique from [36]. The 3D patch of size (32×320×320) is input to the model and trained using batch size of 2. The model is optimized by Adam optimizer with learning rate of 1e -4 . Unlike baseline [16], we do not follow multi-scale training and perform single stage training for 200k iterations. For Lucchi, we follow training details of [16,36] for semantic segmentation. For fair comparison with previous works, we use the same evaluation metrics as in the literature for both datasets. We use 3D AP-75 metric [36] for MitoEM-R and MitoEM-H datasets. For Lucchi, we use jaccard-index coefficient (Jaccard) and dice similarity coefficient (DSC)."
3D Mitochondria Instance Segmentation with Spatio-Temporal Transformers,4.1,Results,"State-of-the-Art Comparison: Table 1 shows the comparison on MitoEm-R and MitoEM-H validation sets. Our STT-UNET achieves state-of-the-art performance on both sets. Compared to the recent [16], our STT-UNET achieves an absolute gains of 4.1% and 2.9% on MitoEM-R and MitoEM-H validation sets, respectively. Note that [16] employs two decoders for MitoEM-H. In contrast, we utilize only a single decoder for both MitoEM-H and MitoEM-R sets, while still achieving improved segmentation performance. Fig 3 presents the segmentation predictions of our approach on example input regions from the validation set.Our approach achieves promising segmentation results despite the noise in the input samples.    Ablation Study: Table 3 shows a baseline comparison when progressively integrating our contributions: SST module and semantic foreground-background adversarial loss. The introduction of SST module improves performance from 0.921 to 0.941 with a gain of 2.7%. The performance is further improved by 1%, when introducing our semantic foreground-background adversarial loss. Our final approach achieves absolute gains of 3.7% and 2.6% over the baseline on MitoEM-R and MitoEM-H, respectively. We also compare our approach with other attention mechanism in literature such as divided space-time attention [1] and axial attention [35] with our method achieving favorable results with gain of 0.9% and 1.1%, respectively likely due to computing spatial and temporal in parallel and later fusing them through a deformable convolution. Further, we compare our approach with [16] on MitoEM-v2 test set achieving a gain of 4% on MitoEM-R, where the postprocessing from [18] is used to differentiate the mitochondria instances for both methods. Table 4 shows ablation study with feature fusion strategies in our SST module: addition, concat and deformable-conv. The best results are obtained with deformable-conv on both datasets. For encoding spatial and temporal information, we analyze two design choices with SST module: cascaded and split, as shown in Table 5. The best results are obtained using our split design choice (row 3) with spatial and temporal information encoded in parallel and later combined. We also evaluate with different input volumes: 4,8,16,32. We observe best results are obtained when using 32 input volume."
3D Mitochondria Instance Segmentation with Spatio-Temporal Transformers,5,Conclusion,"We propose a hybrid CNN-transformers based encoder-decoder approach for 3D mitochorndia instance segmentation. We introduce a split spatio-temporal attention (SST) module to capture long-range dependencies within the cubic volume of human and rat mitochondria samples. The SST module computes spatial and temporal attention in parallel, which are later fused. Further, we introduce a semantic adversarial loss for better delineation of mitochondria instances from background. Experiments on three datasets demonstrate the effectiveness of our approach, leading to state-of-the-art segmentation performance."
3D Mitochondria Instance Segmentation with Spatio-Temporal Transformers,,Table 2,presents the comparison on Lucchi test set. Our method sets a new state-of-the-art on this dataset in terms of both Jaccard and DSC.
Overall Survival Time Prediction of Glioblastoma on Preoperative MRI Using Lesion Network Mapping,1,Introduction,"Glioblastomas (GBMs, known as grade IV gliomas) are the most common primary malignant brain tumors with high spatial heterogeneity and varying degrees of aggressiveness [22]. Patients with GBM generally have a very poor survival rate; the median overall survival time is about 14 months [17]; and the overall survival time is affected by many factors, including patient characteristics (e.g., age and physical status), tissue histopathology (e.g., cellular density and nuclear atypia), and molecular pathology (e.g., mutations and gene expression levels) [1,14,15]. Although these factors, particularly molecular information, have usually proved to be strong predictors of survival in GBM, there remain substantial challenges and unmet clinical needs to exploit easily accessible, noninvasive neuroimaging data acquired preoperatively to predict overall survival time of GBM patients, which can benefit treatment planning.To do so, magnetic resonance imaging (MRI) and its derived radiomics have been widely used to study GBM preoperative prognosis over the last few decades. For example, Anand et al. [2] first applied a forest of trees to assign an importance value to each of the 1022 radiomic features extracted from T1 MRI, and then the 32 most important features were fed to the random forest regressor for predicting overall survival time of a GBM patient. Based on patches from multi-modal MRI images, Nie et al. [19] trained a 3D convolutional neural network (CNN) to learn the high-level semantic features, which were eventually input to a support vector machine (SVM) for classifying long-and short-term GBM survivors. In addition, an integrated model by fusing radiomics features, MRI-based CNN features, and clinical features, was presented for GBM survival group classification, resulting in better performance than using any single type of features [12].Although both MRI and its derived radiomics features have been demonstrated to have predictive power for survival analysis in the aforementioned literature, they do not account for brain's functional alternations caused by tumors, which are clinically significant as biologically-interpretable biomarkers of recovery and therapy. These alternations can be reflected by changes in resting-state functional MRI (fMRI)-derived functional connectivities/connections (FCs) between the blood oxygenation level-dependence (BOLD) time series of paired brain regions. Therefore, the use of FCs to predict overall survival time for GBM has recently attracted increasing attention [7,16,24], and more importantly, survival-related FC patterns or brain regions were found to guide therapeutic solutions aimed at inhibiting tumor-brain communication.Nevertheless, current FC-based survival prediction still suffers from two main deficiencies when applied to GBM prognosis. First, due to mass effect and physical infiltration of GBM in the brain, FCs estimated directly from GBM patients' resting-state fMRI might be inaccurate, especially when the tumors are near or in the regions of interest. Second, resting-state fMRI data are not routinely collected for GBM clinical practices, which restricts the size of annotated datasets such that it is infeasible to train a reliable prediction model based on deep learning for survival prediction. In order to circumvent these issues, in this paper we introduce a novel neuroimaging feature family, namely functional lesion network (FLN) maps that are generated by our augmented lesion network mapping (A-LNM), for overall survival time prediction of GBM patients. Our A-LNM is motivated by lesion network mapping (LNM) [8] which can localize neurological deficits to functional brain networks and identify regions relate to a clinical syndrome. By embedding the lesion into a normative functional connectome and computing functional connectivity between the lesion and the rest of the brain using fMRI of all healthy subjects in the normative cohort, LNM has been successfully employed to the identification of the brain network underlying particular symptoms or behavioral deficits in stoke [4,13].The details of our workflow are described as follows.1) We first manually segment the whole tumor (regarded as lesion in this paper) on structural MRI for all GBM patients, and the resulting lesion masks are mapped onto a reference brain template, e.g., the MNI152 2mm 3 template.2) The proposed A-LNM is next used to generate FLN maps for each GBM patient by using resting-state fMRI from a large cohort of healthy subjects. Specifically, for each patient, we correlate the mean BOLD time series of all voxels within the lesion with the BOLD time series of every voxel in the whole brain for all N subjects in the normative cohort, producing N functional disconnection (FDC) maps of voxel-wise correlation values (transformed to zscores). These resulting N FDC maps are partitioned into M disjoint subsets of equal size, and M FLN maps are separately obtained by averaging the FDC maps in each of the M subsets. Similar to data augmentation schemes, we can artificially boost data volume (i.e., FLN maps) up to M times through producing M FLN maps for each patient in the A-LNM, which helps to mitigate the risk of over-fitting and improve the performance of overall survival time prediction when learning a deep neural network from a small sized dataset.For this reason, we propose the name ""augmented LNM (A-LNM)"", compared to the traditional LNM where only one FLN map is generated per patient by averaging all the N FDC maps. 3) Finally, these augmented FLN maps are fed to a 3D ResNet-based backbone network followed by the average pooling operation and fully-connected layers for GBM survival prediction.To our knowledge, this paper is the first to demonstrate a successful extension of LNM for survival prediction in GBM. To evaluate the predictive power of the FLN maps generated by our A-LNM, we conduct extensive experiments on 235 GBM patients in the training dataset of BraTS 2020 [18] to classify the patients into three overall survival time groups viz. long, mid, and short. Experimental results show that our A-LNM based survival prediction framework outperforms previous state-of-the-art methods. In addition, an explainable analysis driven by the Gradient-weighted Class Activation Mapping (Grad-CAM) [10] for survivalrelated brain regions is fulfilled. "
Overall Survival Time Prediction of Glioblastoma on Preoperative MRI Using Lesion Network Mapping,2,Materials and Methods,"2.1 Materials GSP1000 Processed Connectome. It publicly released preprocessed restingstate fMRI data of 1000 healthy right-handed subjects with an average age 21.5 ± 2.9 years and approximately equal numbers of males and females from the Brain Genomics Superstruct Project (GSP) [5], where the concrete image acquisition parameters and preprocessing procedures can be found as well. Specifically, a slightly modified version of Yeo's Computational Brain Imaging Group (CBIG) fMRI preprocessing pipeline (https://github.com/bchcohenlab/CBIG) was employed to obtain either one or two preprocessed resting-state fMRI runs of each subject that had 120 time points per run and were spatially normalized into the MNI152 template with 2mm 3 voxel size. We downloaded and used the first-run preprocessed resting-state fMRI of each subject for the following analysis.BraTS 2020. It provided an open-access pre-operative imaging training dataset to segment brain tumors of glioblastoma (GBM, belonging to high grade glioma) and low grade glioma (LGG) patients, as well as to predict overall survival time of GBM patients [18]. This training dataset contained 133 LGG and 236 GBM patients, and each patient had four MRI modalities, including T1, post-contrast T1-weighted, T2-weighted, and T2 Fluid Attenuated Inversion Recovery. Manual expert segmentation delineated three tumor sub-regions, i.e., the GD-enhancing tumor, the peritumoral edema, and the necrotic and non-enhancing tumor core.The union of all the three tumor sub-regions was considered as the whole tumor, which is regarded as the lesion in this paper."
Overall Survival Time Prediction of Glioblastoma on Preoperative MRI Using Lesion Network Mapping,2.2,Methods,"In this paper, we propose to investigate the feasibility of the novel neuroimaging features, i.e., FLN maps, for overall survival time prediction of GBM patients in the training dataset of the BraTS 2020, in which one patient alive was excluded, and the remaining 235 patients consisted of 89 short-term survivors (less than 10 months), 59 mid-term survivors (between 10 and 15 months), and 87 long-term survivors (more than 15 months). To this end, our framework for the three-class survival classification is shown in Fig. 1, and the details are described as follows.Lesion Mapping Procedures. As stated above, the whole tumor is referred to as a lesion for each GBM patient. From the manual expert segmentation labels of lesions in the 235 GBM patients of the BraTS 2020, we co-register the lesion masks to the MNI152 2mm 3 template by employing a Symmetric Normalization algorithm in ANTsPy [3]."
Overall Survival Time Prediction of Glioblastoma on Preoperative MRI Using Lesion Network Mapping,,Augmented Lesion Network Mapping (A-LNM).,"After lesion mapping, we introduce a modified LNM (called augmented LNM (A-LNM) in this paper) to generate FLN maps for each GBM patient by using resting-state fMRI of all 1000 GSP healthy subjects, as described below. i) For each patient, the lesion is viewed as a seed region to calculate FDC in the healthy subjects with restingstate fMRI. Specifically, to compute FDC, the mean BOLD time series of voxels within each lesion is correlated with the BOLD time series of every voxel in the whole brain for all the 1000 healthy subjects, yielding 1000 FDC maps of voxelwise correlation values (transformed to z-scores), where an FDC map is actually a three-dimensional voxel-wise matrix of size 91 × 109 × 91 (spatial resolution: 2mm 3 voxel size). ii) Different from the commonly used LNM where the resulting 1000 FDC maps are thresholded or averaged to obtain a single FLN map for each patient, the A-LNM generates many FLN maps for each patient in a manner that partitions all the 1000 FDC maps into disjoint subsets of equal size and averages each subset to produce one FLN map. One can clearly see that similar to data augmentation schemes, we artificially boost the number of training samples (i.e., FLN maps) by our A-LNM, which helps to mitigate the risk of over-fitting and improve the performance of overall survival time prediction when learning a deep neural network from such a small sized training set used in this paper. Note that in Sect. 3 of this paper, according to experimental results, we divided the 1000 FDC maps into 100 subsets, and randomly chose 10 out of the resulting 100 FLN maps for each patient as input to the downstream prediction model.Deep Neural Network for Overall Survival Time Prediction. By taking the obtained FLN maps as input, we apply a 3D ResNet-based backbone network transferred from the encoder of MedicalNet [6] to extract CNN features from each FLN map. The features are then combined using the average pooling operation and fed to a fully-connected layer with kernel size (1, 1, 1) to classify each GBM patient into one of the three overall survival time groups (i.e., short-term survival, mid-term survival, and long-term survival)."
Overall Survival Time Prediction of Glioblastoma on Preoperative MRI Using Lesion Network Mapping,3.1,Experimental Settings,"Implementation Details. Our proposed method was implemented in PyTorch 1.13.1 on NVIDIA A100 Tensor Core GPUs. The loss function was the standard cross-entropy loss. The Adam optimizer with the weight decay of 10 -5 was adopted. Three 3D ResNet-based backbones with different numbers of layers (10, 50, and 101) were performed, where the initial learning rates were set as 10 -4 , 10 -4 , and 10 -5 , respectively, and would decrease by a factor of 5 if the classification performance is not improved within 5 epochs. The number of epochs for training was 50, and the batch size was fixed as 64.Performance Evaluation. We evaluated the classification performance of our proposed method using 235 GBM patients in the BraTS 2020 training dataset, because only these 235 patients had both overall survival time and manual expert segmentation labels of lesions. In all experiments, we conducted five-fold crossvalidation ten times in order to reduce the effect of sampling bias. Moreover, the A-LNM was performed ten times randomly to avoid particular data distribution and obtain more reliable results. The classification results were reported in terms of accuracy, macro precision (macro-P), macro recall (macro-R), and macro F1 score (macro-F1), respectively."
Overall Survival Time Prediction of Glioblastoma on Preoperative MRI Using Lesion Network Mapping,3.2,Comparison Studies,"Quantitative Comparison of Different Prediction Models. As this paper is the first application of FLN maps in the overall survival time prediction for GBM, comparison among the classification performance of different models using the same type of features, i.e., the A-LNM or the LNM derived FLN maps, is demanded for model selection. To validate the effectiveness of the 3D ResNetbased backbones for GBM survival prediction, we made quantitative comparison of a ridge classifier (RC) with PCA [23], a support vector classifier (SVC) with PCA, a logistic regression (LR) with PCA, and three 3D ResNet-based backbones with different numbers of layers (10, 50, and 101). As presented in Table 1, all the 3D ResNet-based backbones outperformed the other three machine learning-based models. In addition, all the 3D ResNet-based backbones achieved better classification results by using the A-LNM derived FLN maps than the LNM derived FLN maps, which implies that the A-LNM derived FLN maps have stronger predictive power than the LNM derived FLN maps. Quantitative Comparison of Different Types of Features. Subsequently, we compared the predictive power of FLN maps and the other four widely used types of features, i.e., clinical features with an LR [27], biophysics features with an SVC [9], radiomics features with a gradient boosting classifier (GBC) [21], and MRI-based features with a 3D CNN model [9]. These competing models were executed following the instructions in their respective papers to achieve the best performance. Quantitative results of different types of features are displayed in Table 2. One can see that FLN maps showed the strongest predictive power on all the four metrics. Specifically, the 3D ResNet-10 backbone with the A-LNM derived FLN maps improved the classification performance by 10.5% to 18.5% in terms of accuracy, which again demonstrates the superiority of the A-LNM derived FLN maps for GBM survival prediction. "
Overall Survival Time Prediction of Glioblastoma on Preoperative MRI Using Lesion Network Mapping,3.3,Brain Regions in Relation to GBM Survival,"To identify the most discriminative brain regions associated with overall survival time in GBM, we estimated the relative contribution of each voxel to the classification performance in our proposed method by using the Grad-CAM [10]. To obtain steady results, as shown in Fig. 2(A), the voxels with top 5% weights in the class activation maps (CAMs) of all candidate models were overlapped by class, and the position covered by more than half of the models is displayed. The CAMs of three classes of survivors overlapped in Fig. 2(B) where both coincident and non-coincident areas exist.The association of an increased degree of invasion within the frontal lobe with decreased survival time can be observed, which is in concordance with a previous study [20]. Patients whose frontal lobe is affected by tumors showed more executive dysfunction, apathy, and disinhibition [11]. On the dominant left hemisphere, the CAMs of long-term survivors and mid-term survivors overlapped at the superior temporal gyrus and Wernicke's area which are involved in the sensation of sound and language comprehension respectively, and have been associated with decreased survival in patients with high-grade glioma [26]. In addition, the CAM of mid-term survivors covered more areas of the middle and inferior temporal gyri which were considered as one of the higher level ventral streams of visual processing linked to facial recognition [25]. "
Overall Survival Time Prediction of Glioblastoma on Preoperative MRI Using Lesion Network Mapping,4,Conclusion,"In this paper, we introduce a novel neuroimaging feature family, called A-LNM derived FLN maps, for overall survival time prediction of GBM patients. A-LNM was presented to generate plenty of FLN maps for each GBM patient by partitioning the FDC maps obtained from resting-state fMRI of 1000 GSP healthy subjects into disjoint subsets of equal size and averaging each subset. We applied a 3D ResNet-based backbone network to extract features from the generated FLN maps and classify GBM patients into three overall survival time groups. Experimental results on the BraTS 2020 training dataset validated the effectiveness of the A-LNM derived FLN maps for GBM survival prediction. Moreover, a visualization analysis implemented by the Grad-CAM revealed the brain regions associated with GBM survival. In future work, we will try to fuse the FLN maps and MRI-based radiomics features to study their combined predictive power for GBM survival analysis."
Learnable Subdivision Graph Neural Network for Functional Brain Network Analysis and Interpretable Cognitive Disorder Diagnosis,1,Introduction,"Studies on brain functional dynamics show that the brain network contains a variety of distinct functional configurations (brain states) during the course of brain cognition activities [5,11]. These distinct brain states depict the heterogeneous functional signature of the brain network (e.g., Visual, Attention, etc.) [1]. Generally, brain states can be characterized by discriminate connections constructed from brain networks, and various strategies have been developed to identify brain states with the objective of understanding how heterogeneous information is represented in the brain [9,24]. For example, the whole-brain functional connectivity patterns derived from independent component analysis (ICA) are employed for brain network analysis [13,18]. However, they typically struggle to perform well on high-dimensional data and need further design techniques for feature selection.Recently, graph neural networks (GNNs) have been proven to be helpful in brain network analysis, due to their powerful ability in analyzing graphstructured data [21]. However, the existing GNN methods for heterogeneity mostly deal with the semantic heterogeneity from different modalities [22,23] or the connectivity heterogeneity among nodes [10], and rarely consider the functional heterogeneity of the whole brain network. Therefore, it may lead to a suboptimal performance on disorder diagnosis. On the other hand, GNN as a deep learning model is typically poorly interpretable in brain network analysis [14]. Although several methods for GNN interpretation have been proposed, most of them concentrate on node-level or subject-level analysis. For instance, BrainGNN provides insights on the salient brain region of interest (ROIs) through specific node pooling operation [15]; IBGNN discusses the neural system mapping of subjects in different categories with an explanation generator mask [4]. However, few studies have been conducted on the interpretable analysis of brain states at network-level, especially the relationship between heterogeneous functional features of brain networks and corresponding disease diagnosis remains unexplored.To address the above issues, we propose a learnable subdivision graph neural network to investigate brain networks with heterogeneous functional signatures, and the functional brain state can be combined with corresponding latent subspace for interpretable brain disorder diagnosis. The main contributions of this paper are as follows: 1)We propose a novel Learnable Subdivision Graph Neural Network (LSGNN) model for brain network analysis, which implements the extraction of heterogeneous features of brain networks under various functional configurations. 2) We develop a novel assignment method, that can encode brain networks into multiple latent feature subspaces in a learnable way. 3) Our method instills the interpretability of the latent space corresponding to brain states, which is beneficial to unveiling insights into the relationship between the signature of functional brain networks and cognitive disorder diagnosis."
Learnable Subdivision Graph Neural Network for Functional Brain Network Analysis and Interpretable Cognitive Disorder Diagnosis,2,Method,"The framework of the LSGNN method is presented in Fig. 1, which is composed of three major components: the functional subdivision block (FSB), multiple GNN modules including the GNN layer (with shared weights) and graph pooling layer, and the functional aggregation block (FAB). Here, FSB is designed to automatically learn an assignment matrix to obtain the mask of each brain state. The inputs of FSB are acquired using two separate GNN layers with the same structure but different parameters. Furthermore, multiple GNN modules are designed to learn brain network representations under distinct brain states. Finally, FAB is developed to aggregate the information into a joint latent space, and acquire a comprehensive brain network representation for brain disorder diagnosis."
Learnable Subdivision Graph Neural Network for Functional Brain Network Analysis and Interpretable Cognitive Disorder Diagnosis,2.1,Preliminary,"Problem Definition. The input of proposed model is a set of N weighted brain networks. For each network G = {V, E, A}, V = {v i } M i=1 involves M nodes defined by the ROIs on a specific brain parcellation [7], and E records the distinct edge connections in each subject, which is represented with a weighted adjacency matrix A ∈ R M ×M describing the correlation strengths among ROIs. The model finally outputs the prediction result ŷn for each subject n. The GNN Module. The GNN module is superior in extracting structural information from the network, which is attributed by two kinds of layers [3]: 1) Message-passing-based GNN layers that extracts the embedding of ROI nodes through iteratively updating information with structural connections. The propagation rule can be formulated in matrix form as: H (l+1) = GNN A, H (l) ; θ (l) , where H (l+1) ∈ R M ×D are the embeddings computed after l step of the GNN layers, and D is the embedding dimension. GNN is the message propagation function using a combination of linear transformations and ReLU non-linearities, which depends on the adjacency matrix A, trainable parameters θ (l) , and the node embeddings H (l) generated from the previous step. The input node embeddings H 0 are initialized using the node features on the graph. 2) The graph pooling layer. It realizes the aggregation of global information at the graph level using readout to convert all node embeddings into a graph embedding vector.Here, the readout function can be performed using the average pool."
Learnable Subdivision Graph Neural Network for Functional Brain Network Analysis and Interpretable Cognitive Disorder Diagnosis,2.2,Functional Subdivision Block,"Considering the heterogeneity of brain networks induced by intrinsic functional brain states, we propose to encode brain networks into multiple latent feature subspaces corresponding to functional brain states. The key to achieve this goal is to build an assignment matrix that can allocate the feature representation of each dimension in the brain network to the latent subspace related to distinct brain states.Here, we innovatively design a learnable assignment method, which is determined by the embedding matrix and automatically updated with the training of the whole model. Specifically, we first utilize two separate GNN layers to generate embeddings for preliminary node feature matrix H F = GNN feat (A, H; θ feat ) and input embeddings of assignment matrix H S = GNN assig (A, H; θ assig ), respectively. Note that these two GNN layers use the same data as input, but have distinct parameterizations and play separate roles. We continue to transpose the input embeddings of assignment matrix as H T S ∈ R D×M , and the generation of assignment matrix S ∈ R D×C as follows:where the softmax function is applied in a row-wise fashion, and the output dimension of MLP 1 corresponds to a pre-defined number C of brain states. With the assignment matrix and node embedding matrix in hand, we further extract distinct node embeddings in different latent subspaces corresponding to brain states. For each column {S c } C c=1 in the assignment matrix, it is essentially a mask vector where each element represents the probability that the feature is assigned to the brain state c. Therefore, we can obtain the brain network representation in each brain state through the product of feature and its corresponding assignment probability.where R is a repeat function that extends the mask vector to the same dimension of the node embedding matrix, denotes element-wise multiplication, and MLP 2 maps each new node embedding matrix to distinct feature latent subspaces, which characterize heterogeneous information under different brain states."
Learnable Subdivision Graph Neural Network for Functional Brain Network Analysis and Interpretable Cognitive Disorder Diagnosis,2.3,Functional Aggregation Block,"After assigning the node embedding matrix of brain network into distinct latent subspaces, we utilize multiple GNN modules ( each consists of a GNN layer and a graph pooling layer) to obtain the graph embedding of brain network H c ∈ R 1×D corresponding to each brain state respectively. These GNN layers are performed with shared weight to reduce the complexity of the model. Considering that each brain state has different contributions to the final brain network representation, we propose a functional aggregation block based on the self-attention mechanism to aggregate the information into a joint latent space, and acquire a comprehensive brain network representation for brain disorder diagnosis.In practice, the graph embedding in every brain state is first packed together into a matrixwhere the weight matrices for W q , W k , W v are the learned linear mappings. Therefore, we can calculate the self-attention by mapping a query and a set of key-value pairs, and combining with a mean pooling to obtain the whole graph embedding vector H ∈ R 1×D for the brain network.Here, the dot product is adopted to reduce computational and storage costs, softmax is used to normalize the self-attention and the scale √ D prevents the saturation led by softmax function. Finally, we fed the whole graph embedding vector into a fully connected layer to predict the diagnostic result ŷn ."
Learnable Subdivision Graph Neural Network for Functional Brain Network Analysis and Interpretable Cognitive Disorder Diagnosis,2.4,Objective Function,"In this work, we design an objective function composed of three components: First, we employ conventional supervised cross-entropy objective towards ground-truth y n disorder prediction, defined asSecond, since each row of the assignment matrix S i represents the probability of allocating the feature of this dimension to different latent subspaces, it should generally be close to a one-hot vector, i.e., each dimension feature is assigned to each latent subspace. Therefore, we design an entropy loss to reduce the uncertainty of mapping distribution by minimizing entropy H Si :Finally, to ensure the generalization ability of the model and reduce over-fitting, we add an L2 normalization term. To summarize, the total loss of the proposed model can be formulated as:where α, β are hyperparameters that determine the relative importance of feature fusion loss items."
Learnable Subdivision Graph Neural Network for Functional Brain Network Analysis and Interpretable Cognitive Disorder Diagnosis,3.1,Dataset and Experimental Settings,"We evaluate our framework on publicly available Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset [16], which includes 193 normal controls (NC), 240 early mild cognitive impairment (EMCI), and 149 late mild cognitive impairment (LMCI). We use them to form three binary classification tasks abbreviated as N-E, N-L, and E-L. The fMRI data is preprocessed in a standardized protocol including slice time correction, motion correction, spatial and temporal filtering, and covariates regression [2]. We follow the general process of the GNN-based method in brain network analysis and use the AAL atlas [19] to define 90 ROIs for every subject. We continue to construct brain networks using Pearson correlations.We compare our proposed model with five different methods, including one conventional model (1) SVM [6] where functional brain networks are reshaped as feature vectors and are then put into models, two representative GNN models: (2) GCN [12], (3) GAT [20], and two state-of-the-art GNN-based models specifically designed for brain networks: (4) BrainGNN [15] and (5) IBGNN [4].All deep learning experiments are conducted on NVIDIA GeForce GTX TITAN X GPUs with PyTorch [17] framework. We perform a grid search to determine the better choice for α = 10 -3 and β = 10 -2 , and set parameter c = 7 representing different configurations of the brain as suggested in [1,8]. We refer readers of interest to supplementary materials for detailed experimental settings. All reported results are averaged across 10 times ten-fold cross-validations. We finally adopt four commonly used metrics to evaluate all methods, including classification accuracy (ACC), sensitivity (SEN), specificity (SPE), and AUC."
Learnable Subdivision Graph Neural Network for Functional Brain Network Analysis and Interpretable Cognitive Disorder Diagnosis,3.2,Result Analysis,"Table 1 shows the classification results of all methods on all tasks. We can have the following observations. First, compared with the conventional machine learning method (i.e., SVM), deep learning models generally achieve better performance on all tasks in terms of four evaluation metrics. It is indicated that the brain features obtained by automatic learning with neural networks may be better than the traditional handcrafted features in the diagnosis of brain diseases. Second, two GNN models specifically designed for brain networks (i.e., BrainGNN, and IBGNN) achieve better results than classical GNN models (i.e., GCN, and GAT), demonstrating the significance of considering the biomedical characteristics of brain networks when applying GNN-related models to brain network analysis. Finally, the effectiveness of our design for heterogeneous properties of the brain network is demonstrated by its superior performance compared with other SOTA models. Moreover, our method is statistically significantly better than other methods (with p < 0.05) based on pairwise t-test. For instance, in terms of ACC values, the proposed LSGNN obtains the improvement of at least 3% compared with the best alternatives (i.e., IBGNN) on all tasks. It is concluded that LSGNN is feasible to separate the heterogeneous feature of brain networks in latent space, which is beneficial to capture complex information through multiple GNN modules under distinct brain states, thus improving the ability of brain network representation. "
Learnable Subdivision Graph Neural Network for Functional Brain Network Analysis and Interpretable Cognitive Disorder Diagnosis,3.3,Ablation Study,"We conduct ablation studies to verify the effectiveness of 1) the learnable assignment method in the FSB module, 2) the self-attention mechanism-based fusion method in the FAB module, and 3) entropy loss L E . Specifically, In the FSB module, we compare the proposed learnable method with a fixed method, where we utilize k-means to cluster feature embedding into several latent subspaces with the same number as the learnable assignment matrix done. In the FAB module, we try a typically simple feature fusion method (i.e., feature concatenation) without self-attention. For the loss function, we conduct comparative experiments on whether to include entropy loss. The classification results of all six methods on the N-E task are listed in Table 2. It can be observed that the proposed learnable assignment method and self-attention mechanism-based fusion module are effective in cognitive disorder diagnosis. Furthermore, the results of entropy loss ablation prove that the introduction of entropy loss greatly enhances the robustness of the proposed model.  To investigate brain states and revealing the impact of different brain states on diseases, we plot the functional brain states under distinct brain network feature subspaces on the N-E task. Specifically, we first calculate the assignment matrix using Eq. 2 and further compute the sum of each column combined with normalization. Each element is the probability weight of each brain state in the whole feature space, which also reflects the impact of each brain state on the final diagnosis task. The weight values of different brain states are sorted in the second line in Fig. 2. It is evident that the divergence of the impacts from distinct brain states is significant. For example, the weight of state 7 surpasses that of state 1 by over 10 times. The possible reason is that the heterogeneous functions (e.g. Visual, Attention, Memory, etc.) conferred from distinct brain states may have unequal contributions to the cognitive activities.Then, for each brain state c, we extract node embedding matrix H Fc and construct a corresponding functional connectivity matrix using Pearson correlation. Finally, we obtain distinct brain states by computing the average results of testing samples on the N-E task. From Fig. 2, we have an interesting observation that the brain state that has a great impact on diagnosis shows rich club regions, which reveals that there exist multiple subnetworks in the brain network that may correlate to different brain functions."
Learnable Subdivision Graph Neural Network for Functional Brain Network Analysis and Interpretable Cognitive Disorder Diagnosis,4,Conclusion,"In this paper, we propose a novel learnable subdivision graph neural network method for functional brain network analysis and interpretable cognitive disorder diagnosis. Specifically, brain networks are embedded into multiple latent feature subspaces corresponding to functional configurations in a learnable way. Experimental results of the cognitive disorder diagnosis tasks verify the effectiveness of our proposed method. A direct future direction based on this work is to utilize heterogeneous graph construction techniques to describe brain network patterns. This allows for consideration of brain network heterogeneity from the initial step of brain network modeling, which could lead to a better understanding of brain networks."
Learnable Subdivision Graph Neural Network for Functional Brain Network Analysis and Interpretable Cognitive Disorder Diagnosis,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3_6.
Unsupervised Learning for Feature Extraction and Temporal Alignment of 3D+t Point Clouds of Zebrafish Embryos,1,Introduction,"Zebrafish are widely used model organisms in many experiments due to their fully-sequenced genome, easy genetic manipulation, high fecundity, external fertilization, rapid development and nearly transparent embryos [3]. The spatiotemporal resolution of modern light-sheet microscopes allows imaging the embryonic development at the single-cell level [4]. Fluorescently labeled nuclei can be detected, segmented and tracked in these data sets and the extracted 3D+t point clouds can be used for analyzing the development of a single embryo in unprecedented detail [4]. In many experiments, an important task is to compare the level of growth between different individuals, especially the research on mutants and the growth under particular conditions like pollution and exposure to potentially harmful chemicals [5,6]. Thus, automatically obtaining an accurate temporal alignment that synchronizes the developmental stages of two or more individuals is an important component of such comparative analyses.Existing approaches are mostly based on the automatic alignment of manually identified landmarks or operate in the image domain. However, to the best of our knowledge there is no approach available yet to obtain a spatiotemporal alignment of 3D+t point clouds of biological specimens in an automatic and unsupervised fashion. In [5], the authors generate sets of landmarks based on the deformation of embryos, and the landmarks are paired to generate the temporal registration between two sequences of embryos. Michelin et al. [13] formulate the temporal alignment problem as an image-to-sequence registration applicable to more complex organisms like the floral meristem. The method introduced in [14] pairs the 3D images of ascidian embryos by finding the symmetry plane and by computing the transformation that optimizes the cell-to-cell mapping. In [15], landmarks of multiple mouse embryos are manually identified and subsequently used to automatically obtain a spatiotemporal alignment with their custom-made Tardis method. In the past couple of years, deep neural networks emerged as a powerful approach to learn descriptive representations of images and point clouds that can be flexibly used for various tasks. The authors of [2] use an image-based convolutional neural network and a PointNet-based [8] architecture in a supervised fashion to obtain an automatic staging of zebrafish embryos.In this work, we present a deep learning-based method for the temporal alignment of 3D+t point clouds of developing embryos. Firstly, an autoencoder is employed to extract descriptive features from the point clouds of each time frame. As an autoencoder designed explicitly for point clouds, FoldingNet [1] is used as the basic architecture and we propose several modifications to improve its applicability to point cloud data of developing organisms. As the next step, the extracted latent features of the autoencoder are used in a regression network to temporally align different embryos. The final output are pairwisely aligned time frames of two different 3D+t point clouds. We show that the autoencoder learns discriminative and descriptive feature vectors that allow to recover the temporal ordering of different time frames. In addition to quantitatively assessing the alignment accuracy of the regression network, we demonstrate the effectiveness of the latent features by visualizing the reconstructed 3D point clouds and lowdimensional representations obtained with t-SNE [7]. Being a fully-automatic and unsupervised approach, our method does not require any time-consuming human labeling effort and additionally avoids any subjective biases."
Unsupervised Learning for Feature Extraction and Temporal Alignment of 3D+t Point Clouds of Zebrafish Embryos,2,Methods,"Our method is based on the FoldingNet [1] as the point cloud feature extractor. Several modifications are added to both the network and the loss function. The temporal alignment is realized with a regression network using the latent features and a subsequent consistency-check is used as a postprocessing to further improve the results. Finally, we introduce a new method to synthesize validation data sets with known ground truth from a set of unlabeled embryo point clouds."
Unsupervised Learning for Feature Extraction and Temporal Alignment of 3D+t Point Clouds of Zebrafish Embryos,2.1,Autoencoder,"FoldingNet: FoldingNet is an autoencoder specifically designed for 3D point clouds [1]. In the encoder, a k-nearest-neighbor graph is built and the local information of each sub-region is extracted using the graph layers. A global maxpooling operation is applied to the local features to obtain the one-dimensional latent feature vector in the bottleneck layer. As a symmetric function, the pooling operation adds permutation invariance to cope with the disorderliness of point clouds. In the decoder, the feature vector is duplicated and concatenated with a fixed grid of points. The latent features lead to the deformation of the point grid in a 3-layer multi-layer perceptron (MLP) and a 3D structure is constructed. With a second folding operation, more details are rebuilt and the input is reconstructed. Unlike the original FoldingNet [1], we use a spherical template (M evenly distributed 3D points on a spherical surface) instead of a planar template, which provides a better initialization for reconstructing spherical objects from the learned representations. Since the shape of the embryos varies from a hemisphere to an ellipsoid during development, the spherical template simplifies the folding operation and improves the reconstruction quality in combination with the Modified Chamfer Distance loss (see next section).Modified Chamfer Distance: The Chamfer Distance (CD) is one of the most widely used similarity measures for point clouds and is used as the loss function of FoldingNet. The discrepancy between two point clouds is calculated as the sum of the distances between the closest pairs of points as follows:where S in , S out are the input and reconstructed point clouds, respectively. However, this approach does not consider the local density distribution since all points are treated independently. In this work, we introduce the Modified Chamfer Distance (MCD), in which the point-to-region distance replaces the pointto-point distance. The new loss is the summation of the k-nearest-neighbors of each point in the target point cloud and defined as:where d is the Euclidean distance between a point p and its respective nearest neighbor q i . As the embryos grow, the density distribution changes significantly in different parts. The utilized data set [4] is spatially prealigned such that the animal and vegetal pole align with the y-axis and the prospective dorsal part with the positive x-axis. During epiboly, the embryo grows from a hemisphere to a complete sphere in the negative y-direction. As development progresses into the bud stage, density increases in the direction of the positive x-axis and thus the center of gravity moves to the right. We found that a FoldingNet trained with MCD consistently yielded more accurate reconstructions and alignment results compared to CD (Suppl. Fig. 1, Suppl. Fig. 6). In Fig. 1, the input and the reconstructed point clouds of an embryo from the hold-out test set are visualized using ParaView [9] to qualitatively illustrate the effectiveness of the FoldingNet that was trained with the MCD loss.  "
Unsupervised Learning for Feature Extraction and Temporal Alignment of 3D+t Point Clouds of Zebrafish Embryos,2.2,Regression Network,"The features extracted by the autoencoder are used to generate the temporal alignment of different embryos. We select one embryo as the reference and train an MLP regression network that maps autoencoder-generated feature vectors to frame numbers. The regression MLP consists of a sequence of fully-connected layers with ReLU activation. The input is a latent feature vector with length 256. In each dense layer, the size of the vector is reduced by a factor of two. The penultimate layer converts the 8-dimensional vector directly to the output node. The mean squared error is utilized as the loss function to compare the output to the ground truth: the sequence of time frame indices from 1 to 370. To temporally synchronize a new embryo with the reference, we present extracted features of all its frames to the trained regression network and generate a new index sequence. The desired alignment result is obtained by comparing the generated sequence with the reference embryo (Fig. 2). Postprocessing: By definition, the sequence of time frame indices must be monotonically increasing. However, since the alignment results are generated from the point cloud of each time frame individually, the relationship between the time points in the sequence is not considered. So the generated sequences are not guaranteed to be monotonically increasing. We use a simple postprocessing strategy to generate monotonically increasing and more accurate alignment results. For an aligned sequence with oscillations, we generate its monotonically increasing upper and lower boundaries, and the desired result is obtained by calculating the mean values of those boundaries (Suppl. Fig. 3).3 Experimental Results and Discussions"
Unsupervised Learning for Feature Extraction and Temporal Alignment of 3D+t Point Clouds of Zebrafish Embryos,3.1,Data Sets and Evaluation,"The data set used in this work was published in [4] and consists of four wild-type zebrafish embryos that were imaged from 4.7 to 10.0 hpf (hours post fertilization) with one minute time intervals. Each embryo is represented as a 3D point cloud and has 370 time frames. The staging of these data sets was performed at a single time point (10 hpf) and the 370 preceding frames were selected irrespective of potential developmental differences. We thus only know that the temporal windows of the four embryos largely overlap but do not have frameaccurate annotations of the actual developmental time (see [2,4,12] for details).To simplify the training approach, a fixed number of points is randomly chosen from each point cloud using the PyTorch Geometric library [10] and we use the data loader implemented in the PyTorch 3D Points library [11]. We choose 4096 points since the size of the original point clouds ranges from 4160 to 19794. For improved generalizability and orientation invariance of the FoldingNet-based autoencoder, the point clouds are randomly rotated between 0 and 360 • along each axis as data augmentation. For additional augmentation, we generated randomized synthetic variants of the four embryos as described in [2]. Since there are no labeled data to evaluate the temporal alignment results, we introduce a new method to artificially generate ground truth for validation by randomly varying the speed of development of a selected embryo (sin, cos, Gaussian, and linear-based stretching/compression of the time axis with interpolated/skipped intermediate frames). A Gaussian-distributed point jitter (μ = 0, σ 2 = 5) is applied to the shifted embryos to make them substantially differ from the originals."
Unsupervised Learning for Feature Extraction and Temporal Alignment of 3D+t Point Clouds of Zebrafish Embryos,3.2,Experimental Settings,"The FoldingNet-based autoencoder is implemented with PyTorch Lightning. The number of neighbors is set to 16 in the KNN-graph layers and to 20 in the MCD. The autoencoder is trained with an initial learning rate of 0.0001 for 250 epochs and a batch size of 1. At each iteration, the embryo from a single time frame with the size 4096 × 3 is given to the network and the encoder converts it to a 1D codeword of length 256, which is the latent feature vector.For network training with the four wild-type embryos, we use a 4-fold crossvalidation scheme with three embryos for training and one for testing in each fold. The regression network is trained for 700 epochs with a learning rate of 0.00001. The hyperparameters of the regression network are determined empirically based on the convergence of the training loss, since there is no validation or test set available. The temporal alignment is validated with the embryo from the test set to make the result independent from training the autoencoder and the embryo is aligned to its shifted variants. To reduce the influence of randomness, we repeat the alignment of each test embryo three times and take the average value of all experiments of the four embryos."
Unsupervised Learning for Feature Extraction and Temporal Alignment of 3D+t Point Clouds of Zebrafish Embryos,3.3,Experimental Results,"Temporal Alignment Results: The resulting alignment with different shifting methods is depicted in Fig. 3. In Fig. 3(a) and Fig. 3(b), a cosine-and sine-shifted embryo is aligned to the original one, where the cosine-based temporal shifting lets the embryo develop faster at the beginning and then slow down while the sine-based shifting is defined contrarily. In Fig. 3(c), a Gaussian-distributed random difference is added to the shifted embryo. Furthermore, Fig. 3(d) illustrates an embryo that develops approximately three times faster than the original one. The alignment error is defined as the average number of mismatched indices, which is the difference between the sequence of the aligned indices and the ground truth in the x-direction. As a result, an average mismatch of only 3.83 min in a total developmental period of 5.3 h is achieved (Table 1). Impact of Spatial Transformations: In the previous experiments, the embryos to be aligned were generated by changing the development speed and by scattering the points to make them different from the original ones. However, the embryos to be aligned in real applications could be located and oriented differently in space. Thus, we add some data augmentation for the alignment network to increase its rotation invariance. The point clouds are randomly rotated between ±20 • along each axis. Moreover, the shifted embryos to be aligned are rotated between ±15 • . The average alignment error obtained for rotated embryos is 3.48 min (Suppl. Fig. 4). However, a larger variance can be observed, which indicates that rotation can have an impact on the alignment accuracy in some cases. Since the overall accuracy is still high, however, our approach proofs to be robust for aligning embryos with slightly varying orientations. In addition to the orientation, embryos may also be positioned differently in the sample chamber. To potentially improve the translation invariance, we tested if centering all point clouds at the origin of the coordinate axes before inputting them to the alignment network has a positive effect (Suppl. Fig. 5). Although centering makes the approach invariant to spatial translation, we find that the alignment accuracy is reduced (aligned sequences have a more significant variance, and the average error increases to 5.74 min). We hypothesize that the relative displacement of the point cloud's centroid to the origin of the coordinate system (which is removed by centering) may play an important role in determining the developmental stage and the level of completion of the epiboly phase. An overview of obtained alignment results is provided in Table 1.Visualizing the Learned Representation: To confirm that the autoencoders actually learned a representation suitable for temporal alignment, we visualize a chronologically color-coded scatter plot of the learned 256-dimensional feature vectors of all 370 time frames using the t-SNE algorithm [7] (Fig. 4). The original features are clustered to a narrow band and the color changes smoothly as the index increases, which indicates that different time frames are well distinguishable using the representation learned by the autoencoder. When the point clouds are rotated and centered, the projections of the feature vectors become more dispersed as illustrated in Fig. 4(b) and 4(c). Nevertheless, the color changes smoothly and the features are suitable for distinguishing different time frames. This is in line with the observed increased variance for the temporal alignment results with maintained good average alignment accuracy."
Unsupervised Learning for Feature Extraction and Temporal Alignment of 3D+t Point Clouds of Zebrafish Embryos,4,Conclusion,"In this work we present a fully-unsupervised approach to temporally align 3D+t point clouds of zebrafish embryos. A FoldingNet-based autoencoder is implemented to extract low-dimensional descriptive features from large-scaled 3D+t point clouds of embryonic development. Several modifications are made to the network and the loss function to improve their applicability for this application.The embryos are temporally aligned by applying a regression network to the features extracted by the autoencoder. A postprocessing method is designed to provide consistent and accurate alignments. As no frame-accurate ground truthis available yet, we assess the effectiveness of our method via a 4-fold cross validation and a synthetically generated ground truth. An average mismatch of only 3.83 min in a developmental period of 5.3 h is achieved. Finally, we performed several ablation studies to show the impact of rotation and spatial translation of the point clouds to the alignment results. By aligning embryos with different spatial locations and deflected central axis, a relatively small error rate of 5.74 min can still be achieved. According to feedback from a biological expert the achievable manual alignment accuracy is on the order of 30 min and potentially exhibits intra-and inter-rater variabilities. As the first unsupervised method designed for the automatic spatiotemporal alignment of 3D+t point clouds, our method achieves high accuracy and eradicates the need for any human interaction. This will particularly help to minimize human effort, to speedup experimental analysis and to avoid any subjective biases.In future works, our approach could be applied to more data sets and other model organisms with different scales and development periods to further validate its applicability. We're currently conducting an extensive effort to obtain frame-accurate manual labels from multiple raters in a randomized study, to better assess the actual performance that we can expect under real-world conditions including intra-and inter-rater variability. In the long term, we envision an iteratively optimized spatiotemporal average model of multiple wild-type embryos to finally obtain a 3D+t reference atlas that can be used to precisely analyze developmental differences of corresponding anatomical regions across experiments."
Unsupervised Learning for Feature Extraction and Temporal Alignment of 3D+t Point Clouds of Zebrafish Embryos,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3_58.
A Coupled-Mechanisms Modelling Framework for Neurodegeneration,1,Introduction,"Computational models of neurodegeneration aim to emulate the underlying physical process of how disease initiates and progresses over the brain from E. Thompson and A. Schroder-Contributed equally to this work as the co-second authors.a mechanistic point of view [6,10,25]. A better understanding of disease mechanisms and inter-individual variability through these models will aid in the development of new treatments and disease prevention strategies. Two types of components typically contribute to such models: models of pathology appearancehow and where pathology spontaneously appears in different brain regions; and models of pathology spreading -how pathology spreads from region to region. Both components are often linked to brain connectivity so that spontaneous appearance arises according to network topologies, and spreading is facilitated by network edges or brain-connectivity pathways.Network metrics link spontaneous appearance of pathology to brain connectivity via various mechanistic hypotheses. For instance, Zhou et al. [27] relate disease patterns with several network topologies: i) centrality [3], indicating that regions with denser connections are more vulnerable to disease due to heavier nodal stress; ii) segregation [1], the converse of centrality, stating that regions with sparse connections are more susceptible due to lack of trophic sources; iii) shared vulnerability, expounding that connected regions have evenly distributed disease because they have common characteristics such as gene expressions [10].For the spreading component, dynamical systems models emulate the spatiotemporal propagation process along the brain connectivity architecture [9,11,15,16,23]. One popular example is the network diffusion model [15] (NDM), which assumes that the pathology purely diffuses from epicenters. Weickenmeier et al. [25] combine the disease diffusion process with a local production term in a single model, which emulates the full process of how the protein diffuses from epicenters and replicates locally, gradually reaching a plateau. Thus, this model is able to reconstruct the process from disease onset to later stages.However, such models make specific choices on the underlying mechanism in the particular physical process. The complexity and heterogeneity of neurodegenerative conditions suggests that multiple processes may contribute and vary among individuals. To avoid making such assumptions, Garbarino et al. [7] use a data-driven, linear combination of several network topological descriptors extracted from the structural connectome to match the disease patterns fitted by a Gaussian Process progression model. They find that the combination, which they refer to as a ""mechanistic profile"" better matches observed pathology patterns than any single characteristic. However, this considers appearance and spreading as interchangeable rather than interacting mechanisms. Secondly, although [7] does produce individual-level as well as group-level mechanistic profiles, the individual-level profiles assume that all the subjects lie on the same disease progression trajectory. This does not fully capture the heterogeneity (such as different epicenters [24], diffusion rate) within groups and underestimates the variability in composition of the mechanistic profile among subjects.In this work, we introduce an alternative model framework that non-linearly couples the effects of spontaneous appearance and spreading. We construct a Bayesian framework with an appropriate sparsity structure to estimate the mechanistic profile, in a similar way to [7] but including interaction of model components and quantification of uncertainty. We account for the heterogeneity of neurodegenerative diseases in a more complex way than [7], by allowing factors like epicenters, rates of diffusion and production, and the weights of network metrics to vary among individuals. The resulting mechanistic profiles highlight distinct subgroups of individuals within an Alzheimer's disease (AD) cohort, in which each subgroup has similar combinations of network metrics."
A Coupled-Mechanisms Modelling Framework for Neurodegeneration,2.1,Model Definition,"Baseline Model. One model of disease spreading [14,25] based on the Fisher-Kolmogorov equation, assumes that the concentration of toxic protein can be emulated by an ordinary differential equation (ODE) system, which involves the combination of two physical processes: 1) the diffusion of toxic proteins along the structural network from an epicentre(s), as described by the NDM [15]; 2) its local production and aggregation at each node. The diffusion component includes the graph Laplacian matrix L calculated from the structural connectome [15], as a substrate for disease diffusion with the rate k. The production and aggregation part provides a monotonically increasing trend converging to a plateau level v with a common speed α shared by all regions. The model evolves the pathology concentration c at time t according to:where denotes the element-wise product. It constructs the disease progression process from onset to late stage based on the single mechanism of network proximity. It assumes a constant local production rate across regions, thus the growth of concentration only depends on the level of biomarker propagating from the epicenter along the structural connectome to each node. This does not take into account the synergistic effect of other mechanisms.Coupled Model. Following [25], we retain the network proximity mechanism for the diffusion component of our model, but we weight the local production process with the combination of P network metrics M = [m 1 , ..., m P ]. We use w = [w 1 , ..., w P ] T to represent the weighting, or extent of contribution of each characteristic. In contrast to the baseline model, by weighting the rate of production with the combination of network metrics, we further incorporate pathology appearance proportional to various brain network topologies to the disease spreading process. The new model expresses this coupling as follows:Network Metrics. The network metrics considered by the model are listed in Table 1 and a visualization is shown in Fig. 1. StructureC, InvGeodist, FunctionC represent the structural connectome, the matrix of the inverse of geodesic distance along the cortical surface and the functional connectome.  BetweennessCentrality is the fraction of all shortest paths that contain a specific node. ClusterCoefficient quantifies the extent to which one node is clustered with its neighbours. ClosenessCentrality is proportional to the reciprocal of the sum of the shortest path length between the node and others. WeightedDegree is the sum of the weights of the connections. Calculation of metrics has been done using the Brain Connectivity Toolbox [18]. Metrics m1 to m5 have been normalized between [0,1] to maintain the same scale. Visualization is done using NetworkX (https://networkx. org/)."
A Coupled-Mechanisms Modelling Framework for Neurodegeneration,2.2,Bayesian Framework,"In order to quantify the uncertainty of the estimation, we construct a Bayesian inference framework for our dynamic system, thus we are able to obtain distributions of parameters rather than deterministic values.Parameter Distributions. In this work we focus on modelling the dynamics of tau protein, which is widely hypothesized to be a key causative agent in AD. Its concentration can be measured in vivo by positron emission tomography (PET). We assume for subject i at jth scan time t ij , the measurement of tau concentration ĉ(t ij ) follows a normal distributionwhere the mean is the model prediction with best-fit parameters and the error is quantified by the standard deviation σ. The time gap δ ij (in years) between the baseline scan and the jth follow-up scan is available in the dataset. However the time from the disease onset to the baseline scan is unknown. Thus we need to estimate such time t onset i such that t ij = t onset i + δ ij . This time parameterization enforces the relevant locations among all scans fixed by given δ ij .Descriptions of the key model parameters are displayed in Table 2. We enforce the rate of spreading and production to be positive by selecting a Half-Normal prior. The hyper-parameters of the rates are decided according to the research findings that the annual change rate of tau-PET signal is quite slight [12,20]. Explorations of the proper choice of the hyper-parameters has also been done by simulating different parameter levels and comparing the generated trajectories to the measured data distribution. w Feature weights Dirichlet (β1 . . . βP ) a For ease of notation and clarity in the subsequent discussions, we drop the subject index i for the individual-level model parameters.Feature Selection with Sparsity. We account for the feature importance by estimating the weights of network metrics w = [w 1 , ..., w P ] T ∼ Dirichlet(β 1 , . . . , β P ).We seek the minimal set of network components that explain the data to define the mechanistic profile. Thus, we apply sparsity to the weight in a Bayesian way by introducing the Horseshoe prior [4] to the hyper-parameters of the Dirichlet distribution:This horseshoe structure includes a global shrinkage parameter τ and local shrinkage parameters λ l , each following a half Cauchy distribution: λ l , τ ∼ HalfCauchy(0, 1).The flat tail from Cauchy Distribution allows the features with strong contribution to remain with a heavy tail of the density, while the sharp rise in density near 0 shrinks the weight of the features with weak signal."
A Coupled-Mechanisms Modelling Framework for Neurodegeneration,2.3,Variational Inference,"Suppose x, z and θ represent the collections of observations, hidden variables and parameters respectively. Due to the complexity of the model structure, the posterior p θ (z | x) we are interested in is often intractable and hard to obtain analytically. Thus we use the variational distribution q φ (z) with φ as the variational parameters to approximate the posterior. The evidence lower bound, ELBO ≡ E q φ (z) [log p θ (x, z)log q φ (z)], can be used to approach the log likelihood, since the gap between them is the Kullback-Leibler divergence between the variational distribution and the posterior, which is larger or equal to 0:Thus the objective of the optimization is to maximize the ELBO. We use a normal distribution with a diagonal covariance matrix as the variational distribution to sample the hidden variables in the latent space, and then use proper parameter transformation to obtain the constrained hidden variables. The process is accomplished with the use of Pyro [2], a probabilistic programming framework."
A Coupled-Mechanisms Modelling Framework for Neurodegeneration,3.1,Data Processing,"Brain Networks. Three types of connectivity were used to extract features for our coupled models: 1) the structural connectome, which contains the number of white matter fibre trajectories; 2) the matrix of the geodesic distance along the cortical surface; 3) the functional connectome, which reflects the synchrony of neural functioning among regions. The structural connectome is an average of 18 young and healthy subjects' connectomes from the Human Connectome Project [22]. We generated streamlines using probabilistic, anatomically constrained tractography, processed using MRtrix3 [21], and filtered the streamlines using the SIFT algorithm [19]. The geodesic distance matrix and the functional connectome are obtained from the Microstructure-Informed Connectomics Database [17], and is an average of 50 healthy subjects' matrices. We define the brain regions according to the Desikan-Killiany Atlas [5].Tau-PET Data. We model the dynamics of tau protein measured by PET scans. We use the tau-PET standardized uptake value ratios (SUVRs) downloaded from the Alzheimer's Disease Neuroimaging Initiative (ADNI) 1 (adni.loni.usc.edu) [13]. We exclude subcortical regions, which are impacted by off-target binding of the radiotracer [8]. Two-component Gaussian mixture modelling is applied to the SUVR signals of all subjects. We treat the distribution with the lower mean as the distribution of negative signals, and define the mean plus one standard deviation of this distribution as the threshold for tau-positivity.1 Data used in preparation of this article were obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database (adni.loni.usc.edu). As such, the investigators within the ADNI contributed to the design and implementation of ADNI and/or provided data but did not participate in analysis or writing of this report. A complete listing of ADNI investigators can be found at: http://adni.loni.usc.edu/ wp-content/uploads/how to apply/ADNI Acknowledgement List.pdf "
A Coupled-Mechanisms Modelling Framework for Neurodegeneration,,Selection of Subjects.,"We include N = 110 subjects with at least two tau-PET scans, amyloid beta positive status and at least one region with positive tau signal, including healthy, cognitively impaired and AD subjects, since we aim to focus on the people with a potential to accumulate abnormal pathological tangles. We normalise the data of all the subjects (i = 1,..,N) between 0 and 1 by (tau itau min )/(tau maxtau min ) where tau min and tau max are calculated across all the subjects and regions, thus the differences in data scales among subjects and regions are maintained. Setting of Epicentres. For initialization, we assume pathology starts from candidate epicentres, to simulate the full process of disease progression from very early stages, i.e. prior to the baseline scan. We rank 34 pairs of bilateral cortex regions according to the total number of subjects that have positive tau signals, and pick the top eight pairs of regions as the candidate epicentres where the propagation of pathology is likely to start: inferior temporal cortex, banks of the superior temporal sulcus, fusiform gyrus, lateral orbitofrontal cortex, middle temporal gyrus, entorhinal cortex, parahippocampal gyrus and temporal pole. The four epicentres identified by Vogel et al. [24] are all included."
A Coupled-Mechanisms Modelling Framework for Neurodegeneration,3.2,Results,"We fix the initial tau level at each candidate epicenter and the end level of the plateau of all the subjects to be 0.01 and 1.5 and fit each subject using the baseline model and our coupled model respectively. For evaluation we use Pearson R correlation between the measured and model fitted values. Figure 2 compares the distribution of R correlations which reflect the performance of the baseline model and our coupled model on each individual. According to the one-side t test, the R correlations from the coupled model are significantly larger than those from the baseline model (p-value = 3.5732e-22). Especially, fitting of subjects with particularly low performance in the baseline model are noticeably improved.Figure 3 visualizes the model-fitted tau signals versus measured tau signals in all 68 cortex regions. The distribution of tau fitted by our coupled model is closer to the measured pattern. Finally, we subtype the individuals according to the top two most dominant mechanisms of pathology appearance interacting with the network spreading model. Specifically, we encode each subject with a vector containing the rank of each metric according to the obtained weights, and assign the subjects having the same rank of the top two metrics to the same group. We consider a group containing at least 6 people (5% of all) as one subtype. As a result, 83 out of 110 subjects have been assigned to six subtypes. Figure 4A displays the feature importance distribution for an individual, while Fig. 4B places the feature distributions of the subjects belonging to the same subtype within each of the six plots. It can be observed that structural centrality appears most frequently as a dominant feature, followed by structural segregation."
A Coupled-Mechanisms Modelling Framework for Neurodegeneration,4,Conclusions,"We introduce a new Bayesian modelling framework that couples pathology appearance and spreading, by embedding the mechanistic profiles that consist of combinations of network metrics into the dynamic system of disease spreading. This improves the fitting of the observed pathology pattern, and provides a potential way to subtype subjects according to mechanistic profiles. For future work, we will validate the cohort-level mechanistic profiles derived from the identified subtypes using external datasets, and also verify the subtypes using other algorithms such as the SuStaIn [26]. Furthermore, we will incorporate uncertainty from connectomes. We will also perform further comparisons with other state-of-the-art models, such as the topological profiles by [7], which is currently hard to compare directly due to various differences in the model design."
A Coupled-Mechanisms Modelling Framework for Neurodegeneration,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3 45.
B-Cos Aligned Transformers Learn Human-Interpretable Features,1,Introduction,"Making artificial neural networks more interpretable, transparent, and trustworthy remains one of the biggest challenges in deep learning. They are often still considered black boxes, limiting their application in safety-critical domains such as healthcare. Histopathology is a prime example of this. For years, the number of pathologists has been decreasing while their workload has been increasing [23]. Consequently, the need for explainable computer-aided diagnostic tools has become more urgent.As a result, research in explainable artificial intelligence is thriving [20]. Much of it focuses on convolutional neural networks (CNNs) [13]. However, with the rise of transformers [31] in computational pathology, and their increasing application to cancer classification, segmentation, survival prediction, and mutation detection tasks [26,32,33], the old tools need to be reconsidered. Visualizing filter maps does not work for transformers, and Grad-CAM [30] has known limitations for both CNNs and transformers.The usual way to interpret transformer-based models is to plot their multihead self-attention scores [8]. But these often lead to fragmented and unsatisfactory explanations [10]. In addition, there is an ongoing controversy about their trustworthiness [5]. To address these issues, we propose a novel family of transformer architectures based on the B-cos transform originally developed for CNNs [7]. By aligning the inputs and weights during training, the models are implicitly forced to learn more biomedically relevant and meaningful features (Fig. 1). Overall, our contributions are as follows:• We propose the B-cos Vision Transformer (BvT) as a more explainable alternative to the Vision Transformer (ViT) [12]. • We extensively evaluate both models on three public datasets: NCT-CRC-HE-100K [18], TCGA-COAD-20X [19], Munich-AML-Morphology [25]. • We apply various post-hoc visualization techniques and conduct a blind study with domain experts to assess model interpretability. • We derive the B-cos Swin Transformer (Bwin) based on the Swin Transformer [21] (Swin) in a generalization study."
B-Cos Aligned Transformers Learn Human-Interpretable Features,2,Related Work,"Explainability, interpretability, and relevancy are terms used to describe the ability of machine learning models to provide insight into their decision-making process. Although these terms have subtle differences, they are often used interchangeably in the literature [15]. Recent research on understanding vision models has mostly focused on attribution methods [13,20], which aim to identify important parts of an image and highlight them in a saliency map. Gradient-based approaches like Grad-CAM [30] or attribution propagation strategies such as Deep Taylor Decomposition [27] and LRP [6] are commonly used methods. Perturbation-based techniques, such as SHAP [22], are another way to extract salient features from images. Besides saliency maps, one can also visualize the activations of the model using Activation Maximization [14].However, it is still controversial whether the above methods can correctly reflect the behavior of the model and accurately explain the learned function (model-faithfulness [17]). For example, it has been shown that some saliency maps are independent of both the data on which the model was trained and the model parameters [2]. In addition, they are often considered unreliable for medical applications [4]. As a result, inherently interpretable models have been proposed as a more reliable and transparent solution. The most recent contribution are B-cos CNNs [7], which use a novel nonlinear transformation (the B-cos transformation) instead of the traditional linear transformation.Compared to CNNs, there is limited research on understanding transformers beyond attention visualization [10]. Post-hoc methods such as Grad-CAM [30] Fig. 3. Rollout, Attention-Last (Attn-Last), Grad-CAM, LRP, LRP of the second layer (LRP-Second), LRP of the last layer (LRP-Last), and Transformer Attribution (TA) applied on the test set of Munich-AML-Morphology. The image shows an eosinophil, which is characterized by its split, but connected nucleus, large specific granules (pink structures in the cytoplasm), and dense chromatin (dark spots inside the nuclei) [29]. Across all visualization techniques, BvT focuses on these exact features unlike ViT.and Activation Maximization [14] used for CNNs can also be applied to transformers. But in practice, the focus is on visualizing the raw attention values (see Attention-Last [16], Integrated Attention Maps [12], Rollout [1], or Attention Flow [1]). More recent approaches such as Generic Attention [9], Transformer Attribution [10], and Conservative Propagation [3] go a step further and introduce novel visualization techniques that better integrate the attention modules with contributions from different parts of the network. Note that these methods are all post-hoc methods applied after training to visualize the model's reasoning.On the other hand, the ConceptTransformer [28], achieves better explainability by cross-attending user-defined concept tokens in the classifier head during training. More recently, HIPT [11] combines multi-scale images and DINO [8] pre-training to learn hierarchical visual concepts in a self-supervised fashion. Unlike all of these methods, interpretability is already an integral part of our architecture. Therefore, these methods can be easily applied to our models. In Fig. 3 and Fig. 6, we show that the B-Cos Transformer produces superior feature maps over various post-hoc approaches -suggesting that our architecture does indeed learn human-plausible features that are independent of the specific visualization technique used."
B-Cos Aligned Transformers Learn Human-Interpretable Features,3,Methods,"We focus on the original Vision Transformer [12]: The input image is divided into non-overlapping patches, flattened, and projected into a latent space of dimension d. Class tokens [cls] are then prepended to these patch embeddings. In addition, positional encodings [pos] are added to preserve topological information. In the scaled dot-product attention [31], the model learns different features (query Q, key K, and value V ) from the input vectors through a linear transformation. Both query and key are then correlated with a scaled dot-product and normalized with a softmax. These self-attention scores are then used to weight the value by importance:To extract more information, this process is repeated h times in parallel (multi-headed self-attention). Each self-attention layer is followed by a fullyconnected layer consisting of two linear transformations and a ReLU activation.We propose to replace all linear transforms in the original ViT (Fig. 2)c(x, w) = cos(∠(x, w)), ∠...angle between vectorswith the B-cos* transform [7] B-cos*(x;where B ∈ N. Similar to [7], an additional nonlinearity is applied after each B-cos* transform. Specifically, each input is processed by two B-cos* transforms, and the subsequent MaxOut activation passes only the larger output. This ensures that only weight vectors with higher cosine similarity to the inputs are selected, which further increases the alignment pressure during optimization. Thus, the final B-cos transform is given byTo see the significance of these changes, we look at Eq. 4 and deriveFig. 4. We compute the central kernel alignment (CKA), which measures the representation similarity between each hidden layer. Since the B-cos transform aligns the weights with the inputs, BvT (ours) achieves a more uniform representation structure compared to ViT (values closer to 1). When trained with the binary cross-entropy loss (BCE) instead of the categorical cross-entropy loss (CCE), the alignment is higher.Since |c(x, ŵ)| ≤ 1, equality is only achieved if x and w are collinear, i.e., if they are aligned. Intuitively, this forces the weight vector to be more similar to the input. Query, key, and value thus capture more patterns in an image -which the attention mechanism can then attend to. This can be shown visually by plotting the centered kernel alignment (CKA). It measures the similarity between layers by comparing their internal representation structure. Compared to ViTs, BvTs achieve a highly uniform representation across all layers (Fig. 4)."
B-Cos Aligned Transformers Learn Human-Interpretable Features,4,Implementation and Evaluation Details,"Task-Based Evaluation: Cancer classification and segmentation is an important first step for many downstream tasks such as grading or staging. Therefore, we choose this problem as our target. We classify image patches from the public colorectal cancer dataset NCT-CRC-HE-100K [18]. We then apply our method to TCGA-COAD-20X [19], which consists of 38 annotated slides from the TCGA colorectal cancer cohort, to evaluate the effectiveness of transfer learning. This dataset is highly unbalanced and not color normalized compared Fig. 5. In a blinded study, domain experts ranked models (lower is better) based on whether the models focus on biomedically relevant features that are known in the literature to be important for diagnosis. We then performed the Conover post-hoc test after Friedman with adjusted p-values according to the two-stage Benjamini-Hochberg procedure. BvT ranks above ViT with p < 0.1 (underlined) and p < 0.05 (bold). to the first dataset. Additionally, we demonstrate that the B-cos Vision Transformer is adaptable to domains beyond histopathology by training the model on the single white blood cell dataset Munich-AML-Morphology [25], which is also highly unbalanced and also publicly available.Domain-Expert Evaluation: Our primary objective is to develop an extension of the Vision Transformer that is more transparent and trusted by medical professionals. To assess this, we propose a blinded study with four steps: (i) randomly selecting images from the test set of TCGA-COAD-20X (32 samples) and Munich-AML-Morphology (56 samples), (ii) plotting the last-layer attention and transformer attributions for each image, (iii) anonymizing and randomly shuffling the outputs, (iv) submitting them to two domain experts in histology and cytology for evaluation. Most importantly, we show them all the available saliency maps without pre-selecting them to get their unbiased opinion.Implementation Details: In our experiments, we compare different variants of the B-cos Vision Transformer and the Vision Transformer. Specifically, we implement two versions of ViT: ViT-T/8 and ViT-S/8. They only differ in parameter size (5M for T models and 22M for S models) and use the same patch size of 8. All BvT models (BvT-T/8 and BvT-S/8) are derivatives of the corresponding ViT models. The B-cos transform used in the BvT models has an exponent of B = 2. We use AdamW with a cosine learning rate scheduler for optimization and a separate validation set for hyperparameter selection. Following the findings of [7], we add [1 -r, 1 -g, 1 -b] to the RGB channels [r, g, b] of BvT. This allows us to encode each pixel with the direction of the color channel vector, forcing the model to capture more color information. Furthermore, we train models with two different loss functions: the standard categorical cross-entropy loss (CCE) and the binary cross-entropy loss (BCE) with one-hot encoded entries. It was suggested in [7] that BCE is a more appropriate loss for B-cos CNNs. We explore whether this is also true for transformers in our experiments. Additional details on training, optimization, and datasets can be found in the Appendix."
B-Cos Aligned Transformers Learn Human-Interpretable Features,5,Results and Discussion,"Task-Based Evaluation: When trained from scratch, all BvT models underperform their ViT counterparts by about 2% on NCT-CRC-HE-100K and 3% on Munich AML-Morphology (Table 1). However, when we use the pretrained weights from NCT-CRC-HE-100K and transfer them to TCGA-COAD-20X for fine-tuning, BvT outperforms ViT by up to 5% (Table 1). We believe this is due to the simultaneous optimization of two objectives: classification loss and weight-input alignment. With a pre-trained model, BvT is likely to focus more on the former. In addition, we observe that models trained with BCE tend to perform worse than those trained with CCE. However, their saliency maps seem to be more interpretable (see Fig. 3)."
B-Cos Aligned Transformers Learn Human-Interpretable Features,,Domain-Expert Evaluation:,"The results show that BvTs are significantly more trustworthy than ViTs (p < 0.05). This indicates that BvT consistently attends to biomedically relevant features such as cancer cells, nuclei, cytoplasm, or membrane [24] (Fig. 5). In many visualization techniques, we see that BvT, unlike ViT, focuses exclusively on these structures (Fig. 3). In contrast, ViT attributes high attention to seemingly irrelevant features, such as the edges of the cells. A third expert points out that ViT might overfit certain patterns in this dataset, which could aid the model in improving its performance."
B-Cos Aligned Transformers Learn Human-Interpretable Features,6,Generalization to Other Architectures,"We aim to explore whether the B-cos transform can enhance the interpretability of other transformer-based architectures. The Swin Transformer (Swin) [21] is a popular alternative to ViT (e.g., it is currently the SOTA feature extractor for histopathological images [33]). Swin utilizes window attention and feed-forward layers. In this study, we replace all its linear transforms with the B-cos transform, resulting in the B-cos Swin Transformer (Bwin). However, unlike BvT and ViT, it is not obvious how to visualize the window attention. Therefore, we introduce a modified variant here that has a regular ViT/BvT block in the last layer.In our experiments (Table 2), we observe that Bwin outperforms Swin by up to 2.7% and 4.8% in F1-score on NCT-CRC-HE-100K and Munich-AML-Morphology, respectively. This is consistent with the observations made in Sect. 5: When BvT is trained from scratch, the model faces a trade-off between learning the weight and input alignment and finding the appropriate inductive bias to solve the classification task. By reintroducing many of the inductive biases of CNNs through the window attention in the case of Swin or transfer learning in the case of BvT, the model likely overcomes this initial problem.Moreover, we would like to emphasize that the modified models have no negative impact on the model's performance. In fact, all metrics remain similar or even improve. The accumulated attention heads (we keep 50% of the mass) demonstrate that Bwin solely focuses on nuclei and other cellular features (Fig. 6). Conversely, Swin has very sparse attention heads, pointing to a few spots. Consistent with the BvT vs ViT blind study, our pathologists also agree that Bwin is more plausible than Swin (p < 0.05)."
B-Cos Aligned Transformers Learn Human-Interpretable Features,7,Conclusion,"We have introduced the B-cos Vision Transformer (BvT) and the B-cos Swin Transformer (Bwin) as two alternatives to the Vision Transformer (ViT) and the Swin Transformer (Swin) that are more interpretable and explainable. These models use the B-cos transform to enforce similarity between weights and inputs. In a blinded study, domain experts clearly preferred both BvT and Bwin over ViT and Swin. We have also shown that BvT is competitive with ViT in terms of quantitative performance. Moreover, using Bwin or transfer learning for BvT, we can even outperform the original models."
B-Cos Aligned Transformers Learn Human-Interpretable Features,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3_50.
Learning Asynchronous Common and Individual Functional Brain Network for AD Diagnosis,1,Introduction,"Building and analyzing functional brain network (FBN) based on resting-state magnetic resonance imaging (rs-fMRI) have become a promising approach to functional brain disease diagnosis, e.g., Alzheimer's disease (AD) and Parkinson's disease. Rs-fMRI probes neural activity by the fluctuations in the bloodoxygen-level-dependent (BOLD) signals. The strength of functional connectivities (FCs) between brain regions is measured by the correlation between pairwise BOLD signals. FBN represents the interaction patterns between brain regions during brain functioning and can be used to identify abnormal changes caused by brain diseases [9,28,32]. Such abnormal alterations can be used as diagnostic biomarkers of brain diseases [3,11,14,16,21,22].Traditional FBNs are constructed at the individual level using Pearson's correlation [26]or sparse representation (SR) [12]. Although this approach is popular due to its simplicity and efficiency, it suffers from multiple limitations. Firstly, due to the unpredictable interferences of noises and artifacts, a potential issue is that the constructing of FBNs at the individual level inevitably leads to large variability in the topographical structure of the networks. So the subtle diseasespecific FCs change in the FBNs of patients in comparison with the healthy are likely to be overwhelmed by such large variability [31]. Such variability results in unsatisfactory diagnosis accuracy and poor generality for the classifier. Another issue with the traditional FBN-based methods is that the construction and analysis of FBNs are conducted in two successive steps. Such a two-step approach may not be optimal considering the lack of communication between the two steps. Specifically, the FBNs constructed in the first step do not necessarily work well with the feature extraction methods in the second step since there are no interactions and there is no aligned objective towards solving the target task.It has been demonstrated that the construction of the common FBN at population level as prior knowledge is beneficial to reduce the effects of the first issue of large variability aforementioned [24]. For example, G. Varoquaux [24] adds a common sparse structure across all subjects as the prior knowledge to construct group consistent individual FBN. Many methods based on Group Sparse Representation (GSR) [15,27] estimate the FBN for all subjects using Group Lasso with l 1,2 -norm constraint to alleviate the variability. These results indicate that a typical whole FBN can be decomposed into a common and an individual FBN components. The common FBN component is shared by all subjects and can be used as prior knowledge regularization to increase the stability of FBN for a subject and facilitate the detection of disease-specific FCs. Inspired by this finding, we propose to learn common and individual FBNs adaptively within Transformer framework to increase the model's stability and efficacy. Unlike the traditional methods, both the common and individual FBNs in the proposed method are built by specially designed modules, whose parameters are jointly optimized within the FBN analysis modules of the network in an end-to-end manner, and this could improve the model's flexibility and discriminability.Another equally important issue with the existing methods is that the current measures of FC strength only take the synchronous functional interaction into account and ignore the potential asynchronous interactions, as illustrated in Fig. 1(a). Specifically, the current FC measures, e.g., correlation or SR, implicitly assume that the activities of different brain regions start exactly at the same time, as shown in Fig. 1(b). However, this is not necessarily always the actual case in brain functioning, and certain brain regions may not be synchronized. The time-lagged effects could originate from two-fold: on the one hand, the information flow within the brain takes time, which would cause asynchronous rs-fMRI observations [7,19]; on the other hand, even when various brain regions get activated at the same time, their individual activities may have different patterns and durations, which will also lead to asynchronous time-lagged interactions [4,5]. In this case, only measuring the FCs synchronicity cannot fully capture the underlying functional interactions between brain regions, and this would degrade the model performance. To reflect the asynchronous FCs, we propose to divide the whole BOLD signal into short-term segments with the sliding windows [20]. Then their asynchronous interactions are measured in a specially designed attention module in the Transformer framework, as shown in Fig. 1(c).To validate the effectiveness of the proposed method, we conducted experimental studies on rs-fMRI data sets of ADNI2 and ADNI3 for early AD diagnosis, i.e., classification of mild cognitive impairment (MCI) and normal control (NC). The experimental results show that the proposed method consistently achieves state-of-the-art performance."
Learning Asynchronous Common and Individual Functional Brain Network for AD Diagnosis,2,Proposed Method,"The overall network of our method is shown in Fig. 2(a). Firstly, the raw rs-fMRI BOLD signalby applying 1D temporal convolution for input mapping, where N denotes the number of brain nodes, T denotes frames, and D denotes the feature dimension. Then, before feeding into the L-th Transformer encoder, X 0 needs to be position encoded for incorporating node index as spatial identity. The position encoding matrix P is defined as: P (pos, :, 2dim) = sin(pos/10000 2dim/D ); P (pos, :, 2dim + 1) = cos(pos/10000 2dim/D ), where pos ∈ [0, • • •, N -1] is the node index, the symbol ':' refers to all indexes in the corresponding dimension, and dimindicates the index of the feature dimension. P is added to X 0 , i.e., X 0 = X 0 + P . If we measure the interactions between brain regions directly, we can obtain synchronous FCs. In order to consider the timelagged interaction between brain regions, we use a sliding window to segment the signals into short-time segments, the details are illustrated in Fig. 2(b) and introduced in detail later. Then, an improved version of the multi-head self-attention module is proposed to measure the dependencies between brain regions to obtain FBN. Conceptually, each encoder module can be formulated as a transformation function as:where f l (•) denotes the transformation function of the l-th encoder layer with input from X l-1 , which is the output of the (l-1)-th layer, and a l (•) denotes the attention module to construct FBN. The details of a l (•) are illustrated in Fig. 2(c) and introduced later. The final feature X L of the L-th layer is fed into the fully connected layer for classification."
Learning Asynchronous Common and Individual Functional Brain Network for AD Diagnosis,2.1,Attention-Based Sparse Common-and-Individual FBN Construction Module (ASCFCM),"In order to respect the sparsity property of the human brain and introduce population-level constraint, we redesign the kernel self-attention module in [30] and propose an attention-based sparse common-and-individual FBN construction module (ASCFCM), which will be introduced in detail below.Revisiting Kernel Self-attention Mechanism: Here, we first review the kernel attention module proposed in [30] and then introduce how it is redesigned to fit the target task. In order to measure the FCs between N nodes, the l-th layer takes X l-1 ∈ R N ×T ×D l-1 as input data and generates queries Q, keys K, and values V through the kernel embedding ϕ(•) and linear projection matrix, where D v is set as D l-1 /H, and H is the number of heads. The resulting attention matrix A = QK T is N × N , and A i,j is actually a Gaussian RBF kernel function [30] with the input features of the i-th and j-th brain regions from X l-1 , denoted as x i and x j , i.e., A i,j = ϕ(x i ), ϕ(x j ) = exp(-β x ix j 2 ). The kernel parameter β can be adaptively learned during the model training process. One advantage of this method over the conventional methods is that it significantly reduces the number of parameters in calculating Q and K. Another advantage is that it can effectively model the non-linearity between the features of brain nodes and can better model the complex interactions between brain regions. With the obtained attention matrix A, the feature mapping function of the l-th layer a l (•) can be defined as:where softmax( A √ T Dv ) is a N × N matrix and it can be viewed as an adaptively built FBN, presenting the dependencies between N brain regions.Sparsity: From Eq. ( 2), the original attention scores are transformed by softmax(•). Due to the non-negative nature, i.e., softmax(z i ) = exp(zi) j exp(zj ) , all the FCs between brain regions are positive, leading to a dense FBN. However, the brain is a sparse network [2], which is characterized by a limited number of FCs for most brain regions. In this case, the resulting FBN built with softmax(•) does not respect the nature of the human brain, and this may degrade the effectiveness of detecting disease-specific patterns. To address this issue, we propose to build a sparse FBN by using sparsemax(•) [17]. Let Δ K := {p ∈ R K |1 T p = 1, p ≥ 0} be the K-dimensional probability distribution simplex. The key idea of sparse transformation is mapping an input z into the nearest simplex from Δ K by sparsemax(z) := argminThe optimization could produce a sparse distribution with proper regularization, as in [17], leading to a sparse FBN."
Learning Asynchronous Common and Individual Functional Brain Network for AD Diagnosis,,Population-Level Common Pattern:,"As mentioned previously, the above FBN has large variability since it is constructed with the rs-fMRI data of each subject. Previous research indicates that human brains have similar topologies, which can be modeled as prior knowledge separately to regularize the model, helping to alleviate the issue of large variability. Therefore, we parameterize a data-driven common FBN module as a regularization, as shown in Fig. 2(c). The resulting common FBN, denoted as C, is shared by all the subjects and integrated with FBN at the individual level for joint diagnosis. The final a l (•) in Eq. 1 is defined as:"
Learning Asynchronous Common and Individual Functional Brain Network for AD Diagnosis,2.2,Cross Spatiotemporal Asynchronous FCs,"To simulate the time-lagged asynchronous FCs, we segment the BOLD signalsl by using a sliding window, as shown in Fig. 2(b). The length and number of segments, denoted as T 1 and n, respectively, are determined by the length and steps of the sliding window. Measuring the FCs between these new segments breaks the limitations of synchronous FCs, yielding cross-spatiotemporal interaction patterns, as shown in Fig. 2(d). These segments will be concatenated to restore the original number of nodes, i.e., nN → N , to avoid exponential expansion of the network complexity (see Fig. 2(e)). By doing this, both the synchronous FCs in the traditional methods and the asynchronous ones targeted in this paper can be simultaneously modeled, providing more clues for disease diagnosis."
Learning Asynchronous Common and Individual Functional Brain Network for AD Diagnosis,3.1,Data and Preprocessing,"Both rs-fMRI data sets of ADNI2 and ADNI3 from the Alzheimer's Disease Neuroimaging Initiative (ADNI) (https://adni.loni.usc.edu/) are used to evaluate the effectiveness of the proposed method with the most challenging task of differentiating MCI from NC. The subject number is 410 (147 MCI vs. 263 NC) for ADNI2 and 425 (168 MCI vs. 257 NC) for ADNI3. The ADNI2 data are acquired on a 3 Tesla (Philips) scanner with TR/TE set at 3000/30 ms and a flip angle of 80 • . The ADNI3 data are acquired with a flip angle of 90 • . The preprocessing is carried out using DPARSFA toolbox (http://rfmri.org/DPARSF) and SPM-12 (ehttps://www.fil.ion.ucl.ac.uk/spm/software/). We perform a standard approach to processing rs-fMRI by following [6], including discarding the first 10 volumes of each time series, slice timing, head motion correction, and Montreal Neurological Institute spatial normalization. Empirically, the AAL [23] atlas is used to extract all voxels of time series values in the 90 regions of interest (ROI)."
Learning Asynchronous Common and Individual Functional Brain Network for AD Diagnosis,3.2,Experimental Settings,"We conduct a 5-fold cross validation in our evaluation to obtain stable results, and all the methods involved in the comparison use the same partitions of data for fairness. The performance of the model is measured by four metrics: Area Under the receiver operating characteristic Curve (AUC), accuracy (ACC), sensitivity (SEN), and specificity (SPE). The encoder layers of the proposed network L are set to 5. Asynchronous FCs are explored in the first two layers. The sliding window of each layer has a length of 32 and a step size of 16 while the number of heads H in each layer is set as 2. The model input X raw ∈ R 90×128×1 is transformed into X 0 ∈ R 90×128×6 by a mapping with a kernel size of 3 × 1.The output feature dimension D l of each layer is set as 12. The drop rate of the attention score is 0.2, and the batch size is 16. We train 250 epochs with stochastic gradient descent (SGD), whose learning rate is 0.1 and momentum is 0.9. The code is publicly available at https://github.com/seuzjj/ACIFBN."
Learning Asynchronous Common and Individual Functional Brain Network for AD Diagnosis,3.3,Experimental Results,"The average performance of the proposed method and the competing methods is summarized in Table 1, and their ROC curves are shown in Fig. 3(a). As seen, our proposed method achieves the highest accuracy compared with other classical methods, i.e., 83.2% on ADNI2 and 78.1% on ADNI3, obtaining an improvement of 4.7% and 5.7%, respectively, over the current state-of-the art methods. Our result increases by 14.9% on ADNI2 over the base model-Kernel Transformer. These results are encouraging, indicating the significant efficacy and stability of our method.   "
Learning Asynchronous Common and Individual Functional Brain Network for AD Diagnosis,3.4,Ablation Study,"In order to further verify the effectiveness of the proposed method and the contribution of each component, we conduct ablation studies and the results are shown in Table 2. As seen, introducing common FBN improves the accuracy significantly from 68.3% to 79.2%, and this is probably attributed to the efficacy of it as prior knowledge regularizer. On top of it, considering the sparsity property of FBN further increases the accuracy to 82.3%. Modeling asynchronous FCs, denoted as 'asynchrony', further boost the model's accuracy, reaching 83.2%.To verify the effects of the settings during dividing rs-fMRI signals into segments, we set the window lengths as different values in the two asynchronous FCs modeling layers, seen in Fig. 3(b). The step of windows is half of the length. It can be consistently observed on ADNI2 and ADNI3 that the model performance first improves with the decreasing segment lengths, and then start to decrease. This implies that modeling asynchronous FCs in a wide range is beneficial, however, too short segments would harm the performance due to increased model complexity or weak of temporal information within short segments."
Learning Asynchronous Common and Individual Functional Brain Network for AD Diagnosis,4,Visualization and Conclusion,"The learned individual FBNs carry diagnostic clues, which are visualized in Fig. 4 for MCI and NC comparison. As seen, increased connectivities could be identified in the superior frontal gyrus (SFGmed) of the default mode network (DMN) and central executive network (CEN), the middle frontal gyrus (MFG) of the frontal parietal network (FPN), and the part inferior frontal gyrus (ORBinf) of CEN, while decreased connectivities could be found in the anterior cingulate and paracingulate gyri (ACG) of the DMN. These results are consistent with the literature [1,18]. The effects of modeling asynchronous FCs are shown in Fig. 3(c). As seen, the dominant FCs are mainly synchronous from the diagonal blocks (inside the black block), while certain asynchronous FCs (inside the red block) are also well captured. This is reasonable since most brain regions interact synchronously, while a portion of them would have asynchronous time-lagged functional responses.In sum, this paper proposes to adaptively construct and analyze asynchronous common and individual functional brain networks within a specially designed Transformer-based network, which is more flexible, discriminative and capable of capturing both synchronous and asynchronous FCs. The experimental results on two ADNI data sets well demonstrate the effectiveness of the proposed method."
Predicting Diverse Functional Connectivity from Structural Connectivity Based on Multi-contexts Discriminator GAN,1,Introduction,"Recently, the study of exploring structural-functional relationship raises lots of attentions in neuroscience which helps to reveal individual behaviors of human brain [1]. Typically, Structural Connectivity (SC) [16] represents the fibers connection between the brain regions while Functional Connectivity (FC) [4] represents the Blood-Oxygen-Level-Dependent (BOLD) signal correlations between the brain regions. In comparing, SC is relatively static since it demonstrates the anatomical structure of brain, and FC is relatively dynamic and diverse since it demonstrates the development of the brain activities [2]. To explore the relationship and mapping between them, some works predict SC from FC [18,19] while some works map the structural to functional [21] and both of these works achieve unique prediction. It is accountable for predicting SC from FC since SC is relatively static. However, since the FC is relatively dynamic, the predicting FC from SC is quite challenging and it should be diverse predictions rather than deterministic prediction. In general, it is necessary and challenging to explore the one-to-many relationship between the one subjects's SC and the FC by predicting diverse FC from SC [2]. The significance of this work is that once we have anatomical structure of brain represented by SC, we can predict diverse developments of brain activities represented by FC, which helps to reveal individual brain's static-dynamic structural-functional mode.In this work, we propose the diverse generations with the idea of conditional GAN [12], i.e. the input of the generator is random noise to realize diverse generations and is in condition of SC to generate FC. To improve the quality of generation, some works introduce the reconstruct supervision in generator while some works provide more supervisions in discriminator. In this work, we propose MCGAN, a multi-contexts discriminator based generative adversarial network to take both advantages. The comparison is shown in Fig. 1. Specifically, the multi-contexts discriminator provides three kinds of supervisions, i.e. edge-level, node-level and graph-level, to strengthen the generator. We adopt edge-based graph convolution method [11] to model the FC encoding. In addition, we utilize monte-carlo mean samples to enlarge the FC data for supervision. We validate our MCGAN on the HCP dataset [15] and ADNI dataset [10] and the results show that our method can generate diverse and meaningful FC. To the best of our knowledge, our method is the first to explore one-to-many SC-FC relationship."
Predicting Diverse Functional Connectivity from Structural Connectivity Based on Multi-contexts Discriminator GAN,2.1,Model Overview,"The framework of our proposed MCGAN is shown in Fig. 2 and detailed below. The objective of this work is to predict diverse functional connectivity (FC) with the instruction of structural connectivity (SC). The proposed MCGAN is based on the generative adversarial network (GAN) framework [3], which consists of two parts. (a) Generator. For FC generation of subject i, the input of the generator is random noise which introduces the diverse generations and is in condition of SC of subject i. We utilize monte-carlo mean FC samples of subject i for supervision. (b) Multi-contexts Discriminator. The discriminator distinguishes the generated FC and real one. We introduce multi contexts supervision to discriminator, i.e. edge-level, node-level and graph-level, to strengthen the generator. We adopt edge-based graph convolution method [11] to model the FC encoding. In the prediction stage, for a specific subject, the generator predicts diverse FC with corresponding numbers of random noise input and in condition of the same SC of subject."
Predicting Diverse Functional Connectivity from Structural Connectivity Based on Multi-contexts Discriminator GAN,2.2,MCGAN,"The adversarial objective of generating FC from SC with GAN model is defined as:( where z ∈ R n×n denotes the noise sample from N (0, I) to introduce the diversity of the generated FC. x ∈ R n×n denotes the SC is the condition which instructs the generation of FC [12] and y ∈ R n×n denotes the real FC."
Predicting Diverse Functional Connectivity from Structural Connectivity Based on Multi-contexts Discriminator GAN,,Multi-contexts Discriminator,"The original discriminator learns the global representation of synthetic or real data to distinguish them. However, it is insufficient to provide powerful feedback to encourage the generator to predict realistic data. Many works improve the discriminator and show that the stronger the discriminator is, the better the generator is [13,14]. Motivated by this insight, we propose a multi-contexts discriminator, which provides edge-level, node-level and graph-level supervisions to implicitly and explicitly strengthen the generator. The multi-contexts discriminator shown in Fig. 3 consists of two parts, context encoding and multi contexts discrimination.Context Encoding. The context encoding is for feature extraction. For brain network, regions are represented as nodes and the links between regions are represented as edges. Since FC is defined as the correlation between the active brain regions, it represents the edge feature. However, most of the graph convolution networks (GCN) model the node feature extraction. To address the FC encoding, we adopt the edge-based graph convolution method for feature extraction [11].There are three basic modules, i.e. E2N module (edge to node), N2E module (node to edge), N2G module (node to graph). E2N module. Given a FC E ∈ R n×n , where E i,j denotes the edge between region i and region j, it aggregates the linking edges of region i into a node representation:where w i is trainable weight and N i is the feature of i t h node.N2E module. It propagates the feature of node i and node j to their linking edge:The network stacks the graph convolution layers for FC encoding. A graph convolution layer is comprised of a E2N module and a N2E module for FC feature learning and reserve the structure of FC. Meanwhile, the skip connection [5] is applied in graph convolution layer. N2G module. It integrates all the nodes feature into a graph representation:The output layer of the network is comprised of a E2N module and a N2G module to encode the FC representation into the node representation and the graph representation in series.Multi Contexts Discrimination. The original discriminator D(•) classifies the input data to be real or fake. In contrast, our proposed multi-contexts discriminator provides three kind of supervisions, i.e. edge-level, node-level and graph-level, to implicitly and explicitly strengthen the generator. Mathematically, our discriminator loss is comprised of three parts:In the output layer, we first use three MLP to transform the three kinds of context representation. Then the D edge (•), D node (•), D graph (•) respectively classify the transformed edge representation, node representation and graph representation to be real or fake. Correspondingly, the objective of the generator is:The multi contexts supervision improves the discriminator, which implicitly strengthens the generator in adversarial training process. Meanwhile, it feedbacks the fine-grained information to the generator by backpropagation, which explicitly strengthens the generator and encourages it to predict realistic FC."
Predicting Diverse Functional Connectivity from Structural Connectivity Based on Multi-contexts Discriminator GAN,,Monte-Carlo Mean Samples of FC,"FC represents the development of brain activities so that it is relatively dynamic. While SC represents the fibers connection to indicate the anatomical structure of brain [2], which is relatively static. Since the limited of FC data and the supervision of the generated FC should not be limited in finite subspace, we augment the FC data via monte-carlo mean:where y i denotes multiple real FC of each subject and m can be fixed or dynamic during training, which denotes the monte-carlo sampling, m = 1, 2, ...M , M is the maximum samples for each subject of dataset. The linear combination of FC enlarges the data space for supervision, making the generator predict diverse FC."
Predicting Diverse Functional Connectivity from Structural Connectivity Based on Multi-contexts Discriminator GAN,,Reconstruction Loss of Generator,We adopt reconstruction loss in generator to make it predict realistic FC which includes the mean absolute error (MAE) and Pearson's correlation coefficient (PCC) between the generated FC and the training FC sample.
Predicting Diverse Functional Connectivity from Structural Connectivity Based on Multi-contexts Discriminator GAN,,Full Objective,"In summary, the objective function of the MCGAN is defined as:where L adv optimizes the adversarial training of the generator and discriminator, L rec optimizes the generator for realistic prediction."
Predicting Diverse Functional Connectivity from Structural Connectivity Based on Multi-contexts Discriminator GAN,3.1,Setup,"Datasets. We evaluate our MCGAN on the Human Connectome Project (HCP) dataset [15] and Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset [10]. We apply the standard preprocessing procedures [20] for both datasets. We use diffusion magnetic resonance imaging (diffusion MRI) as the SC and resting state functional magnetic resonance imaging (rs-fMRI) as FC. We divide the BOLD signal into 20 slices for HCP and 10 slices for ADNI for each subject and obtain FC by calculating Pearson's correlation coefficient (PCC) between the segmented signal series. Both of the SC and FC are normalized.Evaluation Metrics. (a) Quality. Instinctively, for each subject, the generated FC should be similar to one of the real FC. Therefore, for each generated FC of a subject, we calculate the minimum mean absolute error and the maximum Pearson's correlation coefficient to evaluate the quality of generation with the most similar real FC. (b) Diversity. We use Frechet Inception distance (FID) [6] to evaluate the diversity of the FC generation. FID compares the distribution of the generated and real data, to simultaneously evaluate the quality and diversity.The lower FID score indicates the better quality and diversity.Implementation Details. Both of the generator and discriminator are in condition of SC and we set the almost equivalent parameters for them to ensure the efficient adversarial training. We respectively set the learning rate of 0.0001 and 0.0004 for generator and discriminator. At prediction stage, we generate corresponding number of FC samples with dataset for each subject (i.e. 20 for HCP and 10 for ADNI)."
Predicting Diverse Functional Connectivity from Structural Connectivity Based on Multi-contexts Discriminator GAN,3.2,Results,"Main Results. We compare our MCGAN with the original GAN [3], Pixel-GAN which is with reconstruct supervision in generator [9], UNet-GAN which is with more supervisions in discriminator [14]. In addition, many works perform the deterministic prediction on structural or functional prediction [8,17,18]. We compare our MCGAN with MGCN-GAN which is a deterministic prediction to map FC to SC [18]. The results are shown in Table 1. PixelGAN achieves good reconstruction but not well in diversity, which is demonstrated in original paper. UNet-GAN achieves ordinary diversity but fails to reconstruct meaningful FC. MGCN-GAN achieves the best reconstruction since it is a deterministic prediction method but lacks of diversity. In comparing, our MCGAN achieves quality-diversity trade-off.Ablation Study. We conduct ablation study to investigate the contribution of different components and the results are shown in Table 2. The multi contexts discrimination improves the quality and diversity, indicating that it strengthens the generator for better prediction. The reconstruction provides the supervision to better quality and the monte-carlo mean samples provide the better diversity.Visualization. We generate multiple FC for each subject and we visualize two FC of each subject. The visualization of the generated and real FC is shown in Fig. 4. Each column denotes the generated FC and the corresponding most similar real FC. It shows that our method achieves diverse predictions rather than only one deterministic prediction.  "
Predicting Diverse Functional Connectivity from Structural Connectivity Based on Multi-contexts Discriminator GAN,4,Discussion,"Despite this work predicts diverse FC to explore one-to-many SC-FC relationship, the generated FC is not quite diverse enough to some extent. In our implementation, we attempt to use the diffusion model [7] to generate FC and it achieves better effect in diversity than this work. By visualization, we find that it generates diverse FC. However, since the diffusion model is trained with the only objective of noise prediction for its particularity and without the reconstruction task, it does not perform well in the evaluation metrics of MAE and PCC, indicating that it can not generate meaningful FC similar to this work. In general, the prediction of FC from SC should be diverse since the SC is relatively static and the FC is relatively dynamic and diverse. To the best of our knowledge, our method is the first to explore one-to-many SC-FC relationship and this task is challenging and significant. We lead to the future work for further exploration and there are many questions to be resolved. Moreover, to reveal individual brain's static-dynamic structural-functional mode, predicting original functional signals is more flexible than predicting FC since it directly explores the development of the brain activities."
Predicting Diverse Functional Connectivity from Structural Connectivity Based on Multi-contexts Discriminator GAN,5,Conclusion,"In this work, for diverse generations, we propose a multi-contexts discriminator based GAN named MCGAN, which provides three kind of supervisions to strengthen the generator, including edge-level, node-level and graph-level. We adopt edge-based graph convolution method for FC encoding and we utilize monte-carlo mean samples to enlarge the FC data for supervision. The experiments show that our method can generate diverse and meaningful FC from SC. We are the first to explore the one-to-many relationship between one subject's individual SC and multiple FC, which helps to reveal individual brain's staticdynamic structural-functional mode. We lead to the future work for further exploration and there are many questions to be resolved."
CoactSeg: Learning from Heterogeneous Data for New Multiple Sclerosis Lesion Segmentation,1,Introduction,"Multiple sclerosis (MS) is a common inflammatory disease in the central nervous system (CNS), affecting millions of people worldwide [7] and even leading to the disability of young population [19]. During the clinical treatment of MS, lesion changes, especially the emergence of new lesions, are crucial criteria for estimating the effects of given anti-inflammatory disease-modifying drugs [2]. However, MS lesions are usually small, numerous, and appear similar to Gliosis or other types of brain lesions, e.g., ischemic vasculopathy [8]. Identifying MS lesion changes from multi-time-point data is still a heavy burden for clinicians. Therefore, automatically quantifying MS lesion changes is essential in constructing a computer-aided diagnosis (CAD) system for clinical applications.Deep learning has been widely used for MS lesion segmentation from brain MRI sequences [20,25]. For example, the icobrain 5.1 framework [16] combined supervised and unsupervised approaches and designed manual rules to fuse the final segmentation results. Some works [10,28] further studied the complementary features from other MRI modalities for MS lesion segmentation. Meanwhile, to train a better deep model, class-imbalance issues [26] and prior brain structures [27] have been respectively investigated to improve the performance. With the impressive performance achieved by existing pure MS lesion segmentation methods [11], recent attention has been shifted to analyze the longitudinal MS changes [5,6], such as stable, new, shrinking, and enlarging lesions, with the focus on new MS lesion segmentation [9]. However, collecting adequate well-labeled longitudinal MS lesion data for model learning is highly challenging since it needs multi-time-point data from the same set of patients, and requires costly and time-consuming expert annotations. Figure 1 shows the three types of heterogeneous MS lesion data: newlesion annotated two-time-point data, all-lesion annotated two-time-point data, and all-lesion annotated single-time-point data, each of which is associated with different costs. New-lesion annotated two-time-point data is the ideal one for learning new lesion segmentation, but with the highest data acquisition and annotation costs. Annotating all lesions in two-time-point data can reduce the annotation cost, but it requires accurate brain registration and rule-based postprocessing to identify lesion changes, which cannot avoid noise accumulation and often leads to sub-optimal performance. All-lesion annotated single-time-point data is with the cheapest data acquisition and annotation costs. This motivates us to raise the question: ""Can we leverage all-lesion annotated single-time-point data to promote the new MS lesion segmentation?"" Therefore, in this paper, we proposed a deep Coaction Segmentation (Coact-Seg) model that can unify heterogeneous data and annotations for the new MS lesion segmentation task. Specifically, CoactSeg takes three-channel inputs, including the baseline, follow-up, and corresponding differential brains, and produces all-lesion and new-lesion segmentation results at the same time. Moreover, a longitudinal relation constraint (e.g., new lesions should only appear at the follow-up scans) is proposed to regularize the model learning in order to integrate the two tasks (new and all lesion segmentation) and boost each other. Extensive experiments on two MS datasets demonstrate that our proposed CoactSeg model is able to achieve superior performance for both new and all MS lesion segmentation, e.g., obtaining 63.82% Dice on the public MICCAI-21 dataset [4] and 72.32% Dice on our in-house MS-23v1 dataset, respectively. It even outperforms two neuro-radiologists on MICCAI-21.Overall, the contributions of this work are three-fold:-We propose a simple unified model CoactSeg that can be trained on both newlesion annotated two-time-point data and all-lesion annotated single-timepoint data in the same way, with the same input and output format; -We design a relation regularizer to ensure the longitudinal relations among all and new lesion predictions of the baseline, follow-up, and corresponding differential brains; -We construct an in-house MS-23v1 dataset, which includes 38 Oceania singletime-point 3D FLAIR scans with manual all-lesion annotations by experienced human experts. We will release this dataset publicly. "
CoactSeg: Learning from Heterogeneous Data for New Multiple Sclerosis Lesion Segmentation,2,Datasets,"We trained and evaluated our CoactSeg model on two MS segmentation datasets, as shown in Table 1. On the public MICCAI-21 dataset 1 , we only use its training set since it does not provide official labels of testing samples. Specifically, 40 two-time-point 3D FLAIR scans are captured by 15 MRI scanners at different locations. Among them, 11 scans do not contain any new MS lesions. The follow-up data were obtained around 1-3 years after the first examination. Four neuro-radiologists from different centers manually annotated new MS lesions, and a majority voting strategy was used to obtain the final ground truth. For pre-processing, the organizers only performed a rigid brain registration, and we further normalized all MRI scans to a fixed resolution of [0.5, 0.75, 0.75] mm. Since the public MS lesion data is not adequate [1,3,4], we further collected 38 single-time-point 3D FLAIR sequences as a new MS dataset (MS-23v1). Specifically, all samples were anonymized and captured by a 3T Siemens scanner in Alfred Health, Australia. To the best of our knowledge, this will be the first open-source dataset from Oceania for MS lesion segmentation, contributing to the diversity of existing public MS data. Two neuro-radiologists and one senior neuro-scientist segmented all MS lesions individually and in consensus using the MRIcron segmentation tool 2 . The voxel spacing of all samples is then normalized to an isotropic resolution of [0.8, 0.8, 0.8] mm.Finally, when conducting the mixed training, we used a fixed data split in this paper (i.e., 62 samples for training and 16 for validation in total). Note that we followed the setting of the public challenge [4], which selects the new validation set from MICCAI-21 that does not include samples without any new MS lesions.   all-lesion and new-lesion predictions as"
CoactSeg: Learning from Heterogeneous Data for New Multiple Sclerosis Lesion Segmentation,3.2,Multi-head Architecture,"For single-time-point samples x s ∈ X s , x s b and x s fu are identical as x s , and the difference map becomes an all-zero matrix x 0 d , with p s1 al , p s2 al and p s nl being the corresponding all-lesion and new-lesion predictions of x s . For two-time-point data x t ∈ X t , x t b and x t fu respectively denote the first and second time-point data samples, with p t1 al , p t2 al and p t nl being the all-lesion segmentation results at the first and second time-point and the new-lesion results of x t , respectively.In this way, we unify the learning of both single and two-time-point data with heterogeneous annotations by using the same model F θ , with the same input and output formats. Note that, inspired by semi-supervised learning [22][23][24], we mix x s and x t samples into each batch for training. Given the heterogeneous annotations, i.e., all-lesion labels for single-time-point data and new-lesion labels for two-time-point data, we apply the following corresponding supervisions:where Dice refers to the common Dice loss for medical segmentation tasks. We use a 3D VNet [15] as the backbone of F θ and three prediction heads are designed as individual convolutional blocks. Note that, the last prediction head also receives the features from the first two in order to capture the all-lesion information. Compared to the recent work [30] for exploiting heterogeneous data, our architecture avoids the complicated design of dynamic prediction heads."
CoactSeg: Learning from Heterogeneous Data for New Multiple Sclerosis Lesion Segmentation,3.3,Longitudinal Relation Regularization,"Human experts usually identify new MS lesions by comparing the brain MRI scans at different time points. Inspired by this, we further propose a longitudinal relation constraint to compare samples from different time points:where ⊗ is a masking operation. The first term in ( 3) is to encourage the alllesion predictions p s1 al and p s2 al to be the same since there is no brain difference for single-time-point data. The second and third terms in (3) are to ensure that the new-lesion region can be correctly segmented as the foreground in p t2 al and as the background in p t1 al in two-time-point data with only new lesion labels y t nl . Finally, the overall loss function to train our CoactSeg model becomes a weighted sum of L al , L nl , and the regularization L rr :where λ 1 and λ 2 are constants to balance different tasks."
CoactSeg: Learning from Heterogeneous Data for New Multiple Sclerosis Lesion Segmentation,4,Results,"Implementation Details. For training, we normalize all inputs as zero mean and unit variance. Then, among common augmentation operations, we use the random flip or rotation to perturb inputs. Since MS lesions are always small, we apply a weighted cropping strategy to extract 3D patches of size 80 × 80 × 80 to relieve the class imbalance problem [26]. Specifically, if the input sample contains the foreground, we randomly select one of the foreground voxels as the patch center and shift the patch via a maximum margin of [-10, 10] voxels. Otherwise, we randomly crop 3D patches. The batch size is set as eight (i.e., four new-lesion two-time-point samples and four all-lesion single-time-point samples). We apply Adam optimizer with a learning rate of 1e-2. The overall training iterations are 20k. In the first 10k iterations, λ 1 and λ 2 are set to 1 and 0, respectively, in order to train the model for segmenting MS lesions at the early training stage. After that, we set λ 2 as 1 to apply the relation regularization. During testing, we extract the overlapped patches by a stride of 20 × 20 × 20 and then re-compose them into the entire results. Note that we follow [18] to mask the non-brain regions and all experiments are only conducted in the brain regions with the same environment (Hardware: Single NVIDIA Tesla V100 GPU; Software: PyTorch 1.8.0, Python 3.8.10; Random Seed: 1337). The computational complexity of our model is 42.34 GMACs, and the number of parameters is 9.48 M. dataset) are used to evaluate the proposed CoactSeg. Besides common segmentation metrics [13] including Dice, Jaccard, 95% Hausdorff Distance (95HD), and Average Surface Distance (ASD), we further follow [3] to use the instance-level F1 score (F1) to denote the lesion-wise segmentation performance. Here, tiny lesions (i.e., fewer than 11 voxels) are not included in the F1 calculation as [3].   [14], our model can even predict new lesions with low contrast (indicated by the enlarged yellow rectangles in Fig. 3). Table 2 gives the quantitative results on MICCAI-21. We can see that: 1) Our model achieves good segmentation performance for new MS lesion segmentation and outperforms the second-best method [14] by 7.01% in Dice; 2) Compared with human experts, our proposed model also outperforms two of them (i.e., #3 and #4) in terms of the segmentation and the shape-related metrics; 3) For the lesion-wise F1 score, our method significantly reduces the performance gap between deep models and human experts, achieving a comparable F1 with expert #3 (i.e., 61.96% vs. 62.88%)."
CoactSeg: Learning from Heterogeneous Data for New Multiple Sclerosis Lesion Segmentation,,"Performance for MS Lesion Segmentation. Two MS tasks (i.e., newlesion segmentation on MICCAI-21 and all-lesion segmentation on our MS-23v1","Figure 4 shows the all-lesion segmentation results of our CoactSeg model on our in-house MS-23v1 dataset. It can be seen that CoactSeg is able to segment most MS lesions, even for very tiny ones (highlighted by red arrows). Moreover, we can see that the segmentation results of the first two prediction heads are relatively consistent (i.e., the 2nd and 3rd columns of Fig. 4), demonstrating the effectiveness of our proposed relation regularization. "
CoactSeg: Learning from Heterogeneous Data for New Multiple Sclerosis Lesion Segmentation,5,Conclusion,"In this paper, we have presented a unified model CoactSeg for new MS lesion segmentation, which can predict new MS lesions according to the two-time-point inputs and their differences while at the same time segmenting all MS lesions. Our model effectively exploits heterogeneous data for training via a multi-head architecture and a relation regularization. Experimental results demonstrated that introducing all-lesion single-time-point data can significantly improve the new-lesion segmentation performance. Moreover, the relation constraint also facilitates the model to capture the longitudinal MS changes, leading to a further performance gain. Our in-house MS-23v1 dataset will be made public to help the MS lesion research. Future works will explore more longitudinal relations to study the fine-grained MS changes as well as consider more powerful constraints to address the domain gap [21] and fairness [29] problems. Moreover, we plan to collect and annotate more MS lesion data to improve the possibility of training large-scale deep models for clinical applications [17]."
Diffusion-Based Data Augmentation for Nuclei Image Segmentation,1,Introduction,"Nuclei segmentation is a fundamental step in medical image analysis. Accurately segmenting nuclei helps analyze histopathology images to facilitate clinical diagnosis and prognosis. In recent years, many deep learning based nuclei segmentation methods have been proposed [5,18,19,23]. Most of these methods are fully-supervised so the great segmentation performance usually relies on a large number of labeled images. However, manually labeling the pixels belonging to all nucleus boundaries in an image is time-consuming and requires domain knowledge. In practice, it is hard to obtain an amount of histopathology images with dense pixel-wise annotations but feasible to collect a few labeled images. A question is raised naturally: can we expand the training dataset with a small proportion of images labeled to reach or even exceed the segmentation performance of the fully-supervised baseline? Intuitively, since the labeled images are samples from the population of histopathology images, if the underlying distribution of histopathology images is learned, one can generate infinite images and their pixel-level labels to augment the original dataset. Therefore, it is demanded to develop a tool that is capable of learning distributions and generating new paired samples for segmentation.Generative adversarial network (GANs) [2,4,12,16,20] have been widely used in data augmentation [11,22,27,31]. Specially, a newly proposed GAN-based method can synthesize labeled histopathology image for nuclei segmentation [21]. While GANs are able to generate high quality images, they are known for unstable training and lack of diversity in generation due to the adversarial training strategy. Recently, diffusion models represented by denoising diffusion probabilistic model (DDPM) [8] tend to overshadow GANs. Due to the theoretical basis and impressive performance of diffusion models, they were soon applied to a variety of vision tasks, such as inpainting, superresolution [30], text-to-image translation, anomaly detection and segmentation [1,9,24,26]. As likelihood-based models, diffusion models do not require adversarial training and outperform GANs on the diversity of generated images [3], which are naturally more suitable for data augmentation. In this paper, we propose a novel diffusionbased augmentation framework for nuclei segmentation. The proposed method consists of two steps: unconditional nuclei structure synthesis and conditional histopathology image synthesis. We develop an unconditional diffusion model and a nuclei-structure conditioned diffusion model (Fig. 1) for the first and second step, respectively. On the training stage, we train the unconditional diffusion model using nuclei structures calculated from instance maps and the conditional diffusion model using paired images and nuclei structures. On the testing stage, the nuclei structures and the corresponding images are generated successively by the two models. As far as our knowledge, we are the first to apply diffusion models on histopathology image augmentation for nuclei segmentation.Our contributions are: (1) a diffusion-based data augmentation framework that can generate histopathology images and their segmentation labels from scratch; (2) an unconditional nuclei structure synthesis model and a conditional histopathology image synthesis model; (3) experiments show that with our method, by augmenting only 10% labeled training data, one can obtain segmentation results comparable to the fully-supervised baseline."
Diffusion-Based Data Augmentation for Nuclei Image Segmentation,2,Method,"Our goal is to augment a dataset containing a limited number of labeled images with more samples to improve the segmentation performance. To increase the diversity of labeled images, it is preferred to synthesize both images and their corresponding instance maps. We propose a two-step strategy for generating new labeled images. Both steps are based on diffusion models. The overview of the proposed framework is shown in Fig. 2. In this section, we introduce the two steps in detail."
Diffusion-Based Data Augmentation for Nuclei Image Segmentation,2.1,Unconditional Nuclei Structure Synthesis,"In the first step, we aim to synthesize more instance maps. Since it is not viable to directly generate an instance map, we instead choose to generate its surrogate nuclei structure, which is defined as the concatenation of pixel-level semantic and distance transform. Pixel-level semantic is a binary map where 1 or 0 indicates whether a pixel belongs to a nucleus or not. The distance transform consists of the horizontal and the vertical distance transform, which are obtained by calculating the normalized distance of each pixel in a nucleus to the horizontal and the vertical line passing through the nucleus center [5]. Clearly, the nuclei structure is a 3-channel map with the same size as the image. As nuclei instances can be identified from the nuclei structure, we can easily construct the corresponding instance map by performance marker-controlled watershed algorithm on the nuclei structure [29]. Therefore, the problem of synthesizing instance map transfers to synthesizing nuclei structures. We deploy an unconditional diffusion model to learn the distribution of nuclei structures.Denote a true nuclei structure as y 0 , which is sampled from real distribution q(y). To maximize data likelihood, the diffusion model defines a forward and a reverse process. In the forward process, small amount of Gaussian noise are successively added to the sample y 0 in T steps by:where t ∼ N (0, I) and {β t ∈ (0, 1)} T t=1 is a variance schedule. The resulting sequence {y 0 , ..., y T } forms a Markov chain. The conditional probability of y t given y t-1 follows a Gaussian distribution:In the reverse process, since q(y t-1 |y t ) cannot be easily estimated, a model p θ (y t-1 |y t ) (typically a neural network) will be learned to approximate q(y t-1 |y t ). Specifically, p θ (y t-1 |y t ) is a also Gaussian distribution:The objective function is the variational lower bound loss: L = L T + L T -1 + ... + L 0 , where every term except L 0 is a KL divergence between two Gaussian distributions. In practice, a simplified version of L t is commonly used [8]:where α t = 1β t and ᾱt = t i=1 α i . Clearly, the optimization objective of the neural network parameterized by θ is to predict the Gaussian noise t from the input y t at time t.After the network is trained, one can progressively denoise a random point from N (0, I) by T steps to produce a new sample:For synthesizing nuclei structures, we train an unconditional DDPM on nuclei structures calculated from real instance maps. Following [8], the network of this unconditional DDPM has a U-Net architecture."
Diffusion-Based Data Augmentation for Nuclei Image Segmentation,2.2,Conditional Histopathology Image Synthesis,"In the second step, we synthesize histopathology images conditioned on nuclei structures. Without any constraint, an unconditional diffusion model will generate diverse samples. There are usually two ways to synthesize images constrained by certain conditions: classifier-guided diffusion [3] and classifier-free guidance [10]. Since classifier-guided diffusion requires training a separate classifier which is an extra cost, we choose classifier-free guidance to control sampling process.Let θ (x t , t) and θ (x t , t, y) be the noise predictor of unconditional diffusion model p θ (x|y) and conditional diffusion model p θ (x), respectively. The two models can be learned with one neural network. Specifically, p θ (x|y) is trained on paired data (x 0 , y 0 ) and p θ (x) can be trained by randomly discarding y (i.e. y = ∅) with a certain drop rate ∈ (0, 1) so that the model learns unconditional and conditional generation simultaneously. The noise predictor θ (x t , t, y) of classifier-free guidance is a combination of the above two predictors:where θ (x t , t) = θ (x t , t, y = ∅), w is a scalar controlling the strength of classifier-free guidance.Unlike the network of unconditional nuclei structure synthesis which inputs the noisy nuclei structure y t and outputs the prediction of t (y t , t), the network of conditional nuclei image synthesis takes the noisy nuclei image x t and the corresponding nuclei structure y as inputs and the prediction of t (x t , t, y) as output. Therefore, the conditional network should be equipped with the ability to well align the paired histopathology image and nuclei structure. Since nuclei structures and histopathology images have different feature spaces, simply concatenating or passing them through a cross-attention module [7,15,17] before entering the U-Net will degrade image fidelity and yield unclear correspondence between synthetic nuclei image and its nuclei structure. Inspired by [28], we embed information of the nuclei structure into feature maps of nuclei image by the spatially-adaptive normalization (SPADE) module [25]. In other words, the spatial and morphological information of nuclei modulates the normalized feature maps such that the nuclei are generated in the right places while the background is left to be created freely. We include the SPADE module in different levels of the network to utilize the multi-scale information of nuclei structure. The network of conditional nuclei image synthesis also applies a U-Net architecture. The encoder is a stack of Resblocks and attention blocks (AttnBlocks). Each Resblock consists of 2 GroupNorm-SiLU-Conv and each Attnblocks calculates the self-attention of the input feature map. The decoder is a stack of CondResBlocks and attention blocks. Each CondResBlock consists of SPADE-SiLU-Conv which takes both feature map and nuclei structure as inputs."
Diffusion-Based Data Augmentation for Nuclei Image Segmentation,3.1,Implementation Details,"Datasets. We conduct experiments on two datasets: MoNuSeg [13] and Kumar [14]. The MoNuSeg dataset has 44 labeled images of size 1000 × 1000, 30 for training and 14 for testing. The Kumar dataset consists of 30 1000×1000 labeled images from seven organs of The Cancer Genome Atlas (TCGA) database. The dataset is splited into 16 training images and 14 testing images. Paired Sample Synthesis. To validate the effectiveness of the proposed augmentation method, we create 4 subsets of each training dataset with 10%, 20%, 50% and 100% nuclei instance labels. Precisely, we first crop all images of each dataset into 256 × 256 patches with stride 128, then obtain the features of all patches with pretrained ResNet50 [6] and cluster the patches into 6 classes by Kmeans. Patches close to the cluster centers are selected. The encoder and decoder of the two networks have 6 layers with channels 256, 256, 512, 512, 1024 and 1024. For the unconditional nuclei structure synthesis network, each layer of the encoder and decoder has 2 ResBlocks and last 3 layers contain AttnBlocks. The network is trained using the AdamW optimizer with a learning rate of 10 -4 and a batch size of 4. For the conditional histopathology image synthesis network, each layer of the encoder and the decoder has 2 ResBlocks and 2 CondResBlocks respectively, and last 3 layers contain AttnBlocks. The network is first trained in a fully-conditional style (drop rate = 0) and then finetuned in a classifier free style (drop rate = 0.2). We use AdamW optimizer with learning rates of 10 -4 and 2 × 10 -5 for the two training stages, respectively. The batch size is set to be 1. For the diffusion process of both steps, we set the total diffusion timestep T to 1000 with a linear variance schedule {β 1 , ..., β T } following [8].For MoNuSeg dataset, we generate 512/512/512/1024 synthetic samples for 10%/20%/50%/100% labeled subsets; for Kumar dataset, 256/256/256/512 synthetic samples are generated for 10%/20%/50%/100% labeled subsets. The synthetic nuclei structures are generate by the nuclei structure synthesis network and the corresponding images are generated by the histopathology image synthesis network with the classifier-free guidance scale w = 2. Each follows the reverse diffusion process with 1000 timesteps [8]. We then obtain the augmented subsets by adding the synthetic paired images to the corresponding labeled subsets.Nuclei segmentation. The effectiveness of the proposed augmentation method can be evaluated by comparing the segmentation performance of using the four labeled subsets and using the corresponding augmented subsets to train a segmentation model. We choose to train two nuclei segmentation models -Hover-Net [5] and PFF-Net [18]. To quantify the segmentation performance, we use two metrics: Dice coefficient and Aggregated Jaccard Index (AJI) [14]. "
Diffusion-Based Data Augmentation for Nuclei Image Segmentation,3.2,Effectiveness of the Proposed Data Augmentation Method,"Fig. 3 shows the synthetic samples from the models trained on the subset with 10% labeled images. We have the following observations. First, the synthetic samples look realistic: the patterns of synthetic nuclei structures and textures of synthetic images are close to the real samples. Second, due to the conditional mechanism of the image synthesis network and the classifier-guidance sampling, the synthetic images are well aligned with the corresponding nuclei structures, which is the prerequisite to be additional segmentation training samples. Third, the synthetic nuclei structures and images show great diversity: the synthetic samples resemble different styles of the real ones but with apparent differences.We then train segmentation models on the four labeled subsets of MoNuSeg and Kumar dataset and corresponding augmented subsets with both real and synthetic labeled images. With a specific labeling proportion, say 10%, we name the original subset as 10% labeled subset and the augmented on as 10% augmented subset. Specially, 100% labeled subset is the fully-supervised baseline. Table 1 show the segmentation performances with Hover-Net. For MoNuSeg dataset, it is clear that the segmentation metrics drop with fewer labeled images. For example, with only 10% labeled images, Dice and AJI reduce by 2.4% and 3.1%, respectively. However, by augmenting the 10% labeled subset, Dice and AJI exceed the fully-supervised baseline by 0.9% and 1.3%. For the 20% and 50% case, the two metrics obtained by augmented subset are of the same level as using all labeled images. Note that the metrics of 10% augmented subset are higher than those of 20% augmented subset, which might be attributed to the indetermination of the diffusion model training and sampling. Interestingly, augmenting the full dataset also helps: Dice increases by 1.3% and AJI increases by 1.6% compared with the original full dataset. Therefore, the proposed augmentation method consistently improves segmentation performance of different labeling proportion. For Kumar dataset, by augmenting 10% labeled subset, AJI increases to a level comparable with that using 100% labeled images; by augmenting 20% and 50% labeled subset, AJIs exceed the fully-supervised baseline. These results demonstrate the effectiveness of the proposed augmentation method that we can achieve the same or higher level segmentation performance of the fully-supervised baseline by augmenting a dataset with a small amount of labeled images.Generalization of the Proposed Data Augmentation. Moreover, we have similar observations when using PFF-Net as the segmentation model. Table 2 shows the segmentation results with PFF-Net. For both MoNuSeg and Kumar datasets, all the four labeling proportions metrics notably improve with synthetic samples. This indicates the generalization of our proposed augmentation method. "
Diffusion-Based Data Augmentation for Nuclei Image Segmentation,4,Conclusion,"In this paper, we propose a novel diffusion-based data augmentation method for nuclei segmentation in histopathology images. The proposed unconditional nuclei structure synthesis model can generate nuclei structures with realistic nuclei shapes and spatial distribution. The proposed conditional histopathology image synthesis model can generate images of close resemblance to real histopathology images and high diversity. Great alignments between synthetic images and corresponding nuclei structures are ensured by the special design of the conditional diffusion model and classifier-free guidance. By augmenting datasets with a small amount of labeled images, we achieved even better segmentation results than the fully-supervised baseline on some benchmarks. Our work points out the great potential of diffusion models in paired sample synthesis for histopathology images."
Diffusion-Based Data Augmentation for Nuclei Image Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3 57.
Weakly Supervised Cerebellar Cortical Surface Parcellation with Self-Visual Representation Learning,1,Introduction,"Parcellating the cerebellum (i.e., little brain) into neurobiologically meaningful regions of interest (ROIs) plays an important role in both structural and functional analysis [1,2]. Therefore, automatic cerebellar parcellation methods are highly demanded to facilitate the analysis of increasingly larger cerebellum imaging datasets. Compared with the highly folded cerebral cortex, the cerebellar cortex has a more complex shape with even more tightly convoluted and much thinner cortical folds. Hence, few algorithms have been proposed for automatic cerebellar cortex parcellation.Most existing methods mainly conduct the cerebellar cortex parcellation in the Euclidean space [3]. Generally, they treat the cerebellar cortex as a 3D volume, and then either use the traditional multi-atlas registration and label-fusion based methods or directly learn the cerebellar cortex parcellation model in the volumetric space, without reconstructing the cerebellar cortical surfaces and thus ignoring the topological and geometric properties of the cerebellar cortex. These methods are thus inappropriate for accurate and anatomically meaningful cerebellar cortex analysis, as the neighboring relationship in 3D space used in these methods cannot correctly reflect the actual cytoarchitectural neighborship. Specifically, two spatially neighboring points in the cerebellum volume do not necessarily mean they are neighboring in cytoarchitecture and geodesic. They could sit in completely different gyri or sulci, due to the extremely convoluted structure of the cerebellar cortex. This can cause severe consequences when requesting the parcellation consistency across ""neighbors"" and confuse the message aggregation during the parcellation model learning.To address this issue, it is highly necessary to respect the cerebellar geometry, which provides macro-measurable imaging clues characterizing the cytoarchitecture of the cerebellar cortex. Therefore, in this paper, first, we reconstruct the cerebellar surfaces to restore its convoluted geometry. Specifically, we reconstruct the inner cerebellar surface (the interface between the cerebellar gray matter and white matter), and the pial/outer cerebellar cortical surface (the interface between the cerebellar gray matter and cerebrospinal fluid (CSF)). The reconstructed cerebellar surfaces are represented by triangular meshes, which can be regarded as an undirected graph. Second, we develop a graph convolutional neural network-based method to do the parcellation on the reconstructed cerebellar surfaces. Specifically, we first extract the informative surface patches from the cerebellar surfaces with the neighborship determined by their intrinsic geodesic distance. And then, we feed these surface patches into the graph convolutional neural network to train a parcellation model, which learns a highly nonlinear mapping from the geometric features of the training patches to the patch labels. One practical challenge for learning this mapping is the need for a large amount of manually labeled data, which is extremely expensive and time-consuming for vertex-level labeling. Therefore, we split the mapping learning into two steps, in the first step, we use massive patches to learn effective low-dimension representations in the latent space. Leveraging the concept of contrastive learning, we require similar representations for the patches from the same region, while distinct representations for patches from different regions. Then, in the second step, we learn the mapping from the low-dimensional latent representation to the parcellation labels. Of note, the representation learning only requires a tag to denote whether the patches are from the same region or not, which is clearly a weaker supervision compared to the accurate patch-level labels.This paper makes three contributions: 1) we propose a practical pipeline to reconstruct the cerebellar cortical surface that respects its intrinsic geometry; 2) we conduct the parcellation on the original reconstructed cerebellar cortical surfaces, without the requirement to project them onto a simplified shape, like the sphere, which will inevitably introduce distortions during the projection; 3) we leverage the self-representation learning with weak supervision information to reduce the manual labeling cost for the parcellation network training. To the best of our knowledge, this is the first method that conducts the cerebellar cortex parcellation directly using the original reconstructed cerebellar surfaces. "
Weakly Supervised Cerebellar Cortical Surface Parcellation with Self-Visual Representation Learning,2,Method,"Our cerebellar cortex parcellation method can be divided into 3 steps: 1) we reconstruct the cerebellar surfaces and compute the salient geometric features to characterize the geometry of the cerebellar surfaces; 2) we use weakly supervised information to learn effective latent representations from massive informative surface patches. These representations sit in low dimensional latent space, and they are similar for patches from the same region, while are distinct for patches from different regions; 3) we learn a straightforward mapping from the low dimensional latent space to the parcellation labels."
Weakly Supervised Cerebellar Cortical Surface Parcellation with Self-Visual Representation Learning,2.1,Cerebellar Surface Reconstruction and Geometric Feature Computation,"Given the tissue map of the cerebellum [4], we can reconstruct the cerebellar cortical surfaces, including the inner cortical surface and the pial/outer cortical surface. The pipeline is illustrated in Fig. 1. Specifically, we first do the cerebellum tissue segmentation and then correct the topological errors. Then, we reconstruct the inner cerebellar surface and deform it to the outer cerebellar surface. This framework is similar to the typical cerebral cortical surface reconstruction pipelines. We adopt this typical strategy because, a) it can well preserve the geometry of the highly folded cerebellar cortex; b) it can establish the vertex-wise correspondence between the inner and outer cerebellar surfaces, which will substantially facilitate the subsequent analysis. More details about widely used cortical surface reconstruction pipelines can be referred to, such as FreeSurfer [5], dHCP [6], FastSurfer [7], HCP [8], and iBEAT V2.0 [9].With the reconstructed cerebellar surfaces, we can then compute typical geometric features, including the average convexity, mean curvature, curvedness, shape index, and Laplacian spectral embedding [10]. These geometric features encode the local and global geometries of the cerebellar sulci and gyri, which can be used to identify the boundary of each parcellation region (typically appearing at the sulcal bottoms of the cerebellar surface). Specifically, the average convexity encodes the integrated normal movement of a vertex during inflating the inner cerebellar surface and mainly reflects the coarsescale geometrical information of cerebellar cortical folding [11]; the mean curvature is computed as the average of the minimal and maximal principal curvatures of the inner cerebellar cortical surface and encodes the fine-scale local geometric information of cerebellar cortical folding [11]; also, based on the minimum and maximum principal curvatures, we can compute the curvedness and shape index [12], which characterize the local shape information of the cerebellar cortical surface [11]; the Laplacian spectral embedding reflects the vibration mode of a graph [10], which can be used as a global feature to characterize the cerebellar cortex. Figure 1(f) visualizes these geometric features on the inner cerebellar cortical surface and the corresponding parcellation map. In addition, since the cerebellar cortical surface is not separated into left and right hemispheres, we computed the vertex-wise distance to the geometric center of the surface as additional features, named centroid relation features, which can provide useful clues for the localization of symmetric regions. "
Weakly Supervised Cerebellar Cortical Surface Parcellation with Self-Visual Representation Learning,2.2,Weakly Supervised Cerebellar Patch Representation Learning,"Provided with the computed geometric features of the cerebellar surface, we can train a neural network to conduct the parcellation. However, treating the entire cerebellar surface as a single instance would require dense manual labeling for the network training, which is time-consuming and labor-intensive. Therefore, we choose the patch-wise strategy for the parcellation network training. The advantage is that it can significantly enlarge the training samples, while this comes at the cost of requiring the model to possess strong localization ability for the local patch. Directly learning a mapping from the geometric features of a local patch to the parcellation labels is very challenging for the cerebellar surface due to the shape complexity. The local sulci of the cerebellar surface are deeper, and the gyri appear more consistent shape patterns, compared to the cerebral cortical surfaces. This indicates that more training samples are required to enable the network to possess the localization ability from the local patches for a robust parcellation model. To further reduce the cost of the expensive manual labeling, we split the parcellation network training into two steps. In the first step, we learn distinctive latent representations for patches from different regions in a weakly supervised manner. Specifically, we enforce similar representations for patches from the same region, while distinct representations for patches from different regions. This can be achieved with a contrastive learning framework [13]. Herein, we name it weakly supervised since in this step, we only require a tag to denote whether the patches are from the same region or not and we do not require the precise labels of which region it belongs to.Following the above motivation, we can formulate the weakly supervised patchbased representation learning below. We first extract the local patches from the cerebellar surface, which can be regarded as an undirected graph (V , E), with V being the vertex set and E being the edge set. For any vertex, we can extract a local patch, which contains a set of vertices v i k whose geodesic distances to v i are bounded by the predefined maximal geodesic distance ρ max , as illustrated in Fig. 2. Considering the prior knowledge that parcellation boundaries of the cerebellar surface generally appear at the sulcal fundi or gyral crests, we sample more patches from those locations, and meanwhile reduce patch samples for vertices sitting on sulcal walls. This can be achieved by thresholding the mean curvature map of the cerebellar surface, since the sulcal fundi and gyral crests typically have larger curvature magnitude, as illustrated in Fig. 2(a), where white dots denote the sample points near gyri crests, and black dots denote the sample points near sulcal fundi. Figure 2(c) shows a typical surface patch with various geometric features and parcellation labels.Once the local patches are extracted, we feed them into a graph convolution based neural network to learn their latent representation. Herein, we use the widely used residual network [14] as the basic block for constructing our latent representation learning framework. Figure 3 illustrates the learning framework. Specifically, given 3 local patches (p 1 , p 2 , p 3 ), where p 1 and p 2 are from the same region and p 3 is from a different region. After the encoding, we can denote their latent representations to be (z 1 , z 2 , z 3 ). Then, we enforce (z 1 , z 2 ) to be similar (using the cosine criterion) in the latent space while (z 1 , z 3 ) and (z 2 , z 3 ) to be distinct. This can be achieved by minimizing the normalized temperature scaled cross entropy (NT-Xent) loss, which is defined as:where τ is a hyperparameter (named temperature). It is worth noting that, a) it has been validated that this loss has superior performances in many representation learning tasks than other contrastive losses [13]; b) during the training, we can select multiple triplet patches into a single batch and use the average loss over the entire batch to make the training more robust. Since we have extracted a large number of patches from the cerebellar surface, after combining patches according to their tags, we can obtain an even larger set of training samples to ensure network convergence."
Weakly Supervised Cerebellar Cortical Surface Parcellation with Self-Visual Representation Learning,2.3,Mapping from Latent Space to Parcellation Labels,"Through representation learning, we have encoded the patches with multiple channels of geometric features into the latent space. This step not only makes the extracted patches more distinct in the latent space from the regional perspective but also significantly reduces the potential feature dimension, which can greatly facilitate the parcellation network training, compared to training the parcellation network directly from the patchwise geometric features.Therefore, given the patch-wise latent representations, we further train a multilayer perceptron (MLP) to accomplish the parcellation task. Specifically, we use the parcellation labels from the manually labeled cerebellar surface to supervise the MLP training. Herein, the patch center label is used as the patch label to train the MLP and the popular cross-entropy loss is adopted.However, since each patch is parcellated independently without considering the spatial consistency, it is possible to generate isolated parcellation labels and cause inconsistency in a geodesic neighborhood. To improve the parcellation, we further use the graph cuts method [15] to explicitly impose spatial consistency. Specifically, since most of the regions are separated at the sulci fundi of the cerebellar cortex according to the manual labeling protocol, we explicitly formulate parcellation as a cost minimization problem, i.e., E = E d + λE s . Here, E d is the data fitting term, which is defined as:, where p v (l v ) is the probability of assigning vertex v as label l v ; E s is the smoothness term, which is defined as:where vertex v * is the direct neighbor of v, and C v,v * (l v , l v * ) is the cost to label vertex v as l v and also label vertex v * as l v * . Herein, we used the formula from [16] to define C v,v * (l v , l v * ); finally, λ is a weight used to balance them. 3 Experiments"
Weakly Supervised Cerebellar Cortical Surface Parcellation with Self-Visual Representation Learning,3.1,Dataset and Implementation,"To validate our method, we manually labeled 10 subjects from BCP dataset [17] in a vertex-wise manner using an inhouse developed toolkit. Specifically, for each cerebellum MRI, we first segmented it into white matter, gray matter, and CSF using a deep convolutional neural network [4]. Then, following the processing pipeline in Sect. 2.1, we reconstructed the geometrically accurate and topologically correct cerebellar surfaces [18,19]. Finally, each inner cerebellar surface is labeled into 17 lobules following the SUIT parcellation strategy [3] by an expert. To ensure the labeling quality, each manually labeled region is further validated and cross-modified by another 2 experts to alleviate the subjective bias and ensure the labeling quality. Due to the limited subject number, we adopted these manually labeled cerebellums to validate our method in a 10-fold cross validation. We implemented our method mainly based on the pytorch (https://pytorch.org/) and pytorch geometric (https://pyg.org/) packages. The graph convolution operator [20] implemented in the pytorch geometric package is adopted as the major building block to construct our network. For the contrastive learning framework, we used 18 ResNet blocks for the encoding and 3 fully connected layers for the projection. After the cerebellar cortical surface reconstruction, each scan typically has around 90k vertices. We extract 8k patches from each training scan. The ρ max is set to 15mm. For the NT-Xent loss, the hyperparameter τ is set to 0.5. For the graph-cut cost, the weight balance λ is set to 1. For the testing subject, we perform the parcellation for each patch."
Weakly Supervised Cerebellar Cortical Surface Parcellation with Self-Visual Representation Learning,3.2,Comparison with the State-of-the-Art Methods,"Since we conducted our parcellation on the original cerebellar surface, which can be regarded as a graph, we compared our method with state-of-the-art graph convolutional neural network based methods [20][21][22][23] for the cerebellum parcellation task. The Dice similarity coefficient (DSC) between the predicted label and the manual label is adopted as a quantitative evaluation of the parcellation performance. Table 1 reported the average DSC of the parcellation for all regions acquired by our proposed method and the comparison methods. It can be seen that our method achieves best performance, indicating the effectiveness of the latent patch feature embedding obtained by our proposed weakly supervised contrastive learning strategy. "
Weakly Supervised Cerebellar Cortical Surface Parcellation with Self-Visual Representation Learning,3.3,Different Features' Influence Analysis,"We also validated the influence of each geometric feature on the final parcellation performance. We conducted the ablation study by removing parts of the geometric features for the parcellation. Table 2 reported the average DSC using different feature combinations. It can be seen that the raw geometric features, i.e., the average convexity, mean curvature, curvedness, and shape index, are difficult to capture the localization ability. This is reasonable since the cerebellar cortex has much deeper sulci, and appear more similar geometric patterns, compared to the cerebral cortex. Therefore, the local patches have relatively low localization ability. However, after adding the spectral feature and the centroid relation feature, the parcellation performance is greatly improved."
Weakly Supervised Cerebellar Cortical Surface Parcellation with Self-Visual Representation Learning,4,Conclusion,"In this paper, we propose an automated method for anatomically meaningful cerebellar cortical surface parcellation. We firstly reconstruct the geometric accurate and topologically correct cerebellar surfaces and then compute several widely used geometric features to comprehensively characterize the geometries of the cerebellar surface. Next, we extract local surface patches from the reconstructed cerebellar surfaces with the neighborship defined by the intrinsic geodesic metric. These extracted local surface patches are projected to a low dimensional latent space with a contrastive learning framework, i.e., patches from the same region are enforced to have similar representations, while patches from different regions have distinct representations. After that, we train a neural network to map the latent representations to the parcellation labels. Comparison to the state-of-the-art methods has validated the superior performance of our method. Currently, our work has two limitations, a) the quantitative evaluation is based on a small number of the subjects, due to the expensive manual labeling cost. In the future, we plan to involve and release more manually labeled cerebellar cortical surfaces to further improve the generalizability of the current framework, enhance the representation learning and validate our method on larger datasets; b) a graph cut post-processing is needed to remove potential inconsistent labelling. In the future, we plan to directly add the neighborhood smooth labeling constraint into the network cost function to obtain an end-to-end cerebellar cortical surface parcellation method."
Dynamic Graph Neural Representation Based Multi-modal Fusion Model for Cognitive Outcome Prediction in Stroke Cases,1,Introduction,"Post-Stroke Cognitive Impairment (PSCI) is common following stroke, and more than half of people will suffer from PSCI in the first year after stroke. Therefore, predicting cognitive impairment after stroke is an important task [2,5]. However, the determinants and mechanisms of PSCI are insufficiently understood, and doctors cannot make a clear risk prediction based on single factors, e.g., the locations of infarct and White Matter Hyperintensities (WMH) lesions, resulting in frequent underdiagnosis of PSCI [16,17]. Therefore, accurate prediction of PSCI becomes challenging and critical for devising appropriate post-stroke prevention strategies.In recent years, Convolutional Neural Networks (CNN) have shown promising performance in brain disease diagnosis and prediction [4]. Compared with traditional machine learning methods [14], CNNs excel at extracting high-level information about neuroanatomy from Magnetic Resonance Imaging (MRI) [11]. However, brain MRIs provide only a partial view of the underlying changes that lead to cognitive decline, and the extracted features using CNNs are difficult to interpret by clinicians. Hence, some studies use a pre-defined set of image features, like cortical features, extracted from MRIs via Freesurfer [18]. However, the aforementioned approaches all focus on neuroimaging data, ignoring the importance of clinical data such as patient demographics, family history, or laboratory measurements. Some successful multi-modal fusion models have been proposed and achieved some improvements in predictive tasks, but image and clinical data are only integrated through simple concatenation or affine transformations [3,12], without considering the specificity of patient brain anatomy [10].To overcome the aforementioned issues of existing methods, we propose a novel multi-modal fusion model to solve this challenging task. Our main contributions are as follows: (1) Both MRI data and clinical data are studied in this task, where subject-specific anatomy, infarcts localization and WMH distribution are explored from MRIs, while clinical (tabular) data provides personal information and clinical indicators such as stroke history and stroke severity.(2) Dynamic graphs neural representation is first proposed for PSCI prediction, which can not only integrate multi-modal information but also take subjectspecific brain anatomy into account. An effective missing information compensation module is proposed to reduce the impact of incomplete clinical data. (3) A detailed ablation study is conducted to verify the effectiveness of each proposed module. The proposed method outperforms competing models by a large margin. Additionally, the Top-15 brain structural regions strongly associated with PSCI are uncovered by the proposed method, which further emphasizes the relevance of the proposed method."
Dynamic Graph Neural Representation Based Multi-modal Fusion Model for Cognitive Outcome Prediction in Stroke Cases,2,Methodology,"Our approach consists of three steps (illustrated in Fig. 1): (i) graph construction and node feature extraction, (ii) missing information compensation module, and (iii) dynamic graph neural representation. "
Dynamic Graph Neural Representation Based Multi-modal Fusion Model for Cognitive Outcome Prediction in Stroke Cases,2.1,Graph Construction and Node Feature Extraction,"We represent the nodes of the graph with 131 structural brain regions obtained from the Hammers' brain atlas [7,9] (more details are given in Sect. 3.1). The edges of the graph are established based on the neighboring relationship of each brain structural area. Specifically, if two areas are adjacent, we create an edge between the corresponding nodes; otherwise, no edge is present. To each node, we assign a total of eighteen features, comprising fifteen first-order statistical features (maximum, minimum, median, mean, and variance) extracted from the corresponding brain structural regions in Trace Diffusion-Weighted MRI (DW-MRI) and Mean Diffusivity (MD) DW-MRI and FLAIR MRI, along with three stroke lesion features: structural volume, infarct volume, and WMH volume."
Dynamic Graph Neural Representation Based Multi-modal Fusion Model for Cognitive Outcome Prediction in Stroke Cases,2.2,Missing Information Compensation Module,"When dealing with clinical data, some patients may not have complete records, resulting in missing information. To mitigate the impact of missing data, we propose a module shown in Fig. 1(c). Rather than using a naive approach of filling in missing elements with an average value or zero, we suggest learning the offset value caused by missing elements based on the known information to compensate for the missing data. We denote a patient's 1D tabular record as X = {x i }, and define a vector V with the same size as X. In V , bits for missing elements are represented as '1', and the remaining bits are '0'. We denote the set of indices of missing elements as M = {m|v m = 1}. As a complete X is processed by a linear layer, each element x i can be expressed asWhen the value of some input values x m , m ∈ M is unknown due to missing tabular data, we can only compute x i = j / ∈M w ij x j + b i and the difference x ix i is the impact the missing tabular data can have. We propose a module to learn this difference to mitigate the effect of missing values byHere, φ 1 represents two normal linear layers, φ 2 is a linear layer without considering the bias, φ 3 represents a normal linear layers, LReLU is a leaky rectified linear activation, and • is the element-wise multiplication operator. When some elements are missing, this module will estimate the offset; and when processing a complete clinical record, the output of Eq. ( 2) should always be zero. Hence, the neural representation of each patient's clinical tabular record will be calculated using Eq. ( 3)."
Dynamic Graph Neural Representation Based Multi-modal Fusion Model for Cognitive Outcome Prediction in Stroke Cases,2.3,Dynamic Graph Neural Representation,"We regard the tabular's neural representation R X as global information and the node feature d i in each subject-specific graph G = {D, E|d i ∈ D} as local information (here, E denotes the adjacency matrix): First, we set up an attention mechanism between R X and d i , which will capture the contribution of each node and enhance node feature. Then, a multi-head graph self-attention is introduced for this task, which pays attention to the weight of edges in the graphs. It transforms the static graph to several dynamically subgraphs, influencing the broadcasting process of the graph. Next, we use graph summary block which consists of a normal linear layer and a leaky rectified linear unit activation function (LReLU), to summarize the graph G and output a graph-based global feature R G , i.e., graph's neural representation. Finally, we fuse the graph's neural representation R G and the updated tabular's neural representation RX via concatenation, which can be used as the input of a multilayer perceptron (MLP) for classification as shown in Fig. 1. Here, focal loss function is employed, which is formulated as,Here p is the predicted probability of the correct class, α = 0.2 is the weighting factor for each sample, γ = 0.05 is a tunable focusing parameter.Node Attention Block. This is a parameterized function that learns the mapping between query (q t ∈ R 1×C ) coming from the tabular's neural representation R X , and the corresponding key (k g ∈ R N ×C ) and value (v g ∈ R N ×C ) representations in graph G. Here, N is the number of nodes, C is the channel of node feature. Hence, the node attention block fuses information from tabular (global level) and image data (local/node level) by measuring the correlation between q t and k g , and the attention weight (A 1 ∈ R 1×N ) is computed as follow,Using the computed attention weight, the output of the node attention block is computed to update graph G as,Multi-head Graph Self-attention Block. Inspired by [15] and in order to optimize the broadcasting of the graph, the multi-head self-attention is proposed to learn the edge weights during training. Following with the node attention, it is also a parameterized function that learns the mapping between the reused key (k g ∈ R N ×C ) and the query (Then, the shapes of query q g , key k g and updated graph feature G are reshaped as [h, N, C/h]. Here, h is the number of heads. The attention weight (A 2 ∈ R h×N ×N ) is computed by measuring the similarity between q g and k g according to,Using the computed attention weight and considering the fixed adjacency matrix E, the output of graph multi-head self-attention block G ∈ R h×N ×C/h is computed and reshaped as [N, C] for the preparation of starting the next graph neural representation unit.3 Materials and Experiments"
Dynamic Graph Neural Representation Based Multi-modal Fusion Model for Cognitive Outcome Prediction in Stroke Cases,3.1,Data and Preparation,"Our study utilized clinical data from 418 stroke patients, collected within five days of the onset of their acute stroke symptoms, and consisted of both clinical data and image data. Each clinical tabular records contains 20 variables, including 3 variables of basic personal information (age, sex, education) and 17 variables of clinical indicators (i.e., smoking, drinking, hypertension, diabetes, atrial fibrillation, previous stroke, BMI, circulating low-density lipoprotein cholesterol levels, stroke severity, pre-stroke function, cognitive impairment in the acute post-stroke phase, stroke lesion volume, lacunes number, Fazekas score of WMH, Fazekas score of WMH in deep white matter, cerebral microbleeds number, perivascular spaces level). Patients were followed up, and clinical experts classified them as either PSCI negative or PSCI positive based on detailed neuropsychological tests conducted 12 months after stroke onset. Out of the 418 patients, there were 332 positive cases and 86 negative cases. All studies were approved by the local ethics committees. We split the patients into two groups with an equal proportion of PSCI candidates for five-fold cross validation, 80% for training and 20% for testing. In the preparation step, the MRIs are rigidly aligned using [1]. Based on each patient's T1 data, the whole brain was segmented into 131 structural regions using MALPEM [7,9], from which we constructed the subject-specific brain graphs. Meanwhile, based on each patient's Trace DW-MRI and FLAIR MRI, the infarct lesions and WMH lesions were semi-automatically segmented and manually corrected by a clinical expert."
Dynamic Graph Neural Representation Based Multi-modal Fusion Model for Cognitive Outcome Prediction in Stroke Cases,3.2,Evaluation Measures,"To validate the performance of our method, we employed five measures including Balanced Accuracy (BAcc), Classification Accuracy (Acc), Precision (Pre), Sensitivity (Sen), Specificity (Spe), and the area under the receiver operating characteristic curve (AUC). Among them, BAcc is a performance metric used to evaluate the effectiveness of a model on imbalanced datasets."
Dynamic Graph Neural Representation Based Multi-modal Fusion Model for Cognitive Outcome Prediction in Stroke Cases,3.3,Experimental Design,"To validate the performance of our method, we perform a series of experiments on our in-house dataset. Based on the multi-modal's information (i.e., clinical data and image data), we designed three experiments with different data inputs. (I) By only using the clinical data as input, the effectiveness of missing information compensation module is studied. (II) By only using the image-based features as input, the importance of graph-based analysis for PSCI prediction is investigated.(III) Using both as input, an ablation experiment is performed, where we learn the evolution process and the superiority of our proposed fusion model. The proposed model is implemented on PyTorch library (version 1.13.0) with one NVIDIA GPU (Quadro RTX A6000), and trained for 400 epochs with a batch size of 32. Adam optimizer is employed with weight decay of 0.008, an initial learning rate of 0.0005; and CosineAnnealingLR scheduling technique is used to adjust the learning rate with the maximum number of iterations of 10."
Dynamic Graph Neural Representation Based Multi-modal Fusion Model for Cognitive Outcome Prediction in Stroke Cases,4,Results,"Table 1 summarizes the results of the five-fold cross-validation experiments. We used different models for the experiments, where A represents an MLP model, B represents an MLP model with the proposed missing information compensation module shown in Fig. 1(c), C represents another MLP model, D represents a Graph Convolutional Network (GCN) [8], B+D represents a fusion model by concatenating the features from B and D, B+E represents a fusion model by concatenating the features from B and E, where E is a GCN with a multi-head graph self-attention block as explained in Sect. 2.3. Finally, F represents our proposed method as shown in Fig. 1. "
Dynamic Graph Neural Representation Based Multi-modal Fusion Model for Cognitive Outcome Prediction in Stroke Cases,4.1,Effectiveness of Missing Information Compensation,"To assess the efficacy of compensating for missing information, a standard MLP model is trained solely on clinical data with missing elements filled in with zeros. Subsequently, the same MLP model is trained with a missing information compensation module, utilizing the identical training set. Comparing the performances of model A and B, it is obvious that the proposed missing information compensation module plays an important role, achieving a BAcc of 0.709 ± 0.054 and AUC score of 0.741 ± 0.072, outperforming Method A."
Dynamic Graph Neural Representation Based Multi-modal Fusion Model for Cognitive Outcome Prediction in Stroke Cases,4.2,Importance of Graph-Based Analysis,"To investigate the importance of graph-based analysis for this task, an MLP model embedded with GCNs is trained only on imaging-based features, and the comparison method is a standard MLP model. Comparing the performances of model C and D, it is clear that the suggested graph-based model makes an notable improvement on both BAcc and AUC, with the rise of BAcc from 0.671 to 0.700 and of Auc from 0.648 to 0.711, which demonstrates that the consideration of subject-specific brain anatomy is very important for PSCI prediction. "
Dynamic Graph Neural Representation Based Multi-modal Fusion Model for Cognitive Outcome Prediction in Stroke Cases,4.3,Superiority of Proposed Fusion Model,"Finally, we explore how to effectively fuse clinical data and image data for PSCI prediction. As a baseline approach, we directly concatenate the features learned from model B and D. The first evolutionary process is to upgrade static brain map analysis to dynamic brain map analysis, thus the performance of model B+E is explored, where the proposed multi-head graph self-attention block is first embedded into the GCN layers. Then, a better fusion pattern is discovered, i.e., dynamic graph neural representation in our final proposed method. The experimental results demonstrate that B+E model results in a higher BAcc of 0.761 compared to baseline. Furthermore, our final proposed method outperforms all other methods, achieving a BAcc as high as 0.796 and an AUC of 0.800.To explore the contribution of each brain structural region in this task, we first average the attention map A 1 across all subjects and visualize it, as illustrated in Fig. 2(top). Additionally, to adjust for the influence of volume, we compute the contribution per unit volume for each region by dividing by its corresponding volume, as illustrated in Fig. 2(bottom). Our observations indicate that the network appears to concentrate on clinically significant regions that impact cognition, including the Cerebral White Matter, Basal Forebrain and 3rd Ventricle [6,13]. Nevertheless, we have also identified some ambiguous relationships that necessitate further investigation."
Dynamic Graph Neural Representation Based Multi-modal Fusion Model for Cognitive Outcome Prediction in Stroke Cases,5,Conclusion and Discussion,"In this study, we tackle the challenge of predicting PSCI using both brain MRI and clinical non-imaging data. Our proposed method utilizes a dynamic graph neural representation that fully leverages the structural information of the brain. By incorporating node attention and self-attention, we effectively merge clinical tabular and image information. Our approach achieves a BAcc of 0.796 on the test dataset. Moreover, visual attention maps allow us to identify the contributions of different brain structures to this prediction task. The results of our study are consistent with prior medical research. However, there are certain brain regions that require further exploration by clinicians. In addition, there are several areas our proposed method is highly clinically relevant: (1) Risk prediction to identify (i) low-risk patients, and (ii) high-risk patients (for those early and targeted treatment and rehabilitation is recommended). ( 2) Detailed phenotyping and characterization of high-risk patients and follow-up. (3) Identification of novel biomarkers and risk factors."
Dynamic Functional Connectome Harmonics,1,Introduction,"Nonlinear dimensionality reduction techniques such as diffusion embedding and principal component analysis applied to FC data provide the primary axes along which FC is organized [11]. Laplacian embedding is a related nonlinear dimensionality reduction technique, which yields the normal modes or functional connectome harmonics, which are the focus of this study. Connectome harmonics have a straightforward interpretation as the brain analog of the Fourier basis and can be used to filter the underlying fMRI signal.A growing body of evidence suggests that the strongest components of FC are underpinned by brief but strong patterns of activation, or brain states, resembling the dominant components of FC (e.g., the default mode network) rather than sustained, low-magnitude signal coherence between brain regions implicated in those strongest components of FC [3,10,14]. Simultaneously, few studies have analyzed how functional connectome harmonics depend on the timescale on which they are computed. If briefly activated brain states mediate the primary axes of FC, we would expect momentary functional connectome harmonics to be stable with respect to timescale.The robustness of connectivity patterns across populations and the reliability and reproducibility of FC gradients across subjects and choices of parameters have been well studied [1,9,17]. However, it is not well understood to what degree the magnitude of and relationship between the activation timeseries of certain brain states is reproducible within and across subjects. Further, the reliability of time-varying FC harmonics has yet to be studied. Given both our hypothesis that functional connectome harmonics represent discrete brain states and evidence that discrete brain states underpin FC, we expect that the activation energy of principal functional connectome harmonics with respect to the underlying fMRI signal will be reproducible within individuals.Previous work has shown that network configuration is dynamic during task and rest, with the community allegiance of brain regions changing over time [4]. The propensity of brain regions to change network allegiance, or flexibility, has been implicated in learning, development, and psychiatric disorders [2,8,19]. Previous studies have defined cortical flexibility in terms of the frequency of community allegiance change and demonstrated relationships between flexibility, learning, and development. Functional connectome harmonics correspond to axes along which connectivity is organized, yielding both a convenient basis to represent underlying functional activation and a spectral representation of each brain region. The spectral coordinates define the position of a brain region in terms of the primary axes of FC, with similar spectral coordinates indicating similar network allegiance profiles. By computing time-varying functional connectome harmonics, we can study the temporal dynamics of spectral coordinates, which provides a new lens through which to examine cognitive flexibility. Previous studies have relied on community detection algorithms to compute the degree of change in network allegiance of brain regions, which are hindered by free parameters governing the number of communities, the strength of connection between adjacent time points, and the choice of parcellation due to computational restraints on community detection algorithms. Dynamic functional connectome harmonics present a simpler method for computing flexibility: change in network allegiance-and therefore flexibility-is proportional to change in spectral coordinates given by connectome harmonics."
Dynamic Functional Connectome Harmonics,2,Methods,"We used the resting-state fMRI time series of 44 subjects from the HCP minimal preprocessed test-retest dataset mapped to the 32k MSM-All surface atlas [7]. For each subject, the test and retest datasets contain data from two rs-fMRI sessions for a total of 176 sessions. To investigate the variation in of functional connectome harmonics at different time scales, we computed FC matrices at 4 window lengths: w = 0.5, 1, 2, and 3.5 min. For each window length, each 28 min time series was divided into N w = 28 w windows, and an FC matrix was computed for each window. FC between two surface vertices was computed as the correlation between the z-normalized time series.In light of studies demonstrating that thresholding FC matrices increases the reliability of FC gradients [9], the top 10% of the correlations for each row in each FC matrix was retained, and the FC matrices were made symmetric by taking the average of each matrix and its transpose.For each FC matrix Â(t) at timepoint t, the normalized graph Laplacian matrix,, with D(t) i,i = Nv j=0 Â(t) i,j was computed. The first N h eigenvectors of L(t), solutions to L(t)ψ(t) k = λ(t) k ψ(t) k , were computed, yielding the functional connectome harmonics of window t,,924 is the number of vertices, and N h = 7 is the number of harmonics computed. This process is carried out for all windows for each acquisition, yielding the dynamic functional connectome harmonics for a particular subject and window length:To extract a single set of harmonics for each acquisition at each window length, the window-wise harmonics from all windows for that acquisition were stacked horizontally into a large matrix of size N v × (N h N w ) on which PCA was performed, yielding one set of principal components for each acquisition and each subject. We refer to the components obtained via this approach as the most-prevalent harmonics, ψ, as they encapsulate orthogonal patterns of maximum variation across all window-wise harmonics. This process was also repeated on the group level for the test and retest cohorts separately at each window length, yielding the most prevalent harmonics Ψ for the test and retest groups. For computation of Ψ , the matrix on which PCA was performed is of size N v × (N h N w N a N s ), where N a = 2 is the number of acquisitions per subject for either test or retest, and N s = 44 is the number of subjects.To investigate the reproducibility of dynamic functional connectome harmonics as a basis for reconstructing the underlying fMRI signal, we utilize the harmonics as operators on functional timeseries. For a vertex-wise functional time series X = [x(1), . . . , x(N T )], where x(t) is the fMRI activation at timepoint t, we define its N h × N T spectral coefficient timeseries matrix α with respect to harmonics ψ aswhere each row α k of α is the spectral coefficient timeseries of timeseries X for harmonic ψ k . Each α k of α gives the activation power of harmonic k at each time point, and the L2 norm of α k yields the activation energy E(ψ k , X) = ||α k || of harmonic k across the entire timeseries. Further, we define the harmonic-filtered timeseries X and the representation efficiency ratio γ asA set of harmonics {ψ k |k = 1, . . . , N h } defines a spectral coordinate vectorfor each vertex i on the surface, specifying a position in the harmonic embedding space. To allow for comparison of spectral coordinates across a series of consecutive time windows, we first compute the most-prevalent harmonics ψ of the time series, and perform Procrustes alignment [18] between each set of window-wise harmonics ψ(t) and the most-prevalent harmonics ψ from that acquisition using the implementation from [17]. These aligned harmonic time series define a trajectory in spectral space for each vertex. Vertices that have similar network allegiance have similar spectral coordinates, taking on similar values in each harmonic. A large change in spectral coordinates between timepoints t and t + 1 indicates a change in FC topology relative to that vertex. We therefore define flexibility in terms of the distance between spectral coordinates at consecutive time points. For N w sets of aligned harmonics computed at consecutive windows, we define the flexibility of vertex i as.(In order to evaluate our method of computing flexibility, we also employ a standard community detection based technique for comparison with our method wherein a graph modularity metric is optimized to obtain a community partition using a multi-layer temporal graph [2,13]. Due to computational complexity of the modularity optimization process involved, we apply this standard flexibility metric to FC data parcellated using the FreeSurfer Destrieux Atlas [5]."
Dynamic Functional Connectome Harmonics,3,Results,"Our most-prevalent harmonics comprise meaningful patterns of activation on the cortex (Fig. 1). On the group level, Ψ1 reflects the default mode network (DMN) and Ψ2 the task positive network (TPN) [6]. Our harmonics largely resemble FC gradients computed in the literature, although we do not observe the visual vs somatosensory component, which has been observed as the second gradient in static FC studies [11,17]. The absence of this harmonic at shorter timescales may indicate that it is mediated by longer-term, lower magnitude signal co-fluctuations and within visual and somatosensory cortices.We evaluated the stability of the group level most-prevalent harmonics across repeated measurements (test and retest group cohorts) and across varying window length using cosine similarity and found high similarity (≥0.70) between all pairings, indicating high stability (Fig. 1). Further, we evaluated the intra-subject stability of the individual most-prevalent harmonics using cosine similarity and found high stability across repeated measurements (4 scans per subject) for the first 3 harmonics. Table 1 displays the mean ± the standard deviation of the intra-subject cosine similarity between individual most-prevalent harmonics, and of the inter-window cosine similarity between dynamic harmonics of a given subject for k = 1, 2, 3. Note that individual most-prevalent and dynamic harmonics were Procrustes aligned before comparison in Table 1.We investigated how the harmonics relate to functional timeseries by examining the stability of the harmonic activation energy of the first two harmonics, ||α 1 || and ||α 2 ||, the representation efficiency ratio γ, and the correlation between α 1 and α 2 . For 0.5 0.81 ± 0.11 0.79 ± 0.10 0.72 ± 0.13 0.22 ± 0.10 0.17 ± 0.08 0.12 ± 0.06 1 0.83 ± 0.09 0.80 ± 0.09 0.74 ± 0.11 0.41 ± 0.12 0.33 ± 0.11 0.25 ± 0.09 2 0.84 ± 0.09 0.82 ± 0.09 0.75 ± 0.11 0.58 ± 0.11 0.50 ± 0.12 0.40 ± 0.12 3.5 0.86 ± 0.07 0.82 ± 0.09 0.76 ± 0.11 0.70 ± 0.09 0.61 ± 0.11 0.52 ± 0.12 each session and each subject, we computed each quantity using the most-prevalent harmonics ψ of that subject and session and investigated the reliability of these harmonicderived metrics using the intraclass correlation coefficient (ICC), with results summarized in Table 2. ICC results indicate larger variability in each metric across subjects than across sessions with high significance (p ≤ 1 × 10 -13 for all metrics), with moderate to good reliability for ||α 1 || and ρ(α 1 , α 2 ), and moderate reliability for ||α 2 || and γ. We also found that ρ(α 1 , α 2 ) had a mean of -0.153 and standard deviation of .07, indicating that the activation timecourses of the harmonics most closely resembling the DMN and TPN are anticorrelated. We computed flexibility using our dynamic functional connectome harmonic formulation (Eq. 3) for each subject and session for each window length. Average flexibility computed using w = 2 is displayed in Fig. 2a. Importantly, there were not significant differences in the qualitative map of flexibility with varying window length. As a sanity-check on our definition of flexibility, we also computed flexibility using traditional methods from the literature [2,8,14,19], using the Leidenalg community detection algorithm [15] with interlayer connection strength ω = 1 and resolution parameter r = 1, and window length w = 2. Flexibility was computed as the ratio of the total number of community assignment changes to the total number of possible transitions for each node (parcel). Using the community-detection based flexibility approach, we find very similar distribution of flexibility values, but with higher values in the prefrontal cortex, and slightly lower values in the occipital cortex. Flexibility results from both methods with the exception of the high values in the occipital cortex are in keeping with previous studies on flexibility, indicating that our definition of flexibility is an effective method for measuring the propensity of brain regions to change community allegiance [12]. Importantly, our definition of flexibility involves fewer parameters than conventional methods, and yields a vertex-wise map of flexibility."
Dynamic Functional Connectome Harmonics,4,Discussion,"Individual most-prevalent harmonics are found to be similar across acquisitions after Procrustes alignment, while there is notably lower average similarity between dynamic harmonics, particularly at shorter timescales (Table 1). This is not unexpected, as we anticipate variability in FC between windows. Higher intra-subject similarity in ψk compared with inter-window ψ(t) k similarity indicates that our PCA-based method of extracting the most-prevalent harmonics from the dynamic harmonics achieves its goal of extracting the common brain states for a given acquisition and that those states are reproducible within subjects. Further, our most-prevalent harmonics are advantageous over naive averaging approaches, as they preserve the orthogonality constraint intrinsic to connectome harmonics.Our most-prevalent functional connectome harmonics show invariance under change in timescale and strongly resemble FC ""gradients"" in the literature computed on static FC [11]. As connectome harmonics represent the primary axes along which FC is organized, their invariance to timescale is significant: persistence of the same harmonics at long, medium, and short timescales supports the hypothesis that the harmonics themselves are brain states whose activation fluctuates. Notably, at the group level, our second harmonic Ψ2 resembles the TPN, whereas in the literature the second gradient is found to be an axis spanning from visual to somatosensory areas. Although our Ψ3 has high intensity values in the visual cortex, it is not a clear axis between visual and somatosensory regions. This difference between our results and static FC gradient results may indicate that the second FC gradient from the literature is the result of longer-timescale, lower-magnitude signal variations that differentiate the visual and somatosensory cortices.Our analysis indicates that our individual most-prevalent harmonics ψ are a reproducible basis with which to decompose the underlying fMRI signal. The projection coefficient timeseries of a given harmonic, α k , represents the timecourse of relative activation of ψk . Importantly, we found that the magnitude of these activation timecourses for the first and second most-prevalent harmonic (corresponding to the DMN and TPN), as well as the correlation between them were reliable across repeated measurements in individual subjects, with significantly higher variability across subjects than across scans (Table 2).This indicates that the strength of activation and the temporal relationship between the activation of ψ1 and ψ2 are reliable, and suggests that our method of extracting most-prevalent harmonics provides a reliable subject-specific method for analyzing the manifestation and relationship between different connectivity patterns. It is also worth noting that we found that ρ(α 1 , α 2 ) was negative in general, indicating that activation of the DMN and TPN is anticorrelated, in keeping with previous studies [6,16]. In light of this, further analysis of the strength of activation of meaningful harmonics and their interrelationships could provide meaningful insights into differences in network dynamics in diseased populations.Our dynamic functional connectome harmonic definition of flexibility yields a vertex-wise map of cortical flexibility for each subject, requires fewer parameters, and is less computationally expensive than standard definitions of flexibility. Our group-average flexibility result (Fig. 2a) shows strong resemblance to the result computed using the community-detection based flexibility definition (Fig. 2b). Transmodal  regions such as the angular gyrus, temporal-parietal junction, prefrontal and medial prefrontal cortices, precuneus, and middle temporal gyrus display high flexibility due to their involvement in complex high order cognitive functions. In contrast, unimodal regions such as motor, somatosensory, limbic, and auditory cortex demonstrate low flexibility. An interesting feature of our flexibility result is high flexibility in the visual cortex. Although this has not been consistently found in previous studies, it is worth noting that many of the seminal works on flexibility deal with task-based fMRI data [2,14], wherein flexibility in the visual cortex may be lower than at rest. One possible explanation is that during rest, daydreaming and imagination lead to rapid switching in the recruitment of the visual cortex, leading to high flexibility values."
Dynamic Functional Connectome Harmonics,5,Conclusion,"Our most-prevalent functional connectome harmonics and their dynamic counterparts provide a powerful lens through which to study the organization of FC as well as its dynamics. We demonstrate that the most-prevalent harmonics are invariant when the timescale at which their underlying connectivity matrices are computed is changed, and that they are highly stable across repeated measurements on the group and individual level. Further, we show that our harmonics provide a reliable basis with which to filter fMRI timeseries and to extract the activation timecourses of individual harmonic brain states. Importantly, the reproducibility of these harmonic decompositions indicates that they provide a subject-specific method with which to study the dynamics of specific brain states and their interrelationships. We also present a novel formulation of cortical flexibility defined in terms of dynamic functional connectome harmonics and show that it gives vertex resolution flexibility maps that qualitatively similar results to previous definitions of flexibility which are constrained by free parameters and computational complexity."
Simulation of Arbitrary Level Contrast Dose in MRI Using an Iterative Global Transformer Model,1,Introduction,"Gadolinium-Based Contrast Agents (GBCAs) are widely used in MRI scans owing to their capability of improving the border delineation and internal morphology of different pathologies and have extensive clinical applications [1]. However, GBCAs have several disadvantages like contraindications in patients with reduced renal function [2], patient inconvenience, high operation costs and environmental side effects [3]. Therefore, there is an increased emphasis on the paradigm of ""as low as reasonably achievable"" (ALARA) [4]. To tackle these concerns of GBCAs, several dose reduction [5,6] and elimination approaches [7] have been proposed. However, these deep learning(DL)-based dose reduction approaches require high quality low-dose contrast-enhanced (CE) images paired with pre-contrast and full-dose CE images. Acquiring such a dataset requires modification of the standard imaging protocol and involves additional training of the MR technicians. Therefore, it is important to simulate the process of T1w low-dose image acquisition, using images from the standard protocol. Moreover, it is crucial for these dose reduction approaches to establish the minimum dose level required for different pathologies as these are dependent on the scanning protocol and the GBCA compound injected. Therefore the simulation tool should also have the ability to synthesize images with multiple contrast enhancement levels, that correspond to multiple arbitrary dose levels.Currently MRI dose simulation is done using physics-based models [8]. However, these physics-based methods are dependent on the protocol parameters and the type of GBCA and their relaxation parameters. Deep learning (DL) models have been widely used in medical imaging application due to their high capacity, generazibility, and transferability [9,10]. The performance of these DL models heavily depend on the availability of high quality data. There is a dearth of datadriven approaches to MRI dose-simulation given the lack of diverse ground truth data of the different dose levels. To this effect, we introduce a vision transformer based DL model1 that can synthesize brain 2 MRI images that correspond to arbitrary dose levels, by training on a highly imbalanced dataset with only T1w pre-contrast, T1w 10% low-dose, and T1w CE standard dose images. The model backbone consists of a novel Global transformer (Gformer) with subsampling attention that can learn long-range dependencies of contrast uptake features. The proposed method also consists of a rotational shift operation that can further capture the shape irregularity of the contrast uptake regions. We performed extensive quantitative evaluation in comparison to other state-of-the art methods. Additionally, we show the clinical utility of the simulated T1w low-dose images using downstream tasks. To the best of our knowledge, this is the first DL based MRI dose simulation approach."
Simulation of Arbitrary Level Contrast Dose in MRI Using an Iterative Global Transformer Model,2,Methods,"Iterative Learning Design: DL based models tend to perform poorly when the training data is highly imbalanced [11]. Furthermore, the problem of arbitrary dose simulation requires the interpolation of intermediate dose-levels using a minimum number of data points. Iterative models [12,13]   applications as they work on the terminal images to generate step-wise intermediate solutions. We first utilize this design paradigm for the dose simulation task and train an end-to-end model on a highly imbalanced dataset where only T1w pre-contrast, T1w low-dose, and T1w post-contrast are available.As shown in Fig. 1 where P pre , P post , and P low denote the pre-contrast, post-contrast, and predicted low-dose images, respectively and P i-1 denotes the image with a higher enhancement than P i . This way, the intermediate outputs { P i } k i=1 having different enhancement levels, correspond to images with different contrast dose level with a uniform interval. This iterative model essentially learns a gradual dose reduction process, in which each iteration step removes a certain amount of contrast enhancement from the full-dose image."
Simulation of Arbitrary Level Contrast Dose in MRI Using an Iterative Global Transformer Model,,Loss Functions and Model Convergence:,"The proposed iterative model aims to learn a mapping from the post-contrast & pre-contrast images to the synthesized low-dose images P low and is trained with the true 10% low-dose image P low as the ground truth. We used the L 1 and structural similarity index measure (SSIM) losses. To tackle the problem of gradient explosion or vanishing, ""soft labels"" are generated using linear scaling. These ""soft labels"" serve as a reference to the intermediate outputs during the iterative training process and also aid model convergence, without which the model has to directly learn from post-contrast to low-dose. Given k iterations, the soft label {S i } k-1 i=1 for iteration i is calculated as follows:where P post and P pre denote the skull-stripped post-contrast and pre-contrast images. γ = 0.1 represents the dose level of the final prediction, and τ = 0.1 denotes the threshold to extract the estimated contrast uptake U = ReLU( P post -P preτ ). Finally, the total losses are calculated aswhere L e = L L1 + L SSIM and α = 0.1 and β = 1. The ""soft labels"" are assigned a small loss weight so that they do not overshadow the contribution of the real low-dose image. Additionally, in order to recover the high frequency texture information and to improve the overall perceptual quality, adversarial [14] and perceptual losses [15] are applied on ( P low , P low ) with a weight of 0.1.Global Transformer (Gformer): Transformer models have risen to prominence in a wide range of computer vision applications [10,16]. Traditional Swin transformers compute attention on non-overlapping local window patches. To further exploit the global contrast information, we propose a hybrid global transformer (Gformer) as a backbone for the dose simulation task. As illustrated in Fig. 1(b), the proposed model design includes six sequential Gformer blocks as the backbone module with shortcuts. As shown in Fig. 1(c), the Gformer block contains a convolution block, a rotational shift module, a sub-sampling process, and a typical transformer module. The convolution layer extracts granular local information of the contrast uptake while the self-attention emphasizes more on the coarse global context, thereby paying attention to the overall contrast uptake structure.Subsampling Attention: The sub-sampling is a key element in the Gformer block which generates a number of sub-images from the whole image as attention windows as shown in Fig. 2. Gformer performs self-attention on the sub-sampled images, which encompasses global contextual information with minimal selfattention overhead on small feature maps. Given the entire feature map M e ∈ R b×c×h×w , where b, c, h, and w are the batch size, channel dimension, height, and width, respectively, the subsampling process aggregates the strided positions to the sub-feature maps as follows, where d denotes sampling a position every d pixels, andd is the subsampled feature map. We set h, d = 0 to avoid any information loss during subsampling. These d 2 sub-feature maps are stacked onto the batch dimension as the attention windows for the transformer block shown in Fig. 1"
Simulation of Arbitrary Level Contrast Dose in MRI Using an Iterative Global Transformer Model,,(c).,"Rotational Shift: Image rotation has been widely used as a data augmentation technique in preprocessing and model training. Here, to further capture the heterogeneous nature of the contrast uptake areas, we employ the rotational shift as a module to facilitate the representation power of the Gformer. To prevent information loss on the edges due to rotation, only small angles (e.g., 10 • , 20 • ) are used for rotation and residual shortcuts are also applied. Specifically, given the feature map M o ∈ R b×c×h×w , rotational shift is performed around the vertical axis of height/width. The rotated feature map M r ∈ R b×c×h×w is obtained by the following equation:where λ is the rotation angle. (p, q, x, y) and (p , q , x , y ) denote the pixel index in the feature map tensor before and after rotational shift, respectively."
Simulation of Arbitrary Level Contrast Dose in MRI Using an Iterative Global Transformer Model,3,Experiments and Results,"Dataset: With IRB approval and informed consent, we retrospectively used 126 clinical cases (113 training, 13 testing) from a internal private dataset3 using Gadoterate meglumine contrast agent (Site A). For downstream task assessment we used 159 patient studies from another site (Site B) using Gadobenate dimeglumine. The detailed cohort description is given in Table 1. The clinical indications for both sites included suspected tumor, post-op tumor follow-up and routine brain. For each patient, 3D T1w MPRAGE scans were acquired for the pre-contrast, low-dose, and post-contrast images. These paired images were then mean normalized and affine co-registered (pre-contrast as the fixed image) using SimpleElastix [17]. The images were also skull-stripped, to account for differences in fat suppression, using the HD-BET brain extraction tool [18] for generating the ""soft labels"".   Evaluation Settings: We quantitatively evaluated the proposed model using PSNR, SSIM, RMSE, and LPIPS perceptual metrics [19], between the synthesized and true low-dose images. We replaced the Gformer backbone with other state-of-the-art methods to compare the efficacy of the different methods. Particularly, the following backbone networks were studied: simple linear scaling (""Scaling"") approach, Rednet [20], Mapnn [13], Restormer [21], and SwinIR [22]. Unet [23] and Swin-Unet [24] models were not assessed due to their tendency to synthesize blurry artifacts in the iterative modelling. throughput metric (number of images generated per second) was also calculated to assess the inference efficiency.Evaluation Results: Figure 4(a) shows that the proposed model is able to generate images that correspond to different dose levels. As shown in the zoomed inset, the hyperintensity of the contrast uptake in these images gradually reduces at each iteration. Figure 4(b) shows that the pathological structure in the synthesized low-dose image is similar to that of the ground truth. Figure 4(c) also shows that the model is robust to hyperintensities that are not related to contrast uptake. Figure 3 and Table 2 show that proposed model can synthesize enhancement patterns that look close to the true low-dose and that it performs better than the other competing methods with a reasonable inference throughput. Quantitative Assessment of Contrast Uptake: The above pixel-based metrics do not specifically focus on the contrast uptake region. In order to assess the contrast uptake patterns of the intermediate images, we used the following metrics as described in [25]: contrast to noise ratio(CNR), contrast to background ratio(CBR), and contrast enhancement percentage(CEP). The ROI for the contrast uptake was computed as the binary mask of the corresponding ""soft labels"" in Eq. 2. As shown in Fig. 5, the value of the contrast specific metrics increases in a non-linear fashion as the iteration step increases. Downstream tasks: In order to demonstrate the clinical utility of the synthesized low-dose images, we performed two downstream tasks: 1) Low-dose to full-dose synthesis Using the DL-based algorithm to predict full-dose image from pre-contrast and low-dose images described in [5], we synthesized T1CE volumes using true low-dose (T1CE-real-ldose) and Gformer (rot) synthesized low-dose (T1CE-synth-ldose). We computed the PSNR and SSIM metrics of T1CE vs T1CE-synth/T1CE vs T1CE-synth-sim which are 29.82 ± 3.90 dB/28.10 ± 3.20 dB and 0.908 ± 0.031/0.892 ± 0.026 respectively. This shows that the synthesized low-dose images perform similar4 to that of the low-dose image in the dose reduction task. For this analysis we used the data from Site B.  2) Tumor segmentation Using the T1CE volumes synthesized in the above step, we perform tumor segmentation using the winning solution of BraTS 2018 challenge [26]. Let M true , M ldose and M ldose-sim be the whole tumor (WT) masks generated using T1CE, T1CE-real-ldose and T1CE-synth-ldose (+ T1, T2 and FLAIR images) respectively. The mean Dice scores Dice(M true , M ldose ) and Dice(M true , M ldose-sim ) on the test set were 0.889±0.099 and 0.876±0.092 respectively. Figure 6 shows visual examples of tumor segmentation performance. This shows that the clinical utility provided by the synthesized low-dose is similar5 to that of the actual low-dose image."
Simulation of Arbitrary Level Contrast Dose in MRI Using an Iterative Global Transformer Model,4,Discussions and Conclusion,"We have proposed a Gformer-based iterative model to simulate low-dose CE images. Extensive experiments and downstream task performance have verified the efficacy and clinical performance of the proposed model compared to other state-of-the art methods. In the future, further reader studies are required to assess the diagnostic equivalence of the simulated low-dose images. The model can be guided using physics-based models [27] that estimate contrast enhancement level using signal intensity. This simulation technique can easily be extended to other anatomies and contrast agents."
Simulation of Arbitrary Level Contrast Dose in MRI Using an Iterative Global Transformer Model,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3_9.
