Paper Title,Header Number,Header Title,Text
AMAE: Adaptation of Pre-trained Masked Autoencoder for Dual-Distribution Anomaly Detection in Chest X-Rays,1,Introduction,"To reduce radiologists' reading burden and make the diagnostic process more manageable, especially when the number of experts is scanty, computer-aided diagnosis (CAD) systems, particularly deep learning-based anomaly detection [1,2,22], have witnessed the flourish due to their capability to detect rare anomalies for different imaging modalities including chest X-ray (CXR). Nonetheless, unsupervised anomaly detection methods [20,26] are strongly preferred due to the difficulties of highly class-imbalanced learning and the tedious annotation of anomaly data for developing such systems. Most current anomaly detection methods are formulated as a one-class classification (OCC) problem [18], where the goal is to model the distribution of normal images used for training and thus detect abnormal cases that deviate from normal class at test time. On this basis, image reconstruction based, e.g., autoencoder [9] or generative models [20], self-supervised learning (SSL) based, e.g., contrastive learning [26], and embedding-similarity-based methods [7] have been proposed for anomaly detection. Some recent self-supervised methods proposed synthetic anomalies using cut-and-paste data augmentation [12,19] to approximate real sub-image anomalies. Nonetheless, their performances lag due to the lack of real anomaly data. More importantly, these methods have often ignored readily available unlabeled images. More recently, similar to our method, DDAD [3] leverages readily available unlabeled images for anomaly detection, but it requires training an ensemble of several reconstruction-based networks. Self-supervised model adaptation on unlabeled data has been widely investigated using convolutional neural networks (CNNs) in many vision tasks via self-training [17], contrastive learning [22,26], and anatomical visual words [10]. Nonetheless, the adaptation of vision transformer (ViT) [8] architectures largely remains unexplored, particularly for anomaly detection. Recently, masked autoencoder (MAE) [11] based models demonstrated great scalability and substantially improved several selfsupervised learning benchmarks [27].In this paper, inspired by the success of the MAE approach, we propose a two-stage algorithm for ""Adaptation of pre-trained Masked AutoEncoder"" (AMAE) to leverage simultaneously normal and unlabeled images for anomaly detection in chest X-rays. As for Stage 1 of our method, (i) AMAE creates synthetic anomalies from only normal training images, and the usefulness of pretrained MAE [11] is evaluated by training a lightweight classifier using a proxy task to detect synthetic anomalies. (ii) For the Stage 2, AMAE customizes the recipe of MAE adaptation based on an unlabeled training set. In particular, we propose an adaptation strategy based on reconstructing the masked-out input images. The rationale behind the proposed adaptation strategy is to assign pseudo-labels to unlabeled images and train two separate modules to measure the distribution discrepancy between normal and pseudo-labeled abnormal images. (iii) We conduct extensive experiments across three chest X-ray datasets and verify the effectiveness of our adaptation strategy in apprehending anomalous features from unlabeled images. In addition, we evaluate the model with different anomaly ratios (ARs) in an unlabeled training set and show consistent performance improvement with increasing AR."
AMAE: Adaptation of Pre-trained Masked Autoencoder for Dual-Distribution Anomaly Detection in Chest X-Rays,2,Method,"Notation. We first formally define the problem setting for the proposed dualdistribution anomaly detection. Contrary to previous unsupervised anomaly detection methods, AMAE fully uses unlabeled images, yielding a training data T train = T n ∪ T u consisting of both normal T n and unlabeled T u training sets. We denote the normal training set as T n = {x ni } N i=1 , with N normal images, and the unlabeled training set as T u = {x ui } M i=1 , with M unlabeled images to be composed of both normal and abnormal images. At test time, given a test set T test = {(x ti , y i )} S i=1 with S normal or abnormal images, where y i ∈ {0, 1} is the corresponding label to x ti (0 for normal (negative) and 1 for abnormal (positive) image), the trained anomaly detection model should identify whether the test image is abnormal or not.Architecture. Our architecture is <-shaped: the ViT-small (ViT-S/16) [8] encoder f followed by a ViT head g and lightweight (3-layer) multilayer perception (MLP) based projection head h, simultaneously. Starting from the pretrained MAE on 0.3M unlabeled chest X-rays and officially released checkpoints, we use exactly the same ViT encoder f and decoder g as MAE [27]."
AMAE: Adaptation of Pre-trained Masked Autoencoder for Dual-Distribution Anomaly Detection in Chest X-Rays,2.1,Stage 1-Proxy Task to Detect Synthetic Anomalies,"AMAE starts the first training stage using only normal training images by defining a proxy task to detect synthetic anomalies shorn of real known abnormal images. For this purpose, we utilize the state-of-the-art (SOTA) anatomy-aware cut-and-paste augmentation, AnatPaste [19], to create synthetic anomalies from only a set of normal training images T n . AnatPaste integrates an anatomical mask x mask created from unsupervised lung region segmentation, which guides generating anomalous images via cutting a patch from a normal chest radiograph x n and randomly pasting it at another image location x paste as:where Aug (•) is the AnatPaste augmentation (see [19] for more details). Given a normal training set T n , for each normal image x n ∼ T n , we create a synthetic anomaly, denoted as Aug (x i ). In preparation for input to the frozen ViT encoder, f 0 (obtained by MAE pre-training), each input image with the h × w spatial resolution is split into T = (h/p) × (w/p) patches of size p × p. Then, for every input patch, a token is created by linear projection with an added positional embedding. The sequence of tokens is then fed to the frozen ViT encoder f 0 consisting stack of transformer blocks, yielding the embeddings of tokens z 1  i , z 2 i ∈ R T ×d corresponding to i th normal and synthetic anomaly images. The returned embeddings z 1  i , z 2 i ∈ R T ×d are pooled via average pooling to form d-dimensional embeddings, which are fed to an MLP anomaly classifier projection head h (see Fig. 1 for schematic overview). Subsequently, we only train an anomaly classifier projection head h on top of the frozen embeddings to detect synthetic anomalies using the cross-entropy loss l ce as follows:We set the label for the normal image to 0 and 1 otherwise (synthetic anomaly). The above gradient-based optimization produces a trained classifier projection head h 0 . Thus, the whole architecture can be trained with much fewer parameters while making only the classifier projection head specialized at recognizing anomalies without influencing the ViT encoder."
AMAE: Adaptation of Pre-trained Masked Autoencoder for Dual-Distribution Anomaly Detection in Chest X-Rays,2.2,Stage 2-MAE Inter-Discrepancy Adaptation,"The proposed MAE adaptation scheme is inspired by [3] to model the dual distribution of training data. Unlike [3], which treats all unlabeled images similarly, we propose assigning pseudo-labels to unlabeled images and formulating anomaly detection by measuring the distribution discrepancy between normal and pseudo-labeled abnormal images from unlabeled sets. We use a pre-trained anomaly classifier (Stage 1) to assign pseudo labels to unlabeled images. To begin with, for each unlabeled image x u ∼ T u , we consider the anomaly detection model's confidence from Stage 1 (h 0 • f 0 (x u )). Those images on which the model is highly confident (normal or abnormal) are treated as reliable images used for adaptation. For this purpose, we collect all output probabilities and opt for a threshold per class t c corresponding to each class's top K-th percentile (K = 50) of all given confidence values. Those unlabeled images deemed reliable yield two subsets: a subset of pseudo-labeled normal images T un and a subset of pseudo-labeled abnormal images T ua . We then utilize two MAE-based modules, Module A and Module B (see Fig. 2, Top), using the same MAE architecture and pixel-wise mean squared error (MSE) optimization in [11]. Within each module, the input patches for each image are masked out using a set of L random masks m (j) ∈ {0, 1}in which a different small subset of the patches (ratio of 25%) is retained each time to be fed to the ViT encoder f . The lightweight ViT decoder g receives unmasked patches' embeddings and adds learnable masked tokens to replace the masked-out patches. Subsequently, the full set of embeddings of visible patches and masked tokens with added positional embeddings to all tokens is processed by the ViT decoder g to reconstruct the missing patches of each image in pixels. This yields the reconstructed image x(j) = g • f m (j) (x) , which is then compared against the input image x to optimize both ViT encoder All pixels in the t th patch of both input image and reconstructed images are multiplied by m (j) t ∈ {0, 1}. The above self-supervised loss term averages L pixel-wise mean squared errors for each image. Module A is trained on a combination of the normal training set T n and pseudo-labeled normal images from the unlabeled set T un . In contrast, Module B is trained using only pseudo-labeled abnormal images from an unlabeled set T ua . Optimization for Eq. 3 always starts from pre-trained f 0 and g 0 , and we reset the MAE weights to f 0 and g 0 before training each module. A high discrepancy between the reconstruction outputs of the two modules can indicate potential abnormal regions. Similar to the training stage, we apply L random masks to the test image x t ∼ T test to obtain L reconstructions (see Fig. 2, Bottom). Thus, the anomaly score based on the inter-discrepancy of the two MAE modules is computed as follows:where p is the index of pixels, μA and μB are the mean maps of L reconstructed images from Module A and Module B, respectively. The pixel-level anomaly scores for each image are averaged, yielding the image-level anomaly score."
AMAE: Adaptation of Pre-trained Masked Autoencoder for Dual-Distribution Anomaly Detection in Chest X-Rays,3,Experiments,"Datasets. We evaluated our method on three public CXR datasets: 1) the RSNA Pneumonia Detection Challenge dataset 1 , 2) the VinBigData Chest X-ray Abnormalities Detection Challenge dataset (VinDrCXR) 2 [15], and a subset of 3) the curated NIH dataset (NIH-CXR) 3 [21,25], by including only posteroanterior view images of both male and female patients aged over 18. We show a summary of each dataset's repartitions in Table 1. Except for NIH, where we use only the normal set T n (OCC setting), for the other two datasets, we utilize both T n and T u and exact repartition files from [3] for model training.1 https://www.kaggle.com/c/rsna-pneumonia-detection-challenge. Implementation Details. We adopt AdamW [13] optimizer and set the learning rate (lr) and batch size to 2.5e-4 and 16, where we linearly warm up the lr for the first 20 epochs and decay it following a cosine schedule thereafter till 150 epochs. We follow the exact recipe as [27] for other hyperparameters (see Supplementary Material). The number of generated masks L per image is set to 2 and 4 for the adaptation and test stages (see ablation in Supplementary Material). We use PyTorch 1.9 [16] and train each model on a single GeForce RTX 2080 Ti GPU. We use the area under the ROC curve (AUC) and average precision (AP) for the evaluation metrics.Comparison with SOTA Methods. Table 2 compares AMAE with a comprehensive collection of SOTA methods, including self-supervised synthetic anomaly and reconstruction (Rec.) based methods using their official codes and under two experimental protocols. We use the Y protocol to indicate if access to the unlabeled images is possible in which an AR of 60% of T u is assumed in the experiments; otherwise, we use N. Under protocol N (OCC setting), except for VinDr-CXR, AMAE-Stage 1 achieves SOTA results on two CXR benchmarks, demonstrating the effectiveness of pre-trained ViT using MAE and synthetic anomalies. In particular, AMAE-Stage 1 surpasses the best-performing synthetic anomaly-based method, AnatPaste [19], with the same synthesis approach as ours but using ResNet18 as a feature extractor. ), e.g., improved AUC from 89% to 92% on AR=80%, suggesting high-quality pseudo-labeled images. We also analyze the discriminative capability of our adaptation with and without pseudo labeling by levering all unlabeled images in Module B. We utilize the χ 2-distance between the histograms of anomaly scores (AS) of normal and abnormal images in the RSNA test set (Fig. 3 (b)), showing a more substantial discriminative capability of incorporating pseudo labeling (improved χ 2-distance from 38.53 to 58.37). Finally, the ViT encoder obtained by MAE pre-training (Stage 1) surpasses DenseNet-121 (DN121), either pre-trained by MAE [27] or an advanced contrastive learning method (MoCo v2 [5]) on the RSNA dataset (Fig. 3 (c))."
AMAE: Adaptation of Pre-trained Masked Autoencoder for Dual-Distribution Anomaly Detection in Chest X-Rays,4,Conclusion,"We present AMAE, an adaptation strategy of the pre-trained MAE for dual distribution anomaly detection in CXRs, which makes our method capable of more effectively apprehending anomalous features from unlabeled images. Experiments on the three CXR benchmarks demonstrate that AMAE is generalizable to different model architectures, achieving SOTA performance. As for the limitation, an adequate number of normal training images is still required, and we will extend our pseudo-labeling scheme in our future work for robust anomaly detection bypassing any training annotations."
AMAE: Adaptation of Pre-trained Masked Autoencoder for Dual-Distribution Anomaly Detection in Chest X-Rays,,Fig. 1 .,
AMAE: Adaptation of Pre-trained Masked Autoencoder for Dual-Distribution Anomaly Detection in Chest X-Rays,,Fig. 2 .,
AMAE: Adaptation of Pre-trained Masked Autoencoder for Dual-Distribution Anomaly Detection in Chest X-Rays,,Fig. 3 .,
AMAE: Adaptation of Pre-trained Masked Autoencoder for Dual-Distribution Anomaly Detection in Chest X-Rays,,Table 1 .,"* , g * = arg min f,g"
AMAE: Adaptation of Pre-trained Masked Autoencoder for Dual-Distribution Anomaly Detection in Chest X-Rays,,Table 2 .,2 https://www.kaggle.com/c/vinbigdata-chest-xray-abnormalities-detection.3 https://nihcc.app.box.com/v/ChestXray-NIHCC/file/371647823217.
AMAE: Adaptation of Pre-trained Masked Autoencoder for Dual-Distribution Anomaly Detection in Chest X-Rays,,,"inter )) performs favorably against the SOTA method (DDAD[3]), and AMAE without adaptation (Stage 1)."
AMAE: Adaptation of Pre-trained Masked Autoencoder for Dual-Distribution Anomaly Detection in Chest X-Rays,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0_19.
Unsupervised Domain Adaptation for Anatomical Landmark Detection,1,Introduction,"Anatomical landmark detection is a fundamental step in many clinical applications such as orthodontic diagnosis [11] and orthognathic treatment planning [6]. However, manually locating the landmarks can be tedious and time-consuming. And the results from manual labeling can cause errors due to the inconsistency in landmark identification [5]. Therefore, it is of great need to automate the task of landmark detection for efficiency and consistency.In recent years, deep learning based methods have achieved great progresses in anatomical landmark detection. For supervised learning, earlier works [6,20,27] adopted heatmap regression with extra shape constraints. Later, graph network [16] and self-attention [11] were introduced to model landmark dependencies in an end-to-end manner for better performance.Despite the success of recent methods, they mostly focus on single-domain data, which assume the training and test sets follow the same distribution. However, such an assumption is not always true in practice, due to the differences in patient populations and imaging devices. Figure 1 shows that cephalogram images from two domains can be very different in both histogram and visual appearance. Therefore, a well trained model may encounter severe performance degradation in practice due to the domain shift of test data. A straightforward solution to this issue is to largely increase the size and diversity of training set, but the labeling is prohibitively expensive, especially for medical images. On the other hand, unsupervised domain adaptation (UDA) [10] aims to transfer the knowledge learned from the labeled source domain to the unlabeled target domain, which is a potential solution to the domain shift problem as unlabeled data is much easier to collect. The effectiveness of UDA has been proven in many vision tasks, such as image classification [10], object detection [7], and pose estimation [3,19,24]. However, its feasibility in anatomical landmark detection still remains unknown.In this paper, we aim to investigate anatomical landmark detection under the setting of UDA. Our preliminary experiments show that a well-performed model will yield significant performance drop on cross-domain data, where the mean radial error (MRE) increases from 1.22 mm to 3.32 mm and the success detection rate (SDR) within 2 mm drops from 83.76% to 50.05%. To address the domain gap, we propose a unified framework, which contains a base landmark detection model, a self-training strategy, and a domain adversarial learning module. Specifically, self-training is adopted to effectively leverage the unlabeled data from the target domain via pseudo-labels. To handle confirmation bias [2], we propose landmark-aware self-training (LAST) to select pseudo-labels at the landmark-level with dynamic thresholds. Furthermore, to address the covariate shift [26] issue (i.e., unaligned data distribution) that may degrade the performance of self-training, a domain adversarial learning (DAL) module is designed to learn domain-invariant features via adversarial training. Our experiments on two anatomical datasets show the effectiveness of the proposed framework. For example, on cephalometric landmark detection, it reduces the domain gap in MRE by 47% (3.32 mm → 1.75 mm) and improves the SDR (2 mm) from 50.05% to 69.15%. We summarize our contributions as follows. 1. We investigated anatomical landmark detection under the UDA setting for the first time, and showed that domain shift indeed causes severe performance drop of a well-performed landmark detection model. 2. We proposed a novel framework for the UDA of anatomical landmark detection, which significantly improves the cross-domain performance and consistently outperforms other state-of-the-art UDA methods."
Unsupervised Domain Adaptation for Anatomical Landmark Detection,2,Method,"Figure 2 shows the overall framework, which aims to yield satisfactory performance in target domain under the UDA setting. During training, it leverages both labeled source domain data S = {x S i , y S i } N i=1 and unlabeled target domain data T = {x T j } M j=1 . For evaluation, it will be tested on a hold-out test set from target domain. The landmark detection model is able to predict landmarks with confidence, which is detailed in Sect. 2.1. To reduce domain gap, we further propose LAST and DAL, which are introduced in Sects. 2.2 and 2.3, respectively."
Unsupervised Domain Adaptation for Anatomical Landmark Detection,2.1,Landmark Detection Model,"Recently, coordinate regression [11,16] has obtained better performance than heatmap regression [6,27]. However, coordinate based methods do not output confidence scores, which are necessary for pseudo-label selection in self-training [4,14]. To address this issue, we designed a model that is able to predict accurate landmarks while providing confidence scores. As shown in Fig. 3 (a), the model utilizes both coordinate and heatmap regression, where the former provides coarse but robust predictions via global localization, then projected to the local maps of the latter for prediction refinement and confidence measurement.Global Localization. We adopt Transformer decoder [12,15] for coarse localization due to its superiority in global attentions. A convolutional neural network (CNN) is used to extract feature f ∈ R C×H×W , where C, H, and W represents number of channels, map height and width, respectively. By using f as memory, the decoder takes landmark queries q ∈ R L×C as input, then iteratively updates them through multiple decoder layers, where L is the number of landmarks. Finally, a feed-forward network (FFN) converts the updated landmark queries to coordinates ŷc ∈ R L×2 . The loss function L coord is defined to be the L1 loss between the predicted coordinate ŷc and the label y c .Local Refinement. This module outputs a score map ŷs ∈ R L×H×W and an offset map ŷo ∈ R 2L×H×W via 1 × 1 convolutional layers by taking f as input. The score map indicates the likelihood of each grid to be the target landmark, while the offset map represents the relative offsets of the neighbouring grids to the target. The ground-truth (GT) landmark of the score map is smoothed by a Gaussian kernel [23], and L2 loss is used for loss function L score . Since the offset is a regression problem, L1 is used for loss L offset , and only applied to the area where its GT score is larger than zero. During inference, different from [6,18,23], the optimal local grid is not selected by the maximum score of ŷs , but instead the projection of the coordinates from global localization. Then the corresponding offset value is added to the optimal grid for refinement. Also, the confidence of each prediction can be easily obtained from the score map via projection.The loss function of the landmark detection model can be summarized aswhere S is source domain data, λ s and λ o are balancing coefficients. Empirically, we set λ s = 100 and λ o = 0.02 in this paper."
Unsupervised Domain Adaptation for Anatomical Landmark Detection,2.2,Landmark-Aware Self-training,"Self-training [14] is an effective semi-supervised learning (SSL) method, which iteratively estimates and selects reliable pseudo-labeled samples to expand the labeled set. Its effectiveness has also been verified on several vision tasks under the UDA setting, such as object detection [7]. However, very few works explored self-training for the UDA of landmark detection, but mostly restricted to the paradigm of SSL [9,21]. Since UDA is more challenging than SSL due to domain shift, reliable pseudolabels should be carefully selected to avoid confirmation bias [2]. Existing works [9,19,21] follow the pipeline of image classification by evaluating reliability at the image-level, which we believe is not representative because the landmarks within an image may have different reliabilities (see Fig. 3 (b)). To avoid potential noisy labels caused by the image-level selection, we propose LAST, which selects reliable pseudo-labels at the landmark-level. To achieve this, we use a binary mask m ∈ {0, 1} L to indicate the reliability of each landmark for a given image, where value 1 indicates the label is reliable and 0 the opposite. To decide the reliability of each landmark, a common practice is to use a threshold τ , where the l-th landmark is reliable if its confidence score s l > τ. During loss calculation, each loss term is multiplied by m to mask out the unreliable landmark-level labels. Thus, the loss for LAST iswhere M represents the mask operation, T = {x T j , y, and y T is the estimated pseudo-labels from the last self-training round. Note that the masks of the source domain data S always equal to one as they are ground truths. However, the landmark-level selection leads to unbalanced pseudo-labels between landmarks, as shown in Fig. 3 (c). This is caused by the fixed threshold τ in self-training, which cannot handle different landmarks adaptively. To address this issue, we introduce percentile scores [4] to yield dynamic thresholds (DT) for different landmarks. Specifically, for the l-th landmark, when the pseudolabels are sorted based on confidence (high to low), τ l r is used as the threshold, which is the confidence score of r-th percentile. In this way, the selection ratio of pseudo-labels can be controlled by r, and the unbalanced issue can be addressed by using the same r for all the landmarks. We set the curriculum to be r = Δ • t, where t is the t-th self-training round and Δ is a hyperparameter that controls the pace. We use Δ = 20%, which yields five training rounds in total."
Unsupervised Domain Adaptation for Anatomical Landmark Detection,2.3,Domain Adversarial Learning,"Although self-training has been shown effective, it inevitably contains bias towards source domain because its initial model is trained with source domain data only. In other words, the data distribution of target domain is different from the source domain, which is known as covariate shift [26]. To mitigate it, we introduce DAL to align the distribution of the two by conducting an adversarial training between a domain classifier and the feature extractor. Specifically, the feature f further goes through a global average pooling (GAP) and a fully connected (FC) layer, then connects to a domain classifier D to discriminate the source of input x. The classifier can be trained with binary cross-entropy loss:where d is domain label, with d = 0 and d = 1 indicating the images are from source and target domain, respectively. The domain classifier is trained to minimize L D , while the feature extractor F is encouraged to maximize it such that the learned feature is indistinguishable to the domain classifier. Thus, the adversarial objective function can be written asTo simplify the optimization, we adopt gradient reversal layer (GRL) [10] to mimic the adversarial training, which is placed right after the feature extractor. During backpropagation, GRL negates the gradients that pass back to the feature extractor F so that F is actually maximized. In this way, the adversarial training can be done via the minimization of L D , i.e., L DAL = L D . Finally, we have the overall loss function as follows:where λ D is a balancing coefficient."
Unsupervised Domain Adaptation for Anatomical Landmark Detection,3,Experiments,
Unsupervised Domain Adaptation for Anatomical Landmark Detection,3.1,Experimental Settings,"In this section, we present experiments on cephalometric landmark detection. See lung landmark detection in Appendix A.Source Domain. The ISBI 2015 Challenge provides a public dataset [22], which is widely used as a benchmark of cephalometric landmark detection. It contains 400 images in total, where 150 images are for training, 150 images are Test 1 data, and the remaining are Test 2. Each image is annotated with 19 landmarks by two experienced doctors, and the mean values of the two are used as GT. In this paper, we only use the training set as the labeled source domain data.Target Domain. The ISBI 2023 Challenge provides a new dataset [13], which was collected from 7 different imaging devices. By now, only the training set is released, which contains 700 images. For UDA setting, we randomly selected 500 images as unlabeled target domain data, and the remaining 200 images are for evaluation. The dataset provides 29 landmarks, but we only use 19 of them, i.e., the same landmarks as the source domain [22]. Following previous works [11,16], all the images are resized to 640 × 800. For evaluation metric, we adopt MRE and SDR within four radius (2 mm, 2.5 mm, 3 mm, and 4 mm). "
Unsupervised Domain Adaptation for Anatomical Landmark Detection,3.2,Results,"For the comparison under UDA setting, several state-of-the-art UDA methods were implemented, including FDA [25], UMT [8], SAC [1], and AT [17]. Additionally, the base model trained with source domain data only is included as the lower bound, and the model trained with equal amount of labeled target domain data is used as the upper bound. "
Unsupervised Domain Adaptation for Anatomical Landmark Detection,3.3,Model Analysis,"We first do ablation study to show the effectiveness of each module, which can be seen in Table 2. The baseline model simply uses vanilla self-training [14] for  To show the superiority of our base model, we replace it by standard heatmap regression [23] (HM), which obtains degraded results in both MRE and SDR. Furthermore, we conduct analysis on subdomain discrepancy, which shows the effectiveness of our method on each subdomain (see Appendix B)."
Unsupervised Domain Adaptation for Anatomical Landmark Detection,3.4,Qualitative Results,"Figure 4 shows the qualitative results of the source-only base model, AT [17], and our method on target domain test data. The green dots are GTs, and red dots are predictions. It can be seen from the figure that our model makes better predictions than the other two (see yellow rectangles). We also notice that some landmarks are quite challenging, where all the three fail to give accurate predictions (see cyan rectangles)."
Unsupervised Domain Adaptation for Anatomical Landmark Detection,4,Conclusion,"In this paper, we investigated anatomical landmark detection under the UDA setting. To mitigate the performance drop caused by domain shift, we proposed a unified UDA framework, which consists of a landmark detection model, a self-training strategy, and a DAL module. Based on the predictions and confidence scores from the landmark model, a self-training strategy is proposed for domain adaptation via landmark-level pseudo-labels with dynamic thresholds. Meanwhile, the model is encouraged to learn domain-invariant features via adversarial training so that the unaligned data distribution can be addressed. We constructed a UDA setting based on two anatomical datasets, where the experiments showed that our method not only reduces the domain gap by a large margin, but also outperforms other UDA methods consistently. However, a performance gap still exists between the current UDA methods and the supervised model in target domain, indicating more effective UDA methods are needed to close the gap."
Unsupervised Domain Adaptation for Anatomical Landmark Detection,,Fig. 1 .,
Unsupervised Domain Adaptation for Anatomical Landmark Detection,,Fig. 2 .,
Unsupervised Domain Adaptation for Anatomical Landmark Detection,,Fig. 3 .,
Unsupervised Domain Adaptation for Anatomical Landmark Detection,,Fig. 4 .,
Unsupervised Domain Adaptation for Anatomical Landmark Detection,,Table 1 .,
Unsupervised Domain Adaptation for Anatomical Landmark Detection,,,
Unsupervised Domain Adaptation for Anatomical Landmark Detection,,Table 2 .,
Unsupervised Domain Adaptation for Anatomical Landmark Detection,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0_66.
"CT-Guided, Unsupervised Super-Resolution Reconstruction of Single 3D Magnetic Resonance Image",1,Introduction,"High-resolution magnetic resonance (MR) images (MRI) provide a wealth of structural details, which facilitate early and precise diagnosis [1]. However, images obtained in clinical practice are anisotropic due to the limitation of scanThis study was partially supported by the National Natural Science Foundation of China via project U20A20199.time and signal-noise ratio [2]. In order to speed up clinical scanning procedures, only a limited number of two-dimensional (2D) slices are acquired, despite the fact that the interested anatomical structures are in three-dimensional (3D). The acquired medical images have low inter-plane resolution, i.e., large spacing between slices. Such anisotropic images will lead to misdiagnosis and can greatly impact the performance of various clinical tasks, including computer-aided diagnosis and computer-assisted interventions. Therefore, we investigate the problem of reducing the slice spacing [3] via super-resolution (SR) reconstruction. Specifically, we refer to the image with large slice spacing as a low-resolution (LR) image and the image with small slice spacing as a high-resolution (HR) image. Our goal is to reconstruct the HR image from the LR input, which is an ill-posed inverse problem and presents significant challenges.Deep learning-based algorithms for single MR image super-resolution show great potential in restoration of HR images from LR inputs [4]. Pham et al. [5] proposed the SRCNN method, which applied convolutional neural networks (CNN) to image super-resolution of MRI and achieved a better performance than the conventional methods, such as B-spline interpolation and low-rank total variation (LRTV) [6] method. Chaudhariet al. [7] proposed a 3D residual network, which learned the residual-based transformations between paired LR and HR images for the SR reconstruction of MRI. Chen et al. [8] proposed a densely connected super-resolution network (DCSRN), which reused the block features through the dense connection in the SR reconstruction of MRI. Chen et al. [9] extended this work by using generative adversarial network (GAN) [10] in SR reconstruction of MRI in order to improve the realism of the recovered images. Feng et al. [11,12] proposed a multi-contrast MRI SR method, which aimed to learn clearer anatomical structure and edge information with the help of auxiliary contrast MRI. Despite significant progress, however, there are still spaces for further improvement. Most networks require a large amount of paired LR and HR MR images for training, which are unrealistic in clinical practice. To address the challenge of organizing paired images, methods based on unpaired images have been proposed [13,14]. However, HR MR images are still difficult to obtain, as acquiring HR MR images in clinical settings requires a significant amount of time. In contrast, CT images are acquired in clinical routine. Therefore, it is of great significance to use HR CT images as a guidance to synthesize HR MR images from LR MR images.To this end, we propose a CT-guided, unsupervised MRI super-resolution reconstruction method based on joint cross-modality image translation (CIT) and super-resolution reconstruction, eliminating the requirement of HR MR images for training. Specifically, our network design features a super-resolution Network (SRNet) and a cross-modality image translation network (CITNet) based on disentanged representation learning. After pretraining, the SRNet can generate pseudo HR MR images from LR MR images. The generated pseudo HR MR images are then taken together with the HR CT images as the input to the CITNet, which can generate quality-improved pseudo HR MR images by combining disentangled content code of the input CT data with the attribute  "
"CT-Guided, Unsupervised Super-Resolution Reconstruction of Single 3D Magnetic Resonance Image",2.1,Super-Resolution Network (SRNet),"We choose to use the residual dense network (RDN) as the SRNet. The RDN utilizes cascaded residual dense blocks (RDBs), a powerful convolutional block that leverages residual and dense connections to fully aggregate hierarchical features.For further details on the structure of the RDN, please refer to the original paper [15]. Mathematically, we denote the SRNet as F s (•; Θ s ) with trainable parameters Θ s ."
"CT-Guided, Unsupervised Super-Resolution Reconstruction of Single 3D Magnetic Resonance Image",2.2,Cross-Modality Image Translation Network (CITNet),"The CITNet is inspired by MUNIT [16]. As depicted in Fig. 1The encoder in each domain disentangles an input image separately into a domain-invariant content space C and a domain-specific attribute space A. And the generator networks combine a content code with an attribute code to generate translated images in the target domain. For instance, when translating CT image y H ∈ Y to MR image x H ∈ X , we first randomly sample from the prior distribution p(A x ) ∼ N (0, I) to obtain an MRI attribute code A x , which is empirically set as a 8-bit vector. We then combine A x with the disentangled content code of the CT imageand A y is also sampled from the prior distribution p(A y ) ∼ N (0, I).Disentangled Representation Learning. Cross-modality image translation is based on disentangled representation learning, trained with self-and crosscycle reconstruction losses. As shown in Fig. 1-(C.1, C.2), the self-reconstruction loss L self is utilized to regularize the training when the content and attribute code originate from the same domain, whereas the cross-cycle consistency loss L cycle is used when the content and attribute code come from different domains. The self-reconstruction and cross-cycle reconstruction losses are defined as follows:whereSpecially, in the cross-cycle translation processes, we employe a latent reconstruction loss to maintain the invertible mapping between the image and the latent space. In details, we have:We further use pretrained vgg16 network, denoted as φ(•), to extract highlevel features for computing the perceptual loss [17]:where C, H, W indicate the channel number and the image size, respectively. Adversarial Learning. As shown in Fig. 1-(A.2), we use GAN [10] to learn the translation between MR and CT image domains better. A GAN typically contains a generation network and a discrimination network. We use the discriminator D X to judge whether the image is from MR image domain, and the discriminator D Y to judge whether the image is from CT image domain. The auto-encoders try to generate the image of the target domain to fool the discriminators so that the distribution of the translated images can match that of the target images. The minmax game is trained by:Joint Optimization. The SRNet and the CITNet are jointly optimized by minimizing following loss function:where λ 1 , λ 2 , and λ 3 are parameters controlling the relative weights of different losses."
"CT-Guided, Unsupervised Super-Resolution Reconstruction of Single 3D Magnetic Resonance Image",2.3,Training Strategy,"Empirically, we found that training the network shown in Fig. 1-(A) end to end did not converge. We thus design the following three-stage training strategy."
"CT-Guided, Unsupervised Super-Resolution Reconstruction of Single 3D Magnetic Resonance Image",,Stage 1. Let's denote the downsampling function as D(•).,"In this stage, we pretrain the SRNet using the HR CT images, as shown in Fig. 1-( further pretrain the SRNet with pseudo MR images, as shown in Fig. 1-(B.2), for another T iterations. At each iteration, we first sample a batch of LR MR images x L and input them into the SRNet to get the pseudo HR MR images xH = F s (x L ; Θ s ). We then downsample xH to get corresponding pseudo LR MR images xL = D(x H ). The SRNet is trained with the paired pseudo LR-HR MR images by minimizing L1 loss F s (x L ; Θ s ) -F s (D(F s (x L ; Θ s )); Θ s ) 1 . The idea behind such a pretraining stategy is that since both CT and MR images share the common structural information, the model pretrained with CT images in stage 1 facilitates the super-resolution reconstruction of pseudo HR MR images in stage 2. On the other hand, the training done in stage 2 can help the SRNet to learn MRI-specific domain information.Stage 3. The MR images generated by the model pretrained at the first two stages can be further improved. In stage 3, we conduct joint optimization of the SRNet and the CITNet as shown in Fig. 1-(C), for another 8 × T iterations. At each iteration, we first trainand the SRNet by minimizing L disentangle as defined in Eq. (7).The training procedure of our method is illustrated by Algorithm 1. Implementation Details. To train the proposed network, each training sample is unpaired LR MRI and HR CT images. All images are normalized to the range between -1.0 and 1.0. Optimization is performed using Adam with a batch size of 1. The initial learning rate is set to 0.0001 and decreased by a factor of 5 every 2 epochs. We empirically set λ 1 = 10, λ 2 = λ 3 = 1 and T = 100, 000.Table 1. The mean and the standard deviation when the proposed method was compared with the state-of-the-art (SOTA) unsupervised [18][19][20] and supervised [15,21] methods on both datasets. Paired T-Tests of all evaluation metrics achieved by ours and other methods are all smaller than 0.0001.    Experimental Results. We compare our method with the conventional algorithm bicubic interpolation, and the state-of-the-art (SOTA) unsupervised SR methods including TSCN [18], ZSSR [19], SMORE [20] as well as the SOTA supervised methods including RDN [15] and ReconResNet [21]. Well-established metrics including Peak Signal-to-Noise Ratio (PSNR) [22,23], Structural Similarity Index Metrics (SSIM) [24], and Learned Perceptual Image Patch Similarity (LPIPS) [25] are used to assess the performance of different methods.Table 1 shows the mean and the standard deviation of the evaluation results of each method on both datasets. Figure 2 and Fig. 3 respectively show the superresolution results on data from Site1 and Site2, when the scale factor is set as K = 4, as well as the corresponding LR and ground truth (GT) images. Both qualitative and quantitative results demonstrated that our method achieved better results than other SOTA unsupervised SR methods. It achieved comparable performance when compared with the supervised SR methods.Our method is trained in two pretrain stages and one joint optimization stage. We thus conduct ablation study on dataset from Site1 to analyze the quality of the generated pseudo HR MR images at each stage. As shown in Table 2, quantitatively, the quality of the generated pseudo HR MR images is become better and better, demonstrating the effectiveness of the training strategy."
"CT-Guided, Unsupervised Super-Resolution Reconstruction of Single 3D Magnetic Resonance Image",4,Conclusion,"In this paper, we proposed a CT-guided, unsupervised MRI super-resolution reconstruction method based on joint cross-modality image translation and super-resolution reconstruction, eliminating the requirement of HR MRI for training. We conducted experiments on two datasets respectively acquired from two different clinical centers to validate the effectiveness of the proposed method. Quantitatively and qualitatively, the proposed method achieved superior performance over the SOTA unsupervised SR methods."
"CT-Guided, Unsupervised Super-Resolution Reconstruction of Single 3D Magnetic Resonance Image",,Fig. 1 .,
"CT-Guided, Unsupervised Super-Resolution Reconstruction of Single 3D Magnetic Resonance Image",,2 MethodologyFigure 1,
"CT-Guided, Unsupervised Super-Resolution Reconstruction of Single 3D Magnetic Resonance Image",,Fig. 2 .,
"CT-Guided, Unsupervised Super-Resolution Reconstruction of Single 3D Magnetic Resonance Image",,Fig. 3 .,
"CT-Guided, Unsupervised Super-Resolution Reconstruction of Single 3D Magnetic Resonance Image",,Fig. 4 .,
"CT-Guided, Unsupervised Super-Resolution Reconstruction of Single 3D Magnetic Resonance Image",,,"Y , GX , GY and SRNet by minimizing L disentangle END FOR"
"CT-Guided, Unsupervised Super-Resolution Reconstruction of Single 3D Magnetic Resonance Image",,Table 2 .,
Multi-scale Cross-restoration Framework for Electrocardiogram Anomaly Detection,1,Introduction,"The electrocardiogram (ECG) is a monitoring tool widely used to evaluate the heart status of patients and provide information on cardiac electrophysiology. Developing automated analysis systems capable of detecting and identifying abnormal signals is crucial in light of the importance of ECGs in medical diagnosis and the need to ease the workload of clinicians. However, training a classifier on labeled ECGs that focus on specific diseases may not recognize new abnormal statuses that were not encountered during training, given the diversity and rarity of cardiac diseases [8,16,23]. On the other hand, anomaly detection, which is trained only on normal healthy data, can identify any potential abnormal status and avoid the failure to detect rare cardiac diseases [10,17,21].The current anomaly detection techniques, including one-class discriminative approaches [2,14], reconstruction-based approaches [15,30], and self-supervised learning-based approaches [3,26], all operate under the assumption that models trained solely on normal data will struggle to process anomalous data and thus the substantial drop in performance presents an indication of anomalies. While anomaly detection has been widely used in the medical field to analyze medical images [12,24] and time-series data [18,29], detecting anomalies in ECG data is particularly challenging due to the substantial inter-individual differences and the presence of anomalies in both global rhythm and local morphology. So far, few studies have investigated anomaly detection in ECG [11,29]. TSL [29] uses expert knowledge-guided amplitude-and frequency-based data transformations to simulate anomalies for different individuals. BeatGAN [11] employs a generative adversarial network to separately reconstruct normalized heartbeats instead of the entire raw ECG signal. While BeatGAN alleviates individual differences, it neglects the important global rhythm information of the ECG.This paper proposes a novel multi-scale cross-restoration framework for ECG anomaly detection and localization. To our best knowledge, this is the first work to integrate both local and global characteristics for ECG anomaly detection. To take into account multi-scale data, the framework adopts a two-branch autoencoder architecture, with one branch focusing on global features from the entire ECG and the other on local features from heartbeat-level details. A multiscale cross-attention module is introduced, which learns to combine the two feature types for making the final prediction. This module imitates the diagnostic process followed by experienced cardiologists who carefully examine both the entire ECG and individual heartbeats to detect abnormalities in both the overall rhythm and the specific local morphology of the signal [7]. Each of the branches employs a masking and restoration strategy, i.e., the model learns how to perform temporal-dependent signal inpainting from the adjacent unmasked regions within a specific individual. Such context-aware restoration has the advantage of making the restoration less susceptible to individual differences. During testing, anomalies are identified as samples or regions with high restoration errors.To comprehensively evaluate the performance of the proposed method on a large number of individuals, we adopt the public PTB-XL database [22] with only patient-level diagnoses, and ask experienced cardiologists to provide signal point-level localization annotations. The resulting dataset is then introduced as a large-scale challenging benchmark for ECG anomaly detection and localization. The proposed method is evaluated on this challenging benchmark as well as on two traditional ECG anomaly detection benchmarks [6,13]. The experimental results have shown that the proposed method outperforms several stateof-the-art methods for both anomaly detection and localization, highlighting its potential for real-world clinical diagnosis."
Multi-scale Cross-restoration Framework for Electrocardiogram Anomaly Detection,2,Method,"In this paper, we focus on unsupervised anomaly detection and localization on ECGs, training based on only normal ECG data. Formally, given a set of N normal ECGs denoted as {x i , i = 1, ..., N }, where x i ∈ R D represents the vectorized representation of the i-th ECG consisting of D signal points, the objective is to train a computational model capable of identifying whether a new ECG is normal or anomalous, and localize the regions of anomalies in abnormal ECGs."
Multi-scale Cross-restoration Framework for Electrocardiogram Anomaly Detection,2.1,Multi-scale Cross-restoration,"In Fig. 1, we present an overview of our two-branch framework for ECG anomaly detection. One branch is responsible for learning global ECG features, while the other focuses on local heartbeat details. Our framework comprises four main components: (i) masking and encoding, (ii) multi-scale cross-attention module, (iii) uncertainty-aware restoration, and (iv) trend generation module. We provide detailed explanations of each of these components in the following sections.Masking and Encoding. Given a pair consisting of a global ECG signal x g ∈ R D and a randomly selected local heartbeat x l ∈ R d segmented from x g for training, as shown in Fig. 1, we apply two random masks, M g and M l , to mask x g and x l , respectively. To enable multi-scale feature learning, M l is applied to a consecutive small region to facilitate detail restoration, while M g is applied to several distinct regions distributed throughout the whole sequence to facilitate global rhythm restoration. The masked signals are processed separately by global and local encoders, E g and E l , resulting in global feature, where denotes the element-wise product. Multi-scale Cross-attention. To capture the relationship between global and local features, we use the self-attention mechanism [20] on the concatenated feature of f in g and f in l . Specifically, the attention mechanism is expressed asis the square root of the feature dimension used as a scaling factor. Self-attention is achieved by settingThe cross-attention feature, f ca , is obtained from the self-attention mechanism, which dynamically weighs the importance of each element in the combined feature. To obtain the final outputs of the global and local features, f out g and f out l , containing cross-scale information, we consider residual connections: ). An uncertainty-aware restoration loss is used to incorporate restoration uncertainty into the loss functions,where for each function, the first term is normalized by the corresponding uncertainty, and the second term prevents predicting a large uncertainty for all restoration pixels following [12]. The superscript k represents the position of the k-th element of the signal. It is worth noting that, unlike [12], the uncertainty-aware loss is used for restoration, but not for reconstruction.Trend Generation Module. The trend generation module (TGM) illustrated in Fig. 1  The restoration loss is defined as the Euclidean distance between x g and xt ,This process guides global feature learning using time-series trend information, emphasizing rhythm characteristics while de-emphasizing morphological details.Loss Function. The final loss function for optimizing our model during the training process can be written aswhere α and β are trade-off parameters weighting the loss function. For simplicity, we adopt α = β = 1.0 as the default."
Multi-scale Cross-restoration Framework for Electrocardiogram Anomaly Detection,2.2,Anomaly Score Measurement,"For each test sample x, local ECGs from the segmented heartbeat set {x l,m , m = 1, ..., M } are paired with the global ECG x g one at a time as inputs. The anomaly score A(x) is calculated to estimate the abnormality,where the three terms correspond to global restoration, local restoration, and trend restoration, respectively. For localization, an anomaly score map is generated in the same way as Eq. ( 3), but without summing over the signal points.The anomalies are indicated by relatively large anomaly scores, and vice versa."
Multi-scale Cross-restoration Framework for Electrocardiogram Anomaly Detection,3,Experiments,"Datasets. Three publicly available ECG datasets are used to evaluate the proposed method, including PTB-XL [22], MIT-BIH [13], and Keogh ECG [6].-  [4] 2021 0.695 0.611 0.790 0.674 0.458 0.648 0.671 0.650 CAE-M [28] 2021 0.657 0.618 0.802 0.715 0.457 0.708 0.671 0.661 TranAD [18] 2022 0.647 0.623 0.820 0.720 0.446 0.780 0.680 0.674 AnoTran [25] 2022 0.739 0.502 0.792 0.799 0.498 0.748 0.711 0.684 BeatGAN [11]  Evaluation Protocols. The performance of anomaly detection and localization is quantified using the area under the Receiver Operating Characteristic curve (AUC), with a higher AUC value indicating a better method. To ensure comparability across different annotation levels, we used patient-level, heartbeat-level, and signal point-level AUC for each respective setting. For heartbeat-level classification, the F1 score is also reported following [11].Implementation Details. The ECG is pre-processed by a Butterworth filter and Notch filter [19] to remove high-frequency noise and eliminate ECG baseline wander. The R-peaks are detected with an adaptive threshold following [5], which does not require any learnable parameters. The positions of the detected R-peaks are then used to segment the ECG sequence into a set of heartbeats. We use a convolutional-based autoencoder, following the architecture proposed in [11]. The model is trained using the AdamW optimizer with an initial learning rate of 1e-4 and a weight decay coefficient of 1e-5 for 50 epochs on a single NVIDIA GTX 3090 GPU, with a single cycle of cosine learning rate used for decay scheduling. The batch size is set to 32. During testing, the model requires 2365M GPU memory and achieves an inference speed of 4.2 fps."
Multi-scale Cross-restoration Framework for Electrocardiogram Anomaly Detection,3.1,Comparisons with State-of-the-Arts,"We compare our method with several time-series anomaly detection methods, including heartbeat-level detection method BeatGAN [11], patient-level detection method TSL [29], and several signal point-level anomaly localization methods [1,4,9,18,25,27,28,30]. For a fair comparison, we re-trained all the methods under the same experimental setup. For those methods originally designed for signal point-level tasks only [1,9,18,25,30], we use the mean value of anomaly localization results as their heartbeat-level or patient-level anomaly scores.Anomaly Detection. The anomaly detection performance on PTB-XL is summarized in Table 1. The proposed method achieves 86.0% AUC in patient-level anomaly detection and outperforms all baselines by a large margin (10.3%). Table 2 displays the comparison results on MIT-BIH, where the proposed method achieves a heartbeat-level AUC of 96.9%, showing an improvement of 2.4% over the state-of-the-art BeatGAN (94.5%). Furthermore, the F1-score of the proposed method is 88.3%, which is 6.7% higher than BeatGAN (81.6%). Anomaly Localization. Table 1 presents the results of anomaly localization on our proposed benchmark for multiple individuals. The proposed method achieves a signal point-level AUC of 74.7%, outperforming all baselines (3.2% higher than BeatGAN). It is worth noting that TSL, which is not designed for localization, shows poor performance in this task. Table 3 shows the signal point-level anomaly localization results for each independent individual on Keogh ECG. Overall, the proposed method achieves the best or second-best performance compared to other methods on six subsets and the highest mean AUC among all subsets (74.9%, 2.5% higher than BeatGAN), indicating its effectiveness. The proposed method shows a lower standard deviation (±10.5) across the seven subsets compared to TranAD (±11.3) and BeatGAN (±11.0), which indicates good generalizability of the proposed method across different subsets.Anomaly Localization Visualization. We present visualization results of anomaly localization on several samples from our proposed benchmark in Fig. 2, with ground truths annotated by experienced cardiologists. Regions with higher anomaly scores are indicated by darker colors. Our proposed method outperforms BeatGAN in accurately localizing various types of ECG anomalies, including both periodic and episodic anomalies, such as incomplete right bundle branch block and premature beats. Our method though provides narrower localization results than ground truths, as it is highly sensitive to abrupt unusual changes in signal values, but still represents the important areas for anomaly identification, a fact confirmed by experienced cardiologists."
Multi-scale Cross-restoration Framework for Electrocardiogram Anomaly Detection,3.2,Ablation Study and Sensitivity Analysis,"Ablation studies were conducted on PTB-XL to confirm the effectiveness of individual components of the proposed method. Table 4 shows that each module contributes positively to the overall performance of the framework. When none of the modules were employed, the method becomes a ECG reconstruction approach with a naive L2 loss and lacks cross-attention in multi-scale data. When individually adding the MR, MC, UL, and TGM modules to the baseline model without any of them, the AUC values improve from 70.4% to 80.4%, 80.3%, 72.8%, and 71.2%, respectively, demonstrating the effectiveness of each module. Moreover, as the modules are added in sequence, the performance improves step by step from 70.4% to 86.0% in AUC, highlighting the combined impact of all modules on the proposed framework.We conduct a sensitivity analysis on the mask ratio, as shown in Table 5. Restoration with a 0% masking ratio can be regarded as reconstruction, which takes an entire sample as input and its target is to output the input sample. Results indicate that the model's performance first improves and then declines as the mask ratio increases from 0% to 70%. This trend is due to the fact that a low mask ratio can limit the model's feature learning ability during restoration, while a high ratio can make it increasingly difficult to restore the masked regions. Therefore, there is a trade-off between maximizing the model's potential and ensuring a reasonable restoration difficulty. The optimal mask ratio is 30%, which achieves the highest anomaly detection result (86.0% in AUC)."
Multi-scale Cross-restoration Framework for Electrocardiogram Anomaly Detection,4,Conclusion,"This paper proposes a novel framework for ECG anomaly detection, where features of the entire ECG and local heartbeats are combined with a maskingrestoration process to detect anomalies, simulating the diagnostic process of cardiologists. A challenging benchmark, with signal point-level annotations provided by experienced cardiologists, is proposed, facilitating future research in ECG anomaly localization. The proposed method outperforms state-of-the-art methods, highlighting its potential in real-world clinical diagnosis."
Multi-scale Cross-restoration Framework for Electrocardiogram Anomaly Detection,,Fig. 1 .,
Multi-scale Cross-restoration Framework for Electrocardiogram Anomaly Detection,,,
Multi-scale Cross-restoration Framework for Electrocardiogram Anomaly Detection,,Fig. 2 .,
Multi-scale Cross-restoration Framework for Electrocardiogram Anomaly Detection,,,g to restore the global ECG xt
Multi-scale Cross-restoration Framework for Electrocardiogram Anomaly Detection,,Table 1 .,"is an anomaly subsequence that corresponds to a pre-ventricular contraction, while the remaining sequence is used as normal data to train the model. The ECGs are partitioned into fixed-length sequences of 320 by a sliding window with a stride of 40 during training and 160 during testing."
Multi-scale Cross-restoration Framework for Electrocardiogram Anomaly Detection,,Table 2 .,
Multi-scale Cross-restoration Framework for Electrocardiogram Anomaly Detection,,Table 3 .,
Multi-scale Cross-restoration Framework for Electrocardiogram Anomaly Detection,,Table 4 .,
Multi-scale Cross-restoration Framework for Electrocardiogram Anomaly Detection,,Table 5 .,
Multi-scale Cross-restoration Framework for Electrocardiogram Anomaly Detection,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0_9.
Multi-modal Variational Autoencoders for Normative Modelling Across Multiple Imaging Modalities,1,Introduction,"Normative modelling is a popular method to study heterogeneous brain disorders. Normative models assume disease cohorts sit at the tails of a healthy population distribution and quantify individual deviations from healthy brain patterns. Typically, a normative analysis constructs a normative model per variable, e.g., using Gaussian Process Regression (GPR) [9]. Recently, to model complex non-linear interactions between features, deep-learning approaches using adversarial (AAE) and variational autoencoder (VAE) models have been proposed [8,11]. These models have a uni-modal structure with a single encoder and decoder network. So far, almost all deep-learning normative models have modelled only one modality. However, many brain disorders show deviations from the norm in features of multiple imaging modalities to a varying degree. Often it is unknown which modality will be the most sensitive. Thus, it is advantageous to develop normative models suitable for multiple modalities.Most previous deep-learning normative and anomaly detection models measure deviations in the feature space [4,7,11]. However, for multi-modal models built from modalities containing highly different, but complementary information (e.g., T1 and DTI features as used here), we may not expect to see significantly greater deviations in the feature space compared to uni-modal methods. Indeed previous work has shown that, when using VAEs, even for one modality, measuring deviation in the latent space outperforms metrics in the feature space [8] and provides a single measure of abnormality. As such, we develop a latent deviation metric suitable to measuring deviations in multi-modal data.There are many approaches to extending VAEs to integrate information from multiple modalities and learn informative joint latent representations. Most multi-modal VAE frameworks learn separate encoder and decoder networks for each modality and aggregate the encoding distributions to learn a joint latent representation. Wu and Goodman [16] introduced a multi-modal VAE (mVAE) where each encoding distribution is treated as an 'expert' and the Productof-Experts (PoE), which takes a product of the experts' densities, is used to approximate a joint encoding distribution. The PoE approach treats all experts as equally credible taking a uniform contribution from every modality. In practice, however, different levels of noise, complexity and information are present in different modalities. Furthermore, if we have an overconfident miscalibrated expert, i.e. a sharp, shifted probability distribution, the joint distribution will have low density in the region observed by the other experts and a biased mean prediction. This can result in a suboptimal latent space and data reconstruction. Shi et al. [13] address this problem by combining latent representations across modalities using a Mixture-of-Experts (MoE) approach. For MoE, the joint distribution is given by a mixture of the experts' densities so that the density is spread over all regions covered by the experts and overconfident experts do not monopolize the resulting prediction. However, MoE is less sensitive to consensus across modalities and will give lower probability to regions where experts are in agreement than PoE. Alternatively, we propose a mVAE modelling the joint encoding distribution as a generalised Product-of-Experts (gPoE) [2]. We optimise modality specific weightings to account for different information content between experts and enable the model to down-weight experts which cause erroneous predictions. Depending on the application, either MoE or gPoE will be most appropriate and so we consider both methods for normative modelling.As far as we are aware, only one other multi-modal VAE normative modelling framework has been proposed in the literature which uses the PoE (PoE-normVAE) [7]. However, Kumar et al. [7] rely on measuring deviations in the feature space, which we argue does not leverage the benefits of multi-modal models. Here, we present an improved factorisation of the joint representation by modelling it as a weighted product or sum of each encoding distribution.Our contributions are two-fold. Firstly, we present two novel multi-modal normative modelling frameworks, MoE-normVAE and gPoE-normVAE, which capture the joint distribution between different imaging modalities. Our proposed models outperform baseline methods on two neuroimaging datasets. Secondly, we present a deviation metric, based on the latent space, suitable for detecting deviations in multi-modal normative distributions. We show that our metric better leverages the benefits of multi-modal normative models compared to feature space-based metrics."
Multi-modal Variational Autoencoders for Normative Modelling Across Multiple Imaging Modalities,2,Methods,"Multi-modal Variational Autoencoder (mVAE). Let X = {x m } M m=1 be the observations of M modalities. We use a mVAE to learn a multi-modal generative model (Fig. 1c), where modalities are conditionally independent given a common latent variable, of the form p θ (X, z) = p(z) M m=1 p θm (x m | z). The likelihood distributions p θm (x m | z) are parameterised by decoder networks with parameters θ = {θ 1 , . . . , θ M }. The goal of VAE training is to maximise the marginal likelihood of the data. However, as this is intractable, we instead optimise an evidence lower bound (ELBO):where the second term is the KL divergence between the approximate joint posterior q φ (z | X) and the prior p(z). We model the posterior, likelihood, and prior distributions as isotropic gaussians.Approximate Joint Posterior. To train the mVAE, we must specify the form of the joint approximate posterior q φ (z | X). Wu and Goodman [16] choose to factorise the joint posterior as a Product-of-Experts (PoE);, where the experts, i.e., individual posterior distributions q φm (z | x m ), are parameterised by encoder networks with parameters), the parameters of joint posterior distribution can be computed [5];(see Supp. for proofs). However, overconfident but miscalibrated experts may bias the joint posterior distribution (see Fig. 1b) which is undesirable for learning informative latent representations between modalities [13].Shi et al. [13] instead factorise the approximate joint posterior as a Mixtureof-Experts (MoE); qIn the MoE setting, each uni-modal posterior q φ (z | x m ) is evaluated with the generative model p θ (X, z) such that the ELBO becomes:(2)However, this approach only takes each uni-modal encoding distribution separately into account during training. Thus, there is no explicit aggregation of information from multiple modalities in the latent representation for reconstruction by the decoder networks. For modalities with a high degree of modality-specific variation, this enforces an undesirable upperbound on the ELBO potentially leading to a sub-optimal approximation of the joint distribution [3].Generalised Product-of-Experts Joint Posterior. We propose an alternative approach to mitigate the problem of overconfident experts by factorising the joint posterior as a generalised Product-of-Experts (gPoE) [2];where α m is a weighting for modality m such that M m=1 α m = 1 for each latent dimension and 0 < α m < 1. We optimise α during training allowing the model to weight experts in such a way as to learn an approximate joint posterior q φ (z | X) where the likelihood distribution p θ (X | z) is maximised. This provides a means to down-weigh overconfident experts. Furthermore, as α is learnt per latent dimension, different modality weightings can be learnt for different vectors, thus explicitly incorporating modality specific variation in addition to shared information in different dimensions of the joint latent space. Similarly to the PoE approach, we can compute the parameters of the joint posterior distribution;Recently, a gPoE mVAE was proposed for learning joint representations of hand-poses and surgical videos [6]. However, we emphasize that our approach differs in application and offers a more lightweight implementation (Joshi et al. [6] require training of auxiliary networks to learn α per sample)."
Multi-modal Variational Autoencoders for Normative Modelling Across Multiple Imaging Modalities,,Multi-modal Normative Modelling.,"We propose two mVAE normative modelling frameworks shown in Fig. 1a. MoE-normVAE, which uses a MoE joint posterior distribution, and gPoE-normVAE, which uses a gPoE joint posterior distribution. For both models, the encoder φ and decoder θ parameters are trained to characterise a healthy population cohort. normVAE models assume abnormality due to disease effects can be quantified by measuring deviations in the latent space [8] or the feature space [11]. At test time, the clinical cohort is passed through the encoder and decoder networks. Deviations of test subjects from the multi-modal latent space of the healthy controls and data reconstruction errors are measured. We compare our methods to the previously proposed PoE-normVAE [7] and three uni-modal models; two single modality and one multi-modality with a concatenated input.To compare our normVAE models to a classical normative approach, we trained one GPR (using the PCNToolkit) per feature on a sub-set of 2000 healthy UK Biobank individuals and used extreme value statistics to calculate subjectlevel abnormality index [9]. We used a top 5% abnormality threshold (set using the healthy training cohort) to calculate a significance ratio (see Eq. 6). Multi-modal Latent Deviation Metric. Previous works using autoencoders as normative models mostly relied on feature-space based deviation methods [7,11]. That is, they compare the input value for subject j for the i-th brain region x ij to the value reconstructed by the autoencoder x ij :2 . Kumar et al. [7] propose the following normalised z-score metric on the data reconstruction (a univariate feature space metric):where μ norm d norm ij is the mean and σ norm d norm ij the standard deviation of the deviations d norm ij of a holdout healthy control cohort. However, in the multi-modal setting, feature space-based deviation metrics may not highlight the benefits of multi-modal models over their uni-modal counterparts. The goal of the joint latent representation is to capture information from all modalities. Thus, decoders for each modality must extract the information from the joint latent representation, which now carries information from all other modalities as well. Therefore, data reconstructions capture only information relevant to a particular modality and may also be poorer compared to uni-modal methods. As such, particularly when incorporating modalities with a high degree of modality-specific variation, we believe latent space deviation metrics would better capture deviations from normative behaviour across multiple modalities. Then, once an abnormal subject has been identified, feature space metrics can be used to identify deviating brain regions (e.g. Supp. Fig. 3).We propose a latent deviation metric to measure deviations from the joint normative distribution. To account for correlation between latent vectors and derive a single multivariate measure of deviation, we measure the Mahalanobis distance from the encoding distribution of the training cohort:where z j ∼ q (z j | X j ) is a sample from the joint posterior distribution for subject j, μ(z norm ) is the mean and Σ(z norm ) the covariance of the healthy cohort latent position. We use robust estimates of the mean and covariance to account for outliers within the healthy control cohort. For closer comparison with D ml , we derive the following multivariate feature space metric:where d j = {d ij , . . . , d Ij } is the reconstruction error for subject j for brain regions (i = 1, ..., I), μ(d norm ) is the mean and Σ(d norm ) the covariance of the healthy cohort reconstruction error.Assessing Deviation Metric Performance. For each model, we calculated D ml and D mf for a healthy holdout cohort and disease cohort. For each deviation metric, we identified individuals whose deviations were significantly different from the healthy training distribution (p < 0.001) [15]. Ideally, we want a model which correctly identifies disease individuals as outliers and healthy individuals as sitting within the normative distribution. As such, we use the following significance ratio (positive likelihood ratio) to assess model performance:In order to calculate significance ratios, we calculated D uf relative to the training cohort for the healthy holdout and disease cohorts (Bonferroni adjusted p=0.05/N features ) [7]."
Multi-modal Variational Autoencoders for Normative Modelling Across Multiple Imaging Modalities,3,Experiments,"Data Processing. To train the normVAE models, we used 10,276 healthy subjects from the UK Biobank [14] (application number: 70047). We used preprocessed (provided by the UK Biobank [1]) grey-matter volumes for 66 cortical (Desikan-Killiany atlas) and 16 subcortical brain regions, and Fractional Anisotropy (FA) and Mean Diffusivity (MD) measurements for 35 white matter tracts (John Hopkins University atlas). At test time, we used 2,568 healthy controls from a holdout cohort and 122 individuals with one of several neurodegenerative disorders; motor neuron disease, multiple sclerosis, Parkinson's disease, dementia/Alzheimer/cognitive-impairment and other demyelinating disease.We also tested the models using an external dataset. We extracted 213 subjects from the Alzheimer's Disease Neuroimaging Initiative (ADNI) 1 [10] dataset with significant memory concern (SMC; N=27), early mild cognitive impairment (EMCI; N=63), late mild cognitive impairment (LMCI; N=34), Alzheimer's disease (AD; N=43) as well as healthy controls (HC; N=45). We used the healthy controls to fine-tune the models in a transfer learning approach. The same T1 and DTI features as for the UK Biobank were extracted for the ADNI dataset.Rather than conditioning on covariates as done in some related work [7,8], we adjusted for confounding effects prior to analysis. Non-linear age and linear ICV affects where removed from the DTI and T1 MRI features of both datasets [12]. Each brain ROI was normalised by removing the mean and dividing by the standard deviation of the healthy control cohort brain regions."
Multi-modal Variational Autoencoders for Normative Modelling Across Multiple Imaging Modalities,,UK Biobank Results.,"As expected, we see greater significance ratios for all models when using D ml rather than D mf (Table 1). When using D mf or D uf , Table 1. Significance ratio calculated from D ml , D mf , and D uf for the UK Biobank. See Supp. for results in figure form. Using GPR, we observed a significance ratio of 6.01, poorer performance than our models (using D ml ). all models perform similiarly. Using D ml over D mf leads to a 4-fold increase in the signficance ratio. Further, our proposed models give the best overall performance across different L dim with the highest significance ratio for gPoE-normVAE with L dim =10. Generally, all multi-modal normVAE showed better performance than the uni-modal models suggesting that by modelling the joint distribution between modalities, we can learn better normative models.ADNI Results. Previous work [8] explored the ability of a uni-modal T1 nor-mVAE to detect deviations in the ADNI cohorts. Figure 2a shows the latent deviation D ml for different diagnosis in the ADNI cohort for the T1 normVAE, DTI normVAE, PoE-normVAE and gPoE-normVAE models. All models reflect the increasing disease severity with increasing disease stage. The gPoE-normVAE model showed greater sensitivity to disease stage as suggested by the higher F statistic and p-values from an ANOVA analysis. We measured the Pearson correlation with composite measures of memory and executive function (Fig. 2b) and found that our proposed model exhibited greater correlation with both cognition scores than baseline approaches. Finally, we see that the sensitivity to disease severity for the gPoE-normVAE model extends to the feature space where we see a general increase in average D uf from the LMCI to AD cohort (Supp. Figs. 3a and 3b respectively).   "
Multi-modal Variational Autoencoders for Normative Modelling Across Multiple Imaging Modalities,4,Discussion and Further Work,"We have built on recent works [7,8,11] and introduced two novel mVAE normative models, which provide an alternative method of learning the joint normative distribution between modalities to address the limitations of current approaches.Our models provide a more informative joint representation compared to baseline methods as evidenced by the better significance ratio for the UK Biobank dataset and greater sensitivity to disease staging and correlation with cognitive measures in the ADNI dataset. We also proposed a latent deviation metric suitable for detecting deviations in the multivariate latent space of multi-modal normative models which gave an approximately 4-fold performance increase over metrics based on the feature space. Further work will involve extending our models to more data modalities, such as genetic variants, to better characterise the behaviour of a physiological system. We note that, for fair comparison across models, we remove the effects of confounding variables prior to analysis. However, confounding effects could be removed during analysis via condition variables [8]. Another limitation of normVAE models introduced here is the use of ROI level data. Data processing software, such as FreeSurfer, may fail to accurately capture abnormality in images, particularly if large lesions are present. Further work involves creating normative models designed for voxel level data to better capture disease effects.Normative models have been successfully applied to the study of a range of heterogeneous diseases. Diseases often present abnormalities across a range of neuroimaging, biological and physiological features which provide different information about the underlying disease process. Normative systems that incorporate features from different data modalities offer a holistic picture of the disease and will be capable of detecting abnormalities across a broad range of different diseases. Furthermore, multi-modal normative modelling captures the relationship between different modalities in healthy individuals, with disruption to this relationship potentially leading to a disease signal. Code is publicly available at https://github.com/alawryaguila/multimodal-normative-models."
Multi-modal Variational Autoencoders for Normative Modelling Across Multiple Imaging Modalities,,Fig. 1 .,
Multi-modal Variational Autoencoders for Normative Modelling Across Multiple Imaging Modalities,,,
Multi-modal Variational Autoencoders for Normative Modelling Across Multiple Imaging Modalities,,Fig. 2 .,
Multi-modal Variational Autoencoders for Normative Modelling Across Multiple Imaging Modalities,,,
Multi-modal Variational Autoencoders for Normative Modelling Across Multiple Imaging Modalities,,Acknowledgements,". This work is supported by the EPSRC-funded UCL Centre for Doctoral Training in Intelligent, Integrated Imaging in Healthcare (i4health) and the Department of Health's NIHR-funded Biomedical Research Centre at University College London Hospitals."
Multi-modal Variational Autoencoders for Normative Modelling Across Multiple Imaging Modalities,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0 41.
Dense Transformer based Enhanced Coding Network for Unsupervised Metal Artifact Reduction,1,Introduction,"CT technology can recover the internal details of the human body in a noninvasive way and has been widely used in clinical practice. However, if there is metal in the tissue, metal artifacts (MA) will appear in the reconstructed CT image, which will corrupt the image and affect the medical diagnosis [1,6].In light of the clinical need for MA reduction, various traditional methods [5,6,12,19] have been proposed to solve the problem by using interpolation and iterative optimization. As machine learning research increasingly impacts medical imaging, deep learning based methods have been proposed for MA reduction. Specifically, these methods can be roughly divided into supervised and unsupervised categories according to the degree of supervision. In the supervised category, the methods [9,14,16,18] based on the dual domain (sinogram and image domains) can achieve good performance for MA reduction. However, supervised learning methods are hindered by the lack of large-scale real-world data pairs consisting of ""images with MA"" and ""images without MA"" representing the same region. The lack of such data can lead algorithms trained on synthetic data to over-fit simulated data pairs, resulting in difficulties in generalizing to clinical settings [11]. Furthermore, although sinogram data can bring additional information, it is difficult to collect in realistic settings [8,15]. Therefore, unsupervised methods based only on the image domain are strongly needed in practice.For unsupervised methods in the image domain, Liao et al. [8] used Generative Adversarial Networks (GANs) [2] to disentangle the MA from the underlying clean structure of the artifact-affected image in latent space by using unpaired data with and without MA. Although the method can separate the artifact component in the latent space, the features from the latent space can't represent rich low-level information of the original input. Further, it's also hard for the encoder to represent long-range correspondence across different regions. Accordingly, the restored image loses texture details and can't retain structure from the CT image. In the same unsupervised setting, Lyu et al. [11] directly separate the MA component and clean structure in image space using a CycleGAN-based method [25]. Although implementation in the image space makes it possible to construct dual constraints, directly operating in the image space affects the algorithm's performance upper limit, because it is difficult to encode in the image space as much low-level information as the feature space.Considering the importance of low-level features in the latent space for generating the artifact-free component, we propose a novel Dense Transformer based Enhanced Coding Network (DTEC-Net) for unsupervised metal artifact reduction, which can obtain low-level features with hierarchical information and map them to a clean image space through adversarial training. DTEC-Net contains our developed Hierarchical Disentangling Encoder (HDE), which utilizes longrange correspondences obtained by a lightweight transformer and a high-order dense process to produce the enhanced coded sequence. To ease the burden of decoding the sequence, we also propose a second-order disentanglement method to finish the sequence decomposition. Extensive empirical results show that our method can not only reduce the MA greatly and generate high-quality images, but also surpasses the competing unsupervised approaches."
Dense Transformer based Enhanced Coding Network for Unsupervised Metal Artifact Reduction,2,Methodology,"We design a Hierarchical Disentangling Encoder(HDE) that can capture lowlevel sequences and enable high-performance restoration. Moreover, to reduce the burden of the decoder group brought by the complicated sequences, we propose a second-order disentanglement mechanism. The intuition is shown in Fig. 1."
Dense Transformer based Enhanced Coding Network for Unsupervised Metal Artifact Reduction,2.1,Hierarchical Disentangling Encoder (HDE),"As shown in Fig. 2(a), the generator of DTEC-Net consists of three encoders and four decoders. We design the HDE to play the role of Encoder1 for enhanced  [8]. The data relationship is shown in [8]. In addition to the difference in disentanglement, DTEC-Net and ADN also have different inner structures.coding. Specifically, for the HDE's input image x a ∈ R 1×H×W with MA, HDE first uses a convolution for the preliminary feature extraction and produces a high-dimensional tensor x l0 with c channels. Then, x l0 will be encoded by three Dense Transformers for Disentanglement (DTDs) in a first-order reuse manner [4,23]. Specifically, the output x li of the ith DTD can be characterized as:In Eq. (1), f s-hde represents the channel compression of the concatenation of multiple DTDs' outputs, and N represents the total number of DTDs in the HDE. As shown in Fig. 3, HDE can obtain the hierarchical information sequence X l {x l0 , ...x lN } and high-level semantic features x h x lN .As shown in Fig. 2(b), Encoder1 of ADN cannot characterize upstream low-level information, and results in limited performance. By using HDE, the upstream of the DTEC-Net's Encoder1 can represent rich low-level information, and be encoded in the efficient way described in Eq. ( 1). After generating the enhanced coding sequences X l with long-range correspondence and densely reused information, DTEC-Net can decode it back to the clean image domain by using the proposed second-order disentanglement for MA reduction, which reduces the decoder group's burden to a large extent."
Dense Transformer based Enhanced Coding Network for Unsupervised Metal Artifact Reduction,2.2,Dense Transformer for Disentanglement (DTD),"In addition to the first-order feature multiplexing given in Eq. ( 1), HDE also uses the DTD to enable second-order feature reuse. The relationship between HDE and DTD is shown in Fig. 3. Inspired by [7,20], DTD first uses a lightweight transformer based on the Swin transformer [7,10] to represent content-based information with long-range correspondence inside of every partition window. It then performs in-depth extraction and second-order reuse.Specifically, the input x 1 ∈ R C×H×W of the DTD will be processed sequentially by the lightweight transformer and groups of convolutions in the form of second-order dense connections. The output x j+1 of the jth convolution with ReLU, which is connected in a second-order dense pattern, can be expressed as:In Eq. ( 2), f cj indicates the jth convolution with ReLU after the lightweight transformer, and the J indicates the total number of convolutions after the lightweight transformer and is empirically set to six. The dense connection method can effectively reuse low-level features [22,23] so that the latent space including these type of features will help the decoder to restore clean images without metal artifacts. Because the low-level information on different channels has different importance to the final restoration task, we use the channel attention mechanism [3] to filter the output of the final convolution layer:where represents the Hadamard product, f MLP indicates a multi-layer perceptron with only one hidden layer, and f pooling represents global pooling.Because the transformer usually requires a large amount of data for training and CT image datasets are usually smaller than those for natural images, we do lightweight processing for the Swin transformer. Specifically, for an input tensor x ∈ R C×H×W of the lightweight transformer, the number of channels will be reduced from C to C in to lighten the burden of the attention matrix. Then, a residual block is employed to extract information with low redundancy.After completing lightweight handling, the tensor will first be partitioned into multiple local windows and flattened to x in ∈ R ( HW P 2 )×P 2 ×Cin according the pre-operation [7,10] of the Swin transformer. P × P represents the window size for partitioning as shown in Fig. 1(b). Then, the attention matrix belonging to the ith window can be calculated by pairwise multiplication between converted vectors in S i {x in (i, j, :)|j = 0, 1, ..., P 2 -1}. Specifically, by using a linear map from R Cin to R Ca for every vector in S i , the query key and value: Q, K, V ∈ R P 2 ×Ca can be derived. Afterwards, the attention matrix for each window can be obtained by the following formula [10]:In actual operation, we use window-based multi-head attention (MSA) [7,10,13] to replace the single-head attention because of the performance improvement [7]. The output of the Swin transformer layer will be unflattened and operated by post processing (POP) which consists of a classic convolution and layer norm (LN) with flatten and unflatten operations. After POP, the lightweight tensor with fewer channels will be unsqueezed to expand the channels, and finally added to the original input x in the form of residuals."
Dense Transformer based Enhanced Coding Network for Unsupervised Metal Artifact Reduction,2.3,Second-Order Disentanglement for MA Reduction (SOD-MAR),"As mentioned in Sect. 2.1, X l represents the hierarchical sequence and facilitates the generator's representation. However, X l needs to be decoded by a highcapacity decoder to match the encoder. Considering that Decoder2 does not directly participate in the restoration branch and already loaded up the complicated artifact part x m in traditional first-order disentanglement learning [8], to reduce the burden of the decoder group, we propose and analyze SOD-MAR. Specifically, Decoder2 of DTEC-Net doesn't decode sequence X l , it only decodes the combination of second-order disentangled information x h ∈ X l and the latent feature x m representing the artifact parts shown in Fig. 2 to complete the process, Decoder2 uses the structure shown in Fig. 4 to finish the decoding step, which is also used by Decoder1 to decode the sequence X l ."
Dense Transformer based Enhanced Coding Network for Unsupervised Metal Artifact Reduction,,(a). In order,"Moreover, we don't only map the x h into Decoder1 and Decoder2 while dropping the X l \{x h } to implement the burden reduction, because the low-level information in X l \{x h } is vital for restoring artifact-free images. Furthermore, x h will be disturbed by noise from the approaching target x a of Decoder2 while information X l \ {x h } upstream from the HDE can counteract the noise disturbance to a certain extent. The reason behind the counteraction is that the update to upstream parameters is not as large as that of the downstream parameters."
Dense Transformer based Enhanced Coding Network for Unsupervised Metal Artifact Reduction,2.4,Loss Function,"Following [8], we use discriminators D 0 , D 1 to constrain the output x c and y a :(5) The above x a , y c represent the input as shown in Fig. 2(a). Following [8], we use the reconstruction loss L rec to constrain the identity map, and also use the artifact consistency loss L art and self-reduction loss L self to control the optimization process. The coefficients for each of these losses are set as in [8]."
Dense Transformer based Enhanced Coding Network for Unsupervised Metal Artifact Reduction,3,Empirical Results,"Synthesized DeepLesion Dataset. Following [8], we randomly select 4186 images from DeepLesion [17] and 100 metal templates [21] to build a dataset. The simulation is consistent with [8,21]. For training, we randomly select 3986 images from DeepLesion combined with 90 metal templates for simulation. The 3986 images will be divided to two disjoint image sets with and without MA after simulation. Then a random combination can form the physically unpaired data with and without MA in the training process. Besides, another 200 images combined with the remaining 10 metal templates are used for the testing process.Real Clinic Dataset. We randomly combine 6165 artifacts-affected images and 20729 artifacts-free images from SpineWeb1  [8] for training, and 105 artifactsaffected images from SpineWeb for testing. Implementation Details. We use peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM) to measure performance. We use mean squared error (MSE) only for measuring ablation experiments. For Synthesized DeepLesion dataset (and Real Clinic dataset), we set the batch size to 2 (and 2) and trained the network for 77 (and 60) epochs using the Adam optimizer. Our DTEC-Net was implemented in Pytorch using an Nvidia Tesla P100. "
Dense Transformer based Enhanced Coding Network for Unsupervised Metal Artifact Reduction,3.1,Ablation Study,"To verify the effectiveness of the proposed methods, ablation experiments were carried out on Synthesized DeepLesion. The results are shown in Table 1.The Impact of DTD in HDE. In this experiment, we change the encoding ability of HDE by changing the number of DTDs. We first use only one DTD to build the HDE, then the PSNR is 0.65 dB lower than our DTEC-Net using three DTDs. Additionally, the average MSE in this case is much higher than DTEC-Net. When the number of DTDs increases to two, the performance improves by 0.25 dB and is already better than the SOTA method [11]. As we further increase the number of DTDs to three, the PSNR and SSIM increase 0.4 dB and 0.003, respectively. The number of DTDs is finally set to three in a trade-off between computation and performance. To match different encoders and decoders and facilitate training, we also adjust the accept headers of Decoder1 to adapt to the sequence length determined by the different numbers of DTDs.Only Transformer in DTD. Although the transformer can obtain better longrange correspondence than convolutions, it lacks the multiplexing of low-level information. For every DTD in DTEC-Net, we delete the second-order feature reuse pattern and only keep the lightweight transformer, the degraded version's results are 0.8 dB lower than our DTEC-Net. At the same time, great instability appears in generative adversarial training. So, only using the transformer cannot achieve good results in reducing metal artifacts.Removing SOD-MAR. Although SOD-MAR mainly helps by easing the burden of decoding as discussed in Sect. 2.3, it also has a performance gain compared to first-order disentanglement. We delete the SOD-MAR in DTEC-Net and let x h be the unique feature decoded by Decoder1. The Performance is 0.2 dB lower than our DTEC-Net, while MSE increases by 1.47.  Unsupervised RCN [24] 32.98 [11] 0.918 [11] Unsupervised ADN [8] 33.60 [8] 0.924 [8] Unsupervised U-DuDoNet [11] 34.54 [11] 0.934 [11] Unsupervised DTEC-Net(Ours) 35.11 0.941"
Dense Transformer based Enhanced Coding Network for Unsupervised Metal Artifact Reduction,3.2,Comparison to State-of-the-Art (SOTA),"For a fair comparison, we mainly compare with SOTA methods under unsupervised settings: ADN [8], U-DuDoNet [11], RCN [24], and CycleGAN [25]. We also compare with the traditional method LI [5] and classical supervised method CNNMAR [21]. The quantitative results of ADN, CycleGAN, CNNMAR and LI are taken from [8], the results of U-DuDoNet and RCN are taken from [11].Because ADN has open-source code, we run their code for qualitative results.Quantitative Results. As shown in Table 2. For the Synthesized DeepLesion Dataset, our method has the highest PSNR and SSIM value and outperforms the baseline ADN by 1.51 dB in PSNR and 0.017 in SSIM. At the same time, it also exceeds the SOTA method U-DuDoNet by 0.57 dB. For the Real Clinic Dataset, the numerical results can't be calculated because the ground truth does not exist. We will present the qualitative results in the appendix. Furthermore, as our work is single-domain based, it has the potential to be easily applied in clinical practice.Qualitative Results. A visual comparison is shown in Fig. 5. Our method not only reduces artifacts to a large extent, but also has sharper edges and richer textures than the compared method. More results are shown in the appendix."
Dense Transformer based Enhanced Coding Network for Unsupervised Metal Artifact Reduction,4,Conclusion,"In this paper, we proposed a Dense Transformer based Enhanced Coding Network (DTEC-Net) for unsupervised metal-artifact reduction. In DTEC-Net, we developed a Hierarchical Disentangling Encoder (HDE) to represent longrange correspondence and produce an enhanced coding sequence. By using this sequence, the DTEC-Net can better recover low-level characteristics. In addition, to decrease the burden of decoding, we specifically design a Second-order Disentanglement for MA Reduction (SOD-MAR) to finish the sequence decomposition. The extensive quantitative and qualitative experiments demonstrate our DTEC-Net's effectiveness and show it outperforms other SOTA methods."
Dense Transformer based Enhanced Coding Network for Unsupervised Metal Artifact Reduction,,Fig. 1 .Fig. 2 .,
Dense Transformer based Enhanced Coding Network for Unsupervised Metal Artifact Reduction,,Fig. 3 .,
Dense Transformer based Enhanced Coding Network for Unsupervised Metal Artifact Reduction,,Fig. 4 .,
Dense Transformer based Enhanced Coding Network for Unsupervised Metal Artifact Reduction,,Fig. 5 .,
Dense Transformer based Enhanced Coding Network for Unsupervised Metal Artifact Reduction,,Table 1 .,
Dense Transformer based Enhanced Coding Network for Unsupervised Metal Artifact Reduction,,Table 2 .,
Dense Transformer based Enhanced Coding Network for Unsupervised Metal Artifact Reduction,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0_8.
MedIM: Boost Medical Image Representation via Radiology Report-Guided Masking,1,Introduction,"Accurate medical representation is crucial for clinical decision-making. Deep learning has shown promising results in medical image analysis, but the accuracy of these models heavily relies on the quality and quantity of data and annotations [21]. Masked image modelling (MIM)-based pre-training approach [3,8,23] such as masked autoencoders (MAE) [8] has shown prospects in improving the image representation under limited annotated data. MIM masks a set of image patches before inputting them into a network and then reconstructs these masked patches by aggregating information from the surrounding context. This ability to aggregate contextual information is essential for vision tasks and understanding medical image analysis [24]. Recently, MIM has witnessed much success in medical domain [4][5][6]11,20,24] such as chest X-ray and CT image analysis.While the random masking strategy is commonly used in current MIM-based works, randomly selecting a percentage of patches to mask. We argue that such a strategy may not be the most suitable approach for medical images due to the domain particularity. Medical images commonly present relatively fixed anatomical structures, while subtle variations between individuals, such as sporadic lesions that alter the texture and morphology of surrounding tissues or organs, may exist. These pathology characteristics may be minute and challenging to perceive visually but are indispensable for early screening and clinical diagnosis. Representation learning should capture these desired target representations to improve downstream diagnosis models' reliability, interpretability, and generalizability. Random masking is less likely to deliberately focus on these important parts. We put forward a straightforward principle, i.e., masking and reconstructing meaningful characteristics, encouraging the network to explore stronger representations from medical images.We advocate utilising radiological reports to locate relevant characteristics and guide mask generation. These reports are routinely produced in clinical practice by expert medical professionals such as radiologists, and can provide a valuable source of semantic knowledge at little to no additional cost [9,17]. When medical professionals read a medical image, they will focus on areas of the image that are relevant to the patient's or clinical conditions. These areas are then recorded in a report, along with relevant information such as whether they are normal or abnormal, the location and density of abnormal areas, and any other materials about the patient's condition. By incorporating reports into the medical image representation learning, the models can simulate the professionals' gaze and learn to focus on the pathology characteristics of images.In this paper, we propose a new approach called MedIM (Masked medical Image Modelling). MedIM aligns semantic correspondences between medical images and radiology reports and reconstructs regions masked by the guidance of learned correspondences. Especially we introduce two masking strategies: knowledge word-driven masking (KWM) and sentence-driven masking (SDM). KWM uses Medical Subject Headings (MeSH) words [14] as the domain knowledge. MeSH words provide a standardized language for medical concepts and conditions. In radiology reports, MeSH words describe imaging modalities, anatomic locations, and pathologic findings, such as ""Heart"", ""Pulmonary"", ""Vascular"", and ""Pneumothorax"" in Fig. 1, and are important semantic components. This inspired KWM to identify regions mapped to MeSH words and generate an attention map, where the highly activated tokens indicate more discriminative cues. We utilize this attention map to selectively mask then restore the high-activated regions, stimulating the network to focus more on regions related to MeSH words during the modelling process. SDM considers multiple sentences in reports, each potentially providing independent information about different aspects of the image. It generates an attention map by identifying regions mapped to one selected sentence, enabling the network to focus on specific aspects of the image mentioned in that sentence during modelling. KWM and SDM identify different sources of discriminative cues and are therefore complementary. MedIM leverages the superiority of both strategies by simultaneously restoring images masked by KWM and SDM in each iteration. This integration creates a more challenging and comprehensive modelling task, which encourages the network to learn more robust and representative medical visual representations. Our MedIM approach is pre-trained on a large chest X-ray dataset of image-report pairs. The learned image representations are transferred to several medical image analysis downstream tasks: multi-label/class image classification and pneumothorax segmentation. Besides, our MedIM pre-trained model can be freely applied to image-text analysis downstream tasks such as image-to-text/text-to-image retrieval.Our contributions mainly include three-fold: (1) we present a novel masking approach MedIM, which is the first work to explore the potential of radiology reports in mask generation for medical images, offering a new perspective to enhance the accuracy and interpretability of medical image representation; (2) we propose two mutual comprehensive masking strategies, KWM and SDM, that effectively identify word-level and sentence-level of discriminative cues to guide the mask generation; (3) we conduct extensive experiments on medical image and image-text downstream tasks, and the performance beats strong competitors like ImageNet pre-training, MIM-based pre-training and advanced medical imagereport pre-training counterparts."
MedIM: Boost Medical Image Representation via Radiology Report-Guided Masking,2,Approach,"As shown in Fig. 1, our MedIM framework has dual encoders that map images and reports to a latent representation, a report-guided mask generation module, and a decoder that reconstructs the images from the masked representation."
MedIM: Boost Medical Image Representation via Radiology Report-Guided Masking,2.1,Image and Text Encoders,"Image Encoder. We use the vision Transformer (ViT) [7] as the image encoderF(•). For an input medical image x, it is first reshaped into a sequence of flattened patches that are then embedded and fed into stacked Transformer layers to obtain the encoded representations of visual tokens E img = F(x) ∈ R Nimg×C , where C is the encoding dimension and N img denotes the number of patches. Text Encoder. We use the BioClinicalBERT [2] model, pre-trained on the MIMIC III dataset [13], as our text encoder T (•). We employ WordPiece [19] for tokenizing free-text medical reports. This technique is particularly useful for handling the large and diverse vocabularies that are common in the medical language. For an input medical report r with N text words, the tokenizer segments each word to sub-words and generates word piece embeddings as the input to the text encoder. The text encoder extracts features for word pieces, which are aggregated to generate the word representations E text = T (r) ∈ R Ntext×C ."
MedIM: Boost Medical Image Representation via Radiology Report-Guided Masking,2.2,Report-Guided Mask Generation,"We introduce two radiology report-guided masking strategies, i.e., KWM and SDM, identifying different cues to guide the mask generation."
MedIM: Boost Medical Image Representation via Radiology Report-Guided Masking,,Knowledge Word-Driven Masking (KWM).,"MeSH words shown in Fig. 1 are important for accurately describing medical images, as they provide a standardized vocabulary to describe the anatomical structures and pathologies observed in the images. Hence the KWM is proposed to focus on the MeSH word tokens during mask generation. Given a report r and its text representations E text , we first match MeSH words in the report based on the MeSH Table [14] and extract the representations of MeSH word tokens, formally aswhere N MeSH represents the number of MeSH words in the report r. Then, we compute an attention map C MeSH to identify image regions mapped to MeSH words as followswhere H = W = N img , T and R represent the transpose and reshape functions, and the softmax function normalizes the elements along the image dimension to find the focused region matched to each MeSH word. The summation operation performs on the text dimension to aggregate the attentions related to all MeSH words.Subsequently, the high-activated masking is presented to remove the discovered attention regions. Here, we define a corresponding binary mask m ∈ {0, 1} H×W formulated as m (i,j) = I(C). Here C [γ * Nimg] MeSH refers to the (γ * N img )-th largest activation in C MeSH , andγ is the masking ratio that determines how many activations would be suppressed. With this binary mask, we can compute the masked representations produced by KWM aswhere [MASK] is a masked placeholder."
MedIM: Boost Medical Image Representation via Radiology Report-Guided Masking,,Sentence-Driven Masking (SDM).,"Medical reports often contain multiple sentences that describe different findings related to the image, which inspires SDM to introduce sentence-level information during mask generation. For the report r, we randomly select a sentence s and extract its representations aswhere N s represents the length of s. Then, an attention map C s can be computed to identify regions mapped to this sentence asAfter that, the high-activated masking is performed based on C s to compute the masked representations M(C s ; λ) sdm . We also select an image-report pair and visualize the corresponding attention map and generated mask procured by KWM and SDM in Fig. 2 to show the superiority of our masking strategies."
MedIM: Boost Medical Image Representation via Radiology Report-Guided Masking,2.3,Decoder for Reconstruction,Both masked representations M(C MeSH ; λ) kwm and M(C s ; λ) sdm are mapped to the decoder D(•) that includes four conv-bn-relu-upsample blocks. We design two independent reconstruction heads to respectively accept the decoded features D(M(C MeSH ; λ) kwm ) and D(M(C s ; λ) sdm ) and generate the final reconstruction results y kwm and y sdm .
MedIM: Boost Medical Image Representation via Radiology Report-Guided Masking,2.4,Objective Function,"MedIM creates a more challenging reconstruction objective by removing then restoring the most discriminative regions guided by radiological reports. We optimize this reconstruction learning process with the mean square error (MSE) loss function, expressed asMedIM also combines the cross-modal alignment constraint, which aligns medical images' visual and semantic aspects with their corresponding radiological reports, benefiting in better identifying the reported-guided discriminative regions during mask generation. We follow the work [17] and compute the objective alignment function L align by exploiting the fine-grained correspondences between images and reports. The final objective of our MedIM is the combination of reconstruction and alignment objectives as L MedIM = αL restore + L align , where α is a weight factor to balance both objectives."
MedIM: Boost Medical Image Representation via Radiology Report-Guided Masking,2.5,Downstream Transfer Learning,"After pre-training, we can transfer the weight parameters of the MedIM to various downstream tasks. For the classification task, we use the commonly used Linear probing, i.e., freezing the pre-trained image encoder and solely training a randomly initialized linear classification head. For the segmentation task, the encoder and decoder are first initialized with the MedIM pre-trained weights, and a downstream-specific head is added to the network. The network is then fine-tuned end-to-end. For the retrieval task, we take an image or report as an input query and retrieve target reports or images by computing the similarity between the query and all candidates using the learned image and text encoders."
MedIM: Boost Medical Image Representation via Radiology Report-Guided Masking,3,Experiments and Results,
MedIM: Boost Medical Image Representation via Radiology Report-Guided Masking,3.1,Experimental Details,"Pre-training Setup. We use the MIMIC-CXR-JPG dataset [12] to pre-train our MedIM framework. Following [17], we only include frontal-view chest images from the dataset and extract the impression and finding sections from radiological reports. As a result, over 210,000 radiograph-report pairs are available. We manually split 80% of pairs for pre-training and 20% of pairs used for downstream to validate in-domain transfer learning. We set the input size to 224 × 224 adopt the AdamW optimizer [16] with a cosine decaying learning rate [15], a momentum of 0.9, and a weight decay of 0.05. We set the initial learning rate to 0.00002, batch size to 144, and maximum epochs to 50. Through the ablation study, we empirically set the mask ratio to 50% and loss weight α to 10."
MedIM: Boost Medical Image Representation via Radiology Report-Guided Masking,,Downstream Setup.,"We validate the transferability of learned MedIM representations on four X-ray-based downstream tasks: (1) multi-label classification on CheXpert [10] dataset using its official split, which contains five individual binary labels: atelectasis, cardiomegaly, consolidation, edema, and pleural effusion;(2) multi-class classification on COVIDx [18] dataset with over 30k chest X-ray images, which aims to classify each radiograph into COVID- image-text/report-text retrieval on the MIMIC-CXR validation dataset. We use the Dice coefficient score (Dice) to measure the segmentation performance, use the mean area under the receiver operator curve (mAUC) to measure the multilabel classification performance, and use the accuracy to measure the multi-class classification performance. We use the recall of the corresponding image/report that appears in the top-k ranked images/reports (denoted by R@k) to measure the retrieval performance [9]. Each downstream experiment is conducted three times and the average performance is reported. More details are in the Appendix."
MedIM: Boost Medical Image Representation via Radiology Report-Guided Masking,3.2,Comparisons with Different Pre-training Methods,"We compare the downstream performance of our MedIM pre-training with five pre-training methods in Table 1 and Table 2. Our MedIM achieves state-of-theart results on all downstream datasets, outperforming ImageNet pre-training [7], MIM-based pre-training MAE [8] and three medical image-report pre-training approaches, GLoRIA [9], MRM [22] and MGCA [17], under different labelling ratios. The superior performance corroborates the effectiveness of our reportguided masking pre-training strategy over other pre-training strategies in learning discriminative information. Besides, our MedIM achieves 88.91% when using only 1% downstream labelled data on CheXpert, better than other competitors with 100% labelled data. These convincing results have demonstrated the enormous potential of MedIM for annotation-limited medical image tasks."
MedIM: Boost Medical Image Representation via Radiology Report-Guided Masking,3.3,Discussions,"Ablation Study. Ablation studies are performed over each component of MedIM, including knowledge word-driven masking (KWM) and Sentence-driven masking (SDM), as listed in Table 3. We sequentially add each component to the vanilla baseline, L align only, thus the downstream performance is gradually improved in Table 3. First, by reconstructing the masked representations produced by KWM, the total performance of three tasks is increased by 3.28 points. This indicates that using MeSH words as knowledge to guide the mask generation can improve the model representations and generalization. Equipped with KWM and SDM, our MedIM can surpass the baseline model by a total of 5.12 points on three tasks, suggesting the superiority of adding the SDM strategy and integrating these two masking strategies.Masking Strategies. To demonstrate the effectiveness of the High-activated masking strategy, we compare it with three counterparts, No masking, Random masking, and Low-activated masking. Here No masking means that the recon- "
MedIM: Boost Medical Image Representation via Radiology Report-Guided Masking,,Methods T2I I2T,"R@1 R@5 R@10 R@1 R@5 R@10  struction is performed based on the complete image encoder representations instead of the masked one. Low-activated masking refers to masking the tokens exhibiting a low response in both KWM and SDM strategies. The comparison on the left side of Fig. 3 reveals that all masking strategies are more effective in improving the accuracy than No masking. Benefiting from mining more discriminative information, our High-activated masking performs better than the Random and Low-activated masking. Besides, we also compare different masking ratios, varying from 25% to 75%, on the right side of Fig. 3."
MedIM: Boost Medical Image Representation via Radiology Report-Guided Masking,4,Conclusion,"We propose a new masking approach called MedIM that uses radiological reports to guide the mask generation of medical images during the pre-training process. We introduce two masking strategies KWM and SDM, which effectively identify different sources of discriminative cues to generate masked inputs. MedIM is pre-trained on a large dataset of image-report pairs to restore the masked regions, and the learned image representations are transferred to three medical image analysis tasks and image-text/report-text retrieval tasks. The results demonstrate that MedIM outperforms strong pre-training competitors and the random masking method. In the future, we will extend our MedIM to handle other modalities, e.g., 3D medical image analysis."
MedIM: Boost Medical Image Representation via Radiology Report-Guided Masking,,Fig. 1 .,
MedIM: Boost Medical Image Representation via Radiology Report-Guided Masking,,Fig. 2 .,
MedIM: Boost Medical Image Representation via Radiology Report-Guided Masking,,Fig. 3 .,
MedIM: Boost Medical Image Representation via Radiology Report-Guided Masking,,Table 1 .,
MedIM: Boost Medical Image Representation via Radiology Report-Guided Masking,,Table 2 .,
MedIM: Boost Medical Image Representation via Radiology Report-Guided Masking,,67 23.96 33.55 8.70 24.63 34.27Table 3 .,
Unsupervised Domain Transfer with Conditional Invertible Neural Networks,1,Introduction,"The success of supervised learning methods in the medical domain led to countless breakthroughs that might be translated into clinical routine and have the potential to revolutionize healthcare [6,13]. For many applications, however, labeled reference data (ground truth) may not be available for training and validating a neural network in a supervised manner. One such application is spectral imaging which comprises various non-interventional, non-ionizing imaging techniques that can resolve functional tissue properties such as blood oxygenation in real time [1,3,23]. While simulations have the potential to overcome the lack of ground truth, synthetic data is not yet sufficiently realistic [9]. Cycle Generative Adversarial Networks (GAN)-based architectures are widely used for domain transfer [12,24] but may suffer from issues such as unstable training, hallucinations, or mode collapse [15]. Furthermore, they have predominantly been used for conventional RGB imaging and one-channel cross-modality domain adaptation, and may not be suitable for other imaging modalities with more channels. We address these challenges with the following contributions:"
Unsupervised Domain Transfer with Conditional Invertible Neural Networks,,Domain Transfer Method:,"We present an entirely new sim-to-real transfer approach based on conditional invertible neural networks (cINNs) (cf. Fig. 1) specifically designed for data with many spectral channels. This approach inherently addresses weaknesses of the state of the art with respect to the preservation of spectral consistency and, importantly, does not require paired images."
Unsupervised Domain Transfer with Conditional Invertible Neural Networks,,Instantiation to Spectral Imaging:,We show that our method can generically be applied to two complementary modalities: photoacoustic tomography (PAT; image-level) and hyperspectral imaging (HSI; pixel-level).
Unsupervised Domain Transfer with Conditional Invertible Neural Networks,,Comprehensive Validation:,"In comprehensive validation studies based on more than 2,000 PAT images (real: ∼1,000) and more than 6 million spectra for HSI (real: ∼6 million) we investigate and subsequently confirm our two main hypotheses: (H1) Our cINN-based models can close the domain gap between simulated and real spectral data better than current state-of-the-art methods regarding spectral plausibility. (H2) Training models on data transferred by our cINN-based approach can improve their performance on the corresponding (clinical) downstream task without them having seen labeled real data."
Unsupervised Domain Transfer with Conditional Invertible Neural Networks,2,Materials and Methods,
Unsupervised Domain Transfer with Conditional Invertible Neural Networks,2.1,Domain Transfer with Conditional Invertible Neural Networks,"Concept Overview. Our domain transfer approach (cf. Fig. 2) is based on the assumption that data samples from both domains carry domain-invariant information (e.g. on optical tissue properties) and domain-variant information (e.g. modality-specific artifacts). The invertible architecture, which inherently guarantees cycle consistency, transfers both simulated and real data into a shared latent space. While the domain-invariant features are captured in the latent space, the domain-variant features can either be filtered (during encoding) or added (during decoding) by utilizing a domain label D. To achieve spectral consistency, we leverage the fact that different tissue types feature characteristic spectral signatures and condition the model on the tissue label Y if available. For unlabeled (real) data, we use randomly generated proxy labels instead. To achieve high visual quality beyond spectral consistency, we include two discriminators Dis sim and Dis real for their respective domains. Finally, as a key theoretical advantage, we avoid mode collapse with maximum likelihood optimization. Implementation details are provided in the following. cINN Model Design. The core of our architecture is a cINN [2] (cf. Fig. 2), comprising multiple (i) scales of N i -chained affine conditional coupling (CC) blocks [7]. These scales are necessary in order to increase the receptive field of the network and are achieved by Haar wavelet downsampling [11]. A CC block consists of subnetworks that can be freely chosen depending on the data dimensionality (e.g. fully connected or convolutional networks) as they are only evaluated in the forward direction. The CC blocks receive a condition consisting of two parts: domain label and tissue label, which are then concatenated to the input along the channel dimension. In the case of PAT, the tissue label is a full semantic and random segmentation map for the simulated and real data, respectively. In the case of HSI, the tissue label is a one-hot encoded vector for organ labels.Model Training. In the following, the proposed cINN with its parameters θ will be referred to as f (x, DY, θ) and its inverse as f -1 for any input x ∼ p D from domain D ∈ {D sim , D real } with prior density p D and its corresponding latent space variable z. The condition DY is the combination of domain label D as well as the tissue label Y ∈ {Y sim , Y real }. Then the maximum likelihood loss ML for a training sample x i is described byFor the adversarial training, we employ the least squares training scheme [18] for generator Gen D = f -1 D • f D and discriminator Dis D for each domain with x D as input from the source domain and x D as input from the target domain:Finally, the full loss for the proposed model comprises the following:Model Inference. The domain transfer is done in two steps: 1) A simulated image is encoded in the latent space with conditions D sim and Y sim to its latent representation z, 2) z is decoded to the real domain via D real with the simulated tissue label Y sim :"
Unsupervised Domain Transfer with Conditional Invertible Neural Networks,2.2,Spectral Imaging Data,"Photoacoustic Tomography Data. PAT is a non-ionizing imaging modality that enables the imaging of functional tissue properties such as tissue oxygenation [22]. The real PAT data (cf. Fig. 3) used in this work are images of human forearms that were recorded from 30 healthy volunteers using the MSOT Acuity Echo (iThera Medical GmbH, Munich, Germany) (all regulations followed under study ID: S-451/2020, and the study is registered with the German Clinical Trials Register under reference number DRKS00023205). In this study, 16 wavelengths from 700 nm to 850 nm in steps of 10 nm were recorded for each image. The resulting 180 images were semantically segmented into the structures shown in Fig. 3 according to the annotation protocol provided in [20]. Additionally, a full sweep of each forearm was performed to generate more unlabeled images, thus amounting to a total of 955 real images. The simulated PAT data (cf. Fig. 3) used in this work comprises 1,572 simulated images of human forearms. They were generated with the toolkit for Simulation and Image Processing for Photonics and Acoustics (SIMPA) [8] based on a forearm literature model [21] and with a digital device twin of the MSOT Acuity Echo.Hyperspectral Imaging Data. HSI is an emerging modality with high potential for surgery [4]. In this work, we performed pixel-wise analysis of HSI images. The real HSI data was acquired with the Tivita R Tissue (Diaspective Vision GmbH, Am Salzhaff, Germany) camera, featuring a spectral resolution of approximately 5 nm in the spectral range between 500 nm and 1000 nm nm. In total, 458 images, corresponding to 20 different pigs, were acquired (all regulations followed under study IDs: 35-9185.81/G-161/18 and 35-9185.81/G-262/19) and annotated with ten structures: bladder, colon, fat, liver, omentum, peritoneum, skin, small bowel, spleen, and stomach (cf. Fig. 3). This amounts to 6,410,983 real spectra in total. The simulated HSI data was generated with a Monte Carlo method (cf. algorithm provided in the supplementary material). This procedure resulted in 213,541 simulated spectra with annotated organ labels."
Unsupervised Domain Transfer with Conditional Invertible Neural Networks,3,Experiments and Results,"The purpose of the experiments was to investigate hypotheses H1 and H2 (cf. Sect. 1). As comparison methods, a CycleGAN [24] and an unsupervised imageto-image translation (UNIT) network [16] were implemented fully convolutionally for PAT and in an adapted version for the one-dimensional HSI data. To make the comparison fair, the tissue label conditions were concatenated with the input, and we put significant effort into optimizing the UNIT on our data.Realism of Synthetic Data (H1) : According to qualitative analyses (Fig. 4) our domain transfer approach improves simulated PAT images with respect to key properties, including the realism of skin, background, and sharpness of vessels.  The PCA plots in a) represent a kernel density estimation of the first and second components of a PCA embedding of the real data, which represent about 67% and 6% of the variance in the real data, respectively. The distributions on top and on the right of the PCA plot correspond to the marginal distributions of each dataset's first two components. b) Violin plots show that the cINN yields spectra that feature a smaller difference to the real data compared to the simulations and the UNIT-generated data.The dashed lines represent the mean difference value, and each dot represents the difference for one wavelength.A principal component analysis (PCA) performed on all artery and vein spectra of the real and synthetic datasets demonstrates that the distribution of the synthetic data is much closer to the real data after applying our domain transfer approach (cf. Fig. 5a)). The same holds for the absolute difference, as shown in Fig. 5b). Slightly better performance was achieved with the cINN compared to the UNIT. Similarly, our approach improves the realism of HSI spectra, as illustrated in Fig. 6, for spectra of five exemplary organs (colon, stomach, omentum, spleen, and fat). The cINN-transferred spectra generally match the real data very closely. Failure cases where the real data has a high variance (translucent band) are also shown."
Unsupervised Domain Transfer with Conditional Invertible Neural Networks,,Benefit of Domain-Transferred Data for Downstream Tasks (H2):,"We examined two classification tasks for which reference data generation was feasible: classification of veins/arteries in PAT and organ classification in HSI. For both modalities, we used the completely untouched real test sets, comprising 162 images in the case of PAT and ∼ 920,000 spectra in the case of HSI. For both tasks, a calibrated random forest classifier (sklearn [19] with default parameters) was trained on the simulated, the domain-transferred (by UNIT and cINN), and real spectra. As metrics, the balanced accuracy (BA), area under receiver operating characteristic (AUROC) curve, and F1-score were selected based on [17].As shown in Table 1, our domain transfer approach dramatically increases the classification performance for both downstream tasks. Compared to physicsbased simulation, the cINN obtained a relative improvement of 37% (BA), 25% (AUROC), and 22% (F1 Score) for PAT whereas the UNIT only achieved a  relative improvement in the range of 20%-27% (depending on the metric). For HSI, the cINN achieved a relative improvement of 21% (BA), 1% (AUROC), and 33% (F1 Score) and it scored better in all metrics except for the F1 Score than the UNIT. For all metrics, training on real data still yields better results."
Unsupervised Domain Transfer with Conditional Invertible Neural Networks,4,Discussion,"With this paper, we presented the first domain transfer approach that combines the benefits of cINNs (exact maximum likelihood estimation) with those of GANs (high image quality). A comprehensive validation involving qualitative and quantitative measures for the remaining domain gap and downstream tasks suggests that the approach is well-suited for sim-to-real transfer in spectral imaging. For both PAT and HSI, the domain gap between simulations and real data could be substantially reduced, and a dramatic increase in downstream task performance was obtained -also when compared to the popular UNIT approach. The only similar work on domain transfer in PAT has used a cycle GANbased architecture on a single wavelength with only photon propagation as PAT image simulator instead of full acoustic wave simulation and image reconstruction [14]. This potentially leads to spectral inconsistency in the sense that the spectral information either is lost during translation or remains unchanged from the source domain instead of adapting to the target domain. Outside the spectral/medical imaging community, Liu et al. [16] and Grover et al. [10] tasked variational autoencoders and invertible neural networks for each domain, respectively, to create the shared encoding. They both combined this approach with adversarial training to achieve high-quality image generation. Das et al. [5] built upon this approach by using labels from the source domain to condition the domain transfer task. In contrast to previous work, which used en-/decoders for each domain, we train a single network as shown in Fig. 2. with a two-fold condition consisting of a domain label (D) and a tissue label (Y ) from the source domain, which has the advantage of explicitly aiding the spectral domain transfer.The main limitation of our approach is the high dimensionality of the parameter space of the cINN as dimensionality reduction of data is not possible due to the information and volume-preserving property of INNs. This implies that the method is not suitable for arbitrarily high dimensions. Future work will comprise the rigorous validation of our method with tissue-mimicking phantoms for which reference data are available.In conclusion, our proposed approach of cINN-based domain transfer enables the generation of realistic spectral data. As it is not limited to spectral data, it could develop into a powerful method for domain transfer in the absence of labeled real data for a wide range of image modalities in the medical domain and beyond."
Unsupervised Domain Transfer with Conditional Invertible Neural Networks,,Fig. 1 .,
Unsupervised Domain Transfer with Conditional Invertible Neural Networks,,Fig. 2 .,
Unsupervised Domain Transfer with Conditional Invertible Neural Networks,,Fig. 3 .,
Unsupervised Domain Transfer with Conditional Invertible Neural Networks,,Fig. 4 .,
Unsupervised Domain Transfer with Conditional Invertible Neural Networks,,Fig. 5 .,
Unsupervised Domain Transfer with Conditional Invertible Neural Networks,,Fig. 6 .,
Unsupervised Domain Transfer with Conditional Invertible Neural Networks,,Table 1 . Classification scores for different training data.,
Anatomy-Driven Pathology Detection on Chest X-rays,1,Introduction,"Chest radiographs (chest X-rays) represent the most widely utilized type of medical imaging examination globally and hold immense significance in the detection of prevalent thoracic diseases, including pneumonia and lung cancer, making them a crucial tool in clinical care [10,15]. Pathology detection and localization -for brevity we will use the term pathology detection throughout this workenables the automatic interpretation of medical scans such as chest X-rays by predicting bounding boxes for detected pathologies. Unlike classification, which only predicts the presence of pathologies, it provides a high level of explainability supporting radiologists in making informed decisions. However, while image classification labels can be automatically extracted from electronic health records or radiology reports [7,20], this is typically not possible for bounding boxes, thus limiting the availability of large datasets for pathology detection. Additionally, manually annotating pathology bounding boxes is a time-consuming task, further exacerbating the issue. The resulting scarcity of large, publicly available datasets with pathology bounding boxes limits the use of supervised methods for pathology detection, such that current approaches typically follow weakly supervised object detection approaches, where only classification labels are required for training. However, as these methods are not guided by any form of bounding boxes, their performance is limited.We, therefore, propose a novel approach towards pathology detection that uses anatomical region bounding boxes, solely defined on anatomical structures, as proxies for pathology bounding boxes. These region boxes are easier to annotate -the physiological shape of a healthy subject's thorax can be learned relatively easily by medical students -and generalize better than those of pathologies, such that huge labeled datasets are available [21]. In summary:-We propose anatomy-driven pathology detection (ADPD), a pathology detection approach for chest X-rays, trained with pathology classification labels together with anatomical region bounding boxes as proxies for pathologies. -We study two training approaches: using localized (anatomy-level) pathology labels for our model Loc-ADPD and using image-level labels with multiple instance learning (MIL) for our model MIL-ADPD. -We train our models on the Chest ImaGenome [21] dataset and evaluate on NIH ChestX-ray 8 [20], where we found that our Loc-ADPD model outperforms both, weakly supervised methods and fully supervised detection with a small training set, while our MIL-ADPD model is competitive with supervised detection and slightly outperforms weakly supervised approaches."
Anatomy-Driven Pathology Detection on Chest X-rays,2,Related Work,"Weakly Supervised Pathology Detection. Due to the scarcity of bounding box annotations, pathology detection on chest X-rays is often tackled using weakly supervised object detection with Class Activation Mapping (CAM) [25], which only requires image-level classification labels. After training a classification model with global average pooling (GAP), an activation heatmap is computed by classifying each individual patch (extracted before pooling) with the trained classifier, before thresholding this heatmap for predicting bounding boxes. Inspired by this approach, several methods have been developed for chest X-rays [6,14,20,23]. While CheXNet [14] follows the original approach, the method provided with the NIH ChestX-ray 8 dataset [20] and the STL method [6] use Logsumexp (LSE) pooling [13], while the MultiMap model [23] uses max-min pooling as first proposed for the WELDON [3] method. Unlike our method, none of these methods utilize anatomical regions as proxies for predicting pathology bounding boxes, therefore leading to inferior performance. Localized Pathology Classification. Anatomy-level pathology labels have been utilized before to train localized pathology classifiers [1,21] or to improve weakly supervised pathology detection [24]. Along with the Chest ImaGenome dataset [21] several localized pathology classification models have been proposed which use a Faster R-CNN [16] to extract anatomical region features before predicting observed pathologies for each region using either a linear model or a GCN model based on pathology co-occurrences. This approach has been further extended to use GCNs on anatomical region relationships [1]. While utilizing the same form of supervision as our method, these methods do not tackle pathology detection.In AGXNet [24], anatomy-level pathology classification labels are used to train a weakly-supervised pathology detection model. Unlike our and the other described methods, it does however not use anatomical region bounding boxes."
Anatomy-Driven Pathology Detection on Chest X-rays,3,Method,
Anatomy-Driven Pathology Detection on Chest X-rays,3.1,Model,"Figure 1 provides an overview of our method. Given a chest X-ray, we apply a DenseNet121 [5] backbone and extract patch-wise features by using the feature map after the last convolutional layer (before GAP). We then apply a lightweight object detection model consisting of a single DETR [2] decoder layer to detect anatomical regions. Following [2], we use learned query tokens attending to patch features in the decoder layer, where each token corresponds to one predicted bounding box. As no anatomical region can occur more than once in each chest X-ray, each query token is assigned to exactly one pre-defined anatomical region, such that the number of tokens equals the number of anatomical regions. This one-to-one assignment of tokens and regions allows us to remove the Hungarian matching used in [2]. As described next, the resulting per-region features from the output of the decoder layer will be used for predictions on each region.For predicting whether the associated region is present, we use a binary classifier with a single linear layer, for bounding box prediction we use a three-layer MLP followed by sigmoid. We consider the prediction of observed pathologies as a multi-label binary classification task and use a single linear layer (followed by sigmoid) to predict the probabilities of all pathologies. Each of these predictors is applied independently to each region with their weights shared across regions.We experimented with more complex pathology predictors like an MLP or a transformer layer but did not observe any benefits. We also did not observe improvements when using several decoder layers and observed degrading performance when using ROI pooling to compute region features."
Anatomy-Driven Pathology Detection on Chest X-rays,3.2,Inference,"During inference, the trained model predicts anatomical region bounding boxes and per-region pathology probabilities, which are then used to predict pathology bounding boxes in two steps, as shown in Fig. 2. In step (i), pathology probabilities are first thresholded and for each positive pathology (with probability larger than the threshold) the bounding box of the corresponding anatomical region is predicted as its pathology box, using the pathology probability as box score. This means, if a region contains several predicted pathologies, then all of its predicted pathologies share the same bounding box during step (i). In step (ii), weighted box fusion (WBF) [19] merges bounding boxes of the same pathology with IoU-overlaps above 0.03 and computes weighted averages (using box scores as weights) of their box coordinates. As many anatomical regions are at least partially overlapping, and we use a small IoU-overlap threshold, this allows the model to either pull the predicted boxes to relevant subparts of an anatomical region or to predict that pathologies stretch over several regions."
Anatomy-Driven Pathology Detection on Chest X-rays,3.3,Training,"The anatomical region detector is trained using the DETR loss [2] with fixed oneto-one matching (i.e. without Hungarian matching). For training the pathology classifier, we experiment with two different levels of supervision (Fig. 3). For our Loc-ADPD model, we utilize anatomy-level pathology classification labels. Here, the target set of observed pathologies is available for each anatomical region individually such that the pathology observation prediction can directly be trained for each anatomical region. We apply the ASL [17] loss function independently on each region-pathology pair and average the results over all regions and pathologies. The decoder feature dimension is set to 512.For our MIL-ADPD model, we experiment with a weaker form of supervision, where pathology classification labels are only available on the per-image level. We utilize multiple instance learning (MIL), where an image is considered a bag of individual instances (i.e. the anatomical regions), and only a single label (per pathology) is provided for the whole bag, which is positive if any of its instances is positive. To train using MIL, we first aggregate the predicted pathology probabilities of each region over all detected regions in the image using LSE pooling [13], acting as a smooth approximation of max pooling. The resulting per-image probability for each pathology is then trained using the ASL [17] loss. In this model, the decoder feature dimension is set to 256.In both models, the ASL loss is weighted by a factor of 0.01 before adding it to the DETR loss. We train using AdamW [12] with a learning rate of 3e-5 (Loc-ADPD) or 1e-4 (MIL-ADPD) and weight decay 1e-5 (Loc-ADPD) or 1e-4 (MIL-ADPD) in batches of 128 samples with early stopping (with 20 000 steps patience) for roughly 7 h on a single Nvidia RTX A6000."
Anatomy-Driven Pathology Detection on Chest X-rays,3.4,Dataset,"Training Dataset. We train on the Chest ImaGenome dataset [4,21,22]1 , consisting of roughly 240 000 frontal chest X-ray images with corresponding scene graphs automatically constructed from free-text radiology reports. It is derived from the MIMIC-CXR dataset [9,10], which is based on imaging studies from 65 079 patients performed at Beth Israel Deaconess Medical Center in Boston, US. Amongst other information, each scene graph contains bounding boxes for 29 unique anatomical regions with annotated attributes, where we consider positive anatomical finding and disease attributes as positive labels for pathologies, leading to binary anatomy-level annotations for 55 unique pathologies. We consider the image-level label for a pathology to be positive if any region is positively labeled with that pathology.We use the provided jpg-images [11] 2 and follow the official MIMIC-CXR training split but only keep samples containing a scene graph with at least five valid region bounding boxes, resulting in a total of 234 307 training samples.During training, we use random resized cropping with size 224 × 224, apply contrast and brightness jittering, random affine augmentations, and Gaussian blurring.Evaluation Dataset and Class Mapping. We evaluate our method on the subset of 882 chest X-ray images with pathology bounding boxes, annotated by radiologists, from the NIH ChestXray-8 (CXR8) dataset [20] 3 from the National Institutes of Health Clinical Center in the US. We use 50% for validation and keep the other 50% as a held-out test set. Note that for evaluation only pathology bounding boxes are required (to compute the metrics), while during training only anatomical region bounding boxes (without considering pathologies) are required. All images are center-cropped and resized to 224 × 224.The dataset contains bounding boxes for 8 unique pathologies. While partly overlapping with the training classes, a one-to-one correspondence is not possible for all classes. For some evaluation classes, we therefore use a many-to-one mapping where the class probability is computed as the mean over several training classes. We refer to the supp. material for a detailed study on class mappings."
Anatomy-Driven Pathology Detection on Chest X-rays,4,Experiments and Results,
Anatomy-Driven Pathology Detection on Chest X-rays,4.1,Experimental Setup and Baselines,"We compare our method against several weakly supervised object detection methods (CheXNet [14], STL [6], GradCAM [18], CXR [20], WELDON [3], Mul-tiMap Model [23], LSE Model [13]), trained on the CXR8 training set using only image-level pathology labels. Note that some of these methods focus on (imagelevel) classification and do not report quantitative localization results. Nevertheless, we compare their localization approaches quantitatively with our method. We also use AGXNet [24] for comparison, a weakly supervised method trained using anatomy-level pathology labels but without any bounding box supervision. It was trained on MIMIC-CXR (sharing the images with our method) with labels from RadGraph [8] and finetuned on the CXR8 training set with imagelevel labels. Additionally, we also compare with a Faster-RCNN [16] trained on a small subset of roughly 500 samples from the CXR8 training set that have been 2 https://physionet.org/content/mimic-cxr-jpg/2.0.0/ (PhysioNet Credentialed Health Data License 1.5.0). annotated with pathology bounding boxes by two medical experts, including one board-certified radiologist.Table 1. Results on the NIH ChestX-ray 8 dataset [20]. Our models Loc-ADPD and MIL-ADPD, trained using anatomy (An) bounding boxes, both outperform all weakly supervised methods trained with image-level pathology (Pa) and anatomy-level pathology (An-Pa) labels by a large margin. MIL-ADPD is competitive with the supervised baseline trained with pathology (Pa) bounding boxes, while Loc-ADPD outperforms it by a large margin."
Anatomy-Driven Pathology Detection on Chest X-rays,,Method,"Supervision IoU@10-70 IoU@10 IoU@30 IoU@50 Box Class mAP AP loc-acc AP loc-acc AP loc-acc MIL-ADPD (ours) An Pa For all models, we only consider the predicted boxes with the highest box score per pathology, as the CXR8 dataset never contains more than one box per pathology. We report the standard object detection metrics average precision (AP) at different IoU-thresholds and the mean AP (mAP) over thresholds (0.1, 0.2, . . . , 0.7), commonly used thresholds on this dataset [20]. Additionally, we report the localization accuracy (loc-acc) [20], a common localization metric on this dataset, where we use a box score threshold of 0.7 for our method."
Anatomy-Driven Pathology Detection on Chest X-rays,4.2,Pathology Detection Results,"Comparison with Baselines. Table 1 shows the results of our MIL-ADPD and Loc-ADPD models and all baselines on the CXR8 test set. Compared to the best weakly supervised method with image-level supervision (CheXNet) our methods improve by large margins (MIL-ADPD by Δ+35.2%, Loc-ADPD by Δ+87.8% in mAP). Improvements are especially high when considering larger IoU-thresholds and huge improvements are also achieved in loc-acc at all thresholds. Both models also outperform AGXNet (which uses anatomy-level supervision) by large margins (MIL-ADPD by Δ + 47.9% and Loc-ADPD by Δ + 105.5% mAP), while improvements on larger thresholds are smaller here. Even when compared to Faster R-CNN trained on a small set of fully supervised samples, MIL-ADPD is competitive (Δ + 6.5%), while Loc-ADPD improves by Δ + 48.0%. However, on larger thresholds (IoU@50) the supervised baseline slightly outperforms MIL-ADPD, while Loc-ADPD is still superior. This shows that using anatomical regions as proxies is an effective approach to tackle pathology detection. While using image-level annotations (MIL-ADPD) already gives promising results, the full potential is only achieved using anatomy-level supervision (Loc-ADPD). Unlike Loc-ADPD and MIL-ADPD, all baselines were either trained or finetuned on the CXR8 dataset, showing that our method generalizes well to unseen datasets and that our class mapping is effective.For detailed results per pathology we refer to the supp. material. We found that the improvements of MIL-ADPD are mainly due to improved performance on Cardiomegaly and Mass detection, while Loc-ADPD consistently outperforms all baselines on all classes except Nodule, often by a large margin.Ablation Study. In Table 1 we also show the results of different ablation studies. Without WBF, results degrade for both of our models, highlighting the importance of merging region boxes. Combining the training strategies of Loc-ADPD and MIL-ADPD does not lead to an improved performance. Different class mappings between training and evaluation set are studied in the supp. material.Qualitative Results. As shown in Fig. 4 Loc-ADPD detects cardiomegaly almost perfectly, as it is always exactly localized at one anatomical region. Other pathologies are detected but often with too large or too small boxes as they only cover parts of anatomical regions or stretch over several of them, which cannot be completely corrected using WBF. Detection also works well for predicting several overlapping pathologies. For qualitative comparisons between Loc-ADPD and MIL-ADPD, we refer to the supp. material."
Anatomy-Driven Pathology Detection on Chest X-rays,5,Discussion and Conclusion,"Limitations. While our proposed ADPD method outperforms all competing models, it is still subject to limitations. First, due to the dependence on region proxies, for pathologies covering only a small part of a region, our models predict the whole region, as highlighted by their incapability to detect nodules. We however note that in clinical practice, chest X-rays are not used for the final diagnosis of such pathologies and even rough localization can be beneficial. Additionally, while not requiring pathology bounding boxes, our models still require supervision in the form of anatomical region bounding boxes, and Loc-ADPD requires anatomy-level labels. However, anatomical bounding boxes are easier to annotate and predict than pathology bounding boxes, and the used anatomylevel labels were extracted automatically from radiology reports [21]. While our work is currently limited to chest X-rays, we see huge potential for modalities where abnormalities can be assigned to meaningful regions."
Anatomy-Driven Pathology Detection on Chest X-rays,,Conclusion.,"We proposed a novel approach tackling pathology detection on chest X-rays using anatomical region bounding boxes. We studied two training approaches, using anatomy-level pathology labels and using image-level labels with MIL. Our experiments demonstrate that using anatomical regions as proxies improves results compared weakly supervised methods and supervised training on little data, thus providing a promising direction for future research."
Anatomy-Driven Pathology Detection on Chest X-rays,,Fig. 1 .,
Anatomy-Driven Pathology Detection on Chest X-rays,,Fig. 2 .,
Anatomy-Driven Pathology Detection on Chest X-rays,,Fig. 3 .,
Anatomy-Driven Pathology Detection on Chest X-rays,,,
Anatomy-Driven Pathology Detection on Chest X-rays,,Fig. 4 .,
Anatomy-Driven Pathology Detection on Chest X-rays,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0_6.
Self-supervised Learning for Physiologically-Based Pharmacokinetic Modeling in Dynamic PET,1,Introduction,"Positron Emission Tomography (PET) is a 3D imaging modality using radiopharmaceuticals, such as F-18-fluorodeoxyglucose (FDG), as tracers. Newly introduced long axial field-of-view PET scanners have enabled dynamic PET (dPET) with frame duration < 1 min [18], allowing the observation of dynamic metabolic processes throughout the body. For a given voxel in space, the radioactivity concentration over time can be described by a characteristic curve known as Time Activity Curve (TAC), measured in [Bq/ml]. TACs can be described by mathematical functions, called physiologically-based pharmacokinetic (PBPK) models or kinetic models (KM) [14]. The parameters of the KM represent physiologically relevant quantities and are often called micro-parameters, whereas their combinations are called macro-parameters [14,20]. While the former can be retrieved only by methods that directly use the KM function, the latter can be computed by simplified linearized methods (such as the Logan and the Patlak-Gjedde plots). The approaches to extract KM parameters can be split in two categories: Volume of Interest (VoI) methods, in which the average TAC in a VoI is used, or voxel-based methods. Despite the former displaying less noise and, therefore, lower variance in the kinetic parameters (KPs), VoI-based methods only provide organ-wise information. On the other hand, voxel-based methods allow the generation of parametric images (KPIs), in which the KPs are visualized at a voxel level, but suffer from motion and breathing artifacts, and require more computational power or simplified linearized methods.Parametric images are reported to be superior in lesion detection and delineation when compared to standard-of-care activity-and weight-normalized static PET volumes, known as Standard Uptake Value (SUV) volumes [6,7]. Changes in the KPs during oncological therapy are associated with pathological response to treatment, whereas this is not true for changes in SUV [15]. Despite the advantages of KPIs in diagnosis, the generation of accurate micro-parametric images is not yet possible in clinical practice.To address the problem of the generation of micro-parametric images, we propose a custom 3D UNet [3] to estimate kinetic micro-parameters in an unsupervised setting drawing inspiration from physics-informed neural networks (PINN). The main contributions of this work are:-A self-supervised formulation of the problem of kinetic micro-parameters estimation -A spatio-temporal deep neural network for parametric images estimation -A quantitative and qualitative comparison with conventional methods for PBPK modelingThe code is available at: https://github.com/FrancescaDB/self_supervised_PBPK_modelling."
Self-supervised Learning for Physiologically-Based Pharmacokinetic Modeling in Dynamic PET,1.1,Related Work,"Finding the parameters of a KM is a classical optimization problem [2,19,21] solved by fitting the KM equation to a measured TAC in a least squares sense [1,14,17]. The non-linearity of the KM functions makes this approach prone to overfitting and local minima, and sensitive to noise [14]. Therefore, non-linear parametric imaging is still too noisy for clinical application [20].To limit the drawbacks of the non-linear parameter fitting, the identification of KPs is commonly performed using simplified linearized versions of the KM [6,20], such as the Logan and the Patlak-Gjedde plots [5,16], which are often included in clinical software for KM such as PMOD 1 .Preliminary works towards KM parameter estimation in dPET imaging have recently begun to be explored. Moradi et al. used an auto-encoder along with a Gaussian process regression block to select the best KM to describe simulated kinetic data [13]. A similar approach was presented for the quantification of myocardial blood flow from simulated PET sinograms [11]. Huang et al. used a supervised 3D U-Net to predict a macro-parametric image using an SUV image as input, and a Patlak image derived from dPET acquisition as ground truth [9]. Cui et al. proposed a conditional deep image prior framework to predict a macro-parametric image using a DNN in an unsupervised setting [4]. Finally, a supervised DNN was used to predict Patlak KPIs from Dynamic PET sinograms [12]. Until now, methods used simulated data [11,13] or static PET [9], were supervised [9,[11][12][13] or predicted macro-parameters [4,9,12]."
Self-supervised Learning for Physiologically-Based Pharmacokinetic Modeling in Dynamic PET,2,Methodology and Materials,"We propose to compute the kinetic micro-parameters in a self-supervised setting by directly including the KM function in the loss function and comparing the predicted TAC to the measured TAC. For this reason, an understanding of the KM is fundamental to describing our pipeline."
Self-supervised Learning for Physiologically-Based Pharmacokinetic Modeling in Dynamic PET,2.1,Kinetic Modelling,"The concentration of the tracer C(t) [Bq/ml] in each tissue can be described as a set of ordinary differential equations [20]. It represents the interaction of two compartments, F (t) (free) and B(t) (bound), and takes as input the radioactivity concentration in blood or plasma A(t) [14]:where [6,20]. Equation 1 describes a general two-tissue compartment (2TC) kinetic model. However, an FDG-TAC is conventionally described by an irreversible 2TC, in which k 4 is set to 0 [20]. Therefore, in the following, we will use k 4 = 0. Moreover, including the blood fraction volume V B [•] allows to correctly model the contribution to the radioactivity in a voxel coming from vessels that are too small to be resolved by the PET scanner [14]. Together, the TAC of each voxel in an FDG dPET acquisition can be modeled as, and solved using the Laplace transform [20]:(2)"
Self-supervised Learning for Physiologically-Based Pharmacokinetic Modeling in Dynamic PET,2.2,Proposed Pipeline,"Our network takes as input a sequence of 2D axial slices and returns a 4-channel output representing the spatial distribution of the KM parameters of a 2TC for FDG metabolisation [14]. The network has a depth of four, with long [3] and short skip connections [10]. The kernel size of the max-pooling is [2, 2, 2]. After the last decoder block, two 3D convolutional layers (with kernel size [3,3,3] and [64, 1, 1]) estimate the KPs per voxel given the output feature of the network. Inside the network the activation function is ELU and critically batch normalization is omitted. The network was trained with an initial learning rate of 10 -4 , which was divided by half every 25 epochs, for a maximum of 500 epochs. Following the approach taken by Küstner et al. for motion correction of 4D spatio-temporal CINE MRI [10], we replaced a conventional 3D convolutional layer with (2+1)D spatial and temporal convolutional layers. The spatial convolutional layer is a 3D convolutional layer with kernel size [1,3,3] in [t, x, y]. Similarly, the temporal convolutional layer has a kernel size of [3,1,1].We imposed that the KPs predicted by the network satisfy Eq. 2 by including it in the computation of the loss. At a pixel level, we computed the mean squared error between the TAC estimated using the corresponding predicted parameters ( TAC i ) and the measured one (TAC i ), as seen in Fig. 1.We introduced a final activation function to limit the output of the network to the valid parameter domain of the KM function. Using the multi-clamp function, each channel of the logits is restricted to the following parameter spaces:The limits of the ranges were defined based on the meaning of the parameter (as in V B ), mathematical requirements (as in the minimum values of k 2 and k 3 , whose sum can not be zero) [6] or previous knowledge on the dataset derived by the work of Sari et al. [16] (as in the maximum values of K 1 , k 2 and k 3 ).We evaluated the performance of the network using the Mean Absolute Error (MAE) and the Cosine Similarity (CS) between TAC i and TAC i ."
Self-supervised Learning for Physiologically-Based Pharmacokinetic Modeling in Dynamic PET,2.3,Curve Fit,"For comparison, parameter optimization via non-linear fitting was implemented in Python using the scipy.optimize.curve_fit function (version 1.10), with step equal to 0.001. The bounds were the same as in the DNN."
Self-supervised Learning for Physiologically-Based Pharmacokinetic Modeling in Dynamic PET,2.4,Dataset,"The dataset is composed of 23 oncological patients with different tumor types. dPET data was acquired on a Biograph Vision Quadra for 65 min, over 62 frames. The exposure duration of the frames were 2 × 10 s, 30 × 2 s, 4 × 10 s, 8 × 30 s, 4 × 60 s, 5 × 120 s and 9 × 300 s. The PET volumes were reconstructed with an isotropic voxel size of 1.6 mm. The dataset included the label maps of 7 organs (bones, lungs, heart, liver, kidneys, spleen, aorta) and one image-derived input function A(t) [Bq/ml] from the descending aorta per patient. Further details on the dataset are presented elsewhere [16].The PET frames and the label map were resampled to an isotropic voxel size of 2.5 mm. Then, the dataset was split patient-wise into training, validation, and test set, with 10, 4, and 9 patients respectively. Details on the dataset split are available in the Supplementary Material (Table 1). The training set consisted of 750 slices and the validation consisted of 300. In both cases, 75 axial slices per patient were extracted in a pre-defined patient-specific range from the lungs to the bladder (included) and were cropped to size 112 × 112 pixels."
Self-supervised Learning for Physiologically-Based Pharmacokinetic Modeling in Dynamic PET,3,Results,"Table 1 shows the results of the 8 ablation studies we performed to find the best model. We evaluated the impact of the design of the convolutional and maxpooling kernels, as well as the choice of the final activation function. The design of the max pooling kernel (i.e., kernel size equal to [2,2,2] or [1,2,2]) had no measurable effects in terms of CS in most of the experiments, with the exception of Exp. 3.2, where max-pooling only in space resulted in a drop of 0.06. When evaluating the MAE, the use of 3D max-pooling was generally better.  [16] in green. The exact values are reported in the Supplementary Material (Table 3 and4) and in [16]."
Self-supervised Learning for Physiologically-Based Pharmacokinetic Modeling in Dynamic PET,,(Color figure online),"The most important design choice is the selection of the final activation function. Indeed, the multi-clamp final activation function was proven to be the best both in terms of CS (Exp 4.1: CS = 0.78 ± 0.05) and MAE (Exp 4.2: MAE = 3.27 ± 2.01). Compared to the other final activation functions, when the multi-clamp is used the impact of the max-pooling design is negligible also in terms of MAE. For the rest of the experiments, the selected configuration is the one from Exp. 4.1 (see Table 1).Figure 2 shows the KPs for four selected organs as computed with the proposed DNN (KP DNN ), as computed with curve fit using only the 9 patients of the test set (KP CF ) and using all 23 patients (KP ref CF ) [16]. The voxel-wise KPs predicted by the DNN were averaged over the available organ masks.In terms of run-time, the DNN needed ≈ 1 min to predict the KPs of the a whole-body scan (≈ 400 slices), whereas curve fit took 8.7 min for a single slice: the time reduction of the DNN is expected to be ≈ 3.500 times."
Self-supervised Learning for Physiologically-Based Pharmacokinetic Modeling in Dynamic PET,4,Discussion,"Even though the choice of the final activation function has a greater impact, the selection of the kernel design is important. Using spatial and temporal convo- lution results in an increase in the performance (+0.01 in CS) and reduces the number of trainable parameters (from 2.1 M to 8.6 K), as pointed out by [10]. Therefore, the convergence is reached faster. Moreover, the use of two separate kernels in time and space is especially meaningful. Pixel counts for a given exposure are affected by the neighboring count measurements due to the limited resolution of the PET scanner [20]. The temporally previous or following counts are independent. In general, there is good agreement between KP DNN , KP CF and KP ref CF . The DNN prediction of K 1 and k 2 in the spleen and k 3 in the lungs is outside the confidence interval of the results published by Sari et al. [16].An analysis per slice of the metrics shows that the CS between TAC i and TAC i changes substantially depending on the region: CS max = 0.87 within the liver boundaries and CS min = 0.71 in the region corresponding to the heart and lungs (see Fig. 3a). This can be explained by the fact that V B is underestimated for the heart and aorta. The proposed network predicts V heart B = 0.376 ± 0.133 and V aorta B = 0.622 ± 0.238 while values of nearly 1 are to be expected. This is likely due to breathing and heartbeat motion artifacts, which cannot be modeled properly with a 2TC KM that assumes no motion between frames.Figure 3b-e shows the central coronal slice of the four KPIs in an exemplary patient. As expected, K 1 is high in the heart, liver, and kidney. Similarly, the blood fraction volume V B is higher in the heart, blood vessels, and lungs.The KP DNN are more homogeneous than KP CF , as can be seen in the exemplary K 1 axial slice shown in Fig. 4. A quantitative evaluation of the smoothness of the images is reported in the Supplementary Material (Fig. 1). Moreover, the distribution in the liver is more realistic in KP DNN , where the gallbladder can be seen as an ellipsoid between the right and left liver lobes. High K 1 regions are mainly within the liver, spleen, and kidney for KP DNN , while they also appear in unexpected areas in the KP CF (e.g., next to the spine or in the region of the stomach).The major limitation of this work is the lack of ground truth and a canonical method to evaluate quantitatively its performance. This limitation is inherent to PBPK modeling and results in the need for qualitative analyses based on expected physiological processes. A possible way to leverage this would be to work on simulated data, yet the validity of such evaluations strongly depends on how realistic the underlying simulation models are. As seen in Fig. 3a, motion (gross, respiratory, or cardiac) has a major impact on the estimation quality. Registering different dPET frames has been shown to improve conventional PBPK models [8] and would possibly have a positive impact on our approach."
Self-supervised Learning for Physiologically-Based Pharmacokinetic Modeling in Dynamic PET,5,Conclusion,"In this work, inspired by PINNs, we combine a self-supervised spatio-temporal DNN with a new loss formulation considering physiology to perform kinetic modeling of FDG dPET. We compare the best DNN model with the most commonly used conventional PBPK method, curve fit. While no ground truth is available, the proposed method provides similar results to curve fit but qualitatively more plausible images in physiology and with a radically shorter run-time.Further, our approach can be applied to other KMs without significantly increasing the complexity and the need for computational power. In general, Eq. 2 should be modified to represent the desired KM [20], and the number of channels of the output of the network should be the same as the KP to be predicted.Overall, this work offers scalability and a new research direction for analysing pharmacokinetics."
Self-supervised Learning for Physiologically-Based Pharmacokinetic Modeling in Dynamic PET,,Fig. 1 .,
Self-supervised Learning for Physiologically-Based Pharmacokinetic Modeling in Dynamic PET,,Fig. 3 .,
Self-supervised Learning for Physiologically-Based Pharmacokinetic Modeling in Dynamic PET,,Fig. 4 .,
Self-supervised Learning for Physiologically-Based Pharmacokinetic Modeling in Dynamic PET,,Table 1 .,"Fig. 2. Comparison between the kinetic parameters obtained with different methods: KPDNN in blue, KPCF in orange and, as plausibility check, KP ref CF"
Self-supervised Learning for Physiologically-Based Pharmacokinetic Modeling in Dynamic PET,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0_28.
Foundation Ark: Accruing and Reusing Knowledge for Superior and Robust Performance,1,Introduction,"Deep learning nowadays offers expert-level and sometimes even super-expertlevel performance, deepening and widening its applications in medical imaging and resulting in numerous public datasets for research, competitions, and challenges. These datasets are generally small as annotating medical images is challenging, but achieving superior performance by deep learning demands massive annotated data for training. For example, Google's proprietary CXR Foundation Model (CXR-FM) was trained on 821,544 labeled and mostly private CXRs [16]. We hypothesize that powerful and robust open foundation models can be trained by aggregating numerous small public datasets. To test this hypothesis, we have chosen CXRs because they are one of the most frequently used modalities, and our research community has accumulated copious CXRs (see Table 1). However, annotations associated with these public datasets are inconsistent in disease coverage. Even when addressing the same clinical issue, datasets created at different institutions tend to be annotated differently. For example, VinDr-CXR [13] is associated with global (image-level) and local (boxed-lesions) labels, while MIMIC-CXR [4] has no expert labels per se but comes with radiology reports. ChestX-ray14 [19] and CheXpert [4] both cover 14 conditions at the image level, and their 14 conditions have overlaps but are not exactly the same. Therefore, this paper seeks to address a critical need: How to utilize a large number of publicly-available images from different sources and their readily-accessible but heterogeneous expert annotations to pretrain generic source (foundation) models that are more robust and transferable to application-specific target tasks.To address this need, we have developed a framework, called Ark for its ability of accruing and reusing knowledge embedded in heterogeneous expert annotations with numerous datasets, as illustrated in Fig. 1. We refer to the pretrained models with Ark as Foundation Ark or simply as Ark for short. To demonstrate Ark's capability, we have trained two models: Ark-5 on Datasets 1-5 and Ark-6 on Datasets 1-6 (Table 1), evaluated them on a wide range of 10 tasks via fine-tuning and on 6 tasks via linear probing, and demonstrated our Ark models outperform the SOTA fully/self-supervised baselines (Table 2) and Google CXR-FM1 (Fig. 2). Ark also exhibits superior robustness over CXR-FM in mitigating underdiagnosis and reducing gender-related biases, with lower false-negative rates and greater robustness to imbalanced data (Fig. 3).This performance enhancement is attributed to a simple yet powerful observation that aggregating numerous public datasets costs nearly nothing but enlarges data size, diversifies patient populations, and accrues expert knowledge from a large number of sources worldwide; thereby offering unprecedented performance yet reducing annotation cost. More important, Ark is fundamentally different from self-supervised learning (SSL) and federated learning (FL) in concept. SSL can naturally handle images from different sources, but their associated expert annotations are left out of pretraining [10]. Clearly, every bit of expert annotation counts, conveying valuable knowledge. FL can utilize data with annotations from different sources, typically involving homogeneous labels, but it mainly concerns data privacy [12]. By contrast, Ark focuses on heterogeneous expert annotations with public data with no concern for data privacy and employs centralized training, which usually offers better performance with the same amount of data and annotation than distributed training as in FL.Through this work, we have made the following contributions: (1) An idea that aggregates public datasets to enlarge and diversify training data; (2) A student-teacher model with multi-task heads via cyclic pretraining that accrues expert knowledge from existing heterogeneous annotations to achieve superior and robust performance yet reduce annotation cost; (3) Comprehensive experiments that evaluate our Ark via fine-tuning, linear-probing, and few-shot learning on a variety of target tasks, demonstrating Ark's better generalizability and transferability in comparison with SOTA methods and Google CXR-FM; and (4) Empirical analyses for a critical yet often overlooked aspect of medical imaging models-robustness to underdiagnosis and gender imbalance, highlighting Ark significantly enhances reliability and safety in clinical decision-making.Table 1. Publicly available datasets are generally small and heterogeneously annotated. Our Ark (Fig. 1) aims to aggregate numerous datasets with heterogeneous annotations to diversify patient population, accrue knowledge from diverse experts, and meet the demand by deep learning for massive annotated training data, offering superior and robust performance (Table 2, Fig. 2 and Fig. 3) yet reducing annotation cost. The labels of CXRs in MIMIC-II are derived from their corresponding radiology reports using NegBio [15] and CheXpert [4]. c SIIM-ACR, originally for pneumothorax segmentation, is converted into a classification task for linear probing, as CXR-FM cannot be evaluated for segmentation using its only released API."
Foundation Ark: Accruing and Reusing Knowledge for Superior and Robust Performance,2,Accruing and Reusing Knowledge,"Our Ark aims to learn superior and robust visual representations from largescale aggregated medical images by accruing and reusing the expert knowledge embedded in all available heterogeneous labels. The following details our Ark.Accruing Knowledge into the Student via Cyclic Pretraining. A significant challenge with training a single model using numerous datasets created for different tasks is label inconsistency (i.e., heterogeneity) (see Table 3 in Appendix). Manually consolidating heterogeneous labels from different datasets would be a hassle. To circumvent this issue, for each task, we introduce a specific classifier, called task head, to learn from its annotation and encode the knowledge into the model. A task head can be easily plugged into Ark, making Ark scalable to additional tasks. With multi-task heads, Ark can learn from multiple tasks concurrently or cyclically. In concurrent pretraining, a mini-batch is formed by randomly sampling an equal number of images from each dataset, and the loss for each image is computed based on its associated dataset id and labels. This idea is intuitive, but the model hardly converges; we suspect that the loss summation over all task heads simultaneously weakens gradients for back-propagation, causing confusion in weight updating. We opt for cyclic pretraining by iterating through all datasets sequentially in each round to accrue expert knowledge from all available annotations, a strategy that, we have found, stabilizes Ark's pretraining and accelerates its convergence.Accruing Knowledge into the Teacher via Epoch-Wise EMA. To further summarize the accrued knowledge and accumulate the learning experiences in the historical dimension, we introduce into Ark a teacher model that shares the same architecture with the student. The teacher is updated using exponential moving average (EMA) [18] based on the student's one epoch of learning at the end of each task. Eventually, the expert knowledge embedded in all labels and all historical learning experiences are accrued in the teacher model for further reuse in the cyclic pretraining and for future application-specific target tasks.Reusing Accrued Knowledge from the Student to Bolster Cyclic Pretraining. If the model learns from multiple tasks sequentially, it may ""forget"" the previously learned knowledge, and its performance on an old task may degrade catastrophically [7]. This problem is addressed naturally in Ark by cyclic pretraining, where the model revisits all the tasks in each round and reuses all knowledge accrued from the previous rounds and tasks to strengthen its learning from the current and future tasks. That is, by regularly reviewing the accrued knowledge through task revisitation, Ark not only prevents forgetting but also enables more efficient and effective learning from multiple tasks iteratively.Reusing Accrued Knowledge from the Teacher to Mitigate Forgetting.To leverage the accumulated knowledge of the teacher model as an additional self-supervisory signal, we incorporate a consistency loss between the student and the teacher, as shown in Fig. 1. To enhance this supervision, we introduce projectors in Ark that map the outputs of the student and teacher encoders to the same feature space. This further reinforces the feedback loop between the student and teacher models, facilitating the transfer of historical knowledge from the teacher to the student as a reminder to mitigate forgetting."
Foundation Ark: Accruing and Reusing Knowledge for Superior and Robust Performance,,Ark has the following properties:,"• Knowledge-centric. Annotating medical images by radiologists for deep learning is a process of transferring their in-depth knowledge and expertise in interpreting medical images and identifying abnormalities to a medium that is accessible for computers to learn. Ark's superior and robust performance is attributed to the accumulation of expert knowledge conveyed through medical imaging annotations from diverse expert sources worldwide. At the core of Ark is acquiring and sharing knowledge: ""knowledge is power"" (Mac Flecknoe) and ""power comes not from knowledge kept but from knowledge shared"" (Bill Gates). • Label-agnostic, task-scalable and annotation-heterogeneous. Ark is label-agnostic as it does not require prior label ""understanding"" of public datasets, but instead uses their originally-provided labels. It is designed with pluggable multi-task heads and cyclic pretraining to offer flexibility and scalability for adding new tasks without manually consolidating heterogeneous labels or training task-specific controllers/adapters [22]. Therefore, Ark intrinsically handles the annotation heterogeneity across different datasets. • Application-versatile. Ark trains versatile foundation models by utilizing a large number of publicly-available images from diverse sources and their readily-accessible diagnostic labels. As shown in Sect. 3, Ark models are more robust, generalizable, and transferable to a wide range of application-specific target tasks across diseases (e.g., pneumothorax, tuberculosis, cardiomegaly) and anatomies (e.g., lung, heart, rib), highlighting Ark's versatility."
Foundation Ark: Accruing and Reusing Knowledge for Superior and Robust Performance,3,Experiments and Results,"Our Ark-5 and Ark-6 take the base version of the Swin transformer (Swin-B) [9] as the backbone, feature five and six independent heads based on the pretraining tasks and their classes, and are pretrained on Datasets 1-5 and 1-6, respectively, with all validation and test data excluded to avoid test-image leaks. In the following, both models are evaluated via transfer learning (in Sects. We also include a comparison with a SOTA domain-adapted model [10] that was first pretrained on ImageNet and then on a large-scale domain-specific dataset comprising 926,028 CXRs from 13 different sources. All downstream models share the same Swin-B backbone, where the encoder is initialized using the pretrained weights and a task-specific classification head is re-initialized based on the number of classes for the target task. We fine-tune all layers in the downstream models under the same experimental setup. We also report the results of training the downstream models from scratch (random initialization) as the performance lower bound. Note that Google CXR-FM cannot be included for comparison as it is not publicly released for fine-tuning."
Foundation Ark: Accruing and Reusing Knowledge for Superior and Robust Performance,,Results and Analysis:,"As shown in Table 2, our Ark models consistently outperform the SOTA fully/self-supervised ImageNet pretrained models on all target tasks. These results highlight the benefit of leveraging additional domain-relevant data in pretraining to reduce the domain gap and further improve the model's performance on target tasks. Furthermore, compared with the self-supervised domain-adapted model that utilizes 926K CXRs for pretraining, Ark models yield significantly superior performance on Dataset 1, 3-5 with only 335K CXRs, and on-par performance on 2.NIHC with 704K CXRs. These results demonstrate the superiority of Ark that accrues and reuses the knowledge retained in heterogeneous expert annotations from multiple datasets, emphasizing the importance of learning from expert labels. Moreover, we observe that Ark-6 consistently outperforms Ark-5, indicating the importance of incorporating more data and annotations from diverse datasets in pretraining. "
Foundation Ark: Accruing and Reusing Knowledge for Superior and Robust Performance,3.2,Ark Provides Generalizable Representations for Segmentation Tasks,"Experimental Setup: To evaluate the generalizability of Ark's representations, we transfer the Ark models to five segmentation tasks involving lungs, heart, clavicles, and ribs, and compare their performance with three SOTA fully/selfsupervised models. We build the segmentation network upon UperNet [20], which consists of a backbone network, a feature pyramid network, and a decoder network. We implement the backbone network with Swin-B and initialize it with the pretrained weights from the Ark and those aforementioned SOTA models. The remaining networks are randomly initialized. We then fine-tune all layers in the segmentation models under the same experimental setup."
Foundation Ark: Accruing and Reusing Knowledge for Superior and Robust Performance,,Results and Analysis:,"As seen in Table 2, Ark models achieve significantly better performance than the SOTA models, demonstrating that Ark learned generalizable representations for delineating organs and bones in CXR. This superior performance is achieved by pretraining using large-scale CXRs and various disease labels from diverse datasets. Clinically, certain thoracic abnormalities can be diagnosed by examining the edges of the lungs, heart, clavicles, or ribs in CXR.For instance, a pneumothorax can be detected by observing a visible ""visceral pleural line"" along part or all of the length of the lateral chest wall [11]. Cardiomegaly can be diagnosed when the heart appears enlarged, with maximum diameter of the heart exceeding a pre-defined cardiothoracic ratio [19]. Fractures can be identified when the edges of the clavicles or ribs appear abnormally displaced or the bone cortex appears offset [3]. Therefore, leveraging diagnostic information from disease labels during pretraining enables Ark models to better capture the nuanced and varied pathological patterns, strengthening the models' ability to represent anatomically specific features that reflect abnormal conditions in various oragns or bones. By contrast, the SimMIM (IN → CXR(926K)) model is pretrained with a self-supervised masked image modeling proxy task, which may use many clues to reconstruct the masked patches that are not necessarily related to pathological conditions, leading to lower performance despite training on more images."
Foundation Ark: Accruing and Reusing Knowledge for Superior and Robust Performance,3.3,Ark Offers Embeddings with Superior Quality over Google CXR-FM,"Experimental Setup: To highlight the benefits of learning from more detailed diagnostic disease labels, we compare our Ark models with Google CXR-FM. CXR-FM was trained on a large dataset of 821,544 CXRs from three different sources, but with coarsened labels (normal or abnormal). By contrast, our Ark models are trained with less data, but aims to fully utilize all labels provided by experts in the original datasets. Furthermore, Ark models employ a much smaller backbone (88M parameters) compared with CXR-FM using EfficientNet-L2 (480M parameters). Since Google CXR-FM is not released and cannot be finetuned, we resorted to its released API to generate the embeddings (informationrich numerical vectors) for all images in the target tasks. For the sake of fairness, we also generated the embeddings from Ark's projector, whose dimension is the same as Google's. To evaluate the quality of the learned representations of these models, we conduct linear probing by training a simple linear classifier for each target task. The performance of both models is evaluated on six target tasks, including an unseen dataset, 10.SIIM, where the images have not been previously seen by the Ark models during pretraining. Additionally, we perform the same evaluation on 10.SIIM with partial training sets or even few-shot samples to further demonstrate the high quality of our Ark models' embeddings. shows that both Ark-5 and Ark-6 consistently outperform CXR-FM in small data regimes, highlighting the superiority of Ark's embeddings, which carry richer information that can be utilized more efficiently. These results demonstrate that Ark models learn higher-quality representations with less pretraining data while employing a much smaller backbone than CXR-FM, highlighting that learning from more granular diagnostic labels, such as Ark, is superior to learning from coarsened normal/abnormal labels."
Foundation Ark: Accruing and Reusing Knowledge for Superior and Robust Performance,3.4,Ark Shows a Lower False-Negative Rate and Less Gender Bias,"Experimental Setup: Underdiagnosis can lead to delayed treatment in healthcare settings and can have serious consequences. Hence, the false-negative rate (FNR) is a critical indicator of the robustness of a computer-aided diagnosis (CAD) system. Furthermore, population-imbalanced data can train biased models, adversely affecting diagnostic performance in minority populations. Therefore, a robust CAD system should provide a low false-negative rate and strong resilience to biased training data. To demonstrate the robustness of our Ark models in comparison with Google CXR-FM, we first compute the FNRs in terms of gender on 1.CXPT and 2.NIHC. We further investigate gender biases in Ark-6 and CXR-FM on 1.CXPT using gender-exclusive training sets. We follow the train/test splits in [8] to ensure a balanced number of cases per class in 40 male/female-only folds. We train linear classifiers on those folds using embeddings from Ark-6 and CXR-FM, and then evaluate these classifiers on the corresponding male/female-only test splits. The biased model will show significant differences in performance when training and test data are of the opposite gender. We detail this setup in Appendix E. Results and Analysis: Figure 3(a) illustrates that Ark models have lower FNRs than CXR-FM for both genders on both tasks, demonstrating that Ark models are less likely to underdiagnose disease conditions than CXR-FM. In Fig. 3(b), the biases in the pretrained models are measured by performance differences between linear classifiers trained on male-only and female-only embeddings. The upper part of Fig. 3(b) depicts the results of testing on female-only sets, where the classifiers trained on male-only embeddings generally perform poorly compared with those trained on female embeddings, revealing gender biases due to data imbalance. Among the 12 diseases, the classifiers trained with Google's embeddings have unbiased performances for only 4 diseases, whereas those using Ark-6's embeddings perform in an unbiased fashion with no significant differences for the 8 diseases. The same situation occurs when testing is performed on male patients as shown in the lower part of Fig. 3(b). The gender bias analysis demonstrates that Ark has greater robustness to the extremely imbalanced data that contributes to gender bias in computer-aided diagnosis."
Foundation Ark: Accruing and Reusing Knowledge for Superior and Robust Performance,4,Conclusions and Future Work,"We have developed Foundation Ark, the first open foundation model, that realizes our vision: accruing and reusing knowledge retained in heterogeneous expert annotations with numerous datasets offers superior and robust performance.Our experimental results are strong on CXRs, and we plan to extend Ark to other modalities. We hope Ark's performance encourages researchers worldwide to share codes and datasets big or small for creating open foundation models, accelerating open science, and democratizing deep learning for medical imaging."
Foundation Ark: Accruing and Reusing Knowledge for Superior and Robust Performance,,Fig. 1 .,
Foundation Ark: Accruing and Reusing Knowledge for Superior and Robust Performance,,Fig. 2 .,
Foundation Ark: Accruing and Reusing Knowledge for Superior and Robust Performance,,Fig. 3 .,
Foundation Ark: Accruing and Reusing Knowledge for Superior and Robust Performance,,,
Foundation Ark: Accruing and Reusing Knowledge for Superior and Robust Performance,,Table 2 .,
LOTUS: Learning to Optimize Task-Based US Representations,1,Introduction,"Ultrasound (US) imaging is a widely used modality in medical diagnosis for screening and follow-up examinations. Hence, precise segmentation of the target organs is crucial for diagnosing or tracking disease progression. Recently, the application of deep learning for ultrasound image segmentation has emerged as a powerful tool. However, accurate segmentation of US images remains a challenging task due to the complexity of the modality, as it has limited resolution and often contains clutter, shadowing and reverberation artefacts. This leads to a general lack of annotated data, and additionally, due to varying operator skills, there is high heterogeneity of ground truth data labels, which is the primary factor hampering solid segmentation performance [6].On the other hand, large pixel-level labeled CT datasets are freely available online. Thus to overcome the lack of ground truth ultrasound data, researchers have utilized ultrasound simulators to generate large sets of ultrasound-like images from CT label maps and use them for training [10]. Simulated ultrasound data automatically provides a labeled pair of the tissue distribution and the resulting b-mode image and can be augmented with rotational, brightness, contrast, probe, and scanner variations.Generally, ultrasound simulators can be categorized into two types based on their modeling techniques: finite difference models of the wave equation, modeling the mechanical propagation of sound waves through tissues, and simulating ray casting through tissue maps represented by ultrasound tissue properties [2,5,11]. Although the former can model higher-order non-linear effects, producing realistic images, generating a single image can take hours. The latter, on the other hand, is much faster and can be integrated into other systems [1,4]. While leveraging automatically generated ultrasound simulations with corresponding labels for training has benefits, models trained on simulations fail when applied directly to real, as they cannot perfectly simulate ultrasound images without distinguishable differences from real ones.Thus, one main challenge when working with simulated data is reducing the domain shift between simulated and real data. In a supervised sense, many works have investigated the realistic parametrization of ultrasound simulators to reduce the domain shift between simulated and real data ultrasound data [12] and augmentation of ultrasound b-modes [13]. Recent domain adaptation models [8,19] employing generative adversarial networks have shown promise in improving image synthesis in an unsupervised manner. Moreover, recent works show their application in combination with segmentation or registration tasks between Xray and CT or MRI scans [3,18]. Further works show their application in ultrasound by closing the real-simulation gap via translation from simulated images to ""realistic"" ones that match the target domain, thereby enabling the application of trained segmentation networks on real images [14,16].However, those methods require separate training for each part of the architecture, limiting the models' flexibility. Notably, [15] proposes using an intermediate representation image with common properties between CT and US for the task of aorta segmentation. However, the intermediate image is not formulated differentiably but is statically calibrated, and the whole pipeline is not trained end-to-end.Contributions. In this paper, we propose a novel approach for learning to optimize task-based ultrasound image representations. During training, we render an intermediate US image representation from segmented public CT scans and use it as input to a segmentation network. Our ultrasound renderer is fully differentiable and learns to optimize the parameters necessary for physics-based ultrasound simulation, guided by the downstream segmentation task. At the same time, we train an image style transfer network between real and simulated data to achieve simultaneous image synthesis as well as automatic segmentation on US images in an end-to-end training setting. In addition, no labels are required for the real ultrasound images, which are also unpaired with the simulated ultrasound images. We evaluate our method on aorta and vessel segmentation. Our quantitative and qualitative results demonstrate that our method learns the optimal image for the task of interest. The source code for our method is publicly available1 .  Building on the mathematical foundations of ray tracing and ultrasound echo generation proposed by [2], we adopt those equations and modify them to be differentiable while still accurately depicting the physics behind generating US B-mode images. Input to the renderer is a 2D label map with tissue labels. Each tissue label has assigned five parameters with default values2 which describe ultrasound-specific tissue characteristics and control the whole rendering generation -attenuation coefficient α, acoustic impedance Z, as well as three parameters that define the speckle distributionμ 0 , μ 1 , σ 0 . For each 2D label map, we use these parameters to define three sub-maps: attenuation, reflection, and scatter maps. We generate those maps by modeling ultrasound waves as rays starting from the transducer, which is the top of the label map, and propagating through media using physical laws. Ray casting is simulated by defining a function E i (d) for each scanline i at a distance d from the transducer, which describes the recorded ultrasound echo signal as:"
LOTUS: Learning to Optimize Task-Based US Representations,2,Methodology,
LOTUS: Learning to Optimize Task-Based US Representations,2.1,Differentiable Ultrasound Renderer,"where R i (d) is the energy reflected from the interfaces between two tissues as the beam passes through them and B i (d) represents the energy backscattered from the scattering points along the scanline. The reflection of the ray is described as:where I i (d) is the remaining energy of the ray, which gets attenuated during tissue traversal. We model I i (d) by approximating the Beer-Lambert Law as:, where α is the attenuation coefficient of the medium and d the distance travelled. To construct the final 2D attenuation map, we calculate, for each ray, the cumulative product of the attenuation as it traverses through various tissues, thereby modeling how the signal's strength diminishes. The reflection coefficient Z = (Z 2 -Z 1 ) 2 /(Z 2 +Z 1 ) 2 , is computed from the acoustic impedances of two adjacent tissues: Z 1 and Z 2 . The P (d) is the Point Spread Function (PSF) along the ray, and G(d) is a boundary map, where 1 is assigned for points on the boundary of the surface and 0 otherwise. For simplicity, we model the PSF as a two-dimensional normalized Gaussian. The amount of the reflected signal, denoted by φ r , equals the result of multiplying the reflection coefficient by the boundary condition. To build our final 2D reflection map, for each ray, we compute the cumulative product of the residual signal, defined as 1φ r . The output represents the fraction of the signal that propagates forward.In additionally to the reflection term, a backscattered energy term B i (d) in the returning echo is calculated:the residual ultrasound wave energy I i (d) is multiplied with the PSF P (d), which has been convolved with a texture T of random scatterers for each (x, y), where:This texture is constructed using two random textures T 0 (x, y) and T 1 (x, y) with Gaussian normalized distributions and the parameters μ 0 , μ 1 , and σ 0 , which represent the brightness, density and standard deviation of scatterers respectively.To make the function fully differentiable, we replace the conditional operation T 1 ≤ μ 1 with a differentiable approximation:where σ(z) = 1 1+e -z is the sigmoid function and β is a scaling factor that adjusts its steepness. The resulting function is fully differentiable as the sigmoid function smoothly approximates the step function and all operations involved are differentiable. Additionally, we apply temporal gain compensation (TGC) to enhance tissues deeper in the image. The final rendered ultrasound image is constructed from the three sub-maps (see Fig. 2) and additionally warped to produce the desired fan shape. At the beginning of the training, we set the default tissuespecific values, which during the training, get changed, guided from the downstream task, and generate optimal US simulation."
LOTUS: Learning to Optimize Task-Based US Representations,2.2,End-to-End Learning,"The proposed method's architecture is shown in Fig. 1. During training, our method follows two main paths: Real → Reconstructed US and CT label map → Segmentation. We explain the meaning of these paths in the order shown in the figure . 
Real → Reconstructed US. Since there is an appearance gap between real and our rendered ultrasound images we incorporate an unpaired and unsupervised image-to-image translation network, CUT [8], which uses a contrastive learning scheme. Given a source image, the Generator learns a function G : X → Y that translates the corresponding image into the target's appearance. We have two domains of unpaired instances: real US images as the source X and rendered US as the target Y. The generator's encoder G enc extracts relevant content characteristics, while the decoder G dec learns to create the desired goal appearance. The Generator network employs an adversarial loss:where the generated images G(x) resemble images from domain Y, and D(.) differentiates between translated and real images y. However, the adversarial loss alone does not ensure that the translated image will preserve the structure of the anatomy. An additional contrastive loss must be imposed, which maximizes mutual information across corresponding image patches from the source and the output image. We use the Patch Sampler from CUT to extract image patches and calculate the contrastive NCE (L NCE ) loss [8]. The final loss is defined as:where, the L NCE is calculated on two pairs, a sample from the source domain (x) paired with the generated output G(x) and a sample from the target domain (y) paired with the G(y) which we denote as the identity image. The loss over the second pair serves as an identity loss and prevents the generator from making unnecessary changes to the image.CT Labelmap → Segmentation: The segmentation network forward pass has a nested structure. First, we obtain a 2D slice from the CT label map and pass it to the differentiable ultrasound renderer. The resulting rendered US is passed through the frozen Generator network, and the identity image output of the Generator is used as an input to the segmentation network to ensure the same distribution as the target domain. We update both the segmentation network and the Renderer using dice loss. The label for computing the dice loss comes directly from the input label map used for generating the rendered US.Stopping Criterion: Once the segmentation network validation loss converges, we employ a small subset of 10 labeled images from the real US domain as a stopping indicator for the entire training pipeline."
LOTUS: Learning to Optimize Task-Based US Representations,3,Experimental Setup,"CT Dataset: We use 12 CT volumes from a publicly available dataset Synapse3  [7] for training. The data comes with labels for multiple organs. These labels were additionally augmented with labels of bones, fat, skin, and lungs using TotalSegmentor [17] to complete the label maps.In-vivo Images: We acquired abdominal ultrasound sweeps from eleven volunteers of age 26 ± 3 (m:7/f:4). For each person, one sweep was acquired with a convex probe 4 . Per sweep, 50 frames were randomly sampled and used for training the CUT network. To compare against a supervised approach, additional images were annotated (500 for the aorta, 400 for vessels) from all volunteers to train 5-fold cross-validation. From each set of annotated images, 100 images were randomly sampled as test sets for both segmentation tasks.Training Details: We train the network with a learning rate of 10 -5 for the segmentation network, 10 -3 for the US Renderer, and 5 -6 for the image adaptation network, with a batch size of 1, Adam optimizer and dice loss. We employ rotation, translation, and scaling augmentations on the CT label maps and split them randomly in an 80-20% ratio for training and validation, respectively. For the supervised approach, we trained the networks, for 120 epochs, with a learning rate of 10 -3 and the Adam optimizer. Experiments. We test the proposed framework quantitatively for two segmentation tasks: all vessels and the aorta only. We evaluate the accuracy of the proposed method by comparing it to a supervised network. For this, we train a 5-fold cross-validation U-Net [9], test on three hold-out subjects, and report the average DSC. We also compare to a fixed rendered image by freezing the US renderer instead of optimizing it. Additionally, we show qualitative results of the proposed method when the downstream tasks are changed Fig. 3."
LOTUS: Learning to Optimize Task-Based US Representations,4,Results and Discussion,"In Table 1, we compare the performance of LOTUS against a fully supervised approach and against a frozen renderer's parameters and report the DSC and Hausdorff distance (HD) in mm for aorta segmentation and DSC only for all vessels segmentation. Our proposed method achieved the highest DSC score of 89.24 ± 0.13 and the lowest HD score of 2.52 ± 1.18 mm for aorta segmentation.For the task of vessels segmentation it also achieved the best DSC of 90.9 ± 0.06. Figure 3 depicts the images obtained during the optimization of the proposed method for different target organs. The upper row shows the rendered US image with default parameters, and the bottom row displays the optimized image representations for the corresponding target organ learned during optimization. It can be observed that the spine, kidney and liver appear brighter, while for vessel and aorta segmentation, the vessels darken and the background becomes uniformly homogeneous. This highlights the ability of the proposed method to learn optimal representations for each downstream task.The results presented in this work demonstrate the effectiveness of LOTUS for segmenting organs in ultrasound images. Our physics-based simulator generates synthetic training data, which is especially useful in scenarios where obtaining labeled data is time-consuming or costly. We believe that learning from transferred labels from CT contributes to a more accurate model since CT data is more accessible and labels are more refined. Our quantitative results indicate that LOTUS can achieve accurate segmentation of aorta boundaries and other vessels. Furthermore, the end-to-end framework enables the differentiable US renderer and the unsupervised image translation to get optimized dynamically during the training. Thus, the intermediate representation image is not static but changes during the training. This illustrates the adaptivity of the proposed method to the downstream task, highlighting its prospective applicability across diverse applications and anatomies.Moreover, rather than directly using the rendered US image as an input to the segmentation network, we use the identity image output from the Generator. This yielded significant improvement in the segmentation result as it learns from a distribution consistent with the reconstructed US while looking similar to the rendered US. As a result, during inference stage, the distribution of the translated real US is closer to the distribution the segmentation network was trained on, thereby improving the performance of the model.One of the challenges when employing generative adversarial networks is that the loss is not an indicator of the best result. We determine the optimal model by utilizing a small subset of labeled images after the convergence of the segmentation network, to ensure robustness during inference. Further stopping criteria can be studied to achieve higher automation of the pipeline.Currently, our model incorporates the basic physics of ultrasound imaging without considering artifacts explicitly. Thus, exploring the robustness of the method against artifacts could yield valuable future improvements."
LOTUS: Learning to Optimize Task-Based US Representations,5,Conclusion,"This paper presents a novel approach to learning task-based ultrasound image representations. LOTUS leverages CT labelmaps to simulate ultrasound data via differentiable ray-casting. The proposed ultrasound simulator is fully differentiable and learns to optimize the parameters for generating physics-based ultrasound images guided by the downstream segmentation task. We also introduce an image adaptation network to achieve simultaneous image synthesis and automatic segmentation on US images in an end-to-end training setting without needing paired real and simulated images. Our method is evaluated on aorta and vessel segmentation tasks and shows promising quantitative results. Furthermore, we demonstrate the potential of our approach for other organs through qualitative results of optimized image representations. The ability to learn from unlabeled data and simulate the ultrasound modality has the potential for various clinical tasks beyond segmentation. We believe that our work has the potential to improve ultrasound imaging interpretation and learning."
LOTUS: Learning to Optimize Task-Based US Representations,,Fig. 1 .,
LOTUS: Learning to Optimize Task-Based US Representations,,Fig. 2 .,
LOTUS: Learning to Optimize Task-Based US Representations,,Fig. 3 .,
LOTUS: Learning to Optimize Task-Based US Representations,,Table 1 .,
LOTUS: Learning to Optimize Task-Based US Representations,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0_42.
Combating Medical Label Noise via Robust Semi-supervised Contrastive Learning,1,Introduction,"The advancement of deep learning models heavily depends on the availability of large-scale datasets with high-quality annotated labels [4]. However, it is costly and time-consuming to obtain a sufficient number of accurate annotations from clinical systems, which inevitably introduces a certain level of noisy label [16]. This phenomenon seriously affects the stability and robustness of medical training and prediction procedures, leading to the production of corrupted representations and inaccurate classification boundaries [27]. Early approaches for combating the label noise mainly focused on the marginal improvements of model robustness, including designing robust loss functions [14,26], preprocessing the image [24,28], and estimating the noise transition matrix [8]. In recent years, some advanced methods [2,19] with semi-supervised learning [22] are designed to leverage the supervision signal provided by pseudo labels to re-correct the noise bias. With the development of contrastive learning technologies, some scholars [15,17] attempt to learn the contrastive representations between the clean and noisy labels by maximizing the similarity of positive pairs and minimizing the similarity of negative pairs. Despite the advances in conventional image processing, few studies have proposed to overcome the medical label noise.To overcome this issue, this paper proposes a robust Semi-supervised Contrastive Learning (SSCL) paradigm, that simultaneously benefits from semisupervised learning and contrastive learning for combating the medical noisy labels and promoting stability and robustness of the diagnostic model. Three important components, i.e., the Mixup Feature Embedding (MFE) module, the Semi-supervised Learning (SSL) module, and the Similarity Contrastive Learning (SCL) module, are proposed in the SSCL framework. The architecture of the SSCL framework is shown in Fig. 1. Specifically, the MFE module is built on a multi-branch architecture with the momentum update mechanism, which can effectively capture the abstract distributed feature representations from various levels of mixup augmented images. Based on the confidence scores provided by the initial classifier, a flexible pseudo-labeling promotion strategy is introduced into the SSL module to effectively select confident samples and generate the pseudo-labels, resulting in a more accurate supervision signal reconstruction. By calculating their similarity distribution of representation learning, a novel pairwise selection strategy in the SCL module is designed to efficiently identify and select more reliable confident pairs out of noisy pairs for contrasting learning. In the training phase, we broaden the scope of penalization by incorporating loss functions, further reducing the impact of noise on statistical classification. The main contributions of our work are as follows:-This paper presents a robust semi-supervised contrastive learning paradigm that effectively incorporates semi-supervised learning and contrastive learning to mitigate the effect of medical label noise. Our approach represents the first attempt to address this issue in the field of medical image analysis. -The pseudo-labeling promotion strategy can re-correct the supervised information of noisy labels, while the pair-wise selection strategy can guide the confident pairs to dominate the contrasting learning process.-The proposed SSCL framework is evaluated on multiple benchmark datasets, and extensive experiments demonstrate the generalization performance of our method in comparison with state-of-the-art baselines.2 Related Work"
Combating Medical Label Noise via Robust Semi-supervised Contrastive Learning,2.1,Conventional Methods with Noisy Labels,"To eliminate the memorization effect of noise labels in the training phase, recent works [14,26,29] were mainly devoted to exploring the effectiveness of robust loss function. Wang et al. [26] proposed a noise-robust loss function that combined with Cross-Entropy (CE) loss, to address the hard class learning problem and noisy label overfitting problem. Yi et al. [14] investigated the representational benefits of the contrastive regularization loss function to learn contrastive representations with noisy labels. With continuous in-depth research on image processing, data augmentation has been proven to have a significant role in combating noisy labels. Zhang et al. [28] presented a data-agnostic and straightforward data augmentation principle to mix up different images geometrically in the feature space, which has been widely used in the field of noise labeling. Moreover, researchers have attempted to leverage the label noise transfer matrix extracted from the data set to solve the noise label problem. Hendrycks et al. [8] directly used the matrix summarizing the probability of one class being flipped into another under noise. Ramaswamy et al. [18] proposed an efficient kernel mean embedding to overcome mixture proportion estimation."
Combating Medical Label Noise via Robust Semi-supervised Contrastive Learning,2.2,Semi-supervised Learning,"Compared with the existing learning methods, semi-supervised learning [3,12,23] has been recognized as an effective technique to solve the problem of noisy labels. As a semi-supervised learning technique, pseudo-labeling is frequently used in conjunction with confidence-based thresholding to retain unlabeled examples and increase the size of labeled training data. In recent years, some advanced works [2,19] demonstrate the capacity of pseudo-label based semi-supervised learning methods in combating medical label noise. Reed et al. [19] augmented the usual prediction objective with a notion of perceptual consistency, and the article referred to this approach as static hard guidance. Arazo et al. [2] proposed dynamic hard and soft bootstrapping losses by individual weight of each sample. Furthermore, the combination of semi-supervised learning and pseudo-labels can be used not only for single-label classification but also for negative learning and multi-label classification [20]."
Combating Medical Label Noise via Robust Semi-supervised Contrastive Learning,2.3,Contrastive Learning,"With the advancement of deep learning, researchers have found the potential of contrastive-based similarity learning frameworks for representation learning [10,15,25]. Unsupervised contrastive learning [7,21] aims to maximize the similarity of positive pairs and minimize the similarity of negative pairs at the instance level. By incorporating clean label information, supervised contrastive learning [13] can obtain more supervised information and achieve better performance. Recently, many state-of-the-art works [15,17] have been proposed to make full use of contrastive learning for combating label noise. MOIT [17] adopted the method of interpolation contrastive learning, and used the supervised information obtained after semi-supervised learning for contrastive learning, so as to reduce the damage of noise labels on contrastive learning. Sel-CL [15] improved MOIT by introducing the concepts of confident use cases and confident pairs which improved the ability to filter noise labels with new detection strategies. 3 Semi-supervised Contrastive Learning"
Combating Medical Label Noise via Robust Semi-supervised Contrastive Learning,3.1,Mixup Feature Embedding,"Mixup. Let D = {(x i , y i )} N i denotes the training minibatch of image-label pairs x i and y i , where N is the batch size. Initially, the operations of data augmentation are first conducted in the MFE module, to generate various levels of hybrid augmented images with Mixup [28]. As shown in Fig. 1, the obtained hybrid data set involves a group of weak augmentation images, i.e., D k , and two groups of strong augmentation images, i.e., D q and D v . The principle of Mixup can be defined as:where λ ∈ [0, 1]∼Beta(α l , α r ) is used to control the mixing strength of training samples, x a and x b are the training samples randomly drawn from each minibatch, and x i is the enhanced image generated by the mixup preprocessing.Feature Embedding. By taking the corresponding augmented images as inputs, the MFE module aims to capture abstract distributed feature representations.The MFE module consists of different branches, including a deep encoder with projection and classifier heads, and a momentum encoder with an information bottleneck (IB) [6]. Motivated from the physical perspective of optimization [7], the momentum update mechanism can be defined as:where θ denotes the encoder parameters, and m ∈ [0, 1) is a momentum coefficient. Specifically, the feature representations Q would be mapped to the samedimensional representations Q by the projection head, while K is used to predict the categorical outputs with classifier. The feature representation V is derived from the momentum encoder, possessing identical dimensions to Q. Moreover, we also utilize Kullback-Leibler divergence [11] to implement the criterion of exploiting IB, resulting in a compact feature representation Ṽ . By alternately learning robust representations from D q and D v , a symmetry objective function is designed to gain the IB loss,"
Combating Medical Label Noise via Robust Semi-supervised Contrastive Learning,3.2,Semi-Supervised Learning,"Selecting Confident Samples. The main core of the proposed SSL module is to recognize the confident samples with clean labels, and re-correct the supervision information of label noise. To this end, we first select confident samples based on their confidence score provided by the classifier. Denote the confident samples with clean label belonging to n-th class aswhere p i ∈ [0, 1] is the classification probability of the enhanced image x i , and γ n is a dynamic confidence threshold for the n-th class to ensure a class-balanced set of identified confident examples."
Combating Medical Label Noise via Robust Semi-supervised Contrastive Learning,,Pseudo-Labeling.,"To generate accurate supervision signals, a flexible pseudolabeling promotion strategy is introduced to replace noisy labels with pseudolabels,Benefiting from the unique semi-supervised learning structure, our SSL module can effectively reduce the impact of noise based on statistical classification. The objective function for semi-supervised learning is defined as:where ω ∈ (0, 1] is a tunable focusing parameter, which is utilized to exploit the benefits of both the noise-robustness and the implicit weighting scheme."
Combating Medical Label Noise via Robust Semi-supervised Contrastive Learning,3.3,Similarity Contrastive Learning,"Selecting Confident Pairs. To achieve a precise estimation of noisy pairs, a novel pair-wise selection strategy is proposed to identify the reliable confident pairs out of noisy pairs. By calculating their similarity distribution of representation learning, the SCL module can transform identified confident examples into a set of associated confident pairs S without knowing noise rates,where τ is considered as a confidence threshold. Therefore, the objective loss on each sample pair for contrastive learning can be defined as:Consistent with Eq. 3, a symmetry loss function is applied for each minibatch,Queue. It is noted that blindly increasing the size of the minibatch will be limited by computing resources [15]. To overcome these issues, a queue with the length of L is also introduced into the SCL module, which can maintain a feature dictionary to store features and decouple the dictionary size from the mini-batch size [7,13]. As the new minibatch is added to the queue, the queue M = {x i , V i } L i=1 is gradually replaced and the oldest features in the queue are removed. The objective loss of the queue is defined as:Objective Loss of SSCL. Based on the analysis of the above modules, the total objective loss for the proposed SSCL method can be obtained by,where α and β are loss weight. In our experiments, both α and β are set to 0.25."
Combating Medical Label Noise via Robust Semi-supervised Contrastive Learning,4,Experiments,
Combating Medical Label Noise via Robust Semi-supervised Contrastive Learning,4.1,Implementation Details and Settings,"By randomly replacing labels for a percentage of the training data with all possible labels, the proposed SSCL method is extensively validated on four benchmarks with symmetric noise. ISIC-19 [30] dataset boasts a training set of 20,400 We compare the proposed SSCL method with several state-of-the-art baselines, including three conventional noise-robustness methods (i.e., CE [9], GCE [29], and Mixup [28]), three state-of-the-art baselines (i.e., MOIT [17], Sel-CL [15], and CTRR [14]). More implementation details are shown in the supplementary material."
Combating Medical Label Noise via Robust Semi-supervised Contrastive Learning,4.2,Comparisons with the State-of-the-arts,"In this part, we evaluate the performance of the proposed SSCL framework with classification accuracy under different label noise rates (NR). As shown in Table 1, the proposed SSCL significantly outperforms the state-of-the-art baselines on almost all evaluation metrics. For example, our SSCL achieves the highest classification accuracy on ISIC-19 and BUSI datasets, which can verify the effectiveness of our SSCL for medical image analysis with noisy supervision. Especially in the case of higher noise, SSCL has a more powerful capability to capture discriminative and robust features and minimize the effect of noisy labels. When the noise rate is 0.8, the proposed SSCL framework achieves a classification accuracy of 93.0% and 66.5% on CIFA-10 and CIFA-100, surpassing Sel-CL by 3.8% and 6.9%. The comparative results consistently demonstrate the superiority and generalizability of the proposed SSCL framework."
Combating Medical Label Noise via Robust Semi-supervised Contrastive Learning,4.3,Parameter Analysis and Ablation Studies and Visualizations,"As the key hyperparameter in Eq. 7, the threshold τ is designed to reduce the wrong sample pairs to achieve the best classification boundary construction. In this part, we empirically conduct the proposed SSCL framework with a range of different values τ . As shown in Fig. 2(a), our SSCL achieves the best performance when τ is increased to 0.8, which can avoid the adverse impact of noisy pairs. Moreover, we also conduct ablation studies by systematically removing each component within the SSCL. In Fig. 2(b), we can observe that all the modules are necessary for the function of the proposed SSCL framework. To further validate the discriminative power of the SSCL framework, we utilized t-SNE [5] to visualize the features extracted by vanilla ResNet-18 and SSCL. Compared with vanilla ResNet-18, the visualization results in Fig. 3 and Fig. 4 clearly demonstrate that SSCL has better characteristics for clustering and finer classification boundary, which can demonstrate the effectiveness of SSCL.  "
Combating Medical Label Noise via Robust Semi-supervised Contrastive Learning,5,Conclusion,"This paper presents a robust and reliable semi-supervised contrastive learning method that benefits greatly from the potential synergistic efficacy of semisupervised learning and contrastive learning, which aims to tackle the challenge of learning with medical noisy labels. By explicitly selecting confident samples and pairs, our approach exhibits a powerful ability to learn discriminative feature representations, mitigating the impact of medical label noise. To demonstrate the effectiveness and versatility of our proposed approach in various practical scenarios, our future works would extend the proposed SSCL method to a broader range of real-world noisy datasets and tasks."
Combating Medical Label Noise via Robust Semi-supervised Contrastive Learning,,Fig. 1 .,
Combating Medical Label Noise via Robust Semi-supervised Contrastive Learning,,Fig. 2 .,
Combating Medical Label Noise via Robust Semi-supervised Contrastive Learning,,Fig. 3 .,
Combating Medical Label Noise via Robust Semi-supervised Contrastive Learning,,Fig. 4 .,
Combating Medical Label Noise via Robust Semi-supervised Contrastive Learning,,Table 1 .,
AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on MRI Brain Tumor,1,Introduction,"Deep learning techniques have greatly improved medical image segmentation by automatically extracting specific tissue or substance location information, which facilitates accurate disease diagnosis and assessment. However, most deep learning approaches for segmentation require fully or partially labeled training datasets, which can be time-consuming and expensive to annotate. To address this issue, recent research has focused on developing segmentation frameworks that require little or no segmentation labels.To meet this need, many researchers have devoted their efforts to Weakly-Supervised Semantic Segmentation (WSSS) [21], which utilizes weak supervision, such as image-level classification labels. Recent WSSS methods can be broadly categorized into two types [4]: Class-Activation-Mapping-based (CAM-based) [9,13,16,19,20,22], and Multiple-Instance-Learning-based (MIL-based) [15] methods.The literature has not adequately addressed the issue of low-resolution Class-Activation Maps (CAMs), especially for medical images. Some existing methods, such as dilated residual networks [24] and U-Net segmentation architecture [3,7,17], have attempted to tackle this issue, but still require many upsampling operations, which the results become blurry. Meanwhile, LayerCAM [9] has proposed a hierarchical solution that extracts activation maps from multiple convolution layers using Grad-CAM [16] and aggregates them with equal weights. Although this approach successfully enhances the resolution of the segmentation mask, it lacks flexibility and may not be optimal.In this paper, we propose an Attentive Multiple-Exit CAM (AME-CAM) for brain tumor segmentation in magnetic resonance imaging (MRI). Different from recent CAM methods, AME-CAM uses a classification model with multipleexit training strategy applied to optimize the internal outputs. Activation maps from the outputs of internal classifiers, which have different resolutions, are then aggregated using an attention model. The model learns the pixel-wise weighted sum of the activation maps by a novel contrastive learning method.Our proposed method has the following contributions:-To tackle the issues in existing CAMs, we propose to use multiple-exit classification networks to accurately capture all the internal activation maps of different resolutions. -We propose an attentive feature aggregation to learn the pixel-wise weighted sum of the internal activation maps. -We demonstrate the superiority of AME-CAM over state-of-the-art CAM methods in extracting segmentation results from classification networks on the 2021 Brain Tumor Segmentation Challenge (BraTS 2021) [1,2,14]. -For reproducibility, we have released our code at https://github.com/windstormer/AME-CAM Overall, our proposed method can help overcome the challenges of expensive and time-consuming segmentation labeling in medical imaging, and has the potential to improve the accuracy of disease diagnosis and assessment."
AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on MRI Brain Tumor,2,Attentive Multiple-Exit CAM (AME-CAM),"The proposed AME-CAM method consists of two training phases: activation extraction and activation aggregation, as shown in Fig. 1. In the activation extraction phase, we use a binary classification network, e.g., ResNet-18, to obtain the class probability y = f (I) of the input image I. To enable multipleexit training, we add one internal classifier after each residual block, which generates the activation map M i of different resolutions. We use a cross-entropy loss to train the multiple-exit classifier, which is defined as where GAP (•) is the global-average-pooling operation, CE(•) is the cross-entropy loss, and L is the image-wise ground-truth label.In the activation aggregation phase, we create an efficient hierarchical aggregation method to generate the aggregated activation map M f by calculating the pixel-wise weighted sum of the activation maps M i . We use an attention network A(•) to estimate the importance of each pixel from each activation map. The attention network takes in the input image I masked by the activation map and outputs the pixel-wised importance score S xyi of each activation map. We formulate the operation as follows:where [•] is the concatenate operation, n(•) is the min-max normalization to map the range to [0,1], and ⊗ is the pixel-wise multiplication, which is known as image masking. The aggregated activation map M f is then obtained by the pixel-wise weighted sum of M i , which isWe train the attention network with unsupervised contrastive learning, which forces the network to disentangle the foreground and the background of the aggregated activation map M f . We mask the input image by the aggregated activation map M f and its opposite (1 -M f ) to obtain the foreground feature and the background feature, respectively. The loss function is defined as follows:where v f i and v b i denote the foreground and the background feature of the i-th sample, respectively. SimM in and SimM ax are the losses that minimize and maximize the similarity between two features (see C 2 AM [22] for details).Finally, we average the activation maps M 1 to M 4 and the aggregated map M f to obtain the final CAM results for each image. We apply the Dense Conditional Random Field (DenseCRF) [12] algorithm to generate the final segmentation mask. It is worth noting that the proposed method is flexible and can be applied to any classification network architecture."
AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on MRI Brain Tumor,3,Experiments,
AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on MRI Brain Tumor,3.1,Dataset,"We evaluate our method on the Brain Tumor Segmentation challenge (BraTS) dataset [1,2,14], which contains 2,000 cases, each of which includes four 3D volumes from four different MRI modalities: T1, post-contrast enhanced T1 (T1-CE), T2, and T2 Fluid Attenuated Inversion Recovery (T2-FLAIR), as well as a corresponding segmentation ground-truth mask. The official data split divides these cases by the ratio of 8:1:1 for training, validation, and testing (5,802 positive and 1,073 negative images). In order to evaluate the performance, we use the validation set as our test set and report statistics on it. We preprocess the data by slicing each volume along the z-axis to form a total of 193,905 2D images, following the approach of Kang et al. [10] and Dey and Hong [6]. We use the ground-truth segmentation masks only in the final evaluation, not in the training process."
AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on MRI Brain Tumor,3.2,Implementation Details and Evaluation Protocol,"We implement our method in PyTorch using ResNet-18 as the backbone classifier. We pretrain the classifier using SupCon [11] and then fine-tune it in our experiments. We use the entire training set for both pretraining and fine-tuning. We set the initial learning rate to 1e-4 for both phases, and use the cosine annealing scheduler to decrease it until the minimum learning rate is 5e-6. We set the weight decay in both phases to 1e-5 for model regularization. We use Adam optimizer in the multiple-exit phase and SGD optimizer in the aggregation phase. We train all classifiers until they converge with a test accuracy of over 0.9 for all image modalities. Note that only class labels are available in the training set.We use the Dice score and Intersection over Union (IoU) to evaluate the quality of the semantic segmentation, following the approach of Xu et al. [23], Tang et al. [18], and Qian et al. [15]. In addition, we report the 95% Hausdorff Distance (HD95) to evaluate the boundary of the prediction mask.Interested readers can refer to the supplementary material for results on other network architectures.  "
AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on MRI Brain Tumor,4.1,Quantitative and Qualitative Comparison with State-of-the-Art,"In this section, we compare the segmentation performance of the proposed AME-CAM with five state-of-the-art weakly-supervised segmentation methods, namely Grad-CAM [16], ScoreCAM [19], LFI-CAM [13], LayerCAM [9], and Swin-MIL [15]. We also compare with an unsupervised approach C&F [5], the supervised version of C&F, and the supervised Optimized U-net [8] to show the comparison with non-CAM-based methods. We acknowledge that the results from fully supervised and unsupervised methods are not directly comparable to the weakly supervised CAM methods. Nonetheless, these methods serve as interesting references for the potential performance ceiling and floor of all the CAM methods. Quantitatively, Grad-CAM and ScoreCAM result in low dice scores, demonstrating that they have difficulty extracting the activation of medical images. LFI-CAM and LayerCAM improve the dice score in all modalities, except LFI-CAM in T1-CE and T2-FLAIR. Finally, the proposed AME-CAM achieves optimal performance in all modalities of the BraTS dataset.Compared to the unsupervised baseline (UL), C&F is unable to separate the tumor and the surrounding tissue due to low contrast, resulting in low dice scores in all experiments. With pixel-wise labels, the dice of supervised C&F improves significantly. Without any pixel-wise label, the proposed AME-CAM outperforms supervised C&F in all modalities.The fully supervised (FSL) Optimized U-net achieves the highest dice score and IoU score in all experiments. However, even under different levels of supervision, there is still a performance gap between the weakly supervised CAM methods and the fully supervised state-of-the-art. This indicates that there is still potential room for WSSS methods to improve in the future.Qualitatively, Fig. 2 shows the visualization of the CAM and segmentation results from all six CAM-based approaches under four different modalities from the BraTS dataset. Grad-CAM (Fig. 2(c)) results in large false activation region, where the segmentation mask is totally meaningless. ScoreCAM eliminates false activation corresponding to air. LFI-CAM focus on the exact tumor area only in the T1 and T2 MRI (row 1 and 3). Swin-MIL can hardly capture the tumor region of the MRI image, where the activation is noisy. Among all, only LayerCAM and the proposed AME-CAM successfully focus on the exact tumor area, but AME-CAM reduces the under-estimation of the tumor area. This is attributed to the benefit provided by aggregating activation maps from different resolutions. "
AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on MRI Brain Tumor,4.2,Ablation Study,
AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on MRI Brain Tumor,,Effect of Different Aggregation Approaches:,"In Table 2, we conducted an ablation study to investigate the impact of using different aggregation approaches after extracting activations from the multiple-exit network. We aim to demonstrate the superiority of the proposed attention-based aggregation approach for segmenting tumor regions in T1 MRI of the BraTS dataset. Note that we only report the results for T1 MRI in the BraTS dataset. Please refer to the supplementary material for the full set of experiments.As a baseline, we first conducted the average of four activation maps generated by the multiple-level activation extraction (Avg. ME). We then applied C 2 AM [22], a state-of-the-art CAM-based refinement approach, to refine the result of the baseline, which we call ""Avg. ME+C 2 AM"". However, we observed that C 2 AM tended to segment the brain region instead of the tumor region due to the larger contrast between the brain tissue and the air than that between the tumor region and its surrounding tissue. Any incorrect activation of C 2 AM also led to inferior results, resulting in a degradation of the average dice score from 0.617 to 0.484. In contrast, the proposed attention-based approach provided a significant weighting solution that led to optimal performance in all cases.Table 3. Ablation study for using single-exit from M1, M2, M3 or M4 of Fig. 1 and the multiple-exit using results from M2 and M3 and using all exits (AME-CAM). The experiments are done on the T1-CE MRI of BraTS dataset. The dice score, IoU, and the HD95 are reported in the form of mean ± std."
AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on MRI Brain Tumor,,Selected Exit,"Dice Effect of Single-Exit and Multiple-Exit: Table 3 summarizes the performance of using single-exit from M 1 , M 2 , M 3 , or M 4 of Fig. 1 and the multipleexit using results from M 2 and M 3 , and using all exits (AME-CAM) on T1-CE MRI in the BraTS dataset.The comparisons show that the activation map obtained from the shallow layer M 1 and the deepest layer M 4 result in low dice scores, around 0.15. This is because the network is not deep enough to learn the tumor region in the shallow layer, and the resolution of the activation map obtained from the deepest layer is too low to contain sufficient information to make a clear boundary for the tumor. Results of the internal classifiers from the middle of the network (M 2 and M 3 ) achieve the highest dice score and IoU, both of which are around 0.5.To evaluate whether using results from all internal classifiers leads to the highest performance, we further apply the proposed method to the two internal classifiers with the highest dice scores, i.e., M 2 and M 3 , called M 2 + M 3 . Compared with using all internal classifiers (M 1 to M 4 ), M 2 + M 3 results in 18.6% and 22.1% lower dice and IoU, respectively. In conclusion, our AME-CAM still achieves the optimal performance among all the experiments of single-exit and multiple-exit.Other ablation studies are presented in the supplementary material due to space limitations."
AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on MRI Brain Tumor,5,Conclusion,"In this work, we propose a brain tumor segmentation method for MRI images using only class labels, based on an Attentive Multiple-Exit Class Activation Mapping (AME-CAM). Our approach extracts activation maps from different exits of the network to capture information from multiple resolutions. We then use an attention model to hierarchically aggregate these activation maps, learning pixel-wise weighted sums.Experimental results on the four modalities of the 2021 BraTS dataset demonstrate the superiority of our approach compared with other CAM-based weakly-supervised segmentation methods. Specifically, AME-CAM achieves the highest dice score for all patients in all datasets and modalities. These results indicate the effectiveness of our proposed approach in accurately segmenting brain tumors from MRI images using only class labels."
AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on MRI Brain Tumor,,Fig. 1 .,
AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on MRI Brain Tumor,,Fig. 2 .,
AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on MRI Brain Tumor,,Table 1 .,LFI-CAM (2021) 0.121 ± 0.120 0.069 ± 0.076 136.246 ± 38.619 LayerCAM (2021) 0.510 ± 0.209 0.367 ± 0.180 29.850 ± 45.877 Swin-MIL (2022) 0.460 ± 0.169 0.314 ± 0.140 46.996 ± 22.821 AME-CAM (ours) 0.695 ± 0.095 0.540 ± 0.108 18.129 ± 12.335
AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on MRI Brain Tumor,,.721 ± 0.086 0.571 ± 0.101 14.940 ± 8.736,Opt. U-net (2021) 0.914 ± 0.058 0.847 ± 0.093 8.093 ± 11.879 4 Results
AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on MRI Brain Tumor,,Table 2 .,
AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on MRI Brain Tumor,,.119 0.471 ± 0.119 21.813 ± 18.219,
AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on MRI Brain Tumor,,.095 0.540 ± 0.108 18.129 ± 12.335,
AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on MRI Brain Tumor,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0 17.
Correlation-Aware Mutual Learning for Semi-supervised Medical Image Segmentation,1,Introduction,"Despite the remarkable advancements achieved through the use of deep learning for automatic medical image segmentation, the scarcity of precisely annotated training data remains a significant obstacle to the widespread adoption of such techniques in clinical settings. As a solution, the concept of semi-supervised segmentation has been proposed to enable models to be trained using less annotated but abundant unlabeled data.Recently, methods that adopt the co-teaching [3,11,19] or mutual learning [25] paradigm have emerged as a promising approach for semi-supervised learning. Those methods adopt two simultaneously updated models, each trained to predict the prediction results of its counterpart, which can be seen as a combination of the notions of consistency regularization [1,4,9,14,15] and entropy minimization [2,7,20,22,24]. In the domain of semi-supervised medical image segmentation, MC-Net [19] has shown significant improvements in segmentation performance.With the rapid advancement of semi-supervised learning, the importance of unlabeled data has garnered increased attention across various disciplines in recent years. However, the role of labeled data has been largely overlooked, with the majority of semi-supervised learning techniques treating labeled data supervision as merely an initial step of the training pipeline or as a means to ensure training convergence [6,15,26]. Recently, methods that can leverage labeled data to directly guide information extraction from unlabeled data have attracted the attention of the community [16]. In the domain of semi-supervised medical image segmentation, there exist shared characteristics between labeled and unlabeled data that possess greater intuitiveness and instructiveness for the algorithm. Typically, partially labeled clinical datasets exhibit similar foreground features, including comparable texture, shape, and appearance among different samples. As such, it can be hypothesized that constructing a bridge across the entire training dataset to connect labeled and unlabeled data can effectively transfer prior knowledge from labeled data to unlabeled data and facilitate the extraction of information from unlabeled data, ultimately overcoming the performance bottleneck of semi-supervised learning methods.Based on the aforementioned conception, we propose a novel Correlation Aware Mutual Learning (CAML) framework to explicitly model the relationship between labeled and unlabeled data to effectively utilize the labeled data. Our proposed method incorporates two essential components, namely the Crosssample Mutual Attention module (CMA) and the Omni-Correlation Consistency module (OCC), to enable the effective transfer of labeled data information to unlabeled data. The CMA module establishes mutual attention among a group of samples, leading to a mutually reinforced representation of co-salient features between labeled and unlabeled data. Unlike conventional methods, where supervised signals from labeled and unlabeled samples are separately back-propagated, the proposed CMA module creates a new information propagation path among each pixel in a group of samples, which synchronously enhances the feature representation ability of each intra-group sample.In addition to the CMA module, we introduce the OCC module to regularize the segmentation model by explicitly modeling the omni-correlation between unlabeled features and a group of labeled features. This is achieved by constructing a memory bank to store the labeled features as a reference set of features or basis vectors. In each iteration, a portion of features from the memory bank is utilized to calculate the omni-correlation with unlabeled features, reflecting the similarity relationship of an unlabeled pixel with respect to a set of basis vectors of the labeled data. Finally, we constrain the omni-correlation matrix of each sub-model to be consistent to regularize the entire framework. With the proposed omni-correlation consistency, the labeled data features serve as anchor groups to guide the representation learning of the unlabeled data feature and explicitly encourage the model to learn a more unified feature distribution among unlabeled data.In summary, our contributions are threefold: (1) We propose a novel Correlation Aware Mutual Learning (CAML) framework that focuses on the efficient utilization of labeled data to address the challenge of semi-supervised medical image segmentation. (2) We introduce the Cross-sample Mutual Attention module (CMA) and the Omni-Correlation Consistency module (OCC) to establish cross-sample relationships directly. (3) Experimental results on a benchmark dataset demonstrate significant improvements over previous SOTAs, especially when only a small number of labeled images are available."
Correlation-Aware Mutual Learning for Semi-supervised Medical Image Segmentation,2,Method,
Correlation-Aware Mutual Learning for Semi-supervised Medical Image Segmentation,2.1,Overview,"Figure 1 gives an overview of CAML. We adopt a co-teaching paradigm like MC-Net [19] to enforce two parallel networks to predict the prediction results of its counterpart. To achieve efficient cross-sample relationship modeling and enable information propagation among labeled and unlabeled data in a mini-batch, we incorporate a Cross-sample Mutual Attention module to the auxiliary segmentation network f a , whereas the vanilla segmentation network f v remains the original V-Net structure. In addition, we employ an Omni-Correlation Consistency regularization to further regularize the representation learning of the unlabeled data. Details about those two modules will be elaborated on in the following sections. The total loss of CAML can be formulated as:where l o represents the proposed omni-correlation consistency loss, while L s and l c are the supervised loss and the cross-supervised loss implemented in the Cross Pseudo Supervision(CPS) module. λ c and λ o are the weights to control l c and l o separately. During the training procedure, a batch of mixed labeled and unlabeled samples are fed into the network. The supervised loss is only applied to labeled data, while all samples are utilized to construct cross-supervised learning. Please refer to [3] for a detailed description of the CPS module and loss design of L s and l c ."
Correlation-Aware Mutual Learning for Semi-supervised Medical Image Segmentation,2.2,Cross-Sample Mutual Attention Module,"To enable information propagation through any positions of any samples in a mini-batch, one can simply treat each pixel's feature vector as a token and perform self-attentions for all tokens in a mini-batch. However, this will make the computation cost prohibitively large as the computation complexity of selfattention is O(n 2 ) with respect to the number of tokens. We on the other hand adopt two sequentially mounted self-attention modules along different dimensions to enable computation efficient mutual attention among all pixels.As illustrated in Fig. 1, the proposed CMA module consists of two sequential transformer encoder layers, termed as E 1 and E 2 , each including a multi-head attention and a MLP block with a layer normalization after each block. For an input feature map a in ∈ R b×c×k , where k = h × w × d , b represents batch size and c is the dimension of a in , E 1 performs intra-sample self-attention on the spatial dimension of each sample. This is used to model the information propagation paths between every pixel position within each sample. Then, to further enable information propagation among different samples, we perform an inter-sample self-attention along the batch dimension. In other words, along the b dimension, the pixels located in the same spatial position from samples are fed into a self-attention module to construct cross-sample relationships.In CAML, we employ the proposed CMA module in the auxiliary segmentation network f a , whereas the vanilla segmentation network f v remains the original V-Net structure. The reasons can be summarized into two folds. From deployment perspective, the insertion of the CMA module requires a batch size of large than 1 to model the attention among samples within a mini-batch, which is not applicable for model inference(batchsize=1). From the perspective of model design, we model the vanilla and the auxiliary branch with different architectures to increase the architecture heterogeneous for better performance in a mutual learning framework."
Correlation-Aware Mutual Learning for Semi-supervised Medical Image Segmentation,2.3,Omni-Correlation Consistency Regularization,"In this chapter, we introduce Omni-Correlation Consistency (OCC) to formulate additional model regularization. The core of the OCC module is omnicorrelation, which is a kind of similarity matrix that is calculated between the feature of an unlabeled pixel and a group of prototype features sampled from labeled instances features. It reflects the similar relationship of an unlabeled pixel with respect to a set of labeled reference pixels. During the training procedure, we explicitly constrain the omni-correlation calculated using heterogeneous unlabeled features from those two separate branches to remain the same. In practice, we use an Omni-correlation matrix to formulate the similarity distribution between unlabeled features and the prototype features.Let g v and g a denote two projection heads attached to the backbones of f v and f a separately, and z v ∈ R m×c and z a ∈ R m×c represent two sets of embeddings sampled from their projected features extracted from unlabeled samples, where m is the number of sampled features and c is the dimension of the projected features. It should be noted that z v and z a are sampled from the embeddings corresponding to the same set of positions on unlabeled samples. Suppose z p ∈ R n×c represents a set of prototype embeddings sampled from labeled instances, where n represents the number of sampled prototype features, the omni-correlation matrix calculation between z v and z p can be formulated as:where cos means the cosine similarity and t is the temperature hyperparameter. sim vp ∈ R m×n is the calculated omni-correlation matrix. Similarly, the similarity distribution sim ap between z a and z p can be calculated by replacing z v with z a .To constrain the consistency of omni-correlation between dual branches, the omni-correlation consistency regularization can be conducted with the crossentropy loss l ce as follows:Memory Bank Construction. We utilize a memory bank T to iteratively update prototype embeddings for OCC computation. Specifically, T initializes N slots for each labeled training sample and updates prototype embeddings with filtered labeled features projected by g v and g a . To ensure the reliability of the features stored in T , we select embeddings on the positions where both f v and f a have the correct predictions and update T with the mean fusion of the projected features projected by g v and g a . For each training sample, following [5], T updates slots corresponding to the labeled samples in the current mini-batch in a query-like manner.Embeddings Sampling. For computation efficiency, omni-correlation is not calculated on all labeled and unlabeled pixels. Specifically, we have developed a confidence-based mechanism to sample the pixel features from the unlabeled data. Practically, to sample z v and z a from unlabeled features, we first select the pixels where f v and f a have the same prediction. For each class, we sort the confidence scores of these pixels, and then select features of the top i pixels as the sampled unlabeled features. Thus, m = i × C, where C represents the number of classes. With regards to the prototype embeddings, we randomly sample j embeddings from each class among all the embeddings contained in T and n = j × C to increase its diversity."
Correlation-Aware Mutual Learning for Semi-supervised Medical Image Segmentation,3,Experiments and Results,"Dataset. Our method is evaluated on the Left Atrium (LA) dataset [21] from the 2018 Atrial Segmentation Challenge. The dataset comprises 100 gadoliniumenhanced MR imaging scans (GE-MRIs) and their ground truth masks, with an isotropic resolution of 0.625 3 mm 3 . Following [23], we use 80 scans for training and 20 scans for testing. All scans are centered at the heart region and cropped accordingly, and then normalized to zero mean and unit variance.Implementation Details. We implement our CAML using PyTorch 1.8.1 and CUDA 10.2 on an NVIDIA TITAN RTX GPU. For training data augmentation, we randomly crop sub-volumes of size 112 × 112 × 80 following [23]. To ensure a fair comparison with existing methods, we use the V-Net [13] as the backbone for all our models. During training, we use a batch size of 4, with half of the images annotated and the other half unannotated. We train the entire framework using the SGD optimizer, with a learning rate of 0.01, momentum of 0.9, and weight decay of 1e-4 for 15000 iterations. To balance the loss terms in the training process, we use a time-dependent Gaussian warming up function for λ U and λ C , where λ(t) = β * e -5(1-t/tmax) 2 , and set β to 1 and 0.1 for λ U and λ C , respectively. For the OCC module, we set c to 64, j to 256, and i to 12800. During inference, prediction results from the vanilla V-Net are used with a general sliding window strategy without any post-processing.Quantitative Evaluation and Comparison. Our CAML is evaluated on four metrics: Dice, Jaccard, 95% Hausdorff Distance (95HD), and Average Surface Distance (ASD). It is worth noting that the previous researchers reported results (Reported Metrics in Table 1) on LA can be confusing, with some studies reporting results from the final training iteration, while others report the best performance obtained during training. However, the latter approach can lead to overfitting of the test dataset and unreliable model selection. To ensure a fair comparison, we perform all experiments three times with a fixed set of randomly selected seeds on the same machine, and report the mean and standard deviation of the results from the final iteration.The results on LA are presented in Table 1. The results of the full-supervised V-Net model trained on different ratios serve as the lower and upper bounds  of each ratio setting. We report the reproduced results of state-of-the-art semisupervised methods and corresponding reported results if available. By comparing the reproduced and reported results, we observe that although the performance of current methods generally shows an increasing trend with the development of algorithms, the performance of individual experiments can be unstable. and the reported results may not fully reflect the true performance.It is evident from Table 1 that CAML outperforms other methods by a significant margin across all settings without incurring any additional inference or post-processing costs. With only 5% labeled data, CAML achieves 87.34% Dice score with an absolute improvement of 4.01% over the state-of-the-art. CAML also achieves 89.62% Dice score with only 10% labeled data. When the amount of labeled data is increased to 20%, the model obtains comparable results with the results of V-Net trained in 100% labeled data), achieving a Dice score of 90.78% compared to the upper-bound model's score of 90.98%. As presented in Table 1, through the effective transfer of knowledge between labeled and unlabeled data, CAML achieves impressive improvements. Table 1 also demonstrated that as the labeled data ratio declines, the model maintains a low standard deviation of results, which is significantly lower than other state-of-the-art methods. This finding suggests that CAML is highly stable and robust. Furthermore, the margin between our method and the state-of-theart semi-supervised methods increases with the decline of the labeled data ratio, indicating that our method rather effectively transfers knowledge from labeled data to unlabeled data, thus enabling the model to extract more universal features from unlabeled data. Figure 2 shows the qualitative comparison results. The figure presents 2D and 3D visualizations of all the compared methods and the corresponding ground truth. As respectively indicated by the orange rectangle and circle in the 2D and 3D visualizations Our CAML achieves the best segmentation results compared to all other methods."
Correlation-Aware Mutual Learning for Semi-supervised Medical Image Segmentation,,Ablation Study.,"In this section, we analyze the effectiveness of the proposed CMA module and OCC module. We implement the MC-Net as our baseline, which uses different up-sampling operations to introduce architecture heterogeneity. Table 2 presents the results of our ablation study. The results demonstrate that under 5% ratio, both CMA and OCC significantly improve the performance of the baseline. By combining these two modules, CAML achieves an absolute improvement of 6.42% in the Dice coefficient. Similar improvements can be observed for a data ratio of 10%. Under a labeled data ratio of 20%, the baseline performance is improved to 90.43% in the Dice coefficient, which is approximately comparable to the upper bound of a fully-supervised model. In this setting, adding the CMA and OCC separately may not achieve a significant improvement. Nonetheless, by combining these two modules in our proposed CAML framework, we still achieve the best performance in this setting, which further approaches the performance of a fully-supervised model."
Correlation-Aware Mutual Learning for Semi-supervised Medical Image Segmentation,4,Conclusion,"In this paper, we proposed a novel framework named CAML for semi-supervised medical image segmentation. Our key idea is that cross-sample correlation should be taken into consideration for semi-supervised learning. To this end, two novel modules: Cross-sample Mutual Attention(CMA) and Omni-Correlation Consistency(OCC) are proposed to encourage efficient and direct transfer of the prior knowledge from labeled data to unlabeled data. Extensive experimental results on the LA dataset demonstrate that we outperform previous state-of-the-art results by a large margin without extra computational consumption in inference."
Correlation-Aware Mutual Learning for Semi-supervised Medical Image Segmentation,,Fig. 1 .,
Correlation-Aware Mutual Learning for Semi-supervised Medical Image Segmentation,,Fig. 2 .,
Correlation-Aware Mutual Learning for Semi-supervised Medical Image Segmentation,,Table 1 .,
Correlation-Aware Mutual Learning for Semi-supervised Medical Image Segmentation,,Table 2 .,
Correlation-Aware Mutual Learning for Semi-supervised Medical Image Segmentation,,Acknowledgements,". This work is funded by the Scientific and Technological Innovation 2030 New Generation Artificial Intelligence Project of the National Key Research and Development Program of China (No. 2021ZD0113302), Beijing Municipal Science and Technology Planning Project (No. Z201100005620008, Z211100003521009)."
Multi-IMU with Online Self-consistency for Freehand 3D Ultrasound Reconstruction,1,Introduction,"Ultrasound (US) imaging has been widely used in clinical diagnosis due to its advantages of safety, repeatability, and real-time imaging. Compared with 2D US, 3D US can provide more comprehensive spatial information. Freehand 3D US can enhance the understanding of physicians about the scanned region of interest without increasing the complexity of scanning [12][13][14]. However, the difficulty in estimating elevation displacement and accumulation error makes it very challenging to infer the relative position only from images. In this regard, it is expected to improve the reconstruction performance with the help of external lightweight sensors, which will not significantly increase the scanning complexity.Sensorless freehand 3D US reconstructs the volume by calculating the relative transformation of a series of US images. Previous studies were mainly based on speckle decorrelation [1,17], which estimates out-of-plane motion through the correlation of speckle patterns in two successive frames. With the development of deep learning technology, recent studies were mainly based on convolutional neural network (CNN). Prevost et al. [15] proposed an end-to-end method based on CNN to estimate the relative motion of US images. Guo et al. [4] proposed a deep contextual learning network (DCL-Net) based on 3D CNN to estimate the trajectory of US probe, and in a more recent study [3], they proposed a deep contextual-contrastive network (DC 2 -Net), which introduced a contrastive learning strategy to improve the reconstruction performance by leveraging the label efficiently. Luo et al. [10,11] proposed an online learning framework (OLF) that improves reconstruction performance by online learning and shape priors.Due to the low cost, small size, and low power consumption of micro-electromechanical-systems (MEMS), the sensor called inertial measurement unit (IMU) has been widely used in navigation systems. Prevost et al. [14] incorporated IMU orientation into neural network to improve reconstruction performance. Luo et al. [12] proposed a deep motion network (MoNet) to mine the valuable information of low signal-to-noise acceleration, and an online self-supervised strategy was designed to further improve reconstruction performance. However, the main disadvantage of IMU is that its measurement noise can not be completely eliminated by calibration. Existing studies have shown that combining multiple IMUs may help reduce noise and improve accuracy [2,16].In this study, we propose a multi-IMU-based online self-consistency network (OSCNet) for freehand 3D US reconstruction. Our contribution is two-fold. First, we equip multiple IMUs (see Fig. 1) to reduce the influence of noise in individual IMU data. We propose a modal-level self-supervised strategy (MSS) to fuse the information from multiple IMUs. MSS improves reconstruction performance by reducing the differences between reconstruction results obtained from each IMU data. Second, to reduce the estimation instability caused by scanning differences such as frame rates, we propose a sequence-level self-consistency strategy (SCS), which improves the hierarchical consistency of prediction results among the scanning sequence and its sub-sequences based on a consistent context. Experimental results show that the proposed OSCNet can effectively fuse the information of multiple IMUs and achieve state-of-the-art reconstruction performance."
Multi-IMU with Online Self-consistency for Freehand 3D Ultrasound Reconstruction,2,Methodology,"Figure 2 illustrates the proposed OSCNet, which consists of two essential components: backbone and online learning. We construct a backbone using the temporal and multi-branch structure from [12]. The main branch in the backbone consists of ResNet [5] for feature extraction and LSTM [6] for processing temporal information. It aids future estimation by leveraging temporal contextual information. Additionally, there is an independent motion branch in the backbone that fuses IMU information from a motion perspective with US images. For more details, please refer to [12].In the training phase, we input an N -length scanning sequence, where θ i includes 3-axis translations t i = (t x , t y , t z ) i and rotation angles φ i = (φ x , φ y , φ z ) i between image I i and I i+1 . The multiple IMU data consists of M independent IMU dataThe pre-processing process for Φ i and A i is consistent with [12]. Compared to traditional offline inference strategies, online learning can leverage valuable information from unlabeled data to improve the model's performance [10,12]. In the testing phase, we propose two online self-supervised strategies based on both the multiple IMU data (modal-level) and the scanning sequence itself (sequencelevel) to improve the performance of the backbone's estimations."
Multi-IMU with Online Self-consistency for Freehand 3D Ultrasound Reconstruction,2.1,Modal-Level Self-supervised Strategy,"Multiple IMUs mounted in different directions provide diverse measurement constraints for the model's estimation, as shown in Fig. 1. This makes it possible to reduce the influence of noise in individual IMU data while adaptively optimizing for estimation. We construct an online modal-level self-supervised strategy (MSS) that leverages the consistency between the backbone's estimation and multiple IMU data to improve the reconstruction performance.As shown in the top of Fig. 2, during the training phase, we repeatedly input the US images and M different IMU data into the backbone to obtain M estimated parameters. We use the average of the M estimated parameters θ = 1 M M j=1 θj as the final output of the backbone. We then calculate training loss between θ and ground truth θ using mean absolute error (MAE) and Pearson correlation loss [4]:where Cov, σ and • 1 denote the covariance, the standard deviation, and L1 normalization, respectively. As shown in the bottom of Fig. 2, during the testing phase, we use each IMU data U j (j = 1, 2, • • • , M) as a weak label to constrain the corresponding estimated parameters θj . We calculate the estimated acceleration Âj at the center point of each image using the estimated θj . To reduce the influence of acceleration noise, we scale the Âj to match the mean-zeroed IMU acceleration.where ( tj i-1 ) -1 represents the translations in the inversion of θj i-1 . Similar to [12], we use Pearson correlation loss to measure the difference between the estimated and IMU acceleration, while the angle is measured using MAE. Therefore, the single-IMU consistency constraint between the estimated parameters and corresponding IMU data can be expressed as:In addition, the consistency among multiple IMU data itself also provides the possibility to improve the reconstruction performance. It constrains the backbone to obtain similar estimated parameters for different IMU data inputs from the same scan. Specifically, we construct multi-IMU consistency constraints as:"
Multi-IMU with Online Self-consistency for Freehand 3D Ultrasound Reconstruction,2.2,Sequence-Level Self-consistency Strategy,"Consistent context should lead to consistent parameter estimation, which constrains the model at the sequence level, reducing the estimation instability caused by scanning differences such as frame rates. Inspired by contrastive learning [7],we construct an online sequence-level self-consistency strategy (SCS). SCS randomly generates sub-sequences with consistent context for each scan. The hierarchical consistency constraint among the generated sub-sequences and the original sequence improves the reconstruction performance of the backbone. Specifically, as shown in Fig. 2, we randomly interval sample and flip each scanning sequence I and its IMU data U to generate a sub-sequence I sub (U sub ) with consistent context. In the testing phase, we obtain the estimated parameters θsub of I sub (U sub ) using the trained backbone. Then compare θsub with the original estimated parameters θ after the same interval sampling and flipping to construct the self-consistency constraint:where H τ converts the parameters, sequences, or IMU data under interval sampling and flipping operation τ . B denotes the backbone."
Multi-IMU with Online Self-consistency for Freehand 3D Ultrasound Reconstruction,3,Experiments,"Materials and Implementation. The equipment we used to collect data includes a portable US machine, four IMU sensors (WT901C-232, WitMotion) and an electromagnetic (EM) positioning system. The US images were acquired with a linear probe at 10 MHz, and the depth was set at 4 cm. As shown in Fig. 1, we bound four IMU sensors to the probe in different orientations (three for 3axis directions and one for redundancy) using a 3D-printed bracket, which can compensate for errors and reduce measurement singularities [9]. The resolutions of the IMU acceleration and angle are 5 × 10 -4 g/LSB and 0.5 • , respectively. We used the EM positioning system to trace the scan route accurately. The direction and angle resolutions of the EM positioning system are 1.4 mm and 0.5 • , respectively. We calibrated the multiple IMU sensors and the EM positioning system using the Levenberg-Marquardt algorithm [8] to ensure accurate measurements and minimise system errors. As shown in Fig. 3, the calibrated IMU data exhibits a generally consistent overall trend, although differences still exist. We constructed two datasets, including arm and carotid, from 36 volunteers. The arm dataset contains 288 scans, with scanning tactics including linear, curved, loop, and sector scan. The carotid dataset contains 216 scans, with scanning tactics including linear, loop, and sector scan. The average lengths of the arm and carotid scans are 323.96 mm and 203.25 mm, respectively. The size of scanned images is 248 × 260 pixels, and the image spacing is 0.15 × 0.15 mm 2 . The collection and use of the data are approved by the local IRB.The arm and carotid datasets were randomly divided into 200/40/48 and 150/30/36 scans based on volunteer level to construct training/validation/test set. To prevent overfitting and enhance the model's robustness, we performed random augmentations on each scan, including sub-sequence intercepting, interval sampling, and sequence inversion. We randomly augmented each training scan to 20 sequences and each test scan to 10 sequences to simulate complex real-world situations. We used the Adam optimizer to optimize the OSCNet. During the training phase, the epochs and batch size are set to 200 and 1, respectively. To avoid overfitting, we set the initial learning rate to 2 × 10 -4 and used a learning rate decay strategy that halves the learning rate every 30 epochs. During the online learning phase, the iteration epoch and learning rate  are set to 60 and 2 × 10 -6 , respectively. All code was implemented in PyTorch and executed on an RTX 3090 GPU.Quantitative and Qualitative Analysis. To demonstrate the effectiveness of our OSCNet, we compared it with three state-of-the-art methods, including CNN [14], DC 2 -Net [3] and MoNet [12]. All comparison methods were trained to convergence using the experimental settings given in the corresponding papers. We adopt six metrics from [12] to evaluate reconstruction performance: final drift rate (FDR), average drift rate (ADR), maximum drift (MD), sum of drift (SD), symmetric Hausdorff distance (HD), and mean error of angle (EA). In addition, ablation experiments are conducted to validate the effectiveness of MSS and SCS as proposed in our OSCNet. Table 1 shows that our OSCNet significantly outperforms CNN, DC 2 -Net, MoNet, and our Backbone in all metrics for both arm and carotid scans (t-test, p < 0.05), except for MoNet's ADR on the carotid scans (t-test, p = 0.10). Notably, sensor-based methods (MoNet and OSCNet) have exhibited improvements in all metrics compared to sensorless methods (DC 2 -Net and CNN). The multi-IMU-based OSCNet outperforms the single-IMU-based MoNet, verifying the effectiveness of multiple IMU integration. Moreover, the ablation experiments further demonstrate that both multiple IMU integration (MSS) and selfconsistency constraint (SCS) greatly improve the reconstruction performance. In addition, Fig. 4 displays the metric decline curves during the online learning phase of MoNet and OSCNet on the arm and carotid datasets. All metric curves exhibit a decreasing trend followed by stabilization. We note that our OSCNet has achieved further improvements compared to MoNet, with 13.56% /7.32%/30.65% and 7.62%/4.00%/29.16% improvement in FDR/ADR/EA on the arm and carotid datasets, respectively. Figure 5 presents several typical reconstruction results of all methods. It can be observed that our OSCNet outperforms other methods in reconstruction results and closely approximates the ground truth across all scanning tactics."
Multi-IMU with Online Self-consistency for Freehand 3D Ultrasound Reconstruction,4,Conclusion,"In this study, we propose a novel multi-IMU-based online self-consistency network (OSCNet) to conduct freehand 3D US reconstruction. We propose an online modal-level self-supervised strategy (MSS) that integrates multiple IMUs to reduce the influence of single IMU noise and enhance reconstruction performance. We propose an online sequence-level self-consistency strategy (SCS) to improve the reconstruction stability using hierarchical consistency among the generated sub-sequences and the original sequence. The experimental results on the arm and carotid datasets show that our OSCNet achieves state-of-the-art reconstruction performance. Future research will focus on exploring more general reconstruction methods."
Multi-IMU with Online Self-consistency for Freehand 3D Ultrasound Reconstruction,,Fig. 1 .,
Multi-IMU with Online Self-consistency for Freehand 3D Ultrasound Reconstruction,,Fig. 2 .,
Multi-IMU with Online Self-consistency for Freehand 3D Ultrasound Reconstruction,,Fig. 3 .,
Multi-IMU with Online Self-consistency for Freehand 3D Ultrasound Reconstruction,,Fig. 4 .,
Multi-IMU with Online Self-consistency for Freehand 3D Ultrasound Reconstruction,,Fig. 5 .,
Multi-IMU with Online Self-consistency for Freehand 3D Ultrasound Reconstruction,,Table 1 .,
Masked Vision and Language Pre-training with Unimodal and Multimodal Contrastive Losses for Medical Visual Question Answering,1,Introduction,"Medical VQA is a specialized domain of VQA that aims to generate answers to natural language questions about medical images. It is very challenging to train deep learning based medical VQA models from scratch, since the medical VQA datasets available for research are relatively small in scale. Many existing works are proposed to leverage pre-trained visual encoders with external datasets to solve downstream medical VQA tasks, such as utilizing denoising autoencoders [1] and meta-models [2]. These methods mainly transfer feature encoders that are separately pre-trained on unimodal (image or text) tasks.Unlike unimodal pretraining approaches, both image and text feature presentations can be enhanced by learning through the visual and language interactions, given relatively richer resources of medical image caption datasets [3][4][5]. Liu et al. followed the work of MOCO [19] that trained teacher model for visual encoder via contrastive loss of different image views (by data augmentations) to improve the generalization of medical VQA [6]. Eslami et al. utilized CLIP [7] for visual model initialization, and learned cross-modality representations from medical image-text pairs by maximizing the cosine similarity between the extracted features of medical images and their corresponding captions [8]. Cong et al. devised an innovative framework, which featured a semantic focusing module to emphasize image regions that were pertinent to the caption and a progressive cross-modality comprehension module that iteratively enhanced the comprehension of the correlation between the image and caption [9]. Chen et al. proposed a medical vision language pre-training approach that used both masked image modelling and masked language modelling to jointly learn representations of medical images and their corresponding descriptions [10]. However, to the best of our knowledge, there have been no existing methods that explore learning both unimodal and multimodal features at the pre-training stage for downstream medical VQA tasks.In this paper, we proposed a new self-supervised vision language pre-training (VLP) approach that applied Masked image and text modeling with Unimodal and Multimodal Contrastive losses (MUMC) in the pre-training phase for solving downstream medical VQA tasks. The model was pretrained on image caption datasets for aligning visual and text information, and transferred to downstream VQA datasets. The unimodal and multimodal contrastive losses in our work are applied to (1) align image and text features;(2) learn unimodal image encoders via momentum contrasts of different views of the same image (i.e. different views are generated by different image masks); (3) learn unimodal text encoder via momentum contrasts. We also introduced a new masked image strategy by randomly masking the patches of the image with a probability of 25%, which serves as a data augmentation technique to further enhance the performance of the model. Our approach outperformed existing methods and sets new benchmarks on three medical VQA datasets [11][12][13], with significant enhancements of 2.2%, 14.7%, and 1.7% respectively. Besides, we conducted an analysis to verify the effectiveness of different components and find the optimal masking probability. We also conducted a qualitative analysis on the attention maps using Grad-CAM [14] to validate whether the corresponding part of the image is attended when answering a question."
Masked Vision and Language Pre-training with Unimodal and Multimodal Contrastive Losses for Medical Visual Question Answering,2,Methods,"In this section, we provide the detailed description of the proposed approach, which includes the network architectures, self-supervised pre-training objectives, and the way to fine-tune on downstream medical VQA tasks."
Masked Vision and Language Pre-training with Unimodal and Multimodal Contrastive Losses for Medical Visual Question Answering,2.1,Model Architecture,"In the pre-training phase, the network architecture comprises an image encoder, a text encoder, and a multimodal encoder, which are all based on the transformer architecture [15]. As shown in Fig. 1(a), the image encoder leverages a 12-layer Vision Transformer (ViT) [16] to extract visual features from the input images, while the text encoder employs a 6-layer transformer which is initialized by the first 6 layers of pre-trained BERT [17]. The last 6 layers of BERT are utilized as the multimodal encoder and incorporated cross-attention at each layer, which fuses the visual and linguistic features to facilitate learning of multimodal interactions. The model is trained on medical image-caption pairs. An image is partitioned into patches of size 16 × 16, and 25% of the patches are randomly masked. The remaining unmasked image patches are converted into a sequence of embeddings by an image encoder. The text, i.e. the image caption is tokenized into a sequence of tokens using a WordPiece [18] tokenizer and fed into the BERT-based text encoder. In addition, the special tokens, [CLS] are appended to the beginning of both the image and text sequence. To transfer the models trained on image caption datasets to the downstream medical VQA tasks, we utilize the weights from the pre-training stage to initialize the image encoder, text encoder and multimodal encoder, as shown in Fig. 1(b). To generate answers, we add an answering decoder with a 6-layer transformer-based decoder to the model, which receives the multimodal embeddings and output text tokens. A [CLS] token serves as the initial input token for the decoder, and a [SEP] token is appended to signify the end of the generated sequence. The downstream VQA model is fine-tuned via the masked language model (MLM) loss [17], using ground-truth answers as targets."
Masked Vision and Language Pre-training with Unimodal and Multimodal Contrastive Losses for Medical Visual Question Answering,2.2,Unimodal and Multimodal Contrastive Losses,"The proposed self-supervised objective attempts to capture the semantic discrepancy between positive and negative samples across both unimodal and multimodal domains at the same time. The unimodal contrastive loss (UCL) aims to differentiate between examples of one modality, such as images or text, in a latent space to make similar examples close. And the multimodal contrastive loss (MCL) learns the alignments between both modalities by maximizing the similarity between images and their corresponding text captions, while separating from the negative examples. In the implementation, we maintain two momentum models for image and text encoders respectively to generate different perspectives or representations of the same input sample, which serve as positive samples for contrastive learning.In detail, we denote the image and caption embeddings from the unimodal image encoder and text encoder as v cls and t cls , which are further processed through the transformations g v and g t , to normalize and map the image and text embeddings to be lowerdimensional representations. The embeddings are inserted into a lookup table, and only the most recent 65,535 pairs of image-text embedding are stored for contrastive learning. We utilize the momentum update technique originally proposed in MoCo [19], which is updated every k iterations where k is a hyperparameter. We denote the ground-truth one-hot similarity by y i2i (V ), y t2t (T ), y i2t (V ), and y t2i (T ), where the probability of negative pairs is 0 and the probability of the positive pair is 1. The unimodal contrastive losses and multimodal contrastive losses can be defined as the cross-entropy H given as follows:where s denotes cosine similarity function,and τ is a learnable temperature parameter."
Masked Vision and Language Pre-training with Unimodal and Multimodal Contrastive Losses for Medical Visual Question Answering,2.3,Image Text Matching,"We adopt the image text matching (ITM) strategy similar to prior works [20,21] as one of the training objectives, by creating a binary classification task with negative text labels randomly sampled from the same minibatch. The joint representation of the image and text are encoded by the multimodal encoder, and utilized as input to the binary classification head. The ITM task is optimized using the cross-entropy loss:the function H (, ) represents a cross-entropy computation, where y itm denotes the groundtruth label and p itm (V , T ) is a function for predicting the class."
Masked Vision and Language Pre-training with Unimodal and Multimodal Contrastive Losses for Medical Visual Question Answering,2.4,Masked Language Modeling,"Masked Language Modeling (MLM) is another pre-trained objective in our approach, that predicts masked tokens in text based on both the visual and unmasked contextual information. For each caption text, 15% of tokens are randomly masked and replaced with the special token, [MASK]. Predictions of the masked tokens are conditioned on both unmasked text and image features. We minimize the cross-entropy loss for MLM:where H (, ) is a cross-entropy calculation, T denotes the masked text token, y mlm represents the ground-truth of the masked text token and p mlm (V , T ) is the predicted probability of a masked token."
Masked Vision and Language Pre-training with Unimodal and Multimodal Contrastive Losses for Medical Visual Question Answering,2.5,Masked Image Strategy,"Besides the training objectives, we introduce a masked image strategy as a data augmentation technique. In our experiment, input images are partitioned into patches which are randomly masked with a probability of 25%, and only the unmasked patches are passed through the network. Unlike the previous methods [10,22], we do not utilize reconstruction loss [23], but use this only as a data augmentation method. This enables us to process more samples at each step, resulting in a more efficient pre-training of vision-language models with a similar memory footprint."
Masked Vision and Language Pre-training with Unimodal and Multimodal Contrastive Losses for Medical Visual Question Answering,3,Experiments,
Masked Vision and Language Pre-training with Unimodal and Multimodal Contrastive Losses for Medical Visual Question Answering,3.1,Datasets,"Our model is pre-trained on three datasets: ROCO [3], MedICaT [4], and the Image-CLEF2022 Image Caption Dataset [5]. ROCO comprises over 80,000 image-caption pairs. MedICaT includes over 217,000 medical images and their corresponding captions. ImageCLEF2022 is another well-known dataset that has nearly 90,000 pairs of medical images and captions.For the downstream medical VQA task, we fine-tune and validate the model on three public medical VQA datasets: VQA-RAD [11], PathVQA [12] and SLAKE [13]. VQA-RAD has 315 radiology images with 3064 question-answer pairs, with 451 pairs used for testing. SLAKE has 14,028 pairs of samples which are further divided into 70% training, 15% validation, and 15% testing subsets. PathVQA is the largest dataset, containing 32,799 pairs of data that are split into training (50%), validation (30%), and test (20%) sets.There are two types of questions: closed-ended questions that have limited answer choices (e.g. ""yes"" or ""no"") and open-ended questions that VQA models are required to generate answers in free text, which are more challenging."
Masked Vision and Language Pre-training with Unimodal and Multimodal Contrastive Losses for Medical Visual Question Answering,3.2,Implementation Details,"Our method was implemented in Python 3.8 and PyTorch 1.10. The experiments were conducted on a server with an Intel Xeon(R) Platinum 8255C and 2 NVIDIA Tesla V100 GPUs with 32 GB memory each. We pre-trained our model on three medical image caption datasets for 40 epochs with a batch size of 64. AdamW [24] optimizer was used with a weight decay of 0.002 and an initial learning rate of 1e -4 , which decayed to 2e -5 by following the cosine schedule. We utilized randomly cropped images of 256 × 256 resolution as input, and also applied RandAugment to augment more training samples [25].For downstream medical VQA tasks, we fine-tuned our model for 30 epochs with a batch size of 8. We used the AdamW optimizer with a reduced learning rate of 2e -5 , which decayed to 1e -8 . Besides, we increased image inputs from a resolution of 256 × 256 to 384 × 384 and interpolated the positional encoding following [16]."
Masked Vision and Language Pre-training with Unimodal and Multimodal Contrastive Losses for Medical Visual Question Answering,3.3,Comparison with the State-of-the-Arts,"We performed a comparative evaluation of our model against the existing approaches [10,26] on three benchmark datasets, VQA-RAD, PathVQA and SLAKE. Consistent with previous research [1, 2, 6, 8-10, 26, 27], we adopt accuracy as the performance metric. We treated VQA as a generative task by calculating similarities between the generated answers and candidate list answers, selecting the highest score as the final answer. As shown in Table 1, our approach outperformed all other methods on all the three datasets in terms of overall performance, and yielded the best accuracy for openended or closed-ended answers. On the VQA-RAD dataset [11], our method achieved an absolute margin of 2.2% overall over the current state-of-the-art method, M3AE, with improvements of 4.3% and 0.7% on open-ended and closed-ended answers respectively. On the largest dataset, PathVQA [12], our method significantly outperformed the previous state-of-the-art model, AMAM [26], by a substantial margin with improvements of 20.8%, 6.0% and 14.7% on the closed-ended, open-ended, and overall answers, respectively. Moreover, on the SLAKE dataset [13], the proposed approach exhibited superior performance compared to the existing state-of-the-art model, M3AE, by a margin of 1.7% in terms of overall answer accuracy."
Masked Vision and Language Pre-training with Unimodal and Multimodal Contrastive Losses for Medical Visual Question Answering,3.4,Ablation Study,"To further verify the effectiveness of the proposed methods in learning multimodal representations, we conducted an ablation study across all three medical VQA datasets. Table 2 shows the overall performance of the medical VQA tasks using various pretraining approaches. Compared to the baseline pre-training tasks (i.e., MLM + ITM), integrating either UCL or MCL significantly improved the performance of the pre-trained model across all medical VQA datasets. Notably, the simultaneous use of UCL and MCL achieved a performance increase of 1.1%, 1.0%, and 0.9% on VQA-RAD, PathVQA, and SLAKE dataset, respectively. Furthermore, to assess the performance of the proposed masked image strategy and identify the optimal masking probability, experiments were conducted by varying the masking probabilities of input images at levels of 0%, 25%, 50% and 75%. As presented in Table 3, the results are consistent among all the three datasets. With 25% masking probability, the model yielded the best results, compared to no masking applied. The performance decreased if 50% and 75% masking probabilities were used."
Masked Vision and Language Pre-training with Unimodal and Multimodal Contrastive Losses for Medical Visual Question Answering,3.5,Visualization,"We utilized Grad-CAM [14] to visualize the cross-attention maps between the questions and images, and analyzed the relevance of the attended image regions for generating the answers. In Fig. 2, it showed some attention maps that overlayed on the original images. For answering open-ended questions, the model accurately attended to the relevant infarct regions, as shown in Fig. 2a and Fig. 2b. In Fig. 2a, to answer the question, ""Where is/are the infarct located?"", the model highlighted the areas that well covered the infarction. Interestingly, the model attended to infarct areas on both hemispheres (Fig. 2b) and generated the answer, ""Bilateral"". Besides the position-related questions, in Fig. 2c, it showed the attention map to answer the closed form question, ""Is there any region in the brain that is lesioned?"". The model successfully attended to the lesion area and provided the correct answer of ""Yes"". Moreover, the model demonstrated its ability to attend to the regions of ribs to answer the counting-related question in Fig. 2d, where the question was ""Are there more than 12 ribs?"", and the model accurately outputted the answer ""Yes"". "
Masked Vision and Language Pre-training with Unimodal and Multimodal Contrastive Losses for Medical Visual Question Answering,4,Conclusion,"In this paper, we propose a new method to tackle the challenge of medical VQA tasks, which is pre-trained on the medical image caption datasets and then transferred to the downstream medical VQA tasks. The proposed self-supervised pre-training approach with unimodal and multimodal contrastive losses leads to significant performance improvement on three public VQA datasets. Also, using masked images as a data augmentation technique is proven to be effective for learning representations on medical visual and language tasks. As a result, our proposed method not only outperformed the state-of-the-art methods by a significant margin, but also demonstrated the potential for model interpretability."
Masked Vision and Language Pre-training with Unimodal and Multimodal Contrastive Losses for Medical Visual Question Answering,,Fig. 1 .,
Masked Vision and Language Pre-training with Unimodal and Multimodal Contrastive Losses for Medical Visual Question Answering,,Fig. 2 .,
Masked Vision and Language Pre-training with Unimodal and Multimodal Contrastive Losses for Medical Visual Question Answering,,Table 1 .,
Masked Vision and Language Pre-training with Unimodal and Multimodal Contrastive Losses for Medical Visual Question Answering,,Table 2 .,
Masked Vision and Language Pre-training with Unimodal and Multimodal Contrastive Losses for Medical Visual Question Answering,,Table 3 .,
Inter-slice Consistency for Unpaired Low-Dose CT Denoising Using Boosted Contrastive Learning,1,Introduction,"Computed tomography (CT) is a common tool for medical diagnosis but increased usage has led to concerns about the possible risks caused by excessive radiation exposure. The well-known ALARA (as low as reasonably achievable) [20] principle is widely adopted to reduce exposure based on the strategies such as sparse sampling and tube flux reduction. However, reducing radiation dose will degrade the imaging quality and then inevitably jeopardize the subsequent diagnoses. Various algorithms have been developed to address this issue, which can be roughly categorized into sinogram domain filtration [16], iterative reconstruction [2,9], and image post-processing [1,8,14].Recently, deep learning (DL) has been introduced for low-dose computed tomography (LDCT) image restoration. The utilization of convolutional neural networks (CNNs) for image super-resolution, as described in [7], outperformed most conventional techniques. As a result, it was subsequently employed for LDCT in [6]. The DIP [21] method is an unsupervised image restoration technique that leverages the inherent ability of untrained networks to capture image statistics. Other methods that do not require clean images are also used in this field [11,23]. Various network architectures have been proposed, such as RED [5] and MAP-NN [18]. The choice of loss function also significantly affects model performance. Perceptual loss [12] based on the pretrained VGG [19] was proposed to mitigate over-smoothing caused by MSE. Most DL techniques for LDCT denoising are supervised models, but unsupervised learning frameworks which do not need paired data for training like GANs [3,10,26], Invertible Network [4] and CUT [25] have also been applied for LDCT [13,15,24].This study presents a novel unsupervised framework for denoising low-dose CT (LDCT) images, which utilizes contrastive learning (CL) and doesn't require paired data. Our approach possesses three major contributions as follows: Firstly, We discard the use of CycleGAN that most unpaired frameworks employ, instead adopting contrastive learning to design the training framework. As a result, the training process becomes more stable and imposes a lesser computational burden. Secondly, our approach can adapt to almost all end-to-end image translation neural networks, demonstrating excellent flexibility. Lastly, the proposed interslice consistency loss makes our model generates stable output quality across slices, in contrast to most slice based methods that exhibit inter-slice instability. Our model outperforms almost all other models in this regard, making it the superior option for LDCT denoising. Further experimental data about this point will be presented in this paper."
Inter-slice Consistency for Unpaired Low-Dose CT Denoising Using Boosted Contrastive Learning,2,Method,"LDCT image denoising can be expressed as a noise reduction problem in the image domain as x = f (x), where x and x denote the denoised output and corresponding LDCT image. f represents the denoising function. Rather than directly denoising LDCT images, an encoder-decoder model is used to extract important features from the LDCT images and predict corresponding NDCT images. Most CNN-based LDCT denoising models are based on supervised learning and require both the LDCT and its perfectly paired NDCT images to learn f . However, it is infeasible in real clinical practice. Currently, some unsupervised models, including CUT and CycleGAN, relax the constraint on requiring paired data for training. Instead, these models can be trained with unpaired data."
Inter-slice Consistency for Unpaired Low-Dose CT Denoising Using Boosted Contrastive Learning,2.1,Contrastive Learning for Unpaired Data,"The task of LDCT image denoising can be viewed as an image translation process from LDCT to NDCT. CUT provides a powerful framework for training a model to complete image-to-image translation tasks. The main concept behind CUT is to use contrastive learning for enhanced feature extraction aided by an adversarial loss.The key principle of contrastive learning is to create positive and negative pairs of samples, in order to help the model gain strong feature representation ability. The loss of contrastive learning can be formulated as:where v, v + , v -denote the anchors, positive and negative pairs, respectively. N is the number of negative pairs. τ is the temperature factor which is set to 0.07 in this paper. The generator G we used contains two parts, an encoder E and a decoder. A simple MLP H is used to module the features extracted from the encoder. The total loss of CUT for image translation is defined as:where D denotes the discriminator. X represents the input images, for which L P atchNCE (G, H, X) utilizes contrastive learning in the source domain (represented by noisy images). Y indicates the images in the target domain, which means NDCT images in this paper. L P atchNCE (G, H, Y ) employs contrastive learning in this target domain. As noted in a previous study [25], this component plays a similar role as the identity loss in CycleGAN. In this work, λ 1 and λ 2 are both set to 1. Since CT images are three-dimensional data, we can identify more negative pairs between different slices. The strategy about how we design positive and negative pairs for our proposed model is illustrated in Fig. 1.As shown in Fig. 1, we select two negative patches from the same slice as the anchor, as well as one from the previous slice and the other from the next slice. It is important to note that these patches are not adjacent, since neighbored slices are nearly identical. Similar to most contrastive learning methods, we use cosine similarity to compute the feature similarity."
Inter-slice Consistency for Unpaired Low-Dose CT Denoising Using Boosted Contrastive Learning,2.2,Contrastive Learning for Inter-slice Consistency,"Due to various constraints, most denoising methods for LDCT can only perform on the slice plane, resulting in detail loss among different slices. While 3D models can mitigate this issue to a certain degree, they require significant computational costs and are prone to model collapse during training, leading to a long training time. Additionally, most methods are unable to maintain structural consistency between slices with certain structures (e.g., bronchi and vessels) appearing continuously across several adjacent slices. To address this issue, we design an inter-slice consistency loss based on contrastive learning. This approach helps to maintain structural consistency between slices, and then improve the overall denoising performance.As illustrated in Fig. 2, we begin by randomly selecting the same patch from both the input (LDCT) and the generated denoised result. These patches are passed through the encoder E, allowing us to obtain the feature representation for each patch. Next, we perform a feature subtraction of each inter-slice pair. The output can be interpreted as the feature difference between slices. We assume that the feature difference between the same pair of slices should be similar, which is formulated as follows:where P denotes the patch selection function. A good denoising generator can minimize the feature difference between similar slices while maximizing the feature difference between different slices. By utilizing contrastive learning, we can treat the former condition as a positive pair and the latter as a negative pair. After computing the cosine similarity of the pairs, a softmax operation is applied to assign 1 to the positive pairs and 0 to the negative pairs.Compared to the original contrastive learning, which focuses on patch pairs, we apply this technique to measure feature differences, which stabilizes the features and improves the consistency between slices. "
Inter-slice Consistency for Unpaired Low-Dose CT Denoising Using Boosted Contrastive Learning,2.3,Boosted Contrastive Learning,"Original contrastive Learning approaches treat every positive and negative pair equally. However, in CT images, some patches may be very similar to others (e.g., patches from the same organ), while others may be completely different. Therefore, assigning the same weight to different pairs may not be appropriate. [25] demonstrated that fine-tuning the weights between pairs can significantly improve the performance of contrastive learning.For our inter-slice consistency loss, only one positive and negative pair can be generated at a time, making it unnecessary to apply reweighting. However, we include additional negative pairs in the patchNCE loss for unpaired translation, making reweighting between pairs more critical than in the original CUT model. As a result, Eq. 1 is updated as follows:where w stands for a weight factor for each negative patch.According to [25], using ""easy weighting"" is more effective for unpaired tasks, which involves assigning higher weights to easy negative samples (i.e., samples that are easy to distinguish from the anchor). This finding contradicts most people's intuition. Nonetheless, we have demonstrated that their discovery is accurate in our specific scenario. The reweighting approach we have employed is defined as follows:In summary, the less similar two patches are, the easier they can be distinguished, the more weight the pair is given for learning purposes."
Inter-slice Consistency for Unpaired Low-Dose CT Denoising Using Boosted Contrastive Learning,3,Experiments,
Inter-slice Consistency for Unpaired Low-Dose CT Denoising Using Boosted Contrastive Learning,3.1,Dataset and Training Details,"While our method only requires unpaired data for training, many of the compared methods rely on paired NDCT. We utilized the dataset provided by the Mayo Clinic called ""NIH-AAPM-Mayo Clinic Low Dose CT Grand Challenge"" [17], which offers paired LD-NDCT images.The model parameters were initialized using a random Gaussian distribution with zero-mean and standard deviation of 10 -2 . The learning rate for the optimizer was set to 10 -4 and halved every 5 epochs for 20 epochs total. The experiments were conducted in Python on a server with an RTX 3090 GPU. Two metrics, peak signal-to-noise ratio (PSNR) and structural similarity index measure (SSIM) [22], were employed to quantitatively evaluate the image quality. The image data from five individuals were used as the training set and the data from other two individuals were used for the test set."
Inter-slice Consistency for Unpaired Low-Dose CT Denoising Using Boosted Contrastive Learning,3.2,Comparison of Different Methods,"To demonstrate the denoising performance of our model, we conducted experiments to compare our method with various types of denoising methods including unsupervised denoising methods that only use LDCT data, fully supervised methods that use perfectly registered LDCT and NDCT pairs, and semisupervised methods, including CycleGAN and CUT, which utilize unpaired data. A representative slice processed by different methods is shown in Fig. 3. The window center is set to 40 and the window width is set to 400.Our framework is flexible and can work with different autoencoder frameworks. In our experiments, the well-known residual encoder-decoder network (RED) was adopted as our network backbone.The quantitative results and computational costs of unsupervised methods are presented in Table 1. It can be seen that our method produces promising denoising results, with obvious numerical improvements compared to other unsupervised and semi-supervised methods.As shown in Table 2, our score is very close to our backbone model when trained fully supervised. Our model even got higher PSNR value.Moreover, our framework is lightweight, which has a similar model scale to RED. It's worth noting that adding perceptual loss to our model will decrease the PSNR result, and it is consistent with the previous studies that perceptual loss may maintain more details but decrease the MSE-based metric, such as PSNR.Furthermore, the reweighting mechanism demonstrates its effectiveness in improving our model's results. The improvement by introducing the reweighting mechanism can be easily noticed.  "
Inter-slice Consistency for Unpaired Low-Dose CT Denoising Using Boosted Contrastive Learning,3.3,Line Plot over Slices,"Although our method may only be competitive with supervised methods, we are able to demonstrate the effectiveness of our proposed inter-slice consistency loss. The line plot in Fig. 4 shows the pixel values at point (200, 300) across different slices.In Fig. 4, it can be observed that our method effectively preserves the interslice consistency of features, which is clinically important for maintaining the structural consistency of the entire volume. Although the supervised model achieves a similar overall score to our model, the results across slices of our model are closer to the ground truth (GT), especially when pixel value changes dramatically.  "
Inter-slice Consistency for Unpaired Low-Dose CT Denoising Using Boosted Contrastive Learning,3.4,Discussion,"Our method achieves competitive results and obtains the highest PSNR value in all the methods with unpaired samples. Although we cannot surpass supervised methods in terms of some metrics, our method produces promising results across consecutive slices that are more consistent and closer to the GT."
Inter-slice Consistency for Unpaired Low-Dose CT Denoising Using Boosted Contrastive Learning,4,Conclusion,"In this paper, we introduce a novel low-dose CT denoising model. The primary motivation for this work is based on the fact that most CNN-based denoising models require paired LD-NDCT images, while we usually can access unpaired CT data in clinical practice. Furthermore, many existing methods using unpaired samples require extensive computational costs, which can be prohibitive for clinical use. In addition, most existing methods focus on a single slice, which results in inconsistent results across consecutive slices. To overcome these limitations, we propose a novel unsupervised method based on contrastive learning that only requires a single generator. We also apply modifications to the original contrastive learning method to achieve SOTA denoising results using relatively a low computational cost.Our experiments demonstrate that our method outperforms existing SOTA supervised, semi-supervised, and unsupervised methods in both qualitative and quantitative measures. Importantly, our framework does not require paired training data and is more adaptable for clinical use."
Inter-slice Consistency for Unpaired Low-Dose CT Denoising Using Boosted Contrastive Learning,,Fig. 1 .,
Inter-slice Consistency for Unpaired Low-Dose CT Denoising Using Boosted Contrastive Learning,,Fig. 2 .,
Inter-slice Consistency for Unpaired Low-Dose CT Denoising Using Boosted Contrastive Learning,,Fig. 3 .,
Inter-slice Consistency for Unpaired Low-Dose CT Denoising Using Boosted Contrastive Learning,,Fig. 4 .,
Inter-slice Consistency for Unpaired Low-Dose CT Denoising Using Boosted Contrastive Learning,,Table 1 .,
Inter-slice Consistency for Unpaired Low-Dose CT Denoising Using Boosted Contrastive Learning,,Table 2 .,
You’ve Got Two Teachers: Co-evolutionary Image and Report Distillation for Semi-supervised Anatomical Abnormality Detection in Chest X-Ray,1,Introduction,"Chest X-ray (CXR) is the most commonly performed diagnostic radiograph in medicine, which helps spot abnormalities or diseases of the airways, blood vessels, bones, heart, and lungs. Given the complexity and workload of clinical CXR reading, there is a growing interest in developing automated methods for anatomical abnormality detection in CXR [19]-especially using deep neural networks (DNNs) [14,17,20], which are expected to expedite clinical workflow and reduce observational oversights. Here, the detection task involves both localization (e.g., with bounding boxes) and characterization (e.g., cardiomegaly) of the abnormalities. However, training accurate DNN-based detection models usually requires large-scale datasets with high-quality per-abnormality annotations, which is costly in time, effort, and expense.To completely relieve the burden of annotation, a few works [2,22,25] resorted to the radiology reports as a form of weak supervision for localization of pneumonia and pneumothorax in CXR. The text report describes important findings in each CXR and is available for most archive radiographs, thus is a valuable source of image-level supervision signal unique to medical image data. However, studies have shown that there are still apparent gaps in performance between imagelevel weakly supervised and bounding-box-level fully supervised detection [1,11]. Alternatively, seeking for a trade-off between annotation effort and model performance, semi-supervised learning aims to achieve reasonable performance with an acceptable quantity of manual annotations. Semi-supervised object detection methods have achieved noteworthy advances in the natural image domain [4,21,23]. Most of these methods were built on the teacher-student distillation (TSD) paradigm [10], where a teacher model is firstly trained on the labeled data, and then a student model is trained on both the labeled data with real annotations and the unlabeled data with pseudo labels generated (predicted) by the teacher. However, compared with objects in natural images, the abnormalities in CXR can be subtle and less well-defined with ambiguous boundaries, thus likely to introduce great noise to the pseudo labels and eventually lead to suboptimal performance of semi-supervised learning with TSD.In this paper, we present a co-evolutionary image and report distillation (CEIRD) framework for semi-supervised anatomical abnormality detection in CXR, incorporating the weak supervision by radiology reports. Above all, on the basis of TSD [10], CEIRD introduces an auxiliary, also semi-supervised, multi-label report classification natural language processing task, whose prediction is used for noise reduction in the pseudo labels of the primary vision detection task, i.e., report-guided pseudo detection label refinement (RPDLR). Then, noting that the performance of the auxiliary language task is crucial to RPDLR, we inversely use the abnormalities detected in the vision task to filter the pseudo labels in the language task, for abnormality-guided pseudo classification label refinement (APCLR). In addition, we implement an iterative coevolution strategy where RPDLR and APCLR are performed alternatively in a loop, where either model is trained while fixing the other and using the other's prediction for pseudo label refinement. To the best of our knowledge, this is the first work that approaches semi-supervised abnormality detection in CXR by grounding report-classified abnormalities with the visual detection results in the paired radiograph [5,24], and vice versa. Besides the cross-modal pseudo label refinement, we additionally propose self-adaptive non-maximum suppression (SA-NMS) for intra-(image-)modal refinement, too, where the predictions by both the teacher and student vision models go through NMS together to produce new pseudo detection labels for training. In this way, the pseudo labels generated by the teacher are dynamically rectified by high-confidence predictions of the student who is getting better as training goes. Experimental results on the MIMIC-CXR [12,13] public benchmark show that our CEIRD outperforms various up-to-date weakly and semi-supervised methods, and that its building elements are effective.To summarize, our contributions include: (1) the complementary RPDLR and APCLR for noise reduction in both vision and language pseudo labels for improved semi-supervised training via mutual grounding, (2) the co-evolution strategy for joint optimization of the primary and auxiliary tasks, and (3) the SA-NMS for dynamic intra-image-modal pseudo label refinement, all contributing to the superior performance of the proposed CEIRD framework."
You’ve Got Two Teachers: Co-evolutionary Image and Report Distillation for Semi-supervised Anatomical Abnormality Detection in Chest X-Ray,2,Method,"Problem Setting. In semi-supervised anatomical abnormality localization, a data set comprising both unlabeled samplesi=1 is provided for training, where x and r are a CXR and accompanying report, respectively, A i = {(y l , B l )} is the annotation for a labeled sample including both bounding boxes {B l } and corresponding categories {y l }, and N l N u for practical use scenario. It is worth noting that {y l } can also be considered as classification labels for the report. The objective is to obtain a detection model that can accurately localize and classify the abnormalities in any testing CXR (without report in practice), by making good use of both the labeled and unlabeled CXRs plus the accompanying reports in the training set. detection in CXR is given, together with a pretrained language model F R s for multi-label abnormality classification of reports. On the one hand, we generate for an unlabeled image x u i pseudo detection labels with F I t and filter the pseudo labels by self-adaptive non-maximum suppression (NMS). Meanwhile, we feed the corresponding report r u i into F R s and use the prediction for report-guided pseudo detection label refinement (RPDLR). To this end, we obtain refined pseudo labels to supervise the student vision model F I s toward better anatomical abnormality localization. On the other hand, we also pass the detection predictions by F I s to a teacher language model F R t for abnormality-guided pseudo classification label refinement (APCLR), to better supervise the student language model F R s on unlabeled data for report-based abnormality classification. In turn, the better language model F R s helps train better vision models via RPDLR, thus both types of models co-evolve during training. Note that the real labels are used to train both student models along with the pseudo ones. After training, we only need the student vision model F I s for abnormality localization in testing CXRs."
You’ve Got Two Teachers: Co-evolutionary Image and Report Distillation for Semi-supervised Anatomical Abnormality Detection in Chest X-Ray,,Method Overview.,
You’ve Got Two Teachers: Co-evolutionary Image and Report Distillation for Semi-supervised Anatomical Abnormality Detection in Chest X-Ray,,Preliminary Pseudo Label Distillation for Semi-supervised Learning.,"Both of our baseline semi-supervised vision and language models follow the teacher-student knowledge distillation (TSD) procedure [10]. For report classification, we first train a teacher model F R t on labeled reports, and then train a student model F R s to predict real labels on labeled reports and pseudo labels produced by F R t on unlabeled ones with the loss function:where L R cls is the cross-entropy loss, {ŷ} = F R s (r) is the prediction by the student model, {y pr } = F R t (r u ) is the set of pseudo labels generated by the teacher model. In each batch, labeled and unlabeled instances are sampled according to a controlled ratio. The resulting report classification model F R s will be utilized later to help with the primary task of abnormality detection in CXR. Similarly, a student vision model F I s for abnormality detection in CXR is trained in semisupervised setting by distilling from a teacher vision model F I t trained on labeled CXRs, with the loss function:where {(ŷ, B)} = F I s (x) are the predictions by the student model, {(y pv , B pv )} = F I t (x u ) are the pseudo class and bounding box labels generated by the teacher model, L I cls is the focal loss [16] for abnormality classification, and L I reg is the smooth L1 loss for bounding box regression.Self-adaptive Non-maximum Suppression. During the TSD, the teacher vision model F I t is kept fixed. While its knowledge suffices for guiding the student vision model F I s in the early stage of TSD, it may somehow impede the learning of F I s when F I s gradually improves by also learning from the large amount of unlabeled data. Therefore, to gradually improve quality and robustness of the pseudo detection labels as F I s learns, we propose to perform self-adaptive non-maximum suppression (SA-NMS) to combine the pseudo labels {(y pv , B pv )} output by F I t and the predictions {(ŷ, B)} by F I s in each mini batch. Specifically, we perform NMS on the combined set of the pseudo labels and predictions: {(y cv , B cv )} = NMS {(y pv , B pv )} {(ŷ, B)} , and replace {(y pv , B pv )} in Eq.(2) with {(y cv , B cv )} for supervision by unlabeled CXRs. In this way, highly confident predictions by the maturing student can rectify imprecise ones by the teacher, leading to better supervision signals stemming from unlabeled data.Report-Guided Pseudo Label Refinement. In routine clinics, almost every radiograph in archive is accompanied by a report describing findings, abnormalities (if any), and diagnosis. Compared with the captions of natural images, the report texts constitute a unique (to medical image analysis) and rich source of extra information in addition to the image modality. To this end, we propose report-guided pseudo detection label refinement (RPDLR) to make use of this cross-modal information for semi-supervised anatomical abnormality detection in CXR. Specifically, we use the student language model F R s (trained with Eq. ( 1)) to refine the pseudo detection labels. Given a pair of unlabeled image x u and report r u , we obtain the set of abnormalities {(y cv , B cv )} detected in x u after SA-NMS, and the set of abnormalities {ŷ} classified in r u by F R s . Then, we only keep the pseudo detection labels whose categories are in the report-classified abnormalities:Eventually, we train the student vision model F I s using {(y v , B v )} in Eq. ( 2)."
You’ve Got Two Teachers: Co-evolutionary Image and Report Distillation for Semi-supervised Anatomical Abnormality Detection in Chest X-Ray,,Co-evolutionary Semi-supervised Learning with Cycle Pseudo Label,"Refinement. As the auxiliary student language model F R s plays an important role in RPDLR, it is reasonable to optimize its performance which in turn would benefit the primary task of abnormality detection. Therefore, we further propose an inverse, abnormality-guided pseudo classification labels refinement (APCLR) to help with semi-supervised training of the report classification model. Similarly in concept to the RPDLP, given a pair of unlabeled image x u and report r u , we obtain the set of abnormalities {(ŷ, B)} detected in x u by the student vision model F I s , and the set of classification pseudo labels {y pr } generated for r u by the teacher language model F R t . We retain only the pseudo labels {y pr j |y pr j ∈ {ŷ}}, by excluding the report-classified abnormalities not detected in the paired CXR.Ideally, one should use an optimal report classification model for refinement of the abnormality detection pseudo labels, and vice versa. However, the two models are mutually dependent on each other in a circle. To solve this dilemma, we implement an alternative co-evolution strategy to refine the abnormality detection and report classification pseudo labels iteratively, in generations. As shown in Fig. 2, the k th generation student vision model F I s,k is distilled from the teacher F I t,k-1 , whose pseudo labels are refined by the prediction of the frozen student language model F R s,k via RPDLR. Subsequently, F I s,k is frozen and used to 1) help train the (k + 1) th student language model F R s,k+1 via APCLR, and 2) serve as the teacher vision model in next generation: F I s,k → F I t,k .1 Note that in each generation the students are reborn with random initialization [8]. Thus the co-evolution continues to optimize the vision and report models cyclically with cross-modal mutual promotion. After training, we only need the K th generation student vision model F I s,K for abnormality detection in upcoming test CXRs."
You’ve Got Two Teachers: Co-evolutionary Image and Report Distillation for Semi-supervised Anatomical Abnormality Detection in Chest X-Ray,3,Experiments,"Dataset and Evaluation Metrics. We conduct experiments on the chest radiography dataset MIMIC-CXR [12,13], with the detection annotations provided by MS-CXR [3]. MIMIC-CXR is a large publicly available dataset of CXR and free-text radiology reports. MS-CXR provides bounding box annotations for part of the CXRs in MIMIC-CXR (1,026 samples). MIMIC-CXR includes 14 categories of anatomical abnormalities for multi-label classification of the reports, while there are only eight categories in the bounding box annotations of MS-CXR. For consistency, we exclude samples in MIMIC-CXR that have abnormality labels outside the eight categories of MS-CXR, leaving 112,425 samples. Thus, in our semi-supervised setting, 1,026 samples are labeled and the rest are not. 2 We split the labeled samples for training, validation, and testing according to the ratio of 7:1:2, and use the remaining samples as our unlabeled training data. We focus on the frontal views in this work. The mean average precision (mAP) [7] with the intersection over union (IoU) threshold of 0.25, 0.5, and 0.75 is employed to evaluate the performance of abnormality detection in CXR.Implementation. The PyTorch [18] framework (1.4.0) is used for experiments.For report classification, we employ the BERT-base uncased model [6] with eight linear heads. Stochastic gradient descent with the momentum of 0.9 and learning rate of 10 -4 is used for optimization. The batch size is set to 16 reports. For abnormality detection, we employ RetinaNet [16] with FPN [15]+ResNet-101 [9] as backbone. We resize all images to 512×512 pixels and use a batch size of 16. Data augmentation including random cropping and flipping is performed.Our implementation and hyper-parameters follow the official settings [16]. Unless otherwise stated, we evolve the vision and language models for two generations, and train both models for 2000 iterations in each generation (including initial training of the teacher models). The ratio of labeled to unlabeled samples in each mini batch during the semi-supervised training is empirically set to 1:1 and 2:1 for the language and vision models, respectively. The source code is available at: https://github.com/jinghanSunn/CEIRD.Comparison with State-of-the-Art (SOTA) Methods. We compare our proposed co-evolution image and report distillation (CEIRD) framework with several up-to-date detection methods, including weakly supervised: CAM [26] (locating objects based on class activation maps), AGXNet [25] (aiding CAMbased localization with report representations), fully supervised on labeled training data only (Sup.), baseline semi-supervised via teacher-student pseudo-label distillation (TSD; see Eq. ( 2)) [10], and three SOTA semi-supervised (STAC [21], LabelMatch [4], and Soft Teacher [23]) object detection methods.The results are shown in Table 1, from which we have the following observations. First, both fully supervised (by the labeled data only) and semi-supervised methods outperform the weakly-supervised by large margins, proving the efficacy of using limited annotations. Second, all semi-supervised methods outperform the fully supervised (by the labeled data only) by various margins, demonstrating  apparent benefit of making use of the unlabeled data, too. Third, our CEIRD achieves the best performance of all the semi-supervised methods for the mAPs evaluated at three different IoU thresholds, outperforming the second best (Soft Teacher) by up to 1.76%. These results clearly demonstrate the advantage of our method which innovatively integrates the semi-supervision by unlabeled images and the weak supervision by texts. In addition, we evaluate a semi-oracle of our method, where the ground truth report labels provided in MIMIC-CXR are used for RPDLR, instead of the auxiliary model's prediction. As we can see, our method is marginally short of the semi-oracle, e.g., 37.20% versus 37.39% for mAP@0.5, suggesting that our co-evolution strategy can effectively mine the relevant information from the reports. We provide in the supplementary material visualizations of example detection results by Soft Teacher and our method, where ours are visually superior with fewer misses (left), fewer false positives (middle), and better localization (right). Lastly, we also provide performance evaluation of the auxiliary report classification task in the supplement.Ablation Study. We conduct ablation studies on the validation data to investigate efficacy of the novel building elements of our CEIRD framework, including: report-guided pseudo detection label refinement (RPDLR), co-evolution strategy (CoE) with abnormality-guided pseudo classification label refinement (APCLR), and self-adaptive non-maximum suppression (SA-NMS). We use the preliminary teacher-to-student pseudo label distillation as baseline (Eq. ( 2)). As shown in Table 2, RPDLR (column (a)) substantially boosts performance upon the baseline by 1.79-4.06% in mAPs thanks to the refined pseudo detection labels. By adopting CoE+APCLR (column (b)), we achieve further performance improvements up to 1.53% as the auxiliary report classification model gets better together. Last but not the least, the introduction of SA-NMS (column (c)) also brings improvements up to 2.14%. These results validate the novel design of our framework. In addition, we conduct experiments to empirically determine the optimal number of generations for the co-evolution. The results are shown in Fig. 3, where the 0 th generation means fully supervised models trained on the labeled data only (i.e., the initial teacher models F I t,0 and F R t,0 ). As we can see, the vision and report models improve in the first two and three generations, respectively, and then remain stable in the following ones, confirming that both models promote each other with the co-evolution strategy. Since our ulti-mate objective is abnormality detection in CXR, we select two generations for comparison with other methods."
You’ve Got Two Teachers: Co-evolutionary Image and Report Distillation for Semi-supervised Anatomical Abnormality Detection in Chest X-Ray,4,Conclusion,"In this work, we proposed a new co-evolutionary image and report distillation (CEIRD) framework for semi-supervised anatomical abnormality detection in chest X-ray. On the basis of a preliminary teacher-student pseudo label distillation, we first presented self-adaptive NMS to mingle highly confident predictions by both the teacher and student for improved pseudo labels. We then proposed report-guided pseudo detection label refinement (RPDLR) that used abnormalities classified from the accompanying radiology reports by an auxiliary language model to eliminate unmatched pseudo labels. Meanwhile, we further proposed an inverse, abnormality-guided pseudo classification label refinement (APCLR) making use of the abnormalities detected in X-ray images for better language model training. In addition, we implemented a co-evolution strategy that looped the RPDLR and APCLR to iteratively optimize the main vision detection model and auxiliary report classification model in an alternative manner. Experimental results showed that our CEIRD framework achieved superior performance to up-to-date semi-/weakly-supervised methods."
You’ve Got Two Teachers: Co-evolutionary Image and Report Distillation for Semi-supervised Anatomical Abnormality Detection in Chest X-Ray,,Fig. 1 .,
You’ve Got Two Teachers: Co-evolutionary Image and Report Distillation for Semi-supervised Anatomical Abnormality Detection in Chest X-Ray,,Fig. 2 .,
You’ve Got Two Teachers: Co-evolutionary Image and Report Distillation for Semi-supervised Anatomical Abnormality Detection in Chest X-Ray,,Fig. 3 .,
You’ve Got Two Teachers: Co-evolutionary Image and Report Distillation for Semi-supervised Anatomical Abnormality Detection in Chest X-Ray,,Table 1 .,
You’ve Got Two Teachers: Co-evolutionary Image and Report Distillation for Semi-supervised Anatomical Abnormality Detection in Chest X-Ray,,Table 2 .,
You’ve Got Two Teachers: Co-evolutionary Image and Report Distillation for Semi-supervised Anatomical Abnormality Detection in Chest X-Ray,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0 35.
Modularity-Constrained Dynamic Representation Learning for Interpretable Brain Disorder Analysis with Functional MRI,1,Introduction,"Resting-state functional magnetic resonance imaging (rs-fMRI) has been increasingly used to help us understand pathological mechanisms of neurological disorders by revealing abnormal or dysfunctional brain connectivity patterns [1][2][3][4]. Brain regions-of-interest (ROIs) or functional connectivity (FC) involved in these patterns can be used as potential biomarkers to facilitate objective quantification of brain pathology [5]. Previous studies have designed various machine and deep learning models to extract fMRI features and explore disease-related imaging biomarkers [6,7]. However, due to the complexity of brain organization and black-box property of many learning-based models, the generated biomarkers are usually difficult to interpret, thus limiting their utility in clinical practice [8,9]. On the other hand, the human brain operates as a modular system, where each module contains a set of ROIs that are densely connected within the module but sparsely connected to ROIs in other modules [10,11]. In particular, the central executive network (CEN), salience network (SN), and default mode network (DMN) are three prominent resting-state neurocognitive modules in the brain, supporting efficient cognition [12]. Unfortunately, existing learning-based fMRI studies usually ignore such inherent modular brain structures [13,14].To this end, we propose a modularity-constrained dynamic representation learning (MDRL) framework for interpretable brain disorder analysis with rs-fMRI. As shown in Fig. 1, the MDRL consists of (1) dynamic graph construction, (2) modularity-constrained spatiotemporal graph neural network (MSGNN) for dynamic graph representation learning, and (3) prediction and biomarker detection. The MSGNN is designed to learn spatiotemporal features via graph isomorphism network and transformer layers, constrained by 3 neurocognitive modules (i.e., central executive network, salience network, and default mode network). To enhance discriminative ability of learned fMRI embeddings, we also encourage the MSGNN to reconstruct topology of input graphs. To our knowledge, this is among the first attempts to incorporate modularity prior to graph learning models for fMRI-based brain disorder analysis. Experimental results on two public and one private datasets validate the effectiveness of the MDRL in detecting three brain disorders with rs-fMRI data."
Modularity-Constrained Dynamic Representation Learning for Interpretable Brain Disorder Analysis with Functional MRI,2,Materials and Methodology,
Modularity-Constrained Dynamic Representation Learning for Interpretable Brain Disorder Analysis with Functional MRI,2.1,Subjects and Image Preprocessing,"Two public datasets (i.e., ABIDE [15] and MDD [16]) and one private HIVassociated neurocognitive disorder (HAND) dataset with rs-fMRI are used. The two largest sites (i.e., NYU and UM) of ABIDE include 79 patients with autism spectrum disorder (ASD) and 105 healthy controls (HCs), and 68 ASDs and 77 HCs, respectively. The two largest sites (i.e., Site 20 and Site 21) of MDD contain 282 patients with major depressive disorder (MDD) and 251 HCs, 86 MDDs and 70 HCs, respectively. The HAND were collected from Beijing YouAn Hospital, with 67 asymptomatic neurocognitive impairment patients (ANIs) with HIV and 70 HCs. Demographics of subjects are reported in Supplementary Materials.All rs-fMRI data were preprocessed using the Data Processing Assistant for Resting-State fMRI (DPARSF) pipeline [17]. Major steps include (1) magnetization equilibrium by trimming the first 10 volumes, (2) slice timing correction and head motion correction, (3) regression of nuisance covariates (e.g., white matter signals, ventricle, and head motion parameters), (4) spatial normalization to the MNI space, (5) bandpass filtering (0.01-0.10 Hz). The average rs-fMRI time series of 116 ROIs defined by the AAL atlas are extracted for each subject."
Modularity-Constrained Dynamic Representation Learning for Interpretable Brain Disorder Analysis with Functional MRI,2.2,Proposed Method,"As shown in Fig. 1, the proposed MDRL consists of (1) dynamic graph construction via sliding windows, (2) MSGNN for dynamic graph representation learning, and (3) prediction and biomarker detection, with details introduced below."
Modularity-Constrained Dynamic Representation Learning for Interpretable Brain Disorder Analysis with Functional MRI,,Dynamic Graph Construction.,"Considering that brain functional connectivity (FC) patterns change dynamically over time [18], we propose to first construct dynamic networks/graphs using a sliding window strategy for each subject. Denote original fMRI time series as S ∈ R N ×M , where N is the number of ROIs and M is the number of time points of blood-oxygen-level-dependent (BOLD) signals in rs-fMRI. We first divide the original time series into T segments along the temporal dimension via overlapped sliding windows, with the window size of Γ and the step size of τ . Then, we construct an FC network by calculating Pearson correlation (PC) coefficients between time series of pairwise ROIs for each of T segments, denoted asThe original feature for the j-th node is represented by the j-th row in X t for segment t. Considering all connections in an FC network may include some noisy or redundant information, we retain the top 30% strongest edges in each FC network to generate an adjacent matrix A t ∈ {0, 1} N ×N for segment t. Thus, the obtained dynamic graph sequence of each subject can be described as"
Modularity-Constrained Dynamic Representation Learning for Interpretable Brain Disorder Analysis with Functional MRI,,Modularity-Constrained Spatiotemporal GNN.,"With the dynamic graph sequence {G t } T t=1 as input, we design a modularity-constrained spatiotemporal graph neural network (MSGNN) to learn interpretable and discriminative graph embeddings, with two unique constraints: 1) a modularity constraint, and 2) a graph topology reconstruction constraint. In MSGNN, we first stack two graph isomorphism network (GIN) layers [19] for node-level feature learning. The nodelevel embedding H t at the segment t learned by GIN layers is formulated as:where ψ is nonlinear activation, ε (i) is a parameter at the i-th GIN layer, I is an identity matrix, and W (i) is the weight for the fully connected layers in GIN."
Modularity-Constrained Dynamic Representation Learning for Interpretable Brain Disorder Analysis with Functional MRI,,1) Modularity Constraint.,"It has been demonstrated that the central executive network (CEN), salience network (SN) and default mode network (DMN) are three crucial neurocognitive modules in the brain and these three modules have been consistently observed across different individuals and experimental paradigms, where CEN performs high-level cognitive tasks (e.g., decision-making and rule-based problem-solving), SN mainly detects external stimuli and coordinates brain neural resources, and DMN is responsible for self-related cognitive functions [10][11][12]. The ROIs/nodes within a module are densely inter-connected, resulting in a high degree of clustering between nodes from the same module.Based on such prior knowledge and clinical experience, we reasonably assume that the learned embeddings of nodes within the same neurocognitive module tend to be similar. We develop a novel modularity constraint to encourage similarity between paired node-level embeddings in the same module. Mathematically, the proposed modularity constraint is formulated as:where h t,k i and h t,k j are embeddings of two nodes in the k-th module (with N k ROIs) at segment t, and K is the number of modules (K = 3 in this work). With Eq. ( 2), we encourage the MSGNN to focus on modular brain structures during representation learning, thus improving discriminative ability of fMRI features."
Modularity-Constrained Dynamic Representation Learning for Interpretable Brain Disorder Analysis with Functional MRI,,2) Graph Topology Reconstruction Constraint.,"To further enhance discriminative ability of learned embeddings, we propose to preserve graph topology by reconstructing adjacent matrices. For the segment t, its adjacent matrix A t can be reconstructed through Ât = σ(H t • H t ), where σ is a nonlinear mapping function. The graph topology reconstruction constraint is then formulated as:where Ψ is a cross-entropy loss function. We then apply an SERO operation [20] to generate graph-level embeddings based on node-level embeddings, formulated as h t = H t Φ(P (2) σ(P (1) H t φ mean )), where Φ is a sigmoid function, P (1) and P (2)  are learnable weight matrices, and φ mean is average operation.3) Temporal Feature Learning . To further capture temporal information, a single-head transformer is used to fuse features derived from T segments, with a self-attention mechanism to model temporal dynamics across segments. We then sum the learned features {h i } T i=1 to obtain the whole-graph embedding.Prediction and Biomarker Detection. The whole-graph embedding is fed into a fully connected layer with Softmax for prediction, with final loss defined as:where L C is a cross-entropy loss for prediction, and λ 1 and λ 2 are two hyperparameters. To facilitate interpretation of our learned graph embeddings, we calculate PC coefficients between paired node embeddings for each segment and average them across segments to obtain an FC network for each subject. The upper triangle of each FC network is flattened into a vector and Lasso [21] (with default parameter) is used to select discriminative features. Finally, we map these features to the original feature space to detect disease-related ROIs and FCs.Implementation. The MDRL is implemented in PyTorch and trained using an Adam optimizer (with learning rate of 0.001, training epochs of 30, batch size of 8 and τ = 20). We set window size Γ = 40 for NYU and Γ = 70 for the rest, and results of MDRL with different Γ values are shown in Supplementary Materials.In the modularity constraint, we randomly select m = 50% of all N k (N k -1) 2 paired ROIs in the k-th module (with N k ROIs) to constrain the MDRL."
Modularity-Constrained Dynamic Representation Learning for Interpretable Brain Disorder Analysis with Functional MRI,3,Experiment,"Competing Methods. We compare the MDRL with 2 shallow methods: 1) linear SVM with node-level statistics (i.e., degree centrality, clustering coefficient, betweenness centrality, and eigenvector centrality) of FC networks as fMRI features (with each FC network constructed using PC), 2) XGBoost with the same features as SVM; and 4 state-of-the-art (SOTA) deep models with default architectures: 3) GCN [22], 4) GAT [23], 5) BrainGNN [9], and 6) STGCN [18]. The term '*' denotes the results of MDRL and a competing method are statistically significantly different (p < 0.05). The term '*' denotes the results of MDRL and a competing method are statistically significantly different (p < 0.05).Experimental Setting. Three classification tasks are performed: 1) ASD vs. HC on ABIDE, 2) MDD vs. HC on MDD, and 3) ANI vs. HC on HAND. A 5-fold cross-validation (CV) strategy is employed. Within each fold, we also perform an inner 5-fold CV to select optimal parameters. Five evaluation metrics are used: area under ROC curve (AUC), accuracy (ACC), sensitivity (SEN), specificity (SPE), and balanced accuracy (BAC). Paired sample t-test is performed to evaluate whether the MDRL is significantly different from a competing method.Classification Results. Results achieved by different methods in three classification tasks on three datasets are reported in Tables 1-2 and Fig. 2. It can be seen that our MDRL generally outperforms two shallow methods (i.e., SVM and XGBoost) that rely on handcrafted node features without modeling wholegraph topological information. Compared with 4 SOTA deep learning methods, our MDRL achieves superior performance in terms of most metrics in three tasks. For instance, for ASD vs. HC classification on NYU of ABIDE (see Table 1), the AUC value of MDRL is improved by 5.7% compared with BrainGNN (a SOTA method designed for brain network analysis). This implies the MDRL can learn discriminative graph representations to boost fMRI-based learning performance.Ablation Study. We compare the proposed MDRL with its three degenerated variants: 1) MDRLw/oM without the modularity constraint, 2) MDRLw/oR without the graph topology reconstruction constraint, and 3) MDRLw/oMR without the two constraints. The results are reported in Fig. 3, from which one can see that MDRL is superior to its three variants, verifying the effectiveness of the two constraints defined in Eqs. ( 2)-( 3). Besides, MDRLw/oM is generally inferior to MDRLw/oR in three tasks, implying that the modularity constraint may contribute more to MDRL than the graph reconstruction constraint.   Discriminative ROIs and Functional Connectivities. The top 10 discriminative FCs detected by the MDRL in three tasks are shown in Fig. 4. The thickness of each line represents discriminative power that is proportional to the corresponding Lasso coefficient. For ASD identification (see Fig. 4 (a)), the FCs involved in thalamus and middle temporal gyrus are frequently identified, which complies with previous findings [24,25]. For MDD detection (see Fig. 4 (b)), we find that several ROIs (e.g., hippocampus, supplementary motor area and thalamus) are highly associated with MDD identification, which coincides with previous studies [26][27][28]. For ANI identification (see Fig. 4 (c)), the detected ROIs such as amygdala, right temporal pole: superior temporal gyrus and parahippocampal gyrus, are also reported in previous research [29][30][31]. This further demonstrates the effectiveness of the MDRL in disease-associated biomarker detection.  "
Modularity-Constrained Dynamic Representation Learning for Interpretable Brain Disorder Analysis with Functional MRI,4,Discussion,"Parameter Analysis. To investigate the influence of hyperparameters, we vary the values of two parameters (i.e., λ 1 and λ 2 ) in Eq. ( 4) and report the results of MDRL in Fig. 5. It can be seen from Fig. 5 that, with λ 1 fixed, the performance of MDRL exhibits small fluctuations with the increase of parameter values of λ 2 , implying that MDRL is not very sensitive to λ 2 in three tasks. With λ 2 fixed, the MDRL with a large λ 1 (e.g., λ 1 = 10) achieves worse performance. The possible reason could be that using a strong graph reconstruct constraint will make the model difficult to converge, thus degrading its learning performance.Influence of Modularity Ratio. In the main experiments, we randomly select m = 50% of all N k (N k -1) 2 paired ROIs in the k-th module (with N k ROIs) to constrain the MDRL. We now vary the modularity ratio m within [0%, 25%, • • • , 100%] and record the results of MDRL in three tasks in Fig. 6. It can be seen from Fig. 6 that, when m < 75%, the ACC and AUC results generally increase as m increases. But when using a large modularity ratio (e.g., m = 100%), the MDRL cannot achieve satisfactory results. This may be due to the oversmoothing problem caused by using a too-strong modularity constraint.Influence of Network Construction. We use PC to construct the original FC networks in MDRL. We also use sparse representation (SR) and low-rank representation (LR) for network construction in MDRL and report results in Table 3. It can be seen from Table 3 that the MDRL with PC outperforms its two variants. The underlying reason could be that PC can model dependencies among regional BOLD signals without discarding any connection information. "
Modularity-Constrained Dynamic Representation Learning for Interpretable Brain Disorder Analysis with Functional MRI,5,Conclusion and Future Work,"In this work, we propose a modularity-constrained dynamic graph representation (MDRL) framework for fMRI-based brain disorder analysis. We first construct dynamic graphs for each subject and then design a modularity-constrained GNN to learn spatiotemporal representation, followed by prediction and biomarker detection. Experimental results on three rs-fMRI datasets validate the superiority of the MDRL in brain disease detection. Currently, we only characterize pairwise relationships of ROIs within 3 prominent neurocognitive modules (i.e., CEN, SN, and DMN) as prior knowledge to design the modularity constraint in MDRL. Fine-grained modular structure and disease-specific modularity constraint will be considered. Besides, we will employ advanced harmonization methods [32] to reduce inter-site variance, fully utilizing multi-site fMRI data for model training."
Modularity-Constrained Dynamic Representation Learning for Interpretable Brain Disorder Analysis with Functional MRI,,Fig. 1 .,
Modularity-Constrained Dynamic Representation Learning for Interpretable Brain Disorder Analysis with Functional MRI,,Fig. 2 .,
Modularity-Constrained Dynamic Representation Learning for Interpretable Brain Disorder Analysis with Functional MRI,,Fig. 3 .,
Modularity-Constrained Dynamic Representation Learning for Interpretable Brain Disorder Analysis with Functional MRI,,Fig. 4 .,
Modularity-Constrained Dynamic Representation Learning for Interpretable Brain Disorder Analysis with Functional MRI,,Fig. 5 .,
Modularity-Constrained Dynamic Representation Learning for Interpretable Brain Disorder Analysis with Functional MRI,,Fig. 6 .,
Modularity-Constrained Dynamic Representation Learning for Interpretable Brain Disorder Analysis with Functional MRI,,Table 1 .,
Modularity-Constrained Dynamic Representation Learning for Interpretable Brain Disorder Analysis with Functional MRI,,Table 2 .,
Modularity-Constrained Dynamic Representation Learning for Interpretable Brain Disorder Analysis with Functional MRI,,Table 3 .,MDRL 72.6(1.7) 65.6(2.1) 65.6(2.0) 60.9(2.6) 57.4(1.9) 56.9(1.4) 66.2(3.4) 63.1(1.3) 63.2(1.3)
Weakly-Supervised Positional Contrastive Learning: Application to Cirrhosis Classification,1,Introduction,"In the medical domain, obtaining a large amount of high-confidence labels, such as histopathological diagnoses, is arduous due to the cost and required technicality. It is however possible to obtain lower confidence assessments for a large amount of images, either by a clinical questioning, or directly by a radiological diagnosis. To take advantage of large volumes of unlabeled or weakly-labeled images, pre-training encoders with self-supervised methods showed promising results in deep learning for medical imaging [1,4,21,[27][28][29]. In particular, contrastive learning (CL) is a self-supervised method that learns a mapping of the input images to a representation space where similar (positive) samples are moved closer and different (negative) samples are pushed far apart. Weak discrete labels can be integrated into contrastive learning by, for instance, considering as positives only the samples having the same label, as in [13], or by directly weighting unsupervised contrastive and supervised cross entropy loss functions, as in [19]. In this work, we focus on the scenario where radiological meta-data (thus, low-confidence labels) are available for a large amount of images, whereas high-confidence labels, obtained by histological analysis, are scarce.Naive extensions of contrastive learning methods, such as [5,10,11], from 2D to 3D images may be difficult due to limited GPU memory and therefore small batch size. A usual solution consists in using patch-based methods [8,23]. However, these methods pose two difficulties: they reduce the spatial context (limited by the size of the patch), and they require similar spatial resolution across images. This is rarely the case for abdominal CT/MRI acquisitions, which are typically strongly anisotropic and with variable resolutions. Alternatively, depth position of each 2D slice, within its corresponding volume, can be integrated in the analysis. For instance, in [4], the authors proposed to integrate depth in the sampling strategy for the batch creation. Likewise, in [26], the authors proposed to define as similar only 2D slices that have a small depth difference, using a normalized depth coordinate d ∈ [0, 1]. These works implicitly assume a certain threshold on depth to define positive and negative samples, which may be difficult to define and may be different among applications and datasets. Differently, inspired by [2,8], here we propose to use a degree of ""positiveness"" between samples by defining a kernel function w on depth positions. This allows us to consider volumetric depth information during pre-training and to use large batch sizes. Furthermore, we also propose to simultaneously leverage weak discrete attributes during pre-training by using a novel and efficient contrastive learning composite kernel loss function, denoting our global method Weakly-Supervised Positional (WSP).We apply our method to the classification of histology-proven liver cirrhosis, with a large volume of (weakly) radiologically-annotated CT-scans and a small amount of histopathologically-confirmed cirrhosis diagnosis. We compare the proposed approach to existing self-supervised methods."
Weakly-Supervised Positional Contrastive Learning: Application to Cirrhosis Classification,2,Method,"Let x t be an input 2D image, usually called anchor, extracted from a 3D volume, y t a corresponding discrete weak variable and d t a related continuous variable. In this paper, y t refers to a weak radiological annotation and d t corresponds to the normalized depth position of the 2D image within its corresponding 3D volume: if V max corresponds to the maximal depth-coordinate of a volume V , we compute d t = pt Vmax with p t ∈ [0, V max ] being the original depth coordinate.Let x - j and x + i be two semantically different (negative) and similar (positive) images with respect to x t , respectively.The definition of similarity is crucial in CL and is the main difference between existing methods. For instance, in unsupervised CL, methods such as SimCLR [5,6] choose as positive samples random augmentations of the anchor x + i = t(x t ), where t ∼ T is a random transformation chosen among a user-selected family T . Negative images x - j are all other (transformed) images present in the batch. Once x - j and x + i are defined, the goal of CL is to compute a mapping function f θ : X → S d , where X is the set of images and S d the representation space, so that similar samples are mapped closer in the representation space than dissimilar samples. Mathematically, this can be defined as looking for a f θ that satisfies the condition:where, with sim a similarity function defined here as sim(a, b) = a T b τ with τ > 0. In the presence of discrete labels y, the definition of negative (x - j ) and positive (x + i ) samples may change. For instance, in SupCon [13], the authors define as positives all images with the same discrete label y. However, when working with continuous labels d, one cannot use the same strategy since all images are somehow positive and negative at the same time. A possible solution [26] would be to define a threshold γ on the distance between labels (e.g., d a , d b ) so that, if the distance is smaller than γ (i.e., ||d ad b || 2 < γ), the samples (e.g., x a and x b ) are considered as positives. However, this requires a user-defined hyperparameter γ, which could be hard to find in practice. A more efficient solution, as proposed in [8], is to define a degree of ""positiveness"" between samples using a normalized kernel function w σ (d,, where K σ is, for instance, a Gaussian kernel, with user defined hyper-parameter σ and 0 ≤ w σ ≤ 1. It is interesting to notice that, for discrete labels, one could also define a kernel as: w δ (y, y i ) = δ(yy i ), δ being the Dirac function, retrieving exactly SupCon [13].In this work, we propose to leverage both continuous d and discrete y labels, by combining (here by multiplying) the previously defined kernels, w σ and w δ , into a composite kernel loss function. In this way, samples will be considered as similar (positive) only if they have a composite degree of ""positiveness"" greater than zero, namely both kernels have a value greater (or different) than 0 (w σ > 0 and w δ = 0). An example of resulting representation space is shown in Fig. 1. This constraint can be defined by slightly modifying the condition introduced in Eq. 1, as:where the indices t, i, j traverse all N images in the batch since there are no ""hard"" positive or negative samples, as in SimCLR or SupCon, but all images are considered as positive and negative at the same time. As commonly done in CL [3], this condition can be transformed into an optimization problem using the max operator and its smooth approximation LogSumExp:arg min(3) By defining P (t) = {i : y i = y t } as the set of indices of images x i in the batch with the same discrete label y i as the anchor x t , we can rewrite our final loss function as:whereIn practice, it is rather easy to find a good value of σ, as the proposed kernel method is quite robust to its variation. A robustness study is available in the supplementary material. For the experiments, we fix σ = 0.1."
Weakly-Supervised Positional Contrastive Learning: Application to Cirrhosis Classification,3,Experiments,"We compare the proposed method with different contrastive and non-contrastive methods, that either use no meta-data (SimCLR [5], BYOL [10]), or leverage only discrete labels (SupCon [13]), or continuous labels (depth-Aware [8]). The proposed method is the only one that takes simultaneously into account both discrete and continuous labels. In all experiments, we work with 2D slices rather than 3D volumes due to the anisotropy of abdominal CT-scans in the depth direction and the limited spatial context or resolution obtained with 3D patchbased or downsampling methods, respectively, which strongly impacts the cirrhosis diagnosis that is notably based on the contours irregularity. Moreover, the large batch sizes necessary in contrastive learning can not be handled in 3D due to a limited GPU memory."
Weakly-Supervised Positional Contrastive Learning: Application to Cirrhosis Classification,3.1,Datasets,"Three histo . It corresponds to absent fibrosis (F0), mild fibrosis (F1), significant fibrosis (F2), severe fibrosis (F3) and cirrhosis (F4). This score is then binarized to indicate the absence or presence of advanced fibrosis [14]: F0/F1/F2 (N = 28) vs. F3/F4 (N = 78). D 2 histo . This is the public LIHC dataset from the Cancer Genome Atlas [9], which presents a histological score, the Ishak score, designated as y 2 histo , that differs from the METAVIR score present in D 1  histo . This score is also distributed through five labels: No Fibrosis, Portal Fibrosis, Fibrous Speta, Nodular Formation and Incomplete Cirrhosis and Established Cirrhosis. Similarly to the METAVIR score in D 1 histo , we also binarize the Ishak score, as proposed in [16,20], which results in two cohorts of 34 healthy and 15 pathological patients.In all datasets, we select the slices based on the liver segmentation of the patients. To gain in precision, we keep the top 70% most central slices with respect to liver segmentation maps obtained manually in D radio , and automatically for D 1  histo and D 2 histo using a U-Net architecture pretrained on D radio [18]. For the latter pretraining dataset, it presents an average slice spacing of 3.23 mm with a standard deviation of 1.29 mm. For the x and y axis, the dimension is 0.79 mm per voxel on average, with a standard deviation of 0.10 mm."
Weakly-Supervised Positional Contrastive Learning: Application to Cirrhosis Classification,3.2,Architecture and Optimization,"Backbones. We propose to work with two different backbones in this paper: TinyNet and ResNet-18 [12]. TinyNet is a small encoder with 1.1M parameters, inspired by [24], with five convolutional layers, a representation space (for downstream tasks) of size 256 and a latent space (after a projection head of two dense layers) of size 64. In comparison, ResNet-18 has 11.2M parameters, a representation space of dimension 512 and a latent space of dimension 128. More details and an illustration of TinyNet are available in the supplementary material, as well as a full illustration of the algorithm flow.Data Augmentation, Sampling and Optimization. CL methods [5,10,11] require strong data augmentations on input images, in order to strengthen the association between positive samples [22]. In our work, we leverage three types of augmentations: rotations, crops and flips. Data augmentations are computed on the GPU, using the Kornia library [17]. During inference, we remove the augmentation module to only keep the original input images.For sampling, inspired by [4], we propose a strategy well-adapted for contrastive learning in 2D medical imaging. We first sample N patients, where N is the batch size, in a balanced way with respect to the radiological/histological classes; namely, we roughly have the same number of subjects per class. Then, we randomly select only one slice per subject. In this way, we maximize the slice heterogeneity within each batch. We use the same sampling strategy also for classification baselines. For D 2  histo , which has fewer patients than the batch size, we use a balanced sampling strategy with respect to the radiological/histological classes with no obligation of one slice per patient in the batch. As we work with 2D slices rather than 3D volumes, we compute the average probability per patient of having the pathology. The evaluation results presented later are based on the patient-level aggregated prediction.Finally, we run our experiments on a Tesla V100 with 16GB of RAM and a 6 CPU cores, and we used the PyTorch-Lightning library to implement our models. All models share the same data augmentation module, with a batch size of B = 64 and a fixed number of epochs n epochs = 200. For all experiments, we fix a learning rate (LR) of α = 10 -4 and a weight decay of λ = 10 -4 . We add a cosine decay learning rate scheduler [15] to prevent over-fitting. For BYOL, we initialize the moving average decay at 0.996. Evaluation Protocol. We first pretrain the backbone networks on D radio using all previously listed contrastive and non-contrastive methods. Then, we train a regularized logistic regression on the frozen representations of the datasets D 1 histo and D 2 histo . We use a stratified 5-fold cross-validation. As a baseline, we train a classification algorithm from scratch (supervised) for each dataset, D 1  histo and D 2 histo , using both backbone encoders and the same 5-fold crossvalidation strategy. We also train a regularized logistic regression on representations obtained with a random initialization as a second baseline (random). Finally, we report the cross-validated results for each model on the aggregated dataset  "
Weakly-Supervised Positional Contrastive Learning: Application to Cirrhosis Classification,4,Results and Discussion,"We present in Table 1 the results of all our experiments. For each of them, we report whether the pretraining method integrates the weak label meta-data, the depth spatial encoding, or both, which is the core of our method. First, we can notice that our method outperforms all other pretraining methods in D 1 histo and D 1+2 histo , which are the two datasets with more patients. For the latter, the proposed method surpasses the second best pretraining method, depth-Aware, by 4%. For D 1  histo , it can be noticed that WSP (ours) provides the best AUC score whatever the backbone used. For the second dataset D 2 histo , our method is on par with BYOL and SupCon when using a small encoder and outperforms the other methods when using a larger backbone.To illustrate the impact of the proposed method, we report in Fig. 2 the projections of the ResNet-18 representation vectors of 10 randomly selected subjects of D 1 histo onto the first two modes of a PCA. It can be noticed that the representation space of our method is the only one where the diagnostic label (not available during pretraining) and the depth position are correctly integrated. Indeed, there is a clear separation between slices of different classes (healthy at the bottom and cirrhotic cases at the top) and at the same time it seems that the depth position has been encoded in the x-axis, from left to right. SupCon performs well on the training set of D radio (figure available in the supplementary material), as well as D 2 histo with TinyNet, but it poorly generalizes to D 1 histo and D 1+2 histo . The method depth-Aware manages to correctly encode the depth position but not the diagnostic class label.To assess the clinical performance of the pretraining methods, we also compute the balanced accuracy scores (bACC) of the trained classifiers, which is compared in Table 2 to the bACC achieved by radiologists who were asked to visually assess the presence or absence of cirrhosis for the N=106 cases of D 1 histo . The reported bACC values correspond to the best scores among those obtained with Tiny and ResNet encoders.Radiologists achieved a bACC of 82% with respect to the histological reference. The two bestperforming methods surpassed this score: depth-Aware and the proposed WSP approach, improving respectively the radiologists score by 2% and 3%, suggesting that including 3D information (depth) at the pretraining phase was beneficial."
Weakly-Supervised Positional Contrastive Learning: Application to Cirrhosis Classification,5,Conclusion,"In this work, we proposed a novel kernel-based contrastive learning method that leverages both continuous and discrete meta-data for pretraining. We tested it on a challenging clinical application, cirrhosis prediction, using three different datasets, including the LIHC public dataset. To the best of our knowledge, this is the first time that a pretraining strategy combining different kinds of meta-data has been proposed for such application. Our results were compared to other stateof-the-art CL methods well-adapted for cirrhosis prediction. The pretraining methods were also compared visually, using a 2D projection of the representation vectors onto the first two PCA modes. Results showed that our method has an organization in the representation space that is in line with the proposed theory, which may explain its higher performances in the experiments. As future work, it would be interesting to adapt our kernel method to non-contrastive methods, such as SimSIAM [7], BYOL [10] or Barlow Twins [25], that need smaller batch sizes and have shown greater performances in computer vision tasks. In terms of application, our method could be easily translated to other medical problems, such as pancreas cancer prediction using the presence of intrapancreatic fat, diabetes mellitus or obesity as discrete meta-labels."
Weakly-Supervised Positional Contrastive Learning: Application to Cirrhosis Classification,,Fig. 1 .,
Weakly-Supervised Positional Contrastive Learning: Application to Cirrhosis Classification,,Fig. 2 .,
Weakly-Supervised Positional Contrastive Learning: Application to Cirrhosis Classification,,,
Weakly-Supervised Positional Contrastive Learning: Application to Cirrhosis Classification,,Table 1 .,
Weakly-Supervised Positional Contrastive Learning: Application to Cirrhosis Classification,,Table 2 .,
Unsupervised 3D Out-of-Distribution Detection with Latent Diffusion Models,1,Introduction,"Methods for out-of-distribution (OOD) detection are a crucial component of any machine learning pipeline that is deployed in the real world. They are particularly necessary for pipelines that employ neural networks, which perform well on data drawn from the distribution they were trained on but can produce unexpected results when given OOD data. For medical applications, methods for OOD detection must be able to detect both far-OOD data, such as images of a different organ or modality to the in-distribution data, and near-OOD data, such as in-distribution data corrupted by imaging artefacts. It is also necessary that these methods can operate on high-resolution 3D data. In this work, we focus on methods trained in a fully unsupervised way; without any labels or access to OOD data at train time.Recently, Latent Transformer Models (LTMs) [9] have proven themselves to be effective for anomaly detection and synthesis in medical data [21,23,27]. These two-stage models first use a VQ-VAE [20] or VQ-GAN [9] to provide a compressed, discrete representation of the imaging data. An autoregressive Transformer [29] can then be trained on a flattened sequence of this representation. LTMs are particularly valuable in medical data, where the high input size makes training a Transformer on raw pixels infeasible. Recently, these models have been shown to be effective for 3D OOD detection by using the Transformer's likelihood of the compressed sequence to identify both far-and near-OOD samples [11]. These models can also provide spatial anomaly maps that highlight the regions of the image considered to be OOD, particularly valuable for highlighting localised artefacts in near-OOD data.However, LTMs have some disadvantages. Firstly, likelihood models have well documented weaknesses when used for OOD detection [3,13,19], caused by focusing on low-level image features [12,26]. It can help to measure likelihood in a more abstract representation space, such as that provided by a VQ-VAE [8], but how to train models that provide optimal representations for assessing likelihood is still an open research problem. For example, [11] showed in an ablation study that LTMs fail at OOD when lower levels of VQ-VAE compression are used. Secondly, the memory requirements of Transformers mean that even with high compression rates, the technique cannot scale to very high-resolution medical data, such as a whole-body CT with an image dimension 512 3 . Finally, the spatial anomalies maps produced by LTMs are low resolution, being in the space of the latent representation rather than that of the image itself.A promising avenue for OOD detection is denoising diffusion probabilistic model (DDPM)-based OOD detection [10]. This approach works by taking the input images and noising them multiple times to different noise levels. The model is used to denoise each of these noised images, which are compared to the input; the key idea is that the model will only successfully denoise in-distribution (ID) data. The method has shown promising results on 2D data [10] but cannot be trivially extended to 3D; as even extending DDPMs to work on high-resolution 2D data is an area of active research. We propose to scale it to 3D volumetric data through the use of Latent Diffusion Models (LDMs). These models, analogous to LTMs, use a first-stage VQ-GAN to compress the input. The DDPM then learns to denoise these compressed representations, which are then decoded and their similarity to the input image is measured directly in the original image space.The proposed LDM-based OOD detection offers the potential to address the three disadvantages of an LTM-based approach. Firstly, as the method is not likelihood based, it is not necessary that the VQ-GAN provides an ill-defined 'good representation'. Rather, the only requirement is that it reconstructs the inputs well, something easy to quantify using reconstruction quality metrics. Secondly, DDPMs have more favourable memory scaling behaviour than Transformers, allowing them to be trained on higher-dimensional representations. Finally, as the comparisons are performed at the native resolution, LDMs can produce high-resolution spatial anomaly maps. We evaluate both the LTM and the proposed LDM model on several far-and near-OOD detection tasks and show that LDMs overcome the three main failings of LTMs: that their performance is less reliant on the quality of the first stage model, that they can be trained on higher dimensional inputs, and that they produce higher resolution anomaly maps."
Unsupervised 3D Out-of-Distribution Detection with Latent Diffusion Models,2,Methods,We begin with a brief overview of LDMs and relevant notation before describing how they are used for OOD detection and to estimate spatial anomaly maps.
Unsupervised 3D Out-of-Distribution Detection with Latent Diffusion Models,2.1,Latent Diffusion Models,"LDMs are trained in two stages. A first stage model, here a VQ-GAN, is trained to compress the input image into a latent representation. A DDPM [14] is trained to learn to sample from the distribution of these latent representations through iterative denoising."
Unsupervised 3D Out-of-Distribution Detection with Latent Diffusion Models,,VQ-GAN:,"The VQ-GAN operates on a 3D input of size x ∈ R H×W ×D and consists of an encoder E that compresses to a latent space z ∈ R h×w×d×n , where n is the dimension of the latent embedding vector. This representation is quantised by looking up the nearest value of each representation in a codebook containing K elements and replacing the embedding vector of length d with the codebook index, k, producing z q ∈ R h×w×d . A decoder G operates on this quantised representation to produce a reconstruction, x ∈ R H×W ×D .In a VQ-VAE [20], E, G and the codebook are jointly learnt with a L 2 loss on the reconstructions and a codebook loss. The VG-GAN [9] aims to produce higher quality reconstructions by employing a discriminator D and training adversarially, and including a perceptual loss component [32] in addition to the L 2 reconstruction loss. Following [28], we also add a spectral loss component to the reconstruction losses [7].The encoder and decoder are convolutional networks of l levels. There is a simple relationship between the spatial dimension of the latent space, the input, and number of levels: h, w, d = H 2 l , W 2 l , D 2 l , so the latent space is 2 3l times smaller spatially than the input image, with a 4 × 2 3l reduction in memory size when accounting for the conversion from a float to integer representation. In practice, most works use l = 3 (512× spatial compression) or l = 4 (4096× spatial compression); it is challenging to train a VQ-GAN at higher compression rates."
Unsupervised 3D Out-of-Distribution Detection with Latent Diffusion Models,,DDPM:,"A DDPM is then trained on the latent embedding z (the de-quantised latent). During training, noise is added to z according to a timestep t and a fixed Gaussian noise schedule defined by β t to produce noised samples z t , such thatwhere we use z 0 to refer to the noise-free latent z, we have 0 ≤ t ≤ T , and α t := 1β t and ᾱt := t s=1 α s . We design β t to increase with t such that the latent z T is close to an isotropic Gaussian. We seek to train a network that can perform the reverse or denoising process, which can also be written as a Gaussian transition:In practice, following [14], we can train a network θ (z t , t) to directly predict the noise used in the forward noising process, . We can train with a simplified loss L simple (θ) = E t,z0, θ (z t ) 2 , and denoise according towhere n ∼ N (0, I).While in most applications an isotropic Gaussian is drawn and iteratively denoised to draw samples from the model, in this work, we take a latent input z 0 and noise to z t for a range of values of t < T and obtain their reconstructions, ẑ0,t = p θ (z 0 |z t )."
Unsupervised 3D Out-of-Distribution Detection with Latent Diffusion Models,2.2,OOD Detection with LDMs,"In [10], an input image x that has been noised to a range of t-values spanning the range 0 < t < T is then denoised to obtain x0,t , and we measure the similarity for each reconstruction, S(x 0,t , x). These multiple similarity measures are then combined to produce a single score per input, with a high similarity score suggesting the input is more in-distribution. Typically, reconstruction methods work by reconstruction through some information bottleneck -for an autoencoder, this might be the dimension of the latent space; for a denoising model, this is the amount of noise applied -with the principal that ID images will be successfully reconstructed through the bottleneck, yielding high similarity with the input, and OOD images will not. Prior works have shown the performance becomes dependent on the choice of the bottleneck -too small and even ID inputs are poorly reconstructed, too large and OOD inputs are well reconstructed [6,18,22,33]. Reconstructing from multiple t-values addresses this problem by considering reconstructions from multiple bottlenecks per image, outperforming prior reconstruction-based methods [10].In order to scale to 3D data, we reconstruct an input x in the latent space of the VQ-GAN, z = E (x). Reconstructions are performed using the PLMS sampler [17], which allows for high-quality reconstructions with significantly fewer reconstruction steps. The similarity is measured in the original image space by decoding the reconstructed latents, S (G (ẑ 0,t ) , x). As recommended by [10], we measure both the mean-squared error (MSE) and the perceptual similarity [32] for each reconstruction, yielding a total of 2N similarity measures for the N reconstructions performed. As the perceptual loss operates on 2D images, we measure it on all slices in the coronal, axial, and sagittal planes and average these values to produce a single value per 3D volume. Each similarity metric is converted into a z-score using mean and standard deviation parameters calculated on the validation set, and are then averaged to produce a single score."
Unsupervised 3D Out-of-Distribution Detection with Latent Diffusion Models,2.3,Spatial Anomaly Maps,"To highlight spatial anomalies, we aggregate a set of reconstruction error maps. We select reconstructions from t-values = [100, 200, 300, 400], calculate the pixelwise mean absolute error (MAE), z-score these MAE maps using the pixelwise mean and standard deviation from the validation set, and then average to produce a single spatial map per input image."
Unsupervised 3D Out-of-Distribution Detection with Latent Diffusion Models,3,Experiments,
Unsupervised 3D Out-of-Distribution Detection with Latent Diffusion Models,3.1,Data,"We use three datasets to test the ability of our method to flag OOD values in both the near-and far-OOD cases. The CROMIS dataset [30,31] consists of 683 head CT scans and was used as the train and validation set for all models, with a 614/69 split. The KCH dataset consists of 47 head CTs acquired independently from CROMIS, and was used as the in-distribution test set. To produce near-OOD data, a number of corruptions were applied to this dataset, designed to represent a number of acquisition/ data preprocessing errors. These were: addition of Gaussian noise to the images at three levels (σ = 0.01, 0.1, 0.2), setting the background to values different to the 0 used during training (0.3, 0.6, 1), inverting the image through either of the three imaging planes, removing a chunk of adjacent slices from either the top or centre of the volume, skullstripping (the models were trained on unstripped images), and setting all pixel values to either 1% or 10% of their true values (imitating an error in intensity scaling during preprocessing). Applying each corruption to each ID image yielded a total of 705 near-OOD images. The Decathlon dataset [1] comprises a range of 3D imaging volumes that are not head CTs and was used to represent far-OOD data. We selected 22 images from each of the ten classes. All CT head images were affinely registered to MNI space, resampled to 1 mm isotropic, and cropped to a 176 × 208 × 176 grid. For the images in the Decathlon dataset, all were resampled to be 1mm isotropic and either cropped or zero-padded depending on size to produce a 176 ×216 ×176 grid. All CT images had their intensities clamped between [-15,100] and then rescaled to lie in the range [0, 1]. All non-CT images were rescaled based on their minimum and maximum values to lie in the [0, 1] range."
Unsupervised 3D Out-of-Distribution Detection with Latent Diffusion Models,3.2,Implementation Details,"All models were implemented in PyTorch v1.13.1 using the MONAI framework v1.1.0 [2]. Code is available at https://github.com/marksgraham/ddpmood. LTM model code can be found at https://github.com/marksgraham/ transformer-ood.LDMs: VQ-GANS were trained with levels l = 2, 3, or 4 levels with 1 convolutional layer and 3 residual blocks per level, each with 128 channels. Training with l = 3/4 represents standard practice, training with l = 2 (64× spatial compression) was done to simulate a situation with higher-resolution input data. All VQ-GANs had an embedding dim of 64, and the 2, 3, 4 level models have a codebook size of 64, 256, 1024, respectively. Models were trained with a perceptual loss weight of 0.001, an adversarial weight loss of 0.01, and all other losses unweighted. Models were trained with a batch size of 64 for 500 epochs on an A100, using the Adam optimizer [16] with a learning rate of 3 × 10 -4 and early stopping if the validation loss did not decrease over 15 epochs. The LDM used a time-conditioned UNet architecture as in [25], with three levels with (128, 256, 256) channels, 1 residual block per level, and attention in the deepest level only. The noise schedule had T = 1000 steps with a scaled linear noise schedule with β 0 = 0.0015 and β T = 0.0195. Models were trained with a batch size of 112 on an A100 with the Adam optimizer, learning rate 2.5 × 10 -5 for 12,000 epochs, with early stopping. During reconstruction, the PLMS scheduler was used with 100 timesteps. Reconstructions were performed from 50 t values spaced evenly over the interval [0, 1000]."
Unsupervised 3D Out-of-Distribution Detection with Latent Diffusion Models,,LTM:,"The Latent Transformer Models were trained on the same VQ-GAN bases using the procedure described in [11], using a 22-layer Transformer with dimension 256 in the attention layers and 8 attention heads. The authors in [11] used the Performer architecture [4], which uses a linear approximation to the attention matrix to reduce memory costs and enable training on larger sequence lengths. Instead, we use the recently introduced memory efficient attention mechanism [24] to calculate exact attention with reduced memory costs. This enables us to train a full Transformer on a 3-level VQ-GAN embedding, with a sequence length of 22 × 27 × 22 = 13, 068. Neither the Performer nor the memory-efficient Transformer was able to train on the 2-level embedding, with a sequence length of 44 × 52 × 44 = 100, 672. Models were trained on an A100 with a batch size of 128 using Adam with a learning rate of 10 -4 ."
Unsupervised 3D Out-of-Distribution Detection with Latent Diffusion Models,4,Results and Discussion,"Results and associated statistical tests are shown in Table 1 as AUC scores, with tests for differences in AUC performed using Delong's method [5]. At 4-levels, the LDM and LTM both perform well, albeit with the proposed LDM performing better on certain OOD datasets. LTM performance degrades when trained on a 3-level model, but LDM performance remains high. The 3-level LTM result Table 1. AUC scores for identifying OOD data, with the CT-2 dataset used as the in-distribution test set. Results shown split according to the number of levels in the VQ-GAN. Tests for difference in AUC compare each LTM and LDM models with the same VQ-GAN base, bold values are differences significant with p < 0.001 and underlined values significant with p < 0.05. Results are shown as N/A for the 2-level LTM as it was not possible to train a Transformer on such a long sequence. is in agreement with the findings in [11]. This is likely caused by the previously discussed tendency for likelihood-based models, such as Transformers, to be sensitive to the quality of the underlying representation. For instance, [12] showed that likelihood-based models can fail unless forced to focus on high-level image features. We posit that at the high compression rates of a 4-level VQ-GAN the representation encodes higher-level features, but at 3-levels the representation can encode lower-level features, making it harder for likelihood-based models to perform well. By contrast, the LDM-based method only requires that the VG-GAN produces reasonable reconstructions. While memory constraints prevented training a 2-level LTM, the more modest requirements on the UNet-based LDM meant it was possible to train. This result has implications for the application of very high-resolution medical data: for instance, a whole-body CT with an image dimension 512 3 would have a latent dimension 32 3 even with 4-level compression, too large to train an LTM on but comfortably within the reach of a LDM. The 2-level LDM had reduced performance on two classes that have many pixels with an intensity close to 0 (Hippocampal MR, and Scaling 1%)."
Unsupervised 3D Out-of-Distribution Detection with Latent Diffusion Models,,Dataset,"Recent research shows that at higher resolutions, the effective SNR increases if the noise schedule is kept constant [15]. It seems this effect made it possible for the 2-level LDM to reconstruct these two OOD classes with low error for many values of t. In future work we will look into scaling the noise schedule with LDM input size. Anomaly maps are shown in Fig. 1 for near-OOD cases with a spatially localised anomaly. The LDM-based maps are high-resolution, as they are generated in image space, and localise the relevant anomalies. The LTM maps are lower resolution, as they are generated in latent space, but more significantly often fail to localise the relevant anomalies. This is most obvious in anomalies that cause missing signal, such as missing chunks, skulls, or image scaling, which are flagged as low-anomaly regions. This is caused by the tendency of likelihood-based models to view regions with low complexity, such as blank areas, as high-likelihood [26]. The anomaly is sometimes picked up but not well localised, notable in the 'chunk top' example at 4-levels. Here, the transition between brain tissue and the missing chunk is flagged as anomalous rather than the chunk itself.Memory and time requirements for all models are tabulated in Supplementary Table A. These confirm the LDM's reduced memory use compared to the LTM. All models run in < 30s, making them feasible in a clinical setting."
Unsupervised 3D Out-of-Distribution Detection with Latent Diffusion Models,5,Conclusion,"We have introduced Latent Diffusion Models for 3D out-of-distribution detection. Our method outperforms the recently proposed Latent Transformer Model when assessed on both near-and far-OOD data. Moreover, we show LDMs address three key weaknesses of LTMs: their performance is less sensitive to the quality of the latent representation they are trained on, they have more favourable memory scaling that allows them to be trained on higher resolution inputs, and they provide higher resolution and more accurate spatial anomaly maps. Overall, LDMs show tremendous potential as a general-purpose tool for OOD detection on high-resolution 3D medical imaging data."
Unsupervised 3D Out-of-Distribution Detection with Latent Diffusion Models,,Fig. 1 .,
Unsupervised 3D Out-of-Distribution Detection with Latent Diffusion Models,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0 43.
Unsupervised Discovery of 3D Hierarchical Structure with Generative Diffusion Features,1,Introduction,"Deep neural networks (DNNs) have been successfully applied to various supervised 3D biomedical image analysis tasks, such as classification [11], segmentation [7], and registration [35]. Acquiring volumetric annotations manually to supervise deep learning models is costly and labor intensive. For example, the supervised training of 3D DNNs for segmentation requires the manual labeling of every voxel of the structures of interest for the entire training set. Additionally, the diversity of existing biomedical 3D volumetric image types (e.g. MRI, CT, electron tomography) and different tasks associated with them precludes image annotations for all existing problems in practice. Furthermore, experts may focus on annotating objects they are already aware of, thereby restricting the possibility of new structural discoveries in large datasets using deep learning. We hypothesize that the nested hierarchical structure intrinsic to many 3D biomedical images [13] might be useful for unsupervised segmentation. As a step in this direction, our goal in this work is to develop a computational approach for unsupervised structure discovery.Recently, unsupervised part discovery in 2D natural images has gained significant attention [6,8,15]. These methods are based on the finding that intermediate activations of deep ImageNet-pre-trained classification models capture semantically meaningful conceptual regions [8]. These regions are robust to pose and viewpoint variations and help high-level image understanding by providing local object representations, leading to more explainable recognition [15]. However, a naive application of part discovery methods to 3D volumetric segmentation is not feasible, due to the lack of good feature extractors for 3D biomedical images [5] and ImageNet-pretrained networks operate only on 2D images.We hypothesize that deep generative models are good feature extractors for unsupervised structure discovery for the following reasons. First, these models do not require expert labels as they are trained in a self-supervised way. Second, the ability to generate high-quality images suggests that these models capture semantically meaningful information. Third, generative representation learning has been successfully applied to global and dense prediction tasks in 2D images [9] and has shown improvements in label efficiency and generalization [19].Besides creating stunning image generation results, diffusion-based generative models [12] are applied to other downstream tasks. Several works use pretrained diffusion models for 2D label-efficient semantic segmentation of natural images [1,4]. In 2D medical imaging, diffusion models are used for self-supervised vessel segmentation [18], anomaly detection [27,29,31,34], denoising [14], and improving supervised segmentation models [32,33]. In 3D medical imaging, diffusion models are used for CT and MR image synthesis [10,33]. Inspired by the success of unsupervised part discovery methods in 2D images and the effective abilities of diffusion models for many downstream tasks we hypothesize that feature representations of generative diffusion models discover intrinsic hierarchical structures in 3D biomedical images. Our work explores this hypothesis. Our Contributions Are:1) We pretrain 3D diffusion models, use them as feature extractors (Fig. 1), and design losses (Fig. 2) for unsupervised 3D structure discovery. 2) We show that features from different stages of ladder-like U-Net-based diffusion models capture different hierarchy levels in 3D biomedical volumes. 3) Our approach outperforms previous 3D unsupervised discovery methods on challenging synthetic datasets and on a real-world brain tumor segmentation (BraTS'19) dataset."
Unsupervised Discovery of 3D Hierarchical Structure with Generative Diffusion Features,2,Background on Diffusion Models,"Diffusion models [12] consist of two parts: a forward pass and a reverse pass. The forward pass is a T -step process of adding a small Gaussian noise, gradually destroying image information and transforming a clean image x 0 into pure Gaussian noise x T . Each step t ∈ 1, T is:where), can be written as:The reverse pass is a corresponding T -step denoising process using a neural network (usually, U-Net [28]) with parameters θ. For small noises, the reverse pass is also Gaussian:(Practically, instead of μ θ (x t , t) and Σ θ (x t , t), models are designed to predict either the noise t at timestep t, or a less noisier version of image x t-1 directly."
Unsupervised Discovery of 3D Hierarchical Structure with Generative Diffusion Features,3,Method,"We formulate the 3D structure discovery task in biomedical images as an unsupervised segmentation into K parts. Given a one-channel 3D imageWe use three losses for unsupervised training (see Fig. 2):For an arbitrary representation h(x 0 ) of an image x 0 with voxels u, the consistency of this representation C(h(x 0 )) across K predicted parts in the form of segmentation M is defined as:where N is the number of voxels. This is a form of volume-normalized K-means loss with z k describing the mean feature value of partition k.Feature Consistency. We pretrain generative 3D diffusion models and use them as feature extractors [4]. Noise is added to a clean image x 0 based on Eq. ( 2) and the noisy image x t ∈ R 1×H×W ×D is passed to the 3D diffusion model. Intermediate activations (either from different stages of ladder-like U-Nets or their concatenation, see Fig. 1) upsampled to the original image size serve as a p-dimensional feature extractor φ(x 0 ) ∈ R p×H×W ×D . The feature consistency loss encourages voxels corresponding to the same parts to have similar features:Visual Consistency. The extracted features are upsampled from low spatial resolutions and therefore do not accurately align with image boundaries. To alleviate this problem, we use a voxel visual consistency loss:where I(x 0 ) is the identity feature extractor, i.e. I(x 0 ) = x 0 .Photometric Invariance. As biomedical images often show acquisition differences (e.g., based on MR or CT scanner), they can be heterogeneous in their voxel intensities [25]. Therefore, robustness of models to voxel-level photometric perturbations might be helpful for unsupervised discovery. We use the Dice loss [22] to encourage invariance to such a photometric transformation T :We assume our images are min-max normalized (x 0 ∈ [0, 1]). We then use gamma-correction of the form T (x 0 ) = x γ 0 as a photometric transformation. We draw γ from the uniform distribution: "
Unsupervised Discovery of 3D Hierarchical Structure with Generative Diffusion Features,4,Experiments,
Unsupervised Discovery of 3D Hierarchical Structure with Generative Diffusion Features,4.1,Datasets,"To compare with state-of-the-art unsupervised 3D segmentation methods we follow [13] and evaluate our method on challenging biologically inspired 3D synthetic datasets and a real-world brain tumor segmentation (BraTS'19) dataset.The synthetic dataset of [13], consists of 120 volumes (80-20-20 split) of size 50 × 50 × 50. Inspired by cryo-electron tomography images, it contains a three-level structure, representing a biological cell, vesicles and mitochondria, as well as protein aggregates. The intensities and locations of the objects are randomized without destroying the hierarchy. The regular variant of the dataset contains cubical and spherical objects, while the irregular variant contains more complex shapes. Pink noise of magnitude m = 0.25 which is commonly seen in biological data [30] is applied to the volume. Figure 3 shows sample slices of both variants.The BraTS'19 dataset [2,3,21] is an established benchmark for 3D tumor segmentation of brain MRIs. Volumes are co-registered to the same template, interpolated to (1 mm) 3 resolution and brain-extracted. Following [13], images are cropped to volumes of size 200 × 200 × 155. As in [13], FLAIR images and corresponding whole tumor (WT) annotations are used for unsupervised segmentation evaluation with the same split of 259 high grade glioma training examples into 180 train, 39 validation, and 40 test samples. The official BraTS'19 validation and test sets are not used as their segmentation masks are not available."
Unsupervised Discovery of 3D Hierarchical Structure with Generative Diffusion Features,4.2,Implementation Details,"All diffusion models use the same architecture shown in Fig. 1. We pretrain them for 50k epochs with batch size 4, using an L1 loss between the denoised and the original images. We use the Adam optimizer, a cosine noise schedule, learning rate 10 -4 and T = 250 steps. The first layer has 64 channels and this number is doubled for the proceeding downsampling layers. Due to memory constraints for BraTS'19, we trained diffusion models at 128 × 128 × 128 resolution. However, the extracted features are upsampled to the original 200 × 200 × 155 resolution.Our segmentation networks (f in Fig. 2) use a 3D U-Net architecture [7,28]. We trained them for 100 epochs using the Adam optimizer, a learning rate of 3 * 10 -4 and the losses in Eq. ( 4). We selected the epoch that gave the best average probability of the segmentation mask for all inputs [26] as our final model. Noisy images at timestep t = 25 are used as input to the diffusion models. Due to the  memory limits, for BraTS'19, we used Stage 2 features, as they have the least number of channels. We set λ f = λ v = λ inv = 1 and γ ∼ U [0.9, 1.1] for all cases.For all experiments we used Pytorch and 4 NVIDIA A6000 GPUs (48 Gb)."
Unsupervised Discovery of 3D Hierarchical Structure with Generative Diffusion Features,4.3,Results,"We compare our method with state-of-the-art unsupervised 3D structure discovery approaches including clustering using 3D feature learning [23], a 3D convolutional autoencoder [24], and self-supervised hyperbolic representations [13].For the synthetic datasets, we used K = 2 (background and cell) for Level 1, K = 4 (background, cell, vesicle, mitochondria) for Level 2, and K = 8 (background, cell, vesicle, mitochondria, and 4 small protein aggregates) for Level 3 predictions. The evaluation metric is the average Dice score on the annotated test labels. As the label order may differ we use the Hungarian algorithm to match the predicted masks with the ground truth segmentations. Table 1 shows the results for the regular and irregular variants of the cryo-ET-inspired synthetic dataset. Our models outperform all previous unsupervised work at all hierarchy levels. For some levels, our models even outperform semi-supervised methods (C ¸içek et al. [7] used 2% of annotated data, Zhao et al. [36] used one annotated volume). We found that simple unsupervised denoising (BM4D [20]) followed by k-means clustering provides a good baseline, although vanilla kmeans clustering on voxel intensities does not perform well due to noise. Results in Fig. 3 demonstrate that our proposed unsupervised method indeed discovers  the hierarchical structure of different levels. We also show in Table 2 that features from early decoder stages of the U-Net-based diffusion models better discover larger objects in the hierarchy, features at intermediate stages better capture intermediate objects, and features at later stages better find smaller objects.For the Brain Tumor Segmentation (BraTS'19) dataset, we use the whole tumor (WT) segmentation mask for evaluation, which is detectable based on the FLAIR images alone. We train segmentation models with K = 3 parts (background, brain, tumor). The evaluation metric, as in the BraTS'19 challenge [21], is Dice score and the 95th percentile of the symmetric Hausdorff distance, which quantifies the surface distance of the predicted segmentation from the manual tumor segmentation in millimeters. Table 3 shows that our model outperforms all prior unsupervised methods for both evaluation metrics. As an approximate upper bound we show for reference the reported results of the 1st place solution [17] on BraTS'19 which is based on supervised training on the full train set and evaluated on the BraTS'19 test set. The qualitative results in Fig. 4 show that our model can detect tumors of different sizes. Our predictions look smoother and do not capture fine details of tumor segmentations.We perform ablation studies on the BraTS'19 dataset (Table 3: below the line). Measuring the impact of each loss, we see that the smallest performance drop is due to a deactivated invariance loss (λ inv = 0) while deactivating the visual consistency (λ v = 0) and feature consistency (λ f = 0) losses results in larger, but similar performance drops. However, to achieve best performance all three components are necessary. We also perform k-means clustering on intensities and features. We observe that using our deep network model dramatically improves performance, although our losses are similar to k-means clustering.This might be due to the fact that predictive modeling involves learning from a distribution of images and a model may therefore extract useful knowledge from a collection of images. To evaluate the significance of the diffusion features, we replaced our diffusion feature extractor with a 3D ResNet from Med3D [5] trained on 23 medical datasets. We use the ""layer1 2 conv2"" features as they showed the best performance. Although performance does not drop significantly when Med3D features are used with our losses, Med3D features do not produce good results when directly used for k-means clustering."
Unsupervised Discovery of 3D Hierarchical Structure with Generative Diffusion Features,5,Conclusion,"In this work, we showed that features from 3D generative diffusion models using a ladder-like U-Net-based architecture can discover intrinsic 3D structures in biomedical images. We trained predictive unsupervised segmentation models using losses that encourage the decomposition of biomedical volumes into nested subvolumes aligned with their hierarchical structures. Our method outperforms existing unsupervised segmentation approaches and discovers meaningful hierarchical concepts on challenging biologically-inspired synthetic datasets and on the BraTS brain tumor dataset. While we tested our approach for unsupervised image segmentation it is conceivable that it could also be useful in semisupervised settings and that could be applied to data types other than images."
Unsupervised Discovery of 3D Hierarchical Structure with Generative Diffusion Features,,Fig. 1 .,
Unsupervised Discovery of 3D Hierarchical Structure with Generative Diffusion Features,,Fig. 2 .,
Unsupervised Discovery of 3D Hierarchical Structure with Generative Diffusion Features,,Fig. 3 .,
Unsupervised Discovery of 3D Hierarchical Structure with Generative Diffusion Features,,Fig. 4 .,
Unsupervised Discovery of 3D Hierarchical Structure with Generative Diffusion Features,,Table 1 .,
Unsupervised Discovery of 3D Hierarchical Structure with Generative Diffusion Features,,Table 2 .,
Unsupervised Discovery of 3D Hierarchical Structure with Generative Diffusion Features,,Table 3 . BraTS'19 results with ablation studies.,
MDA-SR: Multi-level Domain Adaptation Super-Resolution for Wireless Capsule Endoscopy Images,1,Introduction,"Wireless capsule endoscopy (WCE) is an emerging examination technique that offers several advantages over traditional electronic endoscopy, including noninvasiveness, safety, and non-cross-infection. It enables the examination of the T. Liu and Z. Chen-contributed equally to this work. entire human gastrointestinal tract and is widely used in clinical practice [15,23]. Despite its successful clinical applications, the limited volume and data transmission bandwidth of WCE result in image quality drawbacks such as low resolution and poor quality [4]. Recent research has shown that high definition colonoscopy increases the identification of any polyps by 3.8% [19], and a 3-center prospective randomized trial has further proven the value of high resolution in invasive endoscopy [17]. Hence it is desirable to restore the image quality of WCE images via super-resolution techniques.Most super-resolution methods based on deep learning are supervised by paired low-resolution (LR) and high-resolution (HR) images [8,21,26]. However, the corresponding HR WCE image is currently unavailable due to the hardware limitations in capsule size and transmission bandwidth. Alternatively, researchers have adopted predefined simple linear degradation assumptions (e.g., bicubic downsampling, gaussian downsampling) to feasibly synthesize corresponding LR samples from ground-truth HR images. Similarly, Almalioglu et al. [1] adopted this assumption to synthesis paired LR electronic endoscopy images from the HR ones for supervised super-resolution learning. However, this method is difficult to generalize to real WCE images due to the domain gap between WCE images and electronic endoscopy images.What causes this domain gap? It might seem reasonable to adopt the simple linear degradation assumption by simply analogizing the domain gap between a mobile camera and a professional camera to a WCE and an electronic endoscope. However, what cannot be ignored is the different examination environment, where the WCE requires filling the stomach with water, while the electronic endoscopy inflates the stomach, which directly leads to the difference between the two image domains in terms of villi pose and speckle reflection, as shown in Fig. 1. This domain gap cannot be described by a simple linear degradation matrix.Recently, many studies have utilized the CycleGAN [28] to combine explicit domain adaptation into SR. The basic idea is to generate LR versions of HR V Fig. 2. Overview of the proposed MDA-SR, which consists of two parts: adaptive degradation and domian adaptation SR.images with degenerate distributions similar to the real LR images, and then train the SR model on the generated LR-HR paired dataset [20,25,27]. The challenge is that the large domain gap between WCE and electronic endoscopic images makes the generator sensitive to learning shallow differences in content or style and unable to effectively bridge the domain gap.In this work, we propose a Multi-level Domain Adaptation Super-Resolution (MDA-SR) for WCE images to bridge the domain gap between electronic endoscopy images and WCE images. MDA-SR leverages prior knowledge of HR electronic endoscopy images to guide the SR process of WCE images, as illustrated in Fig. 2. First, we train the adaptive degradation at the image level, employing a generative adversarial network to learn the complex and variable degradation distribution in WCE images, while incorporating an adaptive data loss [18] as the fidelity term of the image content. In contrast to previous methods that assume generated LR images are free from domain shift [3,9,25], we propose to minimize the domain gap during the SR process by aligning the latent feature distribution of electronic endoscopy images and WCE images. We further proposed EIQE for improving the reference-free image evaluation metric NIQE to be more suitable for endoscopy images. Through extensive experiments on real WCE images, we demonstrate the superiority of our method over other state-of-the-art SR methods, and its efficacy in reality."
MDA-SR: Multi-level Domain Adaptation Super-Resolution for Wireless Capsule Endoscopy Images,2,Methods,
MDA-SR: Multi-level Domain Adaptation Super-Resolution for Wireless Capsule Endoscopy Images,2.1,Overview of the Proposed Method,"Given a set of WCE images and HR electronic endoscopy images I LR cap , I HR ele , we aim to learn a SR function R (•) that maps an observed I LR cap to its HR version according to the distribution defined by I HR ele in testing. As shown in Fig. 2, the proposed scheme consists of two major parts: an adaptive degradation that generates the LR version of I HR ele , denoted as I LR gen , and a domain adaptation SR that aligns the latent features of the WCE and electronic endoscopy datasets during the SR process."
MDA-SR: Multi-level Domain Adaptation Super-Resolution for Wireless Capsule Endoscopy Images,2.2,Adaptive Degradation,"The purpose of the adaptive degradation is to obtain I LR gen with a degradation distribution similar to I LR cap . To achieve this, we employ the architecture of GAN [5], where the degradation generator G maps the generated I LR gen to I LR cap and the image-level discriminator F image learns to distinguish between generated samples I LR gen and realistic samples I LR cap with adversarial loss:G is optimized by maximizing the loss in Eq. ( 1) against an adversarial F image that tries to minimize the loss. A critical requirement is that the LR image generated by G should be consistent with the low-frequency information of the HR image. To enforce this constraint, we incorporate data loss as an additional supervision information.The process of degradation from HR images to LR images is unknown. We adopt an adaptive downsampling kernel k [18] to approximate the unknown degradation process, since a widely-used approach uses predefined downsampling assumptions, such as bicubic downsampling or s × s average pooling [3]. It has been observed in previous works [2,6,12] that an appropriate downsampling function consists of low-filtering and decimation, we linearize the degradation generator G to a corresponding 2D kernel k:where I HR ele,i denotes an i-th example to estimate the kernel, N is the total number of samples that have been used and ↓ s represents downsampling operation with scale factor s. Finally, the data fidelity term L data is defined as follows:Given the definitions of adversarial and data losses above, the training loss of our adaptive degradation is defined as:where λ is a hyperparameter."
MDA-SR: Multi-level Domain Adaptation Super-Resolution for Wireless Capsule Endoscopy Images,2.3,Domain Adaptation SR,"The SR function R (•) can then be supervised by the aligned image pair set I LR gen , I HR ele obtained from adaptive degradation. As shown in Fig. 2, we use the pixel-wise content loss on the SR results R I LR gen , which ensures the accuracy of HR image composition:To further improve the performance of the WCE SR, it is crucial to bridge the domain gap between WCE images and electronic endoscopy images, even though the adaptive degradation generator G already learns the degradation distribution of WCE images through domain adaptation at the image level. To achieve this, we improve the domain adaptation at the latent level during the SR process.A straightforward way is to adopt a GAN [5] structure to reduce the distribution shift. As shown in Fig. 2, the SR function R (•) consists of an encoder E and a decoder D. We use two encoders E with shared weights to generate the electronic endoscopy latent feature z ele as well as the WCE latent feature z cap . We introduce latent-level discriminator F latent to distinguish the domain for each latent feature, while the encoder E is trained to deceive F latent . The optimization of E and F latent is achieved via the adversarial way, we use LSGAN [11] here:As a result, the discriminator F latent is trained with its corresponding loss in Eq. (7). The SR function R (•) is trained with the following loss function:where μ is a hyperparameter."
MDA-SR: Multi-level Domain Adaptation Super-Resolution for Wireless Capsule Endoscopy Images,3,Experiments,
MDA-SR: Multi-level Domain Adaptation Super-Resolution for Wireless Capsule Endoscopy Images,3.1,Experiment Settings,"Datasets. Our proposed model is trained on WCE image dataset and electronic endoscopy image dataset, and tested on WCE images. The WCE image dataset contains 14090 images, and the electronic endoscopy image dataset contains 2033 images, including 1302 images from the public Kvasir dataset [16] and 731 images from the local hospital. We perform a strict quality selection and remove the problematic images such as blurry, low-resolution and poor quality images. Evaluation Metrics. In WCE SR problem, there is no corresponding groundtruth image used to calculate the evaluation metric. To address this issue, we design a no-reference Endoscopy Image Quality Evaluator (EIQE), which derives the quality-aware features from the Endoscopy Scene Statistic (ESS) model, inspired by the reference-free Natural Image Quality Evaluator (NIQE) [14].The quality of a given test image is then expressed as the distance between a multivariate gaussian (MVG) fit of the ESS features extracted from the test image and a MVG model of the quality-aware features extracted from the corpus of HR electronic endoscopy images. Additionally, we use the no-reference metric BRISQUE [13] for evaluation purposes.To better illustrate the subjective quality, we conduct a mean opinion score (MOS) test for comparison with other methods. We randomly select 100 different WCE images from the test set to subjectively evaluate the quality of the 2x and 4x WCE SR images. Four gastroenterology clinicians rate the visual perceptual qualities by assigning scores. Scores from 0 to 5 are used to indicate the qualities from low to high."
MDA-SR: Multi-level Domain Adaptation Super-Resolution for Wireless Capsule Endoscopy Images,3.2,Training Details,"Throughout the framework, the discriminators F image and F latent use the patchbased discriminator [7] with the instance normalization [22]. The generator G, encoder E and decoder D in the model follow the residual block structure from the EDSR [8]. The parameters in Eq. ( 4) and Eq. ( 8) are set to be λ = 1 and μ = 0.001, respectively. During training, we use the Adam optimizer and set the batch size and learning rate as 10 and 1 × 10 -4 , respectively. To streamline the model training and reduce its complexity, we have divided the training process into two stages. In the first stage, we focus on training the adaptive degradation, which stabilizes the quality of generated LR images after 50 epochs. Following this, we incorporate the domain adaptation SR into the training process by training another 50 epochs. The experiments are implemented with Pytorch platform on NVIDIA GeForce RTX 2080 Ti.  "
MDA-SR: Multi-level Domain Adaptation Super-Resolution for Wireless Capsule Endoscopy Images,3.3,Results and Discussions,"Comparison with Previous Methods. To validate the effectiveness of our proposed method, we compare it with existing state-of-the-art conventional SR methods without domain adaptation [1,8] and SR methods with domain adaptation [18,24,27]. Quantitative results are shown in Table 1, while visualization results are provided in Fig. 3. As EIQE is the most important metric in Table 1 and BRISQUE [13] is provided for reference since BRISQUE is based on natural scene statistic. As can be seen in Table 1, approaches that incorporated domain adaptation generally outperformed those that did not, thus highlighting the value of domain adaptation for the WCE SR problem. For the WCE image dataset, our MDA-SR method achieves the top EIQE performance.Note that in Fig. 3, the proposed method produces results containing clean and natural textures, while the result of ADM [18] are overly sharpened, producing unreal artifacts. The MOS results are shown in Fig. 4, indicating that our MDA-SR model produced the highest scores on average and with relatively smaller variance.  Ablation Study. We use the t-SNE [10] for the visual representation of the image latent features distribution during SR process. It can be observed from Fig. 5(a) that there is a significant domain gap between WCE images and electronic endoscopy images, and from Fig. "
MDA-SR: Multi-level Domain Adaptation Super-Resolution for Wireless Capsule Endoscopy Images,4,Conclusion,"In this paper, we propose a multi-level domain adaptation SR for real WCE images. Our method first utilizes adaptive degradation to simulate the degradation distribution of WCE and generate LR electronic endoscopy images. We then employ implicit domain adaptation at the latent level during the SR process to further bridge the domain gap between WCE images and electronic endoscopy images. Through extensive experiments on real WCE images, we demonstrate the superiority of our method over other state-of-the-art SR methods, and its efficacy in reality. Further evaluation for downstream tasks such as disease classification, region segmentation, or depth and pose estimation from the generated SR WCE images is warranted."
MDA-SR: Multi-level Domain Adaptation Super-Resolution for Wireless Capsule Endoscopy Images,,Fig. 1 .,
MDA-SR: Multi-level Domain Adaptation Super-Resolution for Wireless Capsule Endoscopy Images,,Fig. 3 .,
MDA-SR: Multi-level Domain Adaptation Super-Resolution for Wireless Capsule Endoscopy Images,,Fig. 4 .,
MDA-SR: Multi-level Domain Adaptation Super-Resolution for Wireless Capsule Endoscopy Images,,Fig. 5 .,
MDA-SR: Multi-level Domain Adaptation Super-Resolution for Wireless Capsule Endoscopy Images,,Table 1 .,
MDA-SR: Multi-level Domain Adaptation Super-Resolution for Wireless Capsule Endoscopy Images,,Table 2 .,
MDA-SR: Multi-level Domain Adaptation Super-Resolution for Wireless Capsule Endoscopy Images,,,
S 2 ME: Spatial-Spectral Mutual Teaching and Ensemble Learning for Scribble-Supervised Polyp Segmentation,1,Introduction,"Colorectal cancer is a leading cause of cancer-related deaths worldwide [1]. Early detection and efficient diagnosis of polyps, which are precursors to colorectal cancer, is crucial for effective treatment. Recently, deep learning has emerged as a powerful tool in medical image analysis, prompting extensive research into its potential for polyp segmentation. The effectiveness of deep learning models in medical applications is usually based on large, well-annotated datasets, which in turn necessitates a time-consuming and expertise-driven annotation process. This has prompted the emergence of approaches for annotation-efficient weakly-supervised learning in the medical domain with limited annotations like points [8], bounding boxes [12], and scribbles [15]. Compared with other sparse labeling methods, scribbles allow the annotator to annotate arbitrary shapes, making them more flexible than points or boxes [13]. Besides, scribbles provide a more robust supervision signal, which can be prone to noise and outliers [5]. Hence, this work investigates the feasibility of conducting polyp segmentation using scribble annotation as supervision. The effectiveness of medical applications during in-site deployment depends on their ability to generalize to unseen data and remain robust against data corruption. Improving these factors is crucial to enhance the accuracy and reliability of medical diagnoses in real-world scenarios [22,27,28]. Therefore, we comprehensively evaluate our approach on multiple datasets from various medical sites to showcase its viability and effectiveness across different contexts.Dual-branch learning has been widely adopted in annotation-efficient learning to encourage mutual consistency through co-teaching. While existing approaches are typically designed for learning in the spatial domain [21,25,29,30], a novel spatial-spectral dual-branch structure is introduced to efficiently leverage domain-specific complementary knowledge with synergistic mutual teaching. Furthermore, the outputs from the spatial-spectral branches are aggregated to produce mixed pseudo labels as supplementary supervision. Different from previous methods, which generally adopt the handcrafted fusion strategies [15], we design to aggregate the outputs from spatial-spectral dual branches with an entropy-guided adaptive mixing ratio for each pixel. Consequently, our incorporated tactic of pseudo-label fusion aptly assesses the pixel-level ambiguity emerging from both spatial and frequency domains based on their entropy maps, thereby allocating substantially assured categorical labels to individual pixels and facilitating effective pseudo label ensemble learning. Contributions. Overall, the contributions of this work are threefold: First, we devise a spatial-spectral dual-branch structure to leverage cross-space knowledge and foster collaborative mutual teaching. To our best knowledge, this is the first attempt to explore the complementary relations of the spatial-spectral dual branch in boosting weakly-supervised medical image analysis. Second, we introduce the pixel-level entropy-guided fusion strategy to generate mixed pseudo labels with reduced noise and increased confidence, thus enhancing ensemble learning. Lastly, our proposed hybrid loss optimization, comprising scribblessupervised loss, mutual training loss with domain-specific pseudo labels, and ensemble learning loss with fused-domain pseudo labels, facilitates obtaining a generalizable and robust model for polyp image segmentation. An extensive assessment of our approach through the examination of four publicly accessible datasets establishes its superiority and clinical significance."
S 2 ME: Spatial-Spectral Mutual Teaching and Ensemble Learning for Scribble-Supervised Polyp Segmentation,2,Methodology,
S 2 ME: Spatial-Spectral Mutual Teaching and Ensemble Learning for Scribble-Supervised Polyp Segmentation,2.1,Preliminaries,"Spectral-domain learning [26] has gained increasing popularity in medical image analysis [23] for its ability to identify subtle frequency patterns that may not be well detected by the pure spatial-domain network like UNet [20]. For instance, a recent dual-encoder network, YNet [6], incorporates a spectral encoder with Fast Fourier Convolution (FFC) [4] to disentangle global patterns across varying frequency components and derives hybrid feature representation. In addition, spectrum learning also exhibits advantageous robustness and generalization against adversarial attacks, data corruption, and distribution shifts [19]. In label-efficient learning, some preliminary works have been proposed to encourage mutual consistency between outputs from two networks [3], two decoders [25], and teacher-student models [14], yet only in the spatial domain. As far as we know, spatial-spectral cross-domain consistency has never been investigated to promote learning with sparse annotations of medical data. This has motivated us to develop the cross-domain cooperative mutual teaching scheme to leverage the favorable properties when learning in the spectral space.Besides consistency constraints, utilizing pseudo labels as supplementary supervision is another principle in label-efficient learning [11,24]. To prevent the model from being influenced by noise and inaccuracies within the pseudo labels, numerous studies have endeavored to enhance their quality, including averaging the model predictions from several iterations [11], filtering out unreliable pixels [24], and mixing dual-branch outputs [15] followingwhere α is the random mixing ratio. p 1 , p 2 , and p mix denote the probability maps from the two spatial decoders and their mixture. These approaches only operate in the spatial domain, regardless of single or dual branches, while we consider both spatial and spectral domains and propose to adaptively merge dual-branch outputs with respective pixel-wise entropy guidance."
S 2 ME: Spatial-Spectral Mutual Teaching and Ensemble Learning for Scribble-Supervised Polyp Segmentation,2.2,S 2 ME: Spatial-Spectral Mutual Teaching and Ensemble Learning,"Spatial-Spectral Cross-domain Mutual Teaching. In contrast to prior weakly-supervised learning methods that have merely emphasized spatial considerations, our approach designs a dual-branch structure consisting of a spatial branch f spa (x, θ spa ) and a spectral branch f spe (x, θ spe ), with x and θ being the input image and randomly initialized model parameters. As illustrated in Fig. 1, the spatial and spectral branches take the same training image as the input and extract domain-specific patterns. The raw model outputs, i.e., the logits l spa and l spe , will be converted to probability maps p spa and p spe with Softmax normalization, and further to respective pseudo labels ŷspa and ŷspe by ŷ = arg max p. The spatial and spectral pseudo labels supervise the other branch collaboratively during mutual teaching and can be expressed aswhere ""→"" denotes supervision1 . Through cross-domain engagement, these two branches complement each other, with each providing valuable domain-specific insights and feedback to the other. Consequently, such a scheme can lead to better feature extraction, more meaningful data representation, and domain-specific knowledge transmission, thus boosting model generalization and robustness.Entropy-Guided Pseudo Label Ensemble Learning. In addition to mutual teaching, we consider aggregating the pseudo labels from the spatial and spectral branches in ensemble learning, aiming to take advantage of the distinctive yet complementary properties of the cross-domain features. As we know, a pixel characterized by a higher entropy value indicates elevated uncertainty in terms of its corresponding prediction. We can observe from the entropy maps H spa and H spe in Fig. 1 that the pixels of the polyp boundary exhibit greater difficulties in accurate segmentation, presenting with higher entropy values (the white contours). Considering such property, we propose a novel adaptive strategy to automatically adjust the mixing ratio for each pixel based on the entropy of its categorical probability distribution. Hence, the mixed pseudo labels are more reliable and beneficial for ensemble learning. Concretely, with the spatial and spectral probability maps p spa and p spe , the corresponding entropy maps H spa and H spe can be computed withwhere C is the number of classes that equals 2 in our task. Unlike previous image-level fixed-ratio mixing or random mixing as Eq. ( 1), we can update the mixing ratio between the two probability maps p spa and p spe with the weighted entropy guidance at each pixel location bywhere ""⊗"" denotes pixel-wise multiplication. p s 2 is the merged probability map and can be further converted to the pseudo label by ŷs 2 = arg max p s 2 to supervise the spatial and spectral branch in the context of ensemble learning following ŷs 2 → f spa and ŷs 2 → f spe .(By absorbing strengths from the spatial and spectral branches, ensemble learning from the mixed pseudo labels facilitates model optimization with reduced overfitting, increased stability, and improved generalization and robustness.Hybrid Loss Supervision from Scribbles and Pseudo Labels. Besides the scribble annotations for partial pixels, the aforementioned three types of pseudo labels ŷspa , ŷspe , and ŷs 2 can offer complementary supervision for every pixel, with different learning regimes. Overall, our hybrid loss supervision is based on Cross Entropy loss CE and Dice loss Dice . Specifically, we employ the partial Cross Entropy loss [13] pCE , which only calculates the loss on the labeled pixels, for learning from scribbles followingwhere y denotes the scribble annotations. Furthermore, the mutual teaching loss with supervision from domain-specific pseudo labels is . (8) Holistically, our hybrid loss supervision can be stated aswhere λ mt and λ el serve as weighting coefficients that regulate the relative significance of various modes of supervision. The hybrid loss considers all possible supervision signals in the spatial-spectral dual-branch network and exceeds partial combinations of its constituent elements, as evidenced in the ablation study."
S 2 ME: Spatial-Spectral Mutual Teaching and Ensemble Learning for Scribble-Supervised Polyp Segmentation,3,Experiments,
S 2 ME: Spatial-Spectral Mutual Teaching and Ensemble Learning for Scribble-Supervised Polyp Segmentation,3.1,Experimental Setup,"Datasets. We employ the SUN-SEG [10] dataset with scribble annotations for training and assessing the in-distribution performance. This dataset is based on the SUN database [16], which contains 100 different polyp video cases. To reduce data redundancy and memory consumption, we choose the first of every five consecutive frames in each case. We then randomly split the data into 70, 10, and 20 cases for training, validation, and testing, leaving 6677, 1240, and 1993 frames in the respective split. For out-of-distribution evaluation, we utilize three public datasets, namely Kvasir-SEG [9], CVC-ClinicDB [2], and PolypGen [1] with 1000, 612, and 1537 polyp frames, respectively. These datasets are collected from diversified patients in multiple medical centers with various data acquisition systems. Varying data shifts and corruption like motion blur and specular reflections2 pose significant challenges to model generalization and robustness.Implementation Details. We implement our method with PyTorch [18] and run the experiments on a single NVIDIA RTX3090 GPU. The SGD optimizer is utilized for training 30k iterations with a momentum of 0.9, a weight decay of 0.0001, and a batch size of 16. The execution time for each experiment is approximately 4 h. The initial learning rate is 0.03 and updated with the polyscheduling policy [15]. The loss weighting coefficients λ mt and λ el are empirically set the same and exponentially ramped up [3] from 0 to 5 in 25k iterations. All the images are randomly cropped at the border with maximally 7 pixels and resized to 224×224 in width and height. Besides, random horizontal and vertical flipping are applied with a probability of 0.5, respectively. We utilize UNet [20] and YNet [6] as the respective segmentation model in the spatial and spectral branches. The performance of the scribble-supervised model with partial Cross Entropy [13] loss (Scrib-pCE) and the fully-supervised model with Cross Entropy loss (Fully-CE) are treated as the lower and upper bound, respectively. Five classical and relevant methods, including EntMin [7], GCRF [17], USTM [14], CPS [3], and DMPLS [15] are employed as the comparative baselines and implemented with UNet [20] as the segmentation backbone referring to the WSL4MIS3 repository. For a fair comparison, the output from the spatial branch is taken as the final prediction and utilized in evaluation without post-processing. In addition, statistical evaluations are conducted with multiple seeds, and the mean and standard deviations of the results are reported.  [17] 0.656±0.019 0.541±0.022 0.690±0.017 4.983±0.089 USTM [14] 0.654±0.008 0.533±0.009 0.663±0.011 5.207±0.138 CPS [3] 0.658±0.004 0.539±0.005 0.676±0.005 5.092±0.063 DMPLS [15] 0.656±0.006 0.539±0.005 0.659±0.011 5.208±0.061 S 2 ME (Ours) 0.674±0.003 0.565±0.001 0.719±0.003 4.583±0.014"
S 2 ME: Spatial-Spectral Mutual Teaching and Ensemble Learning for Scribble-Supervised Polyp Segmentation,3.2,Results and Analysis,"Fully-CE 0.713±0.021 0.617±0.023 0.746±0.027 4.405±0.119The performance of weakly-supervised methods is assessed with four metrics,i.e., Dice Similarity Coefficient (DSC), Intersection over Union (IoU), Precision (Prec), and a distance-based measure of Hausdorff Distance (HD). As shown in Table 1 and Fig. 2, our S 2 ME achieves superior in-distribution performance quantitatively and qualitatively compared with other baselines on the SUN-SEG [10] dataset. Regarding generalization and robustness, as indicated in Table 2, our method outperforms other weakly-supervised methods by a significant margin on three unseen datasets, and even exceeds the fully-supervised upper bound on two of them 4 . These results suggest the efficacy and reliability of the proposed solution S 2 ME in fulfilling polyp segmentation tasks with only scribble annotations. Notably, the encouraging performance on unseen datasets exhibits promising clinical implications in deploying our method to real-world scenarios.  "
S 2 ME: Spatial-Spectral Mutual Teaching and Ensemble Learning for Scribble-Supervised Polyp Segmentation,3.3,Ablation Studies,"Network Structures. We first conduct the ablation analysis on the network components. As shown in Table 3, the spatial-spectral configuration of our S 2 ME yields superior performance compared to single-domain counterparts with ME, confirming the significance of utilizing cross-domain features. Pseudo Label Fusion Strategies. To ensure the reliability of the mixed pseudo labels for ensemble learning, we present the pixel-level adaptive fusion strategy according to entropy maps of dual predictions to balance the strengths and weaknesses of spatial and spectral branches. As demonstrated in Table 4, our method achieves improved performance compared to two image-level fusion strategies, i.e., random [15] and equal mixing.Hybrid Loss Supervision. We decompose the proposed hybrid loss L hybrid in Eq. ( 9) to demonstrate the effectiveness of holistic supervision from scribbles, "
S 2 ME: Spatial-Spectral Mutual Teaching and Ensemble Learning for Scribble-Supervised Polyp Segmentation,4,Conclusion,"To our best knowledge, we propose the first spatial-spectral dual-branch network structure for weakly-supervised medical image segmentation that efficiently leverages cross-domain patterns with collaborative mutual teaching and ensemble learning. Our pixel-level entropy-guided fusion strategy advances the reliability of the aggregated pseudo labels, which provides valuable supplementary supervision signals. Moreover, we optimize the segmentation model with the hybrid mode of loss supervision from scribbles and pseudo labels in a holistic manner and witness improved outcomes. With extensive in-domain and out-ofdomain evaluation on four public datasets, our method shows superior accuracy, generalization, and robustness, indicating its clinical significance in alleviating data-related issues such as data shift and corruption which are commonly encountered in the medical field. Future efforts can be paid to apply our approach to other annotation-efficient learning contexts like semi-supervised learning, other sparse annotations like points, and more medical applications."
S 2 ME: Spatial-Spectral Mutual Teaching and Ensemble Learning for Scribble-Supervised Polyp Segmentation,,Fig. 1 .,
S 2 ME: Spatial-Spectral Mutual Teaching and Ensemble Learning for Scribble-Supervised Polyp Segmentation,,Fig. 2 .,
S 2 ME: Spatial-Spectral Mutual Teaching and Ensemble Learning for Scribble-Supervised Polyp Segmentation,,,"Lmt = { CE (lspa, ŷspe)+ Dice(pspa, ŷspe)} ŷspe→fspa + { CE (lspe, ŷspa)+ Dice(pspe, ŷspa)} ŷspa→fspe .(7) "
S 2 ME: Spatial-Spectral Mutual Teaching and Ensemble Learning for Scribble-Supervised Polyp Segmentation,,Table 1 .,
S 2 ME: Spatial-Spectral Mutual Teaching and Ensemble Learning for Scribble-Supervised Polyp Segmentation,,Table 2 .,
S 2 ME: Spatial-Spectral Mutual Teaching and Ensemble Learning for Scribble-Supervised Polyp Segmentation,,(Ours) 0.750±0.003 5.449±0.150 0.632±0.010 5.633±0.008 0.571±0.002 5.247±0.107,
S 2 ME: Spatial-Spectral Mutual Teaching and Ensemble Learning for Scribble-Supervised Polyp Segmentation,,Table 3 .,
S 2 ME: Spatial-Spectral Mutual Teaching and Ensemble Learning for Scribble-Supervised Polyp Segmentation,,Table 4 .,
S 2 ME: Spatial-Spectral Mutual Teaching and Ensemble Learning for Scribble-Supervised Polyp Segmentation,,Table 5 .,
S 2 ME: Spatial-Spectral Mutual Teaching and Ensemble Learning for Scribble-Supervised Polyp Segmentation,,Acknowledgements,". This work was supported by Hong Kong Research Grants Council (RGC) Collaborative Research Fund (CRF C4063-18G), the Shun Hing Institute of Advanced Engineering (SHIAE project BME-p1-21) at the Chinese University of Hong Kong (CUHK), General Research Fund (GRF 14203323), Shenzhen-Hong Kong-Macau Technology Research Programme (Type C) STIC Grant SGDX20210823103535014 (202108233000303), and (GRS) #3110167."
Graph Convolutional Network with Morphometric Similarity Networks for Schizophrenia Classification,1,Introduction,"Schizophrenia is a severe and chronic psychiatric disorder characterized by psychotic episodes, cognitive impairment, and impaired functioning with high rates of disability [4]. Early diagnosis of schizophrenia is beneficial to patients as it leads to better prognosis and reduces symptoms more effectively [15]. However, early detection of schizophrenia is still challenging since the complex and heterogeneous symptoms are associated with schizophrenia, complicating the optimal treatment of patients and limiting positive outcomes. The current diagnosis of schizophrenia is mainly based on patient's clinical symptoms and clinical interviews. Therefore, there is an urgent need to establish an objective approach for an accurate diagnosis of schizophrenia.There is an increasing interest in developing accurate and robust techniques to classify subjects into groups using neuroimaging data. Previous studies focused on handcrafted feature-based machine learning approach, which requires the process of feature extraction and selection prior to disease classification [28,30]. Pre-selected features extracted from neuroimaging data might not be sufficient and generalizable across different disease datasets, thus yielding a substantial variation in classification performance.With the rapid development of deep learning methods, recent studies have shown great potential for integrating neuroimaging data and deep learning models for an automatic diagnosis of brain diseases [12,23]. In particular, graph convolutional network (GCN) has become a promising approach for medical image classification such as schizophrenia [16], Alzheimer's disease [10], and autism spectrum disorder [27]. GCN is capable of preserving graph topological properties while automatically learning features to perform various graph-related tasks such as graph classification and graph representation learning [2]. This is especially important for psychiatric disorders like schizophrenia which has complex and subtle differences in neuroimaging features compared to healthy individuals. Previous studies mainly focused on functional MRI data as the input of GCN for disease classification [18,19]. Parisot et al. proposed a population GCN model combining subject-specific imaging and non-imaging information for autism spectrum disorder and Alzheimer's disease classification [18]. Qin et al. examined the effectiveness of GCN in distinguishing patients with major depressive disorder from healthy controls [19]. However, most studies adopted previously captured information based on functional brain networks which describe statistical dependence between functional MRI time series instead of modeling cortical networks based on similarity in regional cortical morphology estimated from structural MRI of the brain.Recent studies have highlighted the potential of cortical networks, so-called morphometric similarity networks (MSNs), to predict individual differences in brain morphometry [13,14], thereby allowing for their potential utility to capture individual cognitive variation [24]. Using the MSNs, it is also possible to unravel changes in the brain during normal cortical development [26] and psychiatric disorders [17] and to identify patterns of abnormal morphometric similarity that classify autism spectrum disorder [31] and Alzheimer's disease and mild cognitive impairment from healthy individuals [32]. However, much less is known about whether the MSNs are clinically useful phenotype and the ability of MSNs for schizophrenia classification.To fill these gaps, we developed a generalizable graph convolutional framework for population-based schizophrenia classification using the MSNs inferred from structural MRI data. Our contributions are summarized as follows: 1) We propose a new population graph model for integrating MSN-driven features derived from structural MRI and phenotypic information; 2) A novel feature selection strategy is introduced to leverage graph-theoretical measures of the MSNs, which is new and generalizable for graph neural networks; 3) We validate the feasibility of our proposed method by conducting a comprehensive evaluation on a large schizophrenia dataset, which shows superior performance in classification over traditional machine learning approaches; 4) A complete sensitivity analysis for key parameters in our GCN-based classification framework is performed; 5) The most salient regions contributing to classification are primarily identified in the middle temporal gyrus and superior temporal gyrus."
Graph Convolutional Network with Morphometric Similarity Networks for Schizophrenia Classification,2,Materials and Methods,"Figure 1 shows the overview of our proposed classification framework. We construct individual MSNs based on inter-regional similarity of multiple morphometric features extracted from T1-weighted MRI. We formulate a population graph model where each node is represented by graph-theoretical measures of the MSNs and each edge models the similarity between the topological features of the subjects and incorporates the phenotypic information. We apply a threshold to the population graph to remove spurious connections. The edge weights are adaptively updated by using an MLP-based encoder based on non-imaging data of the subjects. Spectral graph convolutions are applied for learning, followed by the MLP-based classification for schizophrenia. "
Graph Convolutional Network with Morphometric Similarity Networks for Schizophrenia Classification,2.1,Datasets,"In this study, structural T1-weighted magnetic resonance imaging (MRI) scans were collected from six public databases, including DecNef [25], COBRE [1], CANDI [11], MCICShare [7], UCLA CNP [3], and CCNMD [20]. A total of 956 subjects comprising 366 patients with schizophrenia and 590 healthy controls were included after quality control (for details, see Supplementary Materials). Ethical approvals and informed consents were obtained locally for each study, covering both participation and subsequent data sharing. Details about sample and demographic information are presented in Supplementary Table S1."
Graph Convolutional Network with Morphometric Similarity Networks for Schizophrenia Classification,2.2,Morphometric Similarity Networks,"The structural MRI scans from all subjects were preprocessed using the recon-all command from FreeSurfer (version 7.2.0) [6]. Cortical parcellation was based on an atlas with 308 cortical regions derived from the Desikan-Killiany atlas [21]. In each subject, this procedure generated 1540 morphometric features (308 regional measures of gray matter volume, surface area, cortical thickness, Gaussian curvature, and mean curvature). Morphometric features for each subject can be expressed as a set of 308 vectors of length 5. Each morphometric feature was standardized across 308 regions so that the data have a mean of zero and a standard deviation of one. A N × N correlation matrix, representing an individual MSN, was constructed by computing the Pearson's correlation coefficient between the z-normalized morphometric features of region i and region j for all (i, j) of regional properties. This procedure resulted in 308 × 308 MSN for each subject."
Graph Convolutional Network with Morphometric Similarity Networks for Schizophrenia Classification,2.3,Graph Construction,"We consider a population comprising N subjects; each subject being associated with a set of imaging and phenotypic information. We define the population graph as an undirected weighted graph G = (V , E, W ), where V is a set of |V | = N nodes, E is a set of edges. W ∈ R N ×N is a weighted adjacency matrix of population graph G. Each node v represents a subject, and the feature vectors for nodes are extracted from imaging data. Each edge is defined as the similarity between nodes. The two main decisions required to build the population model are the definition of the feature vector representing each node and the connectivity of the graph corresponding to its edges and weights.Feature Vector. The MSN has a high dimensionality with 47,278 features considering only the upper triangle of the MSN. Using the Brain Connectivity Toolbox (BCT) [22], we computed the following graph-theoretical measures: strength, betweenness centrality, and clustering coefficient. These measures were considered as node features for subjects. The strength is the sum of weights of edges connected to the given node. The betweenness centrality is defined as the fraction of all shortest paths in the network that contain a given node. Nodes with high values of betweenness centrality participate in many shortest paths. The clustering coefficient is a measure of network segregation and is defined as the fraction of triangles around the given node.We investigated two additional feature selection approaches. Firstly, we applied the ridge classifier to perform recursive feature elimination (RFE). RFE iteratively removes the irrelevant features to achieve the desired number of features while maximizing the ridge classification accuracy. Secondly, we used the k-best selection with ANOVA, a univariate feature selection algorithm that selects the k-best features with the highest F-values.Graph Edge. The process of computing graph edges involved two steps: the calculation of initial edge weights and adaptive learning through encoders. The initial similarity between nodes were determined using imaging features and categorical information. Considering a set H categorical phenotypic measures M = {M h }, such as sex and scan site, the initial edge weight W I between node v and w are defined as follows:(1)where Sim(v, w) is a measure of similarity between node v and node w in the imaging features. γ is a measure of similarity between node v and node w in the categorical variables. ρ is the Pearson's correlation distance. x(v) and x(w) are the topological feature vectors of node v and w, respectively. h denotes the width of the kernel. We set a threshold on W I (v, w) via quantile Q, , and adaptively calculated the edge weights only for the remaining edges. We investigated four different encoders (PAE, EA, L2, and Cosine + Tanh) that were used to determine the edge weights between node v and node w based on phenotypic information (e.g., sex, age, and scan site). The subject's normalized phenotypic inputs were projected into a latent space ϕ ∈ R 64 using MLP. The pairwise association encoder (PAE) [9] scores the association between node v and w as follows: W (v, w) = 0.5 * (S C (ϕ v , ϕ w ) + 1), where S C (ϕ v , ϕ w ) is the cosine similarity between latent vector ϕ v and ϕ w . The Edge Adapter (EA) [8] scores the association between node v and w as follows: W (v, w) = σ ( 64 α|ϕ v -ϕ w |), where |ϕ v -ϕ w | is the absolute difference vector of two latent vectors. α is the 64-dimension trainable parameter used in fully connected layer and σ is the sigmoid function. We also designed the L2 encoder as follows:2 ), where ||ϕ v -ϕ w || 2 2 denotes L2 distance of two latent vectors. Finally, we designed the Cosine + Tanh encoder combining Tanh with cosine similarity as follows:The encoders were optimized with graph convolution models using gradient descent algorithm."
Graph Convolutional Network with Morphometric Similarity Networks for Schizophrenia Classification,2.4,Spectral Graph Convolutions,"To extract spatial features from each graph node, we used the spectral graph convolution in the Fourier domain. A spectral convolution of signal x with a filter g θ = diag(θ ) defined in the Fourier domain is defined as a multiplication in the Fourier domain:, where U is the matrix of the eigenvectors of the normalized Laplacian matrix. The normalized graph Laplacian L is defined aswhere I and D are the identity matrix and the diagonal degree matrix, respectively. To reduce computational complexity of convolution, we used Chebyshev spectral convolutional network (ChebConv) [5] that approximates spectral graph convolution using Chebyshev polynomials. The Chebyshev polynomials are computed recursively aswhere L is the rescaled Laplacian and θ k are filter parameters. The output spatial features from l th convolutional layer with input features H l ∈ R N ×C l can be calculated as:are the trainable weights for the polynomial of order k in the l th layer.The model consists of a GCN with L hidden layers. Each hidden layer is followed by a ReLU activation function to introduce non-linearity. The output layer is full connected and comprises two convolutional layers with 256 and 2 channels. The model was trained using the entire population graph. During training, the training nodes were labeled and the test nodes were masked. Cross-entropy loss computed on the training nodes was used to train the GCN and edge encoder. The performance of the model was evaluated using the test nodes."
Graph Convolutional Network with Morphometric Similarity Networks for Schizophrenia Classification,2.5,Interpretability,"We identified relevant node features that contribute to the classification using GNNExplainer [29]. Given a trained GCN model and its prediction on a test node, explainer maximizes the mutual information between a GCN's prediction and the distribution of subgraph structure. GNNExplainer identified a compact subgraph together with a small subset of node features that have a decisive role in the GCN's prediction. The node feature importance was measured by computing feature masks. Feature importance was determined as an average value across subjects to represent the contribution of each brain region. Finally, we reported the top 10 node features with the largest average value of feature importance."
Graph Convolutional Network with Morphometric Similarity Networks for Schizophrenia Classification,3,Experiments and Results,"Experimental Settings. We used pooled stratified cross-validation to split the samples into training and testing sets for the evaluation of the GCN. The pooled samples were randomly divided into 5-folds, of which 1-fold served as the testing set, and the remaining 4-folds were used as the training set. This strategy ensures that training and testing sets contain the equivalent proportions of each class. The model performance was evaluated on the testing set in terms of balanced accuracy (BAC), sensitivity (SEN), specificity (SPE), F1-score, and area under the receiver operating characteristic curve (AUC).In our experiments, we set Chebyshev polynomial order K = 3, quantile Q = 0.5, and hidden layer L = 3. The number of channels was set to [128,64,32]. All models were trained using Adam optimizer with an initial learning rate 0.01, weight decay 5 × 10 -5 , and dropout rate 0.2 for 300 epochs to avoid overfitting. Our model was implemented with the PyTorch framework and trained on NVIDIA RTX 3090 GPU.Competing Methods. To validate the superiority of our proposed model, we compared the GCN model with traditional machine learning algorithms including support vector machine (SVM), random forest (RF), and K-nearest neighbor (KNN). The upper triangle of the MSN was used as input features. We tuned hyperparameters using 5-fold crossvalidation to find the optimal parameters via grid search."
Graph Convolutional Network with Morphometric Similarity Networks for Schizophrenia Classification,3.1,Results and Analysis,"Results of Schizophrenia Classification. Table 1 presents the performance of our proposed model compared with those obtained by several machine learning models. The GCN model based on clustering coefficient achieved a superior performance compared with other machine learning models with an average accuracy of 80.2%. Identification of Discriminating Regions. Figure 2 shows the top 10 brain regions contributing to GCN classification. The most salient regions contributing to classification were primarily identified in the middle temporal gyrus, superior temporal gyrus, and inferior temporal gyrus. These findings suggest that these regions are crucial in distinguishing patients with schizophrenia from healthy controls. Detailed results are provided in Supplementary Table S2. "
Graph Convolutional Network with Morphometric Similarity Networks for Schizophrenia Classification,3.2,Ablation Studies,"We provide detailed investigation of three key components (the density of MSNs, feature selection strategy, and edge encoder) and their influence on classification results.Influence of the Connection Density of MSNs. We constructed a series of MSNs with different connection densities ranging from 10% to 100% in 10% increments.For each matrix, we computed strength, betweenness centrality, and clustering coefficient to examine their influence on classification performance. In Fig. 3(a), using GCN with strength provides decreased classification performance as the connection density increases. Betweenness centrality shows a lower performance as the density decreases. The use of clustering coefficient is most reliable and provides the highest performance (81.8% accuracy) at connection density of 30%. Influence of the Encoder. We examined the influence of four different types of encoders (PAE, EA, L2, and Cosine + Tanh) on classification performance. Figure 3(c) presents that using PAE as an encoder provides the highest classification performance compared to the other encoders. "
Graph Convolutional Network with Morphometric Similarity Networks for Schizophrenia Classification,,Effect of the,
Graph Convolutional Network with Morphometric Similarity Networks for Schizophrenia Classification,4,Conclusion,"In this study, we proposed an improved GCN structure, which combines graphtheoretical measures of MSNs derived from structural MRI data and non-imaging phenotypic measures for disease classification. This study explored the application value of our proposed GCN model based on cortical networks (MSNs) in differentiating patients with schizophrenia from healthy controls. Using a large multi-site dataset and various validation strategies, reliable and generalizable classification accuracy of 81.8% can be achieved. These results indicate that the MSNs serve as a useful and clinicallyrelevant phenotype and GCN modeling shows promise in detecting individual patients with schizophrenia. Further, we investigated different graph structures and their influence on classification performance. The GCN model making use of subject-specific clustering coefficient as imaging feature vectors and the PAE encoder performed best. By examining the saliency patterns contributing to GCN classification, we identified the most salient regions in the middle temporal gyrus and superior temporal gyrus. These findings suggest the potential utility of GCN for enhancing our understanding of the underlying neural mechanisms of schizophrenia by identifying clinically-relevant disruptions in brain network topology."
Graph Convolutional Network with Morphometric Similarity Networks for Schizophrenia Classification,,Fig. 1 .,
Graph Convolutional Network with Morphometric Similarity Networks for Schizophrenia Classification,,Fig. 2 .,
Graph Convolutional Network with Morphometric Similarity Networks for Schizophrenia Classification,,,
Graph Convolutional Network with Morphometric Similarity Networks for Schizophrenia Classification,,Fig. 3 .,
Graph Convolutional Network with Morphometric Similarity Networks for Schizophrenia Classification,,Table 1 .,
Graph Convolutional Network with Morphometric Similarity Networks for Schizophrenia Classification,,± 2.85 70.32 ± 4.66 82.55 ± 8.44 75.50 ± 3.24 88.50 ± 1.83,
M-FLAG: Medical Vision-Language Pre-training with Frozen Language Models and Latent Space Geometry Optimization,1,Introduction,"Deep learning has made significant progress in medical computer vision [2,7] but requires large annotated datasets, which are often difficult to obtain. Selfsupervised learning (SSL) offers a solution by utilizing large unannotated medical image sets. It also enables vision-language pre-training (VLP), which learns representations for both imaging and text data and their relationships [5,21,26]. Several recent medical VLP approaches such as ConVIRT [32], GLoRIA [11], and MGCA [27] have shown the effectiveness of model pre-training with medical images and radiology reports together, which outperformed the conventionally pre-trained models using image only in downstream tasks [32]. However, training such models is not an easy task as they require extensive resources for training both vision and language models. In particular, most VLP approaches are based on pre-trained BERT [6,18], whose parameters are 5 times larger than a standard ResNet50 [10]. This indicates high computational cost, as well as training complexity and potential instability in joint training [13]. On the other hand, previous works [21,27,32] suggest a training strategy that forces image latent space to match language latent space, which can be sub-optimal with latent space collapse problem [14], reducing its performance for downstream tasks [34]. In this work, we would like to answer the following two questions: (1) Is it necessary to tune pre-trained language models for medical VLP? (2) How to regularize the latent space in pre-training?We propose a novel VLP framework named Medical vision-language pretraining with Frozen language models and Latent spAce Geometry optimization method (M-FLAG). Different from most existing VLP approaches, M-FLAG is computationally efficient as it only requires training the vision model, while keeping the language model frozen. To harmonize the latent spaces in vision and language models, we relax the visual-language alignment objective with a orthogonality loss to alleviate the latent space collapse problem. The main contributions of this work include: (1) To the best of our knowledge, this is the first work to explore the collapsed latent space problem in medical VLP. (2) A novel and effective VLP framework is proposed to alleviate the collapsed latent space problem by explicitly optimizing the latent geometry towards orthogonal using our orthogonality loss in addition to the visual-language alignment loss, encouraging the in-dependency between latent variables and maximizing its informativeness for downstream tasks. (3) M-FLAG consistently outperforms existing medical VLP methods on three downstream tasks: medical image classification, segmentation, and object detection, while reducing 78% trainable parameters due to the frozen language model strategy."
M-FLAG: Medical Vision-Language Pre-training with Frozen Language Models and Latent Space Geometry Optimization,,Related Works:,"To connect vision and language modalities, the idea of VLP was proposed in CLIP [21], which involves learning mutual knowledge from two modalities by maximizing their feature similarity. CLIP [21] and later FLIP [19] focus on learning cross-representation in natural language and images. However, there is a significant lack of research in the medical domain due to the complexity of the medical text and the limited availability of large-scale paired medical image-text datasets. Recently, ConVIRT [32], GLoRIA [11], and MGCA [27] have made notable progress in aligning medical text and images. These methods require significant computational resources and are sometimes limited by the collapse issue of the latent space.It has been suggested that optimal vision and language latent spaces should be of different geometry [8] and latent space uniformity is considered an essential indicator to evaluate the success of learning [28]. Yet, most existing VLP approaches rigorously align the vision latent space to the language space without considering the latent space geometry, which may lead to latent space collapse. As pointed out by [14], latent space collapse indicates significant information loss, which can crucially affect the robustness of the pre-trained model on downstream tasks when transferring the model to unseen domains [3]. To solve this problem, contrastive learning-based approaches can be used to spread visual features over the unit sphere with good uniformity [4,9]. However, it requires a large number of negative samples in each training batch, which inevitably increases computational costs. Differently, here we address this problem by employing a orthogonality loss, which directly aligns the geometry of the latent space towards a uniform hypersphere to tackle the collapse problem."
M-FLAG: Medical Vision-Language Pre-training with Frozen Language Models and Latent Space Geometry Optimization,2,Methods,"The proposed M-FLAG is a simple and light VLP framework that aims to learn visual and text representations by leveraging both medical images and radiology Fig. 1. M-FLAG overview. M-FLAG consists of a vision encoder EV for learning vision latent zv, a frozen language model ET for extracting medical text latent zt, and a projector p(•) to map zv to za for alignment with zt. M-FLAG employs an alignment loss L align for vision-text latent space alignment between za and zt and a orthogonality loss L orth to encourage the orthogonality of zv (Sect. 2.2). Visualization of the first 3 dominant dimensions of latent space zv via PCA [30] shows the M-FLAG alleviates the dimensional collapse in the latent space, while MGCA [27] and GLoRIA [11] suffer the problem to different extents.reports. We employ a freeze strategy for the text encoder E T to mitigate ambiguity in vision-text latent space alignment. Additionally, we explicitly optimize the latent space geometry using a orthogonality loss. By doing so, we encourage the visual latent space to keep a stable geometry and reduce the risk of collapse. Figure 1 illustrates the workflow of M-FLAG and the learned latent space geometry compared to two recent medical VLP approaches."
M-FLAG: Medical Vision-Language Pre-training with Frozen Language Models and Latent Space Geometry Optimization,,Vision Encoder and Frozen Text Encoder:,"The paired medical image and text are denoted as x v , x t , respectively. As illustrated in Fig. 1, we obtain the image embedding z v via the vision encoder E V and the text embedding z t via the frozen text encoder E T .Vision Embedding: E V , the vision embedding z v ∈ R B×N is extracted from the last pooling layer of E V . N denotes the dimension of the latent space and B represents the batch size.Text Embedding: A text encoder E T extracts text embedding of word tokens from a medical report. Similar to BERT [6], a special token [cls] is added, which aggregates the representations of all word tokens into one embedding. The embedding of [cls] is used as the text report representation and denoted as z t ."
M-FLAG: Medical Vision-Language Pre-training with Frozen Language Models and Latent Space Geometry Optimization,2.1,Frozen Language Model,"In this work, we use a frozen text encoder E T , which can be obtained from any general language model. The latent space of z v is thus stable without the risk of latent space perturbation [20,33] due to the joint training of two encoders. Naturally, the computational cost is considerably reduced since the proposed approach only requires the training of a light vision encoder E V and a projector p(•)."
M-FLAG: Medical Vision-Language Pre-training with Frozen Language Models and Latent Space Geometry Optimization,2.2,Alignment and Uniformity,"As illustrated in Fig. 1, after obtaining the visual embedding z v = E V (x v ) and text embedding z t = E T (x t ) using corresponding encoders, the vision embedding z v is projected to z a by a linear projector z a = p(z v ), so that z a is of the same dimension as z t and alignment can be performed.We compute a composite loss L total to train the vision encoder E V and the projector p(•), which consists of two parts, alignment loss L align and orthogonality loss L orth :where {i, j} ∈ {1, ..., dim(z v )} 2 . We implement 2 -normalization on z a , z t , z v to obtain za , zt , zv . L align minimizes the discrepancy between za and zt , while L orth maximizes the independence among latent features in zv , forcing its empirical correlation matrix to be an identity matrix. In other words, we expect different latent feature dimensions to be independent. The objective of the first term on the right side in Eq. ( 3) aims to optimize the diagonal elements of the empirical correlation matrix to 1, while the second term on the right side aims to reduce all non-diagonal elements to 0. Here, (•) T denotes the matrix transpose operation."
M-FLAG: Medical Vision-Language Pre-training with Frozen Language Models and Latent Space Geometry Optimization,3,Experiments,
M-FLAG: Medical Vision-Language Pre-training with Frozen Language Models and Latent Space Geometry Optimization,3.1,Dataset for Pre-training,"M-FLAG is pre-trained on the MIMIC-CXR (MIMIC) dataset [15,16], which contains 227,827 image-text pairs with chest X-ray (CXR) images and radiology reports. Following the preprocessing procedure of [11,27,32], 213,384 image-text pairs are used. We use ResNet50 [10] as E V and frozen CXR-BERT [1] as E T .Pre-training takes 100 epochs on 8 A100 GPUs, with a batch size of 128 for each GPU and a learning rate of 0.001 using the LARS [31] optimizer."
M-FLAG: Medical Vision-Language Pre-training with Frozen Language Models and Latent Space Geometry Optimization,3.2,Datasets for Downstream Tasks,"The pre-trained model is evaluated on 3 downstream tasks across 5 datasets:Medical image classification is implemented on MIMIC, CheXpert (CXP), and NIH [12,16,29] datasets, each consisting of images from 14 disease categories.To reduce sampling bias and maintain consistency, we follow the dataset split in CheXclusion [24] and evaluate the macro AUC scores.Image segmentation is evaluated on two datasets, RSNA [25] (pneumonia segmentation) and SIIM [17] (pneumothorax segmentation). Following [11,27], we use U-Net [23] as the segmentation backbone. The pre-trained model is used as the frozen encoder of the U-net [23] and we only update the decoder of the Unet during fine-tuning. We evaluate segmentation performance using Dice scores.Object detection is implemented on the RSNA [25] dataset for pneumonia detection, using the preprocessing techniques outlined in [27]. Following [27], we use YOLOv3 [22] as the detection framework. We employ the pre-trained vision encoder of M-FLAG as the backbone and only fine-tune the detection head. The detection task is evaluated using mean average precision (mAP) with the intersection of union (IoU) thresholds ranging from 0.4 to 0.75.Table 1 reports the data split details. For all downstream tasks, we fine-tune using 1%, 10%, 100% of the train data on a single A100 GPU. Classification MIMIC [16] [24] 215,187 5,000 23,137 CXP [12] [24] 167,185 5,000 19,027 NIH [29] [24] 100,747 5,000 6,373Segmentation RSNA [25] [11, 27]  "
M-FLAG: Medical Vision-Language Pre-training with Frozen Language Models and Latent Space Geometry Optimization,3.3,Results,
M-FLAG: Medical Vision-Language Pre-training with Frozen Language Models and Latent Space Geometry Optimization,,Medical Image Classification:,"The AUC scores on MIMIC, CXP, and CXR14 are reported in Table 2. It shows that M-FLAG consistently outperforms all baseline methods across almost all datasets and data fractions. Notably, M-FLAG achieves superior performance while using only 22% of the trainable parameters compared to other methods. While MGCA [27] slightly outperforms our method only when fine-tuning on 10% of the CXP dataset, it requires more than five times parameters than M-FLAG.Segmentation and Object Detection: Table 3 shows that M-FLAG outperforms all SOTA methods across all datasets and data fractions in segmentation and detection tasks. In the segmentation task, M-FLAG achieves the highest Dice score across all fractions of both the SIIM and RSNA datasets. Interestingly, even when fine-tuned with only 1% of the data in RSNA, M-FLAG outperforms the ImageNet pre-trained model fine-tuned with 100% of the data. Similarly, in the object detection task, M-FLAG achieves the highest mean average precision (mAP) across all data fractions of the RSNA dataset. When fine-tuned with only 10% of the data, M-FLAG still outperforms the ImageNet pre-trained model with 100% fine-tuning. These results indicate the advantages of using a frozen language model and introducing orthogonality loss during pre-training, which may yield more informative latent representations that are better suited for downstream tasks. Overall, the improvements achieved by M-FLAG across diverse downstream tasks demonstrate its effectiveness and versatility in medical image analysis."
M-FLAG: Medical Vision-Language Pre-training with Frozen Language Models and Latent Space Geometry Optimization,3.4,Dimensional Collapse Analysis,"Recent studies [14,28] have highlighted that latent space learned via selfsupervised learning can suffer from issues such as complete collapse or dimensional collapse, which would lead to poor performance for downstream tasks. Figure 1 bottom right panel shows that both MGCA and GLoRIA [11,27] suffer from dimensional collapse. Figure 2 shows that if the last n layers of the language model in M-FLAG are not frozen, the latent space geometry would also exhibit varying degrees of collapse. This indicates the importance of using a frozen language model. Quantitative results in Tables 2, 3, 4 and 5 and qualitative visualization in Figs. 1 and2 further demonstrate that a collapsed latent space can impair the performance for various downstream tasks, especially for segmentation and detection. These findings highlight the usefulness of a frozen language model in preventing latent space collapse."
M-FLAG: Medical Vision-Language Pre-training with Frozen Language Models and Latent Space Geometry Optimization,3.5,Ablation Study,"Ablation Study: Table 4 presents the results of an ablation study to evaluate the impact of L orth and L align on model performance. Across all experiments, the proposed version of M-FLAG achieves the highest performance, with a clear advantage over implementations that only use L orth or L align in pre-training.The performance of the model pre-trained with only L align drops dramatically in segmentation and detection tasks, although less severe in the classification tasks.On the other hand, the model pre-trained with only L orth does not suffer severe performance drop across the three tasks, indicating that the uniform latent space could have a considerable contribution to the performance of M-FLAG. Overall, these results underscore the importance of both loss functions in M-FLAG and highlight their complementary contributions.  Comparing M-FLAG with Frozen vs. Unfrozen Language Models: We conducted further experiments to evaluate the performance of M-FLAG while unfreezing the last few layers of the language model. This not only increases the number of trainable parameters but also influences the model performance.Table 5 shows that when the language model is unfrozen, the performance slightly drops, compared to M-FLAG with the frozen language model (proposed). M-FLAG achieves a better performance with an average improvement of 2.18% than its Unfreeze 1-6 variants on the NIH dataset and an average improvement of 4.32% on the SIIM dataset."
M-FLAG: Medical Vision-Language Pre-training with Frozen Language Models and Latent Space Geometry Optimization,4,Conclusion,"Simple architecture means low computational cost and stable training. In this work, we propose a simple and efficient VLP framework that includes a frozen language model and a latent space orthogonality loss function. Extensive experiments show that M-FLAG outperforms SOTA medical VLP methods with 78% fewer parameters. M-FLAG also demonstrates its robustness by achieving the highest performance when transferred to unseen test sets and diverse downstream tasks for medical image classification, segmentation, and detection. This indicates the benefits of freezing the language model and regularizing the latent space. The results exhibit promising potential for improving the pre-training of vision-language models in the medical domain. In addition, the latent space geometry explored in this work provides useful insight for future work in VLP."
M-FLAG: Medical Vision-Language Pre-training with Frozen Language Models and Latent Space Geometry Optimization,,Fig. 2 .,
M-FLAG: Medical Vision-Language Pre-training with Frozen Language Models and Latent Space Geometry Optimization,,,
M-FLAG: Medical Vision-Language Pre-training with Frozen Language Models and Latent Space Geometry Optimization,,Table 1 .,
M-FLAG: Medical Vision-Language Pre-training with Frozen Language Models and Latent Space Geometry Optimization,,Table 2 .,
M-FLAG: Medical Vision-Language Pre-training with Frozen Language Models and Latent Space Geometry Optimization,,Table 3 .,
M-FLAG: Medical Vision-Language Pre-training with Frozen Language Models and Latent Space Geometry Optimization,,61.2 64.8 64.6 69.7 70.5 13.7 17.5 25.4,
M-FLAG: Medical Vision-Language Pre-training with Frozen Language Models and Latent Space Geometry Optimization,,Table 4 .,
M-FLAG: Medical Vision-Language Pre-training with Frozen Language Models and Latent Space Geometry Optimization,,Table 5 .,
M-FLAG: Medical Vision-Language Pre-training with Frozen Language Models and Latent Space Geometry Optimization,,Machine Learning -Transfer Learning,
Tracking Adaptation to Improve SuperPoint for 3D Reconstruction in Endoscopy,1,Introduction,"Endoscopy is an important medical procedure with many applications, from routine screening to detection of early signs of cancer and minimally invasive treatment. Automatic analysis and understanding of these videos raises many opportunities for novel assistive and automatization tasks on endoscopy procedures. Obtaining 3D models from the intracorporeal scenes captured in endoscopies is an essential step to enable these novel tasks and build applications, for example, for improved monitoring of existing patients or augmented reality during training or real explorations.3D reconstruction strategies have been studied for long, and one crucial step in these strategies is feature detection and matching which serves as input for Structure from Motion (SfM) pipelines. Endoscopic images are a challenging case for feature detection and matching, due to several well known challenges for these tasks, such as lack of texture, or the presence of frequent artifacts, like specular reflections. These problems are accentuated when all the elements in the scene are deformable, as it is the case in most endoscopy scenarios, and in particular in the real use case studied in our work, the lower gastrointestinal tract explored with colonoscopies. Existing 3D reconstruction pipelines are able to build small 3D models out of short clips from real and complete recordings [1]. One of the current bottle-necks to obtain better 3D models is the lack of more abundant and higher quality correspondences in real data.This work introduces SuperPoint-E, a new model to extract interest points from endoscopic images. We build on the well known SuperPoint architecture [5], a seminal work that delivers state-of-the-art results when coupled with downstream tasks1 . Our main contribution is a novel supervision strategy to train the model. We propose to automatically generate reliable training data from video sequences by tracking feature points from existing detection methods, which do not require training. We select good features with the COLMAP SfM pipeline [21], generating training examples with feature points that can be tracked across several images according to COLMAP result. When used to train SuperPoint, our approach yields a self-supervised method outperforming current ones."
Tracking Adaptation to Improve SuperPoint for 3D Reconstruction in Endoscopy,2,Related Work,"3D reconstruction is an open problem for laparoscopic and endoscopic settings [14] of high interest for the community. This idea is supported for example by recent efforts on collecting new public dataset, to further advance in this field, such as endoscopic recordings from EndoSLAM [16] and EndoMapper [1] datasets. Earlier works like Grasa et al. [7] have evaluated the performance of modern SLAM approaches on endoscopic sequences. Mahmoud et al. [13] improved the performance of such methods in laparoscopic sequences. More recent approaches attempt to tackle specific endoscopy challenges, such as the deformation [18] or the artifacts due to specular reflections in the feature extraction step [2].Well known SfM and SLAM pipelines rely on accurate and robust feature extraction methods. COLMAP [21,22], a public SfM tool, uses SIFT [11] features while ORB-SLAM [15] extracts ORB [19] features because of their efficiency. Both these feature extraction methods count with classical, hand-crafted descriptors that allowed to build such complex applications. However, transferring that performance to endoscopy settings remains a difficult task due to several challenges. Artifacts or the lack of texture result in low amount of correspondences along real endoscopy videos, what motivates the need for improved strategies.Deep learning methods for feature extraction and matching is a very active research field. The survey Ma et al. [12] shows the introduction of deep learning methods to feature detection and matching. Notable mentions are SuperPoint [5] for its self-supervised approach, R2D2 [17] for using reliability metrics as output of the network instead of the features themselves and D2-Net [6] that built a describe-and-detect strategy that aims to improve SfM applicability. Exporting this progress to the matching stage, DISK [24] proposes a formulation of the problem to optimize in an end-to-end manner. Other recent works have extended the networks to take advantage of the advances in attention for the matching task, as in SuperGlue [20] and LoFTR [23].In this work we improve the performance of SuperPoint [5] on endoscopy images. We chose SuperPoint because it is a seminal work that has inspired many follow up works, and it is still among the top performers on current feature matching challenges [10]. Similar to DeTone et al. [4], we explore improvements on feature extraction that provide good properties for downstream tasks. They design an end-to-end method to optimize the visual odometry computed with their features. Differently, we propose to supervise our training with points that have been successfully used for 3D reconstruction using existing SfM pipelines. With this supervision, we train a model able to extract more features with good properties for SfM algorithms, e.g., being spread and out of large specularities."
Tracking Adaptation to Improve SuperPoint for 3D Reconstruction in Endoscopy,3,Tracking Adaptation for Local Feature Learning,"Superpoint supervision is referred to as Homographic Adaptation and assumes that the surfaces are locally plane, which is not the case in our data. Instead, we propose to use 3D reconstructions of points tracked along image sequences. This makes no assumptions about the local surface shapes and we will show in Sect. 4 that this yields a better trained network. We will refer to this as Tracking Adaptation and we will here describe how we obtain the tracks."
Tracking Adaptation to Improve SuperPoint for 3D Reconstruction in Endoscopy,,SfM as Supervision for Feature Extraction.,"We generate examples of good features by identifying features that were successfully reconstructed with existing methods for each sequence in our training set. Our training set contains short sequences (4-7 s) from the complete colonoscopy recordings in EndoMapper dataset where COLMAP software was able to obtain a 3D reconstruction. This is a very challenging domain, and existing SfM pipelines fail in longer videos."
Tracking Adaptation to Improve SuperPoint for 3D Reconstruction in Endoscopy,,3D Reconstruction of Training Set Videos.,"We generate 3D reconstructions for all our training sequences with out-of-the-box COLMAP. In particular, we use the following blocks: feature extractor, exhaustive matcher and mapper. Configuration parameters are detailed in the supplementary materials. We turn on the ""guided matching"" option for the exhaustive matcher module to find the best matches possible. We additionally compute the 3D reconstruction for the same sequences with a modified COLMAP pipeline that uses the official Super-Point and SuperGlue2 implementation with the indoor set of weights. All the parameters are left as default except for the keypoint threshold= 0.015 and the nms radius= 1. After providing the SuperGlue resulting matches to COLMAP, we execute only the mapper module with the same configuration as before. Re-project Good Features to the Training Set Frames. A successful 3D reconstruction includes the computed positions of the cameras that took the images and a point cloud with 3D coordinates of the triangulated points. We use the camera poses, the points' coordinates and the camera calibration parameters to reproject the 3D point cloud points into every image. Not all points were originally detected and triangulated at all frames, so we establish two types of reprojected points. If they were ""originally"" detected and matched in a particular image, we set them to green. Otherwise, we set them to blue (see Fig. 1).For supervision, we only use reprojected points that fall within a reliable track. A reliable track is an interval bounded by green points. So, the reprojected points selected for training are either green or have preceding and subsequent green points along its track.The different appearances of the same 3D point in different frames of the track are our correspondences for training our models. Figure 1 contains examples of reprojected points and an example of a reliable track.Deep Feature Extraction for Endoscopy. SuperPoint uses a fully-convolutional network as backbone and learns to extract good features using homographic adaptation: extracting features that are robust to homographic deformations. It achieves this by using as supervision Y the average detections over several random homographic deformations of the same image. The feature extraction network then is run on an image I and a warped version I of it with a new homography. The network optimizes the loss functionwhere X and D are the detection and description heads' outputs, respectively. Y is the supervision for the detection. S is the correspondence between I and I computed from the homography. L p is the detection loss that measures the discrepancies between the supervision Y and the detection head's output X . λ = 1 is a weighting parameter. L d is the description loss that measures the discrepancies between both description head's outputs D and D using S.Using our new supervision from SfM in the form of tracks of points, we propose a new loss to train SuperPoint that is more aligned with our goal, called tracking adaptation. Instead of an image I and a warped version I , we use different images I a and I b from the same sequence. The supervision Y for the detection in this case is the set of points that have been reprojected on I a and I b from the 3D reconstruction. The detection loss L p is calculated as in the original SuperPoint. We replace the description loss L d for a new tracking losswhere D a and D b are the description head's outputs for I a and I b , respectively. T is the set of all the tracks that appear in both images. l t is a common triplet loss that measures the distance between positive pairs (weighting parameter λ t = 1 and positive margin m p = 1) and the distance between negative pairs (negative margin m n = 0.2). Two descriptors from different images d ai and d bj are a positive pair if they belong to the same track (i = j), and negative pair otherwise (i = j)."
Tracking Adaptation to Improve SuperPoint for 3D Reconstruction in Endoscopy,4,Experiments,"The following experiments demonstrate the proposed feature detection efficacy to obtain 3D models on real colonoscopy videos, comparing different variations of our approach and relevant baseline methods.Dataset. We seek techniques that are applicable to real medical data, so we train and evaluate with subsequences from the EndoMapper dataset [1], which contains a hundred complete endoscopy recordings obtained during regular medical practice. We use COLMAP 3D reconstructions obtained from subsequences from this dataset (11260 frames from 65 reconstructions obtained along 14 different videos for training, and 838 frames from 7 reconstructions from 6 different videos for testing). The exact details are in the supplementary material.Baselines and our Variations. We use COLMAP as our first baseline. It uses SIFT features and a standard guided matching algorithm to produce very accurate camera pose estimates. We also include as baseline the results of SuperPoint (SP) with SuperGlue matches and the COLMAP reconstruction module. The configuration for both baselines is the same as detailed in Sect. 3. We evaluate different variations of the original SuperPoint. All models were trained with a modification of a PyTorch implementation of SuperPoint [9]. Training parameters in supplementary material. The models differ in the supervision used and the loss applied in the training, as detailed in the first four columns of Table 1. Ablation Study. Table 1 (last five columns) summarizes the performance of our approach variations. We run all the models on the Test set subsequences to extract points. Matches between the points in two images are obtained with bi-directional nearest neighbor algorithm with L2 distance. Points and matches are given to COLMAP and the mapper module (configuration in supplementary material) attempts to generate a 3D reconstruction. The reconstruction quality statistics used to illustrate the performance of each detector are:-3DIm : Fraction of images from the subsequence successfully introduced in the reconstruction. The closer to 100% the better. -3DP ts : Number of points that were successfully reconstructed. The more points the better, since it means a denser coverage of the scene. -Err: Mean reprojection error of the 3D points after being reprojected onto the images of the subsequence. -Err-10K: Mean reprojection error of the best 10000 points of the reconstruction. Since all reconstructions have outliers that skew the average, this metric is more representative of the performance of the models.-len(Tr): Mean track length represents the average number of images where a point is being consecutively matched, tracked.SP-E v2 (SP-E moving forward) is our best variation, with the highest amount of reconstructed points and the lowest reprojection error for top 10000."
Tracking Adaptation to Improve SuperPoint for 3D Reconstruction in Endoscopy,,SfM Results,"Comparison. This experiment compares the performance of the considered baselines against the best configuration of our feature extraction model. Table 2 contains a summary of the results. In most metrics we observe a significant improvement using SP-E compared to the others. For example, the number of points at the final reconstruction is more than three times higher (see  the example in Fig. 2). The mean reprojection error of all the points is the lowest for SIFT, possibly due to it being more restrictive in all other aspects (number of images reconstructed, number of points, track length). However, the mean error for the top 10000 points is always lower for SP-E. The reprojection error plots in Figs. 3 and4 provide more insight on this metric. Figures 3 and4 show a more detailed visualization of two representative reconstructions, including a summary of the sequence frames, the point cloud obtained by each method and a plot of the reprojection error for each point in the reconstruction, sorted in increasing error value. Note that even though SP-E obtains many more points, it is not at the cost of quality. Figure 3 shows a scenario where SIFT fails to reconstruct a large part of the subsequence, because it fails on the feature matching on the darker frames depicted in the middle of the sequence. Note how the reconstruction from SP-E is notably denser than the others. Figure 4 shows a scenario where all approaches perform well and SIFT achieves the lowest reprojection error.  We analyze additional aspects of our detected features to showcase the higher quality with respect to other methods in Table 3. To measure the spread of the features over the images we defined a 16 × 16 grid over each image and computed the percentage of those cells that have at least one reconstructed point. We also measure how many extracted points fall on top of specularities (we consider a pixel as part of a specularity if its intensity is higher than 180). For both metrics, our detector achieves significantly better results, showcasing the better properties of our detector for 3D reconstruction.To provide quantitative evaluation of the camera motion estimation, we use a simulated dataset [3] to have ground truth available for the camera trajectory. We took 5 sequences of 100-150 frames from this dataset, and we tested the baselines and our model. We align the ground truth trajectories with the reconstructed ones with Horn's method [8]. SP only reconstructed 3 out of the 5 sequences while SIFT and SP-E correctly reconstructed the 5 sequences, with an average RMSE of 4.61mm and 4.71 mm respectively. Simulated data lacks some of the biggest challenges of endoscopy images (e.g. specularities, deformations), but this experiment suggests that the camera motion estimation quality is similarly good for all methods when they manage to converge. "
Tracking Adaptation to Improve SuperPoint for 3D Reconstruction in Endoscopy,5,Conclusions,"This work presents a novel training strategy for SuperPoint to improve its performance in SfM from endoscopy images. This strategy has two main benefits: we show how to use 3D reconstructions of endoscopy sequences as supervision to train feature extraction models; and we design a new tracking loss to perform tracking adaptation using this supervision. The benefits of our method are explored with an ablation study and against established baselines on SfM and feature extraction. Our proposed model is able to obtain more suitable features for 3D reconstruction, and to reconstruct larger sets of images with much denser point clouds."
Tracking Adaptation to Improve SuperPoint for 3D Reconstruction in Endoscopy,,Fig. 1 .,
Tracking Adaptation to Improve SuperPoint for 3D Reconstruction in Endoscopy,,Fig. 2 .,
Tracking Adaptation to Improve SuperPoint for 3D Reconstruction in Endoscopy,,Fig. 3 .,
Tracking Adaptation to Improve SuperPoint for 3D Reconstruction in Endoscopy,,Fig. 4 .,
Tracking Adaptation to Improve SuperPoint for 3D Reconstruction in Endoscopy,,Table 1 .,"point (Base point detector): SP-O: original superpoint detector; SF*/SP*: SIFT/SP points that were successfully reconstructed after the COLMAP optimization, reprojected in each video frame. match (Matches Supervision): H: Homography based, i.e., Homographic adaptation from original SuperPoint work; TR: The proposed Tracking adaptation. loss (Loss used for training): SP: original SuperPoint training loss; Tr-2 or Tr-N: track -based loss. Tr-2 means that the loss is computed for every pair of images in the track. Tr-N means we optimize simultaneously N views of the track (N=4 in our experiments)."
Tracking Adaptation to Improve SuperPoint for 3D Reconstruction in Endoscopy,,Table 2 .,
Tracking Adaptation to Improve SuperPoint for 3D Reconstruction in Endoscopy,,34851 45471 42727 33277 36403 19286 31851 34838.0 (,"+ Total number of images in the subsequence. * If 10K points are not available, average is computed over all available reconstructed points."
Tracking Adaptation to Improve SuperPoint for 3D Reconstruction in Endoscopy,,Table 3 .,
Tracking Adaptation to Improve SuperPoint for 3D Reconstruction in Endoscopy,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0 56.
Knowledge Boosting: Rethinking Medical Contrastive Vision-Language Pre-training,1,Introduction,"Foundation models have become a significant milestone in artificial intelligence, from theoretical research to practical applications [2], like world-impacting large language model ChatGPT [5] and art-history-defining large generative model   DALL-E [20]. In medical image analysis, foundation models are showing promising future, and pre-training technologies [3,4,8], as the cornerstone of foundation models, facilitated feasibility of computer-aided diagnosis for widespread use.Medical contrastive vision-language pre-training [10,15,21,23,25] has shown great superiority in medical image analysis, because it utilizes easy-accessible expert interpretation from reports to precisely guide the understanding of image semantics. Therefore, contrastive vision-language pre-training will break through the bottleneck of time-consuming and expensive expert annotation [26] and difficulty in learning fine-grained clinical features with pure-image self-supervised methods [28]. It will improve data efficiency, and achieve comparable or better performance when transferred with the zero-shot or few-shot setting, demonstrating the potential of promoting the ecology of medical artificial intelligence.However, semantic overlap and semantic shifting are two significant challenges in medical vision-language contrastive learning (Fig. 2). (a) Semantic Overlap Problem: There is overlapping semantics between negative samples which should be semantic-distinct, e.g. two medical images sharing the same disease are contrasted which brings noise [25]. Once directly learning, crossmodal representations of the same disease are falsely pulled apart, making the model unable to capture the disease-corresponding image feature. (b) Semantic Shifting Problem: Radiologists have writing preferences, e.g. biased for their own familiar concepts and observation view towards similar visual features, and inclined for negation expression towards opposite visual features. Distinct   concepts describing the same image are morphologically dissimilar for text encoder, while the negation expression of concepts is morphologically similar [17]."
Knowledge Boosting: Rethinking Medical Contrastive Vision-Language Pre-training,,Report Ti Report Ti,
Knowledge Boosting: Rethinking Medical Contrastive Vision-Language Pre-training,,Graph,"Once lack of concept correlation and negation identification, representations with similar semantics are falsely pushed apart and those with opposite semantics are falsely pushed together, interfering with the learning of significant representation [7].Rethinking the existing methods and challenges of medical contrastive visionlanguage pre-training [10,21,23,25,26], the lack of clinical knowledge constraints in dual-free-encoding contrastive learning structure is the key problem. Existing methods utilize sample-wise differences to learn mutual information between modalities, improving the representation quality based on the correspondence of learned mutual information and clinical knowledge. However, semantic overlap reduces the learning efficiency of mutual information with the noisy difference, and the mentioned correspondence is vulnerable to semantic shifting. Therefore, if we are able to embed an unbiased, comprehensive representation as knowledge boosting, it will reduce the negative noise and supplement the lacking correspondence. It motivates us to measure the noise with similarities between knowledge representation, and fuse the correspondence between knowledge and modality.In this paper, we propose a novel knowledge-boosting medical contrastive vision-language pre-training framework (KoBo). Our contributions are as followed. 1) Our KoBo pre-trains a powerful image encoder including visual information corresponding with the disease described in texts, where knowledge is embedded in our paradigm (Fig. 1) to boost the learning of vision-language consistency. 2) We propose Knowledge Semantic Enhancement (KSE) module to reduce the negative sample noise with the similarity between open-set sample-wise knowledge embeddings. 3) We propose Knowledge Semantic Guidance (KSG) module to adjust the semantic shifting during pre-training, fusing the modality feature with unbiased knowledge embeddings for supplementing the correspondence between modality mutual information and clinical knowledge."
Knowledge Boosting: Rethinking Medical Contrastive Vision-Language Pre-training,2,Methodology,"Our Knowledge-Boosting Contrastive Vision-Language Pre-training framework (Fig. 3) boosts vision-language learning with additional clinical knowledge. It contains two modules: KSE for reducing the negative effect of semantic overlap, and KSG for adjusting semantic shifting, aimed at learning effective representation by maximizing semantic consistency between paired image and text features."
Knowledge Boosting: Rethinking Medical Contrastive Vision-Language Pre-training,2.1,Framework Formulation,"In the framework, a powerful image encoder Enc I and text encoder Enc T is pretrained, alongside a graph encoder Enc G . Given a pair of medical image and diagnostic report Besides using reports and images as the input for our pre-training network, we also input an external knowledge graph to the whole framework for improving the correspondence of modality features and clinical knowledge. The knowledge refers to relations between clinical pathology concepts in the radiology domain in the format of triplet G = {(c h k , r k , c t k )} NG k=1 , such as UMLS [14]. Domain knowledge embedding for each concept"
Knowledge Boosting: Rethinking Medical Contrastive Vision-Language Pre-training,2.2,Knowledge Semantic Enhancement,"To relieve the semantic overlap problem, where negative sample noise harms the effective learning of vision-language mutual information, we propose a semantic enhancement module to identify the noise using sample-wise similarities. The similarity is estimated upon sample knowledge k i , calculated from domain knowledge embedding E and concept set from texts with negation marker.Getting Sample knowledge: Firstly, we acquire a concept set that contains pathology concepts extracted from texts with Negbio N (•) [17]. The image-view concept set which involves the overall observation is from the whole report, while the text-view set only covers the chosen sentence. Secondly, the image and text sample knowledge, as an auxiliary semantic estimation, is selected from domain knowledge embedding E according to the corresponding concept set from the report and sentence respectively, if not considering the negation problem.Furthermore, considering the challenge that negation expression of concepts commonly exists in radiology reports, which has opposite semantics with similar morphology for text encoder (converging shifting), we randomly generate a No Finding embedding N F and a variant of domain knowledge embedding E = { e 1 , e 2 , ..., e NE } of the same size as E with Xavier distribution. Upon the negation mark of concept, sample knowledge embedding k i = {k i,s } NES s=1 is denoted below:where P is the negation mark of concepts, and e i,s , e i,s is the corresponding position of c i,s in E and E. tunes the variance of negative sample knowledge.are k i from the image-view and text-view concept set."
Knowledge Boosting: Rethinking Medical Contrastive Vision-Language Pre-training,,Estimation of Similarities:,"The semantic similarity is calculated upon sample knowledge. For each image-text pair, a max-match strategy is adopted to match each two sample knowledge embedding with the most similar one for calculating cosine similarities. Sample-wise similarities are aggregated with averages.(2) where N ES is the number of concepts in T Sent i , while N ES is that in T Report i . Knowledge Semantic Enhancement Loss: We utilize the sample-wise semantic similarity to estimate negative sample noise, placed in the sample weight of the contrastive loss [18,26], where paired cross-modal embedding are pushed together and unpaired ones are pulled apart. The importance of estimated noisy negative samples is relatively smaller for a subtle pulling between cross-modal embeddings. The semantic enhancement loss is below:) where τ G is the global temperature, and λ IT , λ T I is the sample similarity measurement. specifically, λ i,i is fixed to zero to persist the positive sample weight."
Knowledge Boosting: Rethinking Medical Contrastive Vision-Language Pre-training,2.3,Knowledge Semantic Guidance,"In this section, we propose a semantic guidance module to solve the semantic shifting problem. Utilizing sample knowledge from Sect. 2.2 which contains concept correlation and negation information, the adverse effects of both disperse and converging shifting are alleviated by fusing domain-sample knowledge with global-local modality embeddings. We design four contrast schemes: knowledge anchor guidance for adjusting disperse shifting, semantic knowledge refinement for filtering converging shifting, vision semantic response for consolidating knowledge fusion, and semantic bridge guidance for narrowing the modality gap. Knowledge Anchor Guidance: Disperse shifting will be adjusted if there are unbiased anchors in semantic space as priors to attract modality embeddings towards clinical semantics, and domain knowledge embedding does a good job. We define knowledge fused embeddings andAT T N(Q, K, V ) means the attention function [10]:where image-weighted and text-weighted knowledge is globally contrasted.Semantic Knowledge Refinement: Wrong-converging pairs have distinct intrinsic responses on sample knowledge from image and text. Hence, we propose to utilize sample knowledge to refine these falsely gathered dissimilar pairs. We definewhere local semantic-weighted image and text embeddings are contrasted.Vision Semantic Response: Instead of matching single token with image subregions in [10], we propose to match the concept with sub-regions. As the concept is a more complete and atomic semantic unit, local response upon concept will better guide the representation learning with a fine-grained semantic match through an in-sample contrast. We define), and the fusion of knowledge will be consolidated as below:where there is an in-sample local contrast between H IS i and vision features."
Knowledge Boosting: Rethinking Medical Contrastive Vision-Language Pre-training,,Semantic Bridge Guidance:,"We propose to narrow disperse shifting enlarged by the modality gap between vision and language. Specifically, the gap is bridged by the fusion of domain knowledge which is better compatible with text:where the image-weighted domain knowledge is contrasted with text features between samples. Finally, L SG is aggregated by these four parts as below:3 ExperimentExperiment Protocol: Pre-training performs on MIMIC-CXR [12] following the pre-process style of [9]. The impression section of reports and frontal view of images are selected to generate 203k image-report pair. Five downstream task datasets (CheXpert [11], Covidx [24], MIMIC-CXR, UMNSRS [16] and SIIM [22]) are applied on eight tasks. Semantic relatedness is to verify the text understanding of radiology concepts, where text embedding with certain prompts predicts the relatedness. A new semantic relatedness benchmark is generated from MIMIC-CXR, adding in the extra negation discriminating. CheXpert5X200 [10](Multi-classification) is from CheXpert, and CheXpertlabeller [11] generates retrieval labels in MIMIC-CXR. More details are in appendix.  For implementation, ResNet50 [6] and Vit [13] are image encoder, and Bio-ClinicalBERT [1] is the text encoder. CompGCN with LTE [27] is our graph encoder, and domain knowledge contains 10,244 concepts in UMLS which exist in MIMIC-CXR. Negbio [17] combined with UMLS disambiguation tool [14] serves as N (•). Embeddings are projected into the dim of 256. Pre-training has the batch size of 100 and max epochs of 50 based on Pytorch on two RTX3090 GPUs. Adam optimizer with the learning rate of 5e-5 and ReduceLR scheduler are applied. τ G is 0.07 and τ L is 0.1. λ in KSG loss are all 0.25, while in KSE loss is 0.1."
Knowledge Boosting: Rethinking Medical Contrastive Vision-Language Pre-training,,KoBo Init,"ImageNet Init Basilar Atelectasis. Comparison Study: Table 1 verifies our powerful representation ability, reaching state-of-art in classification, segmentation, and semantic relatedness compared with existing vision-language pre-training tasks, while our method is also top two for retrieval. In zero-shot classification tasks, our KoBo outperforms MGCA and ConVIRT 0.94% and 3.38% respectively, exceeding most methods even in their training setting. For CheXpert5X200, our framework is second only to MedCLIP which presents a superior performance in this dataset. In three fewshot setting task, our KoBo has an absolute leading position.Ablation Study: As is demonstrated in Fig. 4, we perform module ablation and data amount ablation. (a) For module ablation, both modules bring benefits in representation learning and are respectively effective. When KSG module is removed, our KoBo also extracts effective feature related to pneumonia with a subtle decrease of 0.51%. When KSE is removed, there is a reduction of 1.25% accuracy. (b) For data amount ablation, KoBo has better data robustness with a subtle decrease when training data reduce to 1%. KoBo also has a superior transfer ability with an absolutely better AUC with 1% data than ImageNet with all training data than ImageNet with all training data.Qualitative Analysis: In Fig. 5, our Kobo has learned fine-grained and effective image feature with the fusion of knowledge modeling. The deepest region in the first image gathered on the top left side, showing an obvious expansion on the right lung. There is consistency with the expert annotation and our output logit. The precise location of atelectasis region in CAM of second image and clustering trend interpret for the increase in zero-shot classification."
Knowledge Boosting: Rethinking Medical Contrastive Vision-Language Pre-training,4,Conclusion,"In our paper, we propose a Knowledge-Boosting Contrastive Vision-Language Pre-traing framework (KoBo). Sample and domain knowledge are used to differentiate noisy negative samples and supplement the correspondence between modality and clinical knowledge. Our experiments on eight tasks verify the effectiveness of our framework. We hope that our work will encourage more research on knowledge-granularity alignment in medical vision-language learning."
Knowledge Boosting: Rethinking Medical Contrastive Vision-Language Pre-training,,Fig. 1 .,
Knowledge Boosting: Rethinking Medical Contrastive Vision-Language Pre-training,,Fig. 2 .,
Knowledge Boosting: Rethinking Medical Contrastive Vision-Language Pre-training,,,
Knowledge Boosting: Rethinking Medical Contrastive Vision-Language Pre-training,,Fig. 3 .,
Knowledge Boosting: Rethinking Medical Contrastive Vision-Language Pre-training,,,
Knowledge Boosting: Rethinking Medical Contrastive Vision-Language Pre-training,,Fig. 4 .,
Knowledge Boosting: Rethinking Medical Contrastive Vision-Language Pre-training,,Fig. 5 .,
Knowledge Boosting: Rethinking Medical Contrastive Vision-Language Pre-training,,Encoder Enc G Graph Encoder Enc G Enc Enc Enc Enc Enc Enc Knowledge Semantic Guidance Knowledge Semantic Guidance Knowledge Semantic Enhancement Knowledge Semantic Enhancement LKAG LKAG LSBG LSBG LSKR LSKR LVSR LVSR Global feature vi Global feature vi Local feature Ri Local feature Ri Get Sample Knowledge Get Sample Knowledge Estimate Estimate LSE LSE Domain knowledge feature E Domain knowledge feature E,
Knowledge Boosting: Rethinking Medical Contrastive Vision-Language Pre-training,,Table 1 .,
Knowledge Boosting: Rethinking Medical Contrastive Vision-Language Pre-training,,(b) Ablation on data amount (CheXpert) Performance (ACC%) Performance (AUC%) ImageNet ImageNet 82.75 89.75 93.75 94.99 95.50 81.00 84.00 87.00 90.00 93.00 96.00 Scratch ImageNet w/o KSE w/o KSG Full (a) Ablation on modules (Covidx) Model setting Training data amount KoBo KoBo Better data robustness Superior transfer ability,
Deblurring Masked Autoencoder Is Better Recipe for Ultrasound Image Recognition,1,Introduction,"Recently, as representative of generative self-supervised learning (SSL) methods, masked autoencoder (MAE) [8] has achieved great success in many vision tasks [10,11,24]. In general, MAE belongs to the masked image modeling (MIM) paradigm [29], where some parts of the image are randomly masked, and the purpose of pretraining (i.e., proxy or pretext task) is to recover the missing pixels. After the pretraining, the learned image representation can be transferred to downstream tasks for improved performance. With the advent of MAE, many MAE variants have been proposed [7,22,25]. Tian et al. [22] investigate other image degradation methods during MAE pretraining and find that the optimal practice is enriching masking with spatial misalignment for nature images. Wu et al. [25] design a denoising MAE by introducing Gaussian noising into MAE pretraining, showing that their denoising MAE is robust to additive noises.On the other hand, although numerous work has been proposed for applying MAE to medical imaging across different modalities including pathological images [1,14,19], X-rays [26,31], electrocardiogram [30], immunofluorescence images [15], MRI and CT [4,27,31]. However, the majority of them have not fully exploited the characteristics of medical images and instead, focus on vanilla applications [4,26,30,31]. This is especially problematic given the domain gap between medical and natural images, as well as the unique imaging properties associated with each medical imaging modality [16,18,20]. Furthermore, as an important and widely used medical imaging modality, ultrasound has not been extensively explored in the context of MAE-based approaches.Based on the aforementioned analysis, in this paper, we propose a deblurring masked auto-encoder framework, which is specifically designed for ultrasound image recognition. The primary motivation for the deblurring comes from the unique imaging properties of ultrasound, e.g., high noise-to-signal ratio. Compared with nature images, the subtle details within ultrasound are particularly important for downstream analysis (e.g., microcalcifications is an important sign for malignant nodules, which is represented as tiny bright spots in ultrasound [17,21]). Moreover, the motivation also stems from the findings of our preliminary experiments, which suggest that denoising may not be appropriate for inherently noisy ultrasound images. Therefore, we introduce the opposite direction with a deblurring approach for ultrasound images. Specifically, we first apply blurring operations to the ultrasound images prior to the random masking during pretraining, enabling the model to learn how to de-blur and reconstruct the original image. It should be emphasized that denoising and deblurring are two opposite directions, i.e., denoising first adds noise to the clean image and learns to remove the noise, while deblurring blurs the noisy ultrasound image and learns to sharpen the image. The deblurring facilitates the pretraining in recovering the subtle details within the image, which is crucial for ultrasound image recognition. It should be emphasized that while blurring operation has been shown ineffective for natural images [22], ultrasound images are fundamentally different and may benefit from blurring operation.Furthermore, to the best of our knowledge, this paper is the first attempt to apply the MAE approach to ultrasound image recognition. Our work also addresses some fundamental concerns that are of great interest to the medical imaging community with the example of ultrasound, such as the importance of in-domain data pretraining for MAE in ultrasound, as well as the finding that SSL pretraining is consistently better than the supervised pretraining as with nature images. To conclude, our contributions can be summarized as follows:1. We propose a deblurring MAE framework that is specifically designed for ultrasound images by incorporating a deblurring task into MAE pretraining. This is motivated by the fact that ultrasound images have a high noise-tosignal ratio, and in contrast to denoising for natural images, we demonstrate that deblurring is a better recipe for ultrasound images. 2. We explore the effectiveness of various image blurring methods in our deblurring MAE and find that a simple Gaussian blurring performs the best, showing superior transferability compared with the vanilla MAE. 3. We conduct experiments on more than 10k ultrasound images for pretraining and 4,494 images for downstream thyroid nodule classification. The results demonstrate the effectiveness of the proposed deblurring MAE, achieving state-of-the-art classification performance for ultrasound images.Note that, as a representative MIM approach, the MAE is adopted to validate our proposed deblurring pretraining in this work, our method can also be seamlessly integrated with other MIM-based approaches such as ConvMAE [7]."
Deblurring Masked Autoencoder Is Better Recipe for Ultrasound Image Recognition,2,Method,
Deblurring Masked Autoencoder Is Better Recipe for Ultrasound Image Recognition,2.1,Preliminary: MAE,"The MAE pipeline consists of two primary stages: self-supervised pretraining and transferring for downstream tasks. During the self-supervised pretraining, the model is trained to reconstruct masked input image patches using an asymmetric encoder-decoder architecture. The encoder is typically a ViT [6], which compresses the input image into a latent representation, while the decoder is a lightweight Transformer that reconstructs the original image from the latent representation. The loss used during pretraining is the mean squared error (MSE) between the reconstructed and original images. In the transfer stage, the weights of the pre-trained ViT encoder are transferred and used as a feature extractor, to which task-specific heads are appended for learning various downstream tasks. Typically, there are two common practices in the transfer stage: 1) end-to-end fine-tuning which tunes the entire model, and 2) linear probing, which only tunes the task-specific head."
Deblurring Masked Autoencoder Is Better Recipe for Ultrasound Image Recognition,2.2,Our Proposed Deblurring MAE,"Similar to MAE, our proposed deblurring MAE also contains pretraining and transfer learning for downstream tasks. We employ the same asymmetric encoder-decoder architecture as the original MAE.Deblurring MAE Pretraining. For the pretraining, besides the original masked image modeling task in the MAE, we introduce one additional task, i.e., deblurring, into the pretraining thus making the pretraining as deblurring pretraining. As shown in Fig. 1, the deblurring is achieved by simply inserting an image blurring operation prior to random masking. The pipeline of our deblurring MAE pretraining is illustrated in Eq. 1:Specifically, the original ultrasound image x is first blurred by a chosen image blurring operation Blurring to obtain x b . After that, several patches in the blurred image x b are randomly masked by the Masking operation with a predefined ratio to obtain x m b . Next, the masked blurred image x m b is passed as input to the ViT Encoder, which generates a latent representation h. Finally, the Decoder receives the representation h and outputs reconstructed image x.The image blurring operation Blurring is a commonly used technique for reducing the sharpness or details of an image, resulting in a smoother, lessdetailed appearance. There exist many different methods for image blurring, with most of them involving the averaging of neighboring pixels in some way. In Fig. 1, we provide examples of two representative blurring methods: Gaussian blur and speckle reducing anisotropic diffusion (SRAD) [28].Gaussian blur involves convolving an input image with a Gaussian kernel G(σ), which is a two-dimensional Gauss function that represents a normal distribution with standard deviation of σ. Mathematically, Gaussian blur can be defined as follows:where * denotes the convolution operation, and (u, v) represents the coordinates in the kernel. The degree of blurring (i.e., blurriness) in the resulting image is determined by the standard deviation σ.The SRAD is a nonlinear anisotropic diffusion technique for removing speckled noises, which has been extensively used in medical ultrasound images, due to its edge-sensitivity for speckled images and powerful preservation of useful information. The SRAD operation is implemented by repeating an anisotropic diffusion equation for N iterations. It can be formally given as:where x is the original image, N stands for the number of iterations, t means time. x(i, j, k) and c(i, j, k) represent the image and diffusivity coefficient at iteration k, respectively. ∇x is the gradient of x and div is the divergence operator. The larger N or t leads to a blurrier resulting image. The pixel-wise MSE between the reconstructed image x and the original image x is utilized as the loss function during pretraining: It should be noted that a key difference from MAE is that we compute the loss across all patches, including the masked ones. This operation is necessary due to the fact that our blurring operation covers the entire image. Through the use of the proposed deblurring MAE pretraining, we aim to leverage both masked image modeling and deblurring in order to learn a robust and effective latent representation that could be successfully applied to a range of downstream tasks.Deblurring MAE Transfer. After the deblurring MAE pretraining, only the pre-trained encoder is transferred to the downstream thyroid nodule classification task. One multi-layer perceptron (MLP) head is appended after the pretrained encoder. The transfer learning pipeline is shown in Eq. 4:It should be noted here that, in order to prevent data distribution shift between pretraining and transfer stages, the original image x also needs to be blurred before fed into the pre-trained encoder during transfer learning. The crossentropy loss between ground-truth classification label y and predicted label ŷ is used as the loss function:"
Deblurring Masked Autoencoder Is Better Recipe for Ultrasound Image Recognition,3,Experiments and Results,
Deblurring Masked Autoencoder Is Better Recipe for Ultrasound Image Recognition,3.1,Experimental Settings,"Dataset. All thyroid ultrasound images used in our study for both pretraining and downstream classification were acquired at West China Hospital with ethical approval. We use a total of 10,675 images for pretraining and 4,493 images for the downstream classification. To avoid any potential data leakage, the images used in pretraining were not included in the test set for thyroid nodule classification. The downstream classification dataset contains 2,576 benign and 1,917 malignant cases. We randomly split the dataset into train/validation/test subsets with a 3:1:1 ratio. The classification ground-truth labels were obtained either from the fine-needle aspiration for malignant nodules or clinical diagnosis by senior radiologists for benign nodules. Implementation Details. We use a mask ratio of 75% during the pretraining. We set the batch size to 256 for both pretraining and end-to-end fine-tuning, and 1024 for linear probing. The epochs of pretraining is 12,000 due to our relatively small data. The full detailed experimental settings are presented in the appendix. We implement our approach based on PyTorch. The image size for both pretraining and transfer learning is 224 × 224. For classification, we choose the model that performs the best on the validation set as the final model to evaluate on the test set. Three widely used metrics accuracy (ACC), F1-score (F1), and the area under the receiver operating characteristic (AUROC) are utilized for classification performance evaluation."
Deblurring Masked Autoencoder Is Better Recipe for Ultrasound Image Recognition,3.2,Results and Comparisons,"Our Deblurring MAE vs. Vanilla MAE. First of all, in order to evaluate the effectiveness of the proposed deblurring MAE for ultrasound images, we compare the transfer learning performance between our deblurring MAE and the vanilla MAE. Table 1 and Fig. 2 give the classification performance comparison of these two approaches. For our deblurring MAE, we use Gaussian blurring with σ equal to 1.1 as the blurring operation. In Fig. 2, we report the experimental results of three models: ViT-Base (ViT-B), ViT-Large (ViT-L) and ViT-Huge (ViT-H), and two transfer learning paradigms: end-to-end fine-tuning and linear probing. As shown in the figure, both the fine-tuning and linear probing performance of our proposed deblurring MAE is consistently better than that of the vanilla MAE, which indicates the effectiveness of deblurring for enhancing the transferability of learned representations during ultrasound pretraining.Comparison with State-of-the-Art Approaches. Secondly, we also compare our approach with more approaches and the results are listed in Table 1.We implement two variants of our deblurring MAE which differ in blurring operation: the SRAD with N equals to 40 and t equals to 0.1, and the Gaussian blur with σ equals to 1.1. We compare with methods based on supervised learning or self-supervised learning. In addition, we still add the denoising MAE for comparison, although it has proved to be ineffective for ultrasound images based on our preliminary experiments. We adopt ViT-B as the architecture for these SSL-based methods except SimCLR [2] which uses ResNet-50, and we use two types of data for pretraining, i.e., ImageNet [5] and ultrasound. The results are based on end-to-end fine-tuning. According to Table 1, we can draw the following conclusions:The Deblurring MAE Pretraining Can Improve the Transferability of Learned Representations. First of all, both the two variants of our proposed approach (Ours [SRAD] and Ours [Gaussian]) obtain much higher classification metrics compared with the MAE pretrained using ultrasound, which indicates the learned representation of our deblurring MAE is more effective than the vanilla MAE when transferred to downstream classification. In addition, Table 1 also shows that the performance of our proposed deblurring MAE with Gaussian blurring achieves state-of-the-art performance in terms of all metrics, surpassing all competing SSL or supervised-based approaches, which further demonstrates the superior performance of our proposed deblurring MAE. It is noteworthy that, as the opposite approach to our deblurring MAE, the denoising MAE obtains worse performance compared with the vanilla MAE, suggesting that adding noise to ultrasound images during MAE pretraining is unfavorable.Ultrasound Pretraining is Better than ImageNet Pretraining, Better than Supervised Pretraining. Table 1 shows that the performance of MAE with ultrasound pretraining is better than the ImageNet pretraining, which underlines the importance of in-domain self-supervised pretraining in MAE. In contrast to MAE, our experiments show that the MoCo v3 [3] achieves only marginal improvement with ultrasound pretraining. Furthermore, the MAE Ima-geNet pretraining also performs much better than the ImageNet supervised pretraining. These two conclusions are consistent with other works [8,26]. Ablation Study. We design two sets of ablation studies, i.e., different image blurring methods, and the degree of blurring (blurriness) used in our deblurring MAE. We adopt the ViT-B as the architecture and end-to-end fine-tuning in transfer learning for the ablation experiments. Table 2 presents the performance results of the ablation study, where the 'Baseline' represents the vanilla MAE. Firstly, besides the Gaussian and SRAD, we also try several other blurring methods that are commonly used in the fields including mean, median, motion and defocus blur. We set the kernel size to 5 in mean, median and motion blur, and the radius of defocusing is set to 5 for defocus blur. The performance results are presented on the left side of Table 2. From this table, we can observe that the Gaussian blurring achieves the best F1. And these six blurring methods are not all beneficial for pretraining, where some of them (motion, defocus) perform even worse than the baseline. Secondly, to investigate the effect of blurriness on the pretraining, we conduct ablation experiments on blurriness based on Gaussian blurring. The right side of Table 2 reports the performance results and we can see that the σ with 1.1 obtains the highest F1. In addition, as the σ, i.e., the blurriness continues to increase, the performance drops rapidly, which indicates that only a limited range of blurriness has a positive effect on the pretraining.Hyper-parameter Choices for MAE Pretraining. We conduct experiments to explore hyper-parameter choices for MAE pretraining based on ViT-B, and the results are presented in Fig. 3. Our findings indicate that a masking ratio of 75% and a patch size of 16 achieve the best transfer performance, which is consistent with MAE for natural images [8]. Additionally, we observed that transfer performance improves with an increase in pre-trained images, surpassing ImageNet transfer only when a substantial amount of pre-trained images is used.Visualization. The comparisons of reconstructed image examples among MAE, denoising MAE, and our proposed deblurring MAE are illustrated in Fig. 4. Although there is no strong evidence that reveals the relationship between reconstruction quality in pretraining and downstream task performance in MAE-based approaches, we can still obtain some insights from the reconstruction quality. As shown in Fig. 4, we can clearly observe that the reconstructed images of the denoising MAE are the smoothest and lost most details among all the three approaches, followed by the vanilla MAE, and our deblurring MAE achieves the best reconstruction quality with much finer details. The comparisons indicate that our deblurring MAE can capture critical details that are beneficial for downstream classification. More comparisons can be found in the appendix."
Deblurring Masked Autoencoder Is Better Recipe for Ultrasound Image Recognition,4,Conclusion and Future Work,"In this paper, we propose a novel deblurring MAE by incorporating deblurring into the proxy task during MAE pretraining for ultrasound image recognition. The deblurring task is implemented by inserting image blurring operation prior to the random masking during pretraining. The integration of deblurring enables the pretraining pay more attention to recovering the intricate details presented in ultrasound images, which are critical for downstream image classification. We explore the effect of several different image blurring methods and find that Gaussian blurring achieves the best performance and only a limited range of blurriness has a beneficial effect for pretraining. Based on the optimal blurring method and blurriness, our deblurring MAE achieves state-of-the-art performance in the downstream classification of ultrasound images, indicating the effectiveness of incorporating deblurring into MAE pretraining for ultrasound image recognition. However, this work has some limitations. For example, only one downstream task: nodule classification is evaluated in this study. We plan to extend our approach to include more tasks such as segmentation in the future."
Deblurring Masked Autoencoder Is Better Recipe for Ultrasound Image Recognition,,Fig. 1 .,
Deblurring Masked Autoencoder Is Better Recipe for Ultrasound Image Recognition,,Fig. 3 .,
Deblurring Masked Autoencoder Is Better Recipe for Ultrasound Image Recognition,,Fig. 4 .,
Deblurring Masked Autoencoder Is Better Recipe for Ultrasound Image Recognition,,Table 1 .,
Deblurring Masked Autoencoder Is Better Recipe for Ultrasound Image Recognition,,Table 2 .,
Deblurring Masked Autoencoder Is Better Recipe for Ultrasound Image Recognition,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0 34.
Black-box Domain Adaptative Cell Segmentation via Multi-source Distillation,1,Introduction,"Semantic segmentation plays a vital role in pathological image analysis. It can help people conduct cell counting, cell morphology analysis, and tissue analysis, which reduces human labor [19]. However, data acquisition for medical images poses unique challenges due to privacy concerns and the high cost of manual annotation. Moreover, pathological images from different tissues or cancer types often show significant domain shifts, which hamper the generalization of models trained on one dataset to others. Due to the abovementioned challenges, some researchers have proposed various white-box domain adaptation methods to address these issues.Recently, [8,16] propose to use generative adversarial networks to align the distributions of source and target domains and generate source-domain lookalike outputs for target images. Source-free domain adaptation methods have been also widely investigated due to the privacy protection. [3,5,14] explore how to implicitly align target domain data with the model trained on the source domain without accessing the source domain data. There are also many studies on multi-source white-box domain adaptation. Ahmed et al. [1] propose a novel algorithm which automatically identifies the optimal blend of source models to generate the target model by optimizing a specifically designed unsupervised loss. Li et al. [13] extend the above work to semantic segmentation and proposed a method named model-invariant feature learning, which takes full advantage of the diverse characteristics of the source-domain models.Nonetheless, several recent investigations have demonstrated that the domain adaptation methods for source-free white-box models still present a privacy risk due to the potential leakage of model parameters [4]. Such privacy breaches may detrimental to the privacy protection policies of hospitals. Moreover, the target domain uses the same neural network as the source domain, which is not desirable for low-resource target users like hospitals [15]. We thus present a more challenging task of relying solely on black-box models from vendors to avoid parameter leakage. In clinical applications, various vendors can offer output interfaces for different pathological images. While black-box models are proficient in specific domains, their performances greatly degrade when the target domain is updated with new pathology slices. Therefore, how to leverage the existing knowledge of black-box models to effectively train new models for the target domain without accessing the source domain data remains a critical challenge.In this paper, we present a novel source-free domain adaptation framework for cross-tissue cell segmentation without accessing both source domain data and model parameters, which can seamlessly integrate heterogeneous models from different source domains into any cell segmentation network with high generality. To the best of our knowledge, this is the first study on the exploration of multi-source black-box domain adaptation for cross-tissue cell segmentation. In this setting, conventional multi-source ensemble methods are not applicable due to the unavailability of model parameters, and simply aggregating the black-box outputs would introduce a considerable amount of noise, which can be detrimental to the training of the target domain model. Therefore, we develop two strategies within this new framework to address this issue. Firstly, we propose a pixel-level multi-source domain weighting method, which reduces source domain noise by knowledge weighting. This method effectively addresses two significant challenges encountered in the analysis of cellular images, namely, the uncertainty in source domain output and the ambiguity in cell boundary semantics. Secondly, we also take into account the structured information from cells to images, which may be overlooked during distillation, and design an adaptive knowledge voting strategy. This strategy enables us to ignore low-confidence regions, similar to Cutout [6], but with selective masking of pixels, which effectively balances the trade-off between exploiting similarities and preserving differences of different domains. As a result, we refer to the labels generated through the voting strategy as pseudo-cutout labels."
Black-box Domain Adaptative Cell Segmentation via Multi-source Distillation,2,Method,"Overview: Figure 1 shows a binary cell segmentation task with three source models trained on different tissues and a target model, i.e., the student model in Fig. 1. We only use the source models' predictions on the target data for knowledge transfer without accessing the source data and parameters. The η and η indicate that different perturbations are added to the target images. Subsequently, we feed the perturbed images into the source domain predictor to generate the corresponding raw segmentation outputs. These outputs are then processed by two main components of our framework: a pixel-level weighting method that takes into account the prediction uncertainty and cell boundary ambiguity, and an adaptive knowledge voter that utilizes confidence gates and a dynamic ensemble strategy. These components we designed are to extract reliable knowledge from the predictions of source domain models and reduce noise during distillation. Finally, we obtain a weighted logit for knowledge distillation from pixel level and a high-confidence pseudo-cutout label for further structured distillation from cell to global pathological image."
Black-box Domain Adaptative Cell Segmentation via Multi-source Distillation,,Knowledge Distillation by Weighted Logits Map:,"We denote D N S = {X s , Y s } N as a collection of N source domains and D T = {X i t , Y j t } as single target domain, where the number of labeled instances Y j t X i t . We are only provided with black-box models {f n s } N n=1 trained on multiple source domains {x i s , y i s } N n=1 for knowledge transfer. The parameters {θ n s } N n=1 of these source domain predictors are not allowed to participate in gradient backpropagation as a result of the privacy policy. Thus, our ultimate objective is to derive a novel student model f t : X t → Y t that is relevant to the source domain task. Accordingly, direct knowledge transfer using the output of the source domain predictor may lead to feature bias in the student model due to the unavoidable covariance [20] between the target and source domains. Inspired by [21], we incorporate prediction uncertainty and cell boundary impurity to establish pixel-level weights for multi-source outputs. We assume that k-square-neighbors of a pixel as a cell region, i.e., for a logits map with height H and width W , we define the region as follow:where (i, j) denotes centre of region, and k denotes the size of k-square-neighbors. Firstly, we develop a pixel-level predictive uncertainty algorithm to aid in assessing the correlation between multiple source domains and the target domain. For a given target image x t ∈ X i t , we initially feed it into the source predictors {f n s } N n=1 to obtain their respective prediction {p n s } N n=1 . To leverage the rich semantic information from the source domain predictor predictions, we utilize predictive entropy of the softmax outputs to measure the prediction uncertainty scores. In the semantic segmentation scenario of C-classes classification, we define the pixel-level uncertainty score U (i,j) n as follow:where O n s denotes softmax output,i.e.,O n s = softmax(p n s ) from nth source predictor.Due to the unique characteristics of cell morphology, merely relying on uncertainty information is insufficient to produce high-quality ensemble logits map that accurately capture the relevance between the source and target domains. The target pseudo-label for the nth predictor f n s can be obtained by applying the softmax function to the output and selecting the category with the highest probability score, i.e., Y t = arg max c∈{1,...,C} (softmax(p n s )). Then according to C-classes classification tasks, we divide the cell region into C subsets,After that, we determine the degree of impurity in an area of interest by analyzing the statistics of the boundary region, which represents the level of semantic information ambiguity. Specifically, the number of different objects within the area is considered a proxy for its impurity level, with higher counts indicating higher impurity.The boundary impurity P (i,j) can be calculated as:where | • | denotes the number of pixels in the area. By assigning lower weights to the pixels with high uncertainty and boundary ambiguity, we can obtain pixel-level weight scores W n for each p n s , i.e.,where denotes element-wise matrix multiplication. According to the pixellevel weight, we will obtain an ensemble logits map M = N n=1 W n • p n s . And the object of the knowledge distillation is a classical regularization term [9]:where D kl denotes the Kullback-Leibler (KL) divergence loss.Adaptive Pseudo-Cutout Label: As previously mentioned, the outputs from the source domain black-box predictors have been adjusted by the pixel-level weight. However, they are still noisy and only pixel-level information is considered while ignoring structured information in the knowledge distillation process. Thus, we utilize the output of the black-box predictor on the target domain to produce an adaptive pseudo-cutout label, which will be employed to further regularize the knowledge distillation process. We have revised the method in [7] to generate high-quality pseudo labels that resemble the Cutout augmentation technique. For softmax outputs {O n s } N n=1 from N source predictors, we first set a threshold α to filter low-confidence pixels. To handle pixels with normalized probability values below the threshold, we employ a Cutout-like operation and discard these pixels. Subsequently, we apply an adaptive voting strategy to the N source domain outputs. Initially, during the training of the target model, if at least one source domain output exceeds the threshold, we consider the pixel as a positive or negative sample, which facilitates rapid knowledge acquisition by the model. As the training progresses, we gradually tighten the voting strategy and only retain regional pixels that have received adequate votes. The strategy can be summarised as follow:where α is empirically set as 0.9.Then we will aggregate the voting scores, i.e., V (i,j) = N n=1 V (i,j) n and determine whether to retain each pixel using an adaptive vote gate G ∈ {1, 2, 3, etc.}. By filtering with a threshold and integrating the voting strategy, we generate high-confidence pseudo-labels that remain effective even when the source and target domains exhibit covariance. Finally, we define the ensemble result as a pseudo-cutout label Ps and employ consistency regularization as below:where l ce denotes cross-entropy loss function.Loss Functions: Finally, we incorporate global structural information about the predicted outcome of the target domain into both distillation and semisupervised learning. To mitigate the noise effect of the source domain predictors, we introduce maximize mutual information targets to facilitate discrete representation learning by the network. We define E(p) =i p i log p i as conditional entropy. The object can be described as follow:where the increasing H(Y t ) and the decreasing H(Y t |X t ) help to balances class separation and classifier complexity [15]. We adopt the classical and effective mean-teacher framework as a baseline for semi-supervised learning and update the teacher model parameters by exponential moving average. Also, we apply two different perturbations (η, η ) to the target domain data and feed them into the student model and the mean-teacher model respectively. The consistency loss of unsupervised learning can be defined as below:Finally, we get the overall objective:where L sup denotes the ordinary cross-entropy loss for supervised learning and we set the weight of each loss function to 1 in the training."
Black-box Domain Adaptative Cell Segmentation via Multi-source Distillation,3,Experiments,"Dataset and Setting: We collect four pathology image datasets to validate our proposed approach. Firstly, we acquire 50 images from a cohort of patients with Triple Negative Breast Cancer (TNBC), which is released by Naylor et al [18]. Hou et al. [10] publish a dataset of nucleus segmentation containing 5,060 segmented slides from 10 TCGA cancer types. In this work, we use 98 images from invasive carcinoma of the breast (BRCA). We have also included 463 images of Kidney Renal Clear cell carcinoma (KIRC) in our dataset, which are made publicly available by Irshad et al [11]. Awan et al. [2] publicly release a dataset containing tissue slide images and associated clinical data on colorectal cancer (CRC), from which we randomly select 200 patches for our study. In our experiments, we transfer knowledge from three black-box models trained on different source domains to a new target domain model (e.g.,from CRC, TNBC, KIRC to BRCA). The backbone network for the student model and source domain black-box predictors employ the widely adopted residual U-Net [12], which is commonly used for medical image segmentation. For each source domain network, we conduct full-supervision training on the corresponding source domain data and directly evaluate its performance on target domain data. The upper performance metrics (Source-only upper) are shown in the Table 1. To ensure the reliability of the results, we use the same data for training, validation, and testing, which account for 80%, 10%, and 10% of the original data respectively. For the target domain network, we use unsupervised and semi-supervised as our task settings respectively. In semi-supervised domain adaptation, we only use 10% of the target domain data as labeled data.Experimental Results: To validate our method, we compare it with the following approaches: (1) CellSegSSDA [8], an adversarial learning based semisupervised domain adaptation approach. (2) US-MSMA [13], a multi-source model domain aggregation network. (3) SFDA-DPL [5], a source-free unsupervised domain adaptation approach. (4) BBUDA [17], an unsupervised black-box model domain adaptation framework. A point worth noting is that most of the methods we compared with are white-box methods, which means they can obtain more information from the source domain than us. For single-source domain adaptation approach, CellsegSSDA and SFDA-DPL, we employ two strategies to ensure the fairness of the experiments: (1) single-source, i.e. performing adaptation on each single source, where we select the best results to display in the Table 1; (2) source-combined, i.e. all source domains are combined into a traditional single source. The Table 1 and Fig. 2 demonstrate that our proposed method exhibits superior performance, even when compared to these white-box methods, surpassing them in various evaluation metrics and visualization results.In addition, the experimental results also show that simply combining multiple source data into a traditional single source will result in performance degradation in some cases, which also proves the importance of studying multi-source domain adaptation methods.Ablation Study: To evaluate the impact of our proposed methods of weighted logits(WL), pseudo-cutout label(PCL) and maximize mutual information(MMI) on the model performance, we conduct an ablation study. We compare the baseline model with the models that added these three methods separately. We chose CRC, KIRC and BRCA as our source domains, and TNBC as our target domain. The results of these experiments, presented in the Table 2, show that our proposed modules are indeed useful."
Black-box Domain Adaptative Cell Segmentation via Multi-source Distillation,4,Conclusion,"Our proposed multi-source black-box domain adaptation method achieves competitive performance by solely relying on the source domain outputs, without the need for access to the source domain data or models, thus avoiding information leakage from the source domain. Additionally, the method does not assume the same architecture across domains, allowing us to learn lightweight target models from large source models, improving learning efficiency. We demonstrate the effectiveness of our method on multiple public datasets and believe it can be readily applied to other domains and adaptation scenarios. Moving forward, we plan to integrate our approach with active learning methods to enhance annotation efficiency in the semi-supervised setting. By leveraging multi-source domain knowledge, we aim to improve the reliability of the target model and enable more efficient annotation for better model performance."
Black-box Domain Adaptative Cell Segmentation via Multi-source Distillation,,Fig. 1 .,
Black-box Domain Adaptative Cell Segmentation via Multi-source Distillation,,Fig. 2 .,
Black-box Domain Adaptative Cell Segmentation via Multi-source Distillation,,Table 1 .,
Black-box Domain Adaptative Cell Segmentation via Multi-source Distillation,,Table 2 .,
Additional Positive Enables Better Representation Learning for Medical Images,1,Introduction,"Self-supervised learning (SSL) has been extremely successful in learning good image representations without human annotations for medical image applications like classification [1,23,29] and segmentation [2,4,16]. Usually, an encoder is pre-trained on a large-scale unlabeled dataset. Then, the pre-trained encoder is used for efficient training on downstream tasks with limited annotation [19,24]. Recently, contrastive learning has become the state-of-the-art (SOTA) SSL method due to its powerful learning ability. A recent contrastive learning method learns by pulling the representations of different augmented views of the same image (a.k.a positive pair) together and pushing the representation of different images (a.k.a negative pair) apart [6]. The main disadvantage of this method is its heavy reliance on negative pairs, making it necessary to use a large batch size [6] or memory banks [15] to ensure effective training. To overcome this challenge, BYOL [12] proposes two siamese neural networks -the online and target networks. The online network is trained to predict the target network representation of the same image under a different augmented view, requiring only one positive pair per sample. This approach makes BYOL more resilient to batch size and the choice of data augmentations.As the positive pair in BYOL is generated from the same image, the diversity of features within the positive pair could be quite limited. For example, one skin disease may manifest differently in different patients or locations, but such information is often overlooked in the current BYOL framework. In this paper, we argue that such feature diversity can be increased by adding additional positive pairs from other samples with the same label (a.k.a. True Positives). Identifying such pairs without human annotation is challenging because of the unrelated information in medical images, such as the background normal skin areas in dermoscopic images. One straightforward way to detect positive pairs is using feature similarity: two images are considered positive if their representations are close to each other in the feature space. However, samples with different labels might also be close in the feature space because the learned encoder is not perfect. Considering them as positive might further pull them together after learning, leading to degraded performance.To solve this problem, we propose BYOL-TracIn, which improves vanilla BYOL using the TracIn influence function. Instead of quantifying the similarity of two samples based on feature similarity, we propose using TracIn to estimate their similarity by calculating the impact of training one sample on the other. TracIn [22] is a gradient-based influence function that measures the loss reduction of one sample by the training process of another sample. Directly applying TracIn in BYOL is non-trivial as it requires the gradient of each sample and careful selection of model checkpoints and data augmentations to accurately estimate sample impacts without labels. To avoid per-sample gradient computation, we introduce an efficient method that computes the pairwise TracIn in a mini-batch with only one forward pass. For each image in the mini-batch, the sample from other images with the highest TracIn values is selected as the additional positive pair. Their representation distance is then minimized using BYOL loss. To enhance positive selection accuracy, we propose to use a pre-trained model for pairwise TracIn computation as it focuses more on task-related features compared to an on-the-fly model. Light augmentations are used on the samples for TracIn computation to ensure stable positive identification. To the best of our knowledge, we are the first to incorporate additional positive pairs from different images in BYOL. Our extensive empirical results show that our proposed method outperforms other competing approaches in both semi-supervised and transfer learning settings for medical image classification tasks."
Additional Positive Enables Better Representation Learning for Medical Images,2,Related Work,"Self-supervised Learning. Most SSL methods can be categorized as either generative [10,28] or discriminative [11,21], in which pseudo labels are automatically generated from the inputs. Recently, contrastive learning [6,15,27] as a new discriminative SSL method has dominated this field because of its excellent performance. SimCLR [6] and MoCo [15] are two typical contrastive learning methods that try to attract positive pairs and repulse negative pairs. However, these methods rely on a large number of negative samples to work well. BYOL [12] improves contrastive learning by directly predicting the representation output from another view and achieves SOTA performance. As such, only positive pairs are needed for training. SimSiam [7] further proves that stop-gradient plays an essential role in the learning stability of siamese neural networks. Since the positive pairs in BYOL come from the same image, the feature diversity from different images of the same label is ignored. Our method introduces a novel way to accurately identify such positive pairs and attract them in the feature space.Influence Function. The influence function (IF) was first introduced to machine learning models in [20] to study the following question: which training points are most responsible for a given prediction? Intuitively, if we remove an important sample from the training set, we will get a large increase in the test loss. IF can be considered as an interpretability score that measures the importance of all training samples on the test sample. Aside from IF, other types of scores and variants have also been proposed in this field [3,5,14]. Since IF is extremely computationally expensive, TracIn [22] was proposed as an efficient alternative to estimate training sample influence using first-order approximation. Our method extends the normal TracIn to the SSL setting (i.e., BYOL) with a sophisticated positive pair selection schema and an efficient batch-wise per-sample gradient computation method, demonstrating that aside from model interpretation, TracIn can also be used to guide SSL pre-training."
Additional Positive Enables Better Representation Learning for Medical Images,3,Method,
Additional Positive Enables Better Representation Learning for Medical Images,3.1,Framework Overview,"Our BYOL-TracIn framework is built upon classical BYOL method [12]. Figure 1 shows the overview of our framework. Here, we use x 1 as the anchor sample for an explanation, and the same logic can be applied to all samples in the mini-batch. Unlike classical BYOL where only one positive pair (x 1 and x 1 ) generated from the same image is utilized, we use the influence function, TracIn, to find another sample (x 3 ) from the batch that has the largest impact on the anchor sample.During training, the representations distance of x 1 and x 3 will also be minimized. We think this additional positive pair can increase the variance and diversity of the features of the same label, leading to better clustering in the feature space and improved learning performance. The pairwise TracIn matrix is computed using first-order gradient approximation which will be discussed in the next section. For simplicity, this paper only selects the top-1 additional sample, but our method can be easily extended to include top-k (k > 1) additional samples."
Additional Positive Enables Better Representation Learning for Medical Images,3.2,Additional Positive Selection Using TracIn,"Idealized TracIn and Its First-order Approximation. Suppose we have a training dataset D = {x 1 , x 2 , ..., x n } with n samples. f w (•) is a model with parameter w ∈ R, and (w, x i ) is the loss function when model parameter is w and training example is x i . The training process in iteration t can be viewed as minimizing the training loss (w t , x t ) and updating parameter w t to w t+1 using gradient descent (suppose only x t ∈ D is used for training in each iteration). Then the idealized TracIn of one sample x i on another sample x k can be defined as the total loss reduction by training x i in the whole training process.where T is the total number of iterations. If stochastic gradient descent is utilized as the optimization method, we can approximately express the loss reduction after iteration t asThe parameter change in iteration t is Δw t = w t+1w t = -η t (w t , x t ), in which η t is the learning rate in iteration t, and x t is the training example. Since η t is usually small during training, we can ignore the high order term O(||Δw t || 2 ), and the first-order TracIn can be formulated as:The above equation reveals that we can estimate the influence of x i on x k by summing up their gradient dot products across all training iterations. In practical BYOL training, the optimization is usually done on mini-batches, and it is impossible to save the gradients of a sample for all iterations. However, we can use the TracIn of the current iteration to represent the similarity of two samples in the mini-batch because we care about the pairwise relative influences instead of the exact total values across training. Intuitively, if the TracIn of two samples is large in the current iteration, this means that the training of one sample can benefit the other sample a lot because they share some common features. Therefore, they are similar to each other.Efficient Batch-wise TracIn Computation. Equation 2 requires the gradient of each sample in the mini-batch for pairwise TracIn computation. However, it is prohibitively expensive to compute the gradient of samples one by one. Moreover, calculating the dot product of gradients on the entire model is computationally and memory-intensive, especially for large deep-learning models where there could be millions or trillions of parameters. Therefore, we work with the gradients of the last linear layer in the online predictor.As current deep learning frameworks (e.g., Pytorch and TensorFlow) do not support per-sample gradient when the batch size is larger than 1, we use the following method to efficiently compute the per-sample gradient of the last layer. Suppose the weight matrix of the last linear layer is W ∈ R m×n , where m and n are the numbers of input and output units. f (q) = 2 -2 • q, z /( q 2 • z 2 ) is the standard BYOL loss function, where q is the online predictor output (a.k.a., logits) and z is the target encoder output that can be viewed as a constant during training. We have q = W a, where a is the input to the last linear layer. According to the chain rule, the gradient of the last linear layer can be computed as W f (q) = q f (q)a T , in which the gradient of the logits can be computed by:Therefore, the TracIn of sample x i and x k at iteration t can be computed as:Equation 3 and 4 tell us that the per-sample gradient of the last linear layer can be computed by using the inputs of this layer and the gradient of the output logits for each sample, which can be achieved with only one forward pass on the mini-batch. This technique greatly speeds up the TracIn computation and makes it possible to be used in BYOL.Using Pre-trained Model to Increase True Positives. During the pretraining stage of BYOL, especially in the early stages, the model can be unstable and may focus on unrelated features in the background instead of the target features. This can result in the selection of wrong positive pairs while using TracIn. For example, the model may identify all images with skin diseases on the face as positive pairs, even if they are from different diagnostics, as it focuses on the face feature instead of the diseases. To address this issue, we suggest using a pre-trained model to select additional positives with TracIn to guide BYOL training. This is because a pre-trained model is more stable and well-trained to focus on the target features, thus increasing the selected true positive ratio."
Additional Positive Enables Better Representation Learning for Medical Images,4,Experiments and Results,
Additional Positive Enables Better Representation Learning for Medical Images,4.1,Experimental Setups,"Datasets. We evaluate the performance of the proposed BYOL-TracIn on four publicly available medical image datasets. (1) ISIC 2019 dataset is a dermatology dataset that contains 25,331 dermoscopic images among nine different diagnostic categories [8,9,25]. (2) ISIC 2016 dataset was hosted in ISBI 2016 [13]. It contains 900 dermoscopic lesion images with two classes benign and malignant. (3) ChestX-ray dataset is a chest X-ray database that comprises 108,948 frontal view X-ray images of 32,717 unique patients with 14 disease labels [26]. Each image may have multiple labels. (4) Shenzhen dataset is a small chest X-ray dataset with 662 frontal chest X-rays, of which 326 are normal cases and 336 are cases with manifestations of Tuberculosis [18].Training Details. We use Resnet18 as the backbone. The online projector and predictor follow the classical BYOL [12], and the embedding dimension is set to 256. On both ISIC 2019 and ChestX-ray datasets, we resize all the images to 140×140 and then crop them to 128×128. Data augmentation used in pretraining includes horizontal flipping, vertical flipping, rotation, color jitter, and cropping. For TracIn computation, we use one view with no augmentation and the other view with horizontal flipping and center cropping because this setting has the best empirical results in our experiments. We pre-train the model for 300 epochs using SGD optimizer with momentum 0.9 and weight decay 1 × e -5 . The learning rate is set to 0.1 for the first 10 epochs and then decays following a concise learning rate schedule. The batch size is set to 256. The moving average decay of the momentum encoder is set to 0.99 at the beginning and then gradually updates to 1 following a concise schedule. All experiments are performed on one NVIDIA GeForce GTX 1080 GPU.Baselines. We compare the performance of our method with a random initialization approach without pre-training and the following SOTA baselines that involve pre-training. (1) BYOL [12]: the vanilla BYOL with one positive pair from the same image. (2) FNC [17]: a false negative identification method designed to improve contrastive-based SSL framework. We adapt it to BYOL to select additional positives because false negatives are also equal to true positives for a particular anchor sample. (3) FT [30]: a feature transformation method used in contrastive learning that creates harder positives and negatives to improve the learning ability. We apply it in BYOL to create harder virtual positives. (4) FS: using feature similarity from the current mini-batch to select the top-1 additional positive. ( 5) FS-pretrained: different from the FS that uses the current Table 1. Comparison of all methods on ISIC 2019 and ChestX-ray datasets in the semisupervised setting. We also report the fine-tuning results on 100% datasets. BYOL-Sup is the upper bound of our method. BMA represents the balanced multiclass accuracy. model to compute the feature similarity on the fly, we use a pre-trained model to test whether a well-trained encoder is more helpful in identifying the additional positives. ( 6) BYOL-Sup: the supervised BYOL in which we randomly select one additional positive from the mini-batch using the label information. This baseline is induced as the upper bound of our method because the additional positive is already correct. We evaluate two variants of our method, BYOL-TracIn and BYOL-TracIn-pretrained. The former uses the current training model to compute the TracIn for each iteration while the latter uses a pre-trained model. For a fair comparison, all methods use the same pre-training and finetuning setting unless otherwise specified. For FS-pretrained and BYOL-TracIn-pretrained, the pre-trained model uses the same setting as BYOL. Note that this pre-trained model is only used for positive selection and not involves in training."
Additional Positive Enables Better Representation Learning for Medical Images,4.2,Semi-supervised Learning,"In this section, we evaluate the performance of our method by finetuning with the pre-trained encoder on the same dataset as pre-training with limited annotations. We sample 10% or 50% of the labeled data from ISIC 2019 and ChestXray training sets and finetune the model for 100 epochs on the sampled datasets. Data augmentation is the same as pre-training. Table 1 shows the comparisons of all methods. For ISIC 2019, we report the balanced multiclass accuracy (BMA, suggested by the ISIC challenge). For ChestX-ray, we report the average AUC across all diagnoses. We conduct each finetuning experiment 5 times with different random seeds and report the mean and std.From Table 1, we have the following observations: (1) Compared to Random, all the other methods have better accuracy, which means that pre-training can indeed help downstream tasks.   pervised baselines. Although BYOL-TracIn can improve BYOL, it could be worse than other baselines like FT and FS-pretrained (e.g., 10% on ISIC 2019). This is because some additional positives identified by the on-the-fly model may be false positives, and attracting representations of such samples will degrade the learned features. However, with a pre-trained model in BYOL-TracIn-pretrained, the identification accuracy can be increased, leading to more true positives and better representations. (4) TracIn-pretrained performs better than FS-pretrained in all settings, and the improvement in BMA could be up to 0.006. This suggests that TracIn can be a more reliable metric for assessing the similarity between images when there is no human label information available. (5) Supervised BYOL can greatly increase the BYOL performance on both datasets. Yet our BYOL-TracIn-pretrained only has a marginal accuracy drop from supervised BYOL with a sufficient number of training samples (e.g., 100% on ISIC 2019).To further demonstrate the superiority of TracIn over Feature Similarity (FS) in selecting additional positive pairs for BYOL, we use an image from ISIC 2019 as an example and visualize the top-3 most similar images selected by both metrics using a BYOL pre-trained model in Fig. 2. We can observe that TracIn accurately identifies the most similar images with the same label as the anchor image, whereas two of the images selected by FS have different labels. This discrepancy may be attributed to the fact that the FS of these two images is dominated by unrelated features (e.g., background tissue), which makes it unreliable. More visualization examples can be found in the supplementary."
Additional Positive Enables Better Representation Learning for Medical Images,4.3,Transfer Learning,"To evaluate the transfer learning performance of the learned features, we use the encoder learned from the pre-training to initialize the model on the downstream datasets (ISIC 2019 transfers to ISIC 2016, and ChestX-ray transfers to Shenzhen). We finetune the model for 50 epochs and report the precision and AUC on ISIC 2016 and Shenzhen datasets, respectively. Table 2 shows the comparison results of all methods. We can see that BYOL-TracIn-pretrained always outperforms other unsupervised pre-training baselines, indicating that the additional positives can help BYOL learn better transferrable features."
Additional Positive Enables Better Representation Learning for Medical Images,5,Conclusion,"In this paper, we propose a simple yet effective method, named BYOL-TracIn, to boost the representation learning performance of the vanilla BYOL framework. BYOL-TracIn can effectively identify additional positives from different samples in the mini-batch without using label information, thus introducing more variances to learned features. Experimental results on multiple public medical image datasets show that our method can significantly improve classification performance in both semi-supervised and transfer learning settings. Although this paper only discusses the situation of one additional pair for each image, our method can be easily extended to multiple additional pairs. However, more pairs will introduce more computation costs and increase the false positive rate which may degrade the performance. Another limitation of this paper is that BYOL-TracIn requires a pre-trained model to start with, which means more computation resources are needed to demonstrate its effectiveness."
Additional Positive Enables Better Representation Learning for Medical Images,,Fig. 1 .,
Additional Positive Enables Better Representation Learning for Medical Images,,( 2 ),
Additional Positive Enables Better Representation Learning for Medical Images,,Fig. 2 .,
Additional Positive Enables Better Representation Learning for Medical Images,,Table 2 .,
Additional Positive Enables Better Representation Learning for Medical Images,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0_12. 
Self-Supervised Domain Adaptive Segmentation of Breast Cancer via Test-Time Fine-Tuning,1,Introduction,"In recent years, deep learning (DL) methods have demonstrated remarkable performance in detecting and localizing tumors on ultrasound images [2,27]. Compared with conventional image processing methods, DL methods provide an accurate feature extraction capability on ultrasound images, despite their low resolution and noise disturbance, leading to superior segmentation accuracy [2,5,14]. However, there are some limitations in developing a DL model in a source domain and deploying it in an unseen target domain. The primary limitation is that DL models require a large number of training samples to achieve accurate predictions [8,24]. Yet, acquiring large training datasets and their corresponding labels, especially from a cohort of patients, can be costly or even infeasible, which poses a significant challenge in developing a DL model with high performance [7]. Second, even when large-scale datasets are available through collaborative research from multiple sites, DL models trained on such datasets may yield sub-optimal solutions due to domain gaps caused by differences in images acquired from different sites [20]. Third, due to the small number of datasets from each domain, the images for each individual domain may not capture representative features, limiting the ability of DL models to generalize across domains [3].Domain adaptation (DA) has been extensively studied to alleviate the aforementioned limitations, the goal of which is to reduce the domain gap caused by the diversity of datasets from different domains [12,20,26,29,33]. Example solutions include transfer learning-and style transfer-based methods. Nonetheless, unlike natural images, generating labels can be a challenging task, making it difficult to apply general DA methods; thus bridging domain gaps by DA methods remains limited [26,33]. This is due to sensitive privacy issues in patients' data, particularly in collaborative research, which restricts access to labels from different domains. As a result, conventional DA methods cannot be easily applied [10]. More recently, unsupervised domain adaptation (UDA) has been introduced to address this issue [16,33], aiming to generate semi-predictions (pseudo-labels) in target domains first, followed by producing accurate predictions using the pseudo-labels. One critical limitation of pseudo-label-based UDA is the possibility of error accumulation due to mispredicted pseudo-labels. This can lead to significant degradation of the performance of DL models, as errors can compound and become more pronounced over time [17,25].To alleviate the problem of pseudo-label-based UDA, in this work, we propose an advanced UDA framework based on self-supervised DA with a test-time finetuning network. Test-time adaptation methods have been developed [4,11,13,23] to improve the learning of knowledge in target domains. The distinctive feature of our test-time self-supervised DA is that it enables the DL network (i) to learn knowledge about the features of target domains by fine-tuning the network itself during the test-time phase, rather than generating pseudo-labels and then (ii) to provide precise predictions on images in target domains, by using the fine-tuned network. Specifically, we adopt self-supervised learning and verify the model via thorough mathematical analysis. Our framework was tested on the task of breast cancer segmentation in ultrasound images, but it could also be applied to other lesion segmentation tasks.To summarize, our contributions are three-fold:• We design a self-supervised DA framework that includes a parameter search method and provide a mathematical justification for it. With our framework, we are able to identify the best-performing parameters that result in improved performance in DA tasks. • Our framework is effective at preserving privacy, since it carries out DA using only pre-trained network parameters, without transferring any patient data. • We applied our framework to the task of segmenting breast cancer from ultrasound imaging data, demonstrating its superior performance over competing UDA methods.Our results indicate that our framework is effective in improving the accuracy of breast cancer segmentation from ultrasound images, which could have potential implications for improving the diagnosis and treatment of breast cancer. Sample batches of (t, ?) ∼ T"
Self-Supervised Domain Adaptive Segmentation of Breast Cancer via Test-Time Fine-Tuning,2,Methodology,return ŷ 15: End Output: Predictions ( ŷ) on T Fig. 1. Architecture of our TTFT network (Left) and its pipeline (Right).
Self-Supervised Domain Adaptive Segmentation of Breast Cancer via Test-Time Fine-Tuning,2.1,Test-Time Fine-Tuning (TTFT) Network and Its Pipeline,"Network Architecture. Our proposed TTFT network is based on selfsupervised DA [31], which is a part of UDA and can be seen as multi-task learning, involving both the main and pretext tasks, as shown in Fig. 1. In the main task, an encoder (E), a decoder for segmentation (D seg ), and a segmentation header (H) are included. The main task is the segmentation task,In predicting segmentation labels in the target domain (T ), D FT is also involved in the main task, and the final prediction after the fine-tuning is where L BCE and L GAN represent the loss functions for binary cross-entropy and generative adversarial network [6], respectively. Θ m S includes E S , D S seg , and H S , while Θ p S includes E S , D S gen , and C S . Additionally, D S seg = D S gen .Fine-Tuning in Target Domain. Since the pre-trained model is likely to produce imprecise predictions in T , the model should learn domain knowledge about T . To this end, in the pretext task, for self-supervised learning, the model is fine-tuned in T to generate synthetic images identical to the input images as below:where only D gen is fine-tuned to achieve memory efficiency and to decrease the fine-tuning time, and D S gen is fine-tuned as D S→T gen . Then, D S→T gen is transferred to D FT , and knowledge distillation via self-supervised learning is realized. Hence, the precise predictions in T could be provided byBenefits of Our Dual-Pipeline. Due to the symmetric property of mutual information in information entropy (H), we haveAs a result, the predictions made by the fine-tuned network in the target domain (T ) lead to reduced entropy, as shown below:(Since D S seg is fully optimized for S in a supervised manner, it guarantees a baseline segmentation performance. Furthermore, since D T FT is fine-tuned in T "
Self-Supervised Domain Adaptive Segmentation of Breast Cancer via Test-Time Fine-Tuning,2.2,Parameter Fluctuation: Parameter Randomization Method,"Since the loss function and its values can vary based on the distribution of inputs, and different domains can have different distributions, the local minimum identified in the source domain (S) cannot be considered as the same local minimum in T , as illustrated in Fig. 2. The y-axis of Fig. 2 "
Self-Supervised Domain Adaptive Segmentation of Breast Cancer via Test-Time Fine-Tuning,,indicates 1,
Self-Supervised Domain Adaptive Segmentation of Breast Cancer via Test-Time Fine-Tuning,,|X |,"x L(M (x; θ), x), and the local minimum is different in S and T as Θ S in Fig. 2a andΘ T in Fig. 2c, respectively. A longer fine-tuning time is required to re-position Θ S to Θ T as in Fig. 2c than to re-position θ T to Θ T . Therefore, efficient fine-tuning is necessary to re-position the local minimum in Fig. 2b and this process is known as parameter fluctuation. Note that the parameter fluctuation is followed by the fine-tuning step. Suppose C i be the i th convolution operator in D seg with weight w i , thenseg provides the baseline segmentation performance, D T F T should provide similar feature maps to achieve the baseline performance. To this end, the mid-feature maps generated should be similar, i.e.,where C i represents the convolution in D T F T , F i represents i th feature map, andwhich can be expressed as:Here, we denote w i -w i = f i as the fluctuation vector in the vector space, and the condition f i = 0 indicates that the sum of the fluctuation vectors should be zero under the condition of |f i | < r 1. Hence, we achieve the condition for the parameter fluctuation that the centers of parameters of Θ S and θ T should be the same in the vector space, and the length of the fluctuation vector should be less than a certain small threshold (0 < r 1). Therefore, the parameter fluctuation aims to add random vectors of which length is less than 0 < r 1 on the parameters of Θ S , and the sum of vectors should be zero. To summarize, the parameter fluctuation aims to add randomness on Θ S as follows:(5)3 Experiments"
Self-Supervised Domain Adaptive Segmentation of Breast Cancer via Test-Time Fine-Tuning,3.1,Experimental Set-Ups,"To evaluate the segmentation performance of our TTFT framework, we used three different ultrasound databases: BUS [32], BUSI [1], and BUV [18], which are considered to be different domains. All three databases contain ultrasound imaging data and segmentation masks for breast cancer, with the masks labeled as 0 (background) and 1 (lesion) using a one-hot encoding. The BUS database consists of 163 images along with corresponding labels. The BUSI database contains 780 images, with 133 images belonging to the NORMAL class and having labels containing only 0 values. The BUV database originally consists of ultrasound videos, providing a total of 21,702 frames. While the database also provides labels for the detection task, we processed these labels as segmentation masks using a region growing method [15]. We employed different deep-learning models for evaluation. Specifically, U-Net [22] and FusionNet [21] were employed as our baseline models, since U-Net is a widely used basic model for segmentation, and FusionNet contains advanced residual modules, compared with U-Net. Ours I and Ours II were based on U-Net and FusionNet as the baseline network, respectively. Additionally, MIB-Net [28], which is a state-of-the-art model for breast cancer segmentation using ultrasound images, was employed for comparison. Furthermore, CBST [33] and CT-Net [16] were employed as the comparison models for UDA methods. As the evaluation metrics, dice coefficient (D. Coef), PRAUC, which is an area under a precision-recall curve, and cohen kappa (κ) were employed [30]. Our experimental set-ups included: (i) individual databases were used to assess the baseline segmentation performance (Appendix); (ii) the domain adaptive segmentation performance was assessed using the three databases, where two databases were regarded as the source domain, and the remaining database was regarded as the target domain; and (iii) the ablation study was carried out to evaluate the proposed network architecture along with the randomized re-initialization method."
Self-Supervised Domain Adaptive Segmentation of Breast Cancer via Test-Time Fine-Tuning,3.2,Comparison Analysis,"Since all compared DL models show similar D. Coef, only UDA performance is comparable as a control in our experiments. In this experiment, two databases were used for training, and the remaining database was used for testing. For instance, BUS in Fig. 3 illustrates the BU S database was used for testing, and  the other two databases of BUSI and BUV were used for training. Figs. 3 and4 show quantitative results, and Fig. 5 shows the sample segmentation results. Unlike the experiment using the individual database, U-Net, FusionNet, and MIB-Net showed significantly inferior scores due to domain gaps. In contrast, UDA methods of CBST and CT-Net showed superior scores, compared with others, and the scores were not strongly reduced, compared with the experiment with the single database. Note that, our TTFT framework achieved the best performance compared with other DL models. Additionally, Ours II, based on FusionNet, showed the best scores, potentially due to the advanced residual connection module. Furthermore, as illustrated in Fig 4, our framework provides superior precision scores in a long range of (0, 0.7), indicating that our frameworks estimated unnecessary mispredictions but precise predictions on cancer.   "
Self-Supervised Domain Adaptive Segmentation of Breast Cancer via Test-Time Fine-Tuning,4,Discussion and Conclusion,"In this work, we proposed a DL-based segmentation framework for multi-domain breast cancer segmentation on ultrasound images. Due to the low resolution of ultrasound images, manual segmentation of breast cancer is challenging even for expert clinicians, resulting in a sparse number of labeled data. To address this issue, we introduced a novel self-supervised DA network for breast cancer segmentation in ultrasound images. In particular, we proposed a test-time finetuning network to learn domain-specific knowledge via knowledge distillation by self-supervised learning. Since UDA is susceptible to error accumulation due to imprecise pseudo-labels, which can lead to degraded performance, we employed a self-supervised learning-based pretext task. Specifically, we utilized an autoencoder-based network architecture to generate synthetic images that matched the input images. Moreover, we introduced a randomized re-initialization module that injects randomness into network parameters to reposition the network from the local minimum in the source domain to a local minimum that is better suited for the target domain. This approach enabled our framework to efficiently fine-tune the network in the target domain and achieve better segmentation performance. Experimental results, carried out with three ultrasound databases from different domains, demonstrated the superior segmentation performance of our framework over other competing methods. Additionally, our framework is well-suited to a scenario in which access to source domain data is limited, due to data privacy protocols. It is worth noting that we used vanilla U-Net [22] and FusionNet [21] as baseline models to evaluate the basic performance of our TTFT framework. However, the use of more advanced baseline models could lead to even better segmentation performance, which is a subject for our future work. Moreover, our proposed framework is not limited to breast cancer segmentation on ultrasound images acquired from different domains. It can also be applied to other disease groups or imaging modalities such as MRI or CT."
Self-Supervised Domain Adaptive Segmentation of Breast Cancer via Test-Time Fine-Tuning,,Algorithm 1 :,
Self-Supervised Domain Adaptive Segmentation of Breast Cancer via Test-Time Fine-Tuning,,Fig. 2 .,
Self-Supervised Domain Adaptive Segmentation of Breast Cancer via Test-Time Fine-Tuning,,Fig. 3 .,
Self-Supervised Domain Adaptive Segmentation of Breast Cancer via Test-Time Fine-Tuning,,Fig. 4 .,
Self-Supervised Domain Adaptive Segmentation of Breast Cancer via Test-Time Fine-Tuning,,Fig. 5 .,
Self-Supervised Domain Adaptive Segmentation of Breast Cancer via Test-Time Fine-Tuning,,Fig. 6 .,
Self-Supervised Domain Adaptive Segmentation of Breast Cancer via Test-Time Fine-Tuning,,,
Self-Supervised Domain Adaptive Segmentation of Breast Cancer via Test-Time Fine-Tuning,,Table 1 .,
Self-Supervised Domain Adaptive Segmentation of Breast Cancer via Test-Time Fine-Tuning,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0_52.
Self-Supervised Domain Adaptive Segmentation of Breast Cancer via Test-Time Fine-Tuning,3.3,Ablation Study,"In order to assess the effectiveness of each of the proposed modules, including the parameter fluctuation and fine-tuning methods, the ablation study was carried out. Since our framework contains three types of decoders, including D S seg , D fl seg , and D S→T seg for the fine-tuning, we mainly targeted those decoders in our ablation study. Table 1 illustrates the quantitative results by different types of decoders. The higher D. coef value (+3.4%) of Pre-train + PF than that of Pre-train + Random Init and Pre-train + Offset confirms the effectiveness of the parameter fluctuation in the UDA performance. Additionally, the higher score (+11%) of Fine-tuning than Pre-train shows an outstanding UDA performance of the fine-tuning pipeline. Furthermore, the simultaneous utilization of the dual pipeline with D S seg and D S→T seg is justified by the scores of Pre-train + Fine-tuning. Using dual-pipeline and parameter fluctuation yielded the best performance. However, the utilization of ensemble pipelines of multiple fine-tuning modules was inefficient, since negligible performance improvements (+0.002) were observed, despite the heavy memory utilization.Furthermore, Fig. 6 shows the effectiveness of the parameter fluctuation and fine-tuning methods. We first compared the similarity of feature-maps by decoders, including D S seg , D fl seg , and D S→T seg , with D S seg and D T seg , which was fully optimized decoder in T . Here, a style loss [9] was employed to measure the similarity of feature maps. Our   in S and T are plotted with T-SNE, where the short distance represents the similar features [19]. The generated images became similar to T in order of D S seg , D fl seg , and D S→T seg , which confirmed the effectiveness of the fine-tuning method in terms of knowledge distillation. Additionally, the parameters were successfully re-positioned from the local minimum in S by parameter fluctuation, which was confirmed by the distances from S to D S gen and D fl gen ."
Decoupled Consistency for Semi-supervised Medical Image Segmentation,1,Introduction,"Deep learning technology can significantly assist clinicians in clinical diagnosis through accurate and robust segmentation of lesions or organs from medical images [1]. In comparison to natural images, acquiring pixel-level labels of medical images involves input from clinical professionals, making such labels expensive to produce. In order to effectively alleviate the problem of data labeling, many attempts have been made towards semi-supervised medical image segmentation [2,3,5,6,11]. How to fully utilize unlabeled data in this situation becomes crucial.Pseudo labeling [15,16] and consistency regularization [19][20][21][22] are two powerful techniques for using unlabeled data. The traditional pseudo labeling approach employs a high and fixed threshold to select the predicted pixels with high confidence as the pseudo label of unlabeled data [15]. Nevertheless, with this strategy, only a few samples can exceed the chosen threshold at the start of training. As a result, Zhang et al. [23]. present the Curriculum Pseudo Labeling (CPL) strategy, which adjusts the flexible threshold of each category dynamically during the training process. Despite the positive outcomes, low-confidence pixels will still be removed. Wang et al. [12]. demonstrated the importance of low-confidence pixels in model training. Given that consistency regularization appears to utilize data more, this research will focus more on this method.Despite using all available prediction data, consistency regularization focuses more on how to get two similar predictions, such as data perturbation [9,10], model perturbation [25,28], feature perturbation [7,8], etc. Yet, when it comes to prediction optimization, all data is processed uniformly using a consistency loss function, such as L2 Loss. In light of this, we consider if we may decouple the prediction data into data with distinct functions, and optimize the prediction data by playing to each advantage's strengths. The benefit of this is that we can utilize all the data and, more significantly, fully utilize the advantages of various prediction data.To sum up, this paper proposed a novel decoupled consistency regularization strategy. Specifically, inspired by CPL, we first designed a consistency threshold related to pixel confidence (Wang et al. [24] proved that the ideal dynamic threshold should be related to pixel confidence) to distinguish the consistent part from the inconsistent part, and the threshold increased as the training progressed to the maximum threshold. For the consistent part with high confidence, we use the method of cross pseudo supervision [18] to optimize. For the inconsistent part, we further decouple it into unreliable data that is likely to appear close to the decision boundary and guidance data that is more likely to be present near the high-density area. The guidance data plays the role of guiding the direction, and We don't propagate its gradient back. We refer to this action as directional consistency strategy. Additionally, we incorporate the feature map into the training process, suggest a feature consistency approach, and compute its loss in order to make better use of the data. We evaluated our method on the public PROMISE12 [14] and ACDC [13] datasets. Several experimental findings demonstrate that our strategy can significantly boost performance.Overall, our contributions are four-fold: (1) we proposed a novel decoupled consistent semi-supervised medical image segmentation framework. The framework fully exploits prediction data, decoupled prediction data into data for various functions, and maximizes each function's advantages. (2) A dynamic threshold is proposed that can separate the prediction data into consistent and inconsistent parts. This threshold can effectively reflect the model's learning state and encourage more diversity of data at the start of training. (3) A novel direction consistency strategy is proposed to optimize the inconsistent part. This strategy focuses on optimizing the data around the decision boundary. The results of the experiment demonstrate that the direction consistency strategy is superior to the conventional consistent regularization. ( 4) The feature consistency approach is suggested and the feature map is incorporated into the training, allowing for better data utilization."
Decoupled Consistency for Semi-supervised Medical Image Segmentation,2,Method,"Figure 1 shows the overall pipeline of our method. To describe this work precisely, we first define some mathematical terms. Let D = D l ∪D u be the whole provided dataset. We denote an unlabeled image as x i ∈ D u and a labeled image pair as (x i , y i ) ∈ D l , where y i is ground-truth. θ dA and θ dB represent decoder A and decoder B, respectively. θ dA employs bi-linear interpolation for up-sampling, and θ dB employs original transpose convolution for up-sampling. o A and o B represent the outputs of θ dA and θ dB , respectively. p A (p B ) is the segmentation confidence map, which is the network output o A (o B ) after softmax normalization."
Decoupled Consistency for Semi-supervised Medical Image Segmentation,2.1,Dynamic Consistency Threshold,"In recent years, the pseudo labeling method based on threshold has achieved great success. The sampling strategy of this method can be defined as follows: where θ dA uses bi-linear interpolation for up-sampling and θ dB uses transposed convolution for up-sampling. For the labeled data we calculate the loss Lseg between them and ground-truth, for the consistent part we calculate cross pseudo supervision loss Lcps, for the inconsistent part we calculate directional consistency loss L dc , for feature maps, we calculate the feature consistency losswhere γ ∈ (0, 1) is a threshold used to select pseudo labels. p is the segmentation confidence map. FlexMatch [23] has proved that in the early stage of training, in order to improve the utilization of unlabeled data and promote the diversification of pseudo labels, γ should be relatively small. As the training progresses, γ should maintain a stable proportion of pseudo labels. Therefore, our consistency threshold is defined as follows:where B is the batch size, λ is the weight coefficient that increases with the training and we set λ = t tmax . In order to sample more unlabeled data, we conduct threshold evaluation on p A and p B and select a smaller threshold as our consistency threshold. We initialize λ t as 1 C , where C indicates the number of classes. The consistency threshold γ t is finally defined and adjusted as:where γ A t and γ B t represent the threshold of p A and p B ."
Decoupled Consistency for Semi-supervised Medical Image Segmentation,2.2,Decoupled Consistency,"Inconsistent Part. We decouple the inconsistent part into unreliable data, which is probably going to appear at the decision boundary, and guidance data, which is more likely to occur near the high-density area. These two parts have the same index information. The distinction is that guidance data is more confident than unreliable data. Based on the smoothing assumption, the output of these two parts should be consistent and located in the high-density area. As a result, we should concentrate on optimizing the pixels around the decision boundary in order to bring them closer to the high-density region. We initially sharpen the confidence of these pixels to bring the high-confidence pixels closer to the high-density area. These are the sharpening processes:where o represents the output of the model and T ∈ (0, 1) represents the sharpening temperature. In the experiment, we set T = 0.5. By comparing Sp A and Sp B , the high-confidence parts hSp A , hSp B (See Appendix Algorithm 2.) and low-confidence parts lSp A , lSp B can be obtained. We take L2 loss as the loss function of this part. Note that we only optimize the low-confidence part and do not back-propagate the gradient of the high-confidence part. Therefore, our loss of directional consistency can be written as:Consistent Part. Similar to CPS [18], the method of cross pseudo supervision is adopted for optimization. The details are as follows:where P L A and P L B represent corresponding pseudo labels.Feature Part. We incorporate the feature map into the training process to further utilize the data. In order to reduce the amount of computation, we have carried out an average mapping of the feature map to reduce its dimension(R Cm×Hm×Wm -→ R Hm×Wm ). The mapping process is as follows:where, p > 1, f m represents the feature map of m-th layer, and f mi denotes the i-th slice of f m in the channel dimension, fm represents the corresponding mapping result. In the experiment, we set p = 2. Our feature consistency loss is as follows:where N is the number of pixels of fmi , n is the number of network layers, f e mi and f d mi represent i-th pixels of m-th feature map of decoder and encoder respectively. In this paper, only feature maps of decoder B were used to calculate the loss.Total Loss. The total loss is a weighted sum of the segmentation loss L seg and the other three losses:where L seg is a Dice loss, which is applied for the few labeled data. And hyperparameter β is set as an iteration-dependent warming-up function [29], β = e (-5(1-t tmax ) 2 ) , λ = 1β, α = 0.3 in the experiment."
Decoupled Consistency for Semi-supervised Medical Image Segmentation,3,Experiment and Results,
Decoupled Consistency for Semi-supervised Medical Image Segmentation,3.1,Dataset,"We evaluated our methods on ACDC dataset [13] and PROMERE12 dataset [14]. For the ACDC dataset, we randomly selected 140 scans from 70 subjects, 20 scans from 10 subjects, and 40 scans from 20 subjects as training, validation, and test sets, ensuring that each set contains data from different subjects. For the PROMISE12 dataset, we randomly divided the data into 35, 5, and 10 cases as training, validation, and test sets. And according to the previous work on datasets [27], these two data sets are segmented in 2D (piece by piece). Each slice was resized to 256 × 256, and the intensities of pixels are normalized to the [0, 1] range. "
Decoupled Consistency for Semi-supervised Medical Image Segmentation,3.2,Implementation Details,"All comparisons and ablation experiments are performed using the same experimental setting for a fair comparison. They are conducted on PyTorch using an Intel(R) Xeon(R) CPU and NVIDIA GeForce RTX 1080 Ti GPU. We adopt U-net [4] as our base network. And we use SGD as an optimizer, with a weight attenuation of 0.0005 and momentum of 0.9. The learning rate is 0.01. The batch size is set to 24, in which 12 images are labeled. All methods performed 30000 iterations during training. Moreover, a data augmentation strategy including random flipping and random rotation is exploited to alleviate overfitting."
Decoupled Consistency for Semi-supervised Medical Image Segmentation,3.3,Results,"Comparison with Other Semi-supervised Methods. We use the metrics of Dice, Jaccard, 95% Hausdorff Distance (95HD), and Average Surface Distance (ASD) to evaluate the results. Table 1 gives the averaged performance of three-class segmentation including the myocardium, left and right ventricles on the ACDC dataset. Table 2 shows the segmentation results on the PROMISE12 dataset. It can be seen from the table that our method is superior to other methods. The visualized results in Fig. The visualization result in Fig. 2 shows that the segmentation result of our model is closer to the ground-truth and effectively eliminates most false-positive predictions on the ACDC (highlighted by yellow boxes at the bottom line). Ablation Study. In order to verify the effectiveness of different loss functions of the model, we conducted experiments on the ACDC dataset with 10% labeled data, and the experimental results are shown in Table 3. It can be seen from the table that the dice scores of L cps , L f and L dc are improved by 2.93%, 3.33%, 3.60%, respectively, compared with only using L seg . On the basis of L seg and L f , L dc is also better than L con , even close to our final results. To investigate the usefulness of directional consistency further, we do not employ a threshold and compare it to classical consistency regularization and cross pseudo supervision. The experimental results are shown in Table 4. It can be seen from the table that the L dc is obviously superior to the other two methods, which fully proves the importance of the division of different pixel functions. Also, we discovered that utilizing threshold would improve the effect of L cps . This further demonstrates the significance of our dynamic threshold. "
Decoupled Consistency for Semi-supervised Medical Image Segmentation,4,Conclusion,"This paper proposed a framework DC-Net for semi-supervised medical image segmentation. In view of the current problem of insufficient utilization of unlabeled data, our fundamental concept is to fully exploit the benefits of data with various functionalities. Based on this, we decouple the prediction data into consistent and inconsistent parts through a dynamic threshold. Furthermore, the inconsistent part is further decoupled into guidance data and unreliable data, and optimized by a novel directional consistency strategy. Our method yielded excellent outcomes on both the ACDC and PROMISE12 datasets. In addition, directional consistency shows promising potential in the experiment, and future research will further explore the selection and treatment of directions."
Decoupled Consistency for Semi-supervised Medical Image Segmentation,,Fig. 1 .,
Decoupled Consistency for Semi-supervised Medical Image Segmentation,,,
Decoupled Consistency for Semi-supervised Medical Image Segmentation,,Table 1 .,
Decoupled Consistency for Semi-supervised Medical Image Segmentation,,Table 2 .,
Decoupled Consistency for Semi-supervised Medical Image Segmentation,,Table 3 .,
Decoupled Consistency for Semi-supervised Medical Image Segmentation,,Table 4 .,Lseg Lmse Lcps L dc Dice(%)↑ Jaccard(%)↑ 95HD(voxel)↓ ASD(voxel)↓ Fig. 2. Exemplar results of several semi-supervised segmentation methods and corresponding ground truth (GT) on PROMISE12 dataset (Top two rows) and ACDC dataset (Bottom two rows).
Decoupled Consistency for Semi-supervised Medical Image Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0 53.
PET-Diffusion: Unsupervised PET Enhancement Based on the Latent Diffusion Model,1,Introduction,"Positron emission tomography (PET) is a sensitive nuclear imaging technique, and plays an essential role in early disease diagnosis, such as cancers and Alzheimer's disease [8]. However, acquiring high-quality PET images requires injecting a sufficient dose (standard dose) of radionuclides into the human body, which poses unacceptable radiation hazards for pregnant women and infants even following the As Low As Reasonably Achievable (ALARA) principle [19]. To reduce the radiation hazards, besides upgrading imaging hardware, designing advanced PET enhancement algorithms for improving the quality of low-dose PET (LPET) images to standard-dose PET (SPET) images is a promising alternative.In recent years, many enhancement algorithms have been proposed to improve PET image quality. Among the earliest are filtering-based methods such as non-local mean (NLM) filter [1], block-matching 3D filter [4], bilateral filter [7], and guided filter [22], which are quite robust but tend to over-smooth images and suppress the high-frequency details. Subsequently, with the development of deep learning, the end-to-end PET enhancement networks [9,14,21] were proposed and achieved significant performance improvement. But these supervised methods relied heavily on the paired LPET and SPET data that are rare in actual clinic due to radiation exposure and involuntary motions (e.g., respiratory and muscle relaxation). Consequently, unsupervised PET enhancement methods such as deep image prior [3], Noise2Noise [12,20], and their variants [17] were developed to overcome this limitation. However, these methods still require LPET to train models, which contradicts with the fact that only SPET scans are conducted in clinic.Fortunately, the recent glowing diffusion model [6] provides us with the idea for proposing a clinically-applicable PET enhancement approach, whose training only relies on SPET data. Generally, the diffusion model consists of two reversible processes, where the forward diffusion adds noise to a clean image until it becomes pure noise, while the reverse process removes noise from pure noise until the clean image is recovered. By combining the mechanics of diffusion model with the observation that the main differences between LPET and SPET are manifested as levels of noises in the image [11], we can view LPET and SPET as results at different stages in an integrated diffusion process. Therefore, when a diffusion model (trained only on SPET) can recover noisy samples to SPET, this model can also recover LPET to SPET. However, extending the diffusion model developed for 2D photographic images to PET enhancement still faces two problems: a) three-dimensionsal (3D) PET images will dramatically increase the computational cost of diffusion model; b) PET is the detail-sensitive images and may be introduced/lost some details during the procedure of adding/removing noise, which will affect the downstream diagnosis.Taking all into consideration, we propose the SPET-only unsupervised PET enhancement (uPETe) framework based on the latent diffusion model. Specifically, uPETe has an encoder-<diffusion model>-decoder structure that first uses the encoder to compress input the LPET/SPET images into latent representations, then uses the latent diffusion model to learn/estimate the distribution of SPET latent representations, and finally uses the decoder to recover SPET images from the estimated SPET latent representations. The keys of our uPETe include 1) compressing the 3D PET images into a lower dimensional space for reducing the computational cost of diffusion model, 2) adopting the Poisson noise, which is the dominant noise in PET imaging [20], to replace the Gaussian noise in the diffusion process for avoiding the introduction of details that are not existing in PET images, and 3) designing CT-guided cross-attention to incorporate additional CT images into the inverse process for helping the recovery of structural details in PET.Our work had three main features/contributions: i) proposing a clinicallyapplicable unsupervised PET enhancement framework, ii) designing three targeted strategies for improving the diffusion model, including PET image compression, Poisson diffusion, and CT-guided cross-attention, and iii) achieving better performance than state-of-the-art methods on the collected PET datasets."
PET-Diffusion: Unsupervised PET Enhancement Based on the Latent Diffusion Model,2,Method,"The framework of uPETe is illustrated in Fig. 1. When given an input PET image x (i.e., SPET for training and LPET for testing), x is first compressed into the latent representation z 0 by the encoder E. Subsequently, z 0 is fed into a latent diffusion model followed by the decoder D to output the expected SPET image x. In addition, a specialized encoder E CT is used to compress the CT image corresponding to the input PET image into the latent representation z CT , which is fed into each denoising network for CT-guided cross-attention. In the following, we introduce the details of image compression, latent diffusion model, and implementation."
PET-Diffusion: Unsupervised PET Enhancement Based on the Latent Diffusion Model,2.1,Image Compression,"The conventional diffusion model is computationally-demanding due to its numerous inverse denoising steps, which severely restricts its application to 3D PET enhancement. To overcome this limitation, we adopt two strategies including 1) compressing the input image and 2) reducing the diffusion steps (as described in Sect. 2.3).Similar to [10,18], we adopt an autoencoder (E and D) to compress the 3D PET images into a lower dimensional but more compact space. The crucial aspects of this process is to ensure that the latent representation contains the necessary and representative information for the input image. To achieve this, we train the autoencoder by a combination of perceptual loss [24] and patch-based adversarial loss [5], instead of simple voxel-level loss such as L 2 or L 1 loss. Among them, the perceptual loss, designed on a pre-trained 3D ResNet [2], constrains higher-level information such as texture and semantic content, and the patchbased adversarial loss ensures globally coherent while remaining locally realistic. Let x ∈ R H,W,Z denote the input image and z 0 ∈ R h,w,z,c denote the latent representation. The compression process can be formulated as x = D(z 0 ) = D(E(x)). In this way, we compress the input image by a factor of f = H/h = W/w = Z/z. The results of SPET estimation under different compression rates f are provided in the supplement."
PET-Diffusion: Unsupervised PET Enhancement Based on the Latent Diffusion Model,2.2,Latent Diffusion Model,"After compressing the input PET image, its latent representation is fed into the latent diffusion model, which is the key to achieving the SPET-only unsupervised PET enhancement. As described above, the LPET can be viewed as noisy SPET (even in the compressed space), so the diffusion process from SPET to pure noise actually covers the situations of LPET. That is, the diffusion model trained with SPET is capable of estimating SPET from the noisy sample (diffused from LPET). But the diffusion model is developed from photographic images, which have significant difference with the detail-sensitive PET images. To improve its applicability for PET images, we design several targeted strategies for the diffusion process and inverse process, namely Poisson diffusion and CT-guided cross-attention, respectively.Poisson Diffusion. In conventional diffusion models, the forward process typically employs Gaussian noise to gradually perturb input samples. However, in PET images, the dominant source of noise is Poisson noise, rather than Gaussian noise. Considering this, in our uPETe we choose to adopt Poisson diffusion to perturb the input samples, which facilitates the diffusion model for achieving better performance on the PET enhancement task.Let z t be the perturbation sample in Poisson diffusion, where t = 0, 1, ..., T . Then the Poisson diffusion can be formulate as follows:At each diffusion step, we apply the perturb function to the previous perturbed sample z t-1 by imposing a Poisson noise with an expectation of λ t , which is linearly interpolated from [0, 1] and incremented with t. In our implementation, we apply the same Poisson noise imposition operation as in [20], i.e., applying Poisson deviates on the projected sinograms, to generate a sequence of perturbed samples with increasing Poisson noise intensity as the step number t increases.CT-Guided Cross-Attention. The attenuation correction of PET typically relies on the corresponding anatomical image (CT or MR), resulting in a PET scan usually accompanied by a CT or MR scan. To fully utilize the extramodality images (i.e., CT in our work) as well as improve the applicability of diffusion models, we design a CT-guided cross-attention to incorporate the CT images into the reverse process for assisting the recovery of structural details.As shown in Fig. 1, to achieve a particular SPET estimation, the corresponding CT image is first compressed into the latent representation z CT by encoder E CT . Then z CT is fed into a denoising attention U-Net [16] at each step for calculation of cross-attention, where the query Q and key K are calculated from z CT while the value V is still calculated from the output of the previous layer because our final goal is SPET estimation. Denoting the output of previous layer as z P ET , the CT-guided cross-attention can be formulated as follows:where d is the number of channels, B is the position bias, and Conv(•) denotes the 1 × 1 × 1 convolution with stride of 1."
PET-Diffusion: Unsupervised PET Enhancement Based on the Latent Diffusion Model,2.3,Implementation Details,"Typically, the trained diffusion model generates target images from random noise, requiring a large number of steps T to make the final perturbed sample (z T ) close to pure noise. However, in our task, the target SPET image is generated from a given LPET image during testing, and making z T as close to pure noise as possible is not necessary since the remaining PET-related information can also benefit the image recovery. Therefore, we can considerably reduce the number of diffusion steps T to accelerate the model training, and T is set to 400 in our implementation. We evaluate the quantitative results using two metrics, including Peak Signal to Noise Ratio (PSNR) and Structural Similarity Index Measure (SSIM).  3 Experiments"
PET-Diffusion: Unsupervised PET Enhancement Based on the Latent Diffusion Model,3.1,Dataset,"Our dataset consists of 100 SPET images for training and 30 paired LPET and SPET images for testing. Among them, 50 chest-abdomen SPET images are collected from (total-body) uEXPLORER PET/CT scanner [25], and 20 paired chest-abdomen images are collected by list mode of the scanner with 256 MBq of [ 18 F]-FDG injection. Specifically, the SPET images are reconstructed by using the 1200 s data between 60-80 min after tracer injection, while the corresponding LPET images are simultaneously reconstructed by 120 s data uniformly sampled from 1200 s data. As a basic data preprocessing, all images are resampled to voxel spacing of 2 × 2 × 2 mm 3 and resolution of 256 × 256 × 160, while their intensity range is normalized to [0, 1] by min-max normalization. For increasing the training samples and reducing the dependence on GPU memory, we extract the overlapped patches of size 96 × 96 × 96 from every whole PET image."
PET-Diffusion: Unsupervised PET Enhancement Based on the Latent Diffusion Model,3.2,Ablation Analysis,"To verify the effectiveness of our proposed strategies, i.e. Poisson diffusion process and CT-guided cross-attention, we design another four variant latent diffusion models (LDMs) with the same compression model, including: 1) LDM: standard LDM; 2) LDM-P: LDM with Poisson diffusion process; 3) LDM-CT: LDM with CT-guided cross-attention; 4) LDM-P-CT: LDM with Poisson diffusion process and CT-guided cross-attention. All methods use the same experimental settings, and their quantitative results are given in Table 1.From Table 1, we can have the following observations. (1) LDM-P achieves better performance than LDM. This proves that the Poisson diffusion is more appropriate than the Gaussian diffusion for PET enhancement. (2) LDM-CT with the corresponding CT image for assisting denoising achieves better results than LDM. This can be reasonable as the CT image can provide anatomical information, thus benefiting the recovery of structural details (e.g., organ boundaries) in SPET images. (3) LDM-P-CT achieves better results than all other variants on both PSNR and SSIM, which shows both of our proposed strategies contribute to the final performance. These three comparisons conjointly verify the effective design of our proposed uPETe, where the Poisson diffusion process and CT-guided cross-attention both benefit the PET enhancement."
PET-Diffusion: Unsupervised PET Enhancement Based on the Latent Diffusion Model,3.3,Comparison with State-of-the-Art Methods,"We further compare our uPETe with several state-of-the-art PET enhancement methods, which can be divided into two classes: 1) fully-supervised methods, including LA-GAN [21], Transformer-GAN (Trans-GAN) [13], Dual-frequency GAN (DF-GAN) [9], and AR-GAN [14]; 2) unsupervised methods, including deep image prior (DIP) [3], Noisier2Noise [23], magnetic resonance guided deep decoder (MR-GDD) [17], and Noise2Void [20]. The quantitative and qualitative results are provided in Table 2 and Fig. 3, respectively.Quantitative Comparison: Table 2 shows that our uPETe outperforms all competing methods. Compared to the fully-supervised method AR-GAN which achieves sub-optimal performance, our uPETe does not require paired LPET and SPET, yet still achieves improvement. Additionally, uPETe also achieves noticeable performance improvement to Noise2Void (which is a supervised method). Specifically, the average improvement in PSNR and SSIM on SPET estimation are 1.554 dB and 0.005, respectively. This suggests that our uPETe can generate promising results without relying on paired data, demonstrating its potential for clinical applications.Qualitative Comparison: In Fig. 3, we provide a visual comparison of SPET estimation for two typical cases. First, compared to unsupervised methods such as DIP and Noise2Void, the SPET images estimated by our uPETe have less noise but clearer boundaries. Second, our uPETe performs better on the structural details compared to the fully-supervised methods, i.e., missing unclear tissue (Trans-GAN) or introducing non-existing artifacts in PET image (DF-GAN). Overall, these pieces of evidence demonstrate the superiority of our uPETe over state-of-the-art methods."
PET-Diffusion: Unsupervised PET Enhancement Based on the Latent Diffusion Model,3.4,Generalization Evaluation,"We further evaluate the generalizability of our uPETe to tracer dose changes by simulating Poisson noise on SPET to produce different doses for LPET, which is a common way to generate noisy PET data [20]. Notably, we do not need to retrain the models since they have been trained in Sect. 3.3. The quantitative results of our uPETe and five state-of-the-art methods are provided in Fig. 2. As shown in Fig. 2, our uPETe outperforms the other five methods at all doses and exhibits a lower PSNR descent slope as dose decreases (i.e., λ increases), demonstrating its superior generalizability to dose changes. This is because uPETe is based on diffusion model, which simplifies the complex distribution prediction task into a series of simple denoising tasks and thus has strong generalizability. Moreover, we also find that the unsupervised methods (i.e., uPETe, Noise2Void, and DIP) have stronger generalizability than fully-supervised methods (i.e., AR-GAN, DF-GAN, and Trans-GAN) as they have a smoother descent slope. The main reason is that the unsupervised learning has the ability to extract patterns and features from the data based on the inherent structure and distribution of the data itself [15]."
PET-Diffusion: Unsupervised PET Enhancement Based on the Latent Diffusion Model,4,Conclusion and Limitations,"In this paper, we have developed a clinically-applicable unsupervised PET enhancement framework based on the latent diffusion model, which uses only the clinically-available SPET data for training. Meanwhile, we adopt three strategies to improve the applicability of diffusion models developed from photographic images to PET enhancement, including 1) compressing the size of the input image, 2) using Poisson diffusion, instead of Gaussian diffusion, and 3) designing CT-guided cross-attention to enable additional anatomical images (e.g., CT) to aid the recovery of structural details in PET. Validated by extensive experiments, our uPETe achieved better performance than both state-of-the-art unsupervised and fully-supervised PET enhancement methods, and showed stronger generalizability to the tracer dose changes.Despite the advance of uPETe, our current work still suffers from a few limitations such as (1) lacking theoretical support for our Poisson diffusion, which is just an engineering attempt, and 2) only validating the generalizability of uPETe on a simulated dataset. In our future work, we will complete the design of Poisson diffusion from theoretical perspective, and collect more real PET datasets (e.g., head datasets) to comprehensively validate the generalizability of our uPETe."
PET-Diffusion: Unsupervised PET Enhancement Based on the Latent Diffusion Model,,Fig. 1 .,
PET-Diffusion: Unsupervised PET Enhancement Based on the Latent Diffusion Model,,Fig. 2 .,
PET-Diffusion: Unsupervised PET Enhancement Based on the Latent Diffusion Model,,Fig. 3 .,
PET-Diffusion: Unsupervised PET Enhancement Based on the Latent Diffusion Model,,Table 1 .,
PET-Diffusion: Unsupervised PET Enhancement Based on the Latent Diffusion Model,,Table 2 .,
3D Arterial Segmentation via Single 2D Projections and Depth Supervision in Contrast-Enhanced CT Images,1,Introduction,"Automated segmentation of blood vessels in 3D medical images is a crucial step for the diagnosis and treatment of many diseases, where the segmentation can aid in visualization, help with surgery planning, be used to compute biomarkers, and further downstream tasks. Automatic vessel segmentation has been extensively studied, both using classical computer vision algorithms [16] such as vesselness filters [8], or more recently with deep learning [3,5,6,11,19,21], where state-ofthe-art performance has been achieved for various vessel structures. Supervised deep learning typically requires large, well-curated training sets, which are often laborious to obtain. This is especially the case for 3D vessel segmentation.Manually delineating 3D vessels typically involves visualizing and annotating a 3D volume through a sequence of 2D cross-sectional slices, which is not a good medium for visualizing 3D vessels. This is because often only the cross-section of a vessel is visible in a 2D slice. In order to segment a vessel, the annotator has to track the cross-section of that vessel through several adjacent slices, which is especially tedious for curved or branching vessel trees. Projecting 3D vessels to a 2D plane allows for the entire vessel tree to be visible within a single 2D image, providing a more robust representation and potentially alleviating the burden of manual annotation. Kozinski et al. [13] propose to annotate up to three maximum intensity projections (MIP) for the task of centerline segmentation [13], obtaining results comparable to full 3D supervision. Compared to centerline segmentation, where the vessel diameter is disregarded, training a 3D vessel segmentation model from 2D annotations poses additional segmentationspecific challenges, as 2D projections only capture the outline of the vessels, providing no information about their interior. Furthermore, the axes of projection are crucial for the model's success, given the sparsity of information in 2D annotations.To achieve 3D vessel segmentation with only 2D supervision from projections, we first investigate which viewpoints to annotate in order to maximize segmentation performance. We show that it is feasible to segment the full extent of vessels in 3D images with high accuracy by annotating only a single randomlyselected 2D projection per training image. This approach substantially reduces the annotation effort, even compared to works training only on 2D projections. Secondly, by mapping the 2D annotations to the 3D space using the depth of the MIPs, we obtain a partially segmented 3D volume that can be used as an additional supervision signal. We demonstrate the utility of our method on the challenging task of peripancreatic arterial segmentation on contrast-enhanced arterial-phase computed tomography (CT) images, which feature large variance in vessel diameter. Our contribution to 3D vessel segmentation is three-fold:• Our work shows that highly accurate automatic segmentation of 3D vessels can be learned by annotating single MIPs. • Based on extensive experimental results, we determine that the best annotation strategy is to label randomly selected viewpoints, while also substantially reducing the annotation cost.• By incorporating additional depth information obtained from 2D annotations at no extra cost to the annotator, we almost close the gap between 3D supervision and 2D supervision."
3D Arterial Segmentation via Single 2D Projections and Depth Supervision in Contrast-Enhanced CT Images,2,Related Work,"Learning from Weak Annotations. Weak annotations have been used in deep learning segmentation to reduce the annotation effort through cheaper, less accurate, or sparser labeling [20]. Bai et al. [1] learn to perform aortic image segmentation by sparsely annotating only a subset of the input slices. Multiple instance learning approaches bin pixels together by only providing labels at the bin level. Jia et al. [12] use this approach to segment cancer on histopathology images successfully. Annotating 2D projections for 3D data is another approach to using weak segmentation labels, which has garnered popularity recently in the medical domain. Bayat et al. [2] propose to learn the spine posture from 2D radiographs, while Zhou et al. [22] use multi-planar MIPs for multi-organ segmentation of the abdomen. Kozinski et al. [13] propose to segment vessel centerlines using as few as 2-3 annotated MIPs. Chen et al. [4] train a vessel segmentation model from unsupervised 2D labels transferred from a publicly available dataset, however, there is still a gap to be closed between unsupervised and supervised model performance. Our work uses weak annotations in the form of annotations of 2D MIPs for the task of peripancreatic vessel segmentation, where we attempt to reduce the annotation cost to a minimum by only annotating a single projection per training input without sacrificing performance.Incorporating Depth Information. Depth is one of the properties of the 3D world. Loss of depth information occurs whenever 3D data is projected onto a lower dimensional space. In natural images, depth loss is inherent through image acquisition, therefore attempts to recover or model depth have been employed for 3D natural data. For instance, Fu et al. [9] use neural implicit fields to semantically segment images by transferring labels from 3D primitives to 2D images. Lawin et al. [14] propose to segment 3D point clouds by projecting them onto 2D and training a 2D segmentation network. At inference time, the predicted 2D segmentation labels are remapped back to the original 3D space using the depth information. In the medical domain, depth information has been used in volume rendering techniques [7] to aid with visualization, but it has so far not been employed when working with 2D projections of 3D volumes to recover information loss. We propose to do the conceptually opposite approach from Lawin et al. [14], by projecting 3D volumes onto 2D to facilitate and reduce annotation. We use depth information to map the 2D annotations to the original 3D space at annotation time and generate partial 3D segmentation volumes, which we incorporate in training as an additional loss term."
3D Arterial Segmentation via Single 2D Projections and Depth Supervision in Contrast-Enhanced CT Images,3,Methodology,"Overview. The maximum intensity projection (MIP) of a 3D volume I ∈ R Nx×Ny×Nz is defined as the highest intensity along a given axis:For simplicity, we only describe MIPs along the z-axis, but they can be performed on any image axis. Exploiting the fact that arteries are hyperintense in arterial phase CTs, we propose to annotate MIPs of the input volume for binary segmentation. The hyperintensities of the arteries ensures their visibility in the MIP, while additional processing removes most occluding nearby tissue (Sect. 4).Given a binary 2D annotation of a MIP A ∈ {0, 1} Nx×Ny , we map the foreground pixels in A to the original 3D image space. This is achieved by using the first and last z coordinates where the maximum intensity is observed along any projection ray. Owing to the fact that the vessels in the abdominal cavity are relatively sparse in 2D projections and most of the occluding tissue is removed in postprocessing, this step results in a fairly complete surface of the vessel tree. Furthermore, we can partially fill this surface volume, resulting in a 3D depth map D, which is a partial segmentation of the vessel tree. We use the 2D annotations as well as the depth map to train a 3D segmentation network in a weakly supervised manner.An overview of our method is presented in Fig. 1. In the following, we describe these components and how they are combined to train a 3D segmentation network in more detail.Depth Information. We can view MIP as capturing the intensity of the brightest pixel along each ray r xy ∈ R Nz , where r xy (z) = I(x, y, z). Along each projection ray, we denote the first and last z coordinates which have the same intensity as the MIP to be the forward depth z fw = arg max z I(x, y, z) and backward depth z bw = arg min z I(x, y, z). This information can be utilized for the following: (1) enhancing the MIP visualization, or (2) providing a way to map pixels from the 2D MIP back to the 3D space (depth map). The reason why the maximum intensity is achieved multiple times along a ray is because our images are clipped, which removes a lot of the intensity fluctuations. where α ∈ [0, 1]. Our final loss is a convex combination between: (a) the crossentropy(CE) of the network output projected to 2D and the 2D annotation, as well as (b) the cross-entropy between the network output and the depth map, but only applied to positive pixels in the depth map. Notably, the 2D loss constrains the shape of the vessels, while the depth loss promotes the segmentation of the vessel interior."
3D Arterial Segmentation via Single 2D Projections and Depth Supervision in Contrast-Enhanced CT Images,4,Experimental Design,"Dataset. We use an in-house dataset of contrast-enhanced abdominal computed tomography images (CTs) in the arterial phase to segment the peripancreatic arteries [6]. The cohort consists of 141 patients with pancreatic ductal adenocarcinoma, of an equal ratio of male to female patients. Given a 3D arterial CT of the abdominal area, we automatically extract the vertebrae [15,18] and semi-automatically extract the ribs, which have similar intensities as arteries in arterial CTs and would otherwise occlude the vessels. In order to remove as much of the cluttering surrounding tissue and increase the visibility of the vessels in the projections, the input is windowed so that the vessels appear hyperintense. Details of the exact preprocessing steps can be found in Table 2 of the supplementary material. The dataset contains binary 3D annotations of the peripancreatic arteries carried out by two radiologists, each having annotated half of the dataset. The 2D annotations we use in our experiments are projections of these 3D annotations. For more information about the dataset, see [6].Image Augmentation and Transformation. As the annotations lie on a 2D plane, 3D spatial augmentation cannot be used due to the information sparsity in the ground truth. Instead, we apply an invertible transformation T to the input volume and apply the inverse transformation T -1 to the network output before applying the loss, such that the ground truth need not be altered. A detailed description of the augmentations and transformations used can be found in Table 1 in the supplementary material.Training and Evaluation. We use a 3D U-Net [17] with four layers as our backbone, together with Xavier initialization [10]. A diagram of the network architecture can be found in Fig. 2 in the supplementary material. The loss weight α is tuned at 0.5, as this empirically yields the best performance. Our experiments are averaged over 5-fold cross-validation with 80 train samples, 20 validation samples, and a fixed test set of 41 samples. The network initialization is different for each fold but kept consistent across different experiments run on the same fold. This way, both data variance and initialization variance are accounted for through cross-validation. To measure the performance of our models, we use the Dice score, precision, recall, and mean surface distance (MSD). We also compute the skeleton recall as the percentage of the ground truth skeleton pixels which are present in the prediction. "
3D Arterial Segmentation via Single 2D Projections and Depth Supervision in Contrast-Enhanced CT Images,5,Results,"The Effectiveness of 2D Projections and Depth Supervision. We compare training using single random viewpoints with and without depth information against baselines that use more supervision. Models trained on full 3D ground truth represent the upper bound baseline, which is very expensive to annotate. We implement [13] as a baseline on our dataset, training on up to 3 fixed orthogonal projections. We distinguish between models selected according to the 2D performance on the validation set (2D) which is a fair baseline, and models selected according to the 3D performance on the validation set (3D), which is an unfair baseline as it requires 3D annotations on the validation set.With the exception of the single fixed viewpoint baselines where the models have the tendency to diverge towards over-or segmentation, we perform binary holefilling on the output of all of our other models, as producing hollow objects is a common under-segmentation issue.In Table 1 we compare our method against the 3D baseline, as well as baselines trained on multiple viewpoints. We see that by using depth information paired with training using a single random viewpoint per sample performs almost at the level of models trained on 3D labels, at a very small fraction of the annotation cost. The depth information also reduces model variance compared to the same setup without depth information. Even without depth information, training the model on single randomly chosen viewpoints offers a robust training signal that the Dice score is on par with training on 2 fixed viewpoints under ideal model selection at only half the annotation cost. Randomly selecting viewpoints for training acts as powerful data augmentation, which is why we are able to obtain performance comparable to using more fixed viewpoints. Under ideal 3D-based model selection, three views would come even closer to full 3D performance; however, with realistic 2D-based model selection, fixed viewpoints are more prone to diverge. This occurs because sometimes 2D-based model selection favors divergent models which only segment hollow objects, which cannot be fixed in postprocessing. Single fixed viewpoints contain so little information on their own that models trained on such input fail to learn how to segment the vessels and generally converge to over-segmenting in the blind spots in the projections. We conclude that using random viewpoints is not only helpful in reducing annotation cost but also decreases model variance.In terms of other metrics, randomly chosen projection viewpoints with and without depth improve both recall and skeleton recall even compared to fully 3D annotations, while generally reducing precision. We theorize that this is because the dataset itself contains noisy annotations and fully supervised models better overfit to the type of data annotation, whereas our models converge to following the contrast and segmenting more vessels, which are sometimes wrongfully labeled as background in the ground truth. MSD are not very telling in our dataset due to the noisy annotations and the nature of vessels, as an under-or over-segmented vessel branch can quickly translate into a large surface distance.The Effect of Dataset Size. We vary the size of the training set from |D tr | = 80 to as little as |D tr | = 10 samples, while keeping the size of the validation and test sets constant, and train models on single random viewpoints.In Table 2, we compare single random projections trained with and without depth information at varying dataset sizes to ilustrate the usefulness of the depth information with different amounts of training data. Our depth loss offers consistent improvement across multiple dataset sizes and reduces the overall performance variance. The performance boost is noticeable across the board, the only exception being precision. The smaller the dataset size is, the greater the performance boost from the depth. We perform a Wilcoxon rank-sum statistical test comparing the individual sample predictions of the models trained at various dataset sizes with single random orthogonal viewpoints with or without depth information, obtaining a statistically significant (p-value of < 0.0001). We conclude that the depth information complements the segmentation effectively. "
3D Arterial Segmentation via Single 2D Projections and Depth Supervision in Contrast-Enhanced CT Images,6,Conclusion,"In this work, we present an approach for 3D segmentation of peripancreatic arteries using very sparse 2D annotations. Using a labeled dataset consisting of single, randomly selected, orthogonal 2D annotations for each training sample and additional depth information obtained at no extra cost, we obtain accuracy almost on par with fully supervised models trained on 3D data at a mere fraction of the annotation cost. Limitations of our work are that the depth information relies on the assumption that the vessels exhibit minimal intensity fluctuations within local neighborhoods, which might not hold on other datasets, where more sophisticated ray-tracing methods would be more effective in locating the front and back of projected objects. Furthermore, careful preprocessing is performed to eliminate occluders, which would limit its transferability to datasets with many occluding objects of similar intensities. Further investigation is needed to quantify how manual 2D annotations compare to our 3D-derived annotations, where we expect occluders to affect the annotation process."
3D Arterial Segmentation via Single 2D Projections and Depth Supervision in Contrast-Enhanced CT Images,,Fig. 1 .,
3D Arterial Segmentation via Single 2D Projections and Depth Supervision in Contrast-Enhanced CT Images,,Fig. 2 . 1 . 2 . 3 .,
3D Arterial Segmentation via Single 2D Projections and Depth Supervision in Contrast-Enhanced CT Images,,Table 1 .,
3D Arterial Segmentation via Single 2D Projections and Depth Supervision in Contrast-Enhanced CT Images,,Table 2 .,
3D Arterial Segmentation via Single 2D Projections and Depth Supervision in Contrast-Enhanced CT Images,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0_14.
Mesh2SSM: From Surface Meshes to Statistical Shape Models of Anatomy,1,Introduction,"Statistical shape modeling (SSM) is a powerful tool in medical image analysis and computational anatomy to quantify and study the variability of anatomical structures within populations. SSM has shown great promise in medical research, particularly in diagnosis [12,23], pathology detection [19,25], and treatment planning [27]. SSM has enabled researchers to better understand the underlying biological processes, leading to the development of more accurate and personalized diagnostic and treatment plans [3,9,14,17].Over the years, several SSM approaches have been developed that implicitly represent the shapes (deformation fields [8], level set methods [22]) or explicitly represent them as a ordered set of landmarks or correspondence points (aka point distribution models, PDMs). Here, we focus on the automated construction of PDMs because, compared to deformation fields, point correspondences are easier to interpret by clinicians, are computationally efficient for large datasets, and less sensitive to noise and outliers than deformation fields [5].SSM performance depends on the underlying process used to generate shape correspondences and the quality of the input data. Various correspondence generation methods exist, including non-optimized landmark estimation and parametric and non-parametric correspondence optimization. Non-optimized methods manually label a reference shape and warp the annotated landmarks using registration techniques [10,16,18]. Parametric methods use fixed geometrical bases to establish correspondences [26], while group-wise non-parametric approaches find correspondences by considering the variability of the entire cohort during the optimization process. Examples of non-parametric methods include particlebased optimization [4] and Minimum Description Length (MDL) [7].Traditional SSM methods assume that population variability follows a Gaussian distribution, which implies that a linear combination of training shapes can express unseen shapes. However, anatomical variability can be far more complex than this linear approximation, in which case nonlinear variations normally exist (e.g., bending fingers, soft tissue deformations, and vertebrae with different types). Furthermore, conventional SSM pipelines are computationally intensive, where inferring PDMs on new samples entail an optimization process. Deep learning-based approaches for SSM have emerged as a promising avenue to overcoming these limitations. Deep learning models can learn complex nonlinear representations of the shapes, which can be used to generate shape models. Moreover, they can efficiently perform inference on new samples without computation overhead or re-optimization. Recent works such as FlowSSM [15], ShapeFlow [11], DeepSSM [2], and VIB-DeepSSM [1] have incorporated deep learning to generate shape models. FlowSSM [15] and ShapeFlow [11] operate on surface meshes and use neural networks to parameterize the deformations field between two shapes in a low dimensional latent space and rely on an encoderfree setup. Encoder-free methods randomly initialize the latent representations for each sample that are then optimized to produce the optimal deformations. One major caveat of an encoder-free setup is that inference on new meshes is no longer straightforward; the latent representation has to be re-optimized for every new sample. On the other hand, DeepSSM [2], TL-DeepSSM [2], and VIB-DeepSSM [1] learn the PDM directly from unsegmented CT/MRI images, and hence alleviate the need for PDM optimization given new samples and can bypass anatomy segmentation by operating directly on unsegmented images. However, these methods rely on supervised losses and require volumetric images, segmented images, and established/optimized PDMs for training. This reliance on supervised losses introduces linearity assumptions in generating ground truth PDMs. TL-DeepSSM [2], a variant of DeepSSM [2], differs from the others by not utilizing PCA scores as shape descriptors. Instead, it adopts an established correspondence model hence, similar to the vanilla DeepSSM [2] learns a linear model.In this paper, we introduce Mesh2SSM1 , a deep learning method that addresses the limitations of traditional and deep learning-based SSM approaches. Mesh2SSM leverages unsupervised, permutation-invariant representation learning to learn the low dimensional nonlinear shape descriptor directly from mesh data and uses the learned features to generate a correspondence model of the population. Mesh2SSM also includes an analysis network that operates on the learned correspondences to obtain a data-driven template point cloud (i.e., template point cloud), which can replace the initial template, and hence reducing the bias that could arise from template selection. Furthermore, the learned representation of meshes can be used for predicting related quantities that rely on shape. Our main contributions are:1. We introduce Mesh2SSM, a fully unsupervised correspondence generation deep learning framework that operates directly on meshes. Mesh2SSM uses an autoencoder to extract the shape descriptor of the mesh and uses this descriptor to transform a template point cloud using IM-Net [6]. 2. The proposed method uses an autoencoder that combines geodesic distance features and EdgeConv [28] (dynamic graph convolution neural network) to extract meaningful feature representation of each mesh that is permutationinvariant. 3. Mesh2SSM also includes a variational autoencoder (VAE) [13,21] operating on the learned correspondence points and trained end-to-end with correspondence generation network. This VAE branch serves two purposes: (a) serves as a shape analysis module for the non-linear shape variations and (b) learns a data-specific template from the latent space of the correspondences that is fed back to the correspondence generation network.To motivate the need for the mesh feature encoder and study the effect of the template selection, we considered the box-bump dataset, a synthetic dataset of 3D shapes of boxes with a moving bump. In Fig. 1, we compare Mesh2SSM (sans the VAE analysis branch) with FlowSMM [15] since this approach is the closest to Mesh2SSM. We performed experiments with three templates: medoid, sphere, and box without the bump. Although both methods show some sensitivity to the choice of template, FlowSSM is more sensitive toward the choice of the template than Mesh2SSM. Moreover, FlowSSM fails to identify the correct mode of variation, the horizontal movement of the bump as the primary variation, which can also be inferred by comparing the compactness curves in Fig. 1.c. Mesh2SSM performs best when the template is a medoid shape, which makes the case for learning a data-specific template. Since Mesh2SSM model uses an autoencoder, inference on unseen meshes only requires a single forward pass (1 s per sample); FlowSSM requires re-optimization, increasing the inference time drastically and require a convergence criteria to determine the best number of iterations per sample (0.15 s for one iterations per sample).  [15] with three templates: sphere, box without a bump, and medoid shape. FlowSSM fails to capture the horizontal movement as the primary mode of variation. (c) The compactness curves for both models with different templates. The overview of the proposed pipeline is provided in Fig. 2. This section provides a brief description of each module."
Mesh2SSM: From Surface Meshes to Statistical Shape Models of Anatomy,2,Method,
Mesh2SSM: From Surface Meshes to Statistical Shape Models of Anatomy,2.1,Correspondence Generation,"Given a set of N aligned surface meshes X = {X 1 , X 2 , ...X N }, each mesh X i = (V i , E i ), where V i and E i represent the vertices and edge connectivity, respectively. The goal of the model is to predict a set C i of M 3D correspondence points that fully describe each surface X i and are anatomically consistent across all meshes. This goal is achieved by learning a low dimensional representation of the surface mesh z m ∈ R L using the mesh autoencoder and then z m is used to transform the template point cloud via the implicit field decoder (IM-Net) [6]. The network optimization is driven primarily by point-set to point-set two-way Chamfer distance between the learned correspondence point sets C i and the vertex locations V i of the original meshes. To ensure that the encoder learns useful features for the task, we regularize the optimization using the vertex reconstruction loss of the autoencoder between the input V i and the predicted Vi . The correspondence loss function is given by:where α, γ are the hyperparameters. We consider a combination of L 1 and L 2 two-way Chamfer distance for numerical stability as the magnitude of L 2 loss can be low over epochs and L 1 can compensate for it. The correspondence generation uses two networks:"
Mesh2SSM: From Surface Meshes to Statistical Shape Models of Anatomy,,Mesh Autoencoder (M-AE):,"We use EdgeConv [28] blocks, which are dynamic graph convolution neural network (DGCNN) blocks in the encoder and decoder to capture local geometric features of the mesh. The model takes vertices as input, computes an edge feature set of size k (using nearest neighbors) for each vertex at an EdgeConv layer, and aggregates features within each set to compute EdgeConv responses. The output features of the last EdgeConv layer are then globally aggregated to form a 1D global descriptor z m i of the mesh. The first EdgeConv block uses geodesic distance on the surface of the mesh to calculate the k features. The dynamic feature creation property of EdgeConv and the global pooling make this autoencoder permutation invariant."
Mesh2SSM: From Surface Meshes to Statistical Shape Models of Anatomy,,Implicit Field Decoder (IM-NET):,"The IM-NET [6] architecture consists of fully connected layers with non-linearity and skip-layer connections. This network enforces the notion of correspondence across the samples. The network takes in two inputs, the latent representation of the mesh z m and a template point cloud (a set of unordered points). IM-NET estimates the deformation of each point in the template required to deform the template to each sample, conditioned on z m . Based on the learned deformation, IM-NET directly produces the resultant displaced template point without the computational complexity of the deformation fields. Correspondence is established since the same template is deformed to all the samples."
Mesh2SSM: From Surface Meshes to Statistical Shape Models of Anatomy,2.2,Analysis,The Mesh2SSM model also consists of an analysis branch that acts as a shape analysis module to capture non-linear shape variations identified by the learned correspondences {C i } N i=1 and also learns a data-informed template from the latent space of correspondences to be fed back into the correspondence generation network during training. This branch uses one network module:
Mesh2SSM: From Surface Meshes to Statistical Shape Models of Anatomy,,Shape Variation Autoencoder (SP-VAE):,"The VAE [13,21] is a latent variable model parameterized by an encoder φ, decoder θ, and the prior p(z p ) ∼ N (0, I). The encoder maps the shape represented by the learned correspondence points C to the latent space and the decoder reconstructs the correspondences from the latent representation z p . By capturing the underlying structure of the PDM through a low-dimensional representation, SP-VAE allows for the estimation of the mean shape of the learned correspondences. The SP-VAE is trained using the loss function given by:The main difference between M-AE and a SP-VAE lies in the input and output representations they handle. SP-VAE operates directly on sets of landmarks or correspondences, aiding in the analysis of shape models. It takes a set of correspondences describing a shape as input and aims to learn a compressed latent representation of the shape. Importantly, the SP-VAE maintains the same ordering of correspondences at the input and output, so it does not use permutation-invariant layers or operations like pooling."
Mesh2SSM: From Surface Meshes to Statistical Shape Models of Anatomy,2.3,Training,"We begin with a burn-in stage, where only the correspondence generation module is trained while the analysis module is frozen. After the burn-in stage, alternate optimization of the correspondence and analysis module begins. During the alternate optimization phase, we generate the data-informed template from the latent space of SP-VAE at regular intervals. The learned data-informed template is used in the correspondence generation module in the subsequent epochs. For the learned template, we sample 500 samples from the prior p(z p ) ∼ N (0, I) and pass it through the decoder of SP-VAE to get the reconstructed correspondence point set. The mean template is defined by taking the average of these generated samples. Inference with unseen meshes is straight forward; the meshes are passed through the mesh encoder and IM-NET of the correspondence generation module to get the predicted correspondences. All hyperparameters and network architecture details are mentioned in the supplementary material."
Mesh2SSM: From Surface Meshes to Statistical Shape Models of Anatomy,3,Experiments and Discussion,"Dataset: We use the publicly available Decath-Pancreas dataset of 273 segmentations from patients who underwent pancreatic mass resection [24]. The shapes of the pancreas are highly variable and have thin structures, making it a good candidate for non-linear SSM analysis. The segmentations were isotropically resampled, smoothed, centered, and converted to meshes with roughly 2000 vertices. Although the DGCNN mesh autoencoder used in Mesh2SSM does not require the same number of vertices, uniformity across the dataset makes it computationally efficient; hence, we pad the smallest mesh by randomly repeating the vertices (akin to padding image for convolutions). The samples were randomly divided, with 218 used for training, 26 for validation, and 27 for testing. FlowSSM [15] with two templates: sphere, medoid. The color map and arrows show the signed distance and direction from the mean shape."
Mesh2SSM: From Surface Meshes to Statistical Shape Models of Anatomy,3.1,Results,"We perform experiments with two templates: sphere and medoid. We compare the performance of FlowSSM [15] with Mesh2SSM with the template feedback loop. For Mesh2SSM template, we use 256 points uniformly spread across the surface of the sample. Mesh2SSM and FlowSSM do not have a equivalent latent space for comparison of the shape models, hence, we consider the deformed mesh vertices of FlowSSM as correspondences and perform PCA analysis. Figure 3 shows the top three PCA modes of variations identified by Mesh2SSM and FlowSSM. Similar to the observations made box-bump dataset, FlowSSM is affected by the choice of the template, and the modes of variation differ as the template changes. On the other hand, PDM predicted by Mesh2SSM identifies the same primary modes consistently. Pancreatic cancer mainly presents itself on the head of the structure [20] and for the Decath dataset, we can see the first mode identifies the change in the shape of the head. We evaluate the models based on compactness, generalization, and specificity. Compactness measures  the ability of the model to reconstruct new shape instances with fewer parameters using PCA explained variance. Generalization measures the average surface distance between all test shapes and their reconstructions, and specificity measures the distance between randomly generated PCA samples. Figure 4.a shows the metrics for the pancreas dataset. Mesh2SSM outperforms FlowSSM in all three metrics, despite using only 256 correspondence points compared to FlowSSM's ∼2000 vertices. Mesh2SSM correspondence generation module efficiently parameterizes the surface of the pancreas with a minimum number of parameters. Mesh2SSM template, shown in Fig. 4.b, becomes more detailed as optimization continues, regardless of the starting template. The model can learn correct deformations in the correspondence generation module and identify the correct mean shape in the latent space of SP-VAE in the analysis module. Using the analysis module of Mesh2SSM, we visualized the top three modes of variation identified by sorting the latent dimensions of SP-VAE based on the standard deviations of the latent embeddings of the training dataset. Variations are generated by perturbing the latent representation of a sample in three directions, resulting in non-linear modes such as changes in the size and shape of the pancreas head and narrowing of the neck and body. This is shown in Fig. 4.c for MeshSSM model with medoid starting template. The distance metrics for the reconstructions of the testing samples were also computed. The results of the metrics are summarized in Table 1. The calculation involved the L 1 Chamfer loss between the predicted points (correspondences in the case of Mesh2SSM and the deformed mesh vertices in the case of FlowSSM) and the original mesh vertices. Additionally, the surface to surface distance of the mesh reconstructions (using the correspondences in Mesh2SSM and deformed meshes in FlowSSM) was included. For the pancreas dataset with the medoid as the initial template, Mesh2SSM with the template feedback produced more precise models."
Mesh2SSM: From Surface Meshes to Statistical Shape Models of Anatomy,3.2,Limitations and Future Scope,"As SSM is included a part of diagnostic clinical support systems, it is crucial to address the drawbacks of the models. Like most deep learning models, performance of Mesh2SSM could be affected by small dataset size, and it can produce overconfident estimates. An augmentation scheme and a layer uncertainty calibration are could improve its usability in medical scenarios. Additionally, enforcing disentanglement in the latent space of SP-VAE can make the analysis module interpretable and allow for effective non-linear shape analysis by clinicians."
Mesh2SSM: From Surface Meshes to Statistical Shape Models of Anatomy,4,Conclusion,"The paper presents a new systematic approach of generating non-linear statistical shape models using deep learning directly from meshes, which overcomes the limitations of traditional SSM and current deep learning approaches. The use of an autoencoder for meaningful feature extraction of meshes to learn the PDM provides a versatile and scalable framework for SSM. Incorporating template feedback loop via VAE [13,21] analysis module helps in mitigating bias and capturing non-linear characteristics of the data. The method is demonstrated to have superior performance in identifying shape variations using fewer parameters on synthetic and clinical datasets. To conclude, our method of generating highly accurate and detailed models of complex anatomical structures with reduced computational complexity has the potential to establish statistical shape modeling from non-invasive imaging as a powerful diagnostic tool."
Mesh2SSM: From Surface Meshes to Statistical Shape Models of Anatomy,,Fig. 1 .,
Mesh2SSM: From Surface Meshes to Statistical Shape Models of Anatomy,,Fig. 2 .,
Mesh2SSM: From Surface Meshes to Statistical Shape Models of Anatomy,,Fig. 3 .,
Mesh2SSM: From Surface Meshes to Statistical Shape Models of Anatomy,,Fig. 4 .,
Mesh2SSM: From Surface Meshes to Statistical Shape Models of Anatomy,,Table 1 .,
Mesh2SSM: From Surface Meshes to Statistical Shape Models of Anatomy,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0 59.
Cross-Adversarial Local Distribution Regularization for Semi-supervised Medical Image Segmentation,1,Introduction,"Medical image segmentation is a critical task in computer-aided diagnosis and treatment planning. It involves the delineation of anatomical structures or pathological regions in medical images, such as magnetic resonance imaging (MRI) or computed tomography (CT) scans. Accurate and efficient segmentation is essential for various medical applications, including tumor detection, surgical planning, and monitoring disease progression. However, manual medical imaging annotation is time-consuming and expensive because it requires the domain knowledge from medical experts. Therefore, there is a growing interest in developing semi-supervised learning that leverages both labeled and unlabeled data to improve the performance of image segmentation models [16,27].Existing semi-supervised segmentation methods exploit smoothness assumption, e.g., the data samples that are closer to each other are more likely to to have the same label. In other words, the smoothness assumption encourages the model to generate invariant outputs under small perturbations. We have seen such perturbations being be added to natural input images at data-level [4,9,14,19,21], feature-level [6,17,23,25], and model-level [8,11,12,24,28]. Among them, virtual adversarial training (VAT) [14] is a well-known one which promotes the smoothness of the local output distribution using adversarial examples. The adversarial examples are near decision boundaries generated by adding adversarial perturbations to natural inputs. However, VAT can only create one adversarial sample in a run, which is often insufficient to completely explore the space of possible perturbations (see Sect. 2.1). In addition, the adversarial examples of VAT can also lie together and lose diversity that significantly reduces the quality of adversarial examples [15,20]. Mixup regularization [29] is a data augmentation method used in deep learning to improve model generalization. The idea behind mixup is to create new training examples by linearly interpolating between pairs of existing examples and their corresponding labels, which has been adopted in [2,3,19] to semi-supervised learning. The work [5] suggests that Mixup improves the smoothness of the neural function by bounding the Lipschitz constant of the gradient function of the neural networks. However, we show that mixing between more informative samples (e.g., adversarial examples near decision boundaries) can lead to a better performance enhancement compared to mixing natural samples (see Sect. 3.3).In this paper, we propose a novel cross-adversarial local distribution regularization for semi-supervised medical image segmentation for smoothness assumption enhancement1 . Our contributions are summarized as follows: 1) To overcome the VAT's drawback, we formulate an adversarial local distribution (ALD) with Dice loss function that covers all possible adversarial examples within a ball constraint. 2) To enhance smoothness assumption, we propose a novel cross-adversarial local distribution regularization (Cross-ALD) to encourage the smoothness assumption, which is a random mixing between two ALDs."
Cross-Adversarial Local Distribution Regularization for Semi-supervised Medical Image Segmentation,,3),"We also propose a sufficiently approximation for the Cross-ALD by a multiple particle-based search using semantic feature Stein Variational Gradient Decent (SVGDF), an enhancement of the vanilla SVGD [10]. 4) We conduct comprehensive experiments on ADCD [1] and LA [26] datasets, showing that our Cross-ALD regularization achieves state-of-the-art performance against existing solutions [8,11,12,14,21,22,28]."
Cross-Adversarial Local Distribution Regularization for Semi-supervised Medical Image Segmentation,2,Method,"In this section, we begin by reviewing the minimax optimization problem of virtual adversarial training (VAT) [14]. Given an input, we then formulate a novel adversarial local distribution (ALD) with Dice loss, which benefits the medical semi-supervised image segmentation problem specifically. Next, a crossadversarial local distribution (Cross-ALD) is constructed by randomly combining two ALDs. We approximate the ALD by a particle-based method named semantic feature Stein Variational Gradient Descent (SVGDF). Considering the resolution of medical images are usually high, we enhance the vanilla SVGD [10] from data-level to feature-level, which is named SVGDF. We finally provide our regularization loss for semi-supervised medical image segmentation."
Cross-Adversarial Local Distribution Regularization for Semi-supervised Medical Image Segmentation,2.1,The Minimax Optimization of VAT,"Let D l and D ul be the labeled and unlabeled dataset, respectively, with P D l and P D ul being the corresponding data distribution. Denote x ∈ R d as our ddimensional input in a space X. The labeled image x l and segmentation groundtruth y are sampled from the labeled dataset D l (x l , y ∼ P D l ), and the unlabeled image sampled from D ul is x ∼ P D ul .Given an input x ∼ P D ul (i.e., the unlabeled data distribution), let us denote the ball constraint around the image x aswhere is a ball constraint radius with respect to a norm || • || p , and x is an adversarial example2 . Given that f θ is our model parameterized by θ, VAT [14] trains the model with the loss of vat that a minimax optimization problem:where D KL is the Kullback-Leibler divergence. The inner maximization problem is to find an adversarial example near decision boundaries, while the minimization problem enforces the local smoothness of the model. However, VAT is insufficient to explore the set of of all adversarial examples within the constraint C because it only find one adversarial example x given a natural input x. Moreover, the works [15,20] show that even solving the maximization problem with random initialization, its solutions can also lie together and lose diversity, which significantly reduces the quality of adversarial examples."
Cross-Adversarial Local Distribution Regularization for Semi-supervised Medical Image Segmentation,2.2,Adversarial Local Distribution,"In order to overcome the drawback of VAT, we introduce our proposed adversarial local distribution (ALD) with Dice loss function instead of D KL in [14,15]. ALD forms a set of all adversarial examples x within the ball constraint given an input x. Therefore, the distribution can helps to sufficiently explore all possible adversarial examples. The adversarial local distribution P θ (x |x) is defined with a ball constraint C as follow:where P θ (•|x) is the conditional local distribution, and Z(x; θ) is a normalization function. The Dice is the Dice loss function as shown in Eq. 3where C is the number of classes. p θ ( ŷc |x) and p θ ( ỹc |x ) are the predictions of input image x and adversarial image x , respectively."
Cross-Adversarial Local Distribution Regularization for Semi-supervised Medical Image Segmentation,2.3,Cross-Adversarial Distribution Regularization,"Given two random samples x i , x j ∼ P D (i = j), we define the cross-adversarial distribution (Cross-ALD) denoted Pθ as shown in Eq. 4where γ ∼ Beta(α, α) for α ∈ (0, ∞), inspired by [29]. The Pθ is the Cross-ALD distribution, a mixture between the two adversarial local distributions. Given Eq. 4, we propose the Cross-ALD regularization at two random input images x i , x j ∼ P D (i = j) aswhere H indicates the entropy of a given distribution. When minimizing R(θ, x i , x j ) or equivalently -H(P θ (•|x i , x j )) w.r.t. θ, we encourage P θ (•|x i , x j ) to be closer to a uniform distribution. This implies that the outputs of f ( x ) = f ( x ) = a constant c, where x , x ∼ Pθ (•|x i , x j ). In other words, we encourages the invariant model outputs under small perturbations. Therefore, minimizing the Cross-ALD regularization loss leads to an enhancement in the model smoothness. While VAT only enforces local smoothness using one adversarial example, Cross-ALD further encourages smoothness of both local and mixed adversarial distributions to improve the model generalization."
Cross-Adversarial Local Distribution Regularization for Semi-supervised Medical Image Segmentation,2.4,Multiple Particle-Based Search to Approximate the Cross-ALD Regularization,"In Eq. 2, the normalization Z(x; θ) in denominator term is intractable to find. Therefore, we propose a multiple particle-based search method named SVGDF to sample x (1) , x (2) , . . . , x (N ) ∼ P θ (•|x)). N is the number of samples (or adversarial particles). SVGDF is used to solve the optimization problem of finding a target distribution P θ (•|x)). SVGDF is a particle-based Bayesian inference algorithm that seeks a set of points (or particles) to approximate the target distribution without explicit parametric assumptions using iterative gradient-based updates. Specifically, a set of adversarial particles (x (n) ) is initialized by adding uniform noises, then projected onto the ball C . These adversarial particles are then iteratively updated using a closed-form solution (Eq. 6) until reaching termination conditions (, number of iterations).[k(Φ(x (j),(l) ), Φ(x ))∇ x (j),(l) log P (x (j),(l) |x)where x (n),(l) is a n th adversarial particle at l th iteration (n ∈ {1, 2, ..., N }, and l ∈ {1, 2, ..., L} with the maximum number of iteration L). C is projection operator to the C constraint. τ is the step size updating. k is the radial basis function. Φ is a fixed feature extractor (e.g., encoder of U-Net/V-Net). While vanilla SVGD [10] is difficult to capture semantic meaning of high-resolution data because of calculating RBF kernel (k) directly on the data-level, we use the feature extractor Φ as a semantic transformation to further enhance the SVGD algorithm performance for medical imaging. Moreover, the two terms of φ in Eq. 6 have different roles: (i) the first one encourages the adversarial particles to move towards the high density areas of P θ (•|x) and (ii) the second one prevents all the particles from collapsing into the local modes of P θ (•|x) to enhance diversity (e.g.,pushing the particles away from each other). Please refer to the Cross-ALD Github repository for more details.SVGDF approximates P θ (•|x i ) and P θ (•|x j ) in Eq. 4, where x i , x j ∼ P D ul (i = j). We form sets of adversarial particles as}. The problem (5) can then be relaxed towhere γ ∼ Beta(α, α) for α ∈ (0, ∞)."
Cross-Adversarial Local Distribution Regularization for Semi-supervised Medical Image Segmentation,2.5,Cross-ALD Regularization Loss in Medical Semi-supervised Image Segmentation,"In this paper, the overall loss function total consists of three loss terms. The first term is the dice loss, where labeled image x l and segmentation ground-truth y are sampled from labeled dataset D l . The second term is a contrastive learning loss for inter-class separation cs proposed by [21]. The third term is our Cross-ALD regularization, which is an enhancement of vat to significantly improve the model performance.where λ cs and λ Cross-ALD are the corresponding weights to balance the losses. Note that our implementation is replacing vat loss with the proposed Cross-AD regularization in SS-Net code repository3  [21] to reach the state-of-the-art performance."
Cross-Adversarial Local Distribution Regularization for Semi-supervised Medical Image Segmentation,3,Experiments,"In this section, we conduct several comprehensive experiments using the ACDC4 dataset [1] and the LA5 dataset [26] for 2D and 3D image segmentation tasks, respectively. For fair comparisons, all experiments are conducted using the identical setting, following [21]. We evaluate our model in challenging semi-supervised scenarios, where only 5% and 10% of the data are labeled and the remaining data in the training set is treated as unlabeled. The Cross-ALD uses the U-Net [18] and V-Net [13] architectures for the ACDC and LA dataset, respectively. We compare the diversity between the adversarial particles generated by our method against vanilla SVGD and VAT with random initialization in Sect. 3.1 . We then illustrate the Cross-AD outperforms other recent methods on ACDC and LA datasets in Sect. 3.2. We show ablation studies in Sect. 3.3. The effect of the number particles to the model performance is studied in the Cross-ALD Github repository."
Cross-Adversarial Local Distribution Regularization for Semi-supervised Medical Image Segmentation,3.1,Diversity of Adversarial Particle Comparison,"Settings. We fixed all the decoder models (U-Net for ACDC and V-Net for LA). We run VAT with random initialization and SVGD multiple times to produce adversarial examples, which we compared to the adversarial particles generated using SVGDF. SVGDF is the proposed algorithm, which leverages feature transformation to capture the semantic meaning of inputs. Φ is the decoder of U-Net in ACDC dataset, while Φ is the decoder of V-Net in LA dataset. We set the same radius ball constraint, updating step, and etc. We randomly pick three images from the datasets to generate adversarial particles. To evaluate their diversity, we report the sum squared error (SSE) between these particles. Higher SSE indicates more diversity, and for each number of particles, we calculate the average of the mean of SSEs.  Results. Note that the advantage of SVGD over VAT is that the former generates diversified adversarial examples because of the second term in Eq. 6 while VAT only creates one example. Moreover, vanilla SVGD is difficult to capture semantic meaning of high-resolution medical imaging because it calculates kernel k on image-level. In Fig. 1, our SVGDF produces the most diverse particles compared to SVGD and VAT with random initialization."
Cross-Adversarial Local Distribution Regularization for Semi-supervised Medical Image Segmentation,3.2,Performance Evaluation on the ACDC and la Datasets,"Settings. We use the metrics of Dice, Jaccard, 95% Hausdorff Distance (95HD), and Average Surface Distance (ASD) to evaluate the results. We compare our Cross-ALD to six recent methods including UA-MT [28] (MICCAI '19), SASSNet [8] (MICCAI '20), DTC [11] (AAAI'21) , URPC [12] (MICCAI'21) , MC-Net [22] (MICCAI '21), and SS-Net [21] (MICCAI '22). The loss weights λ Cross-ALD and λ cs are set as an iteration dependent warming-up function [7], and number of particles N = 2. All experiments are conducted using the identical settings in the Github repository 6 [21] for fair comparisons.Results. Recall that our Cross-ALD generates diversified adversarial particles using SVGDF compared to vanilla SVGD and VAT, and further enhances smoothness of cross-adversarial local distributions. In Table 1 and 2, the Cross-ALD can significantly outperform other recent methods with only 5%/10% labeled data training based on the four metrics. Especially, our method impressively gains 14.7% and 2.3% Dice score higher than state-of-the-art SS-Net using 5% labeled data of ACDC and LA, respectively. Moreover, the visualized results of Fig. 2 shows Cross-ALD can segment the most organ details compared to other methods."
Cross-Adversarial Local Distribution Regularization for Semi-supervised Medical Image Segmentation,3.3,Ablation Study,"Settings. We use the same network architectures and parameter settings in Sect. 3.2, and train the models with 5% labeled training data of ACDC and LA. We illustrate that crossing adversarial particles is more beneficial than random  mixup between natural inputs (RanMixup [29]) because these particles are near decision boundaries. Recall that our SVGDF is better than VAT and SVGD by producing more diversified adversarial particles. Applying SVGDF's particles and cs (SVGDF + cs ) to gain the model performance in the semi-supervised segmentation task, while Cross-ALD efficiently enhances smoothness to significantly improve the generalization.Result. Table 3 shows that mixing adversarial examples from VAT outperform those from RanMixup. While SVGDF + cs is better than SVGD and VAT, the proposed Cross-ALD achieves the most outstanding performance among comparisons methods. In addition, our method produces more accurate segmentation masks compared to the ground-truth, as shown in Fig. 2."
Cross-Adversarial Local Distribution Regularization for Semi-supervised Medical Image Segmentation,4,Conclusion,"In this paper, we have introduced a novel cross-adversarial local distribution (Cross-ALD) regularization that extends and overcomes drawbacks of VAT and Mixup techniques. In our method, SVGDF is proposed to approximate Cross-ALD, which produces more diverse adversarial particles than vanilla SVGD and VAT with random initialization. We adapt Cross-ALD to semi-supervised medical image segmentation to achieve start-of-the-art performance on the ACDC and LA datasets compared to many recent methods such as VAT [14], UA-MT [28], SASSNet [8], DTC [11], URPC [12] , MC-Net [22], and SS-Net [21]."
Cross-Adversarial Local Distribution Regularization for Semi-supervised Medical Image Segmentation,,Fig. 1 .,
Cross-Adversarial Local Distribution Regularization for Semi-supervised Medical Image Segmentation,,Fig. 2 .,
Cross-Adversarial Local Distribution Regularization for Semi-supervised Medical Image Segmentation,,Table 1 .,
Cross-Adversarial Local Distribution Regularization for Semi-supervised Medical Image Segmentation,,Table 2 .,
Cross-Adversarial Local Distribution Regularization for Semi-supervised Medical Image Segmentation,,Table 3 .,
Cross-Adversarial Local Distribution Regularization for Semi-supervised Medical Image Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0 18.
Infusing Physically Inspired Known Operators in Deep Models of Ultrasound Elastography,1,Introduction,"Ultrasound Elastography (USE) provides information related to the stiffness of the tissue. Ultrasound (US) data before and after the tissue deformation (which can be caused by an external or internal force) are collected and compared to calculate the displacement map, indicating each individual sample's relative motion. The strain is computed by taking the derivative of the displacement fields. In free-hand palpation, the force is external and applied by the operator by the probe [10].Convolutional Neural Networks (CNN) have been successfully employed for USE displacement estimation [11,12,15]. Unsupervised and semi-supervised training methods have been proposed, which enable the networks to use real US images for training [1,14,17]. The proposed networks have achieved high-quality axial strains. In contrast to axial strain, lateral strain, which is highly required in Poisson's ratio imaging and elasticity reconstruction, has a poor quality due to the low sampling frequency, limited motion and lack of carrier signal in the lateral direction.Recently, physically inspired constraint in unsupervised regularized elastography (PICTURE) has been proposed [5]. This method aims to improve lateral displacement by exploiting the high-quality axial displacement estimation and the relation between the lateral and axial strains defined by the physics of motion. Despite the substantial improvement, the regularization is only applied during the training phase. In addition, only a considerably large feasible range for Poisson's ratio was enforced, thereby providing further opportunities for the network to contravene the laws of physics.Known operators, introduced by Maier et al. [7], have been widely utilized in deep neural networks. The core idea is that some known operations (for example inversion of a matrix) are embedded inside the networks to simplify the training and improving the generalization ability of the network. The known operator can be viewed as the prior knowledge related to the physics of the problem. Maier et al. investigated known operators in different applications such as computed tomography, magnetic resonance imaging, and vessel segmentation, and showed a substantial reduction in the maximum error bounds [7].In this paper, we aim to embed two lateral displacement refinement algorithms in the CNNs to improve the lateral strains. The first algorithm limits the range of Effective Poisson's Ratio (EPR) inside the feasible range during the test time. It is important to note that in contrast to [5], the EPR range is enforced using the regularization during the training phase and the known operators framework during the test phase; therefore, it is enforced during both training and test phases. The second algorithm employs the refinement method proposed be Gou et al. [2] which exploits incompressibility constraint to refine the lateral displacement. The network weight and a demo code are publicly available online at http://code.sonography.ai."
Infusing Physically Inspired Known Operators in Deep Models of Ultrasound Elastography,2,Materials and Methods,"In this section, we first provide a brief overview of PICTURE and underlie some differences to this work. We then introduce our method for incorporating known operators into our deep model and outline our unsupervised training technique. We then present the training and test datasets and finish the section by demonstrating the network architecture."
Infusing Physically Inspired Known Operators in Deep Models of Ultrasound Elastography,2.1,PICTURE,"Let ε x denote axial (x = 1), lateral (x = 2), and out-of-plane (x = 3) strains. Assuming linear elastic, isotropic, and homogeneous material that can move freely in the lateral direction, the lateral strain can be obtained from the axial strain and the Poisson's ratio by ε 2 = -v × ε. Real tissues are inhomogeneous, and boundary conditions exist; therefore, the lateral strain cannot be directly obtained by the axial strain and the Poisson's ratio alone. In such conditions, EPR, which is defined as v e = -ε22 ε11 can be employed [6]. EPR is spatially variant, and it is not equal to Poisson's ratio, particularly in the vicinity of inclusion boundaries or within inhomogeneous tissue. Its value tends to converge towards the Poisson's ratio in homogeneous regions, and it has a similar range of Poisson's ratio, i.e., between 0.2 and 0.5 [9]. In PICTURE, a regularization was defined to exploit this range and the out-of-range EPRs were penalized [5]. PICTURE loss can be obtained from the following procedure: 1-Detect out-of-range EPRs by:where v e is the EPR obtained from the estimated displacements. v emin and v emax are two hyperparameters that specify the minimum and maximum accepted EPR values, which are assumed to be 0.1 and 0.6, respectively.2-Penalize the out-of-range lateral strains using:where < v e > is the average of EPR values within the feasible range. The operator S denotes stop gradient operation, which is employed to avoid the axial strain being affected by this regularization. It should be noted in contrast to [5] in which only out-of-range samples were contributing to the loss, in this work, all samples contribute to L vd to reduce the estimation bias.3-Smoothness of EPR is considered by:4-PICTURE loss is defined as L V = L vd + λ vs × L vs , where λ vs is the weight of the smoothness loss. PICTURE loss is added to the data and smoothness losses of unsupervised training."
Infusing Physically Inspired Known Operators in Deep Models of Ultrasound Elastography,2.2,Known Operators,"The known operators are added to the network in the inference mode only due to the high computational complexity of unsupervised training (outlined in the next section). We employ two known operators to impose physically known constraints on the lateral displacement.The first known operator (we refer to it as Poisson's ratio clipper) limits the EPR to the feasible range of v emin -v emax . Although PICTURE tries to move all EPR values to the feasible range, in [5], it was shown that some samples in test time were still outside of the feasible range. Poisson's ratio clipper is an iterative algorithm since the lateral strains are altered by clipping the EPR values and affecting the neighbor samples' strain values.The second algorithm employs the incompressibility of the tissue which can be formulated by:In free-hand palpation, the force is approximately uniaxial (ε 3 ε 2 ); therefore Eq. 4 can be written as:Guo et al. enforced incompressibility in an iterative algorithm [2]. We made a few changes to increase the method's robustness by adding Gaussian filtering and using a hyper-parameter weight in each iteration. It should be noted that the algorithm can be employed for compressible tissues as well, and the incompressibility constraint is employed for the refinement of the obtained displacement. The proposed algorithms are outlined in Algorithm 1 and 2. The network architecture with the known operators is illustrated in Fig. 1. It is worth highlighting that known operators offer a compelling alternative to regularization. While the latter involves adjusting trained weights based on the training data and keeping them fixed during testing, the former relies on iterative refinement that is adaptable to the test data and does not require any learnable weights."
Infusing Physically Inspired Known Operators in Deep Models of Ultrasound Elastography,2.3,Unsupervised Training,"We followed a similar unsupervised training approach presented in [5] for both PICTURE and kPICTURE methods. The loss function can be written as:    where L D denotes photometric loss which is obtained by comparing the precompressed and warped compressed RF data, L S is smoothness loss in both axial and lateral directions. λ S and λ V specify the weights of the smoothness loss and PICTURE loss, respectively."
Infusing Physically Inspired Known Operators in Deep Models of Ultrasound Elastography,2.4,Dataset and Quantitative Metrics,"We use publicly available data collected from a breast phantom (Model 059, CIRS: Tissue Simulation & Phantom Technology, Norfolk, VA) using an Alpinion E-Cube R12 research US machine (Bothell, WA, USA). The center frequency was 8 MHz and the sampling frequency was 40 MHz. The Young's modulus of the experimental phantom was 20 kPa and contains several inclusions with Young's modulus of higher than 40 kPa. This data is available online at http://code.sonography.ai in [16].In vivo data was collected at Johns Hopkins hospital from patients with liver cancer during open-surgical RF thermal ablation by a research Antares Siemens system using a VF 10-5 linear array with the sampling frequency of 40 MHz and the center frequency of 6.67 MHz. The institutional review board approved the study with the consent of the patients. We selected 600 RF frame pairs of this dataset for the training of the networks.Two well-known metrics of Contrast to Noise Ratio (CNR) and Strain Ratio (SR) are utilized to evaluate the compared methods. Two Regions of Interest (ROI) are selected to compute these metrics and they can be defined as [10]:where the subscript t and b denote the target and background ROIs. The SR is only sensitive to the mean (s X ), while CNR depends on both the mean and the standard deviation (σ X ) of ROIs. For stiff inclusions as the target, higher CNR correlates with better target visibility, and lower SR translates to a higher difference between the target and background strains."
Infusing Physically Inspired Known Operators in Deep Models of Ultrasound Elastography,2.5,Network Architecture and Training,"We employed MPWC-Net++ [4] which has been adapted from PWC-Net-irr [3] for USE. The network architecture with the added known operators is shown in Fig. 1. The training schedule is similar to [5], known operators are not present in the training and only employed during the test phase. The known operators are added in different pyramid levels. This has the advantage of correcting lateral displacements in different pyramid levels. The known operators are added to the last 3 pyramid levels (there are 5 pyramid levels in this network) since the estimate in the first 2 pyramid levels are not accurate enough and adding the known operators would propagate the error. The hyper-parameters' values of unsupervised training and the known operators are given in Supplementary Materials.3 Results and Discussions"
Infusing Physically Inspired Known Operators in Deep Models of Ultrasound Elastography,3.1,Compared Methods,"kPICTURE is compared to the following methods: -OVERWIND, an optimization-based USE method [8].-The post-processing method of Guo et al. [2], which employs the output of OVERWIND as the initial displacement (OVERWIND+ Guo et al.). -PICTURE, which penalize EPR values outside of feasible range [5]. We decided to compare with PICTURE instead of sPICTURE [13] (PICTURE with self-supervision) since self-supervision is not related to the physics of motion. To focus on the effectiveness of the known operators, we, therefore, provide a comparison to its corresponding method PICTURE. The proposed known operators can be applied to the network trained with sPICTURE method as well. We made the network's weight trained using both PICTURE and sPIC-TURE methods publicly available online at http://code.sonography.ai. We also employed a similar hyper-parameters and training schedule for experimental phantom and in vivo data."
Infusing Physically Inspired Known Operators in Deep Models of Ultrasound Elastography,3.2,Results and Discussions,"The lateral strains of ultrasound RF data collected from three different locations of the tissue-mimicking breast phantom are depicted in Fig. 2, and the quantitative results are given in Table 1. Visual inspection of Fig. 2 denotes that the method proposed by Gou et al. [2] improves the displacement obtained by OVERWIND. For example, the inclusion borders in sample 2 are much more clearly visible. The strain images obtained by kPICTURE have a much higher quality than those of PICTURE. Furthermore, kPICTURE has the highest quality strain images among the compared methods. For example, the inclusion on the bottom in sample 1 (highlighted by the arrows) is clearly visible in kPIC-TURE, a substantial improvement over all other methods that do not even show the inclusion.Table 1. Quantitative results of lateral strains for experimental phantoms. Mean and standard deviation (±) of CNR (higher is better) and SR (lower is better) of lateral strains are reported. The pair marked by † is not statistically significant (p-value > 0.05, using Friedman test). The differences between all other numbers are statistically significant (p-value < 0.05).sample (1) sample ( 2 The histograms of EPR values of OVERWIND+Gou et al., PICTURE and kPICTURE are illustrated for the experimental phantom sample (1). To improve visualization, OVERWIND results are not included because the histogram was similar to that of OVERWIND+Gou et al.. Although PICTURE limits the range of EPR using a regularization (Eq. 2), some EPR values are outside the feasible range. kPICTURE further limits the EPR values; only a small number of samples are outside of the physically plausible range.The lateral strain results of in vivo data are depicted in Fig. 3 (b), and axial strains are given in the Supplementary Materials (the quality of axial strains is high in all methods). While PICTURE may produce an adequate strain image, it still contains noisy regions. On the other hand, kPICTURE delivers exceptionally refined strain images and surpasses the other compared methods. The quantitative results given in Table 1 also confirm the visual inspection. The applied known operators and PICTURE assume that the material is isotropic. Their performance on anisotropic materials can be investigated by experiments on anisotropic tissues such as muscles. Furthermore, 3D imaging data can be collected from 2D arrays to have information in out-of-plane direction to be able to formulate known operators and PICTURE loss for anisotropic tissues.It should be noted that after incorporating the known operators, the inference time of the network increased from an average of 195 ms to 240 ms (having 10 iterations for algorithm 1 and 100 iterations for algorithm 2)."
Infusing Physically Inspired Known Operators in Deep Models of Ultrasound Elastography,4,Conclusions,"In this paper, we proposed to incorporate two known operators inside a USE network. The network is trained by physically inspired constraints specifically designed to tackle the long-standing illusive problem of lateral strain imaging. The proposed operators provide a refinement in each pyramid level of the architecture and substantially improve the lateral strain image quality. Tissue mimicking phantom and in vivo results show that the method substantially outperforms previous displacement estimation method in the lateral direction."
Infusing Physically Inspired Known Operators in Deep Models of Ultrasound Elastography,,8 w,
Infusing Physically Inspired Known Operators in Deep Models of Ultrasound Elastography,,4,
Infusing Physically Inspired Known Operators in Deep Models of Ultrasound Elastography,,Fig. 1 .,
Infusing Physically Inspired Known Operators in Deep Models of Ultrasound Elastography,,Fig. 2 .,
Infusing Physically Inspired Known Operators in Deep Models of Ultrasound Elastography,,Fig. 3 .,
Infusing Physically Inspired Known Operators in Deep Models of Ultrasound Elastography,,,
Infusing Physically Inspired Known Operators in Deep Models of Ultrasound Elastography,,,
Infusing Physically Inspired Known Operators in Deep Models of Ultrasound Elastography,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0_45.
Domain Adaptation for Medical Image Segmentation Using Transformation-Invariant Self-training,1,Introduction,"Semantic segmentation is a prerequisite for a broad range of medical imaging applications, including disease diagnosis and treatment [13], surgical workflow analysis [6,9], operation room planning, and surgical outcome prediction [7]. While supervised deep learning approaches have yielded satisfactory performance in semantic segmentation [8,10], their performance is heavily limited by the labeled training dataset distribution. Indeed, a network trained on a dataset acquired with a specific device or configuration can dramatically underperform when evaluated on a different device or conditions. Overcoming this entails new annotations per device, a demand that is hard to meet, especially for semantic segmentation, and even more so in the medical domain, where expert knowledge is essential.Driven by the need to overcome this challenge, numerous semi-supervised learning paradigms have looked to alleviate annotation requirements in the target domain. Semi-supervised learning refers to methods that encourage learning abstract representations from an unlabeled dataset and extending the decision boundaries towards a more-generalized or target dataset distribution. These techniques can be categorized into (i) consistency regularization [4,[15][16][17]19,22], (ii) contrastive learning [2,11], (iii) adversarial learning [22], and (iv) self-training [24][25][26]. Consistency regularization techniques aim to inject knowledge via penalizing inconsistencies for identical images that have undergone different distortions, such as transformations or dropouts, or fed into networks with different initializations [4]. Specifically, the Π model [15] penalizes differences between the predictions of two transformed versions of each input image to reinforce consistent and augmentation-invariant predictions. Temporal ensembling [15] is designed to alleviate the negative effect of noisy predictions by integrating predictions of consecutive training iterations. Cross-pseudo supervision regularizes the networks by enforcing similar predictions from differently initialized networks.More recent deep self-training approaches based on pseudo labels have emerged as promising techniques for unsupervised domain adaptation. These techniques assume that a trained network can approximate the ground-truth labels for unlabeled images. Since no metric guarantees pseudo-label reliability, several methods have been developed to alleviate pseudo-label error back-propagation. To progressively improve pseudo-labeling performance, reciprocal learning [25] adopts a teacher-student framework where the student network performance on the source domain drives the teacher network weights updates. ST++ [24] proposes to evaluate the reliability of image-based pseudo labels based on the consistency of predictions in different network checkpoints. Subsequently, half of the more reliable images are utilized to re-train the network in the first step, and the trained network is used for pseudo-labeling the whole dataset for a second re-training step. Despite the effectiveness of state-of-the-art pseudo-labeling strategies, we argue that one important aspect has been underexplored: how can a trained network self-assess the reliability of its pixel-level predictions?To this end, we propose a novel self-training framework with a selfassessment strategy for pseudo-label reliability. The proposed framework uses transformation-invariant highly-confident predictions in the target dataset for self-training. This objective is achieved by considering an ensemble of highconfidence predictions from transformed versions of identical inputs. To validate the effectiveness of our proposed framework on a variety of tasks, we evaluate our approach on three different semantic segmentation imaging modalities, including video (cataract surgery), optical coherence tomography (retina), and MRI (prostate), as shown in Fig. 1. We perform comprehensive experiments to validate the performance of the proposed framework, namely ""Transformation-Invariant Self-Training"" 1 (TI-ST). The experimental results indicate that TI-ST significantly improves segmentation performance for unlabeled target datasets compared to numerous state-of-the-art alternatives. the ensemble of high-confidence predictions from two versions of the same target sample. Our proposed TI-ST framework simultaneously trains on the source and target domains, so as to progressively bridge the intra-domain distribution gap. Figure 2 depicts our TI-ST framework, which we detail in the following sections."
Domain Adaptation for Medical Image Segmentation Using Transformation-Invariant Self-training,2.1,Model,"At training time, images from the source dataset are augmented using spatial g(•) and non-spatial f (•) transformations and passed through a segmentation network, N (•), by which the network is trained using a standard supervision loss. At the same time, images from the target dataset are also passed to the network. Specifically, we feed two versions of each target image to the network: (1) the original target image x T , and (2) its non-spatially transformed version, xT = f (x T ). Once fed through the network, the corresponding predictions can be defined as ỹT = σ(N (x T )) and ỹT = σ(N ( xT )), where σ(•) is the Softmax operation. We then define a confidence-mask ensemble aswhere refers to Hadamard product used for element-wise multiplication, and Cnf is the high confidence masking function,where τ ∈ (0.5, 1) is the confidence threshold, and H, W , and C are the height, width, and number of classes in the output, respectively. Specifically, M cnf encodes regions of confident predictions that are invariant to transformations. We can then compute the pseudo-ground-truth mask for each input from the target dataset as"
Domain Adaptation for Medical Image Segmentation Using Transformation-Invariant Self-training,2.2,Training,"To train our model, we simultaneously consider both the source and target samples by minimizing the following loss,where L Sup and L P s indicate the supervised and pseudo-supervised loss functions used, respectively. We set λ as a time-dependent weighing function that gradually increases the share of pseudo-supervised loss. Intuitively, our pseudosupervised loss enforces predictions on transformation-invariant highly-confident regions for unlabeled images.Discussion: The quantity and distribution of supervised data are determining factors in neural networks' performance. With highly distributed large-scale supervisory data, neural networks converge to an optimal state efficiently. However, when only limited supervisory data with heterogeneous distribution from the inference dataset are available, using more sophisticated methods to leverage a priori knowledge is essential. Our proposed use of invariance of network predictions with respect to data augmentation is a strong form of knowledge that can be learned through dataset-dependent augmentations. The trained network is then expected to provide consistent predictions under diverse transformations. Hence, the transformation variance of the network predictions can indicate the network's prediction doubt and low confidence correspondingly. We take advantage of this characteristic to assess the reliability of predictions and filter out unreliable pseudo-labels."
Domain Adaptation for Medical Image Segmentation Using Transformation-Invariant Self-training,3,Experimental Setup,"Datasets: We validate our approach on three cross-device/site datasets for three different modalities:-Cataract: instrument segmentation in cataract surgery videos [12,21]. We set the ""Cat101"" [21] as the source dataset and the ""CaDIS"" as the target domain dataset [12]. -OCT: IRF Fluid segmentation in retinal OCTs [1]. We use the high-quality ""Spectralis"" dataset as the source and the lower-quality ""Topcon"" dataset as the target domain.-MRI: multi-site prostate segmentation [18]. We sample volumes from ""BMC"" and ""BIDMC"" as the source and target domain, respectively.We follow a four-fold validation strategy for all three cases and report the average results over all folds. The average number of labeled training images (from the source domain), unlabeled training images (from the target domain), and test images per fold are equal to (207, 3189, 58) for Cataract, (391, 569, 115) for OCT,and (273,195,65) for MRI dataset.Baseline Methods: We compare the performance of our proposed transformation-invariant self-training (SI-ST) method against seven state-of-theart semi-supervised learning methods: Π models [15], temporal ensembling [15], mean teacher [19], cross pseudo supervision (CSP) [4], reciprocal learning (RL) [25], self-training (ST) [24], and mutual correction framework (MCF) [23]."
Domain Adaptation for Medical Image Segmentation Using Transformation-Invariant Self-training,,Networks and Training Settings:,"We evaluate our TI-ST framework using two different architectures: (1) DeepLabV3+ [3] with ResNet50 backbone [14] and ( 2) scSE [20] with VGG16 backbone. Both backbones are initialized with the ImageNet [5] pre-trained parameters. We use a batch size of four for the Cataract and MRI datasets and a batch size of two for the OCT dataset. For all training strategies, we set the number of epochs to 100. The initial learning rate is set to 0.001 and decayed by a factor of γ = 0.8 every two epochs. The input size of the networks is 512×512 for cataract and OCT and 384×384 for the MRI dataset. As spatial transformations g(•), we apply cropping and random rotation (up to 30 degrees). The non-spatial transformations, f (•), include color jittering (brightness = 0.7, contrast = 0.7, saturation = 0.7), Gaussian blurring, and random sharpening. The confidence threshold τ for the self-training framework and the proposed TI-ST framework is set to 0.85 except in the ablation studies (See the next section). In Eq. ( 4), the weighting function λ ramps up from the first epoch along a Gaussian curve equal to exp[-5(1current-epoch/total-epochs)]. The self-supervised loss is set to the cross-entropy loss, and the supervised loss is set to the cross entropy log dice loss, which is a weighted sum of cross-entropy and the logarithm of soft dice coefficient. For the TI-ST framework, we only use non-spatial transformations for the self-training branch for simplicity."
Domain Adaptation for Medical Image Segmentation Using Transformation-Invariant Self-training,4,Results,"Table 1 compares the performance of our transformation-invariant self-training (TI-ST) approach with alternative methods across three tasks and using two network architectures. According to the quantitative results, TI-ST, RL, ST, and CPS are the best-performing methods. Nevertheless, our proposed TI-ST achieves the highest average relative improvement in dice score compared to naive supervised learning (16.18% average improvement). Considering our main competitor (RL), we note that our proposed TI-ST method is a one-stage framework using one network. In contrast, RL is a two-stage framework (requiring a pre-training stage) and uses a teacher-student network. Hence, TI-ST is also more efficient than RL in terms of time and computation. Furthermore, the proposed strategy demonstrates the most consistent results when evaluated on different tasks, regardless of the utilized neural network architecture. Figure 3-(a-b) demonstrates the effect of the pseudo-labeling threshold on TI-ST performance compared with regular ST. We observe that filtering out unreliable pseudo-labels based on transformation variance can remarkably boost pseudo-supervision performance regardless of the threshold. Figure 3-(c) compares the performance of the supervised baseline, ST, and TI-ST with respect to the number of source-domain labeled training images. While ST performance converges when the number of labeled images increases, our TI-ST pushes deci-Fig. 3. Ablation studies on the pseudo-labeling threshold and size of the labeled dataset.  sion boundaries toward the target domain dataset by avoiding training with transformation variant pseudo-labels. We validates the stability of TI-ST vs. ST with different labeling thresholds (0.80 and 0.85) over four training folds in Fig. 4, where TI-ST achieves a higher average improvement relative to supervised learning for different tasks and network architectures. This analysis also shows that the performance of ST is sensitive to the pseudo-labeling threshold and generally degrades by reducing the threshold due to resulting in wrong pseudo labels. However, TI-ST can effectively ignore false predictions in lower thresholds and take advantage of a higher amount of correct pseudo labels. This superior performance is depicted qualitatively in Fig. 5."
Domain Adaptation for Medical Image Segmentation Using Transformation-Invariant Self-training,5,Conclusion,"We proposed a novel self-training framework with a self-assessment strategy for pseudo-label reliability, namely ""Transformation-Invariant Self-Training"" (TI-ST). This method uses transformation-invariant highly-confident predictions in the target dataset by considering an ensemble of high-confidence predictions from transformed versions of identical inputs. We experimentally show the effectiveness of our approach against numerous existing methods across three different source-to-target segmentation tasks, and when using different model architectures. Beyond this, we show that our approach is resilient to changes in the methods hyperparameter, making it well-suited for different applications."
Domain Adaptation for Medical Image Segmentation Using Transformation-Invariant Self-training,,Fig. 1 .,
Domain Adaptation for Medical Image Segmentation Using Transformation-Invariant Self-training,,Fig. 2 .,
Domain Adaptation for Medical Image Segmentation Using Transformation-Invariant Self-training,,Fig. 4 .,
Domain Adaptation for Medical Image Segmentation Using Transformation-Invariant Self-training,,Fig. 5 .,
Domain Adaptation for Medical Image Segmentation Using Transformation-Invariant Self-training,,Table 1 .,
Domain Adaptation for Medical Image Segmentation Using Transformation-Invariant Self-training,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0_32.
SLPD: Slide-Level Prototypical Distillation for WSIs,1,Introduction,"In computational histopathology, visual representation extraction is a fundamental problem [14], serving as a cornerstone of the (downstream) task-specific learning on whole slide pathological images (WSIs). Our community has witnessed the progress of the de facto representation learning paradigm from the supervised ImageNet pre-training to self-supervised learning (SSL) [15,36]. Numerous pathological applications benefit from SSL, including classification of glioma [7], breast carcinoma [1], and non-small-cell lung carcinoma [25], mutation prediction [32], microsatellite instability prediction [31], and survival prediction from WSIs [2,16]. Among them, pioneering works [12,22,27] directly apply the SSL algorithms developed for natural images (e.g., SimCLR [10], CPC [30] and MoCo [11]) to WSI analysis tasks, and the improved performance proves the effectiveness of SSL. However, WSI is quite different from natural images in that it exhibits a hierarchical structure with giga-pixel resolution. Following works turn to designing pathological-specific tasks to explore the inherent characteristics of WSIs for representation learning, e.g., resolution-aware tasks [18,34,37] and color-aware tasks [2,38]. Since the pretext tasks encourage to mine the pathologically relevant patterns, the learned representations are expected to be more suitable for WSI analysis. Nevertheless, these works only consider learning the representations at the patch level, i.e, the cellular organization, but neglecting macro-scale morphological features, e.g., tissue phenotypes and intra-tumoral heterogeneity. As a result, there is still a gap between the pre-trained representations and downstream tasks, as the latter is mainly at the slide level, e.g., subtyping, grading and staging.More recently, some works propose to close the gap via directly learning slidelevel representations in pre-training. For instance, HIPT [8], a milestone work, introduces hierarchical pre-training (DINO [6]) for the patch-level (256 × 256) and region-level (4096 × 4096) in a two-stage manner, achieving superior performance on slide-level tasks. SS-CAMIL [13] uses EfficientNet-B0 for image compression in the first stage and then derives multi-task learning on the compressed WSIs, which assumes the primary site information, e.g., the organ type, is always available and can be used as pseudo labels. SS-MIL [35] also proposes a two-stage pre-training framework for WSIs using contrastive learning (SimCLR [10]), where the differently subsampled bags1 from the same WSI are positive pairs in the second stage. A similar idea can be found in Giga-SSL [20] with delicate patch-and WSI-level augmentations. The aforementioned methods share the same two-stage pre-training paradigm, i.e., patch-to-region/slide. Thus broader context information is preserved to close the gap between pretext and downstream tasks. However, they are essentially instance discrimination where only the self-invariance of region/slide is considered, leaving the intraand inter-slide semantic structures unexplored.In this paper, we propose to encode the intra-and inter-slide semantic structures by modeling the mutual-region/slide relations, which is called SLPD: Slide-Level Prototypical Distillation for WSIs. Specifically, we perform the slide-level clustering for the 4096 × 4096 regions within each WSI to yield the prototypes, which characterize the medically representative patterns of the tumor (e.g., morphological phenotypes). In order to learn this intra-slide semantic structure, we encourage the region representations to be closer to the assigned prototypes. By representing each slide with its prototypes, we further select semantically simi- lar slides by the set-to-set distance of prototypes. Then, we learn the inter-slide semantic structure by building correspondences between region representations and cross-slide prototypes. We conduct experiments on two benchmarks, NSCLC subtyping and BRCA subtyping. SLPD achieves state-of-the-art results on multiple slide-level tasks, demonstrating that representation learning of semantic structures of slides can make a suitable proxy task for WSI analysis. We also perform extensive ablation studies to verify the effectiveness of crucial model components."
SLPD: Slide-Level Prototypical Distillation for WSIs,2,Method,
SLPD: Slide-Level Prototypical Distillation for WSIs,2.1,Overview,"As shown in Fig. 1(a), a WSI exhibits hierarchical structure at varying resolutions under 20× magnification: 1) the 4096×4096 regions describing macro-scale organizations of cells, 2) the 256 × 256 patches capturing local clusters of cells, 3) and the 16 × 16 images characterizing the fine-grained features at the celllevel. Given N unlabeled WSIs, where L n denotes the number of regions of WSI w n , we aim to learn a powerful encoder that maps each x l n to an embedding z l n ∈ R D . SLPD is built upon the two-stage pre-training paradigm proposed by HIPT, which will be described in Sect. 2.2. Fig 1(c-d) illustrates the pipeline of SLPD. We characterize the semantic structure of slides in Sect. 2.2, which is leveraged to establish the relationship within and across slides, leading to the proposed intraand inter-slide distillation in Sect. 2.4 and Sect. 2.5."
SLPD: Slide-Level Prototypical Distillation for WSIs,2.2,Preliminaries,"We revisit Hierarchical Image Pyramid Transformer (HIPT) [8], a cutting-edge method for learning representations of WSIs via self-supervised vision transformers. As shown in Fig. 1 HIPT leverages DINO [6] to pre-train ViT 256 -16 and ViT 4096 -256, respectively. The learning objective of DINO is self-distillation. Taking stage two as an example, DINO distills the knowledge from teacher to student by minimizing the cross-entropy between the probability distributions of two views at region-level:where H(a, b) = -a log b, and p d is the data distribution that all regions are drawn from. The teacher and the student share the same architecture consisting of an encoder (e.g., ViT) and a projection head g t /g s . ẑ and z are the embeddings of two views at region-level yielded by the encoder. The parameters of the student are exponentially moving averaged to the parameters of the teacher."
SLPD: Slide-Level Prototypical Distillation for WSIs,2.3,Slide-Level Clustering,"Many histopathologic features have been established based on the morphologic phenotypes of the tumor, such as tumor invasion, anaplasia, necrosis and mitoses, which are then used for cancer diagnosis, prognosis and the estimation of response-to-treatment in patients [3,9]. To obtain meaningful representations of slides, we aim to explore and maintain such histopathologic features in the latent space. Clustering can reveal the representative patterns in the data and has achieved success in the area of unsupervised representation learning [4,5,24,26].To characterize the histopathologic features underlying the slides, a straightforward practice is the global clustering, i.e., clustering the region embeddings from all the WSIs, as shown in the left of Fig. 1(d). However, the obtained clustering centers, i.e., the prototypes, are inclined to represent the visual bias related to staining or scanning procedure rather than medically relevant features [33]. Meanwhile, this clustering strategy ignores the hierarchical structure ""region→WSI→whole dataset"" underlying the data, where the ID of the WSI can be served as an extra learning signal. Therefore, we first consider the slidelevel clustering that clusters the embeddings within each WSI, which is shown in the right of Fig. 1(d). Specifically, we conduct k-means algorithm before the start of each epoch over L n region embeddings {z l n } Ln l=1 of w n to obtain M prototypes {c m n ∈ R D } M m=1 . Similar operations are applied across other slides, and then we acquire N groups of prototypes {{c m n } M m=1 } N n=1 . Each group of prototypes is expected to encode the semantic structure (e.g., the combination of histopathologic features) of the WSI."
SLPD: Slide-Level Prototypical Distillation for WSIs,2.4,Intra-Slide Distillation,"The self-distillation utilized by HIPT in stage two encourages the correspondence between two views of a region at the macro-scale because the organizations of cells share mutual information spatially. However, the self-distillation, which solely mines the spatial correspondences inside the 4096 × 4096 region, cannot comprehensively understand the histopathologic consistency at the slide-level. In order to achieve better representations, the histopathologic connections between the WSI and its regions should be modeled and learned, which is called intraslide correspondences. With the proposed slide-level clustering in Sect. 2.3, a slide can be abstracted by a group of prototypes, which capture the semantic structure of the WSI. As shown in Fig. 1(e), we assume that the representation z and its assigned prototype c also share mutual information and encourage z to be closer to c with the intra-slide distillation:We omit super-/sub-scripts of z for brevity. Through Eq. 2, we can leverage more intra-slide correspondences to guide the learning process. For further understanding, a prototype can be viewed as an augmented representation aggregating the slide-level information. Thus this distillation objective is encoding such information into the corresponding region embedding, which makes the learning process semantic structure-aware at the slide-level."
SLPD: Slide-Level Prototypical Distillation for WSIs,2.5,Inter-Slide Distillation,"Tumors of different patients can exhibit morphological similarities in some respects [17,21], so the correspondences across slides should be characterized during learning. Previous self-supervised learning methods applied to histopathologic images only capture such correspondences with positive pairs at the patchlevel [22,23], which overlooks the semantic structure of the WSI. We rethink this problem from the perspective how to measure the similarity between two slides accurately. Due to the heterogeneity of the slides, comparing them with the local crops or the averaged global features are both susceptible to being one-sided. To address this, we bridge the slides with their semantic structures and define the semantic similarity between two slides w i and w j through an optimal bipartite matching between two sets of prototypes:where cos(•, •) measures the cosine similarity between two vectors, and S M enumerates the permutations of M elements. The optimal permutation σ * can be computed efficiently with the Hungarian algorithm [19]. With the proposed setto-set distance, we can model the inter-slide correspondences conveniently and accurately. Specifically, for a region embedding z belonging to the slide w and assigned to the prototype c, we first search the top-K nearest neighbors of w in the dataset based on the semantic similarity, denoted as { ŵk } K k=1 . Second, we also obtain the matched prototype pairs {(c, ĉk )} K k=1 determined by the optimal permutation, where ĉk is the prototype of ŵk . Finally, we encourage z to be closer to ĉk with the inter-slide distillation:The inter-slide distillation can encode the sldie-level information complementary to that of intra-slide distillation into the region embeddings.The overall learning objective of the proposed SLPD is defined as:where the loss scale is simply set to α 1 = α 2 = 1. We believe the performance can be further improved by tuning this."
SLPD: Slide-Level Prototypical Distillation for WSIs,3,Experimental Results,"Datasets. We conduct experiments on two public WSI datasets for downstream tasks. With the pre-extracted embeddings, we fine-tune three aggregators (i.e., MIL [28], DS-MIL [22] and ViT WSI -4096 [8]) for 20 epochs and follow other settings in the official code of HIPT.Evaluation Metrics. We adopt the 10-fold cross validated Accuracy (Acc.) and area under the curve (AUC) to evaluate the weakly-supervised classification performance. The data splitting scheme is kept consistent with HIPT. "
SLPD: Slide-Level Prototypical Distillation for WSIs,3.1,Weakly-Supervised Classification,"We conduct experiments on two slide-level classification tasks, NSCLC subtyping and BRCA subtyping, and report the results in Table 1. The region-level embeddings generated by SLPD outperform the patch-level embeddings across two aggregators3 and two tasks (#1∼ 5). This illustrates that learning representations with broader image contexts is more suitable for WSI analysis.Compared with the strong baseline, i.e., the two-stage pre-training method proposed by HIPT (#6), SLPD achieves performance increases of 1.3% and 3.2% AUC on NSCLC and BRCA (#9). Nontrivial performance improvements are also observed under KNN evaluation (#10 vs.#13): +2.3% and +3.1% AUC on NSCLC and BRCA. The superior performance of SLPD demonstrates that learning representations with slide-level semantic structure appropriately can significantly narrow the gap between pre-training and downstream slide-level tasks. Moreover, intra-slide and inter-slide distillation show consistent performance over the baseline, corroborating the effectiveness of these critical components of SLPD."
SLPD: Slide-Level Prototypical Distillation for WSIs,3.2,Ablation Study,"Different Clustering Methods. As discussed in Sect. 2.3, we can alternatively use the global clustering to obtain prototypes and then optimize the network with a similar distillation objective as Eq. 2. For a fair comparison, the total number of prototypes of the two clustering methods is approximately the same.  Different Inter-slide Distillations. The proposed inter-slide distillation is semantic structure-aware at the slide-level, since we build the correspondence between the region embedding and the matched prototype (#4 in Table 2). To verify the necessity of this distillation method, we turn to another design where the inter-slide correspondence is explored through two nearest region embeddings across slides (#3 in Table 2). As can be seen, the region-level correspondences lead to inferior performances, even worse than the baseline (#5 in Table 1), because the learning process is not guided by the slide-level information."
SLPD: Slide-Level Prototypical Distillation for WSIs,,Number of Prototypes.,"As shown in Table 2(#5∼7), the performance of SLPD is relatively robust to the number of prototypes on NSCLC, but is somewhat affected by it on BRCA. One possible reason is that the heterogeneity of invasive breast carcinoma is low [29], and thus the excessive number of prototypes cannot obtain medically meaningful clustering results. Empirically, we set M = 4 on NSCLC and M = 2 on BRCA as the default configuration. We suggest the optimal number of prototypes should refer to clinical practice, by considering tissue types, cell morphology, gene expression and other factors.Number of Slide Neighbors. As demonstrated in Table 2(#5∼7), the performance of SLPD is robust to the number of slide neighbors. Considering that more slide neighbors require more computation resources, we set K = 1 as the default configuration. For more results, please refer to the Supplementary."
SLPD: Slide-Level Prototypical Distillation for WSIs,4,Conclusion,"This paper reflects on slide-level representation learning from a novel perspective by considering the intra-and inter-slide semantic structures. This leads to the proposed Slide-Level Prototypical Distillation (SLPD), a new self-supervised learning approach achieving the more comprehensive understanding of WSIs. SLPD leverages the slide-level clustering to characterize semantic structures of slides. By representing slides as prototypes, the mutual-region/slide relations are further established and learned with the proposed intra-and inter-slide distillation. Extensive experiments have been conducted on multiple WSI benchmarks and SLPD achieves state-of-the-art results. Though SLPD is distillation-based, we plan to apply our idea to other pre-training methods in the future, e.g., contrastive learning [10,11]."
SLPD: Slide-Level Prototypical Distillation for WSIs,,Fig. 1 .,
SLPD: Slide-Level Prototypical Distillation for WSIs,,,"256 -16 is freezed and leveraged to tokenize the patches within 4096 × 4096 regions. Then a region-level vision transformer ViT 4096 -256 aggregates these tokens to obtain region-level representations. With this hierarchical aggregation strategy, a WSI can be represented as a bag of region-level representations, which are then aggregated with another vision transformer, ViT WSI -4096, to perform slide-level prediction tasks."
SLPD: Slide-Level Prototypical Distillation for WSIs,,,
SLPD: Slide-Level Prototypical Distillation for WSIs,,Table 1 .,
SLPD: Slide-Level Prototypical Distillation for WSIs,,Table 2 (,
SLPD: Slide-Level Prototypical Distillation for WSIs,,Table 2 .,
SLPD: Slide-Level Prototypical Distillation for WSIs,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0_25.
Masked Frequency Consistency for Domain-Adaptive Semantic Segmentation of Laparoscopic Images,1,Introduction,"Laparoscopic surgery is a minimally invasive surgical technique in which a camera and surgical instruments are inserted through a series of small skin punctures. During this procedure, the surgeon relies heavily on a screen to visualize the surgical site, which can be a serious challenge. An inaccurate interpretation of abdominal anatomy can result in serious injury to the patient's bile ducts [25]. Therefore, deploying neural networks to accurately identify anatomical structures during laparoscopic surgery can markedly enhance both the quality and safety of the procedure [15]. Despite the marked achievements of deep neural networks in various medical computer vision tasks, the training of supervised models necessitates a substantial volume of precisely annotated images. Because the acquisition of large, high-quality datasets of laparoscopic images is laborintensive and requires expert knowledge, the size and quality of publicly available datasets limit current research on semantic segmentation of laparoscopic images [21]. To overcome these limitations, several active learning [1,18] and domain generalization [14] methods have been developed to minimize the manual annotation required for network training. We take a step further and employ unsupervised domain adaptation (UDA) to eliminate the dependence on manual annotation.The aim of UDA is to train a model on a labeled source and an unlabeled target domain for enhanced target domain performance. Various UDA methods exist, but we concentrate on two types that are relevant to our approach: self-training-based and Fourier transform-based. Self-training-based approaches [2,23,29] apply different data augmentations, multiple models or domain mixtures to the images and gauge the consistency regularization between them. On the other hand, Fourier transform-based approaches [12,27,28] exchange the low frequency components across domains to transform source domain images into target domain ones. While UDA has been extensively investigated in the medical field, the majority of existing research has focused only on domain migration between datasets from different sources [10,11] or segmentation of some distinct categories (e.g. instrument [11,19,20]). In contrast, we utilize computersimulated images and unlabeled laparoscopic images to train a semantic segmentation network for laparoscopic images, which is more demanding and practical, as it deals with a more severe domain shift.We propose a novel module for the UDA task in laparoscopic semantic segmentation, aiming to promote the network's exploration of consistency regularization between high-frequency and low-frequency images. Our approach is motivated by the observation in Fig. 1(a) that computer-generated images lack the high-frequency details present in real images. For example, the computergenerated abdominal wall appears smooth, whereas the real abdominal wall has rich textural information. Inspired by the effectiveness of masked image models [5,9], we randomly mask the high frequency information of the image in the frequency domain and train the network to predict the semantic segmentation result of the image lacking high frequency information. We use the pseudo-label generated by the exponential moving average (EMA) teacher network for supervision. During training, the masking of high frequency regions allows the network to discover a shared latent space for both high and low frequency images. Consequently, real laparoscopic images with high-frequency information and computergenerated images with low-frequency information can share the same feature space, facilitating the transfer of knowledge learned in the generated images to the real images, as illustrated in Fig. 1(b). This paper's primary contributions are as follows: (1) We creatively address the domain-adaptive semantic segmentation task for laparoscopic images, which involves training the model with both unlabeled real images and computergenerated simulated images. (2) To bridge the severe domain shift between generated and real images, we propose a novel masking frequency consistency (MFC) module to reduce the domain gap. MFC encourages the network to learn shared features between high-frequency and low-frequency images. To our knowledge, MFC is the first UDA method that applies masking strategy on frequency domain. (3) We collect a vast number of image frames from public datasets and train them with computer-generated images. We evaluate our method, demonstrating that our MFC approach not only outperforms existing state-of-the-art UDA methods but also achieves performance comparable to fully supervised methods without the need for any manual annotation."
Masked Frequency Consistency for Domain-Adaptive Semantic Segmentation of Laparoscopic Images,2,Method,"This paper addresses the task of domain-adaptive semantic segmentation of laparoscopic images. Suppose we have a source domain of computer-generated simulated laparoscopic images, consisting of N imagesN} , and a target domain of M real laparoscopic images X T = {X T j | j = 1, 2, . . . , M} without annotations. Our objective is to train a network f with robust semantic segmentation capability on the unlabeled target domain.To achieve this, we introduce a masked frequency consistency module for selflearning on the unlabeled target domain images X T i , while supervised loss is used for training on the labeled source domain images X S i . Our approach can integrate with different networks, effectively bridging the domain gap that occurs when applying networks to the target domain."
Masked Frequency Consistency for Domain-Adaptive Semantic Segmentation of Laparoscopic Images,,Fig. 2.,"The overview of our proposed Masked Frequency Consistency (MFC) module, which can be seamlessly integrated with different UDA methods and backbone networks. The MFC module works by augmenting the input image in the frequency domain using a mask, and then using a teacher-student structure to take both the original and the augmented image as inputs. A consistency loss is applied to facilitate the bridging of domains."
Masked Frequency Consistency for Domain-Adaptive Semantic Segmentation of Laparoscopic Images,2.1,Image Frequency Representation,"Considering an RGB image, X ∈ R H×W ×3 , we can generate its frequency representation map by applying the 2D Discrete Fourier Transform F for each channel c ∈ {0, 1, 2}, independently:where (u, v) and (h, w) donate the coordinates in frequency map and image.To facilitate subsequent operations, we rearrange the FFT data so that negative frequency terms precede positive ones, thereby centering the low frequency information. Furthermore, the inverse Fourier transform (iFFT) F -1 is utilized to transform the spectral signals back into the original image space:Computation of both the Fourier transform and its corresponding inverse is achieved through the Fast Fourier Transform (FFT) algorithm [16]."
Masked Frequency Consistency for Domain-Adaptive Semantic Segmentation of Laparoscopic Images,2.2,Masking Strategy,"As illustrated in Fig. 2, MFC module perturbs the frequency information for target domain images. To do this, we define a mask M ∈ {0, 1}H×W that randomly erases parts of the frequency map, thereby reducing the frequency data. Specifically, a patch mask M is randomly sampled as follows:where [•] is the Iverson bracket, U(0, 1) the uniform distribution, b is the patch size, r represents the mask ratio, m and n are the patch indices. After this procedure, the patches in the mask are randomly masked. However, using the random patch mask alone may result in the loss of all low frequency information, which would exacerbate the domain gap and lead to training instability. To avoid this, we set the central elements to 1, thus preserving the low frequency information from the images as:where h and w denote the size of the low-frequency information to be preserved. We utilize the mask M to apply masking in the frequency domain and use the iFFT F -1 to transform the image back into the original spatial domain as the network input. The enhanced image can then be obtained as:where is the Hadamard product between the matrices. Moreover, with conjugate symmetry's disruption inhibiting the imaginary component's cancellation, we employ complex number magnitudes as outputs."
Masked Frequency Consistency for Domain-Adaptive Semantic Segmentation of Laparoscopic Images,2.3,Consistency Regularization,"Consistency regularization is employed to extract common representations between high and low frequency images, thereby enhancing the generality of the network. Specifically, during training, the student segmentation network f S takes the enhanced image X m as input, whereas the original image X serves as the input for the teacher network f T . The weight θ T of the teacher network undergoes updates using the exponential moving average (EMA) [22] of the weight θ S belonging to the student network:where θ T representing the weight from the previous training step. The EMA teacher generates a series of stable pseudo-labels over time, a tactic frequently utilized in both semi-supervised learning [22] and UDA [7][8][9]23].To evaluate the prediction results, we employ the mean squared error (MSE) as as our loss function, which quantifies the divergence between the predictions: (7) where q T denotes the quality weight. Due to potential inaccuracies in pseudolabeling, like prior works [7,8,23], we only use confident pixels surpassing the maximum probability threshold τ , defined as:3 Experiments"
Masked Frequency Consistency for Domain-Adaptive Semantic Segmentation of Laparoscopic Images,3.1,Datasets and Implementation,"Datasets. Our experiments were conducted on three laparoscopic datasets, described as follows: (1) Simulation dataset [17] consists of 20,000 labeled images of 3D scenes assembled from CT data, with 6 categories. In addition, I2I [17] used generative adversarial networks (GAN) to translate these images into five different styles of realistic laparoscopic images, resulting in a total of 100,000 images. We used these two types of images (simulated and translated) as the source domain datasets with annotations. (2) CholecSeg8k [6] is a semantic segmentation dataset containing laparoscopic cholecystectomy images from 17 video clips of the Cholec80 dataset [24], labeled with 13 categories. We used these 17 video clips as a test set. (3) The Cholec80 dataset [24] consists of 80 videos of cholecystectomy procedures with annotations for phase and instrument presence, but no annotations related to segmentation. To train the UDA model, we selected 6819 images from 63 surgical videos, excluding the 17 videos used in CholecSeg8k [6]. Specifically, for the video of the preparation phase of surgery, we extracted one frame per second as an unlabeled target domain image. A more detailed description is available in supplementary material.Dataset Partitioning. Our experiments were performed with two different settings: (1) simulated images to real images and (2) translated images to real images. Considering the 6 categories present in the simulated dataset and the 13 categories in the CholecSeg8k dataset, we performed semantic segmentation on the following 6 categories: Background (BG), Abdominal Wall (AW), Liver, Fat, Gallbladder (GB), and Instruments (INST).Implementation. We used the mmsegmentation [4] codebase and trained each model on a single NVIDIA Tesla V100 GPU. We evaluated the Segformer [26] and DeepLabV2 [3] backbone networks, based on HRDA strategy [8], and initialized all backbone networks with pre-training on ImageNet. Training was performed using AdamW [13], with hyper-parameters taken from previous works [8,9,26]. In all experiments, we trained the models on a batch of randomly cropped 256px × 256px images for 40k iterations, and the batch size is set to 4. As indicated in the ablation studies presented in the supplementary material, the optimal hyper-parameters for the MFC method vary according to the type of input datasets. Therefore, in all subsequent experiments, we set r to 0.7, b to 32, and h and w to 8, without further optimization of these hyper-parameters.State-of-the-Art Methods. We benchmarked our method against contemporary leading approaches, which include UDA methods (DAFormer [7], HRDA [8], and MIC [9]), image translation method [17], and fully supervised network on the CholecSeg8k dataset using cross-validation. For equitable comparison, all methods employed the same segmentation network, initialization, and optimizer. "
Masked Frequency Consistency for Domain-Adaptive Semantic Segmentation of Laparoscopic Images,3.2,Qualitative Evaluation,"In Fig. 3, we present visualizations of the proposed MFC method and other compared methods on the CholecSeg8k dataset. All methods used only the simulated dataset for training, without any manual annotation. Our proposed method has two major advantages: (1) it effectively performs surgical instrument segmentation with minimal interference from reflections and shadows, and (2) it achieves a more accurate distinction between the boundaries of gallbladder and fat. However, we have found that our method exhibits imprecision in distinguishing the liver from the abdominal wall in certain cases.  1. MFC, trained on the simulated data as the source domain, outperform the existing SOTA methods in all categories except for the abdominal wall and liver. Notably, our method significantly improves surgical instrument segmentation. This improvement can be attributed to the fact that our method excludes the disturbing high frequency noise such as reflections and shadows, which are absent in the source domain dataset. Such results indicate that our approach effectively bridges the domain gap between the generated and real images by randomly masking high frequency information. Furthermore, DeepLabV2-based methods underperform SegFormer-based methods in this setting.To verify the effectiveness of the patch mask outlined in Eq. ( 3) and the low frequency mask in Eq. ( 4), we also conduct ablation experiments. The results of variants of our method are presented in Table 1. It is observed that the adoption of either of the two masking strategies enhances the segmentation performance.Translated Images → Real Images. Furthermore, we assessed the performance of various methods using translated images as the source domain, with results summarized in Table 2. These translated images reduced the domain disparity with real images, thereby boosting the performance of UDA methods on the target domain. To gauge the efficacy of our proposed module, we incorporated MFC into two different network backbones. The results show that our approach efficiently improves the mIoU performance of segmentation across different source domain datasets and network backbones."
Masked Frequency Consistency for Domain-Adaptive Semantic Segmentation of Laparoscopic Images,4,Conclusion,"This paper tackles the crucial issue of laparoscopic image segmentation, which is essential for surgical guidance and navigation. We propose a novel UDA module, called MFC, that leverages the consistency between high and low-frequency information in latent space. This consistency facilitates knowledge transfer from computer-simulated to real laparoscopic datasets for segmentation. Experimentally, MFC not only bolsters existing UDA models' performance but also outperforms leading methods, including fully supervised models that rely on annotated data. Our work unveils the potential of using computer-generated image data and UDA techniques for laparoscopic image segmentation. However, a limitation of our approach is that it does not account for the long-tail category distribution prevalent in real-world scenarios, such as venous vessels. Therefore, a future direction of our research is to extend our MFC module to handle rare category segmentation, thereby improving UDA models' generalization capabilities."
Masked Frequency Consistency for Domain-Adaptive Semantic Segmentation of Laparoscopic Images,,Fig. 1 .,
Masked Frequency Consistency for Domain-Adaptive Semantic Segmentation of Laparoscopic Images,,Fig. 3 .,
Masked Frequency Consistency for Domain-Adaptive Semantic Segmentation of Laparoscopic Images,,Table 1 .,
Masked Frequency Consistency for Domain-Adaptive Semantic Segmentation of Laparoscopic Images,,Table 2 .,
Masked Frequency Consistency for Domain-Adaptive Semantic Segmentation of Laparoscopic Images,,.77 87.21 80.10 72.74 70.71 82.64 3.3 Quantitative Evaluation Simulated Images → Real Images.,
Masked Frequency Consistency for Domain-Adaptive Semantic Segmentation of Laparoscopic Images,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0 63.
Can Point Cloud Networks Learn Statistical Shape Models of Anatomies?,1,Introduction,"Statistical Shape Modeling (SSM) enables population-based morphological analysis, which can reveal patterns and correlations between shape variations and clinical outcomes. SSM can help researchers understand the differences between healthy and pathological anatomy, assess the effectiveness of treatments, and identify biomarkers for diseases (e.g., [5,[8][9][10]16]). The traditional pipeline for constructing SSM entails the segmentation of 3D images to acquire either binary volumes or meshes and then aligning these shapes. SSM can then be constructed explicitly via finding surface-to-surface correspondences across the cohort or implicitly via deforming a predefined atlas to each shape sample. Correspondence-based shape models are widely used due to their intuitive interpretation [22]; they comprise of sets of landmarks or correspondence points that are defined consistently using invariant points across populations that vary in their form. Historically, correspondence points were established manually to capture biologically significant features. This cumbersome, subjective process has since been replaced via automated optimization, which defines dense sets of correspondence points, aka a Point Distribution Model (PDM). PDM optimization schemes have been defined using metrics such as entropy and minimum description length [12,13] and via parametric representations [17,24]. A significant drawback of these methods is that PDM optimization must be performed on the entire shape cohort of interest, which is time-consuming and hinders inference. To evaluate a new patient scan using PDM, the new scan must undergo segmentation and alignment, and the PDMs must be optimized again for the entire population of shapes. Moreover, current approaches require a complete, high-resolution mesh or binary volume representation of the shape that is free from noise and artifacts. Therefore, lightweight shape acquisition methods (such as thresholding clinical images, anatomical surface scanning, and shape acquired from stacked or orthogonal 2D slices) cannot be directly used for SSM [26,27]. Deep learning solutions have been proposed to mitigate these limitations by predicting PDMs directly from unsegmented 3D images using convolutional neural networks [1,2,7]. However, these frameworks require PDM supervision and, hence, need the traditional optimization-based workflow to acquire training data.Effective SSM from point clouds would be widely applicable in clinical research, from artery disease progression from point clouds acquired via biplane angiographic and intravascular ultrasound image data fusion [26], to orthopedics implant design from point clouds acquired via 3D body scanning [27]. Applying existing methods for SSM generation from point clouds requires converting them to meshes or rasterizing them into segmentations, which is nontrivial given that point clouds are unordered and do not retain surface normals. SSM directly from point clouds would enable many clinical studies. Recently point cloud deep learning has gained attention, with significant efforts focused on effective point completion networks (PCNs) for generating complete point clouds from partial observations. Most point-completion methods use order-invariant feature encoding and two-stage coarse-to-fine decoding. An important outcome of such networks, which has been overlooked and not reported, is that the learned coarse point clouds are ordered and provide correspondence. In this work, we acknowledge this missed potential and explore the use of PCNs to predict PDM from 3D point clouds in an unsupervised manner. We investigate state-of-the-art PCNs as potential solutions for generating PDMs that (1) accurately represent shapes via uniformly distributed points constrained to the surface and (2) provide good correspondences that capture population-level statistics. 1 We discuss the benefits of this approach, its robustness to missingness and training size, current limitations, and possible improvements. This discussion will bring awareness to the community about the potential for learning SSM from point clouds and ultimately make SSM a more accessible, viable option in future clinical research."
Can Point Cloud Networks Learn Statistical Shape Models of Anatomies?,2,Background,"Point Distribution Models. The goal of SSM is to capture the inherent characteristics or underlying parameters of a shape class that remain when global geometrical information is removed. Given a PDM, correspondence points can be averaged across subjects to provide a mean shape, and principal component analysis (PCA) can be performed to compute the modes of shape variation, which can then be visualized and used in downstream medical tasks. Furthermore, if a PDM contains sub-populations, such as disease versus control, the differences in mean shapes can be quantified and visualized, providing group characterization.Point Cloud Deep Learning. Deep learning from 3D point clouds is an emerging area of research with numerous applications in computer vision, robotics, and medicine (e.g., classification, object tracking, segmentation, registration, pose estimation) [3]. PointNet [20] pioneered a Multi-Layer Perceptron (MLP) and max-pooling-based approach for permutation invariant feature learning from raw point clouds. FoldingNet [31] proposed a point cloud auto-encoder with a foldingbased decoder that utilizes 2D grid deformation for reconstruction. Several convolutional approaches have been proposed, including mapping point clouds to voxel grids to directly apply 3D CNNs [15] and graph-based methods [29].The initial point cloud completion network, PCN [33], utilized PointNet [20] and FoldingNet [31] with a coarse-to-fine decoder. Since then, numerous point cloud completion approaches have been proposed, including point MLP and folding-based extensions, 3D convolution approaches, graph-based methods, generative modeling approaches (including generative adversarial network GANbased and variational autoencoder VAE-based), and transformer-based methods. See [14] for a recent survey. Many approaches follow the general framework of first encoding the point cloud into a permutation-invariant feature representation, then decoding the encoded shape feature to acquire a coarse or sparse point cloud, and finally, refining the coarse point cloud to acquire the dense complete prediction. The general architecture of these methods is shown in Fig. 1."
Can Point Cloud Networks Learn Statistical Shape Models of Anatomies?,3,Methods,
Can Point Cloud Networks Learn Statistical Shape Models of Anatomies?,3.1,Point Completion Networks for SSM,"Our experiments demonstrate that when coarse-to-fine point completion networks are trained on anatomical shapes, the bottleneck captures a populationspecific shape prior. Directly decoding the shape feature representation results in a consistent ordering of the intermediate coarse point cloud across samples, providing a PDM. This phenomenon can be intuitively understood as an application of Occam's razor, where the model prefers to learn the simplest solution, resulting in consistent output ordering. Many point completion networks contain skip connections from the feature and/or input space to the refinement network. In the case where the unordered input point cloud is fed to the refinement network, the ordering of the output dense point cloud is understandably lost.  To study whether point completion networks can learn anatomical SSM, we first extract point clouds from mesh vertices, then train point completion models, and finally evaluate the effectiveness of the predicted coarse point clouds as PDMs. Note this approach is not restricted to input point clouds obtained from meshes; point clouds from any acquisition process can be used. Global geometric information is factored out by aligning all shapes via iterative closest points [6] to a reference shape. We utilized the open-source toolkit ShapeWorks [11] for this step. In our experiments, the aligned, unordered mesh vertices serve as ground truth complete point clouds. The ground truth points are randomly downsampled to 2048 points and permuted to provide input point clouds. As is standard, the point clouds are uniformly scaled to be between -1 and 1 to assist network training. We consider a state-of-the-art model from the major point completion approach categories:-PCN [33] Point completion network loss is based on the L1 Chamfer Distance (CD) [14], which defines the minimum distance between two sets of points. The loss is typically defined as a combination of coarse and dense loss with weighting parameter α, which we consistently set across models. All model's hyperparameters are set to the original implementation values, and training is run until convergence (as assessed by training CD)."
Can Point Cloud Networks Learn Statistical Shape Models of Anatomies?,3.2,Evaluation Metrics,"In addition to CD, Fscore [25] is typically used in point completion to quantify the percentage of points that are reconstructed correctly. In analyzing accurate surface sampling for SSM, we quantify the point-to-face distance (P2F) from each point in the predicted point cloud to the closest surface of the ground truth mesh. To analyze point uniformity, we quantify the variance in the distance of each point to its six nearest neighbors. A uniform PDM would result in a small variance in the point nearest neighbor distance.We also consider PDM correspondence analysis metrics. An ideal PDM is compact, meaning that it represents the distribution of the training data using the minimum number of parameters. We quantify compactness as the number of PCA modes required to capture 99% of the variation in the correspondences. A good PDM should also generalize well from training examples to unseen examples. The generalization metric is defined as the reconstruction error (L2) between predicted correspondences of a held-out point cloud and the correspondences reconstructed via the training PDM. Finally, effective SSM is specific, generating only valid instances of the shape class in the training set. The average distance between correspondences sampled from the training PDM and the closest existing training correspondences provides the specificity metric."
Can Point Cloud Networks Learn Statistical Shape Models of Anatomies?,4,Experiments,"We use five datasets in experiments -one synthetic ellipsoid dataset for a proofof-concept and four real anatomical shapes: proximal femurs, left atrium of the heart, spleen, and pancreas. These datasets vary greatly in size (see Table 1, Column 1) and shape variation. Details and visualization of these cohorts are provided in the supplementary materials. In all experiments, the input point cloud size is 2048, the coarse output size is 512, and the dense output size is 2048. The real datasets were split (90%/10%) into training and testing sets. Stratified sampling via clustering was used to define the test set to ensure it is representative, given the low sample size.Proof-of-Concept: Ellipsoids. As a proof-of-concept, we generate 3D axisaligned ellipsoid shapes with fixed z-radius and random x and y-radius. A testing set of 30 and a training set of just 50 were randomly defined to emulate the scarce data scenario. The results in Table 1 show that all of the model variants performed well with low S2F distance (<0.1mm), and all PDMs correctly captured just two modes of variation (x and y-radius). Results from the PCN [33] model are shown in Fig. 2.Femur. The femur dataset is comprised of 56 femoral heads segmented from CT scans, nine of which have the cam-FAI pathology characterized by an abnormal bone growth lesion that causes hip osteoarthritis [5]. We utilize this pathology to analyze if PDM from point completion networks can correctly characterize  group differences. Table 1 shows the predicted coarse particles are close to the surfaces (P2F distance of 0.1mm), and the PCN [33] model performs best in this regard. The PCN [33] predictions were used to analyze the difference between the normal and CAM pathology mean shapes. Figure 3 shows the pathology is correctly characterized, and the Linear Discrimination of Variation (LDA) plot shows the difference in normal and CAM distributions captured by the PDM."
Can Point Cloud Networks Learn Statistical Shape Models of Anatomies?,,Region of CAM lesion,"Difference from normal to CAM group mean captured by PDM 0 0.5  Left Atrium. The left atrium dataset comprises of 1096 shapes segmented from cardiac LGE MRI images from unique atrial fibrillation patients. This cohort contains significant morphological variation in overall size, the size of the left atrium appendage, and the number and arrangement of the pulmonary veins. This variation is reflected in the large compactness values in Table 1. Despite this variation, the models achieve reasonable CD and P2F scores due to the large training size. Figure 4 shows the PDM predicted by SFN [30], which performed best. From the prediction examples, we can see the model represents the training data very well, even in the worst case, which has an extremely enlarged left atrium appendage. The size of the appendage is appropriately captured in the modes of variation and the performance on the test examples is reasonable with the exception of those that are not well represented by the training data, such as the test set worst case. This highlights the importance of a large, representative training set. To further illustrate this importance, we perform an ablation experiment evaluating the performance of the PCN [33] model with respect to the left atrium training test size (Fig. 4).Pancreas. We utilize the pancreas dataset [23] to analyze the impact of incomplete input point clouds as the point completion networks were designed to address. Cases of incomplete observations frequently arise in clinical research. For example, in the analysis of bones where some are clipped due to scan size or in cases where 3D shape is interpreted from stacked or orthogonal 2D observations. Traditional methods of SSM generation are unable to handle such cases, but the point cloud learning approach has the potential to. Figure 5 shows how the test set error increase as the percentage of missing points increases. The SFN [30] model provides the best results given partial input.Spleen. The spleen dataset [23] is included to provide an example of a small dataset with a large amount of variation to stress test the point completion models. Table 1 shows the models perform the worst on this dataset with regards to CD, Fscore, P2F, and Uniformity. This example illustrates the limitations of this approach to SSM generation. Our experiments demonstrate that point cloud networks can learn accurate SSM of anatomy when provided with a sufficiently large and representative training dataset. The transformer-based SFN [30] architecture provided the best overall results among the models we explored. SFN [30] utilizes k-Nearest Neighbors (kNN) to capture geometric relations in the point cloud, while PointAttN [28] does not. PointAttN [28] has been shown to provide better point completion of complex man-made objects where kNN information could be misleading. However, in the case of anatomical SSM, it is likely that the kNN information assisted SFN [30] performance by providing accurate spatial information, given the more convex shape of organs and bones. Interestingly, the simplest model, PCN [33], achieved similar, and sometimes better, SSM accuracy than more current stateof-the-art methods, despite its inferior performance in point completion benchmarks [18,19,28,30]. This may be attributed to point completion benchmarks involving multiple object class point completion, which is a more challenging task. Another significant difference between our experiments and point completion benchmarks is the PCN datasets have tens of thousands of examples [19,32,33], while we worked with limited training data -the typical scenario in shape analysis. Our experiments demonstrate that PCNs can effectively predict SSM under limited training data when shape variation is minimal, as in the case of proximal femurs. However, they struggle when there is significant shape variation, such as in the spleen cohort. This work indicates promising potential for adapting characteristics of point completion architectures and learning schemes to tailor to the task of predicting SSMs from point clouds. Potential improvements could be made to the training objective, such as penalization for non-uniformity and bottleneck regularization for compact population-statistical learning. Additionally, improvements could be made to address the scarce training data scenario, such as model-based data augmentation and probabilistic transfer learning. Although we evaluated only smooth point cloud inputs, similar architectures have shown success in point cloud denoising tasks, suggesting that our approach may handle noise as well [4,21]. This work establishes the groundwork for future research into the potential of point cloud deep learning for SSM, offering significant benefits over traditional SSM generation, including: (1) reducing the input burden from complete, noisefree shape representations to point clouds, which significantly expands potential use cases, (2) providing fast inference and scalable training given any cohort size, (3) allowing for partial input via simultaneous SSM prediction and completion, (4) enabling sequential or online learning, as well as incremental model updating as clinical studies progress, and (5) eliminating biases introduced by metrics and parametric representations used in classical methods. By enabling SSM from point clouds, we can increase SSM accessibility and potentially accelerate its adoption as a widespread clinical tool."
Can Point Cloud Networks Learn Statistical Shape Models of Anatomies?,,Fig. 1 .,
Can Point Cloud Networks Learn Statistical Shape Models of Anatomies?,,,
Can Point Cloud Networks Learn Statistical Shape Models of Anatomies?,,Fig. 2 .,
Can Point Cloud Networks Learn Statistical Shape Models of Anatomies?,,Fig. 3 .,
Can Point Cloud Networks Learn Statistical Shape Models of Anatomies?,,Fig. 4 .Fig. 5 .,
Can Point Cloud Networks Learn Statistical Shape Models of Anatomies?,,Table 1 .,
Can Point Cloud Networks Learn Statistical Shape Models of Anatomies?,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0_47.
PROnet: Point Refinement Using Shape-Guided Offset Map for Nuclei Instance Segmentation,1,Introduction,"Nuclei segmentation in histopathology images is an important task for cancer diagnosis and immune response prediction [1,13,18]. While several fully supervised deep learning approaches to segment nuclei exist [2,6,8,9,19,25], labeling S. Nam and J. Jeong-Equal contribution.thousands of instances are tedious and the ambiguous nature of nuclei boundaries requires high-level expert annotators. To address this, weakly-supervised nuclei segmentation methods [5,10,15,20,23,28] have emerged as an attractive alternative using cheap and inexact labels e.g., center point annotations. As point labels alone do not provide sufficient foreground information, it is common to use Euclidean distance-based Voronoi diagrams and k-means clustering [7] to generate pseudo segmentation labels for training. However, since Euclidean distance-based schemes only use distance information while ignoring color, they often fail to capture nuclei shape information; resulting in inadequate boundary delineation between adjacent nuclei. Moreover, prior methods [17,21,22] typically assume that point labels are located precisely at the center of the nuclei. In real-world scenarios, point annotation locations may shift from nuclei centers as a result of the expert labeling process, leading to a lower performance after model training.To overcome these challenges, we propose a novel weakly supervised instance segmentation method that effectively distinguishes adjacent nuclei and is robust to point shifts. The proposed model consists of three modules responsible for binary segmentation, boundary delineation, and instance separation. To train the binary segmentation module, we generate pseudo binary segmentation masks using geodesic distance-based Voronoi labels and cluster labels from point annotations. Geodesic distance provides more precise nuclei shape information than previous Euclidean distance-based schemes. To train the offset map module, we generate pseudo offset maps by computing the offset distance between binary segmentation pixel predictions and the point label. The offset information facilitates precise delineation of the boundaries between adjacent nuclei. To make the model robust to center point shifts, we introduce an Expectation Maximization (EM) [4] algorithm-based process to refine point labels. Note that previous approaches [17,21,22] optimize model parameters only using a fixed set of point labels, while we instead alternatively update model parameters and the center point locations. This refinement process ensures that the model maintains high performance even when the point annotation is not exactly located at the center of the nuclei.The contributions of this paper are as follows: (1) We propose an end-toend weakly supervised segmentation model that simultaneously predicts binary mask, offset map, and center map to accurately identify and segment nuclei.(2) By utilizing geodesic distance, we produce more detailed Voronoi and cluster labels that precisely delineate the boundary between adjacent nuclei. (3) We introduce an EM algorithm-based refinement process to encourage model robustness on center-shifted point labels. (4) Ablation and evaluation studies on two public datasets demonstrate our model's ability to outperform state-of-the-art techniques not only with ideal labels but also with shifted labels."
PROnet: Point Refinement Using Shape-Guided Offset Map for Nuclei Instance Segmentation,2,Methodology,"We propose an end-to-end nuclei segmentation method that only uses point annotations P to predict nuclei instance segmentation masks Ŝ. The proposed Fig. 1. Overview of the proposed method. It consists of an encoder and three modules for binary segmentation, offset map and center map prediction. To train offset map and center map modules(blue lines), pseudo labels are generated using point label and predicted binary segmentation mask(green lines). During inference, the instance map, obtained by predicted offset map and center map, is multiplied with predicted binary mask to produce instance segmentation prediction(orange lines). (Color figure online) model consists of three modules: 1) binary segmentation module, 2) offset map module, and 3) center map module (Fig. 1). For a given input image, we extract feature maps with an ImageNet-pretrained VGG16 backbone encoder. The feature maps are further processed through a series of residual units (RUs) and attention units (AUs) to predict a binary segmentation mask B, an offset map Ô, and a center map Ĉ. The RUs are employed to maintain feature information so that subsequent modules can reuse the features from early-stage modules. In contrast, the AUs are used to refine the features of initial modules by using the predictions of later modules. In particular, the AUs use the point predictions to refine the features in the offset module, and the offset predictions to refine the features in the binary module.In the training stage, we first generate a Voronoi label V and a cluster label K along the green lines in Fig. 1 to train the segmentation module. Then, we generate the pseudo offset map O by using B and P . Next, following [29], we generate the center map C by expanding the point label P with Gaussian kernel within a radius r. Herein, our model is trained wih a segmentation loss L B (V,K, B), an offset map loss L O (O, Ô), and a center map loss L C (C, Ĉ). Note that P can not sufficiently enable model robustness to imprecise point annotations. Thus, we employ an EM algorithm to search the optimal model parameters θ to obtain more reliable points P .In the inference stage, B, Ô, Ĉ are predicted following the orange lines in Fig. 1. Then, we generate an instance map I, which shares the same values among the same instances as follows:where (x, y) represents a coordinate and (x Ĉi , y Ĉi ) means the location of i th point obtained from Ĉ. Finally, the instance segmentation output Ŝ is obtained by B × I. 2.1 Loss Functions Using Pseudo Labels Segmentation Loss. We generate V and K to train the binary segmentation module. In [21], V was generated based on Euclidean distance between points without considering color information. As a result, the Voronoi boundaries are often created across nuclei instances, and the offset map's quality was limited. To mitigate this, we instead generate V using Geodesic distance [3,24] by computing distances d i between all center points p i ∈ P and pixels. The boundaries of the diagram in V are defined as 0, while center points and the other regions are defined as 1 and 2, respectively. For k-means clustering, we concatenate the RGB values and the geodesic distance value d i truncated by d * to generate the feature vectorsWe cluster f into three clusters (0 for background, 1 for foreground, and 2 for ignore) to generate K (Fig. 2d). To train the binary segmentation module using V and K, we employ a Voronoi loss L V and a cluster loss L K based on the cross-entropy:where Ω V and Ω K are the set of foreground and background pixels in V and K, N ΩV and N ΩK denote the cardinality of Ω V and Ω K . Following [17], we define the final segmentation loss asCenter Map Loss. To achieve instance-level predictions, we introduce a center map module. The module predicts a keypoint heatmap Ĉ ∈ [0, 1] W ×H where Ĉ = 1 identifies nuclei centers and Ĉ = 0 for other pixels. W and H are the width and height of the input image. To train the module we employ a focal loss, commonly used in point detection problems. This loss can focus on a set of sparse hard examples while preventing easy negatives from dominating the model [16]:where N P denotes the number of point labels. We set the focal loss hyperparameters α = 2 and β = 4 following [14,29]. By placing the center map module at the end of the model, the model is able to retain center point information along the RUs, so that each module can inherently reflect the information into their predictions. Offset Map Loss. We employ an offset map module that considers the shape of each nucleus to improve boundary detection. Inspired by [2], we define an offset vector O(x, y) that indicates the displacement of a point (x, y) to the center of its corresponding nucleus. To train the offset module, we first compute O(x, y) of each nucleus segmented by B. Then, L O is defined as an L1 loss:It is worth noting that in the early stages of training, the pseudo offset map O generated by B and P is unreliable. Thus, we empirically use L O for backpropagation after 20 epochs. We optimize the entire model using the loss, where λ B , λ O and λ C denote loss weights."
PROnet: Point Refinement Using Shape-Guided Offset Map for Nuclei Instance Segmentation,2.2,Refinement via Expectation Maximization Algorithm,"Training with nuclei (center) shifted point labels can lead to blurry center map predictions (see Fig. 3c). This in turn limits model optimization and it's ability to distinguish objects, resulting in poor adjacent nuclei segmentation. To address this, we propose an EM based center point refinement process. Instead of the standard fixed-point label based model optimization, we alternatively optimize both model parameters and point labels.In the E-step, we update the center of each nucleus according to Ô. We use Ô to generate refined point labels P , since Ô is reliable regardless of the point location i.e., center of the nuclei or shifted.where v i is i th Voronoi region and p i is the refined center point. We repeat this for all Voronoi regions to obtain P , and replace P with P if the distance between them is < δ. In the M -step of iteration n, we generate C by adapting the Gaussian mask to P , and then use it to train offset and center map modules. As maximizing a probability distribution is the same as minimizing the loss, the model parameter θ minimizing L is optimized as:Since reliable Ô is necessary to refine nuclei centers, refinement starts after 30 epochs. E and M steps are alternately repeated to correct imprecise annotations bringing them closer to the real nuclei center points."
PROnet: Point Refinement Using Shape-Guided Offset Map for Nuclei Instance Segmentation,3,Experiments,"Dataset. To validate the effectiveness of our model, we use two public nuclei segmentation datasets i.e., CPM17 [26] & MoNuSeg [12]. CPM17 contains 64 H&E stained images with 7,570 annotated nuclei boundaries sized from 500×500 to 600×600. The set is split into 32/32 images for training and testing. Images were normalized and cropped to 300×300. MoNuSeg is a multi-organ nuclei segmentation dataset consisting of 30 H&E stained images (1000×1000) extracted from seven different organs. We used 16 images (4 images from the breast, liver, kidney, and prostate) as training and 14 images (2 images from each breast, liver, kidney, prostate, bladder, brain, and stomach) as testing. For a fair comparison, images were pre-processed before training/testing i.e., normalized and cropped to 250×250 patches following the setting used in [17].To make point labels, we use the center point of full mask annotations. For a realistic scenario, we generate shifted point label. The shift is performed in pixels and is randomly selected between the minimum and maximum values.Implementation Details. For training, all evaluated models were run for 150 epochs with the Adam optimizer [11] using a learning rate of 1e-4, weight decay of 3e-2, and batch size of 4. The GeodisTK [27] library was used to compute geodesic distances. For clustering, we set the maximum distance d * as 90 and 70 on CPM17 and MoNuSeg, respectively. The Gaussian kernel r was set as r = 6 and δ was set as 8 for refinement on CPM17. For MoNuSeg, r = 8 and δ = 8, respectively. A threshold of 0.2 was applied to eliminate the noise and find important points in Ĉ. Finally, a variety of augmentations were employed i.e., random resizing, cropping, and rotations etc., following [17], with loss weights λ B , λ O and λ C empirically set to 1. We used a NVIDIA RTX A5000 GPU and PyTorch version 1.7.1.  [21] 75.0 55.5 75.3 56.9 74.4 53.7 72.2 49.9 70.1 44.9 69.9 45.0 66.3 39.9 61.0 31.5 Mixed Anno [22] 75.3 53.2 75.9 55.5 73.3 52.3 73.1 49.9 73.3 51.6 72.0 49.4 66.0 40.5 66.9 41.8 SPN+IEN [17] 74. Main Results. Table 1 shows the performance of our method against stateof-the-art weakly supervised nuclei segmentation methods [17,21,22] based on Dice and Aggregated Jaccard Index (AJI) metrics. As opposed to the Dice score, AJI is key when evaluating adjacent nuclei separation in instance segmentation tasks. On CPM17, our method outperformed the prior approach by a large margin of +3.4% in Dice and +7.2% in AJI when the point label is located at the nuclei center. More importantly, our approach surpassed prior approaches by a substantial margin when the shift exists. We obtain statistically significant (p-value <0.05) for the AJI of all comparison methods on two datasets in all scenarios. Regarding refinement, we observed that our strategy is more beneficial when points exhibit significant shifts i.e., on both CPM and MoNuSeg. Figure 3 showcases the effectiveness of the refinement process wherein the model generates precise instance and center maps. With the geodesic distance and the refinement process, our proposed method achieved state-of-the-art performance. This demonstrates that our method separates adjacent nuclei accurately, and maintains its robustness, achieving consistent performance even when the point annotations are not located at the center of the nuclei. Additionally, in Fig. 4, we qualitatively show the results to highlight how our method precisely separates adjacent nuclei.  Ablation Studies. We conducted ablation studies to assess the impact of the offset regression module, geodesic distance, and point refinement process (Table 2). When the binary segmentation module is combined only with the center map module without the offset module, the model could separate nuclei only trained by the ideal label. On the other hand, since there was no refinement process due to the absence of the offset map, inaccurate points extracted from the center map are obtained in the real-world scenario. We also demonstrate that labels with Geodesic distance help improve overall performance. This is because it creates confident labels and more decent divides the boundaries between nuclei. Finally, using the full set of modules along with a complete instance map, the model was able to separate adjacent nuclei with precise boundaries, ultimately reporting higher scores. These findings validate the utility of the center map and offset map modules i.e., they synergistically facilitate precise instance delineation and nuclei boundary prediction. The geodesic distance and refinement process also improved the accuracy by contributing to more accurate pseudo labels. Especially, most variants show a significant drop in performance when the annotations shift was over 4 pixels. Compared to other variants, our proposed model is more robust to the point shift in both datasets."
PROnet: Point Refinement Using Shape-Guided Offset Map for Nuclei Instance Segmentation,4,Conclusion,"In this work, we proposed a novel and robust framework for weakly supervised nuclei segmentation. We demonstrated the effectiveness of geodesic distancebased Voronoi diagrams and k-means clustering to generate accurate pseudo binary segmentation labels. This allowed us to generate reliable pseudo offset maps, and then we iteratively improve the pseudo offset maps that facilitate the precise separation of adjacent nuclei as well as progressively refine the location of the center point labels. According to our experimental results, we established a new state-of-art on two publicly available datasets across different levels of point annotation imperfections. We believe being able to use low-precision point annotations while retaining good segmentation performance is an essential step for automatic nuclei segmentation models to become a widespread tool in realworld clinical practice."
PROnet: Point Refinement Using Shape-Guided Offset Map for Nuclei Instance Segmentation,,Fig. 2 .,
PROnet: Point Refinement Using Shape-Guided Offset Map for Nuclei Instance Segmentation,,Fig. 3 .,
PROnet: Point Refinement Using Shape-Guided Offset Map for Nuclei Instance Segmentation,,Fig. 4 .,
PROnet: Point Refinement Using Shape-Guided Offset Map for Nuclei Instance Segmentation,,,
PROnet: Point Refinement Using Shape-Guided Offset Map for Nuclei Instance Segmentation,,Table 1 .,
PROnet: Point Refinement Using Shape-Guided Offset Map for Nuclei Instance Segmentation,,Table 2 .,
PROnet: Point Refinement Using Shape-Guided Offset Map for Nuclei Instance Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0_51.
Many Tasks Make Light Work: Learning to Localise Medical Anomalies from Multiple Synthetic Tasks,1,Introduction,"In recent years, the workload of radiologists has grown drastically, quadrupling from 2006 to 2020 in Western Europe [4]. This huge increase in pressure has led to long patient-waiting times and fatigued radiologists who make more mistakes [3]. The most common of these errors is underreading and missing anomalies (42%); followed by missing additional anomalies when concluding their search after an initial finding (22%) [10]. Interestingly, despite the challenging work environment, only 9% of errors reviewed in [10] were due to mistakes in the clinicians' reasoning. Therefore, there is a need for automated second-reader capabilities, which brings any kind of anomalies to the attention of radiologists. For such a tool to be useful, its ability to detect rare or unusual cases is particularly important. Traditional supervised models would not be appropriate, as acquiring sufficient training data to identify such a broad range of pathologies is not feasible. Unsupervised or self-supervised methods to model an expected feature distribution, e.g., of healthy tissue, is therefore a more natural path, as they are geared towards identifying any deviation from the normal distribution of samples, rather than a particular type of pathology.There has been rising interest in using end-to-end self-supervised methods for anomaly detection. Their success is most evident at the MICCAI Medical Outof-Distribution Analysis (MOOD) Challenge [31], where all winning methods have followed this paradigm so far (2020-2022). These methods use the variation within normal samples to generate diverse anomalies through sample mixing [7,[23][24][25]. However all these methods lack a key component: structured validation. This creates uncertainty around the choice of hyperparameters for training. For example, selecting the right training duration is crucial to avoid overfitting to proxy tasks. Yet, in practice, training time is often chosen arbitrarily, reducing reproducibility and potentially sacrificing generalisation to real anomalies."
Many Tasks Make Light Work: Learning to Localise Medical Anomalies from Multiple Synthetic Tasks,,Contribution:,"We propose a cross-validation framework, using separate selfsupervision tasks to minimise overfitting on the synthetic anomalies that are used for training. To make this work effectively we introduce a number of non-trivial and seamlessly-integrated synthetic tasks, each with a distinct feature set so that during validation they can be used to approximate generalisation to unseen, real-world anomalies. To the best of our knowledge, this is the first work to train models to directly identify anomalies on tasks that are deformation-based, tasks that use Poisson blending with patches extracted from external datasets, and tasks that perform efficient Poisson image blending in 3D volumes, which is in itself a new contribution of our work. We also introduce a synthetic anomaly labelling function which takes into account the natural noise and variation in medical images. Together our method achieves an average precision score of 76.2 for localising glioma and 78.4 for identifying pathological chest X-rays, thus setting the state-of-the-art in self-supervised anomaly detection."
Many Tasks Make Light Work: Learning to Localise Medical Anomalies from Multiple Synthetic Tasks,,Related Work:,"The most prevalent methods for self-supervised anomaly detection are based on generative auto-encoders that analyse the residual error from reconstructing a test sample. This is built on the assumption that a reconstruction model will only be able to correctly reproduce data that is similar to the instances it has been trained on, e.g. only healthy samples. Theoretically, at test time, the residual reconstruction error should be low for healthy tissues but high for anomalous features. This is an active area of research with several recent improvements upon the initial idea [22], e.g., [21] applied a diffusion model to a VQ-VAE [27] to resample the unlikely latent codes and [30] gradually transition from a U-Net architecture to an autoencoder over the training process in order to improve the reconstruction of finer details. Several other methods aim to ensure that the model will not reproduce anomalous regions by training it to restore samples altered by augmentations such as masking out regions [32], interpolating heavily augmented textures [29] or adding coarse noise [9]. [5] sought to identify more meaningful errors in image reconstructions by comparing the reconstructions of models trained on only healthy data against those trained on all available data.However, the general assumption that reconstruction error is a good basis for an anomaly scoring function has recently been challenged. Auto-encoders are unable to identify anomalies with extreme textures [16], are reliant on empirical post-processing to reduce false-positives in healthy regions [2] and can be outperformed by trivial approaches like thresholding of FLAIR MRI [15].Self-supervised methods take a more direct approach, training a model to directly predict an anomaly score using synthetic anomalies. Foreign patch interpolation (FPI) [24] was the first to do this at a pixel-level, by linearly interpolating patches extracted from other samples and predicting the interpolation factor as the anomaly score. Similar to CutPaste [11], [7] fully replaces 3D patches with data extracted from elsewhere in the same sample, but then trains the model to segment the patches. Poisson image interpolation (PII) [25] seamlessly integrates sample patches into training images, preventing the models from learning to identify the anomalies by their discontinuous boundaries. Natural synthetic anomalies (NSA) [23] relaxes patch extraction to random locations in other samples and introduces an anomaly labelling function based on the changes introduced by the anomaly. Some approaches combine self-supervised and reconstruction-based methods by training a discriminator to compute more exact segmentations from reconstruction model errors [6,29]. Other approaches have also explored contrasting self-supervised learning for anomaly detection [12,26]."
Many Tasks Make Light Work: Learning to Localise Medical Anomalies from Multiple Synthetic Tasks,2,Method,"The core idea of our method is to use synthetic tasks for both training and validation. This allows us to monitor performance and prevent overfitting, all without the need for real anomalous data. Each self-supervised task involves introducing a synthetic anomaly into otherwise normal data whilst also producing the corresponding label. Since the relevant pathologies are unknown a priori, we avoid simulating any specific pathological features. Instead, we use a wide range of subtle and well-integrated anomalies to help the model detect many different kinds of deviations, ideally including real unforeseen anomalies. In our experiments, we use five tasks, but more could be used as long as each one is sufficiently unique. Distinct tasks are vital because we want to use these validation tasks to estimate the model's generalisation to unseen classes of anomalies. If the training and validation tasks are too similar, the performance on the validation set may be an overly optimistic estimate of how the model would perform on unseen real-world anomalies.When performing cross-validation over all synthetic tasks and data partitions independently, the number of possible train/validation splits increases significantly, requiring us to train F • (T N CT ) independent models, where T N is the total number of tasks, T is the number of tasks used to train each model and F is the number of data folds, which is computationally expensive. Instead, as in our case T N = F = 5, we opt to associate each task with a single fold of the training data (Fig. 1). We then apply 5CT -fold cross-validation over each combination. In each iteration, the corresponding data folds are collected and used for training or validation, depending on which partition forms the majority.  Synthetic Tasks: Figure 2 shows examples of our self-supervised tasks viewed in both one and two dimensions. Although each task produces visually distinct anomalies, they fall into three overall categories, based on blending, deformation, or intensity variation. Also, all tasks share a common recipe: the target anomaly mask M h is always a randomly sized and rotated ellipse or rectangle (ellipsoids/cuboids in 3D); all anomalies are positioned such that at least 50% of the mask intersects with the foreground of the image; and after one augmentation is applied, the process is randomly repeated (based on a fair coin toss, p = 0.5), for up to a maximum of 4 anomalies per image.The Intra-dataset Blending Task. Poisson image blending is the current state-of-the-art for synthetic anomaly tasks [23,25], but it does not scale naturally to more than two dimensions or non-convex interpolation regions [17]. Therefore, we extend [20] and propose a D-dimensional variant of Poisson image editing following earlier ideas by [17].Poisson image editing [20] uses the image gradient to seamlessly blend a patch into an image. It does this by combining the target gradient with Dirichlet boundary conditions to define a minimisation problem min fin h |∇f in -v| Also, by defining h as the axis-aligned bounding box of M h , we can ensure the boundaries coincide with coordinate lines. This enables us to use the Fourier transform method to solve this partial differential equation [17], which yields a direct relationship between Fourier coefficients of Δf in and v after padding to a symmetric image. To simplify for our use case, an image with shape N 0 × • • •×N D-1 , we replace the Fourier transformation with a discrete sine transform (DST. This follows as a DST is equivalent to a discrete Fourier transform of a real sequence that is odd around the zeroth and middle points, scaled by 0.5, which can be established for our images. With this, the Poisson equation becomes congruent to a relationship of the coefficients,vd where v=(v 0 , ..., v D-1 ) and v is the DST of each component. The solution for fu can then be computed in DST space by dividing the right side through the terms on the left side and the destination image can be obtained through x i = DST -1 ( fu ). Because this approach uses a frequency transform-based solution, it may slightly alter areas outside of M h (where image gradients are explicitly edited) in order to ensure the changes are seamlessly integrated. We refer to this blending process as x = P oissonBlend(x i , x j , M h ) in the following. The intra-dataset blending task therefore results from xintra = P oissonBlend(x, x , M h ) with x, x ∈ D with samples from a common dataset D and is therefore similar to the self-supervision task used in [23] for 2D images.The inter-dataset blending task follows the same process as intra-dataset blending but uses patches extracted from an external dataset D , allowing for a greater variety of structures. Therefore, samples from this task can be defined as xinter = P oissonBlend(x, x , M h ) with x ∈ D, x ∈ D .The sink/source tasks shift all points in relation to a randomly selected deformation centre c. For a given point p, we resample intensities from a new location p. To create a smooth displacement centred on c, we consider the distance p-c 2 in relation to the radius of the mask (along this direction), d. The extent of this displacement is controlled by the exponential factor f > 1. For example, the sink task (Eqn. 1) with a factor of f = 2 would take the intensity at 0.75d and place it at 0.5d, effectively pulling these intensities closer to the centre. Note that unlike the sink equation in [24] this formulation cannot sample outside of the boundaries of P M h meaning it seamlessly blends into the surrounding area. The source task (Eqn. 2) performs the reverse, appearing to push the pixels away from the centre by sampling intensities towards it.The smooth intensity change task aims to either add or subtract an intensity over the entire anomaly mask. To avoid sharp discontinuities at the boundaries, this intensity change is gradually dampened for pixels within a certain margin of the boundary. This smoothing starts at a random distance from the boundary, d s , and the change is modulated by d p /d s .Anomaly Labelling: In order to train and validate with multiple tasks simultaneously we use the same anomaly labelling function across all of our tasks. The scaled logistic function, used in NSA [23], helps to translate raw intensity changes into more semantic labels. But, it also rounds imperceptible differences up to a minimum score of about 0.1. This sudden and arbitrary jump creates noisy labels and can lead to unstable training. We correct this semantic discontinuity by computing labels as [23]. This flipped Gaussian shape is C1 continuous and smoothly approaches zero, providing consistent labels even for smaller changes."
Many Tasks Make Light Work: Learning to Localise Medical Anomalies from Multiple Synthetic Tasks,3,Experiments and Results,"Data: We evaluate our method on T2-weighted brain MR and chest X-ray datasets to provide direct comparisons to state-of-the-art methods over a wide range of real anomalies. For brain MRI we train on the Human Connectome Project (HCP) dataset [28] which consists of 1113 MRI scans of healthy, young adults acquired as part of a scientific study. To evaluate, we use the Brain Tumor Segmentation Challenge 2017 (BraTS) dataset [1], containing 285 cases with either high or low grade glioma, and the ischemic stroke lesion segmentation challenge 2015 (ISLES) dataset [13], containing 28 cases with ischemic stroke lesions. The data from both test sets was acquired as part of clinical routine. The HCP dataset was resampled to have 1mm isotropic spacing to match the test datasets. We apply z-score normalisation to each sample and then align the bounding box of each brain before padding it to a size of 160 × 224 × 160. Lastly, samples are downsampled by a factor of two.For chest X-rays we use the VinDr-CXR dataset [18] including 22 different local labels. To be able to compare with the benchmarks reported in [6] we use the same healthy subset of 4000 images for training along with their test set (DDAD ts ) of 1000 healthy and 1000 unhealthy samples, with some minor changes outlined as follows. First note that [6] derives VinDr-CXR labels using the majority vote of the 3 annotators. Unfortunately, this means there are 52 training samples, where 1/3 of radiologists identified an anomaly, but the majority label is counted as healthy. The same applies to 10 samples within the healthy testing subset. To avoid this ambiguity, we replace these samples with leftover training data that all radiologists have labelled as healthy. We also evaluate using the true test set (VinDr ts ), where two senior radiologists have reviewed and consolidated all labels. For preprocessing, we clip pixel intensities according to the window centre and width attributes in each DICOM file, and apply histogram equalisation, before scaling intensities to the range [-1, 1]. Finally, images are resized to 256 × 256. • indicates that the metrics are evaluated over the same region and at the same resolution as CRADL [12]. Upper right part: Metrics on VinDr-CXR, presented as AP/AUROC on the VinDr and DDAD test splits. Random is the baseline performance of a random classifier. Lower part: a sensitivity analysis of the average AP of each individual fold (mean±s.d.) alongside that of the model ensemble, varying how many tasks we use for training versus validation. Best results are highlighted in bold. Comparison to State-of-the-Art Methods: Validating on synthetic tasks is one of our main motivations; as such, we use a 1/4 (train/val.) task split to compare with benchmark methods. For brain MRI, we evaluate results at the slice and voxel level, computing average precision (AP) and area under the receiver operating characteristic curve (AUROC), as implemented in scikit learn [19]. Note that the distribution shift between training and test data (research vs. clinical scans) adds further difficulty to this task. In spite of this, we substantially improve upon the current state-of-the-art (Table 1 upper left). In particular, we achieve a pixel-wise AP of 76.2 and 45.9 for BraTS and ISLES datasets respectively. To make our comparison as faithful as possible, we also re-evaluate after post-processing our predictions to match the region and resolution used by CRADL, where we see similar improvement. Qualitative examples are shown in Fig. 3. Note that all baseline methods use a validation set consisting of real anomalous samples from BraTS and ISLES to select which anomaly scoring function to use. We, however, only use synthetic validation data. This further verifies that our method of using synthetic data to estimate generalisation works well.For both VinDr-CXR test sets we evaluate at a sample and pixel level, although previous publications have only reported their results at a sample level. We again show performance above the current state-of-the-art (Table 1 upper right). Our results are also substantially higher than previously proposed selfsupervised methods, improving on the current state-of-the-art NSA [23] by 12.6 to achieve 78.4 image-level AP. This shows that our use of synthetic validation data succeeds where their fixed training schedule fails."
Many Tasks Make Light Work: Learning to Localise Medical Anomalies from Multiple Synthetic Tasks,,Ablation and Sensitivity Analysis on Cross-Validation Structure:,"We also investigate how performance changes as we vary the number of tasks used for training and validation (Table 1 lower). For VinDr-CXR, in an individual fold, the average performance increases as training becomes more diverse (i.e. more tasks); however, the performance of the ensemble plateaus. Having more training tasks can help the model to be sensitive to a wider range of anomalous features. But as the number of training tasks increases, so does the overlap between different models in the ensemble, diminishing the benefit of pooling predictions. This could also explain why the standard deviation (across folds) decreases as the number of training tasks increases, since the models are becoming more similar. Our best configuration is close to being competitive with the state-of-the-art semi -supervised method DDAD-ASR [6]. Even though their method uses twice as much training data, as well as some real anomalous data, our purely synthetic method begins to close the gap (AP of [6] 84.3 vs. ours 80.7 on DDAD ts ). For the brain datasets, all metrics generally decrease as the number of training tasks increases. This could be due to the distribution shift between training and test data. Although more training tasks may increase sensitivity to diverse irregularities, this can actually become a liability if there are differences between (healthy) training and test data (e.g. acquisition parameters). More sensitive models may then lead to more ""false"" positives."
Many Tasks Make Light Work: Learning to Localise Medical Anomalies from Multiple Synthetic Tasks,,Discussion:,"We demonstrate the effectiveness of our method in multiple settings and across different modalities. A unique aspect of the brain data is the domain shift. The HCP training data was acquired at a much higher isotropic resolution than the BraTS and ISLES test data, which are both anisotropic. Here we achieve the best performance using more tasks for validation, which successfully reduces overfitting and hypersensitivity. Incorporating greater data augmentations, such as simulating anisotropic spacing, could further improve results by training the model to ignore these transformations. We also achieve strong results for the X-ray data, although precise localisation remains a challenging task. The gap between current performance and clinicially useful localisation should therefore be high priority for future research."
Many Tasks Make Light Work: Learning to Localise Medical Anomalies from Multiple Synthetic Tasks,4,Conclusion,"In this work we use multiple synthetic tasks to both train and validate selfsupervised anomaly detection models. This enables more robust training without the need for real anomalous training or validation data. To achieve this we propose multiple diverse tasks, exposing models to a wide range of anomalous features. These include patch blending, image deformations and intensity modulations. As part of this, we extend Poisson image editing to images of arbitrary dimensions, enabling the current state-of-the-art tasks to be applied beyond just 2D images. In order to use all of these tasks in a common framework we also design a unified labelling function, with improved continuity for small intensity changes. We evaluate our method on both brain MRI and chest X-rays and achieve state-of-the-art performance and above. We also report pixel-wise results, even for the challenging case of chest X-rays. We hope this encourages others to do the same, as accurate localisation is essential for anomaly detection to have a future in clinical workflows."
Many Tasks Make Light Work: Learning to Localise Medical Anomalies from Multiple Synthetic Tasks,,Fig. 1 .,
Many Tasks Make Light Work: Learning to Localise Medical Anomalies from Multiple Synthetic Tasks,,Fig. 2 .,
Many Tasks Make Light Work: Learning to Localise Medical Anomalies from Multiple Synthetic Tasks,,2,
Many Tasks Make Light Work: Learning to Localise Medical Anomalies from Multiple Synthetic Tasks,,Fig. 3 .,
Many Tasks Make Light Work: Learning to Localise Medical Anomalies from Multiple Synthetic Tasks,,Table 1 .,
Improved Multi-shot Diffusion-Weighted MRI with Zero-Shot Self-supervised Learning Reconstruction,1,Introduction,"Magnetic resonance imaging (MRI) is widely used for diagnosis and treatment monitoring as it provides structural and physiological information related to dis-ease progression. Diffusion MRI (dMRI) measures molecular diffusion in biological tissues and provides microscopic details of tissue architecture, as molecules interact with many different obstacles while diffusing throughout tissues [16]. However, dMRI requires repeated acquisitions with different diffusion directions. Echo-planar imaging (EPI), which enables fast encoding per imaging slice, is commonly used for dMRI due to its fast acquisition time. However, single-shot (ss-) EPI is susceptible to severe susceptibility-induced geometric distortion and T 2 -and T * 2 -induced voxel blurring. These artifacts worsen at higher in-plane resolutions as the time required to acquire each line of k-space increases approximately linearly.Multi-shot (ms-) acquisition is an effective approach to mitigate EPI-related artifacts, which segments k-space into multiple portions covered across multiple repetition times (TRs) to reduce the effective echo spacing. However, potential shot-to-shot phase variations across multiple EPI shots can introduce additional artifacts. Recent algorithms, such as low-rank prior methods like low-rank modeling of local k-space neighborhoods (LORAKS) [7,8,14,15,17,18], and multishot sensitivity encoded diffusion data recovery using structured low-rank matrix completion (MUSSELS) [19], have successfully addressed this challenge by jointly reconstructing msEPI images through a low-rank constraint applied across the EPI shots.In recent years, deep learning has emerged as a promising approach for image reconstruction, offering potential solutions to the challenges of existing techniques, including long reconstruction times, residual artifacts at high acceleration factors, and over-smoothing [6,9,10]. One notable development is model-based deep learning (MoDL), which leverages an unrolled convolutional neural network (CNN) and a parallel imaging (PI) forward model to denoise and unalias undersampled data [1]. MoDL has also been applied to multi-shot diffusionweighted echo-planar imaging, known as MoDL-MUSSELS, effectively replacing MUSSELS and significantly reducing reconstruction times while achieving comparable results to state-of-the-art methods [2]. MoDL-MUSSELS includes CNN denoisers in both image-and k-spaces, as recent work has demonstrated that utilizing both domains has yielded improvement in performance based on metrics such as PSNR and SSIM [6]. However, it is worth noting that existing deep learning networks for dMRI have typically been trained in a supervised manner, which requires a significant amount of ground truth images that are not easily acquired in EPI acquisitions.In contrast, self-supervised learning [3,24,25] does not rely on external training data and can be used in denoising, reconstruction, quantitative mapping, and other applications. Recent advancements in zero-shot self-supervised learning (ZS-SSL) have demonstrated successful scan-specific network training without any external database [24]. This approach has shown comparable or superior results to supervised networks. However, in dMRI, where the same volume is repeatedly acquired while changing diffusion directions, ZS-SSL typically requires training separate networks for different directions, which can be impractical.The virtual coil (VC) approach is a highly effective technique for enhancing the performance of parallel MRI [5], particularly in the case of EPI that utilizes partial Fourier acquisition. VC generates virtual coils by incorporating conjugate symmetric k-space signals from actual coils. This integration provides supplementary information for missing data points in k-space, further being useful when combined with partial Fourier acquisition. Conceptually, the utilization of VC consistently ensures an image quality equivalent to or exceeds that of the image reconstructed without VC.In this study, we propose a novel msEPI reconstruction method called zero-MIRID (zero-shot self-supervised learning of Multi-shot Image Reconstruction for Improved Diffusion MRI). Our method jointly reconstructs msEPI data by incorporating zero-shot self-supervised learning-based image reconstruction. Our key contributions are as follows:-We jointly reconstruct multiple-shot images using self-supervised learning.-We train one network for all diffusion directions, which accelerates training speed and improves performance. -We used network denoisers in both k-and image-space and employed the VC [5] to improve the conditioning of the reconstruction. -In the in-vivo experiment, the proposed method demonstrates more robust images and better diffusion metrics than the state-of-art PI technique for dMRI. -To our best knowledge, this study proposes the first self-supervised learning reconstruction for dMRI.Overall, our zero-MIRID method offers a promising approach to enhance msEPI reconstruction in dMRI, providing improved image quality and diffusion metrics through the integration of self-supervised learning techniques."
Improved Multi-shot Diffusion-Weighted MRI with Zero-Shot Self-supervised Learning Reconstruction,2,Method,
Improved Multi-shot Diffusion-Weighted MRI with Zero-Shot Self-supervised Learning Reconstruction,2.1,PI Techniques for dMRI,"For msEPI data, SENSE is commonly used for image reconstruction. SENSE individually reconstructs each shot's data using the spatial variation of the coil sensitivity profile. The m th shot image in the d th diffusion direction, x d,m , can be reconstructed as follow.where F m is the undersampled Fourier transform for the m th shot, C is the coil sensitivity map, and b d,m is the acquired k-space data of d th direction and m th shot.On the other hand, MUSSELS and LORAKS jointly reconstruct multipleshot images using the low-rank property among msEPI data. The images in the d th diffusion direction can be reconstructed using LORAKS as follows.where J is the LORAKS regularization. In this work, we utilized S-LORAKS, which employs phase information and k-space symmetry [14,15].  x d =argmin"
Improved Multi-shot Diffusion-Weighted MRI with Zero-Shot Self-supervised Learning Reconstruction,2.2,Network Design,"where V C is the VC operator, and N i and N k are denoising CNNs in the imageand k-space, respectively. We define Nx = x -Dx, where D is the CNN network, and modified the alternating minimization-based solution in [2] to get the solutions of equation ( 3), as follows.where n is the optimization step (iteration) number, η and ζ is the network denoising terms in k-and image-space, and A = FC. As proposed in the recent ZS-SSL study [24], we split the sampling mask into three different groups, as shown in Fig. 2, where g 3 is the entire sampling mask and g 3 ⊃ g 2 ⊃ g 1 . In the training phase, g 1 was used for network input, while g 2 was used to calculate training losses. In the validating phase, g 2 was used for network input, while g 3 was used to calculate validating losses. In the inferencing phase, g 3 was used for network inputs. The loss in the d th direction in the training phase can be described as follows."
Improved Multi-shot Diffusion-Weighted MRI with Zero-Shot Self-supervised Learning Reconstruction,2.3,Zero-Shot Self-supervised Learning,"where L is the loss function, f is the zero-MIRID reconstruction, and θ is the trainable network parameters. Similarly, the loss in the d th direction in the validating phase can be described as follows.In this study, we used the normalized root mean square error (NRMSE) and normalized mean absolute error (NMAE) as the loss functions."
Improved Multi-shot Diffusion-Weighted MRI with Zero-Shot Self-supervised Learning Reconstruction,2.4,Experiment Details,"In-vivo experiments were conducted on a 3T Siemens Prisma system with a 32-channel head coil. For dMRI, we acquired the diffusion-weighted data in 32 different directions using 2-shot EPI, with each shot accelerated by 5-fold (R=5) and employing 75% Partial Fourier, resulting in 15% coverage of the k-space in each shot relative to a fully-sampled readout. Imaging parameters are; field of view (FOV)=224 × 224 × 128 mm 3 , voxel size =1 × 1 × 4 mm 3 , TR=3.5 s, and effective echo time (TE) =59 ms.SENSE and S-LORAKS reconstructions were performed with MATLAB R2022a using Intel Xeon 6248R and 512 GB RAM. All neural network implementations were conducted with Python, using the Keras library in Tensorflow 2.4.1. NVIDIA Quadro RTX 8000 (RAM: 48 GB) was used to train, validate, and test the network. The denoising CNNs consist of 16 layers of which the depth is 46. For the 16 layer-CNN, we employed a filter size of 3 × 3. The depth of our network is 46, resulting in a total of 583,114 trainable parameters. The DC layer takes ten conjugate gradient steps, and the reconstruction block iterates ten times, where the MoDL paper [1] has demonstrated the saturated performance. For training the model, the Adam optimizer is used with a learning rate of 1e-3. Leaky ReLU was used as the activation function. For every diffusion direction, one g 2 and 50 cases of g 1 were generated. The ratio of the number of k-space points of g 3 :g 2 :g 1 = 1.00:0.80:0.48. We trained a single network for 32 diffusion directions and used that network to reconstruct all directions. For comparison, we trained two separate networks for the individual reconstruction for each shot (zero-SIRID, single-shot image reconstruction). We used the FSL toolbox for diffusion analysis [13,22,23]. To estimate multiple fiber orientations, we used the Bayesian Estimation of Diffusion Parameters Obtained using Sampling Techniques (BEDPOSTX) [4,11,12].Example data and code can be found in the following link: https://github.com/jaejin-cho/miccai2023"
Improved Multi-shot Diffusion-Weighted MRI with Zero-Shot Self-supervised Learning Reconstruction,3,Results,"Figure 3 the reconstructed diffusion-weighted images at 5-fold acceleration per shot in the selected diffusion directions. The reference images were obtained from 5-shot EPI data that covers complementary k-space lines to each other with the S-LORAKS constraint. While SENSE shows severe noise amplification and remaining folding artifacts, zero-SIRID was able to partially mitigate the noise amplification. S-LORAKS jointly reconstructed two shots, considerably reduced noise, and improved the signal-to-noise ratio (SNR). Nonetheless, in the selected diffusion directions, folding artifacts were amplified, and the center of the image shows a dropped signal (pointed by yellow arrows). In contrast, zero-MIRID demonstrated robust image reconstruction even with a high reduction factor per shot. The NRMSE and NAE across the diffusion direction are provided in the supplementary material, demonstrating notable reductions in NRMSE and NMAE when the proposed method is compared to S-LORAKS.Figure 4 presents the average diffusion-weighted image (DWI), fractional anisotropy (FA) map, and 2 nd crossing fiber image calculated from the reconstructed images. S-LORAKS and zero-MIRID produced high-fidelity average  DWIs, whereas SENSE and zero-SIRID show remaining artifacts. SENSE, zero-SIRID, and S-LORAKS show amplified noise in the center of the FA maps, whereas zero-MIRID effectively mitigated the noise. Furthermore, zero-MIRID well preserved the number of 2 nd crossing fibers, often considered a crucial factor in evaluating successful dMRI acquisition [4,12]."
Improved Multi-shot Diffusion-Weighted MRI with Zero-Shot Self-supervised Learning Reconstruction,4,Discussion and Conclusion,"In this study, we proposed an improved image reconstruction method for msEPI and dMRI in a self-supervised deep learning manner. In-vivo experiment demonstrates the proposed method outperformed S-LORAKS, the state-of-art PI method for dMRI.Acquiring reference images of msEPI can be challenging because each shot is typically highly accelerated and shot-to-shot phase variation prevents jointly reconstructing multiple shots efficiently. Advanced PI techniques that jointly reconstruct many EPI shots can improve the PI condition and provide highfidelity images, but using a PI method may induce bias to that particular method. Therefore, supervised learning might not be an ideal solution for msEPI. On the other hand, self-supervised learning, which does not require reference images, could be a more suitable approach for msEPI. Due to the difficulty in obtaining reliable ground truth data, conventional quantitative metrics such as SSIM and NRMSE may be less reliable for evaluation. In dMRI, FA maps and 2 nd crossing fibers could be used for obtaining more suitable metrics.We trained a single network for all diffusion directions, which improved performance and reduced training time (please see the supplementary material). NRMSE and NMAE were reduced from 14.69% to 13.61% and from 15.73% to 14.41%, respectively. The training time for the proposed network was 22:30 min per diffusion direction/slice (on GPU). This is expected to be reduced by transfer learning. Inference took approximately 1 s per direction/slice, and 2-shot LORAKS took approximately 20 s per direction/slice (on CPU). Since the images are highly similar across diffusion directions, training on the entire diffusion direction has a similar effect to increasing the size of the training database, thereby enhancing network training. Moreover, using a single network for all directions reduces training time compared to training separate networks for each direction, from 40:01 min to 22:30 min per diffusion direction and slice.As a future work, the simultaneous multi-slice (SMS) technique [21], which is often used for further acceleration, can be easily incorporated into the current network (please see the preliminary images in the supplementary material). At R sms =5 × 2-fold acceleration, NRMSE and NMAE were significantly reduced compared with SENSE, from 22.91% to 9.07% and from 26.09% to 11.12%, respectively. g-Slider could be a good application as well [20], because RF-encoded images also have highly similar image features."
Improved Multi-shot Diffusion-Weighted MRI with Zero-Shot Self-supervised Learning Reconstruction,,Fig. 1 .,
Improved Multi-shot Diffusion-Weighted MRI with Zero-Shot Self-supervised Learning Reconstruction,,Figure 1,
Improved Multi-shot Diffusion-Weighted MRI with Zero-Shot Self-supervised Learning Reconstruction,,Fig. 2 .,
Improved Multi-shot Diffusion-Weighted MRI with Zero-Shot Self-supervised Learning Reconstruction,,Fig. 3 .,
Improved Multi-shot Diffusion-Weighted MRI with Zero-Shot Self-supervised Learning Reconstruction,,Fig. 4 .,
Improved Multi-shot Diffusion-Weighted MRI with Zero-Shot Self-supervised Learning Reconstruction,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0 44.
Image2SSM: Reimagining Statistical Shape Models from Images with Radial Basis Functions,1,Introduction,"Statistical Shape Modeling (SSM) or morphological analysis, is a widespread tool used to quantify anatomical shape variation given a population of segmented 3D anatomies. Quantifying such subtle shape differences has been crucial in providing individualized treatments in medical procedures, detecting morphological pathologies, and advancing the understanding of different diseases [3,4,7,9,10,16,[19][20][21][27][28][29].The two principal shape representations for building SSMs and performing subsequent statistical analyses are deformation fields and landmarks. Deformation fields encode implicit transformations between cohort samples and a predefined (or learned) atlas. In contrast, landmarks are explicit points spread on shape surfaces that correspond across the population [22,23]. Landmark-based representations have been used extensively due to their simplicity, computational efficiency, and interpretability for statistical analyses [22,28]. Some applications use manually defined landmarks, however, this is labor-intensive, not reproducible, and requires domain expertise (e.g., radiologists). Computational methods (e.g., minimum description length -MDL [14], particle-based shape modeling -PSM [11,12], and frameworks based on Large Deformation Diffeomorphic Metric Mapping [15]) for automatically placing dense correspondence points, aka point distribution models (PDMs), have shifted the SSM field to data-driven characterization of population-level variabilities that is objective, reproducible, and scalable. However, this efficiency suffers when intricate shape surfaces require thousands of points representing localized, convoluted shape features that may live between landmarks. Furthermore, existing methods for landmark-based SSM must go through laborious and computationally expensive steps that require anatomical and technical expertise, starting from anatomy segmentation, shape data preprocessing, and correspondence optimization, to generate PDMs from 3D images. Existing methods (e.g., [1,2,5,6,24,25]) have been able to use deep learning to assuage the arduous process of building a PDM but still require the construction of PDMs (e.g., using a computational method such as PSM [11,12]) to supervise its learning task, making these deep learning based methods restricted and biased towards the shape statistics captured by the SSM method that is used to construct their training data.To address the shortcomings of existing models, we propose Image2SSM, a novel deep-learning-based approach for SSM directly from images that, given pairs of images and segmentations, can produce a statistical shape model using an implicit, continuous surface representation. Once trained, Image2SSM can produce PDMs of new images without the need for anatomy segmentations. Unlike existing deep learning-based methods for SSM from images, Image2SSM only requires image-segmentation pairs and alleviates the need for constructing PDM to supervise learning shape statistics from images. Image2SSM leverages an implicit, radial basis function (RBF)-based, representation of shapes to construct a self-supervised training signal by tasking the network to estimate a sparse set of control points and their respective suface normals that best approximate the underlying surface in the RBF sense. This novel application of RBFs to build SSMs allows statistical analyses on representative points/landmarks, their surface normals, and the shape surfaces themselves due to its compact, informative, yet comprehensive nature. Combined with deep networks to directly learn such a representation from images, this method ushers a next step towards fully end-to-end SSM frameworks that can build better and less restrictive low-dimensional shape representations more conducive to SSM analysis. In summary, the proposed method for SSM has the following strengths.-Using a continuous, but compact surface representation instead of only landmarks that allows performing analyses on points, normals, and surfaces alike. -The RBF shape representation can adapt to the underlying surface geometry, spreading more landmarks over the more complex surface regions. -A deep learning approach that bypasses any conventional correspondence optimization to construct training data for supervision, requiring virtually no hyperparameter tuning or preprocessing steps. -This method uses accelerated computational resources to perform training and outperforms existing deep learning based methods that constructs PDMs from unsegmented images."
Image2SSM: Reimagining Statistical Shape Models from Images with Radial Basis Functions,2,Methods,"Image2SSM is a deep learning method that learns to build an SSM for an anatomical structure of interest directly from unsegmented images. It is trained on a population of I-3D images I = {I i } I i=1 as input and is supervised by their respective segmentations S = {S i } I i=1 . Image2SSM learns an RBF-based shape representation, consisting of a set of J control points P = {P i } I i=1 , and their surface normals N = {N i } I i=1 for each input shape, where the i-th shape point distribution model (PDM) is denoted byand p i,j , n i,j ∈ R 3 . The network is trained end-to-end to minimize a loss that (1) makes the learned control points and their surface normals adhere to the underlying surface, (2) approximates surface normals at each control point to encode a signed distance field to the surface, (3) promotes correspondence of these control points across shapes in the population, and (4) encourages a spread of control points on each surface that adapts to the underlying geometrical complexity. The learned control points define an anatomical mapping, or a metric, among the given shapes that enables quantifying subtle shape differences and performing shape statistics, for example, using principal component analysis (PCA) or other non-linear methods (e.g., [17]). More importantly, once trained, Image2SSM can generate PDMs for new unsegmented images, bypassing the conventional SSM workflow of the manual (or semi-automated) segmentation, data preprocessing, and correspondence optimization. Furthermore, the continuous, implicit nature of the RBF representation enables extracting a proxy geometry (e.g., surface mesh or signed distance transforms -SDFs) at an arbitrary resolution that can be rasterized trivially on graphics hardware [8,26].In this section, we briefly elaborate on the RBF-shape representation, outline the network architecture, motivate the choices and design of the proposed losses, and detail the training protocol of Image2SSM."
Image2SSM: Reimagining Statistical Shape Models from Images with Radial Basis Functions,2.1,Representing Shapes Using RBFs,"Implicit surface representation based on radial basis functions, RBF-shape for short, has been proven effective at representing intricate shapes by leveraging both surface control points and normals to inform shape reconstructions [8,26]. It defines a set of control points at the zero-level set and a pair of off-surface points (aka dipoles) with a signed distance s and -s along the surface normal of each control point. This is illustrated in Fig. 1. We refer to the set of control points and their dipoles as P i for shape i, where P i = [P i , P + i , P - i ] with p ± i,j = p i,j ± sn i,j . Using P i , we define the shape's implicit function, a function that can query a distance to the surface given a point x ∈ R 3 , as follows:  where φ is the chosen RBF basis function (e.g., the thin plate spline φ(x, y) = (|x -y| 2 ) 2 log(|x -y| 2 ), the biharmonic φ(x, y) = |x -y| 2 or the triharmonic φ(x, y) = (|x -y| 2 ) 3 ) and c i ∈ R 3 and c 0 i ∈ R encodes the linear trend of the surface. We obtainby solving a system of equations formed by Eq. 1 over x ∈ P i , along with constraints to keep the linear part separate from the nonlinear deformations captured by the RBF term (first term in Eq. 1) to form a fully determined system. See [8,26] for more details. Ultimately, we can use this function f to query approximate distances to the surface to build a mesh or a signed distance transform for visualization and analysis.This representation can better represent shapes with far fewer control points due to its built-in interpolation capabilities, even further enhanced by informing the system with the point normals. Furthermore, this continuous representation allows Image2SSM to adapt to the underlying surface geometry and correct for control point placement mistakes while training."
Image2SSM: Reimagining Statistical Shape Models from Images with Radial Basis Functions,2.2,Loss Functions,"Image2SSM uses four complementary loss functions to be trained on concurrently, illustrated in Fig. 2. These are (i) surface loss, which aims to promote control point and normal adherence to the shape surface, (ii) normal loss, which attempts to learn the correct normals at each control point, (iii) correspondence loss, which aims to enforce positional correspondence across shapes, and (iv) sampling loss, which promotes a spread of the control points that best describes the underlying surface. Surface Loss: This loss guides control points to lie on the surface. We use l 1norm to force control points to lie on the zero-level set of the distance transform D i by minimizing the absolute distance-to-the-surface evaluated at it. For the i-th shape, this loss is defined as,where D i (p i,j ) is the distance transform value at point p i,j .Normal Loss: This loss aims to estimate the surface normal of each control point. This loss is supervised by the gradient of signed distance transforms (SDF) D = {D i } I i=1 , computed from the binary segmentations S, with respect to x, ∂D = {∂D i } I i=1 , which captures unnormalized surface normals. We use the cosine distance (in degrees) to penalize the deviation of the estimated normals from the normals computed from the distance transforms.Correspondence Loss: The notion of control points correspondence across the shape population can be quantified by the information content of the probability distribution induced by these control points in the shape space, the vector space defined by the shapes' PDMs [11,12]. The correspondence loss is triggered starting from the second epoch, where the mean shape µ = I i=1 P i is allowed to lag behind the update of the control points. Given a minibatch of size K, the correspondence loss is formulated using the differential entropy H of the samples in the minibatch, assuming a Gaussian distribution.where P here indicates the random variable of the shape space.Sampling Loss: This loss makes f encode the signed distance to the surface while encouraging the control points to be adapted to the underlying geometry.Here, we randomly sample Rpoints] that lie within a narrow band of thickness 2s around the surface (i.e., ±s from the zero-level set along the surface normal). The sampling loss minimizes distances between these narrow band points and the closest control point to each, scaled by the severity of the distance-to-surface approximation error. This objective guides control points to areas poorly described by f to progressively improve the signed distance-tosurface approximation and represent the shape more accurately. Let K i ∈ R R×M define the pairwise distances between each narrow band point b i,r and each control point p i,j for the i-th shape, where its r, j-th element k i r,j = b i,r -p i,j 2 . Let softmin(K i ) encode the normalized (over P i ) spatial proximity of each narrow band point to each control point, where r, the jth element of softmin(K i ) is computed as exp (-k i r,j )/+ captures the RBF approximation squared error at the narrow band points, where, where 1 M is a ones-vector of size M . The samples loss can then be written as,where ⊗ indicates the Hadamard (elementwise) multiplication of matrices and mean computes the average over the matrix elements.Image2SSM Loss: Given a minibatch of size K, the total loss of Image2SSM can be written as follows:where α, β, γ, and ζ ∈ R + are weighting hyperparameters of the losses and P K , N K are the control points and normals of the samples in the minibatch. Figure 2 gives a full overview of the network and its interaction with the loses. Image2SSM's network is trained end-to-end with w i s detached from the training so that the loss does not back-propagate through the volatile linear solver."
Image2SSM: Reimagining Statistical Shape Models from Images with Radial Basis Functions,3,Results,"We demonstrate Image2SSM's performance against the state-of-the-art correspondence optimization algorithm, namely the particle-based shape modeling (PSM), using its open-source implementation, ShapeWorks [11], and DeepSSM [5,6], a deep learning method that trains on an existing correspondence model (provided by the PSM in this case) to infer PDMs on new unsegmented images."
Image2SSM: Reimagining Statistical Shape Models from Images with Radial Basis Functions,3.1,Datasets,"We run tests on a dataset consisting of 50 proximal femur CT scans devoid of pathologies in the form of image-segmentation pairs. The femurs are reflected when appropriate and rigidly aligned to a common frame of reference. Due to space limitations, we also show similar results for a large-scale left atrium MRI dataset in the supplementary materials. For ease of comparison, we build SSMs with 128 particles for all algorithms as is sufficient to cover important femur shape features (femoral head with its fovea and the lesser and greater trochanter). Statistical Shape Model: We showcase Image2SSM in creating a statistical shape model on its training data and compare such a model with one optimized by PSM [11]. Figure 3 showcases the modes of variation, the surface-to-surface distances of Image2SSM against PSM, some representative reconstructions, and graphs for compactness (percentage of variance captured), specificity (ability to generate realistic shapes), and generalization (ability to represent unseen shape instance) [13]. We observe that the modes of variation and metrics match expectations in both approaches. We show the effectiveness of Image2SSM in adapting to surface details to achieve lower maximum surface-to-surface distance, and that, unlike PSM, we can achieve reasonable reconstructions using RBF-shape. More on adaptation to detail in Fig. 4.We implement Image2SSM in PyTorch and leverage the Autograd functionality to perform gradient descent using the Adam optimizer [18]. We randomly sample 10,000 3D points within the narrow band of each surface at each iteration. We use the biharmonic kernel for the basis function. However, the performance of Image2SSM is not significantly influenced by the kernel choice. The hyperparameters we use for Image2SSM are α = 1e 2 , β = 1e 2 , γ = 1e 4 , and ζ = 1e 3 for femurs and ζ = 1e 6 for left atria, which were determined based on the validation set. In practice, the runtime of Image2SSM is comparable to PSM for the femurs and roughly 2X faster for the left atria.  Inference Results: We compare the inference capabilities of Image2SSM against DeepSSM on unseen test data. We train DeepSSM with a PDM generated by PSM as supervision. For a fair comparison, we use DeepSSM without its augmented data, since Image2SSM does not require augmentation to learn shape models. Nevertheless, it is possible to generate and train Image2SSM on augmented data with even more facility than with DeepSSM. Figure 4 shows that Image2SSM compares very favorably to DeepSSM qualitatively and in terms of surface-to-surface distance."
Image2SSM: Reimagining Statistical Shape Models from Images with Radial Basis Functions,4,Conclusion,"Image2SSM is a novel deep-learning framework that both builds PDMs from image-segmentation pairs and predicts PDMs from unseen images. It uses an RBF-shape able to capture detail by leveraging surface normals at control points, and allows the SSM to adaptively permeate surfaces with high-level detail. Image2SSM represents another step forward in fully end-to-end PDMs and steers the field to utilizing more compact but comprehensive representations to achieve new analytical paradigms. Future directions include removing the requirement that the image-segmentation pairs must be rougly aligned across the cohort and relaxing the Gaussian assumption from correspondence enforcement."
Image2SSM: Reimagining Statistical Shape Models from Images with Radial Basis Functions,,Fig. 1 .,
Image2SSM: Reimagining Statistical Shape Models from Images with Radial Basis Functions,,f,
Image2SSM: Reimagining Statistical Shape Models from Images with Radial Basis Functions,,Fig. 2 .,
Image2SSM: Reimagining Statistical Shape Models from Images with Radial Basis Functions,,Fig. 3 .,
Image2SSM: Reimagining Statistical Shape Models from Images with Radial Basis Functions,,Fig. 4 .,
Image2SSM: Reimagining Statistical Shape Models from Images with Radial Basis Functions,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0_49.
TPRO: Text-Prompting-Based Weakly Supervised Histopathology Tissue Segmentation,1,Introduction,"Automated segmentation of histopathological images is crucial, as it can quantify the tumor micro-environment, provide a basis for cancer grading and prognosis, and improve the diagnostic efficiency of clinical doctors [6,13,19]. However, pixellevel annotation of images is time-consuming and labor-intensive, especially for histopathology images that require specialized knowledge. Therefore, there is an urgent need to pursue weakly supervised solutions for pixel-wise segmentation. Nonetheless, weakly supervised histopathological image segmentation presents a challenge due to the low contrast between different tissues, intra-class variations, and inter-class similarities [4,11]. Additionally, the tissue structures in histopathology images can be randomly arranged and dispersed, which makes it difficult to identify complete tissues or regions of interest [7].Ours CAM Under the microscope, tumor epithelial Ɵssue may appear as solid nests, acinar structures, or papillary formaƟons. The cells may have enlarged and irregular nuclei."
TPRO: Text-Prompting-Based Weakly Supervised Histopathology Tissue Segmentation,,Tumor epithelial Ɵssue,"Necrosis Ɵssue Tumor-associated stromaNecrosis may appear as areas of pink, amorphous material under the microscope, and may be surrounded by viable tumor cells and stroma.Tumor-associated stroma Ɵssue is the connecƟve Ɵssue that surrounds and supports the tumor epithelial Ɵssue.Fig. 1. Comparison of activation maps extracted from CAM and our method, from left to right: origin image, ground truth, three activation maps of tumor epithelial (red), necrosis (green), and tumor-associated stroma (orange) respectively. On the right side, there are some examples of the related language knowledge descriptions used in our method. It shows that CAM only highlights a small portion of the target, while our method, which incorporates external language knowledge, can encompass a wider and more precise target tissue. (Color figure online)Recent studies on weakly supervised segmentation primarily follow class activation mapping (CAM) [20], which localizes the attention regions and then generates the pseudo labels to train the segmentation network. However, the CAM generated based on the image-level labels can only highlight the most discriminative region, but fail to locate the complete object, leading to defective pseudo labels, as shown in Fig. 1. Accordingly, many attempts have been made to enhance the quality of CAM and thus boost the performance of weakly supervised segmentation. Han et al. [7] proposed an erasure-based method that continuously expands the scope of attention areas to obtain rich content of pseudo labels. Li et al. [11] utilized the confidence method to remove any noise that may exist in the pseudo labels and only included the confident pixel labels for the segmentation training. Zhang et al. [18] leveraged the Transformer to model the long-distance dependencies on the whole histopathological images to improve the CAM's ability to find more complete regions. Lee et al. [10] utilized the ability of an advanced saliency detection model to assist CAM in locating more precise targets. However, these improved variants still face difficulties in capturing the complete tissues. The primary limitation is that the symptoms and manifestations of histopathological subtypes cannot be comprehensively described by an abstract semantic category. As a result, the image-level label supervision may not be sufficient to pinpoint the complete target area.To remedy the limitations of image-level supervision, we advocate for the integration of language knowledge into weakly supervised learning to provide reliable guidance for the accurate localization of target structures. To this end, we propose a text-prompting-based weakly supervised segmentation method (TPRO) for accurate histopathology tissue segmentation. The text information originates from the task's semantic labels and external descriptions of subtype manifestations. For each semantic label, a pre-trained medical language model is utilized to extract the corresponding text features that are matched to each feature point in the image spatial space. A higher similarity represents a higher possibility of this location belonging to the corresponding semantic category. Additionally, the text representations of subtype manifestations, including tissue morphology, color, and relationships to other tissues, are extracted by the language model as external knowledge. The discriminative information can be explored from the text knowledge to help identify and locate complete tissues accurately by jointly modeling long-range dependencies between image and text. We conduct experiments on two weakly supervised histological segmentation benchmarks, LUAD-HistoSeg and BCSS-WSSS, and demonstrate the superior quality of pseudo labels produced by our TPRO model compared to other CAM-based methods.Our contributions are summarized as follows: (1) To the best of our knowledge, this is the first work that leverages language knowledge to improve the quality of pseudo labels for weakly-supervised histopathology image segmentation. (2) The proposed text prompting models the correlation between image representations and text knowledge, effectively improving the quality of pseudo labels. (3) The effectiveness of our approach has been effectively validated by two benchmarks, setting a new state of the art. "
TPRO: Text-Prompting-Based Weakly Supervised Histopathology Tissue Segmentation,2,Method,"Figure 2 displays the proposed TPRO framework, a classification network designed to train a suitable model and extract segmentation pseudo-labels. The framework comprises a knowledge attention module and three encoders: one vision encoder and two text encoders (label encoder and knowledge encoder)."
TPRO: Text-Prompting-Based Weakly Supervised Histopathology Tissue Segmentation,2.1,Classification with Deep Text Guidance,"Vision Encoder. The vision encoder is composed of four stages that encode the input image into image features. The image features are denoted as T s ∈ R Ms×Cs , where 2 ≤ s ≤ 4 indicates the stage number.Label Encoder. The label encoder encodes the text labels in the dataset into N label features, denoted as L ∈ R N ×C l , where N represents the number of classes in the dataset and C l represents the dimension of label features. Since the label features will be used to calculate the similarity with image features, it is important to choose a language model that has been pre-trained on image-text pairs. Here we use MedCLIP1 as our label encoder, which is a model fine-tuned on the ROCO dataset [12] based on CLIP [14].Knowledge Encoder. The knowledge encoder is responsible for embedding the descriptions of subtype manifestations into knowledge features, denoted as K ∈ R N ×C k . The knowledge features guide the image features to focus on regions relevant to the target tissue. To encode the subtype manifestations description into more general semantic features, we employ ClinicalBert [2] as our knowledge encoder. ClinicalBert is a language model that has been fine-tuned on the MIMIC-III [8] dataset based on BioBert [9].Adaptive Layer. We freeze the label and knowledge encoders for training efficiency but add an adaptive layer after the text encoders to better tailor the text features to our dataset. The adaptive layer is a simple FC-ReLU-FC block that allows for fine-tuning of the features extracted from the text encoders.Label-Pixel Correlation. After the input image and text labels are embedded. We employ the inner product to compute the similarity between image features and label features, denoted as F s . Specially, we first reshape the image features from a token format into feature maps. We denote the feature map as I s ∈ R Hs×Ws×Cs , where H s and W s mean the height and width of the feature map. F s is computed with the below formula(1)Then, we perform a global average-pooling operation on the produced similarity map to obtain the class prediction, denoted as P s ∈ R 1×N . We then calculate the binary cross-entropy loss between the class label Y ∈ R 1×N and the class prediction P s to supervise the model training, which is formulated as:Deep Supervision. To leverage the shallow features in the network, we employ a deep supervision strategy by calculating the similarity between the image features from different stages and the label features from different adaptive layers. Class predictions are derived from these similarity maps. The loss of the entire network is computed as:(3)"
TPRO: Text-Prompting-Based Weakly Supervised Histopathology Tissue Segmentation,2.2,Knowledge Attention Module,"To enhance the model's understanding of the color, morphology, and relationships between different tissues, we gather text representations of different subtype manifestations from the Internet and encode them into external knowledge via the knowledge encoder. The knowledge attention module uses this external knowledge to guide the image features toward relevant regions of the target tissues.The knowledge attention module, shown in Fig. 2, consists of two multi-head self-attention modules. The image features T 4 ∈ R M4×C4 and knowledge features after adaptive layer K ∈ R N ×C4 are concatenated in the token dimension to obtain T fuse ∈ R (M4+N )×C4 . This concatenated feature is then fed into the knowledge attention module for self-attention calculation. The output tokens are split, and the part corresponding to the image features is taken out. Noted that the knowledge attention module is added only after the last stage of the vision encoder to save computational resources."
TPRO: Text-Prompting-Based Weakly Supervised Histopathology Tissue Segmentation,2.3,Pseudo Label Generation,"In the classification process, we calculate the similarity between image features and label features to obtain a similarity map F , and then directly use the result of global average pooling on the similarity map as a class prediction. That is, the value at position (i, j, k) of F represents the probability that pixel (i, j) is classified into the k th class. Therefore we directly use F as our localization map. We first perform min-max normalization on it, the formula is as followswhere 1 ≤ c ≤ N means c th class in the dataset. Then we calculate the background localization map by the following formula:where α ≥ 1 denotes a hyper-parameter that adjusts the background confidence scores. Referring to [1] and combined with our own experiments, we set α to 10. Then we stitch together the localization map of foreground and background, denoted as F . In order to make full use of the shallow information of the network, we perform weighted fusion on the localization maps from different stages by the following formula:Finally, we perform argmax operation on F all to obtain the final pseudo-label.3 Experiments "
TPRO: Text-Prompting-Based Weakly Supervised Histopathology Tissue Segmentation,3.2,Implementation Details,"For the classification part, we adopt MixTransformer [17] pretrained on Ima-geNet, MedCLIP, and ClinicalBert [2] as our vision encoder, label encoder, and knowledge encoder, respectively. The hyperparameters during training and evaluation can be found in the supplementary materials. We conduct all of our experiments on 2 NVIDIA GeForce RTX 2080 Ti GPUs."
TPRO: Text-Prompting-Based Weakly Supervised Histopathology Tissue Segmentation,3.3,Compare with State-of-the-Arts,"Comparison on Pseudo-Labels. Table 1 compares the quality of our pseudolabels with those generated by previous methods. CAM [20] and Grad-CAM [15] were evaluated using the same ResNet38 [16] classifier, and the results showed that CAM [20] outperformed Grad-CAM [15], with mIoU values of 70.44% and 56.52% on the LUAD-HistoSeg and BCSS-WSSS datasets, respectively. Tran-sWS [18] consists of a classification and a segmentation branch, and Table 1 displays the pseudo-label scores generated by the classification branch. Despite using CAM [20] for pseudo-label extraction, TransWS [18] yielded inferior results compared to CAM [20]. This could be due to the design of TransWS [18] for single-label image segmentation, with the segmentation branch simplified to binary segmentation to reduce the difficulty, while our dataset consists of multilabel images. Among the compared methods, MLPS [7] was the only one to surpass CAM [20] in terms of the quality of the generated pseudo-labels, with its proposed progressive dropout attention effectively expanding the coverage of target regions beyond what CAM [20] can achieve. Our proposed method outperformed all previous methods on both LUAD-HistoSeg and BCSS-WSSS datasets, with improvements of 2.64% and 5.42% over the second-best method, respectively (Table 2). Comparison on Segmentation Results. To further evaluate our proposed method, we trained a segmentation model using the extracted pseudo-labels and compared its performance with previous methods. Due to its heavy reliance on dataset-specific post-processing steps, HistoSegNet [5] failed to produce the desired results on our datasets. As we have previously analyzed since the datasets we used are all multi-label images, it was challenging for the segmentation branch of TransWS [18] to perform well, and it failed to provide an overall benefit to the model. Experimental results also indicate that the IoU scores of its segmentation branch were even lower than the pseudo-labels of the classification branch. By training the segmentation model of OEEM [11] using the pseudo-labels extracted by CAM [20] in Table 1, we can observe a significant improvement in the final segmentation results. The final segmentation results of MLPS [7] showed some improvement compared to its pseudo-labels, indicating the effectiveness of the Multi-layer Pseudo Supervision and Classification Gate Mechanism strategy proposed by MLPS [7]. Our segmentation performance surpassed all previous methods. Specifically, our mIoU scores exceeded the second-best method by 3.17% and 3.09% on LUAD-HistoSeg and BCSS-WSSS datasets, respectively. Additionally, it is worth noting that we did not use any strategies specifically designed for the segmentation stage."
TPRO: Text-Prompting-Based Weakly Supervised Histopathology Tissue Segmentation,3.4,Ablation Study,"The results of our ablation experiments are presented in Table 3. We set the baseline as the framework shown in Fig. 2 with all text information and deep supervision strategy removed. It is evident that the addition of textual information increases our pseudo-label mIoU by 2.50%. Furthermore, including the deep supervision strategy and knowledge attention module improves our pseudo-label by 0.98% and 2.74%, respectively. These findings demonstrate the significant contribution of each proposed module to the overall improvement of the results. In order to demonstrate the effectiveness of fusing pseudo-labels from the last three stages, we have presented in Table 4 the IoU scores for each stage's pseudolabels as well as the fused pseudo-labels. It can be observed that after fusing the pseudo-labels, not only have the IoU scores for each class substantially increased, but the mIoU score has also increased by 0.91% compared to the fourth stage."
TPRO: Text-Prompting-Based Weakly Supervised Histopathology Tissue Segmentation,4,Conclusion,"In this paper, we propose the TPRO to address the limitation of weakly supervised semantic segmentation on histopathology images by incorporating text supervision and external knowledge. We argue that image-level labels alone cannot provide sufficient information and that text supervision and knowledge attention can provide additional guidance to the model. The proposed method achieves the best results on two public datasets, LUAD-HistoSeg and BCSS-WSSS, demonstrating the superiority of our method."
TPRO: Text-Prompting-Based Weakly Supervised Histopathology Tissue Segmentation,,Fig. 2 .,
TPRO: Text-Prompting-Based Weakly Supervised Histopathology Tissue Segmentation,,Table 1 .,
TPRO: Text-Prompting-Based Weakly Supervised Histopathology Tissue Segmentation,,Table 2 .,
TPRO: Text-Prompting-Based Weakly Supervised Histopathology Tissue Segmentation,,Table 3 .,
TPRO: Text-Prompting-Based Weakly Supervised Histopathology Tissue Segmentation,,Table 4 .,
TPRO: Text-Prompting-Based Weakly Supervised Histopathology Tissue Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0_11.
LSOR: Longitudinally-Consistent Self-Organized Representation Learning,1,Introduction,"The interpretability of deep learning models is especially a concern for applications related to human health, such as analyzing longitudinal brain MRIs. To avoid interpretation during post-hoc analysis [6,14], some methods strive for an interpretable latent representation [9]. One example is self-organizing maps (SOM) [5], which cluster the latent space so that the SOM representations (i.e., the 'representatives of the clusters) can be arranged in a discrete (typically 2D) grid while preserving high-dimensional relationships between clusters. Embedded in unsupervised deep learning models, SOMs have been used to generate interpretable representations of low-resolution natural images [3,8].Intriguing as it sounds, we found their application to (longitudinal) 3D brain MRIs unstable during training and resulted in uninformative SOMs. These models get stuck in local minima so that only a few SOM representations are updated during backpropagation. The issue has been less severe in prior applications [3,8] as their corresponding latent space is of much lower dimension than the task at hand, which requires a high dimension latent space so that it can accurately encode the fine-grained anatomical details in brain MRIs [12,17]. To ensure all SOM representations can be updated during backpropagation, we propose a soft weighing scheme that not only updates the closest SOM representation for a given MRI but also updates all other SOM representations based on their distance to the closest SOM representation [3,8]. Moreover, our model relies on a stop-gradient operator [16], which sets the gradient of the latent representation to zero so that it only focuses on updating the SOM representations. It is especially crucial at the beginning of the training when the (randomly initialized) SOM representations are not good representatives of their clusters. Finally, the latent representations of the MRIs are updated via a commitment loss, which encourages the latent representation of an MRI sample to be close to its nearest SOM representation. In practice, these three components ensure stability during the self-supervised training of the SOM on high-dimensional latent spaces.To generate SOMs informative to neuroscientists, we extend SOMs to the longitudinal setting such that the latent space and corresponding SOM grid encode brain aging. Inspired by [12], we encode pairs of MRIs from the same longitudinal sequence (i.e., same subject) as a trajectory and encourage the latent space to be a smooth trajectory (vector) field. We enforce smoothness by computing for each SOM cluster a reference trajectory, which represents the average aging of that cluster with respect to the training set. The reference trajectories are updated by the exponential moving average (EMA) such that, in each iteration, it aggregates the average trajectory of a cluster with respect to the corresponding training batch (i.e., batch-wise average trajectory). In doing so, the model ensures longitudinal consistency as the (subject-specific) trajectories of a cluster are maximally aligned with the reference trajectory of that cluster.Named Longitudinally-consistent Self-Organized Representation learning (LSOR), we evaluate our method on a longitudinal T1-weighted MRI dataset of 632 subjects from ADNI to encode the brain aging of Normal Controls (NC) and patients diagnosed with static Mild Cognitive Impairment (sMCI  on longitudinal MRIs, i.e., without using any tabular data such as age, cognitive measure, or diagnosis. To visualize aging effects on the grid, we compute (post-hoc) a 2D similarity grid for each MRI that stores the similarity scores between the latent representation of that MRI and all SOM representations. As the SOM grid is an encoding of brain aging, the similarity grid indicates the likelihood of placing the MRI within the ""spectrum"" of aging. Given all MRIs of a longitudinal scan, the change across the corresponding similarity grids over time represents the brain aging process of that individual. Furthermore, we infer brain aging on a group-level by first computing the average similarity grid for an age group and then visualizing the difference of those average similarity grids across age groups. With respect to the downstream tasks of classification (sMCI vs. pMCI) and regression (i.e., estimating the Alzheimer's Disease Assessment Scale-Cognitive Subscale (ADAS-Cog) on all subjects), our latent representations of the MRIs is associated with comparable or higher accuracy scores than representations learned by other state-of-the-art self-supervised methods."
LSOR: Longitudinally-Consistent Self-Organized Representation Learning,2,Method,"As shown in Fig. 1, the longitudinal 3D MRIs of a subject are encoded as a series of trajectories (blue vectors) in the latent space. Following [12,17], we consider a pair of longitudinal MRIs (that corresponds to a blue vector) as a training sample. Specifically, let S denote the set of image pairs of the training cohort, where the MRIs x u and x v of a longitudinal pair (x u , x v ) are from the same subject and x v was acquired Δt years after x u . For simplicity, × refers to u or v when a function is separately applied to both time points. The MRIs are then mapped to the latent space by an encoder F , i.e., z × := F (x × ). On the latent space, the trajectory of the pair is denoted as Δz := (z v -z u )/Δt, which represents morphological changes. Finally, decoder H reconstructs the input MRI x × from the latent representation z × , i.e., x× := H(z × ). Next, we describe LSOR, which generates interpretable SOM representations, and the post-hoc analysis for deriving similarity grids."
LSOR: Longitudinally-Consistent Self-Organized Representation Learning,2.1,LSOR,"Following [3,8], SOM representations are organized in a N r by N c grid (denoted as SOM grid) G = {g i,j } Nr,Nc i=1,j=1 , where g i,j denotes the SOM representation on the i-th row and j-th column. This easy-to-visualize grid preserves the highdimensional relationships between the clusters as shown in by the orange lines in Fig. 1. Given the latent representation z × , its closest SOM representation is denoted as g × , where × := argmin (i,j) z × -g i,j 2 is its 2D grid index in G and • 2 is the Euclidean norm. This SOM representation is also used to reconstruct the input MRI by the decoder, i.e., x× g = H(g × ). To do so, the reconstruction loss encourages both the latent representation z × and its closet SOM representation g × to be descriptive of the input MRI x × , i.e.,where E defines the expected value. The remainder describes the three novel components of our SOM representation.Explicitly Regularizing Closeness. Though L recon implicitly encourages close proximity between z × and g × , it does not inherently optimize g × as z × is not differentiable with respect to g × . Therefore, we introduce an additional 'commitment' loss explicitly promoting closeness between them:Soft Weighting Scheme. In addition to update z × 's closest SOM representation g × , we also update all SOM representations g i,j by introducing a soft weighting scheme as proposed in [10]. Specifically, we design a weight w × i,j to regularize how much g i,j should be updated with respect to z × based on its proximity to the grid location × of g × , i.e.,where δ(w) := w i,j wi,j ensures that the scale of weights is constant during training and τ > 0 is a scaling hyperparameter. Now, we design the following loss L som so that SOM representations close to × on the grid are also close to z × in the latent space (measured by the Euclidean distance z × -g i,j 2 ):To improve robustness, we make two more changes to Eq. 3. First, we account for SOM representations transitioning from random initialization to becoming meaningful cluster centers that preserve the high-dimensional relationships within the 2D SOM grid. We do so by decreasing τ in Eq. 2 with each iteration so that the weights gradually concentrate on SOM representations closer to g × as training proceeds:with τ min being the minimum and τ max the maximum standard deviation in the Gaussian kernel, and t represents the current and T the maximum iteration.The second change to Eq. 3 is to apply the stop-gradient operator sg[•] [16] to z × , which sets the gradients of z × to 0 during the backward pass. The stopgradient operator prevents the undesirable scenario where z × is pulled towards a naive solution, i.e., different MRI samples are mapped to the same weighted average of all SOM representations. This risk of deriving the naive solution is especially high in the early stages of the training when the SOM representations are randomly initialized and may not accurately represent the clusters.Longitudinal Consistency Regularization. We derive a SOM grid related to brain aging by generating an age-stratified latent space. Specifically, the latent space is defined by a smooth trajectory field (Fig. 1, blue box) characterizing the morphological changes associated with brain aging. The smoothness is based on the assumption that MRIs with similar appearances (close latent representations on the latent space) should have similar trajectories. It is enforced by modeling the similarity between each subject-specific trajectory Δz with a reference trajectory that represents the average trajectory of the cluster. Specifically, Δg i,j is the reference trajectory (Fig. 1, green arrow) associated with g i,j then the reference trajectories of all clusters G Δ = {Δg i,j } Nr,Nc i=1,j=1 represent the average aging of SOM clusters with respect to the training set. As all subject-specific trajectories are iteratively updated during the training, it is computationally infeasible to keep track of G Δ on the whole training set. We instead propose to compute the exponential moving average (EMA) (Fig. 1, black box), which iteratively aggregates the average trajectory with respect to a training batch to G Δ :α is the EMA keep rate, k denotes the index of the sample pair, N bs symbolizes the batch size, 1[•] is the indicator function, and |Ω i,j | denotes the number of sample pairs with u = (i, j) within a batch. Then in each iteration, Δh i,j (Fig. 1, purple arrow) represents the batch-wise average of subject-specific trajectories for sample pairs with u = (i, j). By iteratively updating G Δ , G Δ then approximate the average trajectories derived from the entire training set. Lastly, inspired by [11,12], the longitudinal consistency regularization is formulated aswhere θ[•, •] denotes the angle between two vectors. Since Δg is optimized by EMA, the stop-gradient operator is again incorporated to only compute the gradient with respect to Δz in L dir .Objective Function. The complete objective function is the weighted combination of the prior losses with weighing parameters λ commit , λ som , and λ dir :The objective function encourages a smooth trajectory field of aging on the latent space while maintaining interpretable SOM representations for analyzing brain age in a pure self-supervised fashion."
LSOR: Longitudinally-Consistent Self-Organized Representation Learning,2.2,SOM Similarity Grid,"During inference, a (2D) similarity grid ρ is computed by the closeness between the latent representation z of an MRI sample and the SOM representations:std denotes the standard deviation of the distance between z to all SOM representations. As the SOM grid is learned to be associated with brain age (e.g., represents aging from left to right), the similarity grid essentially encodes a ""likelihood function"" of the brain age in z. Given all MRIs of a longitudinal scan, the change across the corresponding similarity grids over time represents the brain aging process of that individual. Furthermore, brain aging on the group-level is captured by first computing the average similarity grid for an age group and then visualizing the difference of those average similarity grids across age groups."
LSOR: Longitudinally-Consistent Self-Organized Representation Learning,3,Experiments,
LSOR: Longitudinally-Consistent Self-Organized Representation Learning,3.1,Experimental Setting,"Dataset. We evaluated the proposed method on all 632 longitudinal T1weighted MRIs (at least two visits per subject, 2389 MRIs in total) from ADNI-1 [13].  Evaluation. We performed five-fold cross-validation (folds split based on subjects) using 10% of the training subjects for validation. The training data was augmented by flipping brain hemispheres and random rotation and translation.To quantify the interpretability of the SOM grid, we correlated the coordinates of the SOM grid with quantitative measures related to brain age, e.g., chronological age, the percentage of subjects with severe cognitive decline, and Alzheimer's Disease Assessment Scale-Cognitive Subscale (ADAS-Cog). We illustrated the interpretability with respect to brain aging by visualizing the changes in the SOM similarity maps over time. We further visualized the trajectory vector field along with SOM representations by projecting the 1024-dimensional representations to the first two principal components of SOM representations. Lastly, we quantitatively evaluated the quality of the representations by applying them to the downstream tasks of classifying sMCI vs. pMCI and ADAS-Cog prediction. We measured the classification accuracy via Balanced accuracy (BACC) and Area Under Curve (AUC) and the prediction accuracy via R2 and rootmean-square error (RMSE). The classifier and predictor were multi-layer per- ceptrons containing two fully connected layers of dimensions 1024 and 64 with a LeakyReLU activation. We compared the accuracy metrics to models using the same architecture with encoders pre-trained by other representation learning methods, including unsupervised methods (AE, VAE [4]), self-supervised method (SimCLR [1]), longitudinal self-supervised method (LSSL [17]), and longitudinal neighborhood embedding (LNE [12]). All comparing methods used the same experimental setup (e.g., encoder-decoder, learning rate, batch size, epochs, etc.), and the method-specific hyperparameters followed [12]."
LSOR: Longitudinally-Consistent Self-Organized Representation Learning,3.2,Results,"Interpretability of SOM Embeddings. Fig. 2 shows the stratification of brain age over the SOM grid G. For each grid entry, we show the average value of chronological age (Fig. 2 Interpretability of Similarity Grid. Visualizing the average similarity grid ρ of the NC and AD at each age range in Fig. 3, we observed that higher similarity (yellow) gradually shifts towards the right with age in both NC and AD (see Supplemental Fig. S2 for sMCI and pMCI cohorts). However, the shift is faster for AD, which aligns with AD literature reporting that AD is linked to accelerated brain aging [15]. Furthermore, the subject-level aging effects shown in Supplemental Fig. S3 reveal that the proposed visualization could capture subtle morphological changes caused by brain aging.Interpretability of Trajectory Vector Field. Fig. 4 plots the PCA projections of the latent space in 2D, which shows a smooth trajectory field (gray arrows) and reference trajectories G Δ (blue arrows) representing brain aging.  This projection also preserved the 2D grid structure (orange) of the SOM representations suggesting that aging was the most important variation in the latent space.Downstream Tasks. To evaluate the quality of the learned representations, we froze encoders trained by each method without fine-tuning and utilized their representations for the downstream tasks (Table 1). On the task of sMCI vs. pMCI classification (Table 1 (left)), the proposed method achieved a BACC of 69.8 and an AUC of 72.4, a comparable accuracy (p > 0.05, DeLong's test) with LSSL [17] and LNE [12], two state-of-the-art self-supervised methods on this task. On the ADAS-Cog score regression task, the proposed method obtained the best accuracy with an R2 of 0.32 and an RMSE of 6.31. It is worth mentioning that an accurate prediction of the ADAS-Cog score is very challenging due to its large range (between 0 and 70) and its subjectiveness resulting in large variability across exams [2] so that even larger RMSEs have been reported for this task [7]. Furthermore, our representations were learned in an unsupervised manner so that further fine-tuning of the encoder would improve the prediction accuracy."
LSOR: Longitudinally-Consistent Self-Organized Representation Learning,4,Conclusion,"In this work, we proposed LSOR, the first SOM-based learning framework for longitudinal MRIs that is self-supervised and interpretable. By incorporating a soft SOM regularization, the training of the SOM was stable in the highdimensional latent space of MRIs. By regularizing the latent space based on longitudinal consistency as defined by longitudinal MRIs, the latent space formed a smooth trajectory field capturing brain aging as shown by the resulting SOM grid. The interpretability of the representations was confirmed by the correlation between the SOM grid and cognitive measures, and the SOM similarity map.When evaluated on downstream tasks sMCI vs. pMCI classification and ADAS-Cog prediction, LSOR was comparable to or better than representations learned from other state-of-the-art self-and un-supervised methods. In conclusion, LSOR is able to generate a latent space with high interpretability regarding brain age purely based on MRIs, and valuable representations for downstream tasks."
LSOR: Longitudinally-Consistent Self-Organized Representation Learning,,Fig. 1 .,
LSOR: Longitudinally-Consistent Self-Organized Representation Learning,,Fig. 2 .,
LSOR: Longitudinally-Consistent Self-Organized Representation Learning,,Fig. 3 .,
LSOR: Longitudinally-Consistent Self-Organized Representation Learning,,,
LSOR: Longitudinally-Consistent Self-Organized Representation Learning,,Fig. 4 .,
LSOR: Longitudinally-Consistent Self-Organized Representation Learning,,,
LSOR: Longitudinally-Consistent Self-Organized Representation Learning,,Table 1 .,
LSOR: Longitudinally-Consistent Self-Organized Representation Learning,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0 27.
VesselVAE: Recursive Variational Autoencoders for 3D Blood Vessel Synthesis,1,Introduction,"Accurate 3D models of blood vessels are increasingly required for several purposes in Medicine and Science [25]. These meshes are typically generated using either image segmentation or synthetic methods. Despite significant advances in vessel segmentation [26], reconstructing thin features accurately from medical images remains challenging [2]. Manual editing of vessel geometry is a tedious and error prone task that requires expert medical knowledge, which explains the scarcity of curated datasets. As a result, several methods have been developed to adequately synthesize blood vessel geometry [29].Within the existing literature on generating vascular 3D models, we identified two primary types of algorithms: fractal-based, and space-filling algorithms. Fractal-based algorithms use a set of fixed rules that include different branching parameters, such as the ratio of asymmetry in arterial bifurcations and the relationship between the diameter of the vessel and the flow [7,33]. On the other hand, space-filling algorithms allow the blood vessels to grow into a specific perfusion volume while aligning with hemodynamic laws and constraints on the formation of blood vessels [9,17,21,22,25]. Although these model-based methods provide some degree of control and variation in the structures produced, they often fail to capture the diversity of real anatomical data.In recent years, deep neural networks led to the development of powerful generative models [30], such as Generative Adversarial Networks [8,12] and Diffusion Models [11], which produced groundbreaking performance in many applications, ranging from image and video synthesis to molecular design. These advances have inspired the creation of novel network architectures to model 3D shapes using voxel representations [28], point clouds [31], signed distance functions [19], and polygonal meshes [18]. In particular, and close to our aim, Wolterink et al. [27] propose a GAN model capable of generating coronary artery anatomies. However, this model is limited to generating single-channel blood vessels and thus does not support the generation of more complex, tree-like vessel topologies.In this work we propose a novel data-driven framework named VesselVAE for synthesizing blood vessel geometry. Our generative framework is based on a Recursive variational Neural Network (RvNN), that has been applied in various contexts, including natural language [23,24], shape semantics modeling [14,15], and document layout generation [20]. In contrast to previous data-driven methods, our recursive network fully exploits the hierarchical organization of the vessel and learns a low-dimensional manifold encoding branch connectivity along with geometry features describing the target surface. Once trained, the Vessel-VAE latent space is sampled to generate new vessel geometries. To the best of our knowledge, this work is the first to synthesize multi-branch blood vessel trees by learning from real data. Experiments show that synth and real blood vessel geometries are highly similar measured with the cosine similarity: radius (.97), length (.95), and tortuosity (.96)."
VesselVAE: Recursive Variational Autoencoders for 3D Blood Vessel Synthesis,2,Methods,"Input. The network input is a binary tree representation of the blood vessel 3D geometry. Formally, each tree is defined as a tuple (T, E), where T is the set of nodes, and E is the set of directed edges connecting a pair of nodes (n, m), with n, m ∈ T . In order to encode a 3D model into this representation, vessel segments V are parameterized by a central axis consisting of ordered points in Euclidean space: V = v 1 , v 2 , . . . , v N and a radius r, assuming a piece-wise tubular vessel for simplicity. We then construct the binary tree as a set of nodes T = n 1 , n 2 , . . . , n N , where each node n i represents a vessel segment v and contains an attribute vector 4 with the coordinates of the corresponding point and its radius r i . See Sect. 3 for details.Network Architecture. The proposed generative model is a Recursive variational Neural Network (RvNN) consisting of two main components: the Encoder (Enc) and the Decoder (Dec) networks. The Encoder transforms a tree structure into a hierarchical encoding on the learned manifold. The Decoder network is capable of sampling from this encoded space to decode tree structures, as depicted in Fig. 1. The encoding and decoding processes are achieved through a depth-first traversal of the tree, where each node is combined with its parent node recursively. The model outputs a hierarchy of vessel branches, where each internal node in the hierarchy is represented by a vector that encodes its own attributes and the information of all subsequent nodes in the tree.Within the RvNN Decoder network there are two essential components: the Node Classifier (Cls) and the Features Decoder Multi-Layer Perceptron (Features Dec-MLP). The Node Classifier discerns the type of node to be decoded, whether it is a leaf node or an internal node with one or two bifurcations. This is implemented as a multi-layer perceptron trained to predict a three-category bifurcation probability based on the encoded vector as input. Complementing the Node Classifier, the Features Dec-MLP is responsible for reconstructing the attributes of each node, specifically its coordinates and radius. Furthermore, two additional components, the Right and Left Dec-MLP, are in charge of recursively decoding the next encoded node in the tree hierarchy. These decoder's branches execute based on the classifier prediction for that encoded node. If the Node Classifier predicts a single child for a node, a right child is assumed by default.In addition to the core architecture, our model is further augmented with three auxiliary, shallow, fully-connected neural networks: f μ , f σ , and g z . Positioned before the RvNN bottleneck, the f μ and f σ networks shape the distribution of the latent space where encoded tree structures lie. Conversely, the g z network, situated after the bottleneck, facilitates the decoding of latent variables, aiding the Decoder network in the reconstruction of tree structures. Collectively, these supplementary networks streamline the data transformation process through the model. All activation functions used in our networks are leaky ReLUs. See the Appendix for implementation details.Objective. Our generative model is trained to learn a probability distribution over the latent space that can be used to generate new blood vessel segments. After encoding, the decoder takes samples from a multivariate Gaussian distribution:, where Enc is the recursive encoder and f μ , f σ are two fully-connected neural networks. In order to recover the feature vectors x for each node along with the tree topology, we simultaneously train the regression network (Features Dec-MLP in Fig. 1) on a reconstruction objective L recon , and the Node Classifier using L topo . Additionally, in line with the general framework proposed by β-VAE [10], we incorporated a Kullback-Leibler (KL) divergence term encouraging the distribution p(z s (x)) over all training samples x to move closer to the prior of the standard normal distribution p(z). We therefore minimize the following equation:where the reconstruction loss is defined asand the topology objective is a three-class cross entropy loss L topo = Σ 3 c=1 x c log(Cls(Dec(x)) c ). Notice that x c is a binary indicator (0 or 1) for the true class of the sample x. Specifically, x c = 1 if the sample belongs to class c and 0 otherwise. Cls(Dec(x)) c is the predicted probability of the sample x belonging to class c (zero, one, or two bifurcations), as output by the classifier. Here, Dec(x) denotes the encoded-decoded node representation of the input sample x. 3D Mesh Synthesis. Several algorithms have been proposed in the literature to generate a surface 3D mesh from a tree-structured centerline [29]. For simplicity and efficiency, we chose the approach described in [6], which produces good quality meshes from centerlines with a low sample rate. The implemented method iterates through the points in the curve generating a coarse quadrilateral Fig. 2. Dataset and pre-processing overview: The raw meshes from the IntraA 3D collection undergo pre-processing using the VMTK toolkit. This step is crucial for extracting centerlines and cross-sections from the meshes, which are then used to construct their binary tree representations. mesh along the segments and joints. The centerline sampling step is crucial for a successful reconstruction outcome. Thus, our re-sampling is not equispaced but rather changes with curvature and radius along the centerline, increasing the frequency of sampling near high-curvature regions. This results in a better quality and more accurate mesh. Finally, Catmull-Clark subdivision algorithm [5] is used to increase mesh resolution and smooth out the surface."
VesselVAE: Recursive Variational Autoencoders for 3D Blood Vessel Synthesis,3,Experimental Setup,"Materials. We trained our networks using a subset of the open-access IntrA dataset1 published by Yang et al. in 2020 [32]. This subset consisted of 1694 healthy vessel segments reconstructed from 2D MRA images of patients. We converted 3D meshes into a binary tree representation and used the network extraction script from the VMTK toolkit2 to extract the centerline coordinates of each vessel model. The centerline points were determined based on the ratio between the sphere step and the local maximum radius, which was computed using the advancement ratio specified by the user. The radius of the blood vessel conduit at each centerline sample was determined using the computed crosssections assuming a maximal circular shape (See Fig. 2). To improve computational efficiency during recursive tree traversal, we implemented an algorithm that balances each tree by identifying a new root. We additionally trimmed trees to a depth of ten in our experiments. This decision reflects a balance between the computational demands of depth-first tree traversal in each training step and the complexity of the training meshes. We excluded from our study trees that exhibited greater depth, nodes with more than two children, or with loops. However, non-binary trees can be converted into binary trees and it is possible to train with deeper trees at the expense of higher computational costs. Ultimately, we were able to obtain 700 binary trees from the original meshes using this approach.Implementation Details. For the centerline extraction, we set the advancement ratio in the VMTK script to 1.05. The script can sometimes produce multiple cross-sections at centerline bifurcations. In those cases, we selected the sample with the lowest radius, which ensures proper alignment with the centerline principal direction. All attributes were normalized to a range of [0, 1]. For the mesh reconstruction we used 4 iterations of Catmull-Clark subdivision algorithm. The data pre-processing pipeline and network code were implemented in Python and PyTorch Framework.Training. In all stages, we set the batch size to 10 and used the ADAM optimizer with β 1 = 0.9, β 2 = 0.999, and a learning rate of 1 × 10 -4 . We set α = .3 and γ = .001 for Eq. 1 in our experiments. To enhance computation speed, we implemented dynamic batching [16], which groups together operations involving input trees of dissimilar shapes and different nodes within a single input graph. It takes approximately 12 h to train our models on a workstation equipped with an NVIDIA A100 GPU, 80 GB VRAM, and 256 GB RAM. However, the memory footprint during training is very small (≤1 GB) due to the use of a lightweight tree representation. This means that the amount of memory required to store and manipulate our training data structures is minimal. During training, we ensure that the reconstructed tree aligns with the original structure, rather than relying solely on the classifier's predictions. We train the classifier using a crossentropy loss that compares its predictions to the actual values from the original tree. Since the number of nodes in each class is unbalanced, we scale the weight given to each class in the cross-entropy loss using the inverse of each class count. During preliminary experiments, we observed that accurately classifying nodes closer to the tree root is critical. This is because a miss-classification of top nodes has a cascading effect on all subsequent nodes in the tree (i.e. skip reconstructing a branch). To account for this, we introduce a weighting scheme that for each node, assigns a weight to the cross-entropy loss based on the number of total child nodes. The weight is normalized by the total number of nodes in the tree.Metrics. We defined a set of metrics to evaluate our trained network's performance. By using these metrics, we can determine how well the generated 3D models of blood vessels match the original dataset distribution, as well as the diversity of the generated output. The chosen metrics have been widely used in the field of blood vessel 3D modeling, and have shown to provide reliable and accurate quantification of blood vessels main characteristics [3,13]. We analyzed tortuosity per branch, the vessel centerline total length, and the average radius of the tree. Tortuosity distance metric [4] is a widely used metric in the field of blood vessel analysis, mainly because of its clinical importance. It measures the amount of twistiness in each branch of the vessel. Vessel's total length and average radius were used in previous work to distinguish healthy vasculature from cancerous malformations. Finally, in order to measure the distance across distributions for each metric, we compute the cosine similarity."
VesselVAE: Recursive Variational Autoencoders for 3D Blood Vessel Synthesis,4,Results,"We conducted both quantitative and qualitative analyses to evaluate the model's performance. For the quantitative analyses, we implemented a set of metrics commonly used for characterizing blood vessels. We computed histograms of the radius, total length, and tortuosity for the real blood vessel set and the generated set (700 samples) in Fig. 3 (a). The distributions are aligned and consistent. We measured the closeness of histograms with the cosine similarity by projecting the distribution into a vector of n-dimensional space (n is the number of bins in the histogram). Since our points are positive, the results range from 0 to 1. We obtain a radius cosine similarity of .97, a total length cosine similarity of .95, and a tortuosity cosine similarity of .96. Results show high similarities between histograms demonstrating that generated blood vessels are realistic. Given the differences with the baselines generated topologies, for a fair comparison, we limited our evaluation to a visual inspection of the meshes.The qualitative analyses consisted of a visual evaluation of the reconstructed outputs provided by the decoder network. We visually compared them to stateof-the-art methods in Fig. 3 (b). The method described by Wolterink and colleagues [27] is able to generate realistic blood vessels but without branches, and the method described by Hamarneh et al. [9] is capable of generating branches with straight shapes, missing on realistic modeling. In contrast, our method is capable of generating realistic blood vessels containing branches, with smooth varying radius, lengths, and tortuosity."
VesselVAE: Recursive Variational Autoencoders for 3D Blood Vessel Synthesis,5,Conclusions,"We have presented a novel approach for synthesizing blood vessel models using a variational recursive autoencoder. Our method enables efficient encoding and decoding of binary tree structures, and produces high-quality synthesized models. In the future, we aim to explore combinations of our approach with representing surfaces by the zero level set in a differentiable implicit neural representation (INR) [1]. This could lead to more accurate and efficient modeling of blood vessels and potentially other non-tree-like structures such as capillary networks. Since the presented framework would require significant adaptations to accommodate such complex topologies, exploring this problem would certainly be an interesting direction for future research. Additionally, the generated geometries might show self-intersections. In the future, we would like to incorporate restrictions into the generative model to avoid such artifacts. Overall, we believe that our proposed approach holds great promise for advancing 3D blood vessel geometry synthesis and contributing to the development of new clinical tools for healthcare professionals."
VesselVAE: Recursive Variational Autoencoders for 3D Blood Vessel Synthesis,,Fig. 1 .,
VesselVAE: Recursive Variational Autoencoders for 3D Blood Vessel Synthesis,,Fig. 3 .,
VesselVAE: Recursive Variational Autoencoders for 3D Blood Vessel Synthesis,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0_7.
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,1,Introduction,"Existing out-of-distribution (OOD) detection methods work well when the indistribution (ID) classes have low heterogeneity (low variance) but fail when in-distribution classes have high heterogeneity [23] or high spatial similarity between ID and OOD classes [9]. Fetal ultrasound (US) anatomy detection is one such application where both the challenges co-exist.In this paper, we propose a Dual-Conditioned Diffusion Model (DCDM) to detect OOD samples when in-distribution data has high variance and test the performance by detecting heart views in fetal US videos as an example application. Specifically, an Ultrasound (US) typically comprises 13 anatomies and their views. However, analysis models are usually developed for anatomy-specific tasks. Hence, to separate heart views from other 12 anatomies (head, abdomen, femur etc.) we develop an OOD detection algorithm. Our in-distribution data comprises five structurally different heart views captured across different cardiac cycles of a beating heart during obstetric US scanning. We develop a diffusionbased model for reconstruction-based OOD detection, which extends [14] with a novel dual conditioning mechanism that alleviates the influence of high interand intra-class variation within different classes by leveraging in-distribution class conditioning (IDCC) and latent image feature conditioning (LIFC). These conditioning mechanisms allow our model to generate images similar to the input image for in-distribution data. The primary contributions of our paper are summarized as follows: 1) We introduce a novel conditioned diffusion model for OOD detection and demonstrate that the dual conditioning mechanism is effective in tackling challenging scenarios where in-distribution data comprises multiple heterogeneous classes and there is a high spatial similarity between ID and OOD classes. 2) Two original conditions are proposed for the diffusion model, which are in-distribution class conditioning (IDCC) and latent image feature conditioning (LIFC). IDCC is proposed to handle high inter-class variance within in-distribution classes and high spatial similarity between ID and OOD classes. LIFC is introduced to counter the intra-class variance within each class. 3) We demonstrate in our experiments that DCDM can detect and separate heart views from other anatomies in fetal ultrasound videos without needing any labelled data for OOD classes. Extensive experiments and ablations demonstrate superior performance over existing OOD detection methods. Our approach is not fetal ultrasound specific and could be applied to other OOD applications."
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,2,Related Work,"OOD detection [31] involves identifying samples that do not belong to the training distribution. Such models can be categorized into: (a) unsupervised OOD detection [23] and (b) supervised OOD detection. [5,11,34]. Unsupervised OOD detection methods can again be divided into two main categories: (i) likelihoodbased approaches [12,21,30], and (ii) reconstruction-based [3,25,33]. Likelihoodbased approaches suffer from several issues, including assigning higher likelihood to OOD samples [4,19], susceptibility to adversarial attacks [8], and calibration issues [28]. Current reconstruction-based approaches are sensitive to dimensions of the bottleneck layer and require rigorous tuning specific to the dataset and task [10]. Additionally, models trained using a generator-discriminator architecture and optimizing adversarial losses can be highly unstable and challenging to train [1,2]. Finally, reconstruction-based methods often rely on highly compressed latent representations, which can lead to loss of important low-level detail. This can be problematic when discriminating between classes with high spatial similarity. Recently, diffusion models have been introduced to address these limitations on tasks such as image synthesis [6], and OOD detection [10].Denoising Diffusion Probabilistic Models (DDPMs) [14] are generative models that work by gradually adding noise to an input image through a forward diffusion process followed by gradually removing noise using a trained neural network in the backward diffusion process [32]. To guide the generative process of a diffusion model (DM), previous work [18,22,24] condition the DDPMs on taskspecific conditioning. In image-to-image translation tasks like super-resolution, colourization, etc., previous papers [24] condition the model by concatenating a resized or grayscale version of the input image to the noised image. This concatenation is unsuitable for reconstruction-based OOD detection as the model will generate similar images for ID and OOD samples. In the context of OOD detection using DMs, previous works [10] have trained unconditional DDPMs and, during inference, sampled using a Pseudo Linear Multi Step (PLMS) [16] sampler for varying noise levels. However, their approach generates 5500 samples to detect OOD samples for each input image which is time-consuming and impractical for settings where shorter inference times are needed. AnoDDPM [29] utilises simplex noise rather than Gaussian noise to corrupt the image (t=250 rather than t=1000) for anomaly detection. However, this approach requires data specific tuning, and is outperformed by [10]. "
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,3,Methods,
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,3.1,Dual Conditioned Diffusion Models,"Diffusion models are generative models that rely on two Markov processes known as forward and backward diffusion [14]. To improve efficiency during training and inference, forward and backward diffusion is applied to the latent space [22]. Autoencoder (AE = E + D) is pretrained separately on ID heart data and can successfully reconstruct the input heart images (SSIM=0.956). The latent variable z 0 is obtained by passing an input image x 0 through a pretrained encoder E. Given the latent vector z 0 and a fixed variance schedule [14] {β t ∈ (0, 1)} T t=1 , the forward diffusion process, defined by Eq. 1, gradually adds Gaussian noise to z 0 to give a noised latent vector z t where α t = 1β t and ᾱt = t i=1 α i :In backward diffusion, we aim to reverse the forward diffusion process and predict z t-1 given z t . To predict (z t-1 |z t ), we train a denoising U-Net [14] denoted as θ (z t , t, d 0 ) that takes the current timestep t, noised latent vector z t and the dual conditioning embedding vector d 0 as input and predicts the noise at timestep t as shown in Eq. 2.The dual embedding vector d 0 is obtained by combining IDCC (f cls ) and LIFC (f img ) vectors, which we explain in Sect. 3.2. The output z t-1 is again input to θ . This process is repeated until z 0 is obtained. The final model optimisation objective is given by Eq. 3 where is the original noise added during the forward diffusion process.(3)Once we obtain z 0 from the backward diffusion process, it is passed on to the decoder D and mapped back to the pixel space to give generated image x 0 ."
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,3.2,Dual Conditioning Mechanism,Image features and in-distribution class information are utilised in our proposed dual conditioning mechanism. This guides the DCDM to generate images that are spatially and semantically similar to the input image for in-distribution samples and dissimilar for OOD samples.
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,,Latent Image Feature Conditioning (LIFC):,"The image conditioning dictates the desired appearance of generated images in terms of shape and texture.In our model, we use the features extracted by a pretrained encoder for conditioning. Empirically, we use the same encoder E as our feature extractor to obtain latent feature vector z 0 as shown in Fig. 1. Specifically, the input image of dimension 224 × 224 × 3 is passed through the encoder E and a feature map with the size of 7 × 7 × 128 is obtained which is followed by global average pooling (GAP) resulting in a feature vector (f img ) with dimension 128. "
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,,In,
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,,Cross Attention Guidance:,"To integrate the dual-conditioning guidance into the diffusion model, we use a cross-attention [27] mechanism inside the denoising U-Net rather than just concatenation [24] as it is more effective [13,17,20] and allows condition diffusion models on various input modalities [22]. Our LIFC and IDCC are first concatenated to give a feature vector with a dimension of 256. This acts as a side input to each UNet block. The features from the UNet block and the conditional features are fused by cross-attention and serve as input to the following UNet block as shown in Fig. 1. For more details,regarding crossattention block refer to Rombach et al. [22]."
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,3.3,In-Distribution Classifier,"The in-distribution classifier (CFR) serves two main functions. First, it provides labels for the class conditioning during inference; second, it is utilized as a feature extractor for calculating the OOD score.Inference Class Guidance. IDCC requires in-distribution class information to generate the class conditional embedding. However, class information is only available during training. To obtain class information during inference, we separately train a ConvNext CNN based classifier (accuracy = 88%) on the indistribution data and use its predictions as the class information. During inference, the input image x 0 is passed through the classifier, and the predicted label is used to generate the class embedding by feeding to the label encoder as shown in Fig. 1. Moreover, as the classifier is only trained on in-distribution data, it classifies an OOD sample to an in-distribution class. The classifier's prediction is utilised by the DCDM and it tries to generate an image belonging to in-distribution class for the OOD samples. This reduces the structural and semantic similarity between the input and the generated image, as demonstrated by our qualitative results (Fig. 2)."
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,,Feature-Based OOD Detection,"To evaluate the performance of the DCDM, the cosine similarity between features of the input image x 0 and the generated image x 0 from the in-distribution classifier is calculated and is referred as an OOD score where f 0 and f 0 are the features of x 0 and x 0 , respectively:An input image x 0 is classified as in-distribution (ID) or OOD based on Eq. 5 where τ is a pre-defined threshold and y pred is the prediction of our feature-based OOD detection algorithm."
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,4,Experiments and Results,"Dataset and Implementation. For our experiments, we utilized a fetal ultrasound dataset of 359 subject videos that were collected as part of the PULSE project [7]. The in-distribution dataset consisted of 5 standard heart views (3VT, 3VV, LVOT, RVOT, and 4CH), while the out-of-distribution dataset comprised of three non-heart anatomies -fetal head, abdomen, and femur. The original images were of size 1008 × 784 pixels and were resized to 224 × 224 pixels.To train the models, we randomly sampled 5000 fetal heart images and used 500 images for evaluating image generation performance. To test the performance of our final model and compare it with other methods, we used an held-out dataset of 7471 images, comprising 4309 images of different heart views and 3162 images (about 1000 for each anatomy) of out-of-distribution classes. Further details about the dataset are given in Supp. Fig. 2 and3.All models were trained using PyTorch version 1.12 with a Tesla V100 32 GB GPU. During training, we used T=1000 for noising the input image and a linearly increasing noise schedule that varied from 0.0015 to 0.0195. To generate samples from our trained model, we used DDIM [26] sampling with T=100. All baseline models were trained and evaluated using the original implementation."
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,4.1,Results,"We evaluated the performance of the dual-conditioned diffusion models (DCDMs) for OOD detection by comparing them with two current state-ofthe-art unsupervised reconstruction-based approaches and one likelihood-based approach. The first baseline is Deep-MCDD [15], a likelihood-based OOD detection method that proposes a Gaussian discriminant-based objective to learn class conditional distributions. The second baseline is ALOCC [23] a GAN-based model that uses the confidence of the discriminator on reconstructed samples to detect OOD samples. The third baseline is the method of Graham et al. [10], where they use DDPM [14] to generate multiple images at varying noise levels for each input. They then compute the MSE and LPIPS metrics for each image compared to the input, convert them to Z-scores, and finally average them to obtain the OOD score.Quantitative Results. The performance of the DCDM, along with comparisons with the other approaches, are shown in the input image and belonging to the same heart view for ID samples while structurally diverse heart views for OOD samples. In Fig. 2 (c) for OOD sample, even-though the confidence is high (0.68), the gap between ID and OOD classes is wide enough to separate the two.Additional qualitative results can be observed in Supp. Fig. 4."
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,4.2,Ablation Study,"Ablation experiments were performed to study the impact of various conditioning mechanisms on the model performance both qualitatively and quantitatively. When analyzed quantitatively, as shown in    separately, improves performance with an AUC of 75.27% and 77.40%, respectively. The best results are achieved when both mechanisms are used (DCDM), resulting in an 11% improvement in the AUC score relative to the unconditional model. Although there is a small margin of performance improvement between the combined model (DCDM) and the LIFC model in terms of AUC, the precision improves by 3%, demonstrating the combined model is more precise and hence the best model for OOD detection.As shown in Fig. 3, the unconditional diffusion model generates a random heart view for a given input for both in-distribution and OOD samples. The IDCC guides the model to generate a heart view according to the in-distribution classifier (CFR) prediction which leads to the generation of similar samples for in-distribution input while dissimilar samples for OOD input. On the other hand, LIFC generates an image with similar spatial information. However, heart views are still generated for OOD samples as the model was only trained on them. When dual-conditioning (DC) is used, the model generates images that are closer aligned to the input image for in-distribution input and high-fidelity heart views for OOD than those generated by a model conditioned on either IDCC or LIFC alone. Supp. Fig. 1 presents further qualitative ablations."
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,5,Conclusion,"We introduce novel dual-conditioned diffusion model for OOD detection in fetal ultrasound videos and demonstrate how the proposed dual-conditioning mechanisms can manipulate the generative space of a diffusion model. Specifically, we show how our dual-conditioning mechanism can tackle scenarios where the in-distribution data has high inter-(using IDCC) and intra-(using LIFC) class variations and guide a diffusion model to generate similar images to the input for in-distribution input and dissimilar images for OOD input images. Our approach does not require labelled data for OOD classes and is especially applicable to challenging scenarios where the in-distribution data comprises more than one class and there is high similarity between the in-distribution and OOD classes."
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,,Fig. 1 .,
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,,,
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,,Fig. 2 .,
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,,Fig. 3 .,
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,,Table 1 .,
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,,Table 1 .,
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,,Table 2,", the unconditional model has the lowest AUC of 69.61%. Incorporating the IDCC guidance or LIFC"
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,,Table 2 .,MethodAccuracy (%) Precision (%) AUC (%)
A Small-Sample Method with EEG Signals Based on Abductive Learning for Motor Imagery Decoding,1,Introduction,"Brain computer interface (BCI) technology plays an increasingly crucial role in the rehabilitation process of patients with nerve damage. However, the lack of large-scale labeled data makes the identification results more vulnerable to be affected. BCI technology with motor imagery (MI), which can decode neural activities through EEG signals to identify the movement intention of the human being, is widely applied in the field of EEG decoding, where the substantial problem of BCI decoding is to extract as much effective information as possible from the multi-channel and non-linear EEG signals for understanding the oscillating activities in the brain [1].According to the development of EEG signal processing technology, the traditional EEG signal feature extraction methods [2,3] rely on rich human experience or expert knowledge. Furthermore, the existing deep learning models for EEG motion decoding [4,5] rely on a large amount of labeled data. More recently, the active learning framework [6,7] based on semi-supervised learning is used to solve a feature extraction problem with a large amount of unlabeled data, but it is sensitive to the initial model accuracy and sampling strategy. Therefore, there is still an unsolved urgent challenge that needs to be considered further, which is how to make biologically reasonable use of the information contained in a large number of unlabeled EEG data on the above basis.The starting point of this paper is that the following biological findings can be observed in the MI decoding [8][9][10]: 1) the EEG rhythm energy in the contralateral motor sensory area (MSA) of the cerebral cortex is significantly reduced, while that in the ipsilateral MSA is increased, 2) different functional brain regions activities can be reflected by different sub-bands of EEG signals, and 3) the distribution density of different rhythms are clearly distinguished. And the rules are similar to the knowledge reasoning used in human decision-making. Then, the purpose of this paper is to employ these knowledge constraints to guide the model to extract MI information from unlabeled EEG data in a mutually beneficial way.Therefore, based on abductive learning (ABL) [11], this article proposes a smallsample EEG decoding method, which can adaptively extract abstract features from complex and dynamic EEG signals, and an efficient knowledge base is designed to constrain the training process of the model. The main contributions are listed as follows:1) A novel EEG decoding method is proposed to tackle the MIR (motion intention recognition) problem with small-sample EEG signals. This method does not rely on strict mathematical assumptions for datasets and its accuracy and robustness are well-maintained under strong interference. 2) A multi-scale feature fusion network is designed to enhance abstract features, which can capture temporal and frequency information across multi-channel EEG signals and spatial relationships among different electrodes. 3) An effective knowledge base module of motor imagery is constructed and symbolized, which can upgrade the model space under this constraint by mining the potential facts of large-scale unlabeled EEG signals."
A Small-Sample Method with EEG Signals Based on Abductive Learning for Motor Imagery Decoding,2,Problem Definition and Method,
A Small-Sample Method with EEG Signals Based on Abductive Learning for Motor Imagery Decoding,2.1,Problem Definition,"In the SSE-ABL framework for EEG decoding, the given input is defined as Input = {X l , X u , KB θ }, where tensor X l denotes labeled EEG data, tensor X u denotes unlabeled EEG data, the data size of X u is much larger than that of X l , and KB θ denotes knowledge base on brain science in MI task. Concretely, X l = {(x l1 , y l1 ), (x l2 , y l2 ), . . . , (x li , y li )}, where variable x li represents multi-channel EEG signals, and y li implies the corresponding labels. And the assignment of X l is to learn a mapping from x to y. X u = {x u1 , x u2 , . . . , x uj |i j}, where x uj denotes unlabeled multi-channel EEG signals, which are utilized to boost representative capability of the above mapping. KB θ consists of a series of first-order logic sentences with learnable parameters θ , which integrate labels EEG data X l and unlabeled data X u to optimize a perceptual model and train parameters θ with the constraint of knowledge base. The final result is regarded as Output = {f , W, θ }, where f is a mapping from EEG signals to motion intention, W ∈ R m×n indicates the proportion of the m th sub-band to the n th channel in EEG data, θ ∈ R k represents the contribution rate of the k th channel to the whole in MIR. The SSE-ABL algorithm yields the corresponding pseudo-labels to the unlabeled data by the classifier optimized by a small amount of labeled EEG signals, and the produced labels may be incorrect due to the small number of training samples, which is difficult to guarantee good performance. Therefore, the SSE-ABL modifies the pseudo-labels and optimizes the internal parameters of the knowledge base at the same time, so that the consistency of them is maximized under the constraint of the knowledge base. Formally, the problem definition of the SSE-ABL can be summarized as an optimization problem of searching Output under a given Input:where y uj is the pseudo-label corresponding to the j th unlabeled instance, which is generated by the perceptual module. δ(•) indicates a heuristic function obtained by optimization, which aims to revise pseudo-labels by logical abduction process. In addition to correcting inconsistent pseudo-labels, this goal also helps the knowledge base to learn accurate parameter θ . It can be seen from Eq. 1 and Eq. 2 that the major challenge is how to mine the effective information of massive unlabeled EEG data under the KB θ constraints and react to the iterative update of itself."
A Small-Sample Method with EEG Signals Based on Abductive Learning for Motor Imagery Decoding,2.2,Architecture of the SSE-ABL Framework,"The proposed method, as shown in Fig. 1, consists of four phases. A workflow outline of the proposed method in this paper is displayed in Algorithm 1.1) Phase 1 (Sample Representation): For the purpose of promoting the ability to describe the local and global details of the brain activities, a time-frequency-space data representation method based on brain region division is proposed, which makes the receptive field of view cover the whole brain region in a fine-grained way. 2) Phase 2 (Multiscale Feature Fusion): Inspired by the neuroscience findings shown in Sect. 1, a multi-scale feature fusion model is proposed to adaptively integrate time-frequency-space information of EEG signals aiming at reducing the potential difference of feature distribution and selectively focusing on the MI-related materials. 3) Phase 3 (Motion Intention Estimation): Based on the common laws of EEG signals distribution, a motor intention evaluation model is constructed, in which the rules are used as the supervisory information to judge the authenticity of motor imagery recognition, so as to ensure that the overall process conforms to the actual criteria. 4) Phase 4 (Abductive Reasoning Optimization): When the classifier is insufficienttrained, the pseudo-labels could be wrong, the SSE-ABL method in this paper needs to correct the wrong pseudo-labels to achieve consistent abductions by using gradient free optimization method under the principle of minimal inconsistency [12,13]. "
A Small-Sample Method with EEG Signals Based on Abductive Learning for Motor Imagery Decoding,2.3,Sample Representation,"The preprocessed EEG signals are first divided into five bands containing delta (1-4 Hz), theta (4-8 Hz), alpha (8-14 Hz), beta (14-31 Hz) and gamma (31-50 Hz) wavebands by using digital band-pass filters whose corresponding cut-off frequency is performed according to the division standard in [1], and the segmentation process is completed by utilizing the local time window method (0.5s). Then, the comprehensive information of these rhythm signals about brain activities is obtained by continuous wavelet transform method (CWT) [14], and the time-frequency expression (tf t ∈ R N ×C×H ×W ) of the EEG data is obtained by convolution layer, normalization layer, nonlinear activation function and max pooling layer operations (defined asConv(•)), as described in Eq. 3.whererepresents the decomposition of the signal f (i) (t) into j sub-bands. The inner product process is the principle of CWT, Cat (x,y) implies that the y th tensor data is concatenated according to the x th dimension. The procedure of fusing time-frequency information directly in multiple frequency bands may ignore the spatial distribution of the electrode. Therefore, multi-channel EEG signals are executed across the channel direction through the channel-by-channel convolutional operations, which aims at capturing spatial dynamic correlation characteristics (S t ∈ R N ×C×H ×W ) among brain regions, as shown in Eq. 4.where g(•) implies 1-D convolutional operation, tensor W and b denotes convolution weights and bias, Append (•) means adding each element in turn. With the above configurations, the mixed sample (MS t ∈ R N ×2C×H ×W ), which is composed of tf t andS t , are represented as 4-D tensor, where (C, H , W ) is the number of channels and the resolution of the feature map respectively."
A Small-Sample Method with EEG Signals Based on Abductive Learning for Motor Imagery Decoding,2.4,Multiscale Feature Fusion,"Given the mixed samples, the multiscale feature fusion model embeds them into feature vectors and extracts the time-frequency-space features of multi-channel EEG signals. We introduce the embedding layers in phase 2 to better describe the semantic and location information of EEG and improve the EEG transformer encoder to pay attention to seizing the global and local information of long-term EEG signals. Concretely, the normalized operation, the position mark of multiple MS t and the order inside it are added to attain the input of the network by conducting token, segment and position embedding steps, respectively, as showed in Eq. 5.Note that the addition here is bitwise addition. Tok, Seg and Pos correspond to the above three operations. Then tensor-patches in MS t are vectorized and carried to the network to calculate temporal and spatial self-attention block described in [15], as showed in Algorithm 2."
A Small-Sample Method with EEG Signals Based on Abductive Learning for Motor Imagery Decoding,2.5,Motion Intention Estimation,"We construct MI classification module based on attention mechanism and knowledge rules, which consists of two parts: domain knowledge expressed by first-order logic formula and learnable parameters weights, as shown in Fig. 2. "
A Small-Sample Method with EEG Signals Based on Abductive Learning for Motor Imagery Decoding,3,Results,"We adopt the 2008 BCI competition IV-2a EEG dataset (https://www.bbci.de/competiti on/iv/) including 9 subjects with a 250 Hz sampling rate and band-pass filtered from 0.5 to 100 Hz, which consists of four MI tasks: imagine left hand (class 1), right hand (class 2), foot (class 3) and tongue (class 4) movements. And the preprocessing operations in this article refers to eliminating the wrong experiments and using digital band-pass filters of 1-50 Hz. In the experiments, all methods with self-teaching plan are set to the same number of modules using the default hyperparameters in the source code.The following experimental tests of the proposed method (SSE-ABL) are designed to conduct: 1) Compared with the current mainstream algorithms containing transformer (TF) [16], BERT [17] and MEET [15] models to verify the performance of the SSE-ABL. 2) By changing proportion of training dataset size to verify the robustness of the SSE-ABL under the interference of different small-sample data, that is, the extrapolation ability.3) The visualization of the underlying information flow of EEG decoding by the proposed method is displayed. As shown in Table 1 are the accuracy results of experiments 1 and 2, which consists of three parts in the case of 10%, 50% and100% labeled data as the training set. And there are 9 subjects and average accuracy in the horizontal direction. It can be seen from the first and second part that SSE-ABL has obvious advantages in the individual accuracy and overall average accuracy of different subjects, up to 89.19%, 80.03% and 92.8 2%, 81.53% respectively. By comparing the first two parts, can be found that when the labeling rate is low, the benefits of using both unlabeled data and MI knowledge are much higher during EEG decoding. Further comparison with the third part shows that the performance can even be competitive the methods with the 100% training set.Then, the visualization results of experiment 3 is exhibited in Fig. 3,which is the analysis of FC 1 , FC 3 , CP 1 , CP 3 and FC 2 , FC 4 , CP 2 , CP 4 channels (corresponding to the left and right brain regions respectively) when imagining a left-handed task. And there are also three parts: the time domain EEG signal (P1), the time-frequency analysis (P2) and visualization analysis of the SSE-ABL (P3). From the time domain perspective shown in the P1, it seems that there is not much discrimination that can separate specific motor imagery tasks. However, it can be found in the P2 that the energy distribution on the left and right sides of the brain can be clearly found by the data representation method proposed in this paper, which can be distinguished by the brightness of the color. At the same time, there is also a phenomenon that the energy of the right brain region is lower than that of the left brain region, which is consistent with biological cognition. Moreover, it can be observed in the P3 that the high energy component of EEG signals is surrounded by a large amount of dark red, which is the information that the SS-ABL algorithm focuses on. And the contribution rates of the above channels to this recognition task are marked under the channels, respectively, which gives us an intuitive understanding of the EEG signal decoding process. "
A Small-Sample Method with EEG Signals Based on Abductive Learning for Motor Imagery Decoding,4,Conclusion,"A novel small-sample method with EEG signals based on abductive reasoning is studied for MI recognition. This method solves the problem of low precision and poor robustness ability of EEG decoding under a small amount of labeled data. The multiscale feature fusion module based on self-attention mechanism improves the ability of adaptive feature mining by capturing time-frequency-space information cover the whole brain region, which realizes the enhancement of abstract features. An effective knowledge base module of motor imagery is constructed and symbolized, which can upgrade the model space under this constraint by mining the potential facts of large-scale unlabeled EEG signals.Through the comparison experiments with other mainstream inversion methods, it can be founded that our method with 10% and 50% labeled EEG data can reach the accuracy of 80.03% and 81.53%, achieving the high precision standard of EEG decoding. In the future, we will consider a fine-grained visualization method of EEG signals in the case of partial data damage, such as signal coupling."
A Small-Sample Method with EEG Signals Based on Abductive Learning for Motor Imagery Decoding,,Fig. 1 .,
A Small-Sample Method with EEG Signals Based on Abductive Learning for Motor Imagery Decoding,,Fig. 2 .,
A Small-Sample Method with EEG Signals Based on Abductive Learning for Motor Imagery Decoding,,Fig. 3 .,
A Small-Sample Method with EEG Signals Based on Abductive Learning for Motor Imagery Decoding,,Table 1 .,
A Small-Sample Method with EEG Signals Based on Abductive Learning for Motor Imagery Decoding,,-10 79.22 76.01 78.90 78.43 72.44 78.66 89.19 88.05 79.40 80.03,
A Small-Sample Method with EEG Signals Based on Abductive Learning for Motor Imagery Decoding,,50 80.78 76.68 79.34 78.91 73.62 79.95 90.56 92.82 81.07 81.53,
Weakly Supervised Lesion Localization of Nascent Geographic Atrophy in Age-Related Macular Degeneration,1,Introduction,"Nascent geographic atrophy (nGA), originally described by Wu et al. [1], describes features of photoreceptor degeneration seen on optical coherence tomography (OCT) imaging that are strongly associated with the development of geographic atrophy (GA), a late stage complication of age-related macular degeneration (AMD). A recent study reported that the development of nGA in individuals with intermediate AMD was associated with a 78-fold increased rate of GA development [2]. Thus, nGA could potentially act as an earlier biomarker of AMD progression, or potentially as an earlier endpoint in intervention studies aiming to slow GA development [3]. Thus being able to easily identify eyes with nGA and localize nGA lesions is important in clinical trials and research.However, identifying and grading the location of nGA lesions in OCT volume scans can be a laborious task, and would be an operationally expensive undertaking in clinical trials. Automation of this task would be invaluable when seeking to quantify the number of nGA lesions present, or when seeking to identify a smaller subset of B-scans for manual expert review (an ""AI-assisted"" approach). While the localization of nGA could be tackled by supervised object detection models -as demonstrated in other types of lesions [4][5][6] -it takes domain experts a large amount of time to provide sufficient number of lesion level annotations (e.g. with a bounding box). On the other hand, weakly supervised methods require only coarse annotations, and they have been popular in computer vision tasks where dense annotations are difficult to obtain [7].In this work, we sought to develop a deep learning-based method to automate the localization of nGA lesions on OCT imaging, trained only on the information about the presence or absence of nGA at the volume level. A weakly supervised algorithm was developed that utilizes the saliency maps from Gradient Class Activation Maps (GradCAM) technique [8]. While existing literature has demonstrated the ability of the GradCAM in post-hoc model interpretation [9][10][11][12], it is unknown whether the saliency map can help further localize the class-related lesions or abnormalities that are often sparse anatomically. We thus explored the possibility of GradCAM in identifying the location of nGA-related abnormalities after training a model for classifying nGA in a 3D volume scan."
Weakly Supervised Lesion Localization of Nascent Geographic Atrophy in Age-Related Macular Degeneration,2,Methods,
Weakly Supervised Lesion Localization of Nascent Geographic Atrophy in Age-Related Macular Degeneration,2.1,Dataset,"This study included participants in the sham treatment arm of the Laser Intervention in the Early Stages of AMD study (LEAD, clinicaltrials.gov identifier, NCT01790802). The LEAD study was conducted according to the International Conference on Harmonization Guidelines for Good Clinical Practice and the tenets of the Declaration of Helsinki. Institutional review board approval was obtained at all sites and all participants provided written informed consent.The participants of LEAD study were required to have bilateral large drusen and a best-corrected visual acuity of 20/40 or better in both eyes at baseline [13]. Participants were evaluated at the baseline and every 6-month follow up visits for up to 36-months. At each visit, OCT imaging was performed following pupillary dilation, by obtaining a 3D volume scan consisting of 49 B-scans (i.e. 2D slices along X-Z direction) covering a 20°× 20°× 1.9 mm region of the macula, with 1024 × 49 × 496 voxels anisotropically sampled along X, Y and Z d directions respectively.Multimodal imaging was used to assess the development of late AMD as an endpoint in the LEAD study, which included nGA detected on OCT imaging. In order to evaluate the association between nGA and the subsequent development of GA as detected on color fundus photographs (CFP; the historical gold standard for atrophic AMD) in a previous study, OCT imaging and CFP were independently re-graded for the presence of nGA and GA respectively [14]. In this sub-study, we included individuals who did not have nGA at baseline based on the above independent re-grading of OCT imaging, and who had at least one follow-up visit.A total of 1,884 OCT volumes from 280 eyes of 140 individuals were included in this analysis (1,910 volumes were collected, but 26 volumes were excluded from the study due to the development of neovascular AMD in the eye). In this study, the development of nGA was assessed by manual grading of all 49 B-scans of each OCT volume scans, and nGA was defined by the subsidence of the outer plexiform layer and inner nuclear layer, and/or the presence of a hyporeflective wedge-shaped band within Henle's fiber layer, as per the original definition [1]. All OCT volume scans were initially assessed by a senior grader, and all visits of any eye deemed to have questionable or definite nGA were then reviewed by two further experienced graders [2].Overall, nGA was graded as being absent and present in 1,766 and 118 OCT volume scans respectively. In the context of this study, note that nGA also includes lesions that could also meet the criteria for having complete retinal pigment epithelium and outer retinal atrophy (cRORA), if the lesion also had choroidal signal hypertransmission and retinal pigment epithelium (RPE) attenuation or disruption of ≥250 µm [15]. Graders also concurrently graded the location of nGA lesions by identifying the B-scans with nGA lesions and by drawing bounding boxes on the B-scans horizontally covering the subsidence, vertically from the inner limiting membrane (ILM) to Bruch's membrane. For the weakly supervised model, the bounding boxes were used only in evaluating the weakly supervised lesion localization, not in model training. The bounding boxes were then used to train a fully supervised object detector to compare the results of the weakly supervised and fully supervised methods."
Weakly Supervised Lesion Localization of Nascent Geographic Atrophy in Age-Related Macular Degeneration,2.2,Deep Learning Architecture,"A late-fusion model with a 2D ResNet backbone was developed to classify 3D OCT volumes, considering their anisotropic nature. As shown in Fig. 1a,B-scans from a 3D OCT volume were fed into a B-scan detector, and the outputs, which are vectors of classification logits for each B-scan, were averaged to generate prediction scores for the volume. Thinking of the B-scans as instances and the OCT volumes as bags, this framework can be categorized as simplified multi-instance learning [16] in which the network was trained on weakly labeled data, using labels on bags (OCT volumes) only. During the training process, given an OCT volume annotated as nGA, the network was forced to identify as many B-scans with nGA lesion to improve the final prediction of nGA, thus the trained model allows prediction of nGA labels on OCT volumes as well as on individual B-scans.The details of the B-scan classifier are shown in Fig. 1b. An individual B-scan of size 1024 × 496 from the volume is downsampled to 512 × 496 and passed through the ResNet-18 backbone, which outputs activation maps of 512 × 16 × 16. A maxpooling layer and an average pooling layer are concatenated to generate a feature vector of 1024. Then a fully connected layer was applied to generate the classification logit for the B-scan.The classification model was evaluated on its own both in terms of volume-wise and slice-wise performance in classifying nGA. After it was confirmed that the classification model worked well, the ability of the model to localize the lesions within individual OCT slices was evaluated.Given an OCT volume and a trained model, saliency maps were generated with the GradCAM technique [8] to visualize regions making larger contributions to the final classification. In Fig. 2a, GradCAM output was overlaid as the yellow channel on the input images for easy visualization of the saliency as well as the original grayscale image. The saliency map from a legitimate model should highlight nGA lesions, thus GradCAM output can help localize nGA lesions. The objective was to localize nGA lesions in the 3D OCT volume, i.e. to identify which B-scans have nGA lesions and to generate a bounding box surrounding the lesions in those B-scans with confidence scores. As illustrated in Fig. 2, the automated image processing pipeline was built upon adaptive thresholding and connected component analysis [17]. For each B-scan with positive logit, one or multiple bounding boxes covering potential lesions were detected. The confidence score for each bounding box was estimated from the individual classification logit of the B-scan classifier as Eq. ( 1)."
Weakly Supervised Lesion Localization of Nascent Geographic Atrophy in Age-Related Macular Degeneration,,S l n h,"where S is the sigmoid function, l is the individual B-scan classification logit, and n is the number of B-scans in a volume, h is the mean saliency in the detected region and Σh is the total mean saliency of all detected regions within the B-scan. A higher confidence score implies a higher possibility that the detected region covers nGA lesions. Since only class labels of 3D OCT volume are required for training, the proposed lesion localization algorithm was weakly supervised. "
Weakly Supervised Lesion Localization of Nascent Geographic Atrophy in Age-Related Macular Degeneration,2.3,"Model Training, Tuning, and Validation Test","Considering the relatively small number of participants in the dataset, a five-fold crossvalidation was applied to evaluate the proposed method's performance. We followed the nomenclature for data splitting as recommended previously [18]. For each fold, the validation test set of OCT volumes were obtained from roughly 20% of the participants stratified on whether the individual developed nGA. The OCT volumes from the remaining 80% individuals were further split into training (64%) and tuning sets (16%), with volumes from one individual only existing in one of the sets. With the proposed data split strategy, the corresponding validation test set was not used in the training or hyperparameter tuning process.Pre-processing was performed on B-scans for standardization. The B-scans were first resized to 512 × 496, followed by rescaling the intensity range to [0, 1]. Data augmentation, including rotation of small angles, horizontal flips, add on of Gaussian noises, and Gaussian blur were randomly applied to improve the model's invariance to those transformations.A Resnet-18 backbone pre-trained on the ImageNet dataset was used. During the model training, the Adam optimizer was used to minimize focal loss. The L2 weight decay regularization was used to improve the model's generalization.As a benchmark for the weakly supervised lesion localization, a fully supervised YOLOv3 object detector [19] with a Resnet-18 backbone was trained using the bounding box information for each B-scan.A successful lesion localization was recorded only if the bounding box output overlapped with the bounding boxes annotated by clinicians with an intersection over union (IoU) value of at least 0.05. The area under the Precision-Recall curve (AUPRC) was calculated to evaluate the model performance. In patient screening, a high recall is preferred over precision. Considering the difference of the two methods, different strategies were used to determine the confidence threshold in calculating the precision and recall values in the validation test dataset. For the weakly supervised method, the threshold for confidence score that would achieve a recall value of 0.98 for nGA volume classification in the training and tuning sets is used. For the supervised method, the confidence threshold which would achieve a recall value of 0.9 for bounding box detection in the turning set is used."
Weakly Supervised Lesion Localization of Nascent Geographic Atrophy in Age-Related Macular Degeneration,3,Results,
Weakly Supervised Lesion Localization of Nascent Geographic Atrophy in Age-Related Macular Degeneration,3.1,Performance and Saliency Map Analysis of the nGA Classification Model on OCT Volumes,"The deep learning based nGA classification model achieved an AUPRC of 0.83(±0.09) in classifying 3D OCT volumes. Based on the trained 3D OCT volume classification model and input OCT volumes, we generated the corresponding saliency map using GradCAM technique. Examples of GradCAM output are shown in Fig. 2. Values in the GradCAM output indicate the importance of the corresponding pixel in the input Bscan to the model's prediction. A higher (brighter) value means the corresponding pixel contributes more to the model's prediction that the input OCT volume is positive. A thresholding was applied to the pixel values to determine the region where the bounding box delineating nGA should be drawn."
Weakly Supervised Lesion Localization of Nascent Geographic Atrophy in Age-Related Macular Degeneration,3.2,Performance of the Weakly Supervised Localization of nGA Lesions,"The weakly supervised algorithm achieved a similar level of performance for localizing nGA when compared to the YOLOv3 based fully supervised method, without utilizing bounding box annotations and these findings are illustrated in Fig. 3. The YOLOv3 based method achieved an AUPRC of 0.77(±0.07) compared to 0.72(±0.08) for the weakly supervised model; no statistically significant difference was observed between the two methods (Wilcoxon signed-rank test for AUPRC, p = 1.0). In the patient screening setting described previously, the YOLOv3 based method achieved a precision and recall of 0.53(±0.16) and 0.88(±0.06), compared to 0.39(±0.13) and 0.88(±0.02) for the weakly supervised method. "
Weakly Supervised Lesion Localization of Nascent Geographic Atrophy in Age-Related Macular Degeneration,4,Conclusion and Discussion,"This study demonstrates that the performance for localizing nGA lesions by only using OCT volume-wise classification labels with the GradCAM technique was on par with a fully supervised approach using B-scan level annotations with the YOLOv3 detector. These findings therefore underscore the potential of a weakly supervised approach for enabling the development of a robust model for lesion localization without the need for laborious, lesion-level annotations on OCT B-scans.One limitation of the GradCAM-based lesion localization is its relatively large bounding box size, often exceeding the annotated region. This is expected, considering the low spatial resolution of GradCAM saliency map, but also potentially because this approach identified contextual features that are distinguishing of nGA lesions that were not annotated by the graders. In addition, the weakly supervised model uses adaptive threshold of the saliency to determine the bounding box size, which was not optimized to match the ground truth grading. This limitation with the larger bounding box size could impact the quantification of the number of nGA lesions present, but it would unlikely have a substantial impact on the task of identifying a subset of OCT B-scans requiring manual review in an AI-assisted evaluation.In conclusion, this study demonstrates that a weakly supervised method, requiring only volume-wise tags, can achieve a similar level of performance for localizing lesions compared to a fully supervised method using slice-wise bounding box labels. A weakly supervised approach could thus minimize the labeling burden when seeking to develop a lesion localization model, and could even leverage existing volume-wise labels for its development."
Weakly Supervised Lesion Localization of Nascent Geographic Atrophy in Age-Related Macular Degeneration,,Fig. 1 .,
Weakly Supervised Lesion Localization of Nascent Geographic Atrophy in Age-Related Macular Degeneration,,Fig. 2 .,
Weakly Supervised Lesion Localization of Nascent Geographic Atrophy in Age-Related Macular Degeneration,,Fig. 3 .,
MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging,1,Introduction,"Transfer learning has become a standard practice in medical image analysis as collecting and annotating data in clinical scenarios can be costly. The pre-trained parameters endow better generalization to DNNs than the models trained from scratch [8,23]. A popular approach to enhancing model transferability is by pretraining on domains similar to the targets [9,21,[27][28][29]. However, utilizing specialized pre-training for all medical applications becomes impractical due to the Fig. 1. The motivation of MetaLR. Previous works fix transferable layers in pre-trained models to prevent them from catastrophic forgetting. It is inflexible and labor-expensive for this method to find the optimal scheme. MetaLR uses meta-learning to automatically optimize layer-wise LR for fine-tuning.diversity between domains and tasks and privacy concerns related to pre-training data. Consequently, recent work [2,6,14,22] has focused on improving the generalization capabilities of existing pre-trained DNN backbones through fine-tuning techniques.Previous studies have shown that the transferability of lower layers is often higher than higher layers that are near the model output [26]. Layer-wise finetuning [23], was thus introduced to preserve the transferable low-level knowledge by fixing lower layers. But recent studies [7] revealed that the lower layers may also be sensitive to small domains like medical images. Given the two issues, transferability for medical tasks becomes more complicated [24,25]. It can even be irregular among layers for medical domains far from pre-training data [7]. Given the diverse medical domains and model architectures, there is currently no universal guideline to follow to determine whether a particular layer should be retrained for a given target domain.To search for optimal layer combinations for fine-tuning, manually selecting transferable layers [2,23] can be a solution, but it requires a significant amount of human labor and computational cost. In order to address this issue and improve the flexibility of fine-tuning strategies, we propose controlling the fine-tuning process with layer-wise learning rates (LRs), rather than simply manually fixing or updating the layers (see Fig. 1). Our proposed algorithm, Meta Learning Rate (MetaLR), is based on meta-learning [13] and adaptively adjusts LRs for each layer according to transfer feedback. It treats the layer-wise LRs as metaknowledge and optimizes them to improve the model generalization. Larger LRs indicate less transferability of corresponding layers and require more updating, while smaller LRs preserve transferable knowledge in the layers. Inspired by [20], we use an online adaptation strategy of LRs with a time complexity of O(n), instead of the computationally-expensive bi-level O(n 2 ) meta-learning. We also enhance the algorithm's performance and stability with a proportional hyper-LR (LR for LR) and a validation scheme on training data batches.In summary, this work makes the following three contributions. 1) We introduce MetaLR, a meta-learning-based LR tuner that can adaptively adjust layerwise LRs based on transfer learning feedback from various medical domains.2) We enhance MetaLR with a proportional hyper-LR and a validation scheme using batched training data to improve the algorithm's stability and efficacy. 3) Extensive experiments on both lesion detection and tumor segmentation tasks were conducted to demonstrate the superior efficiency and performance of Met-aLR compared to current SOTA medical fine-tuning techniques."
MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging,2,Method,"This section provides a detailed description of the proposed MetaLR. It is a meta-learning-based [13,18] approach that determines the appropriate LR for each layer based on its transfer feedback. It is important to note that fixing transferable layers is a special case of this method, where fixed layers always have zero LRs. First, we present the theoretical formulation of MetaLR. Next, we discuss online adaptation for efficiently determining optimal LRs. Finally, we demonstrate the use of a proportional hyper-LR and a validation scheme with batched training data to enhance performance."
MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging,2.1,Formulation of Meta Learning Rate,"Let (x, y) denote a sample-label pair, and {(x i , y i ) | i = 1, ..., N } be the training data. The validation dataset {(x v i , y v i ) | i = 1, ..., M } is assumed to be independent and identically distributed as the training dataset. Let ŷ = Φ(x, θ) be the prediction for sample x from deep model Φ with parameters θ. In standard training of DNNs, the aim is to minimize the expected risk for the training set:  Based on the generalization, one can tune the hyper-parameters of the training process to improve the model. The key idea of MetaLR is considering the layer-wise LRs as self-adaptive hyper-parameters during the training and automatically adjusting them to achieve better model generalization. We denote the LR and model parameters for the layer j at the iteration t as α t j and θ t j . The LR scheduling scheme α = {α t j | j = 1, ..., d; t = 1, ..., T } is what MetaLR wants to learn, affecting which local optimal θ * (α) the model parameters θ t = {θ t j | j = 1, ..., d} will converge to. The optimal parameters θ * (α) are given by optimization on the training data. At the same time, the best LR tuning scheme α * can be optimized based on the feedback for θ * (α) from"
MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging,,Algorithm 1. Online Meta Learning Rate Algorithm,
MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging,,Input:,"Training data D, validation data D v , initial model parameter {θ 0 1 , ..., θ 0 d }, LRs {α 0 1 , ..., α 0 d }, batch size n, max iteration T; Output:Final model parameterStep forward for one step to get { θt 1 (α  4); 7: end for the validation loss. This problem can be formulated as the following bi-level optimization problem:MetaLR aims to use the validation set to optimize α through an automatic process rather than a manual one. The optimal scheme α * can be found by a nested optimization [13], but it is too computationally expensive in practice. A faster and more lightweight method is needed to make it practical."
MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging,2.2,Online Learning Rate Adaptation,"Inspired by the online approximation [20], we propose efficiently adapting the LRs and model parameters online. The motivation of the online LR adaptation is updating the model parameters θ t and LRs {α t j | j = 1, 2, ..., d} within the same loop. We first inspect the descent direction of parameters θ t j on the training loss landscape and adjust the α t j based on the transfer feedback. Positive feedback (lower validation loss) means the LRs are encouraged to increase.We adopt Stochastic Gradient Descent (SGD) as the optimizer to conduct the meta-learning. The whole training process is summarized in Algorithm 1. At the iteration t of training, a training data batch {(x i , y i ) | i = 1, ..., n} and a validation data batch {(x v i , y v i ) | i = 1, ..., n} are sampled, where n is the size of the batches. First, the parameters of each layer are updated once with the current LR according to the descent direction on training batch.This step of updating aims to get feedback for LR of each layer. After taking derivative of the validation loss w.r.t. α t j , we can utilize the gradient to know how the LR for each layer should be adjusted. So the second step of MetaLR is to move the LRs along the meta-objective gradient on the validation data:where η is the hyper-LR. Finally, the updated LRs can be employed to optimize the model parameters through gradient descent truly.For practical use, we constrain the LR for each layer to be α t j ∈ [10 -6 , 10 -2 ]. Online MetaLR optimizes the layer-wise LRs as well as the training objective on a single task, which differentiates it from traditional meta-learning algorithms [12,19] that train models on multiple small tasks."
MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging,2.3,Proportional Hyper Learning Rate,"In practice, LRs are often tuned in an exponential style (e.g., 1e-3, 3e-3, 1e-2) and are always positive values. However, if a constant hyper-LR is used, it will linearly update its corresponding LR regardless of numerical constraints. This can lead to fluctuations in the LR or even the risk of the LR becoming smaller than 0 and being truncated. To address this issue, we propose using a proportional hyper-LR η = β × α t j , where β is a pre-defined hyper-parameter. This allows us to rewrite Eq. (3) as:The exponential update of α t j guarantees its numerical stability."
MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging,2.4,Generalizability Validation on Training Data Batch,"One limitation of MetaLR is that the LRs are updated using separate validation data, which reduces the amount of data available for the training process. This can be particularly problematic for medical transfer learning, where the amount of downstream data has already been limited. In Eq. 2 and Eq. 3, the update of model parameter θ t j and LR α t j is performed using different datasets to ensure that the updated θ t j can be evaluated for generalization without being influenced by the seen data. As an alternative, but weaker, approach, we explore using another batch of training data for Eq. 3 to evaluate generalization. Since this batch was not used in the update of Eq. 2, it may still perform well for validation in meta-learning. The effect of this approach is verified in Sect. 3.2, and the differences between the two methods are analyzed in Sect. 3.4."
MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging,3,Experiments and Analysis,
MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging,3.1,Experimental Settings,"We extensively evaluate MetaLR on four transfer learning tasks (as shown in Table 1). To ensure the reproducibility of the results, all pre-trained models (USCL [9], ImageNet [11], C2L [28], Models Genesis [29]) and target datasets (POCUS [5], BUSI [1], Chest X-ray [17], LiTS [4]) are publicly available. In our work, we consider models pre-trained on both natural and medical image datasets, with three target modalities and three target organs, which makes our experimental results more credible. For the lesion detection tasks, we used ResNet-18 [15] with the Adam optimizer. The initial learning rate (LR) and hyper-LR coefficient β are set to 10 -3 and 0.1, respectively. In addition, we use 25% of the training set as the validation set for meta-learning. For the segmentation task, we use 3D U-Net [10] with the SGD optimizer. The initial LR and hyper-LR coefficient β are set to 10 -2 and 3 × 10 -3 , respectively. The validation set for the LiTS segmentation dataset comprises 23 samples from the training set of size 111. All experiments are implemented using PyTorch 1.10 on an Nvidia RTX A6000 GPU. We report the mean values and standard deviations for each experiment with five different random seeds. For more detailed information on the models and hyper-parameters, please refer to our supplementary material. ImageNet [11] supervised BUSI [1] Breast US Tumor detection 780 images MIMIC-CXR [16] C2L [28] Chest X-ray [17] Lung X-ray Pneumonia detection 5856 images LIDC-IDRI [3] Models Genesis [29] LiTS [4] Liver CT Liver segmentation 131 volumes"
MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging,3.2,Ablation Study,"In order to evaluate the effectiveness of our proposed method, we conduct an ablation study w.r.t. the basic MetaLR algorithm, the proportional hyper-LR, and batched-training-data validation (as shown in Table 2). When applying only the basic MetaLR, we observe only marginal performance improvements for the four downstream tasks. We conjecture that this is due to two reasons: Firstly, the constant hyper-LR makes the training procedures less stable than direct training, which is evident from the larger standard deviation of performance. Secondly, part of the training data are split for validation, which can be detrimental to the performance. After applying the proportional hyper-LR, significant improvements are in both the performance and its stability. Moreover, although the generalization validation on the training data batch may introduce bias, providing sufficient training data ultimately benefits the performance.  "
MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging,3.3,Comparative Experiments,"In our study, we compare MetaLR with several other fine-tuning schemes, including tuning only the last layer / all layers with constant LRs, layer-wise finetuning [23], bi-directional fine-tuning [7], and AutoLR [22]. The U-Net finetuning scheme proposed by Amiri et al. [2] was also evaluated."
MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging,,Results on Lesion Detection Tasks.,"MetaLR consistently shows the best performance on all downstream tasks (Table 3). It shows 1%-2.3% accuracy improvements compared to direct training (i.e., tuning all layers) because it takes into account the different transferabilities of different layers. While manual picking methods, such as layer-wise and bi-directional fine-tuning, also achieve higher performance, they require much more training time (5×-50×) for searching the best tuning scheme. On the other hand, AutoLR is efficient, but its strong hypothesis harms its performance sometimes. In contrast, MetaLR makes no hypothesis about transferability and learns appropriate layer-wise LRs on different domains. Moreover, its performance improvements are gained with only 1.5×-2.5× training time compared with direct training.Results on Segmentation Task. MetaLR achieves the best Dice performance on the LiTS segmentation task (Table 4). Unlike ResNet for lesion detection, the U-Net family has a more complex network topology. With skip connections, there are two interpretations [2] of depths for layers: 1) the left-most layers are the shallowest, and 2) the top layers of the ""U"" are the shallowest. This makes the handpicking methods even more computationally expensive. However, MetaLR  updates the LR for each layer according to their validation gradients, and its training efficiency is not affected by the complex model architecture."
MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging,3.4,Discussion and Findings,"The LRs Learned with MetaLR. For ResNet-18 (Fig. 2 (a)), the layer-wise LRs fluctuate drastically during the first 100 iterations. However, after iteration 100, all layers except the first layer ""Conv1"" become stable at different levels.The first layer has a decreasing LR (from 2.8 × 10 -3 to 3 × 10 -4 ) throughout the process, reflecting its higher transferability. For 3D U-Net (Fig. 2 (b)), the middle layers of the encoder ""Down-128"" and ""Down-256"" are the most transferable and have the lowest LRs, which is difficult for previous fine-tuning schemes to discover. As expected, the randomly initialized ""FC"" and ""Out"" layers have the largest LRs since they are not transferable."
MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging,,The Effectiveness of Proportional Hyper-LR and Training Batches,"Validation. We illustrate the LR curves with a constant hyper-LR instead of a proportional one. The LR curves of ""Block 3-1"" and ""Block 4-2"" become much more fluctuated (Fig. 2 (c)). This instability may be the key reason for the instability of performance when using a constant hyper-LR. Furthermore, we surprisingly find that the learned LRs are similar to the curves learned when validated on the training set when using a separate validation set Fig. 2  "
MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging,4,Conclusion,"In this work, we proposed a new fine-tuning scheme, MetaLR, for medical transfer learning. It achieves significantly superior performance to the previous SOTA fine-tuning algorithms. MetaLR alternatively optimizes model parameters and layer-wise LRs in an online meta-learning fashion with a proportional hyper-LR. It learns to assign lower LRs for the layers with higher transferability and higher LRs for the less transferable layers. The proposed algorithm is easy to implement and shows the potential to replace manual layer-wise fine-tuning schemes. Future works include adapting MetaLR to a wider variety of clinical tasks."
MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging,,,
MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging,,M,
MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging,,Fig. 2 .,
MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging,,Table 1 .,
MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging,,Table 2 .,
MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging,,Table 3 .,
MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging,,Table 4 .,
MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging,,,
Multi-modal Semi-supervised Evidential Recycle Framework for Alzheimer’s Disease Classification,1,Introduction,"Alzheimer's disease (AD) is an irreversible neurodegenerative disease that leaves patients with impairments in memory, language and cognition [7]. Previous work of [6,22] show that the combination of image data and other related data is beneficial to the improvement of model performance, but how to efficiently combine statistical non-imaging data and medical image data is still an open question. Second, although it is not too difficult to obtain and collect patient data, subjective bias in the AD diagnosis process and the time-consuming and complicated process of labeling the diagnostic results lead to the scarcity of labeled data [11]. Therefore, research and development of models that require only a small amount of labeled data to achieve higher accuracy has attracted great attention [14].Semi-supervised learning (SSL) methods are commonly used in medical image analysis to address the lack of manually annotated data [24]. Hang et al. [10] proposed a contrastive self-ensembling framework by introducing the weight formula and reliability-awareness for semi-supervised medical image classification. In [3], Aviles et al., based on the diffusion model and hypergraph learning, proposed a multi-modal hypergraph diffusion network to implement semi-supervised learning for AD classification. In [4], researchers introduced CSEAL, a semi-supervised learning framework that combines consistency-based SSL with uncertainty-based active learning, for multi-label chest X-ray classification tasks. Other studies using evidential learning [5,17] have demonstrated the great potential of this theory in fitting low-dimensional manifolds in high-dimensional spaces for classification with uncertainty estimates. This feature makes the model based on evidential learning promising in the SSL field of medical images.The proposal of evidential deep learning (EDL) [21] allows the model to better estimate the uncertainty in multi-classification tasks. On the binary classification task, controlling evidential regression to obtain a continuous probability value before 0 and 1 can often achieve more accurate results than using the Dirichlet distribution to obtain a discrete distribution of EDL [19]. The residual between the prediction results of the imperfect model and the true distribution of the data can be decomposed into aleatoric uncertainty (AU) and epistemic uncertainty (EU). Theoretically, the former comes from the noise of the data, which usually does not depend on the sample size. Therefore, by iteratively reducing this part of uncertainty, the best classification results can be obtained under a given amount of data. The latter is proportional to the sample size. As sample size increases, the reduction of this part of uncertainty can make the model closer to the observed distribution or fit with more complex conditions, thereby improving the performance of the model itself. Based on this understanding, we exploit the ability of evidential regression of handling uncertainty to decompose the two parts of uncertainty, AU and EU, and proposed a method by adjusting the two parts of uncertainty to achieve semi-supervised classification which shows in Fig. 1.Our main contributions include: 1) Adjusting the loss function of evidential regression so it can obtain more accurate results and better separate AU and EU; 2) Building a multi-layer and multi-step network to implement evidential regression and a semi-supervised learning method of step-by-step training is proposed; 3) A new SOTA of semi-supervised learning is achieved on the ADNI dataset, and performance close to supervised learning can be achieved with only a small amount of labeled data."
Multi-modal Semi-supervised Evidential Recycle Framework for Alzheimer’s Disease Classification,2,Methods,
Multi-modal Semi-supervised Evidential Recycle Framework for Alzheimer’s Disease Classification,2.1,Original Deep Evidential Regression (DER),"DER [2] adopts the simplest setting: y i ∼ N (0, σ 2 i ). In a Bayesian framework, this corresponds to taking the normal inverse Gamma distribution NIG(μ, σ 2 |m), m = (γ, ν, α, β), as a conjugate prior of a normal contribution with unknown mean μ and variance σ 2 . Combining the disturbance parameter with Bayesian inference, the likelihood of an observation y for a given m follows a t-distribution with 2α i degrees of freedom: βi(1+γi)   γiαi . For known m, Animi et al. [2] defined the prediction of y i as E[μ i ] = γ i , and defined AU and EU as u a and u e :And it follows that:where m = NN(w) is specified by a neural network (NN), λ is a hyperparameter, and Φ = 2γ i + α i represents the total evidence gained from training."
Multi-modal Semi-supervised Evidential Recycle Framework for Alzheimer’s Disease Classification,2.2,Evidential Regression Beyond DER,"Although DER has achieved some success in both theoretical and practical applications [2,15], as pointed out by Meinert et al. [16], this theory has some major flaws. First, although the regularization part of loss function L R is added, the constrain on parameter β i is not enough. Second, although the two parts of AU and EU are defined separately, the correlation between them is too high. In practice, disentangling and effectively using uncertainty information for training remains challenging.After practice and theoretical proof, Meinert et al. [16] states that the width of the t-distribution projected by the NIG distribution, that is, w St , can better reflect the noise in data.And, correspondingly, we use the residual 1/ √ ν i part of u a and u e in the original definition to represent EU:where ν i , α i , and β i are part of the parameters of the evidence distribution m = (γ, ν, α, β), and we verify the performance of this new uncertainty estimation method through experiments."
Multi-modal Semi-supervised Evidential Recycle Framework for Alzheimer’s Disease Classification,2.3,Model and Workflow,"With efficient estimation of AU and EU, our model has the basis for implementation. As shown in Fig. 2, our model is divided into three parts: multimodal feature extractor, evidential predictor, and recycle classifier. The multimodal feature extractor form a high-dimensional feature space and the evidential predictor generates the evidential distribution in the feature space. After calculating the classification result and the uncertainty (AU and EU) based on the evidential distribution, the recycle classifier controls the training process and reaches the best performances through a step-by-step recurrent training workflow."
Multi-modal Semi-supervised Evidential Recycle Framework for Alzheimer’s Disease Classification,,AU for Training Classifier.,"Based on the manifold assumption, the real data is gathered on the low-dimensional manifold of the high-dimensional space, and the noise of the data is located on the edge of the manifold for the corresponding category. When using ER to fit the manifold, these noise data will make marginal data with high AU. By optimizing the classifier to iteratively reduce the AU, optimal classification result under the current conditions can be obtained. We use L a to optimize AU:In the above formula λ a = [0.005, 0.01] is a parameter that controls the degree of deviations of the regularization part and w St uses the previous definition, and Φ is the total amount of evidence learned by the model. In order to better motivate the learning of the model, we adopted the work of Liu et al. [15] and used the form of Φ = γ i + 2α i . We used the expanded form of L NIG with minor adjustments according to the optimization objective, specifically,logEU for Training Extractor. If only the AU part is optimized, there will always be this gap between the model prediction and the real data. EU is mainly used to optimize the feature extractor since EU mainly reflects the bias of the model in the prediction. For data D l , given groundtruth labels, we use which can make the model more conservative about making predictions in the next iteration. This reduces our models being affected by misleading evidence and obtains better performance by retaining higher uncertainty to allow the model to have more room to optimize. In order to effectively combine labeled and unlabeled data we adjust the weights of different data:where μ l + μ u = 1, μ l , μ u ∈ [0, 1], are two weight factors.Model. In terms of the feature extractor, we use the latest EfficientNetV2, which, in Feng et al. [8], has achieved good results in combination with EDL.In order to avoid overfitting, we used the minimum model in this network and added Dropout to the output end. At the same time, in order to fill the differences between multi-modality data and model input, we have added the fully connected (FC) layer and convolutional layer (Conv) to adaptive adjust input channels. We employed three evidential FC layers proposed by Amini et al. [2] to form our evidential predictor. At the same time, in order to achieve the optimization of AU and EU separately, we froze some parameters in the first two layers and limited the range of the last layer of parameter adjustment. "
Multi-modal Semi-supervised Evidential Recycle Framework for Alzheimer’s Disease Classification,,Workflow of Recycle,
Multi-modal Semi-supervised Evidential Recycle Framework for Alzheimer’s Disease Classification,3,Experiments and Results,"Data Description. In this paper, we assess the effectiveness of our multimodal semi-supervised evidential recycle framework on the ADNI-2 dataset 1 , which comprises multi-center data consisting of various modalities, including imaging and multiple phenotype data. Specifically, the dataset consists of four categories: normal control (NC), early mild cognitive impairment (EMCI), late mild cognitive impairment (LMCI), and Alzheimer's disease (AD). To ensure the effectiveness of our training and balance the number of categories, we used a sample of 515 patients, utilizing their MRI, PET, demographics, and APOE as inputs. On MRI images, we used 3T T1-weighted and FLAIR MR images, and the preprocessing process used CAT12 and SPM tools. All MRI data were processed using standard pipeline, including anterior commissure (AC)-posterior commissure (PC) correction, intensity correction, and skull stripping. Affine registration is performed to linearly align each MRI to the Colin27 template and resample to 224 × 224 × 91 for subsequent processing. For PET images, we used the official pre-processed AV-45 PET image and resampled them in the same way as the MRIs. We chose to include APOE in our analysis, as it is a well-established genetic risk factor for developing AD.Evaluation. We evaluated our model from three aspects. First, for the sake of comparison, we followed the technical conventions of most similar studies and selected three comparison tasks: AD vs NC, LMCI vs NC, and EMCI vs LMCI.Second, we compared and demonstrated the results of our model under different numbers of ground truth labels to verify that its performance improves as the label data volume increases. Third, we conducted different ablation experiments, which shows in Fig. 3, to prove the validity and rationality of each part of the proposed model framework. Among them, CNNs represents the performance when using only the EfficientNetV2-S model and its original supervised learning classifier without using unlabeled data for training, which is the baseline model.AU and EU represent the training process using only the corresponding parts. DER uses our proposed complete training process but does not use our improved u a and u e estimations, instead continuing to use the estimation method u A and u E which proposed in the original DER paper [2]. To compare performance fairly, we ran all techniques under the same conditions. The results were evaluated on accuracy (ACC), specificity (SPE), and sensitivity (SEN).Implementation Details. The upper bound in performance is the result obtained when the model is trained with all the input data are labeled. In the current supervised learning algorithms, the performance of each algorithm on  each task is not consistent, so we selected three papers in supervised learning, each representing the SOTA performance of the three tasks [18,20,23] for comparison. Our implementation employs PyTorch v1.4.0 and utilizes the Adam optimizer with a learning rate of 1 × 10 -4 and a weight decay of 1 × 10 -5 . We utilize a linear decay scheduler of 0.1 based on the loss functions above. The optimizer is set with β values of [0.9, 0.999] and value of 1 × 10 -8 . In terms of data, since the SSL method needs to learn from unlabeled data, 100% of the data is put into training, and some of the data have ground truth labels. In the test, only the result index of the unlabeled data is calculated, so the training set and the test set are not divided. But in order to determine the threshold of each uncertainty, we randomly selected 10% of the data as the validation set, and calculated the uncertainty independently outside the training process.Results. We compared our model with the semi-supervised learning methods currently achieving the best performance on the ADNI-2 dataset, as well as other top models in the semi-supervised learning field. As shown in Table 1, our model achieved SOTA performance in all three tasks of the semi-supervised learning category. At the same time, compared with other semi-supervised learning algorithms, our results are unprecedentedly close to the best supervised learning methods, indicating the performance of our model under less labeled data and the feasibility of applying this algorithm in clinical settings.Our ablation experiment results are shown in Fig. 3. Firstly, compared with the baseline, our semi-supervised learning algorithm effectively learns classification information from unlabeled data. Secondly, compared with DER, our uncertainty estimation surpasses the original DER method. The AU and EU items demonstrate the importance of optimizing both the AU and EU components in our framework. From Table 2, we can observe that we have outperformed the currently representative advanced semi-supervised learning algorithm DS 3 L [9] in each labeled data count. At the same time, the superiority of our model compared to the baseline method also proves the learning efficiency of our framework. The performance of our model at 20% labeled data count is already very close to the upper bound, which is the result obtained using 100% labeled data. This indicates the strong learning ability of our model in the case of a small labeled data amount.In addition, we have plotted the error rate of our framework under different labeled data counts in Fig. 4. It is apparent that the performance of our model improves as the labeled data amount increases from 5% to 10%, 15%, and 20%. Combining with Table 2, we can observe the well-known transductive effect in the field of semi-supervised learning, which means that beyond a certain data amount, increasing the size of the dataset can only bring marginal performance improvement. This is evident when comparing the model performance under 20%, 40%, 80%, and 100% labeled data counts."
Multi-modal Semi-supervised Evidential Recycle Framework for Alzheimer’s Disease Classification,4,Conclusions,"We proposed an evidential regression-based semi-supervised learning framework, using the characteristics of AU and EU to train classifiers and extractors, respectively. Our model achieves SOTA performance on the ADNI-2 dataset. And due to the characteristics of semi-supervised learning, our model has unique advantages in adding private data, fine-tuning downstream tasks, and avoiding overfitting, which makes our model have great potential in clinical applications."
Multi-modal Semi-supervised Evidential Recycle Framework for Alzheimer’s Disease Classification,,Fig. 1 .,
Multi-modal Semi-supervised Evidential Recycle Framework for Alzheimer’s Disease Classification,,Fig. 2 .,
Multi-modal Semi-supervised Evidential Recycle Framework for Alzheimer’s Disease Classification,,2 Φ(w) + λ u y i -γi wSt 2 Φ,
Multi-modal Semi-supervised Evidential Recycle Framework for Alzheimer’s Disease Classification,,Fig. 3 .,
Multi-modal Semi-supervised Evidential Recycle Framework for Alzheimer’s Disease Classification,,Fig. 4 .,
Multi-modal Semi-supervised Evidential Recycle Framework for Alzheimer’s Disease Classification,,Table 1 .,
Multi-modal Semi-supervised Evidential Recycle Framework for Alzheimer’s Disease Classification,,90 92.95 93.01 89.45 88.50 89.47 87.27 86.94 85.83,
Multi-modal Semi-supervised Evidential Recycle Framework for Alzheimer’s Disease Classification,,Table 2 .,
Multi-modal Semi-supervised Evidential Recycle Framework for Alzheimer’s Disease Classification,,92.95 93.01 89.45 88.50 89.47 87.27 86.94 85.83,
DAS-MIL: Distilling Across Scales for MIL Classification of Histological WSIs,1,Introduction,"Modern microscopes allow the digitalization of conventional glass slides into gigapixel Whole-Slide Images (WSIs) [18], facilitating their preservation and Fig. 1. Overview of our proposed framework, DAS-MIL. The features extracted at different scales are connected (8-connectivity) by means of different graphs. The nodes of both graphs are later fused into a third one, respecting the rule ""part of"". The contextualized features are then passed to distinct attention-based MIL modules that extract bag labels. Furthermore, a knowledge distillation mechanism encourages the agreement between the predictions delivered by different scales.retrieval, but also introducing multiple challenges. On the one hand, annotating WSIs requires strong medical expertise, is expensive, time-consuming, and labels are usually provided at the slide or patient level. On the other hand, feeding modern neural networks with the entire gigapixel image is not a feasible approach, forcing to crop data into small patches and use them for training. This process is usually performed considering a single resolution/scale among those provided by the WSI image.Recently, Multi-Instance Learning (MIL) emerged to cope with these limitations. MIL approaches consider the image slide as a bag composed of many patches, called instances; afterwards, to provide a classification score for the entire bag, they weigh the instances through attention mechanisms and aggregate them into a single representation. It is noted that these approaches are intrinsically flat and disregard the pyramidal information provided by the WSI [15], which have been proven to be more effective than single-resolution [4,13,15,19]. However, to the best of our knowledge, none of the existing proposals leverage the full potential of the WSI pyramidal structure. Indeed, the flat concatenation of features [19] extracted at different resolutions does not consider the substantial difference in the informative content they provide. A proficient learning approach should instead consider the heterogeneity between global structures and local cellular regions, thus allowing the information to flow effectively across the image scales.To profit from the multi-resolution structure of WSI, we propose a pyramidal Graph Neural Network (GNN) framework combined with (self) Knowledge Distillation (KD), called DAS-MIL (Distilling Across Scales). A visual representation of the proposed approach is depicted in Fig. 1. Distinct GNNs provide contextualized features, which are fed to distinct attention-based MIL modules that compute bag-level predictions. Through knowledge distillation, we encour-age agreement across the predictions delivered at different resolutions, while individual scale features are learned in isolation to preserve the diversity in terms of information content. By transferring knowledge across scales, we observe that the classifier self-improves as information flows during training. Our proposal has proven its effectiveness on two well-known histological datasets, Camelyon16 and TCGA lung cancer, obtaining state-of-the-art results on WSI classification."
DAS-MIL: Distilling Across Scales for MIL Classification of Histological WSIs,2,Related Work,"MIL Approaches for WSI Classification. We herein summarize the most recent approaches; we refer the reader to [11,26] for a comprehensive overview."
DAS-MIL: Distilling Across Scales for MIL Classification of Histological WSIs,,Single-Scale.,"A classical approach is represented by AB-MIL [16], which employs a side-branch network to calculate the attention scores. In [28], a similar attention mechanism is employed to support a double-tier feature distillation approach, which distills features from pseudo-bags to the original slide. Differently, DS-MIL [19] applies non-local attention aggregation by considering the distance with the most relevant patch. The authors of [20] and [25] propose variations of AB-MIL, which introduce clustering losses and transformers, respectively. In addition, SETMIL [31] makes use of spatial-encoding transformer layers to update the representation. The authors of [7] leverage DINO [5] as feature extractor, highlighting its effectiveness for medical image analysis. Beyond classical attention mechanisms, there are also algorithms based on Recurrent Neural Networks (RNN) [4], and Graphs Neural Networks (GNN) [32].Multi-Scale. Recently, different authors focused on multi-resolution approaches. DSMIL-LC [19] merges representations from different resolutions, i.e., low instance representations are concatenated with the ones obtained at a higher resolution. MS-RNNMIL [4], instead, fed an RNN with instances extracted at different scales. In [6], a self-supervised hierarchical transformer is applied at each scale. In MS-DA-MIL [13], multi-scale features are included in the same attention algorithm. [10] and [15] exploit multi-resolution through GNN architectures.Knowledge Distillation. Distilling knowledge from a more extensive network (teacher ) to a smaller one (student) has been widely investigated in recent years [21,24] and applied to different fields, ranging from model compression [3] to WSI analysis [17]. Typically, a tailored learning objective encourages the student to mimic the behaviour of its teacher. Recently, self-supervised representation learning approaches have also employed such a schema: as an example, [5,9] exploit KD to obtain an agreement between networks fed with different views of the same image. In [28], KD is used to transfer the knowledge between MIL tiers applied on different subsamples bags. Taking inspiration from [23] and [30], our model applies (self) knowledge distillation between WSI scale resolutions."
DAS-MIL: Distilling Across Scales for MIL Classification of Histological WSIs,3,Method,"Our approach aims to promote the information flow through the different employed resolutions. While existing works [19,20,25] take into account interscales interactions by mostly leveraging trivial operations (such as concatenation of related feature representations), we instead provide a novel technique that builds upon: i) a GNN module based on message passing, which propagates patches' representation according to the natural structure of multi-resolutions WSI; ii) a regulation term based on (self) knowledge distillation, which pins the most effective resolution to further guide the training of the other one(s). In the following, we are delving into the details of our architecture.Feature Extraction. Our work exploits DINO, the self-supervised learning approach proposed in [5], to provide a relevant representation of each patch. Differently from other proposals [19,20,28], it focuses solely on aligning positive pairs during optimization (and hence avoids negative pairs), which has shown to require a lower memory footprint during training. We hence devise an initial stage with multiple self-supervised feature extractors f (•; θ 1 ), . . . , f M (•; θ M ), one dedicated to each resolution: this way, we expect to promote feature diversity across scales. After training, we freeze the weights of these networks and use them as patch-level feature extractors. Although we focus only on two resolutions at time (i.e., M = 2) the approach can be extended to more scales.Architecture. The representations yield by DINO provide a detailed description of the local patterns in each patch; however, they retain poor knowledge of the surrounding context. To grasp a global guess about the entire slide, we allow patches to exchange local information. We achieve it through a Pyramidal Graph Neural Network (PGNN) in which each node represents an individual WSI patch seen at different scales. Each node is connected to its neighbors (8-connectivity) in the euclidean space and between scales following the relation ""part of""1 . To perform message passing, we adopt Graph ATtention layers (GAT) [27].In general terms, such a module takes as input multi-scale patch-level representations X = [X 1 X 2 ], where X 1 ∈ R N1×F and X 2 ∈ R N2×F are respectively the representations of the lower and higher scale. The input undergoes two graph layers: while the former treats the two scales as independent subgraphs A 1 ∈ R N1×N1 and A 2 ∈ R N2×N2 , the latter process them jointly by considering the entire graph A (see Fig. 1, left). In formal terms:where H ≡ [H 1 H 2 ] stands for the output of the PGNN obtained by concatenating the two scales. These new contextualized patch representations are then fed to the attention-based MIL module proposed in [19], which produces bag-level scores y BAG 1 , y BAG 2 ∈ R 1×C where C equals the number of classes. Notably, such a module provides additional importance scores z 1 ∈ R N1 and z 2 ∈ R N2 , which quantifies the importance of each original patch to the overall prediction.Aligning Scales with (Self ) Knowledge Distillation. We have hence obtained two distinct sets of predictions for the two resolutions: namely, a bag-level score (e.g., a tumor is either present or not) and a patch-level one (e.g., which instances contribute the most to the target class). However, as these learned metrics are inferred from different WSI zooms, a disagreement may emerge: indeed, we have observed (see Table 4) that the higher resolutions generally yield better classification performance. In this work, we exploit such a disparity to introduce two additional optimization objectives, which pin the predictions out of the higher scale as teaching signal for the lower one. Further than improving the results of the lowest scale only, we expect its benefits to propagate also to the shared message-passing module, and so to the higher resolution.Formally, the first term seeks to align bag predictions from the two scales through (self) knowledge distillation [14,29]:where KL stands for the Kullback-Leibler divergence and τ is a temperature that lets secondary information emerge from the teaching signal.The second aligning term regards the instance scores. It encourages the two resolutions to assign criticality scores in a consistent manner: intuitively, if a lowresolution patch has been considered critical, then the average score attributed to its children patches should be likewise high. We encourage such a constraint by minimizing the Euclidean distance between the low-resolution criticality grid map z 1 and its subsampled counterpart computed by the high-resolution branch:(2)In the equation above, GraphPooling identifies a pooling layer applied over the higher scale: to do so, it considers the relation ""part of"" between scales and then averages the child nodes, hence allowing the comparison at the instance level.Overall Objective. To sum up, the overall optimization problem is formulated as a mixture of two objectives: the one requiring higher conditional likelihood w.r.t. ground truth labels y and carried out through the Cross-Entropy loss L CE (•; y); the other one based on knowledge distillation:where λ is a hyperparameter weighting the tradeoff between the teaching signals provided by labels and the higher resolution, while β balances the contributions of the consistency regularization introduced in Eq. ( 2)."
DAS-MIL: Distilling Across Scales for MIL Classification of Histological WSIs,4,Experiments,"WSIs Pre-processing. We remove background patches through an approach similar to the one presented in the CLAM framework [20]: after an initial segmentation process based on Otsu [22] and Connected Component Analysis [2], non-overlapped patches within the foreground regions are considered.Optimization. We use Adam as optimizer, with a learning rate of 2 × 10 -4 and a cosine annealing scheduler (10 -5 decay w/o warm restart). We set τ = 1.5, β = 1, and λ = 1. The DINO feature extractor has been trained with two RTX5000 GPUs: differently, all subsequent experiments have been performed with a single RTX2080 GPU using Pytorch-Geometric [12]. To asses the performance of our approach, we adhere to the protocol of [19,28] and use the accuracy and AUC metrics. Moreover, the classifier on the higher scale has been used to make the final overall prediction. Regarding the KD loss, we apply the temperature term to both student and teacher outputs for numerical stability.  [4] 0.806 0.806 0.862 0.911 ABMIL [16] 0.845 0.865 0.900 0.949 CLAM-SB [20] 0.865 0.885 0.875 0.944 CLAM-MB [20] 0.850 0.894 0.878 0.949 Trans-MIL † [25] 0.883 0.942 0.881 0.948 DTFD (AFS) [28] 0.908 0.946 0.891 0.951 DTFD (MaxMinS) [28] 0.899 0.941 0.894 0.961 DSMIL † [19] 0.915 0.952 0.888 0.951 Multi Scale MS-DA-MIL [13] 0.876 0.887 0.900 0.955 MS-MILRNN [4] 0.814 0.837 0.891 0.921 HIPT † [6] 0.890 0.951 0.890 0.950 DSMIL-LC † [19] 0.909 0.955 0.913 0.964 H 2 -MIL † [15] 0.859 0.912 0.823 0.917 DAS-MIL (ours) 0.945 0.973 0.925 0.965Camelyon16. [1] We adhere to the official training/test sets. To produce the fairest comparison with the single-scale state-of-the-art solution, the 270 remaining WSIs are split into training and validation in the proportion 9:1."
DAS-MIL: Distilling Across Scales for MIL Classification of Histological WSIs,,TCGA Lung Dataset.,"It is available on the GDC Data Transfer Portal and comprises two subsets of cancer: Lung Adenocarcinoma (LUAD) and Lung Squamous Cell Carcinoma (LUSC), counting 541 and 513 WSIs, respectively. The aim is to classify LUAD vs LUSC; we follow the split proposed by DSMIL [19]."
DAS-MIL: Distilling Across Scales for MIL Classification of Histological WSIs,4.1,Comparison with the State-of-the-art,"Table 1 compares our DAS-MIL approach with the state-of-the-art, including both single-and multi-scale architectures. As can be observed: i) the joint exploitation of multiple resolutions is generally more efficient; ii) our DAS-MIL yields robust and compelling results, especially on Camelyon16, where it provides 0.945 of accuracy and 0.973 AUC (i.e., an improvement of +3.3% accuracy and +1.9% AUC with respect to the SOTA). Finally, we remark that most of the methods in the literature resort to different feature extractors; however, the next subsections prove the consistency of DAS-MIL benefits across various backbones.  "
DAS-MIL: Distilling Across Scales for MIL Classification of Histological WSIs,4.2,Model Analysis,"On the Impact of Knowledge Distillation. To assess its merits, we conducted several experiments varying the values of the corresponding balancing coefficients (see Table 2). As can be observed, lowering their values (even reaching λ = 0, i.e., no distillation is performed) negatively affects the performance. Such a statement holds not only for the lower resolution (as one could expect), but also for the higher one, thus corroborating the claims we made in Sect. 3 on the bidirectional benefits of knowledge distillation in our multi-scale architecture. We have also performed an assessment on the temperature τ , which controls the smoothing factor applied to teacher's predictions (Table 3). We found that the lowest the temperature, the better the results, suggesting that the teacher scale is naturally not overconfident about its predictions, but rather well-calibrated.Single-Scale vs Multi-Scale.  The Impact of the Feature Extractors and GNNs. Table 5 proposes an investigation of these aspects, which considers both SimCRL [8] and DINO, as well as the recently proposed graph mechanism H 2 -MIL [15]. In doing so, we fix the input resolutions to 5× and 20×. We draw the following conclusions: i) when our DAS-MIL feature propagation layer is used, the selection of the optimal feature extractor (i.e., SimCLR vs Dino) has less impact on performance, as the message-passing can compensate for possible lacks in the initial representation; ii) DAS-MIL appears a better features propagator w.r.t. H 2 -MIL.H 2 -MIL exploits a global pooling layer (IHPool) that fulfils only the spatial structure of patches: as a consequence, if non-tumor patches surround a tumor patch, its contribution to the final prediction is likely to be outweighed by the IHPool module of H 2 -MIL. Differently, our approach is not restricted in such a way, as it can dynamically route the information across the hierarchical structure (also based on the connections with the critical instance)."
DAS-MIL: Distilling Across Scales for MIL Classification of Histological WSIs,5,Conclusion,"We proposed a novel way to exploit multiple resolutions in the domain of histological WSI. We conceived a novel graph-based architecture that learns spatial correlation at different WSI resolutions. Specifically, a GNN cascade architecture is used to extract context-aware and instance-level features considering the spatial relationship between scales. During the training process, this connection is further amplified by a distillation loss, asking for an agreement between the lower and higher scales. Extensive experiments show the effectiveness of the proposed distillation approach."
DAS-MIL: Distilling Across Scales for MIL Classification of Histological WSIs,,λ λ λ Table 3 .,
DAS-MIL: Distilling Across Scales for MIL Classification of Histological WSIs,,Table 1 .,
DAS-MIL: Distilling Across Scales for MIL Classification of Histological WSIs,,Table 2 .,
DAS-MIL: Distilling Across Scales for MIL Classification of Histological WSIs,,Table 4 .,
DAS-MIL: Distilling Across Scales for MIL Classification of Histological WSIs,,,
DAS-MIL: Distilling Across Scales for MIL Classification of Histological WSIs,,Table 5 .,
DAS-MIL: Distilling Across Scales for MIL Classification of Histological WSIs,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0 24.
"An Auto-Encoder to Reconstruct Structure with Cryo-EM Images via Theoretically Guaranteed Isometric Latent Space, and Its Application for Automatically Computing the Conformational Pathway",1,Introduction,"Over the past few years, Cryo Electron Microscopy (Cryo-EM) has made remarkable progress in biomolecule structure analysis, becoming a major structural analysis technique along with X-ray crystallography. Based on a brief history of the early period of the technique, researchers had proposed methods that reconstruct the static structures of protein molecules from a set of single particle Cryo-EM images, e.g., EMAN2 [24], RELION [20,21,31], and cryoSPARC [17,18]. Methods that reconstruct nonstatic structures have also been proposed, e.g., cryoDRGN [28][29][30], e2gmm [2], 3DVA [16], and 3DFlex [15]. In these methods, a 3D density map expresses the structure and helps to find biological significance [25,27].Recently, taking advantage of method capturing the nonstatic structures, researchers have tried to define a plausible conformational pathway (continuous change with 3D density map) from only single particle Cryo-EM images [10,26] to efficiently determine the biological significance (e.g., new drug). The pathway is usually defined via the latent space of a trained deep Auto-Encoder (AE) by the Cryo-EM images. For example, in [10], the authors first collect the latent variables, which are the outputs of the trained encoder in cryoDRGN [28], as inputs of the Cryo-EM images. Then, the variables are mapped into a 2D space by UMAP [13]. Thereafter, a sequence of the 2D points is manually constructed via qualitatively evaluating the corresponding structures to those points. The structure is reconstructed by the trained decoder of cryoDRGN. Lastly, the conformational pathway is defined as a sequence of the reconstructed structures along the sequence of the 2D points. Although the protocol of [10] is state-of-the-art, it can be tedious as it involves the manual construction with qualitative evaluation. The following are two of the most important causes: i) for a trained AE of cryoDRGN, there are no theoretical insights into the relation between the latent distribution and the distribution of the structure and ii) the latent distribution of the trained AE is not consistent with the fixed prior, i.e., standard normal Gaussian.In this study, we propose a deep AE with a trainable prior that is expressed by a Gaussian Mixture Model (GMM). We name the AE cryoTWIN1 . CryoTWIN is trained by single particle Cryo-EM images with the estimated pose orientations under an objective inspired by RaDOGAGA [8]. The trained AE reconstructs the nonstatic structures for the latent variables of the Cryo-EM images. A property of the trained AE is that its latent space is theoretically isometric with a space of the structure, where the latent distribution can be proportional to the distribution of the structure. Additionally, the trained prior can fit the latent distribution. This useful property helps us compute a ridgeline on the trained GMM as a sequence of the latent variables and automatically define the conformational pathway by a sequence of the reconstructed structure along the ridgeline.Our main contributions are as follows: i) we propose cryoTWIN: a deep AE model with the beneficial property for computing the conformational pathway and ii) in our numerical experiments, we confirm that the pathway computed using cryoTWIN is sufficiently consistent with an existing one, that was manually determined by researchers.In Sect. 2, we describe representatives of methods introduced at the beginning of this section in detail. Furthermore, we explain the differences of cryoTWIN compared with RaDOGAGA and cryoDRGN, as our method is partly inspired by them. In Sect. 3, we give a detailed account of cryoTWIN with theoretical guarantees. Additionally, we introduce an algorithm to compute the conformational pathway. In Sect. 4, we present the results obtained in our numerical experiments using a ribosome dataset. Finally, in Sect. 5, we conclude this study and discuss our future work."
"An Auto-Encoder to Reconstruct Structure with Cryo-EM Images via Theoretically Guaranteed Isometric Latent Space, and Its Application for Automatically Computing the Conformational Pathway",2,Related Work,
"An Auto-Encoder to Reconstruct Structure with Cryo-EM Images via Theoretically Guaranteed Isometric Latent Space, and Its Application for Automatically Computing the Conformational Pathway",2.1,Existing Reconstruction Methods,"The representative method from the static structure category is cryoSPARC [17,18]. Given a set of single particle Cryo-EM images and the number of structures, N , this method reconstructs N structures using an efficient stochastic gradient descent technique. Cryo-SPARC has a resolution similar to that of RELION but a smaller computational complexity [20,21,31]. The representative method from the nonstatic structure category is cryoDRGN [28][29][30], whose statistical model is based on spatial-VAE [1] (a variant of VAE [9]). The training objective is to maximize the variational lower-bound. After the training, cryoDRGN can reconstruct continuous structures using the continuous latent variables and the trained decoder."
"An Auto-Encoder to Reconstruct Structure with Cryo-EM Images via Theoretically Guaranteed Isometric Latent Space, and Its Application for Automatically Computing the Conformational Pathway",2.2,RaDOGAGA Revisit,"RaDOGAGA [8] is an AE whose latent space is isometric [5] to the input space, inspired by the rate-distortion theory [3]. Let f θ , g φ , and P ψ denote an encoder, a decoder, and a trainable prior distribution of latent variables, respectively, where θ, φ, and ψ are a set of trainable parameters. Then, the training objective is arg min θ,φ,ψ-log Q zi ) corresponds to the distortion (resp. rate). Here, x i , (i = 1, ..., n) is the i-th original data. Additionally, xzi = g φ (z i + ), z i = f θ (x i ), and each element of is uniformly sampled from [-T /2, T/2]. Moreover, β > 0 is a hyper-parameter, and Q zi is given bywhere U (z) is a rectangular window function:holds for all j (z j is the j-th element of z), and U (z) = 0 otherwise. Let δ 1 and δ 2 be infinitesimal vectors with arbitrary directions. Then, the optimally trained AE, whose decoder is g φ * , has the following isometric property at all z for any δ 1 and δ 2 :where xz = g φ * (z). Because of Eq. ( 2), P (z) ∝ P ( xz ) holds, where P (z) (resp. P ( xz )) is the latent distribution (resp. the probability density function of xz ). The following equation is derived by applying δ 1 = z -z and δ 2 = δ 1 to Eq. ( 2) :"
"An Auto-Encoder to Reconstruct Structure with Cryo-EM Images via Theoretically Guaranteed Isometric Latent Space, and Its Application for Automatically Computing the Conformational Pathway",2.3,Differences Between cryoTWIN and Existing Methods,"To understand the difference between cryoTWIN and RaDOGAGA, let V (resp. I) denote a 3D density map (resp. the Cryo-EM image). For cryoTWIN, only I is required to make the latent space isometric to a space of V , whereas V (usually inaccessible) is required to make RaDOGAGA have the same isometricity. Differences between cry-oTWIN and cryoDRGN are i) the latent distribution of cryoTWIN is theoretically proportional to a distribution of V , whereas cryoDRGN does not hold such property and ii) cryoTWIN can fit the prior to the latent distribution, whereas cryoDRGN can not. "
"An Auto-Encoder to Reconstruct Structure with Cryo-EM Images via Theoretically Guaranteed Isometric Latent Space, and Its Application for Automatically Computing the Conformational Pathway",3,Proposed Method,"We first describe our deep AE, cryoTWIN, with the theoretical guarantees before we explain how to compute the conformational pathway using cryoTWIN. Given a set of single particle Cryo-EM images I = {I i } n i=1 , the AE is trained via two steps: the first step is preprocessing and the second one is to train the AE under a rate distortion theory-based objective, such as RaDOGAGA (see Sect. In Sect. 3.3, we describe an algorithm to compute the conformational pathway."
"An Auto-Encoder to Reconstruct Structure with Cryo-EM Images via Theoretically Guaranteed Isometric Latent Space, and Its Application for Automatically Computing the Conformational Pathway",3.1,cryoTWIN,"As shown in Fig. 1, a deep AE of cryoTWIN consists of P ψ , f θ , and g φ , which denote a trainable GMM (prior), an encoder, and the decoder, respectively. Symbols ψ, θ, and φ represent a set of trainable parameters. The parameter ψ is expressed as , where z i is the latent variable of X i . The parameter w i is used to enforce the latent empirical distribution to follow P ψ . Secondly, using z i , Ri , and a 2D position ν = (s, t, 0) in X i , the decoder g φ returns Xzi (v) = g φ (z i + , v), v = Ri ν, where Xzi (v) means the predicted value at the 3D position v in a 3D Fourier volume and ∈ R d is a random noise vector, each of whose elements is uniformly sampled from [-T /2, T/2]. By computing Xzi (v), v = Ri ν for all 2D positions ν (i.e., for all pairs (s, t)), the predicted Fourier image Xzi is defined via the set { Xzi (v)} s,t .Training Objective: We introduce the -th ( ≥ 1) training with a mini-batch B = {X i } i , | B| = m, where X i = FI i , I i ∈ B ⊂ I. Firstly, partially inspired by [32], we update ψ of the GMM using {(z i , w i )} i , whereSecondly, we update θ and φ based on an objective inspired by rate-distortion theory, i.e., minimization of i) the distortion (reconstruction error), and ii) the rate. The first minimization problem is defined by min θ,φLet us simplify the problem by min θ,φ, where is the Hadamard product, and the (s, t)-thThe second minimization problem is defined by min θ -1 m m i=1 log Q zi , where Q zi is given by Eq. ( 2). Thus, we solve the combined minimization problem of Eq. ( 3):where β > 0 is a hyper-parameter whose appropriate value depends on I. For the solver, we use the RAdam optimizer [11].Prediction After Training: Let ψ * , θ * , and φ * be the trained parameters in the deep AE, whereLet F -1 denote the Inverse FT (IFT). Given a latent variable z, the trained decoder g φ * predicts the corresponding 3D density map similar to cryoDRGN [28] by the following procedure. Firstly, g φ * (z, v) is computed for all possible 3D positions v ∈ R 3 . Secondly, using {g φ * (z, v)} v , the predicted 3D Fourier volume is defined before applying F -1 to the Fourier volume. We define the predicted 3D density map by Vz . Note that the reconstruction error used in Eq. ( 3) is not a naive one, since we theoretically need to guarantee the isometric property between the latent space and a space of Vz in line with Theorem 1."
"An Auto-Encoder to Reconstruct Structure with Cryo-EM Images via Theoretically Guaranteed Isometric Latent Space, and Its Application for Automatically Computing the Conformational Pathway",3.2,Analysis of CryoTWIN Theorem 1. If we assume i) n,"1 for I = {I i } n i=1 (a set of Cryo-EM images), then Eq. ( 3) is approximately equivalent to arg min θ,φwhere V z denotes the true 3D density map with a latent variable z, and β > 0 is a hyper-parameter. The symbol Ṽzi means the predicted 3D density map during the training and it is defined by {g φ (z i + , v)} v and F -1 . Therefore, from Sect. 2.2, the Algorithm 1: Pseudocode for computing conformational pathway Input: Two means of the GMM P ψ * : μ * i as the start and μ * j as the end point, Hyper-parameters: ω > 1 (Larger ω returns less continuous pathway), K, M ∈ N Output: A sequence of reconstructed structures from Vμ * i to Vμ * j 1 Set μ * i and μ * j as to z i→j (0) and z i→j (K), respectively. 2 for k = 1, ..., K -1 do 3 Fix αi and αj to cos( πk 2K ) and (1 -cos( πk 2K )) ω respectively. Let α-i,j denote (C -2)-dimensional vector made by removing the i-th and j-th element from the C-dimensional vector (α1, ..., αC ) . Then, generate M samples for α-i,j under the constraint c =i,j αc = 1 -αiαj and αc ≥ 0. Let α (m) -i,j (m = 1, ..., M ) denote the m-th sample. For all m, define z (m) using αi, αj, αCost function for z (m) (m=1,...,M ) in the latent space .4 Using {z i→j (k)} K k=0 and the trained decoder g φ * , compute the following sequence;latent space of the trained cryoTWIN can be isometric with a space for the predicted 3D density map, i.e., ∀(z, z ); A brief derivation of the theorem is given in Appendix A. The assumption ii) of the theorem is realizable if we set a large integer as the number of components C in the GMM P ψ . This is empirically confirmed in Sect. 4. For assumption iii), as Eq. ( 3) implies the minimization of 3) tends to lead the AE to have the property of ∀z; Vz ≈ V z . Thus, if we collect a sufficient amount of the Cryo-EM images, Eq. ( 4) is realizable for cryoTWIN with large C."
"An Auto-Encoder to Reconstruct Structure with Cryo-EM Images via Theoretically Guaranteed Isometric Latent Space, and Its Application for Automatically Computing the Conformational Pathway",3.3,Computation for Conformational Pathway,"We compute the plausible conformational pathway that is defined as a sequence of 3D density maps. If we assume that an AE of cryoTWIN with large C is trained by the Cryo-EM images {I i } n i=1 , n 1, then because of Eq. ( 4), the pathway can be defined by the following two steps: i) generating a ridgeline from μ * i to μ * j on the GMM P ψ * as a sequence of the latent variables (see the second and third lines in Algorithm 1) and ii) reconstructing the corresponding 3D density maps along the ridgeline using the trained decoder g φ * (see the fourth line in Algorithm 1).  A brief explanation of the algorithm is as follows for a sample z (m) of the third line, the sample always ranges a set of candidate points achieving a local maximum with P ψ * , according to the results of [19]. Additionally, the cost function used to define z i→j (k) is inspired by the objective of constructing Max-Flux path [7]. Moreover, from Eq. ( 4), the cost is proportional toThus, an output of Algorithm 1 can be interpreted as a sequence of 3D density maps, which is generated directly in the space of the 3D density map under Max-Flux objective. We usually cannot access the space of the 3D density map."
"An Auto-Encoder to Reconstruct Structure with Cryo-EM Images via Theoretically Guaranteed Isometric Latent Space, and Its Application for Automatically Computing the Conformational Pathway",4,Numerical Experiment,"Using common single particle Cryo-EM images of ribosomes2 , we conduct two experiments: Expt1 and Expt2. The motivation of Expt1 (resp. Expt2) is to examine whether our theoretical claims work for the ribosomal dataset (resp. whether our computed pathways using the ribosomal dataset are consistent with the existing manually constructed pathways [4]). For both experiments, we employ cryoDRGN [28] as our baseline method. Let {I i } n i=1 denote a set of the Cryo-EM images, where n = 131899, and the image size is 128 × 128. A set of the corresponding estimated orientations { Ri } n i=1 is available here3 , which are the same ones used in the cryoDRGN. Throughout both experiments, we set (100, 3, 1/8) = (C, T, β) with cryoTWIN of Sect. 3.1. The dimension of the latent space is fixed to eight. For Algorithm 1, we set (11, 2, 1000) = (K, ω, M ). Our computational environment is four NVIDIA V100 GPU accelerators with two Intel Xeon Gold 6148 processors.Expt1: Setting): Firstly, we evaluate the isometricity of cryoTWIN by the correlation coefficient between dz 1 • dz 2 and d V1 • d V2 as described by [8] (see Eq. ( 2)). Secondly, we evaluate the gap between P ψ * (z) of cryoTWIN and P (z) using both Kullback-Leibler (KL) divergence based on KLIEP [23] and t-SNE visualization [12]. Results): Firstly, cryoTWIN achieves a correlation coefficient of 0.77, showing closeness to isometricity (by contrast, cryoDRGN has a correlation coefficient of only 0.52); see details in Fig 4 of Appendix B. Secondly, the KL divergence of cryoTWIN (resp. cryoDRGN) is only 0.25 (resp. 5.7), indicating that P ψ * (z) ≈ P (z) holds (resp. does not hold). Additionally, as shown in Fig. 2 a) and b), the visualized distributions with P ψ * (z) and P (z) of cryoTWIN seem similar, whereas those of cryoDRGN are different; see Fig. 5 of Appendix B. Notably, the large KL divergence with cryoDRGN implies that the conformational pathways based on the fixed prior of cryoDRGN are implausible.Expt2: Setting): Firstly, as the preliminary experiment of Expt2, using structural labels of [4], we examine how diversified the reconstructed structures of cryoTWIN are by comparing 4 them with those of cryoDRGN. We annotate the labels to reconstructed structures of cryoTWIN via visual examination and then quantitatively evaluate the accuracies of the annotated labels via Fourier Shell Correlation (FSC) between the two structures reconstructed by cryoTWIN and cryoDRGN with the same label. The annotation is performed by our experts using PyMOL [22]. Additionally, if the FSC is close to one in high and low frequency areas, the two structures are similar enough; see details in the third footnote of [29]. Secondly, we visually examine whether some of the conformational pathways computed by Algorithm 1 can be sufficiently consistent with the sub-paths in the four manually constructed pathways presented in [4]. For the computation, we prepare several pairs (μ * i , μ * j ) satisfying i) both π * i and π * j are large, and ii) Vμ * i and Vμ * j have different labels. The motivation to focus on the sub-paths comes from the following preliminary observations: Given ""B"" and ""E"" in Fig. 2 a) as start and end points, Algorithm 1 estimated only one of the four (B-C2-E1-E2-E4-E5 of Fig. 7 in [4]) as the most probable ridgeline. Thus, to reproduce the four pathways, we needed to aggregate sub-paths generated by the algorithm, whose start and end points were significant. Results): Firstly, by observing both i) the labeled structures shown in Fig. 2 c) and Fig. 3 (and Fig. 6 of Appendix B), and ii) the labeled structures presented in the original study of cryoDRGN [29] (and Fig. 5 c) of Appendix B), we confirm that cryoTWIN is as diversified as cryoDRGN in terms of the reconstructed structure. Additionally, considering the large FSC value in Fig. 2 d), the accuracies of the annotated labels are high. Secondly, considering the four pathways in [4], we confirm that the computed two pathways in Fig. 3 are consistent with the sub-paths in the main four pathways. Note that all the four pathways were successfully reproduced by the aforementioned aggregation in our preliminary experiments."
"An Auto-Encoder to Reconstruct Structure with Cryo-EM Images via Theoretically Guaranteed Isometric Latent Space, and Its Application for Automatically Computing the Conformational Pathway",,Time and Memory Complexities:,"The training time of cryoTWIN (resp. cryoDRGN) is eleven hours (resp. four hours), whereas their memory complexities are comparable. After training cryoTWIN, the running time to compute the pathways including the evaluation time is around one hour, which is much shorter than the running time of the state-of-the-art protocol of a few days [10]."
"An Auto-Encoder to Reconstruct Structure with Cryo-EM Images via Theoretically Guaranteed Isometric Latent Space, and Its Application for Automatically Computing the Conformational Pathway",5,Conclusion and Future Work,"We propose cryoTWIN for computing plausible pathways from Cryo-EM images, and the efficiency is demonstrated in our numerical experiments. For further research, it would be better to estimate the orientation of the image simultaneously in the training of cryoTWIN, since the preliminary estimation gives an bias to the predicted structure, as explained in [28]. Additionally, it is interesting to combine cryoTWIN and molecular dynamics simulators such as flexible fitting [14], as cryoTWIN provides envelopes for molecular structures in various intermediate states. The combined method could be a powerful tool for more practical applications, such as drug discovery."
"An Auto-Encoder to Reconstruct Structure with Cryo-EM Images via Theoretically Guaranteed Isometric Latent Space, and Its Application for Automatically Computing the Conformational Pathway",,Fig. 1 .,
"An Auto-Encoder to Reconstruct Structure with Cryo-EM Images via Theoretically Guaranteed Isometric Latent Space, and Its Application for Automatically Computing the Conformational Pathway",,,
"An Auto-Encoder to Reconstruct Structure with Cryo-EM Images via Theoretically Guaranteed Isometric Latent Space, and Its Application for Automatically Computing the Conformational Pathway",,,
"An Auto-Encoder to Reconstruct Structure with Cryo-EM Images via Theoretically Guaranteed Isometric Latent Space, and Its Application for Automatically Computing the Conformational Pathway",,Fig. 2 .,
"An Auto-Encoder to Reconstruct Structure with Cryo-EM Images via Theoretically Guaranteed Isometric Latent Space, and Its Application for Automatically Computing the Conformational Pathway",,Fig. 3 .,
Pick the Best Pre-trained Model: Towards Transferability Estimation for Medical Image Segmentation,1,Introduction,"The development of deep neural networks has greatly promoted medical imagingbased computer-aided diagnosis. Due to the large amount of learnable parameters in neural networks, sufficient annotated training samples are required for training. However, the labeling process of medical images is tedious and timeconsuming. To address this problem, the common paradigm of transfer learning, which first pre-trains a model on upstream image datasets and then fine-tunes it on various target tasks, has been widely investigated in recent years [10,21,30]. Compared with the distributed training across multiple centers, there are no specific ethical issues or computational design of distributed/federated learning frameworks with the ""pre-train-then-fine-tune"" workflow.Previous works mainly focused on the fine-tuning strategy to effectively adapt the knowledge from the pre-trained models to target tasks [4,12,19,26]. With the increasing number of pre-trained networks provided by the community, model repositories like Hugging Face [25] and PyTorch Hub [18] enable researchers to experiment across a large number of downstream datasets and tasks. These pre-trained models require less training time and have better performance and robustness compared with the learning-from-scratch models. However, it has been observed by recent works [23] that the pre-trained models cannot always benefit the downstream tasks. When the knowledge is transferred from a less relevant source, it may not improve the performance or even negatively affect the intended outcome [24]. A brute-force method is to fine-tune a set of pretrained models with target datasets to find the optimal one. This process is timeconsuming and laborious. Existing methods also measured the task-relatedness between source and target datasets [6,7,22,28]. However, most of these works require source information available while medical images have more privacy and ethical issues and fewer datasets are publicly available than natural images.Considering the issues mentioned above, this work focused on source-free pre-trained model selection for segmentation tasks in the medical image. As shown in Fig. 1, models pre-trained by upstream data constitute the model bank. The main idea is to directly measure the transferability of the pre-trained models without fully training based on the downstream/target dataset. Among the recent works, LEEP [15] and its variant [1,13] were developed to utilize the loglikelihood between the target labels and the predictions from the source model. LogME [27] computed evidence based on the linear parameters assumption and efficiently leverages the compatibility between features and labels. GBC [17] applied the Gaussian distribution to each class, and estimate the separability between classes as the basis for transferability estimation. TransRate [9] evaluated the transferability of models with the compactness and the completeness of embedding space. Cui et.al [5] contended that discriminability and transferability are crucial properties of representations and introduce the information bottleneck theory for transferability estimation. These methods have achieved promising performance on classification and regression tasks without fully considering the properties of medical image segmentation. First, unlike classification and regression problems that can use a single n-dimensional feature vector to represent each image, segmentation problems lack a global semantic representation, which poses difficulties for direct transferability estimation. In addition, most label-comparison-based methods [9,15,17,27] focus on the relationship between the embeddings and downstream labels without exploring the effectiveness of the features themselves. Third, medical images face severe class imbalance problems, with excessive differences between foreground and background. However, existing algorithms rarely give additional attention to the class imbalance problem. Besides, for semantic segmentation tasks, the feature pyramid is critical for the segmentation output of multi-scale objects while existing works neglect it.In our work, we propose a new method using class consistency and feature variety(CC-FV) with an efficient framework to estimate the transferability in medical image segmentation tasks. Class consistency employs the distribution of features extracted from foreground voxels of the same category in each sample to model and calculate their distance, the smaller the distance the better the result; feature diversity utilizes features sampled in the whole global feature map, and the uniformity of the feature distribution obtained by sampling is used to measure the effectiveness of the features themselves. Extensive experiments have proved the superiority of our method compared with baseline methods.2 Methodology"
Pick the Best Pre-trained Model: Towards Transferability Estimation for Medical Image Segmentation,2.1,Problem Formulation,"In our work, a model bank M consisting of pre-trained models {M i } K i=1 are available to be fine-tuned and evaluated with a target dataset, where X j is the image and Y j is the ground truth of segmentation. After fine-tuning, the performance of M i can be measured with the segmentation metric (e.g. Dice score), which is denoted by P i s→t in this paper. Our work is to directly estimate the transferability score T i s→t without fine-tuning the model on target datasets. A perfect transferability score should preserve the ordering, i.e. T i s→t > T j s→t if and only if P i s→t > P j s→t ."
Pick the Best Pre-trained Model: Towards Transferability Estimation for Medical Image Segmentation,2.2,Class Consistency with Feature Variety Constraint TE Method,"The transferability of models from a weakly related source domain to a target domain can be compromised if the domains are not sufficiently comparable [24]. This intrigues us about the question of ""what kind of models are transferable"". The proposed method is intuitive and straightforward: features extracted by the pre-trained model should be consistent within the class of the target dataset while representative and various globally. Therefore, Class Consistency and Feature Variety are considered to estimate the transferability between models and downstream data.Class Consistency. The pre-trained models are trained with specific pretext tasks based on the upstream dataset. Therefore, features extracted by the pretrained models cannot perfectly distinguish the foreground and background of target data. If the features are generalizable, foreground region features will likely follow a similar distribution even without fine-tuning. Given a pair of target data X j and X j , the distribution of the features is modeled with the n-dimensional Gaussian distribution. Since the size of the foreground class varies across the cases, we therefore randomly sample the pixels/voxels of X j and X j for each class and establish the feature distribution F k j , F k j based on the voxels of the k th class to approximate the case-wise distribution of different classes. The class consistency between the data pair is measured by the Wasserstein distance [16] as follows:whereare covariance matrices of F k j and F k j . Compared to some commonly used metrics like KL-divergence or Bhattacharyya distance [17], Wasserstein distance is more stable during the computation of high-dimensional matrices because it is unnecessary to compute the determinant or inverse of a high-dimensional matrix, which can easily lead to an overflow in numerical computation. We calculate the Wasserstein distance of the distribution with voxels of the same class in a sample pair comprised of every two samples in the dataset, and obtained the following definition of class consistency C consGiven that 3D medical images are computationally intensive, and prone to causing out-of-memory problems, in the sliding window inference process for each case, we do not concatenate the output of each patch into the final prediction result, but directly sample from the patched output and concatenate them into the final sampled feature matrix. In the calculation of class consistency, we only sample the foreground voxels with a pre-defined sampling number which is proportional to the voxel number of each class in the image because of the severe class imbalance problem.Feature Variety. Class consistency is not the only criterion for transferability estimation. As a result of learning some trivial solutions, some overfitted models have limited generalization capacity and are difficult to apply to new tasks. We believe that the essential reason for this phenomenon is that class consistency is only concerned with local homogeneity of information while neglecting the integral feature quality assessment. Hence we propose the feature variety constraint, which measures the expressiveness of the features themselves and the uniformity of their probability distribution. Highly complex features are not easily overfitted in the downstream tasks and do not collapse to cause a trivial solution.To calculate the variety of features we need to analytically measure the properties of the feature distribution over the full feature space. Besides, to prevent overfitting and trivial features, we expect the distribution of features in the feature space to be as uniform and dispersed as possible. Therefore we employ the following hyperspherical potential energy E s asHere v is sampled feature of each image with point-wise embedding v i and L is the length of the feature, which is also the number of sampled voxels. We randomly sample from the whole case so that the features can better express the overall representational power of the model. The feature vectors will be more widely dispersed in the unit sphere if the hyperspherical energy (HSE) is lower [3]. For the dataset with N cases, we choose s = 1 and the feature variety F v is formulated asOverall Estimation. As for semantic segmentation problems, the feature pyramid structure is critical for segmentation results [14,29]. Hence in our framework, different decoders' outputs are upsampled to the size of the output and can be used in the sliding window sampling process. Besides, we decrease the sampling ratio in the decoder layer close to the bottleneck to avoid feature redundancy.The final transferability of pre-trained model m to dataset t T m→t iswhere D is the number of decoder layers used in the estimation."
Pick the Best Pre-trained Model: Towards Transferability Estimation for Medical Image Segmentation,3,Experiment,
Pick the Best Pre-trained Model: Towards Transferability Estimation for Medical Image Segmentation,3.1,Experiment on MSD Dataset,"The Medical Segmentation Decathlon (MSD) [2] dataset is composed of ten different datasets with various challenging characteristics, which are widely used in the medical image analysis field. To evaluate the effectiveness of CC-FV, we conduct extensive experiments on 5 of the MSD dataset, including Task03 Liver(liver and tumor segmentation), Task06 Lung(lung nodule segmentation), Task07 Pancreas(pancreas and pancreas tumor segmentation), Task09 Spleen(spleen segmentation), and Task10 Colon(colon cancer segmentation). All of the datasets are 3D CT images. The public part of the MSD dataset is chosen for our experiments, and each dataset is divided into a training set and a test set at a scale of 80% and 20%. For each dataset, we use the other four datasets to pre-train the model and fine-tune the model on this dataset to evaluate the performance as well as the transferability using the correlation between two ranking sequences of upstream pre-trained models. We load all the pre-trained models' parameters except for the last convolutional layer and no parameters are frozen during the fine-tuning process. On top of that, we follow the nnUNet [11] with the selfconfiguring method to choose the pre-processing, training, and post-processing strategy. For fair comparisons, the baseline methods including TransRate [9], LogME [27], GBC [17] and LEEP [15] are also implemented. For these currently available methods, we employ the output of the layer before the final convolution as the feature map and sample it through the same sliding window as CC-FV to obtain different classes of features, which can be used for the calculation. Figure 2 visualizes the average Dice score and the estimation value on Task 03 Liver. The TE results are obtained from the training set only. U-Net [20] and UNETR [8] are applied in the experiment and each model is pre-trained for 250k iterations and fine-tuned for 100k iterations with batch size 2 on a single NVIDIA A100 GPU. Besides, we use the model at the end of training for inference and calculate the final DSC performance on the test set. And we use weighted Kendall's τ [27] and Pearson correlation coefficient for the correlation between the TE results and fine-tuning performance. The Kendall's τ ranges from [-1, 1], and τ=1 means the rank of TE results and performance are perfectly correlated(T i s→t > T j s→t if and only if P i s→t > P j s→t ). Since model selection generally picks the top models and ignores the poor performers, we assign a higher weight to the good models in the calculation, known as weighted Kendall's τ. The Pearson coefficient also ranges from [-1, 1], and measures how well the data can be described by a linear equation. The higher the Pearson coefficient, the higher the correlation between the variables. It is clear that the TE results of our method have a more positive correlation with respect to DSC performance.Table 1 demonstrates that our method surpasses all the other methods. Most of the existing methods are inferior to ours because they are not designed for segmentation tasks with a serious class imbalance problem. Besides, these methods rely only on single-layer features and do not make good use of the hierarchical structure of the model.  2. Correlation between the fine-tuning performance and transferability metrics using Task03 as an example. The vertical axis represents the average Dice of the model, while the horizontal axis represents the transferability metric results. We have standardized the various metrics uniformly, aiming to observe a positive relationship between higher performance and higher transferability estimations.   "
Pick the Best Pre-trained Model: Towards Transferability Estimation for Medical Image Segmentation,3.2,Ablation Study,"In Table 2 we analyze the different parts of our method and compare some other methods. First, we analyze the impact of class consistency C cons and feature variety F v . Though F v can not contribute to the final Kendall's τ directly, C cons with the constraint of F v promotes the total estimation result. Then we compare the performance of our method at single and multiple scales to prove the effectiveness of our multi-scale strategy. Finally, we change the distance metrics in class consistency estimation. KL-divergence and Bha-distance are unstable in high dimension matrics calculation and the performance is also inferior to the Wasserstein distance. Figure 3 visualize the distribution of different classes using t-SNE methods. We can easily find that with models with a pre-training process have a more compact intra-class distance and a higher fine-tuning performance."
Pick the Best Pre-trained Model: Towards Transferability Estimation for Medical Image Segmentation,4,Conclusion,"In our work, we raise the problem of model selection for upstream and downstream transfer processes in the medical image segmentation task and analyze the practical implications of this problem. In addition, due to the ethical and privacy issues inherent in medical care and the computational load of 3D image segmentation tasks, we design a generic framework for the task and propose a transferability estimation method based on class consistency with feature variety constraint, which outperforms existing model transferability estimation methods as demonstrated by extensive experiments."
Pick the Best Pre-trained Model: Towards Transferability Estimation for Medical Image Segmentation,,Fig. 1 .,
Pick the Best Pre-trained Model: Towards Transferability Estimation for Medical Image Segmentation,,Fig.,
Pick the Best Pre-trained Model: Towards Transferability Estimation for Medical Image Segmentation,,Fig. 3 .,
Pick the Best Pre-trained Model: Towards Transferability Estimation for Medical Image Segmentation,,Table 1 .,
Pick the Best Pre-trained Model: Towards Transferability Estimation for Medical Image Segmentation,,Table 2 .,
Pick the Best Pre-trained Model: Towards Transferability Estimation for Medical Image Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0_64.
Modeling Alzheimers’ Disease Progression from Multi-task and Self-supervised Learning Perspective with Brain Networks,1,Introduction,"Alzheimer's disease (AD) is a progressive neurodegenerative disease, which affects the quality of life as it causes memory loss, difficulty in thinking and learning [12,19,21,23]. Establishing relationships between brain networks and cognitive scores plays a vital role in identifying the early stage of AD [15,17]. Though there has been substantial progress in AD diagnostics with brain networks [1,3,5,14], most of the current studies focus on a single time point, without exploring longitudinal modeling for disease progression with brain networks. Some learning-based methods are proposed for the longitudinal prediction of AD progression with multi-modal data but generally fail in utilizing brain networks due to the large heterogeneity of brain networks between individuals as well as developmental stages [2,6,8,20].The cognitive scores prediction with longitudinal brain networks via deep learning models faces many challenges as follows: (i) The available longitudinal brain networks are scarce due to few volunteers or subject dropout [10]. Predicting cognitive scores with limited data is extremely challenging for the deep learning model training. (ii) Longitudinal brain networks provide rich structure information and disease progression characteristics, accounting for poor generalization for the pure supervised learning due to the insufficient supervision. (iii) The relationship between the brain networks and cognitive scores at multiple time points is varied, hindering the accurate prediction performance at multiple time points with a single task model.To cope with the above challenges, we propose a self-supervised multi-task learning paradigm for AD progression modeling with longitudinal brain networks. The proposed paradigm consists of a self-supervised spatio-temporal representation learning module for exploiting the spatio-temporal characteristics of longitudinal brain networks and a temporal multi-task module for modeling the relationship among cognitive scores prediction tasks at multiple time points. In summary, our contributions are threefold: 1) To the best of our knowledge, our work is the first attempt to predict cognitive scores with longitudinal brain networks through a self-supervised multi-task paradigm. 2) We design a self-supervised spatio-temporal representation learning module (SSTR), involving masked graph auto-encoder and temporal contrastive learning are jointly pre-trained to capture the structural and evolutional features of longitudinal brain networks simultaneously. The SSTR module can lead to more robust high-level representations for longitudinal brain networks. 3) We assume that inherent correlations exist among the prediction tasks at multiple future time points. Consequently, we propose a temporal multi-task learning paradigm to assist multiple time points cognitive scores prediction, which enhances the model generalization by exploiting the commonalities and differences among different prediction tasks when limited data is available. "
Modeling Alzheimers’ Disease Progression from Multi-task and Self-supervised Learning Perspective with Brain Networks,2,Method,
Modeling Alzheimers’ Disease Progression from Multi-task and Self-supervised Learning Perspective with Brain Networks,2.1,Problem Formalization,"The input to the proposed model is a set of N subjects, each of which has T longitudinal brain networks. Let ] and Y T +k,p i is the p-th cognitive score of subject i at time T + k. As illustrated in Fig. 1, our aim is to build a model f to predict cognitive scores at time [T + 1, ..., T + k] with brain networks at time [1, ..., T ], which is formulated as:"
Modeling Alzheimers’ Disease Progression from Multi-task and Self-supervised Learning Perspective with Brain Networks,2.2,Overview,"The overview of the proposed SMP-Net is shown in Fig. 2. As shown in Fig. 2, the proposed SMP-Net consists of two modules: Self-supervised spatio-temporal representation learning module (SSTR) for exploiting the spatio-temporal characteristics of longitudinal brain network data itself and a temporal multi-task learning module for modeling the relationship among cognitive scores prediction tasks at multiple time points. SSTR involves a masked graph auto-encoder and a temporal contrastive learning, both of which are jointly pre-trained to learn the structural and evolutional brain networks representation."
Modeling Alzheimers’ Disease Progression from Multi-task and Self-supervised Learning Perspective with Brain Networks,2.3,Self-supervised Spatio-Temporal Representation Learning,"Although brain networks provide rich structure information, the pure supervised learning scheme limits the representation capacity of the models due to insufficient supervision. To solve this problem, we introduce the self-supervised spatio-temporal representation learning module, SSTR. The procedure of SSTR involves two stages as follow: "
Modeling Alzheimers’ Disease Progression from Multi-task and Self-supervised Learning Perspective with Brain Networks,,Stage 1 Masked Graph Auto-encoder for Graph Reconstruction.,"In stage 1, a masked graph auto-encoder, containing a topology-aware encoder and a decoder, is designed to exploit the crucial structural information in brain networks. To sufficiently exploit the graph structure, we randomly mask some nodes and the associated edges. The unmasked nodes and edges fed into the topologyaware encode to learn the latent representations. Let H u indicate the feature map in the encoding stage. We define the adjacent matrix of the unmasked nodes as A u , which is taken as the input of the topology-aware encoder, that isThe topology-aware encoder consists of three parts: 1) The edge convolution with multiple cross-shaped filters for capturing the locality in the graph according towhere w r ∈ R 1×M and w c ∈ R M ×1 are convolution kernels. 2) The node convolution for learning the latent node embedding. It is defined as:n , where w n is the learned filter vector,n ∈ R M ×Dn is the latent unmasked node embedding and D n is the channels in NC.3) The graph aggregation for achieving the global graph embedding through:where w g is the learned filter vector, H g ∈ R M ×Dg is the graph embedding and D g is the dimensionality in GA.The decoder takes the masked nodes and the latent unmasked node embeddings as inputs, and then produces predictions for the masked nodes and edges by graph convolution operations and the masked edge prediction. The graph convolution is defined as: (l) , where A ∈ R M ×M is the binary adjacency matrix, W denotes trainable weight, H ∈ R M ×D n is the node embedding and D n is the hidden layer size of graph convolutional layers. The masked edge prediction is defined as: Â = H (l+1) (H (l+1) )T . The reconstruction loss between the prediction graphs and corresponding targets is, where Ât i is the reconstructed brain networks of subject i at time t.Stage 2 Temporal Contrastive Learning. The longitudinal brain networks of a subject acquired at multiple visits characterize gradual disease progression of the brain over time, which manifests a temporal progression trajectory when projected to the latent space. We assume that brain networks features at two consecutive time points from the same subject are similar, while dissimilar from different subjects. Based on this assumption, we introduce a temporal contrastive loss by enforcing an across-sample relationship in the learning process. Specifically, H t g(i) is the brain network features of subject i at time t, H t g(i) and H t+1 g(j)are considered as the positive sample pair if i = j, otherwise they are considered as the negative sample pair. The temporal contrastive framework aims to enlarge the similarity between positive sample pair, and reduce it between the negative sample pair. The similarity calculation function s can be any distance function, and here we utilize cosine similarity. The loss for temporal contrastive learning can be represented as:where τ is a temperature factor that controls the model's discrimination against negative sample pair and exp(.) is an exponential function."
Modeling Alzheimers’ Disease Progression from Multi-task and Self-supervised Learning Perspective with Brain Networks,2.4,Temporal Multi-task Learning,"Existing studies have demonstrated the effectiveness of multi-task learning for the extraction of a robust feature representation [9,24]. In this regard, to further exploit the correlation among the prediction tasks at multiple future time points, we design a temporal multi-task learning paradigm. Specifically, the temporal multi-task learning module consists of a shared network and multiple taskspecific networks, all of which are designed with a Long Short-Term Memory (LSTM) architecture [4]. The shared network is trained for modeling the shared information h t s among cognitive scores prediction tasks at multiple time points. The q-th task-specific network aims to capture the task-specific information h t q from the shared network and the brain networks features at time t. The temporal multi-task learning module can be seen as an end-to-end architecture with the shared and task-specific parameters of W s , W q . By learning these parameters jointly, we arrive at a collaborative learning method to jointly improve the performance of the prediction tasks at multiple time points. The shared information h t s and task-specific information h t q are formulated as h t s = LST M (Hg t , h t-1 s , W s ) and h t q = LST M ([Hg t , h t s ], h t-1 q , W q ). The output of the temporal multi-task learning module is formulated as:where W 1 , W 2 , b 1 , b 2 are learnable parameters of LSTM. Errors between the actual observations Y t and predictions Ŷ t are used to update the model parameters through the regression loss as follow:The overall loss function L is described as Eq. ( 3), where λ con and λ rec are the weights for contrastive loss and reconstruction loss, respectively.3 Experiments"
Modeling Alzheimers’ Disease Progression from Multi-task and Self-supervised Learning Perspective with Brain Networks,3.1,Dataset and Experimental Settings,"In this work, we choose 219 longitudinal resting-state fMRI scans of 73 subjects from the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset [11] 1 . AAL template is used to obtain 90 ROIs for every subject [18]. We predict nine cognitive scores at time M24, M36 and M48 with brain networks times of M0, M6 and M12 to evaluate our proposed SMP-Net. The number of samples for the three tasks are 73, 35 and 31, respectively. During the model training, the Adam optimizer is used with a momentum of 0.9 and a weight decay of 0.01. The learning rate is set to 10 -3 . The hidden layer size of LSTM and graph convolutional layers are set to 64 and 48, respectively. The values of hyperparameter λ c and λ r are set to 1. The model is trained with 20 epochs in the self-supervised spatio-temporal representation learning stage and 300 epochs in the temporal multi-task learning stage with a batch size of 16. To avoid over-fitting due to the limited subjects, in all experiments, we repeat the 5-fold cross-validation 10 times with different random seeds. We finally report the average results. Three commonly used metrics are adopted to evaluate all methods, including Mean Absolute Error (MAE), Pearson Correlation Coefficient (PCC) and Concordance Correlation Coefficient (CCC). CCC reflects both the correlation and the absolute error between the true and the predicted cognitive scores. Due to limited space, we report the results in terms of CCC in this paper. The results in terms of MAE and CC are shown in the supplementary material. To ensure a fair comparison, the hyperparameters of comparable methods are optimized to achieve their best performance."
Modeling Alzheimers’ Disease Progression from Multi-task and Self-supervised Learning Perspective with Brain Networks,3.2,Effectiveness Evaluation,"We compare the performance of our SMP-Net with three state-of-the-art (SOTA) sequential graph learning methods: evolveGCN [13], stGCN [22] and DySAT [16] as well as a baseline method: GCN [7]. Table 1 summarizes the results of all 3) The temporal multi-task paradigm of SMP-Net effectively exploits the inherent correlation among multiple prediction tasks at different time points, which facilitate to improve the model performance."
Modeling Alzheimers’ Disease Progression from Multi-task and Self-supervised Learning Perspective with Brain Networks,3.3,Discussion,"Ablation Analysis. To valid the effect of each proposed module, we consider the following variants for evaluation: 1) SMP-Net-c: the temporal contrastive loss is removed; 2) SMP-Net-r: the reconstruction loss is removed; 3) SMP-Netrc: both temporal contrastive loss and graph reconstruction loss are removed; 4) SMP-Net-m: the temporal multi-task paradigm is ignored.  indicating the effectiveness of the temporal multi-task paradigm. It also indicates that the multi-task paradigm in SMP-Net is more helpful for the prediction at farther time points. The reason is that prediction tasks at farther time points are more difficult due to the insignificant relationship between the brain networks and the cognitive scores. Temporal multi-task paradigm enforces the long-term prediction to benefit from short-term prediction, making the prediction tasks at farther time points gain more improvements. Moreover, we can observe that models with SSTR perform better than the ones without SSTR. For instance, SMP-Net-m and SMP-Net show superior performance than SMP-Net-r, SMP-Net-c and SMP-Net-rc. This demonstrates that SSTR facilitates the learning of structural and evolutional features in the condition of limited samples and insufficient supervision, thereby leading to more robust high-level representations for downstream tasks.Evaluating Robustness. To evaluate the robustness of the SSTR module, we pre-train SMP-Net with fMRI at three time points (M0, M6, M12) and fine-tune it with different downstream tasks of predicting cognitive scores at different time points. As shown in Fig. 3, MSP-Net provides comparatively stable performance on different fine-tuning tasks, demonstrating that features learned with our pretrained model are robust to the different fine-tuning tasks. "
Modeling Alzheimers’ Disease Progression from Multi-task and Self-supervised Learning Perspective with Brain Networks,4,Conclusion,"This paper proposes an AD progression model SMP-Net from multi-task and self-supervised learning perspective with longitudinal brain networks. In the proposed SMP-Net, self-supervised spatio-temporal representation learning is designed to learn more robust structural and evolutional features from longitudinal brain networks. The temporal multi-task paradigm is designed for boosting the ability of cognitive score prediction at multiple time points. Experimental results on the ADNI dataset with fewer samples demonstrate the advantage of self-supervised spatio-temporal representation learning and temporal multi-task learning."
Modeling Alzheimers’ Disease Progression from Multi-task and Self-supervised Learning Perspective with Brain Networks,,Fig. 1 .,
Modeling Alzheimers’ Disease Progression from Multi-task and Self-supervised Learning Perspective with Brain Networks,,Fig. 2 .,
Modeling Alzheimers’ Disease Progression from Multi-task and Self-supervised Learning Perspective with Brain Networks,,Fig. 3 .,
Modeling Alzheimers’ Disease Progression from Multi-task and Self-supervised Learning Perspective with Brain Networks,,Table 1 .,
Modeling Alzheimers’ Disease Progression from Multi-task and Self-supervised Learning Perspective with Brain Networks,,Table 2,"summarizes the results of ablation studies in terms of CCC. It is apparent that SMP-Net outperforms all of the variants. Specifically, SMP-Net consistently outperforms SMPT-Net-m 11.0%, 12.7% and 23.8% at time M24, M36 and M48, respectively,"
Modeling Alzheimers’ Disease Progression from Multi-task and Self-supervised Learning Perspective with Brain Networks,,Table 2 .,
Modeling Alzheimers’ Disease Progression from Multi-task and Self-supervised Learning Perspective with Brain Networks,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0 30.
vox2vec: A Framework for Self-supervised Contrastive Learning of Voxel-Level Representations in Medical Images,1,Introduction,"Medical image segmentation often relies on supervised model training [14], but this approach has limitations. Firstly, it requires costly manual annotations.Secondly, the resulting models may not generalize well to unseen data domains. Even small changes in the task may result in a significant drop in performance, requiring re-training from scratch [18].Self-supervised learning (SSL) is a promising solution to these limitations. SSL pre-trains a model backbone to extract informative representations from unlabeled data. Then, a simple linear or non-linear head on top of the frozen pre-trained backbone can be trained for various downstream tasks in a supervised manner (linear or non-linear probing). Alternatively, the backbone can be finetuned for a downstream task along with the head. Pre-training the backbone in a self-supervised manner enables scaling to larger datasets across multiple data and task domains. In medical imaging, this is particularly useful given the growing number of available datasets.In this work, we focus on contrastive learning [8,12], one of the most effective approaches to SSL in computer vision. In contrastive learning, the model is trained to produce similar vector representations for augmented views of the same image and dissimilar representations for different images. Contrastive methods can also be used to learn dense, i.e., patch-level or even pixel-or voxellevel representations: pixels of augmented image views from the same region of the original image should have similar representations, while different pixels should have dissimilar ones [23].Several works have implemented contrastive learning of dense representations in medical imaging [2,7,25,26,29]. Representations in [7,25] do not resolve nearby voxels due to the negative sampling strategy and the architectural reasons. This makes them unsuitable for full-resolution segmentation, especially in linear and non-linear probing regimes. In the current SotA dense SSL methods [2,26], authors employ restorative learning in addition to patch-level contrastive learning, in order to pre-train voxel-level representations in full-resolution. In [29], separate global and voxel-wise representations are learned in a contrastive manner to implement efficient dense image retrieval.The common weakness of all the above works is that they do not evaluate their SSL models in linear or non-linear probing setups, even though these setups are de-facto standards for evaluation of SSL methods in natural images [8,13,23]. Moreover, fine-tuned models can deviate drastically from their pre-trained states due to catastrophical forgetting [11], while models trained in linear or non-linear probing regimes are more robust as they have several orders of magnitude fewer trainable parameters.Our contributions are threefold. First, we propose vox2vec, a framework for contrastive learning of voxel-level representations. Our simple negative sampling strategy and the idea of storing voxel-level representations in a feature pyramid form result in high-dimensional, fine-grained, multi-scale representations suitable for the segmentation of different organs and tumors in full resolution. Second, we employ vox2vec to pre-train a FPN architecture on a diverse collection of six unannotated datasets, totaling over 6,500 CT images of the thorax and abdomen. We make the pre-trained model publicly available to simplify the reproduction of our results and to encourage practitioners to utilize this model as a starting point for the segmentation algorithms training. Finally, we compare the pretrained model with the baselines on 22 segmentation tasks on seven CT datasets in three setups: linear probing, non-linear probing, and fine-tuning. We show that vox2vec performs slightly better than SotA models in the fine-tuning setup and outperforms them by a huge margin in the linear and non-linear probing setups. To the best of our knowledge, this is the first successful attempt to evaluate dense SSL methods in the medical imaging domain in linear and non-linear probing regimes."
vox2vec: A Framework for Self-supervised Contrastive Learning of Voxel-Level Representations in Medical Images,2,Related Work,"In recent years, self-supervised learning in computer vision has evolved from simple pretext tasks like Jigsaw Puzzles [22], Rotation Prediction [17], and Patch Position Prediction [10] to the current SotA methods such as restorative autoencoders [13] and contrastive [8] or non-contrastive [9] joint embedding methods.Several methods produce dense or pixel-wise vector representations [6,23,28] to pre-train models for downstream tasks like segmentation or object detection. In [23], pixel-wise representations are learned by forcing local features to remain constant over different viewing conditions. This means that matching regions describing the same location of the scene on different views should be positive pairs, while non-matching regions should be negative pairs. In [28], authors define positive and negative pairs as spatially close and distant pixels, respectively. While in [6], authors minimize the mean square distance between matched pixel embeddings, simultaneously preserving the embedding variance along the batch and decorrelating different embedding vector components.The methods initially proposed for natural images are often used to pretrain models on medical images. In [25], authors propose the 3D adaptation of Jigsaw Puzzle, Rotation Prediction, Patch Position Prediction, and image-level contrastive learning. Another common way for pre-training on medical images is to combine different approaches such as rotation prediction [26], restorative autoencoders [2,26], and image-level contrastive learning [2,26].Several methods allows to obtain voxel-wise features. The model [29] maximizes the consistency of local features in the intersection between two differently augmented images. The algorithm [29] was mainly proposed for image retrieval and uses only feature representations in the largest and smallest scales in separate contrastive losses, while vox2vec produce voxels' representations via concatenation of feature vectors from a feature pyramid and pre-train them in a unified manner using a single contrastive loss. Finally, a number of works propose semisupervised contrastive learning methods [20], however, they require additional task-specific manual labeling."
vox2vec: A Framework for Self-supervised Contrastive Learning of Voxel-Level Representations in Medical Images,3,Method,"In a nutshell, vox2vec pre-trains a neural network to produce similar representations for the same voxel placed in different contexts (positive pairs) and predict distinctive representations for different voxels (negative pairs). In the following Sects. 3.1, 3.2, 3.3, we describe in detail the main components of our method: 1) definition and sampling of positive and negative pairs of voxels; 2) modeling voxel-level representations via a neural network; 3) computation of the contrastive loss. The whole pre-training pipeline is schematically illustrated in Fig. 1. We also describe the methodology of the evaluation of the pre-trained representations on downstream segmentation tasks in Sect. 3.4."
vox2vec: A Framework for Self-supervised Contrastive Learning of Voxel-Level Representations in Medical Images,3.1,Sampling of Positive and Negative Pairs,"We define a positive pair as any pair of voxels that correspond to the same location in a given volume. Conversely, we call a negative pair any pair of voxels that correspond to different locations in the same volume as well as voxels belonging to different volumes. Figure 1 (left) illustrates our strategy for positive and negative pairs sampling. For a given volume, we sample two overlapping 3D patches of size (H, W, D). We apply color augmentations to them, including random gaussian blur, random gaussian sharpening, adding random gaussian noise, clipping the intensities to the random Hounsfield window, and rescaling them to the (0, 1) interval. Next, we sample m different positions from the patches' overlapping region. Each position yields a pair of voxels -one from each patch, which results in a total of m positive pairs of voxels. At each pre-training iteration, we repeat this procedure for n different volumes, resulting in 2•n patches containing N = n • m positive pairs. Thus, each sampled voxel has one positive counterpart and forms negative pairs with all the remaining 2N -2 voxels.In our experiments we set (H, W, D) = (128, 128, 32), n = 10 and m = 1000.We exclude the background voxels from the sampling and do not penalize their representations. We obtain the background voxels by using a simple twostep algorithm: 1) thresholding voxels with an intensity less than -500 HU; 2) keep voxels from the same connected component as the corner voxel of the CT volume, using a flood fill algorithm."
vox2vec: A Framework for Self-supervised Contrastive Learning of Voxel-Level Representations in Medical Images,3.2,Architecture,"A standard architecture for voxel-wise prediction is 3D UNet [24]. UNet's backbone returns a feature map of the same resolution as the input patch. However, our experiments show that this feature map alone is insufficient for modeling self-supervised voxel-level representations. The reason is that producing a feature map with more than 100 channels in full resolution is infeasible due to memory constraints. Meanwhile, to be suitable for many downstream tasks, representations should have a dimensionality of about 1000, as in [8].To address this issue, we utilize a 3D FPN architecture instead of a standard 3D UNet. FPN returns voxel-level representations in the form of a feature pyramid. The pyramid's base is a feature map with 16 channels of the same resolution as the input patch. Each next pyramid level has twice as many channels and two times lower resolution than the previous one. Each voxel's representation is a concatenation of the corresponding feature vectors from all the pyramid levels. We use FPN with six pyramid levels, which results in 1008-dimensional representations. See Fig. 1 (right) for an illustration."
vox2vec: A Framework for Self-supervised Contrastive Learning of Voxel-Level Representations in Medical Images,3.3,Loss Function,"At each pre-training iteration, we fed 2 • n patches to the FPN and obtain the representations for N positive pairs of voxels. We denote the representations in i-th positive pair as h (1) i and h (2) i , i = 1, . . . , N. Following [8], instead of penalizing the representations directly, we project them on 128-dimensional unit sphere via a trainable 3-layer perceptron g(•) followed by l2-normalization: zi ) , i = 1, . . . , N. Similar to [8] we use the InfoNCE loss as a contrastive objective:where"
vox2vec: A Framework for Self-supervised Contrastive Learning of Voxel-Level Representations in Medical Images,3.4,Evaluation Protocol,"We evaluate the quality of self-supervised voxel-level representations on downstream segmentation tasks in three setups: 1) linear probing, 2) non-linear probing, and 3) end-to-end fine-tuning. Linear or non-linear probing means training a voxel-wise linear or non-linear classifier on top of the frozen representations. If the representations are modeled by the UNet model, such classifier can be implemented as one or several 1 × 1 convolutional layers with a kernel size 1 on top of the output feature map. A linear voxel-wise head (linear FPN head) can be implemented as follows. Each pyramid level is separately fed to its own convolutional layer with kernel size 1. Then, as the number of channels on all pyramid levels has decreased, they can be upsampled to the full resolution and summed up. This operation is equivalent to applying a linear classifier to FPN voxel-wise representations described in Sect. 3.2. Linear FPN head has four orders of magnitude fewer parameters than FPN. The architecture of the non-linear voxel-wise head replicates the UNet's decoder but sets the kernel size of all convolutions to 1. It has 50 times fewer parameters than the entire FPN architecture.In the end-to-end fine-tuning setup, we attach the voxel-wise non-linear head, but in contrast to the non-linear probing regime, we also train the backbone."
vox2vec: A Framework for Self-supervised Contrastive Learning of Voxel-Level Representations in Medical Images,4,Experiments,
vox2vec: A Framework for Self-supervised Contrastive Learning of Voxel-Level Representations in Medical Images,4.1,Pre-training,"We use vox2vec to pre-train both FPN and UNet models (further vox2vec-FPN and vox2vec-UNet) in order to ablate the effect of using a feature pyramid instead of single full-resolution feature map for modeling voxel-wise representations. For pre-training, we use 6 public CT datasets [1,3,5,15,21,27], totaling more than 6550 CTs, covering abdomen and thorax domains. We do not use the annotations for these datasets during the pre-training stage. Pre-processing includes the following steps: 1) cropping to the minimal volume containing all the voxels with the intensity greater than -500 HU; 2) interpolation to the voxel spacing of 1 × 1 × 2 mm 3 (intensities are clipped and rescaled at the augmentation step, see Sect. 3.1). We pre-train both models for 100K batches using the Adam optimizer [16] with a learning rate of 0.0003. Both models are trained on a single A100-40Gb GPU for an average of 3 days. Further details about the pre-training setup can be found in Supplementary materials."
vox2vec: A Framework for Self-supervised Contrastive Learning of Voxel-Level Representations in Medical Images,4.2,Evaluation,"We evaluate our method on the Beyond the Cranial Vault Abdomen (BTCV) [19] and Medical Segmentation Decathlon (MSD) [4] datasets. The BTCV dataset consists of 30 CT scans along with 13 different organ annotations. We test our method on 6 CT MSD datasets, which include 9 different organ and tumor segmentation tasks. A 5 fold cross-validation is used for BTCV experiments, and a 3 fold cross-validation for MSD experiments. The segmentation performance of each model on BTCV and MSD datasets is evaluated by the Dice score.For our method, the pre-processing steps are the same for all datasets, as at the pre-training stage, but in addition, intensities are clipped to (-1350, 1000) HU window and rescaled to (0, 1).We compare our results with the current state-of-the-art self-supervised methods [2,26] in medical imaging. The pre-trained weights for the SwinUNETR encoder and TransVW UNet are taken from the official repositories of corresponding papers. In these experiments, we keep the crucial pipeline hyperparameters (e.g., spacing, clipping window, patch size) the same as in the original works. To evaluate the pre-trained SwinUNETR and TransVW in linear and nonlinear probing setups, we use similar linear and non-linear head architectures as for vox2vec-FPN (see Sect. 3.4). SwinUNETR and TransVW cost 391 GFLOPs and 1.2 TFLOPS, correspondingly, compared to 115 GFLOPs of vox2vec-FPN.We train all models for 45000 batches of size 7 (batch size for SwinUNETR is set to 3 due to memory constraints), using the Adam optimizer with a learning rate of 0.0003. In the fine-tuning setup, we freeze the backbone for the first 15000 batches and then exponentially increase the learning rate for the backbone parameters from 0.00003 up to 0.0003 during 1200 batches."
vox2vec: A Framework for Self-supervised Contrastive Learning of Voxel-Level Representations in Medical Images,5,Results,"The mean value and standard deviation of Dice score across 5 folds on the BTCV dataset for all models in all evaluation setups are presented in Table 1. vox2vec-FPN performs slightly better than other models in the fine-tuning setup. However, considering the standard deviation, all the fine-tuned models perform on par with their counterparts trained from scratch.Nevertheless, vox2vec-FPN significantly outperforms other models in linear and non-linear regimes. On top of that, we observe that in non-linear probing regime, it performs (within the standard deviation) as well as the FPN trained from scratch while having x50 times fewer trainable parameters (see Fig. 2). We demonstrate an example of the excellent performance of vox2vec-FPN in both linear and non-linear probing regimes in Supplementary materials.We reproduce the key results on MSD challenge CT datasets, which contain tumor and organ segmentation tasks. Table 2 shows that in the vox2vec representation space, organ voxels can be separated from tumor voxels with a quality comparable to the model trained from scratch. A t-SNE embedding of vox2vec representations on MSD is available in the Supplementary materials. "
vox2vec: A Framework for Self-supervised Contrastive Learning of Voxel-Level Representations in Medical Images,6,Conclusion,"In this work, we present vox2vec -a self-supervised framework for voxel-wise representation learning in medical imaging. Our method expands the contrastive learning setup to the feature pyramid architecture allowing to pre-train effective representations in full resolution. By pre-training a FPN backbone to extract informative representations from unlabeled data, our method scales to large datasets across multiple task domains. We pre-train a FPN architecture on more than 6500 CT images and test it on various segmentation tasks, including different organs and tumors segmentation in three setups: linear probing, nonlinear probing, and fine-tuning. Our model outperformed existing methods in all regimes. Moreover, vox2vec establishes a new state-of-the-art result on the linear and non-linear probing scenarios. Still, this work has a few limitations to consider. We plan to investigate further how the performance of vox2vec scales with the increasing size of the pre-training dataset and the pre-trained architecture size. Another interesting research direction is exploring the effectiveness of vox2vec with regard to domain adaptation to address the challenges of domain shift between different medical imaging datasets obtained from different sources. A particular interest is a lowshot scenario when only a few examples from the target domain are available."
vox2vec: A Framework for Self-supervised Contrastive Learning of Voxel-Level Representations in Medical Images,,FPNFig. 1 .,
vox2vec: A Framework for Self-supervised Contrastive Learning of Voxel-Level Representations in Medical Images,,Table 1 .,Fig. 2. Dice score on BTCV crossvalidation averaged for all organs w.r.t. the number of trainable paramaters of different models in different evaluation setups.
vox2vec: A Framework for Self-supervised Contrastive Learning of Voxel-Level Representations in Medical Images,,Table 2 .,
vox2vec: A Framework for Self-supervised Contrastive Learning of Voxel-Level Representations in Medical Images,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0_58.
Multi-Target Domain Adaptation with Prompt Learning for Medical Image Segmentation,1,Introduction,"Deep learning has brought medical image segmentation into the era of datadriven approaches, and has made significant progress in this field [1,2], i.e., the segmentation accuracy has improved considerably. In spite of the huge success, the deployment of trained segmentation models is often severely impacted by a distribution shift between the training (or labeled) and test (or unlabeled) data since the segmentation performance will deteriorate greatly in such situations. Domain shift is typically caused by various factors, including differences in acquisition protocols (e.g., parameters, imaging methods, modalities) and characteristics of data (e.g., age, gender, the severity of the disease and so on).Domain adaptation (DA) has been proposed and investigated to combat distribution shift in medical image segmentation. Many researchers proposed using adversarial learning to tackle distribution shift problems [3][4][5][6][7]. These methods mainly use the game between the domain classifier and the feature extractor to learn domain-invariant features. However, they easily suffer from the balance between feature alignment and discrimination ability of the model. Some recent researchers begin to explore self-training based DA algorithms, which generate pseudo labels for the 'other' domain samples to fulfill self-training [8][9][10][11]. While it is very difficult to ensure the quality of pseudo labels in the 'other' domain and is also hard to build capable models with noise labels. However, most of these methods cannot well handle the situation when the domains are very diverse, since it is very challenging to learn domain-invariant features when each domain contains domain-specific knowledge. Also, the domain information itself is well utilized in the DA algorithms.To tackle the aforementioned issues, we propose utilizing prompt learning to take full advantage of domain information. Prompt learning [12,13] is a recently emergent strategy to extend the same natural language processing (NLP) model to different tasks without re-training. Prompt learning models can autonomously tune themselves for different tasks by transferring domain knowledge introduced through prompts, and they can usually demonstrate better generalization ability across many downstream tasks. very few works have attempted to apply prompt learning to the computer vision field, and have achieved promising results. [14] introduced prompt tuning as an efficient and effective alternative to full finetuning for large-scale Transformer models. [15] exploited prompt learning to fulfill domain generalization in image classification tasks. The prompts in these models are generated and used in the very early stage of the models, which prevents the smooth combination with other domain adaptation methods.In this paper, we introduce a domain prompt learning method (prompt-DA) to tackle distribution shift in multi-target domains. Different from the recent prompt learning methods, we generate domain-specific prompts in the encoding feature space instead of the image space. As a consequence, it can improve the quality of the domain prompts, more importantly, we can easily consolidate the prompt learning with the other DA methods, for instance, adversarial learning based DA. In addition, we propose a specially designed fusion module to reinforce the respective characteristics of the encoder features and domain-specific prompts, and thus generate domain-aware features. As a way to prove the prompt-DA is compatible with other DAs, a very simple adversarial learning module is jointly adopted in our method to further enhance the model's generalization ability (we denote this model as comb-DA). We evaluate our proposed method on two multi-domain datasets: 1). the infant brain MRI dataset for cross-age segmentation; 2). the BraTS2018 dataset for cross-grade tumor segmentation. Experiments show our proposed method outperforms state-of-the-art methods. Moreover, ablation study demonstrates the effectiveness of the proposed domain prompt learning and the feature fusion module. Our claim about the successful combination of prompt learning with adversarial learning is also well-supported by experiments."
Multi-Target Domain Adaptation with Prompt Learning for Medical Image Segmentation,2,Methodology,"Our proposed prompt-DA network consists of three main components as depicted in Fig. 1(a): a typical encoder-decoder network (e.g., UNet) serving the segmentation baseline, a prompt learning network to learn domain-specific prompts, and a fusion module aggregating the image features and domain-specific prompts to build domain-aware feature representation, where the fused features are fed into the decoder. It is worth noting that our proposed method is compatible with other DA algorithms, and thus we can add an optional extra DA module to further optimize the domain generalization ability, in this paper, we choose an adversarial learning based DA as an example since it is the mostly used DA methods in medical image segmentation (as introduced in Sect. 1).There are various encoder-decoder segmentation networks, many of which are well known. As a result, we donot introduce the details of the encoder-decoder and just choose two typical networks to work as the segmentation backbone, that is, 3D-UNet [16] (convolution based and 3D) and TransUNet [17] (transformer based and 2D). In the following subsections, our focus will be on domain-specific prompt generation and domain-aware feature learning with feature fusion."
Multi-Target Domain Adaptation with Prompt Learning for Medical Image Segmentation,2.1,Learning Domain-Specific Prompts,"In our designed prompt learning based DA method, it is essential to learn domain-specific prompts. Moreover, the quality of generated prompts directly determines the domain-aware features. Therefore, we specially designed a prompt generation module to learn domain-specific prompts which mainly consists of two components, i.e., a classifier and a prompt generator.Our approach incorporates domain-specific information into the prompts to guide the model in adapting to the target domain. To achieve this, we introduce a classifier h(x) that distinguishes the domain (denoted as d) of the input image, shown in Eq. 1.where x is the image or abstracted features from the encoder. To optimize the parameters, we adopt cross-entropy loss to train the classifier, as shown in Eq. 2.where d is the predicted domain information, and d is the ground truth domain information.Prompt Generation: Instead of directly using d as the category information, we fed the second-to-last layer's features (i.e., z) of the classifier to a prompt generation, namely, g(z). In particular, the g(z) is a multi-layer-perception, as defined in Eq. 3.where φ can be a Conv+BN+ReLU sequence. Note this module does not change the size of the feature map, instead, it transforms the extracted category features into domain-specific prompts."
Multi-Target Domain Adaptation with Prompt Learning for Medical Image Segmentation,2.2,Learning Domain-Aware Representation by Fusion,"The learned prompt captures clearly about a certain domain and the features from the encoder describe the semantics as well as spatial information for the images. We can combine them to adapt the image features to domain-aware representations.Basically, suppose we have an image denoted as I, and the prompt encodings for the domain knowledge is g(e(I)) (where e(I) is the features from a shallow layer), E(I) is the encoder features for this image. Then the domain-aware features (i.e., F ) are extracted by a fusion module as Eq. 4.As the learned prompt and encoder feature capture quite different aspects of the input data, we cannot achieve good effect by simply using addition, multiplication or concatenation to serve as the fusion function ψ. Specifically, while the encoder feature emphasizes spatial information for image segmentation, the prompt feature highlights inter-channel information for domain-related characteristics. To account for these differences, we propose a simple attention-based fusion (denoted as AFusion) module to smoothly aggregate the information. This module computes channel-wise and spatial-wise weights separately to enhance both the channel and spatial characteristics of the input. Figure 1(c) illustrates the structure of our proposed module.Our module utilizes both channel and spatial branches to obtain weights for two input sources. The spatial branch compresses the encoder feature in the channel dimension using an FC layer to obtain spatial weights. Meanwhile, the channel branch uses global average pooling and two FC layers to compress the prompt and obtain channel weights. We utilize FC layers for compression and rescaling, denoted as f cp and f re respectively. The spatial and channel weights are computed according to Eq. 5.Afterward, we combine the weights from the spatial and channel dimensions to obtain a token that can learn both high-level and low-level features from both the encoder feature and the prompt, which guides the fusion of the two features. The process is illustrated as follows:This module introduces only a few parameters, yet it can effectively improve the quality of the prompted domain-aware features after feature fusion. In the experimental section (i.e., Sect. 3.3), we conducted relevant experiments to verify that this module can indeed improve the performance of our prompt-DA method."
Multi-Target Domain Adaptation with Prompt Learning for Medical Image Segmentation,2.3,Adversarial Learning to Enhance the Generalization Ability,"As aforementioned, our proposed prompt-DA is fully compatible with other DA algorithms. We thus use adversarial learning, which is widely adopted in medical image DA, to work as an optional component in our network to continuously enhance the domain adaptation ability. Specially, inspired by the adversarial DA in [18], we adopt the classic GAN loss to train the discriminator and prompt generator (Note the adversarial loss, L adv , for the generator will only be propagated to the prompt generator)."
Multi-Target Domain Adaptation with Prompt Learning for Medical Image Segmentation,2.4,Total Loss,"To optimize the segmentation backbone network, we use a combined loss function, L seg , that incorporates both dice loss [19] and cross-entropy loss with a balance factor.By summing the above-introduced losses, the total loss to train the segmentation network can be defined by Eq. 7.where λ is the scaling factor to balance the losses."
Multi-Target Domain Adaptation with Prompt Learning for Medical Image Segmentation,2.5,Implementation Details,"We use basic 3D-UNet [16] or TransUnet [17] as our segmentation network. We use a fully convolutional neural network consisting of four convolutional layers with 3 × 3 kernels and stride of 1 as the domain classifier, with each convolution layer followed by a ReLU parameterized by 0.2. We used three convolutional layers with ReLU activation function as the Prompt Generator and constructed a Discriminator with a similar structure to the Classifier. We adopt Adam as the optimizer and set the learning rate to 0.0002 and 0.002 for the segmentation and domain classifier, respectively. The learning rate will be decayed by 0.1 every quarter of the training process."
Multi-Target Domain Adaptation with Prompt Learning for Medical Image Segmentation,3,Experiments and Results,
Multi-Target Domain Adaptation with Prompt Learning for Medical Image Segmentation,3.1,Datasets,"Our proposed method was evaluated using two medical image segmentation DA datasets. The first dataset, i.e., cross-age infant segmentation [20], was used for cross-age infant brain image segmentation, while the second dataset, i.e., Brats2018 [21], was used for HGG to LGG domain adaptation.The first dataset is for infant brain segmentation (white matter, gray matter and cerebrospinal fluid). To build the cross-age dataset, we take advantage 10 brain MRIs of 6-month-old from iSeg2019 [20], and also build 3-month-old and 12-month-old in-house datasets. In this dataset, we collect 11 brain MRI for both the 3-month-old and 12-month-old infants. We take the 6-month-old data as the source domain, the 3-month-old and 12-month-old as the target domains.The 2nd dataset is for brain tumor segmentation (enhancing tumor, peritumoral edema and necrotic and non-enhancing tumor core), which has 285 MRI samples (210 HGG and 75 LGG). We take HGG as the source domain and LGG as the target domain. "
Multi-Target Domain Adaptation with Prompt Learning for Medical Image Segmentation,3.2,Comparison with State-of-the-Art (SOTA) Method,"We compared our method with four SOTA methods: ADDA [18], CyCADA [22], SIFA [23] and ADR [24]. We directly use the code from the corresponding papers.For fair comparison, we have replaced the backbone of these models with the same we used in our approach. The quantitative comparison results of cross-age infant brain segmentation is presented in Table 1, and due to space limitations, we put the experimental results of the brain tumor segmentation task in Table 1 of Supplementary Material, Sec.3. As observed, our method demonstrates very good DA ability on the crossage infant segmentation task, which improves about 5.46 DICE and 4.75 DICE on 12-month-old and 3-month-old datasets, respectively. When compared to the four selected SOTA DA methods, we also show superior transfer performance in all the target domains. Specially, we outperform other SOTA methods by at least 2.83 DICE and 1.04 DICE on the 12-month-old and 3-month-old tasks.When transferring to a single target domain in the brain tumor segmentation task, our proposed DA solution improves about 3.09 DICE in the target LGG domain. Also, the proposed method shows considerable improvements over ADDA and CyCADA, but very subtle improvements to the SIFA and ADR methods (although ADR shows a small advantage on the Whole category).We also visualize the segmentation results on a typical test sample of the infant brain dataset in Fig. 2, which once again demonstrates the advantage of our method in some detailed regions.  "
Multi-Target Domain Adaptation with Prompt Learning for Medical Image Segmentation,3.3,Ablation Study,"Prompt-DA vs. adv-DA: Since the performance reported in Table 1 is achieved with the method combining prompt-DA and adv-DA, we carry out more studies to investigate: 1). Does prompt-DA itself shows the transfer ability? 2). Is prompt-DA compatible with adv-DA?The corresponding experiments are conducted on the infant brain dataset and experimental results are shown in Table 2. To make the table more readable, we denote: no-DA means only training the segmentation network without any DA strategies; adv-DA presents only using adversarial learning based DA; prompt-DA is the proposed prompt learning based DA and comb-DA is our final DA algorithm which combines both adv-DA and prompt-DA.As observed in Table 2, both adv-DA and prompt-DA can improve the transfer performance on all the target domains. When looking into details, the proposed prompt-DA can improve more (1.44 DICE and 0.65 DICE respectively) compared to the adv-DA on both 12-month-old and 3-month-old with 3D-UNet segmentation backbone. When combined together (i.e., comb-DA), the performance can be further improved by a considerable margin, 2.87 DICE and 1.75 DICE on 12-month-old and 3-month-old respectively, compared to prompt-DA. With TransUNet segmentation backbone, we can find the similar phenomenon. To this end, we can draw conclusions that 1). Prompt-DA itself is beneficial to improve the transfer ability; 2). prompt-DA is quite compatible with adv-DA.Fusion Strategy for Learning Domain-Aware Features: One of the key components of the prompt-DA is to learn domain-aware features through fusion. We have evaluated the effectiveness of our proposed feature fusion strategy in both 3D and 2D models. For comparison, we considered several other fusion strategies, including 'add/mul', which adds or multiplies the encoder feature and prompt directly, 'conv', which employs a single convolutional layer to process the concatenated features, and 'rAFusion', which utilizes a reverse version of the AFusion module, sending the prompt to the spatial branch and the encoder feature to the channel branch. The results of these experiments are presented in Table 3. Our experimental results demonstrate that the proposed AFusion module improves the model's performance significantly, and it is effective for both 3D and 2D models."
Multi-Target Domain Adaptation with Prompt Learning for Medical Image Segmentation,4,Conclusion,"In this paper, we propose a new DA paradigm, namely, prompt learning based DA. The proposed prompt-DA uses a classifier and a prompt generator to produce domain-specific information and then employs a fusion module (for encoder features and prompts) to learn domain-aware representation. We show the effectiveness of our proposed prompt-DA in transfer ability, and also we prove that the prompt-DA is smoothly compatible with the other DA algorithms. Experiments on two DA datasets with two different segmentation backbones demonstrate that our proposed method works well on DA problems."
Multi-Target Domain Adaptation with Prompt Learning for Medical Image Segmentation,,Fig. 1 .,
Multi-Target Domain Adaptation with Prompt Learning for Medical Image Segmentation,,Fig. 2 .,
Multi-Target Domain Adaptation with Prompt Learning for Medical Image Segmentation,,Table 1 .,
Multi-Target Domain Adaptation with Prompt Learning for Medical Image Segmentation,,Table 2 .,
Multi-Target Domain Adaptation with Prompt Learning for Medical Image Segmentation,,Table 3 .,
Multi-Target Domain Adaptation with Prompt Learning for Medical Image Segmentation,,88.57 93.84 88.29 68,
Multi-Target Domain Adaptation with Prompt Learning for Medical Image Segmentation,,03 80.03 78.74 77.93 70.59 74.51 63.18 69.43,
Multi-Target Domain Adaptation with Prompt Learning for Medical Image Segmentation,,81.12 84.19 79.52 66,
Multi-Target Domain Adaptation with Prompt Learning for Medical Image Segmentation,,25 72.57 57.04 66.62 42.61 61.03 61.57 55.07,
Spectral Adversarial MixUp for Few-Shot Unsupervised Domain Adaptation,1,Introduction,"A common challenge for deploying deep learning to clinical problems is the discrepancy between data distributions across different clinical sites [6,15,20,28,29]. This discrepancy, which results from vendor or protocol differences, can cause a significant performance drop when models are deployed to a new site [2,21,23]. To solve this problem, many Unsupervised Domain Adaptation (UDA) methods [6] have been developed for adapting a model to a new site with only unlabeled data (target domain) by transferring the knowledge learned from the original dataset (source domain). However, most UDA methods require sufficient target samples, which are scarce in medical imaging due to the limited accessibility to patient data. This motivates a new problem of Few-Shot Unsupervised Domain Adaptation (FSUDA), where only a few unlabeled target samples are available for training.Few approaches [11,22] have been proposed to tackle the problem of FSUDA. Luo et. al [11] introduced Adversarial Style Mining (ASM), which uses a pretrained style-transfer module to generate augmented images via an adversarial process. However, this module requires extra style images [9] for pre-training. Such images are scarce in clinical settings, and style differences across sites are subtle. This hampers the applicability of ASM to medical image analysis. SM-PPM [22] trains a style-mixing model for semantic segmentation by augmenting source domain features to a fictitious domain through random interpolation with target domain features. However, SM-PPM is specifically designed for segmentation tasks and cannot be easily adapted to other tasks. Also, with limited target domain samples in FSUDA, the random feature interpolation is ineffective in improving the model's generalizability.In a different direction, numerous UDA methods have shown high performance in various tasks [4,[16][17][18]. However, their direct application to FSUDA can result in severe overfitting due to the limited target domain samples [22]. Previous studies [7,10,24,25] have demonstrated that transferring the amplitude spectrum of target domain images to a source domain can effectively convey image style information and diversify training dataset. To tackle the overfitting issue of existing UDA methods, we propose a novel approach called Sensitivityguided Spectral Adversarial MixUp (SAMix) to augment training samples. This approach uses an adversarial mixing scheme and a spectral sensitivity map that reveals model generalizability weaknesses to generate hard-to-learn images with limited target samples efficiently. SAMix focuses on two key aspects. 1) Model generalizability weaknesses: Spectral sensitivity analysis methods have been applied in different works [26] to quantify the model's spectral weaknesses to image amplitude corruptions. Zhang et al. [27] demonstrated that using a spectral sensitivity map to weigh the amplitude perturbation is an effective data augmentation. However, existing sensitivity maps only use single-domain labeled data and cannot leverage target domain information. To this end, we introduce a Domain-Distance-modulated Spectral Sensitivity (DoDiSS) map to analyze the model's weaknesses in the target domain and guide our spectral augmentation. 2) Sample hardness: Existing studies [11,19] have shown that mining hard-to-learn samples in model training can enhance the efficiency of data augmentation and improve model generalization performances. Therefore, to maximize the use of the limited target domain data, we incorporate an adversarial approach into the spectral mixing process to generate the most challenging data augmentations. This paper has three major contributions. 1) We propose SAMix, a novel approach for augmenting target-style samples by using an adversarial spectral mixing scheme. SAMix enables high-performance UDA methods to adapt easily to FSUDA problems. 2) We introduce DoDiSS to characterize a model's generalizability weaknesses in the target domain. 3) We conduct thorough empirical analyses to demonstrate the effectiveness and efficiency of SAMix as a plug-in module for various UDA methods across different tasks."
Spectral Adversarial MixUp for Few-Shot Unsupervised Domain Adaptation,2,Methods,"We denote the labeled source domain as X S = {(x s n , y s n )} N n=1 and the unlabeled K-shot target domain as 1 depicts the framework of our method as a plug-in module for boosting a UDA method in the FSUDA scenario. It contains two components. First, a Domain-Distancemodulated Spectral Sensitivity (DoDiSS) map is calculated to characterize a source model's weaknesses in generalizing to the target domain. Then, this sensitivity map is used for Sensitivity-guided Spectral Adversarial MixUp (SAMix) to generate target-style images for UDA models. The details of the components are presented in the following sections. "
Spectral Adversarial MixUp for Few-Shot Unsupervised Domain Adaptation,2.1,Domain-Distance-Modulated Spectral Sensitivity (DoDiSS),"The prior research [27] found that a spectral sensitivity map obtained using Fourier-based measurement of model sensitivity can effectively portray the generalizability of that model. However, the spectral sensitivity map is limited to single-domain scenarios and cannot integrate target domain information to assess model weaknesses under specific domain shifts. Thus, we introduce DoDiSS, extending the previous method by incorporating domain distance to tackle domain adaptation problems. Fig. 1 (a) depicts the DoDiSS pipeline. It begins by computing a domain distance map for identifying the amplitude distribution difference between the source and target domains in each frequency. Subsequently, this difference map is used for weighting amplitude perturbations when calculating the DoDiSS map."
Spectral Adversarial MixUp for Few-Shot Unsupervised Domain Adaptation,,Domain Distance Measurement.,"To overcome the limitations of lacking target domain images, we first augment the few-shot images from the target domain with random combinations of various geometric transformations, including random cropping, rotation, flipping, and JigSaw [13]. These transformations keep the image intensities unchanged, preserving the target domain style information. The Fast Fourier Transform (FFT) is then applied to all the source images and the augmented target domain images to obtain their amplitude spectrum, denoted as A S and ÂT , respectively. We calculate the probabilistic distributions p S i,j and p T i,j of A S and ÂT at the (i, j) th frequency entry, respectively. The domain distance map at (i, j) is defined as, where W 1 is the 1-Wasserstein distance."
Spectral Adversarial MixUp for Few-Shot Unsupervised Domain Adaptation,,DoDiSS Computation.,"With the measured domain difference, we can now compute the DoDiSS map of a model. As shown in Fig. 1 (a), a Fourier basis is defined as a Hermitian matrix H i,j ∈ R h×w with only two non-zero elements at (i, j) and (-i, -j). A Fourier basis image U i,j can be obtained by 2 -normalized Inverse Fast Fourier Transform (IFFT) of A i,j , i.e., U i,j =To analyze the model's generalization weakness with respect to the frequency (i, j), we generate perturbed source domain images by adding the Fourier basis noise N i,j = r • D W (i, j) • U i,j to the original source domain image x s as x s + N i,j . D W (i, j) controls the 2 -norm of N i,j and r is randomly sampled to be either -1 or 1. The N i,j only introduces perturbations at the frequency components (i, j) to the original images. The D W (i, j) guarantees that images are perturbed across all frequency components following the real domain shift. For RGB images, we add N i,j to each channel independently following [27]. The sensitivity at frequency (i, j) of a model F trained on the source domain is defined as the prediction error rate over the whole dataset X S as in (1), where Acc denotes the prediction accuracy"
Spectral Adversarial MixUp for Few-Shot Unsupervised Domain Adaptation,2.2,Sensitivity-Guided Spectral Adversarial Mixup (SAMix),"Using the DoDiSS map M S and an adversarially learned parameter λ * as a weighting factor, SAMix mixes the amplitude spectrum of each source image with the spectrum of a target image. DoDiSS indicates the spectral regions where the model is sensitive to the domain difference. The parameter λ * mines the heard-tolearn samples to efficiently enrich the target domain samples by maximizing the task loss. Further, by retaining the phase of the source image, SAMix preserves the semantic meaning of the original source image in the generated target-style sample. Specifically, as shown in Fig. 1 (b), given a source image x s and a target image x t , we compute their amplitude and phase spectrum, denoted as (A s , Φ s ) and (A t , Φ t ), respectively. SAMix mixes the amplitude spectrum byThe target-style image is reconstructed by x st λ * = IFFT (A st λ * , Φ s ). The adversarially learned parameter λ * is optimized by maximizing the task loss L T using the projected gradient descent with T iterations and step size of δ:In the training phase, as shown in Fig. 1 (b), the SAMix module generates a batch of augmented images, which are combined with few-shot target domain images to train the UDA model. The overall training objective is to minimizewhere L t is the supervised task loss in the source domain; JS is the Jensen-Shannon divergence [27], which regularizes the model predictions consistency between the source images x s and their augmented versions x st λ * ; L UDA is the training loss in the original UDA method, and μ is a weighting parameter."
Spectral Adversarial MixUp for Few-Shot Unsupervised Domain Adaptation,3,Experiments and Results,"We evaluated SAMix on two medical image datasets. Fundus [5,14] is an optic disc and cup segmentation task. Following [21], we consider images collected from different scanners as distinct domains. The source domain contains 400 images of the REFUGE [14] training set. We took 400 images from the REFUGE validation set and 159 images of RIM-One [5] to form the target domain 1 & 2. We center crop and resize the disc region to 256 × 256 as network input. Camelyon [1] is a tumor tissue binary classification task across 5 hospitals. We use the training set of Camelyon as the source domain (302, 436 images from hospitals 1 -3) and consider the validation set (34, 904 images from hospital 4) and test set (85, 054 images from the hospital 5) as the target domains 1 and 2, respectively. All the images are resized into 256 × 256 as network input. For all experiments, the source domain images are split into training and validation in the ratio of 4 : 1. We randomly selected K-shot target domain images for training, while the remaining target domain images were reserved for testing."
Spectral Adversarial MixUp for Few-Shot Unsupervised Domain Adaptation,3.1,Implementation Details,"SAMix is evaluated as a plug-in module for four UDA models: AdaptSeg [17] and Advent [18] for Fundus, and SRDC [16] and DALN [4] for Camelyon. For a fair comparison, we adopted the same network architecture for all the methods on each task. For Fundus, we use a DeepLabV2-Res101 [3] as the backbone with SGD optimizer for 80 epochs. The task loss L t is the Dice loss. The initial learning rate is 0.001, which decays by 0.1 for every 20 epochs. The batch size is 16. For Camelyon, we use a ResNet-50 [8] with SGD optimizer for 20 epochs. L t is the binary cross-entropy loss. The initial learning rate is 0.0001, which decays by 0.1 every 5 epochs. The batch size is 128. We use the fixed weighting factor μ = 0.01, iterations T = 10, and step size δ = 0.1 in all the experiments."
Spectral Adversarial MixUp for Few-Shot Unsupervised Domain Adaptation,3.2,Method Effectiveness,"We demonstrate the effectiveness of SAMix by comparing it with two sets of baselines. First, we compare the performance of UDA models with and without SAMix. Second, we compare SAMix against other FSUDA methods [9,11].Fundus. Table 1 shows the 10-run average Dice coefficient (DSC) and Average Surface Distance (ASD) of all the methods trained with the source domain and 1-shot target domain image. The results are evaluated in the two target domains. Compared to the model trained solely on the source domain (Source only), the performance gain achieved by UDA methods (AdaptSeg and Advent) is limited. However, incorporating SAMix as a plug-in for UDA methods (Adapt-Seg+SAMix and Advent+SAMix) enhances the original UDA performance significantly (p < 0.05). Moreover, SAMix+Advent surpasses the two FSUDA methods (ASM and SM-PPM) significantly. This improvement is primarily due to spectrally augmented target-style samples by SAMix.To assess the functionality of the target-aware spectral sensitivity map in measuring the model's generalization performance on the target domain, we computed the DoDiSS maps of the four models (AdaptSeg, ASM, SM-PPM, and AdaptSeg+SAMix). The results are presented in Fig. 2(a). The DoDiSS map of AdaptSeg+SAMix demonstrates a clear suppression of sensitivity, leading to improved model performance. To better visualize the results, the model generalizability (average DSC) versus the averaged 1 -norm of the DoDiSS map is presented in Fig. 2 (b). The figure shows a clear trend of improved model performance as the averaged DoDiSS decreases. To assess the effectiveness of SAMix-augmented target-style images in bridging the gap of domain shift, the feature distributions of Fundus images before and after adaptation are visualized in Fig. 2 (c) by t-SNE [12]. Figure 2(c1) shows the domain shift between the source and target domain features. The augmented samples from SAMix build the connection between the two domains with only a single example image from the target domain. Please note that, except the 1-shot sample, all the other target domain samples are used here for visualization only but never seen during training/validation. Incorporating these augmented samples in AdaptSeg merges the source and target distributions as in Fig. 2 (c2).  "
Spectral Adversarial MixUp for Few-Shot Unsupervised Domain Adaptation,3.3,Data Efficiency,"As the availability of target domain images is limited, data efficiency plays a crucial role in determining the data augmentation performance. Therefore, we evaluated the model's performance with varying numbers of target domain images in the training process.   "
Spectral Adversarial MixUp for Few-Shot Unsupervised Domain Adaptation,3.4,Ablation Study,"To assess the efficacy of the components in SAMix, we conducted an ablation study with AdaptSeg+SAMix and DALN+SAMix (Full model) on Fundus and Camelyon datasets. This was done by 1) replacing our proposed DoDiSS map with the original one in [27] (Original map); 2) replacing the SAMix module with the random spectral swapping (FDA, β = 0.01, 0.09) in [25]; 3) removing the three major components (No L UDA , No SAMix, No JS) in a leave-one-out manner. Figure 5 suggests that, compared with the Full model, the model performance degrades when the proposed components are either removed or replaced by previous methods, which indicates the efficacy of the SAMix components."
Spectral Adversarial MixUp for Few-Shot Unsupervised Domain Adaptation,4,Discussion and Conclusion,"This paper introduces a novel approach, Sensitivity-guided Spectral Adversarial MixUp (SAMix), which utilizes an adversarial mixing scheme and a spectral sensitivity map to generate target-style samples effectively. The proposed method facilitates the adaptation of existing UDA methods in the few-shot scenario. Thorough empirical analyses demonstrate the effectiveness and efficiency of SAMix as a plug-in module for various UDA methods across multiple tasks."
Spectral Adversarial MixUp for Few-Shot Unsupervised Domain Adaptation,,Fig. 1 .,
Spectral Adversarial MixUp for Few-Shot Unsupervised Domain Adaptation,,Fig. 2 .,
Spectral Adversarial MixUp for Few-Shot Unsupervised Domain Adaptation,,Fig. 3 .,
Spectral Adversarial MixUp for Few-Shot Unsupervised Domain Adaptation,,Figure 3,
Spectral Adversarial MixUp for Few-Shot Unsupervised Domain Adaptation,,Fig. 4 .,
Spectral Adversarial MixUp for Few-Shot Unsupervised Domain Adaptation,,Fig. 5 .,
Spectral Adversarial MixUp for Few-Shot Unsupervised Domain Adaptation,,Table 1 .,*  
Spectral Adversarial MixUp for Few-Shot Unsupervised Domain Adaptation,,44 62.02 66.35 64.19 11.97 10.85 11.41,
Spectral Adversarial MixUp for Few-Shot Unsupervised Domain Adaptation,,Table 2 .,
Spectral Adversarial MixUp for Few-Shot Unsupervised Domain Adaptation,,.41 82.58 80.84 75.90,Camelyon.
UOD: Universal One-Shot Detection of Anatomical Landmarks,1,Introduction,"Robust and accurate detecting of anatomical landmarks is an essential task in medical image applications [24,25], which plays vital parts in varieties of clinical treatments, for instance, vertebrae localization [20], orthognathic and orthodontic surgeries [9], and craniofacial anomalies assessment [4]. Moreover, anatomical landmarks exert their effectiveness in other medical image tasks such as segmentation [3], registration [5], and biometry estimation [1].In the past years, lots of fully supervised methods [4,8,11,11,20,21,26,27] have been proposed to detect landmarks accurately and automatically. To relieve the burden of experts and reduce the amount of annotated labels, various oneshot and few-shot methods have been come up with. Zhao et al. [23] demonstrate a model which learns transformations from the images and uses the labeled example to synthesize additional labeled examples, where each transformation is composed of a spatial deformation field and an intensity change. Yao et al. [22] develop a cascaded self-supervised learning framework for one-shot medical landmark detection. They first train a matching network to calculate the cosine similarity between features from an image and a template patch, then fine-tune the pseudo landmark labels from coarse to fine. Browatzki et al. [2] propose a semisupervised method that consists of two stages. They first employ an adversarial auto-encoder to learn implicit face knowledge from unlabeled images and then fine-tune the decoder to detect landmarks with few-shot labels.However, one-shot methods are not robust enough because they are dependent on the choice of labeled template and the accuracy of detected landmarks may decrease a lot when choosing a sub-optimal image to annotate. To address this issue, Quan et al. [12] propose a novel Sample Choosing Policy (SCP) to select the most worthy image to annotate. Despite the improved performance, SCP brings an extra computation burden. Another challenge is the scalability of model building when facing multiple domains (such as different anatomical regions). While conventional wisdom is to independently train a model for each domain, Zhu et al. [26] propose a universal model YOLO for detecting landmarks across different anatomies and achieving better performances than a collection of single models. YOLO is regularly supervised using the CNN as backbone and it is unknown if the YOLO model works for a one-shot scenario and with a modern transformer architecture.Motivated by above challenges, to detect robust multi-domain label-efficient landmarks, we design domain-adaptive models and propose a universal oneshot landmark detection framework called Universal One-shot Detection (UOD), illustrated in Fig. 1. A universal model is comprised of domain-specific modules and domain-shared modules, learning the specified features of each domain and common features of all domains to eliminate domain preference and extract representative features for multi-domain data. Moreover, one-shot learning is not robust enough because of the sample selection while multi-domain oneshot learning reaps benefit from different one-shot samples from various domains, in which cross-domain features are excavated by domain-shared modules. Our proposed UOD framework consists of two stages: 1) Contrastive learning for In summary, our contributions can be categorized into three parts: 1) We design the first universal framework for multi-domain one-shot landmark detection, which improves detecting accuracy and relieves domain preference on multidomain data from various anatomical regions. 2) We design a domain-adaptive transformer block (DATB), which is effective for multi-domain learning and can be used in any other transformer network. 3) We carry out comprehensive experiments to demonstrate the effectiveness of UOD for obtaining SOTA performance on three publicly used X-ray datasets of head, hand, and chest."
UOD: Universal One-Shot Detection of Anatomical Landmarks,2,Method,"As Fig. 1 shows, UOD consists of two stages: 1) Contrastive learning and 2) Supervised learning. In stage I, to learn the local appearance of each domain, a universal model is trained via self-supervised learning, which contains domainspecific VGG [15] and UNet [13] decoder with each standard convolution replaced by a domain adaptor [7]. In stage II, to grasp the global constraint and eliminate domain preference, we designed a domain-adaptive transformer (DATR)."
UOD: Universal One-Shot Detection of Anatomical Landmarks,2.1,Stage I: Contrastive Learning,"As Fig. 1 shows, following Yao et al. [22], we employ contrastive learning to train siamese network for matching similar patches of original image and augmented image. Given a multi-domain input image X d ∈ R H d ×W d ×C d belongs to domain d from multi-domain data, we randomly select a target point P and crop a halfsize patch X d p which contains P . After applying data augmentation on X d p , the target point is mapped to P p . Then we feed X d and X d p into the siamese network respectively and obtain the multi-scale feature embeddings. We compute cosine similarity of two feature embeddings from each scale and apply softmax to the cosine similarity map to generate a probability matrix. Finally, we calculate the cross entropy loss of the probability matrix and ground truth map which is produced with the one-hot encoding of P d p to optimize the siamese network for learning the latent similarities of patches. At inferring stage, we replace augmented patch X d p with the augmented one-shot sample patch X d s . We use the annotated one-shot landmarks as target points to formulate the ground truth maps. After obtaining probability matrices, we apply arg max to extract the strongest response points as the pseudo landmarks, which will be used in UOD Stage II."
UOD: Universal One-Shot Detection of Anatomical Landmarks,2.2,Stage II: Supervised Learning,"In stage II, we design a universal transformer to capture global relationship of multi-domain data and train it with the pseudo landmarks generated in stage I. The universal transformer has a domain-adaptive transformer encoder and domain-adaptive convolution decoder. The decoder is based on a U-Net [13] decoder with each standard convolution replaced by a domain adaptor [7]. The encoder is based on Swin Transformer [10] with shifted window and limited self-attention within non-overlapping local windows for computation efficiency. Different from Swin Transformer [10], we design a domain-adaptive transformer block (DATB) and use it to replace the original transformer block. Domain-Adaptive Transformer Encoder. As Fig. 2(a) shows, the transformer encoder is built up with DATB, making full use of the capability of transformer for modeling global relationship and extracting multi-domain representative features. As in Fig. 2(b), a basic transformer block [17] consists of a multi-head self-attention module (MSA), followed by a two-layer MLP with GELU activation. Furthermore, layer normalization (LN) is adopted before each MSA and MLP and a residual connection is adopted after each MSA and MLP. Given a feature map x d ∈ R h×w×c from domain d with height h, width w, and c channels, the output feature maps of MSA and MLP, denoted by ŷd and y d , respectively, are formulated as: where MSA = softmax(QK T )V .As illustrated in Fig. 2(b)(c), DATB is based on Eq. ( 1). Similar to U2Net [7] and GU2Net [26], we adopt domain-specific and domain-shared parameters in DATB. Since the attention probability is dependent on query and key matrix which are symmetrical, we duplicate the query matrix for each domain to learn domain-specific query features and keep key and value matrix domain-shared to learn common knowledge and reduce parameters. Inspired by LayerScale [16], we further adopt learnable diagonal matrix [16] after each MSA and MLP module to facilitate the learning of domain-specific features, which costs few parameters (O(N ) for N × N diagonal). Different from LayerScale [16], proposed domainadaptive diagonal D d 1 and D d 2 are applied for each domain with D d 2 applied after residual connection for generating more representative and direct domain-specific features. The above process can be formulated as:whereOverall Pipeline. Given that a random input 2 ≤ σ and 0 otherwise. We further add an exponential weight to the Gaussian distribution to distinguish close heatmap pixels and obtain the ground truth heatmap Y d n (i, j) = α Ỹ d n (i,j) . As illustrated in Fig. 2, firstly, the input image from a random batch is partitioned into non-overlapping patches and linearly embedded. Next, these patches are fed into cascaded transformer blocks at each stage, which are merged except in the last stage. Finally, a domain-adaptive convolution decoder makes dense prediction to generate heatmaps, which is further used to extract landmarks via threshold processing and connected components filtering."
UOD: Universal One-Shot Detection of Anatomical Landmarks,3,Experiment,"Datasets. For performance evaluation, we adopt three public X-ray datasets from different domains on various anatomical regions of head, hand, and chest. (i) Head dataset is a widely-used dataset for IEEE ISBI 2015 challenge [18,19] which contains 400 X-ray cephalometric images with 150 images for training and 250 images for testing. Each image is of size 2400 × 1935 with a resolution of 0.1 mm × 0.1 mm, which contains 19 landmarks manually labeled by two medical experts and we use the average labels same as Payer et al. [11]. (ii) Hand dataset is collected by [6] which contains 909 X-ray images and 37 landmarks annotated by [11]. We follow [26] to split this dataset into a training set of 609 images and a test set of 300 images. Following [11] we assume the distance between two endpoints of wrist is 50 mm and calculate the physical distance as distance physical = distance pixel × 50 p-q 2 where p, q are the two endpoints of the wrist respectively. (iii) Chest dataset [26] is a popular chest radiography database collected by Japanese Society of Radiological Technology (JSRT) [14] which contains 247 images. Each image is of size 2048 × 2048 with a resolution of 0.175 mm × 0.175 mm. We split it into a training set of 197 images and a test set of 50 images and select 6 landmarks from landmark labels at the boundary of the lung as target landmarks.Implementation Details. UOD is implemented in Pytorch and trained on a TITAN RTX GPU with CUDA version being 11. All encoders are initialized with corresponding pre-trained weights. We set batch size to 8, σ to 3, and α to 10. We adopt binary cross-entropy (BCE) as loss function for both stages. In stage I, we resize each image to the same shape of 384 × 384 and train universal convolution model by Adam optimizer for 1000 epochs with a learning rate of 0.00001. In stage II, we resize each image to the same shape of 576 × 576 and optimize the universal transformer by Adam optimizer for 300 epochs with a learning rate of 0.0001. When calculating metrics, all predicted landmarks are resized back to the original size. For evaluation, we choose model with minimum validation loss as the inference model and adopt two metrics: mean radial error (MRE) 2 and successful detection rates (SDR) within different thresholds t:  "
UOD: Universal One-Shot Detection of Anatomical Landmarks,3.1,Experimental Results,"The Effectiveness of Universal Model: To demonstrate the effectiveness of universal model for multi-domain one-shot learning, we adopt head and hand datasets for evaluation. In stage I, the convolution models are trained in two ways: 1) single: trained on every single dataset respectively, and 2) universal: trained on mixed datasets together. With a fixed one-shot sample for the hand dataset, we change the one-shot sample for the head dataset and report the MRE and SDR of the head dataset. As Fig. 3 shows, universal model performs much better than single model on various one-shot samples and metrics. It is proved that universal model learns domain-shared knowledge and promotes domainspecific learning. Furthermore, the MRE and SDR metrics of universal model have a smaller gap among various one-shot samples, which demonstrates the robustness of universal model learned on multi-domain data.Comparisons with State-of-the-Art Methods: As Table 1 shows, we compare UOD with two open-source landmark detection methods, i.e., YOLO [26] and CC2D [22]. YOLO is a multi-domain supervised method while CC2D is a single-domain one-shot method. UOD achieves SOTA results on all datasets under all metrics, outperforming the other one-shot method by a big margin. On the head dataset, benefiting from multi-domain learning, UOD achieves an MRE of 2.43 mm and an SDR of 86.49% within 4 mm, which is comparative with supervised method YOLO trained with at least 10 annotated labels, and much  "
UOD: Universal One-Shot Detection of Anatomical Landmarks,4,Conclusion,"To improve the robustness and reduce domain preference of multi-domain oneshot learning, we design a universal framework in that we first train a universal model via contrastive learning to generate pseudo landmarks and further use these labels to learn a universal transformer for accurate and robust detection of landmarks. UOD is the first universal framework of one-shot landmark detection on multi-domain data, which outperforms other one-shot methods on three public datasets from different anatomical regions. We believe UOD will significantly reduce the labeling burden and pave the path of developing more universal framework for multi-domain one-shot learning."
UOD: Universal One-Shot Detection of Anatomical Landmarks,,Fig. 1 .,
UOD: Universal One-Shot Detection of Anatomical Landmarks,,Fig. 2 .,
UOD: Universal One-Shot Detection of Anatomical Landmarks,,Fig. 3 .,
UOD: Universal One-Shot Detection of Anatomical Landmarks,,Fig. 4 .,
UOD: Universal One-Shot Detection of Anatomical Landmarks,,Table 1 .,
UOD: Universal One-Shot Detection of Anatomical Landmarks,,Table 2 .,
Source-Free Domain Adaptive Fundus Image Segmentation with Class-Balanced Mean Teacher,1,Introduction,"Medical image segmentation plays an essential role in computer-aided diagnosis systems in different applications and has been tremendously advanced in the past few years [6,12,19,22]. While the segmentation model [3,10,11,21] always requires sufficient labeled data, unsupervised domain adaptation (UDA) approaches have been proposed, learning an adaptive model jointly with unlabeled target domain images and labeled source domain images [9], for example, the adversarial training paradigm [2,8,13,14,16].Although impressive performance has been achieved, these UDA methods may be limited for some real-world medical image segmentation tasks where labeled source images are not available for adaptation. This is not a rare scenario because medical images are usually highly sensitive in privacy and copyright protection such that labeled source images may not be allowed to be distributed. This motivates the investigation of source-free domain adaptation (SFDA) where adapts a source segmentation model trained on labeled source data (in a privateprotected way) to the target domain only using unlabeled data.A few recent SFDA works have been proposed. OSUDA [17] utilizes the domain-specific low-order batch statistics and domain-shareable high-order batch statistics, trying to adapt the former and keep the consistency of the latter. SRDA [1] minimizes a label-free entropy loss guided with a domaininvariant class-ratio prior. DPL [4] introduces pixel-level and class-level pseudolabel denoising schemes to reduce noisy pseudo-labels and select reliable ones. U-D4R [27] applies an adaptive class-dependent threshold with the uncertaintyrectified correction to realize better denoising.Although these methods have achieved some success in model adaptation, they still suffer from two major issues. First, they tend to be fairly unstable. Without any supervision signal from labeled data, the model heavily relies on the predictions generated by itself, which are always noisy and could easily make the training process unstable, causing catastrophic error accumulation after several training epochs as shown in Fig. 1(a). Some works avoid this problem by only training the model for very limited iterations (only 2 epochs in [4,27]) and selecting the best-performing model during the whole training process for testing. However, this does not fully utilize the data and it is non-trivial to select the best-performing model for this unsupervised learning task. Second, they failed to consider the severe foreground and background imbalance of fundus images where the foreground (e.g., cup) region is usually very small (as shown in Fig. 1(b)). This oversight could also lead to a model degradation due to the dominate background learning signal.In this paper, we propose the Class-Balanced Mean Teacher (CBMT) method to address the limitations of existing methods. To mitigate the negative impacts of incorrect pseudo labels, we propose a weak-strong augmented mean teacher learning scheme which involves a teacher model and a student model that are both initialized from the source model. We use the teacher to generate pseudo label from a weakly augmented image, and train the student that takes strongly augmented version of the same image as input. We do not train the teacher model directly by back-propagation but update its weights as the moving average of the student model. This prevents the teacher model from being abruptly impacted by incorrect pseudo labels and meanwhile accumulates new knowledge learned by the student model. To address the imbalance between foreground and background, we propose to calibrate the segmentation loss and highlight the foreground class, based on the prediction statistics derived from the global information. We maintain a prediction bank to capture global information, which is considered more reliable than that inside one image.Our contributions can be summarized as follows: (1) We propose the weakstrong augmented mean teacher learning scheme to address the stable issue of existing methods. (2) We propose the novel global knowledge-guided loss calibration technique to address the foreground and background imbalance problem.(3) Our proposed CBMT reaches state-of-the-art performance on two popular benchmarks for adaptive fundus image segmentation."
Source-Free Domain Adaptive Fundus Image Segmentation with Class-Balanced Mean Teacher,2,Method,"Source-Free Domain Adaptive (SFDA) fundus image segmentation aims to adapt a source model h, trained with N S labeled source images S = {(X i , Y i )} NS i=1 , to the target domain using only N T unlabeled target images T = {X i } NT i=1 . Y i ∈ {0, 1} H×W ×C is the ground truth, and H, W , and C denote the image height, width, and class number, respectively. A vanilla pseudo-labeling-based method generates pseudo labels ŷ ∈ R C from the sigmoided model prediction p = h(x) for each pixel x ∈ X i with source model h:where 1 is the indicator function and γ ∈ [0, 1] is the probability threshold for transferring soft probability to hard label. p k and y k is the k-th dimension of p and y, respectively, denoting the prediction and pseudo label for class k. Then (x, ŷ) is utilized to train the source model h with binary cross entropy loss:Most existing SFDA works refine this vanilla method by proposing techniques to calibrate p and get better pseudo label ŷ, or measure the uncertainty of p and apply a weight when using ŷ for computing the loss [4,27]. While achieving improved performance, these methods still suffer from the unstable issue because noisy ŷ will directly impact h, and the error will accumulate since then the predictions of h will be used for pseudo labeling. Another problem with this method is that they neglect the imbalance of the foreground and background pixels in fungus images, where the foreground region is small. Consequently, the second term in Eq. ( 2) will dominate the loss, which is undesirable.Our proposed CBMT model addresses the two problems by proposing the weak-strong augmented mean teacher learning scheme and the global knowledgeguided loss calibration technique. Figure 1(c) shows the framework of CBMT."
Source-Free Domain Adaptive Fundus Image Segmentation with Class-Balanced Mean Teacher,2.1,Weak-Strong Augmented Mean Teacher,"To avoid error accumulation and achieve a robust training process, we introduce the weak-strong augmented mean teacher learning scheme where there is a teacher model h t and a student model h s both initialized from the source model h. We generate pseudo labels with h t and use the pseudo labels to train h s . To enhance generalization performance, we further introduce a weak-strong augmentation mechanism that feeds weakly and strongly augmented images to the teacher model and the student model, respectively.Concretely, for each image X i , we generate a weakly-augmented version X w i by using image flipping and resizing. Meanwhile, we generate a stronglyaugmented version X s i . The strong augmentations we used include a random eraser, contrast adjustment, and impulse noises. For each pixel x w ∈ X w i , we generate pseudo label ŷw = h t (x) by the teacher model h t with Eq. ( 1). Then, we train the student model h s withwhere Lbce is the refined binary cross entropy loss which we will introduce later. It is based on Eq. ( 2) but addresses the fore-and back-ground imbalance problem. The weakly-strong augmentation mechanism has two main benefits. First, since fundus image datasets are always on a small scale, the model could easily get overfitted due to the insufficient training data issue. To alleviate it, we enhance the diversity of the training set by introducing image augmentation techniques. Second, learning with different random augmentations performs as a consistency regularizer constraining images with similar semantics to the same class, which forms a more distinguishable feature representation.We update the student model by back-propagating the loss defined in Eq. (3). But for the teacher model, we update it as the exponential moving average (EMA) of the student model as,where θ, θ are the teacher and student model weights separately. Instead of updating the model with gradient directly, we define the teacher model as the exponential moving average of students, which makes the teacher model more consistent along the adaptation process. With this, we could train a model for a relatively long process and safely choose the final model without accuracy validation. From another perspective, the teacher model can be interpreted as a temporal ensemble of students in different time steps [18], which enhances the robustness of the teacher model."
Source-Free Domain Adaptive Fundus Image Segmentation with Class-Balanced Mean Teacher,2.2,Global Knowledge Guided Loss Calibration,"For a fundas image, the foreground object (e.g., cup) is usually quite small and most pixel will the background. If we update the student model with Eq. ( 2), the background class will dominate the loss, which dilutes the supervision signals for the foreground class. The proposed global knowledge guided loss calibration technique aims to address this problem.A naive way to address the foreground and background imbalance is to calculate the numbers of pixels falling into the two categories, respectively, within each individual image and devise a loss weighting function based on the numbers. This strategy may work well for the standard supervised learning tasks, where the labels are reliable. But with pseudo labels, it is too risky to conduct the statistical analysis based on a single image. To remedy this, we analyze the class imbalance across the whole dataset, and use this global knowledge to calibrate our loss for each individual image.Specifically, we store the predictions of pixels from all images and maintain the mean loss for foreground and background as,where L is the segmentation loss mentioned above, and ""fg"" and ""bg"" represent foreground/background. The reason we use the mean of the loss, rather than the number of pixels, is that the loss of each pixel indicates the ""hardness"" of each pixel according to the pseudo ground truth. This gives more weight to those more informative pixels, thus more global knowledge is considered.With each average loss, the corresponding learning scheme could be further calibrated. We utilize the ratio of η fg k to η bg k to weight background loss L bg k :The calibrated loss ensures fair learning among different classes, therefore alleviating model degradation issues caused by class imbalance.Since most predictions are usually highly confident (very close to 0 or 1), they are thus less informative. We need to only include pixels with relatively large loss scales to compute mean loss. We realize this by adopting constraint threshold α to select pixels: |f (xi)-γ| | ŷi-γ| > α, where α is set by default to 0.2. α represents the lower bound threshold of normalized prediction, which can filter well-segmented uninformative pixels out."
Source-Free Domain Adaptive Fundus Image Segmentation with Class-Balanced Mean Teacher,3,Experiments,"Implementation Details1 . We apply the Deeplabv3+ [5] with MobileNetV2 [23] backbone as our segmentation model, following the previous works [4,26,27] for a fair comparison. For model optimization, we use Adam optimizer with 0.9 and 0.99 momentum coefficients. During the source model training stage, the initial learning rate is set to 1e-3 and decayed by 0.98 every epoch, and the training lasts 200 epochs. At the source-free domain adaptation stage, the teacher and student model are first initialized by the source model, and the EMA update scheme is applied between them for a total of 20 epochs with a learning rate of 5e-4. Loss calibration parameter η is computed every epoch and implemented on the class cup. The output probability threshold γ is set as 0.75 according to previous study [26] and model EMA update rate λ is 0.98 by default. We implement our method with PyTorch on one NVIDIA 3090 GPU and set batch size as 8 when adaptation.Datasets and Metrics. We evaluate our method on widely-used fundus optic disc and cup segmentation datasets from different clinical centers. Following previous works, We choose the REFUGE challenge training set [20] as the source domain and adapt the model to two target domains: RIM-ONE-r3 [7] and Drishti-GS [24] datasets for evaluation. Quantitatively, the source domain consists of 320/80 fundus images for training/testing with pixel-wise optic disc and cup segmentation annotation, while the target domains have 99/60 and 50/51 images. Same as [26], the fundus images are cropped to 512 × 512 as ROI regions.We compare our CBMT model with several state-of-the-art domain adaptation methods, including UDA methods BEAL [26] and AdvEnt [25] and SFDA methods: SRDA [1], DAE [15] and DPL [4]. More comparisons with U-D4R [27] under other adaptation settings could be found in supplementary materials. General metrics for segmentation tasks are used for model performance evaluation, including the Dice coefficient and Average Symmetric Surface Distance (ASSD). The dice coefficient (the higher the better) gives pixel-level overlap results, and ASSD (the lower the better) indicates prediction boundary accuracy."
Source-Free Domain Adaptive Fundus Image Segmentation with Class-Balanced Mean Teacher,3.1,Experimental Results,"The quantitative evaluation results are shown in Table 1. We include the without adaptation results from [4] as a lower bound, and the supervised learning results Table 1. Quantitative results of comparison with different methods on two datasets, and the best score for each column is highlighted. -means the results are not reported by that method, ± refers to the standard deviation across samples in the dataset. S-F means source-free. from [26] as an upper bound, same as [4]. As shown in the table, both two quantitative metric results perform better than previous state-of-the-art SFDA methods and even show an improvement against traditional UDA methods on  [4,27]), and safely select the last checkpoint as our final result without concerning about model degradation issue, which is crucial in real-world clinical source-free domain adaptation application."
Source-Free Domain Adaptive Fundus Image Segmentation with Class-Balanced Mean Teacher,,Methods,
Source-Free Domain Adaptive Fundus Image Segmentation with Class-Balanced Mean Teacher,,S-F Optic Disc Segmentation Optic Cup Segmentation Dice,
Source-Free Domain Adaptive Fundus Image Segmentation with Class-Balanced Mean Teacher,3.2,Further Analyses,"Ablation Study. In order to assess the contribution of each component to the final performance, we conducted an ablation study on the main modules of CBMT, as summarized in Table 2. Note that we reduced the learning rates by a factor of 20 for the experiments of the vanilla pseudo-labeling method to get comparable performance because models are prone to degradation without EMA updating. As observed in quantitative results, the EMA update strategy avoids the model from degradation, which the vanilla pseudo-labeling paradigm suffers from. Image augmentation and loss calibration also boost the model accuracy, and the highest performance is achieved with both. The loss calibration module achieves more improvement in its solution to class imbalance, while image augmentation is easy to implement and plug-and-play under various circumstances.Hyper-parameter Sensitivity Analysis. We further investigate the impact of different hyper-parameter. Figure 2(a) presents the accuracy with different EMA update rate parameters λ. It demonstrates that both too low and too high update rates would cause a drop in performance, which is quite intuitive: a higher λ leads to inconsistency between the teacher and student, and thus teacher can hardly learn knowledge from the student; On the other hand, a lower λ will always keep teacher and student close, making it degenerated to vanilla pseudolabeling. But within a reasonable range, the model is not sensitive to update rate λ.To evaluate the variation of the loss calibration weight η fg k /η bg k with different constraint thresholds α, we present the results in Table 3. As we discussed in Sect. 2.2, most pixels in an image are well-classified, and if we simply calculate with all pixels (i.e. α = 0), as shown in the first column, the mean loss of background will be severely underestimated due to the large quantity of zeroloss pixel. Besides, as α changes, the calibration weight varies little, indicating the robustness of our calibration technique to threshold α. "
Source-Free Domain Adaptive Fundus Image Segmentation with Class-Balanced Mean Teacher,4,Conclusion,"In this work, we propose a class-balanced mean teacher framework to realize robust SFDA learning for more realistic clinical application. Based on the observation that model suffers from degradation issues during adaptation training, we introduce a mean teacher strategy to update the model via an exponential moving average way, which alleviates error accumulation. Meanwhile, by investigating the foreground and background imbalance problem, we present a global knowledge guided loss calibration module. Experiments on two fundus image segmentation datasets show that CBMT outperforms previous SFDA methods."
Source-Free Domain Adaptive Fundus Image Segmentation with Class-Balanced Mean Teacher,,Fig. 1 .,
Source-Free Domain Adaptive Fundus Image Segmentation with Class-Balanced Mean Teacher,,Fig. 2 .,
Source-Free Domain Adaptive Fundus Image Segmentation with Class-Balanced Mean Teacher,,Table 2 .,
Source-Free Domain Adaptive Fundus Image Segmentation with Class-Balanced Mean Teacher,,Table 3 .,
MedGen3D: A Deep Generative Framework for Paired 3D Image and Mask Generation,1,Introduction,"In medical image analysis, the availability of a substantial quantity of accurately annotated 3D data is a prerequisite for achieving high performance in tasks like segmentation and detection [7,15,23,26,[28][29][30][31][32][33][34][35][36]. This, in turn, leads to more precise diagnoses and treatment plans. However, obtaining and annotating such data presents many challenges, including the complexity of medical images, the requirement for specialized expertise, and privacy concerns.Generating realistic synthetic data presents a promising solution to the above challenges as it eliminates the need for manual annotation and alleviates privacy risks. However, most prior studies [4,5,14,25] have focused on 2D image synthesis, with only a few generating corresponding segmentation masks. For instance, [13] uses dual generative adversarial networks (GAN) [12,34] to synthesize 2D labeled retina fundus images, while [10] combines a label generator [22] with an image generator [21] to generate 2D brain MRI data. More recently, [24] uses WGAN [3] to generate small 3D patches and corresponding vessel segmentations.However, there has been no prior research on generating whole 3D volumetric images with the corresponding segmentation masks. Generating 3D volumetric images with corresponding segmentation masks faces two major obstacles. First, directly feeding entire 3D volumes to neural networks is impractical due to GPU memory constraints, and downsizing the resolution may compromise the quality of the synthetic data. Second, treating the entire 3D volume as a single data point during training is suboptimal because of the limited availability of annotated 3D data. Thus, innovative methods are required to overcome these challenges and generate high-quality synthetic 3D volumetric data with corresponding segmentation masks.We propose MedGen3D, a novel diffusion-based deep generative framework that generates paired 3D volumetric medical images and multi-label masks. Our approach treats 3D medical data as sequences of slices and employs an autoregressive process to sequentially generate 3D masks and images. In the first stage, a Multi-Condition Diffusion Probabilistic Model (MC-DPM) generates mask sequences by combining conditional and unconditional generation processes. Specifically, the MC-DPM generates mask subsequences (i.e., several consecutive slices) at any position directly from random noise or by conditioning on existing slices to generate subsequences forward or backward. Given that medical images have similar anatomical structures, slice indices serve as additional conditions to aid the mask subsequence generation. In the second stage, we introduce a conditional image generator with a seq-to-seq model from [27] and a semantic diffusion refiner. By conditioning on the mask sequences generated in the first stage, our image generator synthesizes realistic medical images aligned with masks while preserving spatial consistency across adjacent slices.The main contributions of our work are as follows: 1) Our proposed framework is the first to address the challenge of synthesizing complete 3D volumetric medical images with their corresponding masks; 2) we introduce a multicondition diffusion probabilistic model for generating 3D anatomical masks with high fidelity and diversity; 3) we leverage the generated masks to condition an image sequence generator and a semantic diffusion refiner, which produces realistic medical images that align accurately with the generated masks; and 4) we present experimental results that demonstrate the fidelity and diversity of the generated 3D multi-label medical images, highlighting their potential benefits for downstream segmentation tasks."
MedGen3D: A Deep Generative Framework for Paired 3D Image and Mask Generation,2,Preliminary,
MedGen3D: A Deep Generative Framework for Paired 3D Image and Mask Generation,2.1,Diffusion Probabilistic Model,"A diffusion probabilistic model (DPM) [16] is a parameterized Markov chain of length T, which is designed to learn the data distribution p(X). DPM builds the Forward Diffusion Process (FDP) to get the diffused data point X t at any time step t by q (X t | X t-1 ) = N X t ; √ 1 -β t X t-1 , β t I , with X 0 ∼ q(X 0 ) and p(X T ) = N (X T ; 0, I). Let α t = 1-β t and ᾱt = t s=1 (1 -β s ), Reverse Diffusion Process (RDP) is trained to predict the noise added in the FDP by minimizing:where θ is predicted noise and θ is the model parameters."
MedGen3D: A Deep Generative Framework for Paired 3D Image and Mask Generation,2.2,Classifier-Free Guidance,"Samples from conditional diffusion models can be improved with classifier-free guidance [17] by setting the condition c as ∅ with probability p. During sampling, the output of the model is extrapolated further in the direction of θ (X t | c) and away from θ (X t | ∅) as follows:where ∅ represents a null condition and s ≥ 1 is the guidance scale."
MedGen3D: A Deep Generative Framework for Paired 3D Image and Mask Generation,3,Methodology,"We propose a sequential process to generate complex 3D volumetric images with masks, as illustrated in Fig. 1. The first stage generates multi-label segmentation, and the second stage performs conditional medical image generation. The details will be presented in the following sections."
MedGen3D: A Deep Generative Framework for Paired 3D Image and Mask Generation,3.1,3D Mask Generator,"Due to the limited annotated real data and GPU memory constraints, directly feeding the entire 3D volume to the network is impractical. Instead, we treat 3D  medical data as a series of subsequences. To generate an entire mask sequence, an initial subsequence of m consecutive slices is unconditionally generated from random noise. Then the subsequence is expanded forward and backward in an autoregressive manner, conditioned on existing slices. Inspired by classifier-free guidance in Sect. 2.2, we propose a general Multi-Condition Diffusion Probabilistic Model (MC-DPM) to unify all three conditional generations (unconditional, forward, and backward). As shown in Fig. 2, MC-DPM is able to generate mask sequences directly from random noise or conditioning on existing slices.Furthermore, as 3D medical data typically have similar anatomical structures, slices with the same relative position roughly correspond to the same anatomical regions. Therefore, we can utilize the relative position of slices as conditions to guide the MC-DPM in generating subsequences of the target region and control the length of generated sequences.Train: For a given 3D multi-label mask M ∈ R D×H×W , subsequneces of m consecutive slices are selected as {M z , M z+1 , . . . , M z+(m-1) }, with z as the randomly selected starting indices. For each subsequence, we determine the conditional slices X C ∈ {R n×H×W , ∅} by selecting either the first or the last n slices, or no slice, based on a probability p C ∈ {p F orward , p Backward , p U ncondition }. The objective of the MC-DPM is to generate the remaining slices, denoted asTo incorporate the position condition, we utilize the relative position of the subsequence z = z/D, where z is the index of the subsequence's starting slice. Then we embed the position condition and concatenate it with the time embedding to aid the generation process. We also utilize a binary indicator for each slice in the subsequence to signify the existence of conditional slices.The joint distribution of reverse diffusion process (RDP) with the conditional slices X C can be written as:where p(X P T ) = N X P T ; 0, I , z = z/D and p θ is the distribution parameterized by the model.Overall, the model will be trained by minimizing the following loss function, withInference: During inference, MC-DPM first generates a subsequence of m slices from random noise given a random location z. The entire mask sequence can then be generated autoregressively by expanding in both directions, conditioned on the existing slices, as shown in Fig. 2. Please refer to the Supplementary for a detailed generation process and network structure."
MedGen3D: A Deep Generative Framework for Paired 3D Image and Mask Generation,3.2,Conditional Image Generator,"In the second step, we employ a sequence-to-sequence method to generate medical images conditioned on masks, as shown in Fig. 3."
MedGen3D: A Deep Generative Framework for Paired 3D Image and Mask Generation,,Image Sequence Generator:,"In the sequence-to-sequence generation task, new slice is the combination of the warped previous slice and newly generated texture, weighted by a continuous mask [27]. We utilize Vid2Vid [27] as our image sequence generator. We train Vid2Vid with its original loss, which includes GAN loss on multi-scale images and video discriminators, flow estimation loss, and feature matching loss.Semantic Diffusion Refiner: Despite the high cross-slice consistency and spatial continuity achieved by vid2vid, issues such as blocking, blurriness and suboptimal texture generation persist. Given that diffusion models have been shown to generate superior images [9], we propose a semantic diffusion refiner utilizing a diffusion probabilistic model to refine the previously generated images.For each of the 3 different views, we train a semantic diffusion model (SDM), which takes 2D masks and noisy images as inputs to generate images aligned with input masks. During inference, we only apply small noising steps (10 steps) to the generated images so that the overall anatomical structure and spatial continuity are preserved. After that, we refine the images using the pre-trained semantic diffusion model. The final refined 3D images are the mean results from 3 views. Experimental results show an evident improvement in the quality of generated images with the help of semantic diffusion refiner."
MedGen3D: A Deep Generative Framework for Paired 3D Image and Mask Generation,4,Experiments and Results,
MedGen3D: A Deep Generative Framework for Paired 3D Image and Mask Generation,4.1,Datasets and Setups,
MedGen3D: A Deep Generative Framework for Paired 3D Image and Mask Generation,,Datasets:,"We conducted experiments on the thoracic site using three thoracic CT datasets and the brain site with two brain MRI datasets. For both generative models and downstream segmentation tasks, we utilized the following datasets:-SegTHOR [19]: 3D thorax CT scans (25 training, 5 validation, 10 testing); -OASIS [20]: 3D brain MRI T1 scans (40 training, 10 validation, 10 testing); For the downstream segmentation task only and the transfer learning, we utilized 10 fine-tuning, 5 validation, and 10 testing scans from each of the 3D thorax CT datasets of StructSeg-Thorax [2] and Public-Thor [7], as well as the 3D brain MRI T1 dataset from ADNI [1].Implementation: For thoracic datasets, we crop and pad CT scans to (96 × 320 × 320). The annotations of six organs (left lung, right lung, spinal cord, esophagus, heart, and trachea) are examined by an experienced radiation oncologist. We also include a body mask to aid in the image generation of body regions. For brain MRI datasets, we use Freesurfer [11] to get segmentations of four regions (cortex, subcortical gray matter, white matter, and CSF), and then crop the volume to (192 × 160 × 160). We assign discrete values to masks of different regions or organs for both thoracic and brain datasets and then combine them into one 3D volume. When synthesizing mask sequences, we resize the width and height of the masks to 128×128 and set the length of the subsequence m to 6. We use official segmentation models provided by MONAI [6] along with standard data augmentations, including spatial and color transformations."
MedGen3D: A Deep Generative Framework for Paired 3D Image and Mask Generation,,Setup:,"We compare the synthetic image quality with DDPM [16], 3D-α-WGAN [18] and Vid2Vid [27], and utilize four segmentation models with different training strategies to demonstrate the benefit for the downstream task.  We compare the fidelity and diversity of our synthetic data with DDPM [16] (train 3 for different views), 3D-α-WGAN [18], and vid2vid [27] by calculating the mean Frèchet Inception Distance (FID) and Learned Perceptual Image Patch Similarity (LPIPS) from 3 different views.According to Table 1, our proposed method has a slightly lower FID score but a similar LPIPS score compared to DDPM which directly generates 2D images from noise. We speculate that this is because DDPM is trained on 2D images without explicit anatomical constraints and only generates 2D images. On the other hand, 3D-α-WGAN [18], which uses much larger 3D training data (146 for thorax and 414 for brain), has significantly worse FID and LPIPS scores than our method. Moreover, our proposed method outperforms Vid2Vid, showing the effectiveness of our semantic diffusion refiner. "
MedGen3D: A Deep Generative Framework for Paired 3D Image and Mask Generation,4.3,Evaluate the Benefits for Segmentation Task,"We explore the benefits of synthetic data for downstream segmentation tasks by comparing Sørensen-Dice coefficient (DSC) of 4 segmentation models, including Unet2D [23], UNet3D [8], UNETR [15], and Swin-UNETR [26]. In Table 2 and3, we utilize real training data (from SegTHOR and OASIS) and synthetic data to train the segmentation models with 5 different strategies, and test on all 3 thoracic CT datasets and 2 brain MRI datasets. In Table 4, we aim to demonstrate whether the synthetic data can aid transfer learning with limited real finetuning data from each of the testing datasets (StructSeg-Thorax, Public-Thor and ADNI) with four training strategies. According to Table 2 and Table 3, the significant DSC difference between 2D and 3D segmentation models underlines the crucial role of 3D annotated data. While purely synthetic data (E2-2) fails to achieve the same performance as real training data (E2-1), the combination of real and synthetic data (E2-3) improves model performance in most cases, except for Unet2D on the Public-Thor dataset. Furthermore, fine-tuning the pre-trained model with real data (E2-4 and E2-5) consistently outperforms the model trained only with real data. Please refer to Supplementary for organ-level DSC comparisons of the Swin-UNETR model with more details.According to Table 4, for transfer learning, utilizing the pre-trained model (E3-2) leads to better performance compared to training from scratch (E3-1). We have included video demonstrations of the generated 3D volumetric images in the supplementary material, which offer a more comprehensive representation of the generated image's quality."
MedGen3D: A Deep Generative Framework for Paired 3D Image and Mask Generation,5,Conclusion,"This paper introduces MedGen3D, a new framework for synthesizing 3D medical mask-image pairs. Our experiments demonstrate its potential in realistic data generation and downstream segmentation tasks with limited annotated data. Future work includes merging the image sequence generator and semantic diffusion refiner for end-to-end training and extending the framework to synthesize 3D medical images across modalities. Overall, we believe that our work opens up new possibilities for generating 3D high-quality medical images paired with masks, and look forward to future developments in this field."
MedGen3D: A Deep Generative Framework for Paired 3D Image and Mask Generation,,Fig. 1 .,
MedGen3D: A Deep Generative Framework for Paired 3D Image and Mask Generation,,Fig. 2 .,
MedGen3D: A Deep Generative Framework for Paired 3D Image and Mask Generation,,Fig. 3 .,
MedGen3D: A Deep Generative Framework for Paired 3D Image and Mask Generation,,4. 2,
MedGen3D: A Deep Generative Framework for Paired 3D Image and Mask Generation,,Fig. 4 .,
MedGen3D: A Deep Generative Framework for Paired 3D Image and Mask Generation,,Table 1 .,
MedGen3D: A Deep Generative Framework for Paired 3D Image and Mask Generation,,Table 2 .,
MedGen3D: A Deep Generative Framework for Paired 3D Image and Mask Generation,,Table 3 .,
MedGen3D: A Deep Generative Framework for Paired 3D Image and Mask Generation,,Table 4 .,
MedGen3D: A Deep Generative Framework for Paired 3D Image and Mask Generation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0 72.
PET Image Denoising with Score-Based Diffusion Probabilistic Models,1,Introduction,"Positron emission tomography (PET) is an imaging modality in nuclear medicine that has been successfully applied in oncology, neurology, and cardiology. By injecting a radioactive tracer into the human body, the molecular-level activity in tissues can be observed. To mitigate the radiation risk to the human body, it is essential to reduce the dose or shorten the scan time, leading to a low signalto-noise ratio and further negatively influencing the accuracy of diagnosis.Recently, the denoising diffusion probabilistic model (DDPM) [6,9,11] has become a hot topic in the generative model community. The original DDPM was designed for generation tasks, and many recent works have proposed extending it for image restoration or image-to-image translation. In supervised mode, Saharia et al. [8] proposed a conditional DDPM to perform single-image super-resolution, which integrates a low-resolution image into each reverse step. In unsupervised mode, to handle the stochasticity of the generative process, Choi proposed iterative latent variable refinement (ILVR) [1] to guarantee the given condition in each transition, thus generating images with the desired semantics. DDPM has also been applied in medical imaging. To explore its generalization ability, Song et al. [12] proposed a fully unsupervised model for medical inverse problems, providing the measuring process and the prior distribution learned with a scorebased generative model. For PET image denoising, Gong et al. [4] proposed two paradigms. One is directly feeding noisy PET images and anatomical priors (if available) into the score function network, which relies on paired high-quality and low-quality PET images. The other is feeding only MR images into the score function network while using noisy PET images in the inference stage under the assumption that PET image noise obeys a Gaussian distribution.In this paper, we propose a conditional diffusion probabilistic model for lowcount PET image denoising in an unsupervised manner without the Gaussian noise assumption or paired datasets. Our model is divided into two stages. In the training stage, we leverage the standard DDPM to train the score function network to learn a prior distribution of PET images. Once the network is trained, we transplant it into the sampling stage, in which we design two conditions to control the generation of high-count PET images given corresponding lowcount PET images. One condition is that the denoised versions of low-count PET images are similar to high-count PET images. The other condition is that when we add noise to high-count PET images, they degrade to low-count PET images. As a result, our model is named the bidirectional condition diffusion probabilistic model (BC-DPM). In particular, to simulate the formation of PET noise, we add noise in the sinogram domain. Additionally, the two proposed conditions are implemented in latent space. Notably, Our model is 'one for all', that is, once we have trained the score network, we can utilize this model for PET images with different count levels."
PET Image Denoising with Score-Based Diffusion Probabilistic Models,2,Method,"Letting X ⊂ X be a high-count PET image dataset and Y ⊂ Y be a low-count PET image dataset, x 0 and y 0 denote instances in X and Y , respectively. Our goal is to estimate a mapping F(Y) = X , and the proposed BC-DPM provides an unsupervised technique to solve this problem. BC-DPM includes two stages. In the training stage, it requires only X without paired (X, Y ), and in the sampling stage, it produces the denoised x 0 for a given y 0 ."
PET Image Denoising with Score-Based Diffusion Probabilistic Models,2.1,Training Stage,"BC-DPM acts the same as the original DDPM in the training stage, it consists of a forward process and a reverse process. In the forward process, x 0 is gradually contaminated by fixed Gaussian noise, producing a sequence of latent space data {x 1 , x 2 , ..., x T }, where x T ∼ N (0, I). The forward process can be described formally by a joint distribution q(x 1:T |x 0 ) given x 0 . Under the Markov property, it can be defined as:where {β 1 , β 2 , ..., β T } is a fixed variance schedule with small positive constants and I represents the identity matrix. Notably, the forward process allows x t to be sampled directly from x 0 :where ᾱt := t s=1 α s , α t := 1β t and ∼ N (0, I). The reverse process is defined by a Markov chain starting with p(x T ) = N (x T ; 0, I):(3) Given the reverse process, p θ (x 0 ) can be expressed by setting up an integral over the x 1:T variables p θ (x 0 ) := p θ (x 0:T )dx 1:T , and the parameter θ can be updated by optimizing the following simple loss function:The θ (x t , t) used in this paper heavily relies on that proposed by Dhariwal et al. [3]. The pseudocode for the training stage is given in Algorithm 1."
PET Image Denoising with Score-Based Diffusion Probabilistic Models,,Algorithm 1:,Training stage.until convergence
PET Image Denoising with Score-Based Diffusion Probabilistic Models,2.2,Sampling Stage,"The main difference between BC-DPM and the original DDPM lies in the sampling stage. Due to the stochasticity of the reverse process p θ (x 0:T ), it is difficult for the original DDPM to generate images according to our expectation. To overcome this obstacle, the proposed BC-DPM models p θ (x 0 |c) given condition c instead of modeling p θ (x 0 ) asCondition c derives from specific prior knowledge from the high-count PET image x 0 and the low-count PET image y 0 . With c, BC-DPM can control the generation of x 0 given y 0 . Then, the core problem is to design a proper condition c. A natural choice is D(y 0 ) ≈ x 0 , that is, the restoration task itself. We must clarify that it will not cause a 'deadlock' for the following two reasons. One is that the final form of the condition D(y 0 ) ≈ x 0 does not involve x 0 , and the other is that we choose a relatively simple denoiser in the condition, which can be viewed as a 'coarse to fine' operation. In practice, we utilize a Gaussian filter GF(•) as the denoiser in this condition. However, the Gaussian filter usually leads to smoothed images. Based on this property, we observe that the PSNR value between GF(y 0 ) and x 0 is usually inferior to that between GF(y 0 ) and GF(x 0 ), which means that the condition GF(y 0 ) ≈ GF(x 0 ) is more accurate than GF(y 0 ) ≈ x 0 . Thus, we choose GF(y 0 ) ≈ GF(x 0 ) in our experiments.However, if we only utilize the above condition, the training is unstable, and distortion may be observed. To address this problem, another condition needs to be introduced. The above condition refers to denoising, so conversely, we can consider adding noise to x 0 ; that is, y 0 ≈ A(x 0 ). According to the characteristics of PET noise, Poisson noise is used in the sinogram domain instead of the image domain. We define this condition as P † (P o(P(x 0 ) + r + s)) ≈ y 0 , where P, P o, P † , r and s represent the Radon transform, Poisson noise insertion, inverse Radon transform, random coincidence and scatter coincidence, respectively. Now, we have two conditions GF(y 0 ) ≈ GF(x 0 ) and P † (P o(P(x 0 )+r + s)) ≈ y 0 from the perspectives of denoising and noise insertion, respectively. Since the conditions involve x 0 , we have to convert the conditions from the original data space into latent space under certain circumstances to avoid estimating x 0 . Let us denote each transition in the reverse process under global conditions as:In Eq. ( 2), x t can be represented by a linear combination of x 0 and . Then, we can express x 0 with x t and :Similarly, applying the same diffusion process to y 0 , we have {y 1 , y 2 , ..., y T }, and y 0 can be expressed with y t and :Replacing x 0 and y 0 with f θ (x t , t) and f θ (y t , t) in Eq. ( 6), respectively, we have:Assume that+ λ(y t-1 + x t-1 -P † (P o(P(x t-1 ) + r + s)), (10) where x t-1 is sampled from p θ (x t-1 |x t ), and λ ∈ [0, 1] is a balancing factor between the two conditions. Thus, we have E q(y t-1 |y 0 ) [p θ (x t-1 |xt, GF(x t-1 ) = GF(y t-1 ), P † (P o(P(x t-1 ) + r + s)) = y t-1 )] ≈ p θ (x t-1 |xt, GF(x t-1 ) = GF(y t-1 ), P † (P o(P(x t-1 ) + r + s)) = y t-1 ).(11)Finally, we have p θ (x t-1 |x t , GF(x 0 ) = GF(y 0 ), P † (P o(P(x 0 ) + r + s)) = y 0 ) = p θ (x t-1 |x t , GF(x t-1 ) = GF(y t-1 ), P † (P o(P(x t-1 ) + r + s)) = y t-1 ), (12) which indicates that under the assumption of Eq. ( 10), the global conditions on (x 0 , y 0 ) can be converted to local conditions on (x t-1 , y t-1 ) in each transition from x t to x t-1 . Now, given a low-count PET image y 0 , to estimate x 0 , we can sample from white noise x T using the following two steps iteratively. The first step is to generate an immediate x t-1 from p θ (x t-1 |x t ). The second step is to generate x t-1 from x t-1 using Eq. (10). In practice, we note that there is no need to operate the two local conditions in each transition; instead, we only need the last l transitions. Generally speaking, The larger l is, the more blurred the image will be. As l decreases, the image gets more noisy. We provide the sampling procedure of BC-DPM in Algorithm 2.Algorithm 2: Sampling stage. Figure 1 illustrates the whole model. In the training stage, q denotes fixed Gaussian noise for the forward process, and p θ denotes a learned transition in the reverse process. Once p θ is trained, it is moved to the sampling stage. In the sampling stage, we first use the same q to diffuse y 0 to {y 1 , y 2 , ..., y T }. Then, we start with white noise x T followed by a transition from x t+1 to x t for each t ∈ {0, 1, 2, ..., T -1}. Each transition consists of p θ and a conditional block. p θ is responsible for sampling an immediate x t from x t+1 . Then, the conditional block takes x t and y t as inputs and outputs x t . Figure 2 shows the detailed structure of the conditional block. There are two parallel branches. One calculates the difference between GF(x t ) and GF(y t ), which represents the condition of denoising, and the other computes the difference between x t and y t , where x t is derived by adding noise to x t in the sinogram domain. Then, we sum the two branches weighted by λ and subtract x t to output the final result x t ."
PET Image Denoising with Score-Based Diffusion Probabilistic Models,3,Experiment,
PET Image Denoising with Score-Based Diffusion Probabilistic Models,3.1,Experimental Setup,"To evaluate the proposed method, real clinical data downloaded from TCIA were tested [2]. The computer simulation modeled the geometry of a CTI ECAT 921 PET scanner, and the system matrix P was modeled using Siddon's refined method to calculate the ray path integral [5]. To simulate low-count PET images, we first generated a noise-free sinogram by forward projecting the original data, obtaining a sinogram with a matrix size of 160 (radial bins) × 192 (azimuthal angles). Then, uniform random events were added to the noise-free sinogram as background, which accounted for 20% of total true coincidences. Independent Poisson noise with different levels was injected, raising the total number of events to 1M, 0.3M, and 0.1M, respectively. Finally, these sinograms were reconstructed by the ML-EM algorithm with 100 iterations. We used 3000 2D slices from 60 patients as the training set and 400 slices from another 10 patients as the test set.Our method was implemented with PyTorch on a GeForce GTX 1080Ti GPU. We trained the network using the AdamW algorithm with β 1 = 0.9, β 2 = 0.999, and weight decay = 0.01. The learning rate was set to 0.0001, and the batch size was 8. In our experiments, similar to DDPM, we set the number of diffusion steps to T = 1000. For the variance schedule in the forward process, we employed a linear schedule from β 1 = 0.0001 to β T = 0.02. In the sampling stage, we evenly sampled 100 steps from 1 to T and then performed generation only on these 100 steps, reducing the number of steps from 1000 to 100 by employing the trick in [7]. For the count levels of 1M, 0.3M, and 0.1M in the real clinical data study, we set l to 5, 10, and 15, respectively. In all cases, we set λ = 0.2 to balance the two conditions. As the diffusion model can generate different results due to stochasticity, we ran the model five times and used the average of the five results as the final result.We compared our method with two conventional methods, Gaussian Filter and BM3D, and two unsupervised/unpaired methods, Noise2Void with parameter transfer (N2V-PT) [10] and unsupervised CycleWGAN [13]."
PET Image Denoising with Score-Based Diffusion Probabilistic Models,3.2,Experimental Results,"Figure 3 shows the results using various methods at three count levels. It can be observed that our method obtains the best performance in all cases. At the 1M  count level, the noise is small, and all methods obtain adequate results. At the 0.3M count level, the noise becomes higher. The Gaussian filter compromises the details for noise reduction. N2V-PT exhibits strange patterns due to the violation of the pixel independence assumption in PET noise. At the extremely low-count level, 0.1M, the Gaussian filter and N2V-PT cannot obtain clinically useful results, while our method can accurately recover some details due to its strong capacity for generation under these conditions. Table 1 reports the quantitative results, showing that our proposed BC-DPM outperforms other methods in terms of both PSNR and SSIM."
PET Image Denoising with Score-Based Diffusion Probabilistic Models,4,Conclusion,"In conclusion, a PET denoising model based on diffusion probabilistic models is proposed in this paper. Our model is trained in an unsupervised manner and denoises low-count PET images without any anatomical prior as a reference. To enable the DPM to generate high-count PET images from corresponding lowcount PET images, we design bidirectional conditions derived from relations between the low-count image and the potential high-count image. One condition is that the denoised low-count image approximates the high-count image. The other is that after adding noise, the high-count image approximates the lowcount image. For implementation, we transfer the bidirectional conditions to latent space, which helps free the model from its dependence on the high-count image. Experiments on real clinical data demonstrate that our model is superior in noise suppression and detail preservation to other state-of-the-art methods."
PET Image Denoising with Score-Based Diffusion Probabilistic Models,,Fig. 1 .,
PET Image Denoising with Score-Based Diffusion Probabilistic Models,,Fig. 2 .,
PET Image Denoising with Score-Based Diffusion Probabilistic Models,,Fig. 3 .,
PET Image Denoising with Score-Based Diffusion Probabilistic Models,,Table 1 .,
PET Image Denoising with Score-Based Diffusion Probabilistic Models,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0 26.
CL-ADDA: Contrastive Learning with Amplitude-Driven Data Augmentation for fMRI-Based Individualized Predictions,1,Introduction,"In combination with machine learning techniques, functional magnetic resonance imaging (fMRI) has recently been widely used in predictions of individual traits (e.g., age and intelligence quotient (IQ)) [8,12] and classifications of neuropsychiatric disorders (e.g., Alzheimer's disease). Representations of human brain function are essential for such predictions, as effective representations can provide information discriminative among individuals and facilitate the final predictions.Contrastive learning techniques can be favorable choices for representations of human brain function [1,3,4,10,11], if the available samples size is large enough for large-batch training (this is not the fact for most fMRI datasets). Contrastive learning can learn effective representations through minimizing/maximizing the distance between similar/dissimilar samples in the representation space [4,11]. SimSiam [5] is a contrastive learning framework that maximizes the similarity between two augmentations of one image. With the involvement of a Siamese structure, SimSiam does not rely on large-batch training. Accordingly, SimSiam can be a favorable choice for fMRI-based representations of human brain function.Generation of similar/dissimilar samples is critical for contrastive learning [4]. Among the few studies on fMRI-based individualized predictions in which contrastive learning is involved [7,9,13,21], fMRI data has often been augmented using classic methods in the region of computer vision, such as random erasing, random cropping and adversarial generation. In the pioneering study [21], two highly similar augmented samples were generated for each subject by excerpting two non-overlapping fMRI temporal segments. The highly similar pairs containing redundancy information can lead to poor performance of contrastive learning [2,24]. Recent neuroscience findings provide an intuitive idea regarding data augmentation for RS-fMRI data. Specifically, fMRI frames with high-and low-amplitude were reported to be of quite different functional significance [23]. Accordingly, a ""discrepant-enough"" positive pair can be generated by acquiring one sample based on fMRI frames with high-amplitude and the other based on frames with low-amplitude.In this study, we proposed a framework named contrastive learning with amplitudedriven data augmentation (CL-ADDA) for effective representations of human brain function and ultimately fMRI-based individualized predictions. Two augmented samples of CL-ADDA were generated through excerpting fMRI frames with relatively high amplitude and those with relatively low amplitude. With two augmented samples of the same subject used as inputs, a SimSiam-based contrastive learning framework was used to learn effective representations of human brain function. For the consideration that label information can guide SimSiam to learn more prediction-relevant representation [15,25], individualized predictions were performed in an end-to-end way through concatenated fully connected layers.Our major contributions are as follows:-SimSiam was utilized to learn representations of human brain function.-A neuroscience-oriented amplitude-driven data augmentation method was introduced to generate positive pairs. -Predictions were made in an end-to-end way to improve the generalizability of the predictive models. -CL-ADDA outperformed a variety of state-of-the-art methods for fMRI-based individualized predictions."
CL-ADDA: Contrastive Learning with Amplitude-Driven Data Augmentation for fMRI-Based Individualized Predictions,2,Method,
CL-ADDA: Contrastive Learning with Amplitude-Driven Data Augmentation for fMRI-Based Individualized Predictions,2.1,Overall Workflow of CL-ADDA,"Figure 1 shows the workflow of the proposed CL-ADDA. fMRI data of one subject is first excerpted into two segments, one composed of frames with high-amplitude and the other composed of frames with low-amplitude. Two functional connectivity (FC) maps (FC high and FC low ) can then be obtained based on the augmented samples, and the two FC maps are used as inputs for the contrastive learning module. The SimSiam structure is used to perform contrastive learning in this study, and the classic convolution in SimSiam is replaced by row and column convolutions to adapt to FC maps. SimSiam learns representations of brain function (r high and r low ) based on the two FC maps, using encoders (F) with shared parameters. Individualized predictions can be made through a predictor ( ) (three fully connected layers in this study) based on the learned representations. The whole model is trained though simultaneously minimizing the distance between the two representations (r high and r low ) and the difference between the predicted and actual label (individual traits in this study). "
CL-ADDA: Contrastive Learning with Amplitude-Driven Data Augmentation for fMRI-Based Individualized Predictions,2.2,Amplitude-Driven Data Augmentation,"Figure 2 provides an illustration of the amplitude-driven data augmentation method. In this study, we generated two augmented samples for each subject based on the amplitude of fMRI frames following [23]. Specifically, we first defined N ROIs and extracted the mean time series of each ROI. We then z-scored each time series and obtained frame-wise co-fluctuation of each ROI pair as follows:where e t jk is the co-fluctuation of ROIs-j and k at time t; x t j x t k is the z-scored fMRI signal amplitude of ROI-j (-k) at time t; e jk is obtained the co-fluctuation time series, and T is the length of fMRI time series. Accordingly, a C  "
CL-ADDA: Contrastive Learning with Amplitude-Driven Data Augmentation for fMRI-Based Individualized Predictions,2.3,Contrastive Learning on Functional Connectivity Maps,"We constructed the contrastive learning model based mainly on SimSiam. Setting no requirement on large batches, SimSiam can be a favorable choice for fMRI-based representation learning [5]. As shown in Fig. 1, the SimSiam structure in this study consists of two branches of the same encoders (F) with shared weights for encoding the two input FC maps in parallel, followed by the same nonlinear projectors (G) with share weights for further processing the brain function representations from the encoders, and one predictor (H) on one branch for transforming the output of the branch to match it to the output of the other branch. A stop-gradient operation is applied on the branch without predictor to avoid model collapsing [5].For the consideration that spatial locality does not exist among adjacent elements on FC maps [14], row and column convolutions were used to construct the backbone of the encoder (F) and extract effective information from the FC maps. Specifically, each encoder (F) in this study consists of one row convolution layer (with C r @1 × N row convolution filter, C r is the channel number, N is the ROI number) and one column convolution layer (with C c @N × 1 column convolution filters, C c is the channel number, outputs C c @1 × 1 representation). Information throughout the brain is expected to be integrated with the use of row and column convolutions. The row and column convolution layers in CL-ADDA are each followed by a batch normalization layer and a Leaky_ReLU layer."
CL-ADDA: Contrastive Learning with Amplitude-Driven Data Augmentation for fMRI-Based Individualized Predictions,2.4,Individualized Prediction and Loss Function,"As shown in Fig. 1, individualized prediction in this study was implemented using two parallel predictors ( ) with shared weights. Each predictor (( )) consists of three fully connected layers, which transform the C c outputs from the corresponding encoder (F) (learned representations of brain function, r high and r low ∈ R C c in Fig. 1) into the predicted individual trait for the branch. For model training, the final prediction is made based on a weighted sum of the predictions from the two branches as follows:where ξ ∈ R N×N denotes a FC map, F denotes the encoder, and α is a hyper-parameter.In the model testing stage, prediction can directly be made as ŷ = (F(ξ )), where ξ can be a FC map calculated based on the whole fMRI scan (rather than augmented data).The whole loss function for CL-ADDA includes two parts: contrastive loss and prediction loss. Contrastive learning minimizes the negative cosine similarity between the two branches:where, G denotes the projector in Fig. 1, and H denotes the predictor in the contrastive learning (Fig. 1); ||.|| 2 is L2-norm.Following [25], we defined contrastive loss as:L1 loss was used as the prediction loss:where y and ŷ are the actual and predicted labels, respectively. The total loss was defined as follows:where λ is hyper-parameter."
CL-ADDA: Contrastive Learning with Amplitude-Driven Data Augmentation for fMRI-Based Individualized Predictions,3,Experiments and Results,
CL-ADDA: Contrastive Learning with Amplitude-Driven Data Augmentation for fMRI-Based Individualized Predictions,3.1,Dataset,"The resting-state fMRI data included in the dataset collected and released by the Cambridge Centre for Ageing and Neuroscience (Cam-CAN) [19] was used in this study.The public dataset contains multi-modal data from a large cohort of adult lifespan population-based samples. After removing the subjects with excessive head motions (translation/rotation more than 2.0 mm/2.0°in/around any of the x, y, or z directions) throughout the scan, 600 subjects remained (18-87, 53.900 ± 18.549 years), and IQ scores of 568 of them were available 32.032 ± 6.762). We preprocessed the data to remove the spatial and temporal artifacts and register the images to standard space (MNI) using FSL. We defined 200 ROIs based on independent component analysis and extracted the ROI time series through regressing the spatial maps of the independent components released by the Human Connectome Project (HCP) [20] against the preprocessed fMRI data using the dual_regress command included in FSL. Later analyses were all based on the extracted ROI time series."
CL-ADDA: Contrastive Learning with Amplitude-Driven Data Augmentation for fMRI-Based Individualized Predictions,3.2,The Performance of CL-ADDA,"Age and IQ predictions were taken as test cases to evaluate the performance of the proposed method, based on the Cam-CAN dataset. Amplitude-driven data augmentation was performed on the 200 ROI time series of each subject, and two FC maps were obtained based on two augmented samples for each subject. The two augmented FC maps were used as inputs for SimSiam for representation learning and later individualized predictions. We performed 1000 epochs of model training, with the batch size set to 128.For the consideration that high-amplitude FC maps (FC high ) may carry more detailed information about individuals' brain function, we empirically weight the predictions based on FC high more, by setting the α in Eq. ( 2) to 0.8 The hyper-parameter λ in Eq. ( 6) was set to 0.5. Adam optimizer with a learning rate of 0.001 was used. Ten-fold cross-validation was used to evaluate the performance of CL-ADDA, and Pearson's correlation coefficient (r) and mean absolute error (MAE) between the predicted and actual labels were used to quantitatively measure this performance. The results show that CL-ADDA performed well on both age and IQ predictions, as indicated by an r-value (MAE) of 0.886 (6.992 years) for age prediction, and 0.620 (4.531) for IQ prediction."
CL-ADDA: Contrastive Learning with Amplitude-Driven Data Augmentation for fMRI-Based Individualized Predictions,3.3,Comparison Experiments,"We compared the performance of our proposed method with six deep learning methods for fMRI-based individualized predictions, namely, spatial-temporal graph convolutional network (ST-GCN) [22], RNN based on gated recurrent units (GRU) [6], pooling regularized graph neural network (PR-GNN) [16], brain graph neural network (BrainGNN) [17], simple fully convolutional network (SFCN) [18], and BrainNetCNN [14]. Each of the six methods has been reported to perform well on fMRI-based individualized predictions, or even provide state-of-the-art results. Each method was implemented based on its online code, with the hyper-parameters set according to its original paper.Table 1 is a list of prediction accuracies based on the seven methods (including CL-ADDA). According to Table 1, CL-ADDA outperformed the methods for comparison by large margins. For instance, compared with the second best (ST-GCN), CL-ADDA demonstrated an r-value increase of 0.085 for IQ prediction. "
CL-ADDA: Contrastive Learning with Amplitude-Driven Data Augmentation for fMRI-Based Individualized Predictions,3.4,Ablation Experiments,"To evaluate the effectiveness of the proposed amplitude-driven data augmentation strategy, we performed age and IQ predictions with the data augmented using classic methods, and the strategy of excerpting non-overlapping segments as proposed in [21]. For classic augmentation, two 175 × 175 augmented FC maps were generated by applying random cropping and Gaussian blurring on the 200 × 200 FC matrix calculated based on the whole fMRI scan [5]. For the non-overlapping segment excerpting strategy, we generated two 200 × 200 augmented FC matrices based on the two non-overlapping fMRI data segments excerpted from the same scan [21]. According to Table 2, the proposed amplitude-driven data augmentation is obviously superior to the other two data augmentation strategies. We further evaluated the effectiveness of contrastive learning, as well as the endto-end individualized prediction strategy. Specifically, (1) we performed individualized age and IQ predictions based on a network composed of one encoder (F) and one predictor (F) to imitate a network with contrastive learning removed. (2) We pre-trained CL-ADDA and then predicted age and IQ using the representations based on the pretrained CL-ADDA to imitate abandoning the end-to-end individualized prediction strategy. According to Table 3, both contrastive learning and the end-to-end individualized prediction strategy were critical for CL-ADDA. The results indicate that supervised contrastive learning can be a favorable choice for neuroimage-based individualized predictions and neuropsychiatric disease classifications. "
CL-ADDA: Contrastive Learning with Amplitude-Driven Data Augmentation for fMRI-Based Individualized Predictions,4,Conclusion,"In this study, we proposed CL-ADDA for effective representation learning and ultimately precise fMRI-based individualized predictions. Originating from a recent neuroscientific finding, the proposed amplitude-driven data augmentation method provides the contrastive learning module discrepant-enough positive pairs for effective representation learning. SimSiam-based contrastive learning enables effective representation learning on fMRI dataset including limited samples. We evaluated the performance of CL-ADDA with age and IQ predictions based on a public dataset, and the experiments demonstrate that CL-ADDA achieved state-of-the-art predictions for both age and IQ."
CL-ADDA: Contrastive Learning with Amplitude-Driven Data Augmentation for fMRI-Based Individualized Predictions,,Fig. 1 .,
CL-ADDA: Contrastive Learning with Amplitude-Driven Data Augmentation for fMRI-Based Individualized Predictions,,,
CL-ADDA: Contrastive Learning with Amplitude-Driven Data Augmentation for fMRI-Based Individualized Predictions,,Fig. 2 .,
CL-ADDA: Contrastive Learning with Amplitude-Driven Data Augmentation for fMRI-Based Individualized Predictions,,Table 1 .,
CL-ADDA: Contrastive Learning with Amplitude-Driven Data Augmentation for fMRI-Based Individualized Predictions,,Table 2 .,
CL-ADDA: Contrastive Learning with Amplitude-Driven Data Augmentation for fMRI-Based Individualized Predictions,,Table 3 .,
Cross-Dataset Adaptation for Instrument Classification in Cataract Surgery Videos,1,Introduction,"Surgical instrument identification and classification are critical to deliver several priorities in surgical data science [21]. Various deep learning methods have been developed to classify instruments in surgical videos using data routinely generated in institutions [2]. However, differences in image capture systems and protocols lead to nontrivial dataset shifts, causing a significant drop in performance of the deep learning methods when tested on new datasets [13]. Using cataract surgery as an example, Fig. 1 illustrates the drop in accuracy of existing methods to classify instruments when trained on one dataset and tested on another dataset [19,28]. Cataract surgery is one of the most common procedures [18], and methods to develop generalizable networks will enable clinically useful applications.Fig. 1. Dataset shift between the CATARACTS dataset (CAT) [6] and D99 [7,9] dataset. Results for models trained on one dataset and tested on another show a significant drop in performance.Domain adaptation methods aim to attempt to mitigate the drop in algorithm performance across domains [13]. Unsupervised Domain Adaptation (UDA) methods are particularly useful when the source dataset is labeled and the target dataset is unlabeled. In this paper, we describe a novel end-to-end UDA method, which we call the Barlow Adaptor, and its application for instrument classification in video images from cataract surgery. We define a novel loss function called the Barlow Feature Alignment Loss (BFAL) that aligns the features learnt by the model between the source and target domains, without requiring any labeled target data. It encourages the model to learn non-redundant features that are domain agnostic and thus tackles the problem of UDA. BFAL can be added as an add-on to existing methods with minimal code changes. The contributions of our paper are threefold:1. We define a novel loss for feature alignment called BFAL that doesn't require large batch sizes and encourages learning non-redundant, domain agnostic features. 2. We use BFAL to generate an end-to-end system called the Barlow Adaptor that performs UDA. We evaluate the effectiveness of this method and compare it with existing UDA methods for instrument classification in cataract surgery images. 3. We motivate new research on methods for generalizable deep learning models for surgical instrument classification using cataract surgery as the test-bed.Our work proposes a solution to the problem of lack of generalizability of deep learning models that was identified in previous literature on cataract surgery instrument classification."
Cross-Dataset Adaptation for Instrument Classification in Cataract Surgery Videos,2,Related Work,"Instrument Identification in Cataract Surgery Video Images. The motivation for instrument identification is its utility in downstream tasks such as activity localization and skill assessment [3,8,22]. The current state-of-the-art instrument identification method called Deep-Phase [28] uses a ResNet architecture to identify instruments and then to identify steps in the procedure. However, a recent study has shown that while these methods work well on one dataset, there is a significant drop in performance when tested on a different dataset [16]. Our analyses reiterate similar findings on drop in performance (Fig. 1) and highlight the effect of domain shift between data from different institutions even for the same procedure.Unsupervised Domain Adaptation. UDA is a special case of domain adaptation, where a model has access to annotated training data from a source domain and unannotated data from a target domain [13]. Various methods have been proposed in the literature to perform UDA. One line of research involves aligning the feature distributions between the source and target domains. Maximum Mean Discrepancy (MMD) is commonly used as a distance metric between the source and target distributions [15]. Other UDA methods use a convolutional neural network (CNN) to generate features and then use MMD as an additional loss to align distributions [1,11,12,20,25,27]. While MMD is a first-order statistic, Deep CORAL [17] penalizes the difference in the second-order covariance between the source and target distributions. Our method uses feature alignment by enforcing a stricter loss function during training.Another line of research for UDA involves adversarial training. Domain Adaptive Neural Network (DANN) [5] involves a minimax game, in which one network minimizes the cross entropy loss for classification in the source domain, while the other maximizes the cross entropy loss for domain classification. Few recent methods generate pseudo labels on the target domain and then train the network on them. One such method is Source Hypothesis Transfer (SHOT) [10], which performs source-free domain adaptation by further performing information maximization on the target domain predictions. While CNN-based methods are widely popular for UDA, there are also methods which make use of the recently proposed Vision Transformer (ViT) [4], along with an ensemble of the above described UDA based losses. A recent approach called Cross Domain Transformer (CDTrans) uses cross-domain attention to produce pseudo labels for training that was evaluated in various datasets [24]. Our proposed loss function is effective for both CNN and ViT-based backbones."
Cross-Dataset Adaptation for Instrument Classification in Cataract Surgery Videos,3,Proposed Method,"In the UDA task, we are given n s observations from the source domain D S . Each of these observations is in the form of a tuple (x s , y s ), where x s denotes an image from the source training data and y s denotes the corresponding label, which is the instrument index present in the image. In addition, we are given n t observations from the target domain D T . Each of these can be represented by x t , which represents the image from the target training data. However, there are no labels present for the target domain during training. The goal of UDA is to predict the labels y t for the target domain data."
Cross-Dataset Adaptation for Instrument Classification in Cataract Surgery Videos,,Barlow Feature Alignment Loss (BFAL).,"We introduce a novel loss, which encourages features between the source and target to be similar to each other while reducing the redundancy between the learnt features. BFAL works on pairs of feature projections of the source and target. More specifically, let f s ∈ R BXD and f t ∈ R BXD be the features corresponding to the source and target domain, respectively. Here B represents the batch size and D represents the feature dimension. Similar to [26], we project these features into a P dimensional space using a fully connected layer called the Projector, followed by a batch normalization to whiten the projections. Let the resultant projections be denoted by p s ∈ R BXP for the source and p t ∈ R BXP for the target domains. Next, we compute the correlation matrix C 1 ∈ R P XP . Each element of C 1 is computed as followsFinally, the BFAL is computed using the L2 loss between the elements of C 1 and the identity matrix I as followswhere μ is a constant. Intuitively, the first term of the loss function can be thought of as a feature alignment term since we push the diagonal elements in the covariance matrix towards 1. In other words, we encourage the feature projections between the source and target to be perfectly correlated. On the other hand, by pushing the off-diagonal elements to 0, we decorrelate different components of the projections. Hence, this term can be considered a redundancy reduction term, since we are pushing each feature vector component to be independent of one another. BFAL is inspired by a recent technique in self-supervised learning, called the Barlow Twins [26], where the authors show the effectiveness of such a formulation at lower batch sizes. In our experiments, we observe that even keeping a batch size of 16 gave good results over other existing methods. Furthermore, BFAL does not require large amounts of data to converge. Barlow Adaptor. We propose an end-to-end method that utilizes data from the labeled source domain and the unlabeled target domain. The architecture corresponding to our method is shown in Fig. 2.There are two main sub-parts of the architecture -the Feature Extractor F , and the Source Classifier C. First, we divide the training images randomly into batches of pairs {x s , x t } and apply F on them, which gives us the features extracted from these sets of images. For the Feature Detector, we show the effectiveness of our novel loss using ViT and ResNet50 both of which have been pre-trained on ImageNet. The features obtained are denoted as f s and f t for the source and target domains, respectively. Next, we apply C on these features to get logits for the classification task. The source classifier is a feed forward neural network, which is initialized from scratch. These logits are used, along with the source labels y s to compute the source cross entropy loss as, where M represents the number of classes, B represents the total minibatches, while m and b represent their respective indices.The features f s and f t are further used to compute the Correlation Alignment(CORAL) loss and the BFAL, which enforce the feature extractor to align its weights so as to learn features that are domain agnostic as well as nonredundant. The BFAL is calculated as mentioned in the previous subsection. The CORAL loss is computed as depicted in Eq. 4, following the UDA method Deep CORAL [17]. While the BFAL focuses on reducing redundancy, CORAL works by aligning the distributions between the source and target domain data. This is achieved by taking the difference between the covariance matrices of the source and target features -f s and f t respectively. The final loss is the weighted sum of the three individual losses as follows:whereEach of these three losses plays a different role in the UDA task. The cross entropy loss encourages the model to learn discriminative features between images with different instruments. The CORAL loss pushes the features between the source and target towards having a similar distribution. Finally, the BFAL tries to make the features between the source and the target non-redundant and same. BFAL is a stricter loss than CORAL as it forces features to not only have the same distribution but also be equal. Further, it also differs from CORAL in learning independent features as it explicitly penalizes non-zero non-diagonal entries in the correlation matrix. While using BFAL alone gives good results, using it in addition to CORAL gives slightly better results empirically. We note these observations in our ablation studies. Between the cross entropy loss and the BFAL, an adversarial game is played where the former makes the features more discriminative and the latter tries to make them equal. The optimal features thus learnt are different in aspects required to identify instruments but are equal for any domain-related aspect. This property of the Barlow Adaptor is especially useful for surgical domains where the background has similar characteristics for most of the images within a domain. For example, for cataract surgery images, the position of the pupil or the presence of blood during the usage of certain instruments might be used by the model for classification along with the instrument features. These features depend highly upon the surgical procedures and the skill of the surgeon, thus making them highly domain-specific and possibly unavailable in the target domain. Using BFAL during training attempts to prevent the model from learning such features."
Cross-Dataset Adaptation for Instrument Classification in Cataract Surgery Videos,4,Experiments and Results,"We evaluate the proposed UDA method for the task of instrument classification using two cataract surgery image datasets. In our experiments, one dataset is used as the source domain and the other is used as the target domain. We use micro and macro accuracies as our evaluation metrics. Micro accuracy denotes the number of correctly classified observations divided by the total number of observations. In contrast, macro accuracy denotes the average of the classwise accuracies and is effective in evaluating classes with less number of samples. Datasets. The first dataset we use is CATARACTS [6], which consists of 50 videos with framewise annotations available for 21 surgical instruments. The dataset is divided into 25 training videos and 25 testing videos. We separate 5 videos from the training set and use them as the validation set for our experiments. The second dataset is called D99 in this work [7,9], which consists of 105 videos of cataract surgery with annotations for 25 surgical instruments. Of the 105 videos, we use 65 videos for training, 10 for validation and 30 for testing. We observe a significant distribution shift between the two datasets as seen in Fig. 1. This is caused by several factors such as lighting, camera resolution, and differences in instruments used for the same steps. For our experiments in this work, we use 14 classes of instruments that are common to both datasets. Table 1 shows a mapping of instruments between the two datasets. For each dataset, we normalize the images using the means and standard deviations calculated from the respective training images. In addition, we resize all images to 224 × 224 size and apply random horizontal flipping with a probability of 0.5 before passing them to the model.Experimental Setup. We train the Barlow Adaptor for multi-class classification with the above-mentioned 14 classes in Pytorch. For the Resnet50 backbone, we use weights pretrained on Imagenet [14] for initialization. For the ViT backbone, we use the base-224 class of weights from the TIMM library [23]. The Source Classifier C and the Projector P are randomly initialized. We use the validation sets to select the hyperparameters for the models. Based on these empirical results, we choose λ from Eq. 3 to be 0.001 and μ from Eq. 2 to be 0.0039. We use SGD as the optimizer with momentum of 0.9 and a batch size of 16. We start the training with a learning rate of 0.001 and reduce it by a factor of 0.33 every 20 epochs. The entire setup is trained with a single NVIDIA Quatro RTX 8000 GPU. We use the same set of hyperparameters for the CNN and ViT backbones in both datasets.Results. Table 2 shows results comparing the performance of the Barlow Adaptor with recent UDA methods. We highlight the effect of domain shift by comparing the source-only models and the target-only models, where we observe a significant drop of 27% and 43% in macro accuracy for the CATARACTS dataset and the D99 dataset, respectively. Using the Barlow Adaptor, we observe an increase in macro accuracy by 7.2% over the source only model. Similarly, we observe an increase in macro accuracy of 9% with the Barlow Adaptor when the source is CATARACTS and the target is the D99 dataset compared with the source only model. Furthermore, estimates of macro and micro accuracy are larger with the Barlow Adaptor than those with other existing methods. Finally, improved accuracy with the Barlow Adaptor is seen with both ResNet and ViT backbones.Ablation Study. We tested the performance gain due to each part of the Barlow Adaptor. Specifically, the Barlow Adaptor has CORAL loss and BFAL as its two major feature alignment losses. We remove one component at a time and observe a decrease in performance with both ResNet and ViT backbones (Table 3). This shows that each loss has a part to play in domain adaptation. Further ablations are included in the supplementary material."
Cross-Dataset Adaptation for Instrument Classification in Cataract Surgery Videos,5,Conclusion,"Domain shift between datasets of cataract surgery images limits generalizability of deep learning methods for surgical instrument classification. We address this limitation using an end-to-end UDA method called the Barlow Adaptor. As part of this method, we introduce a novel loss function for feature alignment called the BFAL. Our evaluation of the method shows larger improvements in classification performance compared with other state-of-the-art methods for UDA. BFAL is an independent module and can be readily integrated into other methods as well. BFAL can be easily extended to other network layers and architectures as it only takes pairs of features as inputs."
Cross-Dataset Adaptation for Instrument Classification in Cataract Surgery Videos,,Fig. 2 .,
Cross-Dataset Adaptation for Instrument Classification in Cataract Surgery Videos,,Table 1 .,
Cross-Dataset Adaptation for Instrument Classification in Cataract Surgery Videos,,Table 2 .,
Cross-Dataset Adaptation for Instrument Classification in Cataract Surgery Videos,,Table 3 .,
Gall Bladder Cancer Detection from US Images with only Image Level Labels,1,Introduction,"GBC is a deadly disease that is difficult to detect at an early stage [12,15]. Early diagnosis can significantly improve the survival rate [14]. Non-ionizing radiation, low cost, and accessibility make US a popular non-invasive diagnostic modality for patients with suspected gall bladder (GB) afflictions. However, identifying signs of GBC from routine US imaging is challenging for radiologists [11]. In recent years, automated GBC detection from US images has drawn increased interest [3,5] due to its potential for improving diagnosis and treatment outcomes. Many of these works formulate the problem as an object detection, since training a image classification model for GBC detection seems challenging due to the reasons outlined in the abstract (also see Fig. 1). Recently, GBCNet [3], a CNN-based model, achieved SOTA performance on classifying malignant GB from US images. GBCNet uses a two-stage pipeline consisting of object detection followed by classification, and requires bounding box annotations for GB as well as malignant regions for training. Such bounding box annotations surrounding the pathological regions are time-consuming and require an expert radiologist for annotation. This makes it expensive and non-viable for curating large datasets for training large DNN models. In another recent work, [5] has exploited additional unlabeled video data for learning good representations for downstream GBC classification and obtained performance similar to [3] using a ResNet50 [13] classifier. The reliance of both SOTA techniques on additional annotations or data, limits their applicability. On the other hand, the image-level malignancy label is usually available at a low cost, as it can be obtained readily from the diagnostic report of a patient without additional effort from clinicians.Instead of training a classification pipeline, we propose to solve an object detection problem, which involves predicting a bounding box for the malignancy. The motivation is that, running a classifier on a focused attention/ proposal region in an object detection pipeline would help tackle the low inter-class and high intra-class variations. However, since we only have image-level labels available, we formulate the problem as a Weakly Supervised Object Detection (WSOD) problem. As transformers are increasingly outshining CNNs due to their ability to aggregate focused cues from a large area [6,9], we choose to use transformers in our model. However, in our initial experiments SOTA WSOD methods for transformers failed miserably. These methods primarily rely on training a classification pipeline and later generating activation heatmaps using attention and drawing a bounding box circumscribing the heatmaps [2,10] to show localization. However, for GBC detection, this line of work is not helpful as we discussed earlier.Inspired by the success of the Multiple Instance Learning (MIL) paradigm for weakly supervised training on medical imaging tasks [20,22], we train a detection transformer, DETR, using the MIL paradigm for weakly supervised malignant region detection. In this, one generates region proposals for images, and then considers the images as bags and region proposals as instances to solve the instance classification (object detection) under the MIL constraints [8]. At inference, we use the predicted instance labels to predict the bag labels. Our experiments validate the utility of this approach in circumventing the challenges in US images and detecting GBC accurately from US images using only image-level labels."
Gall Bladder Cancer Detection from US Images with only Image Level Labels,,Contributions:,"The key contributions of this work are:-We design a novel DETR variant based on MIL with self-supervised instance learning towards the weakly supervised disease detection and localization task in medical images. Although MIL and self-supervised instance learning has been used for CNNs [24], such a pipeline has not been used for transformerbased detection models. -We formulate the GBC classification problem as a weakly supervised object detection problem to mitigate the effect of low inter-class and large intra-class variances, and solve the difficult GBC detection problem on US images without using the costly and difficult to obtain additional annotation (bounding box) or video data. -Our method provides a strong baseline for weakly supervised GBC detection and localization in US images, which has not been tackled earlier. Further, to assess the generality of our method, we apply our method to Polyp detection from Colonoscopy images."
Gall Bladder Cancer Detection from US Images with only Image Level Labels,2,Datasets,"Gallbladder Cancer Detection in Ultrasound Images: We use the public GBC US dataset [3] consisting of 1255 image samples from 218 patients. The dataset contains 990 non-malignant (171 patients) and 265 malignant (47 patients) GB images (see Fig. 2 for some sample images). The dataset contains image labels as well as bounding box annotations showing the malignant regions.Note that, we use only the image labels for training. We report results on 5-fold cross-validation. We did the cross-validation splits at the patient level, and all images of any patient appeared either in the train or validation split. Polyp Detection in Colonoscopy Images: We use the publicly available Kvasir-SEG [17] dataset consisting of 1000 white light colonoscopy images showing polyps (see Fig. 2). Since Kvasir-SEG does not contain any control images, we add 600 non-polyp images randomly sampled from the PolypGen [1] dataset.Since the patient information is not available with the data, we use random stratified splitting for 5-fold cross-validation."
Gall Bladder Cancer Detection from US Images with only Image Level Labels,3,Our Method,"Revisiting DETR: The DETR [6] architectures utilize a ResNet [13] backbone to extract 2D convolutional features, which are then flattened and added with a positional encoding, and fed to the self-attention-based transformer encoder. The decoder uses cross-attention between learned object queries containing positional embedding, and encoder output to produce output embedding containing the class and localization information. The number of object queries, and the decoder output embeddings is set to 100 in DETR. Subsequently, a feed-forward network generates predictions for object bounding boxes with their corresponding labels and confidence scores.Proposed Architecture: Fig. 3 gives an overview of our method. We use a COCO pre-trained class-agnostic DETR as proposal generator. The learned object queries contain the embedded positional information of the proposal. Classagnostic indicates that all object categories are considered as a single object class, as we are only interested in the object proposals. We then finetune a regular, class-aware DETR for the WSOD task. This class-aware DETR is initialized with the checkpoint of the class-agnostic DETR. The learned object queries from the class-agnostic DETR is frozen and shared with the WSOD DETR during finetuning to ensure that the class-aware DETR attends similar locations of the object proposals. The class-agnostic DETR branch is frozen during the finetuning phase. We finally use the MIL-based instance classification with the self-supervised instance learning over the finetuning branch. For GBC classification, if the model generates bounding boxes for the input image, then we predict the image to be malignant, since the only object present in the data is the cancer.MIL Setup: The decoder of the fine-tuning DETR generates R d-dimensional output embeddings. Each embedding corresponds to a proposal generated by the class-agnostic DETR. We pass these embeddings as input to two branches with FC layers to obtain the matrices X c ∈ R R×Nc and X r ∈ R R×Nc , where R is the number of object queries (same as proposals) and N c is the number of object (disease) categories. Let σ(•) denote the softmax operation. We then generate the class-wise and detection-wise softmax matrices C ∈ R R×Nc and D ∈ R R×Nc , where C ij = σ((X c ) T j )i and D ij = σ(X r i )j, and X i denotes the i-th row of X. C provides classification probabilities of each proposal, and D provides the relative score of the proposals corresponding to each class. The two matrices are element-wise multiplied and summed over the proposal dimension to generate the image-level classification predictions, φ ∈ R Nc :Notice, φ j ∈ (0, 1) since C ij and D ij are normalized. Finally, the negative loglikelihood loss between the predicted labels, and image labels y ∈ R Nc is computed as the MIL loss:The MIL classifier further suffers from overfitting to the distinctive classification features due to the mismatch of classification and detection probabilities [24].To tackle this, we further use a self-supervised module to improve the instances."
Gall Bladder Cancer Detection from US Images with only Image Level Labels,,Self-supervised Instance Learning:,"Inspired by [24], we design a instance learning module with N r blocks in a self-supervised framework to refine the instance scores with instance-level supervision. Each block consists of an FC layer. A class-wise softmax is used to generate instance scores x n ∈ R R×(Nc+1) at n-th block. N c + 1 includes the background/ no-finding class. Instance supervision of each layer (n) is obtained from the scores of the previous layer (x (n-1) ). The instance supervision for the first layer is obtained from the MIL head. Suppose ŷn ∈ R R×(Nc+1) is the pseudo-labels of the instances. An instance (p j ) is labelled 1 if it overlaps with the highest-scoring instance by a chosen threshold. Otherwise, the instance is labeled 0 as defined in Eq. 3:The loss over the instances is given by Eq. 4:Here x n ij denotes the score of i-th instance for j-th class at layer n. Following [24], the loss weightis applied to stabilize the loss. Assuming λ to be a scaling value, the overall loss function is given in Eq. 5:"
Gall Bladder Cancer Detection from US Images with only Image Level Labels,4,Experiments and Results,"Experimental Setup: We use a machine with Intel Xeon Gold 5218@2.30GHz processor and 8 Nvidia Tesla V100 GPUs for our experiments. The model is trained using SGD with LR 0.001 (for MIL head), weight decay 10 -6 , and momentum 0.9 for 100 epochs with batch size 32. The LR at backbone and transformer are 0.003, and 0.0003, respectively. We use a cosine annealing of the LR. Comparison with SOTA: Table 1 shows the bounding box localization results of the WSOD task. Our method surpasses all latest SOTA WSOD techniques by 9 points, and establishes itself as a strong WSOD baseline for GBC localization in US images. Our method also achieves 7-point higher AP score for polyp detection. We present visualizations of the predicted bounding boxes in Fig. 4 which shows that the localization by our method is more precise and clinically relevant as compared to the baselines."
Gall Bladder Cancer Detection from US Images with only Image Level Labels,,Generality of the Method:,"We assess the generality of our method by applying it to polyp detection on colonoscopy images. The applicability of our method on two different tasks -(1) GBC detection from US and (2) Polyp detection from Colonoscopy, indicates the generality of the method across modalities."
Gall Bladder Cancer Detection from US Images with only Image Level Labels,,Ablation Study:,"We show the detection sensitivity to the self-supervised instance learning module in Table 2 for two variants, (1) vanilla MIL head on DETR, and (2) MIL with self-supervised instance learning on DETR. Table 2 shows the Average Precision and detection sensitivity for both diseases. The results establish the benefit of using the self-supervised instance learning. Other ablations related to the hyper-parameter sensitivity is given in Supplementary Fig. S1.Classification Performance: We compare our model with the standard CNNbased and Transformer-based classifiers, SOTA WSOD-based classifiers, and SOTA classifiers using additional data or annotations (Table 3). Our method beats the SOTA weakly supervised techniques and achieves 1.2% higher sensitivity for GBC detection. The current SOTA GBC detection models require additional bound-  [25] 0.829 ± 0.030 0.900 ± 0.040 0.875 ± 0.063 PVTv2 [26] 0.824 ± 0.033 0.887 ± 0.057 0.894 ± 0.076 RadFormer [4] 0.921 ± 0.062 0.961 ± 0.049 0.923 ± 0.062 Additional Data/ Annotation USCL [7] 0.889 ± 0.047 0.895 ± 0.054 0.869 ± 0.097 US-UCL [5] 0.920 ± 0.034 0.926 ± 0.043 0.900 ± 0.046 GBCNet [3] 0.921 ± 0.029 0.967 ± 0.023 0.919 ± 0.063 Point-Beyond-Class [18]  TS-CAM [10] 0.704 ± 0.017 0.394 ± 0.042 0.891 ± 0.054 SCM [2] 0.751 ± 0.026 0.523 ± 0.014 0.523 ± 0.016 OD-WSCL [21] 0.805 ± 0.056 0.609 ± 0.076 0.923 ± 0.034 WS-DETR [19] 0.857 ± 0.071 0.812 ± 0.088 0.882 ± 0.034 Point-Beyond-Class [18] 0.953 ± 0.007 0.993 ± 0.004 0.924 ± 0.011 Ours 0.878 ± 0.067 0.785 ± 0.102 0.932 ± 0.022 ing box annotation [3] or, US videos [5,7]. However, even without these additional annotations/ data, our method reaches 86.1% detection sensitivity. The results for polyp classification are reported in Table 4. Although our method has a slightly lower specificity, the sensitivity surpasses the baselines reported in literature [16], and the SOTA WSOD based baselines."
Gall Bladder Cancer Detection from US Images with only Image Level Labels,5,Conclusion,"GBC is a difficult-to-detect disease that benefits greatly from early diagnosis. While automated GBC detection from US images has gained increasing interest from researchers, training a standard image classification model for this task is challenging due to the low inter-class variance and high intra-class variability of malignant regions. Current SOTA models for GBC detection require costly bounding box annotation of the pathological regions, or additional US video data, which limit their applicability. We proposed to formulate GBC detection as a weakly supervised object detection/ localization problem using a DETR with selfsupervised instance learning in a MIL framework. Our experiments show that the approach achieves competitive performance without requiring additional annotation or data. We hope that our technique will simplify the model training at the hospitals with easily available data locally, enhancing the applicability and impact of automated GBC detection."
Gall Bladder Cancer Detection from US Images with only Image Level Labels,,Fig. 1 .,
Gall Bladder Cancer Detection from US Images with only Image Level Labels,,Fig. 2 .,
Gall Bladder Cancer Detection from US Images with only Image Level Labels,,Fig. 3 .,
Gall Bladder Cancer Detection from US Images with only Image Level Labels,,Fig. 4 .,
Gall Bladder Cancer Detection from US Images with only Image Level Labels,,Table 1 .,
Gall Bladder Cancer Detection from US Images with only Image Level Labels,,Table 2 .,
Gall Bladder Cancer Detection from US Images with only Image Level Labels,,Table 3 .,
Gall Bladder Cancer Detection from US Images with only Image Level Labels,,Table 4 .,
Gall Bladder Cancer Detection from US Images with only Image Level Labels,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0 20.
Structured State Space Models for Multiple Instance Learning in Digital Pathology,1,Introduction,"Precision medicine efforts are shifting cancer care standards by providing novel personalised treatment plans with promising outcomes. Patient selection for such treatment regimes is based principally on the assessment of tissue biopsies and the characterisation of the tumor microenvironment. This is typically performed by experienced pathologists, who closely inspect chemically stained histopathological whole slide images (WSIs). Increasingly, clinical centers are investing in the digitisation of such tissue slides to enable both automatic processing as well as research studies to elucidate the underlying biological processes of cancer. The resulting images are of gigapixel size, rendering their computational analysis challenging. To deal with this issue, multiple instance learning (MIL) schemes based on weakly supervised training are used for WSI classification tasks. In such schemes, the WSI is typically divided into a grid of patches, with general purpose features derived from pretrained ImageNet [18] networks extracted for each patch. These representations are subsequently pooled together using different aggregation functions and attention-based operators for a final slide-level prediction.State space models are designed to efficiently model long sequences, such as the sequences of patches that arise in WSI MIL. In this paper, we present the first use of state space models for WSI MIL. Extensive experiments on three publicly available datasets show the potential of such models for the processing of gigapixel-sized images, under both weakly and multi-task schemes. Moreover, comparisons with other commonly used MIL schemes highlight their robust performance, while we demonstrate empirically the superiority of state space models in processing the longest of WSI sequences with respect to commonly used MIL methods."
Structured State Space Models for Multiple Instance Learning in Digital Pathology,2,Related Work,"Using pretrained networks for patch-wise feature extraction is a well established strategy for histopathology analysis [4,20]. An extension of this approach is with MIL, where the patch-wise features of an entire slide are digested simultaneously by an aggregator model, such as attention-based models CLAM [17] and TransMIL [19], the latter being a variant of self-attention transformers [21]. [3] proposes another transformer-based method in the form of a hierarchical ViT. Similar to our multitask experiments, [6] explores combining slide-level and tilelevel annotations with a minimal point-based annotation strategy. One of the key components of MIL methods is the aggregation module that pools together the set of patch representations. Mean or max pooling operations are among the simplest and most effective for aggregating predictions over a whole slide [2]. In contrast, recurrent neural networks (RNN) with long short-term memory (LSTM) [14] model the patches more explicitly as a set of tokens in sequence. In particular, LSTM networks have been shown to work well in different MIL settings including both visual cognition [22] and computational pathology [1].The state space model is a linear differential equation,that is widely studied in control theory, and describes a continuous time process for input and output signals u(t) ∈ R p and y(t) ∈ R q , and state signal x(t) ∈ R n , and where the process is governed by matricesIn HiPPO [9] (high-order polynomial projection operator), continuous time memorisation is posed as a problem of function approximation in a Hilbert space defined by a probability measure μ. For a scaled Legendre probability measure, one obtains the HiPPO matrix A, which enforces uniform weight in the memorisation of all previously observed inputs, in contrast to the exponentially decaying weighting of the constant error carousel of LSTMs [14]. The HiPPO mode of memorisation is shown empirically to be better suited to modeling long-range dependencies (LRD) than other neural memory layers, for which it serves as a drop-in replacement.Whereas in HiPPO, the state matrix A is a fixed constant, the linear state space layer (LSSL) [12] incorporates A as a learnable parameter. However, this increased expressiveness introduces intractable powers of A. In [10], the LSSL is instead reparameterised as the sum of diagonal and low-rank matrices, allowing for the efficient computation of the layer kernel in Fourier space. This updated formulation is known as the structured state space sequence layer (S4). Note that as a linear operator, the inverse discrete Fourier transform is amenable to backpropagation in the context of a neural network. Note also that under this formulation, the hidden state x(t) is only computed implicitly. Finally, [11] presents a simplification of the S4 layer, known as diagonal S4 (S4D), in which A is approximated by a diagonal matrix."
Structured State Space Models for Multiple Instance Learning in Digital Pathology,3,Method,"Given that the patch extraction of whole slide images at high magnifications results in long sequences of patches, we propose to incorporate a state space layer in a MIL aggregation network to better represent each patch sequence."
Structured State Space Models for Multiple Instance Learning in Digital Pathology,3.1,Neural State Space Models,"In practice, neural state space models (SSM) simulate Eq. 1 in discrete time, invoking a recurrence relation on the discretised hidden state,where the sequences u t , x t , and y t are the discretised u(t), x(t), and y(t), and the modified model parameters arise from a bilinear discretisation [12]. As such, SSMs bear an inherent resemblance to RNNs, where the hidden representation x t can be interpreted as a memory cell for the observed sequence over the interval [0, t], and with Du t acting as a skip connection between the input and output at point t. Due to their lack of non-linearities, state space models can also be viewed as a convolution between two discrete sequences. Playing out the recurrence in Eq. 2, one obtains,where u ∈ R L and y ∈ R L are the full input and output sequences, and the sequence K ∈ R L is defined as,which is computed efficiently by the S4D algorithm [11]. Note that although SSM layers are linear, they may be combined with other, non-linear layers in a neural network. Note also that although Eq. 3 is posed as modeling a onedimensional signal, in practice multi-dimensional inputs are modelled simply by stacking SSM layers together, followed by an affine ""mixing"" layer. "
Structured State Space Models for Multiple Instance Learning in Digital Pathology,3.2,MIL Training,"In our pipeline (Fig. 1) WSIs are first divided into a sequence of L patches {u 1 , u 2 , . . . , u L }, where L will vary by slide. A pretrained ResNet50 is then used to extract a 1024-dimensional feature vector from each patch {u 1 , u 2 , . . . , u L }, which constitute the model inputs. We define a SSM-based neural network F to predict a WSI-level class probability given this input sequence,The architecture of F is composed of an initial linear projection layer, used to lower the dimensionality of each vector in the input sequence. A SSM layer is then applied feature-wise by applying the S4D algorithm. That is, Eq. 3, including the skip connection, transforms the sequence {u 1,d , u 2,d , . . . , u L,d } for all features d, and the resulting sequences are concatenated. A linear ""mixing"" layer is applied token-wise, doubling the dimensionality of each token, followed by a gated linear unit [5] acting as an output gate, which restores the input dimensionality. For the SSM layer, we used the official implementation of S4D1 . A max pooling layer merges the SSM layer outputs into a single vector, which is projected by a final linear layer and softmax to give the class probabilities ŷ. The model is trained according to,where ŷcm denotes the probability corresponding to c m , the slide-level label of the sequence corresponding to the m th of M whole slide images."
Structured State Space Models for Multiple Instance Learning in Digital Pathology,3.3,Multitask Training,"One advantage of processing an entire slide as a sequence is the ease with which additional supervision may be incorporated, when available. A patch-level ground truth creates the opportunity for multitask learning, which can enhance the representations learned for slide-level classification. As an extension of our base model in Eq. 6, we train a multitask model to jointly predict a slide-level and patch-level labels. Prior to the max pooling layer of the base model, an additional linear layer is applied to each sequence token, yielding L additional model outputs. This multitask model is trained according to a sum of log losses,where c m,l indexes the class of the l th patch in the m th training slide and λ is a tunable hyperparameter used to modulate the relative importance of each task."
Structured State Space Models for Multiple Instance Learning in Digital Pathology,3.4,Implementation Details,"We extracted patches of size 256 × 256 from the tissue regions of WSIs at 20x magnification. Following CLAM [17], the third residual block of a pretrained ResNet50 [13] was used as a feature extractor, followed by a mean pooling operation, resulting in a 1024-dimensional representation for each patch. These features were used as inputs to all models. All model training was performed under a 10-fold cross-validation, and all reported results are averaged over the validation sets of the folds, aside from CAMELYON16, for which the predefined test set was utilized. Thus, for CAMELYON16, we report test set performances averaged over the validation. Baseline models were chosen to be prior art CLAM [17] and TransMIL [19]. The official code of these two models was used to perform the comparison. In addition, we included a vanilla transformer, a LSTM RNN, and models based on mean and max pooling. Our vanilla transformer is composed of two stacked self-attention blocks, with four attention heads, a model dimension of 256, and a hidden dimension of 256. For the LSTM, we used an embedding size of 256 and a width of 256. The pooling models applied pooling feature-wise across each sequence, then used a random forest with 200 trees for classification. For the S4 models, the dimension of the state matrix A was tuned to 32 for CAMELYON16 and TCGA-RCC, and 128 for TCGA-LUAD. Our models were trained using the Adam [15] optimizer with the lookahead method [23], with a learning rate of 2 • 10 -4 , and weight decay of 10 -4 for TCGA-LUAD and TCGA-RCC and 10 -3 for CAMELYON16. Early stopping with a patience of 10 was used for all our training. Our implementation is publicly available2 ."
Structured State Space Models for Multiple Instance Learning in Digital Pathology,4,Experiments and Discussion,
Structured State Space Models for Multiple Instance Learning in Digital Pathology,4.1,Data,"CAMELYON16 [16] is a dataset that consists of resections of lymph nodes, where each WSI is annotated with a binary label indicating the presence of tumour tissue in the slide, and all slides containing tumors have a pixel-level annotation indicating the metastatic region. In multitask experiments, we use this annotation to give each patch a label indicating local tumour presence. There are 270 WSIs in the training/validation set, and 130 WSIs in the predefined test set. In our experiments, the average patch sequence length arising from CAMELYON16 is 6129 (ranging from 127 to 27444).TCGA-LUAD is a TCGA lung adenocarcinoma dataset that contains 541 WSIs along with genetic information about each patient. We obtained genetic information for this cohort using Xena browser [7]. As a MIL task, we chose the task of predicting the patient mutation status of TP53, a tumor suppressor gene that is highly relevant in oncology studies. The average sequence length is 10557 (ranging from 85 to 34560).TCGA-RCC is a TCGA dataset for three kidney cancer subtypes (denoted KICH, KIRC, and KIRP). It consists of 936 WSIs (121 KICH, 518 KIRC, and 297 KIRP). The average sequence length is 12234 (ranging from 319 to 62235)."
Structured State Space Models for Multiple Instance Learning in Digital Pathology,4.2,Results,"Multiple Instance Learning Results. We evaluate our method on each dataset by accuracy and area under receiver operating characteristic curve (AUROC). For multiclass classification, these were computed in a one-versus-rest manner.Table 1 summarises the comparison between our proposed model and baselines. For the CAMELYON16 dataset, our method performs on par with Trans-MIL and the CLAM models, while it clearly outperforms the other methods. Similarly, in the TCGA-LUAD dataset the proposed model achieves comparable performance with both CLAM models, while outperforming TransMIL and the other methods. We note that TCGA-LUAD proves to be a more challenging dataset for all models. Moreover, our method outperforms CLAM models on the TCGA-RCC dataset, while reporting very similar performance with respect to TransMIL. Overall, looking at the average metrics per model across all three datasets, our proposed method achieves the highest accuracy and the second highest AUROC, only behind CLAM-MB. A pairwise t-test between the proposed method, CLAM, and TransMIL shows that there is no statistical significance performance difference (see supplementary material).We further compare our method with respect to model and time complexity. In Table 2 we report the number of trainable parameters, as well as the inference time for all models. The number of parameters is computed with all models configured to be binary classifiers, and the inference time is computed as the average time over 100 samples for processing a random sequence of 1024-dimensional vectors of length 30000. For our proposed method, we report both models with the different state dimensions (Ours (SSM 32 )) and (Ours (SSM 128 )). Compared with TransMIL, our method runs four times faster and has less than half the parameters. The CLAM models are more efficient in terms of number of trainable parameters, yet CLAM MB is slower. Table 3 shows the effect of modifying parts of the architecture on the results for TCGA-RCC. Most modifications had very little impact on AUROC, but a more significant impact can be seen on the accuracy of the model. Models A and B show that stacking multiple SSM layers results in lower accuracy, which was observed over all three datasets, while models C and D show that modifying the state dimension of the SSM module can have an impact on the accuracy. The optimal state space dimension varies depending on the dataset."
Structured State Space Models for Multiple Instance Learning in Digital Pathology,,Multitask Learning Results.,"We explored the ability of our model to combine slide-and patch-level information on the CAMEYLON16 dataset. We compared our model with the best performing model on CAMELYON16, TransMIL. Both models were trained according to Eq. 7 with λ = 5 tuned by hand. In Table 4 we give slide-level accuracy and AUROC for the two models. We observe that all accuracies and AUROC increase compared with those reported in Table 1. This indicates that the use of patch-level annotations complements the learning of the slide-level label. We furthermore observe that our model outperforms TransMIL when combining slide-and patch-level annotations. We map the sequence of output probabilities to their slide coordinates giving a heatmap localising metastasis (see supplementary material). Performance on Longest Sequences. In order to highlight the inherent ability of SSM models to effectively model long sequences, we performed an experiment on only the largest WSIs of the TCGA-RCC dataset. Indeed, this dataset contains particularly long sequences (up to 62235 patches at 20x). We evaluated the trained models for each fold on a subset of the validation set, only containing sequences with a length in the 85 th percentile. Table 5 shows the obtained average accuracy (weighted by the number of long sequences in each validation set) and AUROC on both CLAM models, TransMIL, and our proposed method. Both in terms of AUROC and accuracy, our method outperforms the other methods on long sequences, while the performances are comparable to Table 1, albeit slightly lower, illustrating the challenge of processing large WSIs. "
Structured State Space Models for Multiple Instance Learning in Digital Pathology,5,Conclusions,"In this work we have explored the ability of state space models to act as multiple instance learners on sequences of patches extracted from histopathology images. These models have been developed for their ability to memorise long sequences, and they have proven competitive with state of the art MIL models across a range of pathology problems. Additionally, we demonstrated the ability of these models to perform multiclass classification, which furthermore allowed us to visualise the localisation of metastasic regions. Finally, we demonstrated that on the longest sequences in our datasets, state space models offer better performance than competing models, confirming their power in modeling long-range dependencies."
Structured State Space Models for Multiple Instance Learning in Digital Pathology,,Fig. 1 .,
Structured State Space Models for Multiple Instance Learning in Digital Pathology,,Table 1 .,* indicates results from[19].
Structured State Space Models for Multiple Instance Learning in Digital Pathology,,Table 2 .,
Structured State Space Models for Multiple Instance Learning in Digital Pathology,,Table 3 .,
Structured State Space Models for Multiple Instance Learning in Digital Pathology,,Table 4 .,
Structured State Space Models for Multiple Instance Learning in Digital Pathology,,Table 5 .,
Automatic Retrieval of Corresponding US Views in Longitudinal Examinations,1,Introduction,"Muscle wasting, also known as muscle atrophy (see Fig. 1), is a common complication in critically ill patients, especially in those who have been hospitalized in the intensive care unit (ICU) for a long period [17]. Factors contributing to muscle wasting in ICU patients include immobilization, malnutrition, inflammation, and the use of certain medications [13]. Muscle wasting can result in weakness, impaired mobility, and increased morbidity and mortality. Assessing the degree of muscle wasting in ICU patients is essential for monitoring their progress and tailoring their rehabilitation program to recover muscular mass through physiotherapy before patient discharge. Traditional methods of assessing muscle wasting, such as physical examination, bioelectrical impedance analysis, and dual-energy X-ray absorptiometry, may be limited in ICUs due to the critical illness of patients [15]. Instead, ultrasound (US) imaging has emerged as a reliable, non-invasive, portable tool for assessing muscle wasting in the ICU [11]. The accuracy and reliability of US imaging in assessing muscle wasting in ICU patients have been demonstrated by Parry et al. [12]. US imaging can provide accurate measurements of muscle size, thickness, and architecture, allowing clinicians to track changes over time. However, these measurements are typically performed manually, which is time-consuming, subject to large variability and depends on the expertise of the operator. Furthermore, operators might be different from day to day and/or start scanning from different positions in each scan which will cause further variability.In recent years, self-supervised learning (SSL) has gained popularity for automated diagnosis in the field of medical imaging due to its ability to learn from unlabeled data [1,6,8,16]. Previous studies on SSL for medical imaging have focused on designing pretext tasks [2,9,10,18]. A class of SSL, contrastive learning (CL), aims to learn feature representations via a contrastive loss function to distinguish between negative and positive image samples. A relatively small number of works have applied CL to US imaging, for example to synchronize different cross-sectional views [7] and to perform view classification [4] in echocardiography (cardiac US).In this paper, we focus on the underinvestigated application of view matching for longitudinal RF muscle US examinations to assess muscle wasting. Our method uses a CL approach (see Fig. 2) to learn a discriminative representation from muscle US data which facilitates the retrieval of similar muscle views from different scans. The novel contributions of this paper are: 1) the first investigation of the problem of muscle US view matching for longitudinal image analysis, and 2) our approach is able to automatically retrieve similar muscle views between different scans, as shown by quantitative validation and qualitatively through a clinical survey."
Automatic Retrieval of Corresponding US Views in Longitudinal Examinations,2,Method,
Automatic Retrieval of Corresponding US Views in Longitudinal Examinations,2.1,Problem Formulation,"Muscle wasting assessment requires matching of corresponding cross-sectional US views of the RF over subsequent (days to weeks apart) examinations. The first acquisition is carried out following a protocol to place the transducer half way through the thigh and perpendicular to the skin, but small variations in translation and angulation away from this standard view are common. This scan produces the reference view at time T 1 (RT 1 ). The problem is as follows: given RT 1 , the task is to retrieve the corresponding view (V T 2 ) at a later time (T 2 ) from a sequence of US images captured by the operator using the transducer at approximately the same location and angle as for T 1 . The main challenges of this problem include: (1) the transducer pose and angle might be different, (2) machine settings might be slightly different, and (3) parts of the anatomy (specifically the RF) might change in shape and size over time. As a result, our aim is to develop a model that can select the most similar view acquired during T 2 to the reference view RT 1 acquired at T 1 ."
Automatic Retrieval of Corresponding US Views in Longitudinal Examinations,2.2,Contrastive Learning Framework for Muscle View Matching,"Inspired by the SimCLR algorithm [5], our model learns representations by maximizing the similarity between two different augmented views of the same muscle US image via a contrastive loss in the latent space. We randomly sample a minibatch of N images from the video sequences over three times T 1 , T 2 and T 3 , and define the contrastive learning on positive pairs (Xi, Xj) of augmented images derived from the minibatch, resulting in 2N samples. Rather than explicitly sampling negative examples, given a positive pair, we consider the other 2(N -1) augmented image pairs within a minibatch as negative.The contrastive loss function for a positive pair (Xi, Xj) is defined as:where 1 ∈ (0, 1), τ is a temperature parameter and sim(•) denotes the pairwise cosine similarity. z is a representation vector, calculated by z = g(f (X)), where f(•) indicates a shared encoder and g(•) is a projection head. L i C is computed across all positive pairs in a mini-batch. Then f (•) and g(•) are trained to maximize similarity using this contrastive loss."
Automatic Retrieval of Corresponding US Views in Longitudinal Examinations,2.3,The Model Architecture,"The model architecture is shown in Fig. 2a. First, we train the contrastive model to identify the similarity between two images, which are a pair of image augmentations created by horizontal flipping and random cropping (size 10×10) applied on a US image (i.e., they represent different versions of the same image). Each image of this pair (Xi, Xj) is fed into an encoder to extract representation vectors (hi, hj) from them. The encoder architecture (Fig. 2b) has four conv layers (kernel 3 × 3) with ReLU and two max-poolings. A projection head (a multilayer perceptron with two dense layers of 512 nodes) follows mapping these representations to the space where the contrastive loss is applied.Second, we use the trained encoder f (•) for the training of our main task (i.e. the downstream task), which is the classification of positive and negative matches (corresponding and non-corresponding views) of our test set. For that, we feed a reference image X ref , and a candidate frame X j to the encoder to obtain the representations hi, hj and feed these in turn to a classification network (shown in Fig. 2c) that contains four dense layers with ReLU activation and a softmax layer."
Automatic Retrieval of Corresponding US Views in Longitudinal Examinations,3,Materials,"The muscle US exams were performed using GE Venue Go and GE Vivid IQ machines, both with linear probes (4.2-13.0 MHz), by five different doctors. During examination, patients were in supine position with the legs in a neutral rotation with relaxed muscle and passive extension. Measurements were taken at the point three fifths of the way between the anterior superior iliac spine and the patella upper pole. The transducer was placed perpendicular to the skin and to the longitudinal axis of the thigh to get the cross-sectional area of the RF. An excess of US gel was used and pressure on the skin was kept minimal to maximise image quality. US measurements were taken at ICU admission (T 1 ), 2-7 d after admission (T 2 ) and at ICU discharge (T 3 ). For this study, 67 Central Nervous System (CNS) and Tetanus patients were recruited and their data were acquired between June 2020 and Feb 2022. Each patient had an average of six muscle ultrasound examinations, three scans for each leg, totalling 402 examinations. The video resolution was 1080 × 1920 with a frame rate of 30fps. This study was performed in line with the principles of the Declaration of Helsinki. Approval was granted by the Ethics Committee of the Hospital for Tropical Diseases, Ho Chi Minh City and Oxford Tropical Research Ethics Committee. The contrastive learning network was trained without any annotations. However, for the view matching classification task, our test data were annotated automatically as positive and negative pairs based upon manual frame selection by a team of five doctors comprising three radiologists and two ultrasound specialists with expertise in muscle ultrasound. Specifically, each frame in an examination was manually labelled as containing a similar view to the reference RT 1 or not. Based upon these labelings, as shown in Fig. 3, the positive pairs are combinations of similar views within each examination (T 1 /T 2 /T 3 ) and between examinations. The rest are considered negative pairs."
Automatic Retrieval of Corresponding US Views in Longitudinal Examinations,4,Experiments and Results,
Automatic Retrieval of Corresponding US Views in Longitudinal Examinations,4.1,Implementation Details,"Our model was implemented using Tensorflow 2.7. During training, input videos underwent experimentation with clip sizes of 256 × 256, 128 × 128, and 64 × 64. Eventually, they were resized to 64 × 64 clips, which yielded the best performance. All the hyperparameters were chosen using the validation set. For the CL training, the standard Adam optimizer was used with learning rate =0.00001, kernel size = 3 × 3, batch size = 128, batch normalization, dropout with p = 0.2 and L2 regularization of the model parameters with a weight = 0.00001. The CL model was trained on 80% of the muscle US data for 500 epochs. For the view retrieval model, the standard Adam optimizer with learning rate = 0.0001, batch size = 42 and dropout of p = 0.2 was used. The classifier was trained on the remaining 20% of the data (of which 80% were used for training, 10% for validation and 10% for testing) and the network converged after 60 epochs. For the supervised baseline model, the standard Adam optimizer was used with learning rate =0.00001, kernel size = 3 × 3, batch size = 40, and batch normalization. Here, we used the same data splitting as our view retrieval classifier. The code we used to train and evaluate our models is available at https://github.com/ hamidehkerdegari/Muscle-view-retrieval."
Automatic Retrieval of Corresponding US Views in Longitudinal Examinations,4.2,Results,"Quantitative Results. We carried out two quantitative experiments. First, we evaluated the performance of the view classifier. Second, we evaluated the quality of the resulting cross-sectional areas segmented using a U-Net [14].The classifier performance was carried out by measuring, for the view retrieval task, the following metrics: Area Under the Curve (AUC), precision, recall, and F1-score. Because there is no existing state of the art for this task, we created two baseline models to compare our proposed model to: first, a naive image-space comparison using normalized cross-correlation (NCC) [3], and second, a supervised classifier. The supervised classifier has the same architecture as our CL model, but with the outputs of the two networks being concatenated after the representation h followed by a dense layer with two nodes and a softmax activation function to produce the probabilities of being a positive or negative pair. Table 1 shows the classification results on our dataset. As shown in Table 1, our proposed method achieved superior performance in terms of AUC, precision, recall, and F1-score compared to all other models. The NCC method demonstrated the lowest performance, as it lacked the capability to accurately capture dynamic changes and deformations in US images which can result in significant structural differences. A representative example of a modelretrieved view for one case is presented in Fig. 4. It shows positive, negative, and middle (i.e., images with a probability value between the highest and lowest values predicted by our model) pairs of images generated by our model from a patient's left leg. As reference, on the left we show the user pick (RT 2 ). To assess the quality of the resulting cross-sections, we calculated the mean relative absolute area difference (d) between the ground truth (a GT ) frame and that of the model predicted frame (a pred ) for each examination as follows:We applied a trained U-Net model (already trained with 1000 different US muscle images and manual segmentations). Results showed an overall cross-sectional mean relative absolute area error of 5.7% ± 0.24% on the test set (Full details provided in Fig. 5, right). To put this number into context, Fig. 5, left visualizes two cases where the relative error is 2.1% and 5.2%.Qualitative Results. We conducted a user study survey to qualitatively assess our model's performance. The survey was conducted blindly and independently by four clinicians and consisted of thirty questions. In each, clinicians were shown two different series of three views of the RF: (1) RT 1 , GT match from T 2 and model prediction from T 2 , and (2) RT 1 , a random frame from T 2 and model prediction from T 2 . They were asked to indicate which (second or third) was the best match with the first image. The first question aimed to determine if the model's performance was on par with clinicians, while the second aimed to determine if the model's selection of images was superior to a randomly picked frame. As shown in Fig. 6, left, clinicians chose the model prediction more often than the GT; however, this difference was not significant (paired Student's t-test, p = 0.44, significance= 0.05). Therefore, our model can retrieve the view as well as clinicians, and significantly better (Fig. 6, right) than randomly chosen frames (paired Student's t-test, p = 0.02, significance= 0.05). "
Automatic Retrieval of Corresponding US Views in Longitudinal Examinations,5,Discussion and Conclusion,"This paper has presented a self-supervised CL approach for automatic muscle US view retrieval in ICU patients. We trained a classifier to find positive and negative matches. We also computed the cross-sectional area error between the ground truth frame and the model prediction in each acquisition time to evaluate model performance. The performance of our model was evaluated on our muscle US video dataset and showed AUC of 73.52% and 5.7% ± 0.24% error in cross-sectional view. Results showed that our model outperformed the supervised baseline approach. This is the first work proposed to identify corresponding ultrasound views over time, addressing an unmet clinical need."
Automatic Retrieval of Corresponding US Views in Longitudinal Examinations,,Fig. 1 .,
Automatic Retrieval of Corresponding US Views in Longitudinal Examinations,,Fig. 2 .,
Automatic Retrieval of Corresponding US Views in Longitudinal Examinations,,Fig. 3 .,
Automatic Retrieval of Corresponding US Views in Longitudinal Examinations,,Fig. 4 .,
Automatic Retrieval of Corresponding US Views in Longitudinal Examinations,,Fig. 5 .,
Automatic Retrieval of Corresponding US Views in Longitudinal Examinations,,Fig. 6 .,
Automatic Retrieval of Corresponding US Views in Longitudinal Examinations,,Table 1 .,
Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision,1,Introduction,"Longitudinal lesion or tumor tracking is a fundamental task in treatment monitoring workflows, and for planning of re-treatments in radiation therapy. Based on longitudinal imaging for a given patient it requires establishing which lesions are corresponding (i.e., same lesion, observed at different timepoints), which lesions have disappeared and which are new compared to prior scanning. This information can be leveraged to assess treatment response, e.g., by analyzing the evolution of size and morphology for a given tumor [1], but also for adaptation of (re-)treatment radiotherapy plans that take into account new tumors.In practice, the development of automatic and reliable lesion tracking solutions is hindered by the complexity of the data (over different modalities), the absence of large, annotated datasets, and the difficulties associated with lesion identification (i.e., varying sizes, poses, shapes, and sparsely distributed locations). In this work, we present a multi-scale self-supervised learning solution for lesion tracking in longitudinal studies using the capabilities of contrastive learning [9]. Inspired by the pixel-wise contrastive learning strategy introduced in [5], we choose to learn pixel-wise feature representations that embed consistent anatomical information from unlabeled (i.e., without lesion-related annotations) and unpaired (i.e., without the use of longitudinal scans) data, overcoming barriers to data collection. To increase the system robustness and emulate the clinician's reading strategies, we propose to use multi-scale embeddings to enable the system to progressively refine the fine-grained location. In addition, as imaging offers contextual information about the human body that is naturally consistent, we design the model to benefit from biologically-meaningful points (i.e., anatomical landmarks). The reasoning behind this strategy is that simple data augmentation methods cannot faithfully model inter-subject variability or possible organ deformations. Hence, we ensure the spatial coherence of the tracked lesion location using well-defined anatomical landmarks.Our proposed method brings two elements of novelty from a technical point of view: (1) the multi-scale approach for the anatomical embedding learning and (2) a positive sampling approach that incorporates anatomically significant landmarks across different subjects. With these two strategies, the goal is to ensure a high degree of robustness in the computation of the lesion matching across different lesion sizes and varying anatomies. Furthermore, a significant focus and contribution of our research is the experimental study at a very large scale: we (1) train a pixel-wise self-supervised system using a very large and diverse dataset of 52,487 CT volumes and (2) evaluate on two publicly available datasets. Notably, one of the datasets, NLST, presents challenging cases with 68% of lesions being very small (i.e., radius < 5 mm)."
Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision,2,Background and Motivation,"The problem of lesion tracking in longitudinal data is typically divided into two steps: (1) detection of lesions and (2) tracking the same lesion over multiple time points. Classical methods to solve this problem rely on image registration, where tracking is performed via image alignment and rule-based correspondence matching [15,16,21]. These approaches are difficult to optimize, especially when scaling across different body regions and fields of view. Appearance-based trackers [19,20] adopt a different strategy by projecting lesions detected beforehand with dedicated detectors [17,18] onto a representation space and employing nearest neighbor analysis. One recent approach, Deep Lesion Tracker (DLT) [8], integrates both strategies to perform appearance-based recognition under anatomical constraints. As a more direct matching approach, Yan et al. [5] uses a self-supervised anatomical embedding model (SAM) to create semantic embeddings for each image pixel, avoiding the detection step. Training exclusively on augmented paired data prevents SAM from accurately representing anatomical changes and deformations that occur over time. This can influence the contextual information of a pixel, which in turn impacts the pixel-wise embeddings on which the similarity-based tracker depends. To overcome this, we propose to train a pixel-wise multi-scale embedding model that accounts for anatomical similarity among different subjects, making the embeddings more effective."
Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision,3,Method,
Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision,3.1,Problem Definition,"Let I 1 (i.e., template or baseline image) and I 2 (i.e., query or follow-up image) be two 3D-CT scans acquired at time t 1 and t 2 , respectively, Additionally, let p 1 and p 2 denote the point of interest (i.e., the lesion center) in both images. The problem of lesion tracking can be formulated as finding the optimal transformation that maps p 1 to its corresponding location, p 2 , in I 2 ."
Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision,3.2,Training Stage,"Let D = {X 1 , X 2 , ..., X N } be a set of N unpaired and unlabeled 3D-CT volumes.As shown in Fig. 1, given an image X ∈ R d×h×w from the training dataset D, we randomly select two overlapping 3D patches (anchor and query), namely X a and X q . To create synthetic paired data that mimics appearance changes across different images, we apply random data augmentation (i.e., random spatial and intensity-related transformations) to the content of X a and X q . We implement a similar augmentation strategy to that described in [5]. Given X a and X q , we use an embedding extraction model to construct a hierarchy of multi-scale semantic embeddings for each image pixel, labeled F a and F q respectively. The embedding at ith scale, 1 ≤ i ≤ s, is denoted as F i a and F i q and is represented as a 4D feature map, with an embedding vector of length L associated with each pixel.Given the nature of contrastive learning, the sampling strategy (extracting negative and positive pixel pairs from augmented 3D paired patches) is essential to achieving discriminative pixel-wise embeddings. We arbitrary sample n pos positive pixel pairs from the overlapping area of X a and X q , denoted by a + = {a + 1 , ..., a + j ..., a + npos }, q + = {q + 1 , ..., q + j , ..., q + npos }, 1 ≤ j ≤ n pos . To further enhance embeddings, 10% of the positive pixel pairs are derived from biologically-meaningful points across different volumes in the batch. We use data-driven models [14] to extract 37 anatomical landmarks, such as the top right lung, suprasternal notch, tracheal bifurcation, etc. Similar to [5], for each positive pixel pair (a + j , q + j ), we select n neg hard and diverse negative pixels, denoted byNext, at each scale, we extract the embedding vectors for positive and negative pixel pairs from F i a , F i q , guided by the corresponding locations, a + , q + , h -, which are downsampled to match the scale. We denote the positive embeddings at ith scale at pixel location a + j , q + j as f a i j , f q i j ∈ R L . Similarly, we denote the negative embeddings at pixel location h - k associated to a positive positive pixel pair (a + j , q + j ) as f i jk ∈ R L . We use L2-norm to normalize the embedding vectors before the loss computation. We use pixel-wise InfoNCE loss [5,10] to enhance the similarity among similar pixels (i.e., positive pairs of pixels) and decrease the similarity among dissimilar pixels (i.e., negative pairs of pixels). Correspondingly, we set the contrastive loss at the ith scale:where τ = 0.5 is a temperature parameter. The final loss is then calculated as the average of all these individual losses."
Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision,3.3,Inference Stage,"Let X a be a 3D-CT volume template with an input point of interest p a ∈ X a , and X q a corresponding query 3D-CT volume. The first step is to project the image X a into a multi-scale feature space, creating a hierarchy of multi-scale semantic embeddings F a for each pixel in the image (i.e., a 4D feature map).Next, we follow a similar process for the query image X a and acquire the pixellevel embeddings F q .To measure the similarity between the embeddings of the input X a at the point of interest p a and the query embeddings F q , we compute cosine similarity maps at each scale:(2)Finally, we combine the multi-scale similarity maps through summation and select the voxel with the highest similarity as the matching point in the query volume."
Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision,4,Experiments,
Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision,4.1,Datasets and Setup,"Datasets: We train the universal and fine-grained anatomical point matching model using an in-house CT dataset (VariousCT). The training dataset contains 52,487 unlabeled 3D CT volumes capturing various anatomies, including chest, head, abdomen, pelvis, and more.The evaluation is based on two datasets, the publicly released Deep Longitudinal Study (DLS) dataset [8] and the National Lung Screening Trial (NLST) dataset [12]. The DLS dataset is a subset of the DeepLesion [11] medical imaging dataset, containing 3891 pairs of lesions with information on their location and size. The dataset covers various types of lesions across different organs. We follow the official data split for DLS dataset and perform evaluation on the testing dataset which comprises 480 lesion pairs. For NLST, we randomly selected a subset of 1045 test images coming from 420 patients with up to 3 studies. A certified radiologist annotated the testing data by identifying the location and size of the pulmonary nodules, resulting in a total of 825 paired annotations. We evaluate lesion tracking in both directions, from baseline to follow-up and from follow-up to baseline [8]. This results in a total of 960 and 1650 testing lesion pairs in DLS and NLST test sets, respectively. The isotropic resolution of all CT volumes is adjusted to 2mm through bilinear interpolation.System Training: Our learning model is implemented in PyTorch and uses the TorchIO library [13] for medical data manipulation and augmentation.We employ a U-Net-based encoder-decoder architecture [2] that utilizes an inflated 3D ResNet-18 [3,4] as its encoder, which extends all 2D convolutions Table 1. Comparison between the proposed solution and several state-of-the-art approaches (reference results are from [8]). The exact same test set was used to compute the performance of each approach listed in the table; however, we retrained only SAM. in the standard ResNet to 3D convolutions and allows the use of pre-trained ImageNet weights. The multi-scale embedding model employs s = 5 scales, and the embedding length is fixed at L = 128 for each scale. Convolution with a stride of (2, 2, 2) is used to reduce the feature map size at the first and fifth levels, while a stride of (1, 2, 2) is employed for intermediary levels 2 to 4. The U-Net decoder uses a convolution layer with a 3 × 3 × 3 kernel after every up-sampling layer to generate the final cascade of feature embeddings. The model is trained with AdamW optimizer [6] for 64 epochs using an early stopping strategy with a patience of 5 epochs, a batch size of 8 augmented 3D paired patches of 32 × 96 × 96, and a learning rate of 0.0001."
Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision,,Method,"For data augmentation, we apply random cropping, scaling, rotation, and Gaussian noise injections. A windowing approach that covers the intensity ranges of lungs and soft tissues is used to scale CT intensity values to [-1, 1]. The sampling hyperparameters consist of 100 positive pixel pairs (n pos = 100), 100 hard negative pixel pairs, and 200 diverse negative pixel pairs (n neg = 300)."
Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision,,Evaluation Metrics:,"We use mean Euclidean distance (MED) to measure the distance between predicted lesion center and ground truth, and the center point matching accuracy (i.e., percentage of accurately matched lesions given the annotated lesion radius), denoted with CPM@Radius. For lesions of large sizes, we set a maximum distance limit of 10 mm as acceptance criteria [8], denoted with CPM@10 mm. The NLST testing dataset has a distinctive feature wherein nodules are relatively small, 68% of annotated lesions have a radius of less than 5 mm (compared to 6% in DLS dataset). To ensure that such small nodules are not missed during evaluation, we relax the minimum distance requirement and consider a distance of 6 mm as a permissible matching error."
Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision,4.2,Evaluation,"For the lesion tracking task on DLS dataset, we quantitatively compare our system against existing trackers in Table 1. These include the Deep Lesion Tracker (DLT) and its variants [8], as well as registration-based trackers [15,16,21] and appearance-based trackers via detector learning [17][18][19][20]. Given the clear superiority of approach [5] compared to all reference solutions, we focus on achieving a direct comparison against SAM [5]. Hence, for performance comparison against self-supervised anatomical embedding tracker, we retrain SAM [5] with images from VariousCT dataset. Our method achieves a matching accuracy of 91.87%, that is 1.84% higher than SAM and 5.74% higher than DLT. To confirm the significance of the improvement achieved by our method compared to SAM [5], we conduct a paired t-test for statistical analysis and show that the improvement is statistically significant (p-value < 10 -6 ). Compared to the self-supervised version of DLT, the difference in performance is significantly greater, the proposed systems outperforms DLT-SSL by more than 10%. When imposing a maximum distance limit of 10 mm between the ground truth and prediction, our method increases performance by 1.46%, showing the importance of the multi-scale approach in lesion On the NLST dataset, our proposed method obtains a center point matching accuracy of 92.12% (Table 2). In the case of longitudinal lung nodule tracking (Fig. 3), it is more frequent to observe significant changes in size and density. As our system relies on the concept of anatomical embedding matching, the most substantial errors in lesion matching for our system occur when there are significant pathological distortions that deviate greatly from one timepoint to another. Examples of such cases are depicted in Fig. 4, based on expert radiologist feedback.  "
Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision,5,Conclusion,"In conclusion, this paper presents an effective method for longitudinal lesion tracking based on multi-scale self-supervised learning. The method is generic, it does not require expert annotations or longitudinal data for training and can generalize to different types of tumors/organs/modalities. The multi-scale approach ensures a high degree of robustness and accuracy for small lesions. Through large-scale experiments and validation on two longitudinal datasets, we highlight the superiority of the proposed method in comparison to state-of-theart. We found that adopting a multi-scale approach (instead of the global/local approach as proposed in [5]) can lead to embeddings that better capture the anatomical location and are able to handle lesions that vary in size or appearance at different scales. Moreover, the changes proposed in this work help to alleviate the confusion caused by left-right body symmetries (e.g., the apices of the lungs). This effect challenged the tracking of small nodules in the lungs using [5]. Our future work aims to enhance the matching accuracy by examining the implications of correlation magnitude, conducting robustness studies on slight variations in tracking initialization, and implementing a more advanced fusion strategy for the multi-scale similarity maps. In addition, we aim to expand to more applications, e.g., treatment monitoring for brain cancer using MRI.Disclaimer: The concepts and information presented in this paper/presentation are based on research results that are not commercially available. Future commercial availability cannot be guaranteed."
Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision,,Fig. 1 .,
Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision,,Fig. 2 .,
Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision,,Fig. 3 .,
Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision,,Fig. 4 .,
Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision,,Table 2 .,
Geometry-Invariant Abnormality Detection,1,Introduction,"The use of machine learning for anomaly detection in medical imaging analysis has gained a great deal of traction over previous years. Most recent approaches have focused on improvements in performance rather than flexibility, thus limiting approaches to specific input types -little research has been carried out to generate models unhindered by variations in data geometries. Often, research assumes certain similarities in data acquisition parameters, from image dimensions to voxel dimensions and fields-of-view (FOV). These restrictions are then carried forward during inference [5,25]. This strong assumption can often be complex to maintain in the real-world and although image pre-processing steps can mitigate some of this complexity, test error often largely increases as new data variations arise. This can include variances in scanner quality and resolution, in addition to the FOV selected during patient scans. Usually training data, especially when acquired from differing sources, undergoes significant preprocessing such that data showcases the same FOV and has the same input dimensions, e.g. by registering data to a population atlas. Whilst making the model design simpler, these pre-processing approaches can result in poor generalisation in addition to adding significant pre-processing times [11,13,26]. Given this, the task of generating an anomaly detection model that works on inputs with a varying resolution, dimension and FOV is a topic of importance and the main focus of this research.Unsupervised methods have become an increasingly prominent field for automatic anomaly detection by eliminating the necessity of acquiring accurately labelled data [4,7] therefore relaxing the stringent data requirements of medical imaging. This approach consists of training generative models on healthy data, and defining anomalies as deviations from the defined model of normality during inference. Until recently, the variational autoencoder (VAE) and its variants held the state-of-the-art for the unsupervised approach. However, novel unsupervised anomaly detectors based on autoregressive Transformers coupled with Vector-Quantized Variational Autoencoders (VQ-VAE) have overcome issues associated with autoencoder-only methods [21,22]. In [22], the authors explore the advantage of tractably maximizing the likelihood of the normal data to model the long-range dependencies of the training data. The work in [21] takes this method a step further through multiple samplings from the Transformer to generate a non-parametric Kernel Density Estimation (KDE) anomaly map.Even though these methods are state-of-the-art, they have stringent data requirements, such as having a consistent geometry of the input data, e.g., in a whole-body imaging scenario, it is not possible to crop a region of interest and feed it to the algorithm, as this cropped region will be wrongly detected as an anomaly. This would happen even in the case that a scan's original FOV was restricted [17].As such, we propose a geometric-invariant approach to anomaly detection, and apply it to cancer detection in whole-body PET via an unsupervised anomaly detection method with minimal spatial labelling. Through adapting the VQ-VAE Transformer approach in [21], we showcase that we can train our model on data with varying fields of view, orientations and resolutions by adding spatial conditioning in both the VQ-VAE and Transformer. Furthermore, we show that the performance of our model with spatial conditioning is at least equivalent to, and sometimes better, than a model trained on whole-body data in all testing scenarios, with the added flexibility of a ""one model fits all data"" approach. We greatly reduce the pre-processing requirements for generating a model (as visualised in Fig. 1), demonstrating the potential use cases of our model in more flexible environments with no compromises on performance."
Geometry-Invariant Abnormality Detection,2,Background,"The main building blocks behind the proposed method are introduced below. Specifically, a VQ-VAE plus a Transformer are jointly used to learn the probability density function of 3D PET images as explored in prior research [21,22,24]. "
Geometry-Invariant Abnormality Detection,2.1,Vector-Quantized Variational Autoencoder,"The VQ-VAE model provides a data-efficient encoding mechanism-enabling 3D inputs at their original resolution-while generating a discrete latent representation that can trivially be learned by a Transformer network [20]. The VQ-VAE is composed of an encoder that maps an image X ∈ R H×W ×D onto a compressed latent representation Z ∈ R h×w×d×nz where n z is the latent embedding vector dimension. Z is then passed through a quantization block where each feature column vector is mapped to its nearest codebook vector. Each spatial code Z ijl ∈ R nz is then replaced by its nearest codebook element e k ∈ R nz , k ∈ 1, ..., K where K denotes the codebook vocabulary size, thus obtaining Z q . Given Z q , the VQ-VAE decoder then reconstructs the observations X ∈ R H×W ×D . The architecture used for the VQ-VAE model used an encoder consisting of three downsampling layers that contain a convolution with stride 2 and kernel size 4 followed by a ReLU activation and 3 residual blocks. Each residual block consists of a kernel of size 3, followed by a ReLU activation, a convolution of kernel size 1 and another ReLU activation. Similar to the encoder, the decoder has 3 layers of 3 residual blocks, each followed by a transposed convolutional layer with stride 2 and kernel size 4. Finally, before the last transposed convolutional layer, a Dropout layer with a probability of 0.05 is added. The VQ-VAE codebook used had 256 atomic elements (vocabulary size), each of length 128. The CT VQ-VAE was identical in hyperparameters except each codebook vector has length 64. See Appendix A for implementation details."
Geometry-Invariant Abnormality Detection,2.2,Transformer,"After training a VQ-VAE model, the next stage is to learn the probability density function of the discrete latent representations. Using the VQ-VAE, we can obtain a discrete representation of the latent space by replacing the codebook elements in Z q with their respective indices in the codebook yielding Z iq . To model the imaging data, we require the discretized latent space Z iq to take the form of a 1D sequence s, which we achieve via a raster scan of the latent. The Transformer is then trained to maximize the log-likelihoods of the latent tokens sequence in an autoregressive manner. By doing this, the Transformer can learn the codebook distribution for position i within s with respect to previous codes p(s i ) = p(s i |s <i ). As with [21], we additionally use CT data to condition the Transformer via cross-attention using a separate VQ-VAE to encode the CT. This transforms the problem to learning the codebook distribution at position i as p(s i ) = p(s i |s <i , c) where c is the entire CT latent sequence. The performer used in this work corresponds to a decoder Transformer architecture with 14 layers, each with 8 heads, and an embedding dimension of 256. Similarly the embedding dimension for the CT data and the spatial conditioning data had an embedding dimension of 256. See Appendix B for implementation details."
Geometry-Invariant Abnormality Detection,2.3,Anomaly Detection via Kernel Density Estimation Maps,"Building on [21], given a sample for inference, a tokenized representation Z iq is extracted from the VQ-VAE. Then, the representation is flattened into s where the trained Transformer model obtains the likelihoods for each token. These inferred likelihoods represent the probability of each token appearing at a certain position in the sequence -p(s i ) = p(s i |s <i , c). This can then be used to single out tokens with low probability, i.e. anomalous tokens. We then resample anomalous tokens p(s i ) < t where t is the resampling threshold chosen empirically using the validation set performance. Anomalous tokens are then replaced with higher likelihood (normal) tokens by resampling from the Transformer. We can then reshape the ""healed"" sequence back into its 3D quantized representation to feed into the VQ-VAE to generate a healed reconstruction X r without anomalies.In this work, abnormalities are defined as deviations between the distribution of ""healed"" reconstructions and the observed data, measured using a Kernel Density Estimation (KDE) approach. We generate multiple healed latent sequences by sampling multiple times for each position i with a likelihood p(s i ) < t. In each resampling, the Transformer outputs the likelihood for every possible token at position i. Based on these probabilities, we can create a multinomial distribution showcasing the probability of each token. We can then randomly sample multiple tokens. Each of these healed latent spaces is then decoded via the VQ-VAE multiple times with dropout. This generates multiple healed representations of the original image. A voxel-wise KDE anomaly map is generated by fitting a KDE independently at each voxel position to estimate the probability density function f across reconstructions. This is then scored at the original intensity of that voxel in the scan. Our KDE implementation used 60 samples for each anomalous token in s, followed by five decodings with dropout, yielding 300 ""healed"" reconstructions that are then used to calculate the KDE."
Geometry-Invariant Abnormality Detection,3,Method,
Geometry-Invariant Abnormality Detection,3.1,VQ-VAE Spatial Conditioning,"To date, there has been little research on generating autoencoder models capable of using images of varying sizes and resolutions (i.e. the input tensor shape to a autoencoder is assumed to be fixed). Although fully convolutional models can ingest images of varying dimensions, we have found that using training data with varying resolutions resulted in poor auto-encoder reconstructions. In this work, we take inspiration from CoordConv [19] as a mechanism to account for some level of spatial awareness, an approach which has been applied to various tasks in medical imaging scenarios with ranging levels of success [1,18].A CoordConv layer is a concatenation of channels to the input image referencing a predefined coordinate system. After concatenation, the input is simply fed through a standard convolutional layer. For a 3D scan, we would have 3 coordinates, ijk, where the i coordinate channel is an h × w × d rank-1 matrix with its first row filled with 0's, its second row with 1's, and so on. This would be the same for the j coordinate channel, except the columns would be filled with constant values, not the rows, and likewise for the k coordinate channel in a depth-wise fashion. These channels are then normalised between [0, 1].The advantage of the CoordConv implementation is the constant scale of 0-1 across the channels regardless of image resolution. For example, two wholebody images with large differences in voxel-size will have CoordConv channels from 0-1 along each axis, thus conveying the notion of spatial resolution to the network. We found when training the VQ-VAE model on data with varying resolutions and dimensions that reconstructions showcased unwanted and significant artifacts, while by adding the CoordConv channels this issue was not present (See Appendix C for examples). Furthermore, when dealing with images of a ranging FOV, we adapted the [0, 1] channel values to convey the image's FOV. For example, suppose a whole body image (neck to upper leg) represented our range [0, 1] where 0 is the upper leg, and 1 is the neck. In that case, we can contract this range to represent the area displayed in the image (Fig. 2). In doing so, we convey information about the FOV to the VQ-VAE through CoordConv layers. Note that while the proposed model assumes only translation and scale changes between samples, it can be trivially extended to a full affine mapping of the coordinate system (including rotations/shearing between samples).We used random crops during training to simulate varying FOVs of wholebody data. The random crop parameters are then used to define the coordinate system. For the implementation of the CoordConv layer, these channels are added once to the original input image and at the beginning of the VQ-VAE decoder, concatenated to the latent space, using the same value ranges but at a lower resolution given the reduced spatial dimension of the latent space."
Geometry-Invariant Abnormality Detection,3.2,Transformer Spatial Conditioning,"Numerous approaches have used Transformers in the visual domain [7,8]. Given that Transformers work natively on 1D sequences, the spatial information in images is often lost. While various works have aimed to convey the spatial information of the original image when projected onto a 1D sequence [14,28], we require our spatial positioning to encode both where in the image ordering a token belongs, and where the token belongs in the context of the whole body. As the images have different FOVs and the image resolution, this results in To do this, we use the same CoordConv principle applied to the input fed to the VQ-VAE. In order to map image coordinates to the token latent representation, we apply average pooling to each CoordConv channel separately, with kernel size and stride equal to the downsampling used in the VQ-VAE (8 used in this research). This gives us three channels i, j, k in the range of [0, 1], the same dimension as our latent space, but at lower spatial resolution to the original input. We then bin each value in each channel and combine the three values using base notation. For example, we use 20 bins (equal bins of 0.05), to which the final quantized spatial value for a given token is given aswhere sp is the quantized spatial value allocated to a given token at position ijk in the latent space, and b represents the binned value along a given channel for that token, and B is a pre-defined bin size. The choice of B = 20 bins was empirically chosen to closely resemble the average latent dimension of images.During training, whole-body images and random crops are used. The spatial conditioning tokens are then generated and fed through an embedding layer of equal dimension to the CT embedding. The two embedded sequences (CT and spatial) are then added together and fed to the Transformer via cross-attention. For reference, this mechanism can be visualised in Fig. 3."
Geometry-Invariant Abnormality Detection,3.3,Data,"For this work we leveraged whole-body PET/CT data from different sources to explore the efficacy of our approach for varying image geometries. 211 scans from NSCLC Radiogenomics [2,3,10,16] combined with 83 scans from a proprietary dataset constitute our lower resolution dataset with voxel dimensions of 3.6 × 3.6×3 mm. From this, we split the data to give 210 training samples, 34 validation and 50 testing. Our higher resolution dataset uses AutoPET [10,15] (1014 scans) with voxel dimensions of 2.036 × 2.036 × 3 mm. From this, 850 scans are used for training, 64 for validation and 100 for testing.All baseline models work in a single space with constant dimensions, obtained by registering the AutoPET images to the space of the NSCLC dataset.For evaluation, we use four testing sets: a lower resolution set derived from both the NSCLC and the private dataset; a higher resolution set from AutoPET; a testing set with random crops of the same NSCLC/private testing dataset and finally a testing set that has been rotated through 90 • using the high resolution testing data. As the cropped and rotated dataset cannot be fed into the baseline models, we pad the images to the common image sizing before inference."
Geometry-Invariant Abnormality Detection,4,Results,"The proposed model was trained on the data described in Sect. 3.3, with random crops applied while training. Model and anomaly detection hyperparameter tuning was done on our validation samples using the best DICE scores. We then test our model and baselines on 4 hold-out test sets: a low-resolution whole-body set, a low-resolution cropped set, a high-resolution rotated set and a high-resolution test set of PET images with varying cancers. The visual results shown in Fig. 4 show outputs rotated back to the original orientation. We measure our models'  performance using the DICE score, obtained by thresholding the residual/density score maps. In addition, we calculate the area under the precision-recall curve (AUPRC) as a suitable measure for segmentation performance under class imbalance. We additionally showcase the performance of the classic VQ-VAE + Transformer approach trained on whole-body data only (without the proposed spatial conditioning), as well as the proposed CoordConv model trained with varying image geometries but without the transformer spatial conditioning to explicitly showcase the added contribution of both spatial conditionings. The full results are presented in Table 1 with visual examples shown in Fig. 4. We can observe that the addition of spatial conditioning improves performance even against the same model without conditioning trained on whole-body data (Mann Whitney U test, P < 0.01 on high resolution and P < 0.001 on cropped data for DICE and AUPRC). For cropped data, models trained on whole-body data fail around cropping borders, as showcased in Fig. 4. This is not the case for the models trained on varying geometries. Note that the VQ-VAE + Transformer trained on varying geometries still shows adequate performance, highlighting the resilience of the Transformer network to varying sequence lengths without any form of spatial conditioning. However, by adding the transformer spatial conditioning, we see improvements across all test sets (most significantly on cropped data and the rotated data P < 0.001) for both evaluation metrics. For the rotated data, we see little performance degradation in the conditioned model thanks to the spatial conditioning. The same model without conditioning showed much lower performance with higher false positives likely due to the model's inability to comprehend the anatomical structures present due to the rotated orientation."
Geometry-Invariant Abnormality Detection,5,Conclusion,"Detection and segmentation of anomalous regions, particularly for cancer patients, is essential for staging, treatment and intervention planning. Generally, the variation scanners and acquisition protocols can cause failures in models trained on data from single sources. In this study, we proposed a system for anomaly detection that is robust to variances in geometry. Not only does the proposed model showcase strong and statistically-significant performance improvements on varying image resolutions and FOV, but also on whole-body data. Through this, we demonstrate that one can improve the adaptability and flexibility to varying data geometries while also improving performance. Such flexibility also increases the pool of potential training data, as they dont require the same FOV. We hope this work serves as a foundation for further exploration into geometry-invariant deep-learning methods for medical-imaging."
Geometry-Invariant Abnormality Detection,,Fig. 1 .,
Geometry-Invariant Abnormality Detection,,Fig. 2 .,
Geometry-Invariant Abnormality Detection,,Fig. 3 .,
Geometry-Invariant Abnormality Detection,,Fig. 4 .,
Geometry-Invariant Abnormality Detection,,Table 1 .,VQ-VAE CoordConv0.57 ± 0.09 0.65 ± 0.08 0.63 ± 0.12 0.32 ± 0.17 0.55 ± 0.09 0.64 ± 0.09 0.61 ± 0.13 0.30 ± 0.15 Full CoordConv 0.58 ± 0.08 0.68 ± 0.10 0.67 ± 0.10 0.65 ± 0.12 0.56 ± 0.09 0.66 ± 0.11 0.64 ± 0.11 0.62 ± 0.12
Geometry-Invariant Abnormality Detection,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0_29.
