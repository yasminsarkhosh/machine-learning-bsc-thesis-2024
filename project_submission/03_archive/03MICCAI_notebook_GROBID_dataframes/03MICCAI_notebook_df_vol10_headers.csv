Paper Title,Header Number,Header Title,Text
A Denoised Mean Teacher for Domain Adaptive Point Cloud Registration,1,Introduction,"Recent deep learning-based registration methods have shown great potential in solving medical image registration problems [10,14]. Most of these methods perform the registration based on the raw volumetric intensity images, e.g. [1,5,6,18,31]. By contrast, only a few recent works [13,21] operate on sparse, purely geometric point clouds extracted from the images, even though this representation promises multiple potential benefits, including computational efficiency, robustness against intensity shifts in the image domain, and anonymity preservation. The latter, for instance, can facilitate public data access and federated learning, as exemplified by a recently released point cloud dataset of lung vessels [21] whose underlying CT scans are not publicly accessible. On the other hand, the sparsity of point clouds and the absence of intensity information make the registration problem more challenging. In particular, unsupervised learning with similarity metrics -as established for dense image registration [5,18] -was shown ineffective for deformable point cloud registration [21], as confirmed by our experiments. Since manual annotations for supervised learning are prohibitively costly, an alternative consists of training on synthetic deformations with known displacements [21], as known from dense registration [7,26]. The inevitable domain gap between synthetic and real deformations, however, involves the risk of suboptimal performance on real data. In this work, we aim to bridge this gap through domain adaptation (DA).DA has widely been studied for classification and segmentation tasks [12], with popular techniques ranging from adversarial feature [11,25] or output [24] alignment to self-supervised feature learning [22]. However, these methods are insufficient for the specific characteristics of the registration problem, involving a more complex output space and requiring the detection of local correspondences. Instead, recent works adapted the Mean Teacher paradigm [23], previously established for domain adaptive classification [9] and segmentation [19], to the registration problem [3,16,29]. The basic idea is to supervise the learning student model with displacement fields (pseudo labels) provided by a teacher model, whose weights represent the exponential moving average of the student's weights. A significant limitation of this method, however, is the inevitable noise in the pseudo labels, potentially misguiding the adaptation process. Prior works addressed this problem by refining the pseudo labels [16] or weighting them according to model uncertainty, estimated through Monte Carlo dropout [29,30]. However, even refined pseudo labels remain inaccurate, and the proposed refinement strategy [16] assumes piecewise rigid motions of 3D objects and does not apply to complex deformations in medical applications. And weighting pseudo labels according to teacher uncertainty [29,30] does not explicitly consider the quality of the actual registrations, completely ignores the quality and certainty of the current student predictions, and can, therefore, not prevent detrimental supervision of the student through inferior teacher predictions.Contributions. We introduce two complementary strategies to denoise the Mean Teacher for domain adaptive point cloud registration, addressing the above limitations (see Fig. 1). Both strategies are based on our understanding of an optimal student-teacher relationship. First, if the student's solution to a problem is superior to that of the teacher, good teachers should not insist on their solution but accept the student's approach. To implement this, inspired by a recent technique to filter pseudo labels for human pose estimation [2], we propose to assess the quality of both the teacher and student registrations with the Chamfer distance and to provide only those registrations of the teacher as supervision to the student that are more accurate. This approach differs from previous uncertaintybased methods [29,30] in two decisive aspects: 1) It explicitly assesses the quality of final registrations, using a model-free and objective measure with little computational overhead compared to multiple forward passes in Monte Carlo dropout.2) The selection process considers both teacher and student predictions and can thus prevent detrimental supervision by the teacher. Our second strategy follows the intuition that good teachers should not pose problems to which they do not know the solution. Instead, they should come up with novel tasks with precisely known solutions. Consequently, we propose a completely novel teacher paradigm, where predicted deformations by the teacher are used to synthesize new training pairs for the student, consisting of the original moving inputs and their warps. These input pairs come with precise noise-free displacement labels and significantly differ from static hand-crafted synthetic deformations [21]. 1) The deformations are based on a real data pair that the teacher aims to align. 2) The deformations are dynamic and become more realistic as the teacher improves. Finally, we unify both strategies in a joint framework for domain adaptive point cloud registration. It is compatible with arbitrary geometric registration models, stable to train, and involves only a few hyper-parameters. We experimentally evaluate the method for inhale-to-exhale registration of lung vessel point clouds on the public PVT dataset [21], demonstrating substantial improvements over diverse competing methods and state-of-the-art performance."
A Denoised Mean Teacher for Domain Adaptive Point Cloud Registration,2,Methods,
A Denoised Mean Teacher for Domain Adaptive Point Cloud Registration,2.1,Problem Setup and Standard Mean Teacher,"In point cloud registration, we are given fixed and moving point clouds F ∈ R NF×3 , M ∈ R NM×3 and aim to predict a displacement vector field ϕ ∈ R NM×3 that spatially aligns M to F as M + ϕ. We address the task in a domain adaptation setting with training data comprising a labeled source dataset S of triplets (M s , F s , ϕ s ) and a shifted unlabeled target dataset T of tuples (M t , F t ). While the formulation of our method is agnostic to the specific domain shift between S and T , in this work, we generate the source samples on the fly as random synthetic deformations of the target clouds using a fixed handcrafted deformation function def : R N ×3 → R N ×3 , i.e. source triplets are given as (defNote that def preserves point correspondences enabling ground truth computation through point-wise subtraction. Given the training data, we aim to learn a function f that predicts deformation vector fields as φ = f (M , F ) with optimal performance in the target domain.Baseline Mean Teacher. To solve the problem, the standard Mean Teacher framework [3,9,23] employs two identical networks, denoted as the student f and teacher f , with parameters θ and θ . While the student's weights θ are optimized through gradient descent, the teacher's weights correspond to the exponential moving average (EMA) of the student and are updated as θ i = αθ i-1 +(1-α)θ i at iteration i with momentum α. Meanwhile, the student is trained by minimizingconsisting of the supervised loss L sup on source data and the consistency loss L con on target data, weighted by λ 1 and λ 2 . L con guides the learning of the student in the target domain with pseudo-supervision from the teacher, which, as a temporal ensemble, is expected to be superior to the student. Nonetheless, predictions by the teacher can still be noisy and inaccurate, limiting the efficacy of the adaptation process."
A Denoised Mean Teacher for Domain Adaptive Point Cloud Registration,2.2,Chamfer Distance-Based Filtering of Pseudo Labels,"In a worst-case scenario, the student might predict an accurate displacement field φt , which is strongly penalized by the consistency loss due to an inaccurate teacher prediction φ t . To prevent such detrimental supervision, we aim to select only those teacher predictions for supervision that are superior to the corresponding student predictions, which, however, is complicated by the absence of ground truth. We, therefore, propose to assess the quality of student and teacher registrations by measuring the similarity/distance between fixed and warped moving clouds, with higher similarities/lower distances indicating more accurate registrations. Among existing similarity measures, we opt for the symmetric Chamfer distance [28], which computes the distance between two point clouds X, Y as(2)While we experimentally found the Chamfer distance insufficient as a direct loss function -presumably due to sparse differentiability and susceptibility to local minima, we still observed a strong correlation between Chamfer distance and actual registration error, making it a suitable choice for our purposes. We also explored other measures (Laplacian curvature [28], Gaussian MMD [8]), which proved slightly inferior (Supp., Table 2). Formally, we thus measure the quality of the student prediction φt = f (M t , F t ) as d CD (M t + φt , F t ) and analogously for the teacher prediction φ t . We then define our indicator functionand reformulate the consistency loss in Eq. 1 as(4)"
A Denoised Mean Teacher for Domain Adaptive Point Cloud Registration,2.3,Synthesizing Inputs with Noise-Free Supervision,"While the above filtering strategy mitigates detrimental supervision, the selected pseudo labels are still inaccurate. To our knowledge, there is no prior work with a similarly ""generative"" teacher model. Altogether, we train the student network by minimizing the loss 3 Experiments"
A Denoised Mean Teacher for Domain Adaptive Point Cloud Registration,3.1,Experimental Setup,"Datasets. We evaluate our method for inhale-to-exhale registration of lung vessel point clouds on the public PVT dataset [21] (https://github.com/uncbiag/ robot, License: CC BY-NC-SA 3.0). The dataset comprises 1,010 such data pairs, which were extracted from lung CT scans as part of the IRB-approved COPDGene study (NCT00608764). Ten of these scan pairs are cases from the Dirlab-COPDGene dataset [4] and thus annotated with 300 landmark correspondences. We use these cases as the test set and split the remaining unlabeled pairs into 800 cases for training and 200 for validation (on synthetic deformations only). The original point clouds in the dataset have a very high resolution (∼100k points), making the processing with deep networks computationally costly. Therefore, we extract distinctive keypoints by local density estimation followed by non-maximum suppression. We extract two sets of such keypoints for each cloud: one with the ∼8k most distinctive points for inference, and another with ∼16k points, from which we randomly sample subsets during training for increased variability (see Sect. 2.3, technical details). Finally, we pre-align each pair by matching the mean and standard deviation of the coordinates.Implementation Details. The registration network f is implemented as the default 4-scale architecture of PointPWC-Net [28], operating on 8192 points per cloud. Following [28], we implement L sup , L con , and L syn as multi-scale losses.Optimization is performed with the Adam optimizer. We first pre-train the network on source data (batch size 4) for 160 epochs and subsequently minimize the joint loss (Eq. 6) for 140 epochs, both with a constant learning rate of 0.001, which requires up to 11 GB and 13/23 h on an RTX2080. For joint optimization, we use mixed batches of 4 source and 4 target samples, set λ 1 = λ 2 = λ 3 = 10, and the EMA-parameter to α = 0.996. While the original PVT data pairs represent the target domain in all experiments, we consider two variants of the function def to synthesize source data pairs: a realistic task-specific 2-scale random field similar to [21] and a simple rigid transformation. This enables us to evaluate our method under two differently severe domain shifts. Since real validation data are unavailable, hyper-parameters of all compared methods were tuned in a synthetic adaptation scenario, with the rigid deformations in the source and the 2-scale random field deformations in the target domain. For further implementation details, we refer to our public code.Comparison Methods. 1) The source-only model is exclusively trained on source data without DA. 2) We adopt the standard Mean Teacher [3]. 3) An uncertainty-aware Mean Teacher (UA-MT), similar to [29,30]. 4) As proposed in [28], we performed purely unsupervised training on target data with a Chamfer loss. However, consistent with the findings in [21], this approach could not converge for complex geometric lung structures. Instead, we use the Chamfer loss on target data as an additional loss to complement supervised source training. 5) We guide the learning on target data with the cycle-consistency method from [17]. 6) As a classical algorithm, we adapt sLBP [13]. 7) We collect the results of two current SOTA methods, S-Robot and D-Robot, from [21], which combine deep networks (Point U-Net, PointPWC-Net), trained on synthetic deformations, with optimal transport modules. Note, however, that the experimental setup in  [21] slightly differs from our setting in terms of more input points (60k vs. 8k) and additional input features (vessel radii), thus accessing more information.Metrics. We interpolate the predicted displacements from the moving input cloud to the annotated moving landmarks with an isotropic Gaussian kernel (σ = 5 mm) and measure the target registration error (TRE) with respect to the fixed landmarks. To assess the smoothness of the predictions, we interpolate the sparse displacement fields to the underlying image grid and measure the standard deviation of the logarithm of the Jacobian determinant (SDlogJ)."
A Denoised Mean Teacher for Domain Adaptive Point Cloud Registration,3.2,Results,"Quantitative results are shown in Table 1 and reveal the following insights: 1) The source-only model benefits from realistic synthetic deformations in the source domain, yielding a 40.9% lower TRE.2) The standard Mean Teacher proves effective under the weaker domain shift (-40.7% TRE compared to source-only) but only achieves a slight improvement of 16.0% in the more challenging scenario, where pseudo labels by the teacher are naturally noisier, in turn limiting the efficacy of the adaptation process. 3) Our proposed strategy to filter pseudo labels (ours w/o L syn ) improves the standard teacher and its uncertainty-aware extension, particularly notable under the more severe domain shift (-59.8/-55.0% TRE). 4) Synthesizing novel data pairs with the teacher (ours w/o L con ) alone is slightly inferior to the standard teacher for realistic deformations in the source domain but substantially superior for simple rigid transformations. 5) Combining our two strategies yields further considerable improvements to TREs of 2.31 and 2.38 mm, demonstrating their complementarity. Thus, our method improves the standard Mean Teacher by 13.5/62.8%, outperforms all competitors by statistically significant margins (p < 0.001 in a Wilcoxon signed-rank test), and sets a new state-of-the-art accuracy. Remarkably, our method achieves almost the same accuracy for simple rigid transformations in the source domain as for complex, realistic deformations. Thus, it eliminates the need for designing taskspecific deformation models, which requires strong domain knowledge. Qualitative results are presented in Fig. 2 and Supp., Fig. 3, demonstrating accurate and smooth deformation fields by our method, as confirmed by the SDlogJ in Table 1, which takes small values for all methods."
A Denoised Mean Teacher for Domain Adaptive Point Cloud Registration,4,Conclusion,"Our work addressed domain adaptive point cloud registration to bridge the gap between synthetic source and real target deformations. Starting from the established Mean Teacher paradigm, we presented two novel strategies to tackle the noise of pseudo labels from the teacher model, which is a persistent, significant limitation of the method. Specifically, we 1) proposed to prevent detrimental supervision through the teacher by filtering pseudo labels according to Chamfer distances of student and teacher registrations and 2) introduced a novel teacherstudent paradigm, where the teacher synthesizes novel training data pairs with perfect noise-free displacement labels. Our experiments for lung vessel registration on the PVT dataset demonstrated the efficacy of our method under two scenarios, outperforming the standard Mean Teacher by up to 62.8% and setting a new state-of-the-art accuracy (TRE = 2.31 mm). As such, our method even favorably compares to popular image-based deep learning methods (VoxelMorph [1] and LapIRN [18], e.g., achieve TREs of 7.98 and 4.99 mm on the original DIR-Lab CT images) but lags behind conventional image-based optimization methods [20] with 0.83 mm TRE. But while the latter require run times of several minutes to process the dense intensity scans with 30M+ voxels, our method processes sparse, purely geometric point clouds with 8k points only, enabling anonymity-preservation and extremely fast inference within 0.2 s. In this light, we see two significant potential impacts of our work: First, our method could generally advance purely geometric keypoint-based medical registration, previously limited by the inefficacy of unsupervised learning with similarity metrics.In particular, medical point cloud registration, currently primarily focusing on lung anatomies, still needs to be investigated for other anatomical structures (abdomen, brain) in future work, which might benefit from our generic approach. Second, our method is conceptionally transferable to dense image registration (e.g., intensity-based similarity metrics [15,27] can replace the Chamfer distance). In this context, it appears of great interest to revisit learning from synthetic deformations [7] within a DA setting or to combine our method with unsupervised learning under metric supervision."
A Denoised Mean Teacher for Domain Adaptive Point Cloud Registration,,Fig. 1 .,
A Denoised Mean Teacher for Domain Adaptive Point Cloud Registration,,Fig. 2 .,
A Denoised Mean Teacher for Domain Adaptive Point Cloud Registration,,,"t , M t + φ t ) by warping M t with φ t . The underlying displacement field is naturally precisely known, enabling noise-free training of the student by minimizing"
A Denoised Mean Teacher for Domain Adaptive Point Cloud Registration,,Table 1 .,
MoCoSR: Respiratory Motion Correction and Super-Resolution for 3D Abdominal MRI,1,Introduction,"Inflammatory bowel disease (IBD) is a relatively common, but easily overlooked disease. Its insidious clinical presentation [1] can lead to a long delay between the initial causative event and the diagnosis [2]. One of its manifestations, Crohn's disease, often exhibits symptoms such as abdominal pain, diarrhoea, fatigue, and cramping pain, which can be accompanied by severe complications [3]. Although Crohn's disease cannot be completely cured, early diagnosis can significantly reduce treatment costs and permanent physical damage [4]. MRI plays a crucial role in diagnosing and monitoring Crohn's disease. In clinical applications and research, high-resolution (HR) MRI is often preferred over endoscopy as it is non-invasive and visualises more details for the small bowel. MRI is preferred over computed tomography (CT) imaging as it does not use radiation, which is an important consideration in younger patients. Unfortunately, MR acquisition for patients with Crohn's disease can easily become compromised by respiratory motion.As a result, many patients' images are degraded by respiration, involuntary movements and peristalsis. Furthermore, due to technical limitations, it is difficult to acquire HR images in all scan orientations. This limits the assessment of the complete volume in 3D. Given these problems, we aim to develop a novel method that can perform both motion correction (MC) and super-resolution (SR) to improve the quality of 3D IBD MRI and to support accurate interpretation and diagnosis.Motion can cause multiple issues for MR acquisition. Abdominal MRI scans are usually 2D multi-slice acquisitions [5]. As a result, 3D bowel motion can lead to intra-and inter-plane corruptions [6], e.g., slice misregistration, slice profile effects, and anisotropic spatial resolution. SR can be used to enhance these scans, but conventional methods often struggle with this type of anisotropic data or may unintentionally hide significant imaging findings.Despite these challenges, MC and SR are crucial because corrupted MR images can lead to inaccurate interpretation and diagnosis [7]. Manual correction or enhancement of these volumes is not feasible."
MoCoSR: Respiratory Motion Correction and Super-Resolution for 3D Abdominal MRI,,Contribution:,"Our method (MoCoSR) alleviates the need for semantic knowledge and manual paired-annotation of individual structures and the requirement for acquiring multiple image stacks from different orientations, e.g., [8].There are several methodological contributions of our work: (1) First, to account for non-isotropic voxel sizes of abdominal images, we reconstruct spatial resolution from corrupted bowel MR images by enforcing cycle consistency.(2) Second, volumes are corrected by incorporating latent features in the LR domain. The complementary spatial information from unpaired quality images is exploited via cycle regularisation to provide an explicit constraint. Third, we conduct extensive evaluations on 200 subjects from a UK Crohn's disease study, and a public abdominal MRI dataset with realistic respiratory motion.(3) Experimental evaluation and analysis show that our MoCoSR is able to generate high-quality MR images and performs favourably against other, alter-native methods. Furthermore, we explore confidence in the generated data and improvements to the diagnostic process. (4) Experiments with existing models for predicting the degree of small bowel inflammation in Crohn's disease patients show that MoCoSR can retain diagnostically relevant features and maintain the original HR feature distribution for downstream image analysis tasks.Related Work: MRI SR. For learning-based MRI super-resolution, [9] discussed the feasibility of learning-based SR methods, where encoder-decoder methods [10][11][12] are commonly used to model a variety of complex structures while preserving details. Most single-forward [13][14][15] and adversarial methods [16,17] rely on paired data to learn the mapping and degradation processes, which is not acceptable in real-world scenarios where data are mismatched. [18] utilizes cyclic consistency structures to address unpaired degradation adaptation in brain SR, however abdominal data would be more complicated and suffer from motion corruption. Joint optimization of MC and SR remains challenging because of the high-dimensionality of HR image space, and LR latent space has been introduced in order to alleviate this issue. Recent studies on SR joint with other tasks (e.g., reconstruction, denoising) have demonstrated improvements in the LR space [11,19,20]. For this purpose, we utilize a cycle consistency framework to handle unpaired data and joint tasks.Automated Evaluation of IBD. In the field of machine learning and gastrointestinal disease, [21] used random forests to segment diseased intestines, which is the first time that image analysis support has been applied to bowel MRI. However, this technique requires radiologists to label and evaluate diseased bowel segments, and patients' scan times are long. In [22] residual networks focused on the terminal ileum to detect Crohn's disease. In this case, quality reconstruction data is extremely important for the detection of relevant structures."
MoCoSR: Respiratory Motion Correction and Super-Resolution for 3D Abdominal MRI,2,Method,"Problem Formulation and Preliminaries: In 3D SR, the degradation process is modeled with: I LR = D(I HR ; k I , ↓ s )+n, D() represents the downsampling with the blur kernel k I , scaling factor s, and noise n. In this work, we propose the motion corruption term M , which operates on LR latent space. And our MC-SR model can be refined toMoCoSR Concept: As shown in Fig. 1, our multi-task framework consists of three parts: a pair of corrupted LR (CLR) encoder and SR decoder on corrupted LR input, a pair of quality SR (QLR) encoder and learned LR (LLR) decoder on HR input and two task-specific discriminators. For the first two pairs, individual features are extracted and scaling is applied to provide our network with the ability to handle multiple tasks at different scales. The discriminators are then used to identify features for each scale. A quality HR volume Y is fed first into the QHR encoder in order to obtain a quality latent feature map Z Q upon which the corrupted LR can be trained. due to the unpaired scenario. For MC, Z Q passes to LLR and CLR to gain ZQ with L MC , where LLR learns the corruption feature and degradation, and CLR encodes the multi-degradation from the corrupted LR input X. For SR, Ỹ is then generated after the SR decoder and ensure the SR purpose with L SR . To ensure training stability, two consistency loss functions L Con are utilized for each resolution space. The arrows indicate the flow of the tasks. A pair of task-specific discriminators is used to improve the performance of each task-related encoder and decoder.Loss Functions: Rather than aiming to reconstruct motion-free and HR images in high dimensional space with paired data, we propose to regularize in low dimensional latent space to obtain a high quality LR feature that can be used for upscaling. A L MC between ZQ downsampled from QHR and Z Q cycled after LLR and CLR, defines in an unpaired manner as follows:As the ultimate goal, SR focuses on the recovery and upscaling of detailed high-frequency feature, L SR is proposed to optimize the reconstruction capability by passing through cycles at various spaces:The dual adversarial L DAdv is applied to improve the generation capability of the two sets of single generating processes in the cyclic network:The corresponding two task-specific discriminators L DM C and L DSR for discriminating between corrupted and quality images followed are used for the purpose of staged reconstruction Z Q and Ŷ of MC at latent space and SR at spatial space, respectively. Furthermore, a joint cycle consistency loss is used to improve the stability of training in both spaces:For MoCoSR generators, the joint loss function is finally defined as follows: Network Architecture: In the paired encoder-decoder structure, we developed a 3D Global and Local Residual Blocks (GLRB) with Local Residual Modules (LRM) in Fig. 2 based on [23]. The GLRB is designed to extract local features and global structural information at 3D level, and then construct blocks for multi-scale features connected to the output of the first layer. The residual output is then added to the input using a residual connection to obtain a staged output. The model implements the extraction of local features while integrating all previous features through the connected blocks and compression layers. The decoders are employed with the upsampling prior to the convolution layers."
MoCoSR: Respiratory Motion Correction and Super-Resolution for 3D Abdominal MRI,3,Experiments,"Data Degradation: We use 64 × 64 × 64 patches. For downsampling and the degradation associated with MRI scanning, (1) Gaussian noise with a standard deviation of 0.25 was added to the image. (2) Truncation at the k-space, retaining only the central region of the data. On top of this, we have developed a motion simulation process to represent the pseudo-periodic multi-factor respiratory motion (PMRM) that occurs during scanning, as shown in Fig. 3 (a). The simulated motion includes the influence of environmental factors that cause the respiratory intensity and frequency to fluctuate within a certain range. This lead to the presence of inter-slice misalignment in image domains. IBD Data Set, Inflammatory Bowel Disease: MRI sequences obtained include axial T2 images, coronal T2 images and axial postcontrast MRI data on a Philips Achieva 1.5 T MR scanner. Abdominal 2D-acquired images exhibit motion shifts between slices and fibrillation artefacts due to the difficulty of holding one's breath/body movement and suppressing random organ motion for extended periods. The dataset contains 200 available sample cases with four classes, healthy, mild, moderate, and severe small bowel Crohn's disease inflammation as shown in Table 1. The abnormal Crohn's disease sample cases, which could contain more than one segment of terminal ileum and small bowel Crohn's disease, were defined based on the review of clinical endoscopic, histological, and radiological images and by unanimous review by the same two radiologists (this criterion has been used in the recent METRIC trial investigating imaging in Crohn's disease [24])."
MoCoSR: Respiratory Motion Correction and Super-Resolution for 3D Abdominal MRI,,Setting:,"We compare with interpolation of bicubic and bilinear techniques, rapid and accurate MR image SR (RAISR-MR) with hashing-based learning [25], which we use as the representative of the model-based methods, and MRI SR with various generative adversarial networks (MRESR [17], CMRSR [16]) as the learning-based representatives. For training, with train/val/test with 70%/10%/20%. Various methods were included in the quantitative evaluation, including single forward WGAN (S-WGAN) and cycle RDB (C-RDB) for ablation experiments on the cycle consistency framework and the GLRB setting. "
MoCoSR: Respiratory Motion Correction and Super-Resolution for 3D Abdominal MRI,,Results:,"The quantitative results are presented in Table 2 on TCGA-LIHC and IBD data sets. There is a performance gap between interpolation and GANbased methods, and the SR method based on learning has a significant mapping advantage for complex gastrointestinal images. MoCoSR achieves the best performance among all evaluation metrics. CMRSR and MRESR cannot guarantee the output quality of mapping from HR back to LR, resulting in poor performance on complex 3D bowel data. Representatives for the qualitative evaluation are shown in Fig. 4. Sensitivity Analysis: We evaluate if our method influences downstream analysis using the example of automatic scoring of Crohn's disease with existing deep networks. If features defining Crohn's disease are hidden by the method, this would affect disease scoring. We use a classifier with an attention mechanism similar to [22], trained on HR raw data. Our evaluation is based on the average possibility of normal and abnormal small bowel inflammation on MRI. The degree of small bowel inflammation on abnormal MRIs was classified by Radiologists as mild, moderate or severe. This outcome was compared against the results of the data constructed from different SR methods.Complete results including LR degraded image, SR image reconstructed by MRESR, CMRSR, and MoCoSR, are shown in Table 3. We tracked and quantified the changes by performing a significance evaluation (t-test) based on p-values < 0.05. The ideal SR data can achieve classification results as close as possible to HR data with lower requirements. Our method obtains similar small-scale attenuation results on both healthy and abnormal samples. The p-value is larger than 0.05 for MoCoSR, i.e., there is no statistically significant difference between the original and reconstructed data for the prediction results. The results of MRESR are volatile but present an unexpected improvement on healthy samples. CMRSR makes the predicted probability much lower than that of HR.Discussion: According to the sensitivity analysis and comparison results, our MoCoSR method shows superior results compared to the forward adversarial reconstruction algorithms and encoder-decoder structures. Combining multiscale image information in the feature space of different resolution image domains yields better results than inter-domain integration. The cycle consistency network splits the different resolution spaces and latent space, which facilitates the flexibility of the neural network to customize the MC according to the specific purpose and ensures consistency of the corrected data with the unpaired data. Furthermore, although these methods can obtain acceptable SSIM and PSNR, the key features used by the classifier for downstream tasks are potentially lost during the reconstructions. Conversely, the reconstruction result will implicitly cause domain shift. This leads to a distribution shift in the samples, which makes the disease prediction biased as shown in Fig. 5. The data generated by ours can  reconstruct the results, retain likely all of the diagnostically valuable features, and maintain the original data distribution. The present sensitivity study is limited to the automatic classification from single domain and down-stream task framework, and future extensions will explore model-based and learning segmentation tasks across data domains and acquisitions."
MoCoSR: Respiratory Motion Correction and Super-Resolution for 3D Abdominal MRI,4,Conclusion,"MoCoSR is a DL-based approach to reconstruct high-quality SR MRI. MoCoSR is evaluated extensively and compared to the various image SR reconstruction algorithms on a public abdominal dataset, simulating different degrees of respiratory motion, and an IBD dataset with inherent motion. MoCoSR demonstrated superior performance. To test if our learned reconstruction preserves clinically relevant features, we tested on a downstream disease scoring method and found no decrease in disease prediction performance with MoCoSR."
MoCoSR: Respiratory Motion Correction and Super-Resolution for 3D Abdominal MRI,,Fig. 1 .,
MoCoSR: Respiratory Motion Correction and Super-Resolution for 3D Abdominal MRI,,Fig. 2 .,
MoCoSR: Respiratory Motion Correction and Super-Resolution for 3D Abdominal MRI,,Fig. 3 .,
MoCoSR: Respiratory Motion Correction and Super-Resolution for 3D Abdominal MRI,,Fig. 4 .,
MoCoSR: Respiratory Motion Correction and Super-Resolution for 3D Abdominal MRI,,Fig. 5 .,
MoCoSR: Respiratory Motion Correction and Super-Resolution for 3D Abdominal MRI,,Table 1 .,
MoCoSR: Respiratory Motion Correction and Super-Resolution for 3D Abdominal MRI,,Table 2 .,
MoCoSR: Respiratory Motion Correction and Super-Resolution for 3D Abdominal MRI,,Table 3 .,
Non-iterative Coarse-to-Fine Transformer Networks for Joint Affine and Deformable Image Registration,1,Introduction,"Image registration is a fundamental requirement for medical image analysis and has been an active research focus for decades [1]. It aims to find a spatial transformation between a pair of fixed and moving images, through which the moving image can be warped to spatially align with the fixed image. Similar to natural image registration [2], medical image registration usually requires affine registration to eliminate rigid misalignments and then performs additional deformable registration to address non-rigid deformations. Traditional methods usually formulate medical image registration as a time-consuming iterative optimization problem [3,4]. Recently, deep registration methods based on deep learning have been widely adopted to perform end-to-end registration [5,6]. Deep registration methods learn a mapping from image pairs to spatial transformations based on training data in an unsupervised manner, which have shown advantages in registration accuracy and computational efficiency [7][8][9][10][11][12][13][14][15][16][17][18].Many deep registration methods perform coarse-to-fine registration to improve registration accuracy, where the registration is decoupled into multiple coarse-to-fine registration steps that are iteratively performed by using multiple cascaded networks [10][11][12][13] or repeatedly running a single network for multiple iterations [14,15]. Mok et al. [13] proposed a Laplacian pyramid Image Registration Network (LapIRN), where multiple networks at different pyramid levels were cascaded. Shu et al. [14] proposed to use a single network (ULAE-net) to perform coarse-to-fine registration with multiple iterations. These methods perform iterative coarse-to-fine registration and extract image features repeatedly in each iteration, which inevitably increases computational loads and prolongs the registration runtime. Recently, Non-Iterative Coarse-to-finE (NICE) registration methods have been proposed to perform coarse-to-fine registration with a single network in a single iteration [16][17][18]. For example, we previously proposed a NICE registration network (NICE-Net) [18,19], where multiple coarse-to-fine registration steps are performed with a single network in a single iteration. These NICE registration methods show advantages in both registration accuracy and runtime on the benchmark task of intra-patient brain MRI registration. Nevertheless, we identified that existing NICE registration methods still have two main limitations.Firstly, existing NICE registration methods merely focus on deformable coarseto-fine registration, while affine registration, a common prerequisite, is still reliant on traditional registration methods [16,18] or extra affine registration networks [17]. Using traditional registration methods incurs time-consuming iterative optimization, while cascading extra networks consumes additional computational resources (e.g., extra GPU memory and runtime). Secondly, existing NICE registration methods are based on Convolution Neural Networks (CNN) and thus are limited by the intrinsic locality (i.e., limited receptive field) of convolution operations. Transformers have been widely adopted in many medical applications for their capabilities to capture long-range dependency [20]. Recently, transformers have also been shown to improve registration with conventional Voxelmorph [7]-like architecture [21][22][23]. However, the benefits of using transformers for NICE registration have not been explored.In this study, we propose a Non-Iterative Coarse-to-finE Transformer network (NICE-Trans) for joint affine and deformable registration. Our technical contributions are two folds: (i) We extend the existing NICE registration framework to affine registration, where multiple steps of both affine and deformable coarse-to-fine registration are performed with a single network in a single iteration. (ii) We explore the benefits of transformers for NICE registration, where Swin Transformer [24] is embedded into the NICE-Trans to model long-range relevance between fixed and moving images. This is the first deep registration method that integrates previously separated affine and deformable coarse-to-fine registration into a single network, and this is also the first deep registration method that exploits transformers for NICE registration. Extensive experiments with seven public datasets show that our NICE-Trans outperforms state-of-the-art registration methods on both registration accuracy and runtime."
Non-iterative Coarse-to-Fine Transformer Networks for Joint Affine and Deformable Image Registration,2,Method,"Image registration aims to find a spatial transformation φ that warps a moving image I m to a fixed image I f , so that the warped image I m•φ = I m • φ is spatially aligned with the I f . In this study, we assume the I m and I f are two single-channel, grayscale volumes defined in a 3D spatial domain ⊂ R 3 , which is consistent with common medical image registration studies [7][8][9][10][11][12][13][14][15][16][17][18]. The φ is parameterized as a displacement field, and we parametrized the image registration problem as a function R θ (I f , I m ) = φ using NICE-Trans. As shown in Fig. 1, our NICE-Trans consists of an intra-image feature learning encoder and an inter-image relevance modeling decoder (refer to Sect. 2.1). Multiple steps of affine and deformable registration are performed within a single network iteration (refer to Sect. 2.2). The θ is a set of learnable parameters that are optimized through unsupervised learning (refer to Sect. 2.3)."
Non-iterative Coarse-to-Fine Transformer Networks for Joint Affine and Deformable Image Registration,2.1,Non-iterative Coarse-to-fine Transformer Networks (NICE-Trans),"The architecture of the proposed NICE-Trans is presented in Fig. 1, which consists of a dual-path encoder to learn image features from I m and I f separately and a single-path decoder to model the spatial relevance between I m and I f . Skip connections are used at multiple scales to propagate features from the encoder to the decoder. Here, we assume the NICE-Trans performs L a and L d steps of affine and deformable registration, resulting in a total of L = L a + L d steps of coarse-to-fine registration.The encoder has two identical, weight-shared paths P m and P f that take I m and I f as input, respectively. Each path consists of L successive Conv modules with 2 × 2 × 2 max pooling applied between two adjacent modules, which produces two L-level feature pyramidswhere the F f i and F m i are the output of the i th Conv module in the P f and P m . Each Conv module consists of two 3 × 3 × 3 convolutional layers followed by LeakyReLU activation with parameter 0.2. This dual-path design can learn uncoupled image features of I m and I f , which enables the NICE-Trans to reuse the learned features at multiple registration steps, thereby discarding the requirement for repeated feature learning.The decoder consists of L-1 SwinTrans modules and a Conv module, with a patch expanding layer [23] applied between two adjacent modules to double the feature resolution and halve the feature dimension. Each SwinTrans module consists of one 1 × 1 × 1 convolutional layer for feature dimension reduction and four successive Swin Transformer blocks [24] including layer normalization, Window/Shifted Window-based Multi-head Self-Attention (W/SW-MSA), Multilayer Perceptron (MLP), and residual connections. The output of each decoder module is fed into an affine or deformable registration head that maps the input features into a displacement field, which produces L displacement fields φ i ∈ {φ 1 , φ 2 , . . . , φ L } for L steps of coarse-to-fine registration (detailed in Sect. 2.2). The output of each patch expanding layer is concatenated with, which is then fed into its later decoder module. The decoder performs finer registration after each decoder module, where the φ L is the final output φ. Detailed architecture settings (e.g., feature dimensions, head numbers of self-attention) are presented in the supplementary materials.Our NICE-Trans differs from the existing NICE-Net [18] mainly in two aspects: (i) our NICE-Trans integrates affine and deformable registration into a unified network, and (ii) our NICE-Trans leverages Swin Transformer to model long-range spatial relevance between I m and I f . In addition, the existing NICE-Net extracts features from the intermediately warped image at each registration step, while our NICE-Trans directly warps the F m to avoid this process and achieves similar performance."
Non-iterative Coarse-to-Fine Transformer Networks for Joint Affine and Deformable Image Registration,2.2,Joint Affine and Deformable Registration,"The output features of the first L a decoder modules are fed into L a affine registration heads, where the features are mapped to a 3 × 4 affine matrix through global average pooling and two fully-connected layers, which are then sampled as a dense displacement field. After the first L a steps of affine registration, the output features of the last L d decoder modules are fed into L d deformable registration heads, where the features are directly mapped to a dense displacement field via a 3 × 3 × 3 convolutional layer.At the beginning of coarse-to-fine registration, the φ 1 is the output of the first registration head. Then, the φ 1 is upsampled (×2) and voxel-wisely added to the output of the second registration head to derive φ 2 . This process is repeated until the φ L is derived, which realizes joint affine and deformable coarse-to-fine registration. In our experiments, we set L a and L d as 1 and 4 (illustrated in Fig. 1) as this setting achieved the best validation results (refer to the supplementary materials). Figure 2 exemplifies a registration result of the NICE-Trans with five steps of coarse-to-fine registration. "
Non-iterative Coarse-to-Fine Transformer Networks for Joint Affine and Deformable Image Registration,2.3,Unsupervised Learning,"The learnable parameters θ are optimized using an unsupervised loss L that does not require labels. The L is defined as L = L sim + σ L reg , where the L sim is an image similarity term that penalizes the differences between the warped image I m•φ and the fixed image I f , the L reg is a regularization term that encourages smooth and invertible transformations φ, and the σ is a regularization parameter.We adopt negative local normalized cross-correlation (NCC) as the L sim , which is a widely used similarity metric in image registration methods [7][8][9][10][12][13][14][15][16][17][18]. For the L reg , we impose a diffusion regularizer on the φ to encourage its smoothness and also adopt a Jacobian Determinant (JD) loss [25] to enhance its invertibility. As the φ is not invertible at voxel p where the Jacobian determinant is negative (|J φ(p)| ≤ 0) [26], the JD loss explicitly penalizes the negative Jacobian determinants of φ. Finally, the L reg is defined as , where the λ is a regularization parameter balancing registration accuracy and transformation invertibility."
Non-iterative Coarse-to-Fine Transformer Networks for Joint Affine and Deformable Image Registration,3,Experimental Setup,
Non-iterative Coarse-to-Fine Transformer Networks for Joint Affine and Deformable Image Registration,3.1,Dataset and Preprocessing,"We evaluated the proposed NICE-Trans on the task of inter-patient brain MRI registration, which is a common benchmark task in medical image registration studies [7][8][9][12][13][14][15][16][17][18]. We followed the dataset settings in [18]: 2,656 brain MRI images acquired from four public datasets (ADNI [27], ABIDE [28], ADHD [29], and IXI [30]) were used for training; two public brain MRI datasets with anatomical segmentation (Mindboggle [31] and Buckner [32]) were used for validation and testing. The Mindboggle dataset contains 100 MRI images and were randomly split into 50/50 images for validation/testing. The Buckner dataset contains 40 MRI images and were used for testing only. In addition to the original settings of [18], we adopted an additional public brain MRI dataset (LPBA [33]) for testing, which contains 40 MRI images.We performed brain extraction and intensity normalization for each MRI image with FreeSurfer [32]. Each image was placed at the same position via Center of Mass (CoM) initialization [34], and then was cropped into 144 × 192 × 160 voxels."
Non-iterative Coarse-to-Fine Transformer Networks for Joint Affine and Deformable Image Registration,3.2,Implementation Details,"We implemented our NICE-Trans using PyTorch on a NVIDIA Titan V GPU with 12 GB memory. We used an ADAM optimizer with a learning rate of 0.0001 and a batch size of 1 to train the NICE-Trans for 100,000 iterations. At each iteration, two images were randomly picked from the training data as the fixed and moving images. A total of 100 image pairs, randomly picked from the validation data, were used to monitor the training process and to optimize hyper-parameters. We set σ as 1 to ensure that the L sim and σ L reg have close values, while the λ was set as 10 -4 to ensure that the percentage of voxels with negative Jacobian determinants is less than 0.05% (refer to the supplementary materials for detailed regularization analysis). Our code will be available in https://github.com/ MungoMeng/Registration-NICE-Trans."
Non-iterative Coarse-to-Fine Transformer Networks for Joint Affine and Deformable Image Registration,3.3,Comparison Methods,"Our NICE-Trans was compared with nine image registration methods, including two traditional methods and seven deep registration methods. The compared traditional methods are SyN [3] and NiftyReg [4]. For these methods, we used cross-correlation as the similarity measure and adopted FLIRT [35] for affine registration. The compared deep registration methods are VoxelMorph (VM) [7], Diffeomorphic VoxelMorph (DifVM) [8], TransMorph [21], Swin-VoxelMorph (Swin-VM) [22], LapIRN [13], ULAE-net [14], and NICE-Net [18]. The VM and DifVM are two commonly benchmarked registration methods in the literature [12][13][14][15][16][17][18][21][22][23]. The TransMorph and Swin-VM are two state-of-the-art methods that embed Swin Transformer into VM-like architecture. The LapIRN, ULAE-net, and NICE-Net are three state-of-the-art coarse-to-fine registration methods. For the compared deep registration methods, we adopted NCC as the similarity loss and followed [17,36] to cascade a CNN-based registration network (AffineNet) for affine registration."
Non-iterative Coarse-to-Fine Transformer Networks for Joint Affine and Deformable Image Registration,3.4,Experimental Settings,"We compared the NICE-Net to the nine comparison methods for subject-to-subject registration. For testing, we randomly picked 100 image pairs from each of the Mindboggle, Buckner, and LPBA testing sets. We used standard evaluation metrics for medical image registration [7][8][9][10][11][12][13][14][15][16][17][18]. The registration accuracy was evaluated using the Dice similarity coefficients (DSC) of segmentation labels, while the smoothness and invertibility of spatial transformations were evaluated using the percentage of Negative Jacobian Determinants (NJD). Generally, a higher DSC and a lower NJD indicate better registration performance. A two-sided P value less than 0.05 is considered to indicate a statistically significant difference between two DSCs.We also performed an ablation study to explore the benefits of transformers. We built a baseline method that has the same architecture as the NICE-Trans but only uses Conv modules. After that, we embedded Swin Transformer into the baseline method, where SwinTrans modules replaced the Conv modules in the encoder (Trans-Encoder), decoder (Trans-Decoder), or both (Trans-All)."
Non-iterative Coarse-to-Fine Transformer Networks for Joint Affine and Deformable Image Registration,4,Results and Discussion,"Table 1 presents the registration performance of our NICE-Trans and all comparison methods. The registration accuracy of all methods degraded by 1-3% in DSC when affine registration was not performed, which demonstrates the importance of affine registration. However, using FLIRT or AffineNet for affine registration incurred extra computational loads and increased the registration runtime. Our NICE-Trans performed joint affine and deformable registration, which enabled it to realize affine registration with negligible additional runtime. Moreover, we suggest that integrating affine and deformable registration into a single network also brings convenience for network training. Training two separate affine and deformable registration networks will prolong the whole training time, while joint training will consume more GPU memory. As for registration accuracy, the TransMorph and Swin-VM achieved higher DSCs than the conventional VM and DifVM, but still cannot outperform the existing CNN-based coarse-to-fine registration methods (LapIRN, ULAE-net, and NICE-Net). Our NICE-Trans leverages Swin Transformer to perform coarse-to-fine registration, which enabled it to achieve the highest DSCs among all methods. This means that our NICE-Trans also has advantages on registration accuracy. We present a qualitative comparison in the supplementary materials, which shows that the registration result produced by our NICE-Trans is more consistent with the fixed image. In addition, there usually exists a trade-off between DSC and NJD as imposing constraints on the spatial transformations limits their flexibility, which results in degraded registration accuracy [13,18]. For example, compared with VM, the DifVM with diffeomorphic constraints achieved better NJDs and worse DSCs. Nevertheless, our NICE-Trans achieved both the best DSCs and NJDs. We suggest that, if we set λ as 0 to maximize the registration accuracy with the cost of transformation invertibility, our NICE-Trans can achieve higher DSCs and outperform the comparison methods by a larger margin (refer to the regularization analysis in the supplementary materials).Table 2 shows the results of our ablation study. Swin Transformer improved the registration performance when embedded into the decoder, but had limited benefits in the encoder. This suggests that Swin Transformer can benefit registration in modeling inter-image spatial relevance while having limited benefits in learning intra-image representations. This finding is intuitive as image registration aims to find spatial relevance between images, instead of finding the internal relevance within an image. Under this aim, embedding transformers in the decoder helps to capture long-range relevance between images and improves registration performance. We noticed that previous studies gained improvements by embedding Swin Transformer in the encoder [21] or leveraging a full transformer network [22]. This is attributed to the fact that they used a VM-like architecture that entangles image representation learning and spatial relevance modeling throughout the whole network. Our NICE-Trans decouples these two parts and provides further insight on using transformers for registration: leveraging transformers to learn intra-image relevance might not be beneficial but merely incurs extra computational loads. It should be acknowledged that there are a few limitations in our study. First, the experiment (Table 1) demonstrated that our NICE-Trans can well address the inherent misalignments among inter-patient brain MRI images, but the sensitivity of affine registration to different degrees of misalignments is still awaiting further exploration. Second, in this study, we evaluated the NICE-Trans on the benchmark task of inter-patient brain MRI registration, while we believe that our NICE-Trans also could apply to other image registration applications (e.g., brain tumor registration [37])."
Non-iterative Coarse-to-Fine Transformer Networks for Joint Affine and Deformable Image Registration,5,Conclusion,"We have outlined a Non-Iterative Coarse-to-finE Transformer network (NICE-Trans) for medical image registration. Unlike the existing image registration methods, our NICE-Trans performs joint affine and deformable coarse-to-fine registration with a single network in a single iteration. The experimental results show that our NICE-Trans can outperform the state-of-the-art coarse-to-fine or transformer-based deep registration methods on both registration accuracy and runtime. Our study also suggests that transformers benefit registration in modeling inter-image spatial relevance while having limited benefits in learning intra-image representations."
Non-iterative Coarse-to-Fine Transformer Networks for Joint Affine and Deformable Image Registration,,Fig. 1 .,
Non-iterative Coarse-to-Fine Transformer Networks for Joint Affine and Deformable Image Registration,,Fig. 2 .,
Non-iterative Coarse-to-Fine Transformer Networks for Joint Affine and Deformable Image Registration,,Table 1 .,"Bold: the best DSC and NJD in each testing dataset and the shortest runtime of completing both affine and deformable registration. *: <0.05, in comparison to NICE-Trans (ours)."
Non-iterative Coarse-to-Fine Transformer Networks for Joint Affine and Deformable Image Registration,,Table 2 .,Bold: the best DSC and NJD in each testing dataset.
Unsupervised 3D Registration Through Optimization-Guided Cyclical Self-training,1,Introduction,"Medical image registration is a fundamental task in medical imaging with applications ranging from multi-modal data fusion to temporal data analysis. In recent years, deep learning has advanced learning-based registration methods [11], which achieve competitive performances at low runtimes and thus constitute a promising alternative to accurate but slow classical optimization methods. A decisive factor in successfully training deep learning-based methods is the choice of a suitable strategy to supervise the learning process. In the literature, there exist three different learning strategies. The first is supervised learning based on manual annotations such as landmark correspondences [9] or semantic labels [16]. However, manual annotations are costly and may introduce a label bias [2]. Alternatively, a second strategy employs synthetic deformation fields to generate image pairs with precisely known displacement fields [7]. However, this introduces a domain gap between synthetic training and real test pairs, limiting the performance at inference time. Elaborated deformation techniques can reduce the gap but require strong domain knowledge, are tailored to specific problems, and do not generalize across tasks. The third widely used training strategy is unsupervised metric-based learning, maximizing a similarity metric between fixed and warped moving images, e.g. implemented in [2,17]. Popular metrics include normalized cross-correlation [19] and MIND [13]. However, the success of this strategy strongly depends on the specific hand-crafted metric, and the performance of the trained deep learning models is often inferior to a classical optimization-based counterpart. Considering the deficiencies of the above training techniques, in this work, we introduce a novel learning strategy for unsupervised registration based on the concept of self-training.Self-training is a widespread training strategy for semi-supervised learning [24] and domain adaptation [29]. The core idea is to pre-train a network on available labeled data and subsequently apply the model to the unlabeled data to generate so-called pseudo labels. Afterwards, one alternates between re-training the model on the union of labeled and pseudo-labeled data and updating the pseudo labels with the current model. This general concept was successfully adapted to diverse tasks and settings, with methods in medical context primarily focusing on segmentation [8,18]. These methods resort to a special form of self-training, the Mean Teacher paradigm [22], where pseudo labels are continuously provided by a teacher model, representing a temporal ensemble of the learning network. A persistent problem of classical and Mean Teacher-based selftraining is the inherent noise of the pseudo labels, which can severely hamper the learning process. As a remedy, some works aim to filter reliable pseudo labels based on model uncertainty [28]. Only recently, the Mean Teacher was adapted to the registration problem, tackling domain adaptation [3] or complementing metric-based supervision for adaptive regularization weighting [25]. Contrary to these methods, we introduce self-training for registration in a fully unsupervised setting, with pseudo labels as the single source of supervision.Contributions. We introduce a novel learning paradigm for unsupervised registration by adapting the concept of self-training to the problem. This involves two principal challenges. First, labeled data for the pre-training stage is unavailable, raising the question of how to generate initial pseudo labels. Second, as a general problem in self-training, the negative impact of noise in the pseudo labels needs to be mitigated. In our pursuit to overcome these challenges, we made two decisive observations (see Fig. 2) when exploring a combination of deep learning-based feature extraction with differentiable optimization algorithms for the displacement prediction, such as [9,20]. First, we found that feature-based optimizers predict reasonable displacement fields and improve the initial registration even when applied to the output of random feature networks (orange line in Fig. 2). We attribute this feature to the inductive bias of deep neural networks, which extract somewhat useful features even with random weights [4]. These predicted displacements thus constitute meaningful initial pseudo labels, solving the first problem and leaving us with the second problem to overcome the noise in the labels. In this context, we made the second observation that the intrinsic regularizing capacity of the optimizers stabilizes the learning from noisy labels. Specifically, training the feature extractor on our initial pseudo labels yielded registrations surpassing the accuracy of the noisy labels used for training (green, red, purple, brown, and magenta lines in Fig. 2). Consequently, we propose a cyclical self-training scheme, alternating between training the feature extractor and updating the pseudo labels. As such, our novel learning paradigm does not require costly manual annotations, prevents the domain shift of synthetic deformations, and is independent of hand-crafted similarity metrics. Moreover, our method significantly differs from previous uncertainty-based pseudo label filtering strategies since it implicitly overcomes the negative impact of noisy labels by combining deep feature learning with regularizing differentiable optimization. We evaluate the method for CT abdomen registration and keypoint-based lung registration, demonstrating substantial improvements over diverse state-of-theart comparison methods."
Unsupervised 3D Registration Through Optimization-Guided Cyclical Self-training,2,Methods,
Unsupervised 3D Registration Through Optimization-Guided Cyclical Self-training,2.1,Problem Setup,"Given a data pair (F , M ) of a fixed and a moving image as input, registration aims at finding a displacement field ϕ that spatially aligns M to F . We address the task in an unsupervised setting, where training dataconsists of |T | unlabeled data pairs. Given the training data, we aim to learn a function f with parameters θ f , (partially) represented by a deep network, which predicts displacement fields as φ = f (F , M ; θ f )."
Unsupervised 3D Registration Through Optimization-Guided Cyclical Self-training,2.2,Cyclical Self-training,"We propose to solve the above problem with a cyclical self-training strategy visualized in Fig. 1. While existing self-training methods assume the availability of some labeled data, annotations are unavailable in our unsupervised setting. To overcome this issue and generate an initial set of pseudo labels for the first stage of self-training, we parameterize the function f as the combination of a deep neural network g for feature extraction with a non-learnable but differentiable feature-based optimization algorithm h for displacement prediction, i.e.The approach is based on our empirical observation that a suitable optimization algorithm h can predict reasonable initial displacement fields φ(0) from random features provided by a network g (0) with random initialization θ (0) g , which is inDeep Network (initialised with )  t) with pseudo labels generated based on the features from the network g (t-1) from the previous stage. For optimal feature learning, the pseudo displacements from the optimizer are further refined and regularized.line with recent studies on the inductive bias of CNNs [4]. We leverage these predicted displacements as pseudo labels to supervise the first stage of self-training, where the parameters of the feature extractor with different initialization θ (1)   g are optimized by minimizing the losswith TRE( φ(1) i , φ(0) i ) denoting the mean over the element-wise target registration error between the displacement fields φ(1) i and φ(0) i . A critical problem of this basic setup is that the network might overfit the initial pseudo labels and learn to reproduce random features. Therefore, in the spirit of recent techniques from contrastive learning [6], we propose to improve the efficacy of feature learning by incorporating asymmetries into the learning and pseudo label streams at two levels. First, we apply different random augmentations to the input pairs in both streams. Second, we augment the pseudo label stream with additional (non-differentiable) fine-tuning and regularization steps after the optimizer to improve the pseudo displacement fields (see Sec. 2.3 for details). As demonstrated in our ablation experiments (Fig. 2, Table 1), both strategies improve feature learning and strengthen the self-improvement effect.Once the first stage of self-training has converged, we repeat the process T times. Specifically, at stage t, we generate refined pseudo labels with the trained network g (t-1) from the previous stage, initialize the learning network g (t) with the weights from g (t-1) and perform a warm restart on the learning rate to escape potential local minima from the previous stage."
Unsupervised 3D Registration Through Optimization-Guided Cyclical Self-training,2.3,Registration Framework,"Our proposed self-training scheme is a flexible, modular framework, agnostic to the input modality and the specific implementation of feature extractor g and optimizer h. This section describes our specific design choices for g and h for image and point cloud registration, with the former being our main focus.Image Registration. To extract features from 3D input volumes, we implement g a standard 3D CNN with six convolution layers with kernel sizes 3 × 3 × 3 and 32, 64, or 128 channels. Each convolution is followed by BatchNorm and ReLU, and every second convolution contains a stride of 2, yielding a downsampling factor of 8. The outputs for both images are mapped to 16-dimensional features using a 1 × 1 × 1 convolution and fed into a correlation layer [21] that captures 125 discrete displacements.As the optimizer, we adapt the coupled convex optimization for learningbased 3D registration from [20], which, given fixed and moving features, infers a displacement field that minimizes a combined objective of smoothness and feature dissimilarity. Our proposed refinement strategy in the pseudo label stream comprises three ingredients. 1) Forward-backward consistency additionally computes the reverse displacement field (F to M ) and then iteratively minimizes the discrepancy between both fields. 2) For a second warp, the moving image is warped with the inferred displacement field before repeating all previous steps. 3) Iterative instance optimization finetunes the final displacement field with Adam by jointly minimizing regularization cost and feature dissimilarity. For the latter, we use the CNN features after the second convolution block and map them with a 1×1×1 convolution to 16 channels. We apply the same refinement steps at test time. Moreover, we propose to leverage the difference between network-predicted and finetuned displacements to estimate the difficulty of the training samples. Consequently, we apply a weighted batch sampling at training that increases the probability of using less difficult registration pairs with a higher agreement between both fields. We rank all training pairs and use a sigmoid function with arguments ranging linearly from -5 to 5 for the weighted random sampler.Point Cloud Registration. For point cloud registration, we implement the feature extractor as a graph CNN and rely on sparse loopy belief propagation for differentiable optimization, as introduced in [9]."
Unsupervised 3D Registration Through Optimization-Guided Cyclical Self-training,3,Experiments,
Unsupervised 3D Registration Through Optimization-Guided Cyclical Self-training,3.1,Experimental Setup,"Datasets. We conduct our main experiments for inter-patient abdomen CT registration using the corresponding dataset of the Learn2Reg (L2R) Challenge1  [15]. The dataset contains 30 abdominal 3D CT scans of different patients with 13 manually labeled anatomical structures of strongly varying sizes. The original image data and labels are from [26]. As part of L2R, they were affinely preregistered into a canonical space and resampled to identical voxel resolutions (2 mm) and spatial dimensions (192 × 160 × 256 vx). Following the data split of L2R, we use 20 scans (190 pairs) for training and the remaining 10 scans (45 pairs) for evaluation. Hence, data split and preprocessing are consistent with compared previous works [9,27]. As metrics, we report the mean Dice overlap (DSC) between the semantic labels and the standard deviation of the logarithmic Jacobian determinant (SDlogJ).We perform a second experiment for inhale-to-exhale lung CT registration on the DIR-Lab COPDGene dataset2  [5], which comprises 10 such scan pairs. For each pair, 300 expert-annotated landmark correspondences are available for evaluation. We pre-process all scans in multiple steps: 1) resampling to 1.75×1.00×1.25 mm for exhale and 1.75×1.25×1.75 mm for inhale, 2) cropping with fixed-size bounding boxes (192 × 192 × 208 vx), centered around automatically generated lung masks, 3) affine pre-registration, aligning the lung masks. Since we focus on keypoint-based registration of the lung CTs, we follow [9] and extract distinctive keypoints from the CTs using the Förstner algorithm with non-maximum suppression, yielding around 1k points in the fixed and 2k points in the moving cloud. In our experiments, we perform 5-fold cross-validation, with each fold comprising eight data pairs for training and two for testing. We report the target registration error (TRE) at the landmarks and the SDlogJ as metrics.Implementation Details. We implement all methods in Pytorch and optimize network parameters with the Adam optimizer. For abdomen registration, we train for T = 8 stages, each stage comprising 1000 iterations with a batch size of 2. The learning rate follows a cosine annealing warm restart schedule, decaying from 10 -3 to 10 -5 at each stage. Hyper-parameters were set based on the DSC on three cases from the training set. For lung registration, the model converged after T = 5 stages of 60 epochs with batch size 4, with an initial learning rate of 0.001, decreased by a factor of 10 at epochs 40 and 52. Here, hyper-parameters were adopted from [9]. For both datasets, training requires 90-100 min and 8 GB on an RTX2080, and input augmentations consist of random affine transformations."
Unsupervised 3D Registration Through Optimization-Guided Cyclical Self-training,3.2,Results,"Abdomen. First, we analyze our method in several ablation experiments. In Fig. 2, we visualize the performance of our method on a subset of classes over several cycles of self-training. We observe consistent improvements over the stages, particularly pronounced at early stages while the performance converges later on. This highlights the self-reinforcing effect achieved through alternating pseudo label updates and network training. In the upper part of Table 1, we verify  the efficacy of incorporating asymmetries (input augmentations, finetuning of pseudo labels) into both streams and weighted sampling. The results confirm the importance of each component to reach optimal performance. In the lower part of Table 1, we evaluate our final model under different test configurations, highlighting the improvements through a second warp and Adam finetuning. Next, we compare our method to a comprehensive set of state-of-the-art unsupervised methods, including classical algorithms [1,10,14] and deep learningbased approaches, trained with MIND [2,12]/NCC [17] supervision or contrastive learning [27]. The results are collected from [9,27]. Moreover, we train our own registration framework with metric-based supervision (MIND [13], NCC [19]) to directly verify the advantage of our self-training strategy. Results are shown in Table 2, Fig. 3, and Supp., Fig. 1. Our method substantially outperforms all comparison methods in terms of DSC (statistical significance is confirmed by a Fig. 3. Qualitative results of selected methods on two cases of the Abdomen CT dataset (axial view). We show overlays of the warped segmentation labels with the fixed scan: liver , stomach , left kidney , right kidney , spleen , gall bladder , esophagus , pancreas , aorta , inferior vena cava , portal vein , left /right adrenal gland.  Lung. For point cloud-based lung registration, we compare our cyclical selftraining strategy to three alternative learning strategies: supervision with manually annotated landmark correspondences as in [9], metric-based supervision with Chamfer distance and local Laplacian penalties as in [23], and training on synthetic rigid/random field deformations. All strategies are implemented for the same baseline registration model from [9]. Moreover, we report the performance of three unsupervised image-based deep learning methods [2,12,17] trained with MIND supervision. Results are shown in Table 3, demonstrating the superiority of our self-training strategy over all competing learning strategies and the reported image-based SOTA methods. Qualitative results of the experiment are shown in Supp., Fig. 2, demonstrating accurate and smooth displacements, as also confirmed by low values of SDlogJ."
Unsupervised 3D Registration Through Optimization-Guided Cyclical Self-training,4,Conclusion,"We introduced a novel cyclical self-training paradigm for unsupervised registration. To this end, we developed a modular registration pipeline of a deep feature extraction network coupled with a differentiable optimizer, stabilizing learning from noisy pseudo labels through regularization and iterative, cyclical refinement. That way, our method avoids pitfalls of popular metric supervision (NCC, MIND), which relies on shallow features or image intensities and is prone to noise and local minima. By contrast, our supervision through optimization-refined and -regularized pseudo labels promotes learning task-specific features that are more robust to noise, and our cyclical learning strategy gradually improves the expressiveness of features to avoid local minima. In our experiments, we demonstrated the efficacy and flexibility of our approach, which outperformed the competing state-of-the-art methods and learning strategies for dense image-based abdomen and point cloud-based lung registration. In summary, we did not only present the first fully unsupervised self-training scheme but also a new perspective on unsupervised learning-based registration. In particular, we consider our strategy complementary to existing techniques (metric-based and contrastive learning), opening up the potential for combined training schemes in future work."
Unsupervised 3D Registration Through Optimization-Guided Cyclical Self-training,,Fig. 1 .,
Unsupervised 3D Registration Through Optimization-Guided Cyclical Self-training,,Fig. 2 .,
Unsupervised 3D Registration Through Optimization-Guided Cyclical Self-training,,Table 1 .,
Unsupervised 3D Registration Through Optimization-Guided Cyclical Self-training,,Table 2 .,
Unsupervised 3D Registration Through Optimization-Guided Cyclical Self-training,,Table 3 .,
Unsupervised 3D Registration Through Optimization-Guided Cyclical Self-training,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5_64.
DULDA: Dual-Domain Unsupervised Learned Descent Algorithm for PET Image Reconstruction,1,Introduction,"Positron Emission Tomography (PET) is a widely used modality in functional imaging for oncology, cardiology, neurology, and medical research [1]. However, PET images often suffer from a high level of noise due to several physical degradation factors as well as the ill-conditioning of the PET reconstruction problem.As a result, the quality of PET images can be compromised, leading to difficulties in accurate diagnosis. Deep learning (DL) techniques, especially supervised learning, have recently garnered considerable attention and show great promise in PET image reconstruction compared with traditional analytical methods and iterative methods. Among them, four primary approaches have emerged: DL-based postdenoising [2,3], end-to-end direct learning [4][5][6], deep learning regularized iterative reconstruction [7][8][9][10] and deep unrolled methods [11][12][13].DL-based post denoising methods are relatively straightforward to implement but can not reduce the lengthy reconstruction time and its results are significantly affected by the pre-reconstruction algorithm. End-to-end direct learning methods utilize deep neural networks to learn the directing mapping from measurement sinogram to PET image. Without any physical constraints, these methods can be unstable and extremely data-hungry. Deep learning regularized iterative reconstruction methods utilize a deep neural network as a regularization term within the iterative reconstruction process to regularize the image estimate and guide the reconstruction process towards a more accurate and stable solution. Despite the incorporation of deep learning, the underlying mathematical framework and assumptions of deep learning regularized iterative methods still rely on the conventional iterative reconstruction methods. Deep unrolled methods utilize a DNN to unroll the iterative reconstruction process and to learn the mapping from sinogram to the reconstructed PET images, which potentially result in more accurate and explainable image reconstruction. Deep unrolled methods have demonstrated improved interpretabillity and yielded inspiring outcomes.However, the aforementioned approaches for PET image reconstruction depend on high quality ground truths as training labels, which can be diffi-cult and expensive to obtain. This challenge is further compounded by the high dose exposure associated with PET imaging. Unsupervised/self supervised learning has gained considerable interest in medical imaging, owing to its ability to mitigate the need for high-quality training labels. Gong et al. proposed a PET image reconstruction approach using the deep image prior (DIP) framework [15], which employed a randomly initialized Unet as a prior. In another study, Fumio et al. proposed a simplified DIP reconstruction framework with a forward projection model, which reduced the network parameters [16]. Shen et al. proposed a DeepRED framework with an approximate Bayesian framework for unsupervised PET image reconstruction [17]. These methods all utilize generative models to generate PET images from random noise or MRI prior images and use sinogram to design loss functions. However, these generative models tend to favor low frequencies and sometimes lack of mathematical interpretability. In the absence of anatomic priors, the network convergence can take a considerable amount of time, resulting in prolonged reconstruction times. Recently, equivariant property [18] of medical imaging system is proposed to train the network without labels, which shows the potential for the designing of PET reconstruction algorithms.In this paper, we propose a dual-domain unsupervised learned descent algorithm for PET image reconstruction, which is the first attempt to combine unsupervised learning and deep unrolled method for PET image reconstruction. The main contributions of this work are summarized as follows: 1) a novel model based deep learning method for PET image reconstruction is proposed with a learnable l 2,1 norm for more general and robust feature sparsity extraction of PET images; 2) a dual domain unsupervised training strategy is proposed, which is plug-and-play and does not need paired training samples; 3) without any anatomic priors, the proposed method shows superior performance both quantitatively and visually."
DULDA: Dual-Domain Unsupervised Learned Descent Algorithm for PET Image Reconstruction,2,Methods and Materials,
DULDA: Dual-Domain Unsupervised Learned Descent Algorithm for PET Image Reconstruction,2.1,Problem Formulation,"As a typical inverse problem, PET image reconstruction can be modeled in a variational form and cast as an optimization task, as follows:where y is the measured sinogram data, y is the mean of the measured sinogram.x is the PET activity image to be reconstructed, L(y|x) is the Poisson loglikelihood of measured sinogram data. P (x; θ) is the penalty term with learnable parameter θ. A ∈ R I×J is the system response matrix, with A ij representing the probabilities of detecting an emission from voxel j at detector i.We expect that the parameter θ in penalty term P can be learned from the training data like many other deep unrolling methods. However, most of these methods directly replace the penalty term [14] or its gradient [11,13] with a network, which loses some mathematical rigor and interpretablities. "
DULDA: Dual-Domain Unsupervised Learned Descent Algorithm for PET Image Reconstruction,2.2,Parametric Form of Learnable Regularization,"We choose to parameterize P as the l 2,1 norm with a feature extraction operator g(x) to be learned in the training data. The smooth nonlinear mapping g is used to extract sparse features and the l 2,1 norm is used as a robust and effective sparse feature regularization. Specifically, we formulate P as follows [19]:where g i,θ (x) is i-th feature vector. We choose g as a multi-layered CNN with nonlinear activation function σ, and σ is a smoothed ReLU:In this case, the gradient ∇g can be computed directly. The Nesterov's smoothing technique is used in P for the derivative calculation of the l 2,1 norm through smooth approximation:where parameter ε controls how close the approximation P ε to the original P ."
DULDA: Dual-Domain Unsupervised Learned Descent Algorithm for PET Image Reconstruction,,Algorithm 1. Learned Descent Algorithm for PET image reconstruction,"Input: Image initialization x0, ρ, γ ∈ (0, 1), ε0, σ, τ > 0, maximum number of iteration I, total phase numbers K and measured Sinogram y 1:"
DULDA: Dual-Domain Unsupervised Learned Descent Algorithm for PET Image Reconstruction,2.3,Learned Descent Algorithm for PET,"With the parametric form of learnable regularization given above, we rewrite Eq. 1 as the objective function: min φ(x; y, θ) = -L(y|x) + P ε (x; θ) (8) We unrolled the learned descent algorithm in several phases as shown in Fig. 1.In each phase k -1, we apply the proximal gradient step in Eq. 8:where the proximal operator is defined as:In order to have a close form solution of the proximal operator, we perform a Taylor approximation of P ε k-1 :After discarding higher-order constant terms, we can simplify the Eq. 10 as:where α k-1 and β k-1 are two parameters greater than 0 andWe also calculate a close-form safeguard v k as:The line search strategy is used by shrinking α k-1 to ensure objective function decay. We choose the u k or v k with smaller objection function value φ ε k-1 to be the next x k . The smoothing parameter ε k-1 is shrinkage by γ ∈ (0, 1) if the ||∇φ ε k-1 (x k )|| < σγε k-1 is satisfied. The whole flow is shown in Algorithm 1. "
DULDA: Dual-Domain Unsupervised Learned Descent Algorithm for PET Image Reconstruction,2.4,Dual-Domain Unsupervised Training,"The whole reconstruction network is indicated by f θ with learned parameter θ.Inspired by Deep image prior [20] and equivariance [18] of PET imaging system, the proposed dual-domain unsupervised training loss function is formulated as:where λ is the parameter that controls the ratio of different domain loss function, which was set to 0.1 in the experiments. For image domain loss L image , the equivariance constraint is used. For example, if the test sample x t first undergoes an equivariant transformation, such as rotation, we obtain x tr . Subsequently, we perform a PET scan to obtain the sinogram data of x tr and x t . The image reconstructed by the f θ of these two sinogram should also keep this rotation properties. The L image is formulate as: where T r denotes the rotation operator, A is the forward projection which also can be seen as a measurement operator. For sinogram domain loss L measure , the data argumentation with random noise ξ is performed on y:"
DULDA: Dual-Domain Unsupervised Learned Descent Algorithm for PET Image Reconstruction,2.5,Implementation Details and Reference Methods,"We implemented DULDA using Pytorch 1.7 on a NVIDIA GeForce GTX Titan X. The Adam optimizer with a learning rate of 10 -4 was used and trained for 100 epochs with batch size of 8. The total unrolled phase was 4. The image x 0 was initialized with the values of one. The smoothing parameter ε 0 and δ were initialized to be 0.001 and 0.002. The step-size α 0 and β 0 were initialized to be 0.01 and 0.02. The system matrix was computed by using Michigan Image Reconstruction Toolbox (MIRT) with a strip-integral model [21]. The proposed DULDA was compared with MLEM [22], total variation regularized EM (EM-TV) [23] and deep image prior method (DIP) [16]. For both MLEM and EM-TV, 25 iterations were adopted. The penalty parameter for EM-TV was 2e -5 .For DIP, we used random noise as input and trained 14000 epochs with the same training settings as DULDA to get the best results before over-fitting.The proposed method can also be trained in a fully supervised manner (we call it SLDA). The loss is the mean square error between the output and the label image. To further demonstrate the effectiveness, we compared SLDA with DeepPET [5] and FBSEM [11], the training settings remained the same. "
DULDA: Dual-Domain Unsupervised Learned Descent Algorithm for PET Image Reconstruction,3.1,Experimental Evaluations,"Forty 128 × 128 × 40 3D Zubal brain phantoms [24] were used in the simulation study as ground truth, and one clinical patient brain images with different dose level were used for the robust analysis. Two tumors with different size were added in each Zubal brain phantom. The ground truth images were firstly forward-projected to generate the noise-free sinogram with count of 10 6 for each transverse slice and then Poisson noise were introduced. 20 percent of uniform random events were simulated. In total, 1600 (40 × 40) 2D sinograms were generated. Among them, 1320 (33 samples) were used in training, 200 (5 samples) for testing, and 80 (2 samples) for validation. A total of 5 realizations were simulated and each was trained/tested independently for bias and variance calculation [15]. We used peak signal to noise ratio (PSNR), structural similarity index (SSIM) and root mean square error (RMSE) for overall quantitative analysis. The contrast recovery coefficient (CRC) [25] was used for the comparison of reconstruction results in the tumor region of interest (ROI) area."
DULDA: Dual-Domain Unsupervised Learned Descent Algorithm for PET Image Reconstruction,3.2,Results,"Figure 2 shows three different slices of the reconstructed brain PET images using different methods. The DIP method and proposed DULDA have lower noise compared with MLEM and EM-TV visually. However, the DIP method shows unstable results cross different slices and fails in the recovery of the small cortex region. The proposed DULDA can recover more structural details and the white matter appears to be more sharpen. The quantitative and bias-variance results are shown in Table 1. We noticed that DIP method performs even worse than MLEM without anatomic priors. The DIP method demonstrates a certain ability to reduce noise by smoothing the image, but this leads to losses in important structural information, which explains the lower PSNR and SSIM. Both DIP method and DULDA have a better CRC and Bias performance compared with MLEM and EM-TV. In terms of supervised training, SLDA also performs best."
DULDA: Dual-Domain Unsupervised Learned Descent Algorithm for PET Image Reconstruction,4,Discussion,"To test the robustness of proposed DULDA, we forward-project one patient brain image data with different dose level and reconstructed it with the trained DULDA model. The results compared with MLEM are shown in Fig. 3. The patient is scanned with a GE Discovery MI 5-ring PET/CT system. The real image has very different cortex structure and some deflection compared with the training data. It can be observed that DULDA achieves excellent reconstruction results in both details and edges across different dose level and different slices.Table 2 shows the ablation study on phase numbers and loss function for DULDA. It can be observed that the dual domain loss helps improve the performance and when the phase number is 4, DULDA achieves the best performance."
DULDA: Dual-Domain Unsupervised Learned Descent Algorithm for PET Image Reconstruction,5,Conclusions,"In this work, we proposed a dual-domain unsupervised model-based deep learning method (DULDA) for PET image reconstruction by unrolling the learned descent algorithm. Both quantitative and visual results show the superior performance of DULDA when compared to MLEM, EM-TV and DIP based method. Future work will focus more on clinical aspects."
DULDA: Dual-Domain Unsupervised Learned Descent Algorithm for PET Image Reconstruction,,Fig. 1 .,
DULDA: Dual-Domain Unsupervised Learned Descent Algorithm for PET Image Reconstruction,,Fig. 2 .,
DULDA: Dual-Domain Unsupervised Learned Descent Algorithm for PET Image Reconstruction,,Fig. 3 .,
DULDA: Dual-Domain Unsupervised Learned Descent Algorithm for PET Image Reconstruction,,Table 1 .,
DULDA: Dual-Domain Unsupervised Learned Descent Algorithm for PET Image Reconstruction,,Table 2 .,
Inverse Consistency by Construction for Multistep Deep Registration,1,Introduction,"Image registration, or finding the correspondence between a pair of images, is a fundamental task in medical image computing. One desirable property for registration algorithms is inverse consistency -the property that the transform found registering image A onto image B, composed with the transform found by registering image B onto image A, yields the identity map. Inverse consistency is useful for several reasons. Practically, it is convenient to have a single transform and its inverse associating two images instead of two transforms of unknown relationship. For within-subject registration, inverse consistency is often a natural assumption as long as images are consistent with each other, e.g., did not undergo surgical removal of tissue. For time series analysis, inverse consistency prevents bias [19]. We propose a novel deep network structure that registers images in multiple steps in a way that is inverse-consistent by construction. Our approach is flexible and allows different transform types for different steps (Fig. 1). "
Inverse Consistency by Construction for Multistep Deep Registration,2,Related Work,"Inverse consistency in deep image registration approaches is commonly promoted via a penalty [7,15,22,27] on the inverse consistency error. Extensive work also exists on optimization-based exactly inverse consistent image registration. For example, by using a symmetric image similarity measure and an inverse consistency loss on the transformations [5] or by performing robust inverse consistent rigid registrations with respect to a middle space [19]. ANTs SyN [2] is an approach to inverse consistent deformable registration, but by default is part of a multi-step affine then SyN pipeline which is not as a whole inverse consistent.Mok et al. [14] introduce a deep-learning framework that is exactly inverse consistent. They take advantage of the fact that a stationary velocity field (SVF) transform representation allows for fast inversion of a transform by integrating the negated velocity field. Thus, by calling their network twice, the second time with the inputs reversed, they can construct a transform. This registration network is inverseconsistent by construction, but only supports one step. Our approach will provide a general inverse consistent multi-step framework.Iglesias et al. [10] introduce a two-step deep registration framework for brain registration that is inverse consistent by construction. First, they independently segment each image with a U-Net into 97 anatomical regions. The centroids of these regions and the corresponding regions of an atlas are then used to obtain an affine transformation to the atlas. This is inverse consistent. Second, each brain image is resampled to the atlas space followed by an SVF-based transformation, where the velocity field is obtained by two calls to their velocity field network:). This symmetrization retains inverse consistency and is conceptually similar to our approach. However, their approach, unlike ours, does not directly extend to N steps and is not trained end to end.There is extensive literature on deep multi-step approaches. The core idea is to conduct the registration in multiple steps with the warped image produced by the previous step being the input to the latter step. Thus, the original input image pairs can be registered progressively. AVSM [22] achieves this by reusing the same neural network at each step. Other works in the literature [7,13,23] setup different neural networks at each step. In addition, these steps are often conducted in a coarse-to-fine manner. Namely, the neural network at the current step registers the input images at a coarse resolution, interpolates the output deformation field to a finer resolution, composes the interpolated deformation field with the composed transformation from previous steps, warps the moving image at the finer resolution, and passes the warped image and target image to the neural network at next step. Greer et al. and Tian et al. [7,23] define an abstract TwoStep operator to represent the process described above. However, this TwoStep operation does not guarantee inverse consistency between the composed forward transformation and the composed backward transformation.To address this issue, we propose a novel operator for multi-step registration to obtain inverse consistent registration by construction.Definitions and Notation. We use subscripted capital letters, e.g., N θ , to represent neural networks that return arrays of numbers, and capital Greek letters Φ, Ψ, and Ξ to represent registration neural networks, i.e., neural networks that return transforms. A transform is a function R D → R D with D denoting the dimension of the images we are registering. N AB θ is shorthand for N θ called on the images I A and I B , likewiseFor a Lie group G and associated algebra g, exp is the (Lie-)exponential map from g → G [6,11]."
Inverse Consistency by Construction for Multistep Deep Registration,3,Lie-Group Based Inverse Consistent Registration,"To design a registration algorithm, one must pick a class of transforms that the algorithm will return. Many types of transforms that are useful for practical medical registration problems happen to also be Lie groups. We describe a procedure for designing a neural network that outputs a member of a specified Lie group in an inverse consistent manner and provide several examples.Recall that a Lie group G is always associated with a Lie algebra g. Create a neural network N θ (of arbitrary design) with two input images and an output that can be considered an element of g.A registration network Φ defined to act as follows on two imagesis inverse consistent, because g(I A , I B ) = -g(I B , I A ) by construction. We explore how this applies to several Lie groups.Rigid Registration. The Lie algebra of rigid rotations is skew-symmetric matrices. N θ outputs a skew-symmetric matrix R and a vector t, so thatwhere Φ (rigid) will output a rigid transformation in an inverse consistent manner.Here, the exponential map is just the matrix exponent.Affine Registration. Relaxing R to be an arbitrary matrix instead of a skewsymmetric matrix in the above construction produces a network that performs inverse consistent affine registration.Nonparametric Vector Field Registration. In the case of the group of diffeomorphisms, the corresponding Lie algebra1 is the space of vector fields. If N θ outputs a vector field, implemented as a grid of vectors which are linearly interpolated, then, by using scaling and squaring [1,3] to implement the Lie exponent, we havewhich is an inverse consistent nonparametric registration network. This is equivalent to the standard SVF technique for image registration, with a velocity field represented as a grid of vectors equal toMLP Registration. An ongoing research question is how to represent the output transform as a multi-layer perceptron (MLP) applied to coordinates. One approach is to reshape the vector of outputs of a ConvNet so that the vector represents the weight matrices defining an MLP (with D inputs and D outputs). This MLP is then a member of the Lie algebra of vector-valued functions, and the exponential map to the group of diffeomorphisms can be computed by solving the following differential equation to t = 1 using an integrator such as fourthorder Runge-Kutta. Again, by defining the velocity field to flip signs when the input image order is flipped, we obtain an inverse consistent transformation:"
Inverse Consistency by Construction for Multistep Deep Registration,4,Multi-step Registration,"The standard approach to composing two registration networks is to register the moving image to the fixed image, warp the moving image and then register the warped moving image to the fixed image again and compose the transforms. This is formalized in [7,23] as TwoStep, i.e.,Unfortunately, TwoStep [7,23] is not always inverse consistent even with inverse consistent arguments. First, although Ψ [ ĨA , I B ] is the inverse of Ψ [I B , ĨA ], it does not necessarily have any relationship with Ψ [ ĨB , I A ] which is the term that appears when swapping the inputs to TwoStep.The inverses are interleaved so that even if they were exact, they can not cancel.Our contribution is an operator, TwoStepConsistent, that is inverse consistent if its components are inverse consistent. We assume that our component networks Φ and Ψ are inverse consistent, and that Φ returns a transform that we can explicitly find the square root of, such that √ Φ AB • √ Φ AB = Φ AB . Note that for transforms defined by Φ AB = exp(g) , √ Φ AB = exp(g/2). Since each network is inverse consistent, we have access to the inverses of the transforms they return. We begin with the relationship that Φ will be trained to fulfill (6) and apply Ψ to register ÎA and ÎBWe isolate the transform in the left half of Eq. ( 8) as our new operator, i.e.,In fact, we can verify thatNotably, This Procedure Extends to N -Step Registration. With the operator of Eq. ( 9), a registration network composed from an arbitrary number of steps may be made inverse consistent. This is because TwoStepConsistent{•, •} is a valid second argument to TwoStepConsistent. For instance, a three-step network can be constructed as TwoStepConsistent{Φ, TwoStepConsistent{Ψ, Ξ}}."
Inverse Consistency by Construction for Multistep Deep Registration,5,Synthetic Experiments,"Inverse Consistent Rigid, Affine, Nonparametric, and MLP Registration. We train networks on MNIST 5 s using the methods in Sects. 3 and 4, demonstrating that the resulting networks are inverse-consistent. Our TwoStepConsistent (TSC) operator can be used on any combination of the networks defined in Sect. 3. For demonstrations, we join an MLP registration network to a vector field registration network, and join two affine networks to two vector field networks. Figure 2 shows successful inverse-consistent sample registrations.Affine Registration Convergence. In addition to being inverse consistent, our method accelerates convergence and stability of affine registration, compared to directly predicting the matrix of an affine transform. Here, we disentangle whether this happens for any approach that parameterizes an affine transform by taking the exponent of a matrix, or whether this acceleration is Fig. 2. We train single-step rigid, affine, vector field parameterized SVF, and neural deformation field (MLP) networks, as well as a two-step registration network (TSC) composed of a neural deformation field step followed by a vector field parameterized SVF step and a 4 step network (NSC) composed of two affine steps and two SVF steps. We observe excellent registration results indicated by the small differences (second row) after applying the estimated transformation Φ AB (third row). Composing with the inverse produces results very close to the identity map (last row) as desired.unique to our inverse consistent method. We also claim that multi-step registration is important for registration accuracy and convergence time and that an inverse consistent multi-step operator, TwoStepConsistent, is thus beneficial.To justify these claims, we investigate training for affine registration on the synthetic Hollow Triangles and Circles dataset from [23] while varying the method used to obtain a matrix from the registration network and the type of multi-step registration used. To obtain an affine matrix, we either directly use the neural network output N AB θ , use exp(N AB θ ), or, as suggested in Sect. 3, use exp(N AB θ -N BA θ ). We either register in one step, use the TwoStep operator from [7,23], or use our new TwoStepConsistent operator. This results in 9 training configurations, which we run 65 times each.We observe that parameterizing an affine registration using the exp(N AB θ -N BA θ ) construction speeds up the first few epochs of training and gets even faster when combined with any multi-step method. In Fig. 3, note that in the top-left corner of the first plot, the green loss curves (corresponding to models using N AB θ -N BA θ ) are roughly vertical, while the other loss curves are roughly horizontal, eventually bending down. After this initial lead, these green curves also converges to a better final loss. Further, all methods that use the N AB θ -N BA θ construction train reliably, while other methods sometimes fail to converge (Fig. 3, right plot). This has a dramatic effect on the practicality of a method since training on 3-D data can take multiple days on expensive hardware.Finally, as expected, the only two approaches that are inverse consistent are the single-step inverse consistent by construction network, and the network using two inverse-consistent by construction subnetworks, joined by the TwoStepConsistent operator. (Fig. 3, middle, dotted and solid green). "
Inverse Consistency by Construction for Multistep Deep Registration,6,Evaluation on 3-D Medical Datasets,"We evaluate on several datasets, where we can compare to earlier registration approaches. We use the network Φ := TSC{Ψ 1 , TSC{Ψ 2 , TSC{Ξ 1 , Ξ 2 }}} with Ξ i inverse-consistent SVF networks backed by U-Nets and Ψ i inverseconsistent affine networks backed by ConvNets2 . We rely on local normalized cross-correlation as our similarity measure, with σ = 5vx, and regularize the SVF networks by the sum of the bending energies of their velocity fields, with λ = 5. We train end to end, minimizing -LNCC(I A • Φ[I A , I B ], I B ) + λL reg for 100,000 iterations (∼2 days on 4 NVIDIA A6000s) with Adam optimization and a learning rate of 1e-4. In all cases, we normalize images to the range (0, 1). We evaluate registration accuracy with and without instance optimization [23,26]. Without instance optimization, registration takes ∼0.23 s on an NVIDIA RTX A6000 on the HCP [24] dataset. With instance optimization, registration takes ∼43 s."
Inverse Consistency by Construction for Multistep Deep Registration,6.1,Datasets,"COPDGene/Dirlab Lung CT. We follow the data selection and preprocessing of [23]. We train on 999 inhale/exhale pairs from COPDGene [18], resampled to 2mm spacing at [175 × 175 × 175], masked with lung segmentations, clipped to [-1000, 0] Hounsfield units, and scaled to (0, 1). We evaluate landmark error (MTRE) on the ten inhale/exhale pairs of the Dirlab challenge dataset [4] 3 . OAI Knee MRI. We train and test on the split published with [22], with 2532 training examples and 301 test pairs from the Osteoarthritis Initiative (OAI) [16] 4 . We evaluate using the mean Dice score of femoral and tibial cartilage. To compare directly to [7,22,23] we train and evaluate at [80 × 192 × 192]. HCP Brain MRI. We train on 1076 brain-extracted T1w images from the HCP dataset [24] and test on a sample of 100 pairs between 36 images via mean Dice over 28 midbrain structures [20,21]. We train and execute the network at [130 × 155 × 130], then compute the Dice score at full resolution."
Inverse Consistency by Construction for Multistep Deep Registration,,OASIS Brain MRI.,"We use the OASIS-1 [12] data preprocessed by [9]. This dataset contains images of 414 subjects. Following the data split in [14], we train on 255 images and test on 153 images 5 . The images in the dataset are of size [160 × 192 × 224], and we crop the center of the image according to the preprocessing in [14], leading to a size of [160 × 144 × 192]. During training, we sample image pairs randomly from the train set. For evaluation, we randomly pick 5 cases as the fixed images and register all the remaining 148 cases to the 5 cases, resulting in 740 image pairs overall."
Inverse Consistency by Construction for Multistep Deep Registration,6.2,Comparisons,"We use publicly-available pretrained weights and code for ANTs [2], PTVReg [25], GradICON [23], SynthMorph [8], SymNet [14], and EasyReg [10]. SymNet, GradICON, and PTVReg are run on the datasets associated with their original publication. SynthMorph, which we evaluate on HCP, was originally trained and evaluated on HCP-A and OASIS. EasyReg was trained on HCP [24] and ADNI [17]. Our ConstrICON method outperforms the state of the art on HCP, OAI, and OASIS registration but underperforms on the DirLab data. Since we use shared hyperparameters between these datasets, which are not tuned to a specific task, we assert that this performance level will likely generalize to new datasets. We find that our method is more inverse consistent than existing inverse consistent by construction methods SymNet and SyNOnly with higher accuracy, and more inverse consistent than inverse-consistent-by-penalty GradI-CON (Table 1). "
Inverse Consistency by Construction for Multistep Deep Registration,7,Conclusion,"The fundamental impact of this work is as a recipe for constructing a broad class of exactly inverse consistent, multi-step registration algorithms. We also are pleased to present registration results on four medically relevant datasets that are competitive with the current state of the art, and in particular are more accurate than existing inverse-consistent-by-construction neural approaches."
Inverse Consistency by Construction for Multistep Deep Registration,,Fig. 1 .,
Inverse Consistency by Construction for Multistep Deep Registration,,Fig. 3 .,
Inverse Consistency by Construction for Multistep Deep Registration,,Table 1 .,
ModeT: Learning Deformable Image Registration via Motion Decomposition Transformer,1,Introduction,"Deformable image registration has always been an important focus in the society of medical imaging, which is essential for the preoperative planning, intraoperative information fusion, disease diagnosis and follow-ups [10,23]. The deformable registration is to solve the non-rigid deformation field to warp the moving image, so that the warped image can be anatomically similar to the fixed image. Let I f , I m ∈ R H×W ×L be the fixed and moving images (H, W, L denote image size), in the deep-learning-based registration paradigm, it is often necessary to employ a spatial transformer network (STN) [13] to apply the estimated sampling grid G ∈ R H×W ×L×3 to the moving image, where G is obtained by adding the regular grid and the deformation field. For any position p ∈ R 3 in the sampling grid, G(p) represents the corresponding relation, which means that the voxel at position p in the fixed image corresponds to the voxel at position G(p) in the moving image. That is to say, image registration can be understood as finding the corresponding voxels between the moving and fixed images, and converting this into the relative positional relationship between voxels, which is very similar to the calculation method of Transformer [8].Transformers have been successfully used in the society of computer vision and have recently made an impact in the field of medical image computing [11,17]. In medical image registration, there are also several related studies that employ Transformers to enhance network structures to obtain better registration performance, such as Transmorph [5], Swin-VoxelMorph [26], Vit-V-Net [6], etc. The use of Transformer in these networks, however, often merely leverages the self-attention mechanism in Transformers to boost the feature learning (the same as the segmentation tasks do), but does not sufficiently design for the registration tasks. Some other methods use cross-attention to model the corresponding relationship between moving and fixed images, such as Attention-Reg [22] and Xmorpher [21]. The cross-attention Transformer (CAT) module is used in the bottom layer of Attention-Reg [22] and each layer in Xmorpher [21] to establish the relationship between the features of moving and fixed images. However, the usage of Transformer in [21,22] is still limited to improving the feature learning, with no additional consideration given to the relationship between the attention mechanism and the deformation estimation. Furthermore, due to the large network structure of [21], only small windows can be created for similarity calculation, which may result in performance degradation. Few studies consider the relationship between attention and deformation estimation, such as Coordinate Translator [18] and Deformer [4]. Deformer [4] uses the calculation mode of multiplication of attention map and Value matrix in Transformer to weight the predicted basis to generate the deformation field, but its attention map calculation is only the concatenation and projection of moving and fixed feature maps, without using similarity calculation part. Coordinate Translator [18] calculates the matching score of the fixed feature map and the moving feature map. Then the computed scores are employed to re-weight the deformation field. However, for feature maps with coarse-level resolution, a voxel often has multiple possibilities of different motion modes [25], which is not considered in [18]. Traditional methods have explored multiple modes of deformations, e.g., probabilistic registration [12], to improve the performance.In this study, we propose a novel motion decomposition Transformer (ModeT) to explicitly model multiple motion modalities by fully exploiting the intrinsic capability of the Transformer structure for deformation estimation. Experiments on two public brain magnetic resonance imaging (MRI) datasets demonstrate our method outcompetes several cutting-edge registration networks and Transformers. The main contributions of our work are summarized as follows:• We propose to leverage the Transformer structure to naturally model the correspondence between images and convert it into the deformation field, "
ModeT: Learning Deformable Image Registration via Motion Decomposition Transformer,2,Method,
ModeT: Learning Deformable Image Registration via Motion Decomposition Transformer,2.1,Network Overview,"The proposed deformable registration network is illustrated in Fig. 1. We employ a pyramidal registration structure, which has the advantage of reducing the scope of attention calculation required at each decoding level and therefore alleviating the computational consumption. Given the fixed image I f and moving image I m as input, the encoder extracts hierarchical features using a 5-layer convolutional block, which doubles the number of channels in each layer. This generates two sets of feature mapsThe feature maps M 5 and F 5 are sent into the ModeT to generate multiple deformation subfields, and then the generated deformation sub-fields are input into the CWM to obtain the fused deformation field ϕ 1 of the coarsest decoding layer as the initial of the total deformation field φ. The moving feature map M 4 is deformed using φ, and the deformed moving feature map is fed into the ModeT along with F 4 to generate multiple sub-fields, which are input into the CWM to get ϕ 2 . Then ϕ 2 is compounded with previous total deformation field to generate the updated φ. The feature maps M 3 and F 3 go through the similar operations. As the decoding feature maps become finer, the number of motion modes at position p decreases, along with the number of attention heads we need to model. At the F 2 /M 2 and F 1 /M 1 levels, we no longer generate multiple deformation sub-fields, i.e., the number of attention heads in ModeT is 1. Finally, the obtained total deformation field φ is used to warp I m to obtain the registered image.To guide the network training, the normalized cross correlation L ncc [19] and the deformation regularization L reg [3] is used:where • is the warping operation, and λ is the weight of the regularization term."
ModeT: Learning Deformable Image Registration via Motion Decomposition Transformer,2.2,Motion Decomposition Transformer (ModeT),"In deep-learning-based registration networks, a position p in the low-resolution feature map contains semantic information of a large area in the original image and therefore may often have multiple possibilities of different motion modalities.To model these possibilities, we employ a multi-head neighborhood attention mechanism to decompose different motion modalities at low-resolution level.The illustration of the motion decomposition is shown in Fig. 2.Let F, M ∈ R c×h×w×l stand for the fixed and moving feature maps from a specific level of the hierarchical encoder, where h, w, l denote feature map size and c is the channel number. The feature maps F and M go through linear projection (proj) and LayerNorm (LN ) [2] to get Q (query) and K (key):Q ={Q (1) , Q (2) , . . . , Q (S) }, K ={K (1) , K (2) , . . . , K (S) }, (2) where the projection operation is shared weight, and the weight initialization is sampled from N (0, 1e -5 ), the bias is initialized to 0. The Q and K are then divided according to channels, and S represents the number of divided heads.We then calculate the neighborhood attention map. We use c(p) to denote the neighborhood of voxel p. For a neighborhood of size n × n × n, ||c(p)|| = n 3 . The neighborhood attention map of multiple heads is obtained by: where B ∈ R S×n×n×n is a learnable relative positional bias, initialized to all zeros. We pad the moving feature map with zeros to calculate boundary voxels because the registration task sometimes requires voxels outside the field-of-view to be warped. Equation (3) shows how the neighborhood attention is computed for the s-th head at position p, so that the semantic information of voxels on low resolution can be decomposed to compute similarity one by one, in preparation for modeling different motion modalities. Moreover, the neighborhood attention operation narrows the scope of attention calculation to reduce the computational effort, which is very friendly to volumetric processing. The next step is to obtain the multiple sub-fields at this level by computing the regular displacement field weighted via the neighborhood attention map:where ϕ (s) ∈ R h×w×l×3 , V ∈ R n×n×n , and V (value) represents the relative position coordinates for the neighborhood centroid, which is not learned so that the multi-head attention relationship can be naturally transformed into a multicoordinate relationship. With the above steps, we obtain a series of deformation sub-fields for this level: ϕ (1) , ϕ (2) , . . . , ϕ (S) (5)"
ModeT: Learning Deformable Image Registration via Motion Decomposition Transformer,2.3,Competitive Weighting Module (CWM),"Multiple low-resolution deformation fields need to be reasonably fused when deforming a high-resolution feature map. As shown in Fig. 3, we first upsample these deformation sub-fields, then convolve them in three layers to get the score of each sub-field, and use softmax to compete the motion modality for each voxel. The convolution uses 3 × 3 × 3 convolution rather than direct projection because deformation fields often require correlation of adjacent displacements to determine if they are reasonable. We formulate above competitive weighting operation to obtain the deformation field ϕ at this level as follows:w (1) , w (2) , . . . , w (S) = W Conv(cat(ϕ (1) , ϕ (2) , . . . , ϕ (S) )),where w (s) ∈ R h×w×l , and ϕ (s) has already been upsampled. W Conv represents the ConvBlock used to calculate weights, as shown in the right part of Fig. 3."
ModeT: Learning Deformable Image Registration via Motion Decomposition Transformer,3,Experiments,"Datasets. Experiments were carried on two public brain MRI datasets, including LPBA [20] and Mindboggle [16]. For LPBA, each MRI volume contains 54 manually labeled region-of-interests (ROIs). All volumes in LPBA were rigidly pre-aligned to mni305. 30 volumes (30×29 pairs) were employed for training and 10 volumes (10×9 pairs) were used for testing. For Mindboggle, each volume contains 62 manually labeled ROIs. All volumes in Mindboggle were affinely aligned to mni152. 42 volumes (42 × 41 pairs from the NKI-RS-22 and NKI-TRT-20 subsets) were employed for training, and 20 volumes from OASIS-TRT-20 (20 × 19 pairs) were used for testing. All volumes were pre-processed by min-max normalization, and skull-stripping using FreeSurfer [9]. The final size of each volume was 160 × 192 × 160 after a center-cropping operation.Evaluation Metrics. To quantitatively evaluate the registration performance, Dice score (DSC) [7] was calculated as the primary similarity metric to evaluate the degree of overlap between corresponding regions. In addition, the average symmetric surface distance (ASSD) [24] was evaluated, which can reflect the similarity of the region contours. The quality of the predicted deformation φ was assessed by the percentage of voxels with non-positive Jacobian determinant (i.e., folded voxels). All above metrics were calculated in 3D. A better registration shall have larger DSC, and smaller ASSD and Jacobian. 56.7 ± 1.5 1.38 ± 0.09 < 0.00001% 70.1 ± 6.2 1.72 ± 0.12 < 0.0004% VM [3] 56.0 ± 1.6 1.49 ± 0.11 < 1% 64.3 ± 3.2 2.03 ± 0.21 < 0.7% TM [5] 60.7 ± 1.5 1.35 ± 0.10 < 0.9% 67.0 ± 3.0 1.90 ± 0.20 < 0.6% I2G [18] 59.8 ± 1.3 1.30 ± 0.07 < 0.03% 71.0 ± 1.4 1.64 ± 0.10 < 0.01% PR++ [14] 61.1 ± 1.4 1.34 ± 0.10 < 0.5% 69.5 ± 2.2 1.76 ± 0.17 < 0.2% XM [21] 53.6 ± 1.5  Implementation Details. Our method was implemented with PyTorch, using a GPU of NVIDIA Tesla V100 with 32GB memory. The regularization term λ and neighborhood size n were set as 1 and 3. For the encoder part, we used the same convolution structure as [18]. In the pyramid decoder, from coarse to fine, the number of attention heads were set as 8, 4, 2, 1, 1, respectively. We used 6 channels for each attention head. The Adam optimizer [15] with a learning rate decay strategy was employed as follows:where lr m represents the learning rate of m-th epoch and lr init = 1e -4 represents the learning rate of initial epoch. We set the batch size as 1, M as 30 for training. In the inference phase, our method averagely took 0.56 second and 9GB memory to register a volume pair of size 160 × 192 × 160.Comparison Methods. We compared our method with several state-of-theart registration methods: (1) SyN [1]: a classical traditional approach, using the SyN Only setting in ANTS. ( 2) VoxelMorph(VM) [3]: a popular single-stage registration network. ( 3) TransMorph(TM) [5]: a single-stage registration network with SwinTransformer enhanced encoder. ( 4) PR++ [14]: a pyramid registration network using 3D correlation layer. ( 5) XMorpher(XM) [21]: a registration network using CAT modules for each level of encoder and decoder. ( 6) Im2grid(I2G) [18]: a pyramid network using a coordinate translator. ( 7) DMR [4]: a registration network using a Deformer and a multi-resolution refinement module.Quantitative and Qualitative Analysis. The numerical results of different methods on datasets Mindboggle and LPBA are reported in Table 1. It can be observed that our method consistently attained the best registration accuracy with respect to DSC and ASSD metrics. For the DSC results, our method surpassed the second-best networks by 1.7% and 1.1% on Mindboggle and LPBA, respectively. We further investigated the statistical significance of our method over comparison methods on DSC and ASSD metrics, by conducting the paired and two-sided Wilcoxon signed-rank test. The null hypotheses for all pairs (our method v.s. other method) were not accepted at the 0.05 level. As a result, our method can be regarded as significantly better than all comparison methods on DSC and ASSD metrics. Table 1 also lists the percentage of voxels with nonpositive Jacobian determinant (%|J φ | ≤ 0). Our method achieved satisfactory performance, which was the best among all deep-learning-based networks. Figure 4 visualizes the registered images from different methods on two datasets. Our method generated more accurate registered images, and internal structures can be consistently preserved using our method. Figure 5 takes the registration of one image pair as an example to show the multi-level deformation fields generated by our method. Our ModeT effectively modeled multiple motion modalities and our CWM fused them together at low-resolution levels. The final deformation field φ accurately warped the moving image to registered with the fixed image."
ModeT: Learning Deformable Image Registration via Motion Decomposition Transformer,4,Conclusion,"We present a motion decomposition Transformer (ModeT) to naturally model the correspondence between images and convert this into the deformation field, which improves the interpretability of the deep-learning-based registration network. The proposed ModeT employs the multi-head neighborhood attention mechanism to identify various motion patterns of a voxel in the low-resolution feature map. Then with the help of competitive weighting module and pyramid structure, the motion modes contained in a voxel can be gradually fused and determined in the coarse-to-fine pyramid decoder. The experimental results have proven the superior performance of the proposed method. In our future study, we attempt to implement our ModeT in a more efficient way, and also investigate more effective fusion strategy to combine the displacement field from multiple attention heads."
ModeT: Learning Deformable Image Registration via Motion Decomposition Transformer,,Fig. 1 .,
ModeT: Learning Deformable Image Registration via Motion Decomposition Transformer,,Fig. 2 .,
ModeT: Learning Deformable Image Registration via Motion Decomposition Transformer,,Fig. 3 .,
ModeT: Learning Deformable Image Registration via Motion Decomposition Transformer,,Fig. 4 .,
ModeT: Learning Deformable Image Registration via Motion Decomposition Transformer,,Fig. 5 .,
ModeT: Learning Deformable Image Registration via Motion Decomposition Transformer,,Table 1 .,
ModeT: Learning Deformable Image Registration via Motion Decomposition Transformer,,,
ModeT: Learning Deformable Image Registration via Motion Decomposition Transformer,,Acknowledgements,". This work was supported in part by the National Natural Science Foundation of China under Grants 62071305, 61701312, 81971631 and 62171290, in part by the Guangdong Basic and Applied Basic Research Foundation under Grant 2022A1515011241, and in part by the Shenzhen Science and Technology Program (No. SGDX 20201103095613036)."
Feature-Conditioned Cascaded Video Diffusion Models for Precise Echocardiogram Synthesis,1,Introduction,"Ultrasound (US) is widely used in clinical practice because of its availability, realtime imaging capabilities, lack of side effects for the patient and flexibility. US is a dynamic modality that heavily relies on operator experience and on-the-fly interpretation, which requires many years of training and/or Machine Learning (ML) support that can handle image sequences. However, clinical reporting is conventionally done via single, selected images that rarely suffice for clinical audit or as training data for ML. Simulating US from anatomical information, e.g. Computed Tomography (CT) [28], Magnetic Resonance Imaging (MRI) [25] or computational phantoms [11,27], has been considered as a possible avenue to provide more US data for both operator and ML training. However, simulations are usually very computationally expensive due to complex scattering, reflection and refraction of sound waves at tissue boundaries during image generation. Therefore, the image quality of US simulations has not yet met the necessary quality to support tasks such as cross-modality registration, multi-modal learning, and robust decision support for image analysis during US examinations. More recently, generative deep learning methods have been proposed to address this issue. While early approaches show promising results, they either focus on generating individual images [16] or require video input data and further conditioning to provide useful results [17,21]. Research in the field of imageconditioned video generation is very scarce [33] and, to the best of our knowledge, we are the first to apply it to medical imaging."
Feature-Conditioned Cascaded Video Diffusion Models for Precise Echocardiogram Synthesis,,Contribution:,"In this paper, we propose a new method for video diffusion [7,30] based on the Elucidated Diffusion Model (EDM) [13] that allows to synthesise plausible video data from single frames together with precise conditioning on interpretable clinical parameters, e.g., Left Ventricular Ejection Fraction (LVEF) in echocardiography. This is the first time diffusion models have been extended for US image and video synthesis. Our contributions are three-fold: (1) We show that discarding the conventional text-embeddings [7,20,23,24,30] to control the reverse diffusion process is desirable for medical use cases where very specific elements must be precisely controlled; (2) We quantitatively improve upon existing methods [21] for counterfactual modelling, e.g., when doctors try to answer questions like ""how would the scan of this patient look like if we would change a given clinical parameter?""; (3) We show that fine-grained control of the conditioning leads to precise data generation with specific properties and outperforms the state-of-the-art when using such data, for example, for the estimation of LVEF in patients that are not commonly represented in training databases.Related Work: Video Generation has been a research area within computer vision for many years now. Prior works can be organized in three categories: (1) pixel-level autoregressive models [2,4,12], (2) latent-level autoregressive model coupled with generators or up-samplers [1,14] and (3) latent-variable transformer-based models with up-samplers [5,37]. Diffusion models have shown reasonable performance on low temporal and spatial resolutions [10] as well as on longer samples with high definition image quality [7,30] conditioned on text inputs. Recently, [38] combined an autoregressive pixel-level model with a diffusion-based pipeline that predicts a correction of the frame, while [3] presents an autoregressive latent diffusion model.Ultrasound simulation has been attempted with three major approaches: (1) physics-based simulators [11,28], (2) cross-modality registration-based methods [15] and (3) deep-learning based methods, usually conditioned on US, MRI or CT image priors [25,34,35] to condition the anatomy of the generated US images. Cine-ultrasound has also attracted some interest. [17] presents a motion-transferbased method for pelvic US video generation, while [21] proposes a causal model for generating echocardiograms conditioned on arbitrary LVEF.LVEF is a major metric in the assessment of cardiac function and diagnosis of cardiomyopathy. The EchoNet-dynamic dataset [19] is used as the go-to benchmark for LVEF-regression methods. Various works [18,22] have attempted to improve on [19] but the most reproducible method remains the use of an R2+1D model trained over fixed-length videos. The R2+1D_18 trained for this work achieves an R 2 score of 0.81 on samples of 64 frames spanning 2 s."
Feature-Conditioned Cascaded Video Diffusion Models for Precise Echocardiogram Synthesis,2,Method,"Diffusion probabilistic models [8,31,32] are the most recent family of generative models. In this work, we follow the definition of the EDM from [13]. Let q(x) represent the real distribution of our data, with a standard deviation of σ q . A family of distributions p(x; σ) can be obtained by adding i.i.d Gaussian noise with a standard deviation of σ to the data. When σ max σ q , the distribution p(x; σ max ) is essentially the same as pure Gaussian noise. The core idea of diffusion models is to sample a pure noise data point x 0 ∼ N (0, σ 2 max I) and then progressively remove the noise, generating images x i with standard deviation σ i such that σ max = σ 0 > σ 1 > ... > σ N = 0, and x i ∼ p(x; σ i ). The final image x N produced by this process is thus distributed according to q(x), the true distribution of the data. To perform the reverse diffusion process, we define a denoising function D(x, σ) trained to minimize the L 2 denoising error for all samples drawn from q for every σ such that:where y is a training data point and n is noise. By following the definition of ordinary differential equations (ODE) we can continuously increase or decrease the noise level of our data point by moving it forward or backward in the diffusion process, respectively. To define the ODE we need a schedule σ(t) that sets the noise level given the time step t, which we set to σ(t) = t. The probability flow ODE's characteristic property is that moving a sample) and this requirement is satisfied by setting dx = -σ(t)σ(t)∇ x log p(x; σ(t))dt where σ denotes the time derivative and ∇ x log p(x; σ) is the score function. From the score function, we can thus write ∇ x log p(x; σ) = (D(x; σ)-x)/σ 2 in the case of our denoising function, such that the score function isolates the noise from the signal x and can either amplify it or diminish it depending on the direction we take in the diffusion process. We define D(x; σ) to transform a neural network F , which can be trained inside D by following the loss described in Eq. ( 1). The EDM also defines a list of four important preconditionings which are defined as5 and c noise (σ) = log(σ t )/4 where σ q is the standard deviation of the real data distribution. In this paper, we focus on generating temporally coherent and realistic-looking echocardiograms. We start by generating a low resolution, low-frame rate video v 0 from noise and condition on arbitrary clinical parameters and an anatomy instead of the commonly used text-prompt embeddings [7,30]. Then, the video is used as conditioning for the following diffusion model, which generates a temporally and/or spatially upsampled video v 1 resembling v 0 , following the Cascaded Diffusion Model (CDM) [9] idea. Compared to image diffusion models, the major change to the Unet-based architecture is to add time-aware layers, through attention, at various levels as well as 3D convolutions (see Fig. 1 and Appendix Fig. 1). For the purpose of this research, we extend [7] to handle our own set of conditioning inputs, which are a single image I c and a scalar value λ c , while following the EDM setup, which we apply to video generation. We formally define the denoising models in the cascade as D θs where s defines the rank (stage) of the model in the cascade, and where D θ0 is the base model. The base model is defined as:where F θ0 is the neural network transformed by D θ0 and D θ0 outputs v 0 . For all subsequent models in the cascade, the conditioning remains similar, but the models also receive the output from the preceding model, such that:This holds ∀s > 0 and inputs I c , v s-1 are rescaled to the spatial and temporal resolutions expected by the neural network F θs as a pre-processing. We apply the robustness trick from [9], i.e, we add a small amount of noise to real videos v s-1 during training, when using them as conditioning, in order to mitigate domain gaps with the generated samples v s-1 during inference.Sampling from the EDM is done through a stochastic sampling method. We start by sampling a noise sample x 0 ∼ N (0, t 2 0 I), where t comes from our previously defined σ(t i ) = t i and sets the noise level. We follow [13] and set constants S noise = 1.003, S tmin = 0.05, S tmax = 50 and one constant S churn dependent on the model. These are used to compute γ i (t i ) = min(S churn /N, √ 2 -1) ∀t i ∈ [S tmin , S tmax ] and 0 otherwise, where N is the number of sampling steps. Then ∀i ∈ {0, ..., N -1}, we sample i ∼ N (0, S noise I) and compute a slightly increased noise level ti = (γ i (t i ) + 1)t i , which is added to the previous sample xi = x i + ( t2 i -t 2 i ) 0.5 i . We then execute the denoising model D θ on that sample and compute the local slope d i = (x i -D θ ( xi ; ti ))/ ti which is used to predict the next sample x i+1 = xi + ( ti+1 -ti )d i . At every step but the last (i.e: ∀i = N -1), we apply a correction to x i+1 such that:The correction step doubles the number of executions of the model, and thus the sampling time per step, compared to DDPM [8] or DDIM [32]. The whole sampling process is repeated sequentially for all models in the cascaded EDM. Models are conditioned on the previous output video v s-1 inputted at each step of the sampling process, with the frame conditioning I c as well as the scalar value λ c .Conditioning: Our diffusion models are conditioned on two components. First, an anatomy, which is represented by a randomly sampled frame I c . It defines the patient's anatomy, but also all the information regarding the visual style and quality of the target video. These parameters cannot be explicitly disentangled, and we therefore limit ourselves to this approach. Second, we condition the model on clinical parameters λ c . This is done by discarding the text-encoders that are used in [10,30] and directly inputting normalized clinical parameters into the conditional inputs of the Unets. By doing so, we give the model fine-grained control over the generated videos, which we evaluate using task-specific metrics.Parameters: As video diffusion models are still in their early stage, there is no consensus on which are the best methods to train them. In our case, we define, depending on our experiment, 1-, 2-or 4-stages CDMs. We also experiment with various schedulers and parametrizations of the model. [26,32] show relatively fast sampling techniques which work fine for image sampling. However, in the case of video, we reach larger sampling times as we sample 64 frames at once. We therefore settled for the EDM [13], which presents a method to sample from the model in much fewer steps, largely reducing sampling times. We do not observe any particular speed-up in training and would argue, from our experience, that the v-parametrization [32] converges faster. We experimentally find our models to behave well with parameters close to those suggested in [13]."
Feature-Conditioned Cascaded Video Diffusion Models for Precise Echocardiogram Synthesis,3,Experiments,"Data: We use the EchoNet-Dynamic [19] dataset, a publicly available dataset that consists of 10,030 4-chamber cardiac ultrasound sequences, with a spatial resolution of 112 × 112 pixels. Videos range from 0.7 to 20.0 s long, with frame rates between 18 and 138 frames per second (fps). Each video has 3 channels, although most of them are greyscale. We keep the original data split of EchoNet-Dynamic which has 7465 training, 1288 validation and 1277 testing videos. We only train on the training data, and validate on the validation data. In terms of labels, each video comes with an LVEF score λ ∈ [0, 100], estimated by a trained clinician. At every step of our training process, we pull a batch of videos, which are resampled to 32 fps. For each video, we retrieve its corresponding ground truth LVEF as well as a random frame. After that, the video is truncated or padded to 64 frames, in order to last 2 s, which is enough to cover any human heartbeat. The randomly sampled frame is sampled from the same original video as the 64-frames sample, but may not be contained in those 64 frames, as it may come from before or after that sub-sample.Architectural Variants: We define three sets of models, and present them in details in Table 1 of the Appendix. We call the models X -Stage Cascaded Models (X SCM) and present the models' parameters at every stage. Every CDM starts with a Base diffusion model that is conditioned on the LVEF and one conditional frame. The subsequent models perform either temporal super resolution (TSR), spatial super resolution (SSR) or temporal and spatial super resolution (TSSR). TSR, SSR and TSSR models receive the same conditioning inputs as the Base model, along with the output of the previous-stage model. Note that [7] does not mention TSSR models and [30] states that extending an SSR model to perform simultaneous temporal and spatial up-sampling is too challenging.Training: All our models are trained from scratch on individual cluster nodes, each with 8 × NVIDIA A100. We use a per-GPU batch size of 4 to 8, resulting in batches of 32 to 64 elements after gradient accumulation. The distributed training is handled by the accelerate library from HuggingFace. We did not see any speed-up or memory usage reduction when enabling mixed precision and thus used full precision. As pointed out by [9] all models in a CDM can be trained in parallel which significantly speeds up experimentation. We empirically find that training with a learning rate up to 5 * 10 -4 is stable and reaches good image quality. We use an Exponential Moving Average (EMA) copy of our model to smooth out the training. We train all our models' stages for 48h, i.e., the 2SCM and 4SCM CDMs are proportionally more costly to train than the 1SCM. As noted by [7,30] training on images and videos improves the overall image quality. As our dataset only consists of videos, we simply deactivate the time attention layers in the Unet with a 25% chance during training, for all models.Results: We evaluate our models' video synthesis capabilities on two objectives: LVEF accuracy (R 2 , MAE, RMSE) and image quality (SSIM, LPIPS, FID, FVD). We formulate the task as counterfactual modelling, where we set (1) a random conditioning frame as confounder, (2) the ground-truth LVEF as a factual conditioning, and (3) a random LVEF in the physiologically plausible range from 15% to 85% as counterfactual conditioning. For each ground truth video, we sample three random starting noise samples and conditioning frames. We use the LVEF regression model to create a feedback loop, following what [21] did, even though their model was run 100× per sample instead of 3×. For each ground truth video, we keep the sample with the best LVEF accuracy to compute all our scores over 1288 videos for each model.The results in Table 1 show that increasing the frame rate improves model fidelity to the given LVEF, while adding more models to the cascade decreases image quality. This is due to a distribution gap between true low-resolution samples and sequentially generated samples during inference. This issue is partially addressed by adding noise to real low-resolution samples during training, but the 1SCM model with only one stage still achieves better image quality metrics. However, the 2SCM and 4SCM models perform equally well on LVEF metrics and outperform the 1SCM model thanks to their higher temporal resolution that precisely captures key frames of the heartbeat. The TSSR model, used in the 2SCM, yields the best compromise between image quality, LVEF accuracy, and sampling times, and is compared to previous literature.We outperform previous work for LVEF regression: counterfactual video generation improves with our method by a large margin of 38 points for the R 2 score as shown in Table 2. The similarity between our factual and counterfactual results show that our time-agnostic confounding factor (i.e. an image instead of a video) prevents entanglement, as opposed to the approach taken in [21]. Our method does not score as high for SSIM as global image similarity metric, which is expected because of the stochasticity of the speckle noise. In [21] this was mitigated by their data-rich confounder. Our results also match other video diffusion models [3,7,10,30] as structure is excellent, while texture tends to be more noisy as shown in Fig. 2.Table 1. Metrics for all CDMs. The Gen. task is the counterfactual generation comparable to [21], the Rec. task is the factual reconstruction task. Frames is the number of frames generated by the model, always spanning 2 s. ‡ Videos are temporally upsampled to 64 frames for metric computation. S. time is the sampling time for one video on an RTX A5000. R 2 , MAE and RMSE are computed between the conditional LVEF λc and the regressed LVEF using the model described in Sect. 1. SSIM, LPIPS, FID and FVD are used to quantify the image quality. LPIPS is computed with VGG [29], FID [6] and FVD [36]    Qualitative Study: We asked three trained clinicians (Consultant cardiologist > 10 years experience, Senior trainee in cardiology > 5 years experience, Chief cardiac physiologist > 15 years experience) to classify 100 samples, each, as real or fake. Experts were not given feedback on their performance during the evaluation process and were not shown fake samples beforehand. All samples were generated with the 1SCM model or were true samples from the EchoNet-Dynamic dataset, resampled to 32fps and 2 s. The samples were picked by alphabetical order from the validation set. Among the 300 samples evaluated, 130 (43.33%) were real videos, 89 (29.67%) were factual generated videos, and 81 (27.0%) were counterfactual generated videos. The average expert accuracy was 54.33%, with an inter-expert agreement of 50.0%. More precisely, experts detected real samples with an accuracy of 63.85%, 50.56% for factual samples and 43.21% for the counterfactual samples. The average time taken to evaluate each sample was 16.2 s. We believe that these numbers show the video quality that our model reaches, and can put in perspective the SSIM scores from Table 1.Downstream Task: We train our LVEF regression model on rebalanced datasets and resampled datasets. We rebalance the datasets by using our 4SCM model to generate samples for LVEF values that have insufficient data. The resampled datasets are smaller datasets randomly sampled from the real training set. We show that, in small data regimes, using generated data to rebalance the dataset improves the overall performance. Training on 790 real data samples yields an R 2 score of 56% while the rebalanced datasets with 790 samples, ∼50% of which are real, reaches a better 59% on a balanced validation set. This observation is mitigated when more data is available. See Appendix Table 2 for all our results.Discussion: Generating echocardiograms is a challenging task that differs from traditional computer vision due to the noisy nature of US images and videos. However, restricting the training domain simplifies certain aspects, such as not requiring a long series of CDMs to reach the target resolution of 112 × 112 pixels and limiting samples to 2 s, which covers any human heartbeat. The limited pixel-intensity space of the data also allows for models with fewer parameters.In the future, we plan to explore other organs and views within the US domain, with different clinical conditionings and segmentation maps."
Feature-Conditioned Cascaded Video Diffusion Models for Precise Echocardiogram Synthesis,4,Conclusion,"Our application of EDMs to US video generation achieves state-of-the-art performance on a counterfactual generation task, a data augmentation task, and a qualitative study by experts. This significant advancement provides a valuable solution for downstream tasks that could benefit from representative foundation models for medical imaging and precise medical video generation."
Feature-Conditioned Cascaded Video Diffusion Models for Precise Echocardiogram Synthesis,,Fig. 1 .,
Feature-Conditioned Cascaded Video Diffusion Models for Precise Echocardiogram Synthesis,,Fig. 2 .,
Feature-Conditioned Cascaded Video Diffusion Models for Precise Echocardiogram Synthesis,,,
Feature-Conditioned Cascaded Video Diffusion Models for Precise Echocardiogram Synthesis,,Table 2 .,MethodConditioning Task S. timeR 2 ↑ MAE ↓ RMSE ↓ SSIM ↑
Feature-Conditioned Cascaded Video Diffusion Models for Precise Echocardiogram Synthesis,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5_14.
FSDiffReg: Feature-Wise and Score-Wise Diffusion-Guided Unsupervised Deformable Image Registration for Cardiac Images,1,Introduction,"Deformable image registration is the process of accurately estimating non-rigid voxel correspondences, such as the deformation field, between the same anatomical structure of a moving and fixed image pair. Fast, accurate, and realistic image registration algorithms are essential to improving the efficiency and accuracy of clinical practices. By observing dynamic changes, such as lesions, physicians can more comprehensively design treatment plans for patients [8,11]. When images during surgery align with preoperative ones, surgeons can locate instruments better and improve surgical prognosis [1]. As reported in [12], cardiac image registration is especially vital in improving heart chamber analysis accuracy, correcting cardiac imaging errors, and guiding cardiac surgeries. Thus, several studies have explored classical [2,16] and deep-learning-based [3,10,14,20] registration methods over the years.Classical registration methods [16] used hand-crafted features to align images by solving computational-expensive optimization problems. Recently, researchers explored the deep-learning-based unsupervised deformable image registration [3,14,19,20] to address the computational burden while reducing the need for accurate ground truth in the registration task. VoxelMorph [3], as the baseline, took moving and fixed image pairs as the input and maximized image pair similarity to train a registration network. To achieve higher accuracy, most unsupervised methods adopted a cascaded network with several sub-networks or an iterative refinement strategy [6,10,14,20]. These strategies made the training procedure complicated and computational resources demanding. Meanwhile, to obtain smoother and more realistic deformation fields, i.e., topology preservation, many existing works introduced explicit diffeomorphic constraints [7,17,19] or additional calculations on cycle consistency [14]. For example, CycleMorph [14] utilized the bidirectional registration consistency to preserve the topology during training. VoxelMorph-Diff [7] adopted velocity field-based deformation field and new diffeomorphic estimation. SYMNet [19] used symmetric deformation field estimation to achieve the goal. However, these schemes did not fully exploit the inherent network features, thereby overlooking these features' ability for better topology preservation. f -m intuitively indicates where significant deformation occurs, as the redboxed area shows. We calculated the per voxel energy score in the diffusion model's latent obtained in [13] as [10] suggests to identify the areas where complex deformation is likely to happen. The result indicates the same area, which was not explicitly utilized in prior work [13]. (Color figure online)Recently, Kim et al. [13] first proposed a diffusion model [9], which is simpler to train than other generative models yet rich in semantics, for the registration task. They used the latent feature from the diffusion model's score function, i.e., the gradient field of a distribution's log-likelihood function [22], as one of the registration network's inputs for a better registration result. However, this method only used the final diffusion score as an image level guidance, which ignored diffusion model's rich task-specific semantics in the feature levels, as proven in [4,18,23]. This resulted in the latent semantics of the diffusion model not being able to directly guide the features learned at the hidden layers of the registration network. As a result, the informativeness of these features for image registration was reduced. Moreover, this method only preserved deformation topology by simply using the diffusion score as the input, thereby ignoring the informative details about areas where significant deformations occur ; see Fig. 1d for unexploited informative semantics. Therefore, the registration network was unable to explicitly prioritize hard-to-register areas, thereby limiting its effectiveness in preserving the deformation topology.To address these issues, we present two novel modules, namely Featurewise Diffusion-Guided Module (FDG) and Score-wise Diffusion-Guided Module (SDG) in the registration network. FDG introduces a direct feature-wise diffusion guidance technique for generating deformation fields by utilizing crossattention to integrate the intermediate features of the diffusion model into the hidden layer of the registration network's decoder. Furthermore, we embed the feature-wise guidance into multiple layers of the registration network and produce the feature-level deformation fields in multiple scales. Finally, after obtaining deformation fields at multiple scales, we upsample and average them to generate the full-resolution deformation field for registration. Our SDG introduces explicit score-wise diffusion guidance for deformation topology preservation by reweighing the similarity-based unsupervised registration loss based on the diffusion score. Through this reweighing scheme, direct attention is given during the optimization process to ensure the preservation of the deformation topology. Our main contribution can be summarized as follows:-We propose a novel feature-wise diffusion-guided module (FDG), which utilizes multi-scale intermediate features from the diffusion model to effectively guide the registration network in generating deformation fields. -We also propose a score-wise diffusion-guided module (SDG), which leverages the diffusion model's score function to guide deformation topology preservation during the optimization process without incurring any additional computational burden. -Experimental results on the cardiac dataset validated the effectiveness of our proposed method."
FSDiffReg: Feature-Wise and Score-Wise Diffusion-Guided Unsupervised Deformable Image Registration for Cardiac Images,2,Method,
FSDiffReg: Feature-Wise and Score-Wise Diffusion-Guided Unsupervised Deformable Image Registration for Cardiac Images,2.1,Baseline Registration Model,"Figure 2a shows the overview of our proposed method. We first sample a perturbed noisy image x t from the fixed target image f following the same scheme in [9], which can be formulated as Eq. 1:wherewhere 0 < β s < 1 is the variance of the noise, t is the noise level. Then we perform the registration training task. Given an input x in consisting of a fixed reference image f , a moving unaligned image m, and the perturbed noisy image x t , we feed this input x in = {f, m, x t } into the registration network's shared encoder  .., N from the i-th layer of the decoder. Of note, we generate the registration decoder's feature map by incorporating the guidance from the diffusion decoder, which can be formulated as Eq. 2:where r i is the i-th layer of the registration decoder, and F i E is the skip connection of features from the shared encoder layer at the same depth.After obtaining the feature map pairs, our FDG module estimates the i-th feature level deformation field φ i from the feature map pair (F i G , F i R ) using linear cross attention [21], which can be defined as Eq. 3:After obtaining all feature-level deformation fields from the shallowest layer to the deepest layer, we generate the final deformation field φ by enlarging and averaging all feature-level deformation fields. This is a commonly adopted method for multi-scale deformation field merging so as to merge features which attend different scales and granularity. The final φ is then fed into the spatial transformation layer with the moving image m to generate the registered image m(φ)."
FSDiffReg: Feature-Wise and Score-Wise Diffusion-Guided Unsupervised Deformable Image Registration for Cardiac Images,2.3,Score-Wise Diffusion-Guided Module,"Given the representation z encoded by the shared encoder z = E β (x in ), the diffusion decoder G σ outputs a diffusion score estimation S = G σ (z). Then, the Score-wise Diffusion-Guided Module (SDG) uses this score to reweigh the similarity-based normalized cross-correlation loss function, formulated as Eq. 4:where m(φ) is the warped moving image, defines the Hadamard product, and ⊗ defines the local normalized cross-correlation function. γ is a hyperparameter to amplify the reweighing effect. By this means, SDG utilizes the diffusion score to explicitly indicate the hardto-register areas, i.e., areas where deformation topology is hard to preserve, then assigning higher weights in the loss function for greater attention, and vice versa for easier-to-register areas. Therefore, the information on deformation topology is effectively incorporated into the optimization process without additional constraints by the SDG module."
FSDiffReg: Feature-Wise and Score-Wise Diffusion-Guided Unsupervised Deformable Image Registration for Cardiac Images,2.4,Overall Training and Inference,"Loss Function. Our network predicts the deformable fields at the feature level and then outputs the registered image. The total loss function of our method is defined as Eq. 5:,where ∼ N (0, I )where L dif f usion is the auxiliary loss function for training the diffusion decoder G σ (Eq. 6), and t is the noise level of x t , following the method in [9]. Our proposed L scoreN CC encourages maximizing the similarity between the registered and reference images while preserving the deformation topology.||∇ φ || 2 is the conventional smoothness penalty on the deformation field. λ and λ φ are hyperparameters, and we empirically set them to 20 in our experiments.Inference. In the inference stage, we perform image registration in the same style as [13]. Instead of the perturbed image x t , we input the original reference image f into the network, and the total network input becomes x in = {f, m, f }. Given this network input x in , our network first generates the deformation field φ between the moving image m and the reference image f and produces the registered moving image m(φ) by feeding the moving image m and the deformation field φ into the spatial transformation layer (STL). The registered moving image is the final output of our network."
FSDiffReg: Feature-Wise and Score-Wise Diffusion-Guided Unsupervised Deformable Image Registration for Cardiac Images,3,Experiments and Results,"Dataset and Preprocessing. Following the previous work [13], we used the publicly available 3D cardiac MR dataset ACDC [5] for experiments. The dataset includes 100 4D temporal cardiac MRI data with corresponding segmentation maps. We selected the 3D image at the end of the diastolic stage as the fixed image and the image at the end of the systolic stage as the moving image. We resampled all scans to the voxel spacing of 1.5×1.5×3.15 mm, then cropped them to the voxel size of 128 × 128 × 32. We normalized the intensity of all images to [-1, 1]. The training set contains 90 image pairs, while the remaining 10 pairs form the test set. The abovementioned preprocessing steps were performed in accordance with the approach described in prior work [13] to ensure a fair comparison. Implementation Details. The proposed framework was implemented using the PyTorch library, version 1.12.0. Following [13], we used DDPM UNet's 3D encoder as our shared encoder and DDPM UNet's 3D decoder as our diffusion decoder. For the registration part, instead of a complete 3D UNet in [13], we only used DDPM UNet's 3D decoder as our registration decoder to generate the deformation field. During the diffusion task, we gradually increased the noise schedule from 10 -6 to 10 -2 over 2000 timesteps. We utilized an Nvidia RTX3090 GPU and the Adam optimization algorithm [15] to train the model with λ = 20, λ φ = 20, γ = 1, batch size B = 1, a learning rate of 2 × 10 -4 , and a maximum of 700 epochs.Evaluation Metrics. We employed three evaluation metrics, i.e., DICE, |J| ≤ 0(%), and SD(|J|) to measure the image registration performance, following existing registration methods [3,13,14]. DICE measures the spatial overlap of anatomical segmentation maps between the warped moving image and the fixed reference image. A higher Dice score indicates better alignment between the warped moving image and the fixed reference image, thus reflecting an improved registration quality. |J| ≤ 0(%) indicates the percentage of non-positive values in the Jacobian determinant of the registration field. This metric indicates the percentage of voxels that lacks a one-to-one registration mapping relation, causing unrealistic deformations and roughness. SD(|J|) refers to the standard deviation of the Jacobian determinant of the registration field. A lower standard deviation indicates that the registration field is relatively smooth and consistent across the image.Compare with the State-of-the-Art Methods. Table 1 shows the comparison of our method with existing state-of-the-art methods including Voxel-Morph [3], VoxelMorph-Diff [7], and DiffuseMorph [13] on the same training and testing dataset. We produced baseline results using the recommended hyperparameters in their paper. The result shows that our proposed method outperforms existing baseline methods by a substantial margin (Wilcoxon signed-rank test, p < 0.005) (Also see Fig. 3). Furthermore, our method aligned better in areas where larger deformation happened, such as myocardium (myo).Table 1. Image registration results with standard deviation in parenthesis on the 3D cardiac dataset. ""LV"", ""Myo"", ""RV"" refers to Left Ventricle, Myocardium, and Right Ventricle, respectively. ""Overall"" refers to the averaged registration result of the left blood pool, myocardium, left ventricle, right ventricle, and these total region, following [13]. ↑: the higher, the better results. ↓: the lower, the better results.   Ablation Study. To validate the effectiveness of our proposed learning strategies, including the Feature-wise Diffusion-Guided module(FDG) and Score-wise Diffusion-Guided module(SDG), we conducted ablative experiments, as shown in Table 2. The network without FDG also uses the denoising diffusion decoder but generates the deformation field from the encoded feature directly, and without SDG means that we optimize the network using the vanilla NCC loss. By integrating multi-scale intermediate latent diffusion features into generating deformation fields, we can see that the network's performance increased by 1%. By deploying the reweighing loss, the Jacobian metric decreased by 60.5%. The result achieved a balance when all components were deployed. These results demonstrated that our proposed components could effectively guide the deformation field generation by using multi-scale diffusion features (Also see Fig. 4).Optimization guided by diffusion score led to better preservation of deformation topology. It is worth noticing that the results without FDG or SDG showed only marginal improvement over baseline results, indicating the importance of feature-level deformation field generation and the reweighing scheme. The ablative study of hyperparameter λ is illustrated in Supp. Fig. 1.  Analysis of γ. Furthermore, to validate SDG's effectiveness on topology preservation, we conducted another ablative study on SDG's hyperparameter γ, as Table 3 shows. Increased γ indicates a more substantial reweighing effect. The results showed that by adding stronger reweighing influence, we could obtain deformation fields with better topology preservation almost without compromising accuracy."
FSDiffReg: Feature-Wise and Score-Wise Diffusion-Guided Unsupervised Deformable Image Registration for Cardiac Images,4,Conclusion,"This work proposes two novel modules for unsupervised deformable image registration: the Feature-wise Diffusion-Guided module (FDG) and the Score-wise Diffusion-Guided module (SDG). Among these modules, FDG can effectively guide the deformation field generation by utilizing the multi-scale intermediate diffusion features. SDG demonstrates its ability to guide the optimization process for better deformation topology preservation using the diffusion score. Extensive experiments show that the proposed framework brings impressive improvements over all baselines. The proposed work models the non-linear deformation semantics using the diffusion model. Therefore, it is sound to generalize to other registration tasks and images, which may be one of the future research directions."
FSDiffReg: Feature-Wise and Score-Wise Diffusion-Guided Unsupervised Deformable Image Registration for Cardiac Images,,,
FSDiffReg: Feature-Wise and Score-Wise Diffusion-Guided Unsupervised Deformable Image Registration for Cardiac Images,,Fig. 2 .,
FSDiffReg: Feature-Wise and Score-Wise Diffusion-Guided Unsupervised Deformable Image Registration for Cardiac Images,,2. 2,
FSDiffReg: Feature-Wise and Score-Wise Diffusion-Guided Unsupervised Deformable Image Registration for Cardiac Images,,Fig. 3 .,
FSDiffReg: Feature-Wise and Score-Wise Diffusion-Guided Unsupervised Deformable Image Registration for Cardiac Images,,Fig. 4 .,
FSDiffReg: Feature-Wise and Score-Wise Diffusion-Guided Unsupervised Deformable Image Registration for Cardiac Images,,Table 2 .,
FSDiffReg: Feature-Wise and Score-Wise Diffusion-Guided Unsupervised Deformable Image Registration for Cardiac Images,,Table 3 .,
FSDiffReg: Feature-Wise and Score-Wise Diffusion-Guided Unsupervised Deformable Image Registration for Cardiac Images,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 62.
Estimation of 3T MR Images from 1.5T Images Regularized with Physics Based Constraint,1,Introduction,"Magnetic resonance imaging (MRI) has seen a tremendous growth over the past three decades as a preferred diagnostic imaging modality. Starting with the 0.25T MRI scanners in 19801980ss [11], clinical scanners have emerged from 1.5T and 3T to the recently approved 7T scanners [9]. Primary reason for the preference of higher magnetic field strength (FS) is the better image quality but it comes with high cost. Generally, the cost of clinical MRI scanners increases at USD 1million/Tesla [14]. Therefore, there is a clear divide in the distribution of MRI scanners across different countries in the world. With the FDA approval of clinical use of 7T MRI scanners-3T and 7T MRI scanners have become preferred scanners in developed countries but inexpensive low field MRI scanners (<1T) and mid field MRI scanners (1.5T) are still a popular choice in developing countries [10,11].Since the past decade, there is a growing interest towards improving the quality of images acquired with low FS MRI scanners by learning the features responsible for image quality from high FS MRI scanners [8,9,19]. Earlier methods in such image translational problems synthesized target image contrast using paired example images [13]. The first work to address estimation of 7T-like images was reported in 2016 [1]. It exploits canonical correlation analysis (CCA) [5] to relate paired 3T-7T images in a more correlated space. With advent of deep learning, 3D convolutional neural network (CNN) was proposed to learn non-linear mapping between 3T and 7T images [3]. Different architectures of deep learning were addressed with better performances than previous methods [2,7,8,18]. However, except Lin et al [10], none of the existing approaches address the problem of improving the image quality of ≤1.5T images to estimate high quality 3T-like MR images. Notably, to the best of our knowledge no approach is reported in literature to estimate the 3T (or 1.5T-like) images from 1.5T (or 0.25T) images. This problem is particularly challenging because of the severity of degradation present in ≤1T MR images [10]. In [10], the mapping between example 3T and simulated 0.36T images using CNNs to estimate 3T from scanner-acquired 0.36T images is learned, and it requires the a-priori knowledge of distribution of tissuespecific SNR for given FS. Since the databases for example images are scarce and SNR of tissues is not known a-priori [12], the unsupervised methods needs to be explored to improve ≤1.5T images.In this work, we address the problem of estimating 3T-like images (x) from 1.5T images (y) in an unsupervised manner. To our best knowledge, this is the first work to develop a method for improving ≤1.5T images to estimate HF images without requiring example images. The proposed method formulates the estimation of 3T HF images as an inverse problem: y = f (x), where f (.) is the unknown degradation model. The novel contributions of our work are as follows: (i) the alternate minimization (AM) framework is formulated to estimate x as well as the mapping kernel f (.) relating x and y, (ii) Acquisition physics based signal scaling is proposed to synthesize the desired image contrast similar to HF image, (iii) the simulated contrast image is used as a regularizer while estimating x from y. The experimental results demonstrate that the proposed approach provides improved quality of MRI images in terms of image contrast and sharp tissue boundaries that is similar to HF images. The experiments further demonstrate the successful application of the improved quality images provided by proposed method in improved tissue segmentation and volume quantification."
Estimation of 3T MR Images from 1.5T Images Regularized with Physics Based Constraint,2,Proposed Method,"The proposed work formulates the estimation of HF image (x) from LF image (y) in an alternate minimization framework:Here x ∈ R m×n and y ∈ R m×n represent the HF and LF MRI images, respectively. The matrix h ∈ R p×p represents transformation kernel convolved using * operator with each patch of y of size p × p and p is empirically chosen as 5.The matrix c ∈ R m×n represent the pixel wise scale when multiplied with y generates the image with contrast similar to HF image. Here, c y represents the Hadamard product of pixel wise scale c and y the 1.5T (or 0.25T) image."
Estimation of 3T MR Images from 1.5T Images Regularized with Physics Based Constraint,2.1,Physics Based Regularizer,"The physics based regularizer exploits the fact that the T1 relaxation time increases with FS. The differences among T1 relaxation times of gray matter (GM), white matter (WM) and cereberospinal fluid (CSF) also increase with FS, leading to increased contrast in the T1-weighted images in the order 3T ≥1.5T ≥ 0.25T. This factor is used to simulate an image from y which convey similar information as x, and is obtained by scaling the signal intensities of y based on changes in signal due to changes in T1 relaxation time with respect to FS-denoted by r. The acquired signal for y denoted by s l can be corrected with relaxation time to simulate the signal s h that would have been acquired for x as:For example, in the spin echo (SE) pulse sequence, acquired signal can be represented as, s = AB 2 0(1-e (-T R/T 1 ) sinθ) (1-e (-T R/T 1 ) cosθ) e (-T E/T2) [6,16]. Here, A represents the proportionality constant. The field strengths for 3T (or 1.5T) and 1.5T (or 0.25T) scanners are denoted by B 0 . The factor r for the given voxel for SE sequence by assuming long TE, and T2 to be same across different FS (assumption derived from literature [12]) be computed asHere, T 1 and T 2 represent the T1/T2 relaxation times, T R-repetition time, T Eecho time and θ-flip angle (FA). The parameters with subscript h and l represent the parameters used for HF and LF MRI acquisitions, respectively. Similarly, respective mathematical formulations can be used for other pulse sequences such as fast SE (FSE) and gradient echo (GRE). After computing r using Eq. ( 3), it can be used to simulate HF acquisition ŝh using Eq. ( 2)The values of T 1 /T 2 relaxation times differ with tissues, hence different values of r should be computed for every voxel considering the tissues present in that voxel. However, if we assign single tissue per voxel using any tissue segmentation technique such as FAST [15], compute r for each voxel to estimate ŝh , this could lead to discontinuity among tissue boundaries in simulated HF image, and is shown in Fig. S1 in supplementary material (SM)."
Estimation of 3T MR Images from 1.5T Images Regularized with Physics Based Constraint,,Compute Relaxation Times for Voxels with More Than One Tissue:,"In real practice there exist many voxels with more than one tissue kind present in them, and is the reason for discontinuities present in Fig. S1. The challenge is that T1 relaxation time for such voxels is not known that is required to estimate r. We address this issue by estimating the T1 relaxation time of such voxel as linear combination of T1 relaxation times of tissues present in the given voxel. The linear weights are directly proportional to the probability of tissues present in the voxel. Though this work performs well with probability maps but we use pixel intensity to denote the probability of tissue to avoid the additional and expensive tissue segmentation step as follows: The top 5 percentile of pixel intensities are assumed to belong to WM and bottom 20 percentile as GM. Consider w and q as T1 relaxation time and corresponding pixel intensity, respectively. We here approximate relation between w and q using linear equation as wWM -wGM qWM -qGM = w-wGM q-qGM . Here, subscript indicates the tissue type. Here, w W M and w GM are T1 relaxation times which are currently assumed to be constant, and taken from literature [12] whereas q W M and q GM are the pixel intensity values computed by averaging pixel intensities falling in mentioned percentiles for WM and GM, respectively. In a simplest case, say for an image q W M and q GM are 1 and 0, then the linear relationship is reduced to w = (w W Mw GM )q + w GM . Hence, for the given pixel intensity we can estimate the corresponding T1 relaxation time as a linear combination of T1 relaxation times of WM and GM. The range of pixel intensities in y is partitioned into several bins. This is followed by approximation of T1 relaxation times (T 1,l ) for different bins for y by assuming w W M and w GM as T1 relaxation times at LF FS. In the similar way, T 1 1,h can be approximated.After approximating T 1,l and T 1,h for all voxels (and empirically choosing the T R h and θ h ) the r is computed using Eq. ( 3) and is used to estimate the HF acquisition using Eq. ( 2). The estimated ŝh is used to compute the pixel wise scale c as c = ŝh /y, i.e., an element wise division operator. The computed c is used in Eq. ( 1) to constrain the solution space of estimated HF image x."
Estimation of 3T MR Images from 1.5T Images Regularized with Physics Based Constraint,3,Experimental Results,"The proposed work has been demonstrated for (i) estimating 3T-like from 1.5T images, (ii) estimating 1.5T-like from 0.25T images, (iii) evaluating accuracy of tissue segmentation using improved images in (i) and (ii), and (iv) comparing (i), (ii) and (iii) with existing methods. The results related to (ii) are summarized in SM due to space constraints. The values for λ 1 and λ 2 are chosen as 1.2 and 0.4, respectively."
Estimation of 3T MR Images from 1.5T Images Regularized with Physics Based Constraint,3.1,Data,"The MRI images used to demonstrate the efficacy of proposed work were acquired from five healthy subjects of age 25 ± 10 years. Three different MRI scanners were used in this study: 0.25T (G-scan Brio, Esaote), 1.5T (Aera Siemens) and 3T (Verio, Siemens). The first three subjects were scanned using each of the three scanners while the other two subjects were scanned with only 1.5T and 3T scanners. The three scanners were located in Post Graduate Institute of Medical Education & Research (PGIMER) Chandigarh, India. Scanning was performed using the standard clinical protocols-optimized for both clinical requirement and work-load of clinical site. All the scans were performed according to the guidelines of the Declaration of Helsinki. The details of pulse sequence and scan parameters used to acquire data in each of the three scanners is mentioned in SM Table S1. The acquired T1 MR image volumes for each scanner and each human subject were pre-processed similar to the human connectome project (HCP) pre-processing pipelines for structural MR images [4]. Please note that proposed approach does not require the LF and HF images to be skull stripped or to have pixel to pixel correspondence. It is only done to provide reference based similarity scores of estimated image with respect to HF image."
Estimation of 3T MR Images from 1.5T Images Regularized with Physics Based Constraint,3.2,Analysis/Ablation Study of Proposed Approach,"The HF image is estimated at different stages of proposed approach to demonstrate the significance of each term in Eq. ( 1), and is shown in Fig. 1(a). In Fig. 1(b), the impact of proposed regularizer on estimation of HF image is demonstrated by changing the values of λ 1 . It can be observed that the image Fig. 1(a)-(ii) obtained just from the data fidelity term leads to sharp image details but without any contrast improvement. However, HF image simulated by the physics based regularizer from Eq. ( 2) improves the contrast but details remain blurred, as is evident from image Fig. 1 "
Estimation of 3T MR Images from 1.5T Images Regularized with Physics Based Constraint,,(a)-(iii). Once the data fidelity and the physics,
Estimation of 3T MR Images from 1.5T Images Regularized with Physics Based Constraint,3.3,Comparison with Existing Approaches,"The performance of proposed approach is compared with existing methods that either address the contrast synthesis or estimation of HF images ScSR [17],CCA [5], MIMECS [13], ED [8]. The comparison is done in four ways: (i) Objective analysis using reference based metrics that requires the ground truth HF image, and pixel to pixel to correspondence between query image and ground truth image-Table 1, (ii) Objective analysis using no-reference based metrics which describe the quality of images solely based on edge sharpness and image contrast-Table 2, (i ii) Subjective analysis that includes rating of images by clinical experts in range 0 to 5, 5 and 0 being the highest and worst quality images, respectively -Table 3 and (iv) Qualitative analysis that includes the analysis of visual appearance of image details -Fig. 2. It can be observed that the existing methods provide better performance in terms of reference based metrics but perform inferior to proposed method in case of no-reference based metrics and subjective scores. This is due to the way the existing methods are designed, i.e., these methods are trained to minimize the mean square error between estimated and HF images, thus they provide higher peak signal to noise ratio (PSNR) and structural similarity index metric (SSIM) but the image details are still blurred which lead to drop in edge sharpness. The validation of argument can also be derived by visually inspecting the images in Fig. 2. It has been observed that encoder-decoder based approach (ED [8]), MIMECS [13], CCA [5] and ScSR [17] provides blurred image details. The possible reasons are (i) minimizing MSE can provide perceptually blurred results, (ii) the weighted averaging involved in [5,13,17] induces blur, (iii) inaccurate pixel to pixel correspondences makes it difficult for supervised methods to learn the actual mapping relating input and target images. The drop in performances of existing approaches due to inaccuracies in image registration is more prominently observed when improving 0.25T images in Fig. S3. The robustness to such inaccuracies by proposed approach due to its unsupervised nature shows its clear advantages over existing methods.   "
Estimation of 3T MR Images from 1.5T Images Regularized with Physics Based Constraint,,Fig. 1 .,
Estimation of 3T MR Images from 1.5T Images Regularized with Physics Based Constraint,,Fig. 2 .,
Estimation of 3T MR Images from 1.5T Images Regularized with Physics Based Constraint,,Fig. 3 .,
Estimation of 3T MR Images from 1.5T Images Regularized with Physics Based Constraint,,Table 1 .,
Estimation of 3T MR Images from 1.5T Images Regularized with Physics Based Constraint,,Table 2 .,
Estimation of 3T MR Images from 1.5T Images Regularized with Physics Based Constraint,,Table 3 .,
Estimation of 3T MR Images from 1.5T Images Regularized with Physics Based Constraint,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 13.
Estimation of 3T MR Images from 1.5T Images Regularized with Physics Based Constraint,3.4,Application to Tissue Segmentation and Volume Quantification,"The segmentation labels for WM, GM and CSF were computed using FAST toolbox in FSL software [15] for 3T-like images estimated by different approaches, and shown in Fig. 3. The improved tissue segmentation for 0.25T images is shown in Fig. S5 in SM. The zoomed windows in both figures indicate that the segmentation label of WM is improved for the estimated 3T reconstructed image by the proposed approach for estimated 3T image from 1.5T image. The quantitative measure used to evaluate performance of different methods is dice ratio, and is reported in Table S2, and its comparison is mentioned in Table S3. The ability to accurately segment tissues from image estimated by proposed approach is shown to be comparable both qualitatively and quantitatively to existing methods. Further, proposed method is shown to provide statistically significant with p < 0.01 improvement in accuracy of WM and GM tissue volume quantification for estimated 3T (and 1.5T images), and is shown in Fig. S6."
Estimation of 3T MR Images from 1.5T Images Regularized with Physics Based Constraint,4,Summary,"We propose a method to estimate HF images from ≤1.5T images in an unsupervised manner. Here, the knowledge of acquisition physics to simulate HF image is exploited, and used it in a novel way to regularize the estimation of HF image. The proposed method demonstrates the benefits over state of the art supervised methods that are severely effected by the inaccuracies if present in image registration process. Lower the FS image is, harder is to get accurate image registration, and thus proposed method proves to be a better choice. Further, it is also demonstrated that the proposed approach provides statistically significant accurate tissue segmentation. The code for this work is publicly shared on https://drive.google.com/drive/folders/1WbzkBJS1BWAje8aF0ty2SWYTQ9i0 B7Yr?usp=sharing."
MEPNet: A Model-Driven Equivariant Proximal Network for Joint Sparse-View Reconstruction and Metal Artifact Reduction in CT Images,1,Introduction,"Computed tomography (CT) has been widely adopted in clinical applications. To reduce the radiation dose and shorten scanning time, sparse-view CT has drawn much attention in the community [10,34]. However, sparse data sampling inevitably degenerates the quality of CT images and leads to adverse artifacts. In addition, when patients carry metallic implants, such as hip prostheses and spinal implants [11,13,24], the artifacts will be further aggravated due to beam hardening and photon starvation. For the joint sparse-view reconstruction and metal artifact reduction task (SVMAR), how to design an effective method for artifact removal and detail recovery is worthy of in-depth exploration.For the sparse-view (SV) reconstruction, the existing deep-learning (DL)based methods can be roughly divided into three categories based on the information domain exploited, e.g., sinogram domain, image domain, and dual domains. Specifically, for the sinogram-domain methods, sparse-view sinograms are firstly repaired based on deep networks, such as U-Net [10] and dense spatial-channel attention network [37], and then artifact-reduced CT images are reconstructed via the filtered-back-projection (FBP) process. For the image-domain methods, researchers have proposed to learn the clean CT images from degraded ones via various structures [18,34,35]. Alternatively, both sinogram and CT images are jointly exploited for the dual reconstruction [4,21,32,36].For the metal artifact reduction (MAR) task, similarly, the current DL-based approaches can also be categorized into three types. To be specific, sinogramdomain methods aim to correct the sinogram for the subsequent CT image reconstruction [6,33]. Image-domain-based works have proposed different frameworks, such as simple residual network [8] and an interpretable structure [22,23,26], to learn artifact-reduced images from metal-affected ones. The dual-domain methods [12,24,25,36] focus on the mutual learning between sinogram and CT image.Albeit achieving promising performance, these aforementioned methods are sub-optimal for the SVMAR task. The main reasons are: 1) Most of them do not consider the joint influence of sparse data sampling and MAR, and do not fully embed the physical imaging constraint between the sinogram domain and CT image domain under the SVMAR scenario; 2) Although a few works focus on the joint SVMAR task, such as [36], the network structure is empirically built based on off-the-shelf modules, e.g., U-Net and gated recurrent units, and it does not fully investigate and embed some important prior information underlying the CT imaging procedure. However, for such a highly ill-posed restoration problem, the introduction of the proper prior is important and valuable for constraining the network learning and helping it evolve in a right direction [24]."
MEPNet: A Model-Driven Equivariant Proximal Network for Joint Sparse-View Reconstruction and Metal Artifact Reduction in CT Images,,1-⋃ ⋃,"Fig. 1. Illustration of the elements in the model Eq. ( 2) for easy understanding.To alleviate these issues, in this paper, we propose a model-driven equivariant proximal network, called MEPNet, which is naturally constructed based on the CT imaging geometry constraint for this specific SVMAR task, and takes into account the inherent prior structure underlying the CT scanning procedure. Concretely, we first propose a dual-domain reconstruction model and then correspondingly construct an unrolling network framework based on a derived optimization algorithm. Furthermore, motivated by the fact that the same organ can be imaged at different angles making the reconstruction task equivariant to rotation [2], we carefully formulate the proximal operator of the built unrolling neural network as a rotation-equivariant convolutional neural network (CNN). Compared with the standard-CNN-based proximal network with only translationequivariance property [3], our proposed method effectively encodes more prior knowledge, e.g., rotation equivariance, possessed by this specific task. With such more accurate regularization, our proposed MEPNet can achieve higher fidelity of anatomical structures and has better generalization capability with fewer network parameters. This is finely verified by comprehensive experiments on several datasets of different body sites. To the best of our knowledge, we should be the first to study rotation equivariance in the context of SVMAR and validate its utility, which is expected to make insightful impacts on the community."
MEPNet: A Model-Driven Equivariant Proximal Network for Joint Sparse-View Reconstruction and Metal Artifact Reduction in CT Images,2,Preliminary Knowledge About Equivariance,"Equivariance of a mapping w.r.t. a certain transformation indicates that executing the transformation on the input produces a corresponding transformation on the output [3,28]. Mathematically, given a group of transformations G, a mapping Φ from the input feature space to the output feature space is said to be group equivariant about G ifwhere f is any input feature map in the input feature space; T g and T g represent the actions of g on the input and output, respectively. The prior work [3] has shown that adopting group equivariant CNNs to encode symmetries into networks would bring data efficiency and it can constrain the network learning for better generalization. For example, compared with the fullyconnected layer, the translational equivariance property enforces weight sharing for the conventional CNN, which makes CNN use fewer parameters to preserve the representation capacity and then obtain better generalization ability. Recently, different types of equivariant CNNs have been designed to preserve more symmetries beyond current CNNs, such as rotation symmetry [1,2,27] and scale symmetry [7,20]. However, most of these methods do not consider specific designs for the SVMAR reconstruction. In this paper, we aim to build a physics-driven network for the SVMAR task where rotation equivariance is encoded."
MEPNet: A Model-Driven Equivariant Proximal Network for Joint Sparse-View Reconstruction and Metal Artifact Reduction in CT Images,3,Dual-Domain Reconstruction Model for SVMAR,"In this section, for the SVMAR task, we derive the corresponding dual domain reconstruction model and give an iterative algorithm for solving it. Dual-Domain Reconstruction Model. Given the captured sparse-view metal-affected sinogram Y svma ∈ R N b ×Np , where N b and N p are the number of detector bins and projection views, respectively, to guarantee the data consistency between the reconstructed clean CT image X ∈ R H×W and the observed sinogram Y svma , we can formulate the corresponding optimization model as [36]:where D ∈ R N b ×Np is the binary sparse downsampling matrix with 1 indicating the missing region; T r ∈ R N b ×Np is the binary metal trace with 1 indicating the metal-affected region; P is forward projection; R(•) is a regularization function for capturing the prior of X; ∪ is the union set; is the point-wise multiplication; H and W are the height and width of CT images, respectively; μ is a trade-off parameter. One can refer to Fig. 1 for easy understanding.To jointly reconstruct sinogram and CT image, we introduce the dual regularizers R 1 (•) and R 2 (•), and further derive Eq. (2) as:where S is the to-be-estimated clean sinogram; λ is a weight factor. Following [24], we rewrite S as Ȳ S for stable learning, where Ȳ and S are the normalization coefficient implemented via the forward projection of a prior image, and the normalized sinogram, respectively. Then we can get the final dual-domain reconstruction model for this specific SVMAR task as:As observed, given Y svma , we need to jointly estimate S and X. For R 1 (•) and R 2 (•), the design details are presented below.Iterative Optimization Algorithm. To solve the model (4), we utilize the classical proximal gradient technique [15] to alternatively update the variables S and X. At iterative stage k, we can get the corresponding iterative rules:where η i is stepsize; prox μiηi (•) is proximal operator, which relies on the regularization term R i (•). For any variable, its iterative rule in Eq. ( 5) consists of two steps: an explicit gradient step to ensure data consistency and an implicit proximal computation prox μiηi (•) which enforces the prior R i (•) on the to-beestimated variable. Traditionally, the prior form R i (•) is empirically designed, e.g., l 1 penalty, which may not always hold in real complicated scenarios. Due to the high representation capability, CNN has been adopted to adaptively learn the proximal step in a data-driven manner for various tasks [5,14,30]. Motivated by their successes, in the next section, we will deeply explore the prior of this specific SVMAR task and carefully construct the network for prox μiηi (•)."
MEPNet: A Model-Driven Equivariant Proximal Network for Joint Sparse-View Reconstruction and Metal Artifact Reduction in CT Images,4,Equivariant Proximal Network for SVMAR,"By unfolding the iterative rules (5) for K iterations, we can easily build the unrolling neural network. Specifically, at iteration k, the network structure is sequentially composed of: x to execute the proximal operators prox μ1η1 and prox μ2η2 , respectively. To build proxNet θ (k) s , we follow [24] and choose a standard-CNN-based structure with four [Conv+BN+ReLU+Conv+BN+Skip Connection] residual blocks, which do not change image sizes. While for proxNet θ (k)x , we carefully investigate that during the CT scanning, the same body organ can be imaged at different rotation angles. However, the conventional CNN for modeling proxNet θ (k)x in [24] has only the translation equivariance property and it cannot preserve such an intrinsic rotation equivariance structure [3]. Against this issue, we propose to replace the standard CNN in [24] with a rotation equivariant CNN. Then we can embed more useful prior, such as rotation equivariance, to constrain the network, which would further boost the quality of reconstructed CT images (refer to Sect. 5.2).Specifically, from Eq. ( 1), for a rotation group G and any input feature map f , we expect to find a properly parameterized convolutional filter ψ which is group equivariant about G, satisfyingwhere π θ is a rotation operator. Due to its solid theoretical foundation, the Fourier-series-expansion-based method [28] is adopted to parameterize ψ as:Fig. 2. The framework of the proposed MEPNet where ""prior-net"" is designed in [24].where x = [x i , x j ] T is 2D spatial coordinates; a mn and b mn are learnable expansion coefficients; ϕ c mn x and ϕ s mn x are 2D fixed basis functions as designed in [28]; p is chosen to be 5 in experiments. The action π θ on ψ in Eq. ( 7) can be achieved by coordinate transformation as:Based on the parameterized filter in Eq. ( 8), we follow [28] to implement the rotation-equivariant convolution for the discrete domain. Compared with other types, e.g., harmonics and partial-differential-operator-like bases [19,27], the basis in Eq. ( 8) has higher representation accuracy, especially when being rotated. By implementing proxNet θ (k) s and proxNet θ (k)x in Eq. ( 6) with the standard CNN and the rotation-equivariant CNN with the p8 group,1 respectively, we can then construct the model-driven equivariant proximal network, called MEP-Net, as shown in Fig. 2. The expansion coefficients, {θ (k) s } K k=1 , θ prior for learning Ȳ [24], η 1 , η 2 , and λ, are all flexibly learned from training data end-to-end. Remark: Our MEPNet is indeed inspired by InDuDoNet [24]. However, MEP-Net contains novel and challenging designs: 1) It is specifically constructed based on the physical imaging procedure for the SVMAR task, leading to a clear working mechanism; 2) It embeds more prior knowledge, e.g., rotation equivariance, via advanced filter parametrization method, which promotes better reconstruction; 3) It is desirable that the usage of more transformation symmetries would further decrease the number of model parameters and improve the generalization. These advantages are validated in the experiments below. "
MEPNet: A Model-Driven Equivariant Proximal Network for Joint Sparse-View Reconstruction and Metal Artifact Reduction in CT Images,5,Experiments,
MEPNet: A Model-Driven Equivariant Proximal Network for Joint Sparse-View Reconstruction and Metal Artifact Reduction in CT Images,5.1,Details Description,"Datasets and Metrics. Consistent with [24], we synthesize the training set by randomly selecting 1000 clean CT images from the public DeepLesion [29] and collecting 90 metals with various sizes and shapes from [33]. Specifically, following the CT imaging procedure with fan-beam geometry in [24,31,36], all the CT images are resized as 416 × 416 pixels where pixel spacing is used for normalization, and 640 fully-sampled projection views are uniformly spaced in 360 • . To synthesize sparse-view metal-affected sinogram Y svma , similar to [36], we uniformly sample 80, 160, and 320 projection views to mimic 8, 4, and 2-fold radiation dose reduction. By executing the FBP process on Y svma , we can obtain the degraded CT image X svma .The proposed method is tested on three datasets including DeepLesion-test (2000 pairs), Pancreas-test (50 pairs), and CLINIC-test (3397 pairs). Specifically, DeepLesion-test is generated by pairing another 200 clean CT images from DeepLesion [29] with 10 extra testing metals from [33]. Pancreas-test is formed by randomly choosing 5 patients with 50 slices from Pancreas CT [17] and pairing each slice with one randomly-selected testing metal. CLINIC-test is synthesized by pairing 10 volumes with 3397 slices randomly chosen from CLINIC [13] with one testing metal slice-by-slice. The 10 testing metals have different sizes as [35] in pixels. For evaluation on different sizes of metals as listed in Table 2 below, we merge the adjacent two sizes into one group. Following [12,24], we adopt peak signal-to-noise ratio (PSNR) and structured similarity index (SSIM) for quantitative analysis. Implementation Details. Our MEPNet is trained end-to-end with a batch size of 1 for 100 epochs based on PyTorch [16] on an NVIDIA Tesla V100-SMX2 GPU card. An Adam optimizer with parameters (β 1 , β 2 ) = (0.5, 0.999) is exploited. The initial learning rate is 2 × 10 -4 and it is decayed by 0.5 every 40 epochs. For a fair comparison, we adopt the same loss function as [24] and also select the total number of iterations K as 10. "
MEPNet: A Model-Driven Equivariant Proximal Network for Joint Sparse-View Reconstruction and Metal Artifact Reduction in CT Images,5.2,Performance Evaluation,"Working Mechanism. Figure 3 presents the sinogram S k and CT image X k reconstructed by MEPNet at different stages. We can easily observe that S k and X k are indeed alternatively optimized in information restoration and artifact reduction, approaching the ground truth Y gt and X gt , respectively. This finely shows a clear working mechanism of our proposed MEPNet, which evolves in the right direction specified by Eq. ( 5). Visual Comparison. Figure 4 shows the reconstructed results of different methods, including FBPConvNet [9], DuDoNet [12], InDuDoNet [24], and the proposed MEPNet, on three degraded images from DeepLesion-test with different sparse-view under-sampling rates and various sizes of metallic implants. 2 As seen, compared with these baselines, our proposed MEPNet can consistently produce cleaner outputs with stronger artifact removal and higher structural fidelity, especially around the metals, thus leading to higher PSNR/SSIM values.Figure 5 presents the cross-domain results on Pancreas-test with ×4 undersampling rate where DL-based methods are trained on synthesized DeepLesion. As seen, DuDoNet produces over-smoothed output due to the lack of physical geometry constraint on the final result. In contrast, MEPNet achieves more efficient artifact suppression and sharper detail preservation. Such favorable generalization ability is mainly brought by the dual-domain joint regularization and the fine utilization of rotation symmetry via the equivariant network, which can reduce the model parameters from 5,095,703 (InDuDoNet) to 4,723,309 (MEP-Net). Besides, as observed from the bone marked by the green box, MEPNet alleviates the rotational-structure distortion generally existing in other baselines. This finely validates the effectiveness of embedding rotation equivariance. Quantitative Evaluation. Table 1 lists the average PSNR/SSIM on three testing sets. It is easily concluded that with the increase of under-sampling rates, all these comparison methods present an obvious performance drop. Nevertheless, our MEPNet still maintains higher PSNR/SSIM scores on different testing sets, showing good superiority in generalization capability. In this paper, for the SVMAR task, we have constructed a dual-domain reconstruction model and built an unrolling model-driven equivariant network, called MEPNet, with a clear working mechanism and strong generalization ability. These merits have been substantiated by extensive experiments. Our proposed method can be easily extended to more applications, including limited-angle and low-dose reconstruction tasks. A potential limitation is that consistent with [24,36], the data pairs are generated based on the commonly-adopted protocol, which would lead to a domain gap between simulation settings and clinical scenarios. In the future, we will try to collect clinical data captured in the sparseview metal-inserted scanning configuration to evaluate our method."
MEPNet: A Model-Driven Equivariant Proximal Network for Joint Sparse-View Reconstruction and Metal Artifact Reduction in CT Images,,,
MEPNet: A Model-Driven Equivariant Proximal Network for Joint Sparse-View Reconstruction and Metal Artifact Reduction in CT Images,,Fig. 3 .,
MEPNet: A Model-Driven Equivariant Proximal Network for Joint Sparse-View Reconstruction and Metal Artifact Reduction in CT Images,,Fig. 4 .Fig. 5 .,
MEPNet: A Model-Driven Equivariant Proximal Network for Joint Sparse-View Reconstruction and Metal Artifact Reduction in CT Images,,Table 1 .,
MEPNet: A Model-Driven Equivariant Proximal Network for Joint Sparse-View Reconstruction and Metal Artifact Reduction in CT Images,,Table 2 .,
MEPNet: A Model-Driven Equivariant Proximal Network for Joint Sparse-View Reconstruction and Metal Artifact Reduction in CT Images,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5_11.
Nonuniformly Spaced Control Points Based on Variational Cardiac Image Registration,1,Introduction,"Image registration is critical for estimating cardiac motion, aiming to estimate the displacements between cardiac anatomical tissues at different time points. Generative models focus on data distribution and tend to model the underlying patterns or data distribution. They make it possible to train the network using fewer data, improve robustness when data is missing, and, most importantly, allow quantifying the uncertainty associated with the output [12,22]. Variational Bayesian (VB) [16] is commonly used in generative models. In recent years, unsupervised registration methods based on Variational Bayesian (VB) have been proposed, including point-set-based and intensity-based [2][3][4]9,14,15,28]. Pointset-based methods extract critical points from two images and simultaneously estimate the probabilistic correspondence and spatial transformation between two point sets [7,18,21,26,27,29]. In these methods, the points and their correspondence are random variables, and the transformation parameters are latent variables. Intensity-based methods estimate the distribution of parameters of transformation [8,11,13,17,19,20]. These methods extract image features and pay attention to image correspondence. This paper focuses on intensity-based VB methods.Gan et al. [11] proposed a probabilistic image registration method based on a parametric transformation model. They pointed out that the spatial locations of control points influence registration accuracy and delicately located control points can improve registration results. On the other hand, most existing priors either each dimension with identical independent distribution [17,19,20] or all dimensions obey a global distribution [8,11]. Priors with identical independent distributions are too simple to constrain the variational posterior; on the contrary, the global distribution might enforce each dimension of the variational posterior to correlate too much.To address the above issues, we propose a probabilistic model based on variational Bayesian using non-uniformly spaced control points for cardiac image registration. Details of our contributions include:-Employing nonuniformly spaced control points in a variational Bayesian image registration model improves registration accuracy. The control points are spaced on the contours of objects, and their intensity and spatial features are extracted using a network. We addressed the inherent disorder challenge in the control-points-based image registration model using CNNs, which can locate control points freely instead of only on grids. Additionally, our approach is not sensitive to location errors of control points. -The global prior is partitioned into several independent priors, which correspond to different control points. We analyzed the KL divergence between the variational posterior and the factorized prior in theory and found that properly factorized priors can close the gap with the variational posterior and increase the evidence of a lower bound in the VB model. -Our approach can provide more available information about registration uncertainty. Our uncertainty maps concentrate on the boundaries of objects instead of spreading over everywhere. It is favorable in real applications, where surgeons only pay attention to regions of interest."
Nonuniformly Spaced Control Points Based on Variational Cardiac Image Registration,2,Method,
Nonuniformly Spaced Control Points Based on Variational Cardiac Image Registration,2.1,Posterior Estimation and Prior in Variational Registration Model,"Given the source image S and the target image T , the goal of image registration is estimating the spatial transformation f z : R d → R d between S and T . The VB model used a variational posterior q(z|T, S) to approximate the intractable registration posterior p(z|T, S) by maximizing the evidence lower bound (ELBO) L(T, S) of log-likelihood p(T, S),In Eq. ( 1), the first term is expected to be significant to ensure registration accuracy. It can be expressed by the similarity between two images, such as the Boltzmann distribution with a parameter λ, p(T |z, S) ∝ exp(-λ(1sim(T, S(f z )))), where sim is the similarity measure. By using the Monte Carlo method, the first term in L(T, S) can be approximated. The second term quantifies the amount of information the model absorbed through learning. It is a measure of the complexity of the model that is expected to be small. The parametric transformation using compact radial basis functions(CSRBFs) with control points {p i } n i=1 to interpolate the dense DVF in our model as), where z = {z i } n i=1 is the latent variable, u is a pixel. ψ is the CSRBF with support r. The value of r is obtained by the distance between control points [11]. The VB-based image registration aims to estimate the variational posterior q(z|T, S) to approximate registration posterior p(z|T, S). We employed multivariate normal distribution as the variational posterior q(z|T, S) = N (μ, Diag(σ 2 )), whereis the distribution parameter of the kth element of the latent variable z.Control points {p i } n i=1 influence the DVF greatly. The network NetGI proposed by Gan et al. [11] spaced the global control points (GPs) and local control points (LPs) uniformly, which cannot describe the anatomical contours of cardiac tissues. When the control points are located in the boundary of objects, one advantage is that it is easier to extract significant features; the other advantage is that these points can dominate the DVF discriminately, which might control the DVF more delicately than uniformly spaced control points. In this paper, we employed the farthest point sampling (FPS) [10] to sample control points on the contours of LV, RV, and MYO. All these non-uniformly spaced control points (NuPs) can roughly reflect the shape of the objects, as illustrated in Fig. 1.Since GPs and NuPs are used, the latent variable can be represented as z = z u z nu, where z u and z nu correspond to GPs and NuPs, respectively. Distribution parameters of z u and z nu , denoted as (μ u , σ 2 u ) and (μ nu , σ 2 nu ), respectively, are estimated by a VAE. Since NuPs locate disorderly, it is challenging for CNNs to extract corresponding features. A specially designed VAE network NuNet  deals with this issue, as shown in Fig. 2. Our NuNet comprises an encoder and a decoder. The encoder of our NuNet contains two branches aiming to predict (μ u , σ 2 u ) and (μ nu , σ 2 nu ), respectively. The upper branch is for GPs with several convolutional layers. To obtain more representative features, the interpolation operation was employed in the feature maps. The lower branch is for NuPs. Since NuPs are disordered and diverse from each other for different image pairs. We embedded the PointNet (PN) architecture proposed by Qi et al. [23] in our NuNet. PointNet aims to extract the geometry features of a set of points without a specific order. In PointNet, two transform blocks are used to align points and features. The FeatureNet (FN) is similar to the PointNet, while the second transform is deleted because only feature matching is required. The decoder contains a CSRBF layer and an interpolation layer, where the CSRBF layer constructs a DVF using the sampled z, and the interpolation layer warps the source image S.Gan et al. [11] proposed a normal distribution p(z) = N (μ, B -1 ) as the global prior p(z). We partition the global prior as p(z) = p(z u )p(z nu ), where p(z u ) = N (0, B -1 u ) and p(z nu ) = N (0, B -1 nu ) correspond to the uniformly and non-uniformly spaced control points, respectively. We prove that the factorized prior results in a small KL divergence between the prior and the variational posterior with a high probability, which is favorable in increasing the ELBO in the VB model. Details can be referred to in the supplement. The conclusion is especially applicable to the control-points-based image registration model; it is favorable to make different control points have different priors. That implies we can regularize the variational posterior finely and control the DVF delicately. The extreme case of the prior factorization leads to the standard normal prior N (0, I). However, the standard normal prior is not conducive to estimate reasonable DVF because it makes control points independent of each other. It is contrary to the idea of CSBRF-based transformation, that is, control points that are close influence each other to the DVF."
Nonuniformly Spaced Control Points Based on Variational Cardiac Image Registration,2.2,Registration Uncertainty,"Our registration network predicts the variance of latent variables, which corresponds to the deviation of parameters of elastic transformation. It is a kind of data uncertainty. We estimate the uncertainty of DVF using its variance. The displacement of pixel u is, where A u is the local region centered at u with radius r. We found that σ 2 i of NuPs is larger than that of GPs in a statistical sense. The reason is that NuPs locate at the boundaries of objects, where large displacements occur in these areas for cardiac motion. However, cardiac motion varies subject to subject, resulting in the different displacements of points located at the boundaries of LV or RV. On the contrary, GPs distribute uniformly and locate mainly in the background with small motion in general. Correspondingly, the displacement variances of GPs are relatively small compared with that of NuPs. Moreover, when the pixel u is close to NuPs, ψ( u-pi r ) 2 is relatively large. Then, it can be concluded that the region where the NuPs are gathered generally has significant variances, such as the corner of the RV and thin myocardium. On the contrary, the regions with sparse control points, such as the background, usually have low uncertainty."
Nonuniformly Spaced Control Points Based on Variational Cardiac Image Registration,3,Experiments,
Nonuniformly Spaced Control Points Based on Variational Cardiac Image Registration,3.1,Datasets and Implement Details,"Four public datasets are used to evaluate our NuNet in experiments, including the York dataset [1], MICCAI2009 challenge dataset [24], ACDC dataset [5], and M&Ms dataset [6]. We combine the York, MICCAI2009, and ACDC as a hybrid dataset. There are 1060, 160, and 486 image pairs for training, validation, and testing in the hybrid dataset, respectively, while 1134, 266, and 859 image pairs in the M&Ms dataset, respectively. Image slices at the end-diastolic (ED) phase and the end-systolic (ES) in one cardiac cycle are the source and target images, respectively. All images are cropped as 128 × 128 containing the heart in the center of the image. The local correlation coefficient between two images is used as the similarity measure. The Dice score, bending energy (BE), the average perpendicular distance (APD, in mm), and the number of nonpositive Jacobian determinants (|J fz | ≤ 0) are used to evaluate the performance. "
Nonuniformly Spaced Control Points Based on Variational Cardiac Image Registration,3.2,Registration Results,"To compare the performance of our proposed approach, five deep learning networks, KrebsDiff [17], DalcaDiff [8], VoxelMorph [4], CycleMorph [14] and NetGI [11] are employed. Two strategies are used to extract NuPs to train and test our network, including extracting NuPs from contours of masks provided by the dataset (mNuPs) and extracting NuPs from contours of predicted results (pNuPs) using a trained U-Net [25]. mNuPs are located precisely on the contours, while pNuPs are the ones with location errors due to network performance. We denote ""Ours+training NuPs+ testing NuPs"" as our approach. For example, ""Ours+mNuPs+pNuPs"" means we use mNuPs to train our network and pNuPs to predict the registration results.Registration results of different networks on two datasets are listed in Table 1. Whether the predictive NuPs are employed for training or testing, our NuNet outperforms other networks regarding Dice and APD for two datasets. It implies that our approach is not sensitive to the location error of NuPs. NetGI had better performances on BE and the number of negative Jacobian determinants. The reason is that the influence between non-uniformly spaced control points varies in different regions, which makes it challenging to control the smoothness of DVF. Besides, the factorized prior regularizes the distribution of latent variables less, leading to more flexible DVFs. As shown in Fig. 3, our NuNet matched the contours of the myocardium and the right ventricle more accurately. Both NetGI and our NuNet achieve smoother DVFs compared with other networks."
Nonuniformly Spaced Control Points Based on Variational Cardiac Image Registration,3.3,Uncertainty,"We provide uncertainty using different hyperparameters λ for NetGI and our NuNet, as shown in Fig. 4. An image pair is input to trained networks to predict the distribution parameters of the latent variable z. Next, z is sampled 500 times to construct DVFs, and the displacement vector's magnitude deviation is used as the uncertainty of a DVF. In Fig. 4, it is observed that the uncertainty estimated by our approach concentrates on the boundaries of objects, while NetGI diffuses the uncertainty around the heart region. The reason is that our NuPs locate on the boundaries of objects, while NetGI spreads local control points uniformly in the heart region. Our approach focuses on uncertainty in specific regions, which provides more valuable uncertainty information."
Nonuniformly Spaced Control Points Based on Variational Cardiac Image Registration,3.4,Ablation Study,"To verify the effectiveness of the different modules of our network, we employ different variants of our network to conduct an ablation study. ""GPs"", ""LPs"",  ""FN"" and ""PN"" represent global control points, local control points, FeatureNet, and PointNet, respectively. Three priors with different covariance matrices are compared. The experimental ablation results are listed in Table 2. All results are average evaluations on two testing datasets using mNuPs. From the first two rows, it can be seen that the upper branch is also vital for registration, even if the background deforms slightly between the two images. By focusing on the global and the local simultaneously, the performance can be improved further, as listed in the fourth row. By comparing the results of the second and third rows, it can be concluded that the FeatureNet embedded in our lower branch can address the disorder issue of intensity features of NuPs. Besides, it is observed from the fifth and seventh rows that the spatial features boosted the performance of our NuNet. Results of the last three rows indicate that our factorized prior generates complex deformation but has little influence to BE and the number of nonpositive Jacobian determinants."
Nonuniformly Spaced Control Points Based on Variational Cardiac Image Registration,4,Conclusion,This paper addressed the issue of non-uniformly spaced control points in the VB-based image registration model for cardiac motion estimation. We employed the FPS algorithm to sample control points from the contour of the heart. The PointNetis embedded in our network to learn the intensity and spatial features. We found that the factorized prior leads to small KL divergence and is beneficial to produce more flexible DVFs. Experimental results on four datasets show that our proposed approach achieves optimal performance compared to state-ofart networks. The uncertainty estimated by our network focuses on important regions and provides more information about uncertainty in applications.
Nonuniformly Spaced Control Points Based on Variational Cardiac Image Registration,,Fig. 1 .,
Nonuniformly Spaced Control Points Based on Variational Cardiac Image Registration,,Fig. 2 .,
Nonuniformly Spaced Control Points Based on Variational Cardiac Image Registration,,Fig. 3 .,
Nonuniformly Spaced Control Points Based on Variational Cardiac Image Registration,,Fig. 4 .,
Nonuniformly Spaced Control Points Based on Variational Cardiac Image Registration,,Table 1 .,
Nonuniformly Spaced Control Points Based on Variational Cardiac Image Registration,,Table 2 .,
Nonuniformly Spaced Control Points Based on Variational Cardiac Image Registration,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 60.
An Unsupervised Multispectral Image Registration Network for Skin Diseases,1,Introduction,"Skin disease is common in clinic, which is characterized by the complexity of pathological morphology and etiology, as well as the diversity of disease types and locations. Multispectral imaging (MSI) has the characteristics of non-tissue contact puncture, no radiation and no need for exogenous contrast agents. The imaging mechanism characterizes specific, correlated and complementary tissue features, which makes it having a broad, promising and advantageous application prospect in the diagnosis of skin diseases [1]. Correspondingly, multispectral imaging also has some shortcomings. On one hand, wavelength and focal length vary with frequency, resulting in non-rigid deformation such as scaling deviation among MSI. On the other hand, the motion of imaging device or patient may introduce further deviation among images. Consequently, MSI registration, that is, the identification and mapping of the same or similar structure or content at the pixel level, is a fundamental and critical process for subsequent tasks such as image fusion, pathological analysis, disease identification and diagnosis.The difficulties for MSI registration are twofold. Firstly, conventional registration method is to register two images [2][3][4][5], while group image registration (GIR) is the joint registration of a group of related images. Current GIR research focuses on time-series MRI. MSI contains multiple images with significant and nonlinear amplitude differences and geometric distortions, not only making the pair-wise image registration not applicable, but also bringing great challenges to GIR due to the inability to take advantage of image intensity or structural similarity. Secondly, the type and location of diseases both affect the light reflection coefficient of skin tissue, making it challenging to find a general registration field (RF) for GIR.In the field of image registration, many inspiring methods based on traditional or deep learning techniques have been developed and applied to computer vision tasks in medical imaging, remote sensing, etc. Whereas, the traditional methods [6,7] are not suitable for MSI dataset with significant non-rigid deformation, gray jump, noise and other factors, which will lead to low efficiency and poor accuracy. The supervised deep learning methods [8,9] have the limitation of relying on the groundtruth of RF which is difficult to obtain in medical images. For the unsupervised deep learning method, G. Balakrishnan et al. [10] proposed a VoxelMorph framework for deformable and pairwise brain image registration based on image intensity. Y. Ye et al. [11] presented a MU-Net framework, which stacks several DNN models on multiple scales to generate a coarse-to-fine registration pipeline. L. Meng et al. [12] proposed an DSIM network for MSI registration, which utilized pyramid structure similarity loss to optimize the network and regress the homography parameters. Although the existing algorithms can achieve relatively accurate registration for images with weak or repeated texture, they are susceptible to significant and nonlinear amplitude differences and geometric distortions among MSI, and generally have the disadvantages of poor accuracy, low robustness and low efficiency in realizing group image registration, leading to unsatisfactory RF results.To address the aforementioned issues, we propose an end-to-end multispectral image registration (MSIR) network with unsupervised learning for multiple types of human skin diseases, which improves the capability of CNN architecture to learn the cross-band transformation relationship among pathological features, so as to obtain an efficient and robust RF solution. First, we design a basic adjacentband pair registration (ABPR) model, which simultaneously models a series of image pairs from adjacent bands based on CNN, and makes full use of the feature transformation relationship between images to obtain their corresponding RFs. Second, we introduce a multispectral attention module (MAM), which is used to achieve extraction and adaptive weight allocation for the high-level pathological features. Third, we design a registration field refinement module (RFRM) to obtain a general RF solution of MSI through rectifying and reconstructing the RFs learned from all adjacent-band MSI pairs. Fourth, we propose an unsupervised center-toward registration loss function, combining a similarity loss for features in the frequency domain and a smoothness loss for RF. We perform extensive experiments on a MSI dataset of multi-type skin diseases. The evaluation results demonstrate that our method not only outperforms prior arts on MSI registration task, but also contributes to the subsequent task of benign and malignant disease classification."
An Unsupervised Multispectral Image Registration Network for Skin Diseases,2,Methodology,"As shown in Fig. 1, the proposed MSIR framework consists of four main components: (1) an ABPR model to extract pixel-wise representations of corresponding features from a pair of images synchronously.  "
An Unsupervised Multispectral Image Registration Network for Skin Diseases,2.1,Adjacent-Band Pair Registration (ABPR) Model,"The ABPR model is based on Unet [13] cascaded with 2D residual blocks [14]. In the feature encoder represented by the gray part, 4 convolution blocks with size of 3 × 3 and stride of 2 are used for down-sampling, and feature maps with channel number of 16, 32, 32 and 32 are obtained. Correspondingly, in the decoder represented by the yellow part, the convolution blocks contain interpolation operations for up-sampling.The input to ABPR model is the concatenation of a pair of MSI images in adjacent bands P i ∈ R H×W ×2 (images I i and I i+1 , 1 ≤ i ≤ n -1. n is the number of bands). The module constructs a function Γ i = F ρ (P i ) to synchronously extracts the mutual transformation relationship. F represents the registration function fit by the designed Unet architecture. ρ refers to the weights and bias parameters of the kernels of the convolutional layers. Γ i ∈ R H×W ×2 is the RF between the given P i , where 2 denotes two channels along the x-axis and y-axis.Since P i in multi-band imaging has unequal contribution to the solution of the final RF, we introduce a multispectral attention module (MAM) into ABPR model, which can not only facilitate the extraction of mutual complementary non-local features between MSI, but also guide the model to pay more attention to features from specific spectra with high impact.Specifically, we carry out global average pooling operation on the high-level features F i of the encoder for image pair P i , and flatten F i into a feature vector V i . Then we obtain a feature map M ∈ R (n-1)×H ×W ×C by concatenating the accumulated n -1 feature vectors in column. The formula for reallocating attention weights is defined as [15]:where W Q , W K and W V are weights of the fully connection layers. d k represents the dimension of V i . Next, we reshape M to obtain the updated high-level features F i and continue the subsequent decoding calculation in ABPR model."
An Unsupervised Multispectral Image Registration Network for Skin Diseases,2.2,Registration Field Refinement Module (RFRM),"In order to make full use of the potential complementarity of corresponding features between cross-band images, we design a RFRM to obtain a general RF of MSI through rectifying and reconstructing a series of RFs learned from multiple adjacent-band image pairs, which is more conducive to further improving the accuracy and generalization of the MSI registration network. First, RFRM concatenates all Γ i ∈ R H×W ×2 learned from n -1 adjacentband image pairs. The obtained RF∈ R (n-1)×H×W ×2 is then refined through three 3D residual blocks, and the reliable Γ ∈ R H×W ×2 is finally generated. T Γ means a coordinate mapping that is parameterized by Γ and its spatial transformation. That is, for each pixel p ∈ I i , there is a Γ such that I i (p) and T Γ (I i )(p) are two corresponding points in adjacent bands."
An Unsupervised Multispectral Image Registration Network for Skin Diseases,2.3,Center-Toward Registration Loss Function,"The ultimate goal of image registration is to obtain a RF from MSI with significant amplitude difference and geometric distortion, so that the perceived images corrected by the RF have the best similarity with each other. The pixel similarity in spatial domain among MSI is prone to large difference due to spectral intensity, while the feature similarity in the frequency domain is more stable. In order to optimize the adaptive center-toward registration of a group of images simultaneously, we propose an unsupervised loss function, including a similarity loss for features in the frequency domain and a smoothness loss for RF. The specific scheme is described in detail as follows:(1) The first is a similarity loss function based on the residual complexity of features in the frequency domain, which is used to penalize differences in appearance and optimize the registration effect under different lighting conditions. The residual complexity loss function is defined as [16]:where m is the number of pixels of the images I and I . DCT denotes discrete cosine transform whose weight is regulated by a hyperparameter α with an empirical value of 0.05. It is worth noting that the similarity loss consists of two components. For the image with band i, we first use the fused Γ and its spatial transformation to obtain the warped image I i = T Γ (I i ). Then we evaluate its similarity to the reference image with the adjacent band I i+1 and the warped image I i+1 = T Γ (I i+1 ), which not only ensures that the transformed images do not deviate from the original spatial distribution, but also realizes center-toward registration of a group of images synchronously and uniformly. Then the total similarity loss can be obtained through adding the residual complexity results from n -1 image pairs. The formula is defined as:(2) The second is an auxiliary loss function that constrains the smoothness of RF and penalizes the local spatial variation. L smooth is constructed based on Γ i through differentiable operation on spatial gradient, and the formula is as follows [10]:where is the gradient operator calculated along the x-axis and y-axis. Then the loss function of our module L total is the weighted sum of L sim and L smooth , which is defined as:where λ is a hyperparameter used to balance the similarity and smoothness of RF, and the empirical value 4 is taken, which is consistent with the Voxelmorph [10] method. All these components are unified in the MSIR network and trained end-to-end."
An Unsupervised Multispectral Image Registration Network for Skin Diseases,3,Experiments,
An Unsupervised Multispectral Image Registration Network for Skin Diseases,3.1,Dataset and Implementation Details,"We validated the proposed MSIR network on an in-house MSI dataset containing 85 cases with multi-type skin diseases collected from our partner hospital from November 2021 to March 2022, including 36 cases with benign diseases (keloid, fibrosarcoma, cyst, lipoma, hemangioma and nevus) and 49 cases with malignant diseases (eczematoid paget disease, squamous cell carcinoma, malignant melanoma and basal cell carcinoma). Specifically, each case consists of 22 scans with wavelengths ranging from 405 nm to 1650 nm and a uniform size of 640 × 512. For each case, a clinician manually annotated four corresponding landmarks for registration on each scan, and their positions were further reviewed by an experienced medical expert. These landmarks are used to evaluate the registration accuracy. The detailed information for the training set and test set is shown in Table 1. We conducted a 4-fold cross-validation in training set to select models and hyperparameters. "
An Unsupervised Multispectral Image Registration Network for Skin Diseases,3.2,Quantitative and Qualitative Evaluation,"In this paper, we adopt three evaluation indexes to assess the registration performance of different methods, including normalized mutual information (NMI), registration feature error (RFE) and target registration point error (TRE) [17].Table 2 shows the quantitative comparisons of the registration performance among our MSIR network and three state-of-the-art competitive methods, including Voxelmorph [10], DSIM [12] framework and MU-Net [11]. The mean value of the initial TRE is 8.77 pixels. It can be observed that our method achieves the superior performance. Specifically, it improves the best NMI to 0.547, RFE to 1.166 and TRE to 4.984 pixels, which significantly outperforms the other competitors. We further conduct ablation study to verify the contributions of individual components, as shown in Table 3. NCC refers to the normalization cross correlation, which describes the relevance and similarity of targets. Compared with NCC loss, the proposed loss function significantly improves the registration performance (0.142 reduction in TRE, 0.163 reduction in RFE and 0.016 increase in NMI). The introduction of MAM further improves the registration performance. On this basis, we introduce an augmented dataset to expand the training set to 50 times of its original size by performing affine transformation operations on MSI, such as translation, rotation, scaling, clipping, oblique cutting, etc. The adoption of the augmented dataset makes the indexes continuously optimized. Quantitative results validate the effectiveness of MSIR network and augmented dataset in improving registration performance.Next, we conduct ablation experiments to explore the registration performance based on images with different subsets of bands, as illustrated in Table 4. VIS and NIR represent the visible bands (405 nm-780 nm) and the near infrared bands (780 nm-1650 nm), respectively. ALL represents the whole bands (405 nm-1650 nm). The results demonstrate that our method can achieve remarkable registration improvement for images of different bands.To verify the generalization of the proposed method, we test the model using a synthetic dataset (with |5| degrees of rotation, |0.02| of scaling, |6| and |8| pixels of translation along the x-axis and y-axis). The visualization results are shown in Fig. 2. It can be seen that MSIR method can achieve accurate registration not only for the MSI of the raw dataset, but also for that of the synthetic dataset with more complex transformation that are challenging for registration.In addition, in order to verify the effect of this registration method to subsequent tasks, we further conduct a classification task for benign and malignant diseases based on the established MSI dataset. Table 5 shows the quantitative comparisons of different classifiers using single-band and all-bands images. Four classifiers are compared, namely KNN [18], SVM [19], Resnet18 [20] and Inception V3 [21]. Columns 2 and 3 represent the worst and best classification accuracy based on single-band images, which come from the wavelengths of 450 nm and 525 nm, respectively. The fourth column is the classification results based on the original MSI dataset without registration, and the last column shows that based on the images processed by MSIR network. It can be seen that the MSI contains more abundant information than the single-band image, which is more conducive to the subsequent analysis. More importantly, due to the contribution of MSIR network for image registration, the classification accuracy on  "
An Unsupervised Multispectral Image Registration Network for Skin Diseases,4,Conclusion,"In this study, an efficient and robust framework for multispectral image registration is proposed and validated on a self-established dataset of multiple types of skin diseases, which holds great potentials for the further analysis, such as the classification of benign and malignant diseases. We intend to release the MSI dataset in future. The quantitative results of experiments demonstrate the superiority of our method over the current state-of-the-art methods."
An Unsupervised Multispectral Image Registration Network for Skin Diseases,,,
An Unsupervised Multispectral Image Registration Network for Skin Diseases,,Fig. 1 .,
An Unsupervised Multispectral Image Registration Network for Skin Diseases,,Fig. 2 .,
An Unsupervised Multispectral Image Registration Network for Skin Diseases,,Table 1 .,
An Unsupervised Multispectral Image Registration Network for Skin Diseases,,Table 2 .,
An Unsupervised Multispectral Image Registration Network for Skin Diseases,,Table 3 .,
An Unsupervised Multispectral Image Registration Network for Skin Diseases,,Table 4 .,
An Unsupervised Multispectral Image Registration Network for Skin Diseases,,Table 5 .,
An Unsupervised Multispectral Image Registration Network for Skin Diseases,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 68.
Computationally Efficient 3D MRI Reconstruction with Adaptive MLP,1,Introduction,"Compared with 2D MRI, 3D MRI has superior volumetric spatial resolution and signal-to-noise ratio. However, 3D MRI, especially high resolution (HR) 3D MRI (e.g., at least 1 mm 3 voxel size), often takes much longer acquisition time than 2D scans. Therefore, it is necessary to accelerate 3D MRI by acquiring sub-sampled k-space. However, it is more challenging to reconstruct HR 3D MRI images than 2D images. For example, HR 3D MRI data can be as large as 380×294×138×64, which is more than 100X larger than common 2D MRI data [13] (e.g., 320×320×1×15, hereafter data dimensions are defined as RO×PE×SPE×Coil, where RO stands for read-out, PE for phase-encoding, and SPE for slice-phase-encoding). Although deep learning (DL) based methods have shown superior reconstruction speed and image quality, they are constrained by GPU memory for 3D MRI reconstruction in the clinical setting. Due to the large 3D image size and computation constraint, the state-ofthe-art methods for 2D MRI reconstruction [12,20] are not directly transferable to 3D MRI reconstruction. Instead of using 3D convolutions, [1] proposed a 2D CNN on the PE-SPE plane for 3D MRI reconstruction. [31] proposed to downsample the 3D volume and reconstruct the smaller 3D image, which is then restored to the original resolution by a super-resolution network. [3,23] used 3D CNN models to reconstruct each coil of 3D MRI data independently. [11] applied the gradient checkpointing technique to save the GPU memory during training. GLEAM [21] splits the network into modules and updates the gradient on each module independently, which reduces memory usage during training.The previous works on 3D MRI reconstruction have several limitations. First, all these methods are based on CNN. In the context of 3D reconstruction, deep CNN networks require significant GPU memory and are difficult to scale. As a result, many models are designed to be relatively small to fit within available resources [1,3,23]. Given that a high-resolution 3D volume can contain over 100 million voxels, the model's fitting power is critical. Small models may lack the necessary fitting power, resulting in suboptimal performance in 3D MRI reconstruction. Second, due to network inductive bias, CNN prioritizes low-frequency information reconstruction and tends to generate smooth images [2,22]. Third, CNN has a limited receptive field due to highly localized convolutions using small kernels. The k-space sub-sampling is equivalent to convolving the underlying aliasing-free image using a kernel that covers the entire field of view (FOV) (orange arrow in Fig. 1). Therefore, the contribution of aliasing artifacts for a voxel comes from all other voxels globally in the sub-sampling directions. Then reconstruction is deconvolution and it is desirable to utilize the global information along the sub-sampled directions (green arrow in Fig. 1). Although convolution-based methods such as large kernels [17,29], dilation, deformable convolution [5] as well as attention-based methods such as Transformers [7,18] can enlarge the receptive field, it either only utilizes limited voxels within the FOV or may lead to massive computation [22]. Recently, multi-layer perceptron (MLP) based models have been proposed for various computer vision tasks [4,14,15,[25][26][27][28]30]. MLP models have better fitting power and less inductive bias than CNN models [16]. MLP performs matrix multiplication instead of convolution, leading to enlarged receptive fields with lower memory and time cost than CNN and attention-based methods. However, MLP requires a fixed input image resolution and several solutions have been proposed [4,15,16,18]. Nevertheless, these methods were proposed for natural image processing and failed to exploit global information from the entire FOV. Img2ImgMixer [19] adapted MLP-Mixer [25] to 2D MRI reconstruction but on fixed-size images. AUTOMAP [32] employs MLP on whole k-space to learn the Fourier transform, which requires massive GPU memory and a fixed input size and thus is impractical even for 2D MRI reconstruction. Fourth, the methods to reduce GPU memory are designed to optimize gradient calculation for training, which is not beneficial for inference when deployed in clinical practice.To tackle these problems, we proposed Recon3DMLP for 3D MRI reconstruction, a hybrid of CNN modules with small kernels for low-frequency reconstruction and adaptive MLP (dMLP) modules with large kernels to boost the high-frequency reconstruction. The dMLP improves the model fitting ability with almost the same GPU memory usage and a minor increase in computation time. We utilized the circular shift operation [18] based on MRI physics such that the proposed dMLP accepts arbitrary image size and can extract global information from the entire FOV. Furthermore, we propose a memory-efficient data fidelity (eDF) module that can reduce >50% memory. We also applied gradient checkpointing, RO cropping, and half-precision (FP16) to save GPU memory. We compared Recon3DMLP with other CNN-based models on an HR 3D multi-coil MRI dataset. The proposed dMLP improves HR 3D reconstruction and outperforms several existing CNN-based strategies under similar GPU memory consumption, which demonstrate that Recon3DMLP is a practical solution for HR 3D MRI reconstruction."
Computationally Efficient 3D MRI Reconstruction with Adaptive MLP,2,Method,
Computationally Efficient 3D MRI Reconstruction with Adaptive MLP,2.1,Recon3DMLP for 3D MRI Reconstruction,"The MRI reconstruction problem can be solved aswhere y is the acquired measurements, x u is the under-sampled image, M and S are the sampling mask and coil sensitivities, F denotes FFT and λ is a weighting scalar. g θ is a neural network with the data fidelity (DF) module [24].The proposed Recon3DMLP adapts the memory-friendly cascaded structure. Previous work has shown that convolutions with small kernels are essential for low-level tasks [27]. Therefore, we added the dMLP module with large kernels after each 3D CNN with small kernels (k = 3) to increase the fitting capacity and utilize the global information. "
Computationally Efficient 3D MRI Reconstruction with Adaptive MLP,2.2,Adaptive MLP for Flexible Image Resolution,"The dMLP module includes the following operations (Fig. 2): 1) circular padding, 2) image patching, 3) FC layers, 4) circular shift, 5) shift alignment and 6) cropping. The input is circular-padded in order to be cropped into patches, and the shared 1D FC layers are applied over the patch dimension. The output is then un-patched into the original image shape. Next, the circular shift is applied along the patched dimension by a step size. The circular padding and shift are based on the DFT periodicity property of images. Then operations 2-4 (FC block) are stacked several times. Due to the shift operation in each FC block, the current patch contains a portion of information from two adjacent patches in the previous FC block, which allows information exchange between patches and thus dMLP can cover the entire FOV. In the end, the shift alignment is applied to roll back the previous shifts in the image domain. The padded region is then cropped out to generate the final output. Since the sub-sampling in k-space is a linear process that can be decomposed as 1D convolutions in the image domain along each sub-sampled direction, we use 1D dMLP for 3D reconstruction."
Computationally Efficient 3D MRI Reconstruction with Adaptive MLP,2.3,Memory Efficient Data Fidelity Module,"In the naive implementation of the DF modulethe coil combined image z is broadcasted to multi-coil data (I -M )F Sz and it increases memory consumption. Instead, we can process the data coil-by-coilwhere c is the coil index. Together with eDF, we also employed RO cropping and gradient checkpointing for training and half-precision for inference."
Computationally Efficient 3D MRI Reconstruction with Adaptive MLP,2.4,Experiments,"We collected a multi-contrast HR 3D brain MRI dataset with IRB approval, ranging from 224×220×96×12 to 336×336×192×32 [3]. There are 751 3D multicoil images for training, 32 for validation, and 29 for testing.We started with a small 3D CNN model (Recon3DCNN) with an expansion factor e = 6, where the channels increase from 2 to 12 in the first convolution layer and reduce to 2 in the last layer in each cascade. We then enlarged Recon3DCNN with increased width (e = 6,12,16,24) and depth (double convolution layers in each cascade). We also replaced the 3D convolution in Recon3DCNN with depth separable convolution [10] or separate 1D convolution for each 3D dimension. We also adapted the reparameterization technique [6] for Recon3DCNN such that the residual connection can be removed during inference to reduce the GPU memory. For comparison, we also adapted a 3D version of cascaded UNet, where each UNet  has five levels with e = 4 at the initial layer and the channels were doubled at each level. To demonstrate the effectiveness of dMLP, we built Recon3DMLP by adding two 1D dMLP on PE (k = 64) and SPE (k = 16) to the smallest Recon3DCNN (e = 6). Since GELU [9,25] has larger memory overhead, we used leaky ReLU for all models. We performed ablation studies on Recon3DMLP by sharing the FC blocks among shifts, removing shifts, reducing patch size to 3 as well as replacing the dMLP with large kernel convolutions (LKconv) using k = 65 for PE and k = 17 for SPE, as well as small kernel convolutions (SKconv) using k = 3. We attempted to adapt ReconFormer1 , a transformer-based model, and Img2ImgMixer2 , an MLP based model. Both models require to specify a fixed input image size when constructing the model and failed to run on datasets with various sizes, indicating the limitation of these methods. Note that the two models were originally demonstrated on the 2D datasets with the same size [8,19]. All models were trained with loss = L1+SSIM and lr = 0.001 for 50 epochs using an NVIDIA A100 GPU with Pytorch 1.10 and CUDA 11.3. The pvalues were calculated by the Wilcoxon signed-ranks test."
Computationally Efficient 3D MRI Reconstruction with Adaptive MLP,3,Results,"We first demonstrate the benefit of eDF and FP16 inference with a small CNN model Recon3DCNN (e = 6) (first and second panels in Table 1). Without eDF and FP16, the model takes >40G inference GPU memory and fails to reconstruct the test data, which indicates the challenge of HR 3D MRI reconstruction. FP16 and eDF reduce at least 11% and 53% inference memory. However, the model with only eDF is slower than the model with only FP16. By combining eDF and FP16, the inference GPU memory is reduced by 71% to 11.5G, which makes the model feasible to be deployed with a mid-range GPU in practice. Hereafter, we applied eDF and FP16 to all models. Next, we aim to improve Recon3DCNN's performance by increasing the width and depth (third panel in Table 1 and Fig. 4). By making the model wider (increase e = 6 to e = 24), the PSNR/SSIM improves significantly (p < 10 -7 ). However, the inference GPU memory also increases by 33%. On the other hand, doubling the depth also improves the performance (p < 10 -5 ), but not as significantly as increasing the model width. Also, the former increases inference time (40%) more than the latter (21%). Also increasing the model depth does not affect the inference GPU memory. Next, we experimented with those commonly used techniques for efficient computation to modify the best CNN model Recon3DCNN (e = 24) (fourth panel in Table 1 and Fig. 4). All those variants lead to a performance drop compared to the original model (p < 10 -7 ), because such methods reduce the model's fitting capacity. Those variants also result in memory increase except Recon3DCNN with reparameterization technique. These results indicate such methods proposed for natural image processing are not suitable for HR 3D MRI reconstruction.The performance of Recon3DCNN improves when becoming larger (i.e., more parameters), which indicates CNN models lack fitting power for HR 3D MR reconstruction. Therefore, we performed an overfitting experiment where models were trained and tested on one data. Figure 3 confirms that Recon3DCNN can not overfit one test data in 10K iterations and models with better fitting ability tend to have better PSNR/SSIM (Table 1). The variants of Recon3DCNN indeed have lower fitting power than the original model. This motivates us to build Recon3DMLP by adding dMLP to Recon3DCNN (e = 6) to increase its capacity while maintaining low memory usage. Recon3DMLP has better fitting ability and less reconstruction error than all models (Figs. 3 and4). Compared to the smaller Recon3DCNN (e = 6), Recon3DMLP has similar GPU memory usage but better PSNR/SSIM (p < 10 -7 ). Compared to the larger Recon3DCNN (e = 24), Recon3DMLP has 24% less GPU memory usage and better PSNR (p < 10 -7 ) and only marginally worse SSIM (p = 0.05). The cascaded 3D UNet has less GPU memory consumption but lower fitting power, worse performance (p < 10 -7 ) and longer inference time than Recon3DCNN (e = 24) and Recon3DMLP.To investigate the source of the gain, we perform ablation studies on Recon3DMLP (last panel in Table 1). By removing the shift operations, the dMLP module can only utilize the global information within the large patch, which leads to a drop in PSNR/SSIM (p < 10 -7 ). When reducing the patch  size to 3 but keeping the shift operations such that the model can only utilize the global information through the shift operations, the performance also drops (p < 10 -7 ) but less than the previous one. This indicates the shift operations can help the model to learn the global information and thus improve the reconstruction results. Also, models with and without shift operations do not significantly differ in GPU memory and time, suggesting the shift operations are computationally efficient. By sharing the FC parameters among shifts, the model has much fewer parameters and performance drops slightly (p < 10 -7 ) while GPU memory and time are similar to the original Recon3DMLP. We also replaced the dMLP modules in Recon3DMLP with convolutions using larger kernels and small kernels, respectively. Recon3DMLP (LKconv) and Recon3DMLP (SKconv) 3have worse performance (p < 10 -3 ) as well as longer time than their counterpart Recon3DMLP and Recon3DMLP (small patch), indicating the dMLP is better than the convolutions for HR 3D MRI reconstruction. We compared the Recon3DMLP with and without dMLP modules and Fig. 5 shows that dMLP modules help to learn the high-frequency information faster."
Computationally Efficient 3D MRI Reconstruction with Adaptive MLP,4,Discussion and Conclusion,"Although MLP has been proposed for vision tasks on natural images as well as 2D MRI reconstruction with fixed input size, we are the first to present a practical solution utilizing the proposed dMLP and eDF to overcome the computational constraint for HR 3D MRI reconstruction with various sizes. Compared with CNN based models, Recon3DMLP improves image quality with a little increase in computation time and similar GPU memory usage.One limitation of our work is using the same shift and patch size without utilizing the multi-scale information. dMLP module that utilizes various patch and shift sizes will be investigated in future work. MLP-based models such as Recon3DMLP may fail if the training data is small."
Computationally Efficient 3D MRI Reconstruction with Adaptive MLP,,Fig. 1 .,
Computationally Efficient 3D MRI Reconstruction with Adaptive MLP,,Fig. 2 .,
Computationally Efficient 3D MRI Reconstruction with Adaptive MLP,,Fig. 3 .,
Computationally Efficient 3D MRI Reconstruction with Adaptive MLP,,Fig. 4 .,
Computationally Efficient 3D MRI Reconstruction with Adaptive MLP,,Fig. 5 .,
Computationally Efficient 3D MRI Reconstruction with Adaptive MLP,,Table 1 .,
Computationally Efficient 3D MRI Reconstruction with Adaptive MLP,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 19.
Transformer-Based Dual-Domain Network for Few-View Dedicated Cardiac SPECT Image Reconstructions,1,Introduction,"The GE Discovery NM Alcyone 530c/570c [1] are dedicated cardiac SPECT systems with 19 cadmium zinc telluride (CZT) detector modules designed for stationary imaging. Limited amount of angular sampling on scanners of this type could affect image quality. Due to the unique geometry of Alcyone scanners, the centers of FOV vary at different angular positions. Hence, unlike CT scanners, there is no straightforward method to combine projections at different positions on Alcyone scanners. Xie et al. [12] proposed to incorporate the displacements between centers of FOV and the rotation angles into the iterative method for multi-angle reconstructions with registration steps within each iteration. Despite its promising results, acquiring multi-angle projections on this scanner is time-consuming and inconvenient in reality. Rotating the detectors also limits the capability of dynamic imaging. Thus, it is desirable to obtain high-quality rotation-based reconstruction directly from the stationary SPECT projection data. This is essentially a few-view reconstruction problem.Previous works have attempted to address this problem by using deeplearning-based image-to-image networks. Xie et al. proposed a 3D U-net-like network to directly synthesize dense-view images from few-view counterparts [12]. Since convolutional networks have limited receptive fields due to small kernel size, Xie et al. further improved their method with a transformer-based image-to-image network for SPECT reconstruction [13]. Despite their promising reconstruction results, these methods use MLEM (maximum likelihood expectation maximization) reconstruction from one-angle few-view acquisition data as network input. The initial MLEM reconstruction may contain severe image artifacts with important image features lost during the iterative reconstruction process, thus would be challenging to be recovered with image-based methods. Learning to reconstruct images directly from the projection data could lead to improved quality.There are a few previous studies proposed to learn the mapping between raw data and images. AUTOMAP [14] utilized fully-connected layers to learn the inverse Fourier transform between k-space data and the corresponding MRI images. While such a technique could be theoretically adapted to other imaging modalities, using a similar approach would require a significant amount of trainable parameters, and thus is infeasible for 3D data. Würfl et al. [10] proposed a back-projection operator to link projections and images to reduce memory burden for CT. However, their back-projection process is not learnable. There are also recent works [5,6,11] that tried to incorporate the embedded imaging physics and geometry of CT scanners into the neural networks to reduce redundant trainable parameters for domain mapping. However, these networks are unable to be generalized to other imaging modalities due to different physical properties. Moreover, these methods are hard to be extended to 3D cases because of the geometric complexity and additional memory/computational burden.Here, we propose a novel 3D Transformer-based Dual-domain (projection & image) Network (TIP-Net) to address these challenges. The proposed method reconstructs high-quality few-view cardiac SPECT using a two-stage process. First, we develop a 3D projection-to-image transformer reconstruction network that directly reconstructs 3D images from the projection data. In the second stage, this intermediate reconstruction is combined with the original few-view reconstruction for further refinement, using an image-domain reconstruction network. Validated on physical phantoms, porcine, and human studies acquired on GE Alcyone 570c SPECT/CT scanners, TIP-Net demonstrated superior performance than previous baseline methods. Validated by cardiac catheterization (Cath) images, diagnostic results from nuclear cardiologists, and cardiac defect quantified by an FDA-510(k)-cleared software, we also show that TIP-Net produced images with higher resolution and cardiac defect contrast on human studies, as compared to previous baselines. Our method could be a clinically useful tool to improve cardiac SPECT imaging."
Transformer-Based Dual-Domain Network for Few-View Dedicated Cardiac SPECT Image Reconstructions,2,Methodology,"Following the acquisition protocol described by Xie et al. [12], we acquired a total of eight porcine and two physical phantom studies prospectively with four angles of projections. The training target in this work is four-angle data with 76 (19×4) projections and the corresponding input is one-angle data with 19 projections. Twenty clinical anonymized 99m Tc-tetrofosmin SPECT human studies were retrospectively included for evaluation. Since only one-angle data was available for the human studies, Cath images and clinical interpretations from nuclear cardiologists were used to determine the presence/absence of true cardiac defects and to assess the images reconstructed by different methods. The use of animal and human studies was approved by the Institutional Animal Care & Use Committee (IACUC) of Yale University."
Transformer-Based Dual-Domain Network for Few-View Dedicated Cardiac SPECT Image Reconstructions,2.1,Network Structure,"The overall network structure is presented in Fig. 1. TIP-Net is divided into two parts. The transformer-based [2] projection-net (P-net) aims at reconstructing 3D SPECT volumes directly from 3D projections obtained from the scanner. Information from the system matrix (IMG bp ) is also incorporated in P-net as prior knowledge for image reconstruction. IMG bp is obtained by multiplying the system matrix S ∈ R 19,456×245,000 with the projection data. The outputs from P-net (IMG p ) serve as an input for image-net (I-net).To reconstruct IMG p in P-net, we may simply project the output from the Transformer block to the size of 3D reconstructed volume (i.e., 70 × 70 × 50). However, such implementation requires a significant amount of GPU memory. To alleviate the memory burden, we proposed to learn the 3D reconstruction in a slice-by-slice manner. The output from the Transformer block is projected to a single slice (70 × 70), and the whole Transformer network (red rectangular in Fig. 1) is looped 50 times to produce a whole 3D volume (70 × 70 × 50). Since different slice has different detector sensitivity, all the 50 loops use different trainable parameters and the i th loop aims to reconstruct the i th slice in the 3D volume. Within each loop, the Transformer block takes the entire 3D projections as input to reconstruct the i th slice in the SPECT volume. With the self-attention mechanism, all 50 loops can observe the entire 3D projections for reconstruction. The 70×70 slice is then combined with IMG bp (only at i th slice), and the resized 3D projection data. The resulting feature maps (70 × 70 × 21) are fed into a shallow 2D CNN to produce a reconstructed slice at the i th loop (70 × 70 × 1), which is expected to be the i th slice in the SPECT volume. I-net takes images reconstructed by P-net (IMG p ), prior knowledge from the system matrix (IMG bp ), and images reconstructed by MLEM (IMG mlem ) using one-angle data for final reconstructions. With such a design, the network can combine information from both domains for potentially better reconstructions compared with methods that only consider single-domain data.Both 3D-CNN 1 and 3D-CNN 2 use the same network structure as the network proposed in [12], but with different trainable parameters."
Transformer-Based Dual-Domain Network for Few-View Dedicated Cardiac SPECT Image Reconstructions,2.2,"Optimization, Training, and Testing","In this work, the TIP-Net was trained using a Wasserstein Generative Adversarial Network (WGAN) with gradient penalty [4]. A typical WGAN contains 2 separate networks, one generator (G) and a discriminator (D). In this work, the network G is the TIP-Net depicted in Fig. 1. Formulated below, the overall objective function for network G includes SSIM, MAE, and Adversarial loss. where I one and I four denote images reconstructed by one-angle data and fourangle data respectively using MLEM. θ G represents trainable parameters of network G. P net (I one ) represents the output of the P-net (IMG p ). The function is formulated as below:where X and Y represent two image volumes used for calculations. MAE and SSIM represent MAE and SSIM loss functions, respectively. The Sobel operator (SO) was used to obtain edge images, and the MAE between them was included as the loss function. λ a = 0.1, λ b = 0.005, λ c = 0.8, and λ d = 0.1 were fine-tuned experimentally. Network D shares the same structure as that proposed in [12]. The Adam method [8] was used for optimizations. The parameters were initialized using the Xavier method [3]. 250 volumes of simulated 4D extended cardiac-torso (XCAT) phantoms [9] were used for network pre-training. Leaveone-out testing process was used to obtain testing results for all the studies."
Transformer-Based Dual-Domain Network for Few-View Dedicated Cardiac SPECT Image Reconstructions,2.3,Evaluations,"Reconstructed images were quantitatively evaluated using SSIM, RMSE, and PSNR. Myocardium-to-blood pool (MBP) ratios were also included for evaluation. For myocardial perfusion images, higher ratios are favorable and typically represent higher image resolution. Two ablated networks were trained and used for comparison. One network, denoted as 3D-CNN, shared the same structure as either 3D-CNN 1 or 3D-CNN 2 but only with IMG mlem as input. The other network, denoted as Dual-3D-CNN, used the same structure as the TIP-Net outlined in Fig. 1 but without any projection-related inputs. Compared with these two ablated networks, we could demonstrate the effectiveness of the proposed projection-to-image module in the TIP-Net. We also compared the TIP-Net with another transformer-based network (SSTrans-3D) [13]. Since SSTrans-3D only considers image-domain data, comparing TIP-Net with SSTrans-3D could demonstrate the effectiveness of the transformer-based network for processing projection-domain information.To compare the performance of cardiac defect quantifications, we used the FDA 510(k)-cleared Wackers-Liu Circumferential Quantification (WLCQ) software [7] to calculate the myocardial perfusion defect size (DS). For studies without cardiac defect, we should expect lower measured defect size as the uniformity of myocardium improves. For studies with cardiac defects, we should expect higher measured defect size as defect contrast improves.Cath images and cardiac polar maps are also presented. Cath is an invasive imaging technique used to determine the presence/absence of obstructive lesions that results in cardiac defects. We consider Cath as the gold standard for the defect information in human studies. The polar map is a 2D representation of the 3D volume of the left ventricle. All the metrics were calculated based on the entire 3D volumes. All the clinical descriptions of cardiac defects in this paper were confirmed by nuclear cardiologists based on SPECT images, polar maps, WLCQ quantification, and Cath images (if available)."
Transformer-Based Dual-Domain Network for Few-View Dedicated Cardiac SPECT Image Reconstructions,3,Results,
Transformer-Based Dual-Domain Network for Few-View Dedicated Cardiac SPECT Image Reconstructions,3.1,Porcine Results,"Results from one sample porcine study are presented in Fig. S1. This pig had a large post-occlusion defect created by inflating an angioplasty balloon in the left anterior descending artery. As pointed out by the green arrows in Fig. S1, all neural networks improved the defect contrast with higher MBP ratios compared with the one-angle images. The TIP-Net results were better than other network results in terms of defect contrast based on defect size measured by WLCQ. TIP-Net also produced images with clearer right ventricles, as demonstrated by the Full-width at Half Maximum (FHMW) values presented in Fig. S1.Quantitative results for 8 porcine and 2 physical phantom studies are included in Table 1. Based on paired t-tests, all the network results are statistically better than one-angle results (p < 0.001), and the TIP-Net results are statistically better than all the other three network results (p < 0.05). The average MBP ratios shown in Table 1 indicate that the proposed TIP-Net produced images with higher resolution compared with image-based networks.Average defect size measurements for porcine and physical phantom studies with cardiac defects are 35.9%, 42.5%, 42.3%, 43.6%, 47.0%, 46.2% for one-angle, 3D-CNN, Dual-3D CNN, TIP-Net, and four-angle image volumes, respectively. Images reconstructed by TIP-Net present overall higher defect contrast and the measured defect size is closest to the four-angle images.For normal porcine and physical phantom studies without cardiac defect, these numbers are 16.0%, 12.0%, 14.7%, 11.2%, 11.5%, and 10.1%. All neural networks showed improved uniformity in the myocardium with lower measured defect size. Both transformer-based methods, TIP-Net and SSTrans-3D, showed better defect quantification than other methods on these normal subjects. "
Transformer-Based Dual-Domain Network for Few-View Dedicated Cardiac SPECT Image Reconstructions,3.2,Human Results,"Chosen by a nuclear cardiologist, results from one representative human study with multiple cardiac defects are presented in Fig. 2. Note that there was no four-angle data for human studies. Based on clinical interpretations from the nuclear cardiologist and Cath results, this patient had multiple perfusion defects in the apical (blue arrows in Fig. 2) and basal (green and yellow arrows in Fig. 2) regions. As presented in Fig. 2, all the networks produced images with better defect contrasts. The proposed TIP-Net further improved the defect contrast, which is favorable and could make the clinical interpretations more confident. Another human study was selected and presented in Fig. 3. This patient had stenosis in the distal left anterior descending coronary artery, leading to a medium-sized defect in the entire apical region (light-green arrows). As validated by the Cath images, polar map, and interpretation from a nuclear cardiologist, TIP-Net produced images with higher apical defect contrast compared with other methods, potentially leading to a more confident diagnostic decision. The average MBP ratios on human studies for one-angle, 3D-CNN, Dual-3D-CNN, SSTrans-3D, and TIP-Net images are 3.13 ± 0.62, 4.08 ± 0.83, 4.31 ± 1.07, 4.17 ± 0.99, and 4.62 ± 1.29, respectively (MEAN ± STD). The higher ratios in images produced by TIP-Net typically indicate higher image resolution.14 patients in the testing data have cardiac defects, according to diagnostic results. The average defect size measurements for these patients are 16.8%, 22.6%, 21.0%, 22.4%, and 23.6% for one-angle, 3D-CNN, Dual-3D-CNN, SSTrans-3D, and TIP-Net results. The higher measured defect size of the TIP-Net indicates that the proposed TIP-Net produced images with higher defect contrast.For the other 6 patients without cardiac defects, these numbers are 11.5%, 12.8%, 13.8%, 12.4%, and 11.8%. These numbers show that TIP-Net did not introduce undesirable noise in the myocardium, maintaining the overall uniformity for normal patients. However, other deep learning methods tended to introduce non-uniformity in these normal patients and increased the defect size."
Transformer-Based Dual-Domain Network for Few-View Dedicated Cardiac SPECT Image Reconstructions,3.3,Intermediate Network Output,"To further show the effectiveness of P-net in the overall TIP-Net design, outputs from P-net (images reconstructed by the network directly from projections) are presented in Fig. S2. The presented results demonstrate that the network can learn the mapping between 3D projections to 3D image volumes directly without the iterative reconstruction process. In the porcine study, limited angular sampling introduced streak artifacts in the MLEM-reconstructed one-angle images (yellow arrows in Fig. S2). P-net produced images with fewer few-view artifacts and higher image resolution.The human study presented in Fig. S2 has an apical defect according to the diagnostic results (blue arrows in Fig. S2). However, this apical defect is barely visible in the one-angle image. P-net produced images with higher resolution and improved defect contrast. Combining both outputs (one-angle images and IMG p ), TIP-Net further enhanced the defect contrast."
Transformer-Based Dual-Domain Network for Few-View Dedicated Cardiac SPECT Image Reconstructions,4,Discussion and Conclusion,"We proposed a novel TIP-Net for 3D cardiac SPECT reconstruction. To the best of our knowledge, this work is the first attempt to learn the mapping from 3D realistic projections to 3D image volumes. Previous works in this direction [5,6,14] were 2D and only considered simulated projections with ideal conditions. 3D realistic projection data have more complex geometry are also affected by other physical factors that may not exist in simulated projections.The proposed method was tested for myocardial perfusion SPECT imaging. Validated by nuclear cardiologists, diagnostic results, Cath images, and defect size measured by WLCQ, the proposed TIP-Net produced images with higher resolution and higher defect contrast for patients with perfusion defects. For normal patients without perfusion defects, TIP-Net maintained overall uniformity in the myocardium with higher image resolution. Similar performance was observed in porcine and physical phantom studies."
Transformer-Based Dual-Domain Network for Few-View Dedicated Cardiac SPECT Image Reconstructions,,Fig. 1 .,
Transformer-Based Dual-Domain Network for Few-View Dedicated Cardiac SPECT Image Reconstructions,,,
Transformer-Based Dual-Domain Network for Few-View Dedicated Cardiac SPECT Image Reconstructions,,Fig. 2 .,
Transformer-Based Dual-Domain Network for Few-View Dedicated Cardiac SPECT Image Reconstructions,,Fig. 3 .,
Transformer-Based Dual-Domain Network for Few-View Dedicated Cardiac SPECT Image Reconstructions,,Table 1 .,
Transformer-Based Dual-Domain Network for Few-View Dedicated Cardiac SPECT Image Reconstructions,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 16.
Fast Reconstruction for Deep Learning PET Head Motion Correction,1,Introduction,"Positron emission tomography (PET) has been widely used in human brain imaging, thanks to the availability of a vast array of specific radiotracers. These compounds allow for studying various neurotransmitters and receptor dynamics for different brain targets [11]. Brain PET images are commonly used to diagnose and monitor neurodegenerative diseases, such as Alzheimer's disease, Parkinson's disease, epilepsy, and certain types of brain tumors [3]. Head motion in PET imaging reduces brain image resolution, lowers tracer distribution estimation, and introduces attenuation correction (AC) mismatch artifacts [12]. Consequently, the capability to monitor and correct head motion is of utmost importance in brain PET studies.The first step of PET head motion correction is motion tracking. When head motion information is acquired, either frame-based motion correction or eventby-event (EBE) motion correction methods can be applied in the reconstruction workflow to derive motion-free PET images. EBE motion correction provides better results for real-time motion tracking compared to frame-based methods, as the latter does not allow for correction of motion that occurs within each dynamic frame [1]. Currently, there are two main categories of head motion tracking methods, hardware-based motion tracking (HMT) and data-driven methods. For HMT, head motion is obtained from external devices. Generally, HMT systems offer accurate tracking results with high time resolution. Marker-based HMT such as Polaris Vicra (NDI, Canada) use light-reflecting markers on the patient's head and track the markers for motion correction [6]. However, Vicra is not routinely used in the clinic, as setup and calibration of the tracking device can be complicated and attaching markers to each patient increases the logistical burden of the scan. In response, some researchers began to use markerless motion tracking systems for brain PET [4,13]. These methods typically rely on the use of cameras and computer vision algorithms to detect and analyze the movement of a person's head in real-time, but these methods still require additional hardware setup. In data-driven motion tracking methods, head motion is estimated from PET reconstructions or raw data. With the development of commercial PET systems and technological advancements such as time of flight (TOF), data-driven head PET motion tracking has shown promising results in reducing motion artifacts and improving image quality. For instance, [12] developed a novel data-driven head motion detection method based on the centroid of distribution (COD) of PET 3D point cloud image (PCI). Image registration methods that seek to align two or more images offer a data-driven solution for correcting head motion. Intensity-based registration methods have been used to track head motion using good-quality PET reconstruction frames to achieve stable performance [14]. However, because of the dynamic change in PET images, current registration-based methods need to split the data into several discrete time frames, e.g., 5 min. Therefore, they will introduce a cumulative error when dealing with inter-frame motion. Finally, inspired by the development of deep learning-based registration methods, a deep learning head motion correction (DL-HMC) network using Vicra as ground truth was proposed [15]. This study achieved accurate motion tracking on single subject testing data, but showed less accurate motion predictions for multi-subject motion studies. Meanwhile, the input images were low-resolution PCIs without TOF and had large voxel spacing, which can negatively affect motion tracking accuracy.In this study, we proposed a new method to perform deep learning-based brain PET motion prediction across multiple subjects by utilizing high-resolution one-second fast reconstruction images (FRIs) with TOF. A novel encoder and data augmentation strategy was also applied to improve model performance. Ablation studies were conducted to assess the individual contributions of key method components. Multi-subject studies were conducted on a dataset of 20 subject and its results were quantitatively and qualitatively evaluated by MOLAR reconstruction studies and corresponding brain Region of Interest (ROI) Standard Uptake Values (SUV) evaluation."
Fast Reconstruction for Deep Learning PET Head Motion Correction,2,Methods,
Fast Reconstruction for Deep Learning PET Head Motion Correction,2.1,Data,"We identified 20 18 F-FPEB studies from a database of brain PET scans acquired on a conventional mCT scanner (Siemens, Germany) at the Yale PET center. All subjects are healthy controls and the mean activity injected was 3.90 ± 1.02 mCi. The mean translational and rotational motions across time and scans are 3.75 ± 6.88 mm and 3.30 ± 8.77 • , respectively. PET list-mode data and Vicra motion tracking information are available for each subject, as well as T1-weighted MR images and MR-space to PET-space transformation matrices. We consider data acquired between 60 and 90 min post injection (30 min total)."
Fast Reconstruction for Deep Learning PET Head Motion Correction,2.2,Fast Reconstruction Images,"To overcome the challenges when using low-quality, noisy PCI for motion correction, ultra-fast reconstruction techniques [14] that generate one-second dynamic fast reconstruction images (FRIs), can be utilized as input for deep learning motion correction methods. Leveraging the availability of CPU-parallel reconstruction platforms [5], we develop a reconstruction package for one-second FRI. Our proposed method employs a TOF-based projector and utilizes pure maximum-likelihood expectation maximization (MLEM) for reconstruction [10]. Attenuation correction (AC) and scatter correction are turned off to avoid AC mismatch and expensive computation. Normalization correction, random correction and decay correction are applied and the iteration number was set to two. Standard Uptake Value (SUV) calculation was also conducted to normalize the activities between different subjects. The final reconstructed image dimension is 150 × 150 × 111, with voxels spaced at 2.04 × 2.04 × 2.00 mm 3 . For comparison purposes, we also computed the same resolution PCI by TOF back-projection of the PET list-mode data along the line-of-response (LOR) with normalization for scanner sensitivity. Both FRI and PCI were resized to 96 × 96 × 64, with voxel spacing of 3.18 × 3.18 × 3.45 mm 3 . As shown in Fig. 1, the resized FRI and PCI from the same time pairs are displayed. Due to the PET corrections, the FRI quality and noise level is superior to the PCI, particularly in areas outside of the head. "
Fast Reconstruction for Deep Learning PET Head Motion Correction,2.3,Motion Correction Framework,"Network Architecture. We propose a modified version of the DL-HMC framework to learn rigid head motion in a supervised manner [15] (Fig. 1). Our proposed method uses two FRIs I ref and I mov from two different time points t ref and t mov to predict the relative rigid motion transformation between the reference and moving time points with respect to the Vicra gold-standard. Our encoder consists of 3 convolution layers with kernel size of 5 3 and an intermediate convolution layer with convolution size of 3 3 . PReLU activation layers follow each convolution. In addition, we add dropout in the regression layers with rate 0.3. In this new architecture, the embedding space consists of feature maps of size 16 × 4 × 4 × 2, which preserves the spatial information in the FRI. No padding is applied to the images or feature maps. The extracted features are fed into the fully connected regression block to predict the six translation and rotation components of the relative rigid motion. Data Augmentation. To improve the performance and generalizability of our network, we use a task-specific data augmentation strategy to expose it to more varied and diverse training data. As a rule of thumb, translations of 2-5 mm and rotations of 2 • -3 • are common and larger magnitudes are expected without restraint or if non-customized supports are used [9]. Due to our sampling strategy during model training, statistically, most of the sampled pairs will have small relative motion. However, during the inference, the relative motion between the moving frames at late time points and the reference frame at the beginning will be large due to the accumulation of motion. Therefore, the model may not be able to make accurate predictions when facing large relative motions. To take this problem into account, we perform data augmentation by simulating an additional relative motion that can be concatenated with the true relative motion. To be specific, the synthetic translation and rotation are uniformly sampled in the range of [-10, 10] mm and [-5, 5] • , respectively. The randomly simulated motion T will be applied to the moving frame to generate a synthetic moving frame T • I ref and be concatenated with the real relative motion to acquire the synthetic relative motion between the reference and the synthetic moving frame. The synthetic moving frame and the synthetic relative motion will be used for training to increase the data variability.Network Training and Inference. To train the network, we randomly sampled image pairs (t ref , t mov ) under the condition (t ref < t mov ). The network was optimized by minimizing the mean square error (MSE) between the predicted motion estimate and Vicra parameters. More specifically, the prediction error for a given pair of reference and moving clouds is defined as L( θ, θ) = θθ 2 with θ = [t x , t y , t z , r x , r y , r z ] the Vicra information for the three translational and three rotational parameters (t x , t y , t z ) and (r x , r y , r z ), respectively, and θ the network prediction. After training the model, we perform motion tracking inference by setting the image from first time point t ref = 3,600 (60 min post-injection) as the reference image and predict the motion from this reference image to all subsequent one-second image frames in the next 30 min (1,800 one-second time points)."
Fast Reconstruction for Deep Learning PET Head Motion Correction,3,Results,"We performed quantitative and qualitative experiments to validate our approach. We evaluated motion correction performance by comparing our proposed method to DL-HMC [15], intensity-based registration using the BioImage Suite (BIS) software package [7] using the one-second FRIs, and ablation studies to demonstrate the effectiveness of our design choices. We qualitatively assessed motion correction performance by reconstructing the PET images with motion tracking result and comparing to DL-HMC and the Vicra gold-standard reconstruction results. We split our dataset of 20 subjects into distinct subsets for training and testing with 14 and 6 subjects, respectively. From the training cohort, we randomly sampled 10% of the time frames to be used as a validation set. Training the network required 6,000 epochs for convergence using a minibatch size of 64. Adam optimization was used with initial learning rate set to 5e-4, γ set to 0.98, and exponential decay with a step size of 150. All computations were performed on a server with Intel Xeon Gold 5218 processors, 256 GB RAM, and an NVIDIA Quadro RTX 8000 GPU (48 GB RAM). The network was implemented in Python (v3.9) using PyTorch (v1.13.1) and MONAI (v1.0.1).Table 1. Quantitative motion correction results. We compared our proposed approach using fast reconstruction image (FRI) as input with DL-HMC and with using point cloud image (PCI) as input. An ablation study quantifies the effect of stochastic data augmentation (DA) and to standard intensity-based registration (BIS). Reported values are MSE (mean ± SD) comparing motion estimates to Vicra gold-standard."
Fast Reconstruction for Deep Learning PET Head Motion Correction,,Method,"Val. loss Test Set Total loss Translation (mm) Rotation ( Quantitative Evaluation. For quantitative comparisons of motion tracking, we compare MSE loss in the validation set and test set. We calculate MSE for the 6 parameter rigid motion as well as the translation and rotational components separately. To verify feasibility of traditional intensity-based registration method on FRIs, we use BIS with a multi-resolution hierarchical representation (3 levels) and minimize the sum of squared differences (SSD) similarity metric (Fig. S1 shows a BIS result on a example testing subject). Compared to Vicra gold-standard, BIS fails to predict the motion. We evaluate the following motion prediction methods (Table 1): (i) DL-HMC with PCI as input (DL-HMC PCI);(ii) DL-HMC with FRI as input (DL-HMC FRI); (iii) proposed network with PCI as input (Proposed PCI); (iv) proposed network with FRI as input (Proposed FRI); and (v) proposed method with FRI but without the data augmentation module (Proposed w/o DA); Results demonstrate that the proposed network with FRI input provides the best motion tracking performance in both validation and testing data. We also observes that using FRI yields a lower loss for the proposed network, indicating that high image quality enhanced the motion correction performance. For testing translation results, Proposed PCI outperforms Proposed FRI and has similar total motion loss, which indicates that the proposed network can still estimate motion on testing subjects even with noisy input. Figure 2 shows motion prediction results for different variations of the proposed method in a single test subject. These results show that the proposed FRI method is more similar to Vicra than the other methods, especially for translation in the x and z directions. However, the Proposed FRI method exhibits higher variance than other methods, which may be a result of the data augmentation distribution. Overall, our experiments demonstrate that the strategies in proposed FRI method enhance the motion tracking performance of the network.Qualitative Reconstruction Evaluation. After inference, the 6 rigid degrees of freedom transformation estimated from the network were used to reconstruct the PET images using Motion-compensation OSEM List-mode Algorithm for Resolution-Recovery Reconstruction (MOLAR) algorithm [5]. We applied PET head motion correction using the Proposed FRI model and compared with Vicra and no motion correction (NMC) reconstruction results. Figure 3 shows the reconstruction results for the same testing subject in quantitative evaluation.Based on the tracer distribution of 18 F-PEB, we selected some frames from reconstructed images to illustrate the proposed FRI motion correction performance. In general, the Proposed FRI results in qualitatively enhanced anatomical interpretation of PET images. In Fig. 3, NMC reconstruction has motion blurring on margins of the brain, while Proposed FRI reconstruction shows welldefined gyrus and sulcus comparable to Vicra reconstruction. 18 F-FPEB tracer is a metabotropic glutamate 5 receptor antagonist with moderate to high accumulation in multiple brain regions such as insula/caudate nucleus, thalamus and temporal lobe. Thus, we compared visualization of these regions among NMC, Proposed FRI and Vicra reconstructions (Fig. 3). Specifically, our Proposed FRI method yields clear delineation of the insula/caudate nucleus, thalamus, and temporal lobe nearly indistinguishable from Vicra reconstructed images.In addition, brain region of interest (ROI) analyses were also performed for quantitative use. Each subject's MR image was segmented into 74 regions using FreeSurfer software [2]. These regions were then merged into twelve large grey matter (GM) ROIs. For all testing subjects, the SUV difference of 12 ROIs from DL-HMC, Proposed PCI, and Proposed FRI reconstruction were calculated for comparison with Vicra reference (Fig. 3 right). The proposed FRI method yields the lowest absolute difference (0.8%) from Vicra images in SUV, while the absolute SUV difference for DL-HMC FRI is 1.5% and for NMC is 1.9%. Specifically, results of proposed FRI method are closest to Vicra results in regions such as thalamus, temporal lobe, insula (absolute activity difference are 0.5%, 0.7%, 0.9%, respectively), which are the target areas of 18 F-FPEB tracer with highest empirical tracer accumulation. The reconstruction and ROI evaluation results indicate that the proposed FRI method holds potential to improve clinical applicability through amelioration of PET motion correction accuracy."
Fast Reconstruction for Deep Learning PET Head Motion Correction,4,Discussion and Conclusion,"In this work, we propose a new head motion correction approach using fast reconstructions as input. The proposed method outperforms other methods in a multi-subject cohort, and ablation studies demonstrate the effectiveness of our strategies. We apply our proposed FRI motion correction to get motion-free reconstruction using MOLAR. The proposed FRI method achieves good image quality and similar ROI evaluation results compared to Vicra gold-standard HMT. In this study, we showed that conventional intensity-based registration fails at performing motion tracking on FRI data. This is likely due to the PET dynamic changes and non-optimal registration parameters. Compared with previous deep learning motion correction [15], the training speed and GPU memory usage of the proposed method are much better thanks to the proposed shallower encoder architecture and our efficient training and testing strategies. Though HMT method such as Vicra achieves good accuracy and time resolution for PET head motion tracking, two common types of Vicra failure may occur: slipping and wobbling. Our method would be robust enough to compensate for the Vicra failure. Because of the limited Vicra data, in the future, we will develop semi-supervised deep learning methods for PET head motion correction. Our study used TOF PET data because it can yield high signal to noise ratio (SNR) for both FRI and PCI due to the better location identification of photons, thus the one-second FRI still retains some essential brain structures. Limitations of this work include partial limited tracking time and low time resolution compared to HMT methods mentioned in Sect. 1. In the future, with the development of PET techniques such as depth-of-interaction [8], higher resolution and sensitivity PET will be available. Such PET will give data-driven PET motion correction a revolutionary opportunity to have more accurate tracking and higher time resolution. We plan to apply the proposed method to other datasets, developing a generalized model for multi-tracer and multi-scanner PET data."
Fast Reconstruction for Deep Learning PET Head Motion Correction,,Fig. 1 .,
Fast Reconstruction for Deep Learning PET Head Motion Correction,,Fig. 2 .,
Fast Reconstruction for Deep Learning PET Head Motion Correction,,Fig. 3 .,
Fast Reconstruction for Deep Learning PET Head Motion Correction,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 67.
DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration,1,Introduction,"Multimodal imaging has become increasingly popular in healthcare due to its ability to provide complementary anatomical and functional information. However, to fully exploit its benefits, it is crucial to perform accurate and robust registration of images acquired from different modalities. Multimodal image registration is a challenging task due to differences in image appearance, acquisition protocols, and physical properties of the modalities. This holds in particular if ultrasound (US) is involved, and has not been satisfactorily solved so far.While simple similarity measures directly based on the images' intensities such as sum of absolute (L1) or squared (L2) differences and normalized crosscorrelation (NCC) [16] work well in monomodal settings, a more sophisticated approach is needed when intensities cannot be directly correlated. Historically, a breakthrough in CT-MRI registration was achieved by Viola and Wells, who proposed Mutual Information [19]. Essentially, it abstracts the problem to the statistical concept of information theory and optimizes image-wide alignment statistics. Broken down to patch level and inspired by ultrasound physics, the Linear Correlation of Linear Combination (LC 2 ) measure has shown to work well for US to MRI or CT registration [2,22]. While dealing well with US specifics, it is not differentiable and expensive to compute.As an alternative to directly assessing similarity on the original images, various groups have proposed to first compute intermediate representations, and then align these with conventional L1 or L2 metrics [5,20]. A prominent example is the Modality-Independent Neighbourhood Descriptor (MIND) [5], which is based on image self-similarity and has with minor adaptations (denoted MIND-SSC for self-similarity context) also been applied to US problems [7]. Most recently, it has been shown that using 2D confidence maps-based weighting and adaptive normalization may further improve registration accuracy [21]. Yet, such feature descriptors are not expressive enough to cope with complex US artifacts and exhibit many local optima, therefore requiring closer initialization.More recently, multimodal registration has been approached using various Machine Learning (ML) techniques. Some of these methods involve the utilization of Convolutional Neural Networks (CNN) to extract segmentation volumes from the source data, transforming the problem into the registration of label maps [13,24]. Although these methods have demonstrated promising results, they are anatomy-specific and require the identification and labeling of structures that are visible in both modalities. Other approaches are trained using ground truth registrations to directly predict the pose [9,12] or to establish keypoint correspondences [1,11]. However, these methods are not generalizable to different anatomies or modalities. Moreover, the paucity of precise and unambiguous ground truth registration, particularly in abdominal MR-US registration, exacerbates the overfitting problem, restricting generalization even within the same modality and anatomy. It has furthermore been proposed in the past to utilize CNNs as a replacement for a similarly metric. In [3,17], the two images being registered are resampled into the same grid in each optimizer iteration, concatenated and fed into a network for similarity evaluation. While such a measure can directly be integrated into existing registration methods, it still suffers from similar limitations in terms of runtime performance and modality dependance.In contrast, we propose in this work to use a small CNN to approximate an expensive similarity metric with a straightforward dot product in its feature space. Crucially, our method does not necessitate to evaluate the CNN at every optimizer iteration. This approach combines ML and classical multimodal image registration techniques in a novel way, avoiding the common limitations of ML approaches: ground truth registration is not required, it is differentiable and computationally efficient, and generalizes well across anatomies and imaging modalities."
DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration,2,Approach,"We formulate image registration as an optimization problem of a similarity metric s between the moving image M and the fixed image F with respect to the parameters α of a spatial transformation T α : Ω → Ω. Most multi-modal similarity metrics are defined as weighted sums of local similarities computed on patches. Denoting M • T α the deformed image, the optimization target can be expressed in the following way:where w(p) is the weight assigned to the point p, s(•, •) defines a local similarity and the [•] operator extracts a patch (or a pixel) at a given spatial location. This definition encompasses SSD but also other more elaborate metrics like LC 2 or MIND. The function w is typically used to reduce the impact of patches with ambiguous content (e.g. with uniform intensities), or can be chosen to encode prior information on the target application.The core idea of our method is to approximate the similarity metric s(P 1 , P 2 ) of two image patches with a dot product φ(P 1 ), φ(P 2 ) where φ(•) is a function that extracts a feature vector, for instance in R 16 , from its input patch. When φ is a fully convolutional neural network (CNN), we can simply feed it the entire volume in order to pre-compute the feature vectors of every voxel with a single forward pass. The registration objective (Eq. 1) is then approximated asthus converting the original problem into a registration of pre-computed feature maps using a simple and differentiable dot product similarity. This approximation is based on the assumption that the CNN is approximately equivariant to the transformation, i.e.Our experiments show that this assumption (implicitly made also by other descriptors like MIND) does not present any practical impediment. Our method exhibits a large capture range and can converge over a wide range of rotations and deformations.Advantages. In contrast to many existing methods, our approach doesn't require any ground truth registration and can be trained using patches from unregistered pairs of images. This is particularly important for multi-modal deformable registration as ground truths are harder to define, especially on ultrasound. The simplicity of our training objective allows the use of a CNN with a limited number of parameters and a small receptive field. This means that the CNN has a negligible computational cost and can generalize well across anatomies and modalities: a single network can be used for all types of images and does not need to be retrained for a new task. Furthermore, the objective function (Eq. 2) can be easily differentiated without backpropagating the gradient through the CNN. This permits efficient gradient-based optimization, even when the original metric is either non-differentiable or costly to differentiate. Finally, we quantize the feature vectors to 8-bit precision further increasing the computational speed of registration without impacting accuracy."
DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration,3,Method,"We train our model to approximate the three-dimensional LC 2 similarity, as it showed good performance on a number of tasks, including ultrasound [2,22]. The LC 2 similarity quantifies whether a target patch can be approximated by a linear combination of the intensities and the gradient magnitude of the source patch. In order to reduce the sensitivity on the scale, our target is actually the average LC 2 over different radiuses of 3, 5, and 7. In order to be consistent with the original implementation of LC 2 we use the same weighting function w based on local patch variance. Note that the network will be trained only once, on a fixed dataset that is fully independent of the datasets that will be used in the evaluation (see Sect. 4).Dataset. Our neural network is trained using patches from the ""Gold Atlas -Male Pelvis -Gentle Radiotherapy"" [14] dataset, which is comprised of 18 patients each with a CT, MR T1, and MR T2 volumes. We resample each volume to a spacing of 2 mm and normalize the voxel intensities to have zero mean and standard variation of one. Since our approach is unsupervised, we don't make use of the provided registration but leave the volumes in their standard DICOM orientation. As LC 2 requires the usage of gradient magnitude in one of the modalities, we randomly pick it from either CT or MR. We would like to report that, initially, we also made use of a proprietary dataset including US volumes. However, as our investigation progressed, we observed that the incorporation of US data did not significantly contribute to the generalization capabilities of our model. Consequently, for the purpose of ensuring reproducibility, all evaluations presented in this paper exclusively pertain to the model trained solely on the public MR-CT dataset.Patch Sampling from Unregistered Datasets. For each pair of volumes (M, F ) we repeat the following procedure 5000 times: (1) Select a patch from M with probability proportional to its weight w; (2) Compute the similarity with all the patches of F ; (3) Uniformly sample t ∈ [0, 1]; (4) Pick the patch of F with similarity score closest to t. Running this procedure on our training data results in a total of 510000 pairs of patches."
DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration,,Architecture and Training.,"We use the same feed-forward 3D CNN to process all data modalities. The proposed model is composed of residual blocks [4], LeakyReLU activations [10] and uses BlurPool [25] for downsampling, resulting in a total striding factor of 4. We do not use any normalization layer, as this resulted in a reduction in performance. The output of the model is 16-channels volume with the norm of each voxel descriptor clipped at 1. The architecture consists of ten layers and a total of 90,752 parameters, making it notably smaller than many commonly utilized neural networks.Augmentation on the training data is used to make the model as robust as possible while leaving the target similarity unchanged. In particular, we apply the same random rotation to both patches, randomly change the sign and apply random linear transformation on the intensity values. We train our model for 35 epochs using the L2 loss and batch size of 256. The training converges to an average patch-wise L2 error of 0.0076 on the training set and 0.0083 on the validation set. The total training time on an NVIDIA RTX4090 GPU is 5 h, and inference on a 256 3 volume takes 70 ms. We make the training code and preprocessed data openly available online1 ."
DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration,4,Experiments and Results,"We present an evaluation of our approach across tasks involving diverse modalities and anatomies. Notably, the experimental data utilized in our analysis differs significantly from our model's training data in terms of both anatomical structures and combination of modalities. To assess the effectiveness of our method, we compare it against LC 2 , which is the metric we approximate, and MIND-SSC [7]. In all experiments, we use a Wilcoxon signed-rank test with p-value 10 -2 to establish the significance of our results.As will be demonstrated in the next subsections, our method is capable of achieving comparable levels of accuracy as LC 2 while retaining the speed and flexibility of MIND-SSC. In particular, on abdominal US registration (Sect. 4.3) our method obtains a significantly larger capture range, opening new possibilities for tackling this challenging problem."
DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration,4.1,Affine Registration of Brain US-MR,"In this experiment, we evaluate the performance of different methods for estimating affine registration of the REtroSpective Evaluation of Cerebral Tumors (RESECT) MICCAI challenge dataset [23]. This dataset consists of 22 pairs of pre-operative brain MRs and intra-operative ultrasound volumes. The initial pose of the ultrasound volumes exhibits an orientation close to the ground truth but can contain a significant translation shift. For both MIND-SSC and DISA-LC 2 , we resample the input volumes to 0.4 mm spacing and use the BFGS [18] optimizer with 500 random initializations within a range of ±10 • and ±25 mm. We report the obtained Fiducial Registration Errors (FRE) in Table 1. DISA-LC 2 is significantly better than MIND-SSC while the difference with LC 2 is not significant. In conclusion, our experiments demonstrate that the proposed DISA-LC 2 , combined with a simple optimization strategy, is capable of achieving equivalent performance to manually tuned LC 2 ."
DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration,4.2,Deformable Registration of Abdominal MR-CT,"Our second application is the Abdomen MR-CT task of the Learn2Reg challenge 2021 [8]. The dataset comprises 8 sets of MR and CT volumes, both depicting the abdominal region of a single patient and exhibiting notable deformations. We estimate dense deformation fields using the methodology outlined in [6] (without inverse consistency) which first estimates a discrete displacement using explicit search and then iteratively enforces global smoothness. Segmentation maps of anatomical structures are used to measure the quality of the registration. In particular, we compute the 25th, 50th, and 75th quantile of the Dice Similarity Coefficient (DSC) and the 95th quantile of the Hausdorff distance (HD95) between the registered label maps. We compare MIND-SCC and DISA-LC 2 used with different strides and followed by a downsampling operation that brings the spacing of the descriptors volumes to 8 mm. The hyperparameters of the registration algorithm have been manually optimized for each approach. Table 2 shows that our method obtains significantly better results than MIND-SCC on the DSC metrics while being not significantly better on HD95."
DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration,4.3,Deformable Registration of Abdominal US-CT and US-MR,"As the most challenging experiment, we finally use our method to achieve deformable registration of abdominal 3D freehand US to a CT or MR volume.We are using a heterogeneous dataset of 27 cases, comprising liver cancer patients and healthy volunteers, different ultrasound machines, as well as optical vs. electro-magnetic external tracking, and sub-costal vs. inter-costal scanning of the liver. All 3D ultrasound data sets are accurately calibrated, with overall system errors in the range of commercial ultrasound fusion options. Between 4 and 9 landmark pairs (vessel bifurcations, liver gland borders, gall bladder, kidney) were manually annotated by an expert. In order to measure the capture range, we start the registration from 50 random rigid poses around the ground truth and calculate the Fiducial Registration Error (FRE) after optimization. For local optimization, LC 2 is used in conjunction with BOBYQA [15] as in the original paper [22], while MIND-SCC and DISA-LC 2 are instead used with BFGS. Due to an excessive computation time, we don't do global optimization with LC 2 while with other methods we use BFGS with 500 random initializations within a range of ±40 • and ±150 mm. We use six parameters to define the rigid pose and two parameters to describe the deformation caused by the ultrasound probe pressure.From the results shown in Table 3 and Fig. 2, it can be noticed that the proposed method obtains a significantly larger capture range than MIND-SCC and LC 2 while being more than 300 times faster per evaluation than LC 2 (the times reported in the table include not just the optimization but also descriptor extraction). The differentiability of our objective function allows our method to converge in fewer iterations than derivative-free methods like BOBYQA. Furthermore, the evaluation speed of our objective function allows us to exhaustively search the solution space, escaping local minima and converging to the correct solution with pose and deformation parameters at once, in less than two seconds.Note that this registration problem is much more challenging than the prior two due to difficult ultrasonic visibility in the abdomen, strong deformations, and ambiguous matches of liver vasculature. Therefore, to the best of our knowledge, these results present a significant leap towards reliable and fully automatic fusion, doing away with cumbersome manual landmark placements."
DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration,5,Conclusion,"We have discovered that a complex patch-based similarity metric can be approximated with feature vectors from a CNN with particularly small architecture, using the same model for any modality. The training is unsupervised and merely requires unregistered data. After features are extracted from the volumes, the actual registration comprises a simple iterative dot-product computation, allowing for global and derivative-based optimization. This novel combination of classical image processing and machine learning elevates multi-modal registration to a new level of performance, generality, but also algorithm simplicity.We demonstrate the efficiency of our method on three different use cases with increasing complexity. In the most challenging scenario, it is possible to perform global optimization within seconds of both pose and deformation parameters, without any organ-specific distinction or successive increase of parameter sizes.While we specifically focused on developing an unsupervised and generic method, a sensible extension would be to specialize our method by including global information, such as segmentation maps, into the approximated measure or by making use of ground-truth registration during training. Finally, the cross-modality feature descriptors produced by our model could be exploited by future research for tasks different from registration such as modality synthesis or segmentation."
DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration,,Fig. 1 .,
DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration,,Fig. 2 .,
DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration,,Table 1 .,
DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration,,Table 2 .,
DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration,,Table 3 .,
DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 72.
Learned Alternating Minimization Algorithm for Dual-Domain Sparse-View CT Reconstruction,1,Introduction,"Sparse-view Computed Tomography (CT) is an important class of low-dose CT techniques for fast imaging with reduced X-ray radiation dose. Due to the significant undersampling of sinogram data, the sparse-view CT reconstruction problem is severely ill-posed. As such, applying the standard filtered-backprojection (FBP) algorithm, [1] to sparse-view CT data results in significant severe artifacts in the reconstructed images, which are unreliable for clinical use. In recent decades, variational methods have become a major class of mathematical approaches that model reconstruction as a minimization problem. The objective function of the minimization problem consists of a penalty term that measures the discrepancy between the reconstructed image and the given data and a regularization term that enforces prior knowledge or regularity of the image. Then an optimization method is applied to solve for the minimizer, which is the reconstructed image of the problem. The regularization in existing variational methods is often chosen as relatively simple functions, such as total variation (TV) [2][3][4], which is proven useful in many instances but still far from satisfaction in most real-world image reconstruction applications due to their limitations in capturing fine structures of images. Hence, it remains a very active research area in developing more accurate and effective methods for high-quality sparse-view CT reconstruction in medical imaging.Deep learning (DL) has emerged in recent years as a powerful tool for image reconstruction. Deep learning parameterizes the functions of interests, such as the mapping from incomplete and/or noisy data to reconstructed images, as deep neural networks. The parameters of the networks are learned by minimizing some loss functional that measures the mapping quality based on a sufficient amount of data samples. The use of training samples enables DL to learn more enriched features, and therefore, DL has shown tremendous success in various tasks in image reconstruction. In particular, DL has been used for medical image reconstruction applications [5][6][7][8][9][10][11][12], and experiments show that these methods often significantly outperform traditional variational methods.DL-based methods for CT reconstruction have also evolved fast in the past few years. One of the most successful DL-based approaches is known as unrolling [10,[13][14][15][16]. Unrolling methods mimic some traditional optimization schemes (such as proximal gradient descent) designed for variational methods to build the network structure but replace the term corresponding to the handcrafted regularization in the original variational model by deep networks. Most existing DL-based CT reconstruction methods use deep networks to extract features of the image or the sinogram [5,7,[9][10][11][12][17][18][19]. More recently, dual-domain methods [6,8,15,18] emerged and can further improve reconstruction quality by leveraging complementary information from both the image and sinogram domains. Despite the substantial improvements in reconstruction quality over traditional variational methods, there are concerns with these DL-based methods due to their lack of theoretical interpretation and practical robustness. In particular, these methods tend to be memory inefficient and prone to overfitting. One major reason is that these methods only superficially mimic some known optimization schemes but lose all convergence and stability guarantees.Recently, a new class of DL-based methods known as learnable descent algorithm (LDA) [16,19,20] have been developed for image reconstruction. These methods start from a variational model where the regularization can be parameterized as a deep network whose parameters can be learned. The objective function is potentially nonconvex and nonsmooth due to such parameterization. Then LDA aims to design an efficient and convergent scheme to minimize the objective function. This optimization scheme induces a highly structured deep network whose parameters are completely inherited from the learnable regularization and trained adaptively using data while retaining all convergence properties. The present work follows this approach to develop a dual-domain sparse-view CT reconstruction method. Specifically, we consider learnable regularizations for image and sinogram as composite objectives, where they unroll parallel subnetworks and extract complementary information from both domains. Unlike the existing LDA, we will design a novel adaptive scheme by modifying the alternating minimization methods [21][22][23][24][25] and incorporating the residual learning architecture to improve image quality and training efficiency."
Learned Alternating Minimization Algorithm for Dual-Domain Sparse-View CT Reconstruction,2,Learnable Variational Model,"We formulate the dual-domain reconstruction model as the following two-block minimization problem:where (x, z) are the image and sinogram to be reconstructed and s is the sparseview sinogram. The first two terms in (1) are the data fidelity and consistency, where A and P 0 z represent the Radon transform and the sparse-view sinogram, respectively, and • ≡ • 2 . The last two terms represent the regularizations, which are defined as the l 2,1 norm of the learnable convolutional feature extraction mappings in (2). If this mapping is the gradient operator, then the regularization reduces to total variation that has been widely used as a hand-crafted regularizer in image reconstruction. On the other hand, the proposed regularizers are generalizations and capable to learn in more adapted domains where the reconstructed image and sinogram become sparse:where θ 1 , θ 2 are learnable parameters. We useis the vector at position i across all channels. The feature extractor g r (•) is a CNN consisting of several convolutional operators separated by the smoothed ReLU activation function as follows:where {w r i } l i=1 denote convolution parameters with d r kernels. Kernel sizes are (3,3) and (3,15) for the image and sinogram networks, respectively. a(•) denotes smoothed ReLU activation function, which can be found in [16]."
Learned Alternating Minimization Algorithm for Dual-Domain Sparse-View CT Reconstruction,3,A Learned Alternating Minimization Algorithm,"This section formally introduces the Learned Alternating Minimization Algorithm (LAMA) to solve the nonconvex and nonsmooth minimization model (1). LAMA incorporates the residue learning structure [26] to improve the practical learning performance by avoiding gradient vanishing in the training process with convergence guarantees. The algorithm consists of three stages, as follows:The first stage of LAMA aims to reduce the nonconvex and nonsmooth problem in (1) to a nonconvex smooth optimization problem by using an appropriate smoothing procedurewhere (r, y) represents either (R, x) or (Q, z) andNote that the non-smoothness of the objective function (1) originates from the non-differentiability of the l 2,1 norm at the origin. To handle the non-smoothness, we utilize Nesterov's smoothing technique [27] as previously applied in [16].The smoothed regularizations take the form of the Huber function, effectively removing the non-smoothness aspects of the problem.The second stage solves the smoothed nonconvex problem with the fixed smoothing factor ε = ε k , i.e. min x,zwhere f (x, z) denotes the first two data fitting terms from (1). In light of the substantial improvement in practical performance by ResNet [26], we propose an inexact proximal alternating linearized minimization algorithm (PALM) [22] for solving (6). With ε = ε k > 0, the scheme of PALM [22] iswhere α k and β k are step sizes. Since the proximal point u x k+1 and u z k+1 are are difficult to compute, we approximate Q ε k (u) and R ε k (u) by their linear approximations at b k+1 and c k+1 , i.e.Then by a simple computation, u x k+1 and u z k+1 are now determined by the following formulaswhere αk = α k p k α k +p k , βk = β k q k β k +q k . In deep learning approach, the step sizes α k , αk , β k and βk can also be learned. Note that the convergence of the sequence {(u z k+1 , u x k+1 )} is not guaranteed. We proposed that if (u z k+1 , u x k+1 ) satisfy the following Sufficient Descent Conditions (SDC):for some η > 0, we accept x k+1 = u x k+1 , z k+1 = u z k+1 . If one of (10a) and (10b) is violated, we compute (v z k+1 , v x k+1 ) by the standard Block Coordinate Descent (BCD) with a simple line-search strategy to safeguard convergence: Let ᾱ, β be positive numbers in (0, 1) computeSet x k+1 = v x k+1 , z k+1 = v z k+1 , if for some δ ∈ (0, 1), the following holds:Otherwise we reduce (ᾱ, β) ← ρ(ᾱ, β) where 0 < ρ < 1, and recompute v x k+1 , v z k+1 until the condition (13) holds. The third stage checks if ∇Φ ε has been reduced enough to perform the second stage with a reduced smoothing factor ε. By gradually decreasing ε, we obtain a subsequence of the iterates that converges to a Clarke stationary point of the original nonconvex and nonsmooth problem. The algorithm is given below."
Learned Alternating Minimization Algorithm for Dual-Domain Sparse-View CT Reconstruction,,Algorithm 1. The Linearized Alternating Minimization Algorithm (LAMA),"Input: Initializations: x0, z0, δ, η, ρ, γ, ε0, σ, λ 1: "
Learned Alternating Minimization Algorithm for Dual-Domain Sparse-View CT Reconstruction,4,Network Architecture,"The architecture of the proposed multi-phase neural networks follows LAMA exactly. Hence we also use LAMA to denote the networks as each phase corresponds to each iteration in Algorithm 1. The networks inherit all the convergence properties of LAMA such that the solution is stabilized. Moreover, the algorithm effectively leverages complementary information through the inter-domain connections shown in Fig. 1 to accurately estimate the missing data. The network is also memory efficient due to parameter sharing across all phases. "
Learned Alternating Minimization Algorithm for Dual-Domain Sparse-View CT Reconstruction,5,Convergence Analysis,"Since we deal with a nonconvex and nonsmooth optimization problem, we first need to introduce the following definitions based on the generalized derivatives. ∞,∞] is locally Lipschitz. The Clarke subdifferential of f at (x, z) is defined as"
Learned Alternating Minimization Algorithm for Dual-Domain Sparse-View CT Reconstruction,,Definition 1. (Clarke subdifferential). Suppose that,"where w 1 , v 1 stands for the inner product in R n and similarly for w 2 , v 2 ."
Learned Alternating Minimization Algorithm for Dual-Domain Sparse-View CT Reconstruction,,Definition 2. (Clarke stationary point) For a locally Lipschitz function f defined as in,"We can have the following convergence result. All proofs are given in the supplementary material. Theorem 1. Let {Y k = (x k , z k )} be the sequence generated by the algorithm with arbitrary initial condition Y 0 = (x 0 , z 0 ), arbitrary ε 0 > 0 and ε tol = 0. Let { Ỹl } =: (x k l +1 , z k l +1 )} be the subsequence, where the reduction criterion in the algorithm is met for k = k l and l = 1, 2, .... Then { Ỹl } has at least one accumulation point, and each accumulation point is a Clarke stationary point."
Learned Alternating Minimization Algorithm for Dual-Domain Sparse-View CT Reconstruction,6,Experiments and Results,
Learned Alternating Minimization Algorithm for Dual-Domain Sparse-View CT Reconstruction,6.1,Initialization Network,"The initialization (x 0 , z 0 ) is obtained by passing the sparse-view sinogram s defined in (1) through a CNN consisting of five residual blocks. Each block has four convolutions with 48 channels and kernel size (3,3), which are separated by ReLU. We train the CNN for 200 epochs using MSE, then use it to synthesize full-view sinograms z 0 from s. The initial image x 0 is generated by applying FBP to z 0 . The resulting image-sinogram pairs are then provided as inputs to LAMA for the final reconstruction procedure. Note that the memory size of our method in Table 1 includes the parameters of the initialization network. Data Metric Views FBP [1] DDNet [5] LDA [16] DuDoTrans [6] Learn++ [15]  "
Learned Alternating Minimization Algorithm for Dual-Domain Sparse-View CT Reconstruction,6.2,Experiment Setup,"Our algorithm is evaluated on the ""2016 NIH-AAPM-Mayo Clinic Low-Dose CT Grand Challenge"" and the National Biomedical Imaging Archive (NBIA) datasets. We randomly select 500 and 200 image-sinogram pairs from AAPM-Mayo and NBIA, respectively, with 80% for training and 20% for testing. We evaluate algorithms using the peak signal-to-noise ratio (PSNR), structural similarity (SSIM), and the number of network parameters. The sinograms have 512 detector elements, each with 1024 evenly distributed projection views.The sinograms are downsampled into 64 or 128 views while the image size is 256 × 256, and we simulate projections and back-projections in fan-beam geometry using distance-driven algorithms [28,29] implemented in a PyTorch-based library CTLIB [30]. Given N training data pairs {(s (i) , x(i) )} N i=1 , the loss function for training the regularization networks is defined as:where μ is the weight for SSIM loss set as 0.01 for all experiments, x(i) is ground truth image, and final reconstructions are (xWe use the Adam optimizer with learning rates of 1e-4 and 6e-5 for the image and sinogram networks, respectively, and train them with a warm-up approach. The training starts with three phases for 300 epochs, then adding two phases for 200 epochs each time until the number of phases reaches 15. The algorithm is implemented in Python using the PyTorch framework. Our experiments were run on a Linux server with an NVIDIA A100 Tensor Core GPU. "
Learned Alternating Minimization Algorithm for Dual-Domain Sparse-View CT Reconstruction,6.3,Numerical and Visual Results,"We perform an ablation study to compare the reconstruction quality of LAMA and BCD defined in (11), (12) versus the number of views and phases. Figure 3 illustrates that 15 phases strike a favorable balance between accuracy and computation. The residual architecture (9) introduced in LAMA is also proven to be more effective than solely applying BCD for both datasets. As illustrated in Sect. 5, the algorithm is also equipped with the added advantage of retaining convergence guarantees.We evaluate LAMA by applying the pipeline described in Sect. 6.2 to sparseview sinograms from the test set and compare with state-of-the-art methods where the numerical results are presented in Table 1. Our method achieves superior results regarding PSNR and SSIM scores while having the second-lowest number of network parameters. The numerical results indicate the robustness and generalization ability of our approach. Additionally, we demonstrate the effectiveness of our method in preserving structural details while removing noise and artifacts through Fig. 2. More visual results are provided in the supplementary materials. Overall, our approach significantly outperforms state-of-the-art methods, as demonstrated by both numerical and visual evaluations."
Learned Alternating Minimization Algorithm for Dual-Domain Sparse-View CT Reconstruction,7,Conclusion,"We propose a novel, interpretable dual-domain sparse-view CT image reconstruction algorithm LAMA. It is a variational model with composite objectives and solves the nonsmooth and nonconvex optimization problem with convergence guarantees. By introducing learnable regularizations, our method effectively suppresses noise and artifacts while preserving structural details in the reconstructed images. The LAMA algorithm leverages complementary information from both domains to estimate missing information and improve reconstruction quality in each iteration. Our experiments demonstrate that LAMA outperforms existing methods while maintaining favorable memory efficiency."
Learned Alternating Minimization Algorithm for Dual-Domain Sparse-View CT Reconstruction,,Fig. 1 .,
Learned Alternating Minimization Algorithm for Dual-Domain Sparse-View CT Reconstruction,,Fig. 2 .,
Learned Alternating Minimization Algorithm for Dual-Domain Sparse-View CT Reconstruction,,Fig. 3 .,
Learned Alternating Minimization Algorithm for Dual-Domain Sparse-View CT Reconstruction,,Table 1 .,
Learned Alternating Minimization Algorithm for Dual-Domain Sparse-View CT Reconstruction,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 17.
StructuRegNet: Structure-Guided Multimodal 2D-3D Registration,1,Introduction,"2D-3D registration refers to the highly challenging process of aligning an input 2D image to its corresponding slice inside a given 3D volume [4]. It has received growing attention in medical imaging due to the various contexts where it applies, like image fusion between 2D real-time acquisitions and either pre-operative 3D images for guided interventions or reference planning volumes for patient positioning in radiation therapy (RT). Another important task is the volumetric reconstruction of a sequence of misaligned slices ex vivo, enabling multimodal comparison toward improved diagnosis. In this respect, overlaying 3D radiology and 2D histology could significantly enhance radiologists' understanding of the links between tissue characteristics and radiologic signals [9]. Indeed, MRI or CT scans are the baseline source of information for cancer treatment but fail to provide an accurate assessment of disease proliferation, leading to high variability in tumor detection [5,13,17]. On the other hand, high-resolution digitized histopathology, called Whole Slide Imaging (WSI), provides cell-level information on the tumor environment from the surgically resected specimens. However, the registration process is substantially difficult due to the visual characteristics, resolution scale, and dimensional differences between the two modalities. In addition, histological preparation involves tissue fixation and slicing, leading to severe collapse and out-of-plane deformations. (Semi-)automated methods have been developed to avoid time-consuming and biased manual mapping, including protocols with 3D mold or landmarks [10,22], volume reconstruction to perform 3D registration [2,18,19,23], or optimization algorithms for direct multimodal comparison [3,15]. More recently, deep learning (DL) has been introduced but is limited to 2D/2D and requires prior plane selection [20]. On the other hand, successful DL methods have been proposed to address the 2D/3D mapping problem for other medical modalities [6,8,16,21]. However, given the extreme deformation that the tissue undergoes during the histological process, additional guidance is needed. One promising solution is to rely on rigid structures that are supposedly more robust during the preparation. Structural information to guide image registration has been studied with the help of segmentations into the training loop [11], or by learning new image representations for refined mapping [12].In this paper, we propose to leverage the structural features of tissue and more particularly the rigid areas to guide the registration process with two distinct contributions: (1) a cascaded rigid alignment driven by stiff regions and coupled with recursive plane selection, and (2) an improved 2D/3D deformable motion model with distance field regularization to handle out-of-plane deformation. To our knowledge, no previous study proposed 2D/3D registration combined with structure awareness. We also use the CycleGAN for image translation and direct monomodal signal comparison [25]. Like [14,24], we combine registration with modality translation and integrate the two aforementioned components. We demonstrate superior quantitative results for Head and Neck (H&N) 3D CT and 2D WSIs than traditional approaches failing due to the histological constraints. In addition, we show that StructuRegNet performs better than the state-of-the-art model from [14] on 3D CT/2D MR for the pelvis in RT."
StructuRegNet: Structure-Guided Multimodal 2D-3D Registration,2,Methods,
StructuRegNet: Structure-Guided Multimodal 2D-3D Registration,2.1,Histology-to-CT Modality Translation,"Our three-step structure-aware pipeline is thoroughly detailed in Fig. 1. For clarity, we focus on the radiology-histology application but the pipeline is versatile to any 2D/3D setting. The modality transfer is a 2D image-to-image translation problem defined as follows: Given a sequence of n slices H = {h 1 , ..., h n } and a volume considered as a full stack of m axial slices CT = {ct 1 , ..., ct m }, we build a CycleGAN with two generators and two discriminators G H→CT , G CT →H , D H and D CT . With a symmetric situation for G CT →H , G H→CT outputs a synthetic CT image, which is then processed by D CT along with randomly sampled original input slices with an associated adversarial loss L adv . The cyclical pattern lies in the similarity between the original images and the reconstructed samples G CT →H • G H→CT (h i ) through a pixel-wise cycle loss L cyc . Finally, we employ two additional metrics: an identity loss L Id to encourage modality-specific feature representation when considering h i being the input for G CT →H with an expected identity synthesis; and a structure consistency MIND loss from [7] to ensure style transfer without content alteration. These losses are the classical implementations for CycleGAN and are detailed in the supplementary material."
StructuRegNet: Structure-Guided Multimodal 2D-3D Registration,2.2,Recursive Cascaded Plane Selection,"We replace the volume reconstruction step with a recursive dual model. We first rotate and translate the 3D CT to match the stack of slices H, which is crucial as an initialization step to help the 2D-3D network to focus on small out-of-plane deformations and avoid local minima. Then, we perform a precise plane selection and solve the spacing gap by adjusting the z-position of each slice to its most similar CT section. These two steps are performed iteratively until convergence, with a recursive algorithm to reduce computational cost (Fig. 2).For rigid initialization, the hypothesis is that the histological specimen is cut with an unknown spacing and angle, but the latter is supposed constant between WSIs. A rigid alignment is thus sufficient to reorient moving CT onto fixed H. Based on a theoretical axial slice sequence Z = (Z 1 , ..., Z m ), we define Fig. 2. Cascaded alignment through rigid structure-aware warping followed by recursive plane selection. The deformed CT from 1. is the input for 2., along with sCT and slice sequence Z. The updated Z from 2. is applied to M h while the rigid deformation is applied to MCT so that new inputs can feed 1. again as iterative refining.H as a sparse 3D volume the same size as CT , filled in with h i at z = Z i and zeros elsewhere (the same applies for the corresponding sCT from the previous module). Because soft tissues undergo too large out-of-plane deformations, we leverage the rigid structures which are supposed not to be distorted or shrunk during the histological process. We extract their segmentation masks M ct , M h for both modalities (see preprocessing in Sect. 3), concatenate and fed them into an encoder followed by a fully connected layer that outputs six transformation parameters (3 rotations, 3 translations). A differentiable spatial transform R finally warps M ct for similarity optimization with M h . Similarly to [14], we adopt a loss L rigid masked on empty slices to avoid the introduction of noise at slices within the gradient where no data is provided, and directly train on the Dice Similarity Coefficient (DSC) between rigid areas:Additionally, R also warps CT without gradient backpropagation and is the input with sCT for plane selection. We then introduce a sequence alignment problem, the objective being to update the slice sequence Z of sCT by mapping it to a corresponding sequence J of 2D images from CT . We define S a similarity matrix, where S(i, j) is the total similarity (measured with MI) when mapping sct Z1 , ..., sct Zi with ct J1 , ..., ctwhich means that each row of S will be filled by computing the sum of the MI for the corresponding column j and the maximum similarity from the last row. Like any dynamic programming method, we want to find the optimal sequence J by following the backward path of S building. To do so, we retrieve the new index j that yielded the maximized similarity for each step J = [max i (S(i, j)] j , and we update Z ← J accordingly. In addition, the J sequence cannot be too different from Z as it would induce overlap between ordered WSIs. We thus constrained the possible matching values with k ∈ [Z i -2, Z i + 2]. Based on these rigid registration and plane selection blocks, we build a cascaded module to iteratively refine the alignment where the intermediate warping becomes the new input. We defined the number of iterations as a hyperparameter to reach a good balance between computational time and similarity maximization. This dual model is crucial for initialization but does not take into account out-ofplane deformations and a perfect alignment is not accessible yet. The deformable framework bridges this gap by focusing on irregular displacements caused by tissue manipulation and refining the rigid warping."
StructuRegNet: Structure-Guided Multimodal 2D-3D Registration,2.3,Deformable 2D-3D Registration,"Given one fixed multi-slice sCT and a moving rigidly warped R(CT ) from the previous module, we adopt an architecture close to Voxelmorph [1]. Still, the rigidly warped CT = R(CT ) and the plane-adjusted sparse sCT are fed through two different encoders for independent feature extraction. The architecture is depicted in Fig. 3. Both latent representations are element-wise subtracted. A decoder is connected to both encoders and generates a displacement field Φ the same size as input images but with (x, y, z)-channels corresponding to the displacement in each spatial coordinate. A differentiable sampler D warps CT in a deformable setting, which is then compared to sCT through a masked Normalized Cross-Correlation (NCC) loss L defo :Finally, we add two sources of regularization. Soft tissues away from bones and cartilage are more subject to shrinkage or disruption, so we harness the information from the cartilage segmentation mask of CT to generate a distance transform map Δ defined as Δ(v) = min m∈MCT ||v -m|| 2 . It maps each voxel v of CT to its distance with the closest point m to the rigid area M CT . We can then control the displacement field, with close tissue being more highly constrained than isolated areas: Φ = Φ (Δ + ), where is the Hadamard product and is a hyperparameter matrix allowing small displacement even for cartilage areas for which distance transform is null. A second regularization takes the form of a loss L regu (Φ ) = v∈R 3 ||∇Φ (v)|| 2 on the volume to constrain spatial gradients and thus encourage smooth deformation, which is essential for empty slices which are excluded from L defo . The total loss is a weighted sum of L defo and L regu . "
StructuRegNet: Structure-Guided Multimodal 2D-3D Registration,3,Experiments,"Dataset and Preprocessing. Our clinical dataset consists of 108 patients for whom were acquired both a pre-operative H&N CT scan and 4 to 11 WSIs after laryngectomy (with a total amount of 849 WSIs). The theoretical spacing between each slice is 5 mm, and the typical pixel size before downsampling is 100K × 100K. Two expert radiation oncologists on CT delineated both the thyroid and cricoid cartilages for structure awareness and the Gross Tumor Volume (GTV) for clinical validation, while two expert pathologists did the same on WSIs. They then meet and agreed to place 6 landmarks for each slice at important locations (not used for training). We ended up with images of size 256 × 256 (×64 for 3D CT) of 1 mm isotropic grid space. We split the dataset patient-wise into three groups for training (64), validation (20), and testing (24). To demonstrate the performance of our model on another application, we also retrieved the datasets from [14] for pelvis 3D CT/2D MR. It is made of 451 pairs between CT and TrueFISP sequences, and 217 other pairs between CT and T2 sequences. We guided the registration thanks to the rigid left/right femoral heads and computed similarity metrics on the 7 additional organs at risk (anal canal, bladder, rectum, penile bulb, seminal vesicle, and prostate). All masks were provided by the authors and were originally segmented by internal experts.Hyperparameters. We drew our code from CycleGAN and Voxelmorph implementations with modifications explained above, and we thank the authors of MSV-RegSynNet for making their code and data available to us [1,14,25]. A detailed description of architectures and hyperparameters can be found in the supplementary material. We implemented our model with Pytorch1.13 framework and trained for 600 (800 for MR/CT) epochs with a batch size of 8 (4 for MR/CT) patients parallelized over 4 NVIDIA GTX 1080 Tis.Evaluation. We benchmarked our method against three baselines: First, to assess the benefit of modality translation over the multimodal loss, we re-used the original 3D VoxelMorph model with MIND as a multimodal metric for optimization. We also modified this approach by masking the loss function to account for the 2D-3D setting. Next, we implemented the modality translation-based MSV-RegSyn-Net and modified it for our application to measure the importance of joint structure-aware initialization and regularization. Finally, to differentiate the latter contributions, we tested two ablation studies: without the cascaded rigid mapping or without the distance field control. According to the MR/CT application in RT, we compared our model against the state-of-the-art results of MSV-RegSynNet which were computed on the same dataset."
StructuRegNet: Structure-Guided Multimodal 2D-3D Registration,4,Results,
StructuRegNet: Structure-Guided Multimodal 2D-3D Registration,4.1,Modality Translation,"Three samples from the test set are displayed in Fig. 1. From a qualitative perspective, the densities of the different tissues are well reconstructed, with rigid structures like cartilage being lighter than soft tissues or tumors. The general shape of the larynx also complies with the original radiologic images. We achieve a mean Structural Similarity (SSIM) index of 0.76/1 between both modalities, demonstrating the strong synthesis capabilities of our network compared to MSV-RegSynNet and our ablative study without initialization process, with an SSIM of 0.72 (respectively 0.69). Therefore, the cascaded rigid initialization is crucial and helps the modality translation module in getting more similar pairs of images for eased synthesis on the next pass."
StructuRegNet: Structure-Guided Multimodal 2D-3D Registration,4.2,Registration,"We present visual results in Fig. 4. The initialization enables an accurate plane selection as proved by the similar shape of cartilages in (b). Even for some severe  difficulties inherent to the histological process like a cut larynx, the model successfully maps both cartilage and soft tissue without completely tearing the CT image thanks to regularization (c-d-e). For quantitative assessment, we computed the DSC as well as the Hausdorff Distance between cartilages, and the average distance between characteristic landmarks disposed before registration(Table 1).Our method outperforms all baselines, proving the necessity of a singular approach to handle the specific case of histology. The popular Voxelmorph framework fails, and the 2D-3D adaptation demonstrates the value of the masked loss function. The superior performance of MSV-RegSynNet advocates for a modality translation-based method compared to a direct multimodal similarity criterion. In addition, the ablation studies prove the benefit of the distance field regularization and more importantly the cascaded initialization. Concerning the GPU runtime, with a 3-step cascade for initialization, the inference remains in a similar time scale to baseline methods and performs mapping in less than 3s. We also compared against MSV-RegSynNet on its own validation dataset for generalization assessment: we yielded comparable results for the first cohort and significantly better ones for the second, which proves that StructuRegNet behaves well on other modalities and that the structure awareness is an essential asset for better registration, as pelvis is a location where organs are moving. Visuals of registration results are displayed in the supplementary material. Eventually, an important clinical endpoint of our study is to compare the GTV delineated on CT with gold-standard tumor extent after co-registration to highlight systematic errors and better understand the biological environment from the radiologic signals. We show in (f) that the GTV delineated on CT overestimates the true tumor extent of around 31%, but does not always encompass the tumor with a proportion of histological tumor contained within the CT contour of 0.86. The typical error cases are the inclusion of cartilage or edema, which highlights the limitations and variability of radiology-based examinations, leading to increased toxicity or untreated areas in RT."
StructuRegNet: Structure-Guided Multimodal 2D-3D Registration,5,Discussion and Conclusion,"We introduced a novel framework for 2D/3D multimodal registration. Struc-tuRegNet leverages the structure of tissues to guide the registration through both initial plane selection and deformable regularization; it combines adversarial training for modality translation with a 2D-3D mapping setting and does not require any protocol for 3D reconstruction. It is worth noticing that even if the annotation of cartilage was manual, automating this process is not a bottleneck as the difference in contrast between soft tissue and stiff areas is clear enough to leverage any image processing tool for this task. Finally, it is entirely versatile as we designed our experiments for CT-WSI but any 3D radiological images are suitable. We achieve superior results than state-of-the-art methods in DL-based registration in a similar time scale, allowing precise mapping of both modalities and a better understanding of the tumor microenvironment. The main limitation lies in the handling of organs without any rigid areas like the prostate. Future work also includes a study with biomarkers from immunohistochemistry mapped onto radiology to go beyond binary tumor masks and move toward virtual biopsy."
StructuRegNet: Structure-Guided Multimodal 2D-3D Registration,,Fig. 1 .,
StructuRegNet: Structure-Guided Multimodal 2D-3D Registration,,Fig. 3 .,
StructuRegNet: Structure-Guided Multimodal 2D-3D Registration,,Fig. 4 .,
StructuRegNet: Structure-Guided Multimodal 2D-3D Registration,,,
StructuRegNet: Structure-Guided Multimodal 2D-3D Registration,,Table 1 .,
StructuRegNet: Structure-Guided Multimodal 2D-3D Registration,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 73.
X2Vision: 3D CT Reconstruction from Biplanar X-Rays with Deep Structure Prior,1,Introduction,"Tomographic imaging estimates body density using hundreds of X-ray projections, but it's slow and harmful to patients. Acquisition time may be too high for certain applications, and each projection adds dose to the patient. A quick, low-cost 3D estimation of internal structures using only bi-planar X-rays can revolutionize radiology, benefiting dental imaging, orthopedics, neurology, and more. This can improve image-guided therapies and preoperative planning, especially for radiotherapy, which requires precise patient positioning with minimal radiation exposure.However, this task is an ill-posed inverse problem: X-ray measurements are the result of attenuation integration across the body, which makes them very Fig. 1. Current methods vs our method. Feed-forward methods do not manage to predict a detailed and matching tomographic volume from a few projections. Iterative methods based on neural radiance fields lack prior for good reconstruction. By learning an embedding for the possible volumes, we can recover an accurate volume from very few projections with an optimization based on a Bayesian formulation.ambiguous. Traditional reconstruction methods require hundreds of projections to get sufficient constraints on the internal structures. With very few projections, it is very difficult to disentangle the structures for even coarse 3D estimation. In other words, many 3D volumes may have generated such projections a priori.Classical analytical and iterative methods [8] fail when very few projections are available. Several works have attempted to largely decrease the number of projections needed for an accurate volumetric reconstruction. Some deep learning methods [7,12,24,25,30] predict directly a 3D volume in a forward way from very few projections. The volume is however not guaranteed to be consistent with the projections and it is not clear which solution is retrieved. Other recent methods have adapted NeRFs [20] to tomographic reconstruction [23,31]. These non-learning methods show good results when the number of input projections remains higher than a dozen but fail when very few projections are provided, as our experiments in Sect. 3.3 show.As illustrated in Fig. 1, to be able to reconstruct a volume accurately given as low as two projections only, we first learn a prior on the volume. To do this, we leverage the potential of generative models to learn a low-dimensional manifold of the target body part. Given projections, we find by a Bayesian formulation the intermediate latent vectors conditioning the generative model that minimize the error between synthesized projections of our reconstruction and these input projections. Our work builds on Hong et al. [10]'s 3D style-based generative model, which we extend via a more complex network and training framework.Compared to other 3D GANs, it is proven to provide the best disentanglement of the feature space related to semantic features [2].By contrast with feed-forward methods, our approach does not require paired projections-reconstructions, which are very tedious to acquire, and it can be used with different numbers of projections and different projection geometries without retraining. Compared to NeRF-based methods, our method exploits prior knowledge from many patients to require only two projections. We evaluate our method on reconstructing cancer patients' head-and-neck CTs, which involves intricate and complicated structures. We perform several experiments to compare our method with a feed-forward-based method [30] and a recent NeRF-based method [23], which are the previous state-of-the-art methods for the very few or few projections cases, respectively.We show that our method allows to retrieve results with the finest reconstructions and better matching structures, for a variety of number of projections. To summarize, our contributions are two-fold: (i) A new paradigm for 3D reconstruction with biplanar X-rays: instead of learning to invert the measurements, we leverage a 3D style-based generative model to learn deep image priors of anatomic structures and optimize over the latent space to match the input projections; (ii) A novel unsupervised method, fast and robust to sampling ratio, source energy, angles and geometry of projections, all of which making it general for downstream applications and imaging systems."
X2Vision: 3D CT Reconstruction from Biplanar X-Rays with Deep Structure Prior,2,Method,"Figure 2 gives an overview of the pipeline we propose. We first learn the lowdimensional manifold of CT volumes of a target body region. At inference, we estimate the Maximum A Posteriori (MAP) volume on this manifold given very few projections: we find the latent vectors that minimize the error between the synthetic projections from the corresponding volume on the manifold and the real ones. In this section, we formalize the problem, describe how we learn the manifold, and detail how we optimize the latent vectors."
X2Vision: 3D CT Reconstruction from Biplanar X-Rays with Deep Structure Prior,2.1,Problem Formulation,"Given a small set of projections {I i } i , possibly as few as two, we would like to reconstruct the 3D tomographic volume v that generates these projections. This is a hard ill-posed problem, and to solve it, we need prior knowledge about the possible volumes. To do this, we look for the maximum a posteriori (MAP) estimate given the projections {I i } i :(1) Term L(v, I i ) is a log-likelihood. We take it as:Fig. 2. Our pipeline. We first learn the low-dimensional manifold of 3D structures using a generative model. Then, given projections, we find the latent vectors that minimize the error between the projections of our generation and the input projections.where A i is an operator that projects volume v under view i. We provide more details about operator A in Sect. 2.3. L p is the perceptual loss [13] between projection of v and the observed projectionIt is crucial as it is the term that embodies prior knowledge about the volume to reconstruct. As discussed in the introduction, we rely on a generative model, which we describe in the next section. Then, we describe how exactly we use this generative model for regularization term R(v) and how this changes our optimization problem."
X2Vision: 3D CT Reconstruction from Biplanar X-Rays with Deep Structure Prior,2.2,Manifold Learning,"To regularize the domain space of solutions, we leverage a style-based generative model to learn deep priors of anatomic structures. Our model relies on Style-GAN2 [15] that we extend in 3D by changing the 2D convolutions into 3D ones as done in 3DStyleGAN [10] except that we start from the StyleGAN2 architecture.Our generator G generates a volume v given a latent vector w and Gaussian noise vectors n = {n j } j : v = G(w, n). Latent vector w ∈ N (w|μ, σ) is computed from an initial latent vector z ∈ N (0, I ) mapped using a learned network m: w = m(z). w controls the global structure of the predicted volumes at different scales by its components w i , while the noise vectors n allow more fine-grained details. The mean μ and standard deviation σ of the mapped latent space can be computed by mapping over initial latent space N (0, I ) after training. The mapping network learns to disentangle the initial latent space relatively to semantic features which is crucial for the inverse problem. We train this model using the non-saturating logistic loss [5] and path length regularization [15]. For the discriminator, we use the non-saturating logistic loss with R1 regularization [19]. We implement adaptive discriminator augmentation from StyleGAN-ADA [14] to improve learning of the model's manifold with limited medical imaging data."
X2Vision: 3D CT Reconstruction from Biplanar X-Rays with Deep Structure Prior,2.3,Reconstruction from Biplanar Projections,"Since our generative model provides a volume v as a function of vectors w and n, we can reparameterize our optimization from Eq. ( 1) into:Note that by contrast with [18] for example, we optimize on the noise vectors n as well: as we discovered in our early experiments, the n are also useful to embed high-resolution details. We take our regularization term R(w, n) as:Term L w (w) =k log N (w k |μ, σ) ensures that w lies on the same distribution as during training. N (•|μ, σ) represents the density of the standard normal distribution of mean μ and standard deviation σ.Term L c (w) =i,j log M(θ i,j |0, κ) encourages the w i vectors to be collinear so to keep the generation of coarse-to-fine structures coherent. M(•; μ, κ) is the density of the Von Mises distribution of mean μ and scale κ, which we take fixed, and θ i,j = arccos( wi•wj wi wj ) is the angle between vectors w i and w j .Term L n (n) =j log N (n j |0, I ) ensures that the n j lie on the same distribution as during training, i.e., a multivariate standard normal distribution. The λ * are fixed weights.Projection Operator. In practice, we take operator A as a 3D cone beam projection that simulates X-ray attenuation across the patient, adapted from [21,27]. We model a realistic X-ray attenuation as a ray tracing projection using material and spectrum awareness:with μ(m, E) the linear attenuation coefficient of material m at energy state E that is known [11], t m the material thickness, I 0 the intensity of the source X-ray.For materials, we consider the bones and tissues that we separate by threshold on electron density. A inverts the attenuation intensities I atten to generate an X-ray along few directions successively. We make A differentiable using [21] to allow end-to-end optimization for reconstruction.3 Experiments and Results"
X2Vision: 3D CT Reconstruction from Biplanar X-Rays with Deep Structure Prior,3.1,Dataset and Preprocessing,"Manifold Learning. We trained our model with a large dataset of 3500 CTs of patients with head-and-neck cancer, more exactly 2297 patients from the publicly available The Cancer Imaging Archive (TCIA) [1,6,16,17,28,32] and 1203 from private internal data, after obtention of ethical approbations. We split this data into 3000 cases for training, 250 for validation, and 250 for testing. We focused CT scans on the head and neck region above shoulders, with a resolution of 80 × 96 × 112, and centered on the mouth after automatic segmentation using a pre-trained U-Net [22]. The CTs were preprocessed by min-max normalization after clipping between -1024 and 2000 Hounsfield Units (HU).3D Reconstruction. To evaluate our approach, we used an external private cohort of 80 patients who had undergone radiotherapy for head-and-neck cancer, with their consent. Planning CT scans were obtained for dose preparation, and CBCT scans were obtained at each treatment fraction for positioning with full gantry acquisition. As can be seen in Fig. 3 and the supplementary material, all these cases are challenging as there are large changes between the original CT scan and the CBCT scans. We identified these cases automatically by comparing the CBCTs with the planning CTs. To compare our reconstruction in the calibrated HU space, we registered the planning CTs on the CBCTs by deformable registration with MRF minimization [4]. We hence obtained 3D volumes as virtual CTs we considered as ground truths for our reconstructions after normalization. From these volumes, we generated projections using the projection module described in Sect. 2.3."
X2Vision: 3D CT Reconstruction from Biplanar X-Rays with Deep Structure Prior,3.2,Implementation Details,"Manifold Learning. We used Pytorch to implement our model, based on Style-GAN2 [15]. It has a starting base layer of 256 × 5 × 6 × 7 and includes four upsamplings with 3D convolutions and filter maps of 256, 128, 64, 32. We also used 8 fully-convolutional layers with dimension 512 and an input latent vector of dimension 512, with tanh function as output activation. To optimize our model, we used lazy regularization [15] and style mixing [15], and added a 0.2 probability for generating images without Gaussian noise to focus on embedding the most information. We augmented the discriminator with vertical and depthoriented flips, rotation, scaling, motion blur and Gaussian noise at a probability of 0.2. Our training used mixed precision on a single GPU Nvidia Geforce GTX 3090 with a batch size of 6, and we optimized the generator, discriminator, and mapping networks using Adam at learning rates 6e-5 and 1e-5 to avoid mode collapse and unstable training. After training for 4 weeks, we achieved stabilization of the Fréchet Inception Distance (FID) [9] and Multi-scale Structural Similarity (MS-SSIM) [29] on the validation set.3D Reconstruction. For the reconstruction, we performed the optimization on GPU V100 PCI-E using Adam, with learning rate of 1e-3. By grid search on the validation set, we selected the best weights that well balance between structure and fine-grained details, λ 2 = 10, λ p = 0.1, λ w = 0.1, λ c = 0.05, λ n = 10. We perform 100 optimization steps starting from the mean of the mapped latent space, which takes 25 s, enabling clinical use."
X2Vision: 3D CT Reconstruction from Biplanar X-Rays with Deep Structure Prior,3.3,Results and Discussion,"Manifold Learning. We tested our model's ability to learn the low-dimensional manifold. We used FID [9] to measure the distance between the distribution of generated volumes and real volumes, and MS-SSIM [29] to evaluate volumes' diversity and quality. We obtained a 3D FID of 46 and a MS-SSIM of 0.92. For reference, compared to 3DStyleGAN [10], our model achieved half their FID score on another brain MRI dataset, with comparable MS-SSIM. This may be due to a more complex architecture, discriminator augmentation, or simpler anatomy.Baselines. We compared our method against the main feed-forward method X2CT-GAN [30] and the neural radiance fields with prior image embedding method NeRP [23] meant for modest sparsely-sampled reconstruction. Recent methods like [24] and [12] were excluded because they provide only minor improvements compared to X2CT-GAN [30] and have similar constraints to feed-forward methods. Additionally, no public implementation is available. [26] uses a flow-based generative model, but the results are of lower quality compared to GANs and similar to X2CT-GAN [30].3D Reconstruction. To evaluate our method's performance with biplanar projections, we focused on positioning imaging for radiotherapy. Figure 3 compares our reconstruction with those of the baselines from biplanar projections. Our method achieves better fitting of the patient structure, including bones, tissues, and air separations, almost matching the real CT volume. X2CT-GAN [30] produced realistic structures, but failed to match the actual structures as it does not enforce consistency with the projections. In some clinical procedures, an earlier CT volume of the patient may be available and can be used as an additional input for NeRP [23]. Without a previous CT volume, NeRP lacks the necessary prior to accurately solve the ill-posed problem. Even when initialised with a previous CT volume, NeRP often fails to converge to the correct volume and introduces many artifacts when few projections are used. In contrast, our method is more versatile and produces better results. We used quantitative metrics (PSNR and SSIM) to evaluate reconstruction error and human perception, respectively. Table 1 shows these metrics for our method and baselines with 1 to 8 cone beam projections. Deviation from projections, as in X2CT-GAN, leads to inaccurate reconstruction. However, relying solely on projection consistency is inadequate for this ill-posed problem. NeRP matches projections but cannot reconstruct the volume correctly. Our approach balances between instant and iterative methods by providing a reconstruction in 25 s with 100 optimization steps, while ensuring maximal consistency. In contrast, NeRP requires 7 min, and X2CT-GAN produces structures instantly but unmatching. Clinical CBCT acquisition and reconstruction by FDK [3] take about 1-2 min and 10 s respectively. Our approach significantly reduces clin-ical time and radiation dose by using instant biplanar projections, making it promising for fast 3D visualization towards complex positioning."
X2Vision: 3D CT Reconstruction from Biplanar X-Rays with Deep Structure Prior,4,"Conclusion, Limitations, and Future Work","We proposed a new unsupervised method for 3D reconstruction from biplanar X-rays using a deep generative model to learn the structure manifold and retrieve the maximum a posteriori volume with the projections, leading to stateof-the-art reconstruction. Our approach is fast, robust, and applicable to various human body parts, making it suitable for many clinical applications, including positioning and visualization with reduced radiation.Future hardware improvements may increase resolution, and our approach could benefit from other generative models like latent diffusion models. This approach may provide coarse reconstructions for patients with rare abnormalities, as most learning methods, but a larger dataset or developing a prior including tissue abnormalities could improve robustness."
X2Vision: 3D CT Reconstruction from Biplanar X-Rays with Deep Structure Prior,,Fig. 3 .,
X2Vision: 3D CT Reconstruction from Biplanar X-Rays with Deep Structure Prior,,,
X2Vision: 3D CT Reconstruction from Biplanar X-Rays with Deep Structure Prior,,,
X2Vision: 3D CT Reconstruction from Biplanar X-Rays with Deep Structure Prior,,Table 1 .,
X2Vision: 3D CT Reconstruction from Biplanar X-Rays with Deep Structure Prior,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5_66.
CortexMorph: Fast Cortical Thickness Estimation via Diffeomorphic Registration Using VoxelMorph,1,Introduction,"Cortical thickness (CTh) is a crucial biomarker of various neurological and psychiatric disorders, making it a primary focus in neuroimaging research. The cortex, a thin ribbon of grey matter at the outer surface of the cerebrum, plays a vital role in cognitive, sensory, and motor functions, and its thickness has been linked to a wide range of neurological and psychiatric conditions, including Alzheimer's disease, multiple sclerosis, schizophrenia, and depression, among others. Structural magnetic resonance imaging (MRI) is the primary modality used to investigate CTh, and numerous computational methods have been developed to estimate this thickness on the sub-millimeter scale. Among these, surface-based methods like Freesurfer [5,6] have been widely used, but they are computationally intensive, making them less feasible for clinical applications. Optimizations based on Deep Learning have brought the running time for a modified Freesurfer pipeline down to one hour. [7] The DiReCT method [4] offers an alternative to surface-based morphometry methods, calculating CTh via a diffeomorphic deformation of the gray-white matter interface (GWI) towards the pial surface (the outer edge of the cortical band). The ANTs package of neuroimaging tools provides an implementation of DiReCT via the function KellyKapowski: for readablility we refer below to KellyKapowski with its default parameters as ANTs-DiReCT. The ANTs cortical thickness pipeline uses ANTs-DiReCT together with a three-class segmentation (grey matter, white matter, cerebrospinal fluid) provided by the Atropos segmentation method, taking between 4 and 15 h depending on the settings and available hardware [1,15]. A more recent version of ANTs provides a deep-learning based alternative to Atropos, giving comparable results to ANTs but accelerating the overall pipeline to approximately one hour, such that now the running time is dominated by the time needed to run ANTs-DiReCT [16]. Meanwhile, Rebsamen et al. have shown that applying DiReCT to the output of a deep-learning-based segmentation model trained on Freesurfer segmentations (rather than Atropos) yields a CTh method which agrees strongly with Freesurfer, while having improved repeatability on repeated scans [12]. Subsequently, a digital phantom using GAN-generated scans with simulated cortical atrophy showed that the method of Rebsamen et al. is more sensitive to cortical thinning than Freesurfer [13].The long running time of methods for determining CTh remains a barrier to application in clinical routine: a running time of one hour, while a substantial improvement over Freesurfer and ANTs cortical thickness, is still far beyond the real-time processing desirable for on-demand cortical morphometry in clinical applications. In terms of both the speed and performance, VoxelMorph and related models are known to outperform classical deformable registration methods, suggesting that a DiReCT-style CTh algorithm based on unsupervised registration models may enable faster CTh estimation. [2,3,18] In this paper, we demonstrate that a VoxelMorph style model can be trained to produce a diffeomorphism taking the GWI to the pial surface, and that this model can be used to perform DiReCT-style CTh estimation in seconds. We trained the model on 320 segmentations derived from the IXI and ADNI datasets, and demonstrate excellent agreement with ANTs-DiReCT on the OASIS-3 dataset. Our model also shows improved performance on the digital CTh phantom of Rusak et al. [13].Fig. 1. End-to-end unsupervised architecture for DiReCT: velocity field z is regressed from WM and WM+GM segmentations, using a Unet. This velocity field is then integrated by seven scaling and squaring layers ( ) to yield forward and reverse deformation fields φz and φ-z, which are used to deform the input images in spatial transformer (ST) blocks. Components of the loss function are marked in orange."
CortexMorph: Fast Cortical Thickness Estimation via Diffeomorphic Registration Using VoxelMorph,2,Methods,
CortexMorph: Fast Cortical Thickness Estimation via Diffeomorphic Registration Using VoxelMorph,2.1,DiReCT Cortical Thickness Estimation,"The estimation of CTh using the DiReCT method [4] proceeds as follows: first a (partial volume) segmentation of the cortical white matter (WM) and cortical grey matter (GM) is obtained. Second, a forward deformation field φ mapping the white-matter (WM) image towards the WM+GM image is computed. This forward deformation field should be a diffeomorphism, in order that the deformation field is invertible and the topology of the inferred pial surface is the same as the GWI. Third, the diffeormorphism is inverted to obtain the reverse the deformation field, taking the pial surface towards the GWI. Finally, the CTh is determined by computing the magnitude of the reverse field at the GWI: specifically, at each voxel of WM adjacent to the GM. In ANTs-DiReCT, the forward transform (from WM to WM+GM) is calculated by a modified greedy algorithm, in which the WM surface is propagated iteratively in the direction of the surface normal until it reaches the outer GM surface or a predefined spatial prior maximum is reached. The approximate inverse field is then determined by numerical means using kernel based splines (as implemented in ITK).The absence of a reliable gold-standard ground truth for CTh makes comparisons between methods difficult. This situation has recently been improved by the publication of a synthetic cortical atrophy phantom: a dataset generated using a GAN conditioned on subvoxel segmentations, consisting of 20 synthetic subjects with 19 induced sub-voxel atrophy levels per subject (ten evenly spaced atrophy levels from 0 to 0.1 mm, and a further nine evenly spaced atrophy levels from 0.1 mm to 1 mm). [13] The purpose of this digital phantom is to explore the ability of CTh algorithms to resolve subtle changes of CTh. The paper of Rusak et al. analyzed the performance of several CTh methods on this dataset, finding that the DL+DiReCT method [12] (which combines a deep network trained on Freesurfer annotations with ANTs-DiReCT) was the most sensitive to cortical atrophy and had the best agreement with the synthetically induced thinning."
CortexMorph: Fast Cortical Thickness Estimation via Diffeomorphic Registration Using VoxelMorph,2.2,CortexMorph: VoxelMorph for DiReCT,"The original VoxelMorph architecture, introduced in [3], utilized a Unet architecture to directly regress a displacement field from a fixed brain image and a moving brain image. Application of a spatial transform layer allows the moving image to be transformed to the space of the fixed image, and compared using a differentiable similarity metric such as mean squared error or cross-correlation. Since the spatial transformation is also a differentiable operation, the network can be trained end-to-end. Later adaptations of the concept employed a regression of a stationary velocity field, with the deformation field being calculated via an integration layer: the principal advantage of this formulation is that integrating through a velocity field yields a diffeomorphism. [2] Since diffeomorphic registration is required in the DiReCT method, we adopt this velocity-field form of VoxelMorph for our purposes.The setup of our VoxelMorph architecture, CortexMorph, is detailed in Fig. 1. The two inputs to the network are a partial volume segmentation of white matter (WM), and a partial volume segmentation of grey matter plus white matter (WM+GM). These are fed as entries into a Unet, the output of which is a velocity field z, which is then integrated using 7 steps of scaling and squaring to yield a displacement field φ z . This displacement field is then applied to the WM image to yield the deformed white matter volume WM•φ z . By integrating -z we obtain the reverse deformation field φ -z , which is applied to the WM+GM image to obtain a deformed volume (WM + GM) • φ -z . This simplifies the DiReCT method substantially: instead of needing to perform a numerical inversion of the deformation field, the reverse deformation field can be calculated directly. The deformed volumes are then compared using a loss function L to their nondeformed counterparts: both directions of deformation are weighted equally in the final objective function. To encourage smoothness, a discrete approximation of the squared gradient magnitude of the velocity field L smooth is added to the loss as a regularizer. [2] As a result, our loss has the following form"
CortexMorph: Fast Cortical Thickness Estimation via Diffeomorphic Registration Using VoxelMorph,2.3,Data and WM/GM Segmentation,"Training data and validation for our VoxelMorph model was derived from two publicly available sources: images from 200 randomly selected elderly individuals from the ADNI dataset [10] and images from 200 randomly selected healthy adults from the IXI dataset (https://brain-development.org/ixi-dataset). From each of these datasets, 160 images were randomly chosen to serve as training data, yielding in total 320 training cases and 80 validation cases. For testing our pipeline, we use two sources different from the training/validation data: the well-known OASIS-3 dataset (2,643 scans of 1,038 subjects, acquired over >10 years on three different Siemens scanners), and the CTh phantom of Rusak et al. [13,14] For WM/GM segmentation, we employed the DeepSCAN model [11,12], which is available as part of DL+DiReCT (https://github.com/SCAN-NRAD/ DL-DiReCT), since this is already known to give high-quality CTh results when combined with ANTs-DiReCT. This model takes as input a T1-weighted image, performs resampling and skull-stripping if necessary (provided by HD-BET [9]) and produces a partial volume segmentation P w of the white matter and P g of the cortex (the necessary inputs to the DiReCT algorithm) with 1mm isovoxel resolution. It also produces a cortical parcellation in the same space (necessary to calculate region-wise CTh measures). We applied this model to the training data, validation data, and the 400 synthetic MRI cases of the CTh phantom, both to produce ANTs-DiReCT CTh measurements and also as an input to our VoxelMorph models."
CortexMorph: Fast Cortical Thickness Estimation via Diffeomorphic Registration Using VoxelMorph,2.4,Training and Model Selection,"Our network was implemented and trained in Pytorch (1.13.1). We utilized a standard Unet (derived from the nnUnet framework [8]) with 3 pooling steps and a feature depth of 24 features at each resolution. The spatial transformer/squaring and scaling layers/gradient magnitude loss were incorporated from the official VoxelMorph repository. For the loss function L we tested both L1 loss and mean squared error (MSE). We tested values of the smoothness parameter lambda between 0 and 0.05. The models were trained with the Adam optimizer, with a fixed learning rate of 10 -3 and weight decay 10 -5 . Patches of size 128 3 were used as training data in batches of size 2.The training regime was fully unsupervised with respect to cortical thickness: neither the deformation fields yielded by ANTs-DiReCT nor the CTh results computed from those deformation fields were used in the objective function. Since we are interested in replacing the iterative implementation of DiReCT with a deep learning counterpart, we used the 80 validation examples for model selection, selecting the model which showed best agreement in mean global CTh with the results of ANTs-DiReCT. The metric for agreement chosen is intraclass correlation coefficient, specifically ICC(2,1) (the proportion of variation explained by the individual in a random effects model, assuming equal means of the two CTh measurement techniques), since this method is sensitive to both absolute agreement and relative consistency of the measured quantity. ICC was calculated using the python package Pingouin [17]."
CortexMorph: Fast Cortical Thickness Estimation via Diffeomorphic Registration Using VoxelMorph,2.5,Testing,"The VoxelMorph model which agreed best with ANTs-DiReCT on the validation set was applied to segmentations of the OASIS-3 dataset, to confirm whether model selection on a small set of validation data would induce good agreement with ANTs-DiReCT on a much larger test set (metric, ICC(2,1)) and to the synthetic CTh phantom of Rusak et al., to determine whether the VoxelMorph model is able to distinguish subvoxel changes in CTh (metric, coefficient of determination (R 2 ))."
CortexMorph: Fast Cortical Thickness Estimation via Diffeomorphic Registration Using VoxelMorph,3,Results,"The best performing model on the validation set (in terms of agreement with DiReCT) was the model trained with MSE loss and a λ of 0.02. When used to measure mean global CTh, this model scored an ICC(2,1) of 0.91 (95% confidence interval [0.9, 0.92]) versus the mean global CTh yielded by ANTs-DiReCT on the OASIS-3 dataset. For comparison, on the same dataset the ICC between Freesurfer and the ANTs-DiReCT method was 0.50 ([95% confidence interval -0.08, 0.8]). A breakdown of the ICC by cortical subregion can be seen in Fig. 2: these range from good agreement (entorhinal right, ICC = 0.87) to poor (caudalanteriorcingulate right, ICC = 0.26), depending on the region. However, ICC(2,1) is a measure of absolute agreement, as well as correlation: all regional Pearson correlation coefficients lie in a range [0.64-0.90] (see supplementary material for a region-wise plot of the Pearson correlation coefficients).  Performance of this model on the CTh digital phantom can be seen in Fig. 3: agreement with the induced level of atrophy is high (metric: Coefficient of Determination between the induced and the measured level of atrophy, across all 20 synthetic subjects) in both the wide range of atrophy (up to 1mm) and the fine-grained narrower range of atrophy (up to 0.1mm), suggesting that the Vox-elMorph model is able to resolve small changes in CTh.Calculating regional CTh took between 2.5 s and 6.4 s per subject (mean, 4.3 s, standard deviation 0.71 s) (Nvidia A6000 GP, Intel Xeon(R) W-11955M CPU)."
CortexMorph: Fast Cortical Thickness Estimation via Diffeomorphic Registration Using VoxelMorph,4,Conclusion,"Our experiments suggest that the classical, iterative approach to cortical thickness estimation by diffeomorphic registration can be replaced with a VoxelMorph network, with ∼ 800 fold reduction in the time needed to calculate CTh from a partial volume segmentation of the cortical grey and white matter. Since such segmentations can also be obtained in a small number of seconds using a CNN or other deep neural network, we have demonstrated for the first time reliable CTh estimation running on a timeframe of seconds. This level of acceleration offers increased feasibility to evaluate CTh in the clinical setting. It would also enable the application of ensemble methods to provide multiple thickness measures for an individual: given an ensemble of, say, 15 segmentation methods, a plausible distribution of CTh values could be reported for each cortical subregion within one minute: this would allow better determination of the presence of cortical atrophy in an individual than is provided by point estimates. We are currently investigating the prospect of leveraging the velocity field to enable fast calculation of other morphometric labels such as grey-white matter contrast and cortical curvature: these too could be calculated with error bars via ensembling.This work allows the fast calculation of diffeomorphisms for DiReCT on the GPU. We did not consider the possibility of directly implementing/accelerating the classical DiReCT algorithm on a GPU in this work. Elements of the ANTs-DiReCT pipeline implement multithreading, yielding for example a 20 min runtime with 4 threads: however, since some parts of the pipeline cannot be parallelized it is unlikely that iterative methods can approach the speed of direct regression by CNN.Given the lack of a gold standard ground truth for CTh, it is necessary when studying a new definition of CTh to compare to an existing silver standard method: this would typically be Freesurfer, but recent results suggest that this may not be the optimal method when studying small differences in CTh. [13] We have focused on comparison to the DL+DiReCT method for this study, since the results of this model on the CTh phantom are already reported and represent the state-of-the-art. For this reason, it made sense to use the outputs of the underlying CNN as inputs to our pipeline. However, the method we describe is general and could be applied to any highly performing segmentation method. Similarly, while we performed model selection to optimize agreement with the CTh values produced by Rebsamen et al., this optimization could easily be tuned to instead optimize agreement with Freesurfer. Alternatively, we could abandon agreement and instead select models based on consistency (given by a different variant of ICC) or Pearson correlation with a baseline model: this could lead to models which deviate from the baseline model but are better able to capture differences between patients or cohorts."
CortexMorph: Fast Cortical Thickness Estimation via Diffeomorphic Registration Using VoxelMorph,,Fig. 2 .,
CortexMorph: Fast Cortical Thickness Estimation via Diffeomorphic Registration Using VoxelMorph,,Fig. 3 .,
CortexMorph: Fast Cortical Thickness Estimation via Diffeomorphic Registration Using VoxelMorph,,,
TriDo-Former: A Triple-Domain Transformer for Direct PET Reconstruction from Low-Dose Sinograms,1,Introduction,"As an in vivo nuclear medical imaging technique, positron emission tomography (PET) enables the visualization and quantification of molecular-level activity and has been extensively applied in hospitals for disease diagnosis and intervention [1,2]. In clinic, to ensure that more diagnostic information can be retrieved from PET images, physicians prefer standard-dose PET scanning which is obtained by injecting standard-dose radioactive tracers into human bodies. However, the use of radioactive tracers inevitably induces potential radiation hazards. On the other hand, reducing the tracer dose during the PET scanning will introduce unintended noise, thus leading to degraded image quality with limited diagnostic information. To tackle this clinical dilemma, it is of high interest to reconstruct standard-dose PET (SPET) images from the corresponding low-dose PET (LPET) data (i.e., sinograms or images).In the past decade, deep learning has demonstrated its promising potential in the field of medical images [3][4][5][6]. Along the research direction of PET reconstruction, most efforts have been devoted to indirect reconstruction methods [7][8][9][10][11][12][13][14][15][16] which leverage the LPET images pre-reconstructed from the original projection data (i.e., LPET sinograms) as the starting point to estimate SPET images. For example, inspired by the preeminent performance of generative adversarial network (GAN) in computer vision [17,18], Wang et al. [9] proposed a 3D conditional generative adversarial network (3D-cGAN) to convert LPET images to SPET images. However, beginning from the pre-reconstructed LPET images rather than the original LPET sinograms, these indirect methods may lose or blur details such as edges and small-size organs in the pre-reconstruction process, leading to unstable and compromised performance.To remedy the above limitation, several studies focus on the more challenging direct reconstruction methods [19][20][21][22][23][24][25][26][27] which complete the reconstruction from the original sinogram domain (i.e., LPET sinograms) to the image domain (i.e., SPET images). Particularly, Haggstrom et al. [19] proposed DeepPET, employing a convolutional neural network (CNN)-based encoder-decoder network to reconstruct SPET images from LPET sinograms. Although these direct methods achieve excellent performance, they still have the following limitations. First, due to the lack of consideration for the boundaries, the reconstruction from the sinogram domain to the image domain often leads to distortion of the reconstructed image in the high-frequency part of the frequency domain, which is manifested as blurred edges. Second, current networks ubiquitously employ CNNbased architecture which is limited in modeling long-range semantic dependencies in data. Lacking such non-local contextual information, the reconstructed images may suffer from missing or inaccurate global structure.In this paper, to resolve the first limitation above, we propose to represent the reconstructed SPET images in the frequency domain, then encourage them to resemble the corresponding real SPET images in the high-frequency part. As for the second limitation, we draw inspiration from the remarkable progress of vision transformer [28] in medical image analysis [29,30]. Owing to the intrinsic self-attention mechanism, the transformer can easily correlate distant regions within the data and capture non-local information. Hence, the transformer architecture is considered in our work.Overall, we propose an end-to-end transformer model dubbed TriDo-Former that unites triple domains of sinogram, image, and frequency to directly reconstruct the clinically acceptable SPET images from LPET sinograms. Specifically, our TriDo-Former is comprised of two cascaded transformers, i.e., a sinogram enhancement transformer (SE-Former) and a spatial-spectral reconstruction transformer (SSR-Former). The SE-Former aims to predict denoised SPET-like sinograms from LPET sinograms, so as to prevent the noise in sinograms from propagating into the image domain. Given that each row of the sinogram is essentially the projection at a certain imaging views angle, dividing it into 2D patches and feeding them directly into the transformer will inevitably break the continuity of each projection view. Therefore, to retain the inner-structure of sinograms and filter the noise, we split a sinogram by rows and obtain a set of 1D sequences of different imaging view angles. Then, the relations between view angles are modeled via the self-attention mechanism in the SE-Former. Note that the SE-Former is designed specifically for the sinogram domain of LPET to effectively reduce noise based on the imaging mechanisms of PET. The denoised sinograms can serve as a better basis for the subsequent sinogram-to-image reconstruction. The SSR-Former is designed to reconstruct SPET images from the denoised sinograms. In pursuit of better image quality, we construct the SSR-Former by adopting the powerful swin transformer [31] as the backbone. To compensate for the easily lost high-frequency details, we propose a global frequency parser (GFP) and inject it into the SSR-Former. The GFP acts as a learnable frequency filter to globally modify the components of specific frequencies of the frequency domain, forcing the network to learn accurate high-frequency details and produce construction results with shaper boundaries. Through the above triple-domain supervision, our TriDo-Former exhausts the model representation capability, thereby achieving better reconstructions.The contributions of our proposed method can be described as follows. (1) To fully exploit the triple domains of sinogram, image, and frequency while capturing global context, we propose a novel triple-domain transformer to directly reconstruct SPET images from LPET sinograms. To our knowledge, we are the first to leverage both triple-domain knowledge and transformer for PET reconstruction. (2) We develop a sinogram enhancement transformer (SE-Former) that is tailored for the sinogram domain of LPET to suppress the noise while maintaining the inner-structure, thereby preventing the noise in sinograms from propagating into the image domain during the sinogram-to-image reconstruction. (3) To reconstruct high-quality PET images with clear-cut details, we design a spatial-spectral transformer (SS-Former) incorporated with the global frequency parser (GFP) which globally calibrates the frequency components in the frequency domain for recovering high-frequency details. (4) Experimental results demonstrate the superiority of our method both qualitatively and quantitatively, compared with other state-of-the-art methods."
TriDo-Former: A Triple-Domain Transformer for Direct PET Reconstruction from Low-Dose Sinograms,2,Methodology,"The overall architecture of our proposed TriDo-Former is depicted in Fig. 1, which consists of two cascaded sub-networks, i.e., a sinogram enhancement transformer (SE-Former) and a spatial-spectral reconstruction transformer (SSR-Former). Overall, taking the LPET sinograms as input, the SE-Former first predicts the denoised SPET-like sinograms which are then sent to SSR-Former to reconstruct the estimated PET (denoted as EPET) images. A detailed description is given in the following sub-sections. "
TriDo-Former: A Triple-Domain Transformer for Direct PET Reconstruction from Low-Dose Sinograms,2.1,Sinogram Enhancement Transformer (SE-Former),"As illustrated in Fig. 1(a), the SE-Former which is responsible for denoising in the input LPET sinograms consists of three parts, i.e., a feature embedding module, transformer encoder (TransEncoder) blocks, and a feature mapping module. Given that each row of sinogram is the 1D projection at an imaging view angle, we first divide the LPET sinograms by rows and perform linear projection in the feature embedding module to obtain a set of 1D sequences, each contains consistent information of a certain view angle. Then, we perform self-attention in the TransEncoder blocks to model the interrelations between projection view angles, enabling the network to better model the general characteristics under different imaging views which is crucial for sinogram denoising. After that, the feature mapping module predicts the residual between the LPET and SPET sinograms which is finally added to the input LPET sinograms to generate the EPET sinograms as the output of SE-Former. We argue that the introduction of residual learning allows the SE-Former to focus only on learning the difference between LPET and SPET sinograms, facilitating faster convergence.Feature Embedding: We denote the input LPET sinogram as S L ∈ R C s ×H s ×W s , where H s , W s are the height, width and C s is the channel dimension. As each row of sinogram is a projection view angle, the projection at the i-th (i = 1, 2, . . . H s ) row can be defined as TransEncoder: Following the standard transformer architecture [28], each TransEncoder block contains a multi-head self-attention (MSA) module and a feed forward network (FFN) respectively accompanied by layer normalization (LN). For j-th (j = 1, 2, . . . , T ) TransEncoder block, the calculation process can be formulated as:where F j denotes the output of j-th TransEncoder block. After applying T identical TransEncoder blocks, the non-local relationship between projections at different view angles is accurately preserved in the output sequence F T ∈ R H s ×d ."
TriDo-Former: A Triple-Domain Transformer for Direct PET Reconstruction from Low-Dose Sinograms,,Feature Mapping:,"The feature mapping module is designed for projecting the sequence data back to the sinogram. Concretely,) and then fed into a linear projection layer to reduce the channel dimension from C to C s . Through these operations, the residual sinogram S R ∈ R C s ×H s ×W s of the same dimension as S L , is obtained. Finally, following the spirit of residual learning, S R is directly added to the input S L to produce the output of SE-Former, i.e., the predicted denoised sinogram"
TriDo-Former: A Triple-Domain Transformer for Direct PET Reconstruction from Low-Dose Sinograms,2.2,Spatial-Spectral Reconstruction Transformer (SSR-Former),"The SSR-Former is designed to reconstruct the denoised sinogram obtained from the SE-Former to the corresponding SPET images. As depicted in Fig. 1 (b), SSR-Former adopts a 4-level U-shaped structure, where each level is formed by a spatial-spectral transformer block (SSTB). Furthermore, each SSTB contains two spatial-spectral transformer layers (SSTLs) and a convolution layer for both global and local feature extraction. Meanwhile, a 3 × 3 convolution is placed as a projection layer at the beginning and the end of the network. For detailed reconstruction and invertibility of sampling, we employ the pixelunshuffle and pixel-shuffle operators for down-sampling and up-sampling. In addition, skip connections are applied for multi-level feature aggregation."
TriDo-Former: A Triple-Domain Transformer for Direct PET Reconstruction from Low-Dose Sinograms,,Spatial-Spectral Transformer Layer (SSTL):,"As shown in Fig. 1(d), an SSTL consists of a window-based spatial multi-head self-attention (W-SMSA) followed by FFN and LN. Following swin transformer [31], a window shift operation is conducted between the two SSTLs in each SSTB for cross-window information interactions. Moreover, to capture the high-frequency details which can be easily lost, we devise global frequency parsers (GFPs) that encourage the model to recover the high-frequency component of the frequency domain through the global adjustment of specific frequencies. Generally, the W-SMSA is leveraged to guarantee the essential global context in the reconstructed PET images, while GFP is added to enrich the high-frequency boundary details. The calculations of the core W-SMSA and GFP are described as follows."
TriDo-Former: A Triple-Domain Transformer for Direct PET Reconstruction from Low-Dose Sinograms,,Window-based Spatial Multi-Head Self-Attention (W-SMSA):,"Denoting the input feature embedding of certain W-SMSA as e in ∈ R C I ×H I ×W I , where H I ,W I and C I represent the height, width and channel dimension, respectively. As depicted in Fig. 1(c), a window partition operation is first conducted in spatial dimension with a window size of M . Thus, the whole input features are divided into N (N = H I ×W I M 2 ) non-overlapping patches e * in = {e m in } N m=1 . Then, a regular spatial self-attention is performed separately for each window after partition. After that, the output patches are gathered through the window reverse operation to obtain the spatial representative feature e spa ∈ R C I ×H I ×W I ."
TriDo-Former: A Triple-Domain Transformer for Direct PET Reconstruction from Low-Dose Sinograms,,Global Frequency Parser (GFP):,"After passing the W-SMSA, the feature e spa are already spatially representative, but still lack accurate spectral representations in the frequency domain. Hence, we propose a GFP module to rectify the high-frequency component in the frequency domain. As illustrated in Fig. 1(e), the GFP module is comprised of a 2D discrete Fourier transform (DFT), an element-wise multiplication between the frequency feature and the learnable global filter, and a 2D inverse discrete Fourier transform (IDFT). Our GFP can be regarded as a learnable version of frequency filters. The main idea is to learn a parameterized attentive map applying on the frequency domain features. Specifically, we first convert the spatial feature e spa to the frequency domain via 2D DFT, obtaining the spectral feature e spe = DFT (e spa ). Then, we modulate the frequency components of e spe by multiplying a learnable parameterized attentive map A ∈ R C I ×H I ×W I to e spe , which can be formulated as:The parameterized attentive map A can adaptively adjust the frequency components of the frequency domain and compel the network to restore the high-frequency part to resemble that of the supervised signal, i.e., the corresponding real SPET images (ground truth), in the training process. Finally, we reverse e spe back to the image domain by adopting 2D IDFT, thus obtaining the optimized feature e spa = DFT (e spe ). In this manner, more high-frequency details are preserved for generating shaper constructions."
TriDo-Former: A Triple-Domain Transformer for Direct PET Reconstruction from Low-Dose Sinograms,2.3,Objective Function,"The objective function for our TriDo-Former is comprised of two aspects: 1) a sinogram domain loss L sino and 2) an image domain loss L img .The sinogram domain loss aims to narrow the gap between the real SPET sinograms S S and the EPET sinograms S E that are denoised from the input LPET sinograms. Considering the critical influence of sinogram quality, we apply the L2 loss to increase the error punishment, thus forcing a more accurate prediction. It can be expressed as:For the image domain loss, the L1 loss is leveraged to minimize the error between the SPET images I S and the EPET images I E while encouraging less blurring, which can be defined as:Overall, the final objective function is formulated by the weighted sum of the above losses, which is defined as:where λ is the hyper-parameters to balance these two terms."
TriDo-Former: A Triple-Domain Transformer for Direct PET Reconstruction from Low-Dose Sinograms,2.4,Details of Implementation,Our network is implemented by Pytorch framework and trained on an NVIDIA GeForce GTX 3090 with 24 GB memory. The whole network is trained end-to-end for 150 epochs in total using Adam optimizer with the batch size of 4. The learning rate is initialized to 4e-4 for the first 50 epochs and decays linearly to 0 for the remaining 100 epochs.The number T of the TransEncoder in SE-Former is set to 2 and the window size M is set to 4 in the W-SMSA of the SSR-Former. The weighting coefficient λ in Eq. ( 6) is empirically set as 10.
TriDo-Former: A Triple-Domain Transformer for Direct PET Reconstruction from Low-Dose Sinograms,3,Experiments and Results,"Datasets: We train and validate our proposed TriDo-Former on a real human brain dataset including 8 normal control (NC) subjects and 8 mild cognitive impairment (MCI) subjects. All PET scans are acquired by a Siemens Biograph mMR system housed in Biomedical Research Imaging Center. A standard dose of 18F-Flurodeoxyglucose ([ 18 F] FDG) was administered. According to standard protocol, SPET sinograms were acquired in a 12-minute period within 60-minute of radioactive tracer injection, while LPET sinograms were obtained consecutively in a 3-min shortened acquisition time to simulate the acquisition at a quarter of the standard dose. The SPET images which are utilized as the ground truth in this study were reconstructed from the corresponding SPET sinograms using the traditional OSEM algorithm [32].Experimental Settings: Due to the limited computational resources, we slice each 3D scan of size 128 × 128 × 128 into 128 2D slices with a size of 128 × 128. The Leave-One-Out Cross-Validation (LOOCV) strategy is applied to enhance the stability of the model with limited samples. To evaluate the performance, we adopt three typical quantitative evaluation metrics including peak signal-to-noise (PSNR), structural similarity index (SSIM), and normalized mean squared error (NMSE). Note that, we restack the 2D slices into complete 3D PET scans for evaluation.Comparative Experiments: We compare our TriDo-Former with four direct reconstruction methods, including (1) OSEM [32] (applied on the input LPET sinograms, serving as the lower bound), (2) DeepPET [19], (3) Sino-cGAN [23], and (4) LCPR-Net [24] as well as one indirect reconstruction methods, i.e., (5) 3D-cGAN [9]. The comparison results are given in Table 1, from which we can see that our TriDo-Former achieves the best results among all the evaluation criteria. Compared with the current state-of-the-art LCPR-Net, our proposed method still enhances the PSNR and SSIM by 0.599 dB and 0.002 for NC subjects, and 0.681 dB and 0.002 for MCI subjects, respectively. Moreover, our model also has minimal parameters and GLOPs of 38 M and 16.05, respectively, demonstrating its speed and feasibility in clinical applications. We also visualize the results of our method and the compared approaches in Fig. 2, where the differences in global structure are highlighted with circles and boxes while the differences in edge details are marked by arrows. As can be seen, compared with other methods which have inaccurate structure and diminished edges, our TriDo-Former yields the best visual effect with minimal error in both global structure and edge details.  "
TriDo-Former: A Triple-Domain Transformer for Direct PET Reconstruction from Low-Dose Sinograms,,Evaluation on Clinical Diagnosis:,"To further prove the clinical value of our method, we further conduct an Alzheimer's disease diagnosis experiment as the downstream task. Specifically, a multi-layer CNN is firstly trained by real SPET images to distinguish between NC and MCI subjects with 90% accuracy. Then, we evaluate the PET images reconstructed by different methods on the trained classification model. Our insight is that, if the model can discriminate between NC and MCI subjects from the reconstructed images more accurately, the quality of the reconstructed images and SPET images (whose quality is preferred in clinical diagnosis) are closer. As shown in Fig. 3, the classification accuracy of our proposed method (i.e., 88.6%) is the closest to that of SPET images (i.e., 90.0%), indicating the huge clinical potential of our method in facilitating disease diagnosis.Ablation Study: To verify the effectiveness of the key components of our TriDo-Former, we conduct the ablation studies with the following variants: (1) replacing SE-Former and SSR-Former with DnCNN [33] (the famous CNN-based denoising network) and vanilla U-Net (denoted as DnCNN + UNet), (2) replacing DnCNN with SE-Former (denoted as SE-Former + UNet), (3) replacing the U-Net with our SSR-Former but removing GFP (denoted as Proposed w/o GFP), and (4) using the proposed TriDo-Former model (denoted as Proposed). According to the results in Table 2, the performance of our model progressively improves with the introduction of SE-Former and SSR-Former. Particularly, when we remove the GFP in SSR-Former, the performance largely decreases as the model fails to recover high-frequency details. Moreover, we conduct the clinical diagnosis experiment and the spectrum analysis to further prove the effectiveness of the GFP, and the results are included in supplementary material. "
TriDo-Former: A Triple-Domain Transformer for Direct PET Reconstruction from Low-Dose Sinograms,4,Conclusion,"In this paper, we innovatively propose a triple-domain transformer, named TriDo-Former, for directly reconstructing the high-quality PET images from LPET sinograms. Our model exploits the triple domains of sinogram, image, and frequency as well as the ability of the transformer in modeling long-range interactions, thus being able to reconstruct PET images with accurate global context and sufficient high-frequency details. Experimental results on the real human brain dataset have demonstrated the feasibility and superiority of our method, compared with the state-of-the-art PET reconstruction approaches."
TriDo-Former: A Triple-Domain Transformer for Direct PET Reconstruction from Low-Dose Sinograms,,Fig. 1 .,
TriDo-Former: A Triple-Domain Transformer for Direct PET Reconstruction from Low-Dose Sinograms,,Fig. 2 .,
TriDo-Former: A Triple-Domain Transformer for Direct PET Reconstruction from Low-Dose Sinograms,,Fig. 3 .,
TriDo-Former: A Triple-Domain Transformer for Direct PET Reconstruction from Low-Dose Sinograms,,,"Therefore, by splitting the sinogram by rows, we obtain a set of 1D sequence data S * L = {s i L } H s i=1 ∈ R H s ×D , where H s is the number of projection view angles and D = C s × W s equals to the pixel number in each sequence data. Then, S * L is linearly projected to sequence S * L ∈ R"
TriDo-Former: A Triple-Domain Transformer for Direct PET Reconstruction from Low-Dose Sinograms,,Table 1 .,
TriDo-Former: A Triple-Domain Transformer for Direct PET Reconstruction from Low-Dose Sinograms,,Table 2 .,
Low-Dose CT Image Super-Resolution Network with Dual-Guidance Feature Distillation and Dual-Path Content Communication,1,Introduction,"Following the ""as low as reasonably achievable"" (ALARA) principle [22], lowdose computer tomography (LDCT) has been widely used in various medical applications, for example, clinical diagnosis [18] and cancer screening [28]. To balance the high image quality and low radiation damage compared to normaldose CT (NDCT), numerous algorithms have been proposed for LDCT superresolution [3,4].In the past decades, image post-processing techniques attracted much attention from researchers because they did not rely on the vendor-specific parameters [2] like iterative reconstruction algorithms [1,23] and could be easily applied to current CT workflows [29]. Image post-processing super-resolution (SR) methods could be divided into 3 categories: interpolated-based methods [16,25], modelbased methods [13,14,24,26] and learning-based methods [7][8][9]17]. Interpolatedbased methods could recover clear results in those flattened regions but failed to reconstruct detailed textures because they equally recover information with different frequencies. And model-based methods often involved time-consuming optimization processes and degraded quickly when image statistics were biased from the image prior [6].With the development of deep learning (DL), various learning-based methods have been proposed, such as EDSR [20], RCAN [31], and SwinIR [19]. Those methods optimized their trainable parameters by pre-degraded low-resolution (LR) and high-resolution (HR) pairs to build a robust model with generalization and finally reconstruct SR images. However, they were designed for known degradation (for example bicubic degradation) and failed to deal with more complex and unknown degradation processes (such as LDCT degradation). Facing more complex degradation processes, blind SR methods have attracted attention. Huang et al. [11] introduced a deep alternating network (DAN) which estimated the degradation kernels and corrected those kernels iteratively and reconstructed results following the inverse process of the estimated degradation. More recently, aiming at improving the quality of medical images further, Huang et al. [12] first composited degradation model proposed for radiographs and proposed attention denoising super-resolution generative adversarial network (AID-SRGAN) which could denoise and super-resolve radiographs simultaneously. To accurately reconstruct HR CT images from LR CT images, Hou et al. [10] proposed a dual-channel joint learning framework which could process the denoising reconstruction and SR reconstruction in parallel.The aforementioned methods still have drawbacks: (1) They treated the regions of interest (ROI) and regions of uninterest equally, resulting in the extra cost in computing source and inefficient use for hierarchical features. (2) Most of them extracted the features with a fixed resolution, failing to effectively leverage multi-scale features which are essential to image restoration task [27,32].(3) They connected the SR task and the LDCT denoising task stiffly, leading to smooth texture, residual artifacts and unclear edges.To deal with those issues, as shown in Fig. 1(a), we propose an LDCT image SR network with dual-guidance feature distillation and dual-path content com-Fig. 1. Architecture of our proposed method. SAM is sampling attention module. CAM is channel attention module. AVG CT is the average image among adjacent CT slices of each patient. munication. Our contributions are as follows: (1) We design a dual-guidance fusion module (DGFM) which could fuse the 3D CT information and ROI guidance by mutual attention to make full use of CT features and reconstruct clearer textures and sharper edges. (2) We propose a sampling attention block (SAB) which consists of sampling attention module (SAM), channel attention module (CAM) and elaborate multi-depth residual connection aiming at the essential multi-scale features by up-sampling and down-sampling to leverage the features in CT images. (3) We design a multi-supervised mechanism based on shared task heads, which introducing the denoising head into SR task to concentrate on the connection between the SR task and the denoising task. Such design could suppress more artifacts while decreasing the number of parameters."
Low-Dose CT Image Super-Resolution Network with Dual-Guidance Feature Distillation and Dual-Path Content Communication,2,Method,
Low-Dose CT Image Super-Resolution Network with Dual-Guidance Feature Distillation and Dual-Path Content Communication,2.1,Overall Architecture,"The pipeline of our proposed method is shown in Fig. 1(a). We first calculate the average CT image of adjacent CT slices of each patient to provide the 3D spatial structure information of CT volume. Meanwhile, the ROI mask is obtained by a pre-trained segmentation network to guide the network to concentrate on the focus area or tissue area. Then those guidance images and the input LDCT image are fed to the dual-guidance feature distillation backbone to extract the deep features. Finally, the proposed dual-path architecture consisting of parametershared SR heads and denoising heads leverages the deep visual features obtained by our backbone to build the connection between the SR task and the denoising task, resulting in noise-free and detail-clear reconstructed results.Dual-Guidance Feature Distillation Backbone. To decrease the redundant computation and make full use of the above-mentioned extra information, we design a dual-guidance feature distillation backbone consisting of a dual-guidance fusion module (DGFM) and sampling attention block(SAB).Firstly, we use a 3 × 3 convolutional layer to extract the shallow features of the three input images. Then, those features are fed into 10 DGFM-SAB blocks to obtain the deep visual features.Especially, the DGFM-SAB block is composed of DGFM concatenated with SAB. Considering the indicative function of ROI, we calculate the correlation matrix between LDCT and its mask and then acquire the response matrix between the correlation matrix and the average CT image by multi-heads attention mechanism:where, F SAB i are the output of i-th SAB. F mask and F AV G represent the shallow features of the input ROI mask and the average CT image respectively. Meanwhile, P rj(•) is the projection function, Sof tmax[•] means the softmax function and F i are the output features of the i-th DGFM. The DGFM helps the backbone to focus on the ROI and tiny structural information by continuously introducing additional guidance information.Furthermore, to take advantage of the multi-scale information which is essential for obtaining the response matrix containing the connections between different levels of features, as shown in Fig. 1(b), we design the sampling attention block (SAB) which introduces the resampling features into middle connection to fuse the multi-scale information. In the SAB, the input features are up-sampled and down-sampled simultaneously and then down-sampled and up-sampled to recover the spatial resolution, which can effectively extract multi-scale features. In addition, as shown in Fig. 1(c), we introduce the channel attention module (CAM) to focus on those channels with high response values, leading to detailed features with high differentiation to different regions. Shared Heads Mechanism. Singly using the SR head that consists of Pixel Shuffle layer and convolution layer fails to suppress the residual artifacts because of its poor noise removal ability. To deal with this problem, we develop a dualpath architecture by introducing the shared denoising head into SR task where the parameters of SR heads and denoising heads in different paths are shared respectively. Two paths are designed to process the deep features extracted from our backbone: (1) The SR path transfers the deep features to those with highfrequency information and reconstructs the SR result, and (2) the denoising path migrates the deep features to those without noise and recovers the clean result secondly. Especially, the parameters of those two paths are shared and optimized by multiple supervised strategy simultaneously. This process could be formulated as:where, "
Low-Dose CT Image Super-Resolution Network with Dual-Guidance Feature Distillation and Dual-Path Content Communication,2.2,Target Function,"Following the multiple supervision strategy, the target function L total is calculated as:where, I gt is the ground truth, BI(•) means bicubic interpolation, • 1 represents the L1 norm and λ 1 , λ 2 are the weight parameters for adjusting the losses."
Low-Dose CT Image Super-Resolution Network with Dual-Guidance Feature Distillation and Dual-Path Content Communication,3,Experiments,
Low-Dose CT Image Super-Resolution Network with Dual-Guidance Feature Distillation and Dual-Path Content Communication,3.1,Datasets and Experiment Setup,"Datasets. Two widely-used public CT image datasets, 3D-IRCADB [5] and PANCREAS [5] We augment the data by rotation and flipping first and then randomly crop them to 128 × 128 patches. Adam optimizer with β 1 = 0.9 and β 2 = 0.99 is used to minimize the target function. λ 1 and λ 2 of our target function are set as 0.2. The batch size is set to 16 and the learning rate is set to 10 -4 which decreases to 5×10 -5 at 200K iterations. Peak signal-to-noise (PSNR) and structural similarity (SSIM) are used as the quantitative indexes to evaluate the performance."
Low-Dose CT Image Super-Resolution Network with Dual-Guidance Feature Distillation and Dual-Path Content Communication,3.2,Ablation Study,"Table 1a shows the experimental result of the dual-guidance ablation study.Introducing the average CT image guidance alone degrades performance compared with the model without guidance for both the scale factor of 2 and 4. And introducing mask guidance alone could improve the reconstruction effect. When the average CT image guidance and the mask guidance are both embedded, the performance will be promoted further. Table 1b presents the result of the shared heads mechanism ablation study. The experimental result proves that introducing the proposed dual-path architecture could promote the reconstruction performance and the model with shared heads is superior than that without them in both reconstruction ability and parameter amount. "
Low-Dose CT Image Super-Resolution Network with Dual-Guidance Feature Distillation and Dual-Path Content Communication,3.3,Comparison with State-of-the-Art Methods,"We compare the performance of our proposed method with other state-of-theart methods, including Bicubic interpolation [16], DAN [11], RealSR [15], SPSR [21], AID-SRGAN [12] and JDNSR [10].Figure 2 shows the qualitative comparison results on the 3D-IRCADB dataset with the scale factor of 2. All methods enhance the image quality to different extents compared with bicubic interpolation. However, for the calcifications within the liver which are indicated by the blue arrows, our method recovers the clearest edges. The results of DAN, SPSR and AID-SRGAN suffers from the artifacts. JDNSR blurs the issue structural information, e.g. the edges of liver and bone. For the inferior vena cava, portal vein, and gallbladder within the kidney, RealSR restores blurred details and textures though it could recover clear edges of calcifications. Figure 3 shows the qualitative comparison results on the PANCREAS dataset with the scale factor of 4. Figure 3 has similar observation as Fig. 2, that is, our method could suppress more artifacts than other methods, especially at the edges of the pancreas and the texture and structure of the issues with in the kidney. Therefore, our method reconstructs more detailed results than other methods.Table 2 shows the quantitative comparison results of different state-of-theart methods with two scale factors on two datasets. For the 3D-IRCADB and PANCREAS datasets, our method outperforms the second-best methods 1.6896/0.0157 and 1.7325/0.0187 on PSNR/SSIM with the scale factor of 2 respectively. Similarly, our method outperforms the second-best methods  "
Low-Dose CT Image Super-Resolution Network with Dual-Guidance Feature Distillation and Dual-Path Content Communication,4,Conclusion,"In this paper, we propose an LDCT image SR network with dual-guidance feature distillation and dual-path content communication. Facing the existing problem that reconstructed results suffer from residual artifacts, we design a dualguidance feature distillation backbone which consists of DGFM and SAB to extract deep visual information. Especially, the DGFM could fuse the average CT image to take the advantage of the 3D spatial information of CT volume and the segmentation mask to focus on the ROI, which provides pixel-wise shallow information and deep semantic features for our backbone. The SAB leverages the essential multi-scale features to enhance the ability for feature extraction. Then, our shared heads mechanism reconstructs the deep features obtained by our backbone to satisfactory results. The experiments compared with 6 state-ofthe-art methods on 2 public datasets demonstrate the superiority of our method."
Low-Dose CT Image Super-Resolution Network with Dual-Guidance Feature Distillation and Dual-Path Content Communication,,Fig. 2 .,
Low-Dose CT Image Super-Resolution Network with Dual-Guidance Feature Distillation and Dual-Path Content Communication,,Fig. 3 .,
Low-Dose CT Image Super-Resolution Network with Dual-Guidance Feature Distillation and Dual-Path Content Communication,,,
Low-Dose CT Image Super-Resolution Network with Dual-Guidance Feature Distillation and Dual-Path Content Communication,,Table 1 .,Experiment Setup. All experiments are implemented on Ubuntu 16.04.12 with an NVIDIA RTX 3090 24G GPU using Pytorch 1.8.0 and CUDA 11.1.74.
Low-Dose CT Image Super-Resolution Network with Dual-Guidance Feature Distillation and Dual-Path Content Communication,,Table 2 .,
Implicit Neural Representations for Joint Decomposition and Registration of Gene Expression Images in the Marmoset Brain,1,Introduction,"Image registration is a crucial prerequisite for image comparison, data integration, and group studies in contemporary medical and neuroscience research. In research and clinical settings, pairs of images often show similar anatomical structures but may contain additional features or artifacts, such as specific staining, electrodes, or lesions, that are not present in the other image. This difficulty of finding corresponding structures for automatically aligning images complicates image registration. In this work, we address the challenging problem of the gene expression image registration in the marmoset brain. Brain atlases of gene expression, created using images of brain tissue processed through in situ hybridization (ISH), offer single-cell resolution of spatial gene expression patterns across the entire brain [2,7]. However, accurately quantifying gene expression requires brain image registration to spatially align ISH images to a common atlas space. The diversity of gene expression patterns in ISH images causes variations in visible anatomical brain structures with respect to the template image. ISH microscopy images are also susceptible to tissue processing artifacts, resulting in non-specific staining and tissue deformations.Traditional pair-wise image registration methods use optimization algorithms to find the deformation field that maximizes the similarity between a pair of images. While several deep learning methods based on convolutional neural networks (CNNs) have been proposed for calculating the deformation field between two images [3], such models typically require large training sets and may suffer from generalization issues when applied to images presenting texture patterns that diverge from the training data. Therefore, classic algorithms, such as Advanced Normalization Tools (ANTs) [1], are still preferred as off-the-shelf tools for image registration in neuroscience due to scarce experimental data and the diversity of data acquisition protocols and registration tasks. Recently, implicit neural representations (INRs) have been utilized for image registration in MRI and CT [14,16], offering a hybrid approach that connects modern deep learning techniques with per-case optimization as used in classical approaches. INRs are defined on continuous coordinate spaces, making them suitable for registration of images that differ in geometry.In this work, we propose a novel INR-based framework well-suited to address the challenging problem of gene expression brain image registration. We associate the registration problem with an image decomposition task. We utilize implicit neural networks to decompose the ISH image into two separate images: a support image and a residual image. The support image corresponds to the part of the ISH image that is well-aligned with the registration template image in respect to the texture. On the contrary, the residual image presents features of the ISH image, such as artifacts or texture patterns (e.g. gene expression), which presumably undermine the registration procedure. The support image is used to improve the deformation field calculations. We also introduce an exclusion loss to encourage clearer separation of the support and residual images. The usefulness of the proposed method is demonstrated using 2D ISH gene expression images of the marmoset brain."
Implicit Neural Representations for Joint Decomposition and Registration of Gene Expression Images in the Marmoset Brain,2,Methods,
Implicit Neural Representations for Joint Decomposition and Registration of Gene Expression Images in the Marmoset Brain,2.1,Registration with Implicit Networks,"The goal of the pairwise image registration is to determine a spatial transformation that maximizes the similarity between the moving image M and the target fixed template image F . INRs serve as a continuous, coordinate based approximation of the deformation field obtained through a fully connected neural network. In this study, as the backbone for our method, we utilized the standard approach to registration with INRs, as described in [14,16]. We used a single implicit deformation network D to map 2D spatial coordinates x ∈ [-1, 1] 2 of the moving image M to a displacement vector Δx ∈ R 2 . Next, the transformation field was determined as Φ(x) = x + Δx and the bilinear interpolation algorithm was applied to obtain the corresponding moved image T Φ (M ).To train the deformation network, the following loss function based on correlation coefficients was applied to assess the similarity between the moved image T Φ (M ) and the fixed template image F :where NCC and LNCC stand for the normalized cross-correlation and local normalized cross-correlation based loss functions averaged over the entire image domain consisting of N elements. NCC was used to stabilize the training of the network, while LNCC ensured good local registration results. Additionally, following the standard approach to INR based registration, we regularized the deformation field based on the Jacobian matrix determinant |J Φ(x) | using following equation [16]:"
Implicit Neural Representations for Joint Decomposition and Registration of Gene Expression Images in the Marmoset Brain,2.2,Registration Guided Image Decomposition,"Our aim is to improve the registration performance associated with the implicit deformation network D. The proposed framework is presented in Fig. 1. We assume that the moving image M can be decomposed with separate implicit networks, S and R, into two images: the support image M S and the residual image M R . Ideally, the support image should correspond to the part of the moving image that contributes to the registration performance. On the contrary, we expect the residual image to include image artifacts and texture patterns (e.g. ISH gene expression patterns) that diverge from the fixed template image and undermine the registration procedure. We impose the following condition based on the mean squared error loss function for the decomposition of the moving image:stating that the support M S and residual M R images should sum up to the moving image M . To ensure that the support image M S contributes to the registration with respect to the fixed image F , we utilize the cross-correlation based loss function L cc (F, T Φ (M S )) (Eq. 1), where T Φ (M S ) stands for the transformed support image M S . Therefore, the deformation network is trained to provide the transformation field Φ(x) both for the moving image and the support image For this, we utilize the following exclusion loss to encourage the gradient structure of the implicit networks S and R to be decorrelated [4]:), ⊗ indicates element-wise multiplication and indices i, j go over all elements of the matrix Γ .In our framework, we jointly optimize all three implicit networks (D, S and R) using the following composite loss function:The first row of Eq. 5 can be perceived as a standard registration loss, while the second row stands for a regularized image reconstruction loss."
Implicit Neural Representations for Joint Decomposition and Registration of Gene Expression Images in the Marmoset Brain,2.3,Evaluation,"We designed the proposed method with the aim to address the problem of ISH gene expression image registration. For the evaluation, we used neonate marmoset brain ISH images collected at the Laboratory for Molecular Mechanisms of Brain Development, RIKEN Center for Brain Science, Wako, Japan (geneatlas.brainminds.jp) [6,12]. We prepared manual annotations for 2D images from 50 gene expression datasets. Atlas template images were created using ANTs [1], based on semi-automatically aligned sets of 2D ISH images from 1942 gene expression datasets. ISH images used to generate the template were converted to gray-scale to meet ANTs requirements and better highlight brain tissue interfaces.Performance of the proposed approach was compared to the SynthMorph network and the ANTs SyN registration algorithm based on mutual information metric, as these two methods do not require pre-training and can serve as off-theshelf registration tools for neuroscience [1,5]. We conducted an ablation study to assess the effectiveness of the proposed representation decomposition approach with and without the exclusion loss. Registration methods were evaluated quantitatively based on Dice scores using manual 2D segmentations prepared for the following five brain structures ranging in size and shape complexity: aqueduct (AQ, 95 masks), hippocampus area (HA, 570 masks), dorsal lateral geniculate (DLG, 370 masks), inferior colliculus (IC, 70 masks) and visual cortex area (VCA, 68 masks). Segmentations were outlined both for the template and ISH 2D images, resulting in 1114 image pairs corresponding to the same brain regions. We also calculated the percentage of the non-positive Jacobian determinant values to assess the deformation field folding. Moreover, we determined the structural similarity index (SSIM) between the moved images and the template fixed images."
Implicit Neural Representations for Joint Decomposition and Registration of Gene Expression Images in the Marmoset Brain,2.4,Implementation,"We utilized sinusoidal representation networks to determine the implicit representations [13]. Each network contained five fully connected hidden layers with 256 neurons. We used the Fourier mapping with six frequencies to encode the input coordinates [15]. The coordinates and the encoded coordinates were additionally concatenated within the middle layer of the network. Weights of the networks were initialized following the original paper except for the last linear layer of the deformation network D, for which we uniformly sampled the weights from [-0.0001, 0.0001] interval to ensure small deformations at initial epochs. Additional details about the network architecture can be found in the supplementary materials. Networks were trained for 1000 epochs using AdamW optimizer with learning rate of 0.0001 on a server equipped with several NVIDIA A100 GPUs [8]. ISH images of size 360 × 420 were downsampled to 256 × 256. Each epoch corresponded to a batch of all image pixel coordinates [13]. After some initial experiments, we set the composite loss function weights (Eq. 5) to  "
Implicit Neural Representations for Joint Decomposition and Registration of Gene Expression Images in the Marmoset Brain,3,Results,
Implicit Neural Representations for Joint Decomposition and Registration of Gene Expression Images in the Marmoset Brain,3.1,Qualitative Results,"Support and residual images generated with the proposed method are shown in Fig. 2. The support images retain the main style and content of the fixed template image, while the residual images include the remaining image contents, along with gene expression patterns not present in the template image. Utilization of the exclusion loss resulted in a clearer and more visually plausible separation between the support and residual images, particularly for gene expression patterns. Figure 3 further highlights the usefulness of the proposed registration guided image decomposition technique. First, our method can be applied to extract microscopy image artifacts, and therefore mitigate their impact on the registration. Second, the proposed method is general and can also be applied to register an ISH gene expression image to a Nissl image. In this case, the color distribution of the support image corresponds to that of a Nissl image, while the residual image presents the local contents of the gene expression image. We also used the proposed method to register an ISH brain image to another ISH image with a different gene expression. For this example, the residual image highlighted the gene expression patterns of the moving image, while the support image showed the gene expression patterns of the fixed image.Figure 4 visually compares the registration performance of the proposed technique, equipped with the exclusion loss, to ANTs. We found that the proposed method provided good results both in respect to the image registration and the transformation of the manual segmentations."
Implicit Neural Representations for Joint Decomposition and Registration of Gene Expression Images in the Marmoset Brain,3.2,Quantitative Results,"Table 1 shows Dice scores obtained for the selected marmoset brain regions. Registration techniques based on INRs outperformed the other methods on four out of five brain regions. ANTs achieved better registration results for only one structure, the VCA, which was the largest among the annotated brain regions    2 show that the registration based on implicit networks provided the most structurally similar results to the template images. With respect to the SSIM metric, our method significantly outperformed other approaches (t-test's p-values < 0.05). ANTs and SynthMorph provided smoother deformation fields compared to the implicit networks, with significantly lower percentage of folding (t-test's p-values < 0.05). However, the percentage of the folding obtained for the implicit networks was small and acceptable, as defined by folds in 0.5% of all pixels [11]. The main disadvantage of the proposed approach was the relatively long optimization time of about 90 s for a single pairwise registration, resulting from the requirement to jointly train three implicit networks."
Implicit Neural Representations for Joint Decomposition and Registration of Gene Expression Images in the Marmoset Brain,4,Conclusion,"Our approach based on implicit networks and registration-guided image decomposition has demonstrated excellent performance for the challenging task of registering ISH gene expression images of the marmoset brain. The results show that our approach outperformed pairwise registration methods based on ANTs and SynthMorph CNN, highlighting the potential of INRs as versatile off-the-shelf tools for image registration. Moreover, the proposed registration-guided image decomposition mechanism not only improved the registration performance, but also could be used to effectively separate the patterns that diverge from the target fixed image. In the future, we plan to investigate the possibility of using image decomposition for simultaneous registration and pattern segmentation, and methods to speed up the training [9]. We also plan to extend our technique to 3D and test it on medical images that include pathologies. "
Implicit Neural Representations for Joint Decomposition and Registration of Gene Expression Images in the Marmoset Brain,,Fig. 1 .,
Implicit Neural Representations for Joint Decomposition and Registration of Gene Expression Images in the Marmoset Brain,,α 1 =,
Implicit Neural Representations for Joint Decomposition and Registration of Gene Expression Images in the Marmoset Brain,,Fig. 2 .,
Implicit Neural Representations for Joint Decomposition and Registration of Gene Expression Images in the Marmoset Brain,,Fig. 3 .,
Implicit Neural Representations for Joint Decomposition and Registration of Gene Expression Images in the Marmoset Brain,,Fig. 4 .,
Implicit Neural Representations for Joint Decomposition and Registration of Gene Expression Images in the Marmoset Brain,,Table 1 .,
Implicit Neural Representations for Joint Decomposition and Registration of Gene Expression Images in the Marmoset Brain,,Table 2 .,
Implicit Neural Representations for Joint Decomposition and Registration of Gene Expression Images in the Marmoset Brain,,,
Implicit Neural Representations for Joint Decomposition and Registration of Gene Expression Images in the Marmoset Brain,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 61.
X-Ray to CT Rigid Registration Using Scene Coordinate Regression,1,Introduction,"Image-guided navigation plays a crucial role in modern surgical procedures. In the field of orthopedics, many surgical procedures such as total hip arthro-plasty, total knee arthroplasty, and pedicle screw injections utilize intraoperative fluoroscopy for surgical navigation [2,4,11]. Due to overlapping anatomical structures in X-ray images, it is often difficult to correctly identify and reason the 3D structure from solely the image. Therefore, registering an intraoperatively acquired X-ray image to the preoperatively acquired CT scan is crucial in performing such procedures [13,15,18,19]. The standard procedure for acquiring highly accurate registration involves embedding fiducial markers into the patient and acquiring a preoperative CT scan to obtain 2D-3D correspondences [6,10,17]. Inserting fiducial markers onto the body involves extra surgical costs and might not be viable for minimally invasive surgeries. To circumvent such issues with the feature-based method, an intensity-based optimization scheme for registration has been extensively studied [1,9]. Since the objective function is highly nonlinear for optimizing pose parameters, a good initialization is necessary for the method to converge in a global minimum. Therefore, it is usually accompanied by initial coarse registration using manual alignment of the 3D model to the image, interrupting the surgical flow. On the other hand, learningbased methods have proved to be efficient in solving the registration task. Existing learning-based methods can be broadly categorized into landmark estimation and direct pose regression. Landmark estimation methods aim to solve for pose using correspondences between 3D landmark annotations and its estimated 2D projection points [3,5,7], while methods based on pose regression estimate the global camera pose in a single inference [12]. Pose regressors are known to overfit training data and generalize poorly to unseen images [14]. This makes the landmark estimation methods stand out in terms of registration quality and generalization. However, there exist two main issues with landmark estimation methods: 1) Annotation cost of a sufficiently large number of landmarks in the CT image. 2) Failure to solve for the pose in extreme views where projected landmarks are not visible or the number of visible landmarks is small. This paper addresses these issues by introducing scene coordinates [16] to establish dense 2D-3D correspondences. Specifically, the proposed method regresses the scene coordinates of the CT-scan model from corresponding Xray images. A rigid transformation that aligns the CT-scan model to the image is then calculated by solving the Perspective-n-point (PnP) problem with the Random sample and consensus (RANSAC) algorithm."
X-Ray to CT Rigid Registration Using Scene Coordinate Regression,2,Method,
X-Ray to CT Rigid Registration Using Scene Coordinate Regression,2.1,Problem Formulation,"The problem of 2D-3D registration can be formulated a finding the rigid transformation that transforms the 3D model defined in the anatomical or world coordinate system into the camera coordinate system. Specifically, given a CTscan volume V CT (x w ) where x w is defined in the world coordinate system, the registration problem is concerned with finding T c w = [R|t] such that the following holds. where {•} is the X-ray transform that can be applied to volumes in the camera coordinate system given an intrinsic matrix K and I, the target X-ray image."
X-Ray to CT Rigid Registration Using Scene Coordinate Regression,2.2,Registration,"The proposed registration pipeline overview is shown in Fig. 1. The proposed method comprises the following four parts: first, the scene coordinates were regressed using a single-view X-ray image as input to the U-Net model; second, the PnP + RANSAC algorithm is used to solve for the pose of the captured X-ray system; third, the CT-scan volume was segmented to obtain a 3D model of the bone regions; and fourth, the computed rigid transformation from world coordinates to camera coordinates is used to generate projection overlay images.Scene Coordinates. The scene coordinates are defined as the points of intersection between the camera's back-projected rays and the 3D model in a world coordinate system (i.e., only the first intersection and the last intersections are considered). The same concept was adapted for X-ray images and their underlying 3D models obtained from the CT-scans. Specifically, given an arbitrary point x ij in the image plane, the scene coordinates X ij satisfy the following conditions.where R and t are the rotation matrix and translation vector that maps points in the world coordinate system to the camera coordinate system, K is the intrinsic matrix, d is the depth, as seen from the camera, of the point X on the 3D model.Uncertainty Estimation. The task of scene coordinate regression is to estimate these X ij for every pixel ij, given an X-ray image I. However, the existence of X ij is not guaranteed for all pixels because back-projected rays may not intersect the 3D model. One of the many ways to address such a case is to prepare a mask (i.e., 1 if the bone area, 0 otherwise) in advance so that only the pixels that lie inside the mask are estimated. As this approach requires an explicit method for estimating the mask image, an alternative approach was adopted in this study. Instead of estimating a single X ij , the mean and variance of the scene coordinates are estimated. The non-intersecting scene coordinates were identified by applying thresholding to the estimated variance (i.e., points with high variance were considered non-existent scene coordinates and were filtered out). This approach assumes that the observed scene coordinates are corrupted with a zero mean, non-zero and non-constant variance, and isotropic Gaussian noise.where u(I, x ij ) and σ(I, x ij ) are the functions that produce the mean and standard deviation of the scene coordinates, respectively. This work represents these functions using a fully convolutional neural network.Loss Function. A U-Net architecture was used to estimate the mean and standard deviation of the scene coordinates at every pixel in a given image. The loss function for the intersecting scene coordinates is derived from the maximum likelihood estimates using the likelihood X ij . This can be expressed as follows:Because it is desirable to have a high variance for non-existent scene coordinates, the loss function for non-existent coordinates is designed as follows:2D-3D Registration. An iterative PnP implementation from OpenCV was run using RANSAC with maximum iteration of 1000 and reprojection error of 10px and 20px for the simulated and real X-ray images respectively. An example of a successful registration is shown in the left part of Fig. 2 below. 3 Experiments and Results"
X-Ray to CT Rigid Registration Using Scene Coordinate Regression,3.1,Dataset,"A dataset containing six annotated CT scans, each with several registered real X-ray images from [7] was used to properly evaluate the proposed method. The annotations included 14 landmarks and 7 segmentation labels. The CT scans were of the pelvic bones of cadaveric specimens. Because there were only a few real X-ray images, simulated X-ray images were generated from each CT-scans to train and test the model. In particular, DeepDRR [3] was used to simulate a Siemens Cios Fusion Mobile C-arm imaging device. Similar to [3], the left anterior oblique/right anterior oblique (LAO/RAO) views were sampled at angles of [45,45] degrees with 1-degree intervals. A random offset was applied in each direction.The offset vector was sampled from a normal distribution with a mean of zero and standard deviations of 90 mm in the lateral direction, and 30 mm in the other two directions. Images intentionally included partially visible structures.A selection of randomly sampled images is displayed on the right side of Fig. 2. Ground-truth scene coordinates for each image were obtained by rendering the depth map of the 3D models and converting them to 3D coordinates using Eq. 2.In total, 8100 simulated X-rays were generated for each specimen. Among these, 5184 images were randomly assigned to the training set, 1296 to the validation set, and the remaining 1620 to the test set."
X-Ray to CT Rigid Registration Using Scene Coordinate Regression,3.2,Implementation Details,"The input image and output scene coordinates had a size of 512 × 512 pixels.The U-Net model had eight output channels, which consisted of three channels for scene coordinates and one channel for standard deviation, multiplied by two for the entry and exit points. Patient-specific models were trained individually for each dataset. Adam optimizer with a constant learning rate of 0.0001 and a batch size of 16 was used. Online data augmentation which includes random inversion, color jitter, and random erasing was applied. The scene coordinates were filtered using a log variance threshold of 0.0 for simulated images and -2.0 for real X-ray images."
X-Ray to CT Rigid Registration Using Scene Coordinate Regression,3.3,Baselines and Evaluation Metrics,"The proposed method was compared with two other baseline methods: PoseNet [8] and DFLNet [7]. PoseNet was implemented using ResNet-50 as the backbone for the feature extractor and was trained using geometric loss. DFLNet uses the same architecture as the proposed method however the last layer regresses 14 heatmaps of the landmarks instead of scene coordinates. Note that the original study's segmentation layer and the gradient-based optimization phase were omitted for architectural comparison. Each baseline was trained in a patientspecific manner following the proposed method. The mean target registration error (mTRE) and Gross Failure Rate (GFR), were used as the evaluation metrics for comparison with the baselines. mTRE is defined in 6, where X k is the position of the ground truth landmark Xk after applying the predicted transformation. The GFR is the ratio of failed cases, defined as the registration results with an mTRE greater than 10 mm. Because we could only obtain the projection of the ground truth landmarks and not the ground truth transformation matrix for the real X-ray images, the projected mTRE (proj. mTRE) was used for the evaluation. It is similar to mTRE except the X k and Xk represent the projected coordinates of the landmarks, in the detector plane (i.e., the pixel coordinates are scaled according to the detector size to match the units)."
X-Ray to CT Rigid Registration Using Scene Coordinate Regression,3.4,Registration Results,"Simulated X-Ray Images. Table 1 shows the mTRE for the 25 th , 50 th , and 95 th percentiles of the total test sample size and the GFR. The proposed method could retain a GFR below 20% for most of the specimens, whereas PoseNet and DFLNet failed to register with more than 20% GFR in most cases. This is because the network in PoseNet cannot reason about the spatial structure or its local relation to the image patches. For DFLNet, this is inevitable because of the visibility issue of the landmark points, mostly located in the pubic region of the pelvis. Comparing the mTRE of each specimen with that of each method, the proposed method achieved an mTRE of 7.98 mm even in the 95 th percentile of Specimen 2. DFLNet achieved the lowest mTRE of 0.98 mm in the 25 th percentile of Specimen 4. This illustrates the highly accurate registration of landmark estimation methods. However, with extreme or partial views such as the one shown in Fig. 3, the method cannot estimate the correct pose parameter because of incorrect landmark localization or insufficiently visible landmarks. Please refer to the supplemental material for the registration overlay results of the different specimens using the proposed method.Real X-Ray Images. Table 2 lists the mTRE values calculated for the projected image points (abbreviated as proj. mTRE) for PoseNet and the proposed method, respectively. DFLNet did not adapt to real X-ray images, therefore, it was omitted from the table. Because our dataset consisted mostly of images with partially visible hips, only a few landmarks were visible in each image. This causes the DFLNet to overfit to the partially visible landmark distribution, whereas our proposed model mitigates this issue by learning the general structure (i.e., every surface point that is visible). The proposed method estimates good transformations (that is proj. mTRE approximately 10 mm in the 50 th percentile). In contrast, the proj. mTRE for PoseNet is significantly higher. This suggests that PoseNet overfitted the training data despite applying domain randomization. This result agrees with previous reports [14] addressing this issue.A visualization of the overlays is presented in the Supplemental Material.   "
X-Ray to CT Rigid Registration Using Scene Coordinate Regression,4,Limitations,"As the proposed method was designed to give initial estimates of the pose parameters, a further refinement step using an intensity-based optimization method would be required to obtain clinically relevant registration accuracy. Although the proposed method provided a good initial estimate, the average runtime for the entire pipeline was 1.75 s which was approximately two orders of magnitude greater than that of PoseNet, which had an average runtime of 0.06 s. This is because RANSAC must determine a good pose from a dense set of correspondences. This issue can be addressed by heuristically selecting a good variance threshold per image that filters out bad correspondences."
X-Ray to CT Rigid Registration Using Scene Coordinate Regression,5,Conclusion,"This paper presented a scene coordinate regression-based approach for the Xray to CT-scan model registration problem. Experiments with simulated and real X-ray images showed that the proposed method performed well even under partially visible structures and extreme view angles, compared with direct pose regression and landmark estimation methods. Testing the model trained solely on simulated X-ray images, on real X-ray images did not result in catastrophic failure. Instead, the results were positive for instantiating further refinement steps."
X-Ray to CT Rigid Registration Using Scene Coordinate Regression,,Fig. 1 .,
X-Ray to CT Rigid Registration Using Scene Coordinate Regression,,Fig. 2 .,
X-Ray to CT Rigid Registration Using Scene Coordinate Regression,,Fig. 3 .,
X-Ray to CT Rigid Registration Using Scene Coordinate Regression,,Table 1 .,
X-Ray to CT Rigid Registration Using Scene Coordinate Regression,,Table 2 .,
X-Ray to CT Rigid Registration Using Scene Coordinate Regression,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 74.
LightNeuS: Neural Surface Reconstruction in Endoscopy Using Illumination Decline,1,Introduction,"Colorectal cancer (CRC) is the third most commonly diagnosed cancer and is the second most common cause of cancer death [23]. Early detection is crucial for a good prognosis. Despite the existence of other techniques, such as virtual colonoscopy (VC), optical colonoscopy (OC) remains the gold standard for colonoscopy screening and the removal of precursor lesions. Unfortunately, we do not yet have the ability to reconstruct densely the 3D shape of large sections of the colon. This would usher exciting new developments, such as post-intervention diagnosis, measuring polyps and stenosis, and automatically evaluating exploration thoroughness in terms of the surface percentage that has been observed. This is the problem we address here. It has been shown that the colon 3D shape can be estimated from single images acquired during human colonoscopies [3]. However, to model large sections of it while increasing the reconstruction accuracy, multiple images must be used. As most endoscopes contain a single camera, the natural way to do this is to use video sequences acquired by these cameras in the manner of structure-from-motion algorithms. An important first step in that direction is to register the images from the sequences. This can now be done reliably using either batch [21] or SLAM techniques [8]. Unfortunately, this solves only half the problem because these techniques provide very sparse reconstructions and going from there to dense ones remains an open problem. And occlusions, specularities, varying albedos, and specificities of endoscopic lighting make it a challenging one.To overcome these difficulties, we rely on two properties of endoscopic images:-Endoluminal cavities such as the gastrointestinal tract, and in particular the human colon, are watertight surfaces. To account for this, we represent its surface in terms of a signed distance function (SDF), which by its very nature presents continuous watertight surfaces. -In endoscopy the light source is co-located with the camera. It illuminates a dark scene and is always close to the surface. As a result, the irradiance decreases rapidly with distance t from camera to surface; more specifically it is a function of 1/t 2 . In other words, there is a strong correlation between light and depth, which remains unexploited to date.To take advantage of these specificities, we build on the success of Neural implicit Surfaces (NeuS) [25] that have been shown to be highly effective at deriving surface 3D models from sets of registered images. As the Neural Radiance Fields (NeRFs) [15] that inspired them, they were designed to operate on regular images taken around a scene, sampling fairly regularly the set of possible viewing directions. Furthermore, the lighting is assumed to be static and distant so that the brightness of a pixel and its distance to the camera are unrelated. Unfortunately, none of these conditions hold in endoscopies. The camera is inside a cavity (in the colon, a roughly cylindrical tunnel) that limits viewing directions. The light source is co-located with the camera and close to the surface, which results in a strong correlation between pixel brightness and distance to the camera. In this paper, we show that, far from being a handicap, this correlation is a key information for neural network self-supervision.NeuS training selects a pixel from an image and samples points along its projecting ray. However, the network is agnostic to the sampling distance. In LightNeuS, we explicitly feed to the renderer the distance of each one of these sampled points to the light source, as shown in Fig. 1. Hence, the renderer can exploit the inverse-square illumination decline. We also introduce and calibrate a photometric model for the endoscope light and camera, so that the inverse square law discussed above actually holds. Together, these two changes make the minimization problem better posed and the automatic depth estimation more reliable.Our results show that exploiting the illumination is key to unlocking implicit neural surface reconstruction in endoscopy. It delivers accuracies in the range of 3 mm, whereas an unmodified NeuS is either 5 times less accurate or even fails to reconstruct any surface at all. Earlier methods [3] have reported similar accuracies but only on very few synthetic images and on short sections of the colon. By contrast, we can handle much longer ones and provide a broad evaluation in a real dataset (C3VD) over multiple sequences. This makes us the first to show accurate results of extended 3D watertight surfaces from monocular endoscopy images."
LightNeuS: Neural Surface Reconstruction in Endoscopy Using Illumination Decline,2,Related Works,"3D Reconstruction from Endoscopic Images. It can help with the effective localization of lesions, such as polyps and adenomas, by providing a complete representation of the observed surface. Unfortunately, many state-of theart SLAM techniques based on feature matching [5] or direct methods [6,7] are impractical for dense endoscopic reconstruction due to the lack of texture and the inconsistent lighting that moves along with the camera. Nevertheless, sparse reconstructions by classical Structure-from-Motion (SfM) algorithms can be good starting points for refinement and densification based on Shape-from-Shading (SfS) [24,28]. However, classical multi-view and SfS methods require strong suboptimal priors on colon surface shape and reflectance.In monocular dense reconstructions, it is common practice to encode shape priors in terms of smooth rigid surfaces [14,17,20]. Recently, [22] proposes a tubular topology prior for NRSfM aimed to process endoluminal cavities where these tubular shapes are prevalent. In contrast, for the same environments, we propose the watertight prior coded by implicit SDF representations.Recent methods for dense reconstruction rely on neural networks to predict per-pixel depth in the 2D space of each image and fuse the depth maps by using multi-view stereo (MVS) [2] or a SLAM pipeline [12,13]. However, holes in the reconstruction appear due to failures in triangulation and inaccurate depth estimation or in areas not observed in any image. Wang et al. [27] show the potential of neural rendering in reconstruction from medical images, although they use a binocular static camera with fixed light source, which is not feasible in endoluminal endoscopy. Unfortunately, most of the previous 3D methods do not provide code [14,22], are not evaluated in biomedical settings [17,20], or do not report reconstruction accuracy [12,13].Neural Radiance Fields (NeRFs) were first proposed to reconstruct novel views of non-Lambertian objects [15]. This method provides an implicit neural representation of a scene in terms of local densities and associated colors. In effect, the scene representation is stored in the weights of a neural network, usually a multilayer perceptron (MLP), that learns its shape and reflectance for any coordinate and viewing direction. NeRFs use volume rendering [9], based on ray-tracing from multiple camera positions. The volume density σ(x) can be interpreted as the differential probability of a ray terminating at an infinitesimal particle at location x. The expected color C(r) of the pixel with camera ray r(t) = o + td is the integration of the radiance emitted by the field at every traveled distance t from near to far bounds t n and t f , such that where c stands for the color. The function T denotes the accumulated transmittance along the ray from t n to t, that is the probability that the ray travels from t n to t without hitting any other particle. The authors propose two MLPs to estimate the volume density function σ : x → [0, 1] and the directional emitted color function c : (x, d) → [0, 1] 3 , so the density of a point does not depend on the viewing direction d, but the color does. This allows them to model non-Lambertian reflectance. In addition, they propose a positional encoding for location x and direction d, which allows high-frequency details in the reconstruction.Neural Implicit Surfaces (NeuS) were introduced in [25] to improve the quality of NeRF representation modelling watertight surfaces. For that, the volume density σ is computed so as to be maximal at the zero-crossings of a signed distance function (SDF) f :The SDF formulation makes it possible to estimate the surface normal as n = ∇f (x). The reflectance of a material is usually determined as a function of the incoming and outgoing light directions with respect to the surface normal. Therefore, the normal is added as an input to the MLP that estimates color c : (x, d, n), as shown in Fig. 1."
LightNeuS: Neural Surface Reconstruction in Endoscopy Using Illumination Decline,3,LightNeuS,"In this section, we present the key contributions that make LightNeuS a neural implicit reconstruction method suitable for endoscopy in endoluminal cavities. In this context, the light source is located next to the camera and moves with it. Furthermore, it is close to the surfaces to be modeled. As a result, for any surface point x = o+td, the irradiance decreases with the square of the distance to the camera t. Hence, we can write the color of the corresponding pixel as [3]:where L e is the radiance emitted by the light source to the surface point, that was modeled and calibrated in the EndoMapper dataset [1] according to the SLS model from [16]. The bidirectional reflectance distribution function (BRDF) determines how much light is reflected to the camera, and the cosine term cos (θ) = -d • n weights the incoming radiance with respect to the surface normal n. Equation ( 3) also takes into account the camera gain g and gamma correction γ."
LightNeuS: Neural Surface Reconstruction in Endoscopy Using Illumination Decline,3.1,Using Illumination Decline as a Depth Cue,"The NeuS formulation of Sect. 2 assumes distant and fixed lighting. However, in endoscopy inverse-square light decline is significant, as quantified in Eq. ( 3).Accounting for this is done by modifying the original NeuS formulation as follows. Figure 1   c(x,d,n) may learn to model non-Lambertian BRDF(x, d), including specular highlights, and the cosine term of Eq. ( 3). However, if the distance t from the light to the point x is not provided to the color network, the 1/t 2 dependency cannot be learned, and surface reconstruction will fail. Our key insight is to explicitly supply this distance as input to the volume rendering algorithm, as shown in red in Fig. 1 and reformulate Eq. (1) asThis conceptually simple change, using illumination decline while training, unlocks all the power of neural surface reconstruction in endoscopy."
LightNeuS: Neural Surface Reconstruction in Endoscopy Using Illumination Decline,3.2,Endoscope Photometric Model,"Apart from illumination decline, there are several significant differences between the images captured by endoscopes and those conventionally used to train NeRFs and NeuS: fish-eye lenses, strong vignetting, uneven scene illumination, and postprocessing.Endoscopes use fisheye lenses to cover a wide field of view, usually close to 170 • . These lenses produce strong deformations, making it unwise to use the standard pinhole camera model. Instead, specific models [10,19] must be used. Hence, we also modified the original NeuS implementation to support these models.The light sources of endoscopes behave like spotlights. In other words, they do not emit with the same intensity in all directions, so L e in Eq. ( 3) is not constant for all image pixels. This effect is similar to the vignetting effect caused by conventional lenses, that is aggravated in fisheye lenses. Fortunately, they can be accurately calibrated [1,16] and compensated for.The post-processing software of medical endoscopes is designed to always display well-exposed images, so that physicians can see details correctly. An adaptive gain factor g is applied by the endoscope's internal logic and gamma correction is also used to adapt to non-linear human vision, achieving better contrast perception in mid tones and dark areas. Endoscope manufacturers know the post-processing logic of their devices, but this information is proprietary and not available to users. Again, gamma correction can be calibrated assuming it is constant [3], and the gain change between successive images can be estimated, for example, by sparse feature matching.All these factors must be taken into account during network training. Thus, our photometric loss is computed using a normalized image:(5)"
LightNeuS: Neural Surface Reconstruction in Endoscopy Using Illumination Decline,4,Experiments,"We validate our method on the C3VD dataset [4], which covers all different sections of the colon anatomy in 22 video sequences. This dataset contains sequences recorded with a medical video colonoscope, Olympus Evis Exera III CF-HQ190L. The images were recorded inside a phantom, a model of a human colon made of silicone. The intrinsic camera parameters are provided. The camera extrinsics for each frame are estimated by 2D-3D registration against the known 3D model. In an operational setting, we could use a structure-from-motion approach such as COLMAP [21] or a SLAM technique such as [8], which have been shown to work well in endoscopic settings. The gain values were easily estimated from the dataset itself. For vignetting, we use the calibration obtained from a colonoscope of the same brand and series from the EndoMapper dataset [1].During training, we follow the NeuS paper approach of using a few informative frames per scene, as separated as possible, by sampling each video uniformly. For each sequence, we train both the vanilla NeuS and our LighNeuS using 20 frames each time. They are extracted uniformly over the duration of the video. We use the same batch size and number of iterations as in the original NeuS paper, 512 and 300k respectively. Once the network is trained, we can extract triangulated meshes from the reconstruction. Since the C3VD dataset comprises a ground-truth triangle mesh, we compute point-to-triangle distances from all the vertices in the reconstruction to the closest ground-truth triangle.In the first rows of Table 1, we report median (MedAE), mean (MAE), and root mean square (RMSE) values of these distances for all vertices seen in at least one image. Columns show the result for 22 sequences. We note 18 sequences where the camera moved at least 1 cm, and the reconstruction yielded a mean error of 2.80 mm. The other four smaller trajectories (<1 cm) lack parallax and the mean error is higher (8.23 mm). This is in the range of reported accuracy in the literature for monocular dense non-watertight depth estimation, 1.1 mm in [14] for high parallax geometry in laparoscopy, which is a much more favorable geometry than the one we have here, or 0.85 mm for the significantly smaller-size cavities of endoscopic endonasal surgery (ESS) [11].In contrast, vanilla NeuS assumes constant illumination. The strong light changes typical of endoscopy fatally mislead the method. We only report numerical results of NeuS in two sequences because in all the rest, the SDF diverges and ends up blown out of the rendering volume, giving no result at all. The NeuS reconstruction exhibits multiple artifacts that make it unusable. Bottom: Our reconstruction is much closer to the ground truth shape. The error is shown in blue if the reconstruction is inside the surface, and in red otherwise. A fully saturated red or blue denotes an error of more than 1 cm and grey denotes no error at all. We provide a qualitative result in Fig. 2 and additional ones in the supplementary material. Note that the watertight prior inherent to an SDF allows the network to hallucinate unseen areas. Remarkably, these unsurveyed areas continue the tubular shape of the colon and we found them to be mostly accurate when compared to the ground truth. For example, the curved areas of the colon where a wall is occluded behind the corner of the curve is reconstructed, as shown in Fig. 3. This ability to ""fill in"" observation gaps may be useful in providing the endoscopist with an estimate of the percentage of unsurveyed area during a procedure.We hypothesize that this desirable behavior stems from the fact that the network learns an empirical shape prior from the observed anatomy of the colon. However, we don't expect this behavior to hold for distant unseen parts, but only for regions closer than 20 mm to one observation. In the last rows of Table 1, we compute accuracy metrics for this extended region. It includes not only surveyed areas, but also neighboring areas that were not observed."
LightNeuS: Neural Surface Reconstruction in Endoscopy Using Illumination Decline,5,Conclusion,"We have presented a method for 3D dense multi-view reconstruction from endoscopic images. We are the first to show that neural radiance fields can be used to obtain accurate dense reconstructions of colon sections of significant length. At the heart of our approach, is exploiting the correlation between depth and brightness. We have observed that, without it, neural reconstruction fails.The current method could be used offline for post-exploration coverage analysis and endoscopist training. But real-time performance could be achieved in the future as the new NeuS2 [26] converges in minutes, enabling automatic coverage reporting. Similar to other reconstruction methods, for now our approach works in areas of the colon where there is little deformation. Several sub-maps of non-deformed areas can be created if necessary. However, this limitation could be overcome by adopting the deformable NeRFs formalism [18]."
LightNeuS: Neural Surface Reconstruction in Endoscopy Using Illumination Decline,,Fig. 1 .,
LightNeuS: Neural Surface Reconstruction in Endoscopy Using Illumination Decline,,,
LightNeuS: Neural Surface Reconstruction in Endoscopy Using Illumination Decline,,Fig. 2 .,
LightNeuS: Neural Surface Reconstruction in Endoscopy Using Illumination Decline,,Fig. 3 .,
LightNeuS: Neural Surface Reconstruction in Endoscopy Using Illumination Decline,,Table 1 . Reconstruction error [mm] on the C3VD dataset. Surveyed: points,
LightNeuS: Neural Surface Reconstruction in Endoscopy Using Illumination Decline,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 48.
JCCS-PFGM: A Novel Circle-Supervision Based Poisson Flow Generative Model for Multiphase CECT Progressive Low-Dose Reconstruction with Joint Condition,1,Introduction,"The substantial reduction of scanning radiation dose and its accurate reconstruction are of great clinical significance for multiphase contrast-enhanced computed tomography (CECT) imaging. 1) Multiphase CECT requires multiple scans at different phases, such as arterial phase, venous phase, delayed phase and etc., to demonstrate the anatomy and lesion with the contrast agent evolution intra human body over time [1]. But such multiphase scans inherently lead to the accumulation of huge radiation dose for patients [2,3]. As shown in Fig. 1(a), after triple-phase CECT scanning, the radiation damage suffered by the patient is three times that of the single phase. Combined with ""as low as reasonably achievable"" (ALARA) principle [4], it is thus extremely urgent to greatly reduce the radiation dose and risk for clinical multiphase CECT examination. 2) However, the low-dose acquired CT image also exists the problems of noise interference and unclear structure. As enlarged region shown in Fig. 1(b), the low-dose CECT behaves much lower signal-to-noise ratio than normal-dose CECT. It brings great difficulty to read the anatomical structure with high noise, especially for inexperienced radiologist. Therefore, the high-quality reconstruction with more readable pattern is clinically crucial for multi-phase low-dose CECT diagnosis. As far as we know, most of the existing methods mainly focus on the singlephase low-dose CT (LDCT) reconstruction. Chen et al. [5] trained a deep CNN to transform LDCT images towards normal-dose CT images, patch by patch. In [6], a shortcut connections aided symmetrical CNN was adopt to predict noise distribution in LDCT. Shan et al. [7] attempted to transfer a trained 2D CNN to a 3D counterpart for low-dose CT image denoising. In [8], an attention residual dense network was developed for LDCT sinogram data denoising. In [9], low-dose sinogram-and image-domain networks were trained in a progressive way. Zhang et al. [10] further connected sinogram-and image-domain networks together for joint training. In [11] and [12], parallel network architectures were put forward for dual-domain information exchange and mutual optimization.Multi-phase low-dose CT reconstruction is still ignored, though single-phase methods behave promising results on their issues [5][6][7][8][9][10][11][12]. Due to multiple scans in a short time, it has the inherent challenges: 1) The serious noise pollution is caused by the higher requirement of using much lower scanning dose to decrease multiphase radiation accumulation, compared to the single-phase imaging. Thus, how to elegantly learn such mapping relation from the lower-dose CECT with more serious noise to normal-dose CECT is extremely critical. 2) Complex multiphase correlation with redundancy and interference is induced by the evolution of contrast agent in the human body. Except redundancy and interference, strong causality also obviously exists among multiphase. But how to deeply explore such consistency and evolution along the multiphase for further reducing the dose of later phase and improving imaging quality is still an open challenge.In this paper, guided with Joint Condition, a novel Circle-Supervision based Poisson Flow Generative Model (JCCS-PFGM) is proposed to make the progressive low-dose reconstruction for multiphase CECT. It deeply explores the correlation among multiphase and the mapping learning of PFGM, to progressively reduce the scanning radiation dose of multiphase CECT to the ultra low level of 5% dose, and achieve the high-quality reconstruction with noise reduction and structure maintenance. It thus significantly reduces the radiation risk of multiple CT scans in a short time, accompanied with clear multiphase CECT examination images. The main contributions of JCCS-PFGM can be summarized as: 1) an effectively progressive low-dose reconstruction mechanism is developed to leverages the imaging consistency and radiocontrast evolution along formerlatter phases, so that enormously reduces the radiation dose needs and improve the reconstruction effect, even for the latter-phase scanning with extremely low dose; 2) a newly-designed circle-supervision strategy is proposed in PFGM to enhance the refactoring capabilities of normalized poisson field learned from the perturbed space to the specified CT image space, so that boosts the explicit reconstruction for noise reduction; 3) a novel joint condition is designed to explore correlation between former phases and current phase, so that extracts the complementary information for current noisy CECT and guides the reverse process of diffusion jointly with multiphase condition for structure maintenance."
JCCS-PFGM: A Novel Circle-Supervision Based Poisson Flow Generative Model for Multiphase CECT Progressive Low-Dose Reconstruction with Joint Condition,2,Methodology,"As shown in Fig. 2, the proposed JCCS-PFGM is progressively performed on multiphase low-dose CECT to reduce the radiation risk in multiple CT imaging and make the high-quality reconstruction with noise reduction and structure maintenance. It is conducted with three special designs: 1) the progressive low-dose reconstruction mechanism (detailed in Sect. 2.1) reasonably utilizes the consistency along the multiphase CECT imaging, via phase-by-phase reducing the radiation dose and introducing the priori knowledge from former-phase reconstruction; 2) the circle-supervision strategy (detailed in Sect. 2.2) embedded in PFGM makes further self-inspection on normal poisson field prediction, via penalizing the deviation between the same-perturbed secondary diffusion; and 3) the joint condition (detailed in Sect. 2.3) integrates the multi-phase consistency and evolution in guiding the reverse process of diffusion, via fusing the complementary information from former phases into current ultra low-dose CECT. "
JCCS-PFGM: A Novel Circle-Supervision Based Poisson Flow Generative Model for Multiphase CECT Progressive Low-Dose Reconstruction with Joint Condition,2.1,Progressive Low-Dose Reconstruction Mechanism,"The progressive low-dose reconstruction mechanism effectively promotes highlevel base from former-phase to latter-phase for successively multiphase CECT reconstruction, instead of the casually equal dose reduction seriously breaking the structure in each phase. It further exploits the inherent consistency traceable along multiphase CECT to reduce the burden of multiphase reconstruction.As show in Fig. 2(a), the reasonable-designed progressive low-dose reconstruction mechanism arranges the dose from relatively high to low along the causal multiphase of phases I, II and III. With such mechanism, the reconstruction of former phase acquire more scanning information, benefit from relatively high dose. And the latter phase is granted with much more reliable priori knowledge, benefit from the consistently traceable former-phase reconstruction. Denote the low-dose CECT at phases I, II and III as x phase-I , x phase-II and x phase-III , the procedure is formulated as: where y phase-I , y phase-II and y phase-III represent reconstruction result, as well as R 1 (•), R 2 (•) and R 3 (•) mean reconstruction model."
JCCS-PFGM: A Novel Circle-Supervision Based Poisson Flow Generative Model for Multiphase CECT Progressive Low-Dose Reconstruction with Joint Condition,2.2,Circle-Supervision Strategy Embedded in PFGM,"The circle-supervision strategy robustly boosts the refactoring capabilities of normalized Poisson field learned by PFGM. So that it further promotes the explicit reconstruction for noise reduction, instead of just CT-similar image generation.PFGM is good at mapping a uniform distribution on a high-dimensional hemisphere into any data distribution [13]. It is inspired by electrostatics, and interpret initial image data points as electrical charges on the z = 0 hyperplane. Thus the initial image is able to be transformed into a uniform distribution on the hemisphere with radius r → ∞. It estimates normalized Poisson field with deep neural network (DNN) and thus further uses backward ordinary differential equation (ODE) to accelerate sampling.Here, we further propose the circle-supervision strategy on the normalized Poisson field which reflects the mapping direction from the perturbed space to the specified CT image space. It remedies the precise perception on the target initial CECT besides mapping direction learning, to enhance the crucial field components. As shown in Fig. 3, after randomly yield perturbed image through forward process, the DNN estimates the normalized Poisson field φ 1 . Then according to the normalized field calculation in the forward process, the Poisson field is returned with denormalization operation, and further temporarily restore the perturbed image into the initial CECT space. The secondary diffusion is conducted with same perturbtion in forward process and DNN in reverse process. Finally, the normal Poisson field of the secondary diffusion φ 2 is estimated. The deviation between φ 1 and φ 2 is penalized to boost the refactoring capabilities. Besides the temporary CECT is also yield in the secondary diffusion to enhance the robustness. Fig. 4. The joint condition comprehensively fuses the consistency and evolution from the former phases to enhance the current ultra low-dose CECT (take the phase III low-dose CECT reconstruction for example). It is composed of self-fusion among former phases for consistency, and cross-fusion between former and current phases for evolution."
JCCS-PFGM: A Novel Circle-Supervision Based Poisson Flow Generative Model for Multiphase CECT Progressive Low-Dose Reconstruction with Joint Condition,2.3,Joint Condition Fusing Multiphase Consistency and Evolution,"The joint condition comprehensively fuses the consistency and evolution from the previous phases to enhance the current ultra low-dose CECT. With such multiphase fusion, the reverse process of diffusion is able to get the successive guide to perceive radiocontrast evolution for structure maintenance As shown in Fig. 4, the joint condition consists of two parts: the self-fusion among previous phases for consistency and the cross-fusion between previous  and current phases for evolution. 1) For the self-fusion among former phases, it firstly encodes the combination of the reconstruction results of the previous phase I y phase-I and phase II y phase-II into the feature domain. And key map K pre , query map Q pre and value map V pre are then generated with further encoder and permutation. K pre and Q pre together establish the correlation weight, i.e., attention map Att pre , among former phases combination to explore the inherent consistency. Att pre further works on the value map V pre to extract the consistent information which is finally added on the first feature representation to get previous-phases embedding feat pre . The procedure is formulated as: 2) For the cross-fusion between previous and current phases, it uses feat pre to generate query map Q cur and value map V cur . The current phase III low-dose CECT x phase-III is encoded for key map K cur . Thus the evolution between the current phase and previous phases is explored between K cur and Q cur by attention map Att cur . Then complimentary evolution from previous phases is extracted from value map V cur with Att cur , and then added into the current phase. The procedure is formulated as: 3 Experiments  "
JCCS-PFGM: A Novel Circle-Supervision Based Poisson Flow Generative Model for Multiphase CECT Progressive Low-Dose Reconstruction with Joint Condition,3.1,Materials and Configurations,
JCCS-PFGM: A Novel Circle-Supervision Based Poisson Flow Generative Model for Multiphase CECT Progressive Low-Dose Reconstruction with Joint Condition,3.2,Results and Analysis,"Overall Performance. As the last column shown in  Comparison with Competing Methods. As shown in Table 2, the proposed JCCS-PFGM gains the best performance compared to FBP, RED-CNN [5], CLEAR [10] and DDPNet [12], with error decrease of 5.66 HU, PSNR increase of 3.62dB PSNR and SSIM improvement by 3.88% on average. Visually, Fig. 5 illustrates that the result from JCCS-PFGM preserved tiny structure in all the phases with the dose assignment scheme: phase I 30% of the total nominal dose, phase II 15% and phase III 5%. In the enlarged ROI where the interpretation is difficult with original LDCT images, our method revealed key details, such as the vessel indicated by the red arrow, much better than the compared methods."
JCCS-PFGM: A Novel Circle-Supervision Based Poisson Flow Generative Model for Multiphase CECT Progressive Low-Dose Reconstruction with Joint Condition,4,Conclusion,"In this paper, we propose JCCS-PFGM to make the progressive low-dose reconstruction for multiphase CECT. JCCS-PFGM t creatively consists of 1) the progressive low-dose reconstruction mechanism utilizes the consistency along the multiphase CECT imaging; 2) the circle-supervision strategy embedded in PFGM makes further self-inspection on normal poisson field prediction; 3) the joint condition integrates the multi-phase consistency and evolution in guiding the reverse process of diffusion. Extensive experiments with promising results from both quantitative evaluations and qualitative assessments reveal our method a great clinical potential in CT imaging."
JCCS-PFGM: A Novel Circle-Supervision Based Poisson Flow Generative Model for Multiphase CECT Progressive Low-Dose Reconstruction with Joint Condition,,Fig. 1 .,
JCCS-PFGM: A Novel Circle-Supervision Based Poisson Flow Generative Model for Multiphase CECT Progressive Low-Dose Reconstruction with Joint Condition,,Fig. 2 .,
JCCS-PFGM: A Novel Circle-Supervision Based Poisson Flow Generative Model for Multiphase CECT Progressive Low-Dose Reconstruction with Joint Condition,,y 1 )Fig. 3 .,
JCCS-PFGM: A Novel Circle-Supervision Based Poisson Flow Generative Model for Multiphase CECT Progressive Low-Dose Reconstruction with Joint Condition,,,
JCCS-PFGM: A Novel Circle-Supervision Based Poisson Flow Generative Model for Multiphase CECT Progressive Low-Dose Reconstruction with Joint Condition,,,
JCCS-PFGM: A Novel Circle-Supervision Based Poisson Flow Generative Model for Multiphase CECT Progressive Low-Dose Reconstruction with Joint Condition,,A,
JCCS-PFGM: A Novel Circle-Supervision Based Poisson Flow Generative Model for Multiphase CECT Progressive Low-Dose Reconstruction with Joint Condition,,Fig. 5 .,
JCCS-PFGM: A Novel Circle-Supervision Based Poisson Flow Generative Model for Multiphase CECT Progressive Low-Dose Reconstruction with Joint Condition,,Table 1 .,
JCCS-PFGM: A Novel Circle-Supervision Based Poisson Flow Generative Model for Multiphase CECT Progressive Low-Dose Reconstruction with Joint Condition,,Table 2 .,
JCCS-PFGM: A Novel Circle-Supervision Based Poisson Flow Generative Model for Multiphase CECT Progressive Low-Dose Reconstruction with Joint Condition,,Table,
JCCS-PFGM: A Novel Circle-Supervision Based Poisson Flow Generative Model for Multiphase CECT Progressive Low-Dose Reconstruction with Joint Condition,,,
Revealing Anatomical Structures in PET to Generate CT for Attenuation Correction,1,Introduction,"Positron emission tomography (PET) is a general nuclear imaging technique, which has been widely used to characterize tissue metabolism, protein deposition, etc. [9]. According to the PET imaging principle, radioactive tracers injected into the body involve in the metabolism and produce γ decay signals externally. However, due to photoelectric absorption and Compton scattering, the decay signals are attenuated when passing through human tissues to external receivers, resulting in incorrect tracer distribution reasoning (see non-attenuation corrected PET (NAC-PET) in Fig. 1(a)). To obtain correct tracer distribution (see AC-PET in Fig. 1(a)), attenuation correction (AC) on the received signals is required. Traditional AC accompanies additional costs caused by the simultaneously obtained MR or CT images which are commonly useless for diagnosis. The additional costs are especially significant for advanced total-body PET/CT scanners [12,13], which have effective sensitivity and low radiation dose during PET scanning but accumulative radiation dose during CT scanning. In another word, CT becomes a non-negligible source of radiation hazards. To reduce the costs, including expense, time, and radiation hazards, some studies proposed to conduct AC by exploiting each PET image itself. Researchers have been motivated to generate pseudo CT images from NAC-PET images [2,7], or more directly, to generate AC-PET images from NAC-PET images [5,11]. Since pseudo CT is convenient to be integrated into conventional AC processes, generating pseudo CT images is feasible in clinics for AC.The pseudo CT images should satisfy two-fold requests. Firstly, the pseudo CT images should be visually similar in anatomical structures to corresponding actual CT images. Secondly, PET images corrected by pseudo CT images should be consistent with that corrected by actual CT images. However, current techniques of image generation tend to produce statistical average values and patterns, which easily erase significant tissues (e.g., bones and lungs). As a result, for those tissues with relatively similar metabolism but large variances in attenuation coefficient, these methods could cause large errors as they are blind to the correct tissue distributions. Therefore, special techniques should be investigated to guarantee the fidelity of anatomical structures in these generated pseudo CT images.In this paper, we propose a deep learning framework, named anatomical skeleton enhanced generation (ASEG), to generate pseudo CT from NAC-PET for attenuation correction. ASEG focuses more on the fidelity of tissue distribution, i.e., anatomical skeleton, in pseudo CT images. As shown in Fig. 1(b), this framework contains two sequential modules structure prediction module G 1 and tissue rendering module G 2 }. G 1 devotes to delineating the anatomical skeleton from a NAC-PET image, thus producing a prior tissue distribution map to G 2 , while G 2 devotes to rendering the tissue details according to both the skeleton and NAC-PET image. We regard G 1 as a segmentation network that is trained under the combination of cross-entropy loss and Dice loss and outputs the anatomical skeleton. For training the generative module G 2 , we further propose the anatomical-consistency constraint to guarantee the fidelity of tissue distribution besides general constraints in previous studies. Experiments on four publicly collected PET/CT datasets demonstrate that our ASEG outperforms existing methods by preserving better anatomical structures in generated pseudo CT images and achieving better visual similarity in corrected PET images."
Revealing Anatomical Structures in PET to Generate CT for Attenuation Correction,2,Method,"We propose the anatomical skeleton enhanced generation (ASEG, as illustrated in Fig. (1) framework that regards the CT generation as two sequential tasks, i.e., skeleton prediction and tissue rendering, instead of simply mapping pseudo CT from NAC-PET. ASEG composes of two sequential generative modules {G 1 , G 2 } to deal with them, respectively. G 1 devotes itself to decoupling the anatomical skeleton from NAC-PET to provide rough prior information of attenuation coefficients to G 2 , particularly for lungs and bones that have the most influential variances. G 2 then devotes to rendering the tissue details in the CT pattern exploiting both the skeleton and NAC-PET images. In short, the skeleton decoupled by G 1 is a prior guidance to G 2 , and in turn, G 2 can serve as a target supervision for G 1 . These two modules are trained with different constraints according to the corresponding tasks. Specially, the general Dice loss and cross-entropy loss [16] are employed to guarantee G 1 for the fidelity of tissue distributions while general mean absolute error and feature matching losses are utilized to guarantee G 2 for potential coarse-to-fine semantic constraint. To improve the fidelity of anatomical structures, we further propose the anatomical consistency loss to encourage G 2 to generate CT images that are consistent in tissue distributions with actual CT images in particular.Network Architecture. As illustrated in Fig. 1(b), our ASEG has two generative modules for skeleton prediction and tissue rendering, respectively, where G 1 and G 2 share the same network structure but G 2 is accompanied by an adversarial network D (not drawn, same structure in [8]). Each generative network consists of an input convolutional layer, four encoding blocks, two residual blocks (RBs) [8], four decoding blocks, and an output convolutional layer. Each encoding block contains a RB and a convolutional layer with strides of 2 × 2 × 2 for downsampling while each decoding block contains an upsampling operation of 2 × 2 × 2 and a convolutional layer. The kernel size for the input and output convolutional layers is 7 × 7 × 7 while for others is 3 × 3 × 3. Skip connections are further used locally in RBs and globally between corresponding layers to empower information transmission. Meanwhile, the adversarial network D consists of five 4 × 4 × 4 convolutional layers with strides of 2 × 2 × 2 for the first four layers and 1 × 1 × 1 for the last layer.Model Formulation. Let X nac and X ac denote the NAC-PET and AC-PET images, and Y be the actual CT image used for AC. Since CT image is highly crucial in conventional AC algorithms, they generally have a relationship asunder an AC algorithm F. To avoid scanning an additional CT image, we attempt to predict Y from X nac as an alternative in AC algorithm. Namely, a mapping G is required to build the relationship between Y and X nac , i.e., Ŷ = G(X nac ).Then, X ac can be acquired byThis results in a pioneering AC algorithm that requires only a commonly reusable mapping function G for all PET images rather than a corresponding CT image Y for each PET image. As verified in some previous studies [1,2,7], G can be assigned by some image generation techniques, e.g. GANs and CNNs. However, since these general techniques tend to produce statistical average values, directly applying them may lead to serious brightness deviation, for those tissues with large intensity ranges. To overcome this drawback, we propose ASEG as a specialized AC technique, which decouple the CT generation process G in two sequential parts, i.e., G 1 for skeleton prediction and G 2 for tissue rendering, as formulated asHerein, G 1 devotes to delineating anatomical skeleton Y as from X nac , thus providing a prior tissue distribution to G 2 while G 2 devotes to rendering the tissue details from X nac and Ŷas = G 1 (X nac ).To avoid annotating the ground truth, Y as can be derived from the actual CT image by a segmentation algorithm (denoted as S : Y as = S(Y )). As different tissues have obvious differences in intensity ranges, we define S as a simple thresholding-based algorithm. Herein, we first smooth each non-normalized CT image with a small recursive Gaussian filter to suppress the impulse noise, and then threshold this CT image to four binary masks according to the Hounsfield scale of tissue density [6], including the air-lung mask (intensity ranges from -950HU to -125HU), the fluids-fat mask (ranges from -125HU to 10HU), the soft-tissue mask (ranges from 10HU to 100HU), and the bone mask (ranges from 100HU to 3000HU), as demonstrated by anatomical skeleton in Fig. 1(b). This binarization trick highlights the difference among different tissues, and thus is easier perceived.General Constraints. As mentioned above, two generative modules {G 1 , G 2 } work for two tasks, namely the skeleton prediction and tissue rendering, respectively. Thus, they are trained with different target-oriented constraints. In the training scheme, the loss function for G 1 is the combination of Dice loss L dice and cross-entropy loss L ce [16], denoted asMeanwhile, the loss function for G 2 combines the mean absolute error (MAE) L mae , perceptual feature matching loss L fm [14], and anatomical-consistency loss L ac , denoted aswhere the anatomical consistency loss L st is explained below.Anatomical consistency. It is generally known that CT images can provide anatomical observation because different tissues have a distinctive appearance in Hounsfield scale (linear related to attenuation coefficients). Therefore, it is crucial to ensure the consistency of tissue distribution in the pseudo CT images, tracking which we propose to use the tissue distribution consistency to guide the network learning. Based on the segmentation algorithm S, both the actual and generated CTs {Y, Ŷ } can be segmented to anatomical structure/tissue distribution masks {S(Y ), S( Ŷ )}, and their consistency can then be measured by Dice coefficient. Accordingly, the anatomical-consistency loss L ac is a Dice loss asDuring the inference phase, only the NAC-PET image of each input subject is required, where the pseudo CT image is derived by Ŷ ≈ G 2 (G 1 (X nac ), X nac )."
Revealing Anatomical Structures in PET to Generate CT for Attenuation Correction,3,Experiments,
Revealing Anatomical Structures in PET to Generate CT for Attenuation Correction,3.1,Materials,"The data used in our experiments are collected from The Cancer Image Archive (TCIA) [4] (https://www.cancerimagingarchive.net/collections/), where a series of public datasets with different types of lesions, patients, and scanners are open-access. Among them, 401, 108, 46, and 20 samples are extracted from the Head and Neck Scamorous Cell Carcinoma (HNSCC), Non-Small Cell Lung Cancer (NSCLC), The Cancer Genome Atlas (TCGA) -Head-Neck Squamous Cell Carcinoma (TCGA-HNSC), and TCGA -Lung Adenocarcinoma (TCGA-LUAD), respectively. We use these samples in HNSCC for training and in other three datasets for evaluation.Each sample contains co-registered (acquired with PET-CT scans) CT, PET, and NAC-PET whole-body scans. In our experiments, we re-sampled all of them to a voxel spacing of 2×2×2 and re-scaled the intensities of NAC-PET/AC-PET images to a range of [0, 1], of CT images by multiplying 0.001. The input and output of our ASEG framework are cropped patches with the size of 192 × 192 × 128 voxels. To achieve full-FoV output, the consecutive outputs of each sample are composed into a single volume where the overlapped regions are averaged."
Revealing Anatomical Structures in PET to Generate CT for Attenuation Correction,3.2,Comparison with Other Methods,"We compared our ASEG with three state-of-the-art methods, including (i ) a U-Net based method [3] that directly learns a mapping from NAC-PET to CT image with MAE loss (denoted as U-Net), (ii ) a conventional GAN-based method [1,2] that uses the U-Net as the backbone and employ the style-content loss and adversarial loss as an extra constraint (denoted as CGAN), and (iii ) an auxiliary GAN-based method [10] that uses the CT-based segmentation (i.e., the simple thresholding S) as an auxiliary task for CT generation (denoted as AGAN). For a fair comparison, we implemented these methods by ourselves in a TensorFlow platform with an NVIDIA 3090 GPU. All methods share the same backbone structure as G * in Fig. 1(b) and follow the same experimental settings. Particularly, the adversarial loss of methods (ii ) and (iii ) are replaced by the perceptual feature matching loss. These two methods could be considered as variants of our method without using predicted prior anatomic skeleton.Quantitative Analysis of CT. As the most import application of CT that is to display the anatomical information, we propose to measure the anatomical consistency between the pseudo CT images and actual CT images, where the Dice coefficients on multiple anatomical regions that extracted from the pseudo/actual CT images are calculated. To avoid excessive self-referencing in evaluating anatomical consistency, instead of employing the simple thresholding segmentation (i.e., S), we resort to the open-access TotalSegmentator [15] to finely segment the actual and pseudo CT images to multiple anatomical structures, and compose them to nine independent tissues for simplifying result , where the following conclusions can be drawn. Firstly, U-Net and CGAN generate CT images with slightly better global intensity similarity but worse anatomical consistency in some tissues than AGAN and ASEG. This indicates that the general constraints (MAE and perceptual feature matching) cannot preserve the tissue distribution since they tend to produce statistical average values or patterns, particularly in these regions with large intensity variants. Secondly, AGAN achieves the worst intensity similarity and anatomical consistency for some organs. Such inconsistent metrics suggest that the global intensity similarity may have a competing relationship with anatomical consistency in the learning procedure, thus it is not advisable to balance them in a single network. Thirdly, CGAN achieves better anatomical consistency than U-Net, but worse than ASEG. It implies that the perceptual feature matching loss can also identify the variants between different tissues implicitly but cannot compare to our strategy to explicitly enhance the anatomical skeleton. Fourthly, our proposed ASEG achieves the best anatomical consistency for all tissues, indicating it is reasonable to enhance tissue variations. In brief, the above results supports the strategy to decouple the skeleton prediction as a preceding task is effective for CT generation.Effectiveness in Attenuation Correction. As the pseudo CT images generated from NAC-PET are expected to be used in AC, it is necessary to further evaluate the effectiveness of pseudo CT images in PET AC. Because we cannot access the original scatters [5], inspired by [11], we propose to resort CGAN to simulate the AC process, denoted as ACGAN and trained on HNSCC dataset. The input of ACGAN is a concatenation of NAC-PET and actual CT, while the output is actual AC-PET. To evaluate the pseudo CT images, we simply use them to take place of the actual CT. Four metrics, including the Peak Signal to Noise Ratio (PSNR), Mean Absolute Error (MAE), Normalized Cross Correlation (NCC), and SSIM, are used to measure ACGAN with pseudo CT images on test datasets (NSCLC, TCGA-HNSC, and TCGA-LUDA). The results are reported in Table 1(b), where the fourth column list the ACGAN results with actual CT images. Meanwhile, we also report the results of direct mapping NAC-PET to AC-PET without CT images in the third column (""No CT""), which is trained from scratch and independent from ACGAN.It can be observed from Table 1(b) that: (1) ACGAN with actual CT images can predict images very close to the actual AC-PET images, thus is qualified to simulate the AC process; (2) With actual or pseudo CT images, ACGAN can predict images closer to the actual AC-PET images than without CT, demonstrating the necessity of CT images in process of PET AC; (3) These pseudo CTs cannot compare to actual CTs, reflecting that there exist some relative information that can hardly be mined from NAC-PET; (4) The pseudo CTs generated by ASEG achieve the best in three metrics (MAE, PSNR, NCC) and second in the other metric (SSIM), demonstrating the advance of our ASEG. Figure 2 displayed the detailed diversity of the AC-PET corrected by different pseudo CTs. It can be found that the structures of AC-PET are highly dependent on CT, particularly the lung regions. However, errors in corners and shapes are relatively large (see these locations marked by red arrows), which indicates there are still some space in designing more advanced mapping methods. Nonetheless, compared to other pseudo CTs, these generated by ASEG result in more realistic AC-PET with fewer errors, demonstrating the AC usability of ASEG."
Revealing Anatomical Structures in PET to Generate CT for Attenuation Correction,4,Conclusion,"In this paper, we proposed the anatomical skeleton-enhance generation (ASEG) to generate pseudo CT images for PET attenuation correction (AC), with the goal of avoiding acquiring extra CT or MR images. ASEG divided the CT generation into the skeleton prediction and tissue rendering, two sequential tasks, addressed by two designed generative modules. The first module delineates the anatomical skeleton to explicitly enhance the tissue distribution which are vital for AC, while the second module renders the tissue details based on the anatomical skeleton and NAC-PET. Under the collaboration of two modules and specific anatomical-consistency constraint, our ASEG can generate more reasonable pseudo CT from NAC-PET. Experiments on a collection of public datasets demonstrate that our ASEG outperforms existing methods by achieving advanced performance in anatomical consistency. Our study support that ASEG could be a promising and lower-cost alternative of CT acquirement for AC. Our future work will extend our study to multiple PET tracers."
Revealing Anatomical Structures in PET to Generate CT for Attenuation Correction,,Fig. 1 .,
Revealing Anatomical Structures in PET to Generate CT for Attenuation Correction,,Fig. 2 .,
Revealing Anatomical Structures in PET to Generate CT for Attenuation Correction,,Table 1 .,"report, e.g., lung (Dice l ), heart (Dice h ), liver (Dice li ), kidneys (Dice k ), blood vessels (Dice k ), digestive system (Dice d ), ribs (Dice r ), vertebras (Dice v ), and iliac bones (Dice ib ). Additionally, the Structure Similarity Index Measure (SSIM) values are also reported to measure the global intensity similarity.Results of various methods are provided in Table1(a)"
Revealing Anatomical Structures in PET to Generate CT for Attenuation Correction,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5_3.
DiffuseIR: Diffusion Models for Isotropic Reconstruction of 3D Microscopic Images,1,Introduction,"Three-dimensional (3D) microscopy imaging is crucial in revealing biological information from the nanoscale to the microscale. Isotropic high resolution across M. Pan and Y. Gan-Equal contribution.all dimensions is desirable for visualizing and analyzing biological structures. Most 3D imaging techniques have lower axial than lateral resolution due to physical slicing interval limitation or time-saving consideration [8,18,23,28,31]. Effective isotropic super-resolution algorithms are critical for high-quality 3D image reconstructions, such as electron microscopy and fluorescence microscopy.Recently, deep learning methods have made significant progress in image analysis [9,13,14,25]. To address the isotropic reconstruction problem, [9] employs isotropic EM images to generate HR-LR pairs at axial and train a super-resolution model in a supervised manner, demonstrating the feasibility of inferring HR structures from LR images. [29,30] use 3D point spread function (PSF) as a prior for self-supervised super-resolution. However, isotropic highresolution images or 3D point spread function (PSF) physical priors are difficult to obtain in practical settings, thus limiting these algorithms. Some methods like [3,21] have skillfully used cycleGAN [32] architecture to train axial superresolution models without depending on isotropic data or physical priors. They learn from unpaired matching between high-resolution 2D slices in the lateral plane and low-resolution 2D slices in the axial plane, achieving impressive performance. However, these methods train models in fixed imaging settings and suffer from degraded performance caused by artifacts and blurring when facing unseen anisotropic factors. This limits their generality in practice [6]. In conclusion, a more robust paradigm needs to be proposed. Recently, with the success of the diffusion model in the image generation field [4,11,17,19,26], researchers applied the diffusion model to various medical image generation tasks and achieved impressive results [1,12,20,22,25]. Inspired by these works, we attempt to introduce diffusion models to address the isotropic reconstruction problem.This paper proposes DiffuseIR, an unsupervised method based on diffusion models, to address the isotropic reconstruction problem. Unlike existing methods, DiffuseIR does not train a specific super-resolution model from low-axialresolution to high-axial-resolution. Instead, we pre-train a diffusion model θ to learn the structural distribution p θ (X lat ) of biological tissue from lateral microscopic images X lat , which resolution is naturally high. Then, as shown in Fig. 1, we propose a Sparse Spatial Condition Sampling (SSCS) to condition the reversediffusion process of θ . SSCS extracts sparse structure context from low-axialresolution slice x axi and generate reconstruction result x 0 ∼ p θ (X lat |x axi ). Since θ learns the universal structural distribution p θ , which is independent of the axial resolution, DiffuseIR can leverage the flexibility of SSCS to reconstruct authentic images with unseen anisotropic factors without requiring re-training. To further improve the quality of reconstruction, we propose a Refine-in-loop strategy to enhance the authenticity of image details with fewer sampling steps.To sum up, our contributions are as follows:(1) We are the first to introduce diffusion models to isotropic reconstruction and propose DiffuseIR. Benefiting from the flexibility of SSCS, DiffuseIR is naturally robust to unseen anisotropic spatial resolutions. (2) We propose a Refine-in-loop strategy, which maintains performance with fewer sampling steps and better preserves the authenticity of the reconstructed image details. (3) We perform extensive experiments on EM data with different imaging settings and achieve SOTA performance. Our unsupervised method is competitive with supervised methods and has much stronger robustness."
DiffuseIR: Diffusion Models for Isotropic Reconstruction of 3D Microscopic Images,2,Methodology,"As shown in Fig. 1, DiffuseIR address isotropic reconstruction by progressively conditions the denoising process of a pre-trained diffusion model θ . Our method consists of three parts: DDPM pre-train, Sparse Spatial Condition Sampling and Refine-in-loop strategy.DDPM Pretrain on Lateral. Our method differs from existing approaches that directly train super-resolution models. Instead, we pre-train a diffusion model to learn the distribution of high-resolution images at lateral, avoiding being limited to a specific axial resolution. Diffusion models [10,19] employ a Markov Chain diffusion process to transform a clean image x 0 into a series of progressively noisier images during the forward process. This process can be simplified as:where α t controls the scale of noises. During inference, the model θ predicts x t-1 from x t . A U-Net θ is trained for denoising process p θ , which gradually reverses the diffusion process. This denoising process can be represented as:During training, we use 2D lateral slices, which is natural high-resolution to optimize θ by mean-matching the noisy image obtained in Eq. 1 using the MSE loss [10]. Only HR slices at lateral plane X lat were used for training, so the training process is unsupervised and independent of the specific axial resolution. So that θ learns the universal structural distribution of biological tissues and can generate realistic HR images following p θ (X lat ).Sparse Spatial Condition Sampling on Axial. We propose Sparse Spatial Condition Sampling (SSCS) to condition the generation process of θ and generate high-axial-resolution reconstruction results. SSCS substitutes every reversediffusion step Eq. 2. We first transform the input axial LR slice x axi to match the lateral resolution by intra-row padding: (α -1) rows of zero pixels are inserted between every two rows of original pixels, where α is the anisotropic spatial factor. We denote M as the mask for original pixels in x con 0 , while (1 -M ) represents those empty pixels inserted. In this way, we obtain x con 0 , which reflects the sparse spatial content at axial, and further apply Eq. 1 to transform noise level:Algorithm 1: Isotropic reconstruction using basic DiffuseIRThen, SSCS sample x t-1 at any time step t, conditioned on x con t-1 . The process can be described as follows:where x * t-1 is obtained by sampling from the model θ using Eq. 2, with x t of the previous iteration. x * t-1 and x con t-1 are combined with M . By iterative denoising, we obtain the reconstruction result x 0 . It conforms to the distribution p θ (X lat ) learned by the pre-trained diffusion model and maintains semantic consistency with the input LR axial slice. Since SSCS is parameter-free and decoupled from the model training process, DiffuseIR can adapt to various anisotropic spatial resolutions by modifying the padding factor according to α while other methods require re-training. This makes DiffuseIR a more practical and versatile solution for isotropic reconstruction.Refine-in-Loop Strategy. We can directly use SSCS to generate isotropic results, but the reconstruction quality is average. The diffusion model is capable of extracting context from the sparse spatial condition. Still, we have discovered a phenomenon of texture discoordination at the mask boundaries, which reduces the reconstruction quality. For a certain time step t, the content of x * t-1 may be unrelated to x con t-1 , resulting in disharmony in x t-1 generated by SSCS. During the denoising of the next time step t-1, the model tries to repair the disharmony of x t-1 to conform to p θ distribution. Meanwhile, this process will introduce new inconsistency and cannot converge on its own. To overcome this problem, we propose the Refine-in-loop strategy: For x t-1 generated by SSCS at time step t, we apply noise to it again and obtain a new x t and then repeat SSCS at time step t. Our discovery showed that this uncomplicated iterative refinement method addresses texture discoordination significantly and enhances semantic precision.The total number of inference steps in DiffuseIR is given by T total = T • K. As T total increases, it leads to a proportional increase in the computation time of our method. However, larger T total means more computational cost. Recent works such as [15,16,24] have accelerated the sampling process of diffusion models by reducing T while maintaining quality. For DiffuseIR, adjusting the sampling strategy is straightforward. Lowering T and raising refinement iterations K improves outcomes with a fixed T total . We introduce and follow the approach presented in DDIM [24] as an example and conducted detailed ablation experiments in Sec. 3 to verify this. Our experiments show that DiffuseIR can benefit from advances in the community and further reduce computational overhead in future work."
DiffuseIR: Diffusion Models for Isotropic Reconstruction of 3D Microscopic Images,3,Experiments and Discussion,"Dataset and Implement Details. To evaluate the effectiveness of our method, we conducted experiments on two widely used public EM datasets, FIB-25 [27] and Cremi [5]. FIB-25 contains isotropic drosophila medulla connectome data obtained with FIB-SEM. We partitioned it into subvolumes of 256 × 256 × 256 as ground truth and followed [9] to perform average-pooling by factor α(x2,x4,x8) along the axis to obtain downsampled anisotropic data. Cremi consists of drosophila brain data with anisotropic spatial resolution. We followed [3] to generate LR images with a degradation network and conduct experiments on lateral slices. All resulting images were randomly divided into the training (70%), validation (15%) and test (15%) set. For the pre-training of the diffusion model, we follow [19] by using U-Net with multi-head attention and the same training hyper-parameters. We use 256×256 resolution images with a batch size of 4 and train the model on 8×V100 GPUs. For our sampling setting, we set T, K = 25, 40, which is a choice selected from the ablation experiments in Sec. 3 that balances performance and speed.Quantitative and Visual Evaluation. To evaluate the effectiveness of our method, we compared DiffuseIR with SoTA methods and presented the quantitative results in Table 1. We use PSNR and SSIM to evaluate results. PSNR is calculated using the entire 3D stack. SSIM is evaluated slice by slice in XZ and YZ viewpoints, with scores averaged for the final 3D stack score, measuring quality and 3D consistency. We use cubic interpolation as a basic comparison. 3DSRUNet [9] is a seminal isotropic reconstruction method, which requires HR and LR pairs as ground truth for supervised training. CycleGAN-IR [3] proposed an unsupervised approach using a CycleGAN [32] architecture, learning from unpaired axial and lateral slices. These methods train specialized models for a fixed anisotropic spatial setting and must be retrained for unseen anisotropic factors α, shown in Table 1. Despite being trained solely for denoising task and not exposed to axial slices, DiffuseIR outperforms unsupervised baselines and is competitive with supervised methods [9]. As shown in Fig. 2, our refine-in-loop strategy produces results with greater visual similarity to the Ground Truth, avoiding distortion and blurriness of details. Notably, the versatility afforded by Table 1. Quantitative evaluation of DiffuseIR against baselines. PSNR↑ and SSIM↑ are used as evaluation metrics. We evaluated the FIB25 and Cremi datasets, considering three anisotropic spatial resolutions, α = 2, 4, 8. Unlike other baselines which train a dedicated model for each α, our method only trains a single, generalizable model.   SSCS allows DiffuseIR to achieve excellent results using only one model, even under different isotropic resolution settings. This indicates that DiffuseIR overcomes the issue of generalization to some extent in practical scenarios, as users no longer need to retrain the model after modifying imaging settings."
DiffuseIR: Diffusion Models for Isotropic Reconstruction of 3D Microscopic Images,,Methzod,"Further Analysis on Robustness. We examined the robustness of our model to variations in both Z-axis resolutions and domain shifts. Specifically, we investigated the following: (a) Robustness to unseen anisotropic spatial factors. The algorithm may encounter unseen anisotropic resolution due to the need for different imaging settings in practical applications. To assess the model's robustness to unseen anisotropic factors, we evaluated the model trained with the anisotropic factor α = 4. Then we do inference under the scenario of anisotropic factor α = 8. For those methods with a fixed super-resolution factor, we use cubic interpolation to upsample the reconstructed result by 2x along the axis. (b) Robustness to the domain shifts. When encountering unseen data in the real world, domain shifts often exist, such as differences in biological structure features and physical resolution, which can impact the model's performance [2,7]. To evaluate the model's ability to handle those domain shifts, we trained our model on one dataset and tested it on another dataset. Analysis: As shown in Fig. 3, DiffuseIR shows greater robustness than other methods. In scenario (a), other methods are trained on specific anisotropic factors for super-resolution of axial LR to lateral HR. This can result in model fragility during testing with unseen anisotropic resolutions. In contrast, DiffuseIR directly learns the universal structural distribution at lateral through generation task, applicable to various axial resolutions. All methods exhibit decreased performance in scenario (b). However, DiffuseIR shows a small performance degradation with the help of the multi-step generation of the diffusion model and sparse spatial constraints imposed by SSCS at each reverse-diffusion step. Further, compared to the pre-vious methods predicting the result by one step, DiffuseIR makes the generating process more robust and controllable by adding constraints at each step to prevent the model from being off-limit. The results show that when the number of total steps is fixed, increase K will lead to higher PSNR.Ablation Study. We conducted extensive ablation experiments Fig. 4. First, to demonstrate the effectiveness of SSCS, we use it only in partially alternate reverse-diffusion steps, such as 1/4 or 1/2 steps. As shown in Fig. 4 (a), increasing the frequency of SSCS significantly improves PSNR while bringing negligible additional computational costs. This indicates that SSCS have a vital effect on the model's performance. Second, for the Refine-in-loop strategy, results show that keeping the total number of steps unchanged (reducing the number of time steps T while increasing the refine iterations K) can markedly improve performance. Figure 4  "
DiffuseIR: Diffusion Models for Isotropic Reconstruction of 3D Microscopic Images,4,Conclusion,"We introduce DiffuseIR, an unsupervised method for isotropic reconstruction based on diffusion models. To the best of our knowledge, We are the first to introduce diffusion models to solve this problem. Our approach employs Sparse Spatial Condition Sampling (SSCS) and a Refine-in-loop strategy to generate results robustly and efficiently that can handle unseen anisotropic resolutions. We evaluate DiffuseIR on EM data. Experiments results show our methods achieve SoTA methods and yield comparable performance to supervised methods. Additionally, our approach offers a novel perspective for addressing Isotropic Reconstruction problems and has impressive robustness and generalization abilities."
DiffuseIR: Diffusion Models for Isotropic Reconstruction of 3D Microscopic Images,5,Limitation,"DiffuseIR leverages the powerful generative capabilities of pre-trained Diffusion Models to perform high-quality isotropic reconstruction. However, this inevitably results in higher computational costs. Fortunately, isotropic reconstruction is typically used in offline scenarios, making DiffuseIR's high computational time tolerable. Additionally, the community is continuously advancing research on accelerating Diffusion Model sampling, from which DiffuseIR can benefit."
DiffuseIR: Diffusion Models for Isotropic Reconstruction of 3D Microscopic Images,,Fig. 1 .,
DiffuseIR: Diffusion Models for Isotropic Reconstruction of 3D Microscopic Images,,Fig. 2 .,
DiffuseIR: Diffusion Models for Isotropic Reconstruction of 3D Microscopic Images,,Fig. 3 .,
DiffuseIR: Diffusion Models for Isotropic Reconstruction of 3D Microscopic Images,,Fig. 4 .,
DiffuseIR: Diffusion Models for Isotropic Reconstruction of 3D Microscopic Images,,,
DiffuseIR: Diffusion Models for Isotropic Reconstruction of 3D Microscopic Images,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5_31.
FreeSeed: Frequency-Band-Aware and Self-guided Network for Sparse-View CT Reconstruction,1,Introduction,"X-ray computed tomography (CT) is an established diagnostic tool in clinical practice; however, there is growing concern regarding the increased risk of cancer induction associated with X-ray radiation exposure [14]. Lowering the dose of CT scans has been widely adopted in clinical practice to address this issue, following the ""as low as reasonably achievable"" (ALARA) principle in the medical community [9]. Sparse-view CT is one of the effective solutions, which reduces the radiation by only sampling part of the projection data for image reconstruction. Nevertheless, images reconstructed by the conventional filtered back-projection (FBP) present severe artifacts, thereby compromising their clinical value.In recent years, the success of deep learning has attracted much attention in the field of sparse-view CT reconstruction. Existing learning-based approaches mainly include image-domain methods [2,4,18] and dual-domain ones [7,13,16], both involving image post-processing to restore a clean CT image from the low-quality one with streak artifacts. For the image post-processing, residual learning [3] is often employed to encourage learning the artifacts hidden in the residues, which has become a proven paradigm for enhancing the performance [2,4,6,16]. Unfortunately, existing image post-processing methods may fail to model the globally distributed artifacts within the image domain. They can also produce over-smoothed images due to the lack of differentiated supervision for each pixel. In this paper, we advance image post-processing to benefit both classical image-domain methods and the dominant dual-domain ones.Motivation. We view the sparse-view CT image reconstruction as a two-step task: artifact removal and detail recovery. For the former, few work has investigated the fact that the artifacts exhibit similar pattern across different sparseview scenarios, which is evident in Fourier domain as shown in Fig. 1: they are aggregated mainly in the mid-frequency band and gradually migrate from low to high frequencies as the number of views increases. Inspired by this, we propose a frequency-band-aware artifact modeling network (FreeNet) that learns the artifact-concentrated frequency components to remove the artifacts efficiently using learnable band-pass attention maps in the Fourier domain.While Fourier domain band-pass maps help capture the pattern of the artifacts, restoring the image detail contaminated by strong artifacts may still be difficult due to the entanglement of artifacts and details in the residues. Consequently, we propose a self-guided artifact refinement network (SeedNet) that provides supervision signals to aid FreeNet in refining the image details contaminated by the artifacts. With these novel designs, we introduce a simple yet effective model termed FREquency-band-awarE and SElf-guidED network (FreeSeed), which enhances the reconstruction by modeling the pattern of artifacts from a frequency perspective and utilizing the artifact to restore the details. FreeSeed achieves promising results with only image data and can be further enhanced once the sinogram is available.Our contributions can be summarized as follows: 1) a novel frequency-bandaware network is introduced to efficiently capture the pattern of global artifacts in the Fourier domain among different sparse-view scenarios; 2) to promote the restoration of heavily corrupted image detail, we propose a self-guided artifact refinement network that ensures targeted refinement of the reconstructed image and consistently improves the model performance across different scenarios; and 3) quantitative and qualitative results demonstrate the superiority of FreeSeed over the state-of-the-art sparse-view CT reconstruction methods. "
FreeSeed: Frequency-Band-Aware and Self-guided Network for Sparse-View CT Reconstruction,2,Methodology,
FreeSeed: Frequency-Band-Aware and Self-guided Network for Sparse-View CT Reconstruction,2.1,Overview,"Given a sparse-view sinogram with projection views N v , let I s and I f denote the directly reconstructed sparse-and full-view images by FBP, respectively. In this paper, we aim to construct an image-domain model to effectively recover I s with a level of quality close to I f . The proposed framework of FreeSeed is depicted in Fig. 2, which mainly consists of two designs: FreeNet that learns to remove the artifact and is built with band-pass Fourier convolution blocks that better capture the pattern of the artifact in Fourier domain; and SeedNet as a proxy module that enables FreeNet to refine the image detail under the guidance of the predicted artifact. Note that SeedNet is involved only in the training phase, additional computational cost will not be introduced in the application. The parameters of FreeNet and SeedNet in FreeSeed are updated in an iterative fashion."
FreeSeed: Frequency-Band-Aware and Self-guided Network for Sparse-View CT Reconstruction,2.2,Frequency-Band-Aware Artifact Modeling Network,"To learn the globally distributed artifact, FreeNet uses band-pass Fourier convolution blocks as the basic unit to encode artifacts from both spatial and frequency aspects. Technically, Fourier domain knowledge is introduced by fast Fourier convolution (FFC) [1], which benefits from the non-local receptive field and has shown promising results in various computer vision tasks [12,17]. The features fed into FFC are split evenly along the channel into a spatial branch composed of vanilla convolutions and a spectral branch that applies convolution after real Fourier transform, as shown in Fig. 2. Despite the effectiveness, a simple Fourier unit in FFC could still preserve some low-frequency information that may interfere with the learning of artifacts, which could fail to accurately capture the banded pattern of the features of sparse-view artifacts in the frequency domain. To this end, we propose to incorporate learnable band-pass attention maps into FFC. Given an input spatial-domain feature map X in ∈ R Cin×H×W , the output X out ∈ R Cout×H×W through the Fourier unit with learnable band-pass attention maps is obtained as follows:where F real and F -1 real denote the real Fourier transform and its inverse version, respectively. f denotes vanilla convolution. "" "" is the Hadamard product. Specifically, for c-th channel frequency domain feature Z (c) in ∈ C U ×V (c = 1, ..., C in ), the corresponding band-pass attention map H (c) ∈ R U ×V is defined by the following Gaussian transfer function:where D (c) is the c-th channel of the normalized distance map with entries denoting the distance from any point (u, v) to the origin. Two learnable parameters, w (c) > 0 and d, represent the bandwidth and the normalized inner radius of the band-pass map, respectively, and are initialized as 1 and 0, respectively. is set to 1 × 10 -12 to avoid division by zero. The right half part of the second row of Fig. 1 shows some samples of the band-pass maps.The pixel-wise difference between the predicted artifact A of FreeNet and the groundtruth artifact A f = I s -I f is measured by 2 loss:(4)"
FreeSeed: Frequency-Band-Aware and Self-guided Network for Sparse-View CT Reconstruction,2.3,Self-guided Artifact Refinement Network,"Areas heavily obscured by the artifact should be given more attention, which is hard to achieve using only FreeNet. Therefore, we propose a proxy network SeedNet that provides supervision signals to focus FreeNet on refining the clinical detail contaminated by the artifact under the guidance of the artifact itself. SeedNet consists of residual Fourier convolution blocks. Concretely, given sparseview CT images I s , FreeNet predicts the artifact A and restored image I = I s -A; the latter is fed into SeedNet to produce targeted refined result I. To guide the network on refining the image detail obscured by heavy artifacts, we design the transformation T that turns A into a mask M using its mean value as threshold: M = T ( A), and define the following masked loss for SeedNet:(5)"
FreeSeed: Frequency-Band-Aware and Self-guided Network for Sparse-View CT Reconstruction,2.4,Loss Function for FreeSeed,"FreeNet and SeedNet in our proposed FreeSeed are trained in an iterative fashion, where SeedNet is updated using L mask defined in Eq. ( 5), and FreeNet is trained under the guidance of the total loss:where α > 0 is empirically set as 1. The pseudo-code for the training process and the exploration on the selection of α can be found in our Supplementary Material.Once the training is complete, SeedNet can be dropped and the prediction is done by FreeNet."
FreeSeed: Frequency-Band-Aware and Self-guided Network for Sparse-View CT Reconstruction,2.5,Extending FreeSeed to Dual-Domain Framework,"Dual-domain methods are effective in the task of sparse-view CT reconstruction when the sinogram data are available. To further enhance the image reconstruction quality, we extend FreeSeed to the dominant dual-domain framework by adding the sinogram-domain sub-network from DuDoNet [7], where the resulting dual-domain counterpart shown in Fig. 3 is called FreeSeed dudo . The sinogramdomain sub-network involves a mask U-Net that takes in the linearly interpolated sparse sinogram S s , where a binary sinogram mask M s that outlines the unseen part of the sparse-view sinogram is concatenated to each stage of the U-Net encoder. The mask U-Net is trained using sinogram loss L sino and Radon consistency loss L rc . We refer the readers to Lin et al. [7] for more information. "
FreeSeed: Frequency-Band-Aware and Self-guided Network for Sparse-View CT Reconstruction,3,Experiments,
FreeSeed: Frequency-Band-Aware and Self-guided Network for Sparse-View CT Reconstruction,3.1,Experimental Settings,"We conduct experiments on the dataset of ""the 2016 NIH-AAPM Mayo Clinic Low Dose CT Grand Challenge"" [8], which contains 5,936 CT slices in 1 mm image thickness from 10 anonymous patients, where a total of 5,410 slices from 9 patients, resized to 256 × 256 resolution, are randomly selected for training and the 526 slices from the remaining one patient for testing without patient overlap. Fan-beam CT projection under 120 kVp and 500 mA is simulated using TorchRadon toolbox [11]. Specifying the distance from the X-ray source to the rotation center as 59.5 cm and the number of detectors as 672, we generate sinograms from full-dose images with multiple sparse views N v ∈ {18, 36, 72, 144} uniformly sampled from full 720 views covering [0, 2π].The models are implemented in PyTorch [10] and are trained for 30 epochs with a mini-batch size of 2, using Adam optimizer [5] with (β 1 , β 2 ) = (0.5, 0.999) and a learning rate that starts from 10 -4 and is halved every 10 epochs. Experiments are conducted on a single NVIDIA V100 GPU using the same setting. All sparse-view CT reconstruction methods are evaluated quantitatively in terms of root mean squared error (RMSE), peak signal-to-noise ratio (PSNR), and structural similarity (SSIM) [15]."
FreeSeed: Frequency-Band-Aware and Self-guided Network for Sparse-View CT Reconstruction,3.2,Overall Performance,"We compare our models (FreeSeed and FreeSeed dudo ) with the following reconstruction methods: direct FBP, DDNet [18], FBPConv [4], DuDoNet [7], and DuDoTrans [13]. FBPConv and DDNet are image-domain methods, while DuDoNet and DuDoTrans are state-of-the-art dual-domain methods effective for CT image reconstruction. Table 1 shows the quantitative evaluation.Not surprisingly, we find that the performance of conventional image-domain methods is inferior to the state-of-the-art dual-domain method, mainly due to the failure of removing the global artifacts. We notice that dual-domain methods underperform FBPConv when N v = 18 because of the secondary artifact induced by the inaccurate sinogram restoration in the ultra-sparse scenario. Notably, FreeSeed outperforms the dual-domain methods in most scenarios. Figure 4 provides the visualization results for different methods. In general, FreeSeed successfully restores the tiny clinical structures (the spines in the first row, and the ribs in the second row) while achieving more comprehensive artifact removal (see the third row). Note that when the sinogram data are available, dual-domain counterpart FreeSeed dudo gains further improvements, showing the great flexibility of our model.  6) FreeNet trained with L mask using 1 norm (FreeSeed 1 ); and (7) FreeNet trained with L mask using 2 norm, i.e., the full version of our model (FreeSeed)."
FreeSeed: Frequency-Band-Aware and Self-guided Network for Sparse-View CT Reconstruction,3.3,Ablation Study,"By comparing the first two rows of Table 2, we find that simply applying FFC provides limited performance gains. Interestingly, we observe that the advantage of band-pass attention becomes more pronounced given more views, which can be seen in the last row of Fig. 1 where the attention maps are visualized by averaging all inner radii and bandwidths in different stages of FreeNet and calculating the map following Eq. ( 2). Figure 1 shows that these maps successfully capture the banded pattern of the artifact, especially in the cases of N v = 36, 72, 144 where artifacts are less entangled with the image content and present a banded shape in the frequency domain. Thus, the band-pass attention maps lead to better convergence. The effectiveness of SeedNet can be seen by comparing Rows (1) and (3) and also Rows ( 4) and (7). Both the baseline and FreeNet can benefit from the SeedNet supervision. Visually, clinical details in the image that are obscured by the heavy artifacts can be further refined by FreeNet; please refer to Fig. S1 in our Supplementary Material for more examples and ablation study. We also find that FreeNet 1+mask does not provide stable performance gains, probably because directly applying a mask on the pixel-wise loss leads to the discontinuous gradient that brings about sub-optimal results, which, however, can be circumvented with the guidance of SeedNet. In addition, we trained FreeSeed with Eq. ( 6) using 1 norm. From the last two rows in Table 2 we find that 1 norm does not ensure stable performance gains when FFC is used."
FreeSeed: Frequency-Band-Aware and Self-guided Network for Sparse-View CT Reconstruction,4,Conclusion,"In this paper, we proposed FreeSeed, a simple yet effective image-domain method for sparse-view CT reconstruction. FreeSeed incorporates Fourier knowledge into the reconstruction network with learnable band-pass attention for a better grasp of the globally distributed artifacts, and is trained using a self-guided artifact refinement network to further refine the heavily damaged image details. Extensive experiments show that both FreeSeed and its dual-domain counterpart outperformed the state-of-the-art methods. In future, we will explore FFC-based network for sinogram interpolation in sparse-view CT reconstruction."
FreeSeed: Frequency-Band-Aware and Self-guided Network for Sparse-View CT Reconstruction,,Fig. 1 .,
FreeSeed: Frequency-Band-Aware and Self-guided Network for Sparse-View CT Reconstruction,,Fig. 2 .,
FreeSeed: Frequency-Band-Aware and Self-guided Network for Sparse-View CT Reconstruction,,Fig. 3 .,
FreeSeed: Frequency-Band-Aware and Self-guided Network for Sparse-View CT Reconstruction,,Fig. 4 .,
FreeSeed: Frequency-Band-Aware and Self-guided Network for Sparse-View CT Reconstruction,,Table 2,
FreeSeed: Frequency-Band-Aware and Self-guided Network for Sparse-View CT Reconstruction,,Table 1 .,
FreeSeed: Frequency-Band-Aware and Self-guided Network for Sparse-View CT Reconstruction,,Table 2 .,
FreeSeed: Frequency-Band-Aware and Self-guided Network for Sparse-View CT Reconstruction,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 24.
Importance Weighted Variational Cardiac MRI Registration Using Transformer and Implicit Prior,1,Introduction,"Cardiac motion estimation is vital in evaluating cardiac function, detecting heart diseases, and understanding cardiac biomechanics. Deformable image registration (DIR) is the critical technique of cardiac motion estimation. It minimizes the differences between the warped moving and fixed images to estimate a displacement vector field (DVF). Unsupervised deep-learning-based image registration has recently become mainstream due to the required non-annotation data [4,16] and rapid inference performance when the network is well-trained.The probabilistic generative model shows potential in the unsupervised learning registration [11][12][13]17,18,22]. It allows the registration framework to be highly adaptable and can be applied to cases with a small amount of data and anatomical variability. Another advantage of the probabilistic generative registration model is that it can provide registration uncertainties [1], which plays a vital role in clinical application [14,21]. However, several issues exist with variational image registration approaches. The first is that traditional convolutions are limited in representing long-range relationships between image features. The second issue is a gap between the objective function ELBO and the log-likelihood of input image pairs, which deteriorates registration accuracy. Besides, nonparametric transformation is commonly used [5,20,27], which has the challenge of regularizing the DVF smooth and topology-preserving.This paper proposed a novel variational image registration model to cope with the above issues by employing the Transformers with the cross-attention mechanism and introducing an importance-weighted ELBO (iwELBO) [7] with an implicit prior. Detailed contributions of our work include:-A novel VAE network architecture is proposed, which employs the Transformer architecture to focus on cross-attention between the moving and fixed images. The predictive results of the transformation parameter distribution using our architecture are more accurate than traditional VAE architecture. -We optimized the importance-weighted ELBO in the variational image registration model. We use approximated aggregated posterior as the prior to regularizing posterior. To our best knowledge, we are the first to combine the iwELBO and aggregated posterior to close the gap between the real and variational posterior. -A parametric transformation based on multi-supports CSRBFs is embedded in our variational registration model. By imposing a sparse constraint, the coefficients of multi-CSRBFs are regularized to be sparse to select the optimal support for multi-support CSRBFs. The parametric transformation model improves registration accuracy significantly and makes it easy to regularize the smoothness of DVFs.2 Proposed Method"
Importance Weighted Variational Cardiac MRI Registration Using Transformer and Implicit Prior,2.1,Importance-Weighted Variational Image Registration Model,"Given the moving and fixed images M, F , and n control points {p i } n i=1 , the parametric transformation based on multi-supports CSRBFs is) is a CSRBF with support c; υ -p i 2 is the Euclidean distance between the pixel υ and p i . s different supports {c k } s are provided for each CSRBF. z = {z k } s k=1 , z k = {z k,i } n i=1 is the latent variable whose distribution is required to be estimated. The parametric transformation can control deformations using different supports. By imposing sparse constraints, selecting the optimal support from these given supports is possible, leading to more flexible deformation results.The variational registration model aims to estimate the posterior of p(z|F, M ). In the vanilla variational registration model, q(z|F, M ) is estimated to approximate p(z|F, M ) by optimizing ELBO = E z ∼q(z|F,M ) log p(F |z ,M )p(z ) q(z|F,M ) , where p(F |z, M) is the probability of occurring F when the moving image M is deformed using a transformation f z with latent variables z. It can be expressed as Boltzmann distribution p(F |z, M) ∝ e sim(F,M (fz )) using a similarity metric sim. p(z) is the prior of z.The importance-weighted evidence lower bound (iwELBO) [7] is defined as,where z 1 , . . . , z K are K-samples of latent variable z sampled from q(z|F, M ). It is assumed that q(z|F, M ) ∼ N (μ(F, M ), Σ(F, M )), and z 1 , . . . , z K is sampled as, the gradient of iwELBO can be interpreted as normalized importance-weighted average gradients of each sample, which implies the sample with larger w k contributes more to iwELBO. It is challenging to compute w k directly due to the high dimensional latent variable z k . We tackle this problem by a trick.iwELBO is a tighter evidence lower bound of the log-likelihood of data; it approaches the log-likelihood log p(F |M ) as K → ∞. From the view of image registration, the iwELBO tends to converge to a transformation with the optimal w k from K samples. When a large hyperparameter λ is used,) is relative smaller compared with the similarity term. That implies the iwELBO prefers the sample z k leading to the optimal similarity between the warped moving and fixed images, which is beneficial to push the network to predict more accurate z. Besides, Huang et al. [15] pointed out that when only one sample corresponding to a high loss is drawn to estimate the iwELBO, iwELBO tolerates this mistake due to the importance of weight. On the contrary, the sample with high loss will be highly penalized in traditional ELBO because the decoder treats the sample as real, observed data."
Importance Weighted Variational Cardiac MRI Registration Using Transformer and Implicit Prior,2.2,Aggregate Posterior as the Prior,"A simple prior, such as the standard Normal in VAE, incurred over-regularization on the posterior and widened the gap between the variational posterior and the real posterior. Many researchers resolve this mismatch by proposing various priors [2,10,[23][24][25]. Tomczak et al. stated that the optimal prior in VAE is the aggregated posterior of data. Takahashi et al. [23] introduced the density ratio trick to estimate this aggregated posterior implicitly. However, all these works are evaluated based on ELBO instead of iwELBO. We derive and approximate the optimal prior based on iwELBO, please refer to part 1 in the supplement.The optimal prior should maximize the expectation of iwELBO:) It can be derived that the optimal prior p * (z) is approximated as the aggregated posterior E p(F,M ) q(z|F, M ). Substituting the optimal prior p * (z) into Eq. ( 3), the objective function is rewritten aswhere p 0 (z) is a simple given prior. To estimate the density ratio log p * (z ) p0(z ) , a binary discriminator T (z) is trained by maximizing [23],where σ is the sigmoid function. The discriminator is a neural network composed of four fully connected layers, with the final layer outputting density ratio. A dropout layer is added before the output to prevent the discriminator network from overfitting. When T (z) is well trained,where z k T Bz k is the bending energy of DVF using multi-supports CSRBFs aiming to regularize DVF smooth. The sampling size for k is 5; λ is 110000.We optimize our network by iterating a two-step procedure. The encoder is updated using Eq. ( 7) by fixing the discriminator. Next, the discriminator is updated using Eq. ( 6) by fixing the encoder. Above two steps are performed alternatively. "
Importance Weighted Variational Cardiac MRI Registration Using Transformer and Implicit Prior,2.3,Network,"Our network architecture consists of an encoder and a decoder, as shown in Fig. 1. The encoder composes of T2T modules [26] and a Transformer to predict μ(F, M )), Σ(F, M ). T2T modules preprocess M and F and then input to our Transformer's T-encoder and T-decoder, which pays self-attention and cross-attention of M and F , respectively. Outputs of the cross-MSA (marked by orange) are fused information of M and F , weighted by similarity. More details of the Transformer Network can be seen in part 2 of the supplementary materials. The inherent ability to capture the correlation between two images makes Transformers easier to extract effective features for image registration. The decoder of our network generates the warped moving image using the DVF obtained by transformation based on multi-supports CSRBFs. The number of Transformer blocks N is 3. T2T modules tackle the partitioning issue in Transformer by modeling the local structure information iteratively. As shown in Fig. 2, overlapped patches are encoded by an unfold operation as a vector token T i and re-encoded as T i using a ViT. T i is reshaped as M i+1 with the size of overlapped patches. The above process is repeated three times to obtain the final vector T i+2 , which is the input of our Transformer.The total loss function of our network L is combing the iwELBO and sparse constraints on z as L = iwELBO -z 1 ."
Importance Weighted Variational Cardiac MRI Registration Using Transformer and Implicit Prior,3,Experiments,"Four public cardiac datasets are used to evaluate our method, including MIC-CAI2009 [19], York [3], ACDC [6], and M&Ms [8]. We combine MICCAI, York, and ACDC as a dataset denoted as ACDC+. Contours of the left ventricle (LV), the right ventricle (RV), or myocardium (MYO) at the end-diastolic (ED) phase or the end-systolic (ES) phase are provided by experts for different datasets. We take the ED and ES images as moving and fixed images, respectively. The training, validation, and testing slices are 1257, 130, 698 for ACDC+ and 1134, 266, 1030 for M&Ms. All images are cropped into the size of 128 × 128 containing the heart. Data augmentations such as flips and rotations are used. The LCC with the size of 9 × 9 is employed as the similarity metric. 64 global control points are evenly spaced on the 128 × 128 image, while 100 local control points are evenly spaced in an area of 64 × 64 in the center of the image that contains the heart. Our network is trained using PyTorch on a computer equipped with an Intel(R) Xeon(R) Silver 4210 CPU and Nvidia RTX 2080Ti GPU. The Adam optimizer with a learning rate of 5e -4 is employed. We use the estimated DVFs to map the contours of the moving image and compare the mapped contours with the contours of the fixed image using various metrics, including the Dice score, the average perpendicular distance (APD, in mm), and 95%-tile Hausdorff Distance (HD, in mm). Moreover, we count the number of anomalies to measure the topological property of the DVFs |J fz | ≤ 0 and calculate the bending energy (BE, ×10 -4 ) of DVFs to measure their smoothness."
Importance Weighted Variational Cardiac MRI Registration Using Transformer and Implicit Prior,3.1,Results,"To compare the performance of our method with unsupervised registration networks KrebsDiff [17], DalcaDiff [11], NetGI [13], VoxelMorph [4], and Trans-morph [9]. KrebsDiff, DalcaDiff, and NetGI are networks of variational registration. Transmorph is a network embedding Transformers. VoxelMorph is a vanilla unsupervised registration network. Registration results of two datasets using different networks are listed in Table 1. Our network outperforms other networks regarding Dice, HD, and APD. NetGI is the most similar registration model to our method, achieving the smoothest DVFs, while DalcaDiff preserved the topology of DVF best due to the diffeomorphism deformation it used. The bending energy and topology-preserving of DVFs using our network are close to that of NetGI and DalcaDiff, which implies that the transformation model based on multi-supports CSRBFs is good at generating smooth and topology-preserving DVFs. Visualization of the registration results using different networks is shown in Fig. 3. The myocardium of the fixed image is marked by green, while the warped moving image is marked by blue. The overlap of the myocardium is marked by red. Here, registration results of the basal, middle, and apical slices are provided. Note that objects in apical slices are small, while our network matches the small myocardium better than other networks. "
Importance Weighted Variational Cardiac MRI Registration Using Transformer and Implicit Prior,3.2,Ablation Study,"Ablation experiments are performed on the ACDC+ dataset to validate the influence of different components in our method. Table 2 lists evaluation results using the different combinations of components. Using ELBO as the objective function, the transformation based on multi-supports CSRBFs improves Dice 5%. Dice is improved 3% when the aggregated posterior is used as the prior. Whether the standard normal or the aggregated posterior as prior, the importance-weighted ELBO improves about 2-3% in Dice compared with ELBO. It is noted that when iwELBO is used, the aggregated posterior cannot improve registration compared with the standard Normal prior. The reason might be that the registration accuracy has a value close to its limit due to iwELBO; in this case, there is no space for the aggregated posterior as prior to improve registration accuracy further. Moreover, we find improvement in registration accuracy using iwELBO comes from the improvement of apical slices registration, details can be referred to in part 3 of the supplement. Further, we replaced the encoder of our network using ViT. Since only the Tencoder existed in ViT, we concatenated the moving and fixed images as input of ViT. In this experiment, different loops in ViT and our Transformer, denoted as ViT-n and Ours-n (n is the number of loops), are employed to compare the performance of self-attention and cross-attention. Table 3 lists comparison results of ViT and our Transformer. It can be seen that cross-attention outperforms selfattention, and more loops are not beneficial in predicting the posterior parameters. "
Importance Weighted Variational Cardiac MRI Registration Using Transformer and Implicit Prior,4,Conclusion,"In this paper, we proposed a novel variational registration model using Transformer to pay attention to cross-attention between images. The importanceweighted ELBO and the aggregated posterior as prior close the gap between the real posterior and the variational posterior. Our transformation using multisupports CSRBFs generates flexible DVFs. Evaluation results on public cardiac datasets show that our method outperforms the state-of-art networks."
Importance Weighted Variational Cardiac MRI Registration Using Transformer and Implicit Prior,,Fig. 1 .Fig. 2 .,
Importance Weighted Variational Cardiac MRI Registration Using Transformer and Implicit Prior,,Fig. 3 .,
Importance Weighted Variational Cardiac MRI Registration Using Transformer and Implicit Prior,,Table 1 .,
Importance Weighted Variational Cardiac MRI Registration Using Transformer and Implicit Prior,,.869 (0.042) 3.84 (1.81) 1.41(0.65),
Importance Weighted Variational Cardiac MRI Registration Using Transformer and Implicit Prior,,Table 2 .,
Importance Weighted Variational Cardiac MRI Registration Using Transformer and Implicit Prior,,.48 (3.16) 0.36 (1.31),
Importance Weighted Variational Cardiac MRI Registration Using Transformer and Implicit Prior,,Table 3 .,
Importance Weighted Variational Cardiac MRI Registration Using Transformer and Implicit Prior,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5_55.
Motion Compensated Unsupervised Deep Learning for 5D MRI,1,Introduction,"Magnetic Resonance Imaging (MRI) is currently the gold standard for assessing cardiac function. It provides detailed images of the heart's anatomy and enables accurate measurements of parameters such as ventricular volumes, ejection fraction, and myocardial mass. Current clinical protocols, which rely on serial breathheld imaging of the different cardiac slices with different views, often require long scan times and are associated with reduced patient comfort. Compressed sensing [1], deep learning [6,10], and motion-compensated approaches [13] were introduced to reduce the breath-hold duration in cardiac CINE MRI. Unfortunately, many subject groups, including pediatric and older subjects, cannot comply with even the shorter breath-hold durations.5D free-breathing MRI approaches that rely on 3D radial readouts [3,11] have been introduced to overcome the above challenges. These methods resolve the respiratory and cardiac motion from either the center of k-space or Superior-Inferior (SI) k-space navigators. The k-space data is then binned into different cardiac/respiratory phases and jointly reconstructed using compressed sensing. The main benefit of this motion-resolved strategy is the ability to acquire the whole heart with isotropic spatial resolution as high as 1 mm 3 . This approach allows the images to be reformatted into different views to visualize specific anatomical regions at different cardiac and/or respiratory phases. Despite the great potential of 5D MRI, current methods have some challenges that limit their use in routine clinical applications. Firstly, the motion-resolved compressed sensing reconstruction is very computationally intensive, and it can take several hours to have a dynamic 3D volume. And secondly, compressed sensing reconstructions require fine tuning of several regularization parameters, which greatly affect the final image quality, depending on the undersampling factor and the binning uniformity.The main focus of this work is to introduce a motion-compensated reconstruction algorithm for 5D MRI. The proposed approach models the images at every time instance as a deformed version of a static image template. Such an image model may not be a good approximation in 2D schemes [13], where the organs may move in and out of the slice. However, the proposed model is more accurate for the 3D case. We introduce an auto-encoder to estimate the cardiac and respiratory phases from the superior-inferior (SI) k-t space navigators. We disentangle the latent variables to cardiac and respiratory phases by using the prior information of the cardiac and respiratory rates. The latent variables allow us to bin the data into different cardiac and respiratory phases. We use an unsupervised deep learning algorithm to recover the image volumes from the clustered data. The algorithm models the deformation maps as points on a smooth lowdimensional manifold in high dimensions, which is a non-linear function of the low-dimensional latent vectors. We model the non-linear mapping by a Convolutional Neural Network (CNN). When fed with the corresponding latent vectors, this CNN outputs the deformation maps corresponding to a specific cardiac or respiratory phase. We learn the parameters of the CNN and the image template from the measured k-t space data. We note that several manifold based approaches that model the images in the time series by a CNN were introduced in the recent years. [5,9,15]. All of these methods rely on motion resolved reconstruction, which is conceptually different from the proposed motion compensated reconstruction.We validate the proposed scheme on cardiac MRI datasets acquired from two healthy volunteers. The results show that the approach is capable of resolving the cardiac motion, while offering similar image quality for all the different phases. In particular, the motion-compensated approach can combine the image information from all the motion states to obtain good quality images."
Motion Compensated Unsupervised Deep Learning for 5D MRI,2,Methods,
Motion Compensated Unsupervised Deep Learning for 5D MRI,2.1,Acquisition Scheme,"In vivo acquisitions were performed on a 1.5T clinical MRI scanner (MAGNE-TOM Sola, Siemens Healthcare, Erlangen, Germany). The free-running research sequence used in this work is a bSSFP sequence, in which all chemically shiftselective fat saturation pulses and ramp-up RF excitations were removed, in order to reduce the specific absorption rate (SAR) and to enable a completely uninterrupted acquisition [8]. K-space data were continuously sampled using a 3D golden angle kooshball phyllotayis trajectory [7], interleaved with the acquisition of a readout oriented along the superior-inferior (SI) direction for cardiac and respiratory self-gating [11]. The main sequence parameters were: radio frequency excitation angle of 55 with an axial slab-selective sinc pulse, resolution of 1.1 mm 3 , FOV of 220 mm 3 , TE/TR of 1.87/3.78 ms, and readout bandwidth of 898 Hz/pixel. The total fixed scan time was 7:58 min."
Motion Compensated Unsupervised Deep Learning for 5D MRI,2.2,Forward Model,"We model the measured k-space data at the time instant t as the multichannel Fourier measurements of ρ t = ρ(r, t), which is the image volume at the time instance t:Here, C denotes the multiplication of the images by the multi-channel coil sensitivities, while F k denotes the multichannel Fourier operator. k t denotes the k-space trajectory at the time instant t. In this work, we group 22 radial spokes corresponding to a temporal resolution of 88 ms. An important challenge associated with the bSSFP acquisition without intermittent fat saturation pulses is the relatively high-fat signal compared to the myocardium and blood pool. Traditional parallel MRI and coil combination strategies often result in significant streaking artifacts from the fat onto the myocardial regions, especially in the undersampled setting considered in this work. We used the coil combination approach introduced in [4] to obtain virtual channels that are maximally sensitive to the cardiac region. A spherical region covering the heart was manually selected as the region of interest (ROI), while its complement multiplied by the distance function to the heart was chosen as the noise mask. We chose the number of virtual coils that preserve 75% of the energy within the ROI. This approach minimizes the strong fat signals, which are distant from the myocardium. We used the JSENSE algorithm [12,14] to compute the sensitivity maps of the virtual coils."
Motion Compensated Unsupervised Deep Learning for 5D MRI,2.3,Image and Motion Models,"The overview of the proposed scheme is shown in Fig. 1. The recovery of ρ t from very few of their measurements b t is ill-posed. To constrain the recovery, we model ρ t as the deformed version of a static image template η(r):Here, φ t is the deformation map and the operator I denotes the deformation of η. We implement (2) using cubic Bspline interpolation. This approach allows us to use the k-space data from all the time points to update the template, once the motion maps φ t are estimated. Classical MoCo approaches use image registration to estimate the motion maps φ t from approximate (e.g. low-resolution) reconstructions of the images ρ(r, t). However, the quality of motion estimates depends on the quality of the reconstructed images, which are often low when we aim to recover the images at a fine temporal resolution (e.g. 88 ms).We propose to estimate the motion maps directly from the measured kt space data. In particular, we estimate the motion maps φ t such that the multichannel measurements of ρ(r, t) specified by (2) match the measurements b t . We also estimate the template η from the k-t space data of all the time points. To constrain the recovery of the deformation maps, we model the deformation maps as the output of a convolutional neural networkin response to low-dimensional latent vectors z t . Here, G θ is a convolutional neural network, parameterized by the weights θ. We note that this approach constrains the deformation maps as points on a low-dimensional manifold. They are obtained as non-linear mappings of the low-dimensional latent vectors z t , which capture the motion attributes. The non-linear mapping itself is modeled by the CNN."
Motion Compensated Unsupervised Deep Learning for 5D MRI,2.4,Estimation of Latent Vectors from SI Navigators,"We propose to estimate the latent vectors z t from the SI navigators using an auto-encoder. In this work, we applied a low pass filter with cut-off frequency of 2.8 Hz to the SI navigators to remove high-frequency oscillations. Similarly, an eighth-degree Chebyshev polynomial is fit to each navigator voxel and is subtracted from the signal to remove drifts.The auto-encoder involves an encoder that generates the latent vectors z t = E ϕ (y t ), are the navigator signals. The decoder reconstructs the navigator signals as y t = D ψ (z t ). In this work, we restrict the dimension of the latent space to three, two corresponding to respiratory motion and one corresponding to cardiac motion. To encourage the disentangling of the latent vectors to respiratory and cardiac signals, we use the prior information on the range of cardiac and respiratory frequencies as in [2]. We solve for the auto-encoder parameters from the navigator signals of each subject asHere, Z ∈ R 3×T and Y are matrices whose columns are the latent vectors and the navigator signals at different time points. F is the Fourier transformation in the time domain. denotes the convolution of the latent vectors with band-stop filters with appropriate stop bands. In particular, the stopband of the respiratory latent vectors was chosen to be 0.05-0.7 Hz, while the stopband was chosen as the complement of the respiratory bandstop filter Hz for the cardiac latent vectors. We observe that the median-seeking 1 loss in the Fourier domain is able to offer improved performance compared to the standard 2 loss used in conventional auto-encoders."
Motion Compensated Unsupervised Deep Learning for 5D MRI,2.5,Motion Compensated Image Recovery,"Once the auto-encoder parameters ϕ, ψ described in (3) are estimated from the navigator signals of the subject, we derive the latent vectors as Z = E ϕ * (Y). Using the latent vectors, we pose the joint recovery of the static image template η(r) and the deformation maps asThe above optimization scheme can be solved using stochastic gradient optimization. Following optimization, one can generate real-time images shown in Fig. 3 and Fig. 3 asThe optimization scheme described in (4) requires T non-uniform fast Fourier transform steps per epoch. When the data is recovered with a high temporal resolution, this approach translates to a high computational complexity. To reduce computational complexity, we introduce a clustering scheme. In particular, we use k-means clustering to group the data to N << T clusters. This approach allows us to pool the k-space data from multiple time points, all with similar latent codes.The above approach is very similar to (4). Here, b n , A n , and z n are the grouped k-space data, the corresponding forward operator, and the centroid of the cluster, respectively. Note that once N → T , both approaches become equivalent. In this work, we used N = 30. Once the training is done, one can still generate the realtime images as I (η, G θ [z t ]). In the first step shown in (a), we estimate the latent variables that capture the motion in the data using a constrained auto-encoder, as described in Fig. 3. The auto-encoder minimizes a cost function, which is the sum of an 1 data consistency term and a prior involving cardiac and frequency ranges. To reduce the computational complexity of the image reconstruction, we cluster the latent space using k-means algorithm as shown in (b). The cluster centers are fed in as inputs to the CNN denoted by G θ , which outputs the deformation maps G θ [zn]. We jointly optimize for both the template η and parameters θ of the generator."
Motion Compensated Unsupervised Deep Learning for 5D MRI,2.6,Motion Resolved 5D Reconstruction for Comparison,"We compare the proposed approach against a compressed sensing 5D reconstruction. In particular, we used the SI navigators to bin the data into 16 bins, consisting of four cardiac and four respiratory phases as described. We use a total variation regularization similar to [2] to constrain the reconstructions. We determined the regularization parameter manually to obtain the best reconstructions.We note that the dataset with 6.15 min acquisition is a highly undersampled setting. In addition, because this dataset was not acquired with intermittent fat saturation pulses, it suffers from streaking artifacts that corrupt the reconstructions."
Motion Compensated Unsupervised Deep Learning for 5D MRI,3,Results,"We show the results from the two normal volunteers in Fig. 3 and4, respectively. The images correspond to 2-D slices extracted from the 3D volume, corresponding to different cardiac and respiratory phases. We also show the time profile of the real-time reconstructions ρ(r, t) = I (η, G θ [z t ]) along the red line shown in the top row. We note that the approach can capture the cardiac and respiratory motion in the data. The different phase images shown in the figure were extracted manually from the real-time movies.  [11]. We note that the orange and the green curves estimated using the auto-encoder roughly follow the respiratory motion, while the blue curves capture the cardiac motion.  "
Motion Compensated Unsupervised Deep Learning for 5D MRI,4,Discussion,"The comparisons in Fig. 3 and4 show that the proposed approach is able to offer improved reconstructions, where the cardiac phases are well-resolved. We note that the motion resolved reconstruction of the different phases have different image quality, depending on the number of spokes in the specific phases. By contrast, the proposed motion compensated reconstructions are able to combine the data from different motion states; the improved data efficiency translates to reconstructions with reduced streaking artifacts. Additionally, the auto-encoder accurately characterized the SI navigator and disentangled the cardiac and respiratory latent vectors Fig. 2.We note that the comparison in this work is preliminary. The main focus of this work is to introduce the proposed motion-compensated reconstruction algorithm and the auto-encoder approach to estimate the latent vectors and to demonstrate its utility in 5D MRI. In our future work, we will focus on rigorous studies, including comparisons with 2D CINE acquisitions."
Motion Compensated Unsupervised Deep Learning for 5D MRI,,Fig. 1 .,
Motion Compensated Unsupervised Deep Learning for 5D MRI,,Fig. 2 .,
Motion Compensated Unsupervised Deep Learning for 5D MRI,,Fig. 3 .,
Motion Compensated Unsupervised Deep Learning for 5D MRI,,Fig. 4 .,
LLCaps: Learning to Illuminate Low-Light Capsule Endoscopy with Curved Wavelet Attention and Reverse Diffusion,1,Introduction,"Currently, the golden standard of gastrointestinal (GI) examination is endoscope screening, which can provide direct vision signals for diagnosis and analysis. Benefiting from its characteristics of being non-invasive, painless, and low physical burden, wireless capsule endoscopy (WCE) has the potential to overcome the shortcomings of conventional endoscopy [21]. However, due to the anatomical complexity, insufficient illumination, and limited performance of the camera, low-quality images may hinder the diagnosis process. Blood vessels and lesions with minor color changes in the early stages can be hard to be screened out [15]. Figure 1 shows WCE images with low illumination and contrast. The disease features clearly visible in the normal image become challenging to be found in the low-light images. Therefore, it is necessary to develop a low-light image enhancement framework for WCE to assist clinical diagnosis."
LLCaps: Learning to Illuminate Low-Light Capsule Endoscopy with Curved Wavelet Attention and Reverse Diffusion,,Normal Images Low Light Images,
LLCaps: Learning to Illuminate Low-Light Capsule Endoscopy with Curved Wavelet Attention and Reverse Diffusion,,Angiectasia Erosion,"Foreign body Angiectasia Erosion Foreign body Many traditional algorithms (e.g., intensity transformation [7], histogram equalization [14], and Retinex theory [13]) have been proposed for low-light image enhancement (LLIE). For WCE, Long et al. [15] discussed adaptive fraction-power transformation for image enhancement. However, traditional methods usually require an ideal assumption or an effective prior, limiting their wider applications. Deep learning (DL) provides novel avenues to solve LLIE problems [6,10,16]. Some DL-based LLIE schemes for medical endoscopy have been proposed [5,18]. Gomez et al. [5] offered a solution for laryngoscope lowlight enhancement, and Ma et al. [18] proposed a medical image enhancement model with unpaired training data.Recently, denoising diffusion probabilistic model (DDPM) [8] is the most popular topic in image generation, and has achieved success in various applications. Due to its unique regression process, DDPM has a stable training process and excellent output results, but also suffers from its expensive sampling procedure and lack of low-dimensional representation [19]. It has been proved that DDPM can be combined with other existing DL techniques to speed up the sampling process [19]. In our work, we introduce the reverse diffusion process of DDPM into our end-to-end LLIE process, which can preserve image details without introducing excessive computational costs. Our contributions to this work can be summarized as three-fold:-We design a Low-Light image enhancement framework for Capsule endoscopy (LLCaps "
LLCaps: Learning to Illuminate Low-Light Capsule Endoscopy with Curved Wavelet Attention and Reverse Diffusion,2,Methodology,
LLCaps: Learning to Illuminate Low-Light Capsule Endoscopy with Curved Wavelet Attention and Reverse Diffusion,2.1,Preliminaries,"Multi-scale Residual Block. Multi-scale Residual Block (MSRB) [12] constructs a multi-scale neuronal receptive field, which allows the network to learn multi-scale spatial information in the same layer. Therefore, the network can acquire contextual information from the low-resolution features while preserving high-resolution representations. We establish our CNN branch with six stacked multi-scale residual blocks (MSRB), and every two MSRBs are followed by a 2D convolutional layer (Conv2D). Besides, each MSRB shall require feature learning and the multi-scale feature aggregation module. Specifically, we propose our curved wavelet attention (CWA) module to conduct multi-scale feature learning, and employ the selective kernel feature fusion (SKFF) [29] to combine multi-scale features, as shown in Fig. 2(a).Denoising Diffusion Probabilistic Models. Denoising Diffusion Probabilistic Models (DDPMs) [8] can be summarised as a model consisting of a forward noise addition q(i 1:T |i 0 ) and a reverse denoising process p(i 0:T ), which are both parameterized Markov chains. The forward diffusion process gradually adds noise to the input image until the original input is destroyed. Correspondingly, the reverse process uses the neural network to model the Gaussian distribution and achieves image generation through gradual sampling and denoising."
LLCaps: Learning to Illuminate Low-Light Capsule Endoscopy with Curved Wavelet Attention and Reverse Diffusion,2.2,Proposed Methodology,"Curved Wavelet Attention. Curved Wavelet Attention (CWA) block is the core component of our CNN branch, which is constructed via a curved dual attention mechanism and wavelet transform, as shown in Fig. 2(b). Firstly, the input feature map F in is divided into identity feature F identity and processing feature F p . Medical LLIE shall require high image details. In this case, we transform the F p into wavelet domain F w to extract high-frequency detail information based on discrete wavelet transform. F w is then propagated through the feature selector and dual attention module for deep representation learning. Finally, we conduct reverse wavelet transform (IWT) to get F p , and concatenate it with F identity before the final output convolution layer. We construct our curved dual attention module with parallel spatial and curved attention blocks. The spatial attention (SA) layer exploits the interspatial dependencies of convolutional features [29]. The SA layer performs the global average pooling and max pooling on input features respectively, and concatenates the output F w(mean) and F w(max) to get F cat . Then the feature map will be dimensionally reduced and passed through the activation function.However, literature [6,33] has discussed the problem of local illumination in LLIE. If we simply use a global computing method such as the SA layer, the model may not be able to effectively understand the local illumination/lack of illumination. Therefore, in order to compensate for the SA layer, we design the Curved Attention (CurveA) layer, which is used to model the high-order curve of the input features. Let IL n(c) denote the curve function, c denote the feature location coordinates, and Curve (n-1) denote the pixel-wise curve parameter, we can obtain the curve estimation equation as:The detailed CurveA layer is presented in the top of Fig. 2(b), and the Eq. ( 1) is related to the white area. The Curve Parameter Estimation module consists of a Sigmoid activation and several Conv2D layers, and shall estimate the pixelwise curve parameter at each order. The Feature Rescaling module will rescale the input feature into [0, 1] to learn the concave down curves. By applying the CurveA layer to the channels of the feature map, the CWA block can better estimate local areas with different illumination.Reverse Diffusion Process. Some works [19,26] have discussed combining diffusion models with other DL-based methods to reduce training costs and be used for downstream applications. In our work, we combine the reverse diffusion process of DDPM in a simple and ingenious way, and use it to optimize the shallow output by the CNN branch. Various experiments shall prove the effectiveness of our design in improving image quality and assisting clinical applications.In our formulation, we assume that i 0 is the learning target Y * and i T is the output shallow image from the CNN branch. Therefore, we only need to engage the reverse process in our LLIE task. The reverse process is modeled using a Markov chain:p θ (i t-1 | i t ) are parameterized Gaussian distributions whose mean µ θ (i t , t) and variance Σ θ (i t , t) are given by the trained network. Meanwhile, we simplify the network and directly include the reverse diffusion process in the end-toend training of the entire network. Shallow output is therefore optimized by the reverse diffusion branch to get the predicted image Y . We further simplify the optimization function and only employ a pixel-level loss on the final output image, which also improves the training and convergence efficiency."
LLCaps: Learning to Illuminate Low-Light Capsule Endoscopy with Curved Wavelet Attention and Reverse Diffusion,,Overall Network,"Architecture. An overview of our framework can be found in Fig. 2. Our LLCaps contains a CNN branch (including a shallow feature extractor (SFE), multi-scale residual blocks (MSRBs), an output module (OPM)), and the reverse diffusion process. The SFE is a Conv2D layer that maps the input image into the high-dimensional feature representation F SF E ∈ R C×W ×H [27]. Stacked MSRBs shall conduct deep feature extraction and learning. OPM is a Conv2D layer that recovers the feature space into image pixels. A residual connection is employed here to optimize the end-to-end training and converge process. Hence, given a low-light image x ∈ R 3×W ×H , where W and H represent the width and height, the CNN branch can be formulated as:The shallow output F OP M ∈ R 3×W ×H shall further be propagated through the reverse diffusion process and achieve the final enhanced image Y ∈ R 3×W ×H . The whole network is constructed in an end-to-end mode and optimized by Charbonnier loss [1]. The ε is set to 10 -3 empirically.in which Y and Y * denote the input and ground truth images, respectively."
LLCaps: Learning to Illuminate Low-Light Capsule Endoscopy with Curved Wavelet Attention and Reverse Diffusion,3,Experiments,
LLCaps: Learning to Illuminate Low-Light Capsule Endoscopy with Curved Wavelet Attention and Reverse Diffusion,3.1,Dataset,"We conduct our experiments on two publicly accessible WCE datasets, the Kvasir-Capsule [22] and the Red Lesion Endoscopy (RLE) dataset [3]. Kvasir-Capsule dataset [22] is a WCE classification dataset with three anatomy classes and eleven luminal finding classes. By following [2], we randomly select 2400 images from the Kvasir-Capsule dataset, of which 2000 are used for training and 400 for testing. To create low-light images, we adopt random Gamma correction and illumination reduction following [11,16]. Furthermore, to evaluate the performance on real data, we add an external validation on 100 real images selected from the Kvasir-Capsule dataset. These images are with low brightness and are not included in our original experiments.Red Lesion Endoscopy dataset [3] (RLE) is a WCE dataset for red lesion segmentation tasks (e.g., angioectasias, angiodysplasias, and bleeding). We randomly choose 1283 images, of which 946 images are used for training and 337 for testing. We adopt the same method in the Kvasir-Capsule dataset to generate low-light images. Furthermore, we conduct a segmentation task on the RLE test set to investigate the effectiveness of the LLIE models in clinical applications."
LLCaps: Learning to Illuminate Low-Light Capsule Endoscopy with Curved Wavelet Attention and Reverse Diffusion,3.2,Implementation Details,"We compare the performance of our LLCaps against the following state-of-theart (SOTA) LLIE methodologies: LIME [7], DUAL [31], Zero-DCE [6], Enlight-enGAN [10], LLFlow [24], HWMNet [4], MIRNet [29], SNR-Aware [28], Still-GAN [17], MIRNetv2 [30], and DDPM [8]. Our models are trained using Adam optimizer for 200 epochs with a batch size of 4 and a learning rate of 1 × 10 -4 . For evaluation, we adopt three commonly used image quality assessment metrics: Peak Signal-to-Noise Ratio (PSNR) [9], Structural Similarity Index (SSIM) [25], and Learned Perceptual Image Patch Similarity (LPIPS) [32]. For the external validation set, we evaluate with no-reference metrics LPIPS [32] and Perceptionbased Image Quality Evaluator (PIQE) [23] due to the lack of ground truth images. To verify the usefulness of the LLIE methods for downstream medical tasks, we conduct red lesion segmentation on the RLE test set and evaluate the performance via mean Intersection over Union (mIoU), Dice similarity coefficient (Dice), and Hausdorff Distance (HD). We train UNet [20]   "
LLCaps: Learning to Illuminate Low-Light Capsule Endoscopy with Curved Wavelet Attention and Reverse Diffusion,3.3,Results,"We compare the performance of our LLCaps to the existing approaches, as demonstrated in Table 1 and Fig. 3 quantitatively and qualitatively. Compared with other methods, our proposed method achieves the best performance among all metrics. Specifically, our method surpasses MIRNetv2 [30] by 3.57 dB for the Kvasir-Capsule dataset and 0.33 dB for the RLE dataset. The SSIM of our method has improved to 96.34% in the Kvasir-Capsule dataset and 93.34% in the RLE dataset. Besides that, our method also performs the best in the noreference metric LPIPS. The qualitative results of the comparison methods and Table 1. Image quality comparison with existing methods on Kvasir-Capsule [22] and RLE dataset [3]. The 'External Val' denotes the external validation experiment conducted on 100 selected real low-light images from the Kvasir-Capsule dataset [22]. The red lesion segmentation experiment is also conducted on RLE test set [3].our method on the Kvasir-Capsule and RLE datasets are visualized in Fig. 3 with the corresponding heat maps. Firstly, we can see that directly performing LLIE training on DDPM [8] cannot obtain good image restoration, and the original structures of the DDPM images are largely damaged. EnlightenGAN [10] also does not perform satisfactorily in structure restoration. Our method successfully surpasses LLFlow [24] and MIRNetv2 [30] in illumination restoration. The error heat maps further reflect the superior performance of our method in recovering the illumination and structure from low-light images. Moreover, our solution yields the best on the real low-light dataset during the external validation, proving the superior performance of our solution in real-world applications.Furthermore, a downstream red lesion segmentation task is conducted to investigate the usefulness of our LLCaps on clinical applications. As illustrated in Table 1, LLCaps achieve the best lesion segmentation results, manifesting the superior performance of our LLCaps model in lesion segmentation. Additionally, LLCaps surpasses all SOTA methods in HD, showing LLCaps images perform perfectly in processing the segmentation boundaries, suggesting that our method possesses better image reconstruction and edge retention ability.Besides, an ablation study is conducted on the Kvasir-Capsule dataset to demonstrate the effectiveness of our design and network components, as shown in Table 2. To observe and compare the performance changes, we try to (i) remove the wavelet transform in CWA blocks, (ii) degenerate the curved attention (CurveA) layer in CWA block to a simple channel attention layer [29], and (iii) remove the reverse diffusion branch. Experimental results demonstrate that the absence of any component shall cause great performance degradation. The significant improvement in quantitative metrics is a further testament to the effectiveness of our design for each component.Table 2. Ablation experiments of our LLCaps on the Kvasir-Capsule Dataset [22]. In order to observe the performance changes, we (i) remove the wavelet transform, (ii) degenerate the CurveA layer, and (iii) remove the reverse diffusion branch. "
LLCaps: Learning to Illuminate Low-Light Capsule Endoscopy with Curved Wavelet Attention and Reverse Diffusion,,Wavelet Transform,
LLCaps: Learning to Illuminate Low-Light Capsule Endoscopy with Curved Wavelet Attention and Reverse Diffusion,,Fig. 1 .,
LLCaps: Learning to Illuminate Low-Light Capsule Endoscopy with Curved Wavelet Attention and Reverse Diffusion,,Fig. 2 .,
LLCaps: Learning to Illuminate Low-Light Capsule Endoscopy with Curved Wavelet Attention and Reverse Diffusion,,,
LLCaps: Learning to Illuminate Low-Light Capsule Endoscopy with Curved Wavelet Attention and Reverse Diffusion,,Fig. 3 .,
LLCaps: Learning to Illuminate Low-Light Capsule Endoscopy with Curved Wavelet Attention and Reverse Diffusion,,,
LLCaps: Learning to Illuminate Low-Light Capsule Endoscopy with Curved Wavelet Attention and Reverse Diffusion,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 4.
Contrastive Diffusion Model with Auxiliary Guidance for Coarse-to-Fine PET Reconstruction,1,Introduction,"Positron emission tomography (PET) is a widely-used molecular imaging technique that can help reveal the metabolic and biochemical functioning of body tissues. According to the dose level of injected radioactive tracer, PET images can be roughly classified as standard-(SPET) and low-dose PET (LPET) images. SPET images offer better image quality and more information in diagnosis compared to LPET images containing more noise and artifacts. However, the higher radiation exposure associated with SPET scanning poses potential health risks to the patient. Consequently, it is crucial to reconstruct SPET images from corresponding LPET images to produce clinically acceptable PET images.In recent years, deep learning-based PET reconstruction approaches [7,9,13] have shown better performance than traditional methods. Particularly, generative adversarial networks (GANs) [8] have been widely adopted [12,14,15,18,26,27] due to their capability to synthesize PET images with higher fidelity than regression-based models [29,30]. For example, Kand et al. [11] applied a Cycle-GAN model to transform amyloid PET images obtained with diverse radiotracers. Fei et al. [6] made use of GANs to present a bidirectional contrastive framework for obtaining high-quality SPET images. Despite the promising achievement of GAN, its adversarial training is notoriously unstable [22] and can lead to mode collapse [17], which may result in a low discriminability of the generated samples, reducing their confidence in clinical diagnosis.Fortunately, likelihood-based generative models offer a new approach to address the limitations of GANs. These models learn the distribution's probability density function via maximum likelihood and could potentially cover broader data distributions of generated samples while being more stable to train. As an example, Cui et al. [3] proposed a model based on Nouveau variational autoencoder for PET image denoising. Among likelihood-based generative models, diffusion probabilistic models (DPMs) [10,23] are noteworthy for their capacity to outperform GANs in various tasks [5], such as medical imaging [24] and textto-image generation [20]. DPMs consist of two stages: a forward process that gradually corrupts the given data and a reverse process that iteratively samples the original data from the noise. However, sampling from a diffusion model is computationally expensive and time-consuming [25], making it inconvenient for real clinical applications. Besides, existing conditional DPMs learn the inputoutput correspondence implicitly by adding a prior to the training objective, while this learned correspondence is prone to be lost in the reverse process [33], resulting in the RPET image missing crucial clinical information from the LPET image. Hence, the clinical reliability of the RPET image may be compromised. Motivated to address the above limitations, in this paper, we propose a coarse-to-fine PET reconstruction framework, including a coarse prediction module (CPM) and an iterative refinement module (IRM). The CPM generates a coarse prediction by invoking a deterministic prediction network only once, while the IRM, which is the reverse process of the DPMs, iteratively samples the residual between this coarse prediction and the corresponding SPET image. By combining the coarse prediction and the predicted residual, we can obtain RPET images much closer to the SPET images. To accelerate the sampling speed of IRM, we manage to delegate most of the computational overhead to the CPM [2,28], hoping to narrow the gap between the coarse prediction and the SPET initially. Additionally, to enhance the correspondence between the LPET image and the generated RPET image, we propose an auxiliary guidance strategy at the input level based on the finding that auxiliary guidance can help to facilitate the reverse process of DPMs, and reinforce the consistency between the LPET image and RPET image by providing more LPET-relevant information to the model. Furthermore, at the output level, we suggest a contrastive diffusion strategy inspired by [33] to explicitly distinguish between positive and negative PET slices. To conclude, the contributions of our method can be described as follows:-We introduce a novel PET reconstruction framework based on DPMs, which, to the best of our knowledge, is the first work that applies DPMs to PET reconstruction. -To mitigate the computational overhead of DPMs, we employ a coarse-to-fine design that enhances the suitability of our framework for real-world clinical applications. -We propose two novel strategies, i.e., an auxiliary guidance strategy and a contrastive diffusion strategy, to improve the correspondence between the LPET and RPET images and ensure that RPET images contain reliable clinical information.2 Background: Diffusion Probabilistic Models Diffusion Probabilistic Models (DPMs): DPMs [10,23] define a forward process, which corrupts a given image data x 0 ∼ q(x 0 ) step by step via a fixed Markov chain q(x t |x t-1 ) that gradually adds Gaussian noise to the data:where α 1:T is the constant variance schedule that controls the amount of noise added at each time step, and q(x T ) ∼ N (x T ; 0, I) is the stationary distribution.Owing to the Markov property, a data x t at an arbitrary time step t can be sampled in closed form:where γ t = t i=1 α i . Furthermore, we can derive the posterior distribution ofand σ 2 t are subject to x 0 , x t and α 1:T . Based on this, we can leverage the reverse process from x T to x 0 to gradually denoise the latent variables by sampling from the posterior distribution q(x t-1 |x 0 , x t ). However, since x 0 is unknown during inference, we use a transition distribution p θ (x t-1 |x t ) := q(x t-1 |H θ (x t , t), x t ) to approximate q(x t-1 |x 0 , x t ), where H θ (x t , t) manages to reconstruct x 0 from x t and t, and it is trained by optimizing a variational lower bound of logp θ (x).Conditional DPMs: Given an image x 0 with its corresponding condition c, conditional DPMs try to estimate p(x 0 |c). To achieve that, condition c is concatenated with x t [21] as the input of H θ , denoted as H θ (c, x t , t).Simplified Training Objective: Instead of training H θ to reconstruct the x 0 directly, we use an alternative parametrization D θ named denoising network [10] trying to predict the noise vector ∼ N (0, I) added to x 0 in Eq. 2, and derive the following training objective:where the distribution p γ is the one used in WaveGrad [1]. Note that we also leverage techniques from WaveGrad to let the denoising network D θ conditioned directly on the noise schedule γ rather than time step t, and this gives us more flexibility to control the inference steps. "
Contrastive Diffusion Model with Auxiliary Guidance for Coarse-to-Fine PET Reconstruction,3,Methodology,"Our proposed framework (Fig. 1(a)) has two modules, i.e., a coarse prediction module (CPM) and an iterative refinement module (IRM). The CPM predicts a coarse-denoised PET image from the LPET image, while the IRM models the residual between the coarse prediction and the SPET image iteratively. By combining the coarse prediction and residual, our framework can effectively generate high-quality RPET images. To improve the correspondence between the LPET image and the RPET image, we adopt an auxiliary guidance strategy (Fig. 1(b)) at the input level and a contrastive diffusion strategy (Fig. 1(c)) at the output level. The details of our method are described in the following subsections."
Contrastive Diffusion Model with Auxiliary Guidance for Coarse-to-Fine PET Reconstruction,3.1,Coarse-to-Fine Framework,"To simplify notation, we use a single conditioning variable c to represent the input required by both CPM and IRM, which includes the LPET image x lpet and the auxiliary guidance x aux . During inference, CPM first generates a coarse prediction x cp = P θ (c), where P θ is the deterministic prediction network in CPM. The IRM, which is the reverse process of DPM, then tries to sample the residual r 0 (i.e., x 0 in Sect. 2) between the coarse prediction x cp and the SPET image y via the following iterative process:Herein, the prime symbol above the variable indicates that it is sampled from the reverse process instead of the forward process. When t = 1, we can obtain the final sampled residual r 0 , and the RPET image y can be derived by r 0 + x cp .In practice, both CPM and IRM use the same network architecture shown in Fig. 1(c). CPM generates the coarse prediction x cp by using P θ only once, but the denoising network D θ in IRM will be invoked multiple times during inference. Therefore, it is rational to delegate more computation overhead to P θ to obtain better initial results while keeping D θ small, since the reduction in computation cost in D θ will be accumulated by multiple times. To this end, we set the channel number in P θ much larger than that in the denoising network D θ . This leads to a larger network size for P θ compared to D θ ."
Contrastive Diffusion Model with Auxiliary Guidance for Coarse-to-Fine PET Reconstruction,3.2,Auxiliary Guidance Strategy,"In this section, we will describe our auxiliary guidance strategy in depth which is proposed to enhance the reconstruction process at the input level by incorporating two auxiliary guidance, i.e., neighboring axial slices (NAS) and the spectrum. Our findings indicate that incorporating NAS provides insight into the spatial relationship between the current slice and its adjacent slices, while incorporating the spectrum imposes consistency in the frequency domain.To effectively incorporate these two auxiliary guidances, as illustrated in Fig. 1(c), we replace the ResBlock in the encoder with a Guided ResBlock as done in [19]. During inference, the auxiliary guidance x aux is first downsampled by a factor of 2 k as x k aux , where k = 1, • • • , M, and M is the number of downsampling operations in the U-net encoder. Then x k aux is fed into a feature extractor F θ to generate its corresponding feature map f k aux = F θ (x k aux ), which is next injected into the Guided ResBlock matching its resolution through 1 × 1 convolution.To empower the feature extractor to contain information of its high-quality counterpart y aux , we constrain it with L 1 loss through a convolution layer C θ (•):where opt ∈{NAS, spectrum} denotes the kind of auxiliary guidance."
Contrastive Diffusion Model with Auxiliary Guidance for Coarse-to-Fine PET Reconstruction,3.3,Contrastive Diffusion Strategy,"In addition to the auxiliary guidance at the input level, we also develop a contrastive diffusion strategy at the output level to amplify the correspondence between the condition LPET image and the corresponding RPET image. In detail, we introduce a set of negative samples Neg = y 1 , y 2 , ..., y N , which consists of N SPET slices, each from a randomly selected subject that is not in the current batch for training. Then, for the noisy latent residual r t at time step t, we obtain its corresponding intermediate RPET y, and draw it close to the corresponding SPET y while pushing it far from the negative sample y i ∈ Neg.Before this, we need to estimate the intermediate residual corresponding to r t firstly, denoted as r 0 . According to Sect. 2, the denoising network D θ manages to predict the Gaussian noise added to r 0 , enabling us to calculate r 0 directly from r t :Then r 0 is added to the coarse prediction x cp to obtain the intermediate RPET y = x cp + r 0 . Note that y is a one-step estimated result rather than the final RPET y . Herein, we define a generator p θ (y|r t , c) to represent the above process. Subsequently, the contrastive learning loss L CL is formulated as:Intuitively, as illustrated in Fig. 1(b), the L CL aims to minimize the discrepancy between the training label y and the intermediate RPET y at each time step (first term), while simultaneously ensuring that y is distinguishable from the negative samples, i.e., the SPET images of other subjects (second term). The contrastive diffusion strategy extends contrastive learning to each time step, which allows LPET images to establish better associations with their corresponding RPET images at different denoising stages, thereby enhancing the mutual information between the LPET and RPET images as done in [33]."
Contrastive Diffusion Model with Auxiliary Guidance for Coarse-to-Fine PET Reconstruction,3.4,Training Loss,"Following [28], we modify the objective L DP M in Eq. 3, and train CPM and IRM jointly by minimizing the following loss function:In summary, the final loss function is:where m, n and k are the hyper-parameters controlling the weights of each loss."
Contrastive Diffusion Model with Auxiliary Guidance for Coarse-to-Fine PET Reconstruction,3.5,Implementation Details,"The proposed method is implemented by the Pytorch framework using an NVIDIA GeForce RTX 3090 GPU with 24GB memory. The IRM in our framework is built upon the architecture of SR3 [21], a standard conditional DPM.The number of downsampling operations M is 3, and the negative sample set number N is 10. 4 neighboring slices are used as the NAS guidance and the spectrums are obtained through discrete Fourier transform. As for the weights of each loss, we set m = n = 1, and k = 5e-5 following [33]. We train our model for 500,000 iterations with a batch size of 4, using an Adam optimizer with a learning rate of 1e-4. The total diffusion steps T are 2,000 during training and 10 during inference."
Contrastive Diffusion Model with Auxiliary Guidance for Coarse-to-Fine PET Reconstruction,4,Experiments and Results,"Datasets and Evaluation: We conducted most of our low-dose brain PET image reconstruction experiments on a public brain dataset, which is obtained from the Ultra-low Dose PET Imaging Challenge 2022 [4]. Out of the 206 18F-FDG brain PET subjects acquired using a Siemens Biograph Vision Quadra, 170 were utilized for training and 36 for evaluation. Each subject has a resolution of 128 × 128 × 128, and 2D slices along the z-coordinate were used for training and evaluation. To simulate LPET images, we applied a dose reduction factor of 100 to each SPET image. To quantify the effectiveness of our method, we utilized three common evaluation metrics: the peak signal-to-noise (PSNR), structural similarity index (SSIM), and normalized mean squared error (NMSE). Additionally, we also used an in-house dataset, which was acquired on a Siemens Biograph mMR PET-MR system. This dataset contains PET brain images collected from 16 subjects, where 8 subjects are normal control (NC) and 8 subjects are mild cognitive impairment (MCI). To evaluate the generalizability of our method, all the experiments on this in-house dataset are conducted in a cross-dataset manner, i.e., training exclusively on the public dataset and inferring on the in-house dataset. Furthermore, we perform NC/MCI classification on this dataset as the clinical diagnosis experiment. Please refer to the supplementary materials for the experimental results on the in-house dataset.Comparison with SOTA Methods: We compare the performance of our method with 6 SOTA methods, including DeepPET [9] (regression-based method), Stack-GAN [27], Ea-GAN [31], AR-GAN [16], 3D CVT-GAN [32] (GAN-based method) and NVAE [3] (likelihood-based method) on the public  dataset. Since the IRM contains a stochastic process, we can also average multiple sampled (AMS) results to obtain a more stable reconstruction, which is denoted as Ours-AMS. Results are provided in Table 1. As can be seen, our method significantly outperforms all other methods in terms of PSNR, SSIM, and NMSE, and the performance can be further amplified by averaging multiple samples. Specifically, compared with the current SOTA method 3D CVT-GAN, our method (or ours-AMS) significantly boosts the performance by 0.558 dB (or 0.796 dB) in terms PSNR, 0.003 (or 0.004) in terms of SSIM, and 0.006 (or 0.007) in terms of NMSE. Moreover, 3D CVT-GAN uses 3D PET images as input. Since 3D PET images contain much more information than 2D PET images, our method has greater potential for improvement when using 3D PET images as input. Visualization results are illustrated in Fig. 2. Columns from left to right show the SPET, LPET, and RPET results output by different methods.Rows from top to bottom display the reconstructed results, zoom-in details, and error maps. As can be seen, our method generates the lowest error map while the details are well-preserved, consistent with the quantitative results.Ablation Study: To thoroughly evaluate the impact of each component in our method, we perform an ablation study on the public dataset by breaking down our model into several submodels. We begin by training the SR3 model as our baseline (a). Then, we train a single CPM with an L2 loss (b), followed by the incorporation of the IRM to calculate the residual (c), and the addition of the auxiliary NAS guidance (d), the spectrum guidance (e), and the L CL loss term (f). Quantitative results are presented in Table 2. By comparing the results of (a) and (c), we observe that our coarse-to-fine design can significantly reduce the computational overhead of DPMs by decreasing MParam from 128.740 to 31.020 and BFLOPs from 5973 to 132, while achieving better results. The residual generated in (c) also helps to improve the result of the CPM in (b), leading to more accurate PET images. Moreover, our proposed auxiliary guidance strategy and contrastive learning strategy further improve the reconstruction quality, as seen by the increase in PSNR, SSIM, and NMSE scores from (d) to (f). Additionally, we calculate the standard deviation (SD) of the averaged multiple sampling results to measure the input-output correspondence. The standard deviation (SD) of (c) (6.16e-03) is smaller compared to (a) (3.78e-03). This is because a coarse RPET has been generated by the deterministic process. As such, the stochastic process IRM only needs to generate the residual, resulting in less output variability. Then, the SD continues to decrease (3.78e-03 to 2.49e-03) as we incorporate more components into the model, demonstrating the improved input-output correspondence."
Contrastive Diffusion Model with Auxiliary Guidance for Coarse-to-Fine PET Reconstruction,5,Conclusion,"In this paper, we propose a DPM-based PET reconstruction framework to reconstruct high-quality SPET images from LPET images. The coarse-to-fine design of our framework can significantly reduce the computational overhead of DPMs while achieving improved reconstruction results. Additionally, two strategies, i.e., the auxiliary guidance strategy and the contrastive diffusion strategy, are proposed to enhance the correspondence between the input and output, further improving clinical reliability. Extensive experiments on both public and private datasets demonstrate the effectiveness of our method."
Contrastive Diffusion Model with Auxiliary Guidance for Coarse-to-Fine PET Reconstruction,,Fig. 1 .,
Contrastive Diffusion Model with Auxiliary Guidance for Coarse-to-Fine PET Reconstruction,,Fig. 2 .,
Contrastive Diffusion Model with Auxiliary Guidance for Coarse-to-Fine PET Reconstruction,,Table 1 .,
Contrastive Diffusion Model with Auxiliary Guidance for Coarse-to-Fine PET Reconstruction,,Table 2 .,
Contrastive Diffusion Model with Auxiliary Guidance for Coarse-to-Fine PET Reconstruction,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5_23.
3D Teeth Reconstruction from Panoramic Radiographs Using Neural Implicit Functions,1,Introduction,"Panoramic radiography (panoramic X-ray, or PX) is a commonly used technique for dental examination and diagnosis. While PX produces 2D images from panoramic scanning, Cone-Beam Computed Tomography (CBCT) is an alternative imaging modality which provides 3D information on dental, oral, and maxillofacial structures. Despite providing more comprehensive information than PX, CBCT is more expensive and exposes patients to a greater dose of radiation [3]. Thus, 3D teeth reconstruction from PX is of significant value, e.g., 3D visualization can aid clinicians with dental diagnosis and treatment planning. Other applications include treatment simulation and interactive virtual reality for dental education [12].Previous 3D teeth reconstruction methods from 2D PX have relied on additional information such as tooth landmarks or tooth crown photographs. For example, [14] developed a model which uses landmarks on PX images to estimate 3D parametric models for tooth shapes, while [1] reconstructed a single tooth using a shape prior and reflectance model based on the corresponding crown photograph. Recent advances in deep neural networks have significantly impacted research on 3D teeth reconstruction. X2Teeth [13] performs 3D reconstruction of the entire set of teeth from PX based on 2D segmentation using convolutional neural networks. Oral-3D [22] generated 3D oral structures without supervised segmentation from PX using a GAN model [8]. Yet, those methods relied on synthesized images as input instead of real-world PX images, where the synthesized images are obtained from 2D projections of CBCT [27]. The 2D segmentation of teeth from PX is useful for 3D reconstruction in order to identify and isolate teeth individually. Prior studies on 2D teeth segmentation [11,28] focused on binary segmentation determining the presence of teeth. However, this information alone is insufficient for the construction of individual teeth. Instead, we leverage recent frameworks [17,21] on multi-label segmentation of PX into 32 classes including wisdom teeth.In this paper, we propose Occudent, an end-to-end model to reconstruct 3D teeth from 2D PX images. Occudent consists of a multi-label 2D segmentation followed by 3D teeth reconstruction using neural implicit functions [15]. The function aims to learn the occupancy of dental structures, i.e., whether a point in space lies within the boundaries of 3D tooth shapes. Learning implicit functions is computationally advantageous over conventional encoder-decoder models outputting explicit 3D representations such as voxels, e.g., implicit models do not require large memory footprints to store and process voxels. Considering that 3D tooth shapes are characterized by tooth classes, we generate embeddings for tooth classes as well as segmented 2D tooth shapes. The combined class and shape embeddings are infused into the reconstruction network by a novel module called Conditional eXcitation (CX). CX performs learnable scaling of occupancy features conditioned on tooth class and shape embeddings. The performance of Occudent is evaluated with actual PX as input images, which differs from recent works using synthesized PX images [13,22]. Experiments show Occudent outperforms state-of-the-art baselines both quantitatively and qualitatively. The main contributions are summarized as follows: (1) the first use of a neural implicit function for 3D teeth reconstruction, (2) novel strategies to inject tooth class and shape information into implicit functions, (3) the superiority over existing baselines which is demonstrated with real-world PX images."
3D Teeth Reconstruction from Panoramic Radiographs Using Neural Implicit Functions,2,Methods,"The proposed model, Occudent, consists of two main components: 2D teeth segmentation and 3D teeth reconstruction. The former performs the segmentation of 32 teeth from PX using UNet++ model [29]. The individually segmented tooth and the tooth class are subsequently passed to the latter for the reconstruction. The reconstruction process estimates the 3D representation of the tooth based on a neural implicit function. The overall architecture of Occudent is depicted in Fig. 1. CX uses a trainable weight matrix to encode the condition vector into an excitation vector via a gating function. The input feature is scaled using the excitation vector through component-wise multiplication."
3D Teeth Reconstruction from Panoramic Radiographs Using Neural Implicit Functions,2.1,2D Teeth Segmentation,"The teeth in input PX are segmented into 32 teeth classes. The 32 classes correspond to the traditional numbering of teeth, which includes incisors, canines, premolars and molars in both upper and lower jaws. We pose 2D teeth segmentation as a multi-label segmentation problem [17], since nearby teeth can overlap with each other in the PX image, i.e., a single pixel of the input image can be classified into two or more classes.The input of the model is H ×W size PX image. The segmentation output has dimension C × H × W , where channel dimension C = 33 represents the number of tooth classes: one class for the background and 32 classes for teeth similar to [17]. Hence, the H × W output at each channel is a segmentation output for each tooth class. The segmentation outputs are used to generate tooth patches for reconstruction, which is explained later in detail. For the segmentation, we adopt pre-trained UNet++ [29] as the base model. UNet++ is advantageous for medical image segmentation due to its modified skip pathways, which results in better performance compared to the vanilla UNet [20]."
3D Teeth Reconstruction from Panoramic Radiographs Using Neural Implicit Functions,2.2,3D Teeth Reconstruction,"Neural Implicit Representation. Typical representations of 3D shapes are point-based [7,19], voxel-based [4,26], or mesh-based methods [25]. These methods represent 3D shapes explicitly through a set of discrete points, vertices, and faces. Recently, implicit representation methods based on a continuous function which defines the boundary of 3D shapes have become increasingly popular [15,16,18]. Occupancy Networks [15] is a pioneering work which utilizes neural networks to approximate the implicit function of an object's occupancy. The term occupancy refers to whether a point in space lies in the interior or exterior of object boundaries. The occupancy function maps a 3D point to either 0 or 1, indicating the occupancy of the point. Let o A denote the occupancy function for an object A as follows:In practice, o A can be estimated only by a set of observations of object A, denoted by X A . Examples of observations are projected images or point cloud data obtained from the object. Our objective is to estimate the occupancy function conditioned on X A . Specifically, we would like to find function f θ which estimates the occupancy probability of a point in 3D space based on X A [15]:Inspired by the aforementioned framework, we leverage segmented tooth patch and tooth class as observations denoted by condition vector c. Specifically, the input to the function is a set of T randomly sampled locations within a unit cube, and the function outputs the occupancy probability of the input. Thus, the function is given by fThe model for f θ is depicted in Fig. 1 (b). The sampled locations are projected to 128 dimensional feature vectors using 1D convolution. Next, the features are processed by a sequence of ResNet blocks followed by FC (fully connected) layers. Conditional vector c is used for each block through Conditional eXcitation (CX) which we will explain later."
3D Teeth Reconstruction from Panoramic Radiographs Using Neural Implicit Functions,,Class-Specific Conditional Features.,"A distinctive feature of the tooth reconstruction task is that teeth with the same number share properties such as surface and root shapes. Hence, we propose to use tooth class information in combination with a segmented tooth patch from PX. The tooth class is processed by a learnable embedding layer which outputs a class embedding vector.Next, we create a square patch of the tooth using the segmentation output as follows. A binary mask of the segmented tooth is generated by applying thresholding to the segmentation output. A tooth patch is created by cropping out the tooth region from the input PX, i.e., the binary mask is applied (bitwise AND) to the input PX to obtain the patch. The segmented tooth patch is subsequently encoded using a pre-trained ResNet18 model [9], which outputs a patch embedding vector. The patch and class embeddings are added to yield the condition vector for the reconstruction model. This process is depicted in Fig. 1 (a).Our approach differs from previous approaches, such as Occupancy Networks [15] which uses only single-view images for 3D reconstruction. X2Teeth [13] also addresses the task of 3D teeth reconstruction from 2D PX. However, X2Teeth only uses segmented image features for the reconstruction. By contrast, Occudent leverages a class-specific encoding method to boost the reconstruction performance, as demonstrated in ablation analysis in Supplementary Materials.Conditional eXcitation. To effectively inject 2D observations into the reconstruction network, we propose Conditional eXcitation (CX) inspired by Squeezeand-Excitation Network (SENet) [10]. In SENet, excitation refers to scaling input features according to their importance. In Occudent, the concept of excitation is extended to incorporating conditional features into the network. Firstly, the condition vector is encoded into excitation vector e. Next, the excitation is applied to input feature by scaling the feature components by e. The CX procedure can be expressed as:where c is the condition vector, σ is a gating function, W is a learnable weight matrix, α is a hyperparameter for the excitation result, and F ext is the excitation function. We use sigmoid function for σ, and component-wise multiplication for the excitation, F ext (e, x) = e ⊗ x. The CX module is depicted in Fig. 1 (d). CX differs from SENet in that CX derives the excitation from the condition vector, whereas SENet derives it from input features. Our approach also differs from Occupancy Networks which used Conditional Batch Normalization (CBN) [5,6] which combines conditioning with batch normalization. However, the conditioning process should be independent of input batches because those components serve different purposes in deep learning models. Thus, we propose to separate conditioning from batch normalization, as is done by CX."
3D Teeth Reconstruction from Panoramic Radiographs Using Neural Implicit Functions,3,Experiments,"Dataset. Implementation Details. For the pre-training of the segmentation model, we utilized a combination of cross-entropy and dice loss. For the main segmentation training, we used only dice loss. The segmentation and reconstruction models were trained separately. Following the completion of the segmentation model training, we fixed this model to predict its output for the reconstruction model.Each 3D tooth label was fit in 144 × 80 × 80 size tensor which was then regarded as [-0.5, 0.5] 3 normalized cube in 3D space. For the training of the neural implicit function, a set of T = 100, 000 points was sampled from the unit cube. The preprocessing was consistent with that used in [23]. We trained all the other baseline models with these normalized cubes. For example, for 3D-R2N2 [4], we voxelized the cube to 128 3 size. For a fair comparison, the final meshes produced by each model were extracted and compared using four different metrics. The detailed configuration of our model is provided in Supplementary Materials.Baselines. We considered several state-of-the-art models as baselines, including 3D-R2N2 [4], DeepRetrieval [13,24], Pix2Vox [26], PSGN [7], Occupancy Networks (OccNet) [15], and X2Teeth [13]. To adapt the 3D-R2N2 model to singleview reconstruction, we removed its LSTM component, following the approach in [15]. As for the DeepRetrieval method, we employed the same encoder architecture as 3D-R2N2, and utilized the encoded feature vector of the test image Evaluation Metrics. The evaluation of the proposed method was conducted using the following metrics: volumetric Intersection over Union (IoU), Chamfer-L 1 distance, and Normal Consistency (NC), as outlined in prior work [15]. In addition, we used volumetric precision [13] as a metric given by |D ∩ G|/|D| where G denotes the ground-truth set of points occupied by the object, and D denotes the set of points predicted as the object.Quantitative Comparison. Table 1 presents a quantitative comparison of the proposed model with several baseline models. The results demonstrate that Occudent surpasses the other methods across all the metrics, and the methods based on neural implicit functions (Occudent and OccNet) perform better compared to conventional encoder-decoder approaches, such as Pix2Vox and 3D-R2N2. The performance gap between Occudent and X2Teeth is presumably because real PX images are used as input data. X2Teeth used synthesized images generated from the 2D projections of CBCT in [27]. Thus, both the input 2D shape and the target 3D shape come from the same modality (CBCT). However, the distribution of real PX images may differ significantly from that of 2D-projected CBCT. Explicit methods can be more sensitive to such differences than implicit methods, because typically in explicit methods, input features are directly encoded and subsequently decoded to predict the target shapes [4,13,26]. Overall, the differences in the IoU performances among the baselines are somewhat small. This is because all the baselines are moderately successful in generating coarse tooth shapes. However, Occudent is significantly better at generating details such as root shapes, which will be shown in the subsequent section.Qualitative Comparison. Figure 2A illustrates the qualitative results of the proposed method in generating 3D teeth mesh outputs. From our model, each tooth is generated, and generated teeth are combined along with an arch curve based on a beta function [2]. Figure 2A demonstrates that our proposed method generates the most similar-looking outputs compared to the ground truth. For instance, our model can reconstruct a plausible shape for all tooth types including detailed shapes of molar roots. 3D-R2N2 produces larger and less detailed tooth shapes. PSGN and OccNet are better at generating rough shapes than 3D-R2N2, however, lack in detailed root shapes. As illustrated in Fig. 2B, Occudent produces a more refined mesh of tooth shape representation than voxel-based methods like 3D-R2N2 or X2Teeth. One of the limitations of voxel-based methods is that they heavily depend on the resolution of the output. For example, increasing the output size leads to an exponential increase in model size. By contrast, Occudent employs continuous neural implicit functions to represent shape boundaries, which enables us to generate smoother output and to be robust to the target size."
3D Teeth Reconstruction from Panoramic Radiographs Using Neural Implicit Functions,4,Conclusion,"In this paper, we present a framework for 3D teeth reconstruction from a single PX. To the best of our knowledge, our method is the first to utilize a neural implicit function for 3D teeth reconstruction. The performance of our proposed framework is evaluated quantitatively and qualitatively, demonstrating its superiority over state-of-the-art techniques. Importantly, our framework is capable of accommodating two distinct modalities, PX, and CBCT. Our framework has the potential to be valuable in clinical practice and also can support virtual simulation or educational tools. In the future, further improvements can be made, such as incorporating additional imaging modalities or exploring neural architectures for more robust reconstruction."
3D Teeth Reconstruction from Panoramic Radiographs Using Neural Implicit Functions,,Fig. 1 .,
3D Teeth Reconstruction from Panoramic Radiographs Using Neural Implicit Functions,,Fig. 2 .,
3D Teeth Reconstruction from Panoramic Radiographs Using Neural Implicit Functions,,,
3D Teeth Reconstruction from Panoramic Radiographs Using Neural Implicit Functions,,Table 1 .,
3D Teeth Reconstruction from Panoramic Radiographs Using Neural Implicit Functions,,Acknowledgements,". This work was supported by the Korea Medical Device Development Fund grant funded by the Korea Government (the Ministry of Science and ICT, the Ministry of Trade, Industry and Energy, the Ministry of Health & Welfare, the Ministry of Food and Drug Safety) (Project Number: 1711195279, RS-2021-KD000009); the National Research Foundation of Korea (NRF) Grant through the Ministry of Science and ICT (MSIT), Korea Government, under Grant 2022R1A5A1027646; the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (No. 2021R1A2C1007215); the MSIT, Korea, under the ICT Creative Consilience program (IITP-2023-2020-0-01819) supervised by the IITP (Institute for Information & communications Technology Planning & Evaluation)"
3D Teeth Reconstruction from Panoramic Radiographs Using Neural Implicit Functions,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 36.
Self-supervised MRI Reconstruction with Unrolled Diffusion Models,1,Introduction,"Magnetic Resonance Imaging (MRI) is one of the most widely used imaging modalities due to its excellent soft tissue contrast, but it has prolonged and costly scan sessions. Therefore, accelerated MRI methods are needed to improve its clinical utilization. Acceleration through undersampled acquisitions of a subset of kspace samples (i.e., Fourier domain coefficients) results in aliasing artifacts [8,17]. Many promising deep-learning methods have been proposed to reconstruct images by suppressing aliasing artifacts [1,2,7,9,11,15,16,18,22,24,27,29]. However, many existing methods are limited by suboptimal capture of the data distribution, poor contextual sensitivity, and reliance on fully-sampled acquisitions for model training [7,13,23].A recently emergent framework for learning data distributions in computer vision is based on diffusion models [10,19]. Several recent studies have considered diffusion-based MRI reconstructions, where either an unconditional or a conditional diffusion model is trained to generate images and reconstruction is achieved by later injecting data-consistency projections in between diffusion steps during inference [3,4,6,20,25]. While promising results have been reported, these diffusion methods can show limited reliability due to omission of physical constraints during training, and undesirable reliance on fully-sampled images. There is a more recent work that tried to mitigate fully-sampled data needs by Cui et al. [5]. In this work authors proposed a two-staged training strategy where a Bayesian network is used to learn the fully-sampled data distribution to train a score model which is then used for conditional sampling. Our model differs from this approach since we trained it end-to-end without allowing error propagation from distinct training sessions.To overcome mentioned limitations, we propose a novel self-supervised accelerated MRI reconstruction method, called SSDiffRecon. SSDiffRecon leverages a conditional diffusion model that interleaves linear-complexity cross-attention transformer blocks for denoising with data-consistency projections for fidelity to physical constraints. It further adopts self-supervised learning by prediction of masked-out k-space samples in undersampled acquisitions. SSDiffRecon achieves on par performance with supervised baselines while outperforming selfsupervised baselines in terms of inference speed and image fidelity."
Self-supervised MRI Reconstruction with Unrolled Diffusion Models,2,Background,
Self-supervised MRI Reconstruction with Unrolled Diffusion Models,2.1,Accelerated MRI Reconstruction,"Acceleration in MRI is achieved via undersampling the acquisitions in the Fourier domain as followswhere F p is the partial Fourier operator, C denotes coil sensitivity maps, I is the MR image and y p is partially acquired k-space data. Reconstruction of fully sampled target MR image I from y p is an ill-posed problem since the number of unknowns are higher than the number of equations. Supervised deep learning methods try to solve this ill-posed problem using prior knowledge gathered in the offline training sessions as followswhere I is the reconstruction, and λ(I) is the prior knowledge-guided regularization term. In supervised reconstruction frameworks, prior knowledge is induced from underlying mapping between under-and fully sampled acquisitions. "
Self-supervised MRI Reconstruction with Unrolled Diffusion Models,2.2,Denoising Diffusion Models,"In diffusion models [10], Gaussian noise is progressively mapped on the data via a forward noising processwhere β t refers to the fixed variance schedule. After a sufficient number of forward diffusion steps (T ), x t follows a Gaussian distribution. Then, the backward diffusion process is deployed to gradually denoise x T to get x 0 using a deep neural network as a denoiser as followswhere σ 2 t = βt = 1-ᾱt-1 1-ᾱt β t and θ represents the denoising neural network parametrized during backward diffusion and trained using the following loss [10] where ᾱt = t m=1 α m , α t = 1 -β t and ∼ N (0, I)."
Self-supervised MRI Reconstruction with Unrolled Diffusion Models,3,SSDiffRecon,"In SSDiffRecon, we utilize a conditional diffusion probabilistic model to reconstruct fully-sampled MR images given undersampled acquisitions as input. The reverse diffusion steps are parametrized using an unrolled transformer architecture as shown in Fig. 1. To improve adaptation across time steps in the diffusion process, we inject the time-index t via cross-attention transformers as opposed to the original DDPM models that add time embeddings as a bias term. In what follows we describe the training and inference procedures of SSDiffRecon."
Self-supervised MRI Reconstruction with Unrolled Diffusion Models,,Self-Supervised Training:,"For self-supervised learning, we adopt a k-space masking strategy for diffusion models [26] as followswhere • 1 denotes the L1-norm, F denotes 2D Fourier Transform, C are coil sensitivities, x us is the image derived from undersampled acquisitions, and M l is the random sub-mask within the main undersampling mask M. Here x t recon is the output of the unrolled denoiser network (R θ ) at time instant t ∈ {T, T -1, ..., 0}where M p is the sub-mask of the remaining points in M after excluding M l .Inference: To speed up image sampling, inference starts with zero-filled Fourier reconstruction of the undersampled acquisitions as opposed to a pure noise sample. Conditional diffusion sampling is then performed with the trained diffusion model that iterates through cross-attention transformers for denoising and data-consistency projections. For gradual denoising, we introduce a descending random noise onto the undersampled data within data-consistency layers. Accordingly, the reverse diffusion step at time-index t is given aswhere low ∼ N (0, 0.1I) and z ∼ N (0, I).Unrolled Denoising Network R θ (.): SSDiffRecon deploys an unrolled physics-guided denoiser in the diffusion process instead of UNET as is used in [10]. Our denoiser network consists of the following two fundamental structures as shown in Fig. 1. The entire network is trained end-to-end. Unrolled Denoising Blocks: Each denoising block consists of cross-attention and data-consistency layers sequentially. Let the input of the jth denoising block at time instant t be x t in,j ∈ R (h×w)×n , where h and w denote the height and width of the image, and n denotes the number of feature channels. For the first denoising block n = 2 and (x t in,j = x t us ). First, input is modulated with the affine-transformed global latent variable (w g ∈ R 32 ) via modulated-convolution adopted from [12]. Assuming that the modulated-convolution kernel is given as β j , this operation is expressed as followswhere β u,v j ∈ R 3×3 is the convolution kernel for the u th input channel and the v th output channel, and m is the channel index. Then, the output of modulated convolution goes into the cross-attention transformer where the attention map att t j is calculated using local latent variables w t l at time index t as followswhere Q j (.), K j (.), V j (.) are queries, keys and values, respectively where each function represents a dense layer with input inside the parenthesis, and P.E. is the positional encoding. Then, x t output,j is normalized to zero-mean unit variance and scaled with a learned projection of the attention maps att t j as followsx t output,j = α j (att j )where α j (.) is the learned scale parameter. After repeating the sequence of crossattention layer twice, lastly the data-consistency is performed. To perform dataconsistency the number of channels in x t output,j is decreased to 2 with an additional convolution layer. Then, 2-channel images are converted, where channels represent real and imaginary components, to complex and data-consistency is applied as followswhere F -1 represents the inverse 2D Fourier transform and x t us = x us during training. Then, using another extra convolution, the number of feature maps are increased to n again for the next denoising block.Implementation Details : Adam optimizer is used for self-supervised training with β = (0.9, 0.999) and learning rate 0.002. Default noise schedule paramaters are taken from [10]. 1000 forward and 5 reverse diffusion steps are used for training and inference respectively with batch size equals to 1. M l are sampled from M using uniform distribution by collecting 5% of acquired points. We used network snapshots at 445K and 654K steps which corresponds to 28th and 109th epochs for IXI and fastMRI datasets respectively. A single NVIDIA RTX A5000 gpu is used for training and inference."
Self-supervised MRI Reconstruction with Unrolled Diffusion Models,4,Experimental Results,
Self-supervised MRI Reconstruction with Unrolled Diffusion Models,4.1,Datasets,"Experiments are performed using the following multi-coil and single-coil brain MRI datasets:1. fastMRI: Reconstruction performance illustrated in multi-coil brain MRI dataset [14], 100 subjects are used for training, 10 for validation and 40 for testing. Data from multiple sites are included with no common protocol. T 1 -, T 2 -and Flair-weighted acquisitions are considered. GCC [28] is used to decrease the number of coils to 5 to reduce the computational complexity. 2. IXI: Reconstruction performance illustrated in single-coil brain MRI data from IXI (http://brain-development.org/ixi-dataset/). T 1 -, T 2 -and PDweighted acquisitions are considered. In IXI, 25 subjects are used for training, 5 for validation and 10 for testing.Acquisitions are retrospectively undersampled using variable-density masks.Undersampling masks are generated based on a 2D Gaussian distribution with variance adjusted to obtain acceleration rates of R = [4,8]."
Self-supervised MRI Reconstruction with Unrolled Diffusion Models,4.2,Competing Methods,"We compare the performance of SSDiffRecon with the following supervised and self-supervised baselines:1. DDPM: Supervised diffusion-based reconstruction baseline. DDPM is trained with fully sampled MR images and follows a novel k-space sampling approach during inference introduced by Peng et al. [20]. 1000 forward and backward diffusion steps are used in training and inference respectively. 2. self-DDPM: Self-supervised diffusion-based reconstruction baseline. Self-DDPM is trained using only under-sampled MRI acquisitions. Other than training, the inference procedure is identical to the DDPM. 3. D5C5: Supervised model-based reconstruction baseline. D5C5 is trained using under-and fully sampled paired MR images. Network architecture and training loss are adopted from [21]. 4. self-D5C5: Self-supervised model-based reconstruction baseline. Self-D5C5 is trained using the self-supervision approach introduced in [26] using undersampled acquisitions. The hyperparameters and network architecture are the same as in D5C5. 5. RGAN: CNN-based reconstruction baseline. RGAN is trained using paired under-and fully sampled MR images. Network architecture and hyperparameters are adapted from [7]. 6. self-RGAN: Self-supervised CNN-based reconstruction baseline. Self-RGAN is trained using the self-supervision loss in [26] using only under-sampled images. Network architecture and other hyperparameters are identical to RGAN."
Self-supervised MRI Reconstruction with Unrolled Diffusion Models,4.3,Experiments,"We compared the reconstruction performance using Peak-Signal-to-Noise-Ratio (PSNR, dB) and Structural-Similarity-Index (SSIM, %) between reconstructions and the ground truth images. Hyperparameter selection for each method is performed via cross-validation to maximize PSNR.Ablation Experiments. We perform the following four ablation experiments to show the relative effect of each component in the model on the reconstruction quality as well as the effect of self-supervision in Table 1.1. Supervised: Supervised training of SSDiffRecon using paired under-and fully sampled MR images and pixel-wise loss is performed. Other than training, inference sampling procedures are the same as the SSDiffRecon. 2. UNET: Original UNET architecture in DDPM [10]   "
Self-supervised MRI Reconstruction with Unrolled Diffusion Models,5,Results,"The results are presented in two clusters using a single figure and table for each dataset; fastMRI results are presented in Fig. 3 and Table 3, and IXI results are presented in Fig. 2 and Table 2. The best performed method in each test case is marked in bold in the tables. SSDiffRecon yields 2.55 dB more average PSNR and %1.96 SSIM than the second best self-supervised baseline in IXI, while performing 0.4 dB better in terms of PSNR and %0.25 in terms of SSIM on fastMRI. Visually, it captured most of the high frequency details while other self-supervised reconstructions suffer from either high noise or blurring artifact. Moreover, visual quality of reconstructions is either very close or better than supervised methods as be seen in the figures. It is also important to note that SSDiffRecon is performing only five backward diffusion steps while regular DDPM perform thousand diffusion steps for an equivalent reconstruction performance.  "
Self-supervised MRI Reconstruction with Unrolled Diffusion Models,6,Conclusion,We proposed a novel diffusion-based unrolled architecture for accelerated MRI reconstruction. Our model performs better than self-supervised baselines in a relatively short inference time while performing on-par with the supervised reconstruction methods. Inference time and model complexity analyses are presented in the supplementary materials.
Self-supervised MRI Reconstruction with Unrolled Diffusion Models,,Fig. 1 .,
Self-supervised MRI Reconstruction with Unrolled Diffusion Models,,Fig. 2 .,
Self-supervised MRI Reconstruction with Unrolled Diffusion Models,,Fig. 3 .,
Self-supervised MRI Reconstruction with Unrolled Diffusion Models,,,
Self-supervised MRI Reconstruction with Unrolled Diffusion Models,,Table 1 .,
Self-supervised MRI Reconstruction with Unrolled Diffusion Models,,Table 2 .,
Self-supervised MRI Reconstruction with Unrolled Diffusion Models,,Table 3 .,
Self-supervised MRI Reconstruction with Unrolled Diffusion Models,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 47.
$$\mathrm {H^{2}}$$GM: A Hierarchical Hypergraph Matching Framework for Brain Landmark Alignment,1,Introduction,"The ultimate goal of entire cortex registration (ECR) is to achieve functional region alignment across subjects. However, existing software often sacrifices the local functional alignment accuracy to accomplish ECR [6]. Previous studies have identified a novel brain gyrus landmark, termed the gyral hinge (GH) [10,13,30]. It has been demonstrated that the precise GH alignment over the whole brain is critical for understanding the relationship between brain structure and function [16]. However, achieving accurate and robust GH alignment is challenging due to the massive cortical morphological variations in GHs between subjects [9,16].To address this issue, existing works have introduced the single-scale graph structure to represent and align GHs [16,29,31]. As shown in Fig. 1(a), they model GHs as graph nodes, formulate graph edges with the white matter fiber connections between GHs, and deploy the graph matching algorithms to align GHs [29,31]. Despite the great successes, existing works suffer from two overlooked deficiencies. First, they consider only pairwise relations between GHs to construct graphs that use second-order edges (the one-hop connection between two GHs), without considering underlying complex (e.g., high-order) relations among more than two GHs [23,28]. For example, the high-order relations of GHs within the same brain region cannot be well modeled only in low-order cues [5]. Hence, we are committed to going beyond the second-order relations and seeking a more effective graph structure, hypergraph, to model such high-order relations [2,15,26,32]. Second, existing works focus only on a single-scale, i.e., the point scale [16,29,31], ignoring the hierarchy of brain structure and function at multiple scales [1]. Only considering the single-scale knowledge is insufficient to capture the multi-scale dependence within graphs, inevitably introducing several alignment errors on small-scaled regions [3]. Hence, we aim to introduce multiscale knowledge in hypergraph matching (Fig. 1(b)), exploring more effective message propagation with hierarchical hypergraph learning. To overcome the aforementioned challenges, we propose a Hierarchical Hyper-Graph Matching (H 2 GM) framework for GH alignment, which consists of a Multi-scale Hypergraph Establishment (MsHE), a Multi-scale Hypergraph Matching (MsHM), and an Inter-Scale Consistency (ISC) constraint. Specifically, in the MsHE module, we construct multi-scale hypergraphs with three hierarchical scales (Five Lobes Atlas, DK Atlas [4], GH) and establish correspondences for both inter-scale and inter-subject hypergraphs. As for the MsHM module, we match the hypergraph pairs at each scale to entangle a robust GH alignment with multi-scale high-order cues. Finally, ISC incorporates inter-scale semantic consistency to encourage the agreement of multi-scale knowledge. Experimental results demonstrate that our framework enables more effective GH alignment. In summary, the main contributions are as follows: (1) We propose a H 2 GM framework for GH alignment. To the best of our knowledge, this work is the first attempt to leverage multi-scale hypergraphs to align the brain landmark.(2) We design a MsHE module to extract the high-order relations among GHs at different scales and a MsHM module to propagate GHs features to align GHs. Moreover, the feature distribution of GH optimized by the inter-scale semantic consistency further improves the alignment accuracy. (3) Extensive experiments verify that the proposed matching framework improves the GH alignment remarkably and outperforms state-of-the-art methods. "
$$\mathrm {H^{2}}$$GM: A Hierarchical Hypergraph Matching Framework for Brain Landmark Alignment,2,Method,"The overview of our proposed framework is shown in Fig. 2, which contains two parts. Given two subjects MRI, we extract GHs and establish multi-scale hypergraphsin the MsHE module (Fig. 2(a)), illustrated in Sect. 2.1. Then we perform multi-scale GH alignment in the MsHM module (Fig. 2(b)) to obtain multi-scale correspondence matrix C (k ) (Sect. 2.2). The C (k ) indicating the correspondence between two hypergraphs, i.e., C (k ) i,j = 1 means the i -th GH in source matches to the j -th GH in target at the k scale. The GH alignment can be formulated as the hypergraph matching problem:where θ represents the neural network parameters, the H is the incidence matrix of hypergraph [5], the X is the GHs' features, and the k is the number of scale.We use a neural network to predict C (k ) . The details will be introduced below."
$$\mathrm {H^{2}}$$GM: A Hierarchical Hypergraph Matching Framework for Brain Landmark Alignment,2.1,Multi-scale Hypergraph Establishment (MsHE),"As shown in Fig. 2(a), we utilize the Five Lobes Atlas, DK Atlas [4], and GH as three scales to model the inter-scale relations through a tree structure T [3]. In the tree structure T , the larger-scale brain regions are parent nodes, and the smaller-scale brain regions or GHs are child nodes. As for the multiscale hypergraph incidence matrixs/t , we capture the topological relations among brain atlases to serve as the incidence matrix. The element H (k ) i,j indicates that the i -th GH is included in the j -th hyperedge (brain region or GH). Notably, when k =3, the row sum ofs/t is in the diagonal format. As for the hypergraph nodes V s/t , i.e., GHs, we capture various cues in the subject to serve as the raw node feature. Specifically, considering the limited representation of the single vertex knowledge, we expand each GH on the surface by two rings, resulting in a total of 19 vertices as the GHs' raw features. Each vertex features contain three-dimensional coordinates (×3), normal vector coordinates (×3), curvature (×1), convexity (×1), and cortical thickness (×1). Then, the aforementioned features are contacted to obtain the raw node features, where m/n represent the number of source/target subject GHs, respectively. After that, the raw features are sent to Dynamic Graph CNN [25] (DGCNN) to extract GHs descriptors X (k) s/t ∈ R m/n×D as the final node representation. Notably, the parameters of the DGCNN are shared across all scales. For the hyperedge weight matrix W (k ) s/t , we propose to use a diagonal metric with one-value entries as the initialization, which will be optimized in the following hyperedge relation learning module. Finally, we obtain the multi-scale hypergraphs of source/target subjects ass/t . The proposed multi-scale hypergraphs can well model significant surface morphological information and multi-scale GH relations for better GH alignment."
$$\mathrm {H^{2}}$$GM: A Hierarchical Hypergraph Matching Framework for Brain Landmark Alignment,2.2,Multi-scale Hypergraph Matching (MsHM),"As shown in Fig. 2(b), the hyperedge features at k scale are computed ass/t , and then sent to MsHM to solve the multi-scale matching with four sub-stages, including hyperedge relation learning, self-hyperedge reasoning, bipartite hypergraph matching, and cross-hyperedge reasoning.Hyperedge Relation Learning: To dynamically learn hyperedge relation with better structural information, we propose hyperedge relation learning to model the high-order relation among hyperedges, instead of directly using the handcraft hyperedge weight metric W s/t [5,15]. The layer utilizes transformer [7,22], which comprises several encoder-decoder layers, to generate hyperedge soft edges. Specifically, the transformer takes hyperedge features E s/t as input and encodes them into embedding features. The inner product of these embedded features is then passed through a softmax function to generate the hyperedge weight matrices. This process can be expressed as follows:where f emb is a transformer-based feature embedding function."
$$\mathrm {H^{2}}$$GM: A Hierarchical Hypergraph Matching Framework for Brain Landmark Alignment,,Self-hyperedge Reasoning:,We utilize a self-hyperedge reasoning network to capture the self-correlation of hyperedge features. Propagating features achieve this by hyperedge weight matrix within each hypergraph. It can be written as:wheres/t ∈ R D×D denotes the learnable parameters of self-hyperedge reasoning network. And σ(•) is the nonlinear activation function.
$$\mathrm {H^{2}}$$GM: A Hierarchical Hypergraph Matching Framework for Brain Landmark Alignment,,Bipartite Hypergraph Matching:,"We use bipartite matching to determine a soft correspondence matrix between two subjects, which is achieved using the following expression:, where Gbm consists of an Affinity layer, an Instance normalization layer, a quadratic constrain (QC) layer, and a Sinkhorn layer. Initially, the affinity matrix is computed ast , where M (k) ∈ R D×D is the learnable parameter matrix in the affinity layer. Next, we apply instance normalization [20] to transform A (k) into a matrix with positive elements within finite values. We then introduce QC to minimize the structural difference of matched GH pairs [8,14]. For unmatched GHs, we add an additional row and column of ones to the output of the QC layer matrix, which is then processed through the Sinkhorn layer [19] to obtain a double-stochastic affinity matrix with maximum iteration optimization. After deleting the added row and column, we obtain a soft assignment matrix C (k) . Finally, we use crossentropy-type loss functions to compute the linear assignment cost between the ground truth and the soft assignment matrix, which is defined as follows:where, and this loss can optimize the Eq. 1. The k -scale soft assignment matrix of the MsHM l -th layer output is"
$$\mathrm {H^{2}}$$GM: A Hierarchical Hypergraph Matching Framework for Brain Landmark Alignment,,Cross-Hyperedge Reasoning:,"We further enhance the hyperedge features by exploring cross-correlation through cross-hyperedge reasoning. Different from self-hyperedge reasoning, the proposed cross-hyperedge reasoning enables subject-aware message propagation, facilitating effective interaction between subjects. The more similar a pair of hyperedges is between two subjects, the better features will be aggregated in better alignment. It can be written as:where f cross consists of a feature concatenate and a fully connected layer. Finally, the new GHs features with a symmetric normalization can be written as:where σ(•) is the nonlinear activation function.s/t are the diagonal node degree matrix and hyperedge degree matrix, respectively. Φ ∈ R D×D denotes the learnable parameters of cross-hyperedge reasoning network."
$$\mathrm {H^{2}}$$GM: A Hierarchical Hypergraph Matching Framework for Brain Landmark Alignment,2.3,Inter-Scale Consistency (ISC),"To avoid the potential disagreement among different scales, it is critical to introduce the consistency constraint across scales, which achieves collaborative optimization in different scales and prevents sub-optimal alignment. Specifically, we propose a novel ISC mechanism that leverages the local properties of the tree structure. For a parent hyperedge e p and its child hyperedges {e q c } Q q=1 (Q is the number of children nodes belonging to the same parent node.), which represent the same brain region at different scales, we enforce semantic-level consistency between the child hyperedges and their parent hyperedge to facility the reliable model learning, which is denoted as follows:where L scale denotes the loss function that enforces inter-scale semantic consistency among source/target subjects tree structure T s/t . By incorporating this loss, the model learns to maintain consistent semantic information across different scales for hyperedges corresponding to the same brain region."
$$\mathrm {H^{2}}$$GM: A Hierarchical Hypergraph Matching Framework for Brain Landmark Alignment,2.4,Model Optimization,"In the training of this work, we introduce a hyperparameter to add up L m and L scale . Then, the overall train loss for the GH alignment model is denoted as:where β is a hyperparameter to control the intensity. In testing, we utilize the Hungarian algorithm [12] to convert the soft assignment matrix into a binary matrix. Subsequently, the rows and columns of the binary matrix with a value of 1 represent the GHs correspondences between source and target subjects.  [5] 77.10 ± 8.53 37.01 ± 3.96 6.7 PCA-GM (ICCV2019) [24] 74.32 ± 8.55 35.50 ± 4.07 10.1 RGM (CVPR2021) [7] 74.71 ± 8.67 35.85 ± 4.03 9.7 QC-DGM (CVPR2021) [8] 71.83 ± 9.74 34.72 ± 4.26 11.2 REGTR (CVPR2022) [27] 75.30 ± 8.31 36.30 ± 3.97 8.8 Sigma++ (TPAMI2023) [15] 77 3 Experiments and Results"
$$\mathrm {H^{2}}$$GM: A Hierarchical Hypergraph Matching Framework for Brain Landmark Alignment,3.1,Experimental Setup,"Dataset: We evaluate our proposed framework effectiveness by conducting experiments on the GH alignment dataset, which includes the ground truth of GH correspondences across 250 subjects. Each GH contains brain atlas information, various morphological features from T1-weighted (T1w) MRI, and task activation vectors obtained from the task fMRI (tfMRI). The Five Lobe Atlas consists of 10 brain regions, while the DK Atlas includes 66 brain regions. The T1w MRI and tfMRI are acquired from the WU-Minn Human Connectome Project (HCP) consortium [21], with written informed consent obtained from HCP participants and relevant institutional review boards approving the study.Evaluation: In this study, we adopt the accuracy, the correlation (×10 -2 ), and the mean geodesic errors [17] (MGE, in centimeters) as evaluation metrics for our proposed model. Specifically, accuracy is computed as the average rate of correct GH alignments across all subjects. Furthermore, we use correlation to measure the similarity between the activation vectors of the aligned GH pairs across all subjects. The higher the value of correlation, the higher the confidence that the two GHs align correctly. To evaluate the performance of our model, we compare it against several state-of-the-art learning-based point cloud registration methods, including RGM [7], QC-DGM [8], and REGTR [27], as well as several learningbased graph matching methods, including HNN [5], PCA-GM [24], and Sigma++ [15], and the traditional surface registration method, SurfReg [6]. Notably, we exclude comparisons with non-open-source methods such as [16,29,31].Implementation Details To train our model, we utilize the Adam optimizer [11] with 3 × 10 -5 learning rate, 5 × 10 -4 weight decay, 1 batch size, and 40 epochs. We use L = 3 layers of the MsHM and performe 20 Sinkhorn iterations. We set the hyperparameter β = 10, the raw feature channel d = 171, and the GH descriptor channel D = 2048. We use 200 subjects as the training set and 50 subjects as the test set. We implement our network using the PyTorch library [18].  "
$$\mathrm {H^{2}}$$GM: A Hierarchical Hypergraph Matching Framework for Brain Landmark Alignment,3.2,Experimental Results,"Comparison with State-of-the-Arts: We present the comparison results in Table 1. The accuracy, correlation, and MGE of H 2 GM achieves 78.03 ± 8.21%, (37.99 ± 3.91) × 10 -2 , and 5.8, respectively, outperforming existing works by a large margin. Compared with learning-based approaches, showing our advantages over existing works. Besides, compared to other works, the highest correlation and the lowest MGE obtained by our proposed H 2 GM demonstrates that our proposed framework provides the most precise GH alignment.Ablation Studies: Table 1 displays ablation studies that verify the efficacy of each module of the proposed H 2 GM. Our results demonstrate that inter-scale semantic consistency enhances the effectiveness of our approach, as evidenced by the removal of ISC alignment accuracy 77.66 ± 8.27%. Introducing multi-scale atlases improves alignment accuracy by reducing error tolerance, as indicated by removing the two scales' brain atlas accuracy 74.81 ± 8.66%. Removing the hyperedge relation learning accuracy 77.43 ± 8.28% suggests that the hyperedge structure-aware message is instrumental in improving alignment accuracy."
$$\mathrm {H^{2}}$$GM: A Hierarchical Hypergraph Matching Framework for Brain Landmark Alignment,,Sensitivity Analysis:,"We conduct experiments with varying hyperparameters to investigate the sensitivity of β in Eq. 8 and record the results in Table 2. Our findings indicate that the highest alignment accuracy is achieved when β = 10. However, a β setting that is too small leads to a slight performance decline due to insufficient inter-scale semantic consistency. Conversely, a β setting that is too large also leads to a performance decline.Qualitative Analysis: Figure 3 presents the visualization results of GH alignment. Randomly selected pairs of subjects show that our method achieves the highest accuracy compared to state-of-the-art methods. These findings suggest that exploring higher-order relations between GHs can optimize GH alignment."
$$\mathrm {H^{2}}$$GM: A Hierarchical Hypergraph Matching Framework for Brain Landmark Alignment,4,Conclusion,"In this paper, we propose a novel framework H 2 GM for brain landmark alignment. Specifically, H 2 GM consists of a MsHE module for constructing the multiscale hypergraphs, a MsHM module for matching them, and ISC for incorporating the semantic consistency among scales. Experimental results demonstrate that our proposed H 2 GM outperforms existing approaches significantly."
$$\mathrm {H^{2}}$$GM: A Hierarchical Hypergraph Matching Framework for Brain Landmark Alignment,,Fig. 1 .,
$$\mathrm {H^{2}}$$GM: A Hierarchical Hypergraph Matching Framework for Brain Landmark Alignment,,Fig. 2 .,
$$\mathrm {H^{2}}$$GM: A Hierarchical Hypergraph Matching Framework for Brain Landmark Alignment,,Fig. 3 .,
$$\mathrm {H^{2}}$$GM: A Hierarchical Hypergraph Matching Framework for Brain Landmark Alignment,,Table 1 .,
$$\mathrm {H^{2}}$$GM: A Hierarchical Hypergraph Matching Framework for Brain Landmark Alignment,,Table 2 .,β = 0.1 77.96 ± 8.22 37.94 ± 3.91 6.3 β = 1 77.71 ± 8.24 37.90 ± 3.91 5.9 β = 10 78.03 ± 8.21 78.03 ± 8.21 78.03 ± 8.21 37.99 ± 3.91 37.99 ± 3.91 37.99 ± 3.91 5.8 5.8 5.8 β = 100 76.90 ± 8.30 36.92 ± 3.97 6.9
DisC-Diff: Disentangled Conditional Diffusion Model for Multi-contrast MRI Super-Resolution,1,Introduction,"Magnetic Resonance Imaging (MRI) is the primary management tool for brain disorders [24][25][26]. However, high-resolution (HR) MRI with sufficient tissue contrast is not always available in practice due to long acquisition time [19], where low-resolution (LR) MRIs significantly challenge clinical practice.Super-resolution (SR) techniques promise to enhance the spatial resolution of LR-MRI and restore tissue contrast. Traditional SR methods, e.g., bicubic interpolation [8], iterative deblurring algorithms [7] and dictionary learning-based methods [1] are proposed, which, however, are challenging to restore the highfrequency details of images and sharp edges due to the inability to establish the complex non-linear mapping between HR and LR images. In contrast, deep learning (DL) has outperformed traditional methods, owing to its ability to capture fine details and preserve anatomical structures accurately.Earlier DL-based SR methods [3,6,13,14,20,23,28] focused on learning the one-to-one mapping between the single-contrast LR MRI and its HR counterpart. However, multi-contrast MRI is often required for diagnosing brain disorders due to the complexity of brain anatomy. Single-contrast methods are limited by their ability to leverage complementary information from multiple MRI contrasts, leading to inferior SR quality. As an improvement, multi-contrast SR methods [5,17,21,22,27] are proposed to improve the restoration of anatomical details by integrating additional contrast information. For instance, Zeng et al. designed a CNN consisting of two subnetworks to achieve multi-contrast SR [27]. Lyu et al. presented a progressive network to generate realistic HR images from multicontrast MRIs by minimizing a composite loss of mean-squared-error, adversarial loss, perceptual loss etc. [17]. Feng et al. introduced a multi-stage integration network to extract complex interactions among multi-contrast features hierarchically, enhancing multi-contrast feature fusion [5]. Despite these advancements, most multi-contrast methods fail to 1) estimate restoration uncertainty for a robust model; 2) reduce the risk of mode collapse when applying adversarial loss to improve image fidelity.Conditional diffusion models are a class of deep generative models that have achieved competitive performance in natural image SR [4,10,18]. The model incorporates a Markov chain-based diffusion process along with conditional variables, i.e., LR images, to restore HR images. The stochastic nature of the diffusion model enables the generation of multiple HR images through sampling, enabling inherent uncertainty estimation of super-resolved outputs. Additionally, the objective function of diffusion models is a variant of the variational lower bound that yields stable optimization processes. Given these advantages, conditional diffusion models promise to update MRI SR methods.However, current diffusion-based SR methods are mainly single-contrast models. Several challenges remain for developing multi-contrast methods: 1) Integrating multi-contrast MRI into diffusion models increases the number of conditions. Traditional methods integrate multiple conditions via concatenation, which may not effectively leverage complementary information in multiple MRI contrasts, resulting in high-redundancy features for SR; 2) The noise and outliers in MRI can compromise the performance of standard diffusion models that use Mean Squared Error (MSE) loss to estimate the variational lower bound, leading to suboptimal results; 3) Diffusion models are often large-scale, and so are primarily intended for the generation of 2D images, i.e., treating MRI slices separately. Varied anatomical complexity across MRI slices can result in inconsistent diffusion processes, posing a challenge to efficient learning of SR-relevant features.To address the challenges, we propose a novel conditional disentangled diffusion model (DisC-Diff). To the best of our knowledge, this is the first diffusionbased multi-contrast SR method. The main contribution of our work is fourfold:-We propose a new backbone network disentangled U-Net for the conditional diffusion model, a U-shape multi-stream network composed of multiple encoders enhanced by disentangled representation learning. -We present a disentanglement loss function along with a channel attentionbased feature fusion module to learn effective and relevant shared and independent representations across MRI contrasts for reconstructing SR images. -We tailor a Charbonnier loss [2] to overcome the drawbacks of the MSE loss in optimizing the variational lower bound, which could provide a smoother and more robust optimization process. -For the first time, we introduce an entropy-inspired curriculum learning strategy for training diffusion models, which significantly reduces the impact of varied anatomical complexity on model convergence.Our extensive experiments on the IXI and in-house clinical datasets demonstrate that our method outperforms other state-of-the-art methods."
DisC-Diff: Disentangled Conditional Diffusion Model for Multi-contrast MRI Super-Resolution,2,Methodology,
DisC-Diff: Disentangled Conditional Diffusion Model for Multi-contrast MRI Super-Resolution,2.1,Overall Architecture,"The proposed DisC-Diff is designed based on a conditional diffusion model implemented in [4]. As illustrated in Fig. 1, the method achieves multi-contrast MRI SR through forward and reverse diffusion processes. Given an HR image x 0 ∼ q(x 0 ), the forward process gradually adds Gaussian noise to x 0 over T diffusion steps according to a noise variance schedule β 1 , . . . , β T . Specifically, each step of the forward diffusion process produces a noisier image x t with distribution q(x t | x t-1 ), formulated as:For sufficiently large T , the perturbed HR x T can be considered a close approximation of isotropic Gaussian distribution. On the other hand, the reverse diffusion process p aims to generate a new HR image from x T . This is achieved by constructing the reverse distribution p θ (x t-1 | x t , y, v), conditioned on its associated LR image y and MRI contrast v, expressed as follows:where p θ denotes a parameterized model, θ is its trainable parameters and σ 2 t can be either fixed to t t=0 β t or learned. It is challenging to obtain the reverse distribution via inference; thus, we introduce a disentangled U-Net parameterized model, shown in Fig. 2, which estimates the reverse distribution by learning disentangled multi-contrast MRI representations. Specifically, p θ learns to conditionally generate HR image by jointly optimizing the proposed disentanglement loss L disent and a Charbonnier loss L charb . Additionally, we leverage a curriculum learning strategy to aid model convergence of learning μ θ (x t , y, v, t).Disentangled U-Net. The proposed Disentangled U-Net is a multi-stream net composed of multiple encoders, separately extracting latent representations.We first denote the representation captured from the HR-MRI x t as Z xt ∈ R H×W ×2C , which contains a shared representation S xt and an independent representation I xt (both with 3D shape H × W × C) extracted by two 3 × 3 convolutional filters. The same operations on y and v yield S y , I y and S v , I v , respectively. Effective disentanglement minimizes disparity among shared representations while maximizing that among independent representations. Therefore, S xt/y/v are as close to each other as possible and can be safely reduced to a single representation S via a weighted sum, followed by the designed Squeezeand-Excitation (SE) module (Fig. 2 B) that aims to emphasize the most relevant features by dynamically weighting the features in I xt/y/v or S, resulting in rebalanced disentangled representations Îxt/y/v and Ŝ. Each SE module applies global average pooling to each disentangled representation, producing a length-C global descriptor. Two fully-connected layers activated by SiLU and Sigmoid are then applied to the descriptor to compute a set of weights s = [s 1 , . . . , s i , s i+1 , . . . , s C ], where s i represents the importance of the i-th feature map in the disentangled representation. Finally, the decoder block D shown in Fig. 2 performs upsampling on the concatenated representations [ Îxt , Îy , Îv , Ŝ] and outputs a noise prediction θ (x t , y, v, t) to compute the Gaussian mean in Eq. 3:where α t = 1β t and ᾱt = t s=0 α s ."
DisC-Diff: Disentangled Conditional Diffusion Model for Multi-contrast MRI Super-Resolution,2.2,Design of Loss Functions,"To effectively learn disentangled representations with more steady convergence in model training, a novel joint loss is designed in DisC-Diff as follows.Disentanglement Loss. L disent is defined as a ratio between L shared and L indep , where L shared measures the L 2 distance between shared representations, and L indep is the distance between independent representations:Charbonnier Loss. L charb is a smoother transition between L 1 and L 2 loss, facilitating more steady and accurate convergence during training [9]. It is less sensitive to the non-Gaussian noise in x t , y and v, and encourages sparsity results, preserving sharp edges and details in MRIs. It is defined as:where γ is a known constant. The total loss is the weighted sum of the above two losses:where λ 1 , λ 2 ∈ (0, 1] indicate the weights of the two losses."
DisC-Diff: Disentangled Conditional Diffusion Model for Multi-contrast MRI Super-Resolution,2.3,Curriculum Learning,"Our curriculum learning strategy improves the disentangled U-Net's performance on MRI data with varying anatomical complexity by gradually increasing the difficulty of training images, facilitating efficient learning of relevant features. All MRI slices are initially ranked based on the complexity estimated by Shannon-entropy values of their ground-truth HR-MRI, denoted as an ordered set E = {e min , . . . , e max }. Each iteration samples N images whose entropies follow a normal distribution with e min < μ < e max . As training progresses, μ gradually increases from e min to e max , indicating increased complexity of the sampled images. The above strategy is used for the initial M iterations, followed by uniform sampling of all slices."
DisC-Diff: Disentangled Conditional Diffusion Model for Multi-contrast MRI Super-Resolution,3,Experiments and Results,"Datasets and Baselines. We evaluated our model on the public IXI dataset1 and an in-house clinical brain MRI dataset. In both datasets, our setting is to utilize HR T1-weighted images HR T 1 and LR T2-weighted image LR T 2 created by k-space truncation [3] to restore 2× and 4× HR T2-weighted images, aligning with the setting in [5,22,27]. We split the 578 healthy brain MRIs in the IXI dataset into 500 for training, 6 for validation, and 70 for testing. We apply center cropping to convert each MRI into a new scan comprising 20 slices, each with a resolution 224 × 224. The processed IXI dataset is available for download at this link2 . The clinical dataset is fully sampled using a 3T Siemens Magnetom Skyra scanner on 316 glioma patients. The imaging protocol is as follows: TR T 1 = 2300 ms, TE T 1 = 2.98 ms, FOV T 1 = 256 × 240 mm 2 , TR T 2 = 4840 ms, TE T 2 = 114 ms, and FOV T 2 = 220 × 165 mm 2 . The clinical dataset is split patient-wise into train/validation/test sets with a ratio of 7:1:2, and each set is cropped into R 224×224×30 . Both datasets are normalized using the min-max method without prior data augmentations for training.We compare our method with three single-contrast SR methods (bicubic interpolation, EDSR [12], SwinIR [11]) and three multi-contrast SR methods (Guided Diffusion [4], MINet [5], MASA-SR [16]) (Table 1). Implementation Details. DisC-Diff was implemented using PyTorch with the following hyperparameters: λ 1 = λ 2 = 1.0, diffusion steps T = 1000, 96 channels in the first layer, 2 BigGAN Residual blocks, and attention module at 28×28, 14×14, and 7×7 resolutions. The model was trained for 200,000 iterations (M = 20,000) on two NVIDIA RTX A5000 24 GB GPUs using the AdamW optimizer with a learning rate of 10 -4 and a batch size of 8. Following the sampling strategy in [4], DisC-Diff learned the reverse diffusion process variances to generate HR-MRI in only 100 sampling steps. The baseline methods were retrained with their default hyperparameter settings. Guided Diffusion was modified to enable multicontrast SR by concatenating multi-contrast MRI as input.Quantitative Comparison. The results show that DisC-Diff outperforms other evaluated methods on both datasets at 2× and 4× enlargement scales. Specifically, on the IXI dataset with 4× scale, DisC-Diff achieves a PSNR increment of 1.44 dB and 0.82 dB and an SSIM increment of 0.0191 and 0.0134 compared to state-of-the-art single-contrast and multi-contrast SR methods [11,16]. The results show that without using disentangled U-Net as the backbone, Guided Diffusion performs much poorer than MINet and MASA-SR on the clinical dataset, indicating its limitation in recovering anatomical details of pathology-bearing brain. Our results suggest that disentangling multiple conditional contrasts could help DisC-Diff accurately control the HR image sampling process. Furthermore, the results indicate that integrating multi-contrast information inappropriately may damage the quality of super-resolved images, as evidenced by multi-contrast methods occasionally performs worse than singlecontrast methods, e.g., EDSR showing higher SSIM than MINet on both datasets at 2× enlargement.Visual Comparison and Uncertainty Estimation. Figure 3 shows the results and error maps for each method under IXI (2×) and Clinical (4×) settings, where less visible texture in the error map indicates better restoration. DisC-Diff outperforms all other methods, producing HR images with sharper edges and finer details, while exhibiting the least visible texture. Multi-contrast SR methods consistently generate higher-quality SR images than single-contrast SR methods, consistent with their higher PSNR and SSIM. Also, the lower variation between different restorations at the 2× scale compared to the 4× scale (Last column in Fig. 3) suggests higher confidence in the 2× restoration results.  "
DisC-Diff: Disentangled Conditional Diffusion Model for Multi-contrast MRI Super-Resolution,4,Conclusion,"We present DisC-Diff, a novel disentangled conditional diffusion model for robust multi-contrast MRI super-resolution. While the sampling nature of the diffusion model has the advantage of enabling uncertainty estimation, proper condition sampling is crucial to ensure model accuracy. Therefore, our method leverages a multi-conditional fusion strategy based on representation disentanglement, facilitating a precise and high-quality HR image sampling process. Also, we experimentally incorporate a Charbonnier loss to mitigate the challenge of MRI noise and outliers on model performance. Future efforts will focus on embedding DisC-Diff's diffusion processes into a compact, low-dimensional latent space to optimize memory and training. We plan to integrate advanced strategies (e.g., DPM-Solver++ [15]) for faster image generation and develop a unified model that generalizes across various scales, eliminating iterative training."
DisC-Diff: Disentangled Conditional Diffusion Model for Multi-contrast MRI Super-Resolution,,Fig. 1 .,
DisC-Diff: Disentangled Conditional Diffusion Model for Multi-contrast MRI Super-Resolution,,Fig. 2 .,
DisC-Diff: Disentangled Conditional Diffusion Model for Multi-contrast MRI Super-Resolution,,Fig. 3 .,
DisC-Diff: Disentangled Conditional Diffusion Model for Multi-contrast MRI Super-Resolution,,Table 1 .,
DisC-Diff: Disentangled Conditional Diffusion Model for Multi-contrast MRI Super-Resolution,,Table 2 .,
Global k-Space Interpolation for Dynamic MRI Reconstruction Using Masked Image Modeling,1,Introduction,"CINE Cardiac Magnetic Resonance (CMR) imaging is widely recognized as the gold standard for evaluating cardiac morphology and function [19]. Raw data for CMR is acquired in the frequency domain (k-space). MR reconstruction from k-space data with high spatio-temporal resolutions throughout the cardiac cycle is an essential step for CMR. Short scan times, ideally within a single breathhold, are preferable to minimize patient discomfort and reduce potential image artifacts caused by patient motion. Typically, due to the restricted scanning times, only a limited amount of k-space data can be obtained for each temporal frame. Note that while some k-space data are unsampled, the sampled ones are reliable sources of information. However, the Fourier transform of undersampled k-space corrupts a broad region of pixels in the image domain with aliasing artifacts because of violating the Nyquist-Shannon sampling theorem. Previous works [9,15,21,26,30] have attempted to remove the image artifacts primarily by regularizing on the image domain using conventional/learning-based image priors. However, after the Fourier transform the artifacts may completely distort and/or obscure tissues of interest before the image-domain regularizers kick in, making these methods challenging to recover true tissue structures.On a different principle, k-space interpolation methods first attempt to estimate the full k-space leveraging redundancy in sampled frequency components before Fourier transform. Image domain methods rely on artifacts-specific image priors to denoise the corrupted pixels, making them susceptible to variability in artifact types arising from different undersampling factors. Unlike image domain methods, k-space-based methods have a consistent task of interpolating missing data from reliable sampled ones, even though the undersampling factor may vary. This makes k-space interpolation methods simple, robust and generic over multiple undersampling factors.In this work, we are interested in learning an entirely k-space-based interpolation for the Cartesian undersampled dynamic MR data. An accurate learnable k-space interpolator can be achieved via (1) a rich representation of the sampled k-space data, which can facilitate the exploitation of the limited available samples, and (2) global dependency modeling of k-space to interpolate unsampled data from the learned representation. Modeling global dependencies are beneficial because a local structure in the image domain is represented by a wide range of frequency components in k-space. Furthermore, in the context of dynamic MR, the interpolator also has to exploit temporal redundancies.In the recent past, masked image modeling [11,34] has emerged as a promising method for learning rich generalizable representation by reconstructing the whole image from a masked (undersampled) input. Masked Autoencoders (MAE) [11] are one such model that leverages the global dependencies of the undersampled input using Transformers and learns masked-based rich feature representation. Despite sharing the same reconstruction principle, MAE has not been explored in k-space interpolation of Cartesian undersampled data. In this work, we cast 2D+t k-space interpolation as a masked signal reconstruction problem and propose a novel Transformer-based method entirely in k-space. Further, we intro-duce a refinement module on k-space to boost the accuracy of high-frequency interpolation. Our contributions can be summarized as follows:1. We propose a novel k-space Global Interpolation Network, termed k-GIN, leveraging masked image modeling for the first time in k-space. To the best of our knowledge, our work enables the first Transformer-based k-space interpolation for 2D+t MR reconstruction. 2. Next, we propose k-space Iterative Refinement Module, termed k-IRM that refines k-GIN interpolation by efficiently gathering spatio-temporal redundancy of the MR data. Crucially, k-IRM specializes in learning high-frequency details with the aid of customized High-Dynamic-Range (HDR) loss. 3. We evaluate our approach on 92 in-house CMR subjects and compare it to model-based reconstruction baselines using image priors. Our experiments show that the proposed k-space interpolator outperforms baseline methods with superior qualitative and quantitative results. Importantly, our method demonstrates improved robustness and generalizability regarding varying undersampling factors than the model-based counterparts."
Global k-Space Interpolation for Dynamic MRI Reconstruction Using Masked Image Modeling,2,Related Work,"Reconstruction with image priors is broadly used with either image-only denoising [17,31,36] or with model-based approaches to incorporate the kspace consistency by solving an inverse problem. For the latter, the physicsbased model can be formulated as a low-rank and a sparse matrix decomposition in CMR [14,26], or a motion-compensated MR reconstruction problem [3,27,28], or data consistency terms with convolution-based [9,30] or Transformers-based [12,18] image regularizers.k-space-Domain Interpolation. Methods include works [8,22], which have introduced auto-calibration signals (ACS) in the multi-coil k-space center of Cartesian sampled data. RAKI [2,16] uses convolutional networks for optimizing imaging and scanner-specific protocols. Nevertheless, these methods have limited flexibility during the scanning process since they all require a fixed set of ACS data in k-space. [20,32] introduced k-space interpolation methods which do not require calibration signals. [10] proposed a k-space U-Net under residual learning setting. However, these methods heavily rely on local operators such as convolution and may overlook non-local redundancies. [7] uses Transformers applicable only on radial sampled kspace data. However, using masked image modeling with Transformers in k-space for dynamic MR imaging e.g. 2D+t CMR data has not been studied yet.Hybrid Approaches. Combine information from both k-space and imagedomain. KIKI-Net [6] employs an alternating optimization between the image domain and k-space. [25,33] use parallel architectures for k-space and imagedomain simultaneously. However, their ablation shows limited contribution coming from the k-space compared to the image domain, implying an underexploitation of the k-space. Concurrently, [37] use Transformers in k-space but their performance is heavily dependent on the image domain fine-tuning at the final stage. "
Global k-Space Interpolation for Dynamic MRI Reconstruction Using Masked Image Modeling,3,Method,"Fully sampled complex 2D+t dynamic MR k-space data can be expressed as y ∈ C XY T , X and Y are the height (k x ) and the width (k y ) of the k-space matrix, and T is the number of frames along time. In this work, we express k-space as 2 channels (real and imaginary) data y ∈ R 2XY T . For the MR acquisition, a binary Cartesian sampling mask M ∈ Z Y T |M ij ∈ {0, 1} is applied in the k y -t plane, i.e. all k-space values along the k x (readout direction) are sampled if the mask is 1, and remains unsampled if the mask is 0. Figure 1 shows a pictorial undersampled k-space data. Let us denote the collection of sampled k-space lines as y s and unsampled lines as y u . The dynamic MR reconstruction task is to estimate y u and reconstruct y using y s only. In this work, we propose a novel Transformer-based reconstruction framework consisting of 1) k-GIN to learn global representation and 2) k-IRM to achieve refined k-space interpolation with a focus on high-frequency components."
Global k-Space Interpolation for Dynamic MRI Reconstruction Using Masked Image Modeling,3.1,k-space Global Interpolation Network (k-GIN),"In our proposed approach, we work on the k y -t plane and consider k x as the channel dimension. Further, we propose each point in the k y -t plane to be an individual token. In total, we have Y T number of tokens, out of which Y T/R are sampled tokens for an undersampling factor of R. Our objective is to contextualize global dependencies among every sampled token. For that, we use a ViT/MAE [5,11] encoder E consisting of alternating blocks of multi-head self-attention and multi-layer-perceptrons. The encoder takes advantage of each token's position embedding to correctly attribute its location. Following ViT, we use LayerNorm and GELU activation. We obtain rich feature representation f E = E(y s ) of the sampled k-space from the encoder. Next, we want a preliminary estimate of the undersampled k-space data from the learned feature representation. To this end, we employ a decoder D of similar architecture as the encoder. We initialize all the unsampled tokens y u with a single learnable token shared among them. Subsequently, we add their corresponding position embedding to these unsampled tokens. During the decoding process, the unsampled tokens attend to the well-contextualized features f E and produce an estimate of the whole k-space ŷr = D ([f E , y u ]). Since our masking pattern includes more sampled data in low-frequency than high-frequency components, we observe k-GIN gradually learn from low-frequency to high-frequency. Note that the imbalance of magnitude in k-space results in more emphasis on low-frequency when 1 loss is applied between estimation and ground-truth. We leverage this property into the learning behavior of k-GIN and deliberately use 1 loss between ŷr and y, read as L 1 = ŷr -y 1 ."
Global k-Space Interpolation for Dynamic MRI Reconstruction Using Masked Image Modeling,3.2,k-space Iterative Refinement Module (k-IRM),"Using 1 loss in k-GIN makes it focus more on the low-frequency components learning but the high-frequency estimation is still sub-optimal. Inspired by the iterative refinement strategy [38] which is widely used to improve estimation performance, we propose to augment k-GIN's expressive power, especially in highfrequency components, with k-space Iterative Refinement Module. This consists of three Transformer blocks that operate on three orthogonal planes. All three blocks are identical in architecture. The first block operates on k y -t plane and treats k x as channel dimension. The second block operates on k x -t plane and considers k y as channels, while the final block operates on k x -k y plane with t as channel dimension. Note that for the final refinement block uses 4 × 4 size token while the previous two blocks consider each point as a single token. These configurations enable scalable exploration of the spatio-temporal redundancy present in the output of the k-GIN. We denote ŷ1 , ŷ2 and ŷ3 as the estimation after each block in the k-IRM. We apply the skip connection between each refinement block and iteratively minimize the residual error at each stage.Inspired by [13,24], we applied an approximated logarithm loss function called High-Dynamic Range (HDR) loss for all the stages of k-IRM. HDR loss handles the large magnitude difference in the k-space data and makes the network pay more attention to high-frequency learning. The HDR loss function is defined as:where s(•) is the stop-gradient operator preventing the network back-propagation of estimation in the denominator and controls the operational range of the logarithmic approximation."
Global k-Space Interpolation for Dynamic MRI Reconstruction Using Masked Image Modeling,3.3,Inference,"The inference is identical to the training till obtaining the refined output from the k-IRM. Then we replace k-space estimation at the sampled position with ground-truth k-space values, ensuring the data-consistency. Note that this step is not done during the training as it deteriorates learning k-space representation.Once full k-space has been estimated, we use Fourier transform to obtain the image reconstruction during the inference. Note that image reconstruction is not needed during the training since our framework is entirely based in k-space."
Global k-Space Interpolation for Dynamic MRI Reconstruction Using Masked Image Modeling,4,Data and Experiments,"Dataset. The training was performed on 81 subjects (a mix of patients and healthy subjects) of in-house acquired short-axis 2D CINE CMR, whereas testing was carried out on 11 subjects. Data were acquired with 30/34 multiple receiver coils and 2D balanced steady-state free precession sequence on a 1.5T MR (Siemens Aera with TE=1.06 ms, TR=2.12 ms, resolution=1.9×1.9mm 2 with 8mm slice thickness, 8 breath-holds of 15 s duration). The MR data were acquired with a matrix size of 192×156 with 25 temporal cardiac phases (40ms temporal resolution). Afterwards, these data were converted to single-coil MR imaging and k-space data using coil sensitivity maps, simulating a fully sampled single-coil acquisition. A stack of 12 slices along the long axis was collected, resulting in 415/86 image sequence (2D+t) for training/test.Implementation Details. We use an NVIDIA A6000 GPU to train our framework. The batch size was set to 1 with a one-cycle learning-rate scheduler (max. learning rate 0.0001). We use 8 layers, 8 heads and 512 embedding dimensions for all of our Transformer blocks. We train our network with joint 1 and HDR-loss with tuned to 0.5. Training and inference were carried out on retrospectively undersampled images with masks randomly generated by VISTA [1]. We train the network with R = 4 undersampled data while we test our method on an undersampled factor R = 4, 6 and 8 during the inference. We can use this inference strategy to test our model's generalizability and robustness to different undersampling factors in comparison to the following baseline methods.Baseline Methods and Metrics. We compare the proposed framework with three single-coil MR reconstruction methods that apply image priors: TVnorm Optimization (TV-Optim) which is widely used in reconstruction [21,23], L+S [26] and DcCNN [30]. TV-Optim reconstructs the image using TV-norm [29] as the image regularizer. L+S leverages compressed sensing techniques and addresses the reconstruction using low rankness and sparsity of the CMR as the image prior, whilst DcCNN employs 3D convolutional neural networks in the image domain together with data-consistency terms. We use the same training and inference strategy for DcCNN to test its model robustness. We Fourier transform our interpolated full k-space to obtain the image reconstruction and utilize Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index (SSIM) and Normalized Mean Squared Error (NMSE) to evaluate the reconstruction performance with the baseline quantitatively."
Global k-Space Interpolation for Dynamic MRI Reconstruction Using Masked Image Modeling,5,Results and Discussion,"The quantitative results in Table 1 show consistent superior performance of the proposed method across every single undersampling factor compared to all other baseline methods. Figure 3 shows a qualitative comparison for a typical test sample. It can be seen that the reconstruction methods with image priors can still provide comparable results at R = 4, however, suffer from a large performance drop when acceleration rates get higher, especially at R = 8. The non-trivial hyper-parameters tuning has to be carried out for L+S and TV-Optim to adapt to the specific image prior at different acceleration factors. It is also noteworthy that the proposed method and DcCNN are both trained only on R = 4 undersampled CMR. DcCNN demonstrates inferior reconstruction for R = 8 since there is a mismatch in artifact characteristics between R = 4 and R = 8. On the contrary, the task of interpolating k-space for R = 4 and R = 8 remains the same, i.e., to estimate missing data from sampled data. We efficiently leverage rich contextualized representation of k-GIN to interpolate full k-space even when a lesser number of sampled k-space data are given as input than seen during training. The observation confirms the superior robustness and generalizability of our proposed framework. Next, we conduct an ablation study to validate our architectural design. We carry out experiments to investigate the impact of applying k-IRM. We conduct the interpolation using 1) only k-GIN, 2) k-GIN + k y -t plane refinement, 3) k-GIN + k x -t plane refinement, 4) k-GIN + k x -k y plane refinement and 5) k-GIN with all three refinement blocks. Table 2 in supplementary presents quantitative comparisons amongst five configurations as above. We observe k-IRM offers the best performance when all 3 refinement blocks are used together. In the second ablation, Table 3 in supplementary shows the usefulness of applying 1 in k-GIN and HDR in k-IRM. HDR makes k-GIN's learning inefficient since HDR deviates from its learning principle of ""first low-frequency then high-frequency"". On the other hand, 1 + 1 combination hinders high-frequency learning.Outlook. Previous works [6,25] have speculated limited usefulness coming from k-space in a hybrid setting. However, our work presents strong evidence of kspace representation power which can be leveraged in future work with hybrid reconstruction setup. Furthermore, one can utilize our work as a pre-training task since the image reconstruction itself is an ""intermediate step"" for downstream tasks e.g. cardiac segmentation and disease classification. In the future, one can reuse the learned encoder representation of k-GIN to directly solve downstream tasks without requiring image reconstruction.Limitation. We also acknowledge some limitations of the work. First, we have not evaluated our method on prospectively collected data, which would be our focus in future work. Second, the current study only investigates the single coil setup due to hardware memory limitations. In the future, we will address the multi-coil scenario by applying more memory-efficient Transformers backbones e.g. [4,35]."
Global k-Space Interpolation for Dynamic MRI Reconstruction Using Masked Image Modeling,6,Conclusion,"In this work, we proposed a novel Transformer-based method with mask image modeling to solve the dynamic CMR reconstruction by only interpolating the kspace without any image-domain priors. Our framework leverages Transformers' global dependencies to exploit redundancies in all three k x -, k y -and t-domain. Additionally, we proposed a novel refinement module (k-IRM) to boost highfrequency learning in k-space. Together, k-GIN and k-IRM not only produce high-quality k-space interpolation and superior CMR reconstruction but also generalize significantly better than baselines for higher undersampling factors."
Global k-Space Interpolation for Dynamic MRI Reconstruction Using Masked Image Modeling,,Fig. 1 .,
Global k-Space Interpolation for Dynamic MRI Reconstruction Using Masked Image Modeling,,Fig. 2 .,
Global k-Space Interpolation for Dynamic MRI Reconstruction Using Masked Image Modeling,,Fig. 3 .,
Global k-Space Interpolation for Dynamic MRI Reconstruction Using Masked Image Modeling,,Table 1 .,
Global k-Space Interpolation for Dynamic MRI Reconstruction Using Masked Image Modeling,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5_22.
SAMConvex: Fast Discrete Optimization for CT Registration Using Self-supervised Anatomical Embedding and Correlation Pyramid,1,Introduction,"Deformable image registration [21], a fundamental medical image analysis task, has traditionally been approached as a continuous optimization [1,2,9,19,24] problem over the space of dense displacement fields between image pairs. The iterative process always leads to inefficiency. Recent learning-based approaches that use a deep network to predict a displacement field [3,6,[16][17][18]25,30], yield much faster runtime and have gained huge attention. However, they often struggle with versatile applicability due to the fact that they require training per registration task. Moreover, gathering enough data for the training is not a trivial task in practice. In addition, both optimization and learning-based methods rely on similarity measures computed over the intensity, which prevents the methods to utilize the anatomy correspondence. Several works [8,10] use feature descriptors that provide modality and contrast invariant information but they still can only represent local information and do not contain the global semantic information. Thus, they face challenges in settings with large deformations or complex anatomical differences (e.g., inter-patient Abdomen).To address this issue, we incorporate a Self-supervised Anatomical eMbedding (SAM) [27] into registration. SAM generates a unique semantic embedding for each voxel in CT that describes its anatomical structure, thus, providing semantically coherent information suitable for registration. SAME [15] enhances learning-based registration with SAM embeddings, but it suffers the applicability issue as the other learning-based methods even when the SAM embedding is pre-trained and ready to use out of the box for multiple anatomical regions.Registration has also been formulated as a discrete optimization problem [5,7,20,22,26] that employs a dense set of discrete displacements, called cost volume. The main challenge of this category of approach is the massive size of the search space, as millions of voxels exist in a typical 3D CT scan and each voxel in the moving scan can be reasonably paired with thousands of points in the other scan, leading to a high computational burden. To obtain fast registration with discrete optimization, Heinrich et al. [11] prunes the search space by constructing the cost volume only within the neighborhood of each voxel. However, the magnitude range of the deformation it can solve is limited by the size of the neighborhood window, leading to reliance on an accurate pre-alignment.We propose SAMConvex, a coarse-to-fine discrete optimization method for CT registration. Specifically, SAMConvex presents two main components: (1) a discriminative feature extractor that encodes global and local embeddings for each voxel; (2) a lightweight correlation pyramid that constructs multi-scale 6D cost volume by taking the inner product of SAM embeddings. It enjoys the strengths of state-of-the-art accuracy (Sect. 3.2), good generalization (Sect. 3.2 and Sect. 3.3) and fast runtime (Sect. 3.2). "
SAMConvex: Fast Discrete Optimization for CT Registration Using Self-supervised Anatomical Embedding and Correlation Pyramid,2,SAMConvex Method,
SAMConvex: Fast Discrete Optimization for CT Registration Using Self-supervised Anatomical Embedding and Correlation Pyramid,2.1,Problem Formulation,"Given a source image I s : Ω s → R and a target image I t : Ω t → R within a spatial domain Ω ⊂ R n , our goal is to find the spatial transformation ϕ -1 : Ω t → Ω s . We aim at minimizing the following energy:where the transformation model is the displacement vector field (DVF) ϕ -1 = Id + u with Id being the identity transformation, the E D is the similarity term measuring the similarity between I t and warped imageis the diffusion regularizer that advocates the regularity of ϕ -1 . λ weights between the data matching term and the regularization term."
SAMConvex: Fast Discrete Optimization for CT Registration Using Self-supervised Anatomical Embedding and Correlation Pyramid,2.2,Decoupling to Convex Optimizations,"We conduct the optimization scheme proposed in [4,11,23,29] and we give a brief review here. By introducing an extra term into Eq. 1the optimization then can be decomposed into two sub-optimization problems(with each can be solved via global optimizations. By alternatively optimizing the two subproblems and progressively reducing θ during the alternative optimization, we get v ≈ û and obtain the solution of Eq. 1 as ϕ -1 = Id + v. To be noted, the first problem in Eq. 3 can be solved point-wise because the spatial derivatives of v is not involved. We can search over the cost volume of each point to obtain the optimal solution of the first problem in Eq. 3. For the second problem, we are inspired by mean-field inference [14] and process u using average pooling."
SAMConvex: Fast Discrete Optimization for CT Registration Using Self-supervised Anatomical Embedding and Correlation Pyramid,2.3,"E D (•, •) Using Global and Local Semantic Embedding","Presumably, handling complex registration tasks rely on: (1) distinct features that are robust to inter-subject variation, organ deformation, contrast injection, and pathological changes, and (2) global/contextual information that can benefit the registration accuracy in complex deformation. To achieve these goals, we adopt self-supervised anatomical embedding (SAM) [27] based feature descriptor that encodes both global and local embeddings. The global embeddings memorize the 3D contextual information of body parts on a coarse level, while the local embeddings differentiate adjacent structures with similar appearances, as shown on the left of Fig. 1. The former helps the latter to focus on small regions to distinguish with fine-level features.To be specific, given an image I ∈ R H×W ×D , we utilize a pre-trained SAM model that outputs a global embeddingWe resample the global embedding with bilinear interpolation to match the shape of f global to f local and concatenate them to getWe normalize f global and f local before concatenation and adopt the dot product < •, • > as the similarity measure in the following part, with a higher value indicating better alignment.With f SAM , we construct E D (•, •) as the cost volume in the SAM feature space within a neighborhood of [-N, N ], where N represents searching radius. Given two images I 0 and I 1 , the resulting E D (•, •) can then be formulated aswhere f 0 SAM and f 1 SAM are the SAM embedding of I 0 and I 1 , respectively. x is any voxel in the image domain and d is the voxel displacement within the neighborhood."
SAMConvex: Fast Discrete Optimization for CT Registration Using Self-supervised Anatomical Embedding and Correlation Pyramid,2.4,Coarse-to-Fine Optimization Strategy,"Since E D (•, •) is computed within the neighborhood, the magnitude of deformation is bounded by the size of the neighborhood. Our approach to addressing this issue is based on a surprisingly simple but effective observation: performing registration based on a coarse-to-fine scheme with a small search range at each level instead of a large search range at one resolution benefits to 1) significantly improve the efficiency such as low computation burden and fast running time;2) enlarge receptive field and improve registration accuracy.We first build an image pyramid of I s and I t . At each resolution, we warp the source image with the composed deformation computed from all the previous levels (starting from the coarsest resolution), transform the warped image and target image to the SAM space, and conduct the decoupling to convex optimizations strategy to obtain the deformation between the warped image and target at the current resolution. With such a coarse-to-fine strategy, we estimate a sequence of displacement fields from a starting identity transformation. The final field is computed via the composition of all the displacement fields [30].To be noted, the cost volume at each level is computed by taking the inputs of {1, 1  2 , 1 4 } resolution. Adding one coarser level consumes less computation than doubling the size of the neighborhood at the fine resolution but yields the same search range."
SAMConvex: Fast Discrete Optimization for CT Registration Using Self-supervised Anatomical Embedding and Correlation Pyramid,2.5,Implementation Detail,"We conduct the registration in 3 levels of resolutions. At each level, we solve Eq. 3 via five alternative optimizations with 1 2θ = {0.003, 0.01, 0.03, 0.1, 0.3, 1}. To further improve the registration performance, we append a SAM-based instance optimization after the coarse-to-fine registration with a learning rate of 0.05 for 50 iterations. It solves an instance-specific optimization problem [3] in the SAM feature space. The optimization objective consists of similarity (dot product between SAM feature vectors on the highest resolution) and diffusion regularization terms. All the experiments are run on the CPU of Intel Xeon Platinum 8163 with 16 cores of 2.50 GHz and the GPU of NVIDIA Tesla V100. Code will be available at https://github.com/Alison-brie/SAMConvex."
SAMConvex: Fast Discrete Optimization for CT Registration Using Self-supervised Anatomical Embedding and Correlation Pyramid,3,Experiments,
SAMConvex: Fast Discrete Optimization for CT Registration Using Self-supervised Anatomical Embedding and Correlation Pyramid,3.1,Datasets and Evaluation Metrics,"We split Abdomen and HeadNeck into a training set and test set to accommodate the requirement of the training dataset of comparing learning-based methods. Lung dataset contains 35 CT pairs, which is not sufficient for developing learningbased methods. Hence, it is only used as a testing set for optimization-based methods. All the methods are evaluated on the test set. The SAM is pre-trained on NIH Lymph Nodes dataset [27]. All 3 datasets in this paper are not used for pre-training."
SAMConvex: Fast Discrete Optimization for CT Registration Using Self-supervised Anatomical Embedding and Correlation Pyramid,,Inter-patient Task on Abdomen:,"The Abdomen CT dataset [12] contains 30 abdominal scans with 20 for training and 10 for testing. Each image has 13 manually labeled anatomical structures: spleen, right kidney, left kidney, gall bladder, esophagus, liver, stomach, aorta, inferior vena cava, portal and splenic vein, pancreas, left adrenal gland, and right adrenal gland. The images are resampled to the same voxel resolution of 2 mm and spatial dimensions of 192 × 160 × 256."
SAMConvex: Fast Discrete Optimization for CT Registration Using Self-supervised Anatomical Embedding and Correlation Pyramid,,Inter-patient Task on HeadNeck:,"The HeadNeck CT dataset [28] contains 72 subjects with 13 organs labeled. The manually labeled anatomical structures include the brainstem, left eye, right eye, left lens, right lens, optic chiasm, left optic nerve, right optic nerve, left parotid, right parotid, left and right temporomandibular joint, and spinal cord. We split the dataset into 52, 10, and 10 for training, validation, and test set. For testing, we construct 90 image pairs for registration, with each image, resampled to an isotropic resolution of 2 mm and cropped to 256 × 128 × 224."
SAMConvex: Fast Discrete Optimization for CT Registration Using Self-supervised Anatomical Embedding and Correlation Pyramid,,Intra-patient Task on Lung:,"The images are acquired as part of the radiotherapy planning process for the treatment of malignancies. We collect a lung 4D CT dataset containing 35 patients, each with inspiratory and expiratory breath-hold image pairs, and take this dataset as an extra test set. Each image has labeled malignancies, and we try to align two phases for motion estimation of malignancies. The images are resampled to a spacing of 2 × 2 × 2 mm and cropped to 256 × 256 × 112. All images are used as testing cases. Evaluation Metrics: We use the average Dice score (DSC) to evaluate the accuracy and compute the standard deviation of the logarithm of the Jacobian determinant (SDlogJ) to evaluate the plausibility of deformation fields, also comparing running time (T test ) on the same hardware."
SAMConvex: Fast Discrete Optimization for CT Registration Using Self-supervised Anatomical Embedding and Correlation Pyramid,3.2,Registration Results,"We compare with five widely-used and top-performing deformable registration methods, including NiftyReg [24], Deeds [9], top-ranked ConvexAdam [20] in Learn2Reg Challenge [12], and SOTA learning-based methods LapIRN [18] and SAME [15]. All results are based on the SAM-affine [15] pre-alignment.As shown in Table 1, SAMConvex outperforms the widely-used NiftyReg [24] across the three datasets with an average of 10.0% Dice score improvement. Compared with the best traditional optimization-based method Deeds [9], SAM-Convex performs better or comparatively on the three datasets with approximately 20-100 times faster runtime. We also see a better performance of SAMConvex over ConvexAdam [20]. To be noted, the performance gap between SAMConvex and ConvexAdam is greater on Abdomen CT than the other two datasets. This may be because the deformation is more complex in Abdomen and the anatomical differences between inter-subjects make the registration more challenging. With a coarse-to-fine strategy, SAMConvex can better bear these issues. Although LapIRN [18] has the fastest inference time, it has slightly inferior registration accuracy as compared to SAMConvex. Moreover, it needs training when being applied to a new dataset. Equipped with SAM, SAME achieves the overall 2nd best performance in two inter-patient registration tasks but with a notably higher folding rate overall. Moreover, it also requires individual training for a new dataset and struggles with the intra-patient lung registration task (small dataset and no available training data). In summary, our SAMConvex achieves the overall best performance as compared to other methods with a fast inference time and comparable deformation field smoothness.To better understand the performance of different deformable registration methods, we display organ-specific results of the inter-patient registration task of abdomen CT in Fig. 2 where large deformations and complex anatomical differences exist. As shown, SAMConvex is consistently better than the secondbest and third-best registration methods on most examined abdominal organs, in 11 out of 13 organs."
SAMConvex: Fast Discrete Optimization for CT Registration Using Self-supervised Anatomical Embedding and Correlation Pyramid,3.3,Ablation Study on SAMConvex,"Loss Landscape of SAM. We explore the loss landscapes of SAM-global, SAMlocal, SAM (SAM-global and SAM-local), and MIND via varying the transformation parameters. We conduct experiments on the abdomen dataset2 . Figure 3 shows the comparison of landscapes when we vary the rotation along two axes. The loss landscape falls to flat quickly when the rotation is greater than 20  Robustness to Pre-alignment. We study the robustness of SAMConvex over different affine pre-alignment. Elastix-Affine [13] and SAM-Affine are used as the pre-alignment and results are notated as Initial e and Initial a in Table 2, respectively. Compared to ConvexAdam, our method SAMConvex is less affected by the performance of the Affine pre-alignment. This is in line with our expectations. The global information contained in SAM provides E D (I s • ϕ, I t ) a greater capture range than features that contain local information only. Thus, SAM-Convex can register images with larger transformation, leading to less reliance on the performance of the pre-alignment.Ablation on Coarse-to-Fine. We study how the number of coarse-to-fine layers and how the size of the neighborhood window affect the registration result on the Abdomen dataset. From Table 3, we can conclude that performing registration with a small search range with a coarse-to-fine scheme instead of a cost volume with a large search range can help to improve registration accuracy and computation efficiency.  Discussion About the Differences with ConvexAdam [20]. Apart from the SAM extraction module, first, we introduce a cost volume pyramid to the convex optimization framework to reduce the intensive computation burdens. To be specific, with the coarse-to-fine strategy, one can sparsely search on a coarser level with a smaller search radius in each iteration, reaching the same search range as the complete search with less computational complexity. Second, we explicitly validate the robustness of the SAM feature against geometrical transformations and integrate the SAM feature into an instance-specific optimization pipeline. Our SAMConvex is less sensitive to local minimal, achieving superior performance with comparable running time. Ablation study on pyramid designs (Table 3) and leading accuracy on large deformation registration tasks (Table 1) further support our claims."
SAMConvex: Fast Discrete Optimization for CT Registration Using Self-supervised Anatomical Embedding and Correlation Pyramid,4,Conclusion,"We present SAMConvex, a coarse-to-fine discrete optimization method for CT registration. It extracts per-voxel semantic global and local features and builds a series of lightweight 6D correlation volumes, and iteratively updates a flow field by performing lookups on the correlation volumes. The performance on two interpatient and one intra-patient registration datasets demonstrates state-of-the-art accuracy, good generalization, and high computation efficiency of SAMConvex."
SAMConvex: Fast Discrete Optimization for CT Registration Using Self-supervised Anatomical Embedding and Correlation Pyramid,,Fig. 1 .,
SAMConvex: Fast Discrete Optimization for CT Registration Using Self-supervised Anatomical Embedding and Correlation Pyramid,,Fig. 2 .,
SAMConvex: Fast Discrete Optimization for CT Registration Using Self-supervised Anatomical Embedding and Correlation Pyramid,,,
SAMConvex: Fast Discrete Optimization for CT Registration Using Self-supervised Anatomical Embedding and Correlation Pyramid,,Fig. 3 .,
SAMConvex: Fast Discrete Optimization for CT Registration Using Self-supervised Anatomical Embedding and Correlation Pyramid,,Table 1 .,DSC13 ↑ SDlogJ ↓ Ttest ↓ DSC13 ↑ SDlogJ ↓ Ttest ↓ DSC1 ↑ SDlogJ ↓ Ttest ↓
SAMConvex: Fast Discrete Optimization for CT Registration Using Self-supervised Anatomical Embedding and Correlation Pyramid,,Table 2 .,
SAMConvex: Fast Discrete Optimization for CT Registration Using Self-supervised Anatomical Embedding and Correlation Pyramid,,Table 3 .,
SAMConvex: Fast Discrete Optimization for CT Registration Using Self-supervised Anatomical Embedding and Correlation Pyramid,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 53.
Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,1,Introduction,"Radiotherapy (RT) is one of the cornerstones of cancer patients. It utilizes ionizing radiation to eradicate all cells of a tumor. The total radiation dose is typically divided over 3-30 daily fractions to optimize its effect. As the surrounding normal tissue is also sensitive to radiation, highly accurate delivery is vital. Image guided RT (IGRT) is a technique to capture the anatomy of the day using in room imaging in order to align the treatment beam with the tumor location [1]. Cone Beam CT (CBCT) is the most widely used imaging modality for IGRT.A major challenge especially for CBCT imaging of the thorax and upperabdomen is the respiratory motion that introduces blurring of the anatomy, reducing the localization accuracy and the sharpness of the image.A technique used to alleviate motion artifacts is Respiratory Correlated CBCT (4DCBCT) [16]. From the projections, it is possible to extract a respiratory signal [12], which indicates the position of the organs within the patient during breathing. With this, subsets of the projections can be defined to create reconstructions that resolve the motion. However, only 20 to 60 respiratory periods are imaged. This limits the number of projections available and results in view-aliasing [16]. Additionally, the projections are affected by stochastic measurement noise caused by the finite imaging dose used, which further degrades the quality of the reconstruction even when all projections are used.Several traditional methods based on iterative reconstruction algorithms and motion compensation techniques are used to reduce view-aliasing in 4DCBCTs [7,10,11,14,15]. Although effective, these methods suffer from motion modeling uncertainty and prolonged reconstruction times.Deep learning has been proposed as a way to address view-aliasing with accelerated reconstruction [6]. However, the method cannot reduce measurement noise because it is still present in the images used as targets during training.A different method, called Noise2Inverse, uses an unsupervised approach to reduce measurement noise in the traditional CT setting [4]. There are two ways to apply it to 4DCBCT and both fail to reduce stochastic noise effectively. The first is to apply Noise2Inverse to each respiratory-correlated reconstruction. In this case, the method will struggle because of the very low number of projections that are available. The second is to apply Noise2Inverse directly to all the projections. In this case, the motion artifacts that blur the image will appear again, as Noise2Inverse requires averaging the sub-reconstructions to obtain a clean reconstruction.We propose Noise2Aliasing to address these limitations. The method can be used to provably train models to reduce both view-aliasing artifacts and stochastic noise from 4DCBCTs in an unsupervised way. Training deep learning models for medical applications often needs new data. This was not the case for Noise2Aliasing, and historical clinical data sufficed for training.We validated our method on publicly available data [15] against a supervised approach [6] and applied it to an internal clinical dataset of 30 lung cancer patients. We explore different dataset sizes to understand their effects on the reconstructed images."
Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,2,Theoretical Background,"In this section, we will introduce the concepts and the notation necessary to understand the method and the choices made during implementation.Unsupervised noise reduction with Noise2Noise. Given input-target pairs x, y ∈ R we can define the regression problem in the one-dimensional setting as finding f * : R → R which satisfies the following:which can be minimized point-wise [3], yielding:In Noise2Noise [5], input-target pairs are two samples of the same image that only differ because of some independent mean-zero noise (x + δ 1 , x + δ 2 ) withThen f * will recover the input image without any noise:Denoising for Tomography with Noise2Inverse. During a CT scan, a volume x is imaged by acquiring projections y = Ax using an x-ray source and a detector placed on the opposite side of the volume. The projections can then be used by an algorithm that computes a linear operator R to obtain an approximation of the original distribution of x-ray attenuation coefficients x = Ry. The algorithm can also operate on a subset of the projections. Let J = {1, 2, . . . } be the set of all projections and J ⊂ J , then xJ = R J y J is the reconstruction obtained using only projections y J . Let us now assume that the projections have some meanzero noise ỹi = y i + with E (ỹ i ) = y i . Then, in Noise2Inverse [4] the results from Noise2Noise are extended to find a function f * which removes projection noise when trained using noisy reconstructions xJ = R J ỹJ = R J y J + R J = xJ + R J and the expected MSE as loss function. In particular, they find that the loss function can be decomposed in the following way:where J is a random variable that picks subsets of projections at random and J is its complementary. Given Eq. 2, we observe that function f * which minimizes L is:When using reconstructions from a subset of noisy projections as input and reconstructions from their complementary as its output, a neural network will learn to predict the expected reconstruction without the noise.Property of Expectation over Subsets of Projections Using FDK. Now let J be a random variable that selects subsets of projections J ⊂ J at random such that each projection is selected at least once. Define R J : R D d ×|J| → R Dv to be the FDK reconstruction algorithm [2] that reconstructs a volume of dimensionality D v from projections J each with dimensionality D d (geometrical details on the exact setup are not relevant). The FDK uses, as its fundamental step, the dual Radon transform [9], which is a weighted summation that can be written as an expectation. Then, the following holds:"
Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,3,Noise2Aliasing,"Here, we propose Noise2Aliasing, an unsupervised method capable of reducing both view-aliasing and projection noise in 4DCBCTs. At the core of this method is the following proposition.Proposition. Given the projection set J = {1, 2, . . . }, the FDK reconstruction algorithm R, and the noisy projections ỹ = Ax+ with mean-zero element-wise independent noise. Let J 1 , J 2 be two random variables that pick different subsets at random belonging to a partition of J , andbe the input-target pairs in dataset D of reconstructions using disjoint subsets of noisy projections. Let L be the expected MSE over D with respect to a function f : R Dv → R Dv and the previously-described input-target pairs. Then, we find that the function f * that minimizes L for any given J ∈ J will reconstruct the volume using all the projections and remove the noise :Proof. The loss function L is defined in the following way:Additionally, J 1 , J 2 are disjoint, the noise is mean-zero element-wise, and we are using the FDK reconstruction algorithm which defines a linear operator R. These allow us to use Eq. 5 to find that the function f * that minimizes L is the following:This is sufficient to reduce stochastic noise but we need to further manipulate this expression to address view aliasing. Simplifying notation and using the properties of conditional expectations, we can write:now assume that xj1 is the clean reconstruction that is consistent with the observed noisy reconstruction z obtained from each disjoint subset j 2 , then:Finally, we use the property of the FDK from Eq. 6:"
Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,3.1,Design Choices Based on the Proposition,"The proposition guided the choice of reconstruction method to be FDK and the design of the subset selection method from considerations that are now explained. Equation 12 holds true only when the same underlying clean reconstruction x can be determined from the noisy reconstruction using any subset from a partition of the projections J . This means that, in our dataset, we should have at our disposal reconstructions of the same underlying volume x using disjoint subsets of projections. In 4DCBCTs this is not the case, as separate respiratory phases are being reconstructed, where the organs are in different positions. We can address this problem by carefully choosing subsets of projections that result in respiratory-uncorrelated reconstructions. The reconstructions will display organs in their average position and, therefore, have the same underlying structure. When the projections are selected with the same sampling pattern as the one used in respiratory-correlated reconstructions, then the view-aliasing artifacts display will have the same pattern as the ones present in the 4DCBCTs.Compared to previous work, to obtain the additional effect of reducing projection noise, the respiratory-uncorrelated reconstructions must use non-overlapping subsets of projections. Coincidentally, a previously proposed subset selection method utilized for supervised aliasing reduction fits all these requirements and will, therefore, be used in this work [4]."
Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,4,Experiments,"First, we used the SPARE Varian dataset to study whether Noise2Aliasing can match the performance of the supervised baseline and if it can outperform it when adding noise to the projections. Then, we use the internal dataset to explore the requirements for the method to be applied to an existing clinical dataset. These required around 64 GPU days on NVIDIA A100 GPUs.Training of the model is done on 2D slices. The projections obtained during a scan are sub-sampled according to the pseudo-average subset selection method described in [6] and then used to obtain 3D reconstructions. In Noise2Aliasing these are used for both input and target during training. Given two volumes (x, y), the training pairs (x i (k) , y i (k) ) are the same i-th slice along the k-th dimension of each volume chosen to be the axial plane.The Datasets used in this study are two:1. The SPARE Varian dataset was used to provide performance results on publicly available patient data. To more closely resemble normal respiratory motion per projection image, the 8 min scan has been used from each patient (five such scans are available in the dataset). Training is performed over 4 patients while 1 patient is used as a test set. The hyperparameters are optimized over the training dataset.2. An internal dataset (IRB approved) of 30 lung cancer patients' 4DCBCTs from 2020 to 2022, originally used for IGRT, with 25 patients for training and 5 patients for testing. The scans are 4 min 205 • scans with 120keV source and 512 × 512 sized detector, using Elekta LINACs. The data were anonymized prior to analysis.Projection Noise was added using the Poisson distribution to the SPARE Varian dataset to evaluate the ability of the unsupervised method to reduce it. Given a projected value of p and a photon count π (chosen to be 2500), the rate of the Poisson distribution is defined as πe -p and given a sample q from this distribution, then the new projected value is p =log q π .The Architecture used in this work is the Mixed Scale Dense CNN (MSD) [8], the most successful architecture from Noise2Inverse [4]. The MSD makes use of dilated convolutions to process features at all scales of the image. We use the MSD with depth 200 and width 1, Adam optimizer, MSE loss, a batch size of 16, and a learning rate of 0.0001.The Baselines we compare against are two. The first is the traditional FDK obtained using RTK [13]. The second is the supervised approach proposed by [6], where we replace the model with the MSD, for a fair comparison. In the supervised approach, the model is trained by using as input reconstructions obtained from subsets defined with pseudo-average subset selection while the targets use all of the projections available.The Metrics used in this work are the Root Mean Squared Error (RMSE), Peak Signal-to-Noise Ratio (PSNR), and Structural Similarity Index Measure (SSIM) [17] All the metrics are defined between the output of the neural network and a 3D (CB)CT scan. For the SPARE Varian dataset, we use the ROIs defined provided [15] and used the 3D reconstruction using all the projections available as a ground truth. For the internal dataset, we deformed the planning CT to each of the phases reconstructed using the FDK algorithm and evaluate the metric over only the 4DCBCT volume boundaries."
Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,5,Results and Discussion,"SPARE Varian. Inference speed with the NVIDIA A100 GPU averages 600ms per volume made of 220 slices. From the qualitative evaluation of the methods in Fig. 1, Noise2Aliasing matches the visual quality of the supervised approach on the low-noise dataset on both soft tissue and bones. The metrics in Table 1 show mean and standard deviation across all phases for a single patient. In the lownoise setting, both supervised and Noise2Aliasing outperform FDK with very similar results, often within a single standard deviation. Noise2Aliasing successfully matches the performance of the supervised baseline.  Noisy SPARE Varian. From Fig. 1 and Table 1, the supervised approach reproduces the noise that was seen during training, while Noise2Aliasing manages to remove it consistently, outperforming the supervised approach, especially in the soft tissue area around the lungs, where the noise affects attenuation coefficients the most.Noise2Aliasing is capable of reducing the artifacts present in reconstructions caused by stochastic noise in the projections used, outperforming the supervised baseline.Internal Dataset. Noise2Alisting trained on 25 patients and tested on 5 achieved mean PSNR of 35.24 and SSIM of 0.91, while the clinical method achieved mean PSNR of 29.97 and 0.74 SSIM with p-value of 0.048 for the PSNR and 0.0015 for the SSIM, so Noise2Aliasing was significantly better according to both metrics. Additionally, from Fig. 3 we can see how the breathing extent is matched with sharp reconstruction of the diaphragm. Overall, using more patients results in better noise reduction and sharper reconstructions (see Fig. 2), Fig. 2. Reconstruction using Noise2Aliasing with different-sized datasets. With fewer patients, the model is more conservative and tends to keep more noise, but also smudges the interface between tissues and bones. With more patients, more of the view-aliasing is addressed, and the reconstruction is sharper, however, a few small anatomical structures tend to be suppressed by the model.especially between fat tissue and skin and around the bones. However, the model also tends to remove small anatomical structures as high-frequency objects that cannot be distinguished from the noise.When applied to a clinical dataset, Noise2Aliasing benefits from more patients being included in the dataset, however, qualitatively good performance is already achieved with 5 patients. No additional data collection was required and the method can be applied without major changes to the current clinical practice. "
Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,6,Conclusion,"We have presented Noise2Aliasing, a method to provably remove both viewaliasing and stochastic projection noise from 4DCBCTs using an unsupervised deep learning method. We have empirically demonstrated its performance on a publicly available dataset and on an internal clinical dataset. Noise2Aliasing outperforms a supervised approach when stochastic noise is present in the projections and matches its performance on a popular benchmark. Noise2Aliasing can be trained on existing historical datasets and does not require changing current clinical practices. The method removes noise more reliably when the dataset size is increased, however further analysis is required to establish a good quantitative measurement of this phenomenon. As future work, we plan to study Noise2Aliasing in the presence of changes in the breathing frequency and amplitude between patients and during a scan."
Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,,Fig. 1 .,
Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,,Fig. 3 .,
Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,,Table 1 .,
Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 46.
An Explainable Deep Framework: Towards Task-Specific Fusion for Multi-to-One MRI Synthesis,1,Introduction,"Magnetic resonance imaging (MRI) consists of a series of pulse sequences, e.g. T1-weighted (T1), contrast-enhanced (T1Gd), T2-weighted (T2), and T2-fluidattenuated inversion recovery (Flair), each showing various contrast of water and fat tissues. The intensity contrast combination of multi-sequence MRI provides clinicians with different characteristics of tissues, extensively used in disease diagnosis [16], lesion segmentation [17], treatment prognosis [7], etc. However, some acquired sequences are unusable or missing in clinical settings due to incorrect machine settings, imaging artifacts, high scanning costs, time constraints, contrast agents allergies, and different acquisition protocols between hospitals [5]. Without rescanning or affecting the downstream pipelines, the MRI synthesis technique can generate missing sequences by leveraging redundant shared information between multiple sequences [18].Many studies have demonstrated the potential of deep learning methods for image-to-image synthesis in the field of both nature images [8,11,12] and medical images [2,13,19]. Most of these works introduce an autoencoder-like architecture for image-to-image translation and employ adversarial loss to generate more realistic images. Unlike these one-to-one approaches, MRI synthesis faces the challenge of fusing complementary information from multiple input sequences. Recent studies about multi-sequence fusion can specifically be divided into two groups: (1) image fusion and (2) feature fusion. The image fusion approach is to concatenate sequences as a multi-channel input. Sharma et al. [18] design a network with multi-channel input and output, which combines all the available sequences and reconstructs the complete sequences at once. Li et al. [14] add an availability condition branch to guide the model to adapt features for different input combinations. Dalmaz et al. [9] equip the synthesis model with residual transformer blocks to learn contextual features. Image-level fusion is simple and efficient but unstable -zero-padding inputs for missing sequences lead to training unstable and slight misalignment between images can easily cause artifacts. In contrast, efforts have been made on feature fusion, which can alleviate the discrepancy across multiple sequences, as high-level features focus on the semantic regions and are less affected by input misalignment compared to images. Zhou et al. [23] design operation-based (e.g. summation, product, maximization) fusion blocks to densely combine the hierarchical features. And Li et al. [15] employ self-attention modules to integrate multi-level features. The model architectures of these methods are not flexible and difficult to adapt to various sequence combinations. More importantly, recent studies only focus on proposing end-to-end models, lacking quantifying the contributions for different sequences and estimating the qualities of generated images.In this work, we propose an explainable task-specific fusion sequence-tosequence (TSF-Seq2Seq) network, which has adaptive weights for specific synthesis tasks with different input combinations and targets. Specially, this framework can be easily extended to other tasks, such as segmentation. Our primary contributions are as follows: (1) We propose a flexible network to synthesize the target MRI sequence from an arbitrary combination of inputs; (2) The network shows interpretability for fusion by quantifying the contribution of each input sequence; (3) The network provides reliability for synthesis by highlighting the area the network tried to refine."
An Explainable Deep Framework: Towards Task-Specific Fusion for Multi-to-One MRI Synthesis,2,Methods,"Figure 1 illustrates the overview of the proposed TSF-Seq2Seq network. Our network has an autoencoder-like architecture including an encoder E, a multisequence fusion module, and a decoder G. Available MRI sequences are first encoded to features by E, respectively. Then features from multiple input sequences are fused by giving the task-specific code, which identifies sources and targets with a binary code. Finally, the fused features are decoded to the target sequence by G. Furthermore, to explain the mechanism of multi-sequence fusion, our network can quantify the contributions of different input sequences with the task-specific weighted average module and visualize the TSEM with the task-specific attention module.To leverage shared information between sequences, we use E and G from Seq2Seq [10], which is a one-to-one synthetic model that integrates arbitrary sequence synthesis into single E and G. They can reduce the distance between different sequences at the feature level to help more stable fusion. Details of the multi-sequence fusion module and TSEM are described in the following sections."
An Explainable Deep Framework: Towards Task-Specific Fusion for Multi-to-One MRI Synthesis,2.1,Multi-sequence Fusion,"Define a set of N sequences MRI: X = {X i |i = 1, ..., N } and corresponding available indicator A ⊂ {1, ..., N } and A = ∅. Our goal is to predict the target set∈ A} by giving the available set X A = {X i |i ∈ A} and the corresponding task-specific code c = {c src , c tgt } ∈ Z 2N . As shown in Fig. 1, c src and c tgt are zero-one codes for the source and the target set, respectively. To fuse multiple sequences at the feature level, we first encode images and concatenate the features as f = {E(X i )|i = 1, ..., N }. Specifically, we use zero-filled placeholders with the same shape as E(X i ) to replace features of i / ∈ A to handle arbitrary input sequence combinations. The multi-sequence fusion module includes: (1) a task-specific weighted average module for the linear combination of available features; (2) a task-specific attention module to refine the fused features.Task-Specific Weighted Average. The weighted average is an intuitive fusion strategy that can quantify the contribution of different sequences directly. To learn the weight automatically, we use a trainable fully connected (FC) layer to predict the initial weight ω 0 ∈ R N from c.where W and b are weights and bias for the FC layer, = 10 -5 to avoid dividing 0 in the following equation. To eliminate distractions and accelerate training, we force the weights of missing sequences in ω 0 to be 0 and guarantee the outputwhere • refers to the element-wise product and •, • indicates the inner product.With the weights ω, we can fuse multi-sequence features as f by the linear combination.Specially, f ≡ E(X i ) when only one sequence i is available, i.e. A = {i}. It demonstrates that the designed ω can help the network excellently inherit the synthesis performance of pre-trained E and G. In this work, we use ω to quantify the contribution of different input combinations.Task-Specific Attention. Apart from the sequence-level fusion of f , a taskspecific attention module G A is introduced to refine the fused features at the pixel level. The weights of G A can adapt to the specific fusion task with the given target code. To build a conditional attention module, we replace convolutional layers in convolutional block attention module (CBAM) [20] with Hyper-Conv [10]. HyperConv is a dynamic filter whose kernel is mapped from a shared weight bank, and the mapping function is generated by the given target code.As shown in Fig. 1, channel attention and spatial attention can provide adaptive feature refinement guided by the task-specific code c to generate residual attentional fused features f A .Loss Function. To force both f and f + f A can be reconstructed to the target sequence by the conditional G, a supervised reconstruction loss is given as,where• 1 refers to a L 1 loss, and L p indicates the perceptual loss based on pre-trained VGG19. λ r and λ p are weight terms and are experimentally set to be 10 and 0.01."
An Explainable Deep Framework: Towards Task-Specific Fusion for Multi-to-One MRI Synthesis,2.2,Task-Specific Enhanced Map,"As f A is a task-specific contextual refinement for fused features, analyzing it can help us understand more what the network tried to do. Many studies focus on visualizing the attention maps to interpret the principle of the network, especially for the transformer modules [1,6]. However, visualization of the attention map is limited by its low resolution and rough boundary. Thus, we proposed the TSEM by subtracting the reconstructed target sequences with and without f A , which has the same resolution as the original images and clear interpretation for specific tasks. TSEM3 Experiments"
An Explainable Deep Framework: Towards Task-Specific Fusion for Multi-to-One MRI Synthesis,3.1,Dataset and Evaluation Metrics,"We use brain MRI images of 1,251 subjects from Brain Tumor Segmentation 2021 (BraTS2021) [3,4,17], which includes four aligned sequences, T1, T1Gd, T2, and Flair, for each subject. We select 830 subjects for training, 93 for validation, and 328 for testing. All the images are intensity normalized to [-1, 1] and central cropped to 128 × 192 × 192. During training, for each subject, a random number of sequences are selected as inputs and the rest as targets. For validation and testing, we fixed the input combinations and the target for each subject. The synthesis performance is quantified using the metrics of peak signal noise rate (PSNR), structural similarity index measure (SSIM), and learned perceptual image patch similarity (LPIPS) [21], which evaluate from intensity, structure, and perceptual aspects."
An Explainable Deep Framework: Towards Task-Specific Fusion for Multi-to-One MRI Synthesis,3.2,Implementation Details,"The models are implemented with PyTorch and trained on the NVIDIA GeForce RTX 3090 Ti GPU. E comprises three convolutional layers and six residual blocks. The initial convolutional layer is responsible for encoding intensities to features, while the second and third convolutional layers downsample images by a factor of four. The residual blocks then extract the high-level representation. The 28.5 ± 2.5 0.883 ± 0.040 9.65 ± 3.57 DiamondGAN [14] 28.2 ± 2.5 0.877 ± 0.041 10.20 ± 3.33 ResViT [9] 28.3 ± 2.4 0.882 ± 0.039 9.87 ± 3.30 Seq2Seq [10] (Average) 28.5 ± 2.3 0.880 ± 0.038 11.61 ± 3.87 TSF-Seq2Seq (w/o fA) 28.3 ± 2.6 0.876 ± 0.044 9.61 ± 4.00 TSF-Seq2Seq 28.8 ± 2.6 0.887 ± 0.042 8.89 ± 3.80 channels are 64, 128, 256, and 256, respectively. G has an inverse architecture with E, and all the convolutional layers are replaced with HyperConv. The E and G from Seq2Seq are pre-trained using the Adam optimizer with an initial learning rate of 2 × 10 -4 and a batch size of 1 for 1,000,000 steps, taking about 60 h. Then we finetune the TSF-Seq2Seq with the frozen E using the Adam optimizer with an initial learning rate of 10 -4 and a batch size of 1 for another 300,000 steps, taking about 40 h."
An Explainable Deep Framework: Towards Task-Specific Fusion for Multi-to-One MRI Synthesis,3.3,Quantitative Results,"We compare our method with one-to-one translation, image-level fusion, and feature-level fusion methods. One-to-one translation methods include Pix2Pix [12] and Seq2Seq [10]. Image-level fusion methods consist of MM-GAN [18], DiamondGAN [14], and ResViT [9]. Feature-level fusion methods include Hi-Net [23] and MMgSN-Net [15]. Figure 2 shows the examples of synthetic T2 of comparison methods input with the combinations of T1Gd and Flair. Table 1 reports the sequence synthesis performance for comparison methods organized by the different numbers of input combinations. Note that, for multiple inputs, one-to-one translation methods synthesize multiple outputs separately and average them as one. And Hi-Net [23] and MMgSN-Net [15] only test on the subset with two inputs due to fixed network architectures. As shown in Table 1, the proposed method achieves the best performance in different input combinations."
An Explainable Deep Framework: Towards Task-Specific Fusion for Multi-to-One MRI Synthesis,3.4,Ablation Study,"We compare two components of our method, including (1) task-specific weighted average and (2) task-specific attention, by conducting an ablation study between Seq2Seq, TSF-Seq2Seq (w/o f A ), and TSF-Seq2Seq. TSF-Seq2Seq (w/o f A ) refers to the model removing the task-specific attention module. As shown in Table 1, when only one sequence is available, our method can inherit the performance of Seq2Seq and achieve slight improvements. For multi-input situations, the task-specific weighted average can decrease LPIPS to achieve better perceptual performance. And task-specific attention can refine the fused features to achieve the best synthesis results."
An Explainable Deep Framework: Towards Task-Specific Fusion for Multi-to-One MRI Synthesis,3.5,Interpretability Visualization,"The proposed method not only achieves superior synthesis performance but also has good interpretability. In this section, we will visualize the contribution of different input combinations and TSEM.  Sequence Contribution. We use ω in Eq. 2 to quantify the contribution of different input combinations for synthesizing different target sequences. Figure 3 shows the bar chart for the sequence contribution weight ω with different taskspecific code c. As shown in Fig. 3, both T1 and T1Gd contribute greatly to the sequence synthesis of each other, which is expected because T1Gd are T1weighted scanning after contrast agent injection, and the enhancement between these two sequences is indispensable for cancer detection and diagnosis. The less contribution of T2, when combined with T1 and/or T1Gd, is consistent with the clinical findings [22,23] that T2 can be well-synthesized by T1 and/or T1Gd.TSEM vs. Attention Map. Figure 4 shows the proposed TSEM and the attention maps extracted by ResViT [9]. As shown in Fig. 4, TSEM has a higher resolution than the attention maps and can highlight the tumor area which is hard to be synthesized by the networks. Table 2 reports the results of PSNR for regions highlighted or not highlighted by TSEM with a threshold of the 99th percentile. To assist the synthesis models deploying in clinical settings, TSEM can be used as an attention and uncertainty map to remind clinicians of the possible unreliable synthesized area. "
An Explainable Deep Framework: Towards Task-Specific Fusion for Multi-to-One MRI Synthesis,4,Conclusion,"In this work, we introduce an explainable network for multi-to-one synthesis with extensive experiments and interpretability visualization. Experimental results based on BraTS2021 demonstrate the superiority of our approach compared with the state-of-the-art methods. And we will explore the proposed method in assisting downstream applications for multi-sequence analysis in future works."
An Explainable Deep Framework: Towards Task-Specific Fusion for Multi-to-One MRI Synthesis,,Fig. 1 .,
An Explainable Deep Framework: Towards Task-Specific Fusion for Multi-to-One MRI Synthesis,,Fig. 2 .,
An Explainable Deep Framework: Towards Task-Specific Fusion for Multi-to-One MRI Synthesis,,Fig. 3 .,
An Explainable Deep Framework: Towards Task-Specific Fusion for Multi-to-One MRI Synthesis,,Fig. 4 .,
An Explainable Deep Framework: Towards Task-Specific Fusion for Multi-to-One MRI Synthesis,,Table 1 .,
An Explainable Deep Framework: Towards Task-Specific Fusion for Multi-to-One MRI Synthesis,,Table 2 .,Number of inputs TSEM > 99% TSEM < 99% Total
An Explainable Deep Framework: Towards Task-Specific Fusion for Multi-to-One MRI Synthesis,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 5.
Topology-Preserving Computed Tomography Super-Resolution Based on Dual-Stream Diffusion Model,1,Introduction,"Computed tomography (CT) is a prevalent imaging modality with applications in biology, disease diagnosis, interventional imaging, and other areas. Highresolution CT (HRCT) is beneficial for clinical diagnosis and surgical planning because it can provide detailed spatial information and specific features, usually employed in advanced clinical routines [1]. HRCT usually requires high-precision CT machines to scan for a long time with high radiation doses to capture the internal structures, which is expensive and can impose the risk of radiation exposure [2]. These factors make HRCT relatively less available, especially in towns and villages, compared to low-resolution CT (LRCT). However, degradation in spatial resolution and imaging quality brought by LRCT can interfere with the original physiological and pathological information, adversely affecting the diagnosis [3]. Consequently, how to produce high-resolution CT scans at a smaller radiation dose level with lower scanning costs is a holy grail of the medical imaging field (Fig. 1). With the advancement of artificial intelligence, super-resolution (SR) techniques based on neural networks indicate new approaches to this problem. By inferring detailed high-frequency features from LRCT, super-resolution can introduce additional knowledge and restore lost information due to lowresolution scanning. Deep-learning (DL) based methods, compared to traditional methods, can incorporate hierarchical features and representations from prior knowledge, resulting in improved results in SR tasks [4]. According to different neural-network frameworks, these SR methods can be broadly categorized into two classes: 1) convolutional neural network (CNN) based model [5][6][7], and 2) generative adversarial network (GAN) based model [2,8,9]. Very recently, the diffusion model is emerging as the most promising deep generative model [11], which usually consists of two stages: a forward stage to add noises and a reverse stage to separate noises and recover the original images. The diffusion model shows impressive generative capabilities for many tasks, including image generation, inpainting, translation, and super-resolution [10,12,13].While DL-based methods can generate promising results, there can still be geometric distortions and artifacts along with structural edges in the superresolved results [15,16]. These structural features always represent essential physiological structures, including vasculature, fibrosis, tumor, and other lesions. The distortion and infidelity of these features can lead to potential misjudgment for diagnosis, which is unacceptable for clinical application. Moreover, the target image size and spatial resolution of HRCT for most existing SR methods is about 512 × 512 and 0.8 × 0.8 mm 2 . With the progress in hardware settings, ultra-high-resolution CT (UHRCT) with an image size of 1024 × 1024 and spatial resolution of 0.3 × 0.3 mm 2 can be available very recently [17]. Though UHRCT can provide much more detailed information, to our best knowledge, SR tasks targeting UHRCT have rarely been discussed and reported.In this paper, we propose a novel dual-stream conditional diffusion model for CT scan super-resolution to generate UHRCT results with high image quality and structure fidelity. The conditional diffusion model takes the form p(y|x), where x is the LRCT, and y is the targeted UHRCT [14]. The novel diffusion model incorporates a dual-stream structure-preserving network and a novel imaging enhancement operator in the denoising process. The imaging enhancement operator can simultaneously extract the vascular and blob structures in the CT scans and provide structure prior to the dual-stream network. The dualstream network can fully exploit the prior information with two branches. One branch optimizes the SR results in the image domain, and the other branch optimizes the results in the structure domain. In practice, we use a convolution-based lightweight module to simulate the filtering operations, which enables faster and easier back-propagation in the training process. Furthermore, we constructed a new ultra-high resolution CT scan dataset obtained with the most advanced CT machines. The dataset contained 87 UHRCT scans with a spatial resolution of 0.34×0.34 mm 2 and an image size of 1024×1024. Extensive experiments, including qualitative and quantitative comparisons in both image consistency, structure fidelity, and high-level tasks, demonstrated the superiority of our method. Our contributions can be summarized as follows:1) We proposed a novel dual-stream diffusion model framework for CT superresolution. The framework incorporates a dual-stream structure-preserving network in the denoising process to realize better physiological structure restoration. 2) We designed a new image enhancement operator to model the vascular and blob structures in medical images. To avoid non-derivative operations in image enhancement, we proposed a novel enhancement module consisting of lightweight convolutional layers to replace the filtering operation for faster and easier back-propagation in structural domain optimization.3) We established an ultra-high-resolution CT scan dataset with a spatial resolution of 0.34 × 0.34 mm 2 and an image size of 1024 × 1024 for training and testing the SR task. 4) We have conducted extensive experiments and demonstrated the excellent performance of the proposed SR methods in both the image and structure domains. In addition, we have evaluated our proposed method on high-level tasks, including vascular-system segmentation and lesion detection on the SRCT, indicating the reliability of our SR results. "
Topology-Preserving Computed Tomography Super-Resolution Based on Dual-Stream Diffusion Model,2,Method,
Topology-Preserving Computed Tomography Super-Resolution Based on Dual-Stream Diffusion Model,2.1,Dual-Stream Diffusion Model for Structure-Preserving Super-Resolution,"To preserve the structure and topology relationship in the denoising progress, we designed a novel Dual-Stream Diffusion Model (DSDM) for better superresolution and topology restoration (Fig. 2). In the DSDM framework, given a HRCT slice y, we generate a noisy version ỹ, and train the network G DSSP to denoise ỹ with the corresponding LRCT slice x and a noise level indicator γ.The optimization is defined asIn the denoising process, we used a dual-stream structure-preserving (DSSP) network for supervised structure restoration. The DSSP network optimizes the denoised results in the image domain and the structure domain, respectively. The structural domain, obtained with the image enhancement operator, is concatenated with the LRCT slice as the input of the structure branch. The final SR results are obtained after the feature fusion model between the image map and the structure map."
Topology-Preserving Computed Tomography Super-Resolution Based on Dual-Stream Diffusion Model,2.2,Imaging Enhancement Operator for Vascular and Blob Structure,"We introduced an enhancement operator in the DSSP network to model the vascular and blob structures, which can represent important physiological information according to clinical experience, and provide the prior structural information for the DSSP network. For one pixelT in the CT slice, let I (x) denote the imaging intensity at this point. The 2 × 2 Hessian matrix at the scale s is defined as [18] where G (x, s) is the 2D Gaussian kernel. The two eigenvalues of the Hessian matrix are denoted as λ = (λ 1 , λ 2 ) and here we agree that |λ 1 | <= |λ 2 |. The eigenvalues of the Hessian matrix can reflect the geometric shape, curvature, and brightness of the local images. For the blob-like structures, the three eigenvalues are about the same, λ 1 ≈ λ 2 ; for the vascular-like structures, λ 2 can be much larger than the absolute value of λ 1 , |λ 2 | >> |λ 1 | [19]. The eigenvalue relations at scale s can be indicated by several different functions. Here we proposed a novel structure kernel function, which is defined asκ 1 and κ 2 are the parameters to control the sensitivity for the vascular-like structures and blob-like structures, respectively. λ τ is the self-regulating factor.When λ 1 is about the same with λ 2 , λ τ is closed to λ 1 ; and when λ 1 is much smaller to λ 2 , λ τ is closed to λ 2 , which can achieve a balance between two conditions."
Topology-Preserving Computed Tomography Super-Resolution Based on Dual-Stream Diffusion Model,2.3,Objective Function,"We designed a new loss function to ensure that the final SR result can be optimized in both the image domain and the structure domain. Denoting the reference image as y t , the pixel-wise loss in the imaging domain is formulated asG image DSSP (y t ) is the recovered image from the image-domain branch. L1 loss yields a significantly higher consistency and lower diversity, while L2 loss can better capture the outliers. Here we used a parameter λ L1 to balance these two losses.In the meantime, a structure-constraint loss is also necessary to help the network achieve better performance in structure consistency. The loss function consists of two parts, which measure the consistency of the image-domain branch and the structure-domain branch. Denoting the structure-domain output as G struct DSSP (y t ), the structure-constraint loss can be presented asHowever, the image enhancement described above involves overly complex calculations, making back-propagation difficult in the training process. Here we utilized a convolution-based operator O Fc (•) to simplify the calculation, which consists of several lightweight convolutional layers to simulate the operation of image enhancement. In this way, we transform the complex filtering operation into a simple convolution operation, thus back-propagation can be easily processed. The loss function is then modified asThe total objective function is the sum of two losses.3 Experiments and Conclusion"
Topology-Preserving Computed Tomography Super-Resolution Based on Dual-Stream Diffusion Model,3.1,Datasets and Evaluation,"We constructed three datasets for framework training and evaluation; two of them were in-house data collected from two CT scanners(the ethics number is 20220359), and the other was the public Luna16 dataset [22]. More details about the two in-house datasets are described in the Supplementary Materials.We evaluated our SR model on three CT datasets:• Dataset 1: 2D super-resolution from 256×256 to 1024×1024, with the spatial resolution from 1.36 × 1.36 mm 2 to 0.34 × 0.34 mm 2 .• Dataset 2: 3D super-resolution from 256 × 256 × 1X to 512 × 512 × 5X, with the spatial resolution from 1.60 × 1.60 × 5.00 mm 3 to 0.80 × 0.80 × 1.00 mm 3 . • Dataset 3: 2D super-resolution from 256 × 256 to 512 × 512 on the Luna16 dataset.We compare our model with other SOTA super-resolution methods, including bicubic interpolation, SRCNN [7], SRResNet [6], Cycle-GAN [2], and SR3 [12]. Performance is assessed qualitatively and quantitatively, using PSNR, SSIM [23], Visual Information Fidelity (VIF) [24], and Structure Mean Square Error (SMSE). VIF value is correlated well with the human perception of SR images, which can measure diagnostic acceptance and information maintenance. SMSE is proposed to evaluate the structure difference between the ground truth and SRCT. Specially, we obtained the structural features of the ground truth and SRCT with Frangi filtering and then calculated the pixel-wise difference [20].  "
Topology-Preserving Computed Tomography Super-Resolution Based on Dual-Stream Diffusion Model,3.2,Qualitative and Quantitative Results,"Qualitative comparisons are shown in Fig. 3 and the quantitative results are shown in Table 1. The super-resolution results with our proposed methods achieve the highest scores in both image restoration and structure consistency for most indices, and there are no obvious secondary artifacts introduced in the SR results. Although the GAN-based methods and SR3 can produce sharp details, they tend to generate artifacts for the vascular systems, which is more evident in the structure-enhanced figures. The problem of inconsistent structure is also reflected in the value of VIF and SMSE on both GAN-based methods and SR3.Lesion Detection and Vessel Segmentation on Super-Resolved CT. To further evaluate the information maintenance of our SR methods, we conducted some high-level tasks, including lung nodules detection and pulmonary airway and blood vessel segmentation on the super-resolved CT scans. We compared the performance of different methods on SRCT and the ground truth. For nodule detection, these methods included U-Net, V-Net [25], ResNet [26], DCNN [27] and 3D-DCNN [28]. For the vessel segmentation, these methods included 3D U-Net, V-Net [25], nnUNet [31], Nardelli et al. [30] and Qin et al. [29]. Figure 4 shows the quantitative results of the performance comparison. The performance of these high-level tasks on the SR results is comparable to or even better than that on the ground-truth CT. Such results, to some extent, demonstrated that our SR method does not introduce artifacts or structural inconsistencies and cause misjudgment, while the improved spatial resolution and image quality generated by our proposed results shows great potential in improving the performance of high-level tasks.  "
Topology-Preserving Computed Tomography Super-Resolution Based on Dual-Stream Diffusion Model,4,Conclusion,"In this paper, we have established a dual-stream diffusion model framework to address the problem of topology distortion and artifact introduction that generally exists in the medical super-resolution results. We first propose a novel image enhancement operator to model the vessel and blob structures in the CT slice, which can provide a structure prior to the SR framework. Then, we design a dualstream diffusion model that employs a dual-stream ream structure-preserving network in the denoising process. The final SR outputs are optimized not only by convolutional image-space losses but also by the proposed structure-space losses. Extensive experiments have shown that our SR methods can achieve high performance in both image restoration and structure fidelity, demonstrating the promising performance of information preservation and the potential of applying our SR results to downstream tasks."
Topology-Preserving Computed Tomography Super-Resolution Based on Dual-Stream Diffusion Model,,Fig. 1 .,
Topology-Preserving Computed Tomography Super-Resolution Based on Dual-Stream Diffusion Model,,Fig. 2 .,
Topology-Preserving Computed Tomography Super-Resolution Based on Dual-Stream Diffusion Model,,2,
Topology-Preserving Computed Tomography Super-Resolution Based on Dual-Stream Diffusion Model,,Fig. 3 .,
Topology-Preserving Computed Tomography Super-Resolution Based on Dual-Stream Diffusion Model,,Fig. 4 .,
Topology-Preserving Computed Tomography Super-Resolution Based on Dual-Stream Diffusion Model,,Table 1 .,Cycle-GAN[2] 37.32 0.993 0.901 0.462 32.31 0.918 0.881 0.822 37.82 0.921 0.915 0.282 SR3[12] 37.18 0.974 0.812 0.474 36.85 0.957 0.916 0.859 39.57 0.968 0.902 0.274 Our Proposed 40.75 0.992 0.
Topology-Preserving Computed Tomography Super-Resolution Based on Dual-Stream Diffusion Model,,977 0.162 38.76 0.979 0.967 0.274 38.91 0.977 0.974 0.162,
Topology-Preserving Computed Tomography Super-Resolution Based on Dual-Stream Diffusion Model,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 25.
Accurate Multi-contrast MRI Super-Resolution via a Dual Cross-Attention Transformer Network,1,Introduction,"Magnetic Resonance Imaging (MRI) has revolutionized medical diagnosis by providing a non-invasive imaging tool with multiple contrast options [1,2]. However, generating high-resolution MRI images can pose difficulties due to hardware limitations and lengthy scanning times [3,4]. To tackle this challenge, super-resolution techniques have been developed to improve the spatial resolution of MRI images [5]. However, while several neural network-based super-resolution methods (e.g., EDSR [6], SwinIR [7], and ELAN [8]) have emerged from the computer vision field, they primarily utilize single-contrast data, ignoring the valuable complementary multi-contrast information that is easily accessible in MRI.Recent studies have shown that multi-contrast data routinely acquired in MRI examinations can be used to develop more powerful super-resolution methods tailored for MRI by using fully sampled images of one contrast as a reference (Ref) to guide the recovery of high-resolution (HR) images of another contrast from low-resolution (LR) inputs [9]. In this direction, MINet [10] and SANet [11] have been proposed and demonstrated superior performance over previous single-image super-resolution approaches. However, these methods rely on relatively simple techniques, such as channel concatenation or spatial addition between LR and Ref images, or using channel concatenation followed by self-attention to identify similar textures between LR and Ref images. These approaches may overlook the complex relationship between LR and Ref images and lead to inaccurate super-resolution.Recent advances in super-resolution techniques have led to the development of hard-attention-based texture transfer methods (such as TTSR [12], MASA [13], and McMRSR [14]) using the texture transformer architecture [12]. However, these methods may still underuse the rich information in multi-contrast MRI data. As illustrated in Fig. 1(a), these methods focus on spatial attention and only seek the most relevant patch for each query. They also repetitively use low-resolution attention maps from down-sampled Ref images (Ref ↓↑ ), which may not be sufficient to capture the complex relationship between LR and Ref images, potentially resulting in suboptimal feature transfer. These limitations can be especially problematic for noisy low-field MRI data, where down-sampling the Ref images (as the key in the transformer) can cause additional image blurring and information loss.As shown in Fig. 1(b), our proposed approach is inspired by the transformerbased cross-attention approach [15], which provides a spatial cross-attention mechanism using full-powered transformer architecture without Ref image downsampling, as well as the UNETR++ architecture [16], which incorporates channel attention particularly suitable for multi-contrast MRI images that are anatomically aligned. Building upon these developments, the proposed Dual Cross-Attention Multi-contrast Super Resolution (DCAMSR) method can flexibly search the reference images for shareable information with multi-scale attention maps and well capture the information both locally and globally via spatial and channel attention. Our contributions are summarized as follows: 1) We present a novel MRI super-resolution framework different from existing hardattention-based methods, leading to efficient learning of shareable multi-contrast information for more accurate MRI super-resolution. 2) We introduce a dual cross-attention transformer to jointly explore spatial and channel information, substantially improving the feature extraction and fusion processes. 3) Our proposed method robustly outperforms the current state-of-the-art single-image as well as multi-contrast MRI super-resolution methods, as demonstrated by extensive experiments on the high-field fastMRI [17] and more challenging low-field M4Raw [18] MRI datasets. "
Accurate Multi-contrast MRI Super-Resolution via a Dual Cross-Attention Transformer Network,2,Methodology,"Overall Architecture. Our goal is to develop a neural network that can restore an HR image from an LR image and a Ref image. Our approach consists of several modules, including an encoder, a dual cross-attention transformer (DCAT) and a decoder, as shown in Fig. 2. Firstly, the LR is interpolated to match the resolution of HR. Secondly, we use the encoder to extract multi-scale features from both the up-sampled LR and Ref, resulting in features F LR and F Ref . Thirdly, the DCAT, which contains of dual cross-attention (DCA), Layer Normalization (LN) and feed-forward network (FFN), is used to search for texture features from F LR and F Ref . Fourthly, the texture features are aggregated with F LR through the Fusion module at each scale. Finally, a simple convolution is employed to generate SR from the fused feature.Encoder. To extract features from the up-sampled LR, we employ an encoder consisting of four stages. The first stage uses the combination of a depth-wise convolution and a residual block. In stages 2-4, we utilize a down-sampling layer and a residual block to extract multi-scale features. In this way, the multiscale features for the LR ↑ are extracted as  The core of DCAT is dual cross-attention mechanism, which is diagrammed in Fig. 3. Firstly, we project F LR and F Ref to q, k and v. For the two crossattention branches, the linear layer weights for q and k are shared, while those for v are different:where q share ,k share ,v spatial and v channel are the parameter weights for shared queries, shared keys, spatial value layer, and channel value layer, respectively. In spatial cross-attention, we further project k share and v spatial to k project and v project through linear layers, to reduce the computational complexity. The spatial and channel attentions are calculated as: Finally, X spatial and X channel are reduced to half channel via 1 × 1 convolutions, and then concatenate to obtain the final feature:For the whole DCAT, the normalized features LN (F LR ) and LN (F Ref ) are fed to the DCA and added back to F LR . The obtained feature is then processed by the FFN in a residual manner to generate the texture feature. Specifically, the DCAT is summarized as:Feeding the multi-scale features of LR ↑ and Ref to DCAT, we can generate the texture features in multi-scales, denoted as T exture H×W , T extureDecoder. In the decoder, we start from the feature F is then up-sampled and feed to Fusion along with T extureis up-sampled and feed to Fusion along with T exture H×W , generating F used H×W . Finally, F used H×W is processed with a 1 × 1 convolution to generate SR.In the Fusion module, following [13], the texture feature T exture and input feature F LR are first fed to Spatial Adaptation Module (SAM), a learnable structure ensuring the distributions of T exture consistent with F LR , as shown in Fig. 2(d). The corrected texture feature is then concatenated with the input feature F LR and further incorporated via a convolution and a residual block, as shown in Fig. 2(c).Loss Function. For simplicity and without loss of generality, L 1 loss between the restored SR and ground-truth is employed as the overall reconstruction loss."
Accurate Multi-contrast MRI Super-Resolution via a Dual Cross-Attention Transformer Network,3,Experiments,"Datasets and Baselines. We evaluated our approach on two datasets: 1) fastMRI, one of the largest open-access MRI datasets. Following the settings of SANet [10,11], 227 and 24 pairs of PD and FS-PDWI volumes are selected for training and validation, respectively. 2) M4Raw, a publicly available dataset including multi-channel k-space and single-repetition images from 183 participants, where each individual haves multiple volumes for T1-weighted, T2weighted and FLAIR contrasts [18]. 128 individuals/6912 slices are selected for training and 30 individuals/1620 slices are reserved for validation. Specifically, T1-weighted images are used as reference images to guide T2-weighted images. To generate the LR images, we first converted the original image to k-space and cropped the central low-frequency region. For down-sampling factors of 2× and 4×, we kept the central 25% and 6.25% values in k-space, respectively, and then transformed them back into the image domain using an inverse Fourier transform. The proposed method is compared with SwinIR [7], ELAN [8], SANet (the journal version of MINet) [11], TTSR [12], and MASA [13].Implementation Details. All the experiments were conducted using Adam optimizer for 50 epochs with a batch size of 4 on 8 Nvidia P40 GPUs. The initial learning rate for SANet was set to 4 × 10 -5 according to [11], and 2 × 10 -4 for the other methods. The learning rate was decayed by a factor of 0.1 for the last 10 epochs. The performance was evaluated for enlargement factors of 2× and 4× in terms of PNSR and SSIM.Quantitative Results. The quantitative results are summarized in Table 1. The proposed method achieves the best performance across all datasets for both single image super-resolution (SISR) and multi-contrast super-resultion (MCSR). Specifically, our LR-guided DCAMSR version surpasses state-of-theart methods such as ELAN and SwinIR in SISR, and even outperforms SANet (a MCSR method). Among the MCSR methods, neither SANet, TTSR or MASA achieves better results than the proposed method. In particular, the PSNR for MASA is even 0.18 dB lower than our SISR version of DCAMSR at 4× enlargement on M4Raw dataset. We attribute this performance margin to the difficulty of texture transformers in extracting similar texture features between Ref and Ref ↓↑ . Despite the increased difficulty of super-resolution at 4× enlargement, our model still outperforms other methods, demonstrating the powerful texture transfer ability of the proposed DCA mechanism.Qualitative Evaluation. Visual comparison is shown in Fig. 4, where the upsampled LR, the ground-truth HR, the restored SR and the error map for each method are visualized for 4× enlargement on both datasets. The error map depicts the degree of restoration error, where the more prominent texture indicating the poorer restoration quality. As can be seen, the proposed method produces the least errors compared with other methods.Ablation Study. We conducted ablation experiments on the M4Raw dataset and the results are shown in Table 2. Three variations are tested: w/o reference,  where LR ↑ is used as the reference instead of Ref ; w/o multi-scale attention, where only the lowest-scale attention is employed and interpolated to other scales; and w/o channel attention, where only spatial attention is calculated. The improvement from w/o reference to DCAMSR demonstrates the effectiveness of MCSR compared with SISR. The performance degradation of w/o multiscale attention demonstrates that the lowest-scale attention is not robust. The improvement from w/o channel attention to DCAMSR shows the effectiveness of the channel attention. Moreover, our encoder and decoder have comparable parameter size to MASA but we achieved higher scores, as shown in Table 1, demonstrating that the spatial search ability of DCAMSR is superior to the original texture transformer.Discussion. Our reported results on M4Raw contain instances of slight interscan motion [18], demonstrating certain resilience of our approach to image misalignment, but more robust solutions deserve further studies. Future work may also extend our approach to 3D data."
Accurate Multi-contrast MRI Super-Resolution via a Dual Cross-Attention Transformer Network,4,Conclusion,"In this study, we propose a Dual Cross-Attention Multi-contrast Super Resolution (DCAMSR) framework for improving the spatial resolution of MRI images. As demonstrated by extensive experiments, the proposed method outperforms existing state-of-the-art techniques under various conditions, proving a powerful and flexible solution that can benefit a wide range of medical applications."
Accurate Multi-contrast MRI Super-Resolution via a Dual Cross-Attention Transformer Network,,Fig. 1 .,
Accurate Multi-contrast MRI Super-Resolution via a Dual Cross-Attention Transformer Network,,Fig. 2 .,
Accurate Multi-contrast MRI Super-Resolution via a Dual Cross-Attention Transformer Network,,,
Accurate Multi-contrast MRI Super-Resolution via a Dual Cross-Attention Transformer Network,,Fig. 3 .,
Accurate Multi-contrast MRI Super-Resolution via a Dual Cross-Attention Transformer Network,,"H 8 × W 8 LRH 4 × W 4 LR,",
Accurate Multi-contrast MRI Super-Resolution via a Dual Cross-Attention Transformer Network,,Fig. 4 .,
Accurate Multi-contrast MRI Super-Resolution via a Dual Cross-Attention Transformer Network,,Table 1 .,
Accurate Multi-contrast MRI Super-Resolution via a Dual Cross-Attention Transformer Network,,Table 2 .,
Accurate Multi-contrast MRI Super-Resolution via a Dual Cross-Attention Transformer Network,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 30.
Differentiable Beamforming for Ultrasound Autofocusing,1,Introduction,"Ultrasound images are reconstructed by time sampling the reflected pressure signals measured by individual transducer elements in order to focus at specific spatial locations. The sample times are calculated so as to compensate for the time-of-flight from the elements to the desired spatial locations, often by assuming a constant speed of sound (SoS) in the medium, e.g., 1540 m m/s. However, the human body is highly heterogeneous, with slower SoS in adipose layers than in fibrous and muscular tissues. If unaccounted for, these differences lead to phase aberration, geometric distortions, and loss of focus and contrast [1]. This degradation is a fundamental limitation of current ultrasound image reconstruction and impacts downstream tasks such as diagnostics, volumetry, and registration.Historically, phase aberration has been described using simplified phasescreen models [5,19], which assume that distortions generated from an unknown SoS can be modeled by a gross time delay offset at every element [1]. More recently, several methods have been proposed to estimate SoS distribution of the medium from aberration measurements as a step before actual image correction. A family of these methods still relies on simplified physical models of wave propagation to derive tractable inverse problems. These include assuming a horizontally layered medium [9] or coherent plane wavefront propagation at different angulations [17]. To reinforce specific assumptions about SoS heterogeneity, regularization is often introduced, including total variation for focal inclusion geometries [15] and Tikhonov regularization for smoothly varying layered SoS distributions [14,17]. While these methods perform well for one class of SoS inversion problems, it is challenging to generalize their applicability to arbitrary SoS distributions, which are generally found in clinical scenarios. Work has been carried out to find more generalizable estimation based on training neural network models to end-to-end learn SoS distributions or optimize the regularization function basis [4,16,18]. However, these methods require thousands of training instances, which can currently practically only be obtained from in-silico simulations and show challenges generalizing to real data.Recent developments in artificial intelligence have been facilitated by the release of open-source tensor libraries, which can perform automatic differentiation of composable transformations on vector data. These libraries are the backbone of complex neural network architectures that use automatic reversemode differentiation (back-propagation) to iteratively optimize weights based on a set of training instances. These libraries also simplify and optimize portability to high-performance computing platforms. We hypothesize that such libraries can likewise be extended to model the pipeline of ultrasound image reconstruction as a composition of differentiable operations, allowing optimization based on a single data instance.In this work, we propose an ultrasound imaging paradigm that jointly achieves sound speed estimation and image quality enhancement via differentiable beamforming. We formulate image reconstruction as a differentiable function of a spatially heterogeneous SoS map, and optimize it based on quality metrics extracted from the final reconstructed images (Fig. 1)."
Differentiable Beamforming for Ultrasound Autofocusing,2,Methods,
Differentiable Beamforming for Ultrasound Autofocusing,2.1,Beamforming Multistatic Synthetic Aperture Data,"In ultrasound imaging, radiofrequency data (RF) represents the time series signal proportional to the pressure measured by each probe array sensor. A multistatic synthetic aperture dataset contains the RF pulse-echo responses of every pair of transmit and receive elements. We denote the signal due to the i-th transmit element, and j-th receive element as u ij (t). This signal can be focused to an arbitrary spatial location x k by sampling u ij (t) at the time corresponding to the time-of-flight τ from the transmit element at x i to x k and back to the receive element at x j , achieved via 1D interpolation of the RF signal:((We describe our time-of-flight model in greater detail below in Sect. 2.3.) The interpolated signals are then summed across the transmit (N t ) and receive (N r ) apertures to obtain a focused ultrasound image:This process of interpolation and summation is called delay-and-sum (DAS) beamforming."
Differentiable Beamforming for Ultrasound Autofocusing,2.2,Differentiable Beamforming,"DAS is composed of elementary differentiable operations and is consequently itself differentiable. Therefore, DAS can be incorporated into an automatic differentiation (AD) framework to allow for differentiation with respect to any desired input parameters θ. For a given loss function L(u(x k ; θ)) that measures the ""quality"" of the beamforming, θ can be optimized using gradient descent to identify the optimal θ using update steps Δθ:This differentiable framework is flexible, providing many ways to parameterize the beamforming. In this work, we will show the promise of differentiable beamforming on the task of sound speed estimation by optimizing for slowness s in a time of flight delay model (i.e. θ = s)."
Differentiable Beamforming for Ultrasound Autofocusing,2.3,Time of Flight Model,"Here, we parameterize the slowness (i.e. the reciprocal of the sound speed) as a function of space. Specifically, we define the slowness at a set of control points as s = {s(x k )} k , which can be interpolated to obtain the slowness at arbitrary x. The time-of-flight from x 1 to x 2 is the integral of the slowness along the path:For simplicity and direct comparison with previous sound speed estimation models [17], a straight ray model of wave propagation is used."
Differentiable Beamforming for Ultrasound Autofocusing,2.4,Loss Functions for Sound Speed Optimization,"Speckle Brightness Maximization. Diffuse ultrasound scattering produces an image texture called speckle. Speckle brightness can be used as a criterion of focus quality [13]. Written as a loss, this is the negative average pixel magnitude:Coherence Factor Maximization. Coherence factor [6,11], also referred to as the F criterion or ""focusing criterion"", defined between 0 and 1, is the measure of the coherent signal sum over the incoherent signal sum of the receive aperture. When received signals are in focus (i.e. in equal phase), CF achieves the maximum value of 1. We use the negative CF as a loss:Phase-Error Minimization. The van Cittert Zernike theorem of optics [12] states that when imaging diffuse scatterers using a given transmit and receive sub-aperture T a and R a (i.e. subset of the available array elements), the resulting signal is almost perfectly correlated with the signal from a second set of apertures T b and R b when the two apertures share a common midpoint. The measured phase-shift between both signals should approach zero when aberration is corrected. Figure 2 illustrates this concept of phase error.We estimate the phase shift as the complex angle between DAS signals u a and u b of the respective subapertures (T a , R a ) and (T b , R b ), calculated using (2):The phase shift error (PE) is defined for a set of all aperture pairs (a, b) with common midpoint asFig. 2. Phase error minimization in correlated common mid-point sub-apertures. Phase error is computed as the angle of the cross correlation of complex beamformed signals from different sub-apertures sharing a common midpoint. When the correct slowness is used for the beamforming, the phase error is minimized."
Differentiable Beamforming for Ultrasound Autofocusing,3,Experimental Setup,
Differentiable Beamforming for Ultrasound Autofocusing,3.1,Implementation of Differentiable Beamformer,"A differentiable DAS beamformer was implemented in Python using JAX1  [3], which provides out of the box GPU acceleration. DAS was parameterized by the slowness map, where the time-of-flights for beamforming were calculated via bilinear interpolation of the slowness along a discretized path from the transmitting element to a location of interest and from the location to a receiving element. The loss was computed on 5×5 pixel patches (λ/2 pixel spacing) on a regular 15×21 grid spanning the image. The sound speed map was then optimized via gradient descent. For the phase error loss, 17-element subapertures were used for beamforming. The beamformed data for every subaperture pair with a common midpoint were cross-correlated with a 5 × 5 path to compute the phase shift. We further leveraged acoustic reciprocity to combine the results for reciprocal transmit/receive subapertures. This phase-shift measurement was then used for the final phase error loss. The GPU-based implementation runs in ∼300 s for 300 iterations on an NVIDIA RTX A6000. The code for this work can be found on GitHub2 ."
Differentiable Beamforming for Ultrasound Autofocusing,3.2,Comparison with State-of-the-Art Methods,"As a baseline for performance comparison, the Computed Ultrasound Tomography in Echo Mode (CUTE) method developed by Stähli et al. [17] was implemented in MATLAB; this method has been shown to achieve sound speed reconstruction of both layered and focal lesion geometries. The method shows some similarities in using phase error minimization from different apertures (albeit in the angular domain) and ray tracing paths. However, it relies on a coherent plane wavefront propagation model and Tikhonov regularization to build a tractable inverse problem."
Differentiable Beamforming for Ultrasound Autofocusing,3.3,Datasets,"In-Silico. The CUDA-accelerated binaries of the k-Wave simulation suite [10] were used to generate multistatic RF data of 3D phantom model acquisitions.To compare with the baseline [17], simulations were first generated using planewave transmissions (115 transmits in steering range of -28.5 • :0.5 • :28.5 • ) and then converted to FSA format using REFoCUS [2] in the rtbf framework [7].A linear 128 element linear probe was simulated, with a pitch of 0.3 mm and a center frequency of 4.8 MHz with a 100% bandwidth. The simulation domain was 60 × 51 x 7.4 mm 3 . Iso-echoic phantoms were generated whereby the sound speed was modulated relative to the density of a region so the average brightness remained constant while the sound-speed variation introduced phase aberration.In-Vivo. In-vivo data was collected on a Verasonics Vantage research system with a L12-3v linear transducer (192 elements, 0.2 mm pitch, 5 MHz center frequency). Three abdominal liver views, which contained subcutaneous adipose, musculoskeletal tissue and liver parenchyma, were collected from a healthy volunteer under a protocol approved by an institutional review board."
Differentiable Beamforming for Ultrasound Autofocusing,4,Results,"Figure 3 shows SoS maps for in-silico phantom data. In the uncorrected (naive) B-modes, regions of darkening and smeared speckle can be seen as acoustic intensity diminishes due to aberration. In the quadrant phantom (a), a distinct spatial skewing can be observed from left to right. On the corrected images, image brightness is enhanced, iso-echogenic speckle distributions are revealed, aberrated regions are reduced, and the boundary between quadrants shows a congruent left-to-right and top-to-bottom transition. Similarly, in the inclusion phantom (b), characteristic triangles can be seen to the left and right of the inclusion in the naive B-mode. These triangular offshoots are artifacts produced by total wave reflection on the lateral lesion boundaries when the ultrasound wave encounters an SoS transition at grazing incidence. Moreover, diffraction of waves through the lesion lead to aberration errors behind the lesion. In the corrected B-mode, these dark regions are enhanced, and the image has an overall more homogeneous brightness pattern.The sound speed distributions generated with differentiable beamforming are in general agreement with the ground truth sound speed distributions. Table 1 quantitatively compares the mean absolute error (MAE) and standard deviation (std) with respect to the ground truth. For all phantoms, differential beamforming achieved lower (better) error metrics than the baseline. Homogeneous phantoms were best reconstructed via CF loss function, while inhomogeneous phantoms were best reconstructed via PE loss function.Figure 4 shows preliminary results with differential beamforming for the reconstructed in-vivo data. The SoS reconstruction successfully delineates abdominal layers including subcutaneous adipose fat (average 1494 m m/s), muscle (average 1551 m m/s) and liver parenchyma (average 1530 m m/s) in agreement with the literature values [8].   "
Differentiable Beamforming for Ultrasound Autofocusing,5,Discussion and Conclusion,"Differentiable beamforming can be used to solve for unknown quantities with gradient descent. Here, we parameterized beamforming as a function of the slowness and optimized with respect to several candidate loss functions, showing that phase error was best for heterogeneous targets. The differentiable beamformer simultaneously provided B-mode image correction and quantitative sound speed characterization beyond the state-of-the-art across several challenging cases. Preliminary in-vivo quantitative SoS data for liver was shown, which has direct clinical applications such as in the noninvasive assessment of non-alcoholic fatty liver disease, as well as image enhancement in general. Importantly, the differentiable beamformer allows us to incorporate fundamental physics principles like wave propagation, reducing the number of parameters to optimize. In the future, more complex wave propagation physics, such as refraction models, can be added to SoS optimization. In addition to sound speed, this work can be readily adapted to a broad set of applications such as beamforming with flexible arrays, where element positions are unknown, or passive cavitation mapping, where the origin of the signal is uncertain. Because the gradients flow through the entire imaging pipeline, the differentiable beamformer is also highly compatible with deep learning techniques. For instance, a model can be trained in a self-supervised fashion to identify optimal sound speed updates to accelerate convergence. Differentiable beamforming also enables the end-toend optimization of imaging parameters for downstream tasks in computer-aided medical diagnostics."
Differentiable Beamforming for Ultrasound Autofocusing,,Fig. 1 .,
Differentiable Beamforming for Ultrasound Autofocusing,,Fig. 3 .,
Differentiable Beamforming for Ultrasound Autofocusing,,Fig. 4 .,
Differentiable Beamforming for Ultrasound Autofocusing,,Table 1 .,"14.3 ± 16.4 8.3 ± 7.5 7.5 ± 5.9 6.1 ± 4.4 Inclusion layer [17], Fig. 3b 19.8 ± 18.1 16.3 ± 14.8 15.0 ± 11.1 7.5 ± 5.0"
CycleSTTN: A Learning-Based Temporal Model for Specular Augmentation in Endoscopy,1,Introduction,"During endoscopic procedures, such as colonoscopy, the camera light source produces abundant specular highlight reflections on the visualised anatomy. This is due to its very close proximity to the scene coupled with the presence of wet tissue. These reflections can occlude texture and produce salient artifacts, which may reduce the accuracy of surgical vision algorithms aiming at scene understanding, including depth estimation and 3D reconstruction [5,16]. To resolve these challenges, a simple solution is to detect and mask out these regions before performing any other downstream visual task. However, this approach comes with limitations. In the context of sparse feature point detection, simply discarding points falling on specularities results in excessive filtering, often leading to an insufficient number of detected features. As for dense estimation problems, such as pixel depth regression and optical flow, masking out specular regions makes interpolation necessary.Alternatively, videos can be pre-processed to inpaint specular highlights with its hidden texture inferred from neighbouring frames [7]. This allows running other algorithms with reflection-free data. However, this adds a significant computational overhead and requires the processing of temporal frame windows that restrict online inference applications. Given that state-of-the-art feature detection methods are deep learning models, an appealing approach would be to learn robustness to reflections during training, as this would result in an single-step end-to-end inference model without any pre or post-processing overhead.In this paper, we propose to learn robustness to specular reflections via data augmentation. We use a CycleGAN [24] methodology that takes advantage of a pre-trained specular highlight removal network in adversarial training. Our proposed generator network, based on STTN [7,23], performs video-to-video translation. In doing so, we create a cycle structure for STTN, that we call CycleSTTN. To demonstrate the effectiveness of our approach, we use it to improve the performance of the SuperPoint feature detector/descriptor. We combine the proposed method with that of [7] to add and remove specularities as data augmentation. The contributions of this paper can be summarised as:-We propose the CycleSTTN training pipeline as an extension of STTN to a cyclic structure. -We use CycleSTTN to train a model for synthetic generation of temporally consistent and realistic specularities in endoscopy videos. We compare results of our method against CycleGAN. -We demonstrate CycleSTTN as a data augmentation technique that improves the performance of SuperPoint feature detector in endoscopy videos."
CycleSTTN: A Learning-Based Temporal Model for Specular Augmentation in Endoscopy,2,Related Work,"Barbed et al. investigated the challenge of specular reflections when performing feature detection in endoscopy images [3]. As a solution, they design a loss function to explicitly avoid specular regions and focus on detecting points in the remaining parts of the image. However, this ignores the fact that features extracted from these points will still be contaminated by specularity pixels in their neighborhood. Following a different strategy, we propose data augmentation as a way to induce specularity robustness in both point detection and feature extraction. Data augmentation for lighting conditions in surgical data has been explored extensively before. Classical techniques for modelling specular highlights include the use of a parametric model to add specularities and illumination to real images [1,13]. Another technique for augmentation uses synthetic data or phantoms, where different light sources and environment variables can be captured. However, synthetic data lacks real textures and artifacts. Thus many image-toimage translation techniques have been developed to add more realistic textures to synthetic data [15,17,21,22]. These methods rely on CycleGAN [24] for unsupervised learning and thus, are able to map from real to synthetic domain and vice-versa. This allows for the generation of real images with the same structure but different lighting, texture, and blurriness. To have more control over the augmentations, some methods add a controllable noise vector to the network input that modifies the image lighting, specular reflections, and texture. However, this vector does not have a physical meaning, and it is thus challenging to independently control the different environment variables by directly manipulating its values. To address this, [14] uses two separate noise inputs, one controlling texture and specular reflection and another controlling color and light. However, all these approaches still use multiple steps to finally create new real data, which might lead to loss of important information in the process.Single-step approaches have also been developed that augment real data directly, but the generated images have different structures and thus, do not create paired data [9,20]. A CycleGAN model has been proposed to map from images with specularities to images without specularities [11] using manually labelled patches cropped from frames, however, this work focuses on specular highlight removal, and does not test the data augmentation capabilities of generating synthetic specularities. In [12] a classification model is used to categorize data for unpaired training of CycleGAN. From the output of CycleGAN, they generate a paired dataset, however, only quality metrics are used to filter out images in the generated paired dataset. Furthermore, most of these approaches are applicable to single frames and are not able to generate synthetic videos with temporally consistent specularities. While some other methods use a temporal component for endoscopic video augmentation [17,21], they do not have a single-step structure and have not been applied to generate/remove specular highlights."
CycleSTTN: A Learning-Based Temporal Model for Specular Augmentation in Endoscopy,3,Methods,"We use the STTN model as our video-to-video translation architecture and T-PatchGAN [6] as the discriminator. STTN contains an encoder followed by a spatio-temporal transformer and a decoder [7,23]. While STTN was originally proposed to remove occlusions in videos, in this paper, we use two instances of this model, ST T N R and ST T N A , to respectively remove and add specular occlusions. We start by pre-training these models separately using their respective discriminators D R and D A . Then, we continue training them simultaneously in an adversarial manner with a CycleGAN methodology. We denote the complete training pipeline as CycleSTTN (Fig. 1), which is divided into 3 sections:1. Paired Dataset Generation We generate a dataset of paired videos with and without specularities. We train a generator for specularity removal fol-lowing the same methodology as in [7]. This model is denoted as (ST T N R0 , D R0 ), where ST T N is the generator and D is the discriminator. For a set of real endoscopic videos with specularities V A and specularity masks M , we run ST T N R0 to generate their inpainted counterparts without specularities V R (Fig. 1 -Step 1)."
CycleSTTN: A Learning-Based Temporal Model for Specular Augmentation in Endoscopy,2.,ST T N A Pre-training,"Using the paired dataset (V A , V R ) we train a new model to add specularities. We denote it as ST T N A0 with D A0 as its discriminator. This is shown in Fig. 1 as step 2."
CycleSTTN: A Learning-Based Temporal Model for Specular Augmentation in Endoscopy,3.,"ST T N R , ST T N A Joint Training By initializing with the models from",Step  
CycleSTTN: A Learning-Based Temporal Model for Specular Augmentation in Endoscopy,3.1,Model Inputs,"The STTN architecture receives as input a paired sequence of frames and masks. Originally masks are meant to represent occluded image regions that should be inpainted, however, in this work, we do not always use them in this way.When training ST T N R we define the mask inputs as regions to be inpainted (to remove specularities). However, when training ST T N A , input masks are set to 1 for all pixels, since we do not want to enforce specific locations for specularity generation; we want to encourage the model to learn these patterns from data."
CycleSTTN: A Learning-Based Temporal Model for Specular Augmentation in Endoscopy,3.2,Losses,"The loss function of the T-PatchGAN discriminators are shown below, such that E is the expected value of the data distributions as done in [23]:(1)where F ake A = ST T N A (V R ) represents fake videos with added specularities, and analogously F ake R = ST T N R (V A ) represents fake videos with removed specularities. Further, we also define F ake R = M.F ake R + V A (1 -M ) for the discriminator loss, where inpainted occluded regions from F ake R are overlaid over V A . M denotes masks with 1 values in specular regions of V A and 0 otherwise. For the generators, an adversarial loss was used as done in [7]:The identity loss was the only loss modified from the original STTN model [7]. The identity loss for ST T N R and ST T N A are:Here L idtR ensures that if a video does not have specularities, it would stay the same when fed into the model that removes specularities. Whereas, L idtA ensures predicted videos with specularities, F ake A , resemble real specular videos V A .Finally, we added cycle loss terms:The total generator losses L R and L A for removing and adding specularities are shown below, such that the loss weights λ are all set to 1 except for λ adv , which is set to 0.01 as advised by [23]:In summary, we adopted the original STTN model [7] and changed the training pipeline, model inputs, and losses as shown in Fig. 1. In particular, (a) the training pipeline was transformed into a multi-task one of adding and removing specularities, where ST T N R0 from [7] was used as an initialization model. (b) For ST T N A , specularity masks were removed from model inputs. (c) Identity losses and cycle losses were also added while masked based losses were removed."
CycleSTTN: A Learning-Based Temporal Model for Specular Augmentation in Endoscopy,4,Experiments and Results,
CycleSTTN: A Learning-Based Temporal Model for Specular Augmentation in Endoscopy,4.1,Datasets and Parameters,"To evaluate our pipeline, we use 373 videos from the Hyper Kvasir dataset [4] to generate our paired dataset (V R , V A ) as described in Sect. 3 and Fig. 1. 343 video pairs were used for training and 30 for testing with an upper limit of 927 frames per video. Models were trained on NVIDIA A100-SXM4-40GB GPUs. We use the same training parameters as [7,23] with the exception of batch size, which we changed from 8 to 3 for training (ST T N A1 , ST T N R1 ). CycleGAN models were trained with suggested parameters in CycleGAN's public repository 1 , with the exception of batch size, which was changed from 1 to 3.In our experimental analysis, we use our proposed models shown in Fig. 1 along with CycleGAN models, which use ResNet with 9 residual blocks as the generator. All these models are listed in Table 1. We note that even though ST T N R1 was trained with masks, it seems it was affected by the cycle loss and was only able to give decent results without a mask as input. "
CycleSTTN: A Learning-Based Temporal Model for Specular Augmentation in Endoscopy,4.2,Pseudo Evaluation Experiments,"We input the pseudo ground truth V R to our models, ST T N A0 and ST T N A1 . We compare the output F ake A to real videos V A . We conduct non-temporal testing by using single frame inputs, as opposed to video inputs, to demonstrate the temporal effect. We report the Peak Signal to Noise Ratio (PSNR), Structural Similarity Index (SSIM), and Mean Square Error (MSE) metrics. We show visual results for different models in Fig. 2. When V R (videos with no specularities) are used as input, our CycleSTTN model ST T N A1 shows the highest similarity to the ground truth (V A ). With V A (videos with specularities) as input, ST T N A1 is able to add more realistic specularities that flow smoothly from one frame to another. CycleGAN based models were not able to add new specularities with V A as input. For ResN et A0 , this is expected, due to the original CycleGAN identity loss. When changing the identity loss, ResN et A1 only intensifies specularities and darkens the background texture. This was further validated through results shown in Table 2, where ST T N A1 has the best SSIM, PSNR and MSE values. We can also see that ST T N A1 results are only slightly  worse without temporal testing according to PSNR and MSE, yet slightly better according to SSIM. Thus, the temporal component only benefits training as a regularizer (i.e. our models outperforms CycleGAN based models). However, during inference, temporal testing is not necessary since it yields similar results to non-temporal testing.The pseudo evaluation shows that generated specularities closely resemble real ones in appearance and location. While not fully enforcing physical realism, this augmentation improves upon traditional warping methods (Sect. 4.3)."
CycleSTTN: A Learning-Based Temporal Model for Specular Augmentation in Endoscopy,4.3,Relative Pose Estimation Experiments,"We use our models as data augmentation to re-train the feature detector proposed in [3], an adaptation of SuperPoint [8] to endoscopy. While [3] is originally trained with a specularity loss term that encourages the network to ignore specularity regions, we omit this term in our training. We want our models to be robust to specularities, rather than just avoiding them. We use the pre-trained model in [3] to generate data labels and initialize our models. SuperPoint is trained by generating a sparse set of point matches in an image and its warped version via homography. Augmentations already used by SuperPoint include traditional random brightness, contrast, Gaussian noise, speckle noise, blur, and shade. We use our models as augmentations to original and warped images separately by randomly choosing between (no augmentation, specularity addition, and specularity removal). SuperPoint models with various augmentations are listed in Table 3. These models are trained using 131 randomly chosen videos from V A (with cropped boundaries) and 14 for validation for 25,000 iterations with the same training parameters as [3]. Temporal (non-temporal) refers to feeding an image and its warped version together (separately) to the augmentation model. We evaluate the quality of point detections by using them to estimate relative camera motion in endoscopic sequences. We first apply brute-force matching of detected points in image pairs and then estimate motion via RANSAC [10]. The test data for this experiment is the same as in [3]. It includes 6 sequences (14191 frames) from the EndoMapper dataset [2] with a relative camera motion pseudo ground truth based on structure-from-motion (SFM: COLMAP [18,19]). Reported metrics include the precision of inlier points from RANSAC-estimated (threshold = 10 px) essential matrices as compared to inlier points using pseudo ground truth essential matrix (from COLMAP). To generate the pseudo ground truth inliers, the same distance metric used in RANSAC was applied to the pseudo ground truth essential matrix (from COLMAP). We also report the Rota-tion error, which is the geodesic angle in degrees between the RANSAC-based pose estimation and the pseudo ground truth pose (from COLMAP).In Table 3, all specularity augmentations (models 2-15) improve Super-Point relative to not using them (model 1), which demonstrates their usefulness. Most STTN-based augmentations (models 2-9) produce better results than CycleGAN-based ones (models 10-15). Overall, the best performing augmentation is (ST T N A1 , ST T N R0 ), showing the effectiveness of our system. This makes sense since the best removal and addition models are ST T N R0 and ST T N A1 according to the rotation error. However, it appears that non-temporal testing sometimes gives lower rotation errors than temporal testing. This could be due to the unrealistic nature of warped images used as consecutive frames. As also discussed in Sect. 4.2, temporal testing does not improve results, only temporal training does."
CycleSTTN: A Learning-Based Temporal Model for Specular Augmentation in Endoscopy,5,Conclusion,"In conclusion, we introduce CycleSTTN, a temporal CycleGAN applied to generate temporally consistent and realistic specularities in endoscopy. Our model outperforms CycleGAN, as demonstrated by mean PSNR, MSE, and SSIM metrics using a pseudo ground truth dataset. We also observe a positive effect of our model as augmentation for training a feature extractor, resulting in improved inlier precision and rotation errors. However, our evaluation relies on SFM generated ground truth, different testing and training datasets, and indirect metrics, which may introduce some uncertainty. Nevertheless, augmentation shows great promise as an addition for training various endoscopic computer vision tasks to enhance performance and provide insights into the impact of specific artifacts."
CycleSTTN: A Learning-Based Temporal Model for Specular Augmentation in Endoscopy,,,
CycleSTTN: A Learning-Based Temporal Model for Specular Augmentation in Endoscopy,,Fig. 1 .,
CycleSTTN: A Learning-Based Temporal Model for Specular Augmentation in Endoscopy,,Fig. 2 .,
CycleSTTN: A Learning-Based Temporal Model for Specular Augmentation in Endoscopy,,Table 1 .,
CycleSTTN: A Learning-Based Temporal Model for Specular Augmentation in Endoscopy,,Table 2 .,
CycleSTTN: A Learning-Based Temporal Model for Specular Augmentation in Endoscopy,,Table 3 .,
CycleSTTN: A Learning-Based Temporal Model for Specular Augmentation in Endoscopy,,Acknowledgments,". This research was funded in part, by the Wellcome/EPSRC Centre for Interventional and Surgical Sciences (WEISS) [203145/Z/16/Z]; the Engineering and Physical Sciences Research Council (EPSRC) [EP/P027938/1, EP/R004080/1, EP/P012841/1]; the Royal Academy of Engineering Chair in Emerging Technologies Scheme; H2020 FET (GA863146); and the UCL Centre for Digital Innovation through the Amazon Web Services (AWS) Doctoral Scholarship in Digital Innovation 2022/2023. For the purpose of open access, the author has applied a CC BY public copyright licence to any author accepted manuscript version arising from this submission."
Reflectance Mode Fluorescence Optical Tomography with Consumer-Grade Cameras,1,Introduction,"Near-infrared (NIR) fluorescence imaging can allow the detection of fluorophores up to 4 cm depth in tissue [11]. Recently, with the availability of clinically approved NIR fluorophores such as indocyanine green or ICG, fluorescence imaging is increasingly being employed for intra-operative guidance during surgically excision of malignant tumors and lymph nodes [6,15,16]. Fluorescence imaging is also a workhorse for small animal or preclinical research with multiple commercial devices utilizing sensitive front or back-illuminated and cooled CCD camera detectors available at prices ranging from 250-600K USD [9,14].A majority of fluorescence imaging applications including Fluorescence Guided Surgery (FGS) rely upon visible 2D surface imaging [5,8,17,20] while reconstruction of the invisible 3D target in tissue is not widely used for reflectance mode imaging despite a large number of publications in 3D fluorescence diffuse optical tomography (FDOT) since early 1990s [1,4,10,18,19]. The primary cause of this impasse is the ill-posedness of the mathematical inverse problem underlying the 3D reconstruction of the target in tissue from boundary measurements.The prime motivation of our work is to enable an efficient 3D tumor shape reconstruction for FGS in an operating room environment, where we do not have full control of the ambient light and we cannot rely on sophisticated time or frequency domain imaging instrumentation and setup. In these situations, one has to use clinical cameras producing rapid Continuous Wave (CW) fluorescence boundary measurements [19] in reflectance mode (i.e., the transmission of the light through the domain is not measured), and with low signal-to-noise ratio which further exacerbates the ill-posedness of FDOT problem. The standard approach for solving FDOT problem with CW measurements is based on Born approximation which works well in the case of a small compared to the computational domain target and a very large number of reflectance-transmission type measurements made by ""slow in acquisition"" light sources and detector arrays of highly sensitive cooled CCD cameras or photomultiplier tube arrays collecting both reflected and transmitted light [18]. None of these is suitable for FGS settings where time is limited, just a few reflectance mode CW-measurements are available, and the target can be large compared to the imaged domain.We propose an Incremental Fluorescent Target Reconstruction (IFTR) scheme, based on the recent advances in quadratic and conic convex optimization and sparse regularization, which can recover a relatively large 3D target in tissuelike media. In our experiments, IFTR scheme demonstrates accurate reconstruction of 3D targets from reflectance mode CW-measurements collected at the top surface of the domain. To our best knowledge, this is the first report where the 3D shape of tumor-like target has been recovered from reflectance mode steady-state CW measurements. Previously such results were reported in FDOT literature only for time-consuming frequency-domain or time-domain measurements [12] where photon path-length information is available. Moreover, the data is acquired almost instantly by an inexpensive (<100 Euros) camera with flexible fiber-optics making it suitable for endoscopic FGS in contrast to the standard slow in acquisition frequency-based measurements obtained by expensive (USD100K+ range) stationary cameras. Lastly, IFTR scheme is implemented using FEniCS [3], a highlevel Python package for FEM discretization of the physical model, and CVXPY [2,7], a convex optimization package making this method easy to reuse/adjust for a different setup. The code and data produced for this work are released as an open source at https://github.com/IBM/DOT."
Reflectance Mode Fluorescence Optical Tomography with Consumer-Grade Cameras,2,Methods,"Figure 1 describes the setup representing a typical surgical field while excising tumors. We simulate the provision of 3D surgical guidance via a flexible endo-  scope type fluorescence imager. For such provision we need to solve the following FDOT problem: estimate the spatial shape χ of the ICG tagged tumor target (the cube in green) within the tissue domain Ω ∈ R 3 (the area in grey) from measurements y. Measurements are obtained by illuminating the tissue domain with NIR light at ICG peak excitation wavelength via the expanded beam from endoscope fiber bundle, and then measuring the light emitted by the tumor like target diffusing to the top face of the phantom surface ∂Ω obs , by the fiber bundle with suitable emission filter which is coupled to a camera at the backend.In this section we briefly describe the mathematical formulation of the FDOT problem and introduce the IFTR scheme for solving it.Forward and Inverse Problems. Photon propagation in tissue-like media is described by a coupled system of elliptic Partial Differential Equations (PDEs) for determining photon fluence φ (W/cm 2 ) at excitation and fluorescence emission wavelengths through out the domain. Wavelength and space dependent absorption and scattering coefficients and fluorophore properties comprise the coefficients of this PDE system (see Appendix A). The discretization of coupled diffusion PDEs is obtained by applying a standard FEM methodology [13]: domain Ω is covered by a uniform grid comprised of N nodes {x i } N i=1 ; each function φ is approximated by a vector φ ∈ R N with components φ i = φ(x i ); PDEs are approximated using weak formulations incorporating boundary conditions. This results in a system of algebraic equations:where the first equation describes the excitation photon fluence φ x ∈ R N , and the second describes photon emission fluence φ m ∈ R N ; subscripts x and m indicate excitation and emission respectively. Vectors f , χ ∈ R N are the source of excitation light and target's shape indicator, i.e., a binary vector such that χ i = 1 if x i belongs to the target and 0 otherwise. S x/m (•) ∈ R N ×N are the stiffness matrices obtained by discretizing the diffusion terms of excitation/emission PDEs respectively and additionally S x depends on χ. M ∈ R N ×N is the mass matrix and denotes Hadamard (elementwise) product such that M χ = M diag(χ) and M χφ x = M φ x χ. Finally, vector of measurements y ∈ R K is related to the emission fluence φ m as followsHere T ∈ R K×N is a binary matrix that selects components of φ m corresponding to the observed grid nodes and K is a number of observed nodes.In the following if target indicator χ is given then the system (1) is referred to as the forward FDOT problem to compute unknown excitation and emission fluence φ x , φ m . If vector χ is unknown but measurements of emission fluence are present then the system (1)-( 2) is referred to as the FDOT inverse problem."
Reflectance Mode Fluorescence Optical Tomography with Consumer-Grade Cameras,,Search Space Regularization.,"In what follows we propose an algorithm that estimates target's indicator χ from data y, i.e. solves the inverse FDOT problem. To reduce the ill-posedness of the inverse problem ( 1)-( 2) we introduce several regularization schemes. These regularizations describe prior knowledge about the desired solution χ and thus reduce the search space of admissible targets.The first regularization represents an assumption that the correct χ is a binary vector. Since binary constraints are not convex, we adopt a more relaxed condition on χ referred to as the box constraints: 0 ≤ χ ≤ 1.The second regularization describes the piece-wise constant structure of the indicator χ, and is referred to as the piece-wise total variation (PTV). It is obtained by extending the notion of total variation which has been successfully applied in optical tomography. To this end, assume m(j), n(j), and j ∈ I are indices corresponding to the j-th pair of neighboring nodes. Let the domain Ω be split into N ptv non-overlapping subdomains, e.g., cuboids, and the index I is correspondingly split into non-overlapping sub-indices I i , i = 1, . . . , N ptv of nodes pairs that belong to Ω i . PTV is obtained as a sum of total variations computed using sub-indices I i :and is also written in a matrix form assuming matrix V encodes subtraction across node pairs across all sub-indices. The third regularization aims to reduce a null space of the inverse problem in the boundary layer of a thickness , reflecting the assumption that the target is under the surface. It is referred to as the boundary regularization and is defined as W χ = 0 where W selects components of χ that belong to the boundary layer.Finally, the fourth regularization referred to as the minimum volume regularization requires that χ has at least m 0 non-zero components:Optimization Framework. In this subsection we present an Incremental Fluorescent Target Reconstruction (IFTR) scheme solving the inverse problem (1)- (2). Noting that the nonlinearity of the inverse problem stems from the fact that χ φ x is a bi-linear vector function the IFTR scheme employs the following splitting method: (i) for n = 0, 1, . . . fix χ n and compute φ nx as the unique solution of linear excitation equation:then (ii) fix the obtained φ n x and compute χ n+1 as the unique solution of one of the 3 convex optimization problems:"
Reflectance Mode Fluorescence Optical Tomography with Consumer-Grade Cameras,,Variant I. This variant relies upon direct inversion of the emission equation matrix S -1,"m to find χ n+1 :Variant II. This variant imposes the emission equation as an inequality constraint:Here, depending on the value of p we take x 1 = x 2 or x 2 = 1 T x and E m is a parameter defining emission equation constraint tolerance.Variant III. This variant uses the emission equation as a term of the loss function:We note that all the three variants depend on parameter p = 1, 2 which defines the type of optimization problem that should be solved: i) if p = 1 we get conic optimization problems of the loss function in the form • 2 which would be treated as conic constraints); ii) if p = 2 we get quadratic optimization problems.To get a good initial guess for χ 0 we borrow from the Born approximation which suggests that excitation field φ x can be approximated by the background excitation obtained by solving excitation equation with no ICG, i.e., χ 0 = 0. Iterating this splitting method for n = 0, 1, 2, . . . we obtain a sequence of updates χ n that converge into a vicinity of the true χ provided data is ""representative enough"". We conclude the presentation of IFTR scheme with stopping criteria of the iterative process. For this we use a standard Dice coefficient d(•, •) and a binary projector b(•): the scheme stops once the following condition is met"
Reflectance Mode Fluorescence Optical Tomography with Consumer-Grade Cameras,3,Data Collection,"To validate the IFTR scheme, we performed an experiment capturing the essential elements of FGS applications. Figure 1 describes the experiment setup. The tissue phantom was composed of a 13×13×30 mm (inner dimensions) glass box filled with a 1% liposyn solution, which is a fat emulsion with scattering absorption properties mimicking human soft tissue [12]. The fluorescent target used was a 8×8×8 mm (inner dimensions) acrylic spectrophotometry cuvette filled with a 5% BSA, 1% liposyn, 7μM ICG solution.Figure 2 depicts the imaging system consisting of relatively inexpensive components: a Raspberry Pi Computer (4B/2GB), 12MP RGB camera (Raspberry Pi, SC0261) with IR filter removed, 16 mm telephoto lens (Raspberry Pi, SC0123), 700-800 nm band stop filter(Midwest Optical Sytems, DB850-25.4), 785 nm laser (Roithner Lasertechnik, RLTMDL-785-300-5) as the excitation source, and a polyscope fiber bundle (PolyDiagnost, PD-PS-0095). The detector and lens are approximately €100 combined, with just 8 bits of dynamic range. This is in stark contrast to the ultra-sensitive 16-bit scientific cameras priced an order of magnitude more used in other studies.We collected 3 sets of experimental measurements (see Fig. 3): i) y 6 mm top is emission fluence collected on the top surface from the target immersed 6 mm under the top surface of tissue phantom; ii) y 3 mm top is emission fluence from the target immersed 3 mm under the top surface; and iii) y 3 mm  top&side is also emission "
Reflectance Mode Fluorescence Optical Tomography with Consumer-Grade Cameras,4,Experimental Validation,"Performance of the proposed IFTR scheme is characterized by a set of numerical experiments. IFTR scheme was implemented using the FEniCS package for FEM matrices computation and CVXPY for the construction of the loss functions and constraints. Additionally CVXPY provides a common interface to various state-of-the-art optimization solvers making it very easy to switch between them. Although we tested IFTR with 4 commonly used solvers: OSQP, SCS and ECOS (distributed together with CVXPY) and MOSEK (required an additional installation) we report results for the solvers that performed the best. Thus, for the quadratic optimization formulation we selected OSQP and for the conic optimization formulation we selected MOSEK. The resulting configurations were compared in terms of Dice coefficient d(χ est , χ true ) comparing estimated target and the true target as well as execution time. The results of the performed experiments are summarised in Fig. 4 where the left column of panels presents Dice coefficients and the right column of panels presents respective execution time. Each row of panels in Fig. 4 corresponds (from the top to the bottom) to the experiment using one of three measurements vectors: 1) y 6 mm  top ; 2) y 3 mm top and 3) y 3 mm top&side . The first experiment is the most challenging as it recovers the target 6 mm deep under the surface. Yet, variant I of IFTR obtains good reconstruction with both quadratic OSQP and conic MOSEK solvers for which Dice score reaches value of 0.831. Good quality reconstruction is indeed confirmed on the left panel in Fig. 5 depicting target recovered by IFTR variant I with quadratic OSQP solver and plotted over the true target. The second experiment recovers the target 3 mm deep which is easier and thus more IFTR variants are capable of obtaining good reconstructions as suggested on the left panel in the middle row in Fig. 4. Additionally, the right panel in Fig. 5 depicts the target reconstructed by IFTR variant II with conic MOSEK solver. We stress that both of these experiments employ reflectance-mode measurements suggesting the proposed IFTR scheme is promising for adoption in FGS-related applications.The third experiment demonstrates the consistency of IFTR scheme: adding side measurements allows all variants to obtain good reconstructions and further increases Dice coefficients. This, however, comes at a price of increased computational demands, particularly for variant I solved with quadratic OSQP solver. The performed experiments reveal that it is difficult to pick a single winning configuration of IFTR scheme but there are several considerations: i) variant I provides the lowest errors but is the slowest variant with MOSEK solver has been consistently faster than OSQP; ii) variant II is the fastest variant but it is more sensitive to the amount of measurement compared to others; iii) variant III is less sensitive to the amount of measurements compared to Variant II has similar execution time but is less accurate.We also note that IFTR scheme is robust with respect to PTV regularization parameter. This was achieved by scaling the data misfit and PTV term to similar magnitude: we normalised the misfit term by the norm of the observations vector and rescaled PTV term by the number of subdomains and each local total variation weight by the number of nodes in that subdomain. The robustness to regularization parameter choice was confirmed by our experiments with several different values of such parameter. Another relevant consideration is that PTV impacts the loss function in a different way compared to a standard L1 or L2 regularization: the latter has the unique global minimizer (0-vector) while the former has many global minimizers and IFTR benefits from this."
Reflectance Mode Fluorescence Optical Tomography with Consumer-Grade Cameras,5,Conclusions,"In this work we proposed novel IFTR scheme for solving FDOT problem. It performs a splitting of the bi-linearity of the original non-convex problem into a sequence of convex ones. Additionally, IFTR restricts the search space by a set of regularizers promoting piece-wise constant structure of target's indicator function which in turn allows to recover fluorescent targets from only the reflectance mode CW measurements collected by a consumer grade camera.Although the scheme was tested using proof-of-concept experimental data and cubical shape target the method is general and depending on mesh discretization level, scalable to arbitrary domain and target shapes. Thus, the obtained results suggest strong potential for adoption of IFTR scheme in FGS related applications."
Reflectance Mode Fluorescence Optical Tomography with Consumer-Grade Cameras,,A Continuous formulation of the FDOT problem,"Near-infrared photon propagation in tissue like media is described by the following coupled system of PDEs:Here ( 9) is the excitation equation describing the excitation (at wavelength 785 nm) photon fluence φ x , W/cm 2 , and (10) -emission equation describing emission (at wavelength 830 nm) photon fluence φ m , W/cm 2 , subscripts x and m indicate excitation and emission respectively. The parameters of those equations are taken according to the laboratory experiment setup. Γ = 0.016 is a constant representing the dimensionless quantum efficiency of ICG fluorescence emission. D x/m , cm and k x/m , cm -1 refer to coefficients in excitation and emission equations, which determine light scattering and absorption properties of tissues:3 μ ax/mi + μ ax/mf + μ sx/m , k x/m = μ ax/mi + μ ax/mf (11) where μ sx = 9.84 cm -1 and μ sm = 9.84 cm -1 are the scattering coefficients of Liposyn at excitation and emission wavelength respectively; μ axi = 0.023 cm -1 and μ ami = 0.0289 cm -1 are the absorption coefficients of Liposyn at excitation and emission wavelengths; μ axf = μ ICG χ, cm -1 is the absorption coefficient of the unknown ICG-tagged target and thus depends on the target's shape modelled by an indicator function χ and ICG absorption coefficient at excitation wavelength μ ICG = 3.5 cm -1 ; μ amf = 0 cm -1 as we assume there is no selfabsorption of ICG fluorescence emission at the concentration ranges employed in this work and for practical applications [12].The system ( 9)-( 10) is complemented by Robin-type boundary conditions modelling the excitation source applied at the surface of the domain Ω:where γ = 2.5156 -dimensionless constant depending on the optical reflective index mismatch at the boundary."
Reflectance Mode Fluorescence Optical Tomography with Consumer-Grade Cameras,,Fig. 1 .,
Reflectance Mode Fluorescence Optical Tomography with Consumer-Grade Cameras,,Fig. 2 .,
Reflectance Mode Fluorescence Optical Tomography with Consumer-Grade Cameras,,Fig. 3 .,
Reflectance Mode Fluorescence Optical Tomography with Consumer-Grade Cameras,,Fig. 4 .,
Reflectance Mode Fluorescence Optical Tomography with Consumer-Grade Cameras,,Fig. 5 .,
Reflectance Mode Fluorescence Optical Tomography with Consumer-Grade Cameras,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 49.
Learning Deep Intensity Field for Extremely Sparse-View CBCT Reconstruction,1,Introduction,"Cone-beam computed tomography (CBCT) is a common 3D imaging technique used to examine the internal structure of an object with high spatial resolution and fast scanning speed [20]. During CBCT scanning, the scanner rotates around the object and emits cone-shaped beams, obtaining 2D projections in the detection panel to reconstruct 3D volume. In recent years, beyond dentistry, CBCT has been widely used to acquire images of the human knee joint for applications such as total knee arthroplasty and postoperative pain management [3,4,9,15].To maintain image quality, CBCT typically requires hundreds of projections involving high radiation doses from X-rays, which could be a concern in clinical practice. Sparse-view reconstruction is one of the ways to reduce radiation dose by reducing the number of scanning views (10× fewer). In this paper, we study a more challenging problem, extremely sparse-view CBCT reconstruction, aiming to reconstruct a high-quality CT volume from fewer than 10 projection views.Compared to conventional CT (e.g., parallel beam, fan beam), CBCT reconstructs a 3D volume from 2D projections instead of a 2D slice from 1D projections, as comparison shown in Fig. 1, resulting in a significant increase in spatial dimensionality and computational complexity. Therefore, although sparse-view conventional CT reconstruction [2,23,25,26] has been developed for many years, these methods cannot be trivially extended to CBCT. CBCT reconstruction can be divided into dense-view (≥100), sparse-view (20∼50), extremely sparseview (≤10), and single/orthogonal-view reconstructions depending on the number of projection views required. A typical example of dense-view reconstruction is FDK [6], which is a filtered-backprojection (FBP) algorithm that accumulates intensities by backprojecting from 2D views, but requires hundreds of views to avoid streaking artifacts. To reduce required projection views, ART [7] and its extensions (e.g., SART [1], VW-ART [16]) formulate reconstruction as an iterative minimization process, which is useful when projections are limited. Nevertheless, such methods often take a long computational time to converge and cope poorly with extremely sparse projections; see results of SART in Table 1. With the development of deep learning techniques and computing devices, learning-based approaches are proposed for CBCT sparse-view reconstruction. Lahiri et al. [12] propose to reconstruct a coarse CT with FDK and use 2D CNNs to denoise each slice. However, the algorithm has not been validated on medical datasets, and the performance is still limited as FDK introduces extensive streaking artifacts with sparse views. Recently, neural rendering techniques [5,14,19,21,29] have been introduced to reconstruct CBCT volume by parameterizing the attenuation coefficient field as an implicit neural representation field (NeRF), but they require a long time for per-patient optimization and do not perform well with extremely sparse views due to lack of prior knowledge; see results of NAF in Table 2. For single/orthogonal-view reconstruction, voxel-based approaches [10,22,27] are proposed to build 2D-to-3D generation networks that consist of 2D encoders and 3D decoders with large training parameters, leading to high memory requirements and limited spatial resolution. These methods are special designs with the networks [10,27] or patient-specific training data [22], which are difficult to extend to general sparse-view reconstruction.In this work, our goal is to reconstruct a CBCT of high image quality and high spatial resolution from extremely sparse (≤10) 2D projections, which is an important yet challenging and unstudied problem in sparse-view CBCT reconstruction. Unlike previous voxel-based methods that represent the CT as discrete voxels, we formulate the CT volume as a continuous intensity field, which can be regarded as a continuous function g(•) of 3D spatial points. The property of a point p in this field represents its intensity value v, i.e., v = g(p). Therefore, the reconstruction problem can be reformulated as regressing the intensity value of an arbitrary 3D point from a stack of 2D projections I, i.e., v = g(I, p). Based on the above formulation, we develop a novel reconstruction framework, namely DIF-Net (Deep Intensity Field Network). Specifically, DIF-Net first extracts feature maps from K given 2D projections. Given a 3D point, we project the point onto the 2D imaging panel of each view i by corresponding imaging parameters (distance, angle, etc.) and query its view-specific features from the feature map of view i . Then, K view-specific features from different views are aggregated by a cross-view fusion module for intensity regression. By introducing the continuous intensity field, it becomes possible to train DIF-Net with a set of sparsely sampled points to reduce memory requirement, and reconstruct the CT volume with any desired resolution during testing. Compared with NeRF-based methods [5,14,19,21,29], the design of DIF-Net shares the similar data representation (i.e., implicit neural representation) but additional training data can be introduced to help DIF-Net learn prior knowledge. Benefiting from this, DIF-Net can not only reconstruct high-resolution CT in a very short time since only inference is required for a new test sample (no retraining), but also performs much better than NeRF-based methods with extremely limited views.To summarize, the main contributions of this work include 1.) we are the first to introduce the continuous intensity field for supervised CBCT reconstruction; 2.) we propose a novel reconstruction framework DIF-Net that reconstructs CBCT with high image quality (PSNR: 29.3 dB, SSIM: 0.92) and high spatial resolution (≥256 3 ) from extremely sparse (≤10) views within 1.6 s; 3.) we conduct extensive experiments to validate the effectiveness of the proposed sparse-view CBCT reconstruction method on a clinical knee CBCT dataset."
Learning Deep Intensity Field for Extremely Sparse-View CBCT Reconstruction,2,Method,
Learning Deep Intensity Field for Extremely Sparse-View CBCT Reconstruction,2.1,Intensity Field,"We formulate the CT volume as a continuous intensity field, where the property of a 3D point p ∈ R 3 in this field represents its intensity value v ∈ R. The intensity field can be defined as a continuous function g : R 3 → R, such that v = g(p). Hence, the reconstruction problem can be reformulated as regressing the intensity value of an arbitrary point p in the 3D space from K projections I = {I 1 , I 2 , . . . , I K }, i.e., v = g(I, p). Based on the above formulation, we propose a novel reconstruction framework, namely DIF-Net, to perform efficient sparseview CBCT reconstruction, as the overview shown in Fig. 2. "
Learning Deep Intensity Field for Extremely Sparse-View CBCT Reconstruction,2.2,DIF-Net: Deep Intensity Field Network,"DIF-Net first extracts feature maps {F 1 , F 2 , . . . , F K } ⊂ R C×H×W from projections I using a shared 2D encoder, where C is the number of feature channels and H/W are height/width. In practice, we choose U-Net [18] as the 2D encoder because of its good feature extraction ability and popular applications in medical image analysis [17]. Then, given a 3D point, DIF-Net gathers its view-specific features queried from feature maps of different views for intensity regression.View-Specific Feature Querying. Considering a point p ∈ R 3 in the 3D space, for a projection view i with scanning angle α i and other imaging parameters β (distance, spacing, etc.), we project p to the 2D imaging panel of view i and obtain its 2D projection coordinates p i = ϕ(p, α i , β) ∈ R 2 , where ϕ(•) is the projection function. Projection coordinates p i are used for querying view-specific features f i ∈ R C from the 2D feature map F i of view i :where π(•) is bilinar interpolation. Similar to perspective projection, the CBCT projection function ϕ(•) can be formulated aswhere R(α i ) ∈ R 4×4 is a rotation matrix that transforms point p from the world coordinate system to the scanner coordinate system of view i , A(β) ∈ R 3×4 is a projection matrix that projects the point onto the 2D imaging panel of view i , and H : R 3 → R 2 is the homogeneous division that maps the homogeneous coordinates of p i to its Cartesian coordinates. Due to page limitations, the detailed formulation of ϕ(•) is given in the supplementary material.Cross-View Feature Fusion and Intensity Regression. Given K projection views, K view-specific features of the point p are queried from different views to form a feature listThen, the cross-view feature fusion δ(•) is introduced to gather features from F (p) and generate a 1D vector f = δ(F (p)) ∈ R C to represent the semantic features of p. In general, F (p) is an unordered feature set, which means that δ(•) should be a set function and can be implemented with a pooling layer (e.g., max/avg pooling). In our experiments, the projection angles of the training and test samples are the same, uniformly sampled from 0 • to 180 • (half rotation). Therefore, F (p) can be regarded as an ordered list (K ×C tensor), and δ(•) can be implemented by a 2-layer MLP (K → K 2 → 1) for feature aggregation. We will compare different implementations of δ(•) in the ablation study. Finally, a 4-layer MLP (C → 2C → C 2 → C 8 → 1) is applied to f for the regression of intensity value v ∈ R."
Learning Deep Intensity Field for Extremely Sparse-View CBCT Reconstruction,2.3,Network Training,"Assume that the shape and spacing of the original CT volume are H × W × D and (s h , s w , s d ) mm, respectively. During training, different from previous voxelbased methods that regard the entire 3D CT image as the supervision target, we randomly sample a set of N points {p 1 , p 2 , . . . , p N } with coordinates ranging from (0, 0, 0) to (s h H, s w W, s d D) in the world coordinate system (unit: mm) as the input. Then DIF-Net will estimate their intensity values V = {v 1 , v 2 , . . . , v N } from given projections I. For supervision, ground-truth intensity values V = {v 1 , v2 , . . . , vN } can be obtained from the ground-truth CT image based on the coordinates of points by trilinear interpolation. We choose mean-square-error (MSE) as the objective function, and the training loss can be formulated asBecause background points (62%, e.g., air) occupy more space than foreground points (38%, e.g., bones, organs), uniform sampling will bring imbalanced prediction of intensities. We set an intensity threshold 10 -5 to identify foreground and background areas by binary classification and sample N 2 points from each area for training."
Learning Deep Intensity Field for Extremely Sparse-View CBCT Reconstruction,2.4,Volume Reconstruction,"During inference, a regular and dense point set to cover all CT voxels is sampled, i.e., to uniformly sample H × W × D points from (0, 0, 0) to (s h H, s w W, s d D). Then the network will take 2D projections and points as the input and generate intensity values of sampled points to form the target CT volume. Unlike previous voxel-based methods that are limited to generating fixed-resolution CT volumes, our method enables scalable output resolutions by introducing the representation of continuous intensity field. For example, we can uniformly sample H s × W s × D s points to generate a coarse CT image but with a faster reconstruction speed, or sample sH × sW × sD points to generate a CT image with higher resolution, where s > 1 is the scaling ratio."
Learning Deep Intensity Field for Extremely Sparse-View CBCT Reconstruction,3,Experiments,"We conduct extensive experiments on a collected knee CBCT dataset to show the effectiveness of our proposed method on sparse-view CBCT reconstruction. Compared to previous works, our DIF-Net can reconstruct a CT volume with high image quality and high spatial resolution from extremely sparse (≤ 10) projections at an ultrafast speed."
Learning Deep Intensity Field for Extremely Sparse-View CBCT Reconstruction,3.1,Experimental Settings,"Dataset and Preprocessing. We collect a knee CBCT dataset consisting of 614 CT scans. Of these, 464 are used for training, 50 for validation, and 100 for testing. We resample, interpolate, and crop (or pad) CT scans to have isotropic voxel spacing of (0.8, 0.8, 0.8) mm and shape of 256 × 256 × 256. 2D projections are generated by digitally reconstructed radiographs (DRRs) at a resolution of 256 × 256. Projection angles are uniformly selected in the range of 180 • .Implementation. We implement DIF-Net using PyTorch with a single NVIDIA RTX 3090 GPU. The network parameters are optimized using stochastic gradient descent (SGD) with a momentum of 0.98 and an initial learning rate of 0.01. The learning rate is decreased by a factor of 0.001 1/400 ≈ 0.9829 per epoch, and we train the model for 400 epochs with a batch size of 4. For each CT scan, N = 10, 000 points are sampled as the input during one training iteration. For the full model, we employ U-Net [18] with C = 128 output feature channels as the 2D encoder, and cross-view feature fusion is implemented with MLP.Baseline Methods. We compare four publicly available methods as our baselines, including traditional methods FDK [6] and SART [1], NeRF-based method NAF [29], and data-driven denoising method FBPConvNet [11]. Due to the increase in dimensionality (2D to 3D), denoising methods should be equipped with 3D conv/deconvs for a dense prediction when extended to CBCT reconstruction, which leads to extremely high computational costs and low resolution (≤ 64 3 ). For a fair comparison, we use FDK to obtain an initial result and apply the 2D network for slice-wise denoising.Evaluation Metrics. We follow previous works [27][28][29] to evaluate the reconstructed CT volumes with two quantitative metrics, namely peak signal-to-noise ratio (PSNR) and structural similarity (SSIM) [24]. Higher PSNR/SSIM values represent superior reconstruction quality."
Learning Deep Intensity Field for Extremely Sparse-View CBCT Reconstruction,3.2,Results,"Performance. As shown in Table 1, we compare DIF-Net with four previous methods [1,6,22,29] under the setting of reconstruction with different output  resolutions (i.e., 128 3 , 256 3 ) and from different numbers of projection views (i.e., 6, 8, and 10). Experiments show that our proposed DIF-Net can reconstruct CBCT with high image quality even using only 6 projection views, which significantly outperforms previous works in terms of PSNR and SSIM values. More importantly, DIF-Net can be directly applied to reconstruct CT images with different output resolutions without the need for model retraining or modification. As visual results are shown in Fig. 3, FDK [6] produces results with many streaking artifacts due to lack of sufficient projection views; SART [1] and NAF [29] produce results with good shape contours but lack detailed internal information; FBPConvNet [11] reconstructs good shapes and moderate details, but there are still some streaking artifacts remaining; our proposed DIF-Net can reconstruct high-quality CT with better shape contour, clearer internal information, and fewer artifacts. More visual comparisons of the number of input views are given in the supplementary material.  Reconstruction Efficiency. As shown in Table 2, FDK [6] requires the least time for reconstruction, but has the worst image quality; SART [1] and NAF [29] require a lot of time for optimization or training; FBPConvNet [11] can reconstruct 3D volumes faster, but the quality is still limited. Our DIF-Net can reconstruct high-quality CT within 1.6 s, much faster than most compared methods.In addition, DIF-Net, which benefits from the intensity field representation, has fewer training parameters and requires less computational memory, enabling high-resolution reconstruction.Ablation Study. Tables 3 and4 show the ablative analysis of cross-view fusion strategy and the number of training points N . Experiments demonstrate that 1.) MLP performs best, but max pooling is also effective and would be a general solution when the view angles are not consistent across training/test data, as discussed in Sect. 2.2; 2.) fewer points (e.g., 5,000) may destabilize the loss and gradient during training, leading to performance degradation; 10,000 points are enough to achieve the best performance, and training with 10,000 points is much sparser than voxel-based methods that train with the entire CT volume (i.e., 256 3 or 128 3 ). We have tried to use a different encoder like pre-trained ResNet18 [8] with more model parameters than U-Net [18]. However, ResNet18 does not bring any improvement (PSNR/SSIM: 29.2/0.92), which means that U-Net is powerful enough for feature extraction in this task."
Learning Deep Intensity Field for Extremely Sparse-View CBCT Reconstruction,4,Conclusion,"In this work, we formulate the CT volume as a continuous intensity field and present a novel DIF-Net for ultrafast CBCT reconstruction from extremely sparse (≤10) projection views. DIF-Net aims to estimate the intensity value of an arbitrary point in 3D space from input projections, which means 3D CNNs are not required for feature decoding, thereby reducing memory requirement and computational cost. Experiments show that DIF-Net can perform efficient and high-quality CT reconstruction, significantly outperforming previous stateof-the-art methods. More importantly, DIF-Net is a general sparse-view reconstruction framework, which can be trained on a large-scale dataset containing various body parts with different projection views and imaging parameters to achieve better generalization ability. This will be left as our future work."
Learning Deep Intensity Field for Extremely Sparse-View CBCT Reconstruction,,Fig. 1 .,
Learning Deep Intensity Field for Extremely Sparse-View CBCT Reconstruction,,Fig. 2 .,
Learning Deep Intensity Field for Extremely Sparse-View CBCT Reconstruction,,Fig. 3 .,
Learning Deep Intensity Field for Extremely Sparse-View CBCT Reconstruction,,Table 1 .,
Learning Deep Intensity Field for Extremely Sparse-View CBCT Reconstruction,,.91 29.6/.92 30.7/.94 27.1/.89 28.3/.90 29.3/.92,
Learning Deep Intensity Field for Extremely Sparse-View CBCT Reconstruction,,Table 2 .,
Learning Deep Intensity Field for Extremely Sparse-View CBCT Reconstruction,,Table 3 .,
Learning Deep Intensity Field for Extremely Sparse-View CBCT Reconstruction,,Table 4 .,
Learning Deep Intensity Field for Extremely Sparse-View CBCT Reconstruction,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 2.
CoLa-Diff: Conditional Latent Diffusion Model for Multi-modal MRI Synthesis,1,Introduction,"Magnetic resonance imaging (MRI) is critical to the diagnosis, treatment, and follow-up of brain tumour patients [26]. Multiple MRI modalities offer complementary information for characterizing brain tumours and enhancing patient L. Jiang and Y. Mao-Contribute equally in this work.management [4,27]. However, acquiring multi-modality MRI is time-consuming, expensive and sometimes infeasible in specific modalities, e.g., due to the hazard of contrast agent [15]. Trans-modal MRI synthesis can establish the mapping from the known domain of available MRI modalities to the target domain of missing modalities, promising to generate missing MRI modalities effectively. The synthetic methods leveraging multi-modal MRI, i.e., many-to-one translation, have outperformed single-modality models generating a missing modality from another available modality, i.e., one-to-one translation [23,33]. Traditional multi-modal methods [21,22], e.g., sparse encoding-based, patch-based and atlasbased methods, rely on the alignment accuracy of source and target domains and are poorly scalable. Recent generative adversarial networks (GANs) and variants, e.g., MM-GAN [23], DiamondGAN [13] and ProvoGAN [30], have been successful based on multi-modal MRI, further improved by introducing multi-modal coding [31], enhanced architecture [7], and novel learning strategies [29].Despite the success, GAN-based models are challenged by the limited capability of adversarial learning in modelling complex multi-modal data distributions [25] Recent studies have demonstrated that GANs' performance can be limited to processing and generating data with less variability [1]. In addition, GANs' hyperparameters and regularization terms typically require fine-tuning, which otherwise often results in gradient vanish and mode collapse [2].Diffusion model (DM) has achieved state-of-the-art performance in synthesizing natural images, promising to improve MRI synthesis models. It shows superiority in model training [16], producing complex and diverse images [9,17], while reducing risk of modality collapse [12].For instance, Lyu et al. [14] used diffusion and score-marching models to quantify model uncertainty from Monte-Carlo sampling and average the output using different sampling methods for CT-to-MRI generation; Özbey et al. [19] leveraged adversarial training to increase the step size of the inverse diffusion process and further designed a cycle-consistent architecture for unpaired MRI translation.However, current DM-based methods focus on one-to-one MRI translation, promising to be improved by many-to-one methods, which requires dedicated design to balance the multiple conditions introduced by multi-modal MRI. Moreover, as most DMs operate in original image domain, all Markov states are kept in memory [9], resulting in excessive burden. Although latent diffusion model (LDM) [20] is proposed to reduce memory consumption, it is less feasible for many-to-one MRI translation with multi-condition introduced. Further, diffusion denoising processes tend to change the original distribution structure of the target image due to noise randomness [14], rending DMs often ignore the consistency of anatomical structures embedded in medical images, leading to clinically less relevant results. Lastly, DMs are known for their slow speed of diffusion sampling [9,11,17], challenging its wide clinical application.We propose a DM-based multi-modal MRI synthesis model, CoLa-Diff, which facilitates many-to-one MRI translation in latent space, and preserve anatomical structure with accelerated sampling. Our main contributions include: -present a denoising diffusion probabilistic model based on multi-modal MRI.As far as we know, this is the first DM-based many-to-one MRI synthesis model. -design a bespoke architecture, e.g., similar cooperative filtering, to better facilitate diffusion operations in the latent space, reducing the risks of excessive information compression and high-dimensional noise. -introduce structural guidance of brain regions in each step of the diffusion process, preserving anatomical structure and enhancing synthesis quality. -propose an auto-weight adaptation to balance multi-conditions and maximise the chance of leveraging relevant multi-modal information."
CoLa-Diff: Conditional Latent Diffusion Model for Multi-modal MRI Synthesis,2,Multi-conditioned Latent Diffusion Model,"Figure 1 illustrates the model design. As a latent diffusion model, CoLa-diff integrates multi-condition b from available MRI contrasts in a compact and low-dimensional latent space to guide the generation of missing modality x ∈ R H×W ×1 . Precisely, b constitutes available contrasts and anatomical structure masks generated from the available contrasts. Similar to [9,20], CoLa-Diff invovles a forward and a reverse diffusion process. During forward diffusion, x 0 is encoded by E to produce κ 0 , then subjected to T diffusion steps to gradually add noise and generate a sequence of intermediate representations: {κ 0 , . . . , κ T }.The t-th intermediate representation is denoted as κ t , expressed as:where ᾱt = t i=1 α i , α i denotes hyper-parameters related to variance. The reverse diffusion is modelled by a latent space network with parameters θ, inputting intermediate perturbed feature maps κ t and y (compressed b) to predict noise level θ (κ t , t, y) for recovering feature maps κt-1 from previous,To enable effective learning of the underlying distribution of κ 0 , the noise level needs to be accurately estimated. To achieve this, the network employs similar cooperative filtering and auto-weight adaptation strategies. κ0 is recovered by repeating Eq. 2 process for t times, and decoding the final feature map to generate synthesis images x0 ."
CoLa-Diff: Conditional Latent Diffusion Model for Multi-modal MRI Synthesis,2.1,Latent Space Network,"We map multi-condition to the latent space network for guiding noise prediction at each step t. The mapping is implemented by N transformer blocks (Fig. 1 (D)), including global self-attentive layers, layer-normalization and position-wise MLP. Following the latent diffusion model (LDM) [20], the network θ (κ t , t, y) is trained to predict the noise added at each step using(3)To mitigate the excessive information losses that latent spaces are prone to, we replace the simple convolution operation with a residual-based block (three sequential convolutions with kernels 1 * 1, 3 * 3, 1 * 1 and residual joins [8]), and enlarge the receptive field by fusion (5 * 5 and 7 * 7 convolutions followed by AFF [6]) in the down-sampling section. Moreover, to reduce high-dimensional noise generated in the latent space, which can significantly corrupt the quality of multi-modal generation. We design a similar cooperative filtering detailed below.Similar Cooperative Filtering. The approach has been devised to filter the downsampled features, with each filtered feature connected to its respective upsampling component (shown in Fig. 1 (F)). Given f , which is the downsampled feature of κ t , suppose the 2D discrete wavelet transform φ [24] decomposes the features into low frequency component f (i)A and high frequency componentsA , where i is the number of wavelet transform layers. Previous work [5] has shown to effectively utilize global information by considering similar patches. However, due to its excessive compression, it is less suitable for LDM. Here, we group the components and further filter by similar block matching δ [18] or thresholding γ, use the inverse wavelet transform φ -1 (•) to reconstruct the denoising results, given f * ."
CoLa-Diff: Conditional Latent Diffusion Model for Multi-modal MRI Synthesis,2.2,Structural Guidance,"Unlike natural images, medical images encompass rich anatomical information. Therefore, preserving anatomical structure is crucial for MRI generation. However, DMs often corrupt anatomical structure, and this limitation could be due to the learning and sampling processes of DMs that highly rely on the probability density function [9], while brain structures by nature are overlapping in MRI density distribution and even more complicated by pathological changes. Previous studies show that introducing geometric priors can significantly improve the robustness of medical image generation. [3,28]. Therefore, we hypothesize that incorporating structural prior could enhance the generation quality with preserved anatomy. Specifically, we exploit FSL-FAST [32] tool to segment four types of brain tissue: white matter, grey matter, cerebrospinal fluid, and tumour. The generated tissue masks and inherent density distributions (Fig. 1 (E)) are then used as a condition y i to guide the reverse diffusion.The combined loss function for our multi-conditioned latent diffusion is defined aswhere KL is the KL divergence loss to measure similarity between real q and predicted p θ distributions of encoded images.where D KL is the KL divergence function."
CoLa-Diff: Conditional Latent Diffusion Model for Multi-modal MRI Synthesis,2.3,Auto-Weight Adaptation,"It is critical to balance multiple conditions, maximizing relevant information and minimising redundant information. For encoded conditions y ∈ R h×w×c , c is the number of condition channels. Set the value after auto-weight adaptation to ỹ, the operation of this module is expressed as (shown in Fig. 1 (E))The embedding outputs are adjusted by embedding weight μ. The autoactivation is governed by the learnable weight ν and bias o. where is a small constant added to the equation to avoid the issue of derivation at the zero point. The normalization method can establish stable competition between channels, G = {G c } S c=1 . We use L 2 normalization for cross-channel operations:where S denotes the scale. We use an activation mechanism for updating each channel to facilitate the maximum utilization of each condition during diffusion model training, and further enhance the synthesis performance. Given the learnable weightwhich gives new representations ỹc of each compressed conditions after the automatic weighting. S(•) denotes the Sigmoid activation function."
CoLa-Diff: Conditional Latent Diffusion Model for Multi-modal MRI Synthesis,3,Experiments and Results,
CoLa-Diff: Conditional Latent Diffusion Model for Multi-modal MRI Synthesis,3.1,Comparisons with State-of-the-Art Methods,"Datasets and Baselines. We evaluated CoLa-Diff on two multi-contrast brain MRI datasets: BRATS 2018 and IXI datasets. The BRATS 2018 contains MRI scans from 285 glioma patients. Each includes four modalities: T1, T2, T1ce, and FLAIR. We split them into (190:40:55) for training/validation/testing. For each subject, we automatically selected axial cross-sections based on the perceptible effective area of the slices, and then cropped the selected slices to a size of 224 × 224. The IXI1 dataset consists of 200 multi-contrast MRIs from healthy brains, plit them into (140:25:35) for training/validation/testing. For preprocessing, we registered T2-and PD-weighted images to T1-weighted images using FSL-FLIRT [10], and other preprocessing are identical to the BRATS 2018. We compared CoLa-Diff with four state-of-the-art multi-modal MRI synthesis methods: MM-GAN [23], Hi-Net [33], ProvoGan [30] and LDM [20]. Implementation Details. Our code is publicly available at https://github. com/SeeMeInCrown/CoLa Diff MultiModal MRI Synthesis. The hyperparameters of CoLa-Diff are defined as follows: diffusion steps to 1000; noise schedule to linear; attention resolutions to 32, 16, 8; batch size to 8, learning rate to 9.6e -5.The noise variances were in the range of β 1 = 10 -4 and β T = 0.02. An exponential moving average (EMA) over model parameters with a rate of 0.9999 was employed. The model is trained on 2 NVIDIA RTX A5000, 24 GB with Adam optimizer on PyTorch. An acceleration method [11] based on knowledge distillation was applied for fast sampling. Quantitative Results. We performed synthesis experiments for all modalities, with each modality selected as the target modality while remaining modalities and the generated region masks as conditions. Seven cases were tested in two datasets (Table 1). The results show that CoLa-Diff outperforms other models by up to 6.01 dB on PSNR and 5.74% on SSIM. Even when compared to the best of other models in each task, CoLa-Diff is a maximum of 0.81 dB higher in PSNR and 0.82% higher in SSIM. Qualitative Results. The first three and last three rows in Fig. 2 illustrate the synthesis results of T1ce from BRATS and PD from the IXI, respectively. From the generated images, we observe that CoLa-Diff is most comparable to the ground truth, with fewer errors shown in the heat maps. The synthesis uncertainty for each region is derived by performing 100 generations of the same slice and calculating the pixel-wise variance. From the uncertainty maps, CoLa-Diff is more confident in synthesizing the gray and white matter over other comparison models. Particularly, CoLa-Diff performs better in generating complex brain sulcus and tumour boundaries. Further, CoLa-Diff could better maintain the anatomical structure over comparison models."
CoLa-Diff: Conditional Latent Diffusion Model for Multi-modal MRI Synthesis,3.2,Ablation Study and Multi-modal Exploitation Capabilities,"We verified the effectiveness of each component in CoLa-Diff by removing them individually. We experimented on BRATS T1+T1ce+FLAIR→T2 task with four absence scenarios (Table 2 top). Our results show that each component contributes to the performance improvement, with Auto-weight adaptation bringing a PSNR increase of 1.9450dB and SSIM of 4.0808%.To test the generalizability of CoLa-Diff under the condition of varied inputs, we performed the task of generating T2 on two datasets with progressively increasing input modalities (Table 2 bottom). Our results show that our model performance increases with more input modalities: SSIM has a maximum uplift value of 1.9603, PSNR rises from 26.6355 dB to 28.3126 dB in BRATS; from 32.164 dB to 32.8721 dB in IXI. The results could further illustrate the ability of CoLa-Diff to exploit multi-modal information. "
CoLa-Diff: Conditional Latent Diffusion Model for Multi-modal MRI Synthesis,4,Conclusion,"This paper presents CoLa-Diff, a DM-based multi-modal MRI synthesis model with a bespoke design of network backbone, similar cooperative filtering, structural guidance and auto-weight adaptation. Our experiments support that CoLa-Diff achieves state-of-the-art performance in multi-modal MRI synthesis tasks. Therefore, CoLa-Diff could serve as a useful tool for generating MRI to reduce the burden of MRI scanning and benefit patients and healthcare providers."
CoLa-Diff: Conditional Latent Diffusion Model for Multi-modal MRI Synthesis,,Fig. 1 .,
CoLa-Diff: Conditional Latent Diffusion Model for Multi-modal MRI Synthesis,,,
CoLa-Diff: Conditional Latent Diffusion Model for Multi-modal MRI Synthesis,,Fig. 2 .,
CoLa-Diff: Conditional Latent Diffusion Model for Multi-modal MRI Synthesis,,Table 1 .,
CoLa-Diff: Conditional Latent Diffusion Model for Multi-modal MRI Synthesis,,32.24±2.95 96.95±2.26 30.20±2.38 94.49±2.15 32.86±2.83 96.57±2.27,
CoLa-Diff: Conditional Latent Diffusion Model for Multi-modal MRI Synthesis,,Table 2 .,
Unified Brain MR-Ultrasound Synthesis Using Multi-modal Hierarchical Representations,1,Introduction,"Medical imaging is essential during diagnosis, surgical planning, surgical guidance, and follow-up for treating brain pathology. Images from multiple modalities are typically acquired to distinguish clinical targets from surrounding tissues. For example, intra-operative ultrasound (iUS) imaging and Magnetic Resonance Imaging (MRI) capture complementary characteristics of brain tissues that can be used to guide brain tumor resection. However, as noted in [30], multi-modal data is expensive and sparse, typically leading to incomplete sets of images. For example, the prohibitive cost of intra-operative MRI (iMRI) scanners often hampers the acquisition of iMRI during surgical procedures. Conversely, iUS is an affordable tool but has been perceived as difficult to read compared to iMRI [5]. Consequently, there is growing interest in synthesizing missing images from a subset of available images for enhanced visualization and clinical training.Medical image synthesis aims to predict missing images given available images. Deep-learning based methods have reached the highest level of performance [29], including conditional generative adversarial (GAN) models [6,14,15,21] and conditional variational auto-encoders [3]. However, a key limitation of these techniques is that they must be trained for each subset of available images.To tackle this challenge, unified approaches have been proposed. These approaches are designed to have the flexibility to handle incomplete image sets as input, improving practicality as only one network is used for generating missing images. To handle partial inputs, some studies proposed to fill missing images with arbitrary values [4,17,18,24]. Alternatively, other work aim at creating a common feature space that encodes shared information from different modalities. Feature representations are extracted independently for each modality. Then, arithmetic operations (e.g., mean [7,11,28], max [2] or a combination of sum, product and max [32]) are used to fuse these feature representations. However, these operations do not force the network to learn a shared latent representation of multi-modal data and lack theoretical foundations. In contrast, Multi-modal Variational Auto-Encoders (MVAEs) provide a principled probabilistic fusion operation to create a common representation space [8,30]. In MVAEs, the common representation space is low-dimensional (e.g., R 256 ), which usually leads to blurry synthetic images. In contrast, hierarchical VAEs (HVAEs) [19,22,26,27] allow for learning complex latent representations by using a hierarchical latent structure, where the coarsest latent variable (z L ) represents global features, as in MVAEs, while the finer variables capture local characteristics. However, HVAEshave not yet been extended to multi-modal settings to synthesize missing images.In this work, we introduce Multi-Modal Hierarchical Latent Representation VAE (MHVAE), the first multi-modal VAE approach with a hierarchical latent representation for unified medical image synthesis. Our contribution is four-fold.First, we integrate a hierarchical latent representation into the multi-modal variational setting to improve the expressiveness of the model. Second, we propose a principled fusion operation derived from a probabilistic formulation to support missing modalities, thereby enabling image synthesis. Third, adversarial learning is employed to generate realistic image synthesis. Finally, experiments on the challenging problem of iUS and MR synthesis demonstrate the effectiveness of the proposed approach, enabling the synthesis of high-quality images while establishing a mathematically grounded formulation for unified image synthesis and outperforming non-unified GAN-based approaches and the state-of-the-art method for unified multi-modal medical image synthesis."
Unified Brain MR-Ultrasound Synthesis Using Multi-modal Hierarchical Representations,2,Background,"Variational Auto-Encoders (VAEs). The goal of VAEs [16] is to train a generative model in the form of p(x, z) = p(z)p(x|z) where p(z) is a prior distribution (e.g. isotropic Normal distribution) over latent variables z ∈ R H and where p θ (x|z) is a decoder parameterized by θ that reconstructs data x ∈ R N given z. The latent space dimension H is typically much lower than the image space dimension N , i.e. H N . The training goal with respect to θ is to maximize the marginal likelihood of the data p θ (x) (the ""evidence""); however since the true posterior p θ (z|x) is in general intractable, the variational evidence lower bound (ELBO) is instead optimized. The ELBO L VAE (x; θ, φ) is defined by introducing an approximate posterior q φ (z|x) with parameters φ:where KL[q||p] is the Kullback-Leibler divergence between distributions q and p."
Unified Brain MR-Ultrasound Synthesis Using Multi-modal Hierarchical Representations,,Multi-modal Variational Auto-Encoders (MVAE).,"Multi-modal VAEs [8,25,30] introduced a principled probabilistic formulation to support missing data at training and inference time. Multi-modal VAEs assume that M paired images x = (x 1 , ..., x M ) ∈ R M ×N are conditionally independent given a shared representation z as higlighted in Fig. 1, i.e. p θ (x|z) = M i=1 p(x i |z). Instead of training one single variational network q φ (z|x) that requires all images to be presented at all times, MVAEs factorize the approximate posterior as a combination of unimodal variational posteriors (q φ (z|x i )) M i=1 . Given any subset of modalities π ⊆ {1, ..., M }, MVAEs have the flexibility to approximate the π-marginal posteriors p(z|(x i ) i∈π ) using the |π| unimodal variational posteriors (q φ (z|x i )) i∈π . MVAE [30] and U-HVED [8] factorize the π-marginal variational posterior as a product-of-experts (PoE), i.e.:(2)"
Unified Brain MR-Ultrasound Synthesis Using Multi-modal Hierarchical Representations,3,Methods,"In this paper, we propose a deep multi-modal hierarchical VAE called MHVAE that synthesizes missing images from available images. MHVAE's design focuses on tackling three challenges: (i) improving expressiveness of VAEs and MVAEs using a hierarchical latent representation; (ii) parametrizing the variational posterior to handle missing modalities; (iii) synthesizing realistic images."
Unified Brain MR-Ultrasound Synthesis Using Multi-modal Hierarchical Representations,3.1,Hierarchical Latent representation,"be a complete set of paired (i.e. co-registered) images of different modalities where M is the total number of image modalities and N the number of pixels (e.g. M = 2 for T 2 MRI and iUS synthesis). The images x i are assumed to be conditionally independent given a latent variable z. Then, the conditional distribution p θ (x|z) parameterized by θ can be written as:Given that VAEs and MVAEs typically produce blurry images, we propose to use a hierarchical representation of the latent variable z to increase the expressiveness the model as in HVAEs [19,22,26,27]. Specifically, the latent variable z is partitioned into disjoint groups, as shown in Fig. 1 i.e. z = {z 1 , ...z L }, where L is the number of groups. The prior p(z) is then represented by:where p(z L ) = N (z L ; 0, I) is an isotropic Normal prior distribution and the conditional prior distributions p θ l (z l |z >l ) are factorized Normal distributions with diagonal covariance parameterized using neural networks, i.e. p θ l (z l |z >l ) = N (z l ; μ θ l (z >l ), D θ l (z >l )). Note that the dimension of the finest latent variable z 1 ∈ R H1 is similar to number of pixels, i.e. H 1 = O(N ) and the dimension of the latent representation exponentially decreases with the depth, i.e. H L H 1 . Reusing Eq. 1, the evidence log (p θ (x)) is lower-bounded by the tractable variational ELBO L ELBO MHVAE (x; θ, φ):where q φ (z|x) = L l=1 q φ (z l |x, z >l ) is a variational posterior that approximates the intractable true posterior p θ (z|x)."
Unified Brain MR-Ultrasound Synthesis Using Multi-modal Hierarchical Representations,3.2,Variational Posterior's Parametrization for Incomplete Inputs,"To synthesize missing images, the variational posterior (q φ (z l |x, z >l )) L l=1 should handle missing images. We propose to parameterize it as a combination of unimodal variational posteriors. Similarly to MVAEs, for any set π ⊆ {1, ..., M } of images, the conditional posterior distribution at the coarsest level L is expressedwhere p(z L ) = N (z L ; 0, I) is an isotropic Normal prior distribution and q φL (z|x i ) is a Normal distribution with diagonal covariance parameterized using CNNs.For the other levels l ∈ {1, .., L -1}, we similarly propose to express the conditional variational posterior distributions as a product-of-experts:where q φ i l (z l |x i , z >l ) is a Normal distribution with diagonal covariance parameterized using CNNs, i.e.). This formulation allows for a principled operation to fuse content information from available images while having the flexibility to handle missing ones. Indeed, at each level l ∈ {1, ..., L}, the conditional variational distributions q PoE φ l ,θ l (z l |x π , z >l ) are Normal distributions with mean μ φ l ,θ l (x π , z >l ) and diagonal covariance D φ l ,θ l (x π , z >l ) expressed in closed-form solution [12] as:with D θL (z >L ) = I and μ θL (z >L ) = 0."
Unified Brain MR-Ultrasound Synthesis Using Multi-modal Hierarchical Representations,3.3,Optimization Strategy for Image Synthesis,"The joint reconstruction and synthesis optimization goal is to maximize the expected evidence E x∼p data [log(p(x))]. As the ELBO defined in Eq. 5 is valid for any approximate distribution q, the evidence, log(p θ (x)), is in particular lowerbounded by the following subset-specific ELBO for any subset of images π:Hence, the expected evidence E x∼p data [log(p(x))] is lower-bounded by the average of the subset-specific ELBO, i.e.:Consequently, we propose to average all the subset-specific losses at each training iteration. The image decoding distributions are modelled as Normal with variance σ, i.e. p θ (x i |z 1 ) = N (x i ; μ i (z 1 ), σI), leading to reconstruction losses log(p θ (x i |z 1 )), which are proportional to ||x iμ i (z 1 )|| 2 . To generate sharper images, the L 2 loss is replaced by a combination of L 1 loss and GAN loss via a PatchGAN discriminator [14]. Moreover, the expected KL divergences are estimated with one sample as in [19]. Finally, the loss associated with the subsetspecific ELBOs Eq. ( 9) is:Following standard practices [4,14], images are normalized in [-1, 1] and the weights of the L 1 and GAN losses are set to λ L1 = 100 and λ GAN = 1."
Unified Brain MR-Ultrasound Synthesis Using Multi-modal Hierarchical Representations,4,Experiments,"In this section, we report experiments conducted on the challenging problem of MR and iUS image synthesis.Data. We evaluated our method on a dataset of 66 consecutive adult patients with brain gliomas who were surgically treated at the Brigham and Women's hospital, Boston USA, where both pre-operative 3D T2-SPACE and pre-dural opening intraoperative US (iUS) reconstructed from a tracked handheld 2D probe were acquired. The data will be released on TCIA in 2023. 3D T2-SPACE scans were affinely registered with the pre-dura iUS using NiftyReg [20] following the pipeline described in [10]. Three neurological experts manually checked registration outputs. The dataset was randomly split into a training set (N = 56) and a testing set (N = 10). Images were resampled to an isotropic 0.5 mm resolution, padded for an in-plane matrix of (192,192)  using SPADE [21], Pix2Pix [14], MVAE [30], ResViT [4] and MHVAE (ours) without and with GAN loss. As highlighted by the arrows, our approach better preserves anatomy compared to GAN-based approach and produces more realistic approach than the transformer-based approach (ResViT).for the encoder and decoder from MobileNetV2 [23] are used with Squeeze and Excitation [13] and Swish activation. The image decoders (μ i ) M i=1 correspond to 5 ResNet blocks. Following state-of-the-art bidirectional inference architectures [19,27], the representations extracted in the contracting path (from x i to (z l ) l ) and the expansive path (from z L to x i and (z l ) l<L ) are partially shared. Models are trained for 1000 epochs with a batch size of 16. To improve convergence, λ GAN is set to 0 for the first 800 epochs. Network architecture is presented in Appendix, and the code is available at https://github.com/ReubenDo/ MHVAE.Evaluation. Since paired data was available for evaluation, standard supervised evaluation metrics are employed: PSNR (Peak Signal-to-Noise Ratio), SSIM (Structural Similarity), and LPIPS [31] (Learned Perceptual Image Patch Similarity). Quantitative results are presented in Table 1, and qualitative results are shown in Fig. 2. Wilcoxon signed rank tests (p < 0.01) were performed.Ablation Study. To quantify the importance of each component of our approach, we conducted an ablation study. First, our model (MHVAE) was compared with MVAE, the non-hierarchical multi-modal VAE described in [30]. It can be observed in Table 1 that MHVAE (ours) significantly outperformed MVAE. This highlights the benefits of introducing a hierarchy in the latent representation. As shown in Fig. 2, MVAE generated blurry images, while our approach produced sharp and detailed synthetic images. Second, the impact of the GAN loss was evaluated by comparing our model with (λ GAN = 0) and without (λ GAN = 1) the adversarial loss. Both models performed similarly in terms of evaluation metrics. However, as highlighted in Fig. 2, adding the GAN loss led to more realistic textures with characteristic iUS speckles on synthetic iUS. Finally, the image similarity between the target and reconstructed images (i.e., target image used as input) was excellent, as highlighted in Table 1. This shows that the learned latent representations preserved the content information from input modalities. State-of-the-Art Comparison. To evaluate the performance of our model (MHVAE) against existing image synthesis frameworks, we compared it to two state-of-the-art GAN-based conditional image synthesis methods: Pix2Pix [14] and SPADE [21]. These models have especially been used as synthesis backbones in previous MR/iUS synthesis studies [6,15]. Results in Table 1 show that our approach statistically outperformed these GAN methods with and without adversarial learning. As shown in Fig. 2, these conditional GANs produced realistic images but did not preserve the brain anatomy. Given that these models are not unified, Pix2Pix and SPADE must be trained for each synthesis direction (T 2 → iUS and iUS → T 2 ). In contrast, MHVAE is a unified approach where one model is trained for both synthesis directions, improving inference practicality without a drop in performance. Finally, we compared our approach with ResViT [4], a transformer-based method that is the current state-of-the-art for unified multi-modal medical image synthesis. Our approach outperformed or reached similar performance depending on the metric. In particular, as shown in Fig. 2 and in Table 1 for the perceptual LPIPS metric, our GAN model synthesizes images that are visually more similar to the target images. Finally, our approach demonstrates significantly lighter computational demands when compared to the current SOTA unified image synthesis framework (ResViT), both in terms of time complexity (8G MACs vs. 487G MACs) and model size (10M vs. 293M parameters). Compared to MVAEs, our hierarchical multi-modal approach only incurs a marginal increase in time complexity (19%) and model size (4%).Overall, this set of experiments demonstrates that variational auto-encoders with hierarchical latent representations, which offer a principled formulation for fusing multi-modal images in a shared latent representation, are effective for image synthesis."
Unified Brain MR-Ultrasound Synthesis Using Multi-modal Hierarchical Representations,5,Discussion and Conclusion,"Other Potential Applications. The current framework enables the generation of iUS data using T 2 MRI data. Since image delineation is much more efficient on MRI than on US, annotations performed on MRI could be used to train a segmentation network on pseudo-iUS data, as performed by the top-performing teams in the crossMoDA challenge [9]. For example, synthetic ultrasound images could be generated from the BraTS dataset [1], the largest collection of annotated brain tumor MR scans. Qualitative results shown in Appendix demonstrate the ability of our approach to generalize well to T 2 imaging from BraTS. Finally, the synthetic images could be used for improved iUS and T 2 image registration."
Unified Brain MR-Ultrasound Synthesis Using Multi-modal Hierarchical Representations,,Conclusion and Future Work.,"We introduced a multi-modal hierarchical variational auto-encoder to perform unified MR/iUS synthesis. By approximating the true posterior using a combination of unimodal approximates and optimizing the ELBO with multi-modal and uni-modal examples, MHVAE demonstrated state-of-the-art performance on the challenging problem of iUS and MR synthesis. Future work will investigate synthesizing additional imaging modalities such as CT and other MR sequences."
Unified Brain MR-Ultrasound Synthesis Using Multi-modal Hierarchical Representations,,Fig. 1 .,
Unified Brain MR-Ultrasound Synthesis Using Multi-modal Hierarchical Representations,,Fig. 2 .,
Unified Brain MR-Ultrasound Synthesis Using Multi-modal Hierarchical Representations,,,"L ) were set to 1 × 1 and 256. The spatial and feature dimensions are respectively doubled and halved after each level to reach a feature representation of dimension 8 for each pixel, i.e. z 1 ∈ R 196×196×8 and z L ∈ R 1×1×256 . This leads to 7 latent variable levels, i.e. L = 7. Following state-of-the-art NVAE architecture[27], residual cells"
Unified Brain MR-Ultrasound Synthesis Using Multi-modal Hierarchical Representations,,Table 1 .,
Unified Brain MR-Ultrasound Synthesis Using Multi-modal Hierarchical Representations,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 43.
Make-A-Volume: Leveraging Latent Diffusion Models for Cross-Modality 3D Brain MRI Synthesis,1,Introduction,"Medical images are essential in diagnosing and monitoring various diseases and patient conditions. Different imaging modalities, such as computed tomography (CT) and magnetic resonance imaging (MRI), and different parametric images, such as T1 and T2 MRI, have been developed to provide clinicians with a comprehensive understanding of the patients from multiple perspectives [7]. However, in clinical practice, it is commonly difficult to obtain a complete set of multiple modality images for diagnosis and treatment due to various reasons, such as modality corruption, incorrect machine settings, allergies to specific contrast agents, and limited available time [5,10]. Therefore, cross-modality medical image synthesis is useful by allowing clinicians to acquire different characteristics across modalities and facilitating real-world applications in radiology and radiation oncology [28,32].With the rise of deep learning, numerous studies have emerged and are dedicated to medical image synthesis [4,7,18]. Notably, generative adversarial networks (GANs) [8] based approaches have garnered significant attention in this area due to their success in image generation and image-to-image translation [11,33]. Moreover, GANs are also closely related to cross-modality medical image synthesis [2,10,32]. However, despite their efficacy, GANs are susceptible to mode collapse and unstable training, which can negatively impact the performance of the model and decrease the reliability in practice [1,17]. Recently, the advent of denoising diffusion probabilistic models (DDPMs) [9,24] has introduced a new scheme for high-quality generation, offering desirable features such as better distribution coverage and more stable training when compared to GAN-based counterparts. Benefiting from the better performance [6], diffusionbased models may be deemed much more reliable and dominant and recently researchers have made the first attempts to employ diffusion models for medical image synthesis [12][13][14]19].Different from natural images, most medical images are volumetric. Previous studies employ 2D networks as backbones to synthesize slices of medical volumetric data due to their ease of training [18,32] and then stack 2D results for 3D synthesis. However, this fashion induces volumetric inconsistency, particularly along the z-axis when following the standard way of placing the coordinate system. Although training 3D models may avoid this issue, it is challenging and impractical due to the massive amount of volumetric data required, and the higher dimension of the data would result in costly memory requirements [3,16,26]. To sum up, balancing the trade-off between training and volumetric consistency remains an open question that requires further investigation.In this paper, we propose Make-A-Volume, a diffusion-based pipeline for cross-modality 3D brain MRI synthesis. Inspired by recent works that factorize video generation into multiple stages [23,31], we introduce a new paradigm for volumetric medical data synthesis by leveraging 2D backbones to simultaneously facilitate high-fidelity cross-modality synthesis and mitigate volumetric inconsistency for medical data. Specifically, we employ a latent diffusion model (LDM) [20] to function as a slice-wise mapping that learns cross-modality trans-lation in an image-to-image manner. Benefiting from the low-dimensional latent space of LDMs, the high memory requirements for training are mitigated. To enable the 3D image synthesis and enhance volumetric smoothness among medical slices, we further insert and fine-tune a series of volumetric layers to upgrade the slice-wise model to a volume-wise model. In summary, our contributions are three-fold: (1) We introduce a generic paradigm for 3D image synthesis with 2D backbones, which can mitigate volumetric inconsistency and training difficulty related to 3D backbones. (2) We propose an efficient latent diffusion-based framework for high-fidelity cross-modality 3D medical image synthesis. (3) We collected a large-scale high-quality dataset of paired susceptibility weighted imaging (SWI) and magnetic resonance angiography (MRA) brain images. Experiments on these in-house and public T1-T2 brain MRI datasets show the volumetric consistency and superior quantitative result of our framework."
Make-A-Volume: Leveraging Latent Diffusion Models for Cross-Modality 3D Brain MRI Synthesis,,Fig. 1. Overview of our proposed two-stage Make-A-Volume framework.,"A latent diffusion model is used to predict the noises added to the image and synthesize independent slices from Gaussian noises. We insert volumetric layers and quickly finetune the model, which extends the slice-wise model to be a volume-wise model and enables synthesizing volumetric data from Gaussian noises."
Make-A-Volume: Leveraging Latent Diffusion Models for Cross-Modality 3D Brain MRI Synthesis,2,Method,
Make-A-Volume: Leveraging Latent Diffusion Models for Cross-Modality 3D Brain MRI Synthesis,2.1,Preliminaries of DDPMs,"In the diffusion process, DDPMs produce a series of noisy inputs x 0 , x 1 , ..., x T , via sequentially adding Gaussian noises to the sample over a predefined number of timesteps T . Formally, given clean data samples which follow the real distribution x 0 ∼ q(x), the diffusion process can be written down with variances β 1 , ..., β T asEmploying the property of DDPMs, the corrupted data x t can be sampled easily from x 0 in a closed form:where α t = 1β t , ᾱt = t s=1 α s , and ∼ N (0, 1) is the added noise. In the reverse process, the model learns a Markov chain process to convert the Gaussian distribution into the real data distribution by predicting the parameterized Gaussian transition p(x t-1 |x t ) with the learned model θ:In the model training, the model tries to predict the added noise with the simple mean squared error (MSE) loss:"
Make-A-Volume: Leveraging Latent Diffusion Models for Cross-Modality 3D Brain MRI Synthesis,2.2,Slice-Wise Latent Diffusion Model,"To improve the computational efficiency of DDPMs that learn data in pixel space, Rombach et al. [20] proposes training an autoencoder with a KL penalty or a vector quantization layer [15,27], and introduces the diffusion model to learn the latent distribution. Given calibrated source modality image x c and target modality image x, we leverage a slice-wise latent diffusion model to learn the cross-modality translation. With the pretrained encoder E, x c and x are compressed into a spatially lower-dimensional latent space of reduced complexity, generating z c and z. The diffusion and denoising processes are then implemented in the latent space and a U-Net [21] is trained to predict the noise in the latent space. The input consists of the concatenated z c and z and the network learns the parameterized Gaussian transition p θ (z t-1 |z t , z c ) = N (z t-1 ; μ θ (z t , t, z c ), σ 2 t I). After learning the latent distribution, the slice-wise model can synthesize target latent ẑ from Gaussian noise, given the source latent z c . Finally, the decoder D restores the slice to the image space via x = D(ẑ)."
Make-A-Volume: Leveraging Latent Diffusion Models for Cross-Modality 3D Brain MRI Synthesis,2.3,From Slice-Wise Model to Volume-Wise Model,"Figure 1 illustrates an overview of the Make-A-Volume framework. The first stage involves a latent diffusion model that learns the cross-modality translation in an image-to-image manner to synthesize independent slices from Gaussian noises. Then, to extend the slice-wise model to be a volume-wise model, we insert volumetric layers and quickly fine-tune the U-Net. As a result, the volume-wise model synthesizes volumetric data without inconsistency from Gaussian noises.In the slice-wise model, distribution of the latent z ∈ R bs×c×h×w is learned by the U-Net, where b s , c, h, w are the batch size of slice, channels, height, and width dimensions respectively, and there is where little volume-awareness is introduced to the network. Since we target in synthesizing volumetric data and assume each volume consists of N slices, we can factorize the batch size of slices as b s = b v n, where B v represents the batch size of volumes. Now, volumetric layers are injected and help the U-Net learn to latent feature f ∈ R (bv×n)×c×h×w with volumetric consistency. The volumetric layers are basic 1D convolutional layers and the i-th volumetric layer l i v takes in feature f and outputs f as:Here, the 1D conv layers combined with the pretrained 2D conv layers, serve as pseudo 3D conv layers with little extra memory cost. We initialize the volumetric 1D convolution layers as Identity Functions for more stable training and we empirically find tuning is efficient. With the volume-aware network, the model learns volume data {x i } n i=1 , predicts {z i } n i=1 , and reconstruct {x i } n i=1 . For diffusion model training, in the first stage, we randomly sample timestep t for each slice. However, when tuning the second stage, the U-Net with volumetric layers learns the relationship between different slices in one volume. As a result, fixing t for each volume data is necessary and we encourage the small t values to be sampled more frequently for easy training. In detail, we sample the timestep t with replacement from multinomial distribution, and the pre-normalized weight (used for computing probabilities after normalization) for timestep t equals 2Tt, where T is the total number of timesteps. Therefore, we enable a seamless translation from the slice-wise model which processes slices individually, to a volume-wise model with better volumetric consistency."
Make-A-Volume: Leveraging Latent Diffusion Models for Cross-Modality 3D Brain MRI Synthesis,3,Experiments,"Datasets. The experiments were conducted on two brain MRI datasets: SWIto-MRA (S2M) dataset and RIRE [30] 1 T1-to-T2 dataset. To facilitate SWIto-MRA brain MRI synthesis applications, we collected a high-quality SWI-to-MRA dataset. This dataset comprises paired SWI and MRA volume data of 111 patients that were acquired at Qilu Hospital of Shandong University using one 3.0T MRI scanner (i.e., Verio from Siemens). The SWI scans have a voxel spacing of 0.3438 × 0.3438 × 0.8 mm and the MRA scans have a voxel spacing of 0.8984 × 0.8984×2.0 mm. While most public brain MRI datasets lack high-quality details along z-axis and therefore are weak to indicate volumetric inconsistency, this volume data provides a good way to illustrate the performances for volumetric synthesis due to the clear blood vessels. We also evaluate our method on the public RIRE dataset [30]. The RIRE dataset includes T1 and T2-weighted MRI volumes, and 17 volumes were used in the experiments.Implementation Details. To summarize, for the S2M dataset, we randomly select 91 paired volumes for training and 20 paired volumes for inference; for the RIRE T1-to-T2 dataset, 14 volumes are randomly selected for training and 3 volumes are used for inference. All the volumes are resized to 256 × 256 × 100 for S2M and 256 × 256 × 35 for RIRE, where the last dimension represents the z-axis dimension, i.e., the number of slices in one volume for 2D image-to-image setting. Our proposed method is built upon U-Net backbones. We use a pretrained KL autoencoder with a downsampling factor of f = 4. We train our model on an NVIDIA A100 80 GB GPU. Quantitative Results. We compare our pipeline to several baseline methods, including 2D-based methods: (1) Pix2pix [11], a solid baseline for image-to-image translation;(2) Palette [22], a diffusion-based method for 2D image translation; 3D-based methods: (3) a 3D version of Pix2pix, created by modifying the 2D backbone as a 3D backbone in the naive Pix2pix approach; and (4) a 3D version of CycleGAN [33]. Naive 3D diffusion-based models are not included due to the lack of efficient backbones and the matter of timesteps' sampling efficiency. We report the results in terms of mean absolute error (MAE), Structural Similarity Index (SSIM) [29], and peak signal-to-noise ratio (PSNR). Table 1 presents a quantitative comparison of our method and baseline approaches on the S2M and RIRE datasets. Our method achieves better performance than the baselines in terms of various evaluation metrics. To accelerate the sampling of diffusion models, we implement DDIM [25] with 200 steps and report the results accordingly. It is worth noting that for the baseline approaches, the 3D version method (Pix2pix 3D) outperforms the corresponding 2D version (Pix2pix) at the cost of additional memory usage. For the Palette method, we implemented the 2D version but were unable to produce high-quality slices stably and failure cases dramatically affected the metrics results. Nonetheless, we included this method due to its great illustration of volumetric inconsistency.Qualitative Results. Figure 2 presents a qualitative comparison of different methods, showcasing two axial slices of clear vessels. Our method synthesizes better images with more details, as shown in the qualitative results. The areas requiring special attention are highlighted with red arrows and red rectangles. It is worth noting that the synthesized axial slices not only depend on the source slice but also on the volume knowledge. For instance, for S2M case 1, the target slice shows a clear vessel cross-section that is based on the shape of the vessels in the volume. In Fig. 3, we provide coronal and sagittal views. For methods that rely on 2D generation, we synthesize individual slices and concatenate them to create volumes. It is clear to observe the volumetric inconsistency examining the coronal and sagittal views of these volumes. For instance, Palette synthesizes 2D slices unstably, where some good slices are synthesized but others are of poor quality. As a result, volumetric inconsistency severely impacts the performance of volumes. While 2D baselines inherently introduce inconsistency in the coronal and sagittal views, 3D baselines also generate poor results than ours, particularly in regard to blood vessels and ventricles.   Ablation Analysis. We conduct an ablation study to show the effectiveness of volumetric fine-tuning. Table 2 presents the quantitative results, demonstrating that our approach is able to increase the model's performance beyond that of the slice-wise model, without incurring significant extra training expenses. Figure 4 illustrates that fine-tuning volumetric layers helps to mitigate volumetric artifacts and produce clearer vessels, which is crucial for medical image synthesis. "
Make-A-Volume: Leveraging Latent Diffusion Models for Cross-Modality 3D Brain MRI Synthesis,4,Conclusion,"In this paper, we propose Make-A-Volume, a diffusion-based framework for crossmodality 3D medical image synthesis. Leveraging latent diffusion models, our method achieves high performance and can serve as a strong baseline for multiple cross-modality medical image synthesis tasks. More importantly, we introduce a generic paradigm for volumetric data synthesis by utilizing 2D backbones and demonstrate that fine-tuning volumetric layers helps the two-stage model capture 3D information and synthesize better images with volumetric consistency.We collected an in-house SWI-to-MRA dataset with clear blood vessels to evaluate volumetric data quality. Experimental results on two brain MRI datasets demonstrate that our model achieves superior performance over existing baselines. Generating coherent 3D and 4D data is at an early stage in the diffusion models literature, we believe that by leveraging slice-wise models and extending them to 3D/4D models, more work can help achieve better volume synthesis with reasonable memory requirements. In the future, we will investigate more efficient approaches for more high-resolution volumetric data synthesis."
Make-A-Volume: Leveraging Latent Diffusion Models for Cross-Modality 3D Brain MRI Synthesis,,Fig. 2 .,
Make-A-Volume: Leveraging Latent Diffusion Models for Cross-Modality 3D Brain MRI Synthesis,,Fig. 3 .,
Make-A-Volume: Leveraging Latent Diffusion Models for Cross-Modality 3D Brain MRI Synthesis,,Fig. 4 .,
Make-A-Volume: Leveraging Latent Diffusion Models for Cross-Modality 3D Brain MRI Synthesis,,Table 1 .,
Make-A-Volume: Leveraging Latent Diffusion Models for Cross-Modality 3D Brain MRI Synthesis,,.243 0.788 29.446 10.794 0.676 24.332 Ours 1000 steps 4.801 0.801 30.143 10.619 0.684 25.458,
Make-A-Volume: Leveraging Latent Diffusion Models for Cross-Modality 3D Brain MRI Synthesis,,Table 2 .,
Multi-perspective Adaptive Iteration Network for Metal Artifact Reduction,1,Introduction,"Metal artifact could significantly affect the clinical diagnoses with computed tomography (CT) images, and how to effectively reduce it is a critical but challenging issue. Specifically, metal artifact is caused by metallic implants and often exhibits as bright and dark streaks in the reconstructed images [7]. These streaks could hinder the perception of the actual contents, posing a serious obstacle for radiologists in making an accurate diagnosis [26]. Making matters worse, with the increasing employment of metallic implants, metal artifacts in CT images have become more widespread. In this case, effectively reducing metal artifacts while maintaining the tissue details is of great clinical significance [20].Since the metal artifacts are structured and non-local, they are tough to be removed from images directly [8]. Most of the traditional methods propose to reduce metal artifacts in the sinogram domain [6,13,21]. For instance, linear interpolation (LI) [6] and normalization metal artifacts reduction (NMAR) [13] weakened metal artifacts by substituting metal trace regions with interpolated data. However, severe secondary artifacts are induced by interpolation errors. Another commonly used iterative reconstruction algorithm [12] is computationally expensive. Some other researchers also explore a combination of multiple traditional methods to leverage their advantages [5] and improve their performance.In contrast to the traditional methods mentioned above, deep learning-based MAR methods are undergoing more intensive studies and become a dominant approach to MAR. Depending on the performing domain, they can be categorized into three types, i.e., sinogram domain-based, image domain-based, and dual-domain-based. Specifically, i). the sinogram domain-based methods leverage the advantage that the signals of metal artifacts are concentrated in form of metal trace(s) and can be easily separated from the informative image contents in the sinogram domain [3,15,17]. However, they are restricted by the availability of the physical scanning configurations, and slight disturbance in the sinogram could cause serious artifacts in the image domain; ii). the image domain-based methods directly reduce metal artifacts in the image domain by utilizing residual [22] or adversarial learning [10,24,25,29] techniques. However, the deep intertwinement of artifacts and the image contents in the image domain makes them arduous to be differentiated, limiting the network performance; iii). the dual-domain-based methods utilize both the sinogram and image domains to reduce metal artifacts [8,28]. They typically involve alternative reduction of metal artifacts in the sinogram domain and refinement of image contents in the image domain, e.g., dual-domain data consistent recurrent network [32] and deep unrolling dual-domain network [23]. However, they cannot completely resolve the problem of secondary artifact and still require physical scanning configurations. In addition, most of the three above types of deep learning-based methods lack interpretability since they perform MAR in a black-box mechanism.To address the issues of the existing methods aforementioned, we propose a Multi-perspective Adaptive Iteration Network(MAIN), and our main contributions are as follows:1) Multi-perspective Regularizations: Based on the insightful analysis on the limitations of using sinograms in MAR, this paper innovatively identifies that the desirable properties of wavelet transform could well address the issues of sinograms in MAR. i.e., the spatial distribution characteristics of metal artifacts under different domains and resolutions. Based on this, we integrate multi-domain, multi-frequency band, and multi-constraint into our scheme by exploiting wavelet transform. Therefore, we explicitly formulate such knowledge as a multi-perspective optimization model as shown in Fig. is the element-wise multiplication; and W denotes the adaptive wavelet transform.technique [1]. The network is designed according to the algorithm to keep it in accordance with the procedure of the theoretical MAR optimization, making the network more interpretable.3) Adaptive Wavelet Transform: In order to increase the flexibility and adaptivity of the proposed model, the proposed model conducts wavelet transforms with neural technology rather than the traditional fixed wavelet transform."
Multi-perspective Adaptive Iteration Network for Metal Artifact Reduction,2,Method,"Mathematical Model. In the image domain, it is easy to segment metal regions with much higher CT values [22]. As the metal regions have no human tissues, the clean CT image X can be defined as:where Y ∈ R H×W is a metal-corrupted image, A and I denote the metal artifact and binary non-metal mask, respectively. is the element-wise multiplication. To obtain a promising solution, various regularization terms representing prior constrains are introduced as:where W denotes the wavelet transform, f 1 (.), f 2 (.) and f 3 (.) are regularization functions, and λ 1 , λ 2 and λ 3 are regularization weights. Specifically, f 1 (.) and f 2 (.) represent wavelet domain constraints of X and A respectively, and f 3 (.) introduces prior knowledge of X in the image domain. ε is an arbitrarily small number, and .2 F is the Frobenius norm. When the wavelet components are transformed back to the image domain, the image will be blurred due to information loss. To recover a more precise image, let U = I X and introduce an error feedback item [14] into Eq. ( 2), where X represents the CT image in the image domain obtained after performing inverse wavelet transform. Then, the optimization problem becomes:where β is an adaptive weight.Optimization Algorithm. In this paper, an alternating minimization strategy is used to solve Eq. ( 3). At the (k + 1) th iteration, A (k+1) , X (k+1) and U (k+1) are derived as the following three sub-problems:The Proximal Gradient Descent algorithm(PGD) [1] is applied to solve each sub-problems above. Taking the first sub-problem as an example, the Taylor formula is utilized to introduce A (k) into the approximation of A (k+1) . Taylor's second-order expansion can be expressed as, the quadratic approximation of A (k+1) can be expressed as:Assuming that ∇ 2 g(A (k) ) is a non-zero constant that is replaced by 1 η1 . Besides, g(A (k) ) is replaced by another constant (η 1 ∇g(A (k) )) 2 to form a perfect square trinomial since changing constant does not affect the minimization of our objective function. Therefore, Eq. ( 5) is reformatted asIn code implementation, a trainable parameter is used to represent η 1 . It is excepted that the application of adaptive parameter can assist network fitting A more preferably. In order to reveal the iterative procedure more apparently, an intermediate variable A (k+0.5) is introduced:Based on the above derivations, an iteration equation can be formulated as [22]:where prox η1 is the proximal operator [22] related to f 1 (.). Since f 1 (.) denotes the constraint on metal artifacts in wavelet domain, the iterative solution of A is carried out in wavelet domain. Since we eventually want to obtain artifacts in the image domain, W T transforms the artifacts from wavelet domain to image domain. Similarly, X (k+1) and U (k+1) are derived as:where prox η2 and prox η3 are the proximal operator related to f 2 (.) and f 3 (.) respectively.Network Design.  8) and ( 9) to respectively emulate the proximal operators of prox η1 , prox η2 and prox η3 . At the (k + 1) th block, A (k) , X (k) and U (k) are first decomposed by adaptive wavelet transform module [18]. In the wavelet domain, the proxN et A and proxN et X are built by following the DnCNN [30], subsequently, A (k+1) and X (k+1) are computed. Next, A (k+1) and X (k+1) are converted to the image domain. And a lightweight U-Net [14,19] is employed as proxN et U ."
Multi-perspective Adaptive Iteration Network for Metal Artifact Reduction,3,Experiments,"Synthetic Data. A synthetic dataset is generated by following the simulation procedure in [31]. Specifically, 1,200 clean CT images from Deeplesion [27] and 100 metal masks are collected for image synthesis. For network training, 1,000 CT images and 90 metal masks are randomly selected, creating 90,000 unique combinations. An additional 2,000 images are synthesized for test using the remaining 200 CT images and 10 metal masks. The pixel sizes of the 10 test masks are: 2054, 879, 878, 448, 242, 115, 115, 111, 53, and 32. Clinical Data. In addition to the above synthetic data, the proposed model is also evaluated on the publicly available clinical dataset CLINIC-metal [9]. To keep consistent with Yu et al. [28], we segment the metal masks using a threshold of 2,000 HU. Then, the linear interpolated image is computed with the same procedure as synthetic data.Implementation Details. The MAIN network is implemented using PyTorch [16] and trained with a single NVIDIA A6000 GPU for 120 epochs. We use the Adam optimizer with parameters (β1, β2) = (0.5, 0.999), a learning rate of 5e -5 , and a batch size of 16 to train the network. To enhance the stability of the model training, various image augmentations, such as image rotation and transposition, are applied.Baseline and Evaluation Metric. Six state-of-the-art methods for metal artifact reduction are compared, including traditional method (NMAR [13]) and deep learning-based approaches (ACDNet [22], DuDoNet++ [11], DuDoNet [8] and CNNMAR [31]). DuDoNet and DuDoNet++ are reimplemented strictly adhering to the original methodology since they lacked open-source code. The Root Mean Square Error (RMSE) and Structural Similarity Index (SSIM) are used for quantitative assessment on the synthetic dataset. As reference images are unavailable on the clinical dataset, only visual comparison is conducted in terms of metal artifact reduction.   Moreover, Fig. 4(a) displays the statistical results of different methods on the test set. Figure 4(b) shows the noise power spectrum (NPS) maps [2]. Figure 4(c) shows the intensity profiles [4] of different MAR results. These results demonstrate that the proposed method is highly stable and effective in dealing with different metallic implants.Experimental Results on Clinical Data. Figure 5 shows the visual comparison on a clinical CT image with metal artifacts. It can be observed that the secondary artifacts caused by NMAR are severe, and other baselines would blur the tissue near the metallic implants. In comparison, our MAIN has achieved the best results in reducing metal artifacts and recovering tissue details.Ablation Studies. We re-configure our network and retrain the model in image domain and wavelet domain, respectively. We also utilize 'db3' wavelet to reconfigure the network and compare the corresponding model with the adaptive wavelet. It can be observed from Fig. 6 and Table 2 that our approach for reducing metal artifacts in both the wavelet and image domains is more effective than the single domain scheme. Moreover, the effectiveness of the adaptive wavelet transform is also confirmed.   "
Multi-perspective Adaptive Iteration Network for Metal Artifact Reduction,,1 . 2 )Fig. 1 .,
Multi-perspective Adaptive Iteration Network for Metal Artifact Reduction,,Figure 2,
Multi-perspective Adaptive Iteration Network for Metal Artifact Reduction,,Fig. 2 .,
Multi-perspective Adaptive Iteration Network for Metal Artifact Reduction,,Fig. 3 .,
Multi-perspective Adaptive Iteration Network for Metal Artifact Reduction,,Fig. 4 .Fig. 5 .,
Multi-perspective Adaptive Iteration Network for Metal Artifact Reduction,,Fig. 6 .,
Multi-perspective Adaptive Iteration Network for Metal Artifact Reduction,,Table 2 .,
Multi-perspective Adaptive Iteration Network for Metal Artifact Reduction,,Table 1 .,
Multi-perspective Adaptive Iteration Network for Metal Artifact Reduction,,628 ± 0.26 4 Results,
Dual Arbitrary Scale Super-Resolution for Multi-contrast MRI,1,Introduction,"Magnetic Resonance Imaging (MRI) is one of the most widely used medical imaging modalities, as it is non-invasive and capable of providing superior soft tissue contrast without causing ionizing radiation. However, it is challenging to acquire high-resolution MR images in practical applications [8] due to the inherent shortcomings of the systems [19,23] and the inevitable motion artifacts of the subjects during long acquisition sessions.Super-resolution (SR) techniques are a promising way to improve the quality of MR images without upgrading hardware facilities. Clinically, multi-contrast MR images, e.g., T1, T2 and PD weighted images are obtained from different pulse sequences [14,21], which can provide complementary information to each other [3,7]. Although weighted images reflect the same anatomy, they excel at demonstrating different physiological and pathological features. Different time is required to acquire images with different contrast. In this regard, it is promising to leverage an HR reference image with a shorter acquisition time to reconstruct the modality with a longer scanning time. Recently, some efforts have been dedicated to multi-contrast MRI SR reconstruction. Zeng et al. proposed a deep convolution neural network to perform single-and multi-contrast SR reconstruction [27]. Dar et al. concatenated information from two modalities into the generator of a generative adversarial network (GAN) [6], and Lyu et al. introduced a GAN-based progressive network to reconstruct multi-contrast MR images [15]. Feng et al. used a multi-stage feature fusion mechanism for multi-contrast SR [7]. Li et al. adopted a multi-scale context matching and aggregation scheme to gradually and interactively aggregate multi-scale matched features [12]. Despite their effectiveness, these networks impose severe restrictions on the resolution of the reference image, largely limiting their applications. In addition, most existing multi-contrast SR methods only work with fixed integer scale factors and treat different scale factors as independent tasks. For example, they train a single model for a certain integer scale factor (×2, ×4). In consequence, using these fixed models for arbitrary scale SR is inadequate. Furthermore, in practical medical applications, it is common for radiologists to zoom in on MR images at will to see localized details of the lesion. Thus, there is an urgent need for an efficient and novel method to achieve superresolution of arbitrary scale factors in a single model.In recent years, several methods have been explored for arbitrary scale super-resolution tasks on natural images, such as Meta-SR [9] and Arb-SR [24]. Although they can perform arbitrary up-sampling within the training scales, their generalization ability is limited when exceeding the training distribution, especially for large scale factors. Inspired by the success of implicit neural representation in modeling 3D shapes [5,10,16,18,20], several works perform implicit neural representations to the 2D image SR problem [4,17]. Since these methods can sample pixels at any position in the spatial domain, they can still perform well beyond the distribution of the training scale. Also, there is an MRI SR method that combines the meta-upscale module with GAN and performs arbitrary scale SR [22]. However, the GAN-based method generates unrealistic textures, which affects the diagnosis accuracy.To address these issues, we propose an arbitrary-scale multi-contrast MRI SR framework. Specifically, we introduce the implicit neural representation to multi-contrast MRI SR and extend the concept of arbitrary scale SR to the reference image domain. Our contributions are summarized as follows: Our Dual-ArbNet outperforms several state-of-the-art approaches on two benchmark datasets: fastMRI [26] and IXI [1]."
Dual Arbitrary Scale Super-Resolution for Multi-contrast MRI,2,Methodology,
Dual Arbitrary Scale Super-Resolution for Multi-contrast MRI,2.1,Background: Implicit Neural Representations,"As we know, computers use 2D pixel arrays to store and display images discretely. In contrast to the traditional discrete representation, the Implicit Neural Representation (INR) can represent an image I ∈ R H×W in the latent space F ∈ R H×W ×C , and use a local neural network (e.g., convolution with kernel 1) to continuously represent the pixel value at each location. This local neural network fits the implicit function of the continuous image, called Implicit Decoding Function (IDF). In addition, each latent feature represents a local piece of continuous image [4], which can be used to decode the signal closest to itself through IDF. Thus, by an IDF f (•) and latent feature F , we can arbitrarily query pixel value at any location, and restore images of arbitrary resolution."
Dual Arbitrary Scale Super-Resolution for Multi-contrast MRI,2.2,Network Architecture,"The overall architecture of the proposed Dual-ArbNet is shown in Fig. 1. The network consists of an encoder and an implicit fusion decoder. The encoder performs feature extraction and alignment of the target LR and the reference image. The implicit fusion decoder predicts the pixel values at any coordinate by fusing the features and decoding through IDF, thus achieving reconstruction.Encoder. In the image encoder, Residual Dense Network (RDN) [29] is used to extract image latent features for the network, and the reference image branch shares weights with the target LR image branch to achieve consistent feature extraction and reduce parameters. To aggregate the neighboring information in the reconstruction process, we further unfold the features of 3 × 3 neighborhoods around each pixel, expanding the feature channels nine times.Since the resolution of target LR and reference image are different, we have to align them to target HR scale for further fusion. With the target image shaped H tar × W tar and reference image shaped where z ∈ {ref, tar} indicates the reference and target image, I tar and I ref are the input target LR and reference image. In this way, we obtain the latent feature nearest to each HR pixel for further decoding, and our method can handle Arbitrary scale SR for target images with Arbitrary resolution of reference images (Dual-Arb).Decoder. As described in Sect. 2.1, the INR use a local neural network to fit the continuous image representation, and the fitting can be referred to as Implicit Decoding Function (IDF). In addition, we propose a fusion branch to efficiently fuse the target and reference latent features for IDF decoding. The overall decoder includes a fusion branch and a shared IDF, as shown in Fig. 1(see right).Inspired by [25,29], to better fuse the reference and target features in different dimensions, we use ResBlock with Channel Attention (CA) and Spatial Attention (SA) in our fusion branch. This 5 layers lightweight architecture can capture channel-wise and spatial-wise attention information and fuse them efficiently. The fusion process can be expressed as:where L i indicates the i-th fusion layer. Then, we equally divide the fused feature F The IDF in our method is stacked by convolution layer with kernel size 1 (conv 1 ) and sin activation function sin(•). The conv 1 and sin(•) are used to transform these inputs to higher dimension space [17], thus achieving a better representation of the IDF. Since conv 1 (x) can be written as W •x+b without using any adjacent features, this decoding function can query SR value at any given coordinate. Akin to many previous works [4,17], relative coordinate information P (x, y) and scale factors S ref , S tar are necessary for the IDF to decode results continuously. At each target pixel (x, y), we only use local fused feature F fusion , which represents a local piece of continuous information, and coordinate P (x, y) relative to the nearest fused feature, as well as scale factors {S ref , S tar }, to query in the IDF. Corresponding to the fusion layer, we stack 6 convolution with activation layers. i-th layer's decoding function f (i) can be express as:where (x, y) is the coordinate of each pixel, and z ∈ {ref, tar} indicates the reference and target image. denotes element-wise multiplication, and cat is the concatenate operation. W (i) and b (i) are weight and bias of i-th convolution layer. Moreover, we use the last layer's output f (5) (•) as the overall decoding function f (•). By introducing the IDF above, the pixel value at any coordinates I z,SR (x, y) can be reconstructed:where Skip(•) is skip connection branch with conv 1 and sin(•), z ∈ {ref, tar}.Loss Function. An L1 loss between target SR results I target,SR and HR images I HR is utilized as reconstruction loss to improve the overall detail of SR images, named as L rec . The reconstructed SR images may lose some frequency information in the original HR images. K-Loss [30] is further introduced to alleviate the problem. Specifically, K SR and K HR denote the fast Fourier transform of I target,SR and I HR . In k-space, the value of mask M is set to 0 in the highfrequency cut-off region mentioned in Sect. 3, otherwise set to 1. L2 loss is used to measure the error between K SR and K HR . K-Loss can be expressed as:To this end, the full objective of the Dual-ArbNet is defined as:We set λ K = 0.05 empirically to balance the two losses."
Dual Arbitrary Scale Super-Resolution for Multi-contrast MRI,2.3,Curriculum Learning Strategy,"Curriculum learning [2] has shown powerful capabilities in improving model generalization and convergence speed. It mimics the human learning process by allowing the model to start with easy samples and gradually progress to complex samples. To achieve this and stabilize the training process with different references, we introduce curriculum learning to train our model, named Cur-Random. This training strategy is divided into three phases, including warm-up, pre-learning, and full-training. Although our image encoder can be fed with reference images of arbitrary resolution, it is more common to use LR-ref (scale as target LR) or HR-ref (scale as target HR) in practice. Therefore, these two scales of reference images are used as our settings.In the warm-up stage, we fix the integer SR scale to integer (2×, 3× and 4×) and use HR-Ref to stable the training process. Then, in the pre-learning stage, we use arbitrary scale target images and HR reference images to quickly improve the network's migration ability by learning texture-rich HR images. Finally, in the full-training stage, we train the model with a random scale for reference and target images, which further improves the generalization ability of the network."
Dual Arbitrary Scale Super-Resolution for Multi-contrast MRI,3,Experiments,"Datasets. Two public datasets are utilized to evaluate the proposed Dual-ArbNet network, including fastMRI [26] (PD as reference and FS-PD as target) and IXI dataset [1] (PD as reference and T2 as target). All the complex-valued images are cropped to integer multiples of 24 (as the smallest common multiple of the test scale). We adopt a commonly used down-sampling treatment to crop the k-space. Concretely, we first converted the original image into the k-space using Fourier transform. Then, only data in the central low-frequency region are kept, and all high-frequency information is cropped out. For the down-sampling factors k, only the central 1 k 2 frequency information is kept. Finally, we used the inverse Fourier transform to convert the down-sampled data into the image domain to produce the LR image.We compared our Dual-ArbNet with several recent state-of-the-art methods, including two multi-contrast SR methods: McMRSR [12], WavTrans [13], and three arbitrary scale image SR methods: Meta-SR [9], LIIF [4], Diinn [17].Experimental Setup. Our proposed Dual-ArbNet is implemented in PyTorch with NVIDIA GeForce RTX 2080 Ti. The Adam optimizer is adopted for model training, and the learning rate is initialized to 10 -4 at the full-training stage for all the layers and decreases by half for every 40 epochs. We randomly extract 6 LR patches with the size of 32×32 as a batch input. Following the setting in [9], we augment the patches by randomly flipping horizontally or vertically and rotating 90 • . The training scale factors of the Dual-ArbNet vary from 1 to 4 with stride 0.1, and the distribution of the scale factors is uniform. The performance of the SR reconstruction is evaluated by PSNR and SSIM.Quantitative Results. Table 1 reports the average SSIM and PSNR with respect to different datasets under in-distribution and out-of-distribution large scales. Since the SR scale of McMRSR [12] and WavTrans [13] is fixed to 2× and 4×, we use a 2× model and down-sample the results when testing 1.5×. We use the 4× model and up-sample the results to test 6× and 8×, and down-sample the results to test 3× results. Here, we provide the results with the reference image at HR resolution. As can be seen, our method yields the best results in all datasets. Notably, for out-of-distribution scales, our method performs even significantly better than existing methods. The results confirm that our framework outperforms the state-of-the-art in terms of performance and generalizability.Qualitative Evaluation. Figure 2 provides the reconstruction results and the corresponding error maps of the in-distribution scale (4×) and out-ofdistribution scale (6×). The more obvious the texture in the error map, the worse the reconstruction means. As can be observed, our reconstructed images  As can be seen that the reconstruction results of w/o coord and w/o scale are not optimal because coordinates and scale can provide additional information for the implicit decoder. We observe that w/o ref has the worst results, indicating that the reference image can provide auxiliary information for super-resolving the target image."
Dual Arbitrary Scale Super-Resolution for Multi-contrast MRI,4,Conclusion,"In this paper, we proposed the Dual-ArbNet for MRI SR using implicit neural representations, which provided a new paradigm for multi-contrast MRI SR tasks. It can perform arbitrary scale SR on LR images at any resolution of reference images. In addition, we designed a new training strategy with reference to the idea of curriculum learning to further improve the performance of our model. Extensive experiments on multiple datasets show that our Dual-ArbNet achieves state-of-the-art results both within and outside the training distribution. We hope our work can provide a potential guide for further studies of arbitrary scale multi-contrast MRI SR."
Dual Arbitrary Scale Super-Resolution for Multi-contrast MRI,,Fig. 1 .,
Dual Arbitrary Scale Super-Resolution for Multi-contrast MRI,,,
Dual Arbitrary Scale Super-Resolution for Multi-contrast MRI,,,
Dual Arbitrary Scale Super-Resolution for Multi-contrast MRI,,Fig. 2 .,
Dual Arbitrary Scale Super-Resolution for Multi-contrast MRI,,Table 1 .,
Dual Arbitrary Scale Super-Resolution for Multi-contrast MRI,,Table 2 .,
Dual Arbitrary Scale Super-Resolution for Multi-contrast MRI,,Table 2 (,
Dual Arbitrary Scale Super-Resolution for Multi-contrast MRI,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5_27.
Physics-Informed Neural Networks for Tissue Elasticity Reconstruction in Magnetic Resonance Elastography,1,Introduction,"Tissue elasticity holds enormous diagnostic value for detecting pathological conditions such as liver fibrosis [1,2] and can be mapped by an imaging procedure called magnetic resonance elastography (MRE). During MRE, a mechanical stress is applied to the region of interest and an image is captured of the resulting tissue deformation, then the elasticity is inferred by solving the inverse problem of a partial differential equation (PDE). However, conventional methods for elasticity reconstruction are sensitive to noise, do not incorporate anatomical information, and are often only evaluated on artificial data sets [3][4][5][6][7][8]. Elasticity reconstruction methods utilize a variety of numerical techniques and physical models [9]. Algebraic Helmholtz inversion (AHI) makes simplifying assumptions that enable an algebraic solution to the governing equations [3,5]. However, AHI relies on finite differences, which amplify noise. The finite element method (FEM) requires fewer physical assumptions and solves a variational formulation of the PDE [4,[6][7][8]10], making it more flexible and typically more robust than AHI. However, neither method uses anatomical images, which have been successfully used to predict elasticity with deep learning [11][12][13].Physics-informed neural networks (PINNs) are a recent deep learning framework that uses neural networks to solve PDEs [14]. PINNs represent unknown function(s) in a boundary value problem as neural networks. The boundary conditions and PDE are treated as loss functions and the problem is solved using gradient-based optimization. PINNs have been applied to elasticity reconstruction in other contexts [15][16][17][18], but evaluations have been limited to artificial data sets and prior work has not combined physics-informed learning with automated learning from anatomical MRI.In this work, we develop a method for enhanced tissue elasticity reconstruction in MR elastography using physics-informed learning. We use PINNs to solve the equations of linear elasticity as an optimization problem for a given wave image. Our model simultaneously learns continuous representations of the measured displacement field and the latent elasticity field. We evaluate the method on a numerical simulation and on patient liver MRE data, where we demonstrate improved noise robustness and overall accuracy than AHI or FEM-based inversion. In addition, we show that augmenting our method with an anatomicallyinformed loss function further improves reconstruction quality."
Physics-Informed Neural Networks for Tissue Elasticity Reconstruction in Magnetic Resonance Elastography,2,Background,"Magnetic Resonance Elastography (MRE). An MRE procedure involves placing the patient in an MRI scanner and using a mechanical actuator to induce shear waves in the region of interest (Fig. 1). A motion-encoding gradient (MEG) synchronizes with the mechanical vibration, causing phase shifts in the captured Table 1. Physical equations relating the displacement field u to the shear modulus of elasticity μ during a steady-state harmonic motion from the theory of linear elasticity."
Physics-Informed Neural Networks for Tissue Elasticity Reconstruction in Magnetic Resonance Elastography,,Assumptions Equation,"signal based on the tissue displacement [19]. A wave image that encodes the full 3D displacement field can be acquired using MEGs in each dimension [1]. Next, a map of tissue stiffness called an elastogram is recovered from the wave image by solving an inverse problem. This requires 1) choosing a physical model that relates the motion of an elastic body to its material properties, and 2) solving the governing equation(s) for the unknown material parameters.Linear Elasticity. Physical models of MRE typically assume there is harmonic motion and a linear, isotropic stress-strain relation. Then tissue displacement is a complex vector field u : Ω → C 3 defined on spatial domain Ω ⊂ R 3 , and shear elasticity is characterized by the shear modulus, a complex scalar field μ :The first equation of motion translates into the general form PDE shown in Table 1. The mass density ρ and actuator frequency ω are prescribed based on prior knowledge. The Lamé parameter λ can be ignored if we assume tissue is incompressible (∇•u = 0), reducing the PDE to the heterogeneous form. Finally, assuming that the shear modulus is locally homogeneous (∇μ = 0) simplifies the PDE into the Helmholtz equation. The empirical validity of the homogeneity assumption has been criticized [20,21] and is explored in our experiments."
Physics-Informed Neural Networks for Tissue Elasticity Reconstruction in Magnetic Resonance Elastography,3,Proposed Method,"We use physics-informed neural networks (PINNs) to encode the solution space of the inverse problem. Our PINN framework (Fig. 2) learns continuous representations of the displacement field u(x) and elastic modulus μ(x) while respecting a PDE from Table 1. We incorporate conventional MRI images by including an additional anatomically-informed loss function. In the following sections, we explain the PINN framework and how we incorporate the anatomical images.Physics-Informed Neural Networks. We use a dual-network approach to reconstruct tissue elasticity with PINNs. First, we train a neural network û(x; θ u ) to learn a mapping from spatial coordinates x to displacement vectors u in the wave image by minimizing the mean squared error L wave . The continuous representation of the displacement field enables automatic spatial differentiation. Then, we train a second neural network μ(x; θ µ ) to map from spatial coordinates to the shear modulus μ by minimizing the residual of a PDE, defined by some differential operator D. The PINNs and their spatial derivatives are used to evaluate the differential operator, which is minimized as a loss function L P DE to recover the elasticity field. We combine the loss functions as follows:We train PINNs using either the Helmholtz equation (PINN-HH) or the heterogeneous PDE (PINN-het) as the differential operator D in our experiments. The loss weight hyperparameters λ wave and λ P DE control the contribution of each loss function to the overall objective. We initialized λ P DE to a very low value and slowly stepped it up as the quality of û improved during training."
Physics-Informed Neural Networks for Tissue Elasticity Reconstruction in Magnetic Resonance Elastography,,Incorporating Anatomical Information.,"Prior work has demonstrated that tissue elasticity can be accurately predicted from anatomical MRI [22]. We include an additional output head â(x; θ µ ) from the elasticity PINN that predicts anatomical features a(x) at the corresponding position, as encoded in standard MRI imaging sequences. Then we introduce an additional loss function L anat to minimize the mean squared error between the predicted and true anatomical features. We explore how the relative weight of this loss function λ anat affects elasticity reconstruction performance in our in vivo experiments.We designed our models based on the SIREN architecture, which uses sine activation functions to better represent high spatial frequencies [23]. Both networks û and μ had five linear layers with 128 hidden units per layer and dense connections from all previous layer inputs. The first layer input was scaled by a hyperparameter ω 0 that biases the initial spatial frequency distribution. We employed the weight initialization scheme described in [23] to improve training convergence. We also extended the input vector with polar coordinates when training on patient data. We trained all models for 100,000 total iterations with the Adam optimizer using PyTorch v1.12.1 and DeepXDE v1.5.1 [24][25][26]. The code used for this work is available at https://github.com/batmanlab/MRE-PINN."
Physics-Informed Neural Networks for Tissue Elasticity Reconstruction in Magnetic Resonance Elastography,4,Related Work,"Algebraic Helmholtz Inversion (AHI). One of the most common methods for elasticity reconstruction is algebraic inversion of the Helmholtz equation (AHI) [3]. This approach assumes incompressibility and local homogeneity, uses finite differences to compute the Laplacian of the wave image, and then solves the Helmholtz equation as a linear system to estimate the shear modulus. Despite its simplicity, AHI has an established track record in both research and clinical settings [9]. Filtering is often required to reduce the impact of noise on AHI [5]. [4,6,7,10]. These use variational formulations to reduce the order of differentiation in the PDE. Then, they specify a mesh over the domain and represent unknown fields in terms of compact basis functions. This results in a linear system that can be solved for the elasticity coefficients either directly or iteratively [9]. Direct FEM inversion is more efficient and accurate [21], though it depends more on data quality [8]. We implemented direct FEM inversion of the Helmholtz equation (FEM-HH) and the heterogeneous PDE (FEM-het) with FEniCSx v0.5.1 [27]."
Physics-Informed Neural Networks for Tissue Elasticity Reconstruction in Magnetic Resonance Elastography,,Finite Element Method (FEM). Many techniques have been introduced for elasticity reconstruction based on the finite element method,
Physics-Informed Neural Networks for Tissue Elasticity Reconstruction in Magnetic Resonance Elastography,5,Experiments and Results,"We compare our methods (PINN-HH, PINN-het) to algebraic Helmholtz inversion (AHI) and direct FEM inversion (FEM-HH, FEM-het) on simulated data and liver data from a cohort of patients with NAFLD. We evaluate the overall reconstruction fidelity of each method and the impact of the homogeneity assumption. We assess their robustness to noise on the simulated data, and we study whether incorporating anatomical images enhances performance on patient liver data."
Physics-Informed Neural Networks for Tissue Elasticity Reconstruction in Magnetic Resonance Elastography,5.1,Robustness to Noise on Simulated Data,"We obtained a numerical FEM simulation of an elastic wave in an incompressible rectangular domain containing four stiff targets of decreasing size from the BIOQIC research group (described in Barnhill et. al. 2018 [28]). Six wave fields were generated at frequencies ranging from 50-100 Hz in 10 Hz increments. We applied each reconstruction method to each single-frequency wave image after adding varying levels of Gaussian noise. Then we computed the contrast transfer efficiency (CTE) [29] in each target region as the ratio between the targetbackground contrast in the predicted elastogram and the true contrast, where a CTE of 100% is ideal. We include functions to download the simulation data set in the project codebase. Fig. 3. Reconstruction performance on simulated data. Figure 3A shows the contrast transfer efficiency of each method in each target region of the simulation. Figure 3B shows how the contrast is affected by the noise level in the wave image.Experiment Results. Figure 3A compares the CTE of each method in the different target regions of the simulation, which decrease in size from left to right. On target 1, AHI performs best with 98% CTE followed by FEM-HH and FEMhet with 109% and 88%. PINN-HH and PINN-het had 46% and 51% contrast on target 1. AHI also performed best on target 2, with 104% CTE. Next were PINN-het with 41% contrast and FEM-HH with 38%. PINN-het outperformed the other methods on target 3 with 22% CTE followed by FEM-HH with 11%. Only the two FEM methods appear to have decent contrast on target 4, though this seems to be a false positive due to background variance seen in Fig. 4.Figure 3B shows the effect of wave image noise on the contrast transfer efficiency. The reconstruction quality decays at -50 db of noise for both FEM-HH (p = -1.7e-5) and FEM-het (p = 9.1e-6), each retaining less than 5% contrast. AHI is also sensitive to -50 dB of noise (p = 6.1e-4) but its performance drops more gradually. The contrast from PINN-HH does not decrease significantly until -20 dB (p = 8.8e-4) and PINN-het is insensitive until -10 dB (p = 4.8e-5). This indicates that PINNs are more robust to noise than AHI or direct FEM.Figure 4 displays reconstructed elastograms from each method using the 90 Hz simulated wave image, displayed at far left, next to the ground truth. AHI produces the clearest boundaries between the targets and background. The two FEM methods contain high variability within homogeneous regions, though the heterogeneous PDE appears to decrease the variance. PINN-HH displays textural artifacts that reduce the resolution. PINN-het has fewer artifacts and better resolution of the smaller targets due to the lack of homogeneity assumption."
Physics-Informed Neural Networks for Tissue Elasticity Reconstruction in Magnetic Resonance Elastography,5.2,Incorporating Anatomical Information on Patient Data,"For our next experiment, we obtained abdominal MRE from a study at the University of Pittsburgh Medical Center that included patients at least 18 years old who were diagnosed with non-alcoholic fatty liver disease (NAFLD) and underwent MRE between January 2016-2019 (demographic and image acquisition details can be found in Pollack et al. 2021 [22]). 155 patients had high-quality elastography, wave images, and anatomical MRI sequences. The wave images contained only one real displacement component and we did not have ground truth elasticity, so we used proprietary elastograms collected during the study as the ""gold standard."" We registered MRI sequences (T1 pre in-phase, T1 pre water, T1 pre out-phase, T1 pre fat, and T2) to the MRE using SimpleITK v2.0.0 [30] and incorporated them as the anatomical features a shown in Fig. 2. Liver regions were segmented using a previously reported deep learning model [22].We performed elasticity reconstruction on each of the 155 patient wave images using each method. We investigated the influence of anatomical information when training PINNs by varying the anatomical loss weight λ anat . Reconstruction fidelity was assessed using the Pearson correlation (R) between the predicted elasticity and the gold standard elasticity in the segmented liver regions.Experiment Results. Figure 5A shows correlation distributions between predicted and gold standard elasticity across the different patients. PINN-het had the highest median correlation and least variation between patients (median = 0.84, IQR = 0.04). PINN-HH came in second (median = 0.76, IQR = 0.05) while FEM-het came in third, but had the most variability (median = 0.74, IQR = 0.19). FEM-HH performed slightly worse than FEM-het, but was less variable (median = 0.70, IQR = 0.11). AHI performed the worst on in vivo data (median = 0.63, IQR = 0.12) and had the greatest number of low outliers. Fig. 5. Reconstruction performance on in vivo liver data. Figure 5A shows Pearson's correlations between predicted and gold standard elasticity across patients. Figure 5B shows the effect of the anatomic loss weight on PINN elasticity reconstruction performance. Figure 5B shows the effect of increasing the anatomic loss weight on PINN elasticity reconstruction quality. There was significant improvement in the correlation with the gold standard when the loss weight was increased from 0 to 1e-4 for both PINN-HH (p = 4.9e-56) and PINN-het (p = 3.0e-14), but no significant difference from increasing it further to 1e-2 for either method (PINN-HH: p = 0.51, PINN-het: p = 0.23). Raising the anatomic loss weight to 1e-4 increased the median correlation from 0.76 to 0.85 in PINN-HH and from 0.84 to 0.87 in PINN-het. This suggests that there is a synergistic effect from including physical constraints and anatomical imaging data in elasticity reconstruction.Figure 6 displays reconstructed elastograms for three randomly selected patients. AHI, FEM-HH and FEM-het all tend to overestimate the stiffness and have artifacts around nulls in the wave image. In contrast, PINN-HH and PINNhet more closely resemble the gold standard elastography, especially in regions close to the clinical threshold for fibrosis [31]. Furthermore, neither PINN reconstruction method shows signs of instabilities around wave amplitude nulls."
Physics-Informed Neural Networks for Tissue Elasticity Reconstruction in Magnetic Resonance Elastography,6,Conclusion,"PINNs have several clinically significant advantages over conventional methods for tissue elasticity reconstruction in MRE. They are more robust to noise, which is pervasive in real MRE data. Furthermore, they can leverage anatomical information from other MRI sequences that are standard practice to collect during an MRE exam, and doing so significantly improves reconstruction fidelity. Limitations of this work include the use of the incompressibility assumption to simplify the training framework, and the relatively poor contrast on simulated data. This underscores how accurate reconstruction on simulated data does not always translate to real data, and vice versa. In future work, we will evaluate PINNs for solving the general form of the PDE to investigate the effect of the incompressibility assumption. We will also extend to an operator learning framework in which the model learns to solve the PDE in a generalizable fashion without the need to retrain on each wave image. This would reduce the computation cost and enable further integration of physics-informed and data-driven learning."
Physics-Informed Neural Networks for Tissue Elasticity Reconstruction in Magnetic Resonance Elastography,,Fig. 1 .,
Physics-Informed Neural Networks for Tissue Elasticity Reconstruction in Magnetic Resonance Elastography,,Fig. 2 .,
Physics-Informed Neural Networks for Tissue Elasticity Reconstruction in Magnetic Resonance Elastography,,Fig. 4 .,
Physics-Informed Neural Networks for Tissue Elasticity Reconstruction in Magnetic Resonance Elastography,,Fig. 6 .,
Physics-Informed Neural Networks for Tissue Elasticity Reconstruction in Magnetic Resonance Elastography,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 32.
Co-learning Semantic-Aware Unsupervised Segmentation for Pathological Image Registration,1,Introduction,"Image registration has been widely studied in both academia and industry over the past two decades. In general, the goal of deformable image registration is to estimate a suitable nonlinear transformation that overlaps the pair of images with corresponding spatial relationships [4,22]. This goal is usually achieved by minimizing a well-defined similarity score. However, these methods often assume that there is no spatial non-correspondence between the two images. In the field of medical image analysis, this assumption is often not valid, particularly in cases such as pathology image to atlas registration or pre-operative and post-operative longitudinal registration. Direct registration of pathology images without taking into account the impact of focal tissue can result in missed pixel-level correspondence and large registration errors.A variety of approaches have been proposed to handle the noncorrespondence problem in medical image registration. These methods can be roughly divided into three main categories: 1) Cost function masking. The authors of [5,17] used the segmentation of the non-corresponding regions to mask the image similarity measure in optimization. 2) Converting pathological image to normal appearance. This class of approaches aims to replace or reconstruct the focal area as normal tissue to guide the registration either through low-rank and sparse image decomposition [10,11] or generative models [23]. 3) Non-correspondence detection via intensity criteria. This category of methods can be formulated as joint segmentation and registration to detect non-corresponding regions during the registration process [6,7]. Although these approaches partially handle the issue of non-correspondence in the registration, they still have some serious shortcomings. The cost function masking and image conversion approaches require ground truth or accurate labels during registration and may decrease the alignment accuracy when the focal area is large. The non-correspondence detection approach, which typically relies on a sophisticated designed loss function, is very sensitive to the dataset [1] and difficult to find a set of unified parameters.Therefore, to effectively address the non-correspondence problem in registering pathology images, it is necessary to incorporate both a data-independent segmentation module and a modality-adaptive inpainting module into the registration pipeline. To bridge this gap, we introduce the semantic information of the category based on [21,24]. It employs the non-correspondence in registration to achieve accurate segmentation of the lesion region and uses the segmented mask to reconstruct the lesion area and guide the registration. In this paper, we address the challenge of large alignment errors due to the loss of spatial correspondence in processing pathological images. To overcome this challenge, we propose GIRNet, a tri-net collaborative learning framework that simultaneously updates the segmentation, inpainting, and registration networks. The segmentation network minimizes the mutual information between the lesion and normal tissue based on the semantic information introduced by the registration network, allowing for accurate segmentation of regions with missing spatial correspondence. The registration network, in turn, weakens the adverse effects of the lesions based on the mask generated by the segmentation network. To the best of our knowledge, this is the first work to apply an unsupervised segmentation method based on minimal mutual information (MMI) to pathological image registration, with simultaneous training of segmentation and registration. Our work makes the following key contributions.-We propose a collaborative learning method for the simultaneous optimization of registration, segmentation, and inpainting networks.-We show the effectiveness of using mutual information minimization in an unsupervised manner for pathological image segmentation and registration by incorporating semantic information through the registration process. -We perform a series of experiments to validate our method's superiority in accurately finding lesions and effectively registering pathological images."
Co-learning Semantic-Aware Unsupervised Segmentation for Pathological Image Registration,2,Method,"Our proposed framework (Fig. 1) involves three modules: a register denoted by ψ, a segmenter denoted by θ, and an inpainter denoted by φ. The three modules are trained in a co-learning manner to enable the registration aware of semantic information. Importantly, our proposed training procedure is fully unsupervised which does not require any labeled data for training the network. "
Co-learning Semantic-Aware Unsupervised Segmentation for Pathological Image Registration,2.1,Collaborative Optimization,"The most critical problem in pathological image registration is identifying and dealing with the lesion area. If we naively register a source pathological image S to a template T without caring about the lesion boundary, the deformation field near the boundary would be uncontrollable because a healthy template does not have a lesion. A possible approach here is to initialize an inflating boundary containing the lesion area, followed by calculating the registration loss either outside of the boundary only or based on a modified S that is inpainted within the given boundary. However, the registration error has no sensitivity to the location of the inflated boundary as long as it is larger than the real one. On the other hand, if we compared the inpainted image and the pathological image S within the boundary only, we can notice that their dissimilarity increases when the boundary shrinks as the inpainting algorithm only generates healthy parts. This mechanism can then induce a segmentation module that segments the lesion as the foreground and the remaining as the background, which iteratively serves as the input mask for the inpainting module. Further, as the registration loss is calculated based on the registered inpainted image and the target image, the registration provides a regularization for the inpainting module such that the inpainting is specialized to facilitate the registration. Specially for the input and output of the three modules, RegNet takes images S and T as input and generates the deformation field from S to T and T to S as ϕ ST and ϕ T S respectively. InpNet takes the background (foreground) cropped by SegNet and image T •ϕ T S warped by RegNet as input and outputs foreground (background) with a normal appearance. SegNet takes the pathology image S as input and employs the normal foreground and background inpainted by InpNet to segment the lesion region based on MMI. SegNet and InpNet are actually in an adversarial relationship. Through this joint optimization approach, the three networks collectively work to achieve registration and segmentation of pathological images under entirely unsupervised conditions, without being limited by the specific network structure. For the sake of simplicity, we employ a Unet-like [20] basic structure without any normalization layer."
Co-learning Semantic-Aware Unsupervised Segmentation for Pathological Image Registration,2.2,Network Modules,"RegNet. The primary objective of registration is to generate a deformation field that minimizes the dissimilarity between the source image (S) and the template image (T). The deformation is usually required to satisfy constraints like smoothness and even diffeomorphism. In terms of pathological image registration, the deformation field is only valid off the lesion area. Thus the registration loss should be calculated on the normal area only. Suppose that the lesion area is already obtained as θ(S) and inpainted with normal tissue, the registration loss can then be formulated aswhere ϕ ST = ψ(S, T ), ϕ T S = ψ(T, S) are the deformation fields that warp S → T and T → S respectively. The symbol • denotes element-wise multiplication. Furthermore, L sym denotes the registration loss of SymNet [14], which aims to balance the losses of orientation consistency, regularization and magnitude.SegNet. Minimal Mutual Information (MMI) is a typically used unsupervised segmentation method that distinguishes foreground from background. However, for a pathological image, the lesion regions often have a similar intensity to normal tissues near the boundary, which prevents the MMI from accurate segmentation without the semantic information. To address this limitation, we warp a healthy image T onto a pathology image S using a deformation field ϕ T S = ψ(T, S). This process maximizes the mutual information between corresponding regions of the two images and minimizes that of non-corresponding regions, thereby facilitating accessible lesion segmentation with MMI. Let Ω ∈ R denote the image domain, M denote the mask, F θ = Ω•M and B θ = Ω•M denote the foreground and background, where M = 1 -M, M ∈ {0, 1}. Regarding a pathological image S, when the background (normal) is given, the inpainted foreground (normal) will be different from the true foreground (lesion). When the foreground (lesion) is given, the inpainted background will remain the same as the background (normal). Thus we can formulate the adversarial loss of unsupervised segmentation aswhere D is the distance function given by localized normalized cross-correlation (LNCC) [3]. Appendix A provides a detailed derivation.InpNet. Let M denote the mask and ϕ T S denote the deformation field from T to S. To handle the potential domain differences between the masked image S •M and the aligned image T •ϕ T S , InpNet employs two encoders. The adversarial loss function of InpNet is represented as L MI . To incorporate semantic information, we include an additional similarity term L sim that prevents InpNet from focusing too heavily on the foreground (lesion) and encourages it to produce healthy tissue. The proposed loss function L inp is then formulated as the combination of mutual information loss defined through the normalized correlation coefficient (NCC) and similarity loss through the mean squared error (MSE):withwhere λ represents the weight that balances the contributions of mutual information loss and similarity loss, and T M denotes image T after histogram matching. We modify the histogram of T •ϕ T S to be similar to that of S in order to mitigate the effects of domain differences."
Co-learning Semantic-Aware Unsupervised Segmentation for Pathological Image Registration,3,Experiments,"Our experimental design focuses on two common clinical tasks: atlas-based registration, which involves warping pathology images to a standard atlas template, and longitudinal registration, which involves registering pre-operative images to post-operative images for the purpose of tracking changes over time.Dataset and Pre-processing. For our study, we selected the ICBM 152 Nonlinear Symmetric template as our atlas [9]. We reoriented all MRI scans of the T1 sequence to the RAS orientation with a resolution of 1 mm × 1 mm × 1 mm and align the images to atlas using the mri robust register tool in FreeSurfer [19]. We then cropped the resulting MRI scans to a size of 160 × 192 × 144, without any image augmentation. To evaluate our approach, we employed a 5-fold cross-validation method and divided our data into training and test sets in an 8:2 ratio. 3D Brain MRI. OASIS-1 [12] includes 416 cross-sectional MRI scans from individuals aged 18 to 96, with 100 of them diagnosed with mild to moderate Alzheimer's disease. BraTS2020 [13] provides 369 expert-labeled pre-operative MRI scans of glioblastomas and low-grade gliomas, acquired from multiple institutions for routine clinical use."
Co-learning Semantic-Aware Unsupervised Segmentation for Pathological Image Registration,,3D Pseudo Brain MRI.,"To evaluate the performance of atlas-based registration, it is essential to have the correct mapping of pathological regions to healthy brain regions. To create such a mapping, we created a pseudo dataset by utilizing images from the OASIS-1 and BraTS2020. From the resulting t1 sequences, a pseudo dataset of 300 images was randomly selected for further analysis. Appendix B provides a detailed process for creating the pseudo dataset.Real Data with Landmarks. BraTS-Reg 2022 [2] provides extensive annotations of landmarks points within both the pre-operative and the follow-up scans that have been generated by clinical experts. A total of 140 images are provided, of which 112 are for training, and 28 for testing. Comparison to Pathology Registration. We compared our method (GIR-Net) with competitive algorithms: 1) three cutting-edge deep learning-based unsupervised deformable registration approaches: VoxelMorph [3], VoxelMorph-DF [8] and SymNet [14]. 2) two unsupervised deformable registration methods for pathological images: DRAMMS [18] and DIRAC [16]. DRAMMS is an optimization-based method that reduces the impact of non-corresponding regions. DIRAC jointly estimates regions with absent correspondence and bidirectional deformation fields and ranked first in the BraTSReg2022 challenge.Atlas-Based Registration. After creating the pseudo dataset, we warped brain MR images without tumors to the atlas and used the resulting deformation field as the gold standard for evaluation. We then evaluated the mean deformation error (MDE) [10], which is calculated as the average Euclidean distance between the coordinates of the deformation field and the gold standard within specific regions of interest. These regions include: 1) the tumor region. 2) the normal region near the tumor (within 30 voxels). 3) the normal region far from the tumor (over 30 voxels but within brain tissue). Our results, presented in Fig. 2, show that our method with histogram matching (HM) outperforms other methods in all three regions, particularly in the normal regions (near and far). By utilizing HM, our network achieves an MDE of less than 1 mm compared to the gold standard deformations. These results demonstrate the effectiveness of our method in differentiating the impact of pathology in atlas-based registration tasks. Specifically, DIRAC is unable to eliminate the influence of domain differences and resulting in the largest registration error among the evaluated methods.Longitudinal Registration. To perform the longitudinal registration task, we registered each pre-operative scan to the corresponding follow-up scan of the same patient and measured the mean target registration error (TRE) of the paired landmarks using the resulting deformation field. For this purpose, we leveraged SegNet, trained on BraTS2020, to segment the tumor of BraT-SReg2022 and separated the landmarks into two regions: near tumor and far from tumor. Figure 3 shows the mean TRE for the various registration approaches. In our proposed framework, we replaced RegNet with CIR-DM [15] (denoted as GIR(CIRDM)) without the need for supervised training or pretraining, and achieved comparable performance with the state-of-the-art method DIRAC. Moreover, our GIR approach outperforms other deep learning-based methods and achieved accurate segmentation of pathological images.To quantitatively evaluate the segmentation capability of our proposed framework, we compared its performance with other unsupervised segmentation techniques methods, including unsupervised clustering toolbox AUCseg [25], joint non-correspondence segmentation and registration method NCRNet [1], and DIRAC. We used the mean Dice similarity coefficient (DSC) to evaluate the similarity between predicted masks and the ground truth. As shown in Table 1, AUCseg fails to detect the lesion in T1 scans. Our proposed framework achieved the highest DSC result of 0.83, following post-processing.Ablation Study. We compared the performance of the InpNet trained with histogram matching (HM) and the SegNet trained with ground truth masks (Supervised). The results, shown in Table 1 and Fig. 2, demonstrate that domain differences between S and T have a significant effect on segmentation accuracy (without HM), leading to lower registration quality overall. Additionally, Fig. 4 shows an example of a pseudo image. We reconstructed the spatial correspondence by first using SegNet to localize the lesion and then using InpNet to inpaint it with the normal appearance. "
Co-learning Semantic-Aware Unsupervised Segmentation for Pathological Image Registration,4,Conclusion,"In this paper, we proposed a novel tri-net framework for joint image registration and unsupervised segmentation in medical imaging based on mutual information minimization in collaborative learning. Our experiments demonstrate that the proposed framework is effective for both atlas-based and longitudinal pathology image registration. We also observed that the accuracy of the segmentation network is significantly influenced by the quality of the inpainting, which, in turn, affects the registration outcome. In the future, our research will focus on enhancing the performance of InpNet to address domain differences better to improve the registration results."
Co-learning Semantic-Aware Unsupervised Segmentation for Pathological Image Registration,,Fig. 1 .,
Co-learning Semantic-Aware Unsupervised Segmentation for Pathological Image Registration,,Fig. 2 .,
Co-learning Semantic-Aware Unsupervised Segmentation for Pathological Image Registration,,Fig. 3 .,
Co-learning Semantic-Aware Unsupervised Segmentation for Pathological Image Registration,,Table 1 .Fig. 4 .,
Co-learning Semantic-Aware Unsupervised Segmentation for Pathological Image Registration,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 51.
S3M: Scalable Statistical Shape Modeling Through Unsupervised Correspondences,1,Introduction,"Statistical shape models (SSMs) are a powerful tool to characterize anatomical variations across a population. They have been widely used in medical image analysis and computational anatomy to represent organ structures, with numerous clinically relevant applications such as clustering, classification, and shape regression [2,7,21]. SSMs are generally represented by point-wise correspondences between shapes [4,13], or deformation fields to a pre-defined template [14,28]. Despite the existence of implicit models [2], abstracting shape correspondences in the form of linear point distribution models (PDM) constitutes an appealing and interpretable way to represent shape distributions [3]. Furthermore, many implicit models still rely on correspondence annotations during training [2,7].Creating SSMs is cumbersome and intricate, as significant manual human annotation is necessary. Domain experts typically first segment images in 3D. The labeled 3D organ surfaces must then be aligned and brought into correspondence, typically achieved through deformable image registration methods using manual landmark annotations [6]. This is labor-intensive and error-prone, potentially inducing bias into downstream SSMs and applications [39].Unsupervised methods have been proposed to estimate correspondences for SSMs [4,11,24]. However, they typically require precisely segmented and smooth surfaces to generate accurate inter-organ correspondences. ShapeWorks has been established to produce high-quality correspondences on several organs such as femurs or left atria [1,3,7]. However, as domain experts carefully curate most medical datasets, the robustness of such methods has not yet been thoroughly evaluated concerning label noise and segmentation inaccuracies. The main obstacles that prevent scaling SSMs to larger patient populations are unsupervised correspondence methods that can handle topological variations in noisy annotations, such as those produced by inexperienced annotators or predictions from deep neural networks. Robust methods to deal with these obstacles are required.To address these challenges, we propose S3M, which leverages unsupervised deep geometric features while incorporating a global shape structure. Geometric Deep Learning (GDL) provides techniques to process 3D shapes and geometries, which are robust to noise, 3D rotations, and global deformations. We utilize graph neural networks (GNN) and functional mappings to establish dense surface correspondences of samples without supervision. This approach has significant clinical implications as it enables automatically representing anatomical shapes across large patient populations without requiring manual expert landmark annotations. We demonstrate that our proposed method creates objectively superior SSMs from shapes with noisy surface topologies. Moreover, it accurately corresponds regions of complex anatomies with mesh bifurcations such as the heart, which could ease the modeling of inter-organ relations [12].Our contributions can be summarized as follows:-We propose a novel unsupervised correspondence method for SSM curation based on geometric deep learning and functional correspondence. -S3M exhibits superior performance on two challenging anatomies: thyroid and heart. It generates objectively more suitable SSMs from noisy thyroid labels and challenging multi-chamber heart reconstructions. -To pave the way for unsupervised SSM generation in other medical domains, we open-source the code of our pipeline."
S3M: Scalable Statistical Shape Modeling Through Unsupervised Correspondences,2,Related Work,"Point Distribution Models. A population of shapes must be brought into correspondence to construct a PDM. This has been traditionally achieved through pair-wise registration methods [19,21]. However, pairwise approaches can admit bias as they neglect the population during correspondence generation [19]. More recently, group-wise optimization methods such as ShapeWorks [11] have been adopted as they jointly optimize over a cohort, overcoming such biases. They demonstrate superior prediction of clinically relevant anatomical variations [19]. Furthermore, generic models that can perform well across various organs are sought after.Graph Neural Networks (GNNs) have been used to enable structural feature extraction through message passing. They constitute a powerful tool to process 3D data and extract geometric features [32,33,38] which can be useful for disease prediction [23,30]. Other medical applications involve brain cortex mesh reconstruction [8] and 3D organ-at-risk (ORA) segmentation [22]. We use GNNs for deformable 3D organ shapes and learn to estimate dense correspondences in the presence of noise and anatomical variations.Functional Correspondence. Functional maps abstract the notion of pointto-point correspondences by corresponding arbitrary functions, such as texture or curvature, across shapes. They are extensively used to estimate dense correspondences across deformable shapes [29] and can be incorporated into learning frameworks [17,26]. These methods are typically evaluated on synthetic meshes with dense annotations and limited variable surface topology. In contrast, medical shapes exhibit higher variability, requiring robust surface representation for reliable correspondence matching. More recently, unsupervised functional correspondence models have been proposed [10,31]. These methods demonstrate strong performance on synthetic data without manual correspondence anno-tations. They extract features from the surface geometry using hand-crafted descriptors such as SHOT [36], wave-kernel signatures (WKS) [5] or heat-kernel signatures (HKS) [9]. The extracted features are then typically refined and projected onto the Laplace-Beltrami Operator (LBO) eigenfunctions [29]. μMatch [24] recently leveraged such an unsupervised approach in the medical domain.They employ handcrafted features to extract representations from shapes with a relatively smooth surface topology; however, they fail for shapes with high degrees of surface noise or label inconsistencies. To scale SSM curation to larger datasets encompassing population variance, our method must be robust to a more variable and complex surface topology."
S3M: Scalable Statistical Shape Modeling Through Unsupervised Correspondences,3,Method,"In the following, we propose a method to establish an SSM as a Point Distribution Model (PDM), illustrated in Fig. 1. Robust local features from the surface mesh are extracted using GNNs. These features are then projected onto the truncated eigenspace of the Laplace-Beltrami Operator using m = 20 eigenfunctions [29]. We perform post-processing with a Product Manifold Filter (PMF) [37] to obtain bijective correspondences for SSM generation. The shape model is subsequently created by aggregating correspondences across a dataset of predicted correspondences using the eigendecomposition of the covariance matrix.Geometric Feature Description. Handcrafted descriptors [5,9,36] are unable to represent the complex surface topology of medical data adequately. To cope with surface artifacts and irregular morphologies, we use a graph-based descriptor [32]. We first extract a surface mesh from a 3D volumetric grid using marching cubes [27]. Graph nodes are defined as the mesh's vertices; edges are obtained using a k-nearest neighbor search with k = 10. Node features are given by spatial xyz-coordinates. The graph is fed into three topology adaptive layers [18] using graph convolutions with a specific number of hops to define the number of nodes a message is passed to. Increasing the number of hops (we use 1, 2, 3 hops per layer, respectively) increases the receptive field, incorporating features from more distant nodes. Finally, features pass a linear layer before being projected onto the Laplace-Beltrami eigenfunctions."
S3M: Scalable Statistical Shape Modeling Through Unsupervised Correspondences,,Deformable Correspondence Estimation.,"PDMs require correspondences between samples to model the statistical distribution of the organ. Inspired by methods for geometric shape correspondence, we propose to estimate a functional mapping T to correspond high-level semantics from two input shapes, X i and X j . The LBO extends the Laplace operator to Riemannian manifolds, capturing intrinsic characteristics of the shape independent of its position and orientation in Euclidean space. It can be efficiently computed on a surface mesh using, for example, the cotangent weight discretization scheme [15]. This results in a matrix representation of the LBO from which one can then calculate the associated eigenfunctions ψ i ∈ R n×m for a shape X i ∈ R n×3 . Given a feature vector D i extracted from a surface mesh of shape X i and a neural network T φ , we can approximate a functional mapping between shapes by solving the following optimization problem:Here, A T φ (Di) ∈ R m×m denotes the transformed descriptor, written in the basis of the LBO eigenfunctions of shape X i and C ij ∈ R m×m represents the optimal functional mapping from the descriptor space of X i to the one of X j . Inspired by existing works on shape correspondence [10,31,34], our loss function enforces four separate characteristics on the learned functional mapping, including bijectivity, orthogonality, and isometric properties. We refer to the supplementary materials for the complete definition. Notably, none of these losses uses ground truth correspondences, making the entire process unsupervised."
S3M: Scalable Statistical Shape Modeling Through Unsupervised Correspondences,,Training and Inference.,"During training, two shapes are sampled from the dataset, and the pipeline is optimized with Eq. 1. We increase model robustness by augmenting with rotations and small surface deformations. The point cloud is sub-sampled in each training iteration using farthest point sampling with random initialization. During inference, our model predicts pairwise correspondences. To accumulate these over an entire dataset of N shapes, we choose a template shape X T = arg min Xi N j=0 1 i =j L (C ij , C ji ) as the instance with the lowest average loss to all other shapes in the dataset. As in [29], we extract  point-to-point correspondences between the template X T and another shape X i by matching the transformed LBO eigenfunctions of X i , namely C iT ψ i , with the LBO eigenfunctions of the template ψ T using nearest neighbors. As PDMs require bijective correspondences, we subsequently post-process the results with PMF [37].Statistical Shape Modeling. We use the PDM [13] as the underlying method of the shape model. It takes input points of the form X ∈ R N ×d , where N , d are the number of shape samples and coordinates per shape, respectively. It returns a multi-variate normal distribution. In our case, d = 3n given each shape has n points. We calculate the mean shape X and the empirical covariance matrix S = cov(X) over the N samples [13]. Since S has rank N -1, it has N -1 real eigenvectors v j with eigenvalues λ j . If we consider the sumthen s ∼ N X, S , which is the desired distribution of the model. For the above, the points must be in correspondence across the samples. We thus use the correspondences generated in Sect. 3 to construct the PDM."
S3M: Scalable Statistical Shape Modeling Through Unsupervised Correspondences,4,Experiments,"All experiments are carried out using two publicly available datasets: thyroid ultrasound scans and heart MRI acquisitions. Our model is implemented in PyTorch 1.12 using CUDA 11.6. Training takes between 2.5-3 h on an Nvidia A40 GPU and inference about 0.71 s for a pair of shapes. We use publically available implementations for all baseline methods.Thyroid Dataset (SegThy) [25]. The dataset comprises 3D freehand US scans of healthy thyroids from 32 volunteers aged 24-39. For each volunteer, three physicians acquired three scans each. Ultrasound scans generally exhibit noise induced by physical properties such as phase aberrations and attenuation. This leads to label inconsistencies or topological irregularities that pose a challenge for shape modeling (see Fig. 2). US sweeps were compounded to a 3D grid of resolution 0.12 × 0.12 × 0.12 mm 3 . A single scan from each of the 16 volunteers was manually annotated by experts (ground truth) and used to train QuickNAT [25]. The remaining scans were pseudo-labeled through QuickNAT segmentation predictions exhibiting moderate degrees of noise and inaccuracies (dice score of 0.94 [25]). We divide the dataset into manual and pseudo-label predictions and evaluate them separately. The pseudo-label experiment evaluates the model's performance under topological noise and inaccuracies and is limited to 100 scans due to ShapeWorks memory constraints [20]. We extract a surface mesh for each scan using marching cubes and subsample the meshes to 5000 vertices.Heart Dataset [35]. The data constitutes 30 MRI scans from a single cardiac phase of the heart. Each image has a voxel resolution of 1.25×1.25×2.7 mm 3 . Segmentation is carried out using an automated method, with subsequent manual corrections by domain experts. Labels are provided for: the right/left ventricle, right/left atrium, aorta, and pulmonary artery. We evaluate the capability of the models to reconstruct complex organs using three hierarchical compositions of the heart chambers. Composition 1 consists of the right ventricle, Composition 2 of the left and right atrium and ventricle, and Composition 3 of the whole heart, including the aorta and pulmonary artery. SSM Evaluation. We compare Shapeworks [11], μMatch [24] and SURFMNet [31], with S3M. A four-fold cross-validation is employed. SURFMNet, μMatch, and S3M are trained on the training folds and correspondences are predicted on the training and validation set. Since the particle-based optimization of Shapeworks does not generalize to unseen data, it uses all folds for correspondence estimation. The SSM is built using correspondences from the training set, and evaluated with respect to two standardized metrics: generality and specificity [16]. For generality, we measure how well the SSM can represent unseen instances from the fourth fold through the Chamfer distance between the original shape and its SSM reconstruction. "
S3M: Scalable Statistical Shape Modeling Through Unsupervised Correspondences,5,Results and Discussion,"Experiment 1: Thyroid SSM. Table 1 depicts the performance for all methods on the thyroid shapes. SURFMNet results for the thyroid pseudo-labels are omitted, as the method did not converge. Consistent trends can be observed across all methods for both sets of thyroid labels. The two existing functional map-based methods were outperformed by Shapeworks, while the proposed S3M exceeded the latter's scores. The descriptor is the most significant difference between the three learned functional map methods. Hand-crafted shape descriptors like SHOT and a simple fully-connected residual network architecture do not adequately represent thyroids' noisy and heterogeneous shapes.Our proposed method significantly outperformed Shapeworks in all metrics except the specificity of pseudo-labeled thyroids, where the results are not statistically significant. This was despite the advantage of optimizing correspondences across all shapes in training and validation. S3M can better cope with topological noise and generalizes to unseen samples, demonstrating potential in scaling SSM generation to larger datasets. Furthermore, it does not suffer from increasing memory requirements with the number of samples.  2 depicts the results of the different models on the three heart chamber compositions as previously defined. For the single-organ right atrium (composition 1), our proposed method fares comparably to ShapeWorks. For the more complex compositions 2 and 3, we observe larger increases in generalization and specificity for Shapeworks. μMatch fails to create a convincing SSM for any heart composition. Interestingly, SURFMNet can represent the more complex compositions 2 and 3 better than ShapeWorks, showing the strength of functional maps at representing complex high-level structures. S3M still exceeded the performance of SURFMNet, possibly due to the graph descriptor being better able to represent the surface topology.From the qualitative results in Fig. 3, it becomes more apparent that Shape-Works does not generate an adequate SSM for the more complex compositions. This further supports our proposed method's ability to learn correspondences for intricate and complex surface topologies, even consisting of meshes with bifurcations. The flexibility of our surface representation enables unsupervised correspondence estimation from multiple hierarchical sub-shapes, which is invaluable in multi-organ modeling such as for the heart [6,12]."
S3M: Scalable Statistical Shape Modeling Through Unsupervised Correspondences,,Experiment 3: Thyroid Pseudo-label Generalization,"To further highlight the proposed methods' robustness to network-generated segmentation labels, we additionally measure the reconstruction ability of SSMs created from pseudo-labels on manually annotated thyroid labels under the Chamfer distance (Ours: 1.05 ± 0.10 mm, Shapeworks: 1.84 ± 0.40 mm). Notably, the proposed PDM on pseudo-labels generalizes better than the SSM built on few manual labels (1.25 ± 0.11 mm; see Table 1), suggesting that more data can improve the SSM even if the labels are inaccurate. This is further supported by differences in shape (suppl. Figure 1); the SSM's mean shape generated from pseudo-labels approximates the mean shapes on GT labels (and thus, the true organ shape) more closely."
S3M: Scalable Statistical Shape Modeling Through Unsupervised Correspondences,6,Conclusion,"We present an unsupervised approach for learning correspondences between shapes that exhibit noisy and irregular surface topologies. Our method leverages the strengths of geometric feature extractors to learn the intricacies of organ surfaces, as well as high-level functional bases of the Laplace-Beltrami operator to capture more extensive organ semantics. S3M outperforms existing methods on both manual labels, and label predictions from a network, demonstrating the potential to scale existing SSM pipelines to datasets that encompass more substantial population variance without additional annotation burden. Finally, we show that our model has the potential to learn correspondences between complex multi-organ shape hierarchies such as chambers of the heart, which would ease the manual burden of SSM curation for structures that currently still require meticulous manual landmark annotations."
S3M: Scalable Statistical Shape Modeling Through Unsupervised Correspondences,,Fig. 1 .,
S3M: Scalable Statistical Shape Modeling Through Unsupervised Correspondences,,Fig. 2 .,
S3M: Scalable Statistical Shape Modeling Through Unsupervised Correspondences,,Fig. 3 .,
S3M: Scalable Statistical Shape Modeling Through Unsupervised Correspondences,,Table 1 .,
S3M: Scalable Statistical Shape Modeling Through Unsupervised Correspondences,,,
S3M: Scalable Statistical Shape Modeling Through Unsupervised Correspondences,,Table 2 .,Experiment 2: Whole Heart SSM. Table
S3M: Scalable Statistical Shape Modeling Through Unsupervised Correspondences,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 44.
Generating High-Resolution 3D CT with 12-Bit Depth Using a Diffusion Model with Adjacent Slice and Intensity Calibration Network,1,Introduction,"Unlike natural images that are typically processed in 8-bit depth, medical images, including X-ray, CT, and MR images, are processed in 12-bit or 16-bit depth to retain more detailed information. Among medical images, CT images are scaled using a quantitative measurement known as the Hounsfield unit (HU), which ranges from -1024 HU to 3071 HU in 12-bit depth. However, in both clinical practice and research, the dynamic range of HU is typically clipped to emphasize the region of interest (ROI). Such clipping of CT images, called windowing, can increase the signal-to-noise ratio (SNR) in the ROI. Therefore, most research on CT images performs windowing as a pre-processing method [1,2].Recent advancements in computational resources have enabled the development of 3D deep learning models such as 3D classification and 3D segmentation. 3D models have attracted much attention in the medical domain because they can utilize the 3D integrity of anatomy and pathology. However, the access to 3D medical imaging datasets is severely limited due to the patient privacy. The inaccessibility problem of 3D medical images can be addressed by generating high quality synthetic data. Some researches have shown that data insufficiency or data imbalance can be overcome using a welltrained generative model [3,4]. However, generating images with intact 3D integrity is very difficult. Moreover, generating high-quality images [5] in the 12-bit depth, which is used in real clinical settings, is even more challenging.The present study proposes a 2D-based 3D-volume generation method. To preserve the 3D integrity and transfer spatial information across adjacent axial slices, prior slices are utilized to generate each adjacent axial slice. We call this method Adjacent Slicebased Conditional Iterative Inpainting, ASCII. Experiments demonstrated that ASCII could generate 3D volumes with intact 3D integrity. Recently, score-based diffusion models have shown promising results in image generation [6][7][8][9][10][11][12], super resolution [13] and other tasks [14][15][16][17]. Therefore, ASCII employs a score-based diffusion model to generate images in 12-bit depth. However, since the images were generated in 12-bit depth, errors in the average intensity arose when the images were clipped to the brain parenchymal windowing range. To solve this issue, we propose a trainable intensitycalibration network (IC-Net) that matches the intensity of adjacent slices, which is trained in a self-supervised manner."
Generating High-Resolution 3D CT with 12-Bit Depth Using a Diffusion Model with Adjacent Slice and Intensity Calibration Network,2,Related Works,"Score-based generative models [7][8][9] and denoising diffusion probabilistic models (DDPMs) [6,16] can generate high-fidelity data without an auxiliary network. In contrast, generative adversarial networks (GANs) [18] require a discriminator and variational auto-encoders (VAEs) [19] require a Gaussian encoder. Score-based generative models and diffusion models have two processing steps: a forward process that creates perturbed data with random noise taken from a pre-defined noise distribution in each step, and a backward process that denoises the perturbed data using a score network. The perturbation methods were defined as the stochastic differential equation (SDE) in [8]. A continuous process was defined as {x(t)} T t=0 with x(0) ∼ p data and x(T ) ∼ p T , where t ∈ [0, 1] and p data , p T are the data distribution and prior noise distribution, respectively. The forward process was defined as the following SDE:where f and g are the coefficients of the drift and diffusion terms in the SDE, respectively, and w induces the Wiener process (i.e., Brownian motion). The backward process was defined as the following reverse-SDE:where w is the backward Wiener process. We define each variance σ t as a monotonically increasing function. To solve the reverse-SDE given by above equation we train a score network S θ (x, t) to estimate the score function of the perturbation kernel ∇xlogp t (x t |x 0 ). Therefore, the objective of the score network is to minimize the following loss function: 3 Adjacent Slice-Based Conditional Iterative Inpainting, ASCII"
Generating High-Resolution 3D CT with 12-Bit Depth Using a Diffusion Model with Adjacent Slice and Intensity Calibration Network,3.1,Generating 12-Bit Whole Range CT Image,"VESDEs experimented on four noise schedules and two GAN models [20][21][22] were compared for 12-bit whole range generation. As shown in Fig. 1, setting σ min to 0.01 generated a noisy image (third and fifth column) whereas the generated image was well clarified when σ min was reduced to 0.001 (forth and last column). However, in two GAN models, the important anatomical structures, such as white matter and grey matter, collapsed in the 12-bit generation. The coefficient of variation (CV), which is a measure used to compare variations while eliminating the influence of the mean, measured in the brain parenchyma of the generated images (excluding the bones and air from the windowing) at both noise levels [23]. It is observed that the anatomical structures tended to be more distinguishable when the CV was low. Furthermore, a board-certified radiologist also qualitatively assessed that the images generated with a lower σ min showed a cleaner image. It can be interpreted that reducing σ min lowers CV and therefore improves the image quality.For quantitative comparison, we randomly generated 1,000 slices by each parameter and measured the CV. As shown Fig. 1, the variance of CV was lowest when σ min and σ max were set to 0.001 and 68, respectively. Also, setting σ max to 1,348 is theoretically plausible according to previous study [9] because we preprocessed CT slices in the range of -1 to 1. Finally, we fixed σ min and σ max to 0.001 and 1,348, respectively in subsequent experiments.In the case of GAN, as the convolution operator can be described as a high-pass filter [24], GANs trained through the discriminator's gradient are vulnerable to generating precise details in the low-frequency regions. Therefore, while anatomy such as bone (+1000HU) and air (-500HU) with strong contrast and high SNR are well-generated, anatomy such as parenchyma (20HU-30HU) with low contrast and SNR are difficult to generate accurately. As shown in the first and second column of Fig. 1, we can see that the GAN models seem to generate anatomy with strong contrast and high SNR well but failed to generate the others. The SDE's scheduling can impact how the score-based diffusion model generates the fine-grained regions of images. To generate high-quality 12-bit images, the diffusion coefficient must be set to distinguish 1HU (0.00049). By setting σ max to 1,348 and σ min to 0.001, the final diffusion coefficient (0.00017) was set to a value lower than 1HU. In other words, setting the diffusion coefficient (0.00155) to a value greater than 1HU can generate noisy images. The detailed description and calculation of diffusion coefficient was in Supplementary Material."
Generating High-Resolution 3D CT with 12-Bit Depth Using a Diffusion Model with Adjacent Slice and Intensity Calibration Network,3.2,Adjacent Slice-Based Conditional Iterative Inpainting,"To generate a 3D volumetric image in a 2D slice-wise manner, a binary mask was used, which was moved along the channel axis. A slice x 0 = {-1024HU } D filled with intensity of air was padded before the first slice x 1 of CT and used as the initial seed. Then, the input of the model was given by x t : x t+K-1 , where t ∈ [0, N s -K + 1] and N s and K are the total slice number of CT and the number of contiguous slices, respectively. In addition, we omit augmentation because the model itself might generate augmented images. After training, the first slice was generated through a diffusion process using the initial seed. The generated first slice was then used as a seed to generate the next slice in an autoregressive manner. Subsequently, the next slices were generated through the same process. We call this method adjacent slice-based conditional iterative inpainting, ASCII.Two experiments were conducted. The first experiment preprocessed the CT slices by clipping it with brain windowing [-10HU , 70HU ] and normalizing it to [-1, 1] with σ min set to 0.01 and σ max set to 1,348, while the second experiment preprocessed the whole windowing [-1024HU , 3071HU ] and normalized it to [-1, 1] with σ min set to 0.001 and σ max set to 1,348. As shown Fig. 2, the white matter and gray matter in the first experiment (brain windowing) could be clearly distinguished with maintained continuity of slices, whereas they were indistinguishable and remained uncalibrated among axial slices in the second experiment (whole range)."
Generating High-Resolution 3D CT with 12-Bit Depth Using a Diffusion Model with Adjacent Slice and Intensity Calibration Network,3.3,Intensity Calibration Network (IC-Net),"It was noted that the intensity mismatch problem only occurs in whole range generation. To address this issue, we first tried a conventional non-trainable post-processing, such as histogram matching. However, since each slice has different anatomical structure, the histogram of each slice image was fitted to their subtle anatomical variation. Therefore, anatomical regions were collapsed when the intensities of each slice of 3D CT were calibrated using histogram matching. Finally, we propose a solution for this intensity mismatching, a trainable intensity calibration network: IC-Net.To calibrate the intensity mismatch, we trained the network with a self-supervised manner. First, adjacent two slices from real CT images, x t , x t+1 were clipped using the window of which every brain anatomy HU value can be contained. Second, the intensity of x t+1 in ROI is randomly changed and the result is x t+1 c = x t+1x t+1 * µ + x t+1 , where x t+1 and μ are the mean of x t+1 and shifting coefficient, respectively. And µ was configured to prevent the collapse of anatomical structures.Finally, intensity calibration network, IC-Net was trained to calibrate the intensity of x t+1 to the intensity of x t . The objective of IC-Net was only to calibrate the intensity of x t and preserve both the subtle texture and the shape of a generated slice. The IC-Net uses the prior slice to calibrate the intensity of generated slice. The objective function of IC-Net is given by,As shown in Fig. 3, some important anatomical structures, such as midbrain, pons, medulla oblongata, and cerebellar areas, are blurred and collapsed when histogram matching was used. It can be risky as the outcomes vary depending on the matching seed image. On the other hand, the anatomical structure of the IC-Net matched images did not collapse. Also, the IC-Net does not require to set the matching seed image because it normalizes using the prior adjacent slice. "
Generating High-Resolution 3D CT with 12-Bit Depth Using a Diffusion Model with Adjacent Slice and Intensity Calibration Network,4,Experiments,
Generating High-Resolution 3D CT with 12-Bit Depth Using a Diffusion Model with Adjacent Slice and Intensity Calibration Network,4.1,ASCII with IC-Net in 12-Bit Whole Range CT Generation,"The description of the dataset and model architecture is available in the Supplementary Material. We experimented ASCII on continuous K slices of K = 2 and 3 and called them ASCII(2) and ASCII(3), respectively. We generated a head & neck CT images via ASCII(2) and ASCII(3) with and without IC-Net, and slice-to-3D VAE [25]. Figure 4 demonstrate the example qualitative images. The 3D generated images were shown both in whole range and brain windowing range. The results showed that the both ASCII(2) and ASCII(3) were well calibrated using IC-Net. Also, anatomical continuity and the 3D integrity is preserved while the images were diverse enough. However, there was no significant visual difference between ASCII(2) and ASCII (3). Although the results in whole range appear to be correctly generated all models, the results in brain windowing range showed the differences. The same drawback of convolution operation addressed in the 12-bit generation of GAN based models, which was shown in Fig. 1, was also shown in slice-to-3D VAE.The quantitative results in whole range are shown in Table 1. The mid-axial slice, midsagittal slice, and mid-coronal slice of the generated volumes were used to evaluate the Fréchet Inception Distance (FID) score, which we designated as FID-Ax, FID-Sag, and FID-Cor, respectively. And multi-scales structural similarity index measure (MS-SSIM) and batch-wise squared Maximum Mean Discrepancy (bMMD 2 ) were also evaluated for quantitative metrics. In general, quantitative results indicate that ASCII(2) performs better than ASCII(3). It's possible that ASCII(3) provides too much information from prior slices, preventing it from generating sufficiently diverse images. Additionally, IC-Net significantly improved generation performance, especially in the windowing range. The FID-Ax of ASCIIs was improved by IC-Net from 15.250 to 14.993 and 18.127 to 16.599 in the whole range, respectively. Also, the performance of FID-Cor and FID-Sag had significantly improved when IC-Net was used. The MS-SSIM showed that ASCIIs can generated it diverse enough. The FID-Ax, FID-Cor, and FID-Sag scores of ASCIIs with IC-Net were improved in windowing range. The FID-Ax of ASCIIs was improved by IC-Net from 15.770 to 14.656 and 20.145 to 15.232 in the windowing range, respectively. On the other hand, ASCIIs without IC-Net had poor performance in the windowing range and this means that even when IC-Net is used, structures do not collapse. "
Generating High-Resolution 3D CT with 12-Bit Depth Using a Diffusion Model with Adjacent Slice and Intensity Calibration Network,4.2,Calibration Robustness of IC-Net on Fixed Value Image Shift,"To demonstrate the performance of IC-Net, we conducted experiments with the 7, 14, 21 and 28th slices, which sufficiently contain complex structures to show the calibration performances. The previous slice was used as an input to the IC-Net along with the target slice whose pixel values were to be shifted. And the absolute errors were measured between GT and predicted slice using IC-Net.As shown in Fig. 5, it worked well for most shifting coefficients. The mean absolute error was measured from 1HU to 2HU when the shifting coefficient was set from 0.7 to 1.1. However, the errors were exploded when shifting coefficient was set to 1.2 or 1.3. It was because the images were collapsed when shifting coefficient increases than 1.2 since the intensity deviates from the ROI range [-150HU , 150HU ]. Nevertheless, IC-Net can calibrate intensity to some extent even in the collapsed images as shown Fig. 5. "
Generating High-Resolution 3D CT with 12-Bit Depth Using a Diffusion Model with Adjacent Slice and Intensity Calibration Network,4.3,Visual Scoring Results of ASCII with IC-Net,"Due to the limitations of slice-based methods in maintaining connectivity and 3D integrity, an experienced radiologist with more than 15 years of experience evaluated the images. Seeded with the 13th slices of real CT scans, in which the ventricle appears, ASCII(2) with IC-Net generated a total of 15 slices. Visual scoring shown in Table . 2 on a three-point scale was conducted blindly for 50 real and 50 fake CT scans, focusing on the continuity of eight anatomical structures. Although most of the fake regions were scored similarly to the real ones, the basilar arteries were evaluated with broken continuity. The basilar artery was frequently not generated, because it is a small region. As the model was trained on 5-mm thickness non-contrasted enhanced CT scans, preserving the continuity of the basilar artery is excessively demanding. "
Generating High-Resolution 3D CT with 12-Bit Depth Using a Diffusion Model with Adjacent Slice and Intensity Calibration Network,5,Conclusion,"We proposed a high-performance slice-based 3D generation method (ASCII) and combined it with IC-Net, which is trained in a self-supervised manner without any annotations. In our method, ASCII generates a 3D volume by iterative generation using previous slices and automatically calibrates the intensity mismatch between the previous and next slices using IC-Net. This pipeline is designed to generate high-quality medical image, while preserving 3D integrity and overcoming intensity mismatch caused in 12-bit generation. ASCII had shown promising results in 12-bit depth whole range and windowing range, which are crucial in medical contexts. The integrity of the generated images was also confirmed in qualitative and quantitative assessment of 3D integrity evaluations by an expert radiologist. Therefore, ASCII can be used in clinical practice, such as anomaly detection in normal images generated from a seed image [26]. In addition, the realistic 3D images generated by ASCII can be used to train deep learning models [3,4] in medical images, which frequently suffer from data scarcity."
Generating High-Resolution 3D CT with 12-Bit Depth Using a Diffusion Model with Adjacent Slice and Intensity Calibration Network,,Fig. 1 .,
Generating High-Resolution 3D CT with 12-Bit Depth Using a Diffusion Model with Adjacent Slice and Intensity Calibration Network,,Fig. 2 .,
Generating High-Resolution 3D CT with 12-Bit Depth Using a Diffusion Model with Adjacent Slice and Intensity Calibration Network,,Fig. 3 .,
Generating High-Resolution 3D CT with 12-Bit Depth Using a Diffusion Model with Adjacent Slice and Intensity Calibration Network,,Fig. 4 .,
Generating High-Resolution 3D CT with 12-Bit Depth Using a Diffusion Model with Adjacent Slice and Intensity Calibration Network,,Fig. 5 .,
Generating High-Resolution 3D CT with 12-Bit Depth Using a Diffusion Model with Adjacent Slice and Intensity Calibration Network,,Table 1 .,
Generating High-Resolution 3D CT with 12-Bit Depth Using a Diffusion Model with Adjacent Slice and Intensity Calibration Network,,Table 2 .,
Building a Bridge: Close the Domain Gap in CT Metal Artifact Reduction,1,Introduction,"Metal implants can heavily attenuate X-rays in computed tomography (CT) scans, leading to severe artifacts in reconstructed images. It is essential to remove metal artifacts in CT images for subsequent diagnosis.Recently, with the emergence of deep learning (DL) [21,22], many DLbased approaches have been proposed for metal artifact reduction (MAR) and achieved encouraging results. These methods can be roughly classified into three groups: supervised, unsupervised, and semi-supervised MAR methods. Supervised MAR methods [2,8,10,12,19,20,23,24] directly learn the mapping from synthetic metal-affected data to metal-free one under the guidance of the desired data. Then the learned models are applied to the clinical data. Unfortunately, due to the domain gap between synthetic and clinical data, poor generalization performance usually occurs, leading to unexpected results. Unsupervised MAR methods [5,9,27] can avoid the problem since their training and application are both on the clinical data. Nonetheless, the absence of supervision information makes it easy to distort the anatomical structure in the corrected results. Recently, several semi-supervised MAR methods have been proposed. SFL-CNN [17] and a variant of ADN [9] denoted as SemiADN [14] are two representative works, which utilize the same network to deal with synthetic and clinical data simultaneously. These methods inherit the advantages of both supervised and unsupervised MAR methods, but the mentioned-above domain gap problem remains. In these works, the network attempts to find a balance between the synthetic and clinical data but ultimately results in an unsatisfactory outcome in both domains, leaving room for further improvement.In this work, our goal is to explicitly reduce the domain gap between synthetic and clinical metal-corrupted CT images for improved clinical MAR performance. Some domain adaptation-based networks [7,16,18,25] are designed to close the domain gap and they usually assume that there is a one-to-one correspondence between two domains, i.e. bijection, which is implemented via the constraint of cycle consistency loss. However, this assumption is too harsh to accurately respond to the facts of real situations. Furthermore, when the model learns an identical transformation, this assumption is still met. Hence, maintaining the diversity of image generation is another challenge.To close the domain gap, we propose a novel semi-supervised MAR framework. In this work, the clean image domain acts as the bridge, where the bijection is substituted with two simple mappings and the strict assumption introduced by the cycle-consistency loss is relaxed. Our goal is to convert simulated and clinical metal-corrupted data back and forth. As an intermediate product, clean images are our target. To improve the transformations of two metal-corrupted images into metal-free ones, we propose a feature selection mechanism, denoted as Artifact Filtering Module (AFM), where AFM acts as a filter to eliminate features helpless in recovering clean images."
Building a Bridge: Close the Domain Gap in CT Metal Artifact Reduction,2,Method,
Building a Bridge: Close the Domain Gap in CT Metal Artifact Reduction,2.1,Problem Formulation,"The metal corruption process can be formulated as a linear superposition model as [10,11]:where X ma , X free , and A represent metal-affected CT images, metal-free CT images, and metal artifacts, respectively. X ma is the observation signal and X free is the target signal to be reconstructed. "
Building a Bridge: Close the Domain Gap in CT Metal Artifact Reduction,2.2,Overview,"Figure 1 presents the overall architecture of our proposed method. Let I s be the domain of all synthetic metal-corrupted CT images and I c be the domain of all clinical metal-corrupted CT images. The generators aim to convert them to clean CT image domain I f , where different generators take metal-corrupted CT images from different domains. I f takes the role of a bridge to close the domain gap. The following subsections present the details of these image translation branches."
Building a Bridge: Close the Domain Gap in CT Metal Artifact Reduction,2.3,Image Translation,"According to Eq. 1, if two metal artifact reduction translations are completed, the subsequent two transformations can be obtained by subtracting the output of the network from the original input. Therefore, only two translators are needed.The first translator is used to convert I s into I f , denoted as G s2f and the second translator is used to convert I c into I f , denoted as G c2f . In this work, two translators, G s2f and G c2f , share the same network architecture, consisting of one encoder and one decoder, as shown in Fig. 1 "
Building a Bridge: Close the Domain Gap in CT Metal Artifact Reduction,,(b) and (d).,"Encoding Stage. In most DL-based MAR methods, the networks take a metalcorrupted CT image as input and map it to a metal-free CT image domain.Noise-related features are involved in the entire process, which negatively affects the restoration of clean images. In the principal component analysis (PCA)-based image denoising method [1], by retaining only the most important features, noise and irrelevant information are eliminated. In this work, we propose an artifact filtering module (AFM) for feature selection. At the encoding step, feature maps that contribute little to the reconstruction are also considered to be related to noise, where the encoder acts as a filter and only allows useful information to pass through. Specifically, there are two criteria for feature selection: 1) the selected feature maps contain as much information as possible, which is assessed by Variance (V ar), and 2) the correlation between the selected feature maps should be as small as possible, which is measured by covariance (Cov). Finally, each feature map gets a score as:where λ is a small constant to prevent division by zero and we set λ = 1e -7 in this paper. Feature maps with high scores will be selected. Therefore, we can dynamically select different feature maps according to the inputs.Decoding Stage. At the encoding stage, features that are helpless to reconstruct the clean image are filtered out. Decoder then maps the remaining features, which contain useful information, back into the image domain. To push the generated image to fall into the clean image domain, we employ conditional normalization layers [4,15] and propose a metal-free spatially aware module (MFSAM). The mean and variance of the features are modulated to match those of the metal-free image style by the MFSAM. The details of MFSAM are illustrated in Fig. 1 (c)."
Building a Bridge: Close the Domain Gap in CT Metal Artifact Reduction,,Metal Artifacts Reduction and Generation Stage.,"The framework consists of four image translation branches: two metal artifacts reduction branches I s ←I f and I c ←I f , and two metal artifact generation branches I f ←I s and I f ←I c .(1) I s ←I f : In this transformation, we employ G s2f to learn the mapping from the synthetic metal-affected image domain to the metal-free image domain, which is denoted as:where X s is synthetic metal-affected CT image in I s and X s2f is the corrected result of X s . According to Eq. 1, the metal artifacts A s can be obtained as follows:(2) I c ←I f : In this transformation, we use G c2f to learn the mapping from the clinical metal-affected image domain to the metal-free image domain, resulting in metal-corrected CT image X c2f and the metal artifacts A c . This process is the same as the transformation of I s ←I f and can be formulated as follows:where X c is clinical metal-affected CT image in I c .(3): I f ←I s : We use the artifacts of X s to obtain a synthetic domain metalcorrupted image X c2s by adding A s to the learned metal-free CT image X c2f :(4): I f ←I c : Synthesizing clinical domain metal-corrupted image X s2c can be achieved by adding A c to the learned metal-free CT image X s2f :"
Building a Bridge: Close the Domain Gap in CT Metal Artifact Reduction,2.4,Loss Function,"In our framework, the loss function contains two parts: adversarial loss and reconstruction loss.Adversarial Loss. Due to the lack of paired clinical metal-corrupted and metalfree CT images, as well as paired clinical and synthetic metal-corrupted CT images, we use PatchGAN-based discriminators, D f , D s , and D c , and introduce an adversarial loss for weak supervision. D f learns to distinguish whether an image is a metal-free image, D s learns to determine whether an image is a synthetic metal-affected CT image, and D c learns to determine whether an image is a clinical metal-affected CT image. The total adversarial loss L adv is written as:Reconstruction Loss. The label X gt of X s is employed to guide the G s2f to reduce the metal artifacts. The reconstruction loss L s2f on X x2f can be formulated as:When X syn is transformed into the clinical domain, G c2f can also reduce the metal artifacts with the help of X gt . The reconstruction loss L sc2f on X sc2f can be formulated as:where X sc2f is the MAR results of X s2c .To obtain optimal MAR results, it is necessary to remove any noise-related features while preserving as much of the content information as possible. When the input image is already metal-free, the input image has no noise-related features, and the reconstructed image should not suffer from any information loss. Here, we employed the model error loss to realize this constrain:where X f 2f is a reconstructed image from X f using G s2f and X f 2f is a reconstructed image from X f using G c2f .  Overall Loss. The overall loss function is defined as follows:where λ recon is the weighting parameter and as suggested by [9,14], it was set as 20.0 in our work."
Building a Bridge: Close the Domain Gap in CT Metal Artifact Reduction,3,Experiments,
Building a Bridge: Close the Domain Gap in CT Metal Artifact Reduction,3.1,Dataset and Implementation Details,"In this work, we used one synthesized dataset and two clinical datasets, denoted as SY, CL1 and CL2, respectively. The proposed method was trained on SY and CL1. For data simulation, we followed the procedure of [26] and used the metal-free CT images of the Spineweb dataset [3]. We set the threshold to 2,000 HU [20,24] to obtain 116 metal masks from metal-affected CT images of the Spineweb dataset, where 100 masks are for training and the remaining 16 masks are for testing. We used 6,000 synthesized pairs for training and 2,000 pairs for evaluation. For CL1, we randomly chose 6,000 metal-corrupted CT images and 6,000 metal-free CT images from Spineweb for training and another 224 metal-corrupted CT images for testing. For CL2, clinical metal-corrupted CT images were collected from our local hospital to investigate the generalization performance. The model was implemented with the PyTorch framework and optimized by the Adam optimizer with the parameters (β 1 , β 2 ) = (0.5, 0.999).The learning rate was initialized to 0.0001 and halved every 20 epochs. The network was trained with 60 epochs on one NVIDIA 1080Ti GPU with 11 GB memory, and the batch size was 2."
Building a Bridge: Close the Domain Gap in CT Metal Artifact Reduction,3.2,Comparison with State-of-the-Art Methods,"The proposed method was compared with several classic and state-of-the-art (SOTA) MAR methods: LI [6], NMAR [13], ADN [9], β-cycleGAN [5] and SemiADN [14]. LI and NMAR are traditional MAR methods. ADN and β-cycleGAN are SOTA unsupervised MAR methods. SemiADN is a SOTA semisupervised MAR method. Structural similarity (SSIM) and peak signal-to-noise ratio (PSNR) were adopted as quantitative metrics. Our intuition for determining the number of feature maps selected in AFM is based on the observation that the majority of information in an image is typically related to its content, while noise-related features are few. Therefore, in our work, during the encoding stage, we discarded 2 out of 32 and 4 out of 64 feature maps at the first and second down-sampling stages, respectively."
Building a Bridge: Close the Domain Gap in CT Metal Artifact Reduction,,MAR Performance on SY:,"The quantitative scores are presented in Table 1.The sizes of the 16 metal implants in the testing dataset are: [254,274,270,262,267,363,414,441,438,445,527,732,845,889,837,735] in pixels. They are divided into five groups according to their sizes. It is observed that all methods significantly improve both SSIM and PSNR scores compared with uncorrected CT images. Aided by supervision, SemiADN obtains higher quantitative scores than ADN. Compared with these SOTA unsupervised and semi-supervised MAR methods, our method achieves the best quantitative performance. Figure 2 shows the visual comparisons on SY. The proposed method outperforms all other methods in artifact suppression and effectively preserves anatomical structures around metallic implants, thereby demonstrating its effectiveness.MAR Performance on CL1: Figure 3 presents three representative clinical metal-affected CT images with different metallic implant sizes from small to large. When the metal is small, all methods can achieve good MAR performance but there are differences in tissue detail preservation. ADN and β-cycleGAN are more prone to lose details around the metal. In SemiADN, the missing details are recovered with the help of the supervision signal. However, in the second case, more artifacts are retained in SemiADN than ADN and β-cycleGAN. Compared with these MAR methods, our method is well-balanced between detail preservation and artifact reduction. When the metallic implant gets larger, as shown in  the third case, other methods are limited to reduce artifacts, and SemiADN even aggravates the impact of artifacts. Fortunately, our method is able to effectively suppress metal artifacts, thus demonstrating its potential for practical clinical use."
Building a Bridge: Close the Domain Gap in CT Metal Artifact Reduction,,MAR Performance on CL2:,"To assess the generalization capability of our method, we further evaluated MAR performance on CL2 with the model trained on SYN and CL1. Two practical cases are presented in Fig. 4. ADN and Semi-ADN fail to deal with severe artifacts and even introduce a deviation of HU value, while β-cycleGAN shows a certain ability to suppress these artifacts. Nonetheless, our proposed method outperforms β-cycleGAN in terms of artifact suppression and detail preservation. It can be seen that our method exhibits good generalization ability, which means it can effectively address metal artifacts even in scenarios where the simulated data and clinical metal-corrected data have different clean image domains. It shows the robustness of our proposed method across different imaging configurations."
Building a Bridge: Close the Domain Gap in CT Metal Artifact Reduction,3.3,Ablation Study,"In this section, we investigate the effectiveness of the proposed AFM. Table 2 shows the results of our ablation models, where M1 refers to the model without AFM, and M2 replaces the AFM with channel attention. Table 2 shows that AFM can improve the scores of M1. Although M2 integrates a channel attention mechanism to dynamically adjust the weight of different feature maps, our proposed AFM method achieves higher quantitative scores, which indicates its superior performance. "
Building a Bridge: Close the Domain Gap in CT Metal Artifact Reduction,4,Conclusion,"In this paper, we explicitly bridge the domain gap between synthetic and clinical metal-corrupted CT images. We employ the clean image domain as the bridge and break the cycle-consistency loss, thereby eliminating the necessity for strict bijection assumption. At the encoding step, feature maps with limited influence will be eliminated, where the encoder acts as a bottleneck only allowing useful information to pass through. Experiments demonstrate that the performance of the proposed method is competitive with several SOTA MAR methods in both qualitative and quantitative aspects. In particular, our method exhibits good generalization ability on clinical data."
Building a Bridge: Close the Domain Gap in CT Metal Artifact Reduction,,Fig. 1 .,
Building a Bridge: Close the Domain Gap in CT Metal Artifact Reduction,,Fig. 2 .,
Building a Bridge: Close the Domain Gap in CT Metal Artifact Reduction,,Fig. 3 .,
Building a Bridge: Close the Domain Gap in CT Metal Artifact Reduction,,Fig. 4 .,
Building a Bridge: Close the Domain Gap in CT Metal Artifact Reduction,,Table 1 .,PSNR(dB)/SSIM Small
Building a Bridge: Close the Domain Gap in CT Metal Artifact Reduction,,Table 2 .,
Alias-Free Co-modulated Network for Cross-Modality Synthesis and Super-Resolution of MR Images,1,Introduction,"Magnetic Resonance Imaging (MRI) is widely recognized as a pivotally important neuroimaging technique, which provides rich information about brain tissue anatomy in a non-invasive manner. Several studies have shown that multi-modal MR images offer complementary information on tissue morphology, which help conduct more comprehensive brain region analysis [4,8,20]. For instance, T1weighted images distinguish grey and white matter tissues, while FLAIR images differentiate edematous regions from cerebrospinal fluid [19]. Furthermore, 3D MR imaging with isotropic voxels contains more details than anisotropic ones and facilitates the analyzing procedure [22], especially for automated algorithms whose performance degrades severely when dealing with anisotropic images [6,23]. However, the scanning protocols in the clinic are different from that in research studies. Due to the high costs of radiation examinations, physicians and radiologists prefer to scan a specific MR contrast, anisotropic MRI with 2D scanning protocols, or their combinations [2,10,17]. Such simplified protocols may hinder the potential possibility of utilizing them for further clinical studies. Therefore, there has been ever-growing interest in reconstructing images with the desired modality and resolution from the acquired images, which may have different modalities or resolutions.Relevant literature in this area can be divided into cross-modality synthesis (CMS), super-resolution (SR), or cross-modality super-resolution (CMSR). Most of the works concerning CMS model modality-invariant and coherent high-frequency details to translate between modalities. For example, pGAN [5] extended conditional GAN with perceptual loss and leveraged neighboring cross-sections to translate between T1-and T2-weighted images. AutoGAN [7] explored NAS to automatically search for optimal generator architectures for MR image synthesis. ResViT [4] proposed a transformer-based generator to capture long-range spatial dependencies for multi-contrast synthesis. On the other hand, SR methods are usually designed to build 3D models for reconstruction. DeepResolve [2] employed a ResNet-based 3D CNN to reduce thickness for knee MR images. DCSRN [3] designed a DenseNet-based 3D model to restore HR features of brain MR images. In more special clinical scenarios where both the CMS and SR need to be performed, current works prefer to build a single model that simultaneously performs the task, rather than performing them sequentially. For instance, SynthSR [10] used a modified 3D-UNet to reconstruct MP-RAGE volumes with thickness of 1 mm from other MR images scanned with different protocols. WEENIE [9] proposed a weakly-supervised joint convolutional sparse coding method to acquire high-resolution modality images with low-resolution ones with other modalities.However, these works have two main limitations. On the one hand, these works are limited to specified fields, and cannot be flexibly switched to other tasks or situations. For instance, CMS models described above will fail to produce consistent 3D results due to the lack of modeling inter-plane correlations. Moreover, CMSR models only produce MR images with fixed thickness, which may constrain their further applications. On the other hand, high-frequency details with alias frequencies are often treated carelessly in these generative networks. This may lead to unnatural details and even aliasing artifacts, especially for the ill-posed inverse tasks (SR or CMSR) that require restoring high-frequency details based on low-resolution images.In this paper, we propose an Alias-Free Co-Modulated network (AFCM) to address the aforementioned issues. AFCM is inspired by the recent advances in foundation models [24], which achieve superior results in various tasks and even outperform the models for specific tasks. To flexibly accomplish 2D-CMS and 3D-SR with a single model, we propose to extend style-based modulation [15,16] with image-conditioned and position-embedded representations, termed as comodulation, to ensure consistency between these tasks. The design also enables more flexible SR by reconstructing slices in arbitrary positions with continuous positional embeddings. Moreover, the discrete operators in the generator are carefully redesigned to be alias-free under the Shannon-Nyquist signal processing framework. Our main contributions are as follows: 1) We propose a co-modulated network that can accomplish CMS, SR, and CMSR with a single model. 2) We propose the continuous positional embedding strategy in AFCM, which can synthesize high-resolution MR images with non-integer thickness. 3) With the redesigned alias-free generator, AFCM is capable of restoring high-frequency details more naturally for the reconstructed images. 4) AFCM achieves state-ofthe-art results for CMS, SR, and CMSR of MR images. "
Alias-Free Co-modulated Network for Cross-Modality Synthesis and Super-Resolution of MR Images,2,Methodology,
Alias-Free Co-modulated Network for Cross-Modality Synthesis and Super-Resolution of MR Images,2.1,Co-modulated Network,"Here we propose a novel style-based co-modulated network to accomplish CMS, SR, and CMSR consistently. As shown in Fig. 1, the architecture design of our network is based on the style-based unconditional generative architecture [15,16].Different from original stochastic style representation, we design both positionembedded representation and image-conditioned representation as the modulations, which control the generation process consistently. Remind that we aim to translate low-resolution MR images x i with modality i and thickness p into high-resolution MR images y j with modality j and thickness q. The positionembedded representation w is transformed from the latent code l ∼ N (0, I) and the relative slice index δ = kq/p -kq/p that controls the position k for each output slice y k j . This is accomplished with a mapping network M where the relative slice index is embedded using the class-conditional design [13]. The imageconditioned representation E(x in i ) is obtained by encoding 2m adjacent slices (i.e.,where n = kq/p ) with the encoder E. In this paper, m is experientially set to 2 following [12]. The two representations are then concatenated and produce a style vector s with the affine transform A:The style vector s is then used to modulate the weights of the convolution kernels in the synthesis network D [16]. Moreover, skip connections are implemented between E and D to preserve the structure of conditional images. The target volume y j is created by stacking generated slices together. Note that our network is flexible to perform various tasks. It performs 2D-CMS or 3D-CMS alone when δ is fixed to 0, which is denoted as AFCM multi . In this case, if the information of adjacent cross-sections is unknown, we perform the one-to-one translation with AFCM one where only one slice is given as input (i.e., x in i = x k i ). Moreover, AFCM sr performs SR alone when i = j."
Alias-Free Co-modulated Network for Cross-Modality Synthesis and Super-Resolution of MR Images,2.2,Alias-Free Generator,"We notice that results produced by the vanilla generator are contaminated by ""texture sticking"", where several fine-grained details are fixed in pixel coordinates when δ changes, as illustrated in our ablation study. The phenomenon was found to originate from aliasing caused by carelessly designed operators (i.e., convolution, resampling, and nonlinearity) in the network [14]. To solve the problem, we consider the feature map Z in the generator as a regularly sampled signal, which can represent the continuous signal z with limited frequencies under the Shannon-Nyquist signal processing framework [26]. Therefore, the discrete operation F on Z has its continuous counterpart f (z):where X r is the two-dimensional Dirac comb with sampling rate r , and φ r is the interpolation filter with a bandlimit of r/2 so that z can be represented as φ r * Z. The operator denotes pointwise multiplication and * denotes continuous convolution. The operation F is alias-free when any frequencies higher than r /2 in the output signal, also known as alias frequencies, are efficiently suppressed. In this way, we re-design the generator by incorporating the aliasfree mechanism based on the analysis above, which consists of the antialiasing decoder and encoder which are further illustrated as follows.Antialiasing Decoder. Considering that any details with alias frequencies need to be suppressed, the resampling and nonlinearity operators in the decoder are carefully redesigned following Alias-Free GAN [14]. The convolution preserves the original form as it introduces no new frequencies. In low-resolution layers, the resampling filter is designed as non-critical sinc one whose cutoff frequency varies with the resolution of feature maps. Moreover, nonlinear operation (i.e., LeakyRelu here) is wrapped between upsampling and downsampling to ensure that any high-frequency content introduced by the operation is eliminated. Other minor modifications including the removal of noise, simplification of the network, and flexible layers are also implemented. Please refer to [14] for more details. Note that Fourier features are replaced by feature maps obtained by the encoder E.Antialiasing Encoder. Given that our network has a U-shaped architecture, the encoder also needs to be alias-free so that the skip connections would not introduce extra content with undesired frequency. The encoder consists of 14 layers, each of which is further composed of a convolution, a nonlinear operation, and a downsampling filter. The parameters of the filter are designed to vary with the resolution of the corresponding layer. Specifically, the cutoff frequency geometrically decreases from f c = r N /2 in the first non-critically sampled layer to f c = 2 in the last layer, where r N is the image resolution. The minimum acceptable stopband frequency starts at f t = 2 0.3 • r N /2 and geometrically decreases to f t = 2 2.1 , whose value determines the resolution r = min(ceil(2 • f t ), r N ) and the transition band half-width f h = max(r/2, f t ) -f c . The target feature map of each layer is surrounded by a 10-pixel margin, and the final feature map is resampled to 4 × 4 before formulating the image-conditioned representation."
Alias-Free Co-modulated Network for Cross-Modality Synthesis and Super-Resolution of MR Images,2.3,Optimization,"The overall loss is composed of an adversarial loss and a pixel-wise L 1 loss:where λ is used to balance the losses. The adversarial loss is defined as nonsaturation loss with R 1 regularization, and the discriminator preserves the architecture of that in StyleGAN2 [16]. We combine x in i with real/fake y k j as the input for the discriminator, and embed δ with the projection discriminator strategy [21]. Following the stabilization trick [14], we blur target and reconstructed images using a Gaussian filter with σ = 2 pixels in the early training stage."
Alias-Free Co-modulated Network for Cross-Modality Synthesis and Super-Resolution of MR Images,3,Experiments,
Alias-Free Co-modulated Network for Cross-Modality Synthesis and Super-Resolution of MR Images,3.1,Experimental Settings,"Datasets. We evaluate the performance of AFCM on three brain MRI datasets: CSDH is an in-house dataset comprising 100 patients diagnosed with chronic subdural hematoma (CSDH). It includes T1-weighted images with spatial resolution of 0.75×0.75×1 mm 3 and T2-weighted flair images with spatial resolution of 0.75 × 0.75 × 8 mm 3 . Pixel-wise annotations of liquefied blood clots made by an experienced radiologist are used for segmentation accuracy evaluation.ADNI2 is composed of 50 patients diagnosed with Alzheimer's disease and 50 elderly controls. Each subject has one near-isotropic T1 MP-RAGE scan with thickness of 1 mm and one axial FLAIR scan with thickness of 5 mm. ADNI and CSDH are both further divided into 70, 10, and 20 subjects for training, validating, and testing. For all three datasets, MR images from the same subjects are co-registered using a rigid registration model with ANTs [1].Implementation Details. We use the translation equivariant configuration of the Alias-Free GAN, and discard the rotational equivariance to avoid generating overly-symmetric images [25]. AFCM is trained with batch size 16 on an NVIDIA GeForce RTX 3090 GPU with 24G memory for 100 epochs, which takes about 36 GPU hours for each experiment. The learning rates are initialized as 0.0025/0.0020 for the generator and the discriminator respectively in the first 50 epochs and linearly decrease to 0."
Alias-Free Co-modulated Network for Cross-Modality Synthesis and Super-Resolution of MR Images,3.2,Comparative Experiments,"Cross-Modality Synthesis. In this section, we report three subtasks, namely CMS from T1 to T2, T2 to PD, and PD to T1-weighted MR images on the IXI dataset in Table 1. AFCM one achieves superior results for all tasks when performing one-to-one translation, where the corresponding slice for the target is taken as the input. The improvement in the generation quality can be attributed to the alias-free design of AFCM, which can restore high-frequency details more accurately, as shown in Fig. 2. Moreover, AFCM multi demonstrates higher generation quality and a significant improvement over other methods (p < 0.05) when adjacent slices are used as extra guidance. The inverted translation results provided in the supplementary file also indicate the superiority of our method. Super-Resolution. The SR experiment is performed using the CSDH dataset with downsampling factor (DSF) of 8 and the ADNI dataset with DSF of 5. Following previous work [2], the low-resolution training data are simulated by filtering and downsampling thin-slice images. We compare with SR methods for MR images (Deepresolve [2] and SynthSR [10]), as well as the 3D version of an SR method for natural images (EDSR [18]). It may seem counterintuitive to generate 2D images slice-by-slice instead of reconstructing the entire volume directly. However, results in Fig. 3(right) suggest that AFCM sr yields higher quality than other 3D-based SR models due to the continuous position-embedded representation and the alias-free design for detail-restoration. Although the improvement is not significant compared with the SOTA methods, the qualitative results in the supplementary file demonstrate that AFCM sr produces coherent details compared to other methods whose results are blurred due to imperfect pre-resampling, especially when the DSF is high. We also evaluate whether the synthesized images can be used for downstream tasks. As observed from Fig. 3(right), when using a pre-trained segmentation model to segment the liquefied blood clots in the reconstructed images, AFCM s r can produce most reliable results, which also indicates the superiority of our method for clinical applications.Cross-Modality and Super-Resolution. As previously addressed, it is more challenging to implement CMS and SR simultaneously. One possible approach is to perform CMS and SR in a two-stage manner, which is accomplished with a combination of ResViT and DeepResolve. We also compare our approach with SynthSR [10] (fully supervised version for fair comparison) that directly performs the task. As reported in Fig. 3(right), the performance of the baseline method is improved when either DeepResolve is replaced with AFCM sr for SR (ResViT+AFCM sr ) or ResViT is replaced with AFCM multi for CMS (AFCM multi +DeepResolve). Additionally, although the two-stage performance is slightly lower than SynthSR, AFCM achieves the best results among all combinations (p < 0.05, SSIM). We also qualitatively evaluate whether our network can perform CMSR with flexible target thickness. To this end, we directly use the model trained on CSDH to generate isotropic images with thickness of 0.75mm, which means 10.67× CMSR. As depicted in Fig. 3   Ablation Study. In this section, we evaluate the impact of our alias-free design by performing CMSR on the CSDH dataset. Note that we replace the encoder and decoder in AFCM with vanilla ones [27] as the baseline for comparison. Table in Fig. 4 indicates that although redesigning the decoder leads to the improvement of SSIM, it is when we also redesign the encoder that the dropped metrics (PSNR and Dice) recover, which highlights the importance of redesigning both the encoder and decoder to achieve optimal results. The qualitative results in Fig. 4 also demonstrate that our design successfully suppresses ""texture sticking""."
Alias-Free Co-modulated Network for Cross-Modality Synthesis and Super-Resolution of MR Images,4,Conclusion,"We propose a novel alias-free co-modulated network for CMS, SR, and CMSR of MR images. Our method addresses the problems of task inconsistency between CMS and SR with a novel co-modulated design, and suppresses aliasing artifacts by a redesigned alias-free generator. AFCM is also flexible enough to reconstruct MR images with various non-integer target thickness. The experiments on three independent datasets demonstrate our state-of-the-art performance in CMS, SR, and CMSR of MR images."
Alias-Free Co-modulated Network for Cross-Modality Synthesis and Super-Resolution of MR Images,,Fig. 1 .,
Alias-Free Co-modulated Network for Cross-Modality Synthesis and Super-Resolution of MR Images,,Fig. 2 .,
Alias-Free Co-modulated Network for Cross-Modality Synthesis and Super-Resolution of MR Images,,,
Alias-Free Co-modulated Network for Cross-Modality Synthesis and Super-Resolution of MR Images,,Fig. 3 .,
Alias-Free Co-modulated Network for Cross-Modality Synthesis and Super-Resolution of MR Images,,Fig. 4 .,
Alias-Free Co-modulated Network for Cross-Modality Synthesis and Super-Resolution of MR Images,,Table 1 .,
Alias-Free Co-modulated Network for Cross-Modality Synthesis and Super-Resolution of MR Images,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5_7.
Dual Domain Motion Artifacts Correction for MR Imaging Under Guidance of K-space Uncertainty,1,Introduction,"Magnetic resonance imaging (MRI) is a widely used non-invasive imaging technique. However, MRI is sensitive to subject motion due to the long time for k-space data acquisition [16]. Motion artifacts, appearing as ghosting or blurring artifacts in MR images, degrade the MR image quality [23] and affect the clinical diagnosis. During the scan, it is hard for subjects to remain still, especially for pediatrics or neuro-degenerative patients. Therefore, the correction of motion artifacts in MRI has a great clinical demand.The typical methods for motion artifacts correction in MRI include the prospective and retrospective methods. The prospective methods measure the subject motion using external tracking devices or navigators during the scan for motion correction [11]. The retrospective motion correction methods either explicitly model and correct the motion in the image reconstruction algorithm, or learn the mapping from MR image with motion artifacts to the motion-free MR image using deep learning approach. Specifically, the methods in [2,4,6,9,15] are based on a forward model of subject motion, and jointly estimate the motion parameters and MR image using the optimization algorithm. The methods in [8,12,14] introduce convolutional neural networks (CNNs) into the joint optimization procedure to learn the MR image prior. The deep learning methods in [3,10,18,19,21] directly learn the mapping from motion-corrupted MR image to motion-free MR image by designing various deep networks. Some other methods correct the motion artifacts using additional prior information, such as the different contrasts of the same object [13], self-assisted adjacent slices priors [1].In this paper, we propose a dual domain motion correction network (i.e., D 2 MC-Net) to correct the motion artifacts in 2D multi-slice MRI. Instead of explicitly estimating motion parameters, we design a dual domain regularized model with an uncertainty-guided data consistency term, which models the motion corruption by k-space uncertainty to guide the MRI reconstruction. Then the alternating iterative algorithm of the model is unfolded to be a novel deep network, i.e., D 2 MC-Net. As shown in Fig. 1, the D 2 MC-Net contains multiple stages, and each stage consists of two key components, i.e., k-space uncertainty module (KU-Module) and dual domain reconstruction module (DDR-Module). The KU-Module measures the uncertainty of k-space data corrupted by the motion. The DDR-Module reconstructs motion-free k-space data and MR image in both k-space and image domain under the guidance of the k-space uncertainty. Extensive experiments on fastMRI dataset demonstrate that the proposed D 2 MC-Net achieves the state-of-the-art results under different motion trajectories and motion severities. For example, under severe corruption with piecewise constant motion trajectory, our result in PSNR is at least 2.11 dB higher than the existing methods, e.g., Autofocusing+ [12].Different from the optimization-based methods [2,4,6,8,9,12,14,15], our model is based on modeling the motion corruption by k-space uncertainty without explicitly estimating the motion parameters. Different from the deep learning methods [1,3,10,18,19,21], D 2 MC-Net incorporates an uncertainty-guided data consistency term into the unfolded network to guide MRI reconstruction."
Dual Domain Motion Artifacts Correction for MR Imaging Under Guidance of K-space Uncertainty,2,Methods,
Dual Domain Motion Artifacts Correction for MR Imaging Under Guidance of K-space Uncertainty,2.1,Problem Formulation,"In our approach, we model the motion corruption by measuring the uncertainty of k-space data. Specifically, we assume that the distribution of motion-corrupted k-space data ŷ ∈ C N at each position obeys a non-i.i.d. and pixel-wise Gaussian distribution, where N is the number of the k-space data. Specifically, considering the i-th position of the ŷ, we have where w ∈ [0, 1] N represents the k-space uncertainty with the elements w. p(x) and p(y) are the prior distributions of the motion-free data in image domain and k-space domain. The likelihood distribution log p(ŷ|x, w)) has been modeled by Eq. ( 1). Then the solution of Eq. ( 2) can be converted to a dual domain regularized model with an uncertainty-guided data consistency term to correct the motion-related artifacts:where H I and H K are learnable denoisers with parameters θ I and θ K , which adopt the U-Net [18] architecture in this paper. λ and ρ are trade-off parameters. The first term is the uncertainty-guided data consistency term corresponding to the log-likelihood log p(ŷ|x, w) which enforces consistency between the k-space data of reconstructed MR image and its motion-corrupted k-space data under the guidance of the uncertainty w. The second and third terms are regularizations for imposing image-space prior p(x) and k-space prior p(y)."
Dual Domain Motion Artifacts Correction for MR Imaging Under Guidance of K-space Uncertainty,2.2,Dual Domain Motion Correction Network,"Our proposed D 2 MC-Net is designed based on the alternating optimization algorithm to solve Eq. ( 3). As shown in Fig. 1, taking the motion-corrupted k-space data as input, it reconstructs the motion-free MR images with T stages. Each stage consists of the k-space uncertainty module (KU-Module) and the dual domain reconstruction module (DDR-Module), respectively corresponding to the sub-problems for optimizing the k-space uncertainty w, and the dual domain data including k-space data y and MR image x. The KU-Module estimates the k-space uncertainty w, quantifying the uncertainty of k-space data corrupted by motion. The DDR-Module is responsible for reconstructing the k-space data y and MR image x, under the guidance of the k-space uncertainty w. Details of these two modules at t-th stage are as follows.K-space Uncertainty Module. This module is designed to update k-space uncertainty w in Eq. ( 3). If directly optimizing w in Eq. ( 3), w t = 1 /|Fxt-1-ŷ| at t-th stage, which depends on the difference between the k-space data of reconstructed image F x t-1 and the motion-corrupted k-space data ŷ. We extend this estimate to be a learnable module defined as:where H W is the sub-network with parameters θ W . When t=1, we only send ŷ into the KU-Module because we do not have the estimate of the reconstructed MR images in such case.Dual Domain Reconstruction Module. This module is designed to update k-space data y and MR image x in Eq. ( 3) under the guidance of the uncertainty w. Specifically, given the reconstructed MR image x t-1 from (t-1)-th stage and the k-space uncertainty w t , the k-space data at t-th stage is updated by:where W t = diag(w t ) ∈ [0, 1] N ×N is a diagonal matrix, thus the matrix inversion in Eq. ( 5) can be computed efficiently. Equation ( 5) is defined as k-space reconstruction block (K-Block), solving the sub-problem for optimizing k-space data y in Eq. (3). Equation ( 5) can be implemented by firstly computing H K , followed by the k-space uncertainty-guided data consistency operator UDC K in Eq. ( 5). Similarly, given the updated uncertainty w t and k-space data y t , the MR image at t-th stage is updated by:Equation ( 6) is defined as image reconstruction block (I-Block), solving the subproblem for optimizing MR image x in Eq. ( 3). Equation ( 6) can be implemented by firstly computing H I , followed by the image domian uncertainty-guided data consistency operator UDC I in Eq. ( 6). The K-Block and I-Block are combined as the dual domain reconstruction module (DDR-Module) to sequentially reconstruct the k-space data y t and MR image x t at t-th stage. In summary, by connecting the k-space uncertainty module and dual domain reconstruction module alternately, we construct a multi-stage deep network (i.e., D 2 MC-Net) for motion artifacts correction as shown in Fig. 1."
Dual Domain Motion Artifacts Correction for MR Imaging Under Guidance of K-space Uncertainty,2.3,Network Details and Training Loss,"In the proposed D 2 MC-Net, we use T = 3 stages for speed and accuracy tradeoff. Each stage has three sub-networks (i.e., H W , H K and H I ) as shown in Fig. 1. H K and H I adopt U-Net [18] architecture which contains five encoder blocks and four decoder blocks followed by a 1×1 convolution layer for the final output. Each block consists of two 3 × 3 convolution layers, an instance normalization (IN) layer and a ReLU activation function. The average pooling and bilinear interpolation layers are respectively to reduce and increase the resolution of the feature maps. The number of output feature channels of the encoder and decoder blocks in U-Net are successively 32, 64, 128, 256, 512, 256 128, 64, 32. The structure of H W is Conv→IN→ReLU→Conv→Sigmoid, where Conv denotes a 3 × 3 convolution layer. The number of output feature channels for these two convolution layers are 64 and 2, respectively.The overall loss function in image space and k-space is defined as:where x t and y t are the reconstructed MR image and k-space data at t-th stage.x gt and y gt are the motion-free MR image and k-space data. SSIM [22] is the structural similarity loss. γ is a hyperparameter to balance the different losses in dual domain, and we set γ = 0.001. The Adam optimizer with mini-batch size of 4 is used to optimize the network parameters. The initial value of the learning rate is 1 × 10 -4 and divided by 10 at 40-th epoch. We implement the proposed D 2 MC-Net using PyTorch on one Nvidia Tesla V100 GPU for 50 epochs."
Dual Domain Motion Artifacts Correction for MR Imaging Under Guidance of K-space Uncertainty,3,Experiments,"Dataset. We evaluate our method on the T2-weighted brain images from the fastMRI dataset [7], and we randomly select 78 subjects for training and 39 subjects for testing. The in-plane matrix size of the subjects is resized to 384 × 384, and the number of slices varies from the subjects. Sensitivity maps are estimated using the ESPIRiT algorithm [20] for coil combination.Motion Artifacts Simulation. We simulate in-plane and through-plane motion according to the forward model ŷ = M FT θ x [2], where T θ ∈ R N ×N is the rigid-body motion matrix parameterized by a vector of translations and rotationsAnd we keep 7% of the k-space lines in the center for preventing excessive distortion of the images. The motion vectors are randomly selected from a Gaussian distribution N (0, 10). We follow the motion trajectories (i.e., piecewise constant, piecewise transient and Gaussian) used in the paper [5] to simulate motion. In addition, to generate various motion severities, each motion level has a series of motion-corrupted k-space lines: 0-30%, 0-50%, and 0-70% of the total of k-space lines for mild, moderate, and severe, respectively. Finally, the motioncorrupted volume k-space data is cut into slice data and sent to the proposed D 2 MC-Net. Performance Evaluation. We compare the proposed D 2 MC-Net with four deep learning methods (i.e., U-Net [18], UPGAN [21], SU-Net [1], and Alternating [17]), and an optimization-based method (i.e., Autofocusing+ [12]). The motion-corrupted image without motion correction is denoted as ""Corrupted"".In Table 1, we show the quantitative results of different methods under different motion trajectories and motion severities. Compared with ""Corrupted"", these deep learning methods improve the reconstruction performance. By explicitly estimating motion parameters, Autofocusing+ produces better results than deep learning methods. Our method achieves the best results in all experiments, mainly because the uncertainty-guided data consistency term is introduced into the unfolded deep network to guide MRI reconstruction. The qualitative comparison results under the severe corruption with piecewise constant motion trajectory are shown in Fig. 2. In comparison, our method has the smallest reconstruction error and recovers finer image details while suppressing undesired artifacts. The PSNR and SSIM values in Fig. 2 also demonstrate the superiority of our method. For example, the PSNR value of our method is 3.06 dB higher than that of SU-Net [1].  Effectiveness of the Key Components. We evaluate the effectiveness of these key components, including KU-Module, K-Block, and I-Block in Fig. 1, under the moderate corruption with piecewise constant motion trajectory. In Table 2, (A) ""Baseline"" denotes the reconstruction model  2, our results are better than all the compared variants, showing the effectiveness of the k-space uncertainty and dual-domain reconstruction. Compared with methods that do not use motion-corrupted k-space data (i.e., ""Ours (w = 0)"") and fully use motion-corrupted k-space data (i.e., ""Ours (w = 1)"") in reconstruction, our method selectively uses the motion-corrupted k-space data under the guidance of the learned k-space uncertainty w, and achieves higher performance. Visualization of the Uncertainty w. The estimated k-space uncertainties of all stages are visualized in Fig. 3(b). As we can see, the averaged k-space uncertainty w avg over all stages approximates the real motion trajectory mask m uc with ones indicating the un-corrupted k-space lines.Effect of Different Loss Functions. We also investigate the effect of the kspace loss by adjusting the values of hyperparameters γ in Eq. (7). The PSNR results under the piecewise constant moderate motion are shown in Table 3. From these results, our method achieves the best performance at γ = 0.001."
Dual Domain Motion Artifacts Correction for MR Imaging Under Guidance of K-space Uncertainty,4,Conclusion,"In this paper, we proposed a novel dual domain motion correction network (D 2 MC-Net) to correct the motion artifacts in MRI. The D 2 MC-Net consists of KU-Modules and DDR-Modules. KU-Module measures the uncertainty of kspace data corrupted by motion. DDR-Module reconstructs the motion-free MR images in k-space and image domains under the guidance of the uncertainty estimated by KU-Module. Experiments on fastMRI dataset show the superiority of the proposed D 2 MC-Net. In the future work, we will extend the D 2 MC-Net to be a 3D motion correction method for 3D motion artifacts removal."
Dual Domain Motion Artifacts Correction for MR Imaging Under Guidance of K-space Uncertainty,,Fig. 1 .,
Dual Domain Motion Artifacts Correction for MR Imaging Under Guidance of K-space Uncertainty,,Fig. 2 .,
Dual Domain Motion Artifacts Correction for MR Imaging Under Guidance of K-space Uncertainty,,,
Dual Domain Motion Artifacts Correction for MR Imaging Under Guidance of K-space Uncertainty,,Fig. 3 .Table 3 .,
Dual Domain Motion Artifacts Correction for MR Imaging Under Guidance of K-space Uncertainty,,Table 1 .,
Dual Domain Motion Artifacts Correction for MR Imaging Under Guidance of K-space Uncertainty,,00 0.9761 0.0567 37.79 0.9594 0.0806 35.28 0.9399 0.1066,
Dual Domain Motion Artifacts Correction for MR Imaging Under Guidance of K-space Uncertainty,,Table 2 .,
Progressively Coupling Network for Brain MRI Registration in Few-Shot Situation,1,Introduction,"Deformable medical image registration has many applications in clinic, including but not limited to clinical case tracking, surgical navigation. In recent years, many advanced learning-based medical image registration methods [2,6,7,15] have been proposed. Due to the limitations of accessible labels, it is more common to employ unsupervised ways for optimization. Although unsupervised methods [5,12,14,17,19] can guide the registration network to optimize by maximizing the image-wise similarity between warped and fixed images, the lack of guidance from regions of interest (ROIs) makes the registration performance fall into a bottleneck. Moreover, medical image labeling usually requires professional medical staff, and it's laborious to get large-scale training labels. Integrating segmentation into registration can considerably compensate for the lack of image labels via their complementarity [11,23]. Specifically, warping segmentation result of moving image to align to that of fixed image by deformation field can provide additional supervision of ROIs for registration task. Meanwhile, as a way of data augmentation, the unlabelled images can participate in the segmentation optimization to prevent the overfitting of segmentation in few-shot situation [20,26].Recently, several joint learning methods have been proposed. These methods can be generally divided into two categories in terms of model structure (see Fig. 1), two-two model (i.e., two encoders and two decoders) [10,11,23] and onetwo model (i.e., one encoder and two decoders) [8,18,27]. The two-two models employ two completely independent models for segmentation and registration, and achieve mutual learning by joint loss. DeepAtlas [23] alternately trains registration and segmentation networks to achieve mutual improvement in brain and knee images. He et al. [10] feed the segmentation masks into the registration network to provide the internal texture information of the regions of interest (ROIs) so as to avoid incorrect deformation of internal areas.In comparison, the one-two models are more in line with the general paradigm of multi-task learning, which can be regarded as a process of inductive bias [21], i.e., utilizing the shared encoder to induct the commonality of each task, and then using the respective decoder for preference prediction. They can effectively reduce the risk of overfitting and reduce the parameters of the network through the way of sharing parameters [3]. However, existing one-two models all focus on loss design to depict joint learning [8,18,27]. There is not any explicit feature interaction between two task streams. It is well known that sensible interactions help model capture extra key features or regularize networks to optimize in desired directions [22,24,25,28]. In addition, the semantic information of moving and fixed images are interchangeable and mutually exploitable. As it happens, the deformation field in registration is able to interconvert the context information of moving and fixed images and the converted features can be employed as additional contextual information for their segmentation.Based on the framework of one-two model, we propose a progressively coupling network (PGCNet) as shown in Fig. 2. Specifically, progressive field estimators are designed to calculate multi-level deformation fields from coarse to fine. By warping the moving features and computing the correlation between warped and fixed features at each level, the deformation field is refined progressively. Different from common correlation calculation [13,14], we introduce learnable absolute position coordinates to provide spatial information for registration, which can better measure coordinate correspondence of voxels. In order to closely couple the registration and segmentation branches, we infuse the warped contexts into the segmentation branches. In this way, segmentation supervision sets an extra attention focusing for the registration decoder, which drives the deformation field to project semantic layout from moving image to fixed one. This semantic projection helps improve the registration accuracy. Meanwhile, the registration branch provides more sufficient semantic information for segmentation branch.The main contributions can be summarized as follows: 1) We propose a novel registration learning framework to establish the entangled relationship between registration and segmentation by progressively coupling the moving and fixed segmentation, which promotes both registration and segmentation performance in few-shot situation. 2) To effectively measure the coordinate correspondence of voxels, we design the position correlation calculation, which provides spatial coordinate information for field estimator while measuring feature similarity. 3) Experimental results on the general brain MRI datasets, OASIS and IXI, show that our method outperforms the state-of-the-art methods."
Progressively Coupling Network for Brain MRI Registration in Few-Shot Situation,2,Method,"Our model aims to learn registration and segmentation tasks simultaneously. Let M , F be moving and fixed images, respectively. We parameterize our model as a function with parameter θ, f θ (M, F ) = φ, S M , S F , where φ is the deformation field, S M and S F represent the segmentation results of M and F . Registration task completes the transformation from moving images to fixed ones, and segmentation task generates the segmentation maps of moving and fixed images."
Progressively Coupling Network for Brain MRI Registration in Few-Shot Situation,2.1,Overall Network Structure,"The overall pipeline of our method is shown in Fig. 2, which mainly consists of two parts, shared encoder and pyramid decoder. The parameters of the encoder (rose red block) and segmentation decoder (purple block) are shared. The shared encoder of our model is a 3D residual convolution network, which has 4 feature levels. Except for the input level, we use 3, 5, and 5 residual blocks at the other levels, respectively. A residual block contains two convolutional layers with preactivation [9] structure and shortcut connection, to extract features, respectively. The numbers of channels at the four levels are 8, 16, 32, and 64, respectively, and each convolution layer is followed by Instance Normalization and Leaky-ReLU with a negative slope of 0.2 except for the last output. We first calculate the correlation of the deepest level features and then estimate the initial deformation field based on the correlation matrix. In the segmentation branch, we employ the commonly used skip connection structure, and concatenate the complementary features from the registration branch at each level, which are warped by bidirectional deformation fields."
Progressively Coupling Network for Brain MRI Registration in Few-Shot Situation,2.2,Coupling Decoder,"As shown in Fig. 3, there are four inputs in our coupling decoder: Field (ϕ), Inv-Field (ϕ -), Moving Context (F M ) and Fixed Context (F F ). Among them, Field (ϕ) and Inv-Field (ϕ -) are up-sampled by trilinear interpolation with the factor of 2 from the front level. Moving Context (F M ) and Fixed Context (F F ) are the context information from the corresponding level. We use ϕ to warp F M and input the warped features (F M • ϕ) into the segmentation decoder of fixed image as additional complementary information (coupled features). In this way, the segmentation network obtains richer semantic information. The same operation is done in the segmentation branch of moving image as well. Relying on the projecting ability of the deformation field, we align and combine context information of moving and fixed images. Thus, registration and segmentation branches establish an entangled correspondence to constrain each other to learn a well generalized model. Generally speaking, if the fixed image is precisely segmented, the coupled features, which are projected from moving image to fixed one, usually are well matched. That is, the coupling process can constrain the deformation field to become accurate. Position Correlation Calculation for Registration. In order to measure the similarity between each voxel and its neighbors, the warped (F M • ϕ) and fixed (F F ) context features will conduct correlation calculation to obtain a cost volume. Note that the inverse correlation is calculated between the warped (F F • ϕ -) and moving (F M ) context features, which is not shown in Fig. 3. The spatial correlation between adjacent voxels in moving and fixed images is the key to determining the deformation field. According to the cost volume, we can judge the displacement between moving and fixed images at the current level. Different from existing works [13,14], we embed the position information into feature volumes before correlation calculation. Segmentation and registration tasks have different semantic expression requirements. The position coding can reduce the semantic confusion brought about by the shared encoder and enforce the decoder to understand spatial relationships of voxels. Let E be position coding map and r be the displacement radius, the correlation calculation is formulated as:wherex, y and z represent the coordinate indexes of feature map in three directions, respectively. We move feature map point by point along x, y and z directions with radius r and do dot product to generate a correlation map. The dimension of correlation matrix (cost volume) is (2r + 1) 3 , and we set r = 1 in this work. Then, Field and Inv-Field estimation modules utilize three residual blocks to compute the increment of forward and inverse displacement fields according to the correlation and inverse correlation matrices, respectively. The parameters of the two modules are shared."
Progressively Coupling Network for Brain MRI Registration in Few-Shot Situation,2.3,Loss Function,"The loss functions of our framework comprise three parts, which regularize registration, segmentation, and joint optimization, respectively. The registration loss has similarity and smoothness penalty terms to align the moving and fixed images and ensure the smoothness of the deformation field. We use local normalized cross-correlation [2] with window size ω (ω = 9) as similarity function. Let F and W represent the fixed and warped (M • φ) scans. L 2 loss of deformation field gradient is set as regularization function. The registration loss can be defined as:where λ 1 is a balance hyperparameter and is set as 1 in the experiments.For segmentation, we use the combination of weighted cross entropy and Dice loss, which is formulated as:where N represents the total number of semantic classes, n indicates the channel of the corresponding category, and P is the number of voxels in each channel. S is the segmentation result and S * is the voxel-wise manual segmentation label. In Eq. 3, w n indicates the inverse proportion of voxels in category n to all voxels. For these two segmentation loss functions, their weights are fixed to the same value which means L seg = L wce + L dice . The joint loss function is defined as L dice (S F , S M • φ). It regularizes registration network to provide additional training data for segmentation task. Meanwhile, registration network can pay more attention to the ROI regions."
Progressively Coupling Network for Brain MRI Registration in Few-Shot Situation,3,Experiments and Results,
Progressively Coupling Network for Brain MRI Registration in Few-Shot Situation,3.1,Experimental Settings,"Datasets and Preprocessing. We validate our method by using 414 T1weighted brain MRI scans from OASIS [16] dataset and 576 T1-weighted brain MRI scans from IXI 1 dataset. These scans are preprocessed, which includes skull dissection, spatial normalization. All MRI scans are resampled with the same isotropic voxels of 1 mm × 1 mm × 1 mm, and we center crop the scans from OASIS dataset to 160 × 192 × 144 and the scans from IXI dataset to 160 × 160 × 192, which crops excess background to reduce the memory of GPU.1 IXI dataset is available in https://brain-development.org/ixi-dataset/.The subcortical structures in OASIS and IXI are labeled as 35 and 44 categories by FreeSurfer for evaluation. We randomly select 5 scans as atlas from OASIS dataset, then the remaining scans are randomly divided into 255, 22, 132 for training, validation and testing. We do not use the labels of the training scans, only 5 labels of atlas are used to simulate a few-shot scenario. A total of 1,275, 110, 660 pairs of scans are used for training, validation and testing. Similarly, the IXI dataset is randomly split into 397, 58, 115, and 5 labeled scans are randomly taken as atlas for few-shot situation. The training, validation and testing include 1,985, 290, 575 pairs of scans, respectively.Implementation. All trainings use Adam optimizer with learning rate 1e -4 and the batch size is set as 1. We first train the network by optimizing L reg + L seg for 5,000 iterations and then by optimizing L reg + L seg + L joint for 80,000 iterations. All experiments are performed on 1 Nvidia RTX 3090 GPU.Baseline Methods. We compare our method with a series of registration models. SYN [1] and LDDM [4] are two traditional registration algorithms. Voxelmorph [2] and transmorph [5] are the learning-based methods, which directly predict the deformation field. While LapIRN [17] and ULAE [19] are two progressive methods. We also evaluate these two methods under the semi-supervised setting with auxiliary segmentation labels. The semi-supervised loss function is the same as ours. DeepAtlas [23], UResNet [8], and PC-Reg [10] are joint learning methods, and we adopt the same training strategy as theirs."
Progressively Coupling Network for Brain MRI Registration in Few-Shot Situation,3.2,Experimental Results,"Registration Performance. As shown in Table 1, PGCNet achieves competitive registration accuracy, 87.72% DSC mean and 1.59 HD95 on OASIS dataset and 78.08% DSC mean and 3.19 HD95 on IXI dataset. For VoxelMorph and TransMorph, which directly estimate the deformation field, their DSC mean are relatively low. Coarse-to-fine registration methods, LapIRN and ULAE, can meet the complex transformation needs between images through multiple deformations, but they still have poor performance due to the lack of ROIs supervision information. By introducing additional ROIs supervision information, their performance is improved by 1.1% and 1.8%, respectively. Their improvement is weak because of the limited semi-supervised information. Our method outperforms UResNet, DeepAtlas, and PC-Reg by 12.7%, 5.7%, and 1.7%, respectively, in joint learning models. The poor performance of UResNet can be attributed to its approach of concatenating moving and fixed images to extract features, while only predicting the moving segmentation result. Figure 4 presents visual comparison of various methods. It can be observed that our method exhibits lowest registration error in the third row.Ablation Study. We conduct a series of ablation experiments on OASIS dataset to verify the effectiveness of each module, as shown in Table 2. We take the Table 1. Quantitative results of different registration methods on OASIS and IXI datasets. Average Dice similarity coefficient (DSCmean) and 95% percentile of the Hausdorff distance (HD95) are used to measure the registration accuracy of the models, and the average proportion of folding voxels in the deformation fields (| J φ |< 0) indicates the topological reversibility of the deformation field.   registration-only model as baseline, which has the same encoder and field estimation module as the final model, utilizing common correlation calculation."
Progressively Coupling Network for Brain MRI Registration in Few-Shot Situation,,OASIS IXI,"Joint training (JTrain) introduces semi-supervised loss for registration. And the coupling feature connection (Couple) improves the performance of registration by guiding the deformation field in advance to map semantic features. Position embedding (PEmbedding) assists the field estimator in understanding the correlation between voxels. In addition, we further analyse the factors that affect the performance of segmentation (see Table 3). Coupling feature connection provides more adequate semantic information, resulting in 3.3% improvement, while joint training provides additional trainable data, leading to 2.4% improvement."
Progressively Coupling Network for Brain MRI Registration in Few-Shot Situation,4,Conclusion,"We propose a progressively coupling network (PGCNet), which employs the deformation fields to couple the registration and segmentation branches. This is a novel mode of regularization for registration and segmentation, which promotes their performance in few-shot situation. In addition, position embedding provides additional spatial coordinate information for registration branch. The experimental results indicate that our work is superior to existing methods."
Progressively Coupling Network for Brain MRI Registration in Few-Shot Situation,,Fig. 1 .,
Progressively Coupling Network for Brain MRI Registration in Few-Shot Situation,,Fig. 2 .,
Progressively Coupling Network for Brain MRI Registration in Few-Shot Situation,,Fig. 3 .,
Progressively Coupling Network for Brain MRI Registration in Few-Shot Situation,,Fig. 4 .,
Progressively Coupling Network for Brain MRI Registration in Few-Shot Situation,,Table 2 .,
Progressively Coupling Network for Brain MRI Registration in Few-Shot Situation,,Table 3 .,
Trackerless Volume Reconstruction from Intraoperative Ultrasound Images,1,Introduction,"Liver cancer is the most prevalent indication for liver surgery, and although there have been notable advancements in oncologic therapies, surgery remains as the only curative approach overall [20].Liver laparoscopic resection has demonstrated fewer complications compared to open surgery [21], however, its adoption has been hindered by several reasons, such as the risk of unintentional vessel damage, as well as oncologic concerns such as tumor detection and margin assessment. Hence, the identification of intrahepatic landmarks, such as vessels, and target lesions is crucial for successful and safe surgery, and intraoperative ultrasound (IOUS) is the preferred technique to accomplish this task. Despite the increasing use of IOUS in surgery, its integration into laparoscopic workflows (i.e., laparoscopic intraoperative ultrasound) remains challenging due to combined problems.Performing IOUS during laparoscopic liver surgery poses significant challenges, as laparoscopy has poor ergonomics and narrow fields of view, and on the other hand, IOUS demands skills to manipulate the probe and analyze images. At the end, and despite its real-time capabilities, IOUS images are intermittent and asynchronous to the surgery, requiring multiple iterations and repetitive steps (probe-in -→ instruments-out -→ probe-out -→ instruments-in). Therefore, any method enabling a continuous and synchronous US assessment throughout the surgery, with minimal iterations required would significantly improve the surgical workflow, as well as its efficiency and safety.To overcome these limitations, the use of intravascular ultrasound (IVUS) images has been proposed, enabling continuous and synchronous inside-out imaging during liver surgery [19]. With an intravascular approach, an overall view and full-thickness view of the liver can quickly and easily be obtained through mostly rotational movements of the catheter, while this is constrained to the lumen of the inferior vena cava, and with no interaction with the tissue (contactless, a.k.a. standoff technique) as illustrated in Fig. 1. However, to benefit from such a technology in a computer-guided solution, the different US images would need to be tracked and possibly integrated into a volume for further processing. External US probes are often equipped with an electromagnetic tracking system to track its position and orientation in realtime. This information is then used to register the 3D ultrasound image with the patient's anatomy. The use of such an electromagnetic tracking system in laparoscopic surgery is more limited due to size reduction. The tracking system may add additional complexity and cost to the surgical setup, and the tracking accuracy may be affected by metallic devices in the surgical field [22].Several approaches have been proposed to address this limitation by proposing a trackerless ultrasound volume reconstruction. Physics-based methods have exploited speckle correlation models between different adjacent frames [6][7][8] to estimate their relative position. With the recent advances in deep learning, recent works have proposed to learn a higher order nonlinear mapping between adjacent frames and their relative spatial transformation. Prevost et al. [9] first demonstrated the effectiveness of a convolution neural network to learn the relative motion between a pair of US images. Xie et al. [10] proposed a pyramid warping layer that exploits the optical flow features in addition to the ultrasound features in order to reconstruct the volume. To enable a smooth 3D reconstruction, a case-wise correlation loss based on 3D CNN and Pearson correlation coefficient was proposed in [10,12]. Qi et al. [13] leverages past and future frames to estimate the relative transformation between each pair of the sequence; they used the consistency loss proposed in [14]. Despite the success of these approaches, they still suffer significant cumulative drift errors and mainly focus on linear probe motions. Recent work [15,16] proposed to exploit the acceleration and orientation of an inertial measurement unit (IMU) to improve the reconstruction performance and reduce the drift error. Motivated by the weakness of the state-of-the-art methods when it comes to large non-linear probe motions, and the difficulty of integrating IMU sensors in the case of minimally invasive procedures, we introduce a new method for pose estimation and volume reconstruction in the context of minimally invasive trackerless ultrasound imaging. We use a Siamese architecture based on a Sequence to Vector(Seq2Vec) neural network that leverages image and optical flow features to learn relative transformation between a pair of images.Our method improves upon previous solutions in terms of robustness and accuracy, particularly in the presence of rotational motion. Such motion is predominant in the context highlighted above and is the source of additional nonlinearity in the pose estimation problem. To the best of our knowledge, this is the first work that provides a clinically sound and efficient 3D US volume reconstruction during minimally invasive procedures. The paper is organized as follows: Sect. 2 details the method and its novelty, Sect. 3 presents our current results on ex vivo porcine data, and finally, we conclude in Sect. 4 and discuss future work."
Trackerless Volume Reconstruction from Intraoperative Ultrasound Images,2,Method,"In this work, we make the assumption that the organ of interest does not undergo deformation during the volume acquisition. This assumption is realistic due to the small size of the probe. Let I 0 , I 1 ...I N -1 be a sequence of N frames. Our aim is to find the relative spatial transformation between each pair of frames I i and I j with 0 ≤ i ≤ j ≤ N -1. This transformation is denoted T (i,j) and is a six degrees of freedom vector representing three translations and three Euler angles. To achieve this goal, we propose a Siamese architecture that leverages the optical flow in the sequences in addition to the frames of interest in order to provide a mapping with the relative frames spatial transformation. The overview of our method is presented in Fig. 2.We consider a window of 2k + 3 frames from the complete sequence of length N , where 0 ≤ k ≤ N -3 2 is a hyper-parameter that denotes the number of frames between two frames of interest. Our method predicts two relative transformations between the pairs of frames (I 1 , I k+2 ) and (I k+2 , I 2k+3 ). The input window is divided into two equal sequences of length k + 2 sharing a common frame. Both deduced sequences are used to compute a sparse optical flow allowing to track the trajectory of M points. Then, Gaussian heatmaps are used to describe the motion of the M points in an image-like format(see Sect. 2.2). Finaly, a Siamese architecture based on two shared weights Sequence to Vector (Seq2Vec) network takes as input the Gaussian heatmaps in addition to the first and last frames and predicts the relative transformations. In the following we detail our pipeline."
Trackerless Volume Reconstruction from Intraoperative Ultrasound Images,2.1,Sparse Optical Flow,"Given a sequence of frames I i and I i+k+1 , we aim at finding the trajectory of a set of points throughout the sequence. We choose the M most prominent points from the first frame using the feature selection algorithm proposed in [3]. Points are then tracked throughout each pair of adjacent frames in the sequence by solving Eq. 1 which is known as the Optical flow equation. We use the pyramidal implementation of Lucas-Kanade method proposed in [4] to solve the equation. Thus, yielding a trajectory matrix A ∈ R M ×(k+2)×2 that contains the position of each point throughout the sequence. Figure 3 illustrates an example where we track two points in a sequence of frames."
Trackerless Volume Reconstruction from Intraoperative Ultrasound Images,2.2,Gaussian Heatmaps,"After obtaining the trajectory of M points in the sequencewe only keep the first and last position of each point, which corresponds to the positions in our frames of interest. We use Gaussian heatmaps H ∈ R H×W with the same dimension as the ultrasound frames to encode these points, they are more suitable as input for the convolutional networks. For a point with a position (x 0 , y 0 ), the corresponding heatmap is defined in the Eq. 2.Thus, each of our M points are converted to a pair of heatmaps that represent the position in the first and last frames of the ultrasound sequence. These pairs concatenated with the ultrasound first and last frames form the recurrent neural network sequential input of size (M + 1, H, W, 2), where M + 1 is the number of channels (M heatmaps and one ultrasound frame), H and W are the height and width of the frames and finally 2 represents the temporal dimension."
Trackerless Volume Reconstruction from Intraoperative Ultrasound Images,2.3,Network Architecture,The Siamese architecture is based on a sequence to vector network. Our network maps a sequence of two images having M + 1 channel each to a six degrees of freedrom vector (three translations and three rotation angles). The architecture of Seq2Vec is illustrated in the Fig. 4. It contains five times the same block composed of two Convolutional LSTMs (ConvLSTM) [5] followed by a Batch Normalisation. Their output is then flattened and mapped to a six degrees of freedom vector through linear layers; ReLU is the chosen activation function for the first linear layer. We use an architecture similar to the one proposed in [5] for the ConvLSTM layers. Seq2Vec networks share the same weights.
Trackerless Volume Reconstruction from Intraoperative Ultrasound Images,2.4,Loss Function,"In the training phase, given a sequence of 2k + 3 frames in addition to their ground truth transformations T(1,k+2) , T(k+2,2k+3) and T(1,2k+3) , the Seq2Vec's weights are optimized by minimising the loss function given in the Eq. 3. The loss contains two terms. The first represents the mean square error (MSE) between the estimated transformations (T (1,k+2) , T (k+2,2k+3) ) at each corner point of the frames and their respective ground truth. The second term represents the accumulation loss that aims at reducing the error of the volume reconstruction, the effectiveness of the accumulation loss have been proven in the literature [13]. It is written as the MSE between the estimated T (1,2k+3) = T (k+2,2k+3) ×T (1,k+2) at the corner points of the frames and the ground truth T (1,2k+3) .3 Results and Discussion"
Trackerless Volume Reconstruction from Intraoperative Ultrasound Images,3.1,Dataset and Implementation Details,"To validate our method, six tracked sequences were acquired from an ex vivo swine liver. A manually manipulated IVUS catheter was used (8 Fr lateral firing AcuNav TM 4-10 MHz) connected to an ultrasound system (ACUSON S3000 HELX Touch, Siemens Healthineers, Germany), both commercially available. An electromagnetic tracking system (trakSTAR TM , NDI, Canada) was used along with a 6 DoF sensor (Model 130) embedded close to the tip of the catheter, and the PLUS toolkit [17] along with 3D Slicer [18] were used to record the sequences. The frame size was initially 480 × 640. Frames were cropped to remove the patient and probe characteristics, then down-sampled to a size of 128 × 128 with an image spacing of 0.22 mm per pixel. First and end stages of the sequences were removed from the six acquired sequences, as they were considered to be largely stationary, and aiming to avoid training bias. Clips were created by sliding a window of 7 frames (corresponding to a value of k = 2) with a stride of 1 over each continuous sequence, yielding a data set that contains a total of 13734 clips. The tracking was provided for each frame as a 4×4 transformation matrix.We have converted each to a vector of six degrees of freedom that corresponds to three translations in mm and three Euler angles in degrees. For each clip, relative frame to frame transformations were computed for the frames number 0, 3 and 6. The distribution of the relative transformation between the frames in our clips is illustrated in the Fig. 5. It is clear that our data mostly contains rotations, in particular over the axis x. Heatmaps were calculated for two points (M = 2) and with a quality level of 0.1, a minimum distance of 7 and a block size of 7 for the optical flow algorithm (see [4] for more details). The number of heatmaps M and the frame jump k were experimentally chosen among 0, 2, 4, 6.The data was split into train, validation and test sets by a ratio of 7:1.5:1.5. Our method is implemented in Pytorch1 1.8.2, trained and evaluated on a GeForce RTX 3090. We use an Adam optimizer with a learning rate of 10 -4 . The training process converges in 40 epochs with a batch size of 16. The model with the best performance on the validation data was selected and used for the testing. "
Trackerless Volume Reconstruction from Intraoperative Ultrasound Images,3.2,Evaluation Metrics and Results,"The test data was used to evaluate our method, it contains 2060 clips over which our method achieved a translation error of translation of 0.449 ± 0.189 mm, and an orientation error of orientation 1.3 ± 1.5 • . We have evaluated our reconstruction with a commonly used in state-of-the-art metric called final drift error, which measures the distance between the center point of the final frame according to the real relative position and the estimated one in the sequence. On this basis, each of the following metrics was reported over the reconstructions of our method. Final drift rate (FDR): the final drift divided by the sequence length. Average drift rate (ADR): the average cumulative drift of all frames divided by the length from the frame to the starting point of the sequence. Table 1 shows the evaluation of our method over these metrics compared to the state-of-the-art methods MoNet [15] and CNN [9]. Both state-of-the-art methods use IMU sensor data as additional input to estimate the relative transformation between two relative frames. Due to the difficulty of including an IMU sensor in our IVUS catheter, the results of both methods were reported from the MoNet paper where the models have been trained on arm scans, see [15] for more details. As the Table 1 shows, our method is comparable with state-of-the-art methods in terms of drift errors without using any IMU and with non-linear probe motion as one may notice in our data distribution in the Fig. 5. Figure 6 shows the volume reconstruction of two sequences of different sizes with our method in red against the ground truth slices. Despite the non-linearity of the probe motion, the relative pose estimation results obtained by our method remains very accurate. However, one may notice that the drift error increases with respect to the sequence length. This remains a challenge for the community even in the case of linear probe motions. "
Trackerless Volume Reconstruction from Intraoperative Ultrasound Images,4,Conclusion,"In this paper, we proposed the first method for trackerless ultrasound volume reconstruction in the context of minimally invasive surgery. Our method does not use any additional sensor data and is based on a Siamese architecture that leverages the ultrasound image features and the optical flow to estimate relative transformations. Our method was evaluated on ex vivo porcine data and achieved translation and orientation errors of 0.449±0.189 mm and 1.3±1.5 • respectively with a fair drift error. In the future work, we will extend our work to further improve the volume reconstruction and use it to register a pre-operative CT image in order to provide guidance during interventions.Aknowledgments. This work was partially supported by French state funds managed by the ANR under reference ANR-10-IAHU-02 (IHU Strasbourg)."
Trackerless Volume Reconstruction from Intraoperative Ultrasound Images,,Fig. 1 .,
Trackerless Volume Reconstruction from Intraoperative Ultrasound Images,,Fig. 2 .,
Trackerless Volume Reconstruction from Intraoperative Ultrasound Images,,Fig. 3 .,
Trackerless Volume Reconstruction from Intraoperative Ultrasound Images,,Fig. 4 .,
Trackerless Volume Reconstruction from Intraoperative Ultrasound Images,,Fig. 5 .,
Trackerless Volume Reconstruction from Intraoperative Ultrasound Images,,Fig. 6 .,
Trackerless Volume Reconstruction from Intraoperative Ultrasound Images,,Table 1 .,
Structure-Preserving Synthesis: MaskGAN for Unpaired MR-CT Translation,1,Introduction,"Magnetic resonance imaging (MRI) and computed tomography (CT) are two commonly used cross-sectional medical imaging techniques. MRI and CT produce different tissue contrast and are often used in tandem to provide complementary information. While MRI is useful for visualizing soft tissues (e.g. muscle,  [20] fails to preserve the smooth anatomy of the MRI. (c) AttentionGAN [12] inflates the head area in the synthetic CT, which is inconsistent with the original MRI. Quantitative evaluations in MAE (lower is better) are shown in yellow.fat), CT is superior for visualizing bony structures. Some medical procedures, such as radiotherapy for brain tumors, craniosynostosis, and spinal surgery, typically require both MRI and CT for planning. Unfortunately, CT imaging exposes patients to ionizing radiation, which can damage DNA and increase cancer risk [9], especially in children and adolescents. Given these issues, there are clear advantages for synthesizing anatomically accurate CT data from MRI.Most synthesis methods adopt supervised learning paradigms and train generative models to synthesize CT [1][2][3]6,17]. Despite the superior performance, supervised methods require a large amount of paired data, which is prohibitively expensive to acquire. Several unsupervised MRI-to-CT synthesis methods [4,6,14], leverage CycleGAN with cycle consistency supervision to eliminate the need for paired data. Unfortunately, the performance of unsupervised CT synthesis methods [4,14,15] is inferior to supervised counterparts. Due to the lack of direct constraints on the synthetic outputs, CycleGAN [20] struggles to preserve the anatomical structure when synthesizing CT images, as shown in Fig. 1(b). The structural distortion in synthetic results exacerbates when data from the two modalities are heavily misaligned, which usually occurs in pediatric scanning due to the rapid growth in children.Recent unsupervised methods impose structural constraints on the synthesized CT through pixel-wise or shape-wise consistency. Pixel-wise consistency methods [8,14,15] capture and align pixel-wise correlations between MRI and synthesized CT. However, enforcing pixel-wise consistency may introduce undesirable artifacts in the synthetic results. This problem is particularly relevant in brain scanning, where both the pixel-wise correlation and noise statistics in MR and CT images are different, as a direct consequence of the signal acquisition technique. The alternative shape-wise consistency methods [3,4,19] aim to preserve the shapes of major body parts in the synthetic image. Notably, shape-CycleGAN [4] segments synthesized CT and enforces consistency with the ground-truth MRI segmentation. However, these methods rely on segmentation annotations, which are time-consuming, labor-intensive, and require expert radiological annotators. A recent natural image synthesis approach, called Attention-GAN [12], learns attention masks to identify discriminative structures. Atten-tionGAN implicitly learns prominent structures in the image without using the ground-truth shape. Unfortunately, the lack of explicit mask supervision can lead to imprecise attention masks and, in turn, produce inaccurate mappings of the anatomy, as shown in Fig. 1(c). In this paper, we propose MaskGAN, a novel unsupervised MRI-to-CT synthesis method, that preserves the anatomy under the explicit supervision of coarse masks without using costly manual annotations. Unlike segmentationbased methods [4,18], MaskGAN bypasses the need for precise annotations, replacing them with standard (unsupervised) image processing techniques, which can produce coarse anatomical masks. Such masks, although imperfect, provide sufficient cues for MaskGAN to capture anatomical outlines and produce structurally consistent images. Table 1 highlights our differences compared with previous shape-aware methods [4,12]. Our major contributions are summarized as follows. 1) We introduce MaskGAN, a novel unsupervised MRI-to-CT synthesis method. MaskGAN is the first framework that maintains shape consistency without relying on human-annotated segmentation. 2) We present two new structural supervisions to enforce consistent extraction of anatomical structures across MRI and CT domains. 3) Extensive experiments show that our method outperforms state-of-the-art methods by using automatically extracted coarse masks to effectively enhance structural consistency."
Structure-Preserving Synthesis: MaskGAN for Unpaired MR-CT Translation,2,Proposed Method,"In this section, we first introduce the MaskGAN architecture, shown in Fig. 2, and then describe the three supervision losses we use for optimization."
Structure-Preserving Synthesis: MaskGAN for Unpaired MR-CT Translation,2.1,MaskGAN Architecture,"The network comprises two generators, each learning an MRI-CT and a CT-MRI translation. Our generator design has two branches, one for generating masks and the other for synthesizing the content in the masked regions. The mask branch learns N attention masks A i , where the first N -1 masks capture foreground (FG) structures and the last mask represents the background (BG). The content branch synthesizes N -1 outputs for the foreground structures, denoted as C. Each output, C i , represents the synthetic content for the corresponding foreground region that is masked by the attention mask A i .Intuitively, each channel A i in the mask tensor A focuses on different anatomical structures in the medical image. For instance, one channel emphasizes on synthesizing the skull, while another focuses on the brain tissue. The last channel A N in A corresponds to the background and is applied to the original input to preserve the background contents. The final output is the sum of masked foreground contents and masked background input. Formally, the synthetic CT output generated from the input MRI x is defined asThe synthetic MRI output from the CT scan y is defined similarly based on the attention masks and the contents from the MR generator. The proposed network is trained using three training objectives described in the next sections."
Structure-Preserving Synthesis: MaskGAN for Unpaired MR-CT Translation,2.2,CycleGAN Supervision,"The two generators, G MR and G CT , map images from MRI domain (X) and CT domain (Y ), respectively. Two discriminators, D MR and D CT , are used to distinguish real from fake images in the MRI and CT domains. The adversarial loss for training the generators to produce synthetic CT images is defined as(2) The adversarial loss L MR for generating MRI images is defined in a similar manner. For unsupervised training, CycleGAN imposes the cycle consistency loss, which is formulated as follows(3) The CycleGAN's objective L GAN is the combination of adversarial and cycle consistency loss."
Structure-Preserving Synthesis: MaskGAN for Unpaired MR-CT Translation,2.3,Mask and Cycle Shape Consistency Supervision,"Mask Loss. To reduce spurious mappings in the background regions, MaskGAN explicitly guides the mask generator to differentiate the foreground objects from the background using mask supervision. We extract the coarse mask B using basic image processing operations. Specifically, we design a simple but robust algorithm that works on both MRI and CT scans, with a binarization stage followed by a refinement step. In the binarization stage, we normalize the intensity to the range [0, 1] and apply a binary threshold of 0.1, selected based on histogram inspection, to separate the foreground from the background. In the post-processing stage, we refine the binary image using morphological operations, specifically employing a binary opening operation to remove small artifacts. We perform connected component analysis [11] and keep the largest component as the foreground. Column 6 in Fig. 3 shows examples of extracted masks.We introduce a novel mask supervision loss that penalizes the difference between the background mask A N learned from the input image and the groundtruth background mask B in both MRI and CT domains. The mask loss for the attention generators is formulated asDiscussion. Previous shape-aware methods [4,18] use a pre-trained U-Net [10] segmentation network to enforce shape consistency on the generator. U-Net is pre-trained in a separate stage and frozen when the generator is trained. Hence, any errors produced by the segmentation network cannot be corrected. In contrast, we jointly train the shape extractor, i.e., the mask generator, and the content generator end-to-end. Besides mask loss L mask , the mask generator also receives supervision from adversarial loss L GAN to adjust the extracted shape and optimize the final synthetic results. Moreover, in contrast to previous methods that train a separate shape extractor, our MaskGAN uses a shared encoder for mask and content generators, as illustrated in Fig. 2. Our design embeds the extracted shape knowledge into the content generator, thus improving the structural consistency of the synthetic contents.Cycle Shape Consistency Loss. Spurious mappings can occur when the anatomy is shifted during translation. To preserve structural consistency across domains, we introduce the cycle shape consistency (CSC) loss as our secondary contribution. Our loss penalizes the discrepancy between the background attention mask A MR N learned from the input MRI image and the mask ÃCT N learned from synthetic CT. Enforcing consistency in both domains, we formulate the shape consistency loss asThe final loss for MaskGAN is the sum of three loss objectives weighted by the corresponding loss coefficients: L = L GAN + λ mask L mask + λ shape L shape ."
Structure-Preserving Synthesis: MaskGAN for Unpaired MR-CT Translation,3,Experimental Results,
Structure-Preserving Synthesis: MaskGAN for Unpaired MR-CT Translation,3.1,Experimental Settings,"Data Collection. We collected 270 volumetric T1-weighted MRI and 267 thinslice CT head scans with bony reconstruction performed in pediatric patients under routine scanning protocols1 . We targeted the age group from 6-24 months since pediatric patients are more susceptible to ionizing radiation and experience a greater cancer risk (up to 24% increase) from radiation exposure [7]. Furthermore, surgery for craniosynostosis, a birth defect in which the skull bones fuse too early, typically occurs during this age [5,16]. The scans were acquired by Ingenia 3.0T MRI scanners and Philips Brilliance 64 CT scanners. We then resampled the volumetric scans to the same resolution of 1.0 × 1.0 × 1.0 mm 3 . The dataset comprises brain MR and CT volumes from 262 subjects. 13 MRI-CT volumes from the same patients that were captured less than three months apart are registered using rigid registration algorithms. The dataset is divided into 249, 1 and 12 subjects for training, validating and testing set. Following [13], we conducted experiments on sagittal slices. Each MR and CT volume consists of 180 to 200 slices, which are resized and padded to the size of 224 × 224. The intensity range of CT is clipped into [-1000, 2000]. All models are trained using the Adam optimizer for 100 epochs, with a learning rate of 0.0002 which linearly decays to zero over the last 50 epochs. We use a batch size of 16 and train on two NVIDIA RTX 3090 GPUs.Evaluation Metrics. To provide a quantitative evaluation of methods, we compute the same standard performance metrics as in previous works [6,14] including mean absolute error (MAE), peak signal-to-noise ratio (PSNR), and structural similarity (SSIM) between ground-truth and synthesized CT. The scope of the paper centers on theoretical development; clinical evaluations such as dose calculation and treatment planning will be conducted in future work."
Structure-Preserving Synthesis: MaskGAN for Unpaired MR-CT Translation,3.2,Results and Discussions,"Comparisons with State-of-the-Art. We compare the performance of our proposed MaskGAN with existing state-of-the-art image synthesis methods, including CycleGAN [20], AttentionGAN [12], structure-constrained CycleGAN (sc-CycleGAN) [14] and shape-CycleGAN [4]. Shape-CycleGAN requires annotated segmentation to train a separate U-Net. For a fair comparison, we implement shape-CycleGAN using our extracted coarse masks based on the authors' official code. Note that CT-to-MRI synthesis is a secondary task supporting the primary MRI-to-CT synthesis task. As better MRI synthesis leads to improved CT synthesis, we also report the model's performance on MRI synthesis. Table 2. Quantitative comparison of different methods on the primary MRI-CT task and the secondary CT-MRI task. The results of an ablated version of our proposed MaskGAN are also reported. ± standard deviation is reported over five evaluations. The paired t-test is conducted between MaskGAN and a compared method at p = 0.05. The improvement of MaskGAN over all compared methods is statistically significant."
Structure-Preserving Synthesis: MaskGAN for Unpaired MR-CT Translation,,Methods,"Primary: MRI-to-CT Secondary:CycleGAN [20] 32.12 ± 0.31 31.57 ± 0.12 46.17 ± 0.20 34.21 ± 0.33 29.88 ± 0.24 45.73 ± 0.17 AttentionGAN [12] 28.25 ± 0.25 32.88 ± 0.09 53.57 ± 0.15 30.47 ± 0.22 30.15 ± 0.10 50.66 ± 0.14 sc-CycleGAN [14] 24.55 ± 0.24 32.97 ± 0.07 57.08 ± 0.11 26.13 ± 0.15 31.22 ± 0.07 54.14 ± 0.10 shape-CycleGAN [4] 24.30 ± 0.28 33. Table 2 demonstrates that our proposed MaskGAN outperforms existing methods for statistical significance of p = 0.05 in both tasks. The method reduces the MAE of CycleGAN and AttentionGAN by 29.07% and 19.36%, respectively. Furthermore, MaskGAN outperforms shape-CycleGAN, reducing its MAE by 11.28%. Unlike shape-CycleGAN, which underperforms when trained with coarse segmentations, our method obtains consistently higher results. Figure 3 shows the visual results of different methods. sc-CycleGAN produces artifacts (e.g., the eye socket in the first sample and the nasal cavity in the second sample), as it preserves pixel-wise correlations. In contrast, our proposed MaskGAN preserves shape-wise consistency and produces the smoothest synthetic CT. Unlike adult datasets [4,14], pediatric datasets are easily misaligned due to children's rapid growth between scans. Under this challenging setting, unpaired image synthesis can have non-optimal visual results and SSIM scores. Yet, our MaskGAN achieves the highest quality, indicating its suitability for pediatric image synthesis.We perform an ablation study by removing the cycle shape consistency loss (w/o Shape). Compared with shape-CycleGAN, MaskGAN using only a mask loss significantly reduces MAE by 6.26%. The combination of both mask and cycle shape consistency losses results in the largest improvement, demonstrating the complementary contributions of our two losses.Robustness to Error-Prone Coarse Masks. We compare the performance of our approach with shape-CycleGAN [4] using deformed masks that simulate human errors during annotation. To alter object shapes, we employ random elastic deformation, a standard data augmentation technique [10] that applies random displacement vectors to objects. The level of distortion is controlled by the standard deviation of the normal distribution from which the vectors are sampled. Figure 4 (Left) shows MAE of the two methods under increasing levels of distortion. MAE of shape-CycleGAN drastically increases as the masks become more distorted. Figure 4 (Right) shows that our MaskGAN (d) better preserves the anatomy. "
Structure-Preserving Synthesis: MaskGAN for Unpaired MR-CT Translation,4,Conclusion,This paper proposes MaskGAN -a novel automated framework that maintains the shape consistency of prominent anatomical structures without relying on expert annotated segmentations. Our method generates a coarse mask outlining the shape of the main anatomy and synthesizes the contents for the masked foreground region. Experimental results on a clinical dataset show that MaskGAN significantly outperforms existing methods and produces synthetic CT with more consistent mappings of anatomical structures.
Structure-Preserving Synthesis: MaskGAN for Unpaired MR-CT Translation,,Fig. 1 .,
Structure-Preserving Synthesis: MaskGAN for Unpaired MR-CT Translation,,Fig. 2 .,
Structure-Preserving Synthesis: MaskGAN for Unpaired MR-CT Translation,,Fig. 3 .,
Structure-Preserving Synthesis: MaskGAN for Unpaired MR-CT Translation,,Fig. 4 .,
Structure-Preserving Synthesis: MaskGAN for Unpaired MR-CT Translation,,Table 1 .,
Structure-Preserving Synthesis: MaskGAN for Unpaired MR-CT Translation,,(Ours) Coarse mask No Yes,
Structure-Preserving Synthesis: MaskGAN for Unpaired MR-CT Translation,,,
Structure-Preserving Synthesis: MaskGAN for Unpaired MR-CT Translation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 6.
GSMorph: Gradient Surgery for Cine-MRI Cardiac Deformable Registration,1,Introduction,"Image registration is fundamental to many medical image analysis applications, e.g., motion tracking, atlas construction, and disease diagnosis [5]. Conventional registration methods usually require computationally expensive iterative optimization, making it inefficient in clinical practice [1,19]. Deep learning has recently been widely exploited in the registration domain due to its superior representation extraction capability and fast inference speed [2,7]. Deep-learningbased registration (DLR) formulates registration as a network learning process minimizing a composite objective function comprising one similarity loss to penalize the difference in the appearance of the image pair, and a regularization term to ensure the smoothness of deformation field. Typically, to balance the registration accuracy and smoothness of the deformation field, a hyperparameter is introduced in the objective function. However, performing hyperparameter tuning is labor-intensive, time-consuming, and ad-hoc; searching for the optimal parameter setting requires extensive ablation studies and hence training tens of models and establishing a reasonable parameter search space. Therefore, alleviating, even circumventing, hyperparameter search to accelerate development and deployment of DLR models remains challenging.Recent advances [6,11,13] in DLR have primarily focused on network architecture design to boost registration performance. Few studies [9,16] investigated the potential in preventing hyperparameter searching by hypernetwork [8] and conditional learning [10]. Hoopes et al. [9] leveraged a hyper-network that takes the hyperparameter as input to generate the weight of the DLR network. Although effective, it introduces a large number of additional parameters to the basic DLR network, making the framework computationally expensive. In parallel, Mok et al. [16] proposed to learn the effect of the hyperparameter and condition it on the feature statistics (usually illustrated as style in computer vision [10]) to manipulate the smoothness of the deformation field in the inference phase. Both methods can avoid hyperparameter tuning while training the DLR model. However, they still require a reasonable sampling space and strategy of the hyperparameter, which can be empirically dependent.Gradient surgery (GS) projects conflicting gradients of different losses during the optimization process of the model to mitigate gradient interference. This has proven useful in multi-task learning [20] and domain generalization [15]. Motivated by these studies, we propose utilizing the GS to moderate the discordance between the similarity loss and regularization loss. The proposed method can further avert searching the weight for balancing losses in training the DLR.-We propose GSMorph, a gradient-surgery-based DLR model. Our method can circumvent tuning the hyperparameter in composite loss function with a gradient-level reformulation to reach the trade-off between registration accuracy and smoothness of the deformation field. -Existing GS approaches have operated the parameters' gradients independently or integrally. We propose a layer-wise GS to group by the parameters for optimization to ensure the flexibility and robustness of the optimization process. -Our method is model-agnostic and can be integrated into any DLR network without extra parameters or losing inference speed."
GSMorph: Gradient Surgery for Cine-MRI Cardiac Deformable Registration,2,Methodology,"Deformable image registration estimates the non-linear correspondence field φ between the moving, M , and fixed, F , images (Fig. 1). Such procedure is mathematically formulated as φ = f θ (F, M ). For learning-based registration methods, f θ (usually adopted by a neural network) takes the fixed and moving image pair as input and outputs the deformation field via the optimal parameters θ. Typically, θ can be updated using standard mini-batch gradient descent as follows:where α is the learning rate; L sim is the similarity loss to penalize differences in the appearance of the moving and fixed images (e.g., mean square error, mutual information or local negative cross-correlation); L reg is the regularization loss to encourage the smoothness of the deformation field (this can be computed by the gradient of the deformation field); λ is the hyperparameter balancing the tradeoff between L sim and L reg to achieve desired registration accuracy while preserving the smoothness of the deformation field in the meantime. However, hyperparameter tuning is time-consuming and highly experience-dependent, making it tough to reach the optimal solution. Insight into the optimization procedure in Eq. 1, as registration accuracy and spatial smoothness are potentially controversial in model optimization, the two constraints might have different directions and strengths while going through the gradient descent. Based on this, we provide a geometric view to depict the gradient changes for θ based on the gradient surgery technique. The conflicting relationship between two controversial constraints can be geometrically projected as orthogonal vectors. Depending on the orthogonal relationship, merely updating the gradients of the similarity loss would automatically associate with the updates of the regularization term. In this way, we avoid tuning the hyperparameter λ to optimize θ. The Eq. 1 can then be rewritten into a non-hyperparameter pattern:where Φ(•) is the operation of proposed GS method."
GSMorph: Gradient Surgery for Cine-MRI Cardiac Deformable Registration,2.1,Layer-Wise Gradient Surgery,"Figure 2 illustrates the two scenarios of gradients while optimizing the DLR network via vanilla gradient descent or gradient surgery. We first define that the gradient of similarity loss, g sim , and that of regularization loss, g reg , are conflicting when the angle between g sim and g reg is the obtuse angle, viz. g sim , g reg < 0.In this study, we propose updating the parameters of neural networks by the original g sim independently, when g sim and g reg are non-conflicting, representing g sim has no incompatible component of the gradient along the direction of g reg . Consequently, optimization with sole g sim within a non-conflicting scenario can inherently facilitate the spatial smoothness of deformations. Conversely, as shown in Fig. 2, conflicting gradients are the dominant reason associated with non-smooth deformations. Hence, deconflicting gradients in the optimization of the DLR network to ensure high registration accuracy, as well as smooth deformation, is the primary goal of our study. Following a simple and intuitive procedure, we project the g sim onto the normal plane of the g reg , where the projected similarity gradient g and g reg are non-conflicting along each gradient's direction.Existing studies [15,20] performed the GS in terms of independent parameters or the entire network. Despite the effectiveness, these can be either unstable or inflexible. Considering that a neural network usually extracts features through the collaboration of each parameter group in the convolution layers, we introduce a layer-wise GS to ensure its stability and flexibility. The parameter updating rule is detailed in the Algorithm 1. Specifically, in each gradient updating iteration, we first compute the gradients of two losses for the parameter group inNon-conflicting Conflicting Fig. 2. Visualization of vanilla gradient descent and gradient surgery for non-conflicting and conflicting gradients. Regarding vanilla gradient descent, the gradient, g, is computed based on the average of gsim and greg. Our GS-based approach projects the gsim onto the normal vector of greg to prevent disagreements between the similarity loss and regularization loss. On the other hand, we only update the gsim in non-conflicting scenarios.each layer separately. Then, the conflicting relationship between the two gradients is calculated based on their inner production. Once the two gradients are non-conflicting, the gradients used to update its corresponding parameter group will be only the original gradients of similarity loss; on the contrary, the gradients will be the projected similarity gradients orthogonal to the gradients of regularization, which can be calculated as g i sim -After performing GS on all layer-wise parameter groups in the network, the final gradients will be used to update the model's parameters. "
GSMorph: Gradient Surgery for Cine-MRI Cardiac Deformable Registration,,Algorithm 1. Gradient surgery,
GSMorph: Gradient Surgery for Cine-MRI Cardiac Deformable Registration,2.2,Network Architecture,"Our network architecture (seen in Fig. 1) is similar to VoxelMorph [7] that comprises naive U-Net [18] and spatial transform network (STN) [12]. The U-Net takes the moving and fixed image pair as input and outputs the deformation field, which is used to warp the moving image via STN. The U-Net consists of an encoder and a decoder with skip connections, which forward the features from each layer in the encoder to the corresponding layer in the decoder by concatenation to enhance the feature aggregation and prevent gradient vanishing. The number of feature maps in the encoder part of the network is 16, 32, 64, 128, and 256, increasing the number of features as their size shrinks, and vice versa in the decoder part. Each convolutional block in the encoder and decoder has two sequential convolutions with a kernel size of 3, followed by a batch normalization and a leaky rectified linear unit."
GSMorph: Gradient Surgery for Cine-MRI Cardiac Deformable Registration,3,Experiments and Results,
GSMorph: Gradient Surgery for Cine-MRI Cardiac Deformable Registration,3.1,Datasets and Implementations,"Datasets. In this study, we used two public cardiac cine-MRI datasets for investigation and comparison: ACDC [3] and M&M [4]. ACDC and M&M contain 100 and 249 subjects, respectively. We followed a proportion of 75%, 5%, and 20% to split each dataset for training, validation, and testing. We selected the image from the cine-MRI cardiac sequence at the End Systole (ES) time point of the cardiac cycle as the moving image, and that at the End Diastole (ED) as the fixed one. All images were cropped into the size of 128×128 centralized to the heart. We normalized the intensity of images into the range from 0 to 1 before inputting them into the model. Implementation Details. We implemented our model in PyTorch [17], using a standard PC with an NVIDIA GTX 2080ti GPU. We trained the network through Adam optimizer [14] with a learning rate of 5e-3 and a batch size of 32 for 500 epochs. We also implemented and trained alternative methods for comparison with the same data and similar hyper-parameters for optimization. Our source code is available at https://github.com/wulalago/GSMorph."
GSMorph: Gradient Surgery for Cine-MRI Cardiac Deformable Registration,3.2,Alternative Methods and Evaluation Criteria,"To demonstrate the advantages of our proposed method in medical image registration, we compared it with two conventional deformable registration methods, i.e., Demons [19] and SyN [1], and a widely-used DLR model, Voxel-Morph [7]. These methods usually need laborious effort in hyperparameter tuning. Additionally, we reported the results of VoxelMorph trained with different λ (i.e., 0.1, 0.01, and 0.001, denoted as VoxelMorph-l, VoxelMorphm, VoxelMorph-s). Meanwhile, we compared our approach to one alternative DLR model based on the hyperparameter learning, i.e., HyperMorph [9]. This method only require additional validations in searching the optimal hyperparameter without necessarily tuning it from scratch. Finally, we reformulated two variations of GS based on our concept for further comparison. Specifically, GS-Agr [15] treats the gradient of each parameter independently. It updates the parameter with the gradient of similarity loss in the non-conflicting scenario, and a random gradient sampled from the Gaussian distribution when conflicting. While GS-PCGrad [20] uses the same GS strategy as ours, but with respect to the whole parameters of the entire network. The Initial represents the results without any deformation.In this study, we used six criteria to evaluate the efficacy and efficiency of the investigated methods, including Dice score (Dice) and 95% Hausdorff distance (HD95) to validate the registration accuracy of the regions of interest, Mean square error (MSE) to evaluate the pixel-level appearance difference between the moved and fixed image-pairs, the percentage of pixels with negative Jacobian determinant (NJD) values to compare the smoothness and diffeomorphism of the deformation field, the number of parameters (Param) of the neural network and inference speed (Speed) to investigate the efficiency."
GSMorph: Gradient Surgery for Cine-MRI Cardiac Deformable Registration,3.3,Results,"As summarized in Table 1, our method could obtain the best MSE in the ACDC dataset and Dice in the M&M dataset while achieving comparable performance with the tuned VoxelMorph over other metrics. The Dice and HD95 reported in Table 1 were averaged over three anatomical regions of interest in the heart, i.e., Left ventricle, Myocardium, and Right Ventricle (LV, Myo, and RV). Consequently, the proposed model achieved superior registration accuracy and spatial regularization with faster inference speed than the two conventional registration methods. We also observed that our approach gained higher registration performance than HyperMorph in both datasets. Regarding the GS-based methods, GS-Agr totally collapsed, as the conflicting gradients accounted for most have  been replaced by random noise. On the other hand, GS-PCGrad only yielded an inadequate registration performance with an inclination of over-regularization.The comparison in the GS-based method shows the flexibility and robustness of our approach. Figure 3 illustrates the sample cases of the warped images and the corresponding deformation fields from the compared methods. It can be observed our methods could obtain the moved images most similar to the fixed ones. Voxelmorph could achieve comparable results to us but still require a time-consuming hyperparameter tuning. Overall, the results of the comparisons in Table 1 and Fig. 3 indicate that our method performed the best among all the techniques that we implemented and examined, showing the effectiveness of our model in balancing the trade-off between registration accuracy and smoothness of deformations.In Table 2, we have also reported the number of parameters and inference speed. We observed that DLR methods could obtain faster speed compared with conventional ones in general. As our proposed approach only modified the optimization procedure of the backbone network, it could maintain the original inference speed and the number of parameters. Conversely, HyperMorph intro- duced tremendous extra parameters and loss of inference speed as they adopted the secondary network to generate the conditions or weights of the main network architecture."
GSMorph: Gradient Surgery for Cine-MRI Cardiac Deformable Registration,4,Conclusion,"This work presents a gradient-surgery-based registration framework for medical images. To the best of our knowledge, this is the first study to employ gradient surgery to refine the optimization procedure in learning the deformation fields.In our GSMorph, the gradients from the similarity constraint were projected onto the plane orthogonal to those from the regularization term. In this way, merely updating the gradients in optimizing the registration accuracy would result in a joint updating of the gradients from the similarity and regularity constraints. Then, no additional regularization loss is required in the network optimization and no hyperparameter is further required to explicitly trade off between registration accuracy and spatial smoothness. Our model outperformed the conventional registration methods and the alternative DLR models. Finally, the proposed method is model-agnostic and can be integrated into any DLR network without introducing extra parameters or compromising the inference speed. We believe GSMorph will facilitate the development and deployment of DLR models and alleviate the influence of hyperparameters on performance."
GSMorph: Gradient Surgery for Cine-MRI Cardiac Deformable Registration,,Fig. 1 .,
GSMorph: Gradient Surgery for Cine-MRI Cardiac Deformable Registration,,Fig. 3 .,
GSMorph: Gradient Surgery for Cine-MRI Cardiac Deformable Registration,,,Require:
GSMorph: Gradient Surgery for Cine-MRI Cardiac Deformable Registration,,Table 1 .,
GSMorph: Gradient Surgery for Cine-MRI Cardiac Deformable Registration,,Table 2 .,
RESToring Clarity: Unpaired Retina Image Enhancement Using Scattering Transform,1,Introduction,"Microvasculature and neural tissue in the retina can be directly and noninvasively visualized in vivo [19], and retina images are used for the diagnosis of various diseases including cardiovascular diseases [18], Alzheimer's disease [23], and ophthalmological diseases such as glaucoma and cataract [9]. Therefore, screening diseases using retina images has significant advantages over painful invasive methods such as blood tests or biopsies. However, the screening accuracy highly depends on the quality of the image [21] which can be compromised by various factors such as noise, low contrast, uneven illumination, blurriness, camera model, and experience level of the clinician taking the image [6,16,17,22].These issues make it difficult to discriminate important indicators and biomarkers, leading to the failure of early detection of diseases and proper intervention. Therefore, image enhancement of low-quality retina images is highly necessary. However, retina image enhancement is a challenging task due to several reasons. Intuitively, it requires paired low and high-quality retina images to learn to map low-quality images to their high-quality ones and such data are difficult to acquire in practice [1]. Moreover, enhancing images according to signal-to-noise ratio might discard important anatomical structures, e.g., the shape of the optic disc, vessels, and disease-related features, that are critical for disease screening.To deal with the issues above, a structure-preserving guided retina image filter (SGRIF) was proposed to enhance unpaired retina images with prior knowledge of clouding effect [4]. While dealing with unpaired data, it is biased with de-clouding the artifact caused by the cataract. Several parametric methods were recently proposed including [25] and [24] with simple CycleGAN [26] structure which require authentic unpaired data only. Although they can map the tone of the input image such as color, contrast, and illuminance, to those of high-quality images, their ability to preserve crucial anatomical structures that are essential for accurate image examination [6] remains limited. Later, ISE-CRET [5] and PCENet (Pyramid Constraint Enhancement Network) [13] were proposed, which utilize supervised learning with synthesized low-quality images and authentic high-quality image pairs. These models preserve detailed features effectively, however, generating degraded images with priors remains a laborintensive process, and trained models are specific to the dataset in question.In this context, we propose an unpaired Retina image Enhancement with Scattering Transform (REST) which preserves the anatomical structure (e.g., vessels, optic disc, and cup) and maps the tone (e.g., color and illuminance) effectively by utilizing an unpaired dataset. Our model contains a genuine generator that includes two branches: the Anatomy Preserving Branch (APB) and the Tone Transferring Branch (TTB). The APB incorporates scattering transform, which effectively captures anatomic structures and the TTB employs a multilayer convolution to refine the tone of the image as that of high-quality images. Constructing a cyclic architecture [26] with the proposed generators and discriminators for consistency regularization, the REST successfully learns how to enhance the low-quality retina images with the following contributions: 1) REST only uses authentic unpaired data and does not require any prior knowledge, 2) Successful preservation of anatomic structures is achieved with scattering transform, 3) REST is extensively evaluated qualitatively and quantitatively on two independent datasets. Experiments on UK Biobank and EyeQ [6] datasets demonstrate that the REST can adequately enhance retina images by restoring dark and uncertain regions without compromising anatomical structures."
RESToring Clarity: Unpaired Retina Image Enhancement Using Scattering Transform,2,Related Works,"The SGRIF proposed in [4] is a non-parametric method that utilizes a filter with degradation equations describing clouding effects caused by cataracts on the retina image. SGRIF pose and tackles the enhancement problem as a dehazing problem in computer vision. Using the degradation equation, global structure transfer filter, and global edge-preserving smoothing filter, SGRIF de-clouds the retina image. Although this method does not require image pairs, its performance depends on prior functions for degradation which causes a lack of generalizability. Parametric methods, especially those based on deep learning, perform better at generalization by identifying and improving low-quality factors in input images with the parameters. Simple CycleGAN-based models, such as [25] and [24], use only the unpaired images and do not require prior knowledge but face challenges in preserving detailed information. To address this limitation, recent approaches synthesize low-quality images paired with authentic highquality images for supervised learning, as proposed in [5,12,13]. Although such methods preserve structural information, it is prone to being dataset-specific by the degrading model, and synthesizing low-quality images is time-consuming due to the difficulty in designing an accurate degradation model."
RESToring Clarity: Unpaired Retina Image Enhancement Using Scattering Transform,3,Methods,"We propose a generative framework for unpaired retina image enhancement, which translates low-quality retina images X to high-quality images Y through two separate branches: APB and TTB. As illustrated in Fig. 1, an input image is simultaneously fed into both branches with encoder-decoder structures, and their outputs are combined with a kernel to obtain an enhanced image."
RESToring Clarity: Unpaired Retina Image Enhancement Using Scattering Transform,3.1,Anatomy Preserving Branch,"In order to preserve fine anatomical details, such as the shape of an optic disc, an optic cup, and vessels, the model must accurately capture high-frequency components, i.e., edges. To extract these high-frequency components, we designed the APB, which employs a wavelet scattering transform over multiple encoding layers. Wavelet scattering transform extracts high-frequency factors and invariants with respect to translation and rotation [3] by comprising wavelet filters Ψ with the set of frequency and phase indices Λ and J number of scales, a low pass filter Φ, and a modulus operator [2,11]. Given an image x as an input, the scattering transform consisting of one layer, results in 0-th and first-order scattering coefficients, S 0 J and S 1 J , asThe 0-th order scattering coefficient S 0 J x is computed by convolving ( * ) x with Φ. The first-order coefficients set S 1 J x is obtained by convolving x with a set of wavelet filters Ψ λ and modulus operation followed by a low pass filter Φ. Since the wavelet filter sets Ψ λ have diverse frequencies and angles (i.e., Λ) within 2D image space as well as scales (i.e., J), the filters ensure the scattering transform to capture the structural information with respect to the frequency and direction [3]. The design of APB with scattering transform can be seen in Fig. 1 (top). First, the input image, denoted as x, is processed through a sequence of N encoding operations E APB i , where i denotes the layer index aswhere,(The input of each layer e APB i-1 undergoes both the scattering transform S 1 and convolution with a trainable kernel k 1×1 and k APB Ei , except for the last layer in the encoding process. The C i and SC i are concatenated ( ) to yield an input for a subsequent layer. In the final layer, the input undergoes only convolution with a kernel k APB Ei . After the completing N encoding processes, the decoding process, denoted as D APB i , commences. The final encoded feature map of the encoder, e APB N , undergoes N + 1 decoding processes. The decoding process is composed of a resizing R(•) and convolution with trainable kernel k APB Di asIn the upscaling phase (0 < i ≤ N ), a combination of resizing and convolution techniques is implemented instead of transpose convolution. This approach prevents the occurrence of checkerboard artifacts, which can arise from uneven overlap during the transpose convolution [14]. The artifacts are particularly problematic in the APB since the APB deals with high-frequency features which are easily affected them. The use of a combination of resizing and convolution techniques prevents the occurrence of these artifacts and ultimately produces clearer images. More specifically, we utilized interpolation with a factor of 2 and convolution with the k APB Di kernel. Additionally, to preserve anatomical structures such as vessels and an optic disc of the input image, we introduce skip connections [20] between the encoder and decoder. These connections involve the concatenation of the output of each encoder layer, e i (0 < i < N), with the corresponding decoder layer's input, as illustrated in Fig. 1 (top)."
RESToring Clarity: Unpaired Retina Image Enhancement Using Scattering Transform,3.2,Tone Transferring Branch,"As the quality of a retina image depends on the tone of the image such as color, illumination, and contrast in addition to anatomical structures [6], it is crucial to generate synthetic images that have a similar tone to high-quality images. To ensure that the synthetic images resemble the tone of high-quality images, we designed TTB, which consists of multiple convolutional layers with a U-Net [20] architecture as in Fig. 1 (bottom). Similar to the APB, the input x is subject to a sequence of N encoding and decoding operations, denoted as E TTB i and D TTB i , respectively. For the encoding layers E TTB i , the input e TTB i-1 is convolved with the trainable kernel k TTB i as(After the encoding process, the N sequence of decoding operations D TTB i commences with the layer i = N . In the first decoding layer, the output of the encoder e TTB N is up-scaled using the transposed convolution operator ( ). For subsequent layers (1 ≤ i < N), the output of the previous decoding layer d TTB i+1 and the corresponding output of the encoding layer e TTB i are concatenated processed through the transposed convolution operator asAlong with our objective function containing GAN Loss [7], which will be illustrated in the next section, Sect. 3.3, the sequence of convolution and transposed convolution layers encourage the distribution of the generated image, including the tone of the image, to match that of the high-quality images [26]. Moreover, skip connections that provide direct information from the encoder to the decoder constrain the branch from modifying the input image beyond recognition [20]."
RESToring Clarity: Unpaired Retina Image Enhancement Using Scattering Transform,3.3,Loss Function for Unpaired Image Enhancement,"The popular adversarial training architecture for unpaired image translation [26] is devised to train an enhancement model G comprised of APB and TTB, from Sect. 3.1 and Sect. 3.2, for synthesizing high-quality images Y from low-quality and identity losses L G identity , L F identity [26] as follows:To ensure that generated images look like genuine images, the GAN lossesTo preserve the contents of the input image, we aim to make F (G(x)) ≈ x and G(F (y)) ≈ y with the cycle consistency losses as [26] Lwhere λ c1 and λ c2 are hyperparameters. To make sure that the generators avoid translation when there is no need, i.e., a high-quality image as an input to G should lead to a high-quality image without changes, identity losses are defined as [26] Lwith the hyperparameters λ i1 and λ i2 . With the aforementioned losses, we aim to obtain generators G * , F * that minimize the loss function while discriminators D * X , D * Y that maximize it asAfter training, G * is utilized at the inference stage to enhance retina images."
RESToring Clarity: Unpaired Retina Image Enhancement Using Scattering Transform,4,Experiments,
RESToring Clarity: Unpaired Retina Image Enhancement Using Scattering Transform,4.1,Datasets and Experimental Setup,"Datsets. From UKB, 2000 images, i.e., 1000 high-quality and 1000 low-quality images respectively, were randomly sampled and split into train and test sets by a ratio of 2:1. For the EyeQ, we utilized 23252 labeled images, i.e., 6434 lowquality images labeled 'usable' and 16818 high-quality images labeled 'good' [6] and followed the data splitting protocol into the train, validation, and test sets as in [5]. As ISECRET and PCENet require low-quality pairs and a mask for every image during training, degraded pairs were generated between high-quality ones and masks with the given degrading and masking methods in [5,13]. Evaluation Setup. To quantitatively evaluate the quality of the generated retina images, Fundus Image Quality Assessment (FIQA) [6] and Weighted FIQA (WFQA) [13] scores were adopted. For FIQA, a Multiple Color-space Fusion Network (MCF-Net) [6] which labels each input image of the retina as 'Good', 'Usable', or 'Reject' is utilized. With the output of the MCF-Net, the FIQA score is calculated. The FIQA is calculated as the ratio of the number of images labeled as 'Good' to the total number of inputs, while the WFQA is determined by the ratio of the weighted sum of the number of images labeled as 'Good' and 'Usable' to the total number of inputs, with weights of 2 and 1 assigned to 'Good' and 'Usable' respectively. For evaluation, the FIQA and WFQA scores were calculated by inputting output images from enhancement models to pretrained MCF-Net. For UKB data, both quantitative and qualitative evaluations were done for our model (REST) and four baseline models, i.e., two traditional unpaired image translation models, CycleGANs with ResNet and U-Net [8,20,26] and two latest models designed for unpaired retina image enhancement, ISECRET [5] and PCENet [13]. For EyeQ data, experiments were done for REST and PCENet following the settings in [5], and results are compared with traditional paired image translation model cGAN [15] and unpaired image translation models, CycleGAN [26], CutGAN [15], and ISECRET [5], reported in [5].Implmenemtation. The batch size was set to 4 and the Adam optimizer was adopted with the initial learning rate of 0.0002 linearly decaying. For scattering transform, a Morlet wavelet was used with eight different angles within 2D image space and J was set to 1. A 2D Gabor filter was used as a low-pass filter. Parameters in losses, i.e., λ c1 , λ c2 , λ i1 and λ i2 , were set to 10, 10, 5 and 5 respectively. "
RESToring Clarity: Unpaired Retina Image Enhancement Using Scattering Transform,4.2,Results,"Regarding the experiments on UKB, the result of the qualitative evaluation is illustrated in  3. In Fig. 3, the low-quality input image and synthesized high-quality images by each model are shown sequentially. The second and third rows in Fig. 3 show the details of the image marked with a box in the first row.As can be seen in Fig. 3, REST preserved the anatomical structures such as vessels both in and out of the optic disc and the shape of the optic disc and cup better than the baselines. The overall tone of the image was also successfully transferred to make the contrast and the illuminance become even.For the experiment on the EyeQ dataset, the result of the qualitative evaluation is illustrated in Table 1 (right). REST achieved the second-best FIQA score slightly behind by only 0.001p from ISECRET. However, notice that ISECRET requires synthetic paired images while REST uses only unpaired images. As the low-quality images utilized in the EyeQ experiment, i.e., labeled as 'Usable', are already deemed to be of relatively high quality, the capability of REST that shows superior preservation of anatomical structure in contaminated images was not sufficiently manifested as in the UKB experiment. Still, the results are highly competitive and demonstrate potentials to be used for real applications."
RESToring Clarity: Unpaired Retina Image Enhancement Using Scattering Transform,5,Conclusion,"In the study, we proposed a novel approach to unpaired retina image enhancement, i.e., REST. While using only the authentic unpaired images, the proposed method effectively preserves anatomical structures during the enhancement process. This is done by utilizing APB for preserving the details by the scattering transform and TTB for transferring the tone of the images. Notably, REST has demonstrated commendable performance on both two different datasets."
RESToring Clarity: Unpaired Retina Image Enhancement Using Scattering Transform,,Fig. 1 .,
RESToring Clarity: Unpaired Retina Image Enhancement Using Scattering Transform,,Fig. 2 .,
RESToring Clarity: Unpaired Retina Image Enhancement Using Scattering Transform,,Fig. 3 .,
RESToring Clarity: Unpaired Retina Image Enhancement Using Scattering Transform,,Table 1 .,
RESToring Clarity: Unpaired Retina Image Enhancement Using Scattering Transform,,Table 1 (,
RESToring Clarity: Unpaired Retina Image Enhancement Using Scattering Transform,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 45.
Solving Low-Dose CT Reconstruction via GAN with Local Coherence,1,Introduction,"Computed Tomography (CT) is one of the most widely used technologies in medical imaging, which can assist doctors for diagnosing the lesions in human internal organs. Due to harmful radiation exposure of standard-dose CT, the low dose CT is more preferable in clinical application [4,6,34]. However, when the dose is low together with the issues like sparse-view or limited angles, it becomes quite challenging to reconstruct high-quality CT images. The high-quality CT images are important to improve the performance of diagnosis in clinic [27]. In mathematics, we model the CT imaging as the following procedure y = T (x r ) + δ, (1) where x r ∈ R d denotes the unknown ground-truth picture, y ∈ R m denotes the received measurement, and δ is the noise. The function T represents the forward operator that is analogous to the Radon transform, which is widely used in medical imaging [23,28]. The problem of CT reconstruction is to recover x r from the received y.Solving the inverse problem of ( 1) is often very challenging if there is no any additional information. If the forward operator T is well-posed and δ is neglectable, we know that an approximate x r can be easily obtained by directly computing T -1 (y). However, T is often ill-posed, which means the inverse function T -1 does not exist and the inverse problem of (1) may have multiple solutions. Moreover, when the CT imaging is low-dose, the filter backward projection (FBP) [11] can produce serious detrimental artifact. Therefore, most of existing approaches usually incorporate some prior knowledge during the reconstruction [14,17,26]. For example, a commonly used method is based on regularization:where • p denotes the p-norm and R(x) denotes the penalty item from some prior knowledge.In the past years, a number of methods have been proposed for designing the regularization R. The traditional model-based algorithms, e.g., the ones using total variation [3,26], usually apply the sparse gradient assumptions and run an iterative algorithm to learn the regularizers [12,18,24,29]. Another popular line for learning the regularizers comes from deep learning [13,17]; the advantage of the deep learning methods is that they can achieve an end-to-end recovery of the true image x r from the measurement y [1,21]. Recent researches reveal that convolutional neural networks (CNNs) are quite effective for image denoising, e.g., the CNN based algorithms [10,34] can directly learn the reconstructed mapping from initial measurement reconstructions (e.g., FBP) to the ground-truth images. The dual-domain network that combines the sinograms with reconstructed low-dose CT images were also proposed to enhance the generalizability [15,30].A major drawback of the aforementioned reconstruction methods is that they deal with the input CT 2D slices independently (note that the goal of CT reconstruction is to build the 3D model of the organ). Namely, the neighborhood correlations among the 2D slices are often ignored, which may affect the reconstruction performance in practice. In the field of computer vision, ""optical flow"" is a common technique for tracking the motion of object between consecutive frames, which has been applied to many different tasks like video generation [35], prediction of next frames [22] and super resolution synthesis [5,31]. To estimate the optical flow field, existing approaches include the traditional brightness gradient methods [2] and the deep networks [7]. The idea of optical flow has also been used for tracking the organs movement in medical imaging [16,20,33]. However, to the best of our knowledge, there is no work considering GANs with using optical flow to capture neighbor slices coherence for low dose 3D CT reconstruction.In this paper, we propose a novel optical flow based generative adversarial network for 3D CT reconstruction. Our intuition is as follows. When a patient is located in a CT equipment, a set of consecutive cross-sectional images are generated. If the vertical axial sampling space of transverse planes is small, the corresponding CT slices should be highly similar. So we apply optical flow, though there exist several technical issues waiting to solve for the design and implementation, to capture the local coherence of adjacent CT images for reducing the artifacts in low-dose CT reconstruction. Our contributions are summarized below:1. We introduce the ""local coherence"" by characterizing the correlation of consecutive CT images, which plays a key role for suppressing the artifacts. 2. Together with the local coherence, our proposed generative adversarial networks (GANs) can yield significant improvement for texture quality and stability of the reconstructed images. 3. To illustrate the efficiency of our proposed approach, we conduct rigorous experiments on several real clinical datasets; the experimental results reveal the advantages of our approach over several state-of-the-art CT reconstruction methods."
Solving Low-Dose CT Reconstruction via GAN with Local Coherence,2,Preliminaries,"In this section, we briefly review the framework of the ordinary generative adversarial network, and also introduce the local coherence of CT slices."
Solving Low-Dose CT Reconstruction via GAN with Local Coherence,,Generative Adversarial Network.,"Traditional generative adversarial network [8] consists of two main modules, a generator and a discriminator. The generator G is a mapping from a latent-space Gaussian distribution P Z to the synthetic sample distribution P XG , which is expected to be close to the real sample distribution P X . On the other hand, the discriminator D aims to maximize the distance between the distributions P XG and P X . The game between the generator and discriminator actually is an adversarial process, where the overall optimization objective follows a min-max principle:Local Coherence. As mentioned in Sect. 1, optical flow can capture the temporal coherence of object movements, which plays a crucial role in many videorelated tasks. More specifically, the optical flow refers to the instantaneous velocity of pixels of moving objects on consecutive frames over a short period of time [2]. The main idea relies on the practical assumptions that the brightness of the object more likely remains stable across consecutive frames, and the brightness of the pixels in a local region are usually changed consistently [9].Based on these assumptions, the brightness of optical flow can be described by the following equation:where v = (v w , v h ) represents the optical flow of the position (w, h) in the image. ∇I = (∇I w , ∇I h ) denotes spatial gradients of image brightness, and ∇I t denotes the temporal partial derivative of the corresponding region. Following the Eq. ( 4), we consider the question that whether the optical flow idea can be applied to 3D CT reconstruction. In practice, the brightness of adjacent CT images often has very tiny difference, due to the inherent continuity and structural integrity of human body. Therefore, we introduce the ""local coherence"" that indicates the correlation between adjacent images of a tissue. Namely, adjacent CT images often exhibit significant similarities within a certain local range along the vertical axis of the human body. Due to the local coherence, the noticeable variations observed in CT slices within the local range often occur at the edges of organs. We can substitute the temporal partial derivative ∇I t by the vertical axial partial derivative ∇I z in the Eq. ( 4), where ""z"" indicates the index of the vertical axis. As illustrated in Fig. 1, the local coherence can be captured by the optical flow between adjacent CT slices. "
Solving Low-Dose CT Reconstruction via GAN with Local Coherence,3,GANs with Local Coherence,"In this section, we introduce our low-dose CT image generation framework with local coherence in detail.The Framework of Our Network. The proposed framework comprises three components, including a generator G, a discriminator D and an optical flow estimator F. The generator is the core component, and the flow estimator provides auxiliary warping images for the generation process.Suppose we have a sequence of measurements y 1 , y 2 , • • • , y n ; for each y i , 1 ≤ i ≤ n, we want to reconstruct its ground truth image x r i as the Eq. ( 1). Before performing the reconstruction in the generator G, we apply some prior knowledge in physics and run filter backward projection on the measurement y i in Eq. ( 1) to obtain an initial recovery solution s i . Usually s i contains significant noise comparing with the ground truth x r i . Then the network has two input components, i.e., the initial backward projected image s i that serves as an approximation of the ground truth x r i , and a set of neighbor CT slices N (s i ) = {s i-1 , s i+1 }1 for preserving the local coherence. The overall structure of our framework is shown in Fig. 2. Below, we introduce the three key parts of our framework separately. Optical Flow Estimator. The optical flow F(N (s i ), s i ) denotes the brightness changes of pixels from N (s i ) to s i , where it captures their local coherence. The estimator is derived by the network architecture of FlowNet [7]. The FlowNet is an autoencoder architecture with extraction of features of two input frames to learn the corresponding flow, which is consist of 6 (de)convolutional layers for both encoder and decoder.Discriminator. The discriminator D assigns the label ""1"" to real standarddose CT images and ""0"" to generated images. The goal of D is to maximize the separation between the distributions of real images and generated images:where x g i is the image generated by G (the formal definition for x g i will be introduced below). The discriminator includes 3 residual blocks, with 4 convolutional layers in each residual block.Generator. We use the generator G to reconstruct the high-quality CT image for the ground truth x r i from the low-dose image s i . The generated image is obtained bywhere W(•) is the warping operator. Before generating x g i , N (x g i ) is reconstructed from N (s i ) by the generator without considering local coherence. Subsequently, according to the optical flow F(N (s i ), s i ), we warp the reconstructed images N (x g i ) to align with the current slice by adjusting the brightness values. The warping operator W utilizes bi-linear interpolation to obtain W(N (x g i )), which enables the model to capture subtle variations in the tissue from the generated N (x g i ); also, the warping operator can reduce the influence of artifacts for the reconstruction. Finally, x g i is generated by combining s i and W(N (x g i )). Since x r i is our target for reconstruction in the i-th batch, we consider the difference between x g i and x r i in the loss. Our generator is mainly based on the network architecture of Unet [25]. Partly inspired by the loss in [5], the optimization objective of the generator G comprises three items with the coefficients λ pix , λ adv , λ per ∈ (0, 1):In (7), ""L pixel "" is the loss measuring the pixel-wise mean square error of the generated image x g i with respect to the ground-truth x r i . ""L adv "" represents the adversarial loss of the discriminator D, which is designed to minimize the distance between the generated standard-dose CT image distribution P XG and the real standard-dose CT image distribution P X . ""L percept "" denotes the perceptual loss, which quantifies the dissimilarity between the feature maps of x r i and x g i ; the feature maps denote the feature representation extracted from the hidden layers in the discriminator D (suppose there are t hidden layers):where D j (•) refers to the feature extraction performed on the j-th hidden layer. Through capturing the high frequency differences in CT images, L percept can enhance the sharpness for edges and increase the contrast for the reconstructed images. L pixel and L adv are designed to recover global structure, and L percept is utilized to incorporate additional texture details into the reconstruction process."
Solving Low-Dose CT Reconstruction via GAN with Local Coherence,4,Experiment,"Datasets. First, our proposed approaches are evaluated on the ""Mayo-Clinic low-dose CT Grand Challenge"" (Mayo-Clinic) dataset of lung CT images [19].The dataset contains 2250 two dimensional slices from 9 patients for training, and the remaining 128 slices from 1 patient are reserved for testing. The lowdose measurements are simulated by parallel-beam X-ray with 200 (or 150) uniform views, i.e., N v = 200 (or N v = 150), and 400 (or 300) detectors, i.e., N d = 400 (or N d = 300). In order to further verify the denoising ability of our approaches, we add the Gaussian noise with standard deviation σ = 2.0 to the sinograms after X-ray projection in 50% of the experiments. To evaluate the generalization of our model, we also consider another dataset RIDER with nonsmall cell lung cancer under two CT scans [36] for testing. We randomly select 4 patients with 1827 slices from the dataset. The simulation process is identical to that of Mayo-Clinic. The proposed networks were implemented in the PyTorch framework and trained on Nvidia 3090 GPU with 100 epochs.Baselines and Evaluation Metrics. We consider several existing popular algorithms for comparison. ( 1) FBP [11]: the classical filter backward projection on low-dose sinograms. ( 2) FBPConvNet [10]: a direct inversion network followed by the CNN after initial FBP reconstruction. ( 3) LPD [1]: a deep learning method based on proximal primal-dual optimization. ( 4) UAR [21]: an end-toend reconstruction method based on learning unrolled reconstruction operators and adversarial regularizers. Our proposed method is denoted by GAN-LC.We set λ pix = 1.0, λ adv = 0.01 and λ per = 1.0 for the optimization objective in Eq. ( 7) during our training process. Following most of the previous articles on 3D CT reconstruction, we evaluate the experimental performance by two metrics: the peak signal-to-noise ratio (PSNR) and the structural similarity index (SSIM) [32]. PSNR measures the pixel differences of two images, which is negatively correlated with mean square error. SSIM measures the structure similarity between two images, which is related to the variances of the input images. For both two measures, the higher the better.Results. a similar increasing trend with our approach across different settings but has worse reconstruction quality. To evaluate the stability and generalization of our model and the baselines trained on Mayo-Clinic dataset, we also test them on the RIDER dataset. The results are shown in Table 2. Due to the bias in the datasets collected from different facilities, the performances of all the models are declined to some extents. But our proposed approach still outperforms the other models for most testing cases.To illustrate the reconstruction performances more clearly, we also show the reconstruction results for testing images in Fig. 3. We can see that our network can reconstruct the CT image with higher quality. Due to the space limit, the experimental results of different views N v and more visualized results are placed in our supplementary material. "
Solving Low-Dose CT Reconstruction via GAN with Local Coherence,5,Conclusion,"In this paper, we propose a novel approach for low-dose CT reconstruction using generative adversarial networks with local coherence. By considering the inherent continuity of human body, local coherence can be captured through optical flow, which is small deformations and structural differences between consecutive CT slices. The experimental results on real datasets demonstrate the advantages of our proposed network over several popular approaches. In future, we will evaluate our network on real-world CT images from local hospital and use the reconstructed images to support doctors for the diagnosis and recognition of lung nodules. Our code is publicly available at https://github.com/lwjie595/GANLC."
Solving Low-Dose CT Reconstruction via GAN with Local Coherence,,Image Registration,
Solving Low-Dose CT Reconstruction via GAN with Local Coherence,,Fig. 1 .,
Solving Low-Dose CT Reconstruction via GAN with Local Coherence,,Fig. 2 .,
Solving Low-Dose CT Reconstruction via GAN with Local Coherence,,Fig. 3 .,
Solving Low-Dose CT Reconstruction via GAN with Local Coherence,,Table 1 .,
Solving Low-Dose CT Reconstruction via GAN with Local Coherence,,Table 2 .,
Solving Low-Dose CT Reconstruction via GAN with Local Coherence,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 50.
Geometric Ultrasound Localization Microscopy,1,Introduction,"Ultrasound Localization Microscopy (ULM) has revolutionized medical imaging by enabling sub-wavelength resolution from images acquired by piezo-electric transducers and computational beamforming. However, the necessity of beamforming for ULM remains questionable. Our work challenges the conventional assumption that beamforming is the ideal processing step for ULM and presents an alternative approach based on geometric reconstruction from Time-of-Arrival (ToA) information.The discovery of ULM has recently surpassed the diffraction-limited spatial resolution and enabled highly detailed visualization of the vascularity [8]. ULM borrows concepts from super-resolution fluorescence microscopy techniques to precisely locate individual particles with sub-pixel accuracy over multiple frames. By the accumulation of all localizations over time, ULM can produce a superresolved image, providing researchers and clinicians with highly detailed representation of the vascular structure.While Contrast-Enhanced Ultra-Sound (CEUS) is used in the identification of musculoskeletal soft tissue tumours [5], the far higher resolution capability offered by ULM has great potential for clinical translation to improve the reliability of cancer diagnosis (i.e., enable differentiation of tumour types in kidney cancer [7] or detect breast cancer tissue [1]). Moreover, ULM has shown promise in imaging neurovascular activity after visual stimulation (functional ULM) [14]. The pioneering study by Errico et al. [8] initially demonstrated the potential of ULM by successfully localizing contrast agent particles (microbubbles) using a 2D point-spread-function model. In general, the accuracy in MicroBubble (MB) localization is the key to achieving sub-wavelength resolution [4], for which classical imaging methods [11,17], as well as deep neural networks [1,16], have recently been reported. However, the conventional approach for ULM involves using computational beamformers, which may not be ideal for MB localization. For example, a recent study has shown that ultrasound image segmentation can be learned from radiofrequency data and thus without beamforming [13]. Beamforming techniques have been developed to render irregular topologies, whereas MBs exhibit a uniform geometric structure, for which ULM only requires information about its spatial position. Although the impact of adaptive beamforming has been studiedfor ULM to investigate its potential to refine MB localization [3], optimization of the Point-Spread Function (PSF) poses high demands on the transducer array, data storage, and algorithm complexity.To this end, we propose an alternative approach for ULM, outlined in Fig. 1, that entirely relies on Time-Difference-of-Arrival (TDoA) information, omitting beamforming from the processing pipeline for the first time. We demonstrate a novel geometry framework for MB localization through ellipse intersections to overcome limitations inherent to beamforming. This approach provides a finer distinction between overlapping and clustered spots, improving localization precision, reliability, and computation efficiency. In conclusion, we challenge the conventional wisdom that beamforming is necessary for ULM and propose a novel approach that entirely relies on TDoA information for MB localization. Our proposed approach demonstrates promising results and indicates a considerable trade-off between precision, computation, and memory."
Geometric Ultrasound Localization Microscopy,2,Method,"Geometric modeling is a useful approach for locating landmarks in space. One common method involves using a Time-of-Flight (ToF) round-trip setup that includes a transmitter and multiple receivers [10]. This setup is analogous to the parallax concept in visual imaging, where a triangle is formed between the target, emitter, and receivers, as illustrated in Fig. 2. The target's location can be accurately estimated using trilateration by analyzing the time delay between the transmitted and received signals. However, the triangle's side lengths are unknown in the single receiver case, and all possible travel path candidates form triangles with equal circumferences fixed at the axis connecting the receiver and the source. These candidates reside on an elliptical shape. By adding a second receiver, its respective ellipse intersects with the first one resolving the target's 2-D position. Thus, the localization accuracy depends on the ellipse model, which is parameterized by the known transducer positions and the time delays we seek to estimate. This section describes a precise echo feature extraction, which is essential for building the subsequent ellipse intersection model. Finally, we demonstrate our localization refinement through clustering."
Geometric Ultrasound Localization Microscopy,2.1,Feature Extraction,"Feature extraction of acoustic signals has been thoroughly researched [9,18]. To leverage the geometric ULM localization, we wish to extract Time-of-Arrival (ToA) information (instead of beamforming) at sub-wavelength precision. Despite the popularity of deep neural networks, which have been studied for ToA detection [18], we employ an energy-based model [9] for echo feature extraction to demonstrate the feasibility of our geometric ULM at the initial stage. Ultimately, future studies can combine our proposed localization with a supervised network. Here, echoes f (m k ; t) are modeled as Multimodal Exponentially-Modified Gaussian Oscillators (MEMGO) [9],where t ∈ R T denotes the time domain with a total number of T samples andangular frequency ω k and phase φ k for each echo k. Note that erf(•) is the error function. To estimate these parameters iteratively, the cost function is given by,where y n (t) is the measured signal from waveform channel n ∈ {1, 2, . . . , N} and the sum over k accumulates all echo components mn = [m 1 , m 2 , . . . , m K ] . We get the best echo feature set m n over all iterations j via,for which we use the Levenberg-Marquardt solver. Model-based optimization requires initial estimates to be nearby the solution space. For this, we detect initial ToAs via gradient-based analysis of the Hilbert-transformed signal to set m(1) n as in [9]. Before geometric localization, one must ensure that detected echo components correspond to the same MB. In this work, echo matching is accomplished in a heuristic brute-force fashion. Given an echo component m n,k from a reference channel index n, a matching echo component from an adjacent channel index n ± g with gap g ∈ N is found by k + h in the neighborhood of h ∈ {-1, 0, 1}. A corresponding phase-precise ToA t n,k is obtained by t n±g,k = μ n±g,k+h +φ n,k -Δ , which takes μ n,k and φ n,k from m n for phase-precise alignment across transducer channels after upsampling. Here, Δ is a fixed offset to accurately capture the onset of the MB locations [2]. We validate echo correspondence through a re-projection error in adjacent channels and reject those with weak alignment."
Geometric Ultrasound Localization Microscopy,2.2,Ellipse Intersection,"While ellipse intersections can be approximated iteratively, we employ Eberly's closed-form solution [6] owing to its fast computation property. Although one might expect that the intersection of arbitrarily placed ellipses is straightforward, it involves advanced mathematical modelling due to the degrees of freedom in the ellipse positioning. An ellipse is drawn by radii (r a , r b ) of the major and minor axes with,where the virtual transmitter ûs ∈ R 2 and each receiver u n ∈ R 2 with channel index n represent the focal points of an ellipse, respectively. For the intersection, we begin with the ellipse standard equation. Let any point s ∈ R 2 located on an ellipse and displaced by its center c n ∈ R 2 such that, where M contains the ellipse equation with v n and v ⊥ n as a pair of orthogonal ellipse direction vectors, corresponding to their radial extents (r 0 , r 1 ) as well as the squared norm • 2  2 and vector norm | • |. For subsequent root-finding, it is the goal to convert the standard Eq. ( 5) to a quadratic polynomial with coefficients b j given by, B(x, y) = b 0 + b 1 x + b 2 y + b 3 x 2 + b 4 xy = 0, which, when written in vector-matrix form reads,where B and b carry high-order polynomial coefficients b j found via matrix factorization [6]. An elaborated version of this is found in the supplementary material.Let two intersecting ellipses be given as quadratic equations A(x, y) and B(x, y) with coefficients a j and b j , respectively. Their intersection is found via polynomial root-finding of the equation,where ∀j, d j = a jb j . When defining y = w -(a 2 + a 4 x)/2 to substitute y, we get A(x, w) = w 2 + (a 0 + a 1 x + a 3 x 2 ) -(a 2 + a 4 x) 2 /4 = 0 which after rearranging is plugged into (7) to yield an intersection point s i = [x i , w i ] . We refer the interested reader to the insightful descriptions in [6] for further implementation details."
Geometric Ultrasound Localization Microscopy,2.3,Clustering,"Micro bubble reflections are dispersed across multiple waveform channels yielding groups of location candidates for the same target bubble. Localization deviations result from ToA variations, which can occur due to atmospheric conditions, receiver clock errors, and system noise. Due to the random distribution of corresponding ToA errors [8], we regard these candidates as clusters. Thus, we aim to find a centroid p of each cluster using multiple bi-variate probability density functions of varying sample sizes by,Here, the bandwidth of the kernel is set to λ/4. The Mean Shift algorithm updates the estimate p (j) by setting it to the weighted mean density on each iteration j until convergence. In this way, we obtain the position of the target bubble."
Geometric Ultrasound Localization Microscopy,3,Experiments,"Dataset: We demonstrate the feasibility of our geometric ULM and present benchmark comparison outcomes based on the PALA dataset [11]. This dataset is chosen as it is publicly available, allowing easy access and reproducibility of our results. To date, it is the only public ULM dataset featuring Radio Frequency (RF) data as required by our method. Its third-party simulation data makes it possible to perform a numerical quantification and direct comparison of different baseline benchmarks for the first time, which is necessary to validate the effectiveness of our proposed approach.Metrics: For MB localization assessment, the minimum Root Mean Squared Error (RMSE) between the estimated p and the nearest ground truth position is computed. To align with the PALA study [11], only RMSEs less than λ/4 are considered true positives and contribute to the total RMSE of all frames. In cases where the RMSE distance is greater than λ/4, the estimated p is a false positive. Consequently, ground truth locations without an estimate within the λ/4 neighbourhood are false negatives. We use the Jaccard Index to measure the MB detection capability, which considers both true positives and false negatives and provides a robust measure of each algorithm's performance. The Structural Similarity Index Measure (SSIM) is used for image assessment.For a realistic analysis, we employ the noise model used in [11], which is given by,where σ p = √ B × 10 P/10 and N (0, σ 2 p ) are normal distributions with mean 0 and variance σ 2 p . Here, L C and L A are noise levels in dB, and n(t) is the array of length T containing the random values drawn from this distribution. The additive noise model is then used to simulate a waveform channel y n (t) = y n (t)+ n(t) g(t, σ f ) suffering from noise, where represents the convolution operator, and g(t, σ f ) is the one-dimensional Gaussian kernel with standard deviation σ f = 1.5. To mimic the noise reduction achieved through the use of sub-aperture beamforming with 16 transducer channels [11], we multiplied the RF data noise by a factor of 4 for an equitable comparison.Baselines: We compare our approach against state-of-the-art methods that utilize beamforming together with classical image filterings [8], Spline interpolation [17], Radial Symmetry (RS) [11] and a deep-learning-based U-Net [16] for MB localization. To only focus on the localization performance of each algorithm, we conduct the experimental analysis without temporal tracking. We obtain the results for classical image processing approaches directly from the open-source code provided by the authors of the PALA dataset [11]. As there is no publicly available implementation of [16] to date, we model and train the U-Net [15] according to the paper description, including loss design, layer architecture, and the incorporation of dropout. Since the U-Net-based localization is a supervised learning approach, we split the PALA dataset into sequences 1-15 for testing and 16-20 for training and validation, with a split ratio of 0.9, providing a sufficient number of 4500 training frames.Results: Table 1 provides the benchmark comparison results with state-of-theart methods. Our proposed geometric inference indicates the best localization performance represented by an average RMSE of around one-tenth of a wavelength. Also, the Jaccard Index reflects an outperforming balance of true positive and false negative MB detections by our approach. These results support the hypothesis that our proposed geometric localization inference is a considerable alternative to existing beamforming-based methods. Upon closer examination of the channels column in Table 1, it becomes apparent that our geometric ULM achieves reasonable localization performance with only a fraction of the 128 channels available in the transducer probe. Using more than 32 channels improves the Jaccard Index but at the expense of computational resources. This finding confirms the assumption that transducers are redundant for MB tracking. The slight discrepancy in SSIM scores between our 128-channel results and the 64channel example may be attributed to the higher number of false positives in the former, which decreases the overall SSIM value. We provide rendered ULM image regions for visual inspection in Fig. 3 with full frames in the supplementary material. To enhance visibility, all images are processed with sRGB and additional gamma correction using an exponent of 0.9. The presence of noisy points in Figs. 3b to 3d is attributed to the exces- sive false positive localizations, resulting in poorer SSIM scores. Overall, these visual observations align with the numerical results presented in Table 1. An NVIDIA RTX2080 GPU was used for all computations and time measurements. To improve performance, signal processing chains are often pipelined, allowing for the simultaneous computation of subsequent processes. Table 1 lists the most time-consuming process for each method, which acts as the bottleneck. For our approach, the MEMGO feature extraction is the computationally most expensive process, followed by clustering. However, our method contributes to an overall efficient computation and acquisition time, as it skips beamforming and coherent compounding [12] with the latter reducing the capture interval by two-thirds.Table 2 presents the results for the best-of-3 algorithms at various noise levels L C . As the amount of noise from (9) increases, there is a decline in the Jaccard Index, which suggests that each method is more susceptible to false detections from noise clutter. Although our method is exposed to higher noise in the RF domain, it is seen that L C has a comparable impact on our method. However, it is important to note that the U-Net yields the most steady and consistent results for different noise levels. "
Geometric Ultrasound Localization Microscopy,4,Summary,"This study explored whether a geometric reconstruction may serve as an alternative to beamforming in ULM. We employed an energy-based model for feature extraction in conjunction with ellipse intersections and clustering to pinpoint contrast agent positions from RF data available in the PALA dataset. We carried out a benchmark comparison with state-of-the-art methods, demonstrating that our geometric model provides enhanced resolution and detection reliability with fewer transducers. This capability will be a stepping stone for 3-D ULM reconstruction where matrix transducer probes typically consist of 32 transducers per row only. It is essential to conduct follow-up studies to evaluate the high potential of our approach in an extensive manner before entering a pre-clinical phase. The promising results from this study motivate us to expand our research to more RF data scenarios. We believe our findings will inspire further research in this exciting and rapidly evolving field."
Geometric Ultrasound Localization Microscopy,,Fig. 1 .,
Geometric Ultrasound Localization Microscopy,,Fig. 2 .,
Geometric Ultrasound Localization Microscopy,,Fig. 3 .,
Geometric Ultrasound Localization Microscopy,,Table 1 .,
Geometric Ultrasound Localization Microscopy,,Table 2 .,
Geometric Ultrasound Localization Microscopy,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5_21.
ASCON: Anatomy-Aware Supervised Contrastive Learning Framework for Low-Dose CT Denoising,1,Introduction,"With the success of deep learning in the field of computer vision and image processing, many deep learning-based methods have been proposed and achieved promising results in low-dose CT (LDCT) denoising [1,[4][5][6]9,12,23,24,26]. Typically, they employ a supervised learning setting, which involves a set of image pairs, LDCT images and their normal-dose CT (NDCT) counterparts. These methods typically use a pixel-level loss (e.g. mean squared error or MSE), which can cause over-smoothing problems.To address this issue, a few studies [23,26] used a structural similarity (SSIM) loss or a perceptual loss [11]. However, they all perform in a sample-to-sample manner and ignore the inherent anatomical semantics, which could blur details in areas with low noise levels. Previous studies have shown that the level of noise in CT images varies depending on the type of tissues [17]; see an example in Fig. S1 in Supplementary Materials. Therefore, it is crucial to characterize the anatomical semantics for effectively denoising diverse tissues.In this paper, we focus on taking advantage of the inherent anatomical semantics in LDCT denoising from a contrastive learning perspective [7,25,27]. To this end, we propose a novel Anatomy-aware Supervised CONtrastive learning framework (ASCON), which consists of two novel designs: an efficient selfattention-based U-Net (ESAU-Net) and a multi-scale anatomical contrastive network (MAC-Net). First, to ensure that MAC-Net can effectively extract anatomical information, diverse global-local contexts and a larger input size are necessary. However, operations on full-size CT images with self-attention are computationally unachievable due to potential GPU memory limitations [20]. To address this limitation, we propose an ESAU-Net that utilizes a channel-wise self-attention mechanism [2,22,28] which can efficiently capture both local and global contexts by computing cross-covariance across feature channels.Second, to exploit inherent anatomical semantics, we present the MAC-Net that employs a disentangled U-shaped architecture [25] to produce global and local representations. Globally, a patch-wise non-contrastive module is designed to select neighboring patches with similar semantic context as positive samples and align the same patches selected in denoised CT and NDCT which share the same anatomical information, using an optimization method similar to the BYOL method [7]. This is motivated by the prior knowledge that adjacent patches often share common semantic contexts [27]. Locally, to further improve the anatomical consistency between denoised CT and NDCT, we introduce a pixel-wise contrastive module with a hard negative sampling strategy [21], which randomly selects negative samples from the pixels with high similarity around the positive sample within a certain distance. Then we use a local InfoNCE loss [18] to pull the positive pairs and push the negative pairs.Our contributions are summarized as follows. 1) We propose a novel ASCON framework to explore inherent anatomical information in LDCT denoising, which is important to provide interpretability for LDCT denoising. 2) To better explore anatomical semantics in MAC-Net, we design an ESAU-Net, which utilizes a channel-wise self-attention mechanism to capture both local and global contexts. 3) We propose a MAC-Net that employs a disentangled U-shaped architecture and incorporates both global non-contrastive and local contrastive modules. This enables the exploitation of inherent anatomical semantics at the patch level, as well as improving anatomical consistency at the pixel level. 4) Extensive experimental results demonstrate that our ASCON outperforms other state-ofthe-art methods, and provides anatomical interpretability for LDCT denoising."
ASCON: Anatomy-Aware Supervised Contrastive Learning Framework for Low-Dose CT Denoising,2,Methodology,
ASCON: Anatomy-Aware Supervised Contrastive Learning Framework for Low-Dose CT Denoising,2.1,Overview of the Proposed ASCON,"Figure 1 presents the overview of the proposed ASCON, which consists of two novel components: ESAU-Net and MAC-Net. First, given an LDCT image, X ∈ R 1×H×W , where H × W denotes the image size. X is passed through the ESAU-Net to capture both global and local contexts using a channel-wise self-attention mechanism and obtain a denoised CT image Y ∈ R 1×H×W .Then, to explore inherent anatomical semantics and remain inherent anatomical consistency, the denoised CT Y and NDCT Y are passed to the MAC-Net to compute a global MSE loss L global in a patch-wise non-contrastive module and a local infoNCE loss L local in a pixel-wise contrastive module. During training, we use an alternate learning strategy to optimize ESAU-Net and MAC-Net separately, which is similar to GAN-based methods [10]. Please refer to Algorithm S1 in Supplementary Materials for a detailed optimization."
ASCON: Anatomy-Aware Supervised Contrastive Learning Framework for Low-Dose CT Denoising,2.2,Efficient Self-attention-Based U-Net,"To better leverage anatomical semantic information in MAC-Net and adapt to the high-resolution input, we design the ESAU-Net that can capture both local and global contexts during denoising. Different from previous works that only use self-attention in the coarsest level [20], we incorporate a channel-wise selfattention mechanism [2,28] at each up-sampling and down-sampling level in the U-Net [22] and add an identity mapping in each level, as shown in Fig. 1(a).Specifically, in each level, given the feature map F l-1 as the input, we first apply a 1×1 convolution and a 3×3 depth-wise convolution to aggregate channelwise contents and generate query (Q), key (K), and value (V ) followed by a reshape operation, where Q ∈ R C×HW , K ∈ R C×HW , and V ∈ R C×HW (see Fig. 1(a)). Then, a channel-wise attention map A ∈ R C×C is generated through a dot-product operation by the reshaped query and key, which is more efficient than the regular attention map of size HW × HW [3], especially for high-resolution input. Overall, the process is defined aswhere w(•) first reshapes the matrix back to the original size C ×H ×W and then performs 1 × 1 convolution; α is a learnable parameter to scale the magnitude of the dot product of K and Q. We use multi-head attention similar to the standard multi-head self-attention mechanism [3]. The output of the channelwise self-attention is represented as: F l-1 = Attention(F l-1 ) + F l-1 . Finally, the output F l of each level is defined as: F l = Conv(F l-1 ) + Iden(F l-1 ), where Conv(•) is a two-layer convolution and Iden(•) is an identity mapping using a 1 × 1 convolution; refer to Fig. S2(a) for the details of ESAU-Net."
ASCON: Anatomy-Aware Supervised Contrastive Learning Framework for Low-Dose CT Denoising,2.3,Multi-scale Anatomical Contrastive Network,"Overview of MAC-Net. The goal of our MAC-Net is to exploit anatomical semantics and maintain anatomical embedding consistency, First, a disentangled U-shaped architecture [22] is utilized to learn global representation16 after four down-sampling layers, and learn local representation F l ∈ R 64×H×W by removing the last output layer. And we cut the connection between the coarsest feature and its upper level to make F g and F l more independent [25] (see Fig. S2(b)). The online network and the target network, both using the same architecture above, handle denoised CT Y and NDCT Y , respectively, with F g and F l generated by the online network, and F g and F l generated by the target network (see Fig. 1(b)). The parameters of the target network are an exponential moving average of the parameters in the online network, following the previous works [7,8]. Next, a patch-wise non-contrastive module uses F g and F g to compute a global MSE loss L global , while a pixel-wise contrastive module uses F l and F l to compute a local infoNCE loss L local . Let us describe these two loss functions specifically."
ASCON: Anatomy-Aware Supervised Contrastive Learning Framework for Low-Dose CT Denoising,,Patch-Wise Non-contrastive Module.,"To better learn anatomical representations, we introduce a patch-wise non-contrastive module, also shown in Fig. 1(b). Specifically, for each pixel f (i) g ∈ R 512 in the F g where i ∈ {1, 2, . . . , HW  256 } is the index of the pixel location, it can be considered as a patch due to the expanded receptive field achieved through a sequence of convolutions and down-sampling operations [19]. To identify positive patch indices, we adopt a neighboring positive matching strategy [27], assuming that a semantically similar patch f (j) g exists in the vicinity of the query patch f (i) g , as neighboring patches often share a semantic context with the query. We empirically consider a set of 8 neighboring patches. To sample patches with similar semantics around the query patch f (i) g , we measure the semantic closeness between the query patch f (i) g and its neighboring patches f (j) g using the cosine similarity, which is formulated asWe then select the top-4 positive patches {fg } j∈P (i) based on s(i, j), where P (i) is a set of selected patches (i.e., |P (i) | = 4). To obtain patch-level features g (i) ∈ R 512 for each patch f (i) g and its positive neighbors, we aggregate their features using global average pooling (GAP) in the patch dimension. For the local representation of f (i) g , we select positive patches as same as P (i) , i.e., {f (j) g } j∈P (i) . Formally,From the patch-level features, the online network outputs a projection z g (i) = p g (g (i) ) and a prediction q (z g (i) ) while target network outputs the target projection z g (i) = p g (g (i) ). The projection and prediction are both multilayer perceptron (MLP). Finally, we compute the global MSE loss between the normalized prediction and target projection [7],where N g pos is the indices set of positive samples in the patch-level embedding. Pixel-Wise Contrastive Module. In this module, we aim to improve anatomical consistency between the denoised CT and NDCT using a local InfoNCE loss [18] (see Fig. 1(b)). First, for a query f l (i) ∈ R 64 in the F l and its positive sample f (i) l ∈ R 64 in the F l (i ∈ {1, 2, . . . , HW } is the location index), we use a hard negative sampling strategy [21] to select ""diffcult"" negative samples with high probability, which enforces the model to learn more from the fine-grained details. Specifically, candidate negative samples are randomly sampled from F l as long as their distance from f (i) l is less than m pixels (m = 7). We also use cosine similarity in Eq. ( 2) to select a set of semantically closest pixels, i.e. {f neg |)×256 . The local InfoNCE loss in the pixel level is defined aswhere N l pos is the indices set of positive samples in the pixel level. vand v (j) l ∈ R 256 are the query, positive, and negative sample in z (i) l , respectively."
ASCON: Anatomy-Aware Supervised Contrastive Learning Framework for Low-Dose CT Denoising,2.4,Total Loss Function,"The final loss is defined as L = L global + L local + λL pixel , where L pixel consists of two common supervised losses: MSE and SSIM, defined as L pixel = L MSE + L SSIM . λ is empirically set to 10."
ASCON: Anatomy-Aware Supervised Contrastive Learning Framework for Low-Dose CT Denoising,3,Experiments,
ASCON: Anatomy-Aware Supervised Contrastive Learning Framework for Low-Dose CT Denoising,3.1,Dataset and Implementation Details,"We use two publicly available low-dose CT datasets released by the NIH AAPM-Mayo Clinic Low-Dose CT Grand Challenge in 2016 [15] and lately released in 2020 [16], denoted as Mayo-2016 and Mayo-2020, respectively. There is no overlap between the two datasets. neg | = 24 for Mayo-2020. We use a binary function to filter the background while selecting queries in MAC-Net. For the training strategy, we employ a window of [-1000, 2000] HU. We train our network for 100 epochs on 2 NVIDIA GeForce RTX 3090, and use the AdamW optimizer [14] with the momentum parameters β 1 = 0.9, β 2 = 0.99 and the weight decay of 1.0 × 10 -9 . We initialize the learning rate as 1.0×10 -4 , gradually reduced to 1.0×10 -6 with the cosine annealing [13]. Since MAC-Net is only implemented during training, the testing time of ASCON is close to most of the compared methods."
ASCON: Anatomy-Aware Supervised Contrastive Learning Framework for Low-Dose CT Denoising,3.2,Performance Comparisons,"Quantitative Evaluations. We use three widely-used metrics including peak signal-to-noise ratio (PSNR), root-mean-square error (RMSE), and SSIM. Table 1 presents the testing results on Mayo-2016 and Mayo-2020 datasets. We compare our methods with 5 state-of-the-art methods, including RED-CNN [1], WGAN-VGG [26], EDCNN [12], DU-GAN [9], and CNCL [6]. Table 1 shows that our ESAU-Net with MAC-Net achieves the best performance on both the Mayo-2016 and the Mayo-2020 datasets. Compared to the ESAU-Net, ASCON further improves the PSNR by up to 0.54 dB on Mayo-2020, which demonstrates the effectiveness of the proposed MAC-Net and the importance of the inherent anatomical semantics during CT denoising. We also compute the contrast-tonoise ratio (CNR) to assess the detectability of a selected area of low-contrast lesion and our ASCON achieves the best CNR in Fig. S3.    Visualization of Inherent Semantics. To demonstrate that our MAC-Net can exploit inherent anatomical semantics of CT images during denoising, we select the features before the last layer in ASCON without MAC-Net and ASCON from Mayo-2016. Then we cluster these two feature maps respectively using a K-means algorithm and visualize them in the original dimension, and finally visualize the clustering representations using t-SNE, as shown in Fig. 3. Note that ASCON produces a result similar to organ semantic segmentation after clustering and the intra-class distribution is more compact, as well as the inter-class separation is more obvious. To the best of our knowledge, this is the first time that anatomical semantic information has been demonstrated in a CT denoising task, providing interpretability to the field of medical image reconstruction.Ablation Studies. We start with a ESAU-Net using MSE loss and gradually insert some loss functions and our MAC-Net. Table 2 presents the results of different loss functions. It shows that both the global non-contrastive module and local contrastive module are helpful in obtaining better metrics due to the capacity of exploiting inherent anatomical information and maintaining anatomical consistency. Then, we add our MAC-Net to two supervised models: RED-CNN [1] and U-Net [22] but it is less effective, which demonstrates the importance of our ESAU-Net that captures both local and global contexts during denoising in Table S1. In addition, we evaluate the effectiveness of the training strategies including alternate learning, neighboring positive matching and hard negative sampling in Table S2."
ASCON: Anatomy-Aware Supervised Contrastive Learning Framework for Low-Dose CT Denoising,4,Conclusion,"In this paper, we explore the anatomical semantics in LDCT denoising and take advantage of it to improve the denoising performance. To this end, we propose an Anatomy-aware Supervised CONtrastive learning framework (ASCON), consisting of an efficient self-attention-based U-Net (ESAU-Net) and a multi-scale anatomical contrastive network (MAC-Net), which can capture both local and global contexts during denoising and exploit inherent anatomical information. Extensive experimental results on Mayo-2016 and Mayo-2020 datasets demonstrate the superior performance of our method, and the effectiveness of our designs. We also validated that our method introduces interpretability to LDCT denoising."
ASCON: Anatomy-Aware Supervised Contrastive Learning Framework for Low-Dose CT Denoising,,Fig. 1 .,
ASCON: Anatomy-Aware Supervised Contrastive Learning Framework for Low-Dose CT Denoising,,,
ASCON: Anatomy-Aware Supervised Contrastive Learning Framework for Low-Dose CT Denoising,,,
ASCON: Anatomy-Aware Supervised Contrastive Learning Framework for Low-Dose CT Denoising,,Fig. 2 .,
ASCON: Anatomy-Aware Supervised Contrastive Learning Framework for Low-Dose CT Denoising,,Fig. 3 .,
ASCON: Anatomy-Aware Supervised Contrastive Learning Framework for Low-Dose CT Denoising,,Table 1 .,ESAU-Net (ours) 44.38
ASCON: Anatomy-Aware Supervised Contrastive Learning Framework for Low-Dose CT Denoising,,ASCON (ours) 44.48±1.32 0.60±0.10 97.49±0.86 48.84±1.68 0.37±0.11 99.32±0.18,
ASCON: Anatomy-Aware Supervised Contrastive Learning Framework for Low-Dose CT Denoising,,Table 2 .,local + L
ASCON: Anatomy-Aware Supervised Contrastive Learning Framework for Low-Dose CT Denoising,,global 48.84±1.68 0.37±0.11 99.32±0.18,
ASCON: Anatomy-Aware Supervised Contrastive Learning Framework for Low-Dose CT Denoising,,Acknowledgements,". This work was supported in part by National Natural Science Foundation of China (No. 62101136), Shanghai Sailing Program (No. 21YF1402800), Shanghai Municipal Science and Technology Major Project (No. 2018SHZDZX01) and ZJLab, Shanghai Municipal of Science and Technology Project (No. 20JC1419500), and Shanghai Center for Brain Science and Brain-inspired Technology."
ASCON: Anatomy-Aware Supervised Contrastive Learning Framework for Low-Dose CT Denoising,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 34.
CDiffMR: Can We Replace the Gaussian Noise with K-Space Undersampling for Fast MRI?,1,Introduction,"Magnetic Resonance Imaging (MRI) is an essential non-invasive technique that enables high-resolution and reproducible assessments of structural and functional information, for clinical diagnosis and prognosis, without exposing the patient to radiation. Despite its widely use in clinical practice, MRI still suffers from the intrinsically slow data acquisition process, which leads to uncomfortable patient experience and artefacts from voluntary and involuntary physiological movements [5]. Deep learning has achieved considerable success across various research domains in recent years, including the ability to substantially accelerate MRI reconstruction while requiring fewer measurements. Various kinds of deep learning-based models, including Convolutional Neural Networks [15,21], Recurrent Neural Networks [4,9], Graph Neural Networks [11] or Transformers [12,13], have been explored for MRI reconstruction and achieved impressive success with a high accelerate factor (AF). However, most of these methods are based on a strong degradation prior, i.e., the undersampling mask, which entails a performance drop when the training and testing undersampling masks mismatch [14,16]. Therefore, additional training is required when applying different undersampling mask condition, leading to a waste of computational resources.Diffusion models [10,18,20] represent a group of unconditional generative methods which sample data points that belong to target distribution from a Gaussian distribution. The earliest diffusion models type was known as Denoising Diffusion Probabilistic Models (DDPMs) [10] and Score Matching with Langevin Dynamics (SMLD) [18], which were later unified into a framework by Score-based Stochastic Differential Equation (SDE) [20].Diffusion models have been widely applied for inverse problems [6,19] including MRI Reconstruction [2,3,7,8,14]. Peng et al. [14] proposed a diffusion modelbased MR reconstruction method, called DiffuseRecon, which did not require additional training on specific acceleration factors. Chung et al. [7] designed a score-based model for MRI reconstruction, which performed the reconstruction task iteratively using a numerical SDE solver and data consistency step. Cao et al. [3] proposed a complex diffusion probabilistic model for MRI reconstruction for better preservation of the MRI complex-value information. Gungor et al. [8] introduced an adaptive diffusion prior, namely AdaDiff, for enhancing reconstruction performance during the inference stage. Cao et al. [2] designed a modified high-frequency DDPM model for high-frequency information preservation of MRI data. However, they do share a commonality-the prolonged inference time due to the iterative nature of diffusion models. Chung et al. [6] proposed a new reverse process strategy for accelerating the sampling for the reverse problem, Come-Closer-Diffuse-Faster (CCDF), suggesting that starting from Gaussian noise is necessary for diffusion models. CCDF-MRI achieved outstanding reconstruction results with reduced reverse process steps.Most existing diffusion models, including the original DDPM, SMLD and their variants, are strongly based on the use of Gaussian noise, which provides the 'random walk' for 'hot' diffusion. Cold Diffusion Model [1] rethought the role of the Gaussian noise, and generalised the diffusion models using different kinds of degradation strategies, e.g., blur, pixelate, mask-out, rather than the Gaussian noise applied on conventional diffusion models.In this work, a novel Cold Diffusion-based MRI Reconstruction method (CDiffMR) is proposed (see Fig. 1). CDiffMR introduces a novel K-Space Undersampling Degradation (KSUD) module for the degradation, which means CDiffMR does not depend on the Gaussian noise. Instead of building an implicit transform to target distribution by Gaussian noise, CDiffMR explicitly learns the relationship between undersampled distribution and target distribution by KSUD.We propose two novel k -space conditioning strategies to guide the reverse process and to reduce the required time steps. 1) Starting Point Conditioning (SPC). The k -space undersampled zero-filled images, which is usually regarded as the network input, can act as the reverse process starting point for conditioning. The number of reverse time steps therefore depends on the undersamping rate, i.e., the higher k -space undersampling rate (lower AF, easier task), the fewer reverse time steps required. 2) Data Consistency Conditioning (DCC). In every step of the reverse process, data consistency is applied to further guide the reverse process in the correct way.It is note that our CDiffMR is a one-for-all model. This means that once CDiffMR is trained, it can be reused for all the reconstruction tasks, with any reasonable undersampling rates conditioning, as long as the undersampling rate is larger than the preset degraded images x T at the end of forward process (e.g., 1% undersampling rate). Experiments were conducted on FastMRI dataset [22]. The proposed CDiffMR achieves comparable or superior reconstruction results with respect to state-of-the-art methods, and reaches a much faster reverse process compared with diffusion model-based counterparts. For the sake of clarity, we use 'sampling' or 'undersampling' to specify the k -space data acquisition for MRI, and use 'reverse process' to represent sampling from target data distribution in the inference stage of diffusion models. Our main contributions are summarised as follows:• An innovative Cold Diffusion-based MRI Reconstruction methods is proposed. To best of our knowledge, CDiffMR is the first diffusion model-based MRI reconstruction method that exploits the k -space undersampling degradation. • Two novel k -space conditioning strategies, namely SPC and DCC, are developed to guide and accelerate the reverse process. • The pre-trained CDiffMR model can be reused for MRI reconstruction tasks with a reasonable range of undersampling rates."
CDiffMR: Can We Replace the Gaussian Noise with K-Space Undersampling for Fast MRI?,2,Methodology,"This section details two key parts of the proposed CDiffMR: 1) the optimisation and training schemes and 2) the k -space conditioning reverse process.  min for LogSR schedule (see Fig. 2). D(x, t+1) contains less k -space information compared to D(x, t), and when t = 0 we have:"
CDiffMR: Can We Replace the Gaussian Noise with K-Space Undersampling for Fast MRI?,2.1,Model Components and Training,"where M t is the undersampling mask at step t corresponding to SR t , and M 0 = I is the fully-sampling mask (identity map). F and F -1 denote Fourier and inverse Fourier transform.The restoration operator R θ (•, t) is an improved U-Net with time embedding module, following the official implementation 1 of Denoising Diffusion Implicit Models [17].For the training of the restoration operator R θ (•, t), x true is the fully-sampled images randomly sampled from target distribution X . Practically, time step t is randomly chosen from (1, T ] during the training. The driven optimisation scheme reads:min"
CDiffMR: Can We Replace the Gaussian Noise with K-Space Undersampling for Fast MRI?,2.2,K-Space Conditioning Reverse Process,"Two k -space conditioning strategies, SPC and DCC, are designed to guide and accelerate the reverse process.Starting Point Conditioning enables the reverse process of CDiffMR to start from the half way step T instead of step T (see Fig. 1 and Fig. 2). The starting point of the reverse process depends on the k -space undersampling rate of the reconstruction task. Specifically, for the reconstruction task with M, T can be checked by comparing the sampling rate of M in the degradation schedule, and the corresponding reverse process start step T can be located, which is expressed as:Sampling Rate:With the start step T , the reverse process is conditioned by the reverse process of the initial image x T ← F -1 y. The framework of the reverse process follows Algorithm 2 in [1], whereas we applied DCC strategy for further guiding (Eq. ( 5)). The result of the reverse process x 0 is the final reconstruction result. The whole reverse process is formulated as:x t-1 = x t -D(x 0,t , t) + D(x 0,t , t -1), s.t. t = T ...1. (6)"
CDiffMR: Can We Replace the Gaussian Noise with K-Space Undersampling for Fast MRI?,3,Experimental Results,This section describes in detail the set of experiments conducted to validate the proposed CDiffMR. 
CDiffMR: Can We Replace the Gaussian Noise with K-Space Undersampling for Fast MRI?,3.1,Implementation Details and Evaluation Methods,"The experiments were conducted on the FastMRI dataset [22], which contains single-channel complex-value MRI data. For the FastMRI dataset, we applied 684 proton-density weighted knee MRI scans without fat suppression from the official training and validation sets, which were randomly divided into training set (420 cases), validation set (64 cases) and testing set (200 cases), approximately according to a ratio of 6:1:3. For each case, 20 coronal 2D single-channel complex-value slices near the centre were chosen, and all slices were centrecropped to 320 × 320.Undersampling mask M and M t were generated by the fastMRI official implementation. We applied AF×8 and ×16 Cartesian mask for all experiments.Our proposed CDiffMR was trained on two NVIDIA A100 (80 GB) GPUs, and tested on an NVIDIA RTX3090 (24 GB) GPU. CDiffMR was trained for 100,000 gradient steps, using the Adam optimiser with a learning rate 2e-5 and a batch size 24. We set the total diffusion time step to T = 100 for both LinSR and LogSR degradation schedules. The minimal sampling rate (when t = 100) was set to 1%.For comparison, we used CNN-based methods D5C5 [15], DAGAN [21], Transformers-based method SwinMR [12], novel diffusion model-based method DiffuseRecon [14]. We trained D5C5 and DiffuseRecon following the official setting, while we modified DAGAN and SwinMR for 2-channel input, output and loss function, as they were officially proposed for single-channel reconstruction.In the quantitative experiments, Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index Measure (SSIM), Learned Perceptual Image Patch Similarity (LPIPS) [23] were applied to examine the reconstruction quality. Among them, LPIPS is a deep learning-based perceptual metric, which can match human visual perception well. The inference time was measured on a NVIDIA RTX3090 (24 GB) GPU with an input shape of (1, 1, 320, 320) for ten times. "
CDiffMR: Can We Replace the Gaussian Noise with K-Space Undersampling for Fast MRI?,3.2,Comparison and Ablation Studies,"The quantitative results are reported in Table 1 and further supported by visual comparisons in Fig. 3. The proposed CDiffMR achieves promising results compared to the SOTA MRI reconstruction methods. Compared with the diffusion model-based method DiffuseRecon, CDiffMR achieves comparable or better results with only 1.6-3.4% inference time of DiffuseRecon. For ablation studies, we explored how DDC and SPC affect the speed of reverse process and reconstruction quality (see Fig. 4(A)(B)). "
CDiffMR: Can We Replace the Gaussian Noise with K-Space Undersampling for Fast MRI?,4,Discussion and Conclusion,"This work has exploited Cold Diffusion-based model for MRI reconstruction and proposed CDiffMR. We have designed the novel KSUD for degradation operator D(•, t) and trained a restoration function R θ (•, t) for de-aliasing under various undersampling rates. We pioneered the harmonisation of the degradation function in reverse problem (k -space undersampling in MRI reconstruction) and the degradation operator in diffusion model (KSUD). In doing so, CDiffMR is able to explicitly learn the k -space undersampling operation to further improve the reconstruction, while providing the basis for the reverse process acceleration. Two k -space conditioning strategies, SPC and DCC, have been designed to guide and accelerate the reverse process. Experiments have demonstrated that k -space undersampling can be successfully used as degradation in diffusion models for MRI reconstruction. In this study, two KSUD schedules, i.e., have been designed for controlling the k -space sampling rate of every reverse time steps. According to Table 1, LogSR schedule achieves better perceptual score while LinSR has better fidelity score, where the difference is actually not significant. However, the required reverse process steps of LogSR is much fewer than LinSR's, which significantly accelerates the reverse process. This is because for LogSR schedule, a larger proportion steps are corresponding to lower sampling rate (high AF), therefore the starting point of LogSR is closer to step 0 than LinSR (see Fig. 2 and Fig. 4(A)). For AF×16 reconstruction task, CDiffMR-LinSR theoretically requires 95 reverse steps, while CDiffMR-LogSR only requires 61 reverse steps, and for AF×8 reconstruction task, CDiffMR-LinSR requires 89 reverse steps, while CDiffMR-LogSR only requires 46 reverse steps. The lower AF of the reconstruction task, the less reverse process steps required. Therefore, we recommend CDiffMR-LogSR as it achieves similiar results of CDiffMR-LinSR with much faster reverse process.In the ablation studies, we have further examined the selection of reverse process starting point T . Figure 4(A) has shown the reconstruction quality using different starting point. Reconstruction performance keeps stable with a range of reverse process starting points, but suddenly drops after a tuning point, which exactly matches the theoretical starting point. This experiment has proven the validity of our starting point selection method (Eq. ( 3)), and shown that our theoretical starting point keeps optimal balance between the reconstruction process and reverse process speed.We also explored the validity of DDC in the ablation studies. Figure 4(B) has shown the reconstruction quality with or without DDC using different sampling rate schedule with different AF. The improvement by the DDC is significant with a lower AF (×4), but limited with a higher AF (×8, ×16). Therefore, CDiffMR keeps the DDC due to its insignificant computational cost.The proposed CDiffMR heralds a new kind of diffusion models for solving inverse problems, i.e., applying the degradation model in reverse problem as the degradation module in diffusion model. CDiffMR has proven that this idea performs well for MRI reconstruction tasks. We can envision that our CDiffMR can serve as the basis for general inverse problems."
CDiffMR: Can We Replace the Gaussian Noise with K-Space Undersampling for Fast MRI?,,Fig. 1 .,
CDiffMR: Can We Replace the Gaussian Noise with K-Space Undersampling for Fast MRI?,,Fig. 2 .,
CDiffMR: Can We Replace the Gaussian Noise with K-Space Undersampling for Fast MRI?,,Fig. 3 .,
CDiffMR: Can We Replace the Gaussian Noise with K-Space Undersampling for Fast MRI?,,Fig. 4 .,
CDiffMR: Can We Replace the Gaussian Noise with K-Space Undersampling for Fast MRI?,,,
CDiffMR: Can We Replace the Gaussian Noise with K-Space Undersampling for Fast MRI?,,Table 1 .,
PIViT: Large Deformation Image Registration with Pyramid-Iterative Vision Transformer,1,Introduction,"Deformable image registration is one of the fundamental tasks in computer vision and has been widely used in medical image processing. In recent years, deep learning methods based on convolutional neural networks are widely applied in deformable image registration. Balakrishnan et al. [3] proposed VoxelMorph with a structure similar to Unet and further developed a diffeomorphism implementation of VoxelMorph [8]. Mok et al. [21] proposed SYMNet to achieve accurate diffeomorphic registration by exploiting the cycle consistency of registration. However, when there is a significant difference between the images, it is difficult to learn an accurate deformation field for alignment because large deformation image registration has a high degree of freedom in transformation. Typical registration methods utilize rigid or affine transformation with a low degree of freedom to provide initialized global transformation for large deformation, however, this requires the introduction of additional preprocessing to obtain the corresponding affine matrix [12]  [23]. In order to solve the high degree of freedom of large deformation transformation, the end-to-end deformable image registration methods are mainly divided into two types: iterative registration (Fig. 1 (a)) and pyramid registration (Fig. 1 (b)). (a) Iterative registration achieves coarse-to-fine image registration by cascading several CNNs, which requires huge GPU memory during training. In addition, iterative registration methods learn separate image features in each iteration, which brings additional computational costs when repeatedly extracting features. Typical iterative registration methods include RCN [28] and LapIRN [22]. (b) Pyramid registration achieves coarse-to-fine registration within one iteration by warping feature maps. These methods successively learn feature maps and deformation fields from low to high resolution. Typical pyramid registration methods include Dual-PRNet [14] and NICE-Net [20]. However, current non-iterative registration methods still cannot well solve the image registration problem under the significant differences condition.Inspired by the capabilities of Transformer in NLP, recent researchers have extended Transformer to computer vision tasks [11] [19] and acquired results that surpass CNNs' in many tasks [17]  [27]. Many Transformer-based registration methods have also been proposed for image registration tasks, such as Trans-Morph [7], Swin-VoxelMorph [30] and XMorpher [26]. Compared with CNNbased methods, Transformer-based methods have achieved better registration results, which illustrates that the global receptive field of Transformer is helpful for image registration.In this paper, we propose a novel Pyramid-Iterative Vision Transformer (PIViT) by combining Swin Transformer-based long-range correlation decoder and the proposed pyramid-iterative registration framework shown in Fig. 1 (c). Our main contributions of this work are as: (1) We establish a pyramiditerative registration framework to address large deformation image registration. The framework first extracts feature map pairs via a dual-stream weight-sharing encoder, then performs iterative registration on the low-scale feature space, and finally complements detail information and learns accurate deformation fields during pyramid decoding process. (2) We propose a Swin Transformer-based long-range correlation decoder, which exploits the global receptive field of Swin Transformer on low-scale feature maps to learn high accuracy large deformation fields while maintaining low parameters. (3) Compared with other popular registration methods, the proposed unsupervised end-to-end network is more lightweight and suitable for time-sensitive tasks.Extensive experiments on 3D brain MRI and liver CT registration tasks demonstrate that PIViT achieves state-of-the-art performance in terms of accuracy but consumes less time and parameters."
PIViT: Large Deformation Image Registration with Pyramid-Iterative Vision Transformer,2,The Proposed Method,"In this section, we first propose a novel pyramid-iterative registration framework to solve large deformation image registration. The pyramid-iterative registration framework combines the advantages of iteration and pyramid registration framework to achieve fast and accurate registration. Then, we introduce a long-range correlation decoder based on Swin Transformer into the iterative registration stage of the proposed framework and utilize the global receptive field of the Swin Transformer to capture global correlations, thereby implementing high accurate and fast registration. "
PIViT: Large Deformation Image Registration with Pyramid-Iterative Vision Transformer,2.1,Pyramid-Iterative Registration Framework,"As shown in Fig. 2, the proposed pyramid-iterative registration framework can be divided into three parts: dual-stream feature extraction, low-scale iterative registration and multi-scale pyramid registration.Dual-Stream Feature Extraction: Similar to pyramid registration network, the proposed framework utilizes a weight-sharing feature encoder to construct feature pyramids for the fixed image I f and the moving image I m , respectively. At the i th step (i ∈ [1 • • • N ]), the feature maps of I f and I m are formulated as F i f and F i m , respectively. The weight-sharing feature encoder reuses the same network blocks to extract the feature maps F i f and F i m without adding parameters or complicating the training process while ensuring that F i f and F i m are in the same feature space.Low-Scale Iterative Registration: The pyramid-iterative registration uses two different decoding modules at different scales. To capture large deformation, we adopt low-scale feature maps to obtain the coarse distribution of large deformation fields without considering the fine distribution in this paper. Therefore, at the last N th level of feature pyramid, deformation field is predicted from F N f and F N m multiple times through an iterative structure. Similar to iterativebased registration methods, F N m is warped by the predicted deformation field φ N t , where t is the number of iterations. The warped F N,t m and F N f are used for the next iteration. In the first iteration, the decoder obtains the initial deformation field φ N 1 , and in the subsequent iterations, the residual deformation field Δφ N t is obtained in each prediction and the updated overall deformation field φ N t is obtained. This procedure can be formulated as:where T is the upper limit of iteration, • denotes warping the feature map with deformation fields, and + denotes element-wise summation of deformation fields.Compared with other iterative registration methods, the advantage of iterating only at the N th level is that there is no need to re-extract image features, thus the computational complexity and time consumption of our method can be greatly reduced. This can greatly accelerate the speed of model training and deformation field prediction, and better solve large deformation.Multi-scale Pyramid Registration: After the implementation of low-scale iterative registration, the deformation field φ N T is rescaled by a factor of 2 and the rescaled flow φ N is obtained. The subsequent process is the same as that of the pyramid registration method. At each level, warped feature) are concatenated and the residual deformation field Δφ i is predicted by 3D convolution. Δφ i is used to update φ i+1 so as to obtain the deformation field φ i corresponding to the i th layer. φ i is rescaled by a factor of 2 and warps moving feature F i-1 m . The purpose of introducing multi-scale pyramid registration is to supplement the lack of fine information caused by only using low-scale features in the iterative registration stage. This process is repeated at each level of the feature pyramid until the deformation field is rescaled to the original image resolution. Finally, the pyramid-iterative registration framework obtains the predicted global deformation field."
PIViT: Large Deformation Image Registration with Pyramid-Iterative Vision Transformer,2.2,Long-Range Correlation Decoder,"To capture large deformation at low-scale registration, the study of the decoder is very essential. Therefore, we propose a long-range correlation decoder (LCD) in the iterative registration phase. As shown in Fig. 3, the LCD consists of a Swin transformer-based block and two consecutive convolutions. The Swin transformer-based block models the long-range correlation between F N f and F N m • φ N t-1 using the self-attention mechanism of the transformer, and then the residual flow field Δφ N t is obtained by the convolution block. In order to enhance the information interaction between non-overlapping windows, we adopt the shifted local window attention strategy of the Swin Transformer. The structure of the Swin Transformer-based block is shown in the red frame area in Fig. 3, which consists of shifted window-based self-attention modules (W-SA & SW-SA), followed by a 2-layer MLP. A LayerNorm (LN) layer is applied before each SA and MLP module, and a residual connection is applied after each module.Current Transformer-based registration methods usually directly migrate the Transformer structure to the 3D image registration task, which leads to a large number of parameters and a remarkably long inference time. In contrast, the proposed PIViT models long-range correlations in low-scale iterative registration with LCD to warp corresponding voxels between feature maps to spatial neighborhoods, thus it is not necessary to use the Transformer on large feature maps at high scales. In addition, LCD also removes position embedding, only uses single-head self-attention and reduces the number of channels. These operations accelerate the speed of PIViT and significantly reduce parameters. "
PIViT: Large Deformation Image Registration with Pyramid-Iterative Vision Transformer,2.3,Loss Function,"PIViT is an unsupervised end-to-end registration network. In this section, we design a loss function to train the proposed network. In the final stage of pyramid registration, PIViT obtains the deformation field φ between I m and I f and the warped image I w = I m • φ by using the differential operation based on the spatial transformer network [15]. In order to minimize the difference, we use the normalized cross-correlation (NCC) as a measure of the difference between the warped image I w and fixed image I f .In order to ensure the continuity and smoothness of the deformation field φ in space, a regular term on its spatial gradient is introduced. The complete loss function is:where λ is the regularization hyperparameter."
PIViT: Large Deformation Image Registration with Pyramid-Iterative Vision Transformer,3,Experiments,"Data and Pre-processing: We evaluate the performance of PIViT on brain MRI datasets and liver CT datasets. In the experiments, we compare the proposed method with commonly used 3D convolutional registration methods Voxelmorph [3], Dual-PRNet [14], RCN [28], LKU-Net [16], TransMorph [7], Swin-VoxelMorph [30] and NICE-Net [20]. The accuracy of image registration is measured by Dice score [10]. We choose 2303 brain MRI scans from the ABIDE [9], ADHD [5] and ADNI [24] brain MRI datasets for training and LPBA [25] for testing. LPBA dataset contains 40 brain MRI scans with segmentation ground truth of 56 anatomical structures. For liver CT datasets, 1025 scans from MSD [2] and BFH [29] are selected for training and SLIVER [13], LiTS [6] and LSPIG [28] for testing. The images are all resampled to the size of 128×128×128. In order to better verify the effect of each method on large deformation image registration, we do not perform affine pre-alignment process. Atlas-based and scan-to-scan registrations are performed on brain and liver scans, respectively.Implementation: We set λ to 1 for PIViT to guarantee the smoothness of the deformation field. Algorithm runtimes are computed on an NVIDIA GeForce RTX 3090 GPU and an Intel(R) Xeon(R) Silver 4210R CPU. We implement the model using Keras with a Tensorflow [1] backend and the ADAM [18] optimizer with a learning rate of 1e -4 . The batch size is set as 1 and the networks are trained for 150,000 iterations.Results:  As shown in Table 1, VoxelMorph, VoxelMorph-diff, TransMorph, Swin-VoxelMorph and LKU-Net all get low Dice scores, indicating that these singlestream registration methods are difficult to solve large deformation. However, the iterative registration method RCN and the pyramid registration method NICE-Net obtain relatively good Dice scores, indicating that both iterative and pyramid registration methods can be useful to deal with large deformation. However, the Dice score of the proposed PIViT combining their advantages surpasses that of VoxelMorph by 14.8%, and the improvements compared to RCN and NiceNet also reach 2.5% and 2.0% on LPBA, respectively. On 3 liver datasets, compared with VoxelMorph, PIViT achieves 17.9%, 19.6% and 15.7% improvements, while compared with NICE-Net, PIViT achieves 3.8%, 4.1% and 5.1% improvements, respectively. The experiments indicate that the proposed PIViT implements large deformation fine registration better than other methods.In addition to the superior Dice score, another advantage of the proposed PIViT is its fast and lightweight registration. Table 1 shows that the parameters, training and registration time of PIViT are close to that of VoxelMorph, far less than those of RCN and NICE-Net. These properties of PIViT make it easier to train and more suitable for time-sensitive tasks. Compared to other Transformerbased methods, the proposed PIViT has orders of magnitude optimization in parameters, which is because we only use the Transformer block at low scales, and LCD is tuned and optimized for 3D image registration tasks. What's more, although there is no additional constraint on the diffeomorphism of the deformation field, the deformation field obtained by PIViT has better diffeomorphism properties than those obtained by other methods except VoxelMorph-diff.The visualization result of the experiment on the LPBA dataset is shown in Fig. 4. Obviously, most of the methods produce severe misregistration in the yellow regions of Fig. 4, due to the existence of large deformation. Compared with the registration results of RCN and NICE-Net, the proposed method achieves better alignment on the fine structure, which can be seen in the areas indicated by the red arrow in Fig. 4. It can be seen, since PIViT focuses on lightweight and fast registration of large deformations, its effectiveness on fine registration tasks is still somewhat weak. "
PIViT: Large Deformation Image Registration with Pyramid-Iterative Vision Transformer,,Number of Iterations and Decoder Type:,"In this section, we explore how the number of iterations and decoder type of block in the long-range correlation decoder affect the registration performance. We select three different blocks, i.e. LCD, CNN and GRU [4], to perform iterative decoding and predict low-scale deformation fields. In order to verify the effectiveness of iterative registration and how the number of iterations affects the registration effect, we performed 1 to 5 iterations for each decoder. The Dice scores corresponding to different decoders and number of iterations are shown in Table 2, t represents the time of iteration. Experiments are performed on LPBA and SLIVER datasets. Obviously, among the three decoders, LCD gets the best registration results in a limited number of iterations. When the number of iterations is 1, the proposed structure degenerates into pyramid registration, and when the number of iterations is greater than or equal to 2, the pyramid-iteration structure is used. Obviously, the registration accuracy is greatly improved compared with the pyramid structure when the number of iterations is 2. On this task, when the number of iterations reaches 3, the registration accuracy tends to be stable, which indicates that the large deformation has been basically captured. Compared with the GRU block commonly used in optical flow tasks, LCD requires fewer iterations to converge, which verifies that LCD can better capture long-distance correlation and learn accurate flow."
PIViT: Large Deformation Image Registration with Pyramid-Iterative Vision Transformer,4,Conclusion,"In this paper, we propose an unsupervised pyramid-iterative vision Transformer (PIViT) for large deformation image registration. PIViT is an iterative and pyramid composite framework to achieve fine registration of large deformable images by iterative registration of low-scale feature maps and pyramid feature supplementation on high-scale feature maps. Furthermore, in the iterative decoding stage, a Swin Transformer-based long-range correlation decoder is introduced to capture the long-distance dependencies between feature maps, which further improves the ability to handle large deformation. Experiments on brain MRI scans and liver CT scans demonstrate that our method can accurately register 3D large deformation medical images. Furthermore, our method has significant advantages in terms of parameters and time, which can make it more suitable for time-sensitive tasks."
PIViT: Large Deformation Image Registration with Pyramid-Iterative Vision Transformer,,Fig. 1 .,
PIViT: Large Deformation Image Registration with Pyramid-Iterative Vision Transformer,,Fig. 2 .,
PIViT: Large Deformation Image Registration with Pyramid-Iterative Vision Transformer,,Fig. 3 .,
PIViT: Large Deformation Image Registration with Pyramid-Iterative Vision Transformer,,Fig. 4 .,
PIViT: Large Deformation Image Registration with Pyramid-Iterative Vision Transformer,,Table 1 .,"ods on 4 medical datasets. The Dice score, number of voxels with non-positive Jacobian determinants (|J s | ≤0 ), GPU registration time (GRT), CPU registration time (CRT), network parameters and training time per iteration (TPI) of each method are presented."
PIViT: Large Deformation Image Registration with Pyramid-Iterative Vision Transformer,,Table 2 .,LPBA ↑ SLIVER ↑ Params ↓ TPI ↓ LPBA ↑ SLIVER ↑ Params ↓ TPI ↓ LPBA ↑ SLIVER ↑ Params ↓ TPI ↓
PIViT: Large Deformation Image Registration with Pyramid-Iterative Vision Transformer,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5_57.
InverseSR: 3D Brain MRI Super-Resolution Using a Latent Diffusion Model,1,Introduction,"End-to-end convolutional neural networks (CNNs) have shown remarkable performance compared to classical algorithms [14] on MRI SR. Deep CNNs have been widely applied in a variety of MRI SR situations; for instance, slice imputation on the brain, liver and prostate MRI [29] and brain MRI SR reconstruction on scaling factors ×2, ×3, ×4 [32]. Several techniques based on deep CNNs have been proposed to improve performance, such as densely connected networks [6], adversarial networks [5], and attention network [32]. However, their supervised training requires paired images, which necessitates re-training every time there is a shift in the input distribution [4,16]. As a result, such methods are unsuitable for MRI SR, as it is challenging to obtain paired training data that cover the variability in acquisition protocols and resolution of clinical brain MRI scans across institutions [14].Building image priors through generative models has recently become a popular approach in the field of image SR, for both computer vision [1,2,7,17,19] as well as medical imaging [18,25], as they do not require re-training in the presence of several types of input distribution shifts. While these methods have shown promise in MRI SR, they have so far been limited to 2D slices [18,25], rendering them unsuitable for 3D brain MRIs slice imputation.In this study, we propose solving the MRI SR problem by building powerful, 3D-native image priors through a recently proposed HR image generative model, the latent diffusion model (LDM) [21,22]. We solve the inverse problem by finding the optimal latent code z in the latent space of the pre-trained generative model, which could restore a given LR MRI I, using a known corruption function f . In this study, we focus on slice imputation, yet our method could be applied to other medical image SR problems by implementing different corruption functions f . We proposed two novel strategies for MRI SR: Inverse(LDM), which additionally inverts the input image through the deterministic DDIM model, and InverseSR(Decoder) which inverts the input image through the corruption function f and through the decoder D of the LDM model. We found that for large sparsity, InverseSR(LDM) had a better performance, while for low sparsity, InverseSR(Decoder) performed best. While the LDM model was trained on UK BioBank, we demonstrate our methods on an external dataset (IXI) which was inaccessible to the pre-trained generative model. Both quantitative and qualitative results show that our method achieves significantly better performance compared to two other baseline models. Furthermore, our method can also be applied to tumour/lesion filling by creating tumour/lesion shape masks."
InverseSR: 3D Brain MRI Super-Resolution Using a Latent Diffusion Model,1.1,Related Work,"MRI Super-Resolution. End-to-end deep training [27,29,32] has been proposed recently for MRI SR, which has achieved superior results compared to classical methods. However, these methods require paired data to train, which is hard to acquire because of the large variability present in clinical MRIs [14,23]. To circumvent this limitation, several unsupervised methods have been proposed without requiring access to HR scans [3,8,14]. Dalca et al. [8] proposed a gaussian mixture model for sparse image patches. Brudfors et al. [3] presented an algorithm which could take advantage of multimodal MRI. Iglesias et al. [14] introduced a method to train a CNN for MRI SR on any given combination of contrasts, resolutions and orientations.Solving Inverse Problems Using Generative Models. A common way to solve the inverse problem using an LDM is to use the encoder E to first encode the given image x into the latent space z 0 = E(x) [10,12,20], followed by DDIM (Denoising Diffusion Implicit Models) Inversion [9,24] to encode z 0 into the noise latent code z T [20]. However, this approach does not work for low-resolution images, because the encoder E has only been trained on high-resolution images.Our work is also similar to the optimization-based generative adversarial network (GAN) inversion approach [30], trying to find the optimal latent representation z * in the latent space of GAN, which could be mapped to represent the given image x ≈ G(z * ). More recent works [7,10,13,17,25] have used diffusion models for inverse problems due to their superior performance. However, all these methods require the diffusion model to operate directly in the image space, which for large image resolutions can become GPU-memory intensive."
InverseSR: 3D Brain MRI Super-Resolution Using a Latent Diffusion Model,2,Methods,"3D Brain Latent Diffusion Models. We leverage a state-of-the-art LDM [21] to create high-quality priors for 3D brain MRIs. There are two components in an LDM: an autoencoder and a diffusion model [22]. An encoder E maps each highresolution T1w brain MRI x ∼ p data (x) into a latent vector z 0 = E(x) of size 20 × 28 × 20. The decoder D is trained to map the latent vectors z 0 back into the MRI image domain x. The autoencoder was trained on 31,740 T1w MRIs from the UK Biobank [26] using a combination of an L1 loss, a perceptual loss [31], a patch-based adversarial loss [11] and a KL regularization term in the latent space. The autoencoder was trained on pre-processed MRIs using UniRes [3] into a common MNI space with a voxel size of 1 mm 3 and was then kept unchanged during the LDM training. The latent representations of the T1w brain MRIs were then used to train the LDM. A conditional U-Net θ was then trained to predict the artificial noise by the following objective: DDIM [24] has been used in brain LDM to replace the denoising diffusion probabilistic models (DDPM) during inference to reduce the number of reverse steps with minimal performance loss [21,24]. This network ε θ is conditioned on four conditional variables C: age, gender, ventricular volume and brain volume, which are all introduced by cross-attention layers [22]. Gender is a binary variable, while the rest of the covariates are scaled to [0, 1]. Finally, the pre-trained decoder maps the latent vector into an HR MRI x = D(z 0 ). The architecture of the brain LDM can be found in Fig. 1."
InverseSR: 3D Brain MRI Super-Resolution Using a Latent Diffusion Model,,Deterministic DDIM Sampling.,"In order to obtain a latent representation z T capable of reconstructing a given noisy sample into a high-resolution image, we employ deterministic DDIM sampling [24]:where α 1:T ∈ (0, 1] T is a time-dependent decreasing sequence, zt-represents the ""predicted x 0 "", and √ 1α t-1 • θ (z t , C, t) can be understood as the ""direction pointing to x t "" [24].Corruption Function f . We assume a corruption function f known a-priori that is applied on the HR image x obtained from the generative model, and compute the loss function based on the corrupted image f • x and the given LR input image I. In clinical practice, a prevalent method for acquiring MR images is prioritizing high in-plane resolution while sacrificing through-plane resolution to expedite the acquisition process and reduce motion artifacts [33]. To account for this procedure, we introduce a corruption function that generates masks for non-acquired slices, enabling our method to in-paint the missing slices. For instance, on 1 × 1 × 4 mm 3 undersampled volumes, we create masks for three slices every four slices on the generated HR 1 × 1 × 1 mm 3 volumes. "
InverseSR: 3D Brain MRI Super-Resolution Using a Latent Diffusion Model,,InverseSR(LDM):,"In the case of high sparsity MRI SR, we optimize the noise latent code z * T and its associated conditional variables C * to restore the HR image from the given LR input image I using the optimization method:where DDIM(z T , C, T ) represents T deterministic DDIM sampling steps on the latent z 0 in Eq. 2. We follow the brain LDM model to use the perceptual loss L perc and the L1 pixelwise loss. The loss function is computed on the corrupted image generated from the generative model and the given LR input. A detailed pseudocode description of this method can be found in Algorithm 1."
InverseSR: 3D Brain MRI Super-Resolution Using a Latent Diffusion Model,,InverseSR(Decoder):,"For low sparsity MRI SR, we directly find the optimal latent code z * T using the decoder D:"
InverseSR: 3D Brain MRI Super-Resolution Using a Latent Diffusion Model,3,Experimental Design,"Dataset for Validation: We use 100 HR T1 MRIs from the IXI dataset (http://brain-development.org/ixi-dataset/) to validate our method, after filtering out those scans where registration failed. We note that subjects in the IXI dataset are around 10 years younger on average than those in UK Biobank.The MRI scans from UK Biobank also had the faces masked out, while the scans from IXI did not. This caused the faces of our reconstructions to appear blurred."
InverseSR: 3D Brain MRI Super-Resolution Using a Latent Diffusion Model,,Implementation:,"Conditional variables are all initialized to 0.5. Voxels in all input volumes are normalized to [0,1]. When sampling the pre-trained brain LDM with the DDIM sampler, we run T = 46 timesteps due to computational limitations on our hardware. For InverseSR(LDM), z T is initialized with random gaussian noise. For InverseSR(Decoder), we compute the mean latent code z0 as z0 = S i=1 1 S DDIM(z i T , C, T ) by first sampling S = 10, 000 z i T samples from N (0, I), then passing them through the DDIM model. N = 600 gradient descent steps are used for InverseSR(LDM) to guarantee converging (Algorithm 1, line 5). 600 optimization steps are also utilized in InverseSR(Decoder). We use the Adam optimizer with α = 0.07, β 1 = 0.9 and β 2 = 0.999."
InverseSR: 3D Brain MRI Super-Resolution Using a Latent Diffusion Model,4,Results,"Figure 2 shows the qualitative results on the coronal slices of SR from 4 and 8 mm axial scans. The advantage of our approach is clear compared to baseline methods because it is capable of restoring HR MRIs with smoothness even when the slice thickness is large (i.e., 8 mm). This is the case because the pre-trained LDM we use is able to build a powerful prior over the HR T1w MRI domain. Therefore, the generated images of our method are HR MRIs with smoothness in 3 directions: axial, sagittal and coronal, no matter how sparse the input images I are. Qualitative results of applying our method on tumour and lesion filling are available in the supplementary material.Table 1 shows quantitative results on 100 HR T1 scans from the IXI dataset, which the brain LDM did not have access to during training. We investigated mean peak signal-to-noise ratio (PSNR), and structural similarity index measure (SSIM) [28] values and their corresponding standard deviation. We compare our method to cubic interpolation, as well as a similar unsupervised approach, UniRes [3]. We show our approach and the two compared methods on two different settings of slice imputation: 4 mm and 8 mm thick-sliced axial scans representing low sparsity and high sparsity LR MRIs, respectively. All the metrics are computed on a 3D volume around the brain of size 160 × 224 × 160. For SR at 4 mm, InverseSR(Decoder) achieves the highest mean SSIM and PSNR scores among all compared methods, which are slightly higher than the scores for InverseSR(LDM). For SR at 8 mm, Inverse(LDM) achieves the highest mean SSIM and PSNR and lowest standard error than the two baseline methods, which could be attributed to the stronger prior learned by the DDIM model."
InverseSR: 3D Brain MRI Super-Resolution Using a Latent Diffusion Model,5,Limitations,"One key limitation of our method is the need for large computational resources to perform the image reconstruction, in particular the long Markov chain of sampling steps required by the diffusion model to generate samples. An entire pass through the diffusion model (lines 6-8 in Algorithm 1) is required for every step in the gradient descent method. Another limitation of our method is that it is limited by the capacity and output heterogeneity of the LDM generator. "
InverseSR: 3D Brain MRI Super-Resolution Using a Latent Diffusion Model,6,Conclusions,"In this study, we have developed an unsupervised technique for MRI superresolution. We leverage a recent pre-trained Brain LDM [21] for building powerful image priors over T1w brain MRIs. Unlike end-to-end supervised approaches, which require retraining each time there is a distribution shift over the input, our method is capable of being adapted to different settings of MRI SR problems at test time. This feature is suitable for MRI SR since the acquisition protocols and resolution of clinical brain MRI exams vary across or even within institutions. We proposed two novel strategies for different settings of MRI SR: InverseSR(LDM) for low sparsity MRI and InverseSR(Decoder) for high sparsity MRI. We validated our method on 100 brain T1w MRIs from the IXI dataset through slice imputation using input scans of 4 and 8 mm slice thickness, and compared our method with cubic interpolation and UniRes [3].Experimental results have shown that our approach achieves superior performance compared to the unsupervised baselines, and could create smooth HR images with fine detail even on an external dataset (IXI). Experiments in this paper focus on slice imputation, but our method could be adapted to other MRI under-sampling problems by implementing different corruption functions f . For instance, for reconstructing k-space under-sampled MR images, a new corruption function could be designed by first converting the HR image into k-space, then masking a chosen set of k-space measurements, and then converting back to image space. Instead of estimating a single image, future work could also estimate a distribution of reconstructed images through either variational inference (like the BRGM model [18]) or through sampling methods such as Markov Chain Monte Carlo (MCMC) or Langevin dynamics [15]."
InverseSR: 3D Brain MRI Super-Resolution Using a Latent Diffusion Model,,Fig. 1 .,
InverseSR: 3D Brain MRI Super-Resolution Using a Latent Diffusion Model,,Algorithm 1 . 1 : 2 :,
InverseSR: 3D Brain MRI Super-Resolution Using a Latent Diffusion Model,,Fig. 2 .,
InverseSR: 3D Brain MRI Super-Resolution Using a Latent Diffusion Model,,Table 1 .,
CT Kernel Conversion Using Multi-domain Image-to-Image Translation with Generator-Guided Contrastive Learning,1,Introduction,"Computed tomography (CT) image is reconstructed from sinogram, which is tomographic raw data collected from detectors. According to kernels being used for CT image reconstruction, there is a trade-off between spatial resolution and noise, and it affects intensity and texture quantitative values [1]. When CT image is reconstructed with sharp kernel, spatial resolution and noise increase, and abnormality can be easily detected in bones or lung. In contrast, with soft kernel, spatial resolution and noise reduce, and abnormality can be easily detected in soft tissues or mediastinum. In other words, CT image is reconstructed depending on what anatomical structure is evaluated. Also, even if the same anatomical structure is analyzed, the kernel being used differs depending on whether it is qualitative or quantitative evaluation. For example, CT images reconstructed with soft kernel is required to evaluate quantitative results for lung instead of sharp kernel. Thus, CT images reconstructed with different kernels would be necessary for accurate diagnosis.However, once CT image is reconstructed with a specific kernel, sinogram is usually removed because of its large capacity and limited storage. Therefore, clinicians have difficulty to analyze qualitative or quantitative results without CT image reconstructed with different kernels, and this limitation reveals on retrospective or longitudinal studies that cannot control technical parameters, particularly [2]. Besides, there is another problem that patients should be scanned again and exposed to radiation.Recently, many studies have achieved improvement in kernel conversion [2][3][4][5] using image-to-image translation methods [6][7][8][9][10][11][12][13] based on deep learning, especially generative adversarial networks (GANs) [14]. Nevertheless, it remains challenging that translated image should maintain its anatomical structure of source image in medical domain [9]. It is important for quantitative evaluation as well as qualitative evaluation. To solve this problem, we focus on improving maintenance of structure when the source image is translated.Our contributions are as follows: (1) we propose multi-domain image-to-image translation with generator-guided contrastive learning (GGCL) for CT kernel conversion, which maintains the anatomical structure of the source image accurately; (2) Our proposed GGCL can be easily utilized into other multi-domain image-to-image translation with only changing the discriminator architecture and without adding any additional networks; (3) Experimental results showed that our method can translate CT images from sharp into soft kernels and from soft into sharp kernels compared to other image-to-image translation methods."
CT Kernel Conversion Using Multi-domain Image-to-Image Translation with Generator-Guided Contrastive Learning,2,Method,
CT Kernel Conversion Using Multi-domain Image-to-Image Translation with Generator-Guided Contrastive Learning,2.1,Related Work,"In deep learning methods for CT kernel conversion, there were proposed methods using convolutional neural networks [2,3], but they were trained in a supervised manner. Recently, Yang et al. [5] proposed a new method using the adaptive instance normalization (AdaIN) [15] in an unsupervised manner and it showed significant performance, however, this method still has limitations that the target image for the test phase and additional architecture for AdaIN are needed.Generator-guided discriminator regularization (GGDR) [16] is discriminator regularization method that intermediate feature map in the generator supervises semantic representations by matching with semantic label map in the discriminator for unconditional image generation. It has advantages that we don't need any ground-truth semantic segmentation masks and can improve fidelity as much as conditional GANs [17][18][19].Recently, it has been shown that dense contrastive learning can have a positive effect on learning dense semantic labels. In dense prediction tasks such as object detection and semantic segmentation [20,21], both global and local contrastive learning have been proposed to embed semantic information. Furthermore, it has been demonstrated that patch-wise contrastive learning performs well in style transfer for unsupervised image-to-image translation [12]. This motivated our experiments as it demonstrates that intermediate features can be learned through contrastive learning when learning dense semantic labels."
CT Kernel Conversion Using Multi-domain Image-to-Image Translation with Generator-Guided Contrastive Learning,2.2,Generator-Guided Contrastive Learning,"GGDR [16] uses cosine distance loss between the feature map and the semantic label map for unconditional image generation. However, unlike image generation, the generator has a structure with an encoder and a decoder in image-to-image translation [11], and this is quite important to maintain the structure of source image while translating the style of target image. Thus, it might be helpful for discriminator to inform more fine detail semantic representations by comparing similarity using patch-based contrastive learning [12] (see Fig. 1)."
CT Kernel Conversion Using Multi-domain Image-to-Image Translation with Generator-Guided Contrastive Learning,,Multi-Domain Image-To-Image Translation.,"We apply generator-guided contrastive learning (GGCL) to StarGAN [6] as base architecture which is one of the multi-domain image-to-image translation model to translate kernels into all directions at once and show stability of GGCL. Basically, StarGAN uses adversarial loss, domain classification loss and cycle consistency loss [13] as follows:where L D and L G are the discriminator and generator losses, respectively. They both have L adv , which is the adversarial loss. L r cls and L f cls are the domain classification losses for a real and fake image, respectively. L cyc , which is the cycle consistency loss, has an importance for the translated image to maintain the structure of source image."
CT Kernel Conversion Using Multi-domain Image-to-Image Translation with Generator-Guided Contrastive Learning,,Patch-Based Contrastive Learning.,"Our method is to add PatchNCE loss [12] between ""positive"" and ""negative"" patches from the feature map of the decoder in the generator and ""query"" patch from the semantic label map in the discriminator. The query patch is the same location with positive patch and different locations with N negative patches. So, the positive patch is learned to associate to the query patch more than the N negative patches. GGCL loss is the same as PatchNCE loss, which is the cross-entropy loss calculated for an (N + 1)-way classification, and it follows as: where v, v + and v - n are the vectors which are mapped from query, positive and n-th negative patches, respectively. τ = 0.07 is the same configuration as CUT [12]. Since we use the features from the generator and the discriminator themselves, this requires no additional auxiliary networks and no feature encoding process.Total Objective. GGCL follows the concept of GGDR, which the generator supervises the semantic representations to the discriminator, so it is a kind of the discriminator regularization. Discriminator conducts real/fake classification, domain classification and semantic label map segmentation, so it can be also a kind of the multi-task learning [22]. Our total objective functions for the discriminator and generator are written, respectively, as:where λ cls , λ cyc and λ ggcl are hyper-parameters that weight the importance of domain classification loss, cycle consistency loss and GGCL loss, respectively. We used λ cls = 1, λ cyc = 10 and λ ggcl = 2 in our experiments.3 Experiments and Results Implementation Details. We maintained the original resolution 512 × 512 of CT images and normalized their Hounsfield unit (HU) range from [-1024HU ~3071HU] to [-1 ~1] for pre-processing. For training, the generator and the discriminator were optimized by Adam [23] with β 1 = 0.5, β 2 = 0.999, learning rate 1e-4 and the batch size is 2. We used WGAN-GP [24] and set n critic = 5, where n critic is the number of discriminator updates per each generator update. The feature map and the semantic label map were extracted in 256 × 256 size and resized 128 × 128 using averaging pooling. The number of patches for contrastive learning is 64. All experiments were conducted using single NVIDIA GeForce RTX 3090 24GB GPU for 400,000 iterations. We used peak signal-to-noise ratio (PSNR) [25] and structural similarity index measure (SSIM) [26] for quantitative assessment.Architecture Improvements. Instead of using original StarGAN [6] architecture, we implemented architecture ablations to sample the best quality results, empirically. In generator, original StarGAN runs 4 × 4 transposed convolutional layers for upsampling. However, it causes degradation of visual quality of the translated image because of checkerboard artifact [27]. By using 3 × 3 convolutional layers and 2 × 2 pixelshuffle [28], we could prevent the artifact. In discriminator, we changed the discriminator to U-Net architecture [29] with skip connection, which consists of seven encoder layers for playing the role of patchGAN [8] and six decoder layers for extracting semantic label map, to utilize GGCL. For each decoder layer, we concatenated the feature from the encoder and the decoder layer with the same size, and ran 1 × 1 convolutional layer, then ran 2 × 2 pixelshuffle for upsampling. At the end of the decoder, it extracts semantic label map to compare with the feature map from the decoder layer of the generator. Lastly, we added spectral normalization [30] and leakyReLU activation function in all layers of the discriminator."
CT Kernel Conversion Using Multi-domain Image-to-Image Translation with Generator-Guided Contrastive Learning,3.2,Comparison with Other Image-to-Image Translation Methods,"We compared GGCL with two-domain image-to-image translation methods such as CycleGAN [13], CUT [12], UNIT [10] and multi-domain image-to-image translation methods such as AttGAN [7], StarGAN and StarGAN with GGDR [16] to show the effectiveness of GGCL. In this section, qualitative and quantitative results were evaluated for the translation into B30f, B50f and B70f kernels, respectively.Qualitative Results. We showed the qualitative results of image-to-image translation methods including GGDR and GGCL applied to StarGAN for kernel conversion from Siemens into Siemens (see Fig. 2). For visualization, window width and window level were set 1500 and -700, respectively. While UNIT could not maintain the global structure of the source image and translate the kernel style of the target image, the other methods showed plausible results. However, they could not maintain the fine details like airway wall and vessel in specific kernel conversion, e.g., B50f to B30f, B30f to B50f and B50f to B70f. It could be observed through difference map between the target image and the translated image (see Supplementary Fig. 1). GGCL showed stability of translation for kernel conversion with any directions and maintained the fine details including airway wall, vessel and even noise pattern as well as the global structure of the source image.Quantitative Results. We showed the quantitative results of image-to-image translation methods including GGDR and GGCL applied to StarGAN for kernel conversion from Siemens into the Siemens (see Table 2). In case of two-domain image-to-image translation methods, they showed high PSNR and SSIM performance in translation from B70f into B30f and from B30f into B70f, and UNIT showed the best performance in translation from B30f into B70f. However, they showed low performance in translation into the other kernels, especially soft into sharp, and it indicates that two-domain methods are unstable and cannot maintain the structure of the source image well. In case of multidomain image-to-image translation methods, their performance still seemed unstable, however, when applying GGDR to StarGAN, it showed quite stable and improved the performance in translation into sharp kernels. Furthermore, when applying GGCL, it outperformed GGDR in translation into many kernels, especially from B30f into B70f and from B50f into B70f. "
CT Kernel Conversion Using Multi-domain Image-to-Image Translation with Generator-Guided Contrastive Learning,3.3,Ablation Study,"We implemented ablation studies about the number of patches, size of pooling and loss weight for GGCL to find out the best performance. We evaluated our method while preserving the network architecture. Ablation studies were also evaluated by PSNR and SSIM. All ablation studies were to change one factor and the rest of them were fixed with their best configurations. The results about the number of patches showed improvement when the number of patches was 64 (see Table 3). The size of pooling also affected the performance improvement, and 2 was appropriate (see Supplementary Table 1). Lastly, the results of the loss weight for GGCL showed that 2 was the best performance (see Supplementary Table 2)."
CT Kernel Conversion Using Multi-domain Image-to-Image Translation with Generator-Guided Contrastive Learning,4,Discussion and Conclusion,"In this paper, we proposed CT kernel conversion method using multi-domain imageto-image translation with generator-guided contrastive learning (GGCL). In medical domain image-to-image translation, it is important to maintain anatomical structure of the source image while translating style of the target image. However, GAN based generation has limitation that the training process may be unstable, and the results may be inaccurate so that some fake details may be generated. Especially in unsupervised manner, the anatomical structure of the translated image relies on cycle consistency mainly. If trained unstably, as the translated image to the target domain would be inaccurate, the reversed translated image to the original domain would be inaccurate as well. Then, the cycle consistency would fail to lead the images to maintain the anatomical structure. CycleGAN [13], CUT [12] and UNIT [10] showed this limitation (see Fig. 2 and Table 2), but GGCL solved this problem without any additional networks. The benefit of GGCL was revealed at the translation from soft into sharp kernels. It is a more difficult task than the translation from sharp into soft kernels because spatial resolution should be increased and noise patterns should be clear, so this benefit can be meaningful. Nevertheless, the improvements from GGCL were quite slight compared to GGDR [16] (see Table 2) and inconsistent according to the number of patches (see Table 3). Besides, we did not show results about the different kernels from the external manufacturer. In future work, we will collect different types of kernels from the external manufacturer, and conduct experiments to show better improvements and stability of GGCL. "
CT Kernel Conversion Using Multi-domain Image-to-Image Translation with Generator-Guided Contrastive Learning,,Fig. 1 .,
CT Kernel Conversion Using Multi-domain Image-to-Image Translation with Generator-Guided Contrastive Learning,,Fig. 2 .,
CT Kernel Conversion Using Multi-domain Image-to-Image Translation with Generator-Guided Contrastive Learning,,3.1 Datasets and Implementation Datasets.,
CT Kernel Conversion Using Multi-domain Image-to-Image Translation with Generator-Guided Contrastive Learning,,Table 1 .,
CT Kernel Conversion Using Multi-domain Image-to-Image Translation with Generator-Guided Contrastive Learning,,Table 2 .,
CT Kernel Conversion Using Multi-domain Image-to-Image Translation with Generator-Guided Contrastive Learning,,Table 3 .,
MRIS: A Multi-modal Retrieval Approach for Image Synthesis on Diverse Modalities,1,Introduction,"Recent successes of machine learning algorithms in computer vision and natural language processing suggest that training on large datasets is beneficial for model performance [2,5,18,21]. While several efforts to collect very large medical image datasets are underway [12,19], collecting large homogeneous medical image datasets is hampered by: a) cost, b) advancement of technology throughout long study periods, and c) general heterogeneity of acquired images across studies, making it difficult to utilize all data. Developing methods accounting for different imaging types would help make the best use of available data.Although image harmonization and synthesis [3,14,15,22] methods have been explored to bridge the gap between different types of imaging, these methods are often applied to images of the same geometry. On the contrary, many studies acquire significantly more diverse images; e.g., the OAI image dataset1  [9] contains both 3D MR images of different sequences and 2D radiographs. Similarly, the UK Biobank [19] provides different 3D MR image acquisitions and 2D DXA images. Ideally, a machine learning system can make use of all data that is available. As a related first step in this direction, we explore the feasibility of predicting information gleaned from 3D geometry using 2D projection images. Being able to do so would allow a) pooling datasets that drastically differ in image types or b) relating information from a cheaper 2D screening to more readily interpretable 3D quantities that are difficult for a human observer.We propose an image synthesis method for diverse modalities based on multimodal metric learning and k-NN regression. To learn the metric, we use image retrieval as the target task, which aims at embedding images such that matching pairs of different modalities are close in the embedding space. We use a triplet loss [24] to contrastively optimize the gap between positive and negative pairs based on the cosine distance over the learned deep features. In contrast to the typical learning process, we carefully design the training scheme to avoid interference when training with longitudinal image data. Given the learned embedding, we can synthesize images between diverse image types by k-NN regression through a weighted average based on their distances measured in the embedding space. Given a large database, this strategy allows for a quick and simple estimation of one image type from another.We use knee osteoarthritis as the driving medical problem and evaluate our proposed approach using the OAI image data. Specifically, we predict cartilage thickness maps obtained from 3D MR images using 2D radiographs. This is a highly challenging task and therefore is a good test case for our approach for the following reasons: 1) cartilage is not explicitly visible on radiographs. Instead, the assessment is commonly based on joint space width (JSW), where decreases in JSW suggest decreases in cartilage thickness [1]; 2) the difficulty in predicting information obtained from a 3D image using only the 2D projection data; 3) the large appearance difference between MR images and thickness maps; 4) the need to capture fine-grained details within a small region of the input radiograph. While direct regression via deep neural networks is possible, such approaches lack interpretability and we show that they can be less accurate for diverse images.The main contributions of our work are as follows.1. We propose an image synthesis method for diverse modalities based on multimodal metric learning using image retrieval and k-NN regression. We carefully construct the learning scheme to account for longitudinal data.2. We extensively test our approach for osteoarthritis, where we synthesize cartilage thickness maps derived from 3D MR using 2D radiographs. 3. Experimental results show the superiority of our approach over commonly used image synthesis methods, and the synthesized images retain sufficient information for downstream tasks of KL grading and progression prediction. Left top: encoding the region of interest from radiographs, extracted using the method from [26]. Left bottom: encoding thickness maps, extracted from MR images using the method from [11]. Features are compared using cosine similarity. Right: applying triplet loss on cosine similarity, where nonpaired data is moved away from paired data."
MRIS: A Multi-modal Retrieval Approach for Image Synthesis on Diverse Modalities,2,Method,"In this work, we use multi-modal metric learning followed by k-NN regression to synthesize images of diverse modalities. Our method requires 1) a database containing matched image pairs; 2) target images aligned to an atlas space."
MRIS: A Multi-modal Retrieval Approach for Image Synthesis on Diverse Modalities,2.1,Multi-modal Longitudinally-Aware Metric Learning,"Let {(x i a , y i a )} be a database of multiple paired images with each pair containing two modalities x and y of the a-th subject and i-th timepoint if longitudinal data is available. We aim to learn a metric that allows us to reliably identify related image pairs, which in turn relate structures of different modalities. Specifically, we train our deep neural network via a triplet loss so that matching image pairs are encouraged to obtain embedding vectors closer to each other than mismatched pairs. Figure 1 illustrates the proposed multi-modal metric learning approach, which uses two convolutional neural networks (CNNs), each for extracting the features of one modality. The two networks may share the same architecture, but unlike Siamese networks [4], our CNNs have independent sets of weights. This is because the two modalities differ strongly in appearance.Denoting the two CNNs as f (•; θ) and g(•; φ), where θ and φ are the CNN parameters, we measure the feature distance between two images x and y using cosine similaritywhere the output of f and g are vectors of the same dimension2 . Given a minibatch of N paired images, our goal is to learn a metric such that f (x i a ) and g(y i a ) are close (that is, for the truly matching image pair), while f (x i a ) and g(y j b ) are further apart, where a = b and i, j are arbitrary timepoints of subjects a, b, respectively. We explicitly avoid comparing across timepoints of the same subject to avoid biasing longitudinal trends. This is because different patients have different disease progression speeds. For those with little to no progression, images may look very similar across timepoints and should therefore result in similar embeddings. It would be undesirable to view them as negative pairs. Therefore, our multi-modal longitudinally-aware triplet loss becomes(2) where m is the margin for controlling the minimum distance between positive and negative pairs. We sum over all subjects at all timepoints for each batch.To avoid explicitly tracking the subjects in a batch, we can simplify the above equation by randomly picking one timepoint per subject during each training epoch. This then simplifies our multi-modal longitudinally aware triplet loss to a standard triplet loss of the form"
MRIS: A Multi-modal Retrieval Approach for Image Synthesis on Diverse Modalities,2.2,Image Synthesis,"After learning the embedding space, it can be used to find the most relevant images with a new input, as shown in Fig. 2. Specifically, the features of a query image x are first extracted by the CNN model f θ we described previously. Given a database of images of the target modality S I = {y i a } and their respective embeddings S F = {g(y i a )}, we can then select the top k images with the smallest cosine distance, which will be the most similar images given this embedding. Denoting these k most similar images as K = {ỹ k } we can synthesize an image, ŷ based on a query image, x as a weighted average of the formwhere the weights are normalized weights based on the cosine similarities. This requires us to work in an atlas space for the modality y, where all images in the database S I are spatially aligned. However, images of the modality x do not need to be spatially aligned, as long as sensible embeddings can be captured by f θ . As we will see, this is particularly convenient for our experimental setup, where the modality x is a 2D radiograph and the modality y is a cartilage thickness map derived from a 3D MR image, which can easily be brought into a common atlas space. As our synthesized image, ŷ, is a weighted average of multiple spatially aligned images, it will be smoother than a typical image of the target modality. However, we show in Sect. 3 that the synthesized images still retain the general disease patterns and retain predictive power. Note also that our goal is not image retrieval or image reidentification, where one wants to find a known image in a database. Instead, we want to synthesize an image for a patient who is not included in our image database. Hence, we expect that no perfectly matched image exists in the database and therefore set k > 1. Based on theoretical analyses of k-NN regression [6], we expect the regression results to improve for larger image databases."
MRIS: A Multi-modal Retrieval Approach for Image Synthesis on Diverse Modalities,3,Experimental Results,"Fig. 2. Image synthesis by k-NN regression from the database. Given an unseen image x, we extract its features f θ (x), find the k nearest neighbors in the database {y} based on these features, and use them for a weighted k-NN regression.patients between the ages of 45 to 79 years at the time of recruitment. Each patient is longitudinally followed for up to 96 months.Images. The OAI acquired images of multiple modalities, including T2 and DESS MR images, as well as radiographs. We use the paired DESS MR images and radiographs in our experiments. After excluding all timepoints when patients do not have complete MR/radiograph pairs, we split the dataset into three sets by patient (i.e., data from the same patient are in the same sets): Set 1) to train the image retrieval model (2, 000 patients; 13, 616 pairs). This set also acts as a database during image synthesis; Set 2) to train the downstream task (1, 750 patients; 16, 802 pairs); Set 3) to test performance (897 patients; 8, 418 pairs). Preprocessing. As can be seen from the purple dashed box in Fig. 1, we extract cartilage thickness maps from the DESS MR images using a deep segmentation network [27], register them to a common 3D atlas space [25], and then represent them in a common flattened 2D atlas space [11]. These 2D cartilage thickness maps are our target modality, which we want to predict from the 2D radiographs. Unlike MR images for which a separate scan is obtained for the left and right knees, OAI radiographs include both knees and large areas of the femur and tibia. To separate them, we apply the method proposed in [26], which automatically detects keypoints between the knee joint. As shown in the blue dashed box in Fig. 1, the region of interest for each side of the knee is being extracted using a region of 140 mm * 140 mm around the keypoints. We normalize all input radiographs by linearly scaling the intensities so that the smallest 99% values are mapped to [0, 0.99]. We horizontally flip all right knees to the left as done in [11], randomly rotate images up to 15 degrees, add Gaussian noise, and adjust contrast. Unlike the radiographs, we normalize the cartilage thickness map by dividing all values by 3, which is approximately the 95-th percentile of cartilage thickness. All images are resized to 256 * 256. "
MRIS: A Multi-modal Retrieval Approach for Image Synthesis on Diverse Modalities,3.2,Network Training,"During multi-modal metric learning, our two branches use the ResNet-18 [10] model with initial parameters obtained by ImageNet pre-training [8]. We finetune the networks using AdamW [20] with initial learning rate 10 -4 for radiographs and 10 -5 for the thickness maps. The output embedding dimensions of both networks are 512. We train the networks with a batch size of 64 for a total of 450 epochs with a learning rate decay of 80% for every 150 epochs. We set the margin m = 0.1 in all our experiments. For both downstream tasks, we fine-tune our model on a ResNet-18 pretrained network with the number of classes set to 4 for KLG prediction and 2 for progression prediction. Both tasks are trained with AdamW for 30 epochs, batch size 64, and learning rate decay by 80% for every 10 epochs. The initial learning rate is set to 10 -5 for KLG prediction and 10 -4 for progression prediction."
MRIS: A Multi-modal Retrieval Approach for Image Synthesis on Diverse Modalities,3.3,Results,"This section shows our results for image retrieval, synthesis, and downstream tasks based on the questions posed above. All images synthesized from MRIS are based on the weighted average of the retrieved top k = 20 thickness maps.Image Retrieval. To show the importance of the learned embedding space, we perform image retrieval on the test set, where our goal is to correctly find the corresponding matching pair. Since our training process does not compare images of the same patient at different timepoints, we test using only the baseline images for each patient (1, 794 pairs). During training, we created two thickness map variants: 1) combining the femoral and tibial cartilage thickness maps (Combined); 2) separating the femoral and tibial thickness maps (Femoral/Tibial), which requires training two networks. Table 1 shows the image retrieval recall, where R@k represents the percentage of radiographs for which the correct thickness map is retrieved within the k-nearest neighbors in the embedding space. Combined achieves better results than retrieving femoral and tibial cartilage separately. This may be because more discriminative features can be extracted when both cartilages are provided, which simplifies the retrieval task. In addition, tibial cartilage appears to be easier to retrieve than femoral cartilage. Image Synthesis. To directly measure the performance of our synthesized images on the testing dataset, we show the median ± MAD (median absolute deviation) absolute error compared to the thickness map extracted by MR in Table 2. We created two variants by combining or separating the femoral and tibial cartilage, corresponding to MRIS-C(ombined) and MRIS-S(eparate). Unlike the image retrieval recall results, MRIS-S performs better than MRIS-C (last column of Table 2). This is likely because it should be beneficial to mix and match separate predictions for synthesizing femoral and tibial cartilage. Moreover, MRIS-S outperforms all baseline image synthesis methods [7,13,23].Osteoarthritis is commonly assessed via Kellgren-Lawrence grade [16] on radiographs by assessing joint space width and the presence of osteophytes. KLG=0 represents a healthy knee, while KLG=4 represents severe osteoarthritis. KLG=0 and 1 are often combined because knee OA is considered definitive only when KLG≥ 2 [17]. To assess prediction errors by OA severity, we stratify our results in Table 2 by KLG. Both variants of our approach perform well, outperforming the simpler pix2pix and U-Net baselines for all KLG. The TransUNet approach shows competitive performance, but overall our MRIS-S achieves better results regardless of our much smaller model size. Figure 3 shows examples of images synthesized for the different methods for different severity of OA. Downstream Tasks. The ultimate question is whether the synthesized images can still retain information for downstream tasks. Therefore, we test the ability to predict KLG and OA progression, where we define OA progression as whether or not the KLG will increase within the next 72 months. Table 3 shows that our synthesized thickness maps perform on par with the MR-extracted thickness maps for progression prediction and we even outperform on predicting KLG. MRIS overall performs better than U-Net [23], pix2pix [13] and TransUNet [7]."
MRIS: A Multi-modal Retrieval Approach for Image Synthesis on Diverse Modalities,4,Conclusion,"In this work, we proposed an image synthesis method using metric learning via multi-modal image retrieval and k-NN regression. We extensively validated our approach using the large OAI dataset and compared it with direct synthesis approaches. We showed that our method, while conceptually simple, can effectively synthesize alignable images of diverse modalities. More importantly, our results on the downstream tasks showed that our approach retains diseaserelevant information and outperforms approaches based on direct image regression. Potential shortcomings of our approach are that the synthesized images tend to be smoothed due to the weight averaging and that spatially aligned images are required for the modality to be synthesized."
MRIS: A Multi-modal Retrieval Approach for Image Synthesis on Diverse Modalities,,Fig. 1 .,
MRIS: A Multi-modal Retrieval Approach for Image Synthesis on Diverse Modalities,,Fig. 3 .,
MRIS: A Multi-modal Retrieval Approach for Image Synthesis on Diverse Modalities,,Table 1 .,
MRIS: A Multi-modal Retrieval Approach for Image Synthesis on Diverse Modalities,,21 75.53 84.73 90.64,
MRIS: A Multi-modal Retrieval Approach for Image Synthesis on Diverse Modalities,,Table 2 .,
MRIS: A Multi-modal Retrieval Approach for Image Synthesis on Diverse Modalities,,Table 3 .,
MRIS: A Multi-modal Retrieval Approach for Image Synthesis on Diverse Modalities,3.,Does our prediction retain disease-relevant information for downstream tasks?,We test the performance of our predicted cartilage thickness maps in predicting KLG and osteoarthritis progressors; 4. How does our approach compare to existing image synthesis models? We show that our approach based on simple k-NN regression compares favorably to direct image synthesis approaches.
MRIS: A Multi-modal Retrieval Approach for Image Synthesis on Diverse Modalities,3.1,Dataset,"We perform a large-scale validation of our method using the Osteoarthritis Initiative (OAI) dataset on almost 40,000 image pairs. This dataset includes 4, 796"
Noise Conditioned Weight Modulation for Robust and Generalizable Low Dose CT Denoising,1,Introduction,"Convolutional neural networks (CNN) have emerged as one of the most popular methods for noise removal and restoration of LDCT images [1,2,5,6,14]. While CNNs can produce better image quality than manually designed functions, there are still some challenges that hinder their widespread adoption in clinical settings. Convolutional denoisers are known to perform best when the training and testing images have similar or identical noise variance [15,16]. On the other hand, different anatomical sites of the human body have different tissue densities and compositions, which affects the amount of radiation that is absorbed and scattered during CT scanning; as a result, noise variance in LDCT images also varies significantly among different sites of the human body [13].Furthermore, the noise variance is also influenced by the differences in patient size and shape, imaging protocol, etc. [11]. Because of this, CNN-based denoising networks fail to perform optimally in LDCT denoising. In this study, we have introduced a novel dynamic convolution layer to combat the issue of noise level variability in LDCT images. Dynamic convolution layer is a type of convolutional layer in which the convolutional kernel is generated dynamically at each layer based on the input data [3,4,8]. Unlike the conventional dynamic convolution layer, here we have proposed to use a modulating signal to scale the value of the weight vector(learned via conventional backpropagation) of a convolutional layer. The modulating signal is generated dynamically from the input image using an encoder network. The proposed method is very simple, and learning the network weight is a straightforward one-step process, making it manageable to deploy and train. We evaluated the proposed method on the recently released largescale LDCT database of TCIA Low Dose CT Image and Projection Data [10] and the 2016 NIH-AAPM-Mayo Clinic low dose CT grand challenge database [9]. These databases contain low-dose CT data from three anatomical sites, i.e., head, chest, and abdomen. Extensive experiments on these databases validate the proposed method improves the baseline network's performance significantly. Furthermore, we have shown the generalization ability to the out-of-distribution data, and the robustness of the baseline network is also increased significantly via using the proposed weight-modulated dynamic convolutional layer."
Noise Conditioned Weight Modulation for Robust and Generalizable Low Dose CT Denoising,2,Method,"Motivation: Each convolutional layer in a neural network performs the sum of the product operation between the weight vector and input features. However, as tissue density changes in LDCT images, the noise intensity also changes, leading to a difference in the magnitude of intermediate feature values. If the variation in input noise intensity is significant, the magnitude of the output feature of the convolutional layer can also change substantially. This large variation in input feature values can make the CNN layer's response unstable, negatively impacting the denoising performance. To address this issue, we propose to modulate the weight vector values of the CNN layer based on the noise level of the input image. This approach ensures that the CNN layer's response remains consistent, even when the input noise variance changes drastically.Weight Modulation: Figure 1 depicts our weight modulation technique, which involves the use of an additional anatomy encoder network, E a , along with the backbone denoising network, CNN D . The output of the anatomy encoder, denoted as e x , is a D-dimensional embedding, i.e., e x = E a (∇ 2 (x)). Here, x is the input noisy image, and ∇ 2 (.) is a second-order Laplacian filter. This embedding e x serves as a modulating signal for weight modulation in the main denoising network (CNN D ). Specifically, the lth weight-modulated convolutional layer, F l , of the backbone network, CNN D , takes the embedding e x as input. Then the embedding e x is passed to a 2 Layer MLP, denoted as φ l , which learns a nonlinear mapping between the layer-specific code, denoted as s l ∈ R N l , and the embedding e x , i.e., s l = φ l (e x ). Here, N l represents the number of feature maps in the layer F l . The embedding e x can be considered as the high dimensional code containing the semantics information and noise characteristic of the input image. The non-linear mapping φ l maps the embedding e x to a layer-specific code s l , so that different layers can be modulated differently depending on the depth and characteristic of the features. Let w l ∈ R N l ×N l-1 ×k×k be the weight vector of F l learned via standard back-propagation learning. Here (k × k) is the size of the kernel, N l-1 is the number of feature map in the previous layer. Then the w l is modulated using s l as following,Here, ŵl is the modulated weight value, and represents component wise multiplication. Next, the scaled weight vector is normalized by its L2 norm across channels as follows:Normalizing the modulated weights takes care of any possible instability arise due to high or too low weight value and also ensures that the modulated weight has consistent scaling across channels, which is important for preserving the spatial coherence of the denoised image [7]. The normalized weight vectors, wl are then used for convolution, i.e., f l = F l wl * f l-1 . Here, f l , and f l-1 are the output feature map of lth, l -1th layer, and * is the convolution operation."
Noise Conditioned Weight Modulation for Robust and Generalizable Low Dose CT Denoising,,Relationship with Recent Methods:,"The proposed weight modulation technique leveraged the recent concept of style-based image synthesis proposed in StyleGAN2 [7]. However, StyleGAN2 controlled the structure and style of the generated image by modulating weight vectors using random noise and latent code. Whereas, we have used weight modulation for dynamic filter generation conditioned on input noisy image to generate a consistent output image."
Noise Conditioned Weight Modulation for Robust and Generalizable Low Dose CT Denoising,,Implementation Details:,"The proposed dynamic convolutional layer is very generic and can be integrated into various backbone networks. For our denoising task, we opted for the encoder-decoder-based UNet [12] architecture and replaced some of its generic convolutional layers with our weight-modulated dynamic convolution layer. To construct the anatomy encoder network, we employed ten convolutional blocks and downscaled the input feature map's spatial resolution by a factor of nine through two max-pooling operations inside the network. We fed the output of the last convolutional layer into a global average pooling layer to generate a 512-dimensional feature vector. This vector was then passed through a 2-layer MLP to produce the final embedding, e x ∈ R 512 ."
Noise Conditioned Weight Modulation for Robust and Generalizable Low Dose CT Denoising,3,Experimental Setting,"We used two publicly available data sets, namely, 1. TCIA Low Dose CT Image and Projection Data, 2. 2016 NIH-AAPM-Mayo Clinic low dose CT grand challenge database to validate the proposed method. The first dataset contains LDCT data of different patients of three anatomical sites, i.e., head, chest, and abdomen, and the second dataset contains LDCT images of the abdomen with two different slice thicknesses (3 mm, 1 mm). We choose 80% data from each anatomical site for training and the remaining 20% for testing. We used the Adam optimizer with a batch size of 16. The learning rate was initially set to 1e -4 and was assigned to decrease by a factor of 2 after every 6000 iterations. "
Noise Conditioned Weight Modulation for Robust and Generalizable Low Dose CT Denoising,4,Result and Discussion,"Comparison with Baseline: This section discusses the efficacy of the proposed weight modulation technique, comparing it with a baseline UNet network (M1) and the proposed weight-modulated convolutional network (M2). The networks were trained using LDCT images from a single anatomical region and tested on images from the same region. Table 1 provides an objective comparison between the two methods in terms of PSNR, SSIM, and RMSE for different anatomical regions. The results show that the proposed dynamic weight modulation technique significantly improved the denoising performance of the baseline UNet for all settings. For example, the PSNR for head images was improved by 0.59 dB, and similar improvements were observed for other anatomical regions. Additionally, Table 1 shows the floating point computational requirements of the different methods. It can be seen that the number of FLOPs of the dynamic weight modulation technique is not considerably higher than the baseline network M1, yet the improvement in performance is much appreciable. In Fig. 2, we provide a visual comparison of the denoised output produced by different networks. Two sample images from datasets D1 and D2, corresponding to the abdomen and chest regions, respectively, are shown. The comparison shows that the proposed network M2 outperforms the baseline model M1 in terms of noise reduction and details preservation. For instance, in the denoised image of the abdomen region, the surface of the liver in M1 appears rough and splotchy due to noise, while in M2, the image is crisp, and noise suppression is adequate. Similarly, in the chest LDCT images, noticeable streaking artifacts near the breast region are present in the M1 output, and the boundaries of different organs like the heart and shoulder blade are not well-defined. In contrast, M2 produces crisp and definite boundaries, and streaking artifacts are significantly reduced. Moreover, M1 erases finer details like tiny blood vessels in the lung region, leading to compromised visibility, while M2 preserves small details much better than M1, resulting in output that is comparable with the original NDCT image. Robustness Analysis: In this section, we evaluate the performance of existing denoising networks in a challenging scenario where the networks are trained to remove noise from a mixture of LDCT images taken from different anatomical regions with varying noise variances and patterns. We compared two networks in this analysis: M3, which is a baseline UNet model trained using a mixture of LDCT images, and M4, which is the proposed weight-modulated network, trained using same training data. Table 2 provides an objective comparison between these two methods. We found that joint training has a negative impact on the performance of the baseline network, M3, by a significant margin. Specifically, M3 yielded 0.88 dB lower PSNR than model M1 for head images, which  were trained using only head images. Similar observations were also noted for other anatomical regions like the abdomen and chest. The differences in noise characteristics among the different LDCT images make it difficult for a single model to denoise images efficiently from a mixture of anatomical regions. Furthermore, the class imbalance between small anatomical sites (e.g., head, knee, and prostate) and large anatomical locations (e.g., lung, abdomen) in a training set introduces a bias towards large anatomical sites, resulting in unacceptably lower performance for small anatomical sites. On the other hand, M4 showed robustness to these issues. Its performance was similar to M2 for all settings, and it achieved 0.69 dB higher PSNR than M3. Noise-conditioned weight modulation enables the network to adjust its weight based on the input images, allowing it to denoise every image with the same efficiency.Figure 3 provides a visual comparison of the denoising performance of two methods on LDCT images from three anatomical regions. The adverse effects of joint training on images from different regions are apparent. Head LDCT images, which had the lowest noise, experienced a loss of structural and textural information in the denoising process by baseline M3. For example, the head lobes appeared distorted in the reconstructed image. Conversely, chest LDCT images, which were the noisiest, produced artefacts in the denoised image by M3, significantly altering the image's visual appearance. In contrast, M4 preserved all structural information and provided comparable noise reduction across all anatomical structures. CNN-based denoising networks act like a subtractive method, where the network learns to subtract the noise from the input signal by using a series of convolutional layers. A fixed set of subtracters is inefficient for removing noise from images with various noise levels. As a result, images with low noise are over smoothed and structural information is lost, whereas images with high noise generate residual noise and artefacts. In case of images containing a narrow range of noise levels, such as images from a single anatomical region, the above-mentioned limitation of naive CNN-based denoisers remains acceptable, but when a mixture of images with diverge noise levels is used in training and testing, it becomes problematic. The proposed noise conditioned weight modulation addresses this major limitation of CNN based denoising network, by designing an adjustable subtractor which is adjusted based on the input signal.Figure 4 presents a two-dimensional projection of the learned embedding for all the test images using the TSNE transformation. The embedding has created three distinct clusters in the 2D feature space, each corresponding to images from one of three different anatomical regions. This observation validates our claim that the embedding learned by the anatomy encoder represents a meaningful representation of the input image. Notably, the noise level of low dose chest CT images differs significantly from those of the other two regions, resulting in a   Generalization Analysis: In this section, we evaluate the generalization ability of different networks on out-of-distribution test data using LDCT abdomen images taken with a 1mm slice thickness from dataset D1. We consider four networks for this analysis: 1) M5, the baseline UNet trained on LDCT abdomen images with a 3mm slice thickness from dataset D1, 2) M6, the baseline UNet trained on a mixture of LDCT images from all anatomical regions except the abdomen with a 1mm slice thickness, 3) M7, the proposed weight-modulated network trained on the same training set as M6, and 4) M8, the baseline UNet trained on LDCT abdomen images with a 1mm slice thickness. Objective comparisons among these networks are presented in Table 3. The results show that the performance of M5 and M6 is poor on this dataset, indicating their poor ability to generalize to unseen data. In contrast, M7 performs similarly to the supervised model M8. Next, we compared the denoising performance of different methods visually in Fig. 5. It can be seen that M5 completely failed to remove noise from these images despite the fact the M5 was trained using the abdominal image. Now the output of M6 is better than the M5 in terms of noise removal, but a lot of over-smoothness and loss of structural information can be seen, for example, the over-smooth texture of the liver and removal of blood vessels. M6 benefits from being trained on diverse LDCT images, which allows it to learn robust features applicable to a range of inputs and generalize well to new images. However, the CNN networks' limited ability to handle diverse noise levels results in M6 failing to preserve all the structural information in some cases. In contrast, M7 uses a large training set and dynamic convolution to preserve all structural information and remove noise effectively, comparable to the baseline model M8."
Noise Conditioned Weight Modulation for Robust and Generalizable Low Dose CT Denoising,5,Conclusion,This study proposes a novel noise-conditioned feature modulation layer to address the limitations of convolutional denoising networks in handling variability in noise levels in low-dose computed tomography (LDCT) images. The
Noise Conditioned Weight Modulation for Robust and Generalizable Low Dose CT Denoising,,Fig. 1 .,
Noise Conditioned Weight Modulation for Robust and Generalizable Low Dose CT Denoising,,Fig. 2 .,
Noise Conditioned Weight Modulation for Robust and Generalizable Low Dose CT Denoising,,Fig. 3 .,
Noise Conditioned Weight Modulation for Robust and Generalizable Low Dose CT Denoising,,Fig. 4 .,
Noise Conditioned Weight Modulation for Robust and Generalizable Low Dose CT Denoising,,Fig. 5 .,
Noise Conditioned Weight Modulation for Robust and Generalizable Low Dose CT Denoising,,Table 1 .,
Noise Conditioned Weight Modulation for Robust and Generalizable Low Dose CT Denoising,,Table 2 .,
Noise Conditioned Weight Modulation for Robust and Generalizable Low Dose CT Denoising,,Table 3 .,separate cluster that is located at a slightly greater distance from the other two clusters.
