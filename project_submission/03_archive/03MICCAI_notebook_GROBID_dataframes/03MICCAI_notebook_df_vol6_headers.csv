Paper Title,Header Number,Header Title,Text
Merging-Diverging Hybrid Transformer Networks for Survival Prediction in Head and Neck Cancer,1,Introduction,"Head and Neck (H&N) cancer refers to malignant tumors in H&N regions, which is among the most common cancers worldwide [1]. Survival prediction, a regression task that models the survival outcomes of patients, is crucial for H&N cancer patients: it provides early prognostic information to guide treatment planning and potentially improves the overall survival outcomes of patients [2]. Multi-modality imaging of Positron Emission Tomography -Computed Tomography (PET-CT) has been shown to benefit survival prediction as it offers both anatomical (CT) and metabolic (PET) information about tumors [3,4]. Therefore, survival prediction from PET-CT images in H&N cancer has attracted wide attention and serves as a key research area. For instance, HEad and neCK TumOR segmentation and outcome prediction challenges (HECKTOR) have been held for the last three years to facilitate the development of new algorithms for survival prediction from PET-CT images in H&N cancer [5][6][7].Traditional survival prediction methods are usually based on radiomics [8], where handcrafted radiomics features are extracted from pre-segmented tumor regions and then are modeled by statistical survival models, such as the Cox Proportional Hazard (CoxPH) model [9]. In addition, deep survival models based on deep learning have been proposed to perform end-to-end survival prediction from medical images, where pre-segmented tumor masks are often unrequired [10]. Deep survival models usually adopt Convolutional Neural Networks (CNNs) to extract image features, and recently Visual Transformers (ViT) have been adopted for its capabilities to capture long-range dependency within images [11,12]. These deep survival models have shown the potential to outperform traditional survival prediction methods [13]. For survival prediction in H&N cancer, deep survival models have achieved top performance in the HECKTOR 2021/2022 and are regarded as state-of-the-art [14][15][16]. Nevertheless, we identified that existing deep survival models still have two main limitations.Firstly, existing deep survival models are underdeveloped in utilizing complementary multi-modality information, such as the metabolic and anatomical information in PET and CT images. For survival prediction in H&N cancer, existing methods usually use single imaging modality [17,18] or rely on early fusion (i.e., concatenating multi-modality images as multi-channel inputs) to combine multi-modality information [11,[14][15][16]19]. In addition, late fusion has been used for survival prediction in other diseases such as gliomas and tuberculosis [20,21], where multi-modality features were extracted by multiple independent encoders with resultant features fused. However, early fusion has difficulties in extracting intra-modality information due to entangled (concatenated) images for feature extraction, while late fusion has difficulties in extracting inter-modality information due to fully independent feature extraction. Recently, Tang et al. [22] attempted to address this limitation by proposing a Multi-scale Non-local Attention Fusion (MNAF) block for survival prediction of glioma patients, in which multi-modality features were fused via non-local attention mechanism [23] at multiple scales. However, the performance of this method heavily relies on using tumor segmentation masks as inputs, which limits its generalizability.Secondly, although deep survival models have advantages in performing end-to-end survival prediction without requiring tumor masks, this also incurs difficulties in extracting region-specific information, such as the prognostic information in Primary Tumor (PT) and Metastatic Lymph Node (MLN) regions. To address this limitation, recent deep survival models adopted multi-task learning for joint tumor segmentation and survival prediction, to implicitly guide the model to extract features related to tumor regions [11,16,[24][25][26]. However, most of them only considered PT segmentation and ignored the prognostic information in MLN regions [11,[24][25][26]. Meng et al. [16] performed survival prediction with joint PT-MLN segmentation and achieved one of the top performances in HECKTOR 2022. However, this method extracted entangled features related to both PT and MLN regions, which incurs difficulties in discovering the prognostic information in PT-/MLN-only regions.In this study, we design an X-shape merging-diverging hybrid transformer network (named XSurv, Fig. 1) for survival prediction in H&N cancer. Our XSurv has a merging encoder to fuse complementary anatomical and metabolic information in PET and CT images and has a diverging decoder to extract region-specific prognostic information in PT and MLN regions. Our technical contributions in XSurv are three folds: (i) We propose a merging-diverging learning framework for survival prediction. This framework is specialized in leveraging multi-modality images and extracting regionspecific information, which potentially could be applied to many survival prediction tasks with multi-modality imaging. (ii) We propose a Hybrid Parallel Cross-Attention (HPCA) block for multi-modality feature learning, where both local intra-modality and global inter-modality features are learned via parallel convolutional layers and crossattention transformers. (iii) We propose a Region-specific Attention Gate (RAG) block for region-specific feature extraction, which screens out the features related to lesion regions. Extensive experiments on the public dataset of HECKTOR 2022 [7] demonstrate that our XSurv outperforms state-of-the-art survival prediction methods, including the top-performing methods in HECKTOR 2022. "
Merging-Diverging Hybrid Transformer Networks for Survival Prediction in Head and Neck Cancer,2,Method,"Figure 1 illustrates the overall architecture of our XSurv, which presents an X-shape architecture consisting of a merging encoder for multi-modality feature learning and a diverging decoder for region-specific feature extraction. The encoder includes two PET-/CT-specific feature learning branches with HPCA blocks (refer to Sect. 2.1), while the decoder includes two PT-/MLN-specific feature extraction branches with RAG blocks (refer to Sect. 2.2). Our XSurv performs joint survival prediction and segmentation, where the two decoder branches are trained to perform PT/MLN segmentation and provide PT-/MLN-related deep features for survival prediction (refer to Sect. 2.3). Our XSurv also can be enhanced by leveraging the radiomics features extracted from the XSurv-segmented PT/MLN regions (refer to Sect. 2.4). Our implementation is provided at https://github.com/MungoMeng/Survival-XSurv."
Merging-Diverging Hybrid Transformer Networks for Survival Prediction in Head and Neck Cancer,2.1,PET-CT Merging Encoder,"Assuming N conv , N self , and N cross are three architecture parameters, each encoder branch consists of N conv Conv blocks, N self Hybrid Parallel Self-Attention (HPSA) blocks, and N cross HPCA blocks. Max pooling is applied between blocks and the features before max pooling are propagated to the decoder through skip connections. As shown in Fig. 2(a), HPCA blocks perform parallel convolution and cross-attention operations. The convolution operations are realized using successive convolutional layers with residual connections, while the cross-attention operations are realized using Swin Transformer [27] where the input x in (from the same encoder branch) is projected as Q and the input x cross (from the other encoder branch) is projected as K and V . In addition, Conv blocks perform the same convolution operations as HPCA blocks but discard cross-attention operations; HPSA blocks share the same overall architecture with HPCA blocks but perform self-attention within the input x in (i.e., the x in is projected as Q, K and V ). Conv and HPSA blocks are used first and then followed by HPCA blocks, which enables the XSurv to learn both intra-and inter-modality information. In this study, we set N conv , N self , and N cross as 1, 1, and 3, as this setting achieved the best validation results (refer to the supplementary materials). Other architecture details are also presented in the supplementary materials.The idea of adopting convolutions and transformers in parallel has been explored for segmentation [28], which suggests that parallelly aggregating global and local information is beneficial for feature learning. In this study, we extend this idea to multimodality feature learning, which parallelly aggregates global inter-modality and local intra-modality information via HPCA blocks, to discover inter-modality interactions while preserving intra-modality characteristics. "
Merging-Diverging Hybrid Transformer Networks for Survival Prediction in Head and Neck Cancer,2.2,PT-MLN Diverging Decoder,"As shown in Fig. 1, each decoder branch is symmetric to the encoder branch and thus includes a total of (N conv +N self +N cross ) Conv blocks. The features propagated from skip connections are fed into RAG blocks for feature diverging before entering the Conv blocks in two decoder branches, where the output of the former Conv block is upsampled and concatenated with the output of the RAG block. As shown in Fig. 2 The output of the last Conv block in the PT/MLN branch is fed into a segmentation head, which generates PT/MLN segmentation masks using a sigmoid-activated 1 × 1 × 1 convolutional layer. In addition, the outputs of all but not the first Conv blocks in the PT/MLN branches are fed into global averaging pooling layers to derive PT-/MLNrelated deep features. Finally, all deep features are fed into a survival prediction head, which maps the deep features into a survival score using two fully-connected layers with dropout, L2 regularization, and sigmoid activation."
Merging-Diverging Hybrid Transformer Networks for Survival Prediction in Head and Neck Cancer,2.3,Multi-task Learning,"Following existing multi-task deep survival models [11,16,[24][25][26], our XSurv is endto-end trained for survival prediction and PT-MLN segmentation using a combined loss: L = L Surv +λ(L PT +L MLN ), where the λ is a parameter to balance the survival prediction term L Surv and the PT/MLN segmentation terms L PT /MLN . We follow [15] to adopt a negative log-likelihood loss [30] as the L Surv . For the L PT /MLN , we adopt the sum of Dice [31] and Focal [32] losses. The loss functions are detailed in the supplementary materials. The λ is set as 1 in the experiments as default."
Merging-Diverging Hybrid Transformer Networks for Survival Prediction in Head and Neck Cancer,2.4,Radiomics Enhancement,"Our XSurv also can be enhanced by leveraging radiomics features (denoted as Radio-XSurv). Following [16], radiomics features are extracted from the XSurv-segmented PT/MLN regions via Pyradiomics [33] and selected by Least Absolute Shrinkage and Selection Operator (LASSO) regression. The process of radiomics feature extraction is provided in the supplementary materials. Then, a CoxPH model [9] is adopted to integrate the selected radiomics features and the XSurv-predicted survival score to make the final prediction. In addition, clinical indicators (e.g., age, gender) also can be integrated by the CoxPH model."
Merging-Diverging Hybrid Transformer Networks for Survival Prediction in Head and Neck Cancer,3,Experimental Setup,
Merging-Diverging Hybrid Transformer Networks for Survival Prediction in Head and Neck Cancer,3.1,Dataset and Preprocessing,"We adopted the training dataset of HECKTOR 2022 (refer to https://hecktor.grand-cha llenge.org/), including 488 H&N cancer patients acquired from seven medical centers [7], while the testing dataset was excluded as its ground-truth labels are not released. Each patient underwent pretreatment PET/CT and has clinical indicators. We present the distributions of all clinical indicators in the supplementary materials. Recurrence-Free Survival (RFS), including time-to-event in days and censored-or-not status, was provided as ground truth for survival prediction, while PT and MLN annotations were provided for segmentation. The patients from two centers (CHUM and CHUV) were used for testing and other patients for training, which split the data into 386/102 patients in training/testing sets. We trained and validated models using 5-fold cross-validation within the training set and evaluated them in the testing set.We resampled PET-CT images into isotropic voxels where 1 voxel corresponds to 1 mm 3 . Each image was cropped to 160 × 160 × 160 voxels with the tumor located in the center. PET images were standardized using Z-score normalization, while CT images were clipped to [-1024, 1024] and then mapped to [-1, 1]. In addition, we performed univariate and multivariate Cox analyses on the clinical indicators to screen out the prognostic indicators with significant relevance to RFS (P < 0.05)."
Merging-Diverging Hybrid Transformer Networks for Survival Prediction in Head and Neck Cancer,3.2,Implementation Details,"We implemented our XSurv using PyTorch on a 12 GB GeForce GTX Titan X GPU. Our XSurv was trained for 12,000 iterations using an Adam optimizer with a batch size of 2. Each training batch included the same number of censored and uncensored samples. The learning rate was set as 1e-4 initially and then reset to 5e-5 and 1e-5 at the 4,000 th and 8,000 th training iteration. Data augmentation was applied in real-time during training to minimize overfitting, including random affine transformations and random cropping to 112 × 112 × 112 voxels. Validation was performed after every 200 training iterations and the model achieving the highest validation result was preserved.In our experiments, one training iteration (including data augmentation) took roughly 4.2 s, and one inference iteration took roughly 0.61 s."
Merging-Diverging Hybrid Transformer Networks for Survival Prediction in Head and Neck Cancer,3.3,Experimental Settings,"We compared our XSurv to six state-of-the-art survival prediction methods, including two traditional radiomics-based methods and four deep survival models. The included traditional methods are CoxPH [9] and Individual Coefficient Approximation for Risk Estimation (ICARE) [34]. For traditional methods, radiomics features were extracted from the provided ground-truth tumor regions and selected by LASSO regression. The included deep survival models are Deep Multi-Task Logistic Regression and CoxPH ensemble (DeepMTLR-CoxPH) [14], Transformer-based Multimodal networks for Segmentation and Survival prediction (TMSS) [11], Deep Multi-task Survival model (DeepMTS) [24], and Radiomics-enhanced DeepMTS (Radio-DeepMTS) [16]. DeepMTLR-CoxPH, ICARE, and Radio-DeepMTS achieved top performance in HECKTOR 2021 and 2022. For a fair comparison, all methods took the same preprocessed images and clinical indicators as inputs. Survival prediction and segmentation were evaluated using Concordance index (C-index) and Dice Similarity Coefficient (DSC), which are the standard evaluation metrics in the challenges [6,7,35].We also performed two ablation studies on the encoder and decoder separately: (i) We replaced HPCA/HPSA blocks with Conv blocks and compared different strategies to combine PET-CT images. (ii) We removed RAG blocks and compared different strategies to extract PT/MLN-related information. "
Merging-Diverging Hybrid Transformer Networks for Survival Prediction in Head and Neck Cancer,4,Results and Discussion,"The comparison between our XSurv and the state-of-the-art methods is presented in Table 1. Our XSurv achieved a higher C-index than all compared methods, which demonstrates that our XSurv has achieved state-of-the-art performance in survival prediction of H&N cancer. When radiomics enhancement was adopted in XSurv and DeepMTS, our Radio-XSurv also outperformed the Radio-DeepMTS and achieved the highest Cindex. Moreover, the segmentation results of multi-task deep survival models (TMSS, DeepMTS, and XSurv) are also reported in Table 1. Our XSurv achieved higher DSCs than TMSS and DeepMTS, which demonstrates that our XSurv can locate PT and MLN more precisely and this infers that our XSurv has better learning capability. We attribute these performance improvements to the use of our proposed merging-diverging learning framework, HPCA block, and RAG block, which can be evidenced by ablation studies.The ablation study on the PET-CT merging encoder is shown in Table 2. We found that using PET alone resulted in a higher C-index than using both PET-CT with early or late fusion. This finding is consistent with Wang et al. [19]'s study, which suggests that early and late fusion cannot effectively leverage the complementary information in PET-CT images. As we have mentioned, early and late fusion have difficulties in extracting intra-and inter-modality information, respectively. Our encoder first adopts Conv/HPSA blocks to extract intra-modality information and then leverages HPCA blocks to discover their interactions, which achieved the highest C-index. For PT and MLN segmentation, our encoder also achieved the highest DSCs, which indicates that our encoder also can improve segmentation. In addition, MNAF blocks [22] were compared and showed poor performance. This is likely attributed to the fact that leveraging non-local attention at multiple scales has corrupted local spatial information, which degraded the segmentation performance and distracted the model from PT and MLN regions. To relieve this problem, in Tang et al.'s study [22], tumor segmentation masks were fed into the model as explicit guidance to tumor regions. However, it is intractable to have segmentation masks at the inference stage in clinical practice.The ablation study on the PT-MLN diverging decoder is shown in Table 3. We found that, even without adopting AG, using a dual-branch decoder for PT and MLN segmentation resulted in a higher C-index than using a single-branch decoder, which demonstrates the effectiveness of our diverging decoder design. Adopting vanilla AG [29] or RAG in the dual-branch decoder further improved survival prediction. Compared to the vanilla AG, our RAG contributed to a larger improvement, and this enabled our decoder to achieve the highest C-index. In the supplementary materials, we visualized the attention maps produced by RAG blocks, where the attention maps can precisely locate PT/MLN regions and screen out PT-/MLN-related features. For PT and MLN segmentation, using a single-branch decoder for PT-or MLN-only segmentation achieved the highest DSCs. This is expected as the model can leverage all its capabilities to segment only one target. Nevertheless, our decoder still achieved the second-best DSCs in both PT and MLN segmentation with a small gap. "
Merging-Diverging Hybrid Transformer Networks for Survival Prediction in Head and Neck Cancer,5,Conclusion,"We have outlined an X-shape merging-diverging hybrid transformer network (XSurv) for survival prediction from PET-CT images in H&N cancer. Within the XSurv, we propose a merging-diverging learning framework, a Hybrid Parallel Cross-Attention (HPCA) block, and a Region-specific Attention Gate (RAG) block, to learn complementary information from multi-modality images and extract region-specific prognostic information for survival prediction. Extensive experiments have shown that the proposed framework and blocks enable our XSurv to outperform state-of-the-art survival prediction methods on the well-benchmarked HECKTOR 2022 dataset."
Merging-Diverging Hybrid Transformer Networks for Survival Prediction in Head and Neck Cancer,,Fig. 1 .,
Merging-Diverging Hybrid Transformer Networks for Survival Prediction in Head and Neck Cancer,,Fig. 2 .,
Merging-Diverging Hybrid Transformer Networks for Survival Prediction in Head and Neck Cancer,,,
Merging-Diverging Hybrid Transformer Networks for Survival Prediction in Head and Neck Cancer,,Table 1 .,
Merging-Diverging Hybrid Transformer Networks for Survival Prediction in Head and Neck Cancer,,Table 2 .,
Merging-Diverging Hybrid Transformer Networks for Survival Prediction in Head and Neck Cancer,,Table 3 .,
Wall Thickness Estimation from Short Axis Ultrasound Images via Temporal Compatible Deformation Learning,1,Introduction,"Cardiovascular disease is a leading cause of death in the world. Accurate quantification of left ventricular wall thicknesses (LVWT) from ultrasound images is among the most clinically important and significant indices for evaluating cardiac function and diagnosis of cardiac diseases [2,6]. Figure 1 illustrates short axis (SAX) ultrasound images of basal, middle, and apical myocardium, with the corresponding LVWTs according to the 16-segment myocardium model. In clinical practice, obtaining reliable clinical information mainly depends on radiologists to manually draw the contours of the endocardium and epicardium of the left ventricle (LV). It is time-consuming and laborious. Efforts have been devoted to the automatic estimation of LVWTs, where great challenge exists. First, the myocardium boundary is sensitive to heavy noise, especially for the apical and basal SAX images, and can lead to irregular boundaries and undermine the estimation of LVWTs. Second, the temporal dynamics in the ultrasound video are difficult to be modeled, leading to prediction results that are not well compatible with the temporal dynamics of the whole video.Existing work can be divided into two categories: segmentation-based and direct-regression methods. The direct-regression methods to learn the regress LVWTs from cardiac images directly without identifying the contours first. [5,15] proposed end-to-end cardiac index quantification frameworks based on cascaded convolutional autoencoders and regression networks, using only the values of cardiac indices for supervision. [4] proposed a two-stage network that learns the LV contours first and then estimates the LV indices with a new network. [16] proposed a residual recurrent neural network further improves the estimation by modeling the temporal and spatial of the LV myocardium to achieve accurate frame-by-frame LVWT estimation. However, these models lack explicit temporal dynamic modeling of the whole sequence.Segmentation-based methods segment the myocardium first and then calculate cardiac parameters. To the best of our knowledge, existing segmentation work mainly focus on apical views to evaluate the ejection fraction, and rare work exists for short-axis views. [14] utilizes the underlying motion information to assist in improving segmentation results by accurately predicting optical flow fields. [12,13] proposed appearance-level and shape-level co-learning (CLAS) to enhance the temporal consistency of the predicted masks across the whole sequence and accuracy. This method effectively improves the accuracy and consistency of myocardial segmentation. [1] proposed to introduce residual structure into U-net and [3] proposed a hybrid framework combining a convolutional encoder-decoder structure and a transformer. [7,17] proposed a multi-attention mechanism to guide the network to capture features effectively while suppressing noise, and integrated deep supervision mechanism and spatial pyramid feature fusion to enhance feature extraction. However, these models are not robust to the heavy noise in the SAX images, which may lead to irregular boundaries and undermines the estimation of LVWT.To overcome the above mention challenges and inspired [8] where template transformer was employed for image segmentation, we propose a novel Temporal-Compatible Deformation learning network for myocardium boundary detection from ultrasound SAX images. The primary contributions of this paper are as follows. 1) To overcome the irregular boundaries caused by the heavy noise, we propose a two-stage deformation learning network for myocardium boundary detection. A global affine transformation and a local deformation are used to deform the prior myocardium template to match the myocardium boundary. 2) To make the template-deformed myocardium boundaries compatible across the whole sequence, we propose a bi-direction deformation learning to guarantee that the deformation fields across the whole sequences can be applied to both the myocardium boundaries and the ultrasound images. 3) The proposed TCdeformer achieves excellent performance for LVWT estimation, with an error of less than 1.00 mm, and is comparable with middle-level cardiologists."
Wall Thickness Estimation from Short Axis Ultrasound Images via Temporal Compatible Deformation Learning,2,Methods,"The structures of the myocardium in ultrasound SAX images generally follow a circular ring shape, which is a vital characteristic of prior knowledge in shortaxis myocardial segmentation, especially for ultrasound images with heavy noisy myocardium boundaries. In this paper, we propose a novel method, Temporal Compatibility Deformation Learning, named TC-Deformer, to achieve accurate and plausible myocardium contours and LVWTs estimation. The details are described as follows. Global Affine Transformation. Our deformation learning consists of two stages: global affine transformation and local deformation learning. In this subsection, we will describe our global affine transformation. Considering the diversity of the cardiac structure and data gaps from different machines, we compute the mean value of the thickness measurements and the center position of the circle according to the annotated information to generate the prior template. As shown in Fig. 2(a), the prior template (P) is first concatenated with the convolution features extracted from the SAX image to learn the global affine parameters θ = {(Δx, Δy), s}, which represent the shift and scale from the prior template to the myocardium boundaries in the SAX image. Then we get the affine prior (AT) S AT = φ G (P ) by warping the prior template (P) with the global affine parameters θ. The loss function is as follows:"
Wall Thickness Estimation from Short Axis Ultrasound Images via Temporal Compatible Deformation Learning,2.1,Deformation Learning,"where S is the ground truth of the myocardium segmentation. However, the shapes warping from the prior template with global affine parameters are far from precise due to the individual variation. So, we introduce the local deformation learning to get a precise myocardium shape with the learned dense deformation field.Local Deformation Learning. In this part, we aim to learn a dense deformation field to adjust the AT prior locally to match the myocardium boundaries.As shown in Fig. 2(b), the AT prior is first concatenated with the image convolution feature to learn a dense deformation field φ L ∈ R 256×256×2 , which represents the pixel-level displacement along both horizontal and vertical directions. Our local deformation learning considers the prior shape, the prior position, and the image feature simultaneously, which can help the network learn a more precise deformation field and get a local adjustment of the template. Finally, we take the segmentation Ŝ = φ L (S AT ) warping from the AT prior with the dense deformation filed as the final myocardium segmentation result. The loss function is as follows:where S is the ground truth of the myocardium segmentation. In this work, only frames at the end-systolic (ED) and end-diastolic (ES) phases are annotated by cardiologists for each ultrasound SAX video. To make the template-deformed myocardium boundaries compatible across the whole sequence, we propose a bi-direction deformation learning to guarantee that the deformation fields across the whole sequence can be applied to both the myocardium boundaries and the ultrasound images in the sequence."
Wall Thickness Estimation from Short Axis Ultrasound Images via Temporal Compatible Deformation Learning,2.2,TC-Deformer,"As shown in Fig. 3, first of all, we use the mean θ of the affine transformation parameters {θ 1 , θ 2 , θ 3 , ..., θ T -2 , θ T -1 , θ T } of all frames in the sequence as a videolevel parameter and obtain the mean affine transformation prior template (MAT prior). Next, the MAT prior is combined with the sequential images to learn a series of dense bi-direction deformation fields. Let φ t F be the forward deformation and φ t B the backward deformation for the frame t. Our bi-direction deformation learning (as shown in Fig. 3(b)) aims to constrain that for each frame X t , after forward deformation and backward deformation, we can still obtain the original images. We adopt the structural similarity metric (SSIM) [11] in image quality assessment to quantify the deformation error. For the image cycle, the loss function is as follows:Similarly, for the MAT prior S MAT , we can have the shape consistency constraint when applied to the bi-direction deformation procedure:As the temporally compatible deformation started with a common template, we assume that when warped backward, all the frames will have a similar appearance. So, we introduce a centralization loss, to minimize the deviation between those backward deformed frames:where T represents the time.The total loss function of our TC-Deformer is as follows:where α, β and γ is the hyper-parameters.After myocardial segmentation, we use neural networks to determine two key points, combined with the centroid of the segmentation, to divided it into 16segments according to the 16-segment model of the American Heart Association and calculated the corresponding LVWTs."
Wall Thickness Estimation from Short Axis Ultrasound Images via Temporal Compatible Deformation Learning,3,Experiments and Results,
Wall Thickness Estimation from Short Axis Ultrasound Images via Temporal Compatible Deformation Learning,3.1,Dataset Description and Experimental Setup,"Datasets. In this experiment, we trained our method on ultrasound SAX videos of 141 participants and tested with 60 participants. All the data was collected with the GE Vivid E95 from the Shenzhen People's Hospital and this study was approved by local institutional review boards. For each participant, videos of the basal, middle, and apical SAX views were acquired with multiple cardiac cycles. The mask of the myocardium at the ED and ES phases from one cardiac cycle was annotated by experts (including two senior, three middle-level, and three junior cardiologists). We compare the LVWT of each group with the average wall thickness of the senior doctors to get the average of the errors for different doctors as result. For the training dataset, the senior cardiologists conducted quality control for junior and middle-level cardiologists. All images were annotated by experienced doctors using the Pair annotation software package [9](https://www.aipair.com.cn/en/, Version 2.7, RayShape, Shenzhen, China).Experimental Setup. We resized the images to the same size 256 × 256 and used the Adam optimization strategy during model training. The training is in two stages and the initial learning rate was 0.0001, the total epoch number is 100, and the batch size is 4. The hyperparameters α, β, and γ were set to be 0.1, 0.5, and 0.5, respectively, according to a small validation set. The models are implemented with PyTorch on the NVIDIA A100 Tensor Core GPU. Table 1 shows the segmentation performance on the test set in terms of Dice, Hausdorff distance (HD), and floating-point operations per second (FLOPs). We can conclude that the proposed method achieves excellent segmentation performance and outperforms U-net and the state-of-the-art CLAS, while costing much less computation. Figure 4 shows the segmentation results of myocardium compared with four methods. It indicates that our method has a more reasonable myocardium shape than others, which is important to LVWT. Figure 5 shows the segmentation results of the myocardium in one cardiac cycle. It indicates that our method can obtain smoother contours for the middle frames than CLAS, implying that the temporally compatible deformation learning has a better temporal consistency."
Wall Thickness Estimation from Short Axis Ultrasound Images via Temporal Compatible Deformation Learning,3.2,Results,"Table 2 shows the MAE of the measurements in LVWT for middle-level and junior cardiologists, as while as for the proposed TD-Deformer. It indicates our results of the measurements are better than the junior groups and comparable with the middle-level group. The error of LVWT estimation is less than 1.00 mm. Figure 6 shows the absolute error of the predicted results for 16 segments. We can conclude that for all 16 segments, the prediction results of TC-Deformer are stable and accurate.  "
Wall Thickness Estimation from Short Axis Ultrasound Images via Temporal Compatible Deformation Learning,4,Conclusion,"In this paper, We propose a Temporally Compatible Deformation learning network, named TC-Deformer, to detect the myocardium boundaries and estimate regional left ventricle wall thickness. Our method is designed to avoid the irregular contours that can happen in ultrasound images with heavy noise and can incorporate the temporal dynamics of the myocardium in one cardiac cycle into the deformation field learning. When validated with a dataset of 201 patients, our method achieves less than 1.00 mm estimation error for all 16 myocardium segments and outperforms existing state-of-the-art methods."
Wall Thickness Estimation from Short Axis Ultrasound Images via Temporal Compatible Deformation Learning,,Fig. 1 .,
Wall Thickness Estimation from Short Axis Ultrasound Images via Temporal Compatible Deformation Learning,,Fig. 2 .,
Wall Thickness Estimation from Short Axis Ultrasound Images via Temporal Compatible Deformation Learning,,Fig. 3 .,
Wall Thickness Estimation from Short Axis Ultrasound Images via Temporal Compatible Deformation Learning,,Fig. 4 .,
Wall Thickness Estimation from Short Axis Ultrasound Images via Temporal Compatible Deformation Learning,,Fig. 5 .,
Wall Thickness Estimation from Short Axis Ultrasound Images via Temporal Compatible Deformation Learning,,Fig. 6 .,
Wall Thickness Estimation from Short Axis Ultrasound Images via Temporal Compatible Deformation Learning,,Table 1 .,
Wall Thickness Estimation from Short Axis Ultrasound Images via Temporal Compatible Deformation Learning,,Table 2 .,
DiffMIC: Dual-Guidance Diffusion Network for Medical Image Classification,1,Introduction,"Medical image analysis plays an indispensable role in clinical therapy because of the implications of digital medical imaging in modern healthcare [5]. Medical image classification, a fundamental step in the analysis of medical images, strives to distinguish medical images from different modalities based on certain criteria. An automatic and reliable classification system can help doctors interpret medical images quickly and accurately. Massive solutions for medical image classification have been developed over the past decades in the literature, most of which adopt deep learning ranging from popular CNNs to vision transformers [8,9,22,23]. These methods have the potential to reduce the time and effort required for manual classification and improve the accuracy and consistency of results. However, medical images with diverse modalities still challenge existing methods due to the presence of various ambiguous lesions and fine-grained tissues, such as ultrasound (US), dermatoscopic, and fundus images. Moreover, generating medical images under hardware limitations can cause noisy and blurry effects, which can degrade image quality and thus demand a more effective feature representation modeling for robust classifications.Recently, Denoising Diffusion Probabilistic Models (DDPM) [14] have achieved excellent results in image generation and synthesis tasks [2,6,21,26] by iteratively improving the quality of a given image. Specifically, DDPM is a generative model based on a Markov chain, which models the data distribution by simulating a diffusion process that evolves the input data towards a target distribution. Although a few pioneer works tried to adopt the diffusion model for image segmentation and object detection tasks [1,4,12,29], their potential for high-level vision has yet to be fully explored.Motivated by the achievements of diffusion probabilistic models in generative image modeling, 1) we present a novel Denoising Diffusion-based model named DiffMIC for accurate classification of diverse medical image modalities. As far as we know, we are the first to propose a Diffusion-based model for general medical image classification. Our method can appropriately eliminate undesirable noise in medical images as the diffusion process is stochastic in nature for each sampling step. 2) In particular, we introduce a Dual-granularity Conditional Guidance (DCG) strategy to guide the denoising procedure, conditioning each step with both global and local priors in the diffusion process. By conducting the diffusion process on smaller patches, our method can distinguish critical tissues with fine-grained capability. 3) Moreover, we introduce Condition-specific Maximum-Mean Discrepancy (MMD) regularization to learn the mutual information in the latent space for each granularity, enabling the network to model a robust feature representation shared by the whole image and patches. 4) We evaluate the effectiveness of DiffMIC on three 2D medical image classification tasks including placental maturity grading, skin lesion classification, and diabetic retinopathy grading. The experimental results demonstrate that our diffusion-based classification method consistently and significantly surpasses state-of-the-art methods for all three tasks. Figure 1 shows the schematic illustration of our network for medical image classification. Given an input medical image x, we pass it to an image encoder to obtain the image feature embedding ρ(x), and a dual-granularity conditional guidance (DCG) model to produce the global prior ŷg and local prior ŷl . At the training stage, we apply the diffusion process on ground truth y 0 and different priors to generate three noisy variables y g t , y l t , and y t (the global prior for y g t , the local prior for y l t , and dual priors for y t ). Then, we combine the three noisy variables y g t , y l t , and y t and their respective priors and project them into a latent space, respectively. We further integrate three projected embeddings with the image feature embedding ρ(x) in the denoising U-Net, respectively, and predict the noise distribution sampled for y g t , y l t , and y t . We devise condition-specific maximum-mean discrepancy (MMD) regularization loss on the predicted noise of y g t and y l t , and employ the noise estimation loss by mean squared error (MSE) on the predicted noise of y t to collaboratively train our DiffMIC network. Diffusion Model. Following DDPM [14], our diffusion model also has two stages: a forward diffusion stage (training) and a reverse diffusion stage (inference). In the forward process, the ground truth response variable y 0 is added Gaussian noise through the diffusion process conditioned by time step t sampled from a uniform distribution of [1, T ], and such noisy variables are denoted as {y 1 , ..., y t , .., y T }. As suggested by the standard implementation of DDPM, we adopt a UNet as the denoising network to parameterize the reverse diffusion process and learn the noise distribution in the forward process. In the reverse diffusion process, the trained UNet θ generates the final prediction ŷ0 by transforming the noisy variable distribution p θ (y T ) to the ground truth distribution p θ (y 0 ):"
DiffMIC: Dual-Guidance Diffusion Network for Medical Image Classification,2,Method,"where θ is parameters of the denoising UNet, N (•, •) denotes the Gaussian distribution, and I is the identity matrix."
DiffMIC: Dual-Guidance Diffusion Network for Medical Image Classification,2.1,Dual-Granularity Conditional Guidance (DCG) Strategy,"DCG Model. In most conditional DDPM, the conditional prior will be a unique given information. However, medical image classification is particularly challenging due to the ambiguity of objects. It is difficult to differentiate lesions and tissues from the background, especially in low-contrast image modalities, such as ultrasound images. Moreover, unexpected noise or blurry effects may exist in regions of interest (ROIs), thereby hindering the understanding of high-level semantics. Taking only a raw image x as the condition in each diffusion step will be insufficient to robustly learn the fine-grained information, resulting in classification performance degradation.To alleviate this issue, we design a Dual-granularity Conditional Guidance (DCG) for encoding each diffusion step. Specifically, we introduce a DCG model τ D to compute the global and local conditional priors for the diffusion process. Similar to the diagnostic process of a radiologist, we can obtain a holistic understanding from the global prior and also concentrate on areas corresponding to lesions from the local prior when removing the negative noise effects. As shown in Fig. 1 (c), for the global stream, the raw image data x is fed into the global encoder τ g and then a 1 × 1 convolutional layer to generate a saliency map of the whole image. The global prior ŷg is then predicted from the whole saliency map by averaging the responses. For the local stream, we further crop the ROIs whose responses are significant in the saliency map of the whole image. Each ROI is fed into the local encoder τ l to obtain a feature vector. We then leverage the gated attention mechanism [15] to fuse all feature vectors from ROIs to obtain a weighted vector, which is then utilized for computing the local prior ŷl by one linear layer. Denoising Model. The noisy variable y t is sampled in the diffusion process based on the global and local priors computed by the DCG model following:where ∼ N(0, I), ᾱt = t α t , α t = 1β t with a linear noise schedule {β t } t=1:T ∈ (0, 1) T . After that, we feed the concatenated vector of the noisy variable y t and dual priors into our denoising model UNet θ to estimate the noise distribution, which is formulated as:where f (•) denotes the projection layer to the latent space. [•] is the concatenation operation. E(•) and D(•) are the encoder and decoder of UNet. Note that the image feature embedding ρ(x) is further integrated with the projected noisy embedding in the UNet to make the model focus on high-level semantics and thus obtain more robust feature representations. In the forward process, we seek to minimize the noise estimation loss L :Our method improves the vanilla diffusion model by conditioning each step estimation function on priors that combine information derived from the raw image and ROIs."
DiffMIC: Dual-Guidance Diffusion Network for Medical Image Classification,2.2,Condition-Specific MMD Regularization,"Maximum-Mean Discrepancy (MMD) is to measure the similarity between two distributions by comparing all of their moments [11,17], which can be efficiently achieved by a kernel function. Inspired by InfoVAE [31], we introduce an additional pair of condition-specific MMD regularization loss to learn mutual information between the sampled noise distribution and the Gaussian distribution.To be specific, we sample the noisy variable y g t from the diffusion process at time step t conditioned only by the global prior and then compute an MMDregularization loss as:where K(•, •) is a positive definite kernel to reproduce distributions in the Hilbert space. The condition-specific MMD regularization is also applied on the local prior, as shown in Fig. 1 (a). While the general noise estimation loss L captures the complementary information from both priors, the condition-specific MMD regularization maintains the mutual information between each prior and target distribution. This also helps the network better model the robust feature representation shared by dual priors and converge faster in a stable way."
DiffMIC: Dual-Guidance Diffusion Network for Medical Image Classification,2.3,Training and Inference Scheme,"Total Loss. By adding the noise estimation loss and the MMD-regularization loss, we compute the total loss L dif f of our denoising network as follows:where λ is a balancing hyper-parameter, and it is empirically set as λ=0. 5.Training Details. The diffusion model in this study leverages a standard DDPM training process, where the diffusion time step t is selected from a uniform distribution of [1, T ], and the noise is linearly scheduled with β 1 = 1 × 10 -4 and β T = 0.02. We adopt ResNet18 as the image encoder ρ(•). Following [12], we concatenate y t ,ŷ g ,ŷ l , and apply a linear layer with an output dimension of 6144 to obtain the fused vector in the latent space. To condition the response embedding on the timestep, we perform a Hadamard product between the fused vector and a timestep embedding. We then integrate the image feature embedding and response embedding by performing another Hadamard product between them. The output vector is sent through two consecutive fully-connected layers, each followed by a Hadamard product with a timestep embedding. Finally, we use a fully-connected layer to predict the noise with an output dimension of classes. It is worth noting that all fully-connected layers are accompanied by a batch normalization layer and a Softplus non-linearity, with the exception of the output layer. For the DCG model τ D , the backbone of its global and local stream is ResNet. We adopt the standard cross-entropy loss as the objective of the DCG model. We jointly train the denoising diffusion model and DCG model after pretraining the DCG model 10 epochs for warm-up, thereby resulting in an end-to-end DiffMIC for medical image classification. Inference Stage. As displayed in Fig. 1 (b), given an input image x, we first feed it into the DCG model to obtain dual priors ŷg , ŷl . Then, following the pipeline of DDPM, the final prediction ŷ0 is iteratively denoised from the random prediction y T using the trained UNet conditioned by dual priors ŷg , ŷl and the image feature embedding ρ(x)."
DiffMIC: Dual-Guidance Diffusion Network for Medical Image Classification,3,Experiments,"Datasets and Evaluation: We evaluate the effectiveness of our network on an in-home dataset and two public datasets, e.g., PMG2000, HAM10000 [27], and APTOS2019 [16]. (a) PMG2000. We collect and annotate a benchmark dataset (denoted as PMG2000) for placental maturity grading (PMG) with four categories1 . PMG2000 is composed of 2,098 ultrasound images, and we randomly divide the entire dataset into a training part and a testing part at an 8:2 ratio. (b) HAM10000. HAM10000 [27] is from the Skin Lesion Analysis Toward Melanoma Detection 2018 challenge, and it contains 10,015 skin lesion images with predefined 7 categories. (c) APTOS2019. In APTOS2019 [16], A total of 3,662 fundus images have been labeled to classify diabetic retinopathy into five different categories. Following the same protocol in [10], we split HAM10000 and APTOS2019 into a train part and a test part at a 7:3 ratio. These three datasets are with different medical image modalities. PMG2000 is gray-scale and class-balanced ultrasound images; HAM10000 is colorful but classimbalanced dermatoscopic images; and APTOS2019 is another class-imbalanced dataset with colorful Fundus images. Moreover, we introduce two widely-used metrics Accuracy and F1-score to quantitatively compare our framework against existing SOTA methods. Implementation Details: Our framework is implemented with the PyTorch on one NVIDIA RTX 3090 GPU. We center-crop the image and then resize the spatial resolution of the cropped image to 224×224. Random flipping and rotation for data augmentation are implemented during the training processing. In all experiments, we extract six 32×32 ROI patches from each image. We trained our network end-to-end using the batch size of 32 and the Adam optimizer. The initial learning rate for the denoising model U-Net is set as 1×10 -3 , while for the DCG model (see Sect. 2.1) it is set to 2×10 -4 when training the entire network. Following [20], the number of training epochs is set as 1,000 for all three datasets. In inference, we empirically set the total diffusion time step T as 100 for PMG2000, 250 for HAM10000, and 60 for APTOS2019, which is much smaller than most of the existing works [12,14]. The average running time of our DiffMIC is about 0.056 s for classifying an image with a spatial resolution of 224×224. For HAM10000, our method produces a promising improvement over the secondbest method ProCo of 0.019 and 0.053 in terms of Accuracy and F1-score, respectively. For APTOS2019, our method obtains a considerable improvement over ProCo of 0.021 and 0.042 in Accuracy and F1-score respectively.  Ablation Study: Extensive experiments are conducted to evaluate the effectiveness of major modules of our network. To do so, we build three baseline networks from our method. The first baseline (denoted as ""basic"") is to remove all diffusion operations and the MMD regularization loss from our network. It means that ""basic"" is equal to the classical ResNet18. Then, we apply the vanilla diffusion process onto ""basic"" to construct another baseline network (denoted as ""C1""), and further add our dual-granularity conditional guidance into the diffusion process to build a baseline network, which is denoted as ""C2"". Hence, ""C2"" is equal to removing the MMD regularization loss from our network for image classification. Table 2 reports the Accuracy and F1-score results of our method and three baseline networks on our PMG2000 dataset. Apparently, compared to ""basic"", ""C1"" has an Accuracy improvement of 0.027 and an F1-score improvement of 0.018, which indicates that the diffusion mechanism can learn more discriminate features for medical image classification, thereby improving the PMG performance. Moreover, the better Accuracy and F1-score results of ""C2"" over ""C1"" demonstrates that introducing our dual-granularity conditional guidance into the vanilla diffusion process can benefit the PMG performance. Furthermore, our method outperforms ""C2"" in terms of Accuracy and F1-score, which indicates that exploring the MMD regularization loss in the diffusion process can further help to enhance the PMG results."
DiffMIC: Dual-Guidance Diffusion Network for Medical Image Classification,,Visualization of Our Diffusion Procedure:,"To illustrate the diffusion reverse process guided by our dual-granularity conditional encoding, we used the t-SNE tool to visualize the denoised feature embeddings at consecutive time steps. Figure 2 presents the results of this process on all three datasets. As the time step encoding progresses, the denoise diffusion model gradually removes noise from the feature representation, resulting in a clearer distribution of classes from the Gaussian distribution. The total number of time steps required for inference depends on the complexity of the dataset."
DiffMIC: Dual-Guidance Diffusion Network for Medical Image Classification,4,Conclusion,"This work presents a diffusion-based network (DiffMIC) to boost medical image classification. The main idea of our DiffMIC is to introduce dual-granularity conditional guidance over vanilla DDPM, and enforce condition-specific MMD regularization to improve classification performance. Experimental results on three medical image classification datasets with diverse image modalities show the superior performance of our network over state-of-the-art methods. As the first diffusion-based model for general medical image classification, our DiffMIC has the potential to serve as an essential baseline for future research in this area."
DiffMIC: Dual-Guidance Diffusion Network for Medical Image Classification,,Fig. 1 .,
DiffMIC: Dual-Guidance Diffusion Network for Medical Image Classification,,Fig. 2 .,
DiffMIC: Dual-Guidance Diffusion Network for Medical Image Classification,,Table 1 .,
DiffMIC: Dual-Guidance Diffusion Network for Medical Image Classification,,Comparison with State-of-the-Art Methods: InTable 1,
DiffMIC: Dual-Guidance Diffusion Network for Medical Image Classification,,Table 2 .,
DiffMIC: Dual-Guidance Diffusion Network for Medical Image Classification,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 10.
Polar Eyeball Shape Net for 3D Posterior Ocular Shape Representation,1,Introduction,"Posterior eyeball shape (PES) is related to various ophthalmic diseases, such as glaucoma, high myopia, and retinoblastoma [6,8,25]. Ophthalmologists often define the retinal pigment epithelium (RPE) layer represented as the shape of the posterior eyeball [3,5,19]. The changes in PES are helpful for surgeons to further specifically optimize the treatments of myopia, surgical planning, and diagnosis result [2,12,20]. For example, the PES can assist ophthalmologists to determine the expansion types of myopia, 6 types inferred to [18,22], in order to prevent further increase of myopic degree at an early stage [5]. Besides, the precise PES facilitates surgeons to estimate the cut length of the rectus during surgical planning, optimizing operative outcomes and avoiding refractive error after strabismus surgery [14]. Moreover, posterior eyeball shape is a sensitive indicator to facilitate fundus disease screening, such as glaucoma and tilted disc syndrome (TDS) [3,12].In ophthalmic clinics, existing representations of posterior eyeball shape are mainly based on two medical imaging devices, including Magnetic Resonance Imaging (MRI) and Optical Coherence Tomography (OCT). MRI cannot become the preferred screening device in ophthalmic clinics due to its being expensive and time-consuming. Even most ophthalmic hospitals do not equip MR devices. Moreover, the resolution of ocular MRI is 0.416 × 0.416 × 0.399 mm 2 , while the depth of the retina is around 250 µm [1]. Thus, limited by the resolution of MRI, surgeons only infer the approximate posterior shape from the outer edge of the sclera or inner edge of the retina, as the retinal layers cannot be distinguished from MR images [2,7,10,18,23,26]. Most common OCT devices applied in eye hospitals can provide clear retinal layer imaging, but their field of view (FOV) is very limited, ranging from 4.5*4.5 to 13*13 mm, which nearly equals 2%-19% area of the entire posterior eyeball. Unlike OCT volume correlation tasks that are impacted by flatten distortion of A-scans [13], some researchers proposed to roughly infer shape changes of the posterior eyeball based on the simplified geometric shapes or non-quantitative topography in 2-dimensional (2D) obtained from retinal landmarks or RPE segmented lines [17,20,27]. However, such 2D expressions can not assist surgeons in making decisions. Therefore, we propose a novel task constructing a 3-dimensional (3D) posterior eyeball shape based on small-FOV OCT images.As far as we know, there is no existing work on 3D eyeball reconstruction just using local OCT images. We refer several 3D reconstruction methods using natural images. For example, NeRF and Multi-View Stereo [15,16] need multiview images, which are difficult to obtain in the ophthalmic field. The point cloud completion task [28] recovers local details based on global information, which is different from our target. As the limited FOV of common retinal imaging devices makes capturing global eyeballs difficult, we reconstruct global shapes from local OCT information. Moreover, the curved shape of the eyeball determines that its imaging information is often concentrated in a local area of the image where most of the pixels are empty, resulting in sparse information. Directly downsampling as in most existing point-based algorithms often leads to surface details lost limited by the number of input points. Therefore, to address the above problems, we first propose to adopt a standard posterior eyeball template to provide medical prior knowledge for the construction; then we propose coordinate transformation to handle the sparse representations of retinal in OCT images in this paper. Our main contributions are summarized as follows: "
Polar Eyeball Shape Net for 3D Posterior Ocular Shape Representation,2,Method,"The overview of our proposed Posterior Eyeball Shape Network (PESNet) is shown in Fig. 1(a). The subarea point cloud (size N 1 * 3) from local OCT images and the template point cloud (size N 2 * 3) from the template OCT images are taken as inputs. N 1 and N 2 are points number. The inputs are transformed to the polar grid with size (B, C, R, U, V ) by our proposed Polar Voxelization Block (PVB), respectively. Then two polar grids are input into the dual-branch architecture of PESNet, whose detail is shown in Fig. 1(b). After a 2D ocular surface map (OSM) is predicted from the network, we convert it to a 3D point cloud as our predicted posterior eyeball shape using inverse polar transformation."
Polar Eyeball Shape Net for 3D Posterior Ocular Shape Representation,2.1,Dual-Branch Architecture of PESNet,"The input point cloud is derived from limited FOV OCT images, and the structures between the local area and the complete posterior eyeball are totally different. Thus, considering the structural regularity of ocular shape in OCT images, we propose to aggregate structural prior knowledge for the reconstruction. As shown in Fig. 1(b), the proposed PESNet is constructed by the Shape regression branch (S-branch) and Anatomical prior branch (A-branch) with the same RBs, whose details are shown in Fig. 1(c). Inspired by ResNet [9], we propose the RBs to gradually extract hierarchical features from the R-channel of input grids with the reduction of the R-dimension. The last layer of RB1 to RB4 uses anisotropic 3D convolution layer with (2 × 3 × 3) kernel, (2, 3, 3) stride, and (0, 1, 1) padding. Features from every RB in S-branch are fused with corresponding features from A-branch by our RFB. For RB5, the last layer adopts a maxpooling layer by setting kernel as (4, 1, 1) to reduce the R-dimension to 1 for reducing the computational cost. Then (1 × 1 × 1) convolution layers are adopted to further reduce the feature channels from 128 to 32. Finally, a 2D OSM with size (B, 1, 1, N, N) is predicted from the dual-branch architecture (B = batch size, N = 180 in this paper) by the last RFB5. For the loss of our PESNet, we adopt the combination of smooth L1 loss L1 smooth and perceptual loss L percep [11] as Loss = αL1 s + L perc , and smooth L1 loss is defined as:where O(u, v) and Ô(u, v) denote the predicted and ground truth (GT) OSM, respectively. The (u, v) and U, V represent the positional index and size of OSM.The α denotes a manual set weight value."
Polar Eyeball Shape Net for 3D Posterior Ocular Shape Representation,2.2,Polar Voxelization Block and R-Wise Fusion Block,"Polar Voxelization Block (PVB). The curve shape of the eyeball determines the sparse layer labels of OCT in cartesian coordinates. Sparse convolution significantly increases memory. Inspired by the shape of the eyeball, we propose to use polar transformation to obtain the dense representation of the input. Due to the hierarchical and regular distribution of RPE points in polar grid, the 3D grid is parameterized into a 2D OSM by anisotropic convolution. Therefore, we design the PVB to perform polar transformation and voxelization jointly for two input point clouds P s and P t and output two 3D grids of voxels G s and G t .Given the center coordinate c = (c x , c y , c z ) of the top en-face slice of OCT images as polar origin, the transformation between the cartesian coordinate (x, y, z) and the polar coordinates (r, u, v) can be expressed as Eq. 2. Besides, given a polar point set P : {(r i , u i , v i )|i = 1, . . . , n} and a 3D polar grid G(r, u, v) ∈ {0, 1}, the voxelization process of PVB is defined in Eq. 3:where u,v and r are the elevation angle, azimuth angle, and radius in polar coordinates. The extra π 2 term is added to make the angle range correspond to the integer index of height and width of OSM. Besides, R,U ,V denote the depth, height, and width of the 3D grid (R = 64, U and V = 180 in our paper), which make grid G hold enough resolution with acceptable memory overhand. In addition, φ is a manual set number to control the depth-dimension of G. Since we choose the center of the top en-face slice as the origin, the RPE voxels distribute in layers along the R-axis of G. Therefore, we can compress the 3D polar grid G(r, u, v) into a 2D OSM along the R-axis of grid using Eq. 4, where OSM (u, v) denotes the pixel value at position (u, v), which represents the radius from the polar origin to an ocular surface point along the elevation and azimuth angles specified by index (u, v). The entire posterior eyeball shape can be reconstructed once given OSM and the center coordinate C. This representation of ocular shape is not only much denser than a conventional voxel-based grid (370 * 300 * 256 in our paper) in cartesian coordinates but also maintains more points and spatial structure than common point-based methods."
Polar Eyeball Shape Net for 3D Posterior Ocular Shape Representation,,R-wise Fusion Block (RFB),"The conventional feature fusion methods (denoted as F in Table 1) often concatenate two feature volumes along a certain axis and simultaneously compute the correlation among R-, U-, and V-dimensions with convolution. They do not consider the feature distribution in the dimension. However, for our task, the hierarchical features distributed in the same channel along R-axis have a higher relevance. Separately computing relevance between features inside the R channel is able to enhance the structure guidance of prior information. Thus, we propose RFBs to fuse the feature volumes from S-branch and A-branch. As shown in Fig. 1(c), the first four blocks (RFB1 to RFB4) first concatenate two feature volumes along the R-dimension and then rearrange the channel order in the R-dimension of combined volume to place the correlated features together. Finally, the relevant hierarchical features from S-and A-branches are fused by a convolution layer with (2 × 3 × 3) kernel and (2, 1, 1) stride followed by Batch-Norm and ReLU layers. The output volume of RFB has the same size as each input one. It's worth noting that the last RFB rearranges the order in featureinstead of R-dimension, since R-dimension is already equal to 1, and the last activation layer is Sigmoid to ensure the intensity values of output OSM ranging in [0, 1]."
Polar Eyeball Shape Net for 3D Posterior Ocular Shape Representation,3,Experiments,"Dataset: We build a Posterior Ocular Shape (POS) dataset based on Ultrawidefield (24 × 20 mm 2 ) swept-source OCT device (BM-400K BMizar; TowardPi Medical Technology). The POS dataset contains 55 eyes of thirty-three healthy participants. Simulating local regions captured by common OCT devices, we sample five subareas from every ultra-widefield sample. Both subarea point clouds and ground truth of OSM are derived from segmented RPE label images from all samples (more details are in supplementary). The POS dataset is randomly split into training, validation, and test sets with 70%, 20% and 10%, respectively. Referring to [4,24], the template is obtained by averaging 7 highquality GT OSMs without scanning defects and gaps from the training set and then perform Gaussian smoothing to reduce textual details until it does not change further. Finally, the averaged OSM is converted into the template point cloud by inverse polar transformation."
Polar Eyeball Shape Net for 3D Posterior Ocular Shape Representation,,Implementation Details:,"The intensity values of all GT OSMs are normalized from [0, 63] to [0, 1] for loss calculation. We use unified center coordinates as the polar origin for polar transformation since all OCT samples are aligned to the macular fovea during acquisition. Since the background pixels in predicted OSM are hardly equal to 0, which cause many noise points in reconstructed point cloud, only intensity values that are not equal to OSM (5,5) are converted to points. During training, we set α in the loss function as a linear decay weight after the first 50 epochs, adam optimizer with a base learning rate of 0.001, and a batch size of 10 for 300 epochs. The model achieving the lowest loss on the validation set is saved for evaluation on the testing set. We implemented our PESNet using the PyTorch framework and trained on 2 A100 GPUs.Evaluation Metrics: Since our proposal employs 2D OSM regression in polar coordinates to achieve 3D reconstruction in cartesian coordinates, we evaluate its performance using both 2D and 3D metrics. Specifically, we use Chamfer Distance (CD) to measure the mean distance between predicted point clouds and its GT point cloud, and Structural Similarity Index Measure (SSIM) as 2D metrics to evaluate the OSM regression performance. Additionally, we introduce point density (Density) as an extra metric to evaluate the number ratio of total points between predicted and GT point clouds, defined as Density = |1 -N pred /N GT |, where N pred is the number of predicted point clouds, N GT is the number of GT point clouds. The lower Density, the fewer defects in the valid region of OSMs."
Polar Eyeball Shape Net for 3D Posterior Ocular Shape Representation,,Method Comparison and Ablation Study:,"The effectiveness of the key components of PESNet is investigated through a comparison and ablation study, as presented in Fig. 2 (other samples are shown in supplementary) and Table 1. To provide a benchmark, we re-implemented PointNet, a pioneering pointbased method (details in the supplementary material). Compared to PointNet (Fig. 2(a)), the visual results in OSM and a reconstructed point cloud of 1-branch or 2-branch networks show more details in the valid region and reasonable shape, with better evaluation metrics, as hierarchical structure information is preserved in the dense representation by PVB. Compared to that by 1-branch (Fig. 2(b)), the visual results by 2-branch networks are much closer to the posterior eyeball shape, as the adopted prior knowledge provides structure assistance for reconstruction. As shown in Fig. 2, after incorporating RFB and unshare-weight, the 2D and 3D figures express the curve structure of the eyeball with more information, especially the macular and disc (in the bottom of the eyeball) are back to    the right position. The visualization shows that multi-state fusion can extract more structural details in R-dimension from multi-scale feature volumes of two branches, which can improve the scope and textual details of the valid regions in OSMs. As shown in Table 1, our 2-branch variant exceeds 1-branch ones by 3.43, 0.19, 0.57 at CD, SSIM, and Density after adding A-branch, which further proves that prior knowledge guides to reconstruct a more complete valid region. If incorporating the RFB, the 3D shape (expressed by CD) can be closer to GT.In our PESNet, we adopt unshare-weight for two branches, which produces the best metrics in the last row. Therefore, our proposed algorithm reconstructs the eyeball close to GT considering the specificity of the eyeball."
Polar Eyeball Shape Net for 3D Posterior Ocular Shape Representation,,Ablation Study About Loss Function:,"We compare different loss functions applied in our task, and the results as shown in Table 2. The results reveal two trends: 1): L1 smooth can ensure the correct optimization direction, impelling the OSMs area other than the background close 1, but we find it cannot effectively learn the accurate scope of OSMs from experiments. 2): L perc can achieve suboptimal results, but is too unstable to keep the right direction of optimization, which highly increase training time and decrease reproducibility. For other combinations, their OSMs show either wrong optimization direction or limited scope of valid regions which are also revealed by metrics. Therefore, our loss function combine the advantages of L1 smooth and L perc , and the extra α makes L perc further descent stably. Our combination obtains the best metrics and visualization results, proving that it is the most suitable choice for our task.Validation on Disease Sample: Although our POS dataset only contains the data of healthy eyeballs, we still test our PESNet on two disease cases: one is high myopia and the other is Diabetic Macular Edema. As shown in Fig. 3, although the template lack of priors from the disease eyeball, the range of pixel value and scope of region are close to GT. The result shows the potential of our PESNet to handle the posterior shape reconstruction for the diseased eyeballs. We are collecting more patients' data."
Polar Eyeball Shape Net for 3D Posterior Ocular Shape Representation,4,Conclusion,"In this study, we introduced the task of posterior eyeball shape 3D reconstruction using small-FOV OCT images, and proposed a Posterior Eyeball Shape Network (PESNet) that utilizes polar coordinates to perform 2D ocular surface map (OSM) regression. Our experiments on a self-made posterior ocular shape dataset demonstrate the feasibility of this novel task and confirm that PESNet is capable of leveraging local and anatomical prior information to reconstruct the complete posterior eyeball shape. Additionally, our validation experiment with disease data highlights the potential of PESNet for reconstructing the eyeballs of patients with ophthalmic diseases."
Polar Eyeball Shape Net for 3D Posterior Ocular Shape Representation,,Fig. 1 .,
Polar Eyeball Shape Net for 3D Posterior Ocular Shape Representation,,Fig. 2 .,
Polar Eyeball Shape Net for 3D Posterior Ocular Shape Representation,,Fig. 3 .,
Polar Eyeball Shape Net for 3D Posterior Ocular Shape Representation,,Table 1 .,
Polar Eyeball Shape Net for 3D Posterior Ocular Shape Representation,,Table 2 .,
Polar Eyeball Shape Net for 3D Posterior Ocular Shape Representation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 18.
ALL-IN: A Local GLobal Graph-Based DIstillatioN Model for Representation Learning of Gigapixel Histopathology Images With Application In Cancer Risk Assessment,1,Introduction,"The examination of tissue and cells using microscope (referred to as histology) has been a key component of cancer diagnosis and prognostication since more than a hundred years ago. Histological features allow visual readout of cancer biology as they represent the overall impact of genetic changes on cells [20].The great rise of deep learning in the past decade and our ability to digitize histopathology slides using high-throughput slide scanners have fueled interests in the applications of deep learning in histopathology image analysis. The majority of the efforts, so far, focus on the deployment of these models for diagnosis and classification [27]. As such, there is a paucity of efforts that embark on utilizing machine learning models for patient prognostication and survival analysis (for example, predicting risk of cancer recurrence or expected patient survival). While prognostication and survival analysis offer invaluable insights for patient management, biological studies and drug development efforts, they require careful tracking of patients for a lengthy period of time; rendering this as a task that requires a significant amount of effort and funding.In the machine learning domain, patient prognostication can be treated as a weakly supervised problem, which a model would predict the outcome (e.g., time to cancer recurrence) based on the histopathology images. Their majority have utilized Multiple Instance Learning (MIL) [8] that is a two-step learning method. First, representation maps for a set of patches (i.e., small fields of view), called a bag of instances, are extracted. Then, a second pooling model is applied to the feature maps for the final prediction. Different MIL variations have shown superior performances in grading or subtype classification in comparison to outcome prediction [10]. This is perhaps due to the fact that MIL-based technique do not incorporate patch locations and interactions as well as tissue heterogeneity which can potentially have a vital role in defining clinical outcomes [4,26].To address this issue, graph neural networks (GNN) have recently received more attention in histology. They can model patch relations [17] by utilizing message passing mechanism via edges connecting the nodes (i.e., small patches in our case). However, most GNN-based models suffer from over smoothing [22] which limits nodes' receptive fields [3]. While local contexts mainly capture cell-cell interactions, global patterns such as immune cell infiltration patterns and tumor invasion in normal tissue structures (e.g., depth of invasion through myometrium in endometrial cancer [1]) could capture critical information about outcome [10]. Hence, locally focused methods are unable to benefit from the coarse properties of slides due to their high dimensions which may lead to poor performance.This paper aims to investigate the potential of extracting fine and coarse features from histopathology slides and integrating them for risk stratification in cancer patients. Therefore, the contributions of this work can be summarized as: 1) a novel graph-based model for predicting survival that extracts both local and global properties by identifying morphological super-nodes; 2) introducing a fine-coarse feature distillation module with 3 various strategies to aggregate interactions at different scales; 3) outperforming SOTA approaches in both risk prediction and patient stratification scenarios on two datasets; 4) publishing two large and rare prostate cancer datasets containing more than 220 graphs for active surveillance and 240 graphs for brachytherapy cases. The code and graph embeddings are publicly available at https://github.com/pazadimo/ALL-IN 2 Related Works"
ALL-IN: A Local GLobal Graph-Based DIstillatioN Model for Representation Learning of Gigapixel Histopathology Images With Application In Cancer Risk Assessment,2.1,Weakly Supervised Learning in Histopathology,"Utilizing Weakly Supervised Learning for modeling histopathology problems has been getting popular due to the high resolution of slides and substantial time and financial costs associated with annotating them as well as the development of powerful deep discriminative models in the recent years [24].Such models are used to perform nuclei segmentation [18], identify novel subtypes [12], or later descendants are even able to pinpoint sub-areas with a high diagnostic value [19]."
ALL-IN: A Local GLobal Graph-Based DIstillatioN Model for Representation Learning of Gigapixel Histopathology Images With Application In Cancer Risk Assessment,2.2,Survival Analysis and GNNs in Histopathology,"MIL-based models have been utilized for outcome prediction [29,32] which can also be integrated with attention-based variants [14]. GNNs due to their structural preserving capacity [28] have drawn attention in various histology domains by constructing the graph on cells or patches. However, current GNN-based risk assessment variants are only focused on short-range interactions [16,17] or consider local contexts [10]. We hypothesize that graph-based models' performance in survival prediction improves by leveraging both fine and coarse properties."
ALL-IN: A Local GLobal Graph-Based DIstillatioN Model for Representation Learning of Gigapixel Histopathology Images With Application In Cancer Risk Assessment,3,Method,"Figure 1 summarizes our proposed end-to-end solution. Below, we have provided details of each module."
ALL-IN: A Local GLobal Graph-Based DIstillatioN Model for Representation Learning of Gigapixel Histopathology Images With Application In Cancer Risk Assessment,3.1,Problem Formulation,"For P n , which is the n-th patient, a set of patches {patch j } M j=1 is extracted from the related whole slide images. In addition, a latent vector z j ∈ R 1×d is extracted from patch j using our encoder network (described in Sect. 3.2) that results in feature matrix Z n ∈ R M ×d for P n . Finally, a specific graph (G n ) for the n-th patient (P n ) can be constructed by assuming patches as nodes. Also, edges are connected based on the patches' k-nearest neighbour in the spatial domain resulting in an adjacency matrix A n . Therefore, for each patient such as P n , we have a graph defined by adjacency matrix A n with size M × M and features matrix Z n (G n = graph(Z n , A n )). We estimate K super-nodes as matrix S n ∈ R K×d representing groups of local nodes with similar properties as coarse features for P n 's slides. The final model ( θ ) with parameters θ utilizes G n and S n to predict the risk associated with this patient:"
ALL-IN: A Local GLobal Graph-Based DIstillatioN Model for Representation Learning of Gigapixel Histopathology Images With Application In Cancer Risk Assessment,3.2,Self-supervised Encoder,"Due to computational limits and large number of patches available for each patient, we utilize a self-supervised approach to train an encoder to reduce the inputs' feature space size. Therefore, We use DINO [9], a knowledge distillation model (KDM), with vision transformer (ViT) [13] as the backbone. It utilizes global and local augmentations of the input patch j and passes them to the student (S θ1,V iT ) and teacher (T θ2,V iT ) models to find their respective representations without any labels. Then, by using distillation loss, it makes the representations' distribution similar to each other. Finally, the fixed weights of the teacher model are utilized in order to encode the input patches."
ALL-IN: A Local GLobal Graph-Based DIstillatioN Model for Representation Learning of Gigapixel Histopathology Images With Application In Cancer Risk Assessment,3.3,Local Graph Neural Network,"GNN's objective is to find new nodes' embeddings via integrating local neighbors' interactions with individual properties of patches. By exploiting the message passing mechanism, this module iteratively aggregates features from neighbors of each vertex and generates the new node representations. We employ two graph convolution isomorphism operators (GINconv) [30] with the generalized form as:where is a small positive value and I is the identity matrix. Also, φ denotes the weights of two MLP layers. X n ∈ R M ×d and X n ∈ R M ×d are GINconv's input and output feature matrices for P n , which X n equals Z n for the first layer."
ALL-IN: A Local GLobal Graph-Based DIstillatioN Model for Representation Learning of Gigapixel Histopathology Images With Application In Cancer Risk Assessment,3.4,Super-Nodes Extractor,"In order to find the coarse histo-morphological patterns disguised in the local graph, we propose extracting K Super-nodes, which each represents a weighted cluster of further processed local features. Intuitively, the number of super-nodes K should not be very large or small, as the former encourages them to only represent local clusters and the latter leads to larger clusters and loses subtle details. We exploit the minCUT [5] idea to extract super-nodes in a differentiable process after an auxiliary GINconv to focus more on large-scale interactions and to finally learn the most global correlated super-nodes. Inspired by the relaxation form of the known K-way minCUT problem, we create a continuous cluster matrix C n ∈ R M ×K using MLP layers and can finally estimate the super-nodes features (S n ∈ R M ×d ) as:where W 1 , W 2 are MLPs' weights. Hence, the extracted nodes are directly dependent on the final survival-specific loss. In addition, two additional unsupervised weighted regularization terms are optimized to improve the process:MinCut Regularizer. This term is motivated by the original minCUT problem and intends to solve it for the the patients' graph. It is defined as:where D n is the diagonal degree matrix for A n . Also, T r(.) represents the trace of matrix and A n,norm is the normalized adjacency matrix. R minCU T 's minimum value happens when T r(Therefore, minimizing R minCU T causes assigning strongly similar nodes to a same super-node and prevent their association with others.Orthogonality Regularizer. R minCU T is non-convex and potent to local minima such as assigning all vertexes to a super-node or having multiple super-nodes with only a single vertex. R orthogonal penalizes such solutions and helps the model to distribute the graph's features between super-nodes. It can be formulated as:where ||.|| F is the Frobenius norm, and I is the identity matrix. This term pushes the model's parameters to find coarse features that are orthogonal to each other resulting in having the most useful global features. Overall, utilizing these two terms encourages the model to extract supernodes by leaning more towards the strongly associated vertexes and keeping them against weakly connected ones [5], while the main survival loss still controls the global extraction process."
ALL-IN: A Local GLobal Graph-Based DIstillatioN Model for Representation Learning of Gigapixel Histopathology Images With Application In Cancer Risk Assessment,3.5,Fine-Coarse Distillation,"We propose our fine-coarse morphological feature distillation module to leverage all-scale interactions in the final prediction by finding a local and a global patientlevel representations ( ĥl,n , ĥg,n ). Assume that X n ∈ R M ×d and S n ∈ R K×d are the feature matrices taken from local GNN (Sect. 3.3) and super-nodes for P n , respectively. We explore 3 different attention-based feature distillation strategies for this task, including:-Dual Attention (DA): Two gated self-attention modules for local and global properties with separate weights (W φ,l , W φ,g , W k,l , W k,g , W q,l , W q,g ) are utilized to find patches scores α l ∈ R 1×M and α g ∈ R 1×K and the final features ( ĥl,n , ĥg,n ) as:) where x n,i and s n,i are rows of X n and S n , respectively, and the final representation ( ĥ) is generated as ĥ = cat( ĥl , ĥg ).-Mixed Guided Attention (MGA): In the first strategy, the information flows from local and global features to the final representations in parallel without mixing any knowledge. The purpose of this policy is the heavy fusion of fine and coarse knowledge by exploiting shared weights (W φ,shared , W k,shared , W q,shared , W v,shared ) in both routes and benefiting from the guidance of local representation on learning the global one by modifying Eq. ( 7) to:-Mixed Co-Attention (MCA): While the first strategy allows the extreme separation of two paths, the second one has the highest level of mixing information. Here, we take a balanced policy between the independence and knowledge mixture of the two routes by only sharing the weights without using any guidance."
ALL-IN: A Local GLobal Graph-Based DIstillatioN Model for Representation Learning of Gigapixel Histopathology Images With Application In Cancer Risk Assessment,4,Experiments and Results,
ALL-IN: A Local GLobal Graph-Based DIstillatioN Model for Representation Learning of Gigapixel Histopathology Images With Application In Cancer Risk Assessment,4.1,Dataset,"We utilize two prostate cancer (PCa) datasets to evaluate the performance of our proposed model. The first set (PCa-AS) includes 179 PCa patients who were managed with Active Surveillance (AS). Radical therapy is considered overtreatment in these patients, so they are instead monitored with regular serum prostate-specific antigen (PSA) measurements, physical examinations, sequential biopsies, and magnetic resonance imaging [23]. However, AS may be over-or under-utilized in low-and intermediate-risk PCa due to the uncertainty of current methods to distinguish indolent from aggressive cancers [11]. Although majority of patients in our cohort are classified as low-risk based on NCCN guidelines [21], a significant subset of them experienced disease upgrade that triggered definitive therapy (range: 6.2 to 224 months after diagnosis).The second dataset (PCa-BT) includes 105 PCa patients with low to high risk disease who went through brachytherapy. This treatment involves placing a radioactive material inside the body to safely deliver larger dose of radiation at one time [25]. The recorded endpoint for this set is biochemical recurrence with time to recurrence ranging from 11.7 to 56.1 months. We also utilized the Prostate cANcer graDe Assessment (PANDA) Challenge dataset [7] that includes more than 10,000 PCa needle biopsy slides (no outcome data) as an external dataset for training the encoder of our model."
ALL-IN: A Local GLobal Graph-Based DIstillatioN Model for Representation Learning of Gigapixel Histopathology Images With Application In Cancer Risk Assessment,4.2,Experiments,"We evaluate the models' performance in two scenarios utilizing several objective metrics. Implementation details are available in supplementary material.Hazard (Risk) Prediction. We utilize concordance-index (c-index) that measures the relative ordering of patients with observed events and un-censored cases relative to censored instances [2]. Using c-index, we compare the quality of hazard ranking against multiple methods including two MIL (DeepSet [31], AMIL [14]) and graph-based (DGC [17] and Patch-GCN [10]) models that were utilized recently for histopathology risk assessment. C-index values are available in Table 1. The proposed model with all strategies outperforms baselines across all sets and is able to achieve 0.639 and 0.600 on PCa-AS and PCa-BT, while the baselines, at best, obtain 0.555, and 0.572, respectively. Statistical tests (paired t-test) on c-indices also show that our model is statistically better than all baselines in PCa-AS and also superior to all models, except DGC, in PCa-BT. Superior performance of our MCA policy implies that balanced exploitation of fine and coarse features with shared weights may provide more robust contextual information compared to using mixed guided information or utilizing them independently.Patient Stratification. The capacity of stratifying patients into risk groups (e.g., low and high risk) is another criterion that we employ to assess the utility of models in clinical practice. We evaluate model performances via Kaplan-Meier curve [15] (cut-off set as the ratio of patients with recurrence within 3 years of therapy initiation for PCa-BT and the ratio of upgraded cases for PCa-AS), LogRank test [6] (with 0.05 as significance level), and median outcome associated with risk groups (Table 1 and Fig. 2). Our model stratified PCa-AS patients into high-and low-risk groups with median time to progression of 36.5 and 131.7 months, respectively. Moreover, PCa-BT cases assigned to high-and low-risk groups have median recurrence time of 21.86 and 35.7 months. While none of the baselines are capable of assigning patients into risk groups with statistical significance, our distillation policies achieve significant separation in both PCa-AS and PCa-BT datasets; suggesting that global histo-morphological properties improve patient stratification performance. Furthermore, our findings have significant clinical implications as they identify, for the first time, highrisk prostate cancer patients who are otherwise known to be low-risk based on clinico-pathological parameters. This group should be managed differently from the rest of the low-risk prostate cancer patients in the clinic. Therefore, providing evidence of the predictive (as opposed to prognostic) clinical information that our model provides. While a prognostic biomarker provides information about a patient's outcome (without specific recommendation on the next course of action), a predictive biomarker gives insights about the effect of a therapeutic intervention and potential actions that can be taken.Ablation Study. We perform ablation study (Table 2) on various components of our framework including local nodes, self-supervised ViT-based encoder, and most importantly, super-nodes in addition to fine-coarse distillation module. Although our local-only model is still showing superior results compared to baselines, this analysis demonstrates that all modules are essential for learning the most effective representations. We also assess the impact of our ViT on the baselines (full-results in appendix), showing that it can, on average, improve their performance by an increase of ∼ 0.03 in c-index for PCa-AS. However, the best baseline with ViT still has poorer performance compared to our model in both datasets, while the number of parameters (reported for ViT embeddings' size in Table 1) in our full-model is about half of this baseline. Achieving higher c-indices in our all model versions indicates the important role of coarse features and global context in patient risk estimation in addition to local patterns. "
ALL-IN: A Local GLobal Graph-Based DIstillatioN Model for Representation Learning of Gigapixel Histopathology Images With Application In Cancer Risk Assessment,,Fig. 1 .,
ALL-IN: A Local GLobal Graph-Based DIstillatioN Model for Representation Learning of Gigapixel Histopathology Images With Application In Cancer Risk Assessment,,Fig. 2 .,
ALL-IN: A Local GLobal Graph-Based DIstillatioN Model for Representation Learning of Gigapixel Histopathology Images With Application In Cancer Risk Assessment,,Table 1 .,
ALL-IN: A Local GLobal Graph-Based DIstillatioN Model for Representation Learning of Gigapixel Histopathology Images With Application In Cancer Risk Assessment,,Table 2 .,
ALL-IN: A Local GLobal Graph-Based DIstillatioN Model for Representation Learning of Gigapixel Histopathology Images With Application In Cancer Risk Assessment,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 74.
A One-Class Variational Autoencoder (OCVAE) Cascade for Classifying Atypical Bone Marrow Cell Sub-types,1,Introduction,"Bone marrow (BM) lineage classification and differential counting are at the very base of most diagnoses or clinical management of hematopoietic disorders [7]. Despite advances in cytogenetics, immunophenotyping and molecular technology, morphology of BM cellular lineages is still the gold standard in hematology. This cell morphometry estimation is performed by highly trained and experienced proffesionals and yet, this activity has been reported with high intra-and inter-operator variability [10,13], and even more variable when visually identifying atypical BM cells [11,32]. This examination not only defines the stage of proliferating disorders, but also it characterizes chronic diseases associated with particular distributions of leukocyte sub-types or atypical BM cells [7]. On the one hand cell morphological examination is fully dependent on the operator experience-skills and, on the other hand not objective quantitative measurements are available [1].In order to provide a more quantitative BM cell characterization, different strategies have been proposed, either extracting hand-crafted features or applying deep learning based approaches. Importantly, most of them has been focused on leukemia detection [3,25], which narrows application of automated strategies to a single hematological disease. Regarding classic machine learning approaches using hand-crafted features, different representation spaces have been used in the multiclass task, including color [18], shape [19] and texture features [17]. These features are commonly combined with classification methods (support vector machines-SVM, random forest, linear discriminant analysis-LDA) [5]. Currently deep learning strategies are the best to discriminate only leukemia cells in peripheral blood images with the ResNext [22], the VGG-Net [4] or customized networks [4]. A much more complicated task relies on the discrimination of atypical cells which for peripheral blood samples has been tackled using a ResNext network (accuracy = 0.99) [22]. Likewise, they have been discriminated as part of a multiclass BM cell problem with more sub-types than the atypical ones, using the same ResNext architecture (accuracy = 0.90) [21], or the You Only Look Once (YOLO) [26]. Additionally, the emerging transformers have also being used to differentiate multiple cell sub-types in BM, particularly by using a CoAtNet [9] which integrates transformer-attention layers and ConvNet deep neural network architecture, and provides state-of-the-art performance (accuracy = 0.93) [27].Unlike previous works, the presented approach is focused in quantifying changes in the most common pathological atypical BM cell subtypes, namely myelocytes (MYB), blasts (BLA), promyelocytes (PMO), and erythroblast (EBO), achieving a quantitative strategy to differentiate atypical BM cells regardless the disease they are associated with. Differentiating these classes is crucial since the proposed 4 cell subtypes are the ones which must be counted for a correct diagnosis and morphological characterization of acute myeloid leukemia and myelodysplasic disease [28], currently the most common and aggressive hematological disorders in adults. This classification task is so complex that even common immunohistochemical hematology stains like CD117 may also misclassify these cell sub-types [23]. The introduced methodology builds upon One-class variational autoencoders (OCVAE) and presents a new atypical BM cell representation by concatenating the latent spaces provided by 4 specialized OCVAE with the same architecture. In contrast to previous OCVAE applications focused on one-class training for binary classification [15], or anomaly detection based on the reconstruction quality [29], the introduced methodology presents a cascade of OCVAEs as feature extractor in a 4 class differentiation task. The methodology setting process splits data into two disjoint subsets, where the first one is used to set the OCVAEs parameters for capturing the particular patterns of each cell sub-type, separately. The second subset feeds all the trained OCVAEs for building a concatenated latent-space that serves to train different classifiers (support vector machine-SVM with linear and RBF kernel, and random forest), reporting accuracy, precision, recall and f1-score, as performance metrics. This approach was evaluated on a subset (n = 26, 000) of a public image database [20], demonstrating to outperform previously published strategies, while requiring a lower number of training images."
A One-Class Variational Autoencoder (OCVAE) Cascade for Classifying Atypical Bone Marrow Cell Sub-types,2,Materials and Methods,
A One-Class Variational Autoencoder (OCVAE) Cascade for Classifying Atypical Bone Marrow Cell Sub-types,2.1,The BM Smear Image Dataset,"All experiments in the presented work were performed using a subset of the public image database ""An Expert-Annotated Dataset of Bone Marrow Cytology in Hematologic Malignancies"" [20]. This database is composed of 171, 374 single-cells annotated by type, coming from the BM smears of a group of 945 patients covering a set of diseases, yet the individual hematological disease is not informed. All images were acquired using bright-field microscope with ×40 magnification and oil immersion, applied to May-Grünwald-Giemsa/Pappenheim stained samples. Each image was annotated by an expert morphologist at the Munich Leukemia Laboratory (MLL), assigning one out of 21 possible classes, including the atypical BM cells that the herein presented methodology is working with, i.e., myelocytes (MYB), blasts (BLA), promyelocytes (PMO), and erythroblast (EBO). Particularly, the subset herein used was composed of a balanced version of the aforementioned classes, making a new dataset of 26, 000 images (6, 500 for each of the four classes)."
A One-Class Variational Autoencoder (OCVAE) Cascade for Classifying Atypical Bone Marrow Cell Sub-types,,Data Use Declaration.,"The complete version of the public database used for validating the proposed strategy is provided by Matek et. al., under the TCIA Data Usage Policy and Restrictions, and it is publicly available at TCIA platform, https://doi.org/10.7937/TCIA.AXH3-T579. No ethical compliance statement is presented in this document since it is covered by the original dataset publication [20]."
A One-Class Variational Autoencoder (OCVAE) Cascade for Classifying Atypical Bone Marrow Cell Sub-types,2.2,OCVAE Atypical BM Cell Differentiation Method,"Single Cell Image Pre-processing. The first step of the presented approach is to apply a color-space transformation from RGB to Lab, for reducing possible illumination and stain variability effects. In the Lab color representation, the luminance component is isolated in the L-channel, and the remaining components (a and b) are more robust to the mentioned sources of variability [31]. Particularly, for the herein introduced work, only the b-channel is used to represent the whole set of images, taking into account that non-white blood cell image elements and background are homogenized in this particular Lab component.Atypical BM Cell OCVAE Latent Space Representation. The main part of the introduced atypical BM cell characterization is based on a latent space representation of a variational autoencoder (VAE) bottleneck [16]. As commonly found in autoencoder strategies, the VAE encodes input data x by forcing dimensionality reduction to a bottleneck (latent space z) and decodes the compressed signal (x = f decoding (z)), aiming to minimize the output reconstruction error. The latent space is described by a set of parameterized distributions (mean-μ and variance-σ), and imposes these distributions to be as close as possible to unitary Gaussians. This guarantees a continuous approximation of the latent space by the Parzen theorem, i.e. z∼ N (0, 1), in terms of the Kullback-Leibler divergence (KLD) between the parametric posterior and the true posterior distributions.Furthermore, to enhance VAE cell description, the presented approach takes advantage of the dedicated one-class characterization to force inter-class separability but increasing intra-class proximity. One-class classification strategies have been mainly used to find outlier samples in a given data space [6,24], with successful application in image related tasks [29], even using the reconstruction error of variational autoencoders (VAE) [14]. As shown in Fig. 1, unlike previously published strategies that are mainly set to identify anomalies or fake samples by using the autoencoder reconstruction error in a binary task [2], the presented strategy uses the regularized representation of a VAE bottleneck, without using the reconstruction quality, but the latent space ability to separate the proposed classes. The presented strategy trains a set of 4 specialized VAEs, each adjusted for estimating a representation of one atypical cell class (OCVAE). After that, all testing images pass through each specialized OCVAE (OCVAEs cascade), in a blind characterization process. Finally, all test encoded outputs, composed of means and variances, are concatenated in a single feature space which feeds different classifiers that discriminate image samples of the four atypical BM cell types. Additionally, the obtained feature matrix is normalized after concatenation, decreasing the bias possibility, given the separately trained OCVAEs.In a more detailed description, the implemented VAE encoder is composed of 3 convolutional and 2 max-pooling, intercalated layers, followed by a flatten, a fully connected, and a lambda layer which is customized for sampling the latent space in terms of means μ and variances σ. Here the bottleneck is set to compress the input layer dimension (256 × 256 × 1) in terms of 64 Gaussian distributions, i.e., μ i:{1-64} and σ i:{1-64} . The decoder architecture follows similar encoder's layer organization (3 convolutional and 3 up-sampling layers), but returning the original dimensionality to the reconstructed images.Evaluation. The introduced concatenated OCVAE representation is quantitatively evaluated by classifying the four proposed classes (PMO, BLA, MYB, EBO). The OCVAE cascade training is carried out by using 80% of the dataset (n = 20, 800), i.e., parameters of each specialized OCVAE model are found by using 5, 200 images coming from each of the proposed classes. The remaining 20% of the dataset (n = 5, 200), composed of equal number of cells per class (n = 1, 300), is used to obtain the feature space for evaluating the presented methodology while assuring independence to the parameterization image set. This independent data partition feeds the previously obtained specialized OCVAE encoder models, and the bottleneck values are concatenated to build an atypical BM cell representation. Afterward, the feature concatenation is used to train three different classifiers (SVM with linear and RBF kernels, and Random Forest) for differentiating cell types that compose the sample space. This experiment uses five iterations of a five-fold cross validation over the obtained feature space (20% of images), for reducing possible batch effect. Mean accuracy, precision, recall and f1-score, with their correspondent standard deviations, are presented as performance metrics. Finally, the best classifier, selected based on the performance metrics, is optimized by using different OCVAE parameter combinations as inputs, i.e., latent-space distribution means together with the corresponding variances, only means or only variances.A second experiment is done to provide a baseline comparison in classifying the 4 atypical cell classes, by using different strategies reported in the literature for this task, including classical handcrafted image features and deep neural networks. For this evaluation procedure, classical image processing descriptors were included for providing a classification strategy that depends on both, the interpretability of the feature representation space and the classifier, like the one introduced in this work. This handcrafted representation comprises a set of 144 image features obtained from nucleus and cytoplasm as separated cell elements, and includes RGB-color space intensity statistics (mean, variance, kurtosis, skweness, entropy), Gray-level-Co-occurrence matrix statistics (contrast, dissimilarity, homogeneity, energy, correlation, angular second moment, Minkowski-Bouligand dimension), and shape descriptors (convexity, compactness, elongation, eccentricity, roundness, solidity, area, perimeter). All these features were used to train different classifiers from which an SVM classifier (linear kernel) provides the best performance with the proposed setup. Regarding deep learning approximations, different architectures were used, including two bench marking options Xception [8], ResNet50 [12], the network with the best published results on a related task based on attention-integrated network CoAtnet [9], and the ResNext [30] which was proposed by the database authors [21]. Furthermore, Xception and ResNet50 networks were trained by using imagenet weights along 30 epochs, while ResNext [21] and CoAtnet [27] evaluation uses the weights provided by the corresponding authors. Regarding the experimental setup, this comparison experiment follows the same data organization as presented in the first experiment, i.e., using 80% and 20% of the data, for training and testing respectively."
A One-Class Variational Autoencoder (OCVAE) Cascade for Classifying Atypical Bone Marrow Cell Sub-types,3,Results and Discussion,"The results of the first experiment are shown in Table 1, where the overall classification performance of this multi-class problem demonstrates the best discrimination results were achieved with a SVM classifier (linear kernel), i.e., mean accuracy and recall of 0.881. Interestingly, regardless the implemented classifier, all the obtained performance values are higher than 0.86, indicating that differentiation stability relies on the concatenated OCVAE characterization space. In addition, as presented in Table 2, the OCVAE SVM-linear kernel results can be optimized by feeding the classifier only with the OCVAE latent variances, leading to an improvement of almost 0.06 in all metrics (accuracy = 0.938, precision = 0.935, recall = 0.935, f1-score = 0.932), when comparing the same classifier but using the whole latent space (means and variances). Finally, Fig. 2 presents the results per-class by evaluating the SVM classifier with a linear-kernel, using only variances. Such results evidenced model stability regardless the atypical cell class, with all metrics going above 0.93.Finally, the baseline experiment results, presented in the table 2, demonstrates the performance achieved by ResNet50 (accuracy = 0.624, precision = 0.24, recall = 0.25 and f1-score = 0.248) and Xception network (accuracy = 0.708, precision = 0.697, recall = 0.899, f1-score = 0.72) are lower than the obtained with the proposed strategy. In contrast, handcrafted features (accuracy = 0.843, precision = 0.688, recall = 0.684, f1-score = 0.685) ResNext network (accuracy = 0.79, precision = 0.78, recall = 0.74, f1-score = 0.72), and   transformer-integrated network (CoAtnet-20, 000 images per class), according to the corresponding authors report [27]. Furthermore, the presented approach is computationally simpler and has a smaller risk of overfitting, a frequently reported problem when using data augmentation that may affect model generalization. This may be particularly true with myelocytes class since the set of 6, 557 images is converted into 20, 000. Finally, the mean accuracy improvement (3%), with respect to the state of the art (OCVAE acc= 0.938, Coatnet acc= 0.908), is fair enough for comparison purposes, given the low standard deviation of the proposed approach (acc std= 0.006, f1 std= 0.006, prec std= 0.0058).Besides the classification performance, it is important to highlight that these results are obtained by the OCVAE bottleneck regularization, which reduces variance inflation and maximises the compactness of the feature space [15]. The previously mentioned OCVAE advantages, and the generative properties of this regularized latent-space prevent overfitting, but also as suggested by the results, provide a representation that keeps closer similar image concepts (cells that share similar patterns)."
A One-Class Variational Autoencoder (OCVAE) Cascade for Classifying Atypical Bone Marrow Cell Sub-types,4,Conclusions,"This work presents an atypical BM cell characterization strategy, which uses a concatenation of OCVAE's bottleneck parameters as a cell representation space. The combination of this space and an SVM classifier, demonstrates to successfully discriminate the 4 most common atypical cell types (PMO, BLA, MYO, EBO), while outperforms previously published strategies, with lower requirement of training images. This approach provides a tool for identifying these cell classes regardless the disease they are coming from, increasing the possibility of aiding differential blood counting in the presence of pathological conditions. Future work includes an independent evaluation by using other public databases and a more exhaustive baseline experimentation."
A One-Class Variational Autoencoder (OCVAE) Cascade for Classifying Atypical Bone Marrow Cell Sub-types,,Fig. 1 .,
A One-Class Variational Autoencoder (OCVAE) Cascade for Classifying Atypical Bone Marrow Cell Sub-types,,Fig. 2 .,
A One-Class Variational Autoencoder (OCVAE) Cascade for Classifying Atypical Bone Marrow Cell Sub-types,,Table 1 .,
A One-Class Variational Autoencoder (OCVAE) Cascade for Classifying Atypical Bone Marrow Cell Sub-types,,SVM linear kernel 0.881 (0.006) 0.882 (0.005) 0.881 (0.006) 0.882 (0.005),
A One-Class Variational Autoencoder (OCVAE) Cascade for Classifying Atypical Bone Marrow Cell Sub-types,,Table 2 .,
SegNetr: Rethinking the Local-Global Interactions and Skip Connections in U-Shaped Networks,1,Introduction,"Medical image segmentation has been one of the key aspects in developing automated assisted diagnosis systems, which aims to separate objects or structures in medical images for independent analysis and processing. Normally, segmentation needs to be performed manually by professional physicians, which is time-consuming and error-prone. In contrast, developing computer-aided segmentation algorithms can be faster and more accurate for batch processing. The approach represented by U-Net [1] is a general architecture for medical image segmentation, which generates a hierarchical feature representation of the image through a top-down encoder path and uses a bottom-up decoder path to map the learned feature representation to the original resolution to achieve pixel-by-pixel classification. After U-Net, U-shaped methods based on Convolutional Neural Networks (CNN) have been extended for various medical image segmentation tasks [2][3][4][5][6][7][8][9]. They either enhance the feature representation capabilities of the encoder-decoder or carefully design the attention module to focus on specific content in the image. Although these extensions can improve the benchmark approach, the local nature of the convolution limits them to capturing long-term dependencies, which is critical for medical image segmentation. Recently, segmentation methods based on U-shaped networks have undergone significant changes driven by Transformer [10,11]. Chen et al [12] proposed the first Transformer-based U-shaped segmentation network. Cao et al [13] extended the Swin Transformer [14] directly to the U-shaped structure. The above methods suffer from high computational and memory cost explosion when the feature map size becomes large. In addition, some researchers have tried to build Hybrid Networks by combining the advantages of CNN and Transformer, such as UNeXt [15], TransFuse [16], MedT [17], and FAT-Net [18]. Similar to these works, we redesign the window-based local-global interaction and insert it into a pure convolutional framework to compensate for the deficiency of convolution in capturing global features and to reduce the high computational cost arising from self-attention operations.Skip connection is the most basic operation for fusing shallow and deep features in U-shaped networks. Considering that this simple fusion does not fully exploit the information, researchers have proposed some novel ways of skip connection [19][20][21][22]. UNet++ [19] design a series of dense skip connections to reduce the semantic gap between the encoder and decoder sub-network feature maps. SegNet [20] used the maximum pooling index to determine the location information to avoid the ambiguity problem during up-sampling using deconvolution. BiO-Net [21] proposed bi-directional skip connections to reuse building blocks in a cyclic manner. UCTransNet [22] designed a Transformer-based channel feature fusion method to bridge the semantic gap between shallow and deep features. Our approach focuses on the connection between the spatial locations of the encoder and decoder, preserving more of the original features to help recover the resolution of the feature map in the upsampling phase, and thus obtaining a more accurate segmentation map.By reviewing the above multiple successful cases based on U-shaped structure, we believe that the efficiency and performance of U-shaped networks can be improved by improving the following two aspects: (i) local-global interactions. Often networks need to deal with objects of different sizes in medical images, and local-global interactions can help the network understand the content of the images more accurately. (ii) Spatial connection between encoder-decoder. Semantically stronger and positionally more accurate features can be obtained using the spatial information between encoder-decoders. Based on the above analysis, this paper rethinks the design of the U-shaped network. Specifically, we construct lightweight SegNetr (Segmentation Network with Transformer) blocks to dynamically learn local-global information over non-overlapping windows and maintain linear complexity. We propose information retention skip connection (IRSC), which focuses on the connection between encoder and decoder spatial locations, retaining more original features to help recover the resolution of the feature map in the up-sampling phase. In summary, the contributions of this paper can be summarized as follows: 1) We propose a lightweight U-shape SegNetr segmentation network with less computational cost and better segmentation performance. 2) We investigate the potential deficiency of the traditional U-shaped framework for skip connection and improve a skip connection with information retention. 3) When we apply the components proposed in this paper to other U-shaped methods, the segmentation performance obtains a consistent improvement. "
SegNetr: Rethinking the Local-Global Interactions and Skip Connections in U-Shaped Networks,2,Method,"As shown in Fig. 1, SegNetr is a hierarchical U-shaped network with important components including SegNetr blocks and IRSC. To make the network more lightweight, we use MBConv [24] as the base convolutional building block. SegNetr blocks implement dynamic local-global interaction in the encoder and decoder stages. Patch merging [14] is used to reduce the resolution by a factor of two without losing the original image information. IRSC is used to fuse encoder and decoder features, reducing the detailed information lost by the network as the depth deepens. Note that by changing the number of channels, we can get the smaller version of SegNetr-S (C = 32) and the standard version of SegNetr (C = 64). Next, we will explain in detail the important components in SegNetr."
SegNetr: Rethinking the Local-Global Interactions and Skip Connections in U-Shaped Networks,2.1,SegNetr Block,"The self-attention mechanism with global interactions is one of the keys to Transformer's success, but computing the attention matrix over the entire space requires a quadratic complexity. Inspired by the window attention method [14,23], we construct SegNetr blocks that require only linear complexity to implement local-global interactions. Let the input feature map be X ∈ R H×W ×C . We first extract the feature X MBConv ∈ R H×W ×C using MBConv [24], which provides non-explicit position encoding compared to the usual convolutional layer.Local interaction can be achieved by calculating the attention matrix of non-overlapping small patches (P for patch size). First, we divide X MBConv into a series of patches ( H×W P ×P , P, P, C) that are spatially continuous (Fig. 1 shows the patch size for P = 2) using a computationally costless local partition (LP) operation. Then, we average the information of the channel dimensions and flatten the spatial dimensions to obtain ( H×W P ×P , P × P ), which is fed into the FFN [11] for linear computation. Since the importance of the channel aspect is weighed in MBConv [24], we focus on the computation of spatial attention when performing local interactions. Finally, we use Softamx to obtain the spatial dimensional probability distribution and weight the input features X MBConv . This approach is not only beneficial for parallel computation, but also focuses more purely on the importance of the local space.Considering that local interactions are not sufficient and may have underfitting problems, we also design parallel global interaction branches. First, we use the global partition (GP) operation to aggregate non-contiguous patches on the space. GP adds the operation of window displacement to LP with the aim of changing the overall distribution of features in space (The global branch in Fig. 1 shows the change in patch space location after displacement). The displacement rules are one window to the left for odd patches in the horizontal direction (and vice versa for even patches to the right), and one window up for odd patches in the vertical direction (and vice versa for even patches down). Note that the displacement of patches does not have any computational cost and only memory changes occur. Compared to the sliding window operation of [14], our approach is more global in nature. Then, we decompose the spatially shifted feature map into 2P ( H×W 2P ×2P , 2P, 2P, C) patches and perform global attention computation (similar to the local interaction branch). Even though the global interaction computes the attention matrix over a larger window relative to the local interaction operation, the amount of computation required is much smaller than that of the standard self-attention model.The local and global branches are finally fused by weighted summation, before which the feature map shape needs to be recovered by LP and GP reversal operations (i.e., local reverse (LR) and global reverse (GR)). In addition, our approach also employs efficient designs of Transformer, such as Norm, feedforward networks (FFN) and residual connections. Most Transformer models use fixed-size patches [11][12][13][14]24], but this approach limits them to focus on a wider range of regions in the early stages. This paper alleviates this problem by applying dynamically sized patches. In the encoder stage, we compute local attention using patches of (8, 4, 2, 1) in turn, and the global branch expands patches to the size of (16, 8, 4, 2). To reduce the hyper-parameter setting, the patches of the decoder stage are of the same size as the encoder patches of the corresponding stage."
SegNetr: Rethinking the Local-Global Interactions and Skip Connections in U-Shaped Networks,2.2,Information Retention Skip Connection,"Figure 2 shows three different types of skip connections. U-Net splices the channel dimensions at the corresponding stages of the encoder and decoder, allowing the decoder to retain more high-resolution detail information when performing upsampling. SegNet assists the decoder to recover the feature map resolution by retaining the position information of the down-sampling process in the encoder. We design the IRSC to consider both of these features, i.e., to preserve the location information of encoder features while achieving the fusion of shallow and deep features. Specifically, the patch merging (PM) operation in the encoder reduces the resolution of the input feature map X in ∈ R H×W ×C to twice the original one, while the channel dimension is expanded to four times the original one to obtain. The essence of the PM operation is to convert the information in the spatial dimension into a channel representation without any computational cost and retaining all the information of the input features. The patch reverse (PR) in IRSC is used to recover the spatial resolution of the encoder, and it is a reciprocal operation with PM. We alternately select half the number of channels of X P M (i.e., H 2 × W 2 × 2C) as the input of PR, which can reduce the redundant features in the encoder on the one hand and align the number of feature channels in the decoder on the other hand. PR reduces the problem of information loss to a large extent compared to traditional upsampling methods, while providing accurate location information. Finally, the output features X P R ∈ R H×W × C 2 of PR are fused with the up-sampled features of the decoder for the next stage of learning."
SegNetr: Rethinking the Local-Global Interactions and Skip Connections in U-Shaped Networks,3,Experiments and Discussion,"Datasets. To verify the validity of SegNetr, we selected four datasets, ISIC2017 [25], PH2 [26], TNSCUI [27] and ACDC [28], for benchmarking. ISIC2017 consists of 2000 training images, 200 validation images, and 600 test images. The PH2 and ISIC2017 tasks are the same, but this dataset contains only 200 images without any specific test set, so we use a five-fold cross-validation approach to validate the different models. The TNSCUI dataset has 3644 ultrasound images of thyroid nodules, which we randomly divided into a 6:2:2 ratio for training, validation, and testing. The ACDC contains Cardiac MRI images from 150 patients, and we obtained a total of 1489 slice images from 150 3D images, of which 951 were used for training and 538 for testing. Unlike the three datasets mentioned above, the ACDC dataset contains three categories: left ventricle (LV), right ventricle (RV), and myocardium (Myo). We use this dataset to explore the performance of different models for multi-category segmentation.Implementation Details. We implement the SegNetr method based on the PyTorch framework by training on an NVIDIA 3090 GPU with 24 GB of memory. Use the Adam optimizer with a fixed learning rate of 1e-4. All networks use a cross-entropy loss function and an input image resolution of 224 × 224, and training is stopped when 200 epochs are iteratively optimized. We use the source code provided by the authors to conduct experiments with the same dataset, and data enhancement strategy. In addition, we use the IoU and Dice metrics to evaluate the segmentation performance, while giving the number of parameters and GFLOPs for the comparison models.  1, we compared SegNetr with the baseline U-Net and eight other state-of-the-art methods [5,12,13,15,[18][19][20]29]. On the ISIC2017 dataset, SegNetr and TransUNet obtained the highest IoU (0.775), which is 3.9% higher than the baseline U-Net. Even SegNetr-S with a smaller number of parameters can obtain a segmentation performance similar to that of its UNeXt-L counterpart. By observing the experimental results of PH2, we found that the Transformer-based method Swin-UNet segmentation has the worst performance, which is directly related to the data volume of the target dataset. Our method obtains the best segmentation performance on this dataset and keeps the overhead low. Although we use an attention method based on window displacement, the convolutional neural network has a better inductive bias, so the dependence on the amount of data is smaller compared to Transformer-based methods such as Swin-UNet or TransUNet. TNSCUI and ACDC Results. As shown in Table 2, SegNetr's IoU and Dice are 1.6% and 0.8 higher than those of the dual encoder FATNet, respectively, while the GFLOPs are 32.65 less. In the ACDC dataset, the left ventricle is easier to segment, with an IoU of 0.861 for U-Net, but 1.1% worse than SegNetr. The myocardium is in the middle of the left and right ventricles in an annular pattern, and our method is 0.6% higher IoU than the EANet that focuses on the boundary segmentation mass. In addition, we observe the segmentation performance of the four networks UNeXt, UNeXt-L, SegNetr-S and SegNetr to find that the smaller parameters may limit the learning ability of the network. The proposed method in this paper shows competitive segmentation performance on all four datasets, indicating that our method has good generalization performance and robustness. Additional qualitative results are in the supplementary.In addition, Fig. 3 provides qualitative examples that demonstrate the effectiveness and robustness of our proposed method. The results show that SegNetr is capable of accurately describing skin lesions with less data, and achieves multiclass segmentation with minimized under-segmentation and over-segmentation."
SegNetr: Rethinking the Local-Global Interactions and Skip Connections in U-Shaped Networks,3.2,Ablation Study,"Effect of Local-Global Interactions. The role of local-global interactions in SegNetr can be understood from Table 3. The overall parameters of the network are less when there is no local or global interaction, but the segmentation performance is also greatly affected. With the addition of local or global interactions, the segmentation performance of the network for different categories  is improved. In addition, similar performance can be obtained by running the local-global interaction modules in series and parallel, but the series connection leads to lower computational efficiency and affects the running speed.Effect of Patch Size. As shown in Table 4 (left), different patch size significantly affects the efficiency and parameters of the model. The number of parameters reaches 54.34 M when patches of size 2 are used in each phase, which is an increase of 42.08 M compared to using dynamic patches of size (8,4,2,1).Based on this ablation study, we recommend the use of [ Resolution   14   ] patches size at different stages.Effect of IRSC. Table 4 (right) shows the experimental results of replacing the skip connections of UNeXt, U-Net, U-Net++, and SegNet with IRSC. These methods get consistent improvement with the help of IRSC, which clearly shows that IRSC is useful."
SegNetr: Rethinking the Local-Global Interactions and Skip Connections in U-Shaped Networks,4,Conclusion,"In this study, we introduce a novel framework SegNetr for medical image segmentation, which achieves segmentation performance improvement by optimizing local-global interactions and skip connections. Specifically, the SegNetr block implements dynamic interactions based on non-overlapping windows using parallel local and global branches, and IRSC enables more accurate fusion of shallow and deep features by providing spacial information. We evaluated the proposed method using four medical image datasets, and extensive experiments showed that SegNetr is able to obtain challenging experimental results while maintaining a small number of parameters and GFLOPs. The proposed framework is general and flexible that we believe it can be easily extended to other U-shaped networks."
SegNetr: Rethinking the Local-Global Interactions and Skip Connections in U-Shaped Networks,,Fig. 1 .,
SegNetr: Rethinking the Local-Global Interactions and Skip Connections in U-Shaped Networks,,Fig. 2 .,
SegNetr: Rethinking the Local-Global Interactions and Skip Connections in U-Shaped Networks,,Fig. 3 .,
SegNetr: Rethinking the Local-Global Interactions and Skip Connections in U-Shaped Networks,,Table 1 .,
SegNetr: Rethinking the Local-Global Interactions and Skip Connections in U-Shaped Networks,,Table 2 .,
SegNetr: Rethinking the Local-Global Interactions and Skip Connections in U-Shaped Networks,,Table 3 .,
SegNetr: Rethinking the Local-Global Interactions and Skip Connections in U-Shaped Networks,,Table 4 .,
DARC: Distribution-Aware Re-Coloring Model for Generalizable Nucleus Segmentation,1,Introduction,"Automatic nucleus segmentation has captured wide research interests in recent years due to its importance in pathological image analysis [1][2][3][4]. However, as shown in Fig. 1, the variations in image modalities, staining protocols, scanner types, and tissues significantly affect the appearance of nucleus images, resulting in notable gap between source and target domains [5][6][7]. If a number of target domain samples are available before testing, one can adopt domain adaptation algorithms to transfer the knowledge learned from the source domain to the target domain [8][9][10]. Unfortunately, in real-world applications, it is usually expensive and time-consuming to collect new training sets for the ever changing target domains; moreover, extra computational cost is required, which is usually unrealistic for the end users. Therefore, it is highly desirable to train a robust nucleus segmentation model that is generalizable to different domains.In recent years, the research on domain generalization (DG) has attracted wide attention. Most existing DG works are proposed for classification tasks [12,13] and they can be roughly grouped into data augmentation-, representation learning-, and optimization-based methods. The first category of methods [14][15][16][17] focus on the way to diversify training data styles and expect the enriched styles cover those appeared in target domains. The second category of methods aim to obtain domain-invariant features. This is usually achieved via improving model architectures [18][19][20] or introducing novel regularization terms [21,22]. The third category of methods [23][24][25][26] develop new model optimization strategies, e.g., meta-learning, that improve model robustness via artificially introducing domain shifts during training.It is a consensus that a generalizable nucleus segmentation model should be robust to image appearance variation caused by the change in staining protocols, scanner types, and tissues, as illustrated in Fig. 1. In this paper, we argue that it is also desirable to be robust to the ratio between foreground (nucleus) and background pixel numbers. This ratio changes the statistics of each feature map channel, and affects the robustness of normalization layers, e.g., instance normalization (IN). We will empirically justify its impact in Sect. 2.3."
DARC: Distribution-Aware Re-Coloring Model for Generalizable Nucleus Segmentation,2,Method,
DARC: Distribution-Aware Re-Coloring Model for Generalizable Nucleus Segmentation,2.1,Overview,"In this paper, we adopt a U-Net-based model similar to that in [1] as the baseline. It performs both semantic segmentation and contour detection for nucleus instances. The area of each nucleus instance is obtained via subtraction between the segmentation and contour prediction maps [1]. Details of the baseline model is provided in the supplementary material. To handle domain variations, we adopt IN rather than batch normalization (BN) in the U-Net model.Our proposed Distribution-Aware Re-Coloring model (DARC) is illustrated in Fig. 2. Compared with the baseline, DARC replaces the IN layers with the proposed Distribution-Aware Instance Normalization (DAIN) layers. DARC first re-colors each image to relieve the influence caused by image acquisition conditions. The re-colored image is then fed into the U-Net encoder and the ratio prediction head. This head predicts the ratio between foreground and background pixel numbers. With the predicted ratio, the DAIN layers can estimate feature statistics more robustly and facilitate more accurate nucleus segmentation."
DARC: Distribution-Aware Re-Coloring Model for Generalizable Nucleus Segmentation,2.2,Nucleus Image Re-Coloring,"We propose the Re-Coloring (RC) method to overcome the color change in different domains. Specifically, given a RGB image I, e.g., an H&E or IHC stained image, we first obtain its grayscale image I g . We then feed I g into a simple module T that consists of a single residual block and a 1 × 1 convolutional layer with output channel number of 3. In this way, we obtain an initial re-colored image I r .However, de-colorization results in the loss of fine-grained textures and may harm the segmentation accuracy. To handle this problem, we compensate I r with the original semantic information contained in I. Recent works [40] show that semantic information can be reflected via the order of pixels according to their gray value. Therefore, we adopt the Sort-Matching algorithm [41] to combine the semantic information in I with the color values in I r . Details of RC is presented in Algorithm 1, in which Sort and ArgSort denote channel-wisely sorting the values and obtaining the sorted values and indices respectively, and"
DARC: Distribution-Aware Re-Coloring Model for Generalizable Nucleus Segmentation,,Algorithm 1. Re-Coloring,
DARC: Distribution-Aware Re-Coloring Model for Generalizable Nucleus Segmentation,,Input:,"The input RGB image I ∈ R H×W ×3 ; The module T whose input and output channel numbers are 1 and 3, respectively; Output:The re-colored image Io ∈ R AssignV alue denotes re-assembling the sorted values according to the provided indices. Details of the module T are included in the supplementary material. Via RC, the original fine-grained structure information from I g is recovered in I r . In this way, the re-colored image is advantageous in two aspects. First, the appearance difference between pathological images caused by the change in scanners and staining protocols is eliminated. Second, the re-colored image preserves fine-grained structure information, enabling precise instance segmentation to be possible."
DARC: Distribution-Aware Re-Coloring Model for Generalizable Nucleus Segmentation,2.3,Distribution-Aware Instance Normalization,"Due to dramatic domain gaps, feature statistics may differ significantly between domains [5][6][7], which means that feature statistics obtained from the source domain may not apply to the target domain. Therefore, existing DG works usually replace BN with IN for feature normalization [12,19]. However, for dense-prediction tasks like semantic segmentation or contour detection, adopting IN alone cannot fully address the feature statistics variation problem. This is because feature statistics are also relevant to the ratio between foreground and background pixel numbers. Specifically, an image with more nucleus instances produces more responses in feature maps and thus higher feature statistic values, and vice versa. The difference in this ratio causes interference to nucleus segmentation."
DARC: Distribution-Aware Re-Coloring Model for Generalizable Nucleus Segmentation,,Algorithm 2. Distribution-Aware Instance Normalization,
DARC: Distribution-Aware Re-Coloring Model for Generalizable Nucleus Segmentation,,Input:,"Original feature maps X ∈ R H×W ×C . The C-dimensional feature vector on its pixel (i, j) is denoted as xij;The modules Eμ and E δ that re-estimate feature statistics; Δsra ∈ R 1×1×C that is obtained via running mean of Δs in the training stage;The momentum factor α used to update Δsra ; (Optional) Δs = f (ρ); Output:Normalized feature maps Y ∈ R H×W ×C ; To verify the above viewpoint, we evaluate the baseline model under different foreground-background ratios. Specifically, we first remove the foreground pixels via in-painting [27], and then pad the original testing images with the obtained background patches. We adopt B to denote the ratio between the size of the obtained new image and the original image size. Compared with the original images, the new images have the same foreground regions but more background pixels, and thus have different foreground-background ratios. Finally, we evaluate the performance of the baseline model with different B values. Experimental results are presented in Table 1. It is shown that the value of B affects the model performance significantly.The above problem is common in nucleus segmentation because pathological images from different organs or tissues tend to have significantly different foreground-background ratios. However, this phenomenon is often ignored in existing research. To handle this problem, we propose the Distribution-Aware Instance Normalization (DAIN) method to re-estimate feature statistics that account for different ratios of foreground and background pixels. Details of DAIN is presented in Algorithm 2. The structures of E μ and E δ are included in the supplemental materials. As shown in Fig. 2, to obtain the foreground-background ratio ρ of one input image, we first feed it to the model encoder with Δs ra as the additional input. Δs ra acts as pseudo residuals of feature statistics and is obtained in the training stage via averaging Δs in a momentum fashion. The output features by the encoder are used to predict the foreground-background ratio ρ with a Ratio-Prediction Head (RPH). ρ is then utilized to estimate the residuals of feature statistics: Δs = f (ρ). Here, f is a 1 × 1 convolutional layer that transforms ρ to a feature vector whose dimension is the same as the target layer's channel number. After that, the input image is fed into the model again with Δs as additional input and finally makes more accurate predictions.The training of RPH requires an extra loss term L rph , which is formulated as bellow:where ρ g denotes the ground truth foreground-background ratio, and L BCE and L MSE denote the binary cross entropy loss and the mean squared error, respectively."
DARC: Distribution-Aware Re-Coloring Model for Generalizable Nucleus Segmentation,3,Experiments,
DARC: Distribution-Aware Re-Coloring Model for Generalizable Nucleus Segmentation,3.1,Datasets,"The proposed method is evaluated on four datasets, including two H&E stained image datasets CoNSeP [3] and CPM17 [28] and two IHC stained datasets DeepLIIF [29] and BC-DeepLIIF [29,32]. CoNSeP [3] contains 28 training and 14 validation images, whose sizes are 1000×1000 pixels. The images are extracted from 16 colorectal adenocarcinoma WSIs, each of which belongs to an individual patient, and scanned with an Omnyx VL120 scanner within the department of pathology at University Hospitals Coventry and Warwickshire, UK. CPM17 [28] contains 32 training and 32 validation images, whose sizes are 500 × 500 pixels. The images are selected from a set of Glioblastoma Multiforme, Lower Grade Glioma, Head and Neck Squamous Cell Carcinoma, and non-small cell lung cancer whole slide tissue images. DeepLIIF [29] contains 575 training and 91 validation images, whose sizes are 512 × 512 pixels. The images are extracted from the slides of lung and bladder tissues. BC-DeepLIIF [29,32] contains 385 training and 66 validation Ki67 stained images of breast carcinoma, whose sizes are 512 × 512 pixels."
DARC: Distribution-Aware Re-Coloring Model for Generalizable Nucleus Segmentation,3.2,Implementation Details,"In the training stage, patches of size 224 × 224 pixels are randomly cropped from the original samples. During training, the batch size is 4 and the total number of training iterations is 40,000. We use Adam algorithm for optimization, and the learning rate is initialized as 1e -3 , which is gradually decreased to 1e -5 during training. We adopt the standard augmentation, like image color jittering and Gaussian blurring. In all experiments, the segmentation and contour detection predictions are penalized using the binary cross entropy loss. "
DARC: Distribution-Aware Re-Coloring Model for Generalizable Nucleus Segmentation,3.3,Experimental Results and Analyses,"In this paper, the models are compared using the AJI [33] and Dice scores. In the experiments, models trained on one of the datasets will be evaluated on the three unseen ones. To avoid the influence of the different sample numbers of the datasets, we calculate the average scores within each unseen domain respectively and then average them across domains.In this paper, we re-implement some existing popular domain generalization algorithms for comparisons under the same training conditions. Specifically, we re-implement the TENT [34], BIN [19], DSU [20], Frequency Amplitude Normalization (AmpNorm) [36,37], SAN [35] and EFDMix [40]. We also evaluate the stain normalization [38] and stain mix-up [39] methods that are popular in pathological image analysis. Their performances are presented in Table 2. DARC all replaces all normalization layers with DAIN, while DARC enc replaces the normalization layers in the encoder with DAIN and uses BN in its decoder. As shown in Table 2, DARC enc achieves the best average performance among all methods. Specifically, DARC enc improves the baseline model's average AJI and Dice scores by 4.81% and 7.04%. Compared with the other domain generalization methods, DAIN, DAIN w/o Ratio, DARC all and DARC enc achieve impressive performances on BC-DeepLIIF, which justify that re-estimating the instancewise statistics is important for improving the domain generalization ability of models trained on BC-DeepLIIF. Qualitative comparisons are presented in Fig. 3. Moreover, the complexity analysis between the baseline model and DARC enc is presented in Table 3.We separately evaluate the effectiveness of RC and DAIN, and present the results in Table 2. Also, we train a variant model without foreground-background ratio prediction, which is denoted as 'DAIN w/o Ratio' in Table 2. Compared with the baseline model, RC improves the average AJI and Dice scores by 1.41% and 2.59%, and DAIN improves the average AJI and Dice scores by 1.13% and 4.08%. Compared with the variant model without foreground-background ratio prediction, DAIN improves the average AJI and Dice scores by 0.90% and 3.74%. Finally, the combinations of RC and DAIN, i.e., DARC all and DARC enc , achieve the best average scores. As shown in Table 2, DARC enc improves DARC all by 1.24% and 0.77% on AJI and Dice scores respectively. This is because after the operations by RC and DAIN in the encoder, the obtained feature maps are much more robust to the domain gaps, which enables the decoder to adopt the fixed statistics maintained during training. Moreover, using the fixed statistics is helpful to prevent the decoder from the influence of varied foreground-background ratios on feature statistics."
DARC: Distribution-Aware Re-Coloring Model for Generalizable Nucleus Segmentation,4,Conclusion,"In this paper, we propose the DARC model for generalizable nucleus segmentation. To handle the domain gaps caused by varied image acquisition conditions, DARC first re-colors the input image while preserving its fine-grained structures as much as possible. Moreover, we find that the performance of instance normalization is sensitive to the varied ratios in foreground and background pixel numbers. This problem is well addressed by our proposed DAIN. Compared with existing works, DARC achieves significantly better performance on average across four benchmarks."
DARC: Distribution-Aware Re-Coloring Model for Generalizable Nucleus Segmentation,,Fig. 1 .,
DARC: Distribution-Aware Re-Coloring Model for Generalizable Nucleus Segmentation,,Fig. 2 .,
DARC: Distribution-Aware Re-Coloring Model for Generalizable Nucleus Segmentation,,Fig. 3 .,
DARC: Distribution-Aware Re-Coloring Model for Generalizable Nucleus Segmentation,,Table 1 .,
DARC: Distribution-Aware Re-Coloring Model for Generalizable Nucleus Segmentation,,Table 2 .,
DARC: Distribution-Aware Re-Coloring Model for Generalizable Nucleus Segmentation,,Table 3 .,
DARC: Distribution-Aware Re-Coloring Model for Generalizable Nucleus Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 57.
An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,1,Introduction,"Accurate spatial characterization of tumor immune microenvironment is critical for precise therapeutic stratification of cancer patients (e.g. via immunotherapy). Currently, this characterization is done manually by individual pathologists on standard hematoxylin-and-eosin (H&E) or singleplex immunohistochemistry (IHC) stained images. However, this results in high interobserver variability among pathologists, primarily due to the large (> 50%) disagreement among pathologists for immune cell phenotyping [10]. This is also a big cause of concern for publicly available H&E/IHC cell segmentation datasets with immune cell annotations from single pathologists. Multiplex staining resolves this issue by allowing different tumor and immune cell markers to be stained on the same tissue section, avoiding any phenotyping guesswork from pathologists. Multiplex staining can be performed using expensive multiplex immunofluorescence (mIF) or via cheaper multiplex immunohistochemistry (mIHC) assays. MIF staining (requiring expensive scanners and highly skilled lab technicians) allows multiple markers to be stained/expressed on the same tissue section (no co-registration needed) while also providing the utility to turn ON/OFF individual markers as needed. In contrast, current brightfield mIHC staining protocols relying on DAB (3,3'-Diaminobenzidine) alcohol-insoluble chromogen, even though easily implementable with current clinical staining protocols, suffer from occlusion of signal from sequential staining of additional markers. To this effect, we introduce a new brightfield mIHC staining protocol using alcoholsoluble aminoethyl carbazole (AEC) chromogen which allows repeated stripping, restaining, and scanning of the same tissue section with multiple markers. This requires only affine registration to align the digitized restained images to obtain non-occluded signal intensity profiles for all the markers, similar to mIF staining/scanning.In this paper, we introduce a new dataset that can be readily used out-ofthe-box with any artificial intelligence (AI)/deep learning algorithms for spatial characterization of tumor immune microenvironment and several other use cases.To date, only two denovo stained datasets have been released publicly: BCI H&E and singleplex IHC HER2 dataset [7] and DeepLIIF singleplex IHC Ki67 and mIF dataset [2], both without any immune or tumor markers. In contrast, we release the first denovo mIF/mIHC stained dataset with tumor and immune markers for more accurate characterization of tumor immune microenvironment. We also demonstrate several interesting use cases: (1) IHC quantification of CD3/CD8 tumor-infiltrating lymphocytes (TILs) via style transfer, (2) virtual translation of cheap mIHC stains to more expensive mIF stains, and (3) virtual tumor/immune cellular phenotyping on standard hematoxylin images. "
An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,2,Dataset,The complete staining protocols for this dataset are given in the accompanying supplementary material. Images were acquired at 20× magnification at Moffitt Cancer Center. The demographics and other relevant information for all eight head-and-neck squamous cell carcinoma patients is given in Table 1.
An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,2.1,Region-of-Interest Selection and Image Registration,"After scanning the full images at low resolution, nine regions of interest (ROIs) from each slide were chosen by an experienced pathologist on both mIF and mIHC images: three in the tumor core (TC), three at the tumor margin (TM), and three outside in the adjacent stroma (S) area. The size of the ROIs was standardized at 1356×1012 pixels with a resolution of 0.5 µm/pixel for a total surface area of 0.343 mm 2 . Hematoxylin-stained ROIs were first used to align all the mIHC marker images in the open source Fiji software using affine registration. After that, hematoxylin-and DAPI-stained ROIs were used as references to align mIHC and mIF ROIs again using Fiji and subdivided into 512×512 patches, resulting in total of 268 co-registered mIHC and mIF patches (∼33 co-registered mIF/mIHC images per patient)."
An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,2.2,Concordance Study,We compared mIF and mIHC assays for concordance in marker intensities. The results are shown in Fig. 2. This is the first direct comparison of mIF and mIHC using identical slides. It provides a standardized dataset to demonstrate the equivalence of the two methods and a source that can be used to calibrate other methods.
An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,3,Use Cases,"In this section, we demonstrate some of the use cases enabled by this high-quality AI-ready dataset. We have used publicly available state-of-the-art tools such as Adaptive Attention Normalization (AdaAttN) [8] for style transfer in the IHC CD3/CD8 quantification use case and DeepLIIF virtual stain translation [2,3] in the remaining two use cases.   "
An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,3.1,IHC CD3/CD8 Scoring Using mIF Style Transfer,"We generate a stylized IHC image (Fig. 3) using three input images: (1) hematoxylin image (used for generating the underlying structure of cells in the stylized image), (2) its corresponding mIF CD3/CD8 marker image (used for staining positive cells as brown), and (3) sample IHC style image (used for transferring its style to the final image). The complete architecture diagram is given in the supplementary material. Specifically, the model consists of two sub-networks:(a) Marker Generation: This sub-network is used for generating mIF marker data from the generated stylized image. We use a conditional generative adversarial network (cGAN) [4] for generating the marker images. The cGAN network consists of a generator, responsible for generating mIF marker images given an IHC image, and a discriminator, responsible for distinguishing the output of the generator from ground truth data. We first extract the brown (DAB channel) from the given style IHC image, using stain deconvolution. Then, we use pairs of the style images and their extracted brown DAB marker images to train this sub-network. This sub-network improves staining of the positive cells in the final stylized image by comparing the extracted DAB marker image from the stylized image and the input mIF marker image at each iteration."
An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,,(b) Style Transfer:,"This sub-network creates the stylized IHC image using an attention module, given (1) the input hematoxylin and the mIF marker images and (2) the style and its corresponding marker images. For synthetically generating stylized IHC images, we follow the approach outlined in AdaAttN [8]. We use a pre-trained VGG-19 network [12] as an encoder to extract multi-level feature maps and a decoder with a symmetric structure of VGG-19. We then use both shallow and deep level features by using AdaAttN modules on multiple layers of VGG. This sub-network is used to create a stylized image using the structure of the given hematoxylin image while transferring the overall color distribution of the style image to the final stylized image. The generated marker image from the first sub-network is used for a more accurate colorization of the positive cells against the blue hematoxylin counterstain/background; not defining loss functions based on the markers generated by the first sub-network leads to discrepancy in the final brown DAB channel synthesis.For the stylized IHC images with ground truth CD3/CD8 marker images, we also segmented corresponding DAPI images using our interactive deep learning ImPartial [9] tool https://github.com/nadeemlab/ImPartial and then classified the segmented masks using the corresponding CD3/CD8 channel intensities, as shown in Fig. 4. We extracted 268 tiles of size 512×512 from this final segmented and co-registered dataset. For the purpose of training and testing all the models, we extract four images of size 256 × 256 from each tile due to the size of the external IHC images, resulting in a total of 1072 images. We randomly extracted tiles from the LYON19 challenge dataset [14] to use as style IHC images. Using these images, we created a dataset of synthetically generated IHC images from the hematoxylin and its marker image as shown in Fig. 3.We evaluated the effectiveness of our synthetically generated dataset (stylized IHC images and corresponding segmented/classified masks) using our generated dataset with the NuClick training dataset (containing manually segmented CD3/CD8 cells) [6]. We randomly selected 840 and 230 patches of size 256 × 256 from the created dataset for training and validation, respectively. NuClick training and validation sets [6] comprise 671 and 200 patches, respectively, of size 256 × 256 extracted from LYON19 dataset [14]. LYON19 IHC CD3/CD8 images are taken from breast, colon, and prostate cancer patients. We split their training set into training and validation sets, containing 553 and 118 images, respectively, and use their validation set for testing our trained models. We trained three models including UNet [11], FPN [5], UNet++ [15] with the backbone of resnet50 for 200 epochs and early stopping on validation score with patience of 30 epochs, using binary cross entropy loss and Adam optimizer with learning rate of 0.0001. As shown in Table 2, models trained with our synthetic training set outperform those trained solely with NuClick data in all metrics.We also tested the trained models on 1,500 randomly selected images from the training set of the Lymphocyte Assessment Hackathon (LYSTO) [1], containing image patches of size 299 × 299 obtained at a magnification of 40× from breast, prostate, and colon cancer whole slide images stained with CD3 and CD8 markers. Only the total number of lymphocytes in each image patch are reported in this dataset. To evaluate the performance of trained models on this dataset, we counted the total number of marked lymphocytes in a predicted mask and calculated the difference between the reported number of lymphocytes in each image with the total number of lymphocytes in the predicted mask by the model. In Table 2, the average difference value (DiffCount) of lymphocyte number for the whole dataset is reported for each model. As seen, the trained models on our dataset outperform the models trained solely on NuClick data."
An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,3.2,Virtual Translation of Cheap mIHC to Expensive mIF Stains,"Unlike clinical DAB staining, as shown in style IHC images in Fig. 3, where brown marker channel has a blue hematoxylin nuclear counterstain to stain for all the cells, our mIHC AEC-stained marker images (Fig. 5) do not stain for all the cells including nuclei. In this use case, we show that mIHC marker images can be translated to higher quality mIF DAPI and marker images which stain effectively for all the cells. We used the publicly available DeepLIIF virtual translation module [2,3] for this task. We trained DeepLIIF on mIHC CD3 AECstained images to infer mIF DAPI and CD3 marker. Some examples of testing the trained model on CD3 images are shown in Fig. 5. We calculated the Mean Squared Error (MSE) and Structural Similarity Index (SSIM) to evaluate the quality of the inferred modalities by the trained model. The MSE and SSIM for mIF DAPI was 0.0070 and 0.9991 and for mIF CD3 was 0.0021 and 0.9997, indicating high accuracy of mIF inference."
An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,3.3,Virtual Cellular Phenotyping on Standard Hematoxylin Images,"There are several public H&E/IHC cell segmentation datasets with manual immune cell annotations from single pathologists. These are highly problematic given the large (> 50%) disagreement among pathologists on immune cell phenotyping [10]. In this last use case, we infer immune and tumor markers from the standard hematoxylin images using again the public DeepLIIF virtual translation module [2,3]. We train the translation task of DeepLIIF model using the hematoxylin, immune (CD3) and tumor (PanCK) markers. Sample images/results taken from the testing dataset are shown in Fig. 6."
An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,4,Conclusions and Future Work,"We have released the first AI-ready restained and co-registered mIF and mIHC dataset for head-and-neck squamous cell carcinoma patients. This dataset can be used for virtual phenotyping given standard clinical hematoxylin images, virtual clinical IHC DAB generation with ground truth segmentations (to train highquality segmentation models across multiple cancer types) created from cleaner mIF images, as well as for generating standardized clean mIF images from neighboring H&E and IHC sections for registration and 3D reconstruction of tissue specimens. In the future, we will release similar datasets for additional cancer types as well as release for this dataset corresponding whole-cell segmentations via ImPartial https://github.com/nadeemlab/ImPartial."
An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,,Data use Declaration and Acknowledgment:,"This study is not Human Subjects Research because it was a secondary analysis of results from biological specimens that were not collected for the purpose of the current study and for which the samples were fully anonymized. This work was supported by MSK Cancer Center Support Grant/Core Grant (P30 CA008748) and by James and Esther King Biomedical Research Grant (7JK02) and Moffitt Merit Society Award to C. H. Chung. It is also supported in part by the Moffitt's Total Cancer Care Initiative, Collaborative Data Services, Biostatistics and Bioinformatics, and Tissue Core Facilities at the H. Lee Moffitt Cancer Center and Research Institute, an NCI-designated Comprehensive Cancer Center (P30-CA076292)."
An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,,Fig. 1 .,
An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,,Fig. 2 .,
An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,,Fig. 3 .,
An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,,Fig. 4 .,
An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,,Fig. 5 .,
An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,,Fig. 6 .,
An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,,Table 1 .,
An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,,Table 2 .,
An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 68.
Detection of Basal Cell Carcinoma in Whole Slide Images,1,Introduction,"Skin cancer, the most prevalent cancer globally, has seen increasing incidences over recent decades [1]. It constitutes a third of all cancer diagnoses, affecting one in five Americans [2]. Basal cell carcinoma (BCC), comprising 70% of cases, has surged by 20-80% in the last 30 years, exerting a significant healthcare strain. Timely BCC diagnosis is crucial to avoid complex treatments. Although histological evaluation remains the gold standard for detection [3], deep learning and computer vision advancements can streamline this process. Scanned traditional histology slides result in whole slide images (WSIs) that can be analyzed by deep learning models, significantly easing the histological evaluation burden. Recent advancements underscore the promise of this approach [4][5][6]. Existing skin cancer detection methods [7][8][9] typically employs models like Inception Net and ResNet, designed for natural images like those in the ImageNet dataset. The significant variance in pathology and natural images can compromise these models' accuracy. Neural architecture search (NAS) addresses this issue by auto-designing superior models [10][11][12][13], exploring a vast architecture space. However, current NAS methods often overlook fairness in architecture ranking, impeding the discovery of top-performing models.In this study, we utilized the NAS approach to identify the optimal network for skin cancer detection. To improve the efficiency and accuracy of the search, we developed a new framework named SC-net, which focuses on identifying highly valuable architectures. We observed that conventional NAS methods often overlook fairness ranking during the search, hindering the search for optimal solutions. Our SC-Net framework addresses this by ensuring fair training and precise ranking. The efficacy of SC-net was confirmed by our experimental results, with our ResNet50 achieving 96.2% top-1 accuracy and 96.5% AUC, outperforming baseline methods by 4.8% and 4.7% respectively.Figure 1 shows the proposed framework, which integrates two modules. Module (a) extracts the region of interest (ROI) from WSI and generates patches, while Module (b) uses optimal model architecture from NAS to analyze features from patches and generate classifications. "
Detection of Basal Cell Carcinoma in Whole Slide Images,2,Methods,"The proposed method (Fig. 2) involves dividing the input WSI into patches for training a supernet and the search for optimal architectures [14]. Section 2.2 provides further details about the supernet. A balanced evolutionary algorithm is then used to select the optimal structure from the search space, with the candidate structures' performance evaluated using mini-batch patch data. We evaluate the searched architectures on the skin cancer dataset."
Detection of Basal Cell Carcinoma in Whole Slide Images,2.1,One-Shot Channel Number Search,"To extract an optimal architecture γ ∈ G from a vast search space G, a weightsharing strategy [15][16][17] is used to prevent training from scratch. The search leverages a supernet S with weights W, with each path γ inheriting weights from W. This makes one-shot NAS a two-step optimization process: supernet training and architecture search. The original dataset is typically split into training D t and validation datasets Dv. The weights W of the supernet S are trained by uniformly sampling the network width d and optimizing the sub-network with weights wd ⊂ W. The optimization function is defined as follows:where U (D) is a uniform distribution of network widths, E is the expected value of random variables, and L t is the training loss function. Then, the optimal network width d * corresponds to the network width with the best performance (e.g. classification accuracy) on the validation dataset, i.e.,where F p is the resource budget of FLOPs. The search for Eq. 2 can be efficiently performed by various algorithms, such as random or evolutionary search [18]. Afterward, the performance of the searched optimal width d * is analyzed by training from scratch. "
Detection of Basal Cell Carcinoma in Whole Slide Images,2.2,SC-Net as a Balanced Supernet,"Current approaches for neural architecture search [19][20][21] often employ a unilaterally augmented (UA) principle to evaluate each width, resulting in unfair training of channels in the supernet. As illustrated in Fig. 3(a), to search for a dimension d at a layer with a maximum of n channels, the UA principle assigns the left d channels in the supernet to indicate the corresponding architecture aswhere γ A (d) means the selected d channels from the left (smaller-index) side. However, the UA principle leads to channel training imbalance in the supernet due to its constraints, as illustrated in Fig. 3(a). Channels with smaller indices are used for various sizes, resulting in over-training of the left channel kernels since widths are uniformly sampled. The unfairness can be quantified by T , which represents how often a channel is utilized, reflecting its level of training. Given a layer has a maximum of n channels, the T for the i-th channel under the UA principle isCorrespondingly, the probability of i -th channel being trained can be expressed as P i = n-i+1 n . Therefore, channels closer to the left will get more attempts during training, which leads the degree of training to vary widely between channels. This introduces evaluation bias and leads to sub-optimal results.To mitigate evaluation bias on width, we propose a new SC-net that promotes the fairness of channels during training. As shown in Fig. 3(b), in the proposed supernet, each width is simultaneously evaluated by the sub-networks corresponding to the left and right channels. This can be seen as two identical networks S l and S r that are bilaterally coupled and evaluated using the UA principle, but counting channels in reverse order. Therefore, the number of all channels used for evaluating the width d can be expressed as:where represents the union of two lists with repeatable elements. In detail, the left channel in S l follows the same UA principle setup as in Eq. ( 3), while for the right channel in S r , we count channels from the right H r (d) = [(nd + 1) :(nd)]. Therefore, the training degree of each channel is the sum of the two supernets S l and S r . Since the channels are counted from the right within S r , the training degree of the d-th channel on the left corresponds to the training degree of the (n-d+1)-th channel on the right in Eq. ( 4). Therefore, the training degree T (d) of the d-th channel in our proposed method isTherefore, the training degree T for each channel will always be equal to the same constant value of the width, independent of the channel index, ensuring fairness in terms of channel (filter) levels. Thus the network width can be fairly ranked using our network."
Detection of Basal Cell Carcinoma in Whole Slide Images,2.3,Balanced Evolutionary Search with SC-Net,"Using a trained SC-net, the architecture can be evaluated. However, the search space involved in NAS is large, with more than 10 20 possible architectures, requiring an evolutionary search using the multi-objective NSGA-II algorithm to improve the search performance. During the evolutionary search, the width d of each network is represented by the average precision of its corresponding left and right paths in the supernet S, as shown in Eq. ( 9). The optimal width (not subnetwork) is determined as the one that achieves the best performance when trained from scratch. Here, S l and S r refer to the two paths of S that correspond to the width d during the training process."
Detection of Basal Cell Carcinoma in Whole Slide Images,,"Acc(W, d, D","3 Experiments The dataset, comprised of 194 skin slides acquired from the Southern Sun Pathology laboratory, includes 148 BCC cases and 46 other types (common nevus, SCC), all manually annotated by a dermatopathologist. BCC slides served as positive samples and the rest as negatives. These slides were scanned at ×20 magnification with a 0.44 µm pixel size using a Leica Aperio AT2 Scanner. The patient data were separated between training and testing to prevent overlap. Details are shown in Table 1. The experimental setup involved training models on two NVIDIA RTX A6000 GPUs using PyTorch. These models, initialized from a zero-mean Gaussian with standard deviation σ = 0.001, were trained for 200 epochs with a batch size of 256. Training used the Adam optimizer with a dynamic learning rate reduction strategy, starting with a learning rate of 5e-5 following a cosine schedule. "
Detection of Basal Cell Carcinoma in Whole Slide Images,3.1,Experiment Settings,
Detection of Basal Cell Carcinoma in Whole Slide Images,3.2,Performance Evaluation,"We validated our algorithm using the curated skin cancer dataset and SC-net as a supernet, testing both heavy and light models. We performed a search on ResNet50 and MobileNetV2 models, compared against original ResNet50 (ori ResNet50) and MobileNetV2 (ori MobileNetV2) models as baselines. The resulting models are denoted as s ResNet50 and s MobileNetV2.Comparison with Related Methods. To ensure a fair comparison on our dataset, we selected several papers in the field of pathological image analysis, such as [9,22,23], as well as others using the UA principle, such as [18,24].Evaluation Metrics. Our model was evaluated on: As shown in Table 2, the s ResNet50 model outperformed in all metrics, showing 4.8%, 4.5%, 5.4%, 4.7% and 4.7% improvements in accuracy, sensitivity, specificity, F 1 Score, and AUC, respectively, over ori ResNet50, and surpassing "
Detection of Basal Cell Carcinoma in Whole Slide Images,3.3,Ablation Study,Effect of SC-net as a Supernet.  
Detection of Basal Cell Carcinoma in Whole Slide Images,4,Conclusion and Future Work,"In this paper, we introduce SC-net, a novel NAS framework for skin cancer detection in pathology images. By formulating SC-net as a balanced supernet, we ensure fair ranking and treatment of all potential architectures. With SCnet and evolutionary search, we obtained optimal architectures, achieving 96.2% Top-1 and 96.5% accuracy on a skin cancer dataset, improvements of 4.8% and 4.7% over baselines. Future work will apply our approach to larger datasets for wider-scale validation."
Detection of Basal Cell Carcinoma in Whole Slide Images,5,Compliance with Ethical Standards,"This study was performed in line with the principles of the Declaration of Helsinki. Ethics approval was granted by CSIRO Health and Medical Human Research Ethics Committee (CHMHREC). The ethics approval number is 2021 030 LR, and the validity period is from 07 Apr 2021 to 31 Dec 2024."
Detection of Basal Cell Carcinoma in Whole Slide Images,,Fig. 1 .,
Detection of Basal Cell Carcinoma in Whole Slide Images,,Fig. 2 .,
Detection of Basal Cell Carcinoma in Whole Slide Images,,Fig. 3 .,
Detection of Basal Cell Carcinoma in Whole Slide Images,,Fig. 4 .,
Detection of Basal Cell Carcinoma in Whole Slide Images,,( 1 ),
Detection of Basal Cell Carcinoma in Whole Slide Images,,Table 1 .,
Detection of Basal Cell Carcinoma in Whole Slide Images,,Table 2 .,
Detection of Basal Cell Carcinoma in Whole Slide Images,,Table 3 .,
Detection of Basal Cell Carcinoma in Whole Slide Images,,Table 3,
Detection of Basal Cell Carcinoma in Whole Slide Images,,Table 4 .,
IIB-MIL: Integrated Instance-Level and Bag-Level Multiple Instances Learning with Label Disambiguation for Pathological Image Analysis,1,Introduction,"Pathology is widely recognized as the gold standard for disease diagnosis [15]. As the demand for intelligently pathological image analysis continues to grow, an increasing number of researchers have paid attention to this field [12,14,25]. However, pathological image analysis remains a challenging task due to the complex and heterogeneous nature [19] of obtained whole slide images (WSIs), as well as their huge gigapixel-level size [20]. To address this issue, multiple instance learning (MIL) [1] is usually applied to formulate pathological image analysis tasks into weakly supervised learning problems. In the MIL setting, the entire WSI is regarded as a bag and tiled patches are instances. The primary challenge of MIL arises from its weakly supervised nature, i.e. only the baglevel label for the entire WSI is provided, while labels for individual patches are usually unavailable. Although MIL-based methods have shown impressive potential in solving a wide range of pathological image analysis tasks including cancer grading and subtype diagnosis [23], prognosis prediction [18], genotyperelated tasks such as gene mutation prediction [4], etc., it is still an open question regarding learning an informative and effective representation of the entire WSI for down-streaming task based on MIL architecture.Current MIL methods can be broadly categorized into two types: bag-level MIL and instance-level MIL. Bag-level MIL [9,17], also known as embeddingbased MIL, involves converting patches (instances) into low-dimensional embeddings, which are then aggregated into WSI (bag)-level representations to conduct the analysis tasks [22]. The aggregator can take different architectures such as an attention module [7,13], convolutional neural network (CNN), Transformer [16], or graph neural network [10,28]. Instance-level MIL [2,8,24], on the other hand, focuses its learning process at the instance level, and then obtains the bag-level prediction by simply aggregating instance predictions. Bag-level MIL incorporates instance embeddings to create a bag representation, converting the MIL into a supervised learning problem. Furthermore, it can extract contextual information and correlations between instances. Nonetheless, Bag-level MIL needs to learn informative embeddings of instances and adjust the contributions of these instance embeddings to generate the bag representation simultaneously, which faces the risk of obtaining a suboptimal model given the limited training samples in practice. The instance-level MIL, however, faces the problem of noisy labels, which is caused by the common strategy of assigning the WSI labels to patches and the fact that there are lots of patches irrelevant to the WSI labels [3,6].Considering these conventional MIL methods usually utilize either bag-level or instance-level supervision, leading to suboptimal performance. In this paper, we format the instance-level MIL as a noisy label learning task and propose to solve it by designing an instance-level supervision based on the label disambiguation [21]. Then we propose to combine bag-level and instance-level supervision to improve the performance of MIL. The bag-level and instance-level supervision can corporately optimize the instance embedding learning process and welllearned instance embeddings can facilitate the aggregation module to generate the bag representation. The co-supervision design also makes the MIL to be a multi-task learning framework, where the bag-level supervision channel works to globally summarise the WSI for prediction and the instance-level supervision channel can locally identify key relevant patches. The detailed contributions can be summarized as follows:1) We propose a novel MIL method for pathological image analysis that leverages a specially-designed residual Transformer backbone and organically integrates both Transformer-based bag-level and label-disambiguation-based instancelevel supervision for performance enhancement. 2) We develop a label-disambiguation module that leverages prototypes and confidence bank to tackle the weakly supervised nature of instance-level supervision and reduce the impact of assigned noisy labels.3) The proposed framework outperforms state-of-the-art (SOTA) methods on public datasets and in a practical clinical task, demonstrating its superiors in WSI analysis. Besides, ablation studies illustrate the superiority of our co-supervision design compared to using only one type of supervision. 2 Method"
IIB-MIL: Integrated Instance-Level and Bag-Level Multiple Instances Learning with Label Disambiguation for Pathological Image Analysis,2.1,Overview,"The overall framework of the proposed IIB-MIL is shown in Fig. 1. Similar to previous works [27], IIB-MIL first transforms input huge-size WSI to a set of patch embeddings to simplify the following learning task using a pre-trained encoder, i.e. EfficientNet-B0. Then a specially-designed residual transformer backbone works to calibrate the obtained patch embeddings and encode the context information and correlation of patches. After that, IIB-MIL utilizes both a transformer-based bag-level and a label-disambiguation-based instance-level supervision to cooperatively optimize the model, where the bag-level loss is calculated referring to the WSI labels, while the instance loss is calculated referring to pseudo patch labels calibrated by the Label-Disambiguation module. Since bag-level supervision channel is trained to globally summarise information of all patches for prediction, the bag-level outputs are used as the final predictions during the test stage."
IIB-MIL: Integrated Instance-Level and Bag-Level Multiple Instances Learning with Label Disambiguation for Pathological Image Analysis,2.2,Problem Formulation,"Assume there is a set of N WSIs denoted by S = {S 1 , S 2 , ..., S N }. Each WSI S i has a WSI-level label Y i ∈ {1, ..., C}, where C represents category number. In each S i , there exist M i tiled patches without patch-level labels. To reduce the computational cost, we used a frozen pre-trained encoder to transform patches into K dimensional embeddings {e i,Our proposed IIB-MIL comprehensively integrates obtained embeddings {e i,j , ...} to generate accurate WSI classification."
IIB-MIL: Integrated Instance-Level and Bag-Level Multiple Instances Learning with Label Disambiguation for Pathological Image Analysis,2.3,Backbone Network,"Before bag-level and instance-level supervision channels, we design a residual transformer backbone T (•) : R K → R D to calibrate the obtained patch embeddings and encode the context information and correlation of patches. T (•) maps patch embeddings {e i,j , ...} to a lower-dimensional feature space, denoted as {x i,j , ...}, where x i,j = T (e i,j ), x i,j ∈ R D is the calibrated embedding, T (•) is composed of transformer layers and skip connections (Details are given in the supplementary.)."
IIB-MIL: Integrated Instance-Level and Bag-Level Multiple Instances Learning with Label Disambiguation for Pathological Image Analysis,2.4,Instance-Level Supervision,"At the core of instance-level supervision is the label disambiguation module, which serves to rectify the imprecise labels that have been assigned to patches. It comprises prototypes and a confidence bank, takes instance features and instance classifier predictions as inputs, and generates soft labels as outputs (Fig. 1 (b)). The prototypes, denoted as P ∈ R C×D , are initialized with all-zero vectors and employ momentum-based updates using selected instance features x with the highest probability prob inst of belonging to their corresponding categories. Prototype labels z are determined based on the proximity of patch features to the prototypes. Confidence B ∈ R N ×M ×C is initialized with all WSI labels and uses momentum-based updates with z. Detailed steps are summarized as follows:Step 1: Obtain the instance classifier output. The instance-level classifier, denoted as F inst (•), takes x i,j ∈ R D as input and outputs the predicted instance probability prob inst i,j ∈ R C , as:The probability that x i,j is predicted as class c is denoted asStep 2: Obtain the prototype labels. At t time, the prototype vector for the category c is P c,t ∈ R D . To update P c,t , we select a set of instance features Set c,t that have the highest probabilities prob inst i,j,c of belonging to category c. Specifically, we define Set c,t as:where K is the number of top instance features to select. Then, we use a momentum-based update rule to obtain P c,t+1 :where α is the momentum coefficient that automatically decreases from α = 0.95 to α = 0.8 across epochs. Then, we can obtain prototype labels z i,j ∈ R C using the following equation:The resulting prototype label z i,j ∈ R C is a one-hot vector that indicates the category of the j-th instance in the i-th WSI.Step 3: Obtain Soft Labels from the Confidence Bank Specifically, at time t, the pseudo-target B i,j,t ∈ R C of the instance embedding e i,j is updated by the following:where β is the momentum update parameter with a default value of β = 0.99.Step 4: Calculate Instance-Level Loss. We compute instance-Level Loss using the cross-entropy function:Here, B i,j,c and prob inst i,j,c are the c-th component of the pseudo-target B i,j and predicted probability prob inst i,j , respectively."
IIB-MIL: Integrated Instance-Level and Bag-Level Multiple Instances Learning with Label Disambiguation for Pathological Image Analysis,2.5,Bag-Level Supervision,"For bag-level supervision, instance features x i ∈ R M ×D go through a transformer-based aggregator A(•) : R M ×D → R D and a WSI classifier F bag (•) : R D → R C in turn (Architecture details are given in the supplementary.). Then we obtain the predicted probability of WSI S i as:The bag-level loss function is given by:where Y i ∈ R C is the label of WSI S i ."
IIB-MIL: Integrated Instance-Level and Bag-Level Multiple Instances Learning with Label Disambiguation for Pathological Image Analysis,2.6,Training,"In the training phase, We employ a warm-up strategy in which we update only the Prototypes and do not update the Confidence Bank during the first few epochs. Our approach is trained end-to-end, and the total loss function is :where λ is the hyperparameter that controls the relative importance of the two losses."
IIB-MIL: Integrated Instance-Level and Bag-Level Multiple Instances Learning with Label Disambiguation for Pathological Image Analysis,3,Experiments,
IIB-MIL: Integrated Instance-Level and Bag-Level Multiple Instances Learning with Label Disambiguation for Pathological Image Analysis,3.1,Dataset,"We evaluate our model with three datasets. (1) LUAD-GM Dataset: The objective is to predict the epidermal growth factor receptor (EGFR) gene mutations in patients with lung adenocarcinoma (LUAD) using 723 Whole Slide Image (WSI) slices, where 47% of cases have EGFR mutations. (2) TCGA-NSCLC and TCGA-RCC Datasets: Cancer type classification is performed using The Cancer Genome Atlas (TCGA) dataset. The TCGA-NSCLC dataset comprised two subtypes, lung squamous cell carcinoma (LUSC) and lung adenocarcinoma (LUAD), while the TCGA-RCC dataset included three subtypes: renal chromophobe cell carcinoma (KICH), renal clear cell carcinoma (KIRC), and renal papillary cell carcinoma (KIRP)."
IIB-MIL: Integrated Instance-Level and Bag-Level Multiple Instances Learning with Label Disambiguation for Pathological Image Analysis,3.2,Experiment Settings,"The dataset was randomly split into three parts: training, validation, and testing, with 60%, 20%, and 20% of the samples, respectively. WSIs were preprocessed by cropping them into 1120 × 1120 patches, without overlap. The proposed model was implemented in Pytorch, trained on a 32GB TESLA V100 GPU, using AdamW [11] optimizer. The batch size was set to 4, with a learning rate of 1e -4 and a weight decay of 1e -5 .   [7,12], CNN-MIL [20], DSMIL [9], CLAM [13], ViT-MIL [5], TransMIL [16], SETMIL [27], and DTFD [26]. All methods were evaluated in three tasks, namely gene mutation prediction (with or without EGFR mutation), TCGA-NSCLC subtype classification, and TCGA-RCC subtype classification. IIB-MIL achieved AUCs of 85.62%, 98.11%, and 99.57%. We can also find IIB-MIL outperformed other SOTA methods, in the three tasks with at least 1.78%, 0.74%, and 0.56% performance enhancement (AUC), respectively."
IIB-MIL: Integrated Instance-Level and Bag-Level Multiple Instances Learning with Label Disambiguation for Pathological Image Analysis,4.2,Ablation Studies,"We conducted ablation studies to assess the efficacy of each component in IIB-MIL. The results, in Table 2, indicate that all of the designed components, including the label disambiguation module, instance-level supervision, and baglevel supervision, contribute to the success of IIB-MIL. We also investigated the impact of the warm-up epoch number and found that selecting an appropriate value, such as warmup = 10, can lead to better model performance. Furthermore, we examined the impact of the weighting factor λ, and the outcomes indicated that assigning greater importance to instance-level supervision (λ = 5) helps IIB-MIL enhance its performance, thus demonstrating the effectiveness of the designed label-disambiguation-based instance-level supervision."
IIB-MIL: Integrated Instance-Level and Bag-Level Multiple Instances Learning with Label Disambiguation for Pathological Image Analysis,4.3,Model Interpretation,"Figure 2(a) shows the t-SNE plot of the obtained patch features from the backbone of the IIB-MIL. The patches are unsupervisedly clustered into groups based on their features, indicated by various colors. The numbers displayed within each group represent the average likelihood of the EGFR mutation predicted by the patches. With the help of the label-disambiguation-based instance-level supervision, IIB-MIL can identify highly positive and negative related patches to the WSI-label, i.e., the cyan-blue group and yellow group. Double-checked by pathologists, we find that the cyan-blue group consists of patches from lung adenocarcinoma and the yellow group consists of patches from the squamous cells. This finding aligns with the domain knowledge of pathologists. Figure 2(b) investigates the contribution of each patch in predicting EGFR mutation. The resulting heatmap shows the decision mechanism of IIB-MIL in the accurate distinguishment between EGFR mutation-positive and negative samples."
IIB-MIL: Integrated Instance-Level and Bag-Level Multiple Instances Learning with Label Disambiguation for Pathological Image Analysis,5,Conclusion,"This paper presents IIB-MIL, a novel MIL approach for pathological image analysis. IIB-MIL utilizes a label disambiguation module to establish more precise instance-level supervision. It then combines the instance-level and bag-level supervision to enhance the performance of the IIB-MIL. Experimental results demonstrate that IIB-MIL surpasses current SOTA techniques on publicly available datasets, and holds significant potential for addressing more complex clinical applications, such as predicting gene mutations. Furthermore, IIB-MIL can identify highly relevant patches, providing pathologists with valuable insights into underlying mechanisms."
IIB-MIL: Integrated Instance-Level and Bag-Level Multiple Instances Learning with Label Disambiguation for Pathological Image Analysis,,Fig. 1 .,
IIB-MIL: Integrated Instance-Level and Bag-Level Multiple Instances Learning with Label Disambiguation for Pathological Image Analysis,,Fig. 2 .,
IIB-MIL: Integrated Instance-Level and Bag-Level Multiple Instances Learning with Label Disambiguation for Pathological Image Analysis,,Table 1 .,
IIB-MIL: Integrated Instance-Level and Bag-Level Multiple Instances Learning with Label Disambiguation for Pathological Image Analysis,,Table 2 .,
IIB-MIL: Integrated Instance-Level and Bag-Level Multiple Instances Learning with Label Disambiguation for Pathological Image Analysis,,Results and Discussion 4.1 Comparison with State-of-the Art Methods,
IIB-MIL: Integrated Instance-Level and Bag-Level Multiple Instances Learning with Label Disambiguation for Pathological Image Analysis,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 54.
Multi-scale Prototypical Transformer for Whole Slide Image Classification,1,Introduction,"Histopathological images are regarded as the 'gold standard' in the diagnosis of cancers. With the advent of the whole slide image (WSI) scanner, deep learning has gained its reputation in the field of computational pathology [1][2][3]. However, WSIs are extremely large in the size and lack of pixel-level annotations, making it difficult to adopt the traditional supervised learning methods for WSI classification [4].To address this issue, multiple instance learning (MIL) has been successfully applied to the WSI classification task as a weakly supervised learning problem [5][6][7]. In this context, a WSI is considered as a bag, and the cropped patches within the slide are the instances in this bag. However, the lesion regions usually only account for a small portion of the WSI, resulting in a large number of negative patches. When the positive and negative instances in the bag are highly imbalanced, the MIL models are prone to incorrectly discriminate these positive instances when using simple aggregation operations. To this end, several attention-based MIL models, such as ABMIL [8] and DSMIL [9], apply variants of the attention mechanism to re-weight instance features. Thereafter, the recent works develop the Transformer-based architectures to better model long-range instance correlations via self-attention [10][11][12][13]. However, since the average bag size of a WSI is more than 8000 at 20 × magnification, it is computationally infeasible to use the conventional Transformer and other stacked self-attention network architectures in MIL-related tasks.Recently, prototypical learning is applied in WSI analysis to identify representative instances in the bag [14]. Some works adopt the K-means clustering on all instances in a bag to obtain K cluster centers i.e., instance prototypes, and then use these prototypes to represent the bags [15,16]. These clustering-based MIL algorithms can significantly reduce the redundant instances, and thereby improving the training efficiency for WSI classification. However, it is different for K-means to specify the cluster number as well as the initial cluster centers, and different initial values may lead to different cluster results, thus affecting the performance of MIL. Besides, affected by the feature extractor, the clustering-based MIL algorithms may ignore the most important instances that contain critical diagnostic information. Therefore, it is necessary to develop a method that can fully exploit the potential complementary information between critical instances and prototypes to improve representation learning of prototypes.On the other hand, when pathologists analysis the WSIs, they always observe the tissues at various resolutions [17]. Inspired by this diagnostic manner, some works use multi-scale information of WSIs to improve diagnostic accuracy. For example, Li et al. [9] adopted a pyramidal concatenation mechanism to fuse the multi-scale features of WSIs, in which the feature vectors of low-resolution patches are replicated and concatenated with the those of their corresponding high-resolution patches; Hou et al. [18] propose a heterogeneous graph neural network to learn the hierarchical representation of WSIs from a heterogeneous graph, which is constructed by the feature and spatial-scaling relationship of multi-resolution patches. However, since the number of patches at each resolution is quite different, it requires complex pre-processing to spatially align feature vectors of patches in different resolutions. Therefore, it is significant to develop an efficient and effective patch aggregation strategy to learn multi-scale information from WSIs.In this work, we propose a Multi-Scale Prototypical Transformer (MSPT) for WSI classification. The MSPT includes two key components: a prototypical Transformer (PT) and a multi-scale feature fusion module (MFFM). The specifically developed PT uses a clustering algorithm to extract instance prototypes from the bags, and then re-calibrates these prototypes at each scale with the self-attention mechanism in Transformer [19]. MFFM is designed to effectively fuse multi-scale information of WSIs, which utilizes the MLP-Mixer [20] to learn effective representations by aggregating the multi-scale prototypes generated by the PT. The MLP-Mixer adopts two types of MLP layers to allow information communication in different dimensions of data.The contributions of this work are summarized as follows:1) A novel prototypical Transformer (PT) is proposed to learn superior prototype representation for WSI classification by integrating prototypical learning into the Transformer architecture. It can effectively re-calibrate the cluster prototypes as well as reduce the computational complexity of the Transformer. 2) A new multi-scale feature fusion module (MFFM) is developed based on the MLP-Mixer to enhance the information communication among phenotypes. It can effectively capture multi-scale information in WSI to improve the performance of WSI classification."
Multi-scale Prototypical Transformer for Whole Slide Image Classification,2,Method,
Multi-scale Prototypical Transformer for Whole Slide Image Classification,2.1,MIL Problem Formulation,"MIL is a typical weakly supervised learning method, where the training data consists of a set of bags, and each bag contains multiple instances. The goal of MIL is to learn a classifier that can predict the label of a bag based on the instances in it. In binary classification, a bag can be marked as negative if all in-stances in the bag are negative, otherwise, the bag is labeled as positive with at least one positive instance. In the MIL setting, a WSI is considered as a bag and the numerous cropped patches in WSI are regarded as instances in the bag. A WSI dataset T can be defined as:where x i denotes a patient, y i the label of x i , I j i is the j-th instance of x i , N is the number of patients and n is the number of instances."
Multi-scale Prototypical Transformer for Whole Slide Image Classification,2.2,Multi-scale Prototypical Transformer (MSPT),"The overall architecture of MSPT is shown in Fig. 1. A WSI is first divided into nonoverlapping patches at different resolutions, and a pre-trained ResNet18 [21] is used to extract features from each patch. The learned multi-scale features are then fed into the proposed MSPT, which consists of a PT and an MFFM, to re-calibrate cluster prototypes at each scale and fuse multi-scale information of WSI. Finally, a WSI-level classifier is trained to predict the bag label.Pre-training. It is a time consuming and tedious task for pathologists to annotate the patch-level labels in gigapixel WSIs, thus, a common practice is to use a pre-trained encoder network to extract instance-level features, such as an ImageNet pre-trained encoder or a self-supervised pre-trained encoder. In this work, we follow [9] to adopt SimCLR [22] to pre-training the patch encoder at different resolutions. SimCLR is a self-supervised learning algorithm to pre-trainng a network by maximizing the similarity between positive pairs and minimizing the similarity between negative pairs [22]. After pre-training, the extracted instances of different scales are fed into MSPT for prototype learning and multi-scale learning. "
Multi-scale Prototypical Transformer for Whole Slide Image Classification,,Prototypical Transformer (PT).,"Most tissues in WSIs are redundancy, and therefore, we introduce the instance prototypes to reduce redundant instances. Specifically, for each instance bag X bag ∈ R n×d k , the K-means clustering algorithm is applied on all instances to get K centers (prototypes). These cluster prototypes can be used as instances to represent a new bag P bag ∈ R k×d k . However, the K-means clustering algorithm is sensitive to the initial selection of cluster centers, i.e. different initializations can lead to different results, and the final result may not be the global optimal solution. It is essential to try different initializations and choose the one with the lowest error. However, the WSI dataset generally has a long sequence of instances, which makes the clustering algorithms computationally expensive and slow down as the size of the bag increases.To solve the issue above, we propose to apply the self-attention (SA) mechanism in Transformer to re-calibrate these cluster prototypes. As shown in Fig. 1, the optimization process can be divided into two steps: 1) the initial cluster prototype bag P bag is obtained in the pre-processing stage by using the K-means clustering on X bag ; ; 2) PT uses X bag to optimize P bag via the self-attention mechanism in Transformer. The detailed process is as follows:where W q , W k , W v ∈ R d k ×d k are trainable matrices of query P bag and the key-value pair (X bag , X bag ), respectively, and A map ∈ R k×n is the attention matrix to compute the weight of X bag . Thus, the computational complexity of SA is O(nm) instead of O n 2 , and the k is much less than n. Specifically, for a single clustering prototype p k ∈ P, the SA layer scores the pairwise similarity between p k and x n for all x n ∈ X, which can be written as a row vector [a k1 , a k2 , a k3 , . . . , a kn ] in A map . These attention scores are then weighted to X bag to update the p k ∈ R 1×d k for completing the calibration of the clustering prototypes P ∈ R k×d k .As mentioned above, existing clustering-based MIL methods use the K-means clustering to identify instances prototypes in the bag, where the most important instances that contain the key semantic information may be ignored. On the contrary, our PT can efficiently use all the instances to update the cluster prototypes multiple times. Therefore, the combination of bag instances is no longer static and fixed, but diverse and dynamic. It means that different new bags can be fed into the MFFM each time. In addition, by applying PT to each scale, the number of cluster prototypes obtained at different scales is consistent, so there is no need for additional operations to align multi-scale features.Multi-scale Feature Fusion Module (MFFM). To fuse the output clustered prototypes at different scales in MSPT, we proposed an MFFM, which consists of an MLP-Mixer and a Gated Attention Pooling (GAP). The MLP-Mixer is used to enhance the information communication of the prototype representation, and the GAP is used to get the WSI-level representation for WSI classification.As shown in Fig. 2, The Mixer layer of MLP-Mixer contains one token-mixing MLP and one channel-mixing MLP, each consisting of two fully-connected layers and a GELU activation function [23]. Token-mixing MLP is a cross-location operation to mix all prototypes, while channel-mixing MLP is a pre-location operation to mix features of each prototype. Thus, MLP-Mixer allows the information communication between different prototypes and prototype features to learn superior representation through information aggregation. Specifically, the procedure of MFFM is described as follows:We first perform the feature concatenation operation on the multi-scale output clustering prototypes P20× , P10× , P5× to construct a feature pyramid P:where d k is the feature vector dimension of the prototypes.Then, the P is fed to the MLP-Mixer to obtain the corresponding hidden feature representation H ∈ R k×3d k as follows:where LN denotes the layer normalization, σ denotes the activation function implemented by GELU,k are the weight matrices of MLP layers.c and d s are tunable hidden widths in the token-mixing and channel-mixing MLP, respectively. Finally, the H is fed to the gated attention pooling (GAP) [8] to get the WSI-level representation Z ∈ R 1×3d k for WSI classification:where Y ∈ R 1×d out is the class label probability of the bag, and d out is the number of classes."
Multi-scale Prototypical Transformer for Whole Slide Image Classification,3,Experiments and Results,
Multi-scale Prototypical Transformer for Whole Slide Image Classification,3.1,Datasets,"To evaluate the effectiveness of MSPT, we conducted experiments on two public dataset, namely Camelyon16 [24] and TCGA-NSCLC. Camelyon16 is a WSI dataset for the automated detection of metastases in lymph node tissue slides. It includes 270 training samples and 129 testing samples. After pre-processing, a total of 2.4 million patches at ×20 magnification, 0.56 million patches at ×10 magnification, and 0.16 million patches at ×5 magnification, with an average of about 5900, 1400, and 400 patches per bag. The TCGA-NSCLC dataset includes two sub-types of lung cancer, i.e., Lung Squamous Cell Carcinoma (TGCA-LUSC) and Lung Adenocarcinoma (TCGA-LUAD). We collected a total of 854 diagnostic slides from the National Cancer Institute Data Portal (https:// portal.gdc.cancer.gov). The dataset yields 4.3 million patches at 20× magnification, 1.1 million patches at 10× magnification, and 0.30 million patches at 5× magnification with an average of about 5000, 1200, and 350 patches per bag."
Multi-scale Prototypical Transformer for Whole Slide Image Classification,3.2,Experiment Setup and Evaluation Metrics,"In WSI pre-processing, each slide is cropped into non-overlapping 256 × 256 patches at different magnifications, and a threshold is set to filter out background ones. After patching, we use a pre-trained ResNet18 model to convert each 256 × 256 patch into a 512-dimensional feature vector. We selected accuracy (ACC) and area under curve (AUC) as evaluation metrics. For Camelyon16 dataset, we reported the results of the official testing set. For TCGA-NSCLC, we conducted five cross-validation on the 854 slides, and the results are reported in the format of mean ±SD (standard deviation)."
Multi-scale Prototypical Transformer for Whole Slide Image Classification,3.3,Implementation Details,"For the feature extractor, we employed the SimCLR encoder trained by Lee et al. [9] for the Camelyon16 and TCGA datasets. But [9] only trained SimCLR encoders at 20× and 5× magnification, to align with that setting, we used the same settings to train the SimCLR encoder at 10× magnification on both datasets. For the proposed MSPT, the Adam optimizer was used to update the model weights, the initial learning rate of 1e-4 with a weight decay of 1e-5. The mini-batch size was set as 1. The MSPT models were trained for 150 epochs and they would early stop if the loss would not decrease in the past 30 epochs. All models were implemented by Python 3.8 with PyTorch toolkit 1.11.0 on a platform equipped with an NVIDIA GeForce RTX 3090 GPU."
Multi-scale Prototypical Transformer for Whole Slide Image Classification,3.4,Comparisons Experiment,"Comparison Algorithms. The proposed MSPT was compared to state-of-the-art MILbased algorithms: 1) The traditional pooling operators, such as mean-pooling and maxpooling; 2) the attention-based algorithms, including ABMIL [8] and DSMIL [9]; 3) the Transformer-based algorithm TransMIL [11]; 4) The clustering-based algorithm ReMix [16].  1 shows the comparison results on the Camelyon16 and TCGA-NSCLC datasets. In CAMELYON16, it can be found that the proposed MSPT outperforms all the compared algorithms with the best accuracy of 0.9536, and AUC of 0.9869. Compared to other algorithms, MSPT improves at least 0.78%, and 1.07% on classification ACC and AUC, indicating the effectiveness of MFFM to learn the multi-scale information of WSIs. In addition, PT achieves the best classification results in the single-resolution methods and outperforms ReMix on all indices, which proves PT can effectively re-calibrate the clustering prototypes.In TCGA-NSCLC, the proposed MSPT algorithm again outperforms all the compared algorithms on all indices. It achieves the best classification performance of 0.9289 ± 0.011 and 0.9622 ± 0.015 on the ACC and AUC. Moreover, MSPT improves at least 0.78% and 1.03%, respectively, on the corresponding indices compared with all other algorithms."
Multi-scale Prototypical Transformer for Whole Slide Image Classification,3.5,Ablation Study,"To evaluate the contribution of PT and MFFM in the proposed MSPT, we further conducted a series of ablation studies.Investigation of the Number of Prototypes in PT. To evaluate the effectiveness of the PT, we first changed the number of prototypes K in the range of {1, 2, 4, 8, 16, 32} to get the optimal K for each dataset. Then, the following two variants were compared with PT: (1) Full-bag: the first variant was only trained on all the instances; (2) Prototype-bag: the second variant was only trained on the cluster prototypes.As shown in Fig. 3, the horizontal axes denote the number of prototypes, and the vertical axes denote the classification accuracy. In the Camelyon16 dataset, the performance of both PT and Prototype-bag increases with the increase of K value, and achieves the best results with K = 16. In the TCGA-NSCLC dataset, PT always outperforms the Fullbag and Prototype-bag. These experimental results demonstrate that PT can effectively re-calibrate the clustering prototypes to achieve superior results. Investigation of Multi-scale Fusion. We further compared our MFFM with several other fusion strategies, including (1) Concatenation: this variant concatenated the cluster prototypes of each magnification before the classifier. (2) MS-Max: this variant used max-pooling on the cluster prototypes for each magnification, and then added them. (3) MS-Attention: this variant used attention-pooling [8] on the cluster prototypes for each magnification, and then added them.Table 2 gives the results on the Camelyon16 and TCGA-NSCLC datasets. Compared with other multi-scale variants, the proposed MSPT improves ACC by at least 0.78% and 0.85% on Camelyon16 and TCGA-NSCLC, respectively, which proves that the MLP-Mixer in MFFM can effectively enhance the information communication among phenotypes and their features, thus improving the performance of feature aggregation. More Studies. We provide more empirical studies, i.e., the effect of the multiresolution scheme, the visualization results, and the training budgets, in Supplementary Materials to better understand MSPT."
Multi-scale Prototypical Transformer for Whole Slide Image Classification,4,Conclusion,"In summary, we propose an MSPT for WSI classification that combine the prototypebased learning and multi-scale learning to generate powerful WSI-level representation. The MSPT reduces redundant instances in WSI bags by replacing instances with updatable instance prototypes, and avoids complicated procedures to align patch features at different scales. Extensive experiments validate the effectiveness of the proposed MSPT. In the future, we will develop an attention mechanism based on the magnification level to re-weight the features from different scales before fusion in MSPT."
Multi-scale Prototypical Transformer for Whole Slide Image Classification,,Fig. 1 .,
Multi-scale Prototypical Transformer for Whole Slide Image Classification,,Fig. 2 .,
Multi-scale Prototypical Transformer for Whole Slide Image Classification,,Fig. 3 .,
Multi-scale Prototypical Transformer for Whole Slide Image Classification,,Table 1 .,
Multi-scale Prototypical Transformer for Whole Slide Image Classification,,Table 2 .,
SCOL: Supervised Contrastive Ordinal Loss for Abdominal Aortic Calcification Scoring on Vertebral Fracture Assessment Scans,1,Introduction,"Abdominal Aortic Calcification (AAC) is an established marker of atherosclerotic cardiovascular disease (CVD) [19] and can help in identifying asymptomatic cases at risk for CVD-related hospitalizations and deaths [14]. CVDs are responsible for 32% of global deaths, with atherosclerotic CVD events being the leading cause [17]. Therefore, early detection and management of AAC can improve CVD prevention, and management [14]. AAC can be seen on lateral spine Vertebral Fracture Assessment (VFA) images acquired using Dual-energy X-ray Absorptiometry (DXA), X-rays or Computed Tomography (CT) [13,19]. However, DXA imaging is the most recommended and cost-effective approach for VF assessment, with the least radiation exposure [12,23]. In VFA DXA scans, AAC can be calculated manually using the Kauppila AAC-24 semi-quantitative scale [10]. However, the manual scoring of AAC in VFA images is arduous and subjective [18,19]. Thus, designing an automated system for detecting and quantifying AAC in VFA DXA scans may be the most feasible approach for obtaining valuable CVD risk information in asymptomatic individuals.Very few attempts have been made for automated AAC scoring in VFA DXA scans [4,6,7,18]. Reid et al. [18] trained two CNN models using VFA DXA scans and reported results via ensembling on a limited test set. Gilani et al. [7] performed sequential scoring using a vision-to-language model, but achieved low sensitivity for a moderate-risk class, indicating difficulty in handling complex cases near class boundaries. These works [7,18] considered automated AAC scoring a regression task. However, simple regression losses depend on continuous regression labels and, thus, cannot make feature embeddings separable. Moreover, inter-class similarities, intra-class variations and image artifacts in lowresolution VFA DXA scan further complicate the task. Therefore, using these losses directly for AAC-24 score regression may not be optimal.Contrastive representation learning has shown promising results in medical image classification [2,9,16] and segmentation tasks [3,8,24]. In the classification tasks, supervised contrastive learning (SupCon) [11] aims to bring feature embeddings with the same labels closer together in the latent space and move the dissimilar ones apart. However, SupCon cannot preserve the ordinal information of the regression labels in the latent space [5]. To address this, Dai et al. [5] propose supervised Adaptive Contrastive loss (AdaCon), which depends on an adaptive margin. For calculating the adaptive margin, they assumed that a regression label could be replaced with its Empirical Cumulative Distribution Function (ECDF) [22]. Though this assumption might be valid for large datasets, it may not work with highly skewed, imbalanced and limited-size datasets [5].To this end, we propose a novel Supervised Contrastive Ordinal Loss (SCOL), considering AAC scoring as an ordinal regression problem. We integrate a labeldependent distance metric with the supervised contrastive loss. Unlike AdaCon [5], this metric relies exclusively on discrete regression labels, making it possible to utilize the ordinal information inherent in these labels. Using SCOL, we design an effective Dual-encoder Contrastive Ordinal Learning (DCOL) framework. Unlike previous methods, which either use global [18] or local attentionbased features [7], DCOL assimilates global and local feature embeddings to increase feature diversity, and class separability in latent space.To the best of our knowledge, this is the first framework that explores contrastive learning for automated detection of AAC. Our contributions are summarized as follows: 1) We propose a novel supervised contrastive ordinal loss by incorporating distance metric learning with the supervised contrastive loss to improve inter-class separability and handle intra-class diversity among the AAC genera. 2) We design a Dual-Encoder Contrastive Ordinal Learning framework using the proposed loss to learn separable feature embeddings at global and local levels. 3) We achieve state-of-the-art results on two clinical datasets acquired using DXA machines from multiple manufacturers, demonstrating the generalizability and efficacy of our approach. 4) We compare the Major Adverse Cardiovascular Event (MACE) outcomes for machine-predicted AAC scores and the human-measured scores to explore the clinical relevance of our method. This work aims to contribute clinically in refining automated AAC prediction methods using low-energy VFA scans. Our code is available at [1]. "
SCOL: Supervised Contrastive Ordinal Loss for Abdominal Aortic Calcification Scoring on Vertebral Fracture Assessment Scans,2,Methodology,
SCOL: Supervised Contrastive Ordinal Loss for Abdominal Aortic Calcification Scoring on Vertebral Fracture Assessment Scans,2.1,Supervised Contrastive Ordinal Learning,"Consider a training set T of M image-label pairs, such that T = {(x i , y i )} M i=1 where x i is the i th VFA DXA scan and y i is the corresponding AAC score. Let a, p and n denote the indices of anchor, positive sample, and negative samples in a batch I. P (i) is the set of indices of all the positive samples, i.e., having the same AAC score as y a and N (i) is the set of all other negative indices. Consider an encoder-projector network that maps the anchor image x a in the embedding space such that z a = P roj(Enc(x a )), then the similarity between any two projections z i and z k in the latent space is: sim(z i , z k ) = z T i .z k . Supervised contrastive loss [11] pulls images of the same class (positive samples) close together and pushes the negative ones apart. Following this strategy, we propose contrastive ordinal loss to move the negative sample x n apart from the anchor x a by a distance r (a,n) : such that if a < b < c then r(a, b) must be less than r(a, c) and vice versa. By incorporating this ordinal distance metric with the similarity function, we can maximize the benefit of ordinal information present in regression labels. Furthermore, in our AAC scoring task, this ordinal distance metric can help to increase inter-class separability and minimize the effect of intra-class variations. (For visualization see SM). Inspired by [5], we propose Supervised Contrastive Ordinal Loss (SCOL) for ordinal regression as:where τ is a scaling hyper-parameter for contrastive loss and r (a,n) is the distance metric between two labels y (a) and y (n) . If C is the number of ordinal labels in training set T, then r (a,n) is calculated as:From the above equation, it can be seen that our ordinal distance metric is monotonically increasing, i.e., if a < b < c, then r a,b must be less than r a,c and vice versa. This property allows the metric to maintain the ordinality of the data while improving the class separability in the latent space."
SCOL: Supervised Contrastive Ordinal Loss for Abdominal Aortic Calcification Scoring on Vertebral Fracture Assessment Scans,2.2,Dual-Encoder Contrastive Ordinal Learning,The proposed Dual-Encoder Contrastive Ordinal Learning (DCOL) framework consists of two stages: Stage-I: contrastive ordinal feature learning and Stage-II: relevant AAC risk class prediction via AAC-24 score regression (Fig. 1).
SCOL: Supervised Contrastive Ordinal Loss for Abdominal Aortic Calcification Scoring on Vertebral Fracture Assessment Scans,,Stage-I: It consists of two modules: Local Contrastive Ordinal Learning (LCOL),"and Global Contrastive Ordinal Learning (GCOL). In these modules, we train the global and local encoder-projector networks individually in an end-to-end manner to extract contrastive feature embeddings."
SCOL: Supervised Contrastive Ordinal Loss for Abdominal Aortic Calcification Scoring on Vertebral Fracture Assessment Scans,,Local Contrastive Ordinal Learning:,"In practice, to quantify AAC, clinicians focus on the aortic region adjacent to lumbar vertebrae L1-L4. Following this, in the Localized feature-based Contrastive Ordinal Learning module, LCOL, we integrate a simple yet effective localized attention block with the baseline encoder E l to roughly localize the aorta's position using only regression labels. This attention block is attached with E l after extracting the deep feature map f m from the last convolution layer. Our localized attention block consists of two 2D convolutional layers, followed by batch normalization and ReLu activation layers. This set of layers is then followed by an average pooling layer and sigmoid activation to create an activation map f s for the most salient features in the given image. Multiplying this activation map f s with the initial feature map f m results in extracting the most significant features from f m . These features are then projected into the latent space for processing by our SCOL loss. SCOL encourages the local contrastive embeddings with the same AAC score to move closer and the dissimilar ones apart based on the distance between their labels."
SCOL: Supervised Contrastive Ordinal Loss for Abdominal Aortic Calcification Scoring on Vertebral Fracture Assessment Scans,,Global Contrastive Ordinal Learning: In the Global Contrastive Ordinal,"Learning module, we extract the global representation of a given VFA DXA scan. In encoder E g , we replace the fully connected layers of the vanilla CNN model with a global average pooling (GAP ) layer for feature extraction. These feature embeddings are then passed to the projection network P g . SCOL operates on projected embeddings extracted from the whole lumbar region to maximize the feature separability while preserving the ordinal information in latent space. Both projector networks, P l and P g , consist of two Dense layers with 1280 and 128 neurons each, followed by ReLu activation."
SCOL: Supervised Contrastive Ordinal Loss for Abdominal Aortic Calcification Scoring on Vertebral Fracture Assessment Scans,,Stage-II:,"In AAC-24 score regression, a small change in pixel-level information can move the patient from low to moderate or moderate to high-risk AAC class. Thus, to decrease the effect of intra-class variations and to increase the inter-class separability, we assimilate the features extracted from encoders E l and E g . The resultant feature vector is fed as input to a feed-forward network consisting of two Dense layers with 1280 and 128 neurons each, followed by ReLu activation. Finally, a linear layer predicts the final AAC regression score. This module is trained using root mean squared error loss L rmse calculated as:, where m is the number of samples, y i are actual and y i are predicted AAC scores. The resulting AAC scores are then further classified into three AAC risk classes."
SCOL: Supervised Contrastive Ordinal Loss for Abdominal Aortic Calcification Scoring on Vertebral Fracture Assessment Scans,3,Experiments and Results,
SCOL: Supervised Contrastive Ordinal Loss for Abdominal Aortic Calcification Scoring on Vertebral Fracture Assessment Scans,,Dataset and Annotations:,"We conducted experiments on two de-identified clinical datasets acquired using the Hologic 4500A and GE iDXA scanners. Both datasets are manually annotated by clinicians using the AAC-24 scale [10] which divides the aortic walls into four segments based on the lumbar vertebrae bodies (L1 -L4). Each segment is assigned an AAC score 0: if no calcification (Cal), 1: Cal ≤ 1/3 of aortic wall, 2: Cal > 1/3 but < 2/3 of aortic wall and 3: Cal ≥ 2/3 of aortic wall. The total AAC-24 score can range from 0 to 24 and is further classified into three AAC risk classes using clinical thresholds [15]: Lowrisk (AAC < 2), Moderate-risk (2 ≤ AAC ≤ 5), and High-risk (AAC > 5).The Hologic Dataset [14] has 1,914 single-energy DXA scans acquired using a Hologic 4500A machine. Each scan has dimensions of at least 800 × 287 pixels. Among 1,914 scans, there are 764 scans belonging to the low-risk, 714 to moderate-risk and 436 scans to the high-risk AAC group. The iDXA GE Dataset [7,18] has 1,916 dual-energy VFA DXA scans. Among these, there are 829 belonging to low-risk, 445 to moderate-risk and 642 scans to the high-risk AAC group. These scans are acquired using an iDXA GE machine. These scans have dimensions of 1600 × 300 pixels.Implementation Details: Each VFA scan in both datasets contains a full view of the thoracolumbar spine. To extract the region of interest (ROI), i.e., the abdominal aorta near the lumbar spine, we crop the upper half of the image, resize it to 300×300 pixels and rescale it between 0 and 1. We apply data augmentations including rotation, shear and translation. We implement all experiments in TensorFlow [21], using stratified 10-fold cross-validation on a workstation with NVIDIA RTX 3080 GPU. (For details, see SM.) In stage-I, we adopted an efficient, smaller and faster pre-trained model, EfficientNet-V2S [20], as the backbone for both encoders. We train stage-I for 200 epochs and stage II for 75 epochs using the RMSprops optimizer with the initial learning rate of 3 × 10 -4 and a batch size of 16. τ in the proposed SCOL is 0.2. The inference time for each scan is less than 15 ms. To avoid overfitting, we used early stopping and reduce LR on the plateau while optimizing the loss."
SCOL: Supervised Contrastive Ordinal Loss for Abdominal Aortic Calcification Scoring on Vertebral Fracture Assessment Scans,,Evaluation Metrics:,"The performance of the AAC regression score is evaluated in terms of Pearson's correlation, while for AAC risk classification task Accuracy, F1-Score, Sensitivity, Specificity, Negative Predictive Value (NPV) and Positive Predictive Value (PPV) are used in One Vs. Rest (OvR) setting. Baseline: EfficientNet-V2S model trained in regression mode using RMSE loss. Ablation Study: Table 1 highlights the efficacy of our proposed loss SCOL. We train our dual-encoder contrastive learning framework with proposed SCOL, SupCon [11] and AdaCon [5], individually, on the Hologic dataset. We also evaluate the performance of the local and global contrastive modules (LCL and GCL) with each contrastive loss. Table 1 also shows the strength of integrating the localized attention block with the baseline model trained with RMSE loss.Comparison with the Baseline: In Table 2, we compare the performance of our framework with the baseline on both datasets. For the Hologic dataset, our proposed method improved Pearson's correlation coefficient from a baseline of 0.87 to 0.89 and 3-class classification accuracy from 73%±3.82 to 78%±3.65 with (p < 0.001). While, for the iDXA GE dataset, the proposed method enhanced the Pearson's correlation from a baseline of 0.89 to 0.91 and averaged 3-class classification accuracy from 77% ± 3.9 to 80% ± 5.12 with (p < 0.001). Comparison with the State-of-the-Art (SOTA): Table 3 shows the comparison of our proposed framework with two SOTA methods [7,18] using the iDXA GE dataset. Our approach outperforms [18] by an average of 15.62% in accuracy and 20.7% in sensitivity with (p < 0.001), while in comparison to [7], accuracy is improved by 4.41% and sensitivity by 9.21%, with (p < 0.001)."
SCOL: Supervised Contrastive Ordinal Loss for Abdominal Aortic Calcification Scoring on Vertebral Fracture Assessment Scans,,Clinical Analysis and Discussion:,"To signify the clinical significance, we estimate the AUCs (Fig. For the iDXA GE Dataset, in the cohort of 1877 patients with clinical followup, 160 experienced a MACE event. The AUCs of predicted AAC-24 scores were similar AUC to human AAC-24 (0.64 95%CI 0.60-0.69 vs. 0.63 95%CI 0.59-0.68). The predicted AAC groups had 877 (46.7%), 468 (24.9%), and 532 (28.3%) of people in the low, moderate, and high AAC groups, respectively, with MACE events occurring in 5.1%, 7.5%, and 15.0% of these groups, respectively. The age and sex-adjusted HR for MACE in the moderate AAC group was 1.21 95%CI 0.77-1.89, and 2.64 95% CI 1.80-3.86 for the high AAC group, compared to the low predicted AAC group, which were similar to the HRs of human AAC groups, i.e., for moderate and high AAC groups HR 1.15 95%CI 0.72-1.84 and 2.32 95% CI 1.59-3.38, respectively, compared to the human low AAC group."
SCOL: Supervised Contrastive Ordinal Loss for Abdominal Aortic Calcification Scoring on Vertebral Fracture Assessment Scans,4,Conclusion,"We propose a novel Supervised Contrastive Ordinal Loss and developed a Dualencoder Contrastive Ordinal Learning framework for AAC scoring and relevant AAC risk classification in low-energy VFA DXA scans. Our framework learns contrastive feature embeddings at the local and global levels. Our results demonstrate that the contrastive ordinal learning technique remarkably enhanced interclass separability and strengthened intra-class consistency among the AAC-24 genera, which is particularly beneficial in handling challenging cases near the class boundaries. Our framework with SCOL loss demonstrates significant performance improvements, compared to state-of-the-art methods. Moreover, the ablation studies also establish the effectiveness of our dual-encoder strategy and localized attention block. These results suggest that our approach has great clinical potential for accurately predicting AAC scores and relevant risk classes."
SCOL: Supervised Contrastive Ordinal Loss for Abdominal Aortic Calcification Scoring on Vertebral Fracture Assessment Scans,,Fig. 1 .,
SCOL: Supervised Contrastive Ordinal Loss for Abdominal Aortic Calcification Scoring on Vertebral Fracture Assessment Scans,,Fig. 2 .,
SCOL: Supervised Contrastive Ordinal Loss for Abdominal Aortic Calcification Scoring on Vertebral Fracture Assessment Scans,,Table 1 .,
SCOL: Supervised Contrastive Ordinal Loss for Abdominal Aortic Calcification Scoring on Vertebral Fracture Assessment Scans,,Table 2 .,
SCOL: Supervised Contrastive Ordinal Loss for Abdominal Aortic Calcification Scoring on Vertebral Fracture Assessment Scans,,Table 3 .,
SCOL: Supervised Contrastive Ordinal Loss for Abdominal Aortic Calcification Scoring on Vertebral Fracture Assessment Scans,,Acknowledgement and Data Use Declaration. De-identified labelled images,
Thyroid Nodule Diagnosis in Dynamic Contrast-Enhanced Ultrasound via Microvessel Infiltration Awareness,1,Introduction,"Contrast-enhanced ultrasound (CEUS) as a modality of functional imaging has the ability to assess the intensity of vascular perfusion and haemodynamics in the thyroid nodule, thus considered a valuable new approach in the determination of benign vs. malignant nodules [1]. In practice, CEUS video allows the dynamic observation of microvascular perfusion through intravenous injection of contrast agents. According to clinical experience, for thyroid nodules diagnosis, there are two characteristic that are important when analyzing CEUS video. 1) Dynamic microvessel perfusion. As shown in Fig. 1(A), clinically acquired CEUS records the dynamic relative intensity changes (microvessel perfusion pattern) throughout the whole examination [2]. 2) Infiltrative expansion of microvessel. Many microvessels around nodules are constantly infiltrating and growing into the surrounding tissue. As shown in Fig. 1(B), based on the difference in lesion size displayed by the two modalities, clinical practice shows that gray US underestimates the size of lesions, and CEUS video overestimates the size of some lesions [3]. Although the radiologist's cognition of microvascular invasive expansion is fuzzy, they think it may promote diagnosing thyroid nodules [1].  used the spatial feature enhancement for disease diagnosis based on dynamic CEUS. Furthermore, by combing the US modality, Chen et al. [5] proposed a domain-knowledge-guided temporal attention module for breast cancer diagnosis. However, due to artifacts in CEUS, SOTA classification methods often fail to learn regions where thyroid nodules are prominent (As in Appendix Fig. A1) [6]. Even the SOTA segmentation methods cannot accurately identify the lesion area for blurred lesion boundaries, thus, the existing automatic diagnosis network using CEUS still requires manual labeling of pixel-level labels which will lose key information around the tissues [7]. In particular, few studies have developed the CEUS video based diagnostic model inspired by the dynamic microvessel perfusion, or these existing methods generally ignore the influence of microvessel infiltrative expansion. Whether the awareness of infiltrative area information can be helpful in the improvement of diagnostic accuracy is still unexplored.Here, we propose an explanatory framework for the diagnosis of thyroid nodules based on dynamic CEUS video, which considers the dynamic perfusion characteristics and the amplification of the lesion region caused by microvessel infiltration. Our contributions are twofolds. First, the Temporal Projection Attention (TPA) is proposed to complement and interact with the semantic information of microvessel perfusion from the time dimension. Second, we adopt a group of confidence maps instead of binary masks to perceive the infiltrative expansion area from gray US to CEUS of microvessels for improving diagnosis."
Thyroid Nodule Diagnosis in Dynamic Contrast-Enhanced Ultrasound via Microvessel Infiltration Awareness,2,Method,"The architecture of the proposed framework is shown in Fig. 2. The tasks of lesion area recognition and differential diagnosis are pixel-level and image-level classifications, and some low-level features of these two tasks can be shared interactively [8]. We first fed the CEUS video I ∈ R C×T ×H×W into the cross-task feature extraction (CFA) module to jointly generate the features F iden and F cls for lesion area recognition and differential diagnosis, respectively. After that, in the temporal-based lesions area recognition (TLAR) module, an enhanced V-Net with the TPA is implemented to identify the relatively clear lesion area which are visible on both gray US and CEUS video. Because microvessel invasion expansion causes the tumor size and margin depicted by CEUS video to be larger than that of gray US, we further adopt a group of confidence maps based on Sigmoid Alpha Functions (SAF) to aware the infiltrative area of microvessels for improving diagnosis. Finally, the confidence maps are fused with F cls and fed into a diagnosis subnetwork based on lightweight C3D [9] to predict the probability of benign and malignant. In the CFA, we first use the 3D inception block to extract multi-scale features F muti . The 3D inception block has 4 branches with cascaded 3D convolutions. Multiple receptive fields are obtained through different branches, and then group normalization and ReLU activation are performed to obtain multi-scale features F muti . Then, we use the cross-task feature adaptive unit to generate the features F iden and F cls required for lesions area recognition and thyroid nodules diagnosis via the following formula [10]:where ω iden , ω cls are the learnable weights."
Thyroid Nodule Diagnosis in Dynamic Contrast-Enhanced Ultrasound via Microvessel Infiltration Awareness,2.1,Temporal-Based Lesions Area Recognition (TLAR),"The great challenge of automatic recognition of lesion area from CEUS video is that the semantic information of the lesion area is different in the CEUS video of the different microvessel perfusion periods. Especially in the perfusion period and the regression period, the semantic information of lesions cannot be fully depicted in an isolated CEUS frame. Thus, the interactive fusion of semantic information of the whole microvessel perfusion period will promote the identification of the lesion area, and we design the Temporal Projection Attention (TPA) to realize this idea. We use V-Net as the backbone, which consists of four encoder/decoder blocks for TLAR, and the TPA is used in the bottleneck of the V-Net."
Thyroid Nodule Diagnosis in Dynamic Contrast-Enhanced Ultrasound via Microvessel Infiltration Awareness,,Temporal Projection Attention (TPA). Given a feature,"16 after four down-sampling operations in encoder, its original 3D feature map is projected [11] to 2D plane to get keys and queries:16 , and we use global average pooling (GAP) and global maximum pooling (GMP) as temporal projection operations. Here,16 is obtained by a single convolution. This operation can also filter out the irrelevant background and display the key information of the lesions. After the temporal projection, a group convolution with a group size of 4 is employed on K to extract the local temporal attention L ∈ R C× H 16 × W 16 . Then, we concatenate L with Q to further obtain the global attention G ∈ R C×1× H 16 × W 16 by two consecutive 1 × 1 2D convolutions and dimension expend. Those operations are described as follows:where Gonv(•) is the group convolution, σ denotes the normalization, ""⊕"" is the concatenation operation. The global attention G encodes not only the contextual information within isolated query-key pairs but also the attention inside the keys [12]. After that, based on the 2D global attention G, we multiply V and G to calculate the global temporal fusion attention map16 to enhance the feature representation.Meanwhile, to make better use of the channel information, we use 3 16 . Then, we use parallel average pooling and full connection operation to reweight the channel information of F 4th to obtain the reweighted featureThe obtained global temporal fusion attention maps M are fused with the reweighted feature F 4th to get the output features F fin . Finally, we input F fin into the decoder of the TLAR to acquire the feature map of lesion."
Thyroid Nodule Diagnosis in Dynamic Contrast-Enhanced Ultrasound via Microvessel Infiltration Awareness,2.2,Microvessel Infiltration Awareness (MIA),"We design a MIA module to learn the infiltrative areas of microvessel. The tumors and margin depicted by CEUS may be larger than those depicted by gray US because of continuous infiltrative expansion. Inspired by the continuous infiltrative expansion, a series of flexible Sigmoid Alpha Functions (SAF) simulate the infiltrative expansion of microvessels by establishing the distance maps from the pixel to lesion boundary. Here, the distance maps [13] are denoted as the initial probability distribution P D . Then, we utilize Iterative Probabilistic Optimization (IPO) unit to produce a set of optimized probability maps P = {p 1 , p2 . . . pn } to aware the microvessel infiltration for thyroid nodules diagnosis. Based on SAF and IPO, CEUS-based diagnosis of thyroid nodules can make full use of the ambiguous information caused by microvessel infiltration."
Thyroid Nodule Diagnosis in Dynamic Contrast-Enhanced Ultrasound via Microvessel Infiltration Awareness,,Sigmoid Alpha Function (SAF).,"It is generally believed that the differentiation between benign and malignant thyroid nodules is related to the pixels around the boundaries of the lesion [14], especially in the infiltrative areas of microvessel [3]. Therefore, we firstly build the initial probability distribution P D based on the distance between the pixels and the annotation boundaries by using SAF in order to aware the infiltrative areas. Here, SAF is defined as follows:where α is the conversion factor for generating initial probability distribution P D (when α → ∞, the generated P D is binary mask); C is used to control the function value within the range of [0, 1]; (i, j) is the coordinate point in feature map; D (i, j) indicates the shortest distance from (i, j) to lesion's boundaries.Iterative Probabilistic Optimization (IPO) Unit. Based on the fact that IncepText [15] has experimentally demonstrated that asymmetric convolution can effectively solve the problem of highly variable size and aspect ratio, we use asymmetric convolution in the IPO unit. Asymmetric convolution-based IPO unit can optimize the initial distribution P D to generate optimized probability maps P that can reflect the confidence of benign and malignant diagnosis. Specifically, with the IPO, our network can make full use of the prediction information in low-level iteration layer, which may improve the prediction accuracy of high-level iteration layer. In addition, the parameters in the high-level iteration layer can be optimized through the back-propagation gradient from the high-level iteration layer. IPO unit can be shown as the following formula:where ""⊕"" represents the concatenation operation;ConvBlock consists of a group of asymmetric convolutions (e.g., Conv1 × 5, Conv5 × 1 and Conv1 × 1); n denotes the number of the layers of IPO unit. With the lesion's feature map f 0 from the TLAR module, the initial distribution P D obtained by SAF is fed into the first optimize layer of IPO unit to produce the first optimized probability map p1 . Then, p1 is contacted with f 0 , and used to generate optimized probability map p2 through the continuous operation based on SAF and the second optimize layer of IPO unit. The optimized probability map pi-1 provides prior information for producing the next probability map pi . In this way, we can get a group of probability map P to aware the microvascular infiltration."
Thyroid Nodule Diagnosis in Dynamic Contrast-Enhanced Ultrasound via Microvessel Infiltration Awareness,2.3,Loss Function,"With continuous probability map P obtained from MIA, P are multiplied with the feature F cls . Then, these maps are fed into a lightweight C3D to predict the probability of benign and malignant, as shown in Fig. 2. We use the mean square error L MSE to constrain the generation of P . Assuming that the generated P is ready to supervise the classification network, we want to ensure that the probability maps can accurately reflect the classification confidence. Thus, we design a task focus loss L ta to generate confidence maps P , as follows:where g i is the label of pi , which is generated by the operation of SAF(D (i,j) , α i ); pi denotes pixel in the image domain Ω, σ is a learnable parameter to eliminate the hidden uncertainty information.For differentiating malignant and benign, we employ a hybrid loss L total that consists of the cross-entropy loss L cls , the loss of L MSE computing optimized probability maps P , and task focus loss L ta . The L total is denoted as follows:where λ 1 , λ 2 , λ 3 are the hyper-parameters to balance the corresponding loss. As the weight parameter, we set λ 1 , λ 2 , λ 3 are 0.5,0.2,0.3 in the experiments."
Thyroid Nodule Diagnosis in Dynamic Contrast-Enhanced Ultrasound via Microvessel Infiltration Awareness,3,Experiments,"Dataset. Our dataset contained 282 consecutive patients who underwent thyroid nodule examination at Nanjing Drum Tower Hospital. All patients performed dynamic CEUS examination by an experienced sonographer using an iU22 scanner (Philips Healthcare, Bothell, WA) equipped with a linear transducer L9-3 probe. These 282 cases included 147 malignant nodules and 135 benign nodules. On the one hand, the percutaneous biopsy based pathological examination was implemented to determine the ground-truth of malignant and benign. On the other hand, a sonographer with more than 10 years of experience manually annotated the nodule lesion mask to obtain the pixel-level groundtruth of thyroid nodules segmentation. All data were approved by the Institutional Review Board of Nanjing Drum Tower Hospital, and all patients signed the informed consent before enrollment into the study.Implementation Details. Our network was implemented using Pytorch framework with the single 12 GB GPU of NVIDIA RTX 3060. During training, we first pre-trained the TALR backbone via dice loss for 30 epochs and used Adam optimizer with learning rate of 0.0001. Then, we loaded the pre-trained weights to train the whole model for 100 epochs and used Adam optimizer with learning rate of 0.0001. Here, we set batch-size to 4 during the entire training process The CEUS consisted the full wash-in and wash-out phases, and the resolution of each frame was (600 × 800). In addition, we carried out data augmentation, including random rotation and cropping, and we resize the resolution of input frames to (224 × 224). We adopted 5-fold cross-validation to achieve quantitative evaluation. Three indexes including Dice, Recall, and IOU, were used to evaluate the lesion recognition task, while five indexes, namely average accuracy (ACC), sensitivity (Se), specificity (Sp), F1-score (F1), and AUC, were used to evaluate the diagnosis task.Experimental Results. As in Table 1, we compared our method with SOTA method including V-Net, Unet3D, TransUnet. For the task of identifying lesions, the index of Recall is important, because information in irrelevant regions can be discarded, but it will be disastrous to lose any lesion information. V-Net achieved the highest Recall scores compared to others; thus, it was chosen as the backbone of TLAR. Table 1 revealed that the modules (TPA, SAF, and IPO) used in the network greatly improved the segmentation performance compared to baseline, increasing Dice and Recall scores by 7.60% and 7.23%, respectively. For the lesion area recognition task, our method achieved the highest Dice of 85.54% and Recall of 90.40%, and the visualized results were shown in Fig. 3. To evaluate the effectiveness of the baseline of lightweight C3D, we compared the results with SOTA video classification methods including C3D, R3D, R2plus1D and ConvLSTM. For fair comparison, all methods used the manually annotated lesion mask to assist the diagnosis. Experimental results in Table 2 revealed that our baseline network could be useful for the diagnosis. With the effective baseline, the introduced modules including TLAR, SAF and IPO further improved the diagnosis accuracy, increasing the accuracy by 9.5%. The awareness of microvascular infiltration using SAF and IPO unit was helpful for CEUS-based diagnosis, as it could improve the diagnosis accuracy by 7.69% (As in Table 2). As in Appendix Fig. A1, although SOTA method fails to focus on lesion areas, our method can pinpoint discriminating lesion areas. Influence of α Values. The value of α in SAF is associated with simulating microvessel infiltration. Figure 3 (C) showed that the diagnosis accuracy increased along with the increment of α and then tended to become stable when α was close to 9. Therefore, for balancing the efficiency and performance, the number of IPO was set as n = 3 and α was set as α = {1, 5, 9} to generate a group of confidence maps that can simulate the process of microvessel infiltration. (More details about the setting of n is in Appendix Fig. A4 of the supplementary material.)"
Thyroid Nodule Diagnosis in Dynamic Contrast-Enhanced Ultrasound via Microvessel Infiltration Awareness,4,Conclusion,"The microvessel infiltration leads to the observation that the lesions detected on CEUS tend to be larger than those on gray US. Considering the microvessel infiltration, we propose an method for thyroid nodule diagnosis based on CEUS videos. Our model utilizes a set of confidence maps to recreate the lesion expansion process; it effectively captures the ambiguous information caused by microvessel infiltration, thereby improving the accuracy of diagnosis. This method is an attempt to eliminate the inaccuracy of diagnostic task due to the fact that gray US underestimates lesion size and CEUS generally overestimates lesion size. To the best of our knowledge, this is the first attempt to develop an automated diagnostic tool for thyroid nodules that takes into account the effects of microvessel infiltration. The way in which we fully exploit the information in time dimension through TPA also makes the model more clinically explanatory."
Thyroid Nodule Diagnosis in Dynamic Contrast-Enhanced Ultrasound via Microvessel Infiltration Awareness,,Fig. 1 .,
Thyroid Nodule Diagnosis in Dynamic Contrast-Enhanced Ultrasound via Microvessel Infiltration Awareness,,Fig. 2 .,
Thyroid Nodule Diagnosis in Dynamic Contrast-Enhanced Ultrasound via Microvessel Infiltration Awareness,,Fig. 3 .,
Thyroid Nodule Diagnosis in Dynamic Contrast-Enhanced Ultrasound via Microvessel Infiltration Awareness,,Table 1 .,NetworkUNet3D[18] V-Net[16] TransUNet[17] V-Net+ TPA V-Net+ TPA+SAF V-Net+ TPA+IPO Ours
Thyroid Nodule Diagnosis in Dynamic Contrast-Enhanced Ultrasound via Microvessel Infiltration Awareness,,Table 2 .,
Thyroid Nodule Diagnosis in Dynamic Contrast-Enhanced Ultrasound via Microvessel Infiltration Awareness,,Ours 88.79 ± 1.40 94.26 ± 1.68 88.37 ± 1.80 90.41 ± 1.85 94.54 ± 1.54,
Mitral Regurgitation Quantification from Multi-channel Ultrasound Images via Deep Learning,1,Introduction,"Mitral regurgitation (MR) is the most common heart valve disease. The incidence increases significantly with age, with more than 13% prevalence in the population over 75 years old [12]. MR is a mitral valve lesion caused by organic or functional changes in the mitral leaflets, annulus, papillary muscles, or tendon cords. Prolonged regurgitation can cause changes in the heart size, and lead to impaired systolic and diastolic capacity, resulting in decreased cardiac function and even being life-threatening.In the clinical diagnosis of MR, physicians assess the extent of MR by calculating the effective regurgitant orifice area (EROA) and mitral regurgitant stroke volume (MRSV) of MR patients from ultrasound images, including continuous wave Doppler images (CW) and color Doppler image (CD). The most commonly applied method for calculating EROA and MRSV uses measurements derived from the proximal isovelocity surface area (PISA) method [3,4,6,18,20]. It is a hemispherical isovelocity surface that points to the regurgitant blood flow at the valve orifice when accelerated, and this phenomenon is used for quantitative evaluation of regurgitant flow. Bargiggia et al. [1] used the single-point PISA method to estimate MRSV in the routine clinical diagnosis. However, the underlying assumption that the size of the regurgitant orifice (during systole) is constant can not be held, therefore usually leads to overestimation or underestimation of MRSV. To account for the dynamic variation, Chen et al. [2] proposed an M-mode PISA and Enriquez-Sarano et al. [5] proposed a Serial PISA. However, the average orifice area used in M-mode PISA and the temporal sampling in Serial PISA undermine the accuracy of the estimation. Militaru et al. [11] sought to evaluate the accuracy of a new postprocessing software to quantify MR that allows semi-automated computation of MR severity from 3D color Doppler transesophageal echocardiographic images. The method significantly underestimates MR and can only measure MRSV, ignoring other parameters like EROA, which may be a better predictor. Singh et al. [15] evaluated a semi-automated method using 3D color data sets of MR to quantify MRSV and transmitral dynamic flow curves. However, The EROA is subject to inaccuracy in the setting of altered tissue or color gain, which cannot extrapolate to the effectiveness of this method. Modified PISA [9,14,16] was proposed later to calculate more accurate MRSV and EROA with continued temporal curves of the blood velocity and the PISA radius obtained from multi-channel ultrasound images, including CW image, two-dimensional M-mode ultrasound image (M2D) and M-mode color Doppler image (MCD). However, it is still time-consuming and laborious to implement the method manually. In this paper, we aim to automatize this procedure, where automatic identification of the above-mentioned two curves during the regurgitation period is required.Most of the existing methods for the automatic detection of Doppler image contours were based on noise reduction and boundary tracking algorithms. In [7,17], classic image processing techniques such as low-pass filtering, thresholding, and edge detection were used. However, robustness and generalization in the presence of severe image artifacts cannot be guaranteed. A probabilistic, hierarchical, and discriminant(PHD) framework [21], was successfully applied to the automatic contour tracking of three kinds of Doppler blood flow images. Some related works have focused on model-based image segmentation algorithms. Indeed, knowing the expected shape can improve the tracking of velocity profiles. In the work of Wang et al. [19], a model-based feedback and adaptive weighted tracking algorithm was proposed. The algorithm combines a nonparametric statistical comparison of image intensities to estimate the edges of noisy impulse Doppler signals and a statistical shape model learned during manual tracking of the contours using. As for the M-mode ultrasound images, there is no existing automatic analysis method yet. In this method, we aim to estimate MRSV and EROA from multi-channel ultrasound images: CW, M2D, and MCD images (as illustrated in Fig. 1). While the CW image is used to estimate the blood velocity, the M2D and MCD images are used to estimate the contour of the PISA radius. Besides the presence of heavy noise in the images, a non-trivial challenge is that the M2D image is good at capturing the lower bound of the PISA radius, while the upper bound of the MCD image. To estimate the contour of the PISA radius, complementary information should be well extracted from the two images.The contribution of the paper can be summarized as follows: First, we propose the first fully automatic pipeline for MR quantification from multi-channel ultrasound images based on the modified PISA method. The pipeline includes ECGbased cycle detection, Doppler spectrum segmentation, PISA radius segmentation, and MR quantification; Secondly, we propose a novel adaptive-weighting multi-channel segmentation network, PISA-net, to identify the lower and upper contours of the PISA radius from the complementary and coupled images, i.e., M2D and MCD. The network can adaptively select the related information of the corresponding input image and lead to an accurate estimation of the radius contour. Thirdly, after calculation based on the modified method, our method achieves accurate estimation of MRSV and EROA, with a Pearson correlation of 0.994 with the ground truths for both MR parameters. "
Mitral Regurgitation Quantification from Multi-channel Ultrasound Images via Deep Learning,2,Method,"The overview of the proposed method illustrate in Fig. 2, and it contains three stages:(1) ECG-based cycle detection, (2) Doppler spectrum and PISA radius segmentation, and (3) MR quantification. The details of the proposed pipeline are as follows."
Mitral Regurgitation Quantification from Multi-channel Ultrasound Images via Deep Learning,2.1,ECG-Based Cardiac Cycle Detection,"Multiple cardiac cycles appear in the above-mentioned ultrasound images (Fig. 1). The first step in the pipeline is to segment the image content of each cycle, as shown in Fig. 2. The regions of interest in these ultrasound images, i.e., the blood spectrum in CW, and the texture content in the M2D and MCD are first cropped out according to the meta information of the DICOM file. The ECG signals as shown at the bottom of these images are first extracted and then used to segment the cropped images into multiple single-cycle images."
Mitral Regurgitation Quantification from Multi-channel Ultrasound Images via Deep Learning,2.2,Doppler Spectrum and PISA Radius Segmentation,"We proposed the PISA-net to obtain the Doppler spectrum and PISA radius segmentation from multi-channel images. The architecture of the PISA-net is shown in Fig. 3. We employ a classic U-net structure (Fig. 3(a)) as the baseline model for our task. Considering the above-mentioned challenges, we introduce the adaptive-weighting strategy for features from different images in the encoder so that PISA-net can learn which image should be used to extract the low-level features for the lower and upper bounds of the PISA radius, respectively. To alleviate the effect of the heavy noise in these images, we introduce spatial attention mechanisms for features of the decoder layers, so that the global context can be used to suppress local noisy features.Adaptive-Weighting Multi-channel Attention. PISA radius segmentation requires complementary information of both M2D and MCD images, and may also be affected by the image quality. We utilize the SE block [8] to adaptively weight features from different image channels. The structure of the SE block is shown in Fig. 3(b). Convolution features from M2D and MCD images are first contacted together. Through the global average pooling (GPA), each feature channel is compressed into a real number that can represent the global information of the channel. Then two fully connected layers and a Sigmoid layer are used to generate weights for each feature channel. Finally, the contacted features of M2D and MCD are weighted according to the weight vector to adjust the relative importance. Spatial Attention. The features in the decoder are further enhanced with a spatial attention mechanism using features of the encoder. In this work, the attention gate block [13] (as shown in Fig. 3(c)) is used. Let x l be the output feature map from layer l of the encoder, and g represents features from the previous block. Then, these features are fused by the addition operation and passed through a Sigmoid function to calculate the spatial attention map. The feature map x l is then multiplied with the attention map to the enhanced features x l , which makes the value of irrelevant regions smaller and the value of the target region larger, and therefore improves both the network prediction speed and the segmentation accuracy."
Mitral Regurgitation Quantification from Multi-channel Ultrasound Images via Deep Learning,2.3,MR Quantification,"As shown in Fig. 2, the Doppler spectrum segmentation result of the CW image represents the velocity curve of the blood flow v(t) at time t in the PISA radius. From v(t), the maximum velocity v max and the velocity time integral V T I can be calculated. The PISA-net predicts the upper bound and the lower bound of the radius contour from the M2D and MCD images. The distance between the two bounds represents PISA radius r(t), which can be used to quantify the regurgitant flow rate F (t) and MRSV. Finally, EROA is calculated from the regurgitant flow rate F (t) and the blood flow velocity v(t). Detailed formulas are as follows:where V a is a constant representing the aliasing velocity, and T denotes the duration length of the regurgitation."
Mitral Regurgitation Quantification from Multi-channel Ultrasound Images via Deep Learning,3,Experiment and Results,
Mitral Regurgitation Quantification from Multi-channel Ultrasound Images via Deep Learning,3.1,Experimental Configuration,"We obtained Doppler ultrasound images of 205 MR patients from a local hospital, and 157 of them were collected by GE VividE95 while the rest 48 were collected by PHILIPS CX50. For each patient, three images were included: CW, M2D, and MCD, as shown in Fig. 1. Data use declaration and acknowledgement: Our dataset was collected from Zhongshan Hospital, Fudan University. This study was approved by local institutional review boards. We divide these ultrasound images into a training dataset (159 patients) and a test dataset (46 patients). Among the test set, 45 patients had degenerative mitral regurgitation, and 1 had functional mitral regurgitation. All images were annotated by experienced doctors using the Pair annotation software package (https://www.aipair.com. cn/en/, Version 2.7, RayShape, Shenzhen, China) [10]. We used the Dice score to evaluate the segmentation accuracy, and the Pearson correlation coefficient (corr), mean absolute error (MAE), and mean relative error (MRE) to assess the performance of MR parameters estimation.Our method was implemented using Pytorch 1.7.1 and trained on an NVIDIA A100 GPU. The size of the input image is 3 × 256 × 256. The model was optimized by minimizing the binary cross-entropy loss function and using the Adam optimization algorithm. The learning rate was set as 0.001."
Mitral Regurgitation Quantification from Multi-channel Ultrasound Images via Deep Learning,3.2,Results and Analysis,"Effect of Adaptive Weighting and Multiple Channel. We test the effect of the adaptive weighting mechanism in PISA-net when placed in different positions of the encoder. Table 1 shows the segmentation performance for single inputs and multi-channel inputs, with the adaptive weighting in different layers. It can be drawn that 1)for both the spectrum segmentation and the PISA radius segmentation, the adaptive weighting performs best when placed after the second convolution block of the encoder; and 2) the multi-channel inputs for PISA radius segmentation do help improve the performance, implying that PISA-net can effectively make use of the complementary information in these images. Results in the third and fourth columns of Table 2 also validate this. Segmentation Performance. We compared PISA-net with three different methods: Unet, Unet with SE block(Unet+SE), and Attention Unet(Att-Unet) with different images as the input, and the results are shown in Table 2. The proposed PISA-net achieves the best accuracy, while the Unet gets the lowest accuracy. When M2D and MCD are combined as the input, Unet+SE, Att-Unet, and PISA-net achieve better accuracy than that of single input MCD. These results demonstrate the effectiveness of multi-channel adaptive weighting and spatial attention. Figure 4 shows examples of the summation of the weights learned for features from M2D and MCD images, respectively. It can be observed that our method can learn the weights of the two images adaptively for different samples.   Parameters Quantification. Table 3 is the comparison result for MR parameters quantification by different methods. PISA-net significantly outperforms the other methods for MRSV and EROA, with a Pearson correlation coefficient of 0.994 for both MR parameters. Figure 5(a) shows the Bland-Altman analysis of MRSV and EROA obtained. The PISA-net results in the least estimation bias. Figure 5(b) shows the segmentation results of a bad case for all four methods. PISA-net can still identify the lower and upper contours of the PISA radius from the complementary and coupled images accurately, showing the effectiveness of the adaptive weighting and the spatial attention."
Mitral Regurgitation Quantification from Multi-channel Ultrasound Images via Deep Learning,4,Conclusion,"In this work, we proposed the first fully automatic pipeline for MR quantification from multi-channel ultrasound images (CW, M2D, and MCD), based on the modified PISA method. An adaptive weighting mechanism and a spatial attention mechanism weighting were used to combine features of multi-channel inputs and enhance the local feature with a global context. Extensive experiments demonstrate that the proposed method is capable of delivering good segmentation results and excellent quantification of MR parameters, and has great potential in the clinical application of MR diagnosis."
Mitral Regurgitation Quantification from Multi-channel Ultrasound Images via Deep Learning,,Fig. 1 .,
Mitral Regurgitation Quantification from Multi-channel Ultrasound Images via Deep Learning,,Fig. 2 .,
Mitral Regurgitation Quantification from Multi-channel Ultrasound Images via Deep Learning,,Fig. 3 .,
Mitral Regurgitation Quantification from Multi-channel Ultrasound Images via Deep Learning,,Fig. 4 .,
Mitral Regurgitation Quantification from Multi-channel Ultrasound Images via Deep Learning,,Fig. 5 .,
Mitral Regurgitation Quantification from Multi-channel Ultrasound Images via Deep Learning,,Table 1 .,
Mitral Regurgitation Quantification from Multi-channel Ultrasound Images via Deep Learning,,Table 2 .,
Mitral Regurgitation Quantification from Multi-channel Ultrasound Images via Deep Learning,,Table 3 .,
Label-Free Nuclei Segmentation Using Intra-Image Self Similarity,1,Introduction,"Nuclei segmentation is a fundamental step in histology image analysis. In recent advances, with a large amount of labeled data, fully-supervised learning methods can easily achieve reasonable results [1][2][3][4][5]. However, accurate pixel-level annotation of nuclei is not always accessible for segmentation labeling is a laborintensive and time-consuming procedure. Methods to relieve the high dependency on the accurate annotations of nuclei are highly needed.Unsupervised learning (UL) methods achieved great success in the data dependency problem for nuclei segmentation, which learns from the structural properties in the data without any manual annotations. Based on the character of these methods, we can group them into two categories: the traditional UL methods and the deep learning UL methods. Traditional UL nuclei segmentation methods include watershed [6], contour detection [7], clustering [8,9] and random field [10]. These methods focus on either pixel value or shape information but fail to take advantage of both of them. Moreover, due to the heavily rely on preset parameters, these traditional methods also show weak robustness.Therefore, some researchers [11][12][13][14][15] resort to deep UL segmentation models to better utilize both pixel value and shape information and develop a robust approach. The common and effective way is to employ image clustering by maximizing mutual information between image and predicted labels to distinguish foreground and background regions. Many image-clustering-based deep UL methods for natural tasks still achieve strong performances in nuclei segmentation. Kanezaki et al. [11] constrain a convolutional neural network (CNN) with superpixel level segmentation results. Ji et al. [12] propose the invariant information clustering. While reasonable results are obtained, these deep clustering-based methods still suffer difficulties: (i) Poor segmentation of the regions between adjacent nuclei. Deep clustering models succeed in transferring images to highdimensional feature space and obtaining image segmentation results by means of clustering pixels' features. However, the regions between adjacent nuclei are similar to the nuclei regions in terms of color values and textures (as shown in Fig. 1). Deep clustering-based methods experience difficulties in dealing with these regions due to the lack of supervision. (ii) Underutilization of intra-image self similarity (IISS) information. As shown in Fig. 1, in terms of value, shape and texture, nuclei show a similar appearance within the same image but vary greatly among different images 1 . This phenomenon offers valuable information for networks to use but the current clustering models do not take this into account.To address the above issues and motivated by the IISS property, we hereby propose a novel self-similarity-driven segmentation network (SSimNet) for unsupervised nuclei segmentation. As shown in Fig. 2, instead of designing complex discriminative network architectures, our framework derives knowledge from the IISS property to aid the segmentation. Specifically, we obtain candidate nuclei with some unsupervised image processing. For the obtained candidates, it is common that adjacent nuclei merged into one candidate due to imperfect staining and low image quality, which violate the IISS property. Hence, we filter the candidates based on a custom-designed index that roughly measures if a candidate contains multiple nuclei. The remaining candidates are used as pseudo labels, which we use to train a U-Net (aka SSimNet) to discover the hierarchical features that distinguish nuclei pixels from the background. Finally, we apply the learned SSimNet to produce the final nuclei segmentation.To validate the effectiveness of our method, we conduct extensive experiments on the MoNuSeg dataset [16,17] based on ten existing unsupervised segmentation methods [9,[11][12][13][14][15][18][19][20]. Our method outperforms all comparison methods with an average Dice score of 0.792 and aggregated Jaccard index of 0.498 on the MoNuSeg dataset which is close to the supervised method."
Label-Free Nuclei Segmentation Using Intra-Image Self Similarity,2,Method,"As shown in Fig. 2, our SSimNet aims at unsupervised segmentation of nuclei from histology images. Specifically, by using a matrix factorization on hematoxylin and eosin (H&E) stained histology images, we get the hematoxylin channel image for clustering, active contour refining and softening to generate the final soft candidate label. Then according to the designed unsupervised evaluation metric driven from the IISS property, an SSimNet is trained with highlyrated soft pseudo labels and corresponding original patches. Last, while testing on the test image, to adapt the network to learn nucleus similarity within the same image, we fine tune the network with soft pseudo labels of some patches in current test images. In the following, we elaborate on each part in detail. "
Label-Free Nuclei Segmentation Using Intra-Image Self Similarity,2.1,Candidate Nucleus Generation,"Channel Decomposition. Suppose that we are given a training set I S = {I S i } N i=1 of histopathology images without any manual annotation. For each image, stained tissue colors are results from light attenuation, which depends on the type and amount of dyes that the tissues have absorbed. This property is prescribed by the Beer-Lambert law:where I ∈ R 3×n represents the histology image with three color channels and n pixels, I 0 is the illuminating light intensity of sample with I 0 = 255 for 8-bit images in our cases, W ∈ R 3×r is the stain color matrix that encodes the color appearance of each stain with r representing the number of stains, and H ∈ R r×n is the stain density map. In this work, we follow the sparse non-negative matrix factorization in [21] to get the stain color matrices W = {W i } N i=1 and stain density maps H = {H i } N i=1 for I S . Note that usually histopathology images are stained with H&E and nuclei mainly absorb hematoxylin [22]; therefore, r = 2. We reconstruct the nuclei stain map with the first channel of W and H:Clustering and Active Contour. We transform I T into CIELAB color space and invoke the Fuzzy C-Means method (FCM) with 2 clusters to obtain the candidate foreground pixels. To reduce the noise in clustering results, we use active contour method as a smoothing operation to get hard candidate labels:Label Smoothing. Since hard label is overconfident at the border of nuclei, which is detrimental to the training of the network, we soften the hard label one by one for each connected component in P i using the following formulation:where  4), we obtain our soft candidate labels P from P."
Label-Free Nuclei Segmentation Using Intra-Image Self Similarity,2.2,Data Purification and SSimNet Learning,"So far, soft candidate labels Pi have been acquired for each image I S i . However, it is common that adjacent nuclei are merged into one candidate due to imperfect staining and imaging conditions, which violate the IISS property. To this, we conduct data purification to build a reliable training set for subsequent learning.Data Purification. We sample K patches with overlap from original image I S i . The sampled results are expressed as patch tissueWe design the Unsupervised Shape Measure Index (USMI) and calculate it using the algorithm in Fig. 3(left). Based on thresholding the USMI, we obtain pairs (y i , u i ) N •K i=1 . Note that the smaller USMI is, the more the pseudo label conforms to prior knowledge. Sorting these pairs by USMI from the smallest to largest, only maintain the first α%(0 < α < 100) of data pairs as). Figure 3(right) shows a separation of candidates into two groups (yellow and blue) with a typical yellow patch containing merger nuclei and a blue patch containing isolated nuclei.To further separate possible adjacent nuclei in a blue patch, we follow [23] to construct the Voronoi label as in Fig. 2 by setting the center of connected component as 1, constructing Voronoi diagram, setting Voronoi edge as 0, and ignoring other pixels. Then, a Voronoi tri-label set Z can be acquired.SSimNet Learning and Finetuning. By denoting our segmentation network as F , our final loss function to supervise the network training can be formulated as:where L BCE is the binary cross-entropy loss and L CE is the cross-entropy loss. Also, we can obtain tissue patches and corresponding pseudo labels for each image in the test set termed asBefore evaluation, we first fine tune our network F using SET k for several epochs. As shown in the ablation study, this operation is simple but effective. And this fine tuning process can help the network capture the size and shape information in the current test slice. "
Label-Free Nuclei Segmentation Using Intra-Image Self Similarity,3,Experiments,
Label-Free Nuclei Segmentation Using Intra-Image Self Similarity,3.1,Datasets and Settings,"MoNuSeg. Multi-organ nuclei segmentation [16,17] (MoNuSeg) is used to evaluated our SSimNet. The MoNuSeg dataset consists of 44 H&E stained histopathology images with 28,846 manually annotated nuclei. With 1000 × 1000 pixel resolution, these images were extracted from whole slide images from the The Cancer Genome Atlas (TCGA) repository, representing 9 different organs from 44 individuals.CPM17. The CPM17 dataset [24] is also derived from TCGA repository. The training and test set each consisted of 32 images tiles selected and extracted from a set of Non-Small Cell Lung Cancer (NSCLC), Head and Neck Squamous Cell Carcinoma (HNSCC), Glioblastoma Multiforme (GBM) and Lower Grade Glioma (LGG) tissue images. Moreover, each type cancer has 8 tiles and the size of patch is 500 × 500 or 600 × 600.Settings. We compare our SSimNet with several current unsupervised segmentation methods. We follow the DCGN [15] to conduct comparison experiments. We crop the image indataset into patches of 256 × 256 pixels for training. All the methods were trained for 150 epochs on MoNuSeg and 200 epochs on CPM17 each time and experimented with an initial learning rate of 5e -5 and a decay of 0.98 per epoch. Our experiment repeated ten times on MoNuSeg dataset and only once on CPM17 dataset for an augmented convenience. Specially for our SSimNet training, we set α = 70% for data purification and λ = 0.9 for loss in training. Moreover, we fine tune the network with only five epochs for each image on test set with optimizer parameter saved in checkpoint. "
Label-Free Nuclei Segmentation Using Intra-Image Self Similarity,3.2,Experimental Results,"To evaluate the effectiveness of SSimNet, we compare it with several deep learning based and conventional unsupervised segmentation methods on the mentioned datasets, including minibatch K-Means (termed as mKMeans), Gaussian Mixture Model [9] (termed as GMM), Invariant Information Clustering [12] (termed as IIC), Double DIP [18], Deep Clustering via Adaptive GMM model [19] (termed as DCAGMM), Deep Image Clustering [13] (termed as DIC), Kim's work [20], Kanezaki's work [11], Deep Conditional GMM [14] (termed as DCGMM), and Deep Constrained Gaussian Network [15] (termed as DCGN).For the methods without public codes, we report the results from the original publications for a fair comparison. The results are shown in Table 1.As Table 1 shows, firstly, our SSimNet outperforms all other unsupervised model and performs even close to fully supervised U-Net under the metrics of Dice coefficient and Aggregated Jaccard Index (AJI). Secondly, while the recall of all comparison methods is higher than precision, our SSimNet's recall (0.772) is lower than precision (0.820) and also lower than the state-of-the-art method's recall (0.834). The reason lies in that our method considers mining as strong prior knowledge from tissue slice itself, which renders a tighter constraint on our model, leading the model to predict a lower confidence in the easilyconfused region. Moreover, Figure 4 shows the visualization of two test slice. It also conforms the effectiveness of our method on eliminating the model confusion in the region between adjacent nuclei and the ability in capturing nuclei shape.Besides, we conduct an additional comparison experiment based on CPM17 dataset to demonstrate the generalization of our method. As shown in Table 2, our method again achieves the top performances. Moreover, as the image size of CPM17 is smaller than that of MoNuSeg, the performance gain is not as big as on the MoNuSeg dataset.  "
Label-Free Nuclei Segmentation Using Intra-Image Self Similarity,3.3,Ablation Study,"We perform ablation studies by disabling each component to the SSimNet framework to evaluate their effectiveness. As shown in Table 3, each component in our SSimNet can bring different degrees of improvement, which shows that all of the label softening, data purification and finetuning process are significant parts of our SSimNet and play an indispensable role in achieving superior performance. "
Label-Free Nuclei Segmentation Using Intra-Image Self Similarity,4,Conclusion,"In this paper, we propose an SSimNet framework for label-free nuclei segmentation. Motivated by the intra-image self similarity (IISS) property, which characterize the histology images and nuclei, we design a series of operations to capture the prior knowledge and generate pseudo labels as supervision signal, which is used to learn the SSimNet for final nuclei segmentation. The IISS property renders us a tighter prior constraint for better model building compared to other unsupervised nuclei segmentation. Comprehensive experimental results demonstrate that SSimNet achieves the best performances on the benchmark MoNuSeg and CPM17 datasets, outperforming other unsupervised segmentation methods."
Label-Free Nuclei Segmentation Using Intra-Image Self Similarity,,Fig. 1 .,
Label-Free Nuclei Segmentation Using Intra-Image Self Similarity,,Fig. 2 .,
Label-Free Nuclei Segmentation Using Intra-Image Self Similarity,,,
Label-Free Nuclei Segmentation Using Intra-Image Self Similarity,,Fig. 3 .,
Label-Free Nuclei Segmentation Using Intra-Image Self Similarity,,Fig. 4 .,
Label-Free Nuclei Segmentation Using Intra-Image Self Similarity,,Table 1 .,
Label-Free Nuclei Segmentation Using Intra-Image Self Similarity,,Table 2 .,
Label-Free Nuclei Segmentation Using Intra-Image Self Similarity,,Table 3 .,
Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,1,Introduction,"The tumor microenvironment (TME) is comprised of cancer, immune (e.g. B lymphocytes, and T lymphocytes), stromal, and other cells together with noncellular tissue components [3,5,24,30]. It is well acknowledged that tumors evolve in close interaction with their microenvironment. Quantitatively characterizing TME has the potential to predict tumor aggressiveness and treatment response [3,23,24,30]. Different types of lymphocytes such as CD4+ (helper T cells), CD8+ (cytotoxic T cells), CD20+ (B cells), within the TME naturally interact with tumor and stromal cells. Studies [5,9] have shown that quantifying spatial interplay of these different cell families within the TME can provide more prognostic/predictive value compared to only measuring the density of a single biomarker such as tumor-infiltrating lymphocytes (TILs) [3,24]. Immunotherapy (IO) is the standard treatment for patients with advanced non-small cell lung cancer (NSCLC) [19] but only 27-45% of patients respond to this treatment [21]. Therefore, better algorithms and improved biomarkers are essential for identifying which cancer patients are most likely to respond to IO in advance of treatment. Quantitative features that relate to the complex spatial interplay between different types of B-and T-cells in the TME might unlock attributes that are associated with IO response. In this study, we introduce a novel approach called Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL), representing a unique and interpretable way to characterize the distribution, and higher-order interaction of various cell families (e.g., cancerous cells, stromal cells, lymphocyte subtypes) across digital histopathology slides. We demonstrate the efficacy of TriaAnGIL for characterizing TME in the context of predicting 1) response to IO with immune checkpoint inhibitors (ICI), 2) overall survival (OS), in patients with NSCLC, and 3) providing novel insights into the spatial interplay between different immune cell subtype. TriAnGIL source code is publicly available at http://github.com/sarayar/TriAnGIL."
Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,2,Previous Related Work and Novel Contributions,"Many studies have only looked at the density of a single biomarker (e.g. TILs), to show that a high density of TILs is associated with improved patient survival and treatment response in NSCLC [3,24]. Other works have attempted to characterize the spatial arrangement of cells in TME using computational graphbased approaches. These approaches include methods that connect cells regardless of their type (1) using global graphs (GG) such as Voronoi that connect all nuclei [2,14], or (2) using Cell cluster graphs (CCG) [16] to create multiple nuclear subgraphs based on cell-to-cell proximity to predict tumor aggressiveness and patient outcome [16]. Others have explored (3) the spatial interplay between two different cell types [5].One example approach is Spatial architecture of TIL (SpaTIL) [9] which attempted to characterize the interplay between immune and cancer cells and has proven to be helpful in predicting the recurrence in early-stage NSCLC. All of these approaches point to overwhelming evidence that spatial architecture of cells in TME is critical in predicting cancer outcome. However, these approaches have not been able to exploit higher-order interactions and dependencies between multiple cell types (> 2), relationships that might provide additional actionable insights. The contributions of this work include:(1) TriAnGIL is a computational framework that characterizes the architecture and relationships of different cell types simultaneously. Instead of measuring only simple two-by-two relations between cells, it seeks to identify triadic spatial relations (hyperedges [18,20]  have shown great capabilities in solving complex problems in the biomedical field, these tend to be black-box in nature. A key consideration in cancer immunology is the need for actionable insights into the spatial relationships between different types of immune cells. Not only does TriAnGIL provide predictions that are on par or superior compared to DL approaches, but also provides a way to glean insights into the spatial interplay of different immune cell types. These complex interactions enhance our understanding of the TME and will help pave the way for new therapeutic strategies that leverage these insights."
Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,3,Description of TriAnGIL Methodology,
Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,3.1,Notation,"Our approach consists of constructing heterogeneous graphs step by step and quantifying them by extracting features from them. The graphs are defined by G = (V, E), where V is the set of vertices (nodes) V = {v 1 , ...v N } with τ n vertex types, and E is the collection of pairs of vertices from V, E = {e 1 , ...e M }, which are called edges and φ n is the mapping function that maps every vertex to one of n differential marker expressions in this dataset φ n : V → τ n . G is represented by an adjacency matrix A that allows one to determine edges in constant time."
Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,3.2,Node Linking and Computation of Graph-Interplay Features,"The inputs of TriAnGIL are the coordinates of nuclear centroids and the corresponding cell types. In the TriAnGIL procedure, the centroid of each nucleus in a family is represented as a node of a graph. TriAnGIL is agnostic of the method used for identifying the coordinates and types. Once the different cell families are identified (Fig. 1-B), a list is generated for all possible sets comprising of membership from three [12,18] different cell families. By focusing on every set, TriAnGIL allows for capturing higher-order and balanced spatial triadic relationships [4] between cell families, while keeping the computational complexity relatively low. Therefore, we initiate the process with the first set on the list (α, β, γ), build heterogeneous graphs, extract features, and then select another set until we have explored all possible triads of cell families on the list. The three main steps of TriAnGIL is as follows:1) Quantifying in absence of one family: First, we build a proximity graph (G 1 ) on nodes of α, β, γ based on the Euclidean distance of every two nodes. Two nodes will be connected if their distance is shorter than a given ""interaction rate"", θ regardless of their family and cell type (Fig. 1-C, 1-C1). The interaction rate is a hyper-parameter that controls how close we expect the distance to be so that we consider some interaction between the nodes.We then exclude the nodes of α from all the interactions by removing its edges from G1 and characterize the relationship of β and γ (Fig. 1-C2). Next, we extract a series of features including clustering coefficient, average degree from the resulting subgraph. We repeat this process by removing all the edges of β (Fig. 1-C3) and then γ (Fig. 1-C4). In this manner, a total of 126 features (supplemental Table 1) are extracted (42 features for absence of one family ×3). 2) Triangulation-based connections: A Delaunay triangulation is constructed by the nodes of α, β, γ (Fig. 1-D, and 1-D1). Delaunay triangulation is a planar graph formed by connecting the vertices in a way that ensures no point lies within the circumcircle of any triangle formed by the vertices [7].We then extract 10 features relating to edge length and vertex count. Next, we prune long edges (D 1 ) where the Euclidean distance between connected nodes is more than the ""interaction rate"" (Fig. 1-D2). Next, a series of features were extracted from the remaining subgraph (e.g. number of edges between the nodes of α and β, β and γ, α and γ; complete list of features in supplemental Table 1).3) Triangular interactions: As illustrated in Algorithm 1, from the unpruned Delaunay triangulation that includes the nodes of α, β, γ, we select those triangles (closed triads [8,25]) that link nodes from three distinct families (Fig. 1-E, 1-E1). In other words, we remove triangles with more than one vertex from a single family. Next, we call GetTriangleFeatures() function to quantify triangular relationships by extracting features from the resulting subgraphs (e.g. perimeter and area of triangles; complete list of features in supplemental Table 1)."
Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,4,Experimental Results and Discussion,
Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,4.1,Dataset,"The cohort employed in this study was composed of pre-treatment tumor biopsy specimens from patients with NSCLC from five centers (two centers for training (S t ) and three centers for independent validation (S v )). The entire analysis was carried out using 122 patients in Experiment 1 (73 in S t , and 49 in S v ) and 135 patients in Experiment 2 (81 in S t , and 54 in S v ). Specimens were analyzed with a multiplexed quantitative immunofluorescence (QIF) panel using the method described in [22]. From each whole slide image, 7 representative tiles were obtained and used to train the software InForm to define background, tumor and stromal compartments. Then, individual cells were segmented based on nuclear DAPI staining and the segmentation performance was controlled by direct visualization of samples by a trained observer. Next, the software was trained to identify cell subtypes based on marker expression (CD8, CD4, CD20, CK for tumor epithelial cells and absence of these markers for stromal cells)."
Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,4.2,Comparative Approaches,The efficacy of TriAnGIL was compared against five different approaches.
Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,,TIL density (DenTIL):,"For every patient, multiple density measures including the number of different cells types and their ratios are calculated [3,24] (supplemental Table 2)."
Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,,"GG: A Delaunay triangulation, a Voronoi diagram, and a Minimum Spanning","Tree were constructed [2,14] on all nuclei regardless of their type. Architectural features (e.g., perimeter, triangle area, edge length) were then calculated on these global graphs for each patient."
Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,,CCG:,"For every patient, subgraphs are built on nuclei regardless of their type and only based on their Euclidean distance. Local graph metrics (e.g. clustering coefficient) [16] are then calculated from these subgraphs. SpaTIL: For each patient, first, subgraphs are built on individual cell types based on a distance parameter. The convex hulls are then constructed on these subgraphs. After selecting every two cell types, features are extracted from their convex hulls (e.g. the number of clusters of each cell type, area intersected between clusters [9]; complete list of combinations in supplemental Table 3)."
Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,,GNN:,"A recent study [31] demonstrated that Transformer-based [29] GNNs are able to learn the arrangement of tiles across pathology images for survival analysis. Here, for each tile in the slide, a Delaunay graph was constructed regardless of cell subtypes, and tile-level feature representations (e.g.side length minimum, maximum, mean, and standard deviation, triangle area minimum, maximum, mean, and standard deviation) were aggregated by a Transformer according to their spatial arrangement [31]. Our approach utilized the Weisfeiler-Lehman (WL) test [15] for embedding graphs into Euclidean feature space. Well-known approaches, such as GraphSage [10], are considered as continuous approximations to the WL test. Therefore, our GNN is a valid baseline for heterogeneous graphs."
Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,4.3,Experiment 1: Immunotherapy Response Prediction in Lung Cancer,"Design: TriAnGIL was also trained to differentiate between patients who responded to IO and those who did not. For our study, the responders to IO were identified as those patients with complete response, partial response, and stable disease, and non-responders were patients with progressive disease. A linear discriminant analysis (LDA) classifier was trained on S t to predict which patients would respond to IO. For creating the model, the minimum redundancy maximum relevance (mRMR) method [1] was used to select the top features. The same procedure using mRMR and LDA was performed for the comparative hand-crafted approaches. The ability to identify responders post-IO was assessed by the area under the receiver operating characteristic curve (AUC) in S v ."
Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,,Results:,"The two top predictive TriAnGIL features were found to be the number of edges between stroma and CD4+ cells, and the number of edges between stroma and tumor cells with more interactions between stromal cells and both CD4+ and tumor cells being associated with response to IO. This finding is concordant with other studies [13,17,22,27] that stromal TILs were significantly associated with improved OS. Therefore, TriAnGIL approach is not only predictive of treatment response but more critically it enables biological interpretations that a DL model might not be able to provide. In S v , this LDA classifier was able to distinguish responders from non-responders to IO with AU C T ri =0.  Design: S t was used to construct a least absolute shrinkage and selection operator (LASSO) [28] regularized Cox proportional hazards model [6] using the TriAnGIL features, to obtain risk score for each patient. LASSO features are listed in supplemental Table 4. The median risk score in S t was used as a threshold in both S t and S v to dichotomize patients into low-risk/high-risk categories. Kaplan-Meier (KM) survival curves [26] were plotted and the model performance was summarized by hazard ratio (HR), with corresponding (95% confidence intervals (CI)) using the log-rank test, and Harrell's concordance index (C-index) on S v . The C-index evaluates the correlation between risk predictions and survival times, aiming to maximize the discrimination between high-risk and low-risk patients [11]. OS is the time between the initiation of IO to the death of the patient. The patients were censored if the date of death was unknown.Result: Figure 2 presents some TriAnGIL features in a field of view for a patient with long-term survival and another with short-term survival. More triangular relationships, shorter triangle edges, and smaller triangles with smaller perimeters are found in the long-term survival case when analyzing the triadic interactions within tumor-stroma-CD4, thereby suggesting higher relative presence and closer interaction of these cell families. Figure 3 illustrates the KM plots for the six approaches. We also calculated the concordance index (C-index) for the two prognostic approaches in S v . The C-index for TriAnGIL and GNN methods were 0.64, and 0.63 respectively. Therefore, overall TriAnGIL worked marginally better than GNN, with much higher biological interpretability."
Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,5,Concluding Remarks,"We presented a new approach, Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL), to quantitatively chartacterize the spatial arrangement and relative geographical interplay of multiple cell families across pathological images. Compared to previous spatial graph-based methods, TriAnGIL quantifies the spatial interplay between multiple cell families, providing a more comprehensive portrait of the tumor microenvironment. TriAnGIL was predictive of response after IO (N = 122) and also demonstrated a strong correlation with OS in NSCLC patients treated with IO (N = 135). TriAnGIL outperformed other graph-and DL-based approaches, with the added benefit of provoding interpretability with regard to the spatial interplay between cell families. For instance, TriAnGIL yielded the insight that more interactions between stromal cells and both CD4+ and tumor cells appears to be associated with better response to IO. Although five cell families were studies in this work, TriAnGIL is flexible and could include other cell types (e.g., macrophages). Future work will entail larger validation studies and also evaluation on other use cases."
Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,,Algorithm 1 :,
Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,,Fig. 1 .,
Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,,,
Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,,Fig. 2 . 4 . 4 Experiment 2 :,
Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,,Fig. 3 .,
NASDM: Nuclei-Aware Semantic Histopathology Image Generation Using Diffusion Models,1,Introduction,"Histopathology relies on hematoxylin and eosin (H&E) stained biopsies for microscopic inspection to identify visual evidence of diseases. Hematoxylin has a deep blue-purple color and stains acidic structures such as DNA in cell nuclei. Eosin, alternatively, is red-pink and stains nonspecific proteins in the cytoplasm and the stromal matrix. Pathologists then examine highlighted tissue characteristics to diagnose diseases, including different cancers. A correct diagnosis, therefore, is dependent on the pathologist's training and prior exposure to a wide variety of disease subtypes [30]. This presents a challenge, as some disease variants are extremely rare, making visual identification difficult. In recent years, deep learning methods have aimed to alleviate this problem by designing discriminative frameworks that aid diagnosis [15,28]. Segmentation models find applications in spatial identification of different nuclei types [6]. However, generative modeling in histopathology is relatively unexplored. Generative models can be used to generate histopathology images with specific characteristics, such as visual patterns identifying rare cancer subtypes [4]. As such, generative models can be sampled to emphasize each disease subtype equally and generate more balanced datasets, thus preventing dataset biases getting amplified by the models [7]. Generative models have the potential to improve the pedagogy, trustworthiness, generalization, and coverage of disease diagnosis in the field of histology by aiding both deep learning models and human pathologists. Synthetic datasets can also tackle privacy concerns surrounding medical data sharing. Additionally, conditional generation of annotated data adds even further value to the proposition as labeling medical images involves tremendous time, labor, and training costs. Recently, denoising diffusion probabilistic models (DDPMs) [8] have achieved tremendous success in conditional and unconditional generation of real-world images [3]. Further, the semantic diffusion model (SDM) demonstrated the use of DDPMs for generating images given semantic layout [27]. In this work, (1) we leverage recently discovered capabilities of DDPMs to design a first-of-its-kind nuclei-aware semantic diffusion model (NASDM) that can generate realistic tissue patches given a semantic mask comprising of multiple nuclei types, (2) we train our framework on the Lizard dataset [5] consisting of colon histology images and achieve state-of-the-art generation capabilities, and (3) we perform extensive ablative, qualitative, and quantitative analyses to establish the proficiency of our framework on this tissue generation task."
NASDM: Nuclei-Aware Semantic Histopathology Image Generation Using Diffusion Models,2,Related Work,"Deep learning based generative models for histopathology images have seen tremendous progress in recent years due to advances in digital pathology, compute power, and neural network architectures. Several GAN-based generative models have been proposed to generate histology patches [16,31,33]. However, GANs suffer from problems of frequent mode collapse and overfitting their discriminator [29]. It is also challenging to capture long-tailed distributions and synthesize rare samples from imbalanced datasets using GANs. More recently, denoising diffusion models have been shown to generate highly compelling images by incrementally adding information to noise [8]. Success of diffusion models in generating realistic images led to various conditional [12,21,22] and unconditional [3,9,19] diffusion models that generate realistic samples with high fidelity. Following this, a morphology-focused diffusion model has been presented for generating tissue patches based on genotype [18]. Semantic image synthesis is a task involving generating diverse realistic images from semantic layouts. GAN-based semantic image synthesis works [20,24,25] generally struggled at generating high quality and enforcing semantic correspondence at the same time. To this end, a semantic diffusion model has been proposed that uses conditional denoising diffusion probabilistic model and achieves both better fidelity and diversity [27]. We use this progress in the field of conditional diffusion models and semantic image synthesis to formulate our NASDM framework."
NASDM: Nuclei-Aware Semantic Histopathology Image Generation Using Diffusion Models,3,Method,"In this paper, we describe our framework for generating tissue patches conditioned on semantic layouts of nuclei. Given a nuclei segmentation mask, we intend to generate realistic synthetic patches. In this section, we (1) describe our data preparation, (2) detail our stain-normalization strategy, (3) review conditional denoising diffusion probabilistic models, (4) outline the network architecture used to condition on semantic label map, and (5) highlight the classifier-free guidance mechanism that we employ at sampling time."
NASDM: Nuclei-Aware Semantic Histopathology Image Generation Using Diffusion Models,3.1,Data Processing,"We use the Lizard dataset [5] to demonstrate our framework. This dataset consists of histology image regions of colon tissue from six different data sources at 20× objective magnification. The images are accompanied by full segmentation annotation for different types of nuclei, namely, epithelial cells, connective tissue cells, lymphocytes, plasma cells, neutrophils, and eosinophils. A generative model trained on this dataset can be used to effectively synthesize the colonic tumor micro-environments. The dataset contains 238 image regions, with an average size of 1055 × 934 pixels. As there are substantial visual variations across images, we construct a representative test set by randomly sampling a 7.5% area from each image and its corresponding mask to be held-out for testing. The test and train image regions are further divided into smaller image patches of 128 × 128 pixels at two different objective magnifications: (1) at 20×, the images are directly split into 128 × 128 pixels patches, whereas (2) at 10×, we generate 256 × 256 patches and resize them to 128 × 128 for training. To use the data exhaustively, patching is performed with a 50% overlap in neighboring patches. As such, at (1) 20× we extract a total of 54,735 patches for training and 4,991 patches as a held-out set, while at (2) 20× magnification we generate 12,409 training patches and 655 patches are held out."
NASDM: Nuclei-Aware Semantic Histopathology Image Generation Using Diffusion Models,3.2,Stain Normalization,"A common issue in deep learning with H&E stained histopathology slides is the visual bias introduced by variations in the staining protocol and the raw materials of chemicals leading to different colors across slides prepared at different labs [1]. As such, several stain-normalization methods have been proposed to tackle this issue by normalizing all the tissue samples to mimic the stain distribution of a given target slide [17,23,26]. In this work, we use the structure preserving color normalization scheme introduce by Vahadane et al. [26] to transform all the slides to match the stain distribution of an empirically chosen slide from the training dataset."
NASDM: Nuclei-Aware Semantic Histopathology Image Generation Using Diffusion Models,3.3,Conditional Denoising Diffusion Probabilistic Model,"In this section, we describe the theory of conditional denoising diffusion probabilistic models, which serves as the backbone of our framework. A conditional diffusion model aims to maximize the likelihood p θ (x 0 | y), where data x 0 is sampled from the conditional data distribution, x 0 ∼ q(x 0 | y), and y represents the conditioning signal. A diffusion model consists of two intrinsic processes. The forward process is defined as a Markov chain, where Gaussian noise is gradually added to the data over T timesteps aswhere {β} t=1:T are constants defined based on the noise schedule. An interesting property of the Gaussian forward process is that we can sample x t directly from x 0 in closed form. Now, the reverse process, p θ (x 0:T | y), is defined as a Markov chain with learned Gaussian transitions starting from pure noise, p(x T ) ∼ N(0, I), and is parameterized as a neural network with parameters θ as(Hence, for each denoising step from t to t -1,It has been shown that the combination of q and p here is a form of a variational auto-encoder [13], and hence the variational lower bound (VLB) can be described as a sum of independent terms, L vlb := L 0 + ... + L T -1 + L T , where each term corresponds to a noising step. As described in Ho et al. [8], we can randomly sample timestep t during training and use the expectation E t,x0,y, to estimate L vlb and optimize parameters θ. The denoising neural network can be parameterized in several ways, however, it has been observed that using a noiseprediction based formulation results in the best image quality [8]. Overall, our NASDM denoising model is trained to predicting the noise added to the input image given the semantic layout y and the timestep t using the loss described as follows:Note that the above loss function provides no signal for training Σ θ (x t , y, t). Therefore, following the strategy in improved DDPMs [8], we train a network to directly predict an interpolation coefficient v per dimension, which is turned into variances and optimized directly using the KL divergence between the estimated distribution p θ (x t-1 | x t , y) and the diffusion posterior q(x t-1 | x t , x 0 ) as). This optimization is done while applying a stop gradient to (x t , y, t) such that L vlb can guide Σ θ (x t , y, t) and L simple is the main guidance for (x t , y, t). Overall, the loss is a weighted summation of the two objectives described above as follows:(5)"
NASDM: Nuclei-Aware Semantic Histopathology Image Generation Using Diffusion Models,3.4,Conditioning on Semantic Mask,"NASDM requires our neural network noise-predictor θ (x t , y, t) to effectively process the information from the nuclei semantic map. For this purpose, we leverage a modified U-Net architecture described in Wang et al. [27], where semantic information is injected into the decoder of the denoising network using multi-layer, spatially-adaptive normalization operators. As denoted in Fig. 1, we construct the semantic mask such that each channel of the mask corresponds to a unique nuclei type. In addition, we also concatenate a mask comprising of the edges of all nuclei to further demarcate nuclei instances."
NASDM: Nuclei-Aware Semantic Histopathology Image Generation Using Diffusion Models,3.5,Classifier-Free Guidance,"To improve the sample quality and agreement with the conditioning signal, we employ classifier-free guidance [10], which essentially amplifies the conditional distribution using unconditional outputs while sampling. During training, the conditioning signal, i.e., the semantic label map, is randomly replaced with a null mask for a certain percentage of samples. This leads to the diffusion model becoming stronger at generating samples both conditionally as well as unconditionally and can be used to implicitly infer the gradients of the log probability required for guidance as follows: where ∅ denotes an empty semantic mask. During sampling, the conditional distribution is amplified using a guidance scale s as follows:"
NASDM: Nuclei-Aware Semantic Histopathology Image Generation Using Diffusion Models,4,Experiments,"In this section, we first describe our implementation details and training procedure. Further, we establish the robustness of our model by performing an ablative study over objective magnification and classifier-guidance scale. We then perform quantitative and qualitative assessments to demonstrate the efficacy of our nuclei-aware semantic histopathology generation model. In all following experiments, we synthesize images using the semantic masks of the held-out dataset at the concerned objective magnification. We then compute Fréchet Inception Distance (FID) and Inception Score (IS) metrics between the synthetic and real images in the held-out set."
NASDM: Nuclei-Aware Semantic Histopathology Image Generation Using Diffusion Models,4.1,Implementation Details,"Our diffusion model is implemented using a semantic UNet architecture (Sect. 3.4), trained using the objective in (5). Following previous works [19], we set the trade-off parameter λ as 0.001. We use the AdamW optimizer to train our model. Additionally, we adopt an exponential moving average (EMA) of the denoising network weights with 0.999 decay. Following DDPM [8], we set the total number of diffusion steps as 1000 and use a linear noising schedule with respect to timestep t for the forward process. After normal training with a learning rate of 1e -4, we decay the learning rate to 2e -5 to further finetune the model with a drop rate of 0.2 to enhance the classifier-free guidance capability during sampling. The whole framework is implemented using Pytorch and trained on 4 NVIDIA Tesla A100 GPUs with a batch-size of 40 per GPU. Code will be made public on publication or request. "
NASDM: Nuclei-Aware Semantic Histopathology Image Generation Using Diffusion Models,4.2,Ablation over Guidance Scale (s),"In this study, we test the effectiveness of the classifier-free guidance strategy. We consider the variant without guidance as our baseline. As seen in Fig. 2, increase in guidance scale initially results in better image quality as more detail is added to visual structures of nuclei. However, with further increase, the image quality degrades as the model overemphasizes the nuclei and staining textures. As described in Sect. 3.1, we generate patches at two different objective magnifications of 10× and 20×. In this section, we contrast the generative performance of the models trained on these magnification levels respectively. From the table on right, we observe that the model trained at 20× objective magnification produces better generative metrics. Note that we only train on a subset on 20× mag. to keep the size of the training data constant."
NASDM: Nuclei-Aware Semantic Histopathology Image Generation Using Diffusion Models,4.3,Ablation over Objective Magnification,
NASDM: Nuclei-Aware Semantic Histopathology Image Generation Using Diffusion Models,4.4,Quantitative Analysis,"To the best of our knowledge, ours is the only work that is able to synthesize histology images given a semantic mask, making a direct quantitative comparison tricky. However, the standard generative metric Fréchet Inception Distance (FID) measures the distance between distributions of generated and real images in the Inception-V3 [14] latent space, where a lower FID indicates that the model is able to generate images that are very similar to real data. Therefore, we compare FID and IS metrics with the values reported in existing works [18,32] (ref.Table 1) in their own settings. We can observe that our method outperforms all existing methods including both GANs-based methods as well as the recently proposed morphology-focused generative diffusion model. "
NASDM: Nuclei-Aware Semantic Histopathology Image Generation Using Diffusion Models,4.5,Qualitative Analysis,"We perform an expert pathologist review of the patches generated by the model. We use 30 patches, 17 synthetic and 13 real for this review. We have two experts assess the overall medical quality of the patches as well as their consistency with the associated nuclei masks on likert scale. The survey used for the review can be found on a public google survey1 . It can be seen from this survey (Fig. 4) that the patches generated by the model are found to be more realistic than even the patches in our real set. We now qualitatively discuss the proficiency of our model in generating realistic visual patterns in synthetic histopathology images (refer Fig. 3). We can see that the model is able to capture convincing visual structure for each type of nuclei. In the synthetic images, we can see that the lymphocytes are accurately circular, while neutrophils and eosinophils have a more lobed structure. We also observe that the model is able to mimic correct nucleus-to-cytoplasm ratios for each type of nuclei. Epithelial cells are less dense, have a distinct chromatin structure, and are larger compared to other white blood cells. Epithelial cells are most difficult to generate in a convincing manner, however, we can see that model is able to capture the nuances well and generates accurate chromatin distributions."
NASDM: Nuclei-Aware Semantic Histopathology Image Generation Using Diffusion Models,5,Conclusion and Future Works,"In this work, we present NASDM, a nuclei-aware semantic tissue generation framework. We demonstrate the model on a colon dataset and qualitatively Fig. 4. Qualitative Review: Compiled results from a pathologist review. We have experts assess patches for, their overall medical quality (left), as well as, their consistency with the associated mask (right). We observe that the patches generated by the model do better on all metrics and majority are imperceptible from real patches.and quantitatively establish the proficiency of the framework at this task. In future works, further conditioning on properties like stain-distribution, tissuetype, disease-type, etc. would enable patch generation in varied histopathological settings. Additionally, this framework can be extended to also generate semantic masks enabling an end-to-end tissue generation framework that first generates a mask and then synthesizes the corresponding patch. Further, future works can explore generation of patches conditioned on neighboring patches, as this enables generation of larger tissue areas by composing patches together."
NASDM: Nuclei-Aware Semantic Histopathology Image Generation Using Diffusion Models,,Fig. 1 .,
NASDM: Nuclei-Aware Semantic Histopathology Image Generation Using Diffusion Models,,Fig. 2 .,
NASDM: Nuclei-Aware Semantic Histopathology Image Generation Using Diffusion Models,,Fig. 3 .,
NASDM: Nuclei-Aware Semantic Histopathology Image Generation Using Diffusion Models,,Table 1 .,
DiffDP: Radiotherapy Dose Prediction via a Diffusion Model,1,Introduction,"Radiotherapy, one of the mainstream treatments for cancer patients, has gained notable advancements in past decades. For promising curative effect, a high-quality radiotherapy plan is demanded to distribute sufficient dose of radiation to the planning target volume (PTV) while minimizing the radiation hazard to organs at risk (OARs). To achieve this, radiotherapy plans need to be manually adjusted by the dosimetrists in a trial-and-error manner, which is extremely labor-intensive and time-consuming [1,2]. Additionally, the quality of treatment plans might be variable among radiologists due to their different expertise and experience [3]. Consequently, it is essential to develop a robust methodology to automatically predict the dose distribution for cancer patients, relieving the burden on dosimetrists and accelerating the radiotherapy procedure.Recently, the blossom of deep learning (DL) has promoted the automatic medical image processing tasks [4][5][6], especially for dose prediction [7][8][9][10][11][12][13][14]. For example, Nguyen et al. [7] modified the traditional 2D UNet [15] to predict the dose of prostate cancer patients. Wang et al. [10] utilized a progressive refinement UNet (PRUNet) to refine the predictions from low resolution to high resolution. Besides the above UNetbased frameworks, Song et al. [11] employed the deepLabV3+ [16] to excavate contextual information from different scales, thus obtaining accuracy improvements in the dose prediction of rectum cancer. Mahmood et al. [12] utilized a generative adversarial network (GAN)-based method to predict the dose maps of oropharyngeal cancer. Furthermore, Zhan et al. [13] designed a multi-organ constraint loss to enforce the deep model to better consider the dose requirements of different organs. Following the idea of multi-task learning, Tan et al. [8] utilized isodose line and gradient information to promote the performance of dose prediction of rectum cancer. To ease the burden on the delineation of PTV and OARs, Li et al. [17] constructed an additional segmentation task to provide the dose prediction task with essential anatomical knowledge.Although the above methods have achieved good performance in predicting dose distribution, they suffer from the over-smoothing problem. These DL-based dose prediction methods always apply the L 1 or L 2 loss to guide the model optimization which calculates a posterior mean of the joint distribution between the predictions and the ground truth [17,18], leading to the over-smoothed predicted images without important high-frequency details [19]. We display predicted dose maps from multiple deep models in Fig. 1. As shown, compared with the ground truth, i.e., (5) in Fig. 1, the predictions from (1) to (3) are blurred with fewer high-frequency details, such as ray shapes. These high-frequency features formed by ray penetration reveal the ray directions and dose attenuation with the aim of killing the cancer cells while protecting the OARs as much as possible, which are critical for radiotherapy. Consequently, exploring an automatic method to generate high-quality predictions with rich high-frequency information is important to improve the performance of dose prediction. Currently, diffusion model [20] has verified its remarkable potential in modeling complex image distributions in some vision tasks [21][22][23]. Unlike other DL models, the diffusion model is trained without any extra assumption about target data distribution, thus evading the average effect and alleviating the over-smoothing problem [24]. Figure 1 (4) provides an example in which the diffusion-based model predicts a dose map with shaper and clearer boundaries of ray-penetrated areas. Therefore, introducing a diffusion model to the dose prediction task is a worthwhile endeavor.In this paper, we investigate the feasibility of applying a diffusion model to the dose prediction task and propose a diffusion-based model, called DiffDP, to automatically predict the clinically acceptable dose distribution for rectum cancer patients. Specifically, the DiffDP consists of a forward process and a reverse process. In the forward process, the model employs a Markov chain to gradually transform dose distribution maps with complex distribution into Gaussian distribution by progressively adding pre-defined noise. Then, in the reverse process, given a pure Gaussian noise, the model gradually removes the noise in multiple steps and finally outputs the predicted dose map. In this procedure, a noise predictor is trained to predict the noise added in the corresponding step of the forward process. To further ensure the accuracy of the predicted dose distribution for both the PTV and OARs, we design a DL-based structure encoder to extract the anatomical information from the CT image and the segmentation masks of the PTV and OARs. Such anatomical information can indicate the structure and relative position of organs. By incorporating the anatomical information, the noise predictor can be aware of the dose constraints among PTV and OARs, thus distributing more appropriate dose to them and generating more accurate dose distribution maps.Overall, the contributions of this paper can be concluded as follows: (1) We propose a novel diffusion-based model for dose prediction in cancer radiotherapy to address the over-smoothing issue commonly encountered in existing DL-based dose prediction methods. To the best of our knowledge, we are the first to introduce the diffusion model for this task. (2) We introduce a structure encoder to extract the anatomical information available in the CT images and organ segmentation masks, and exploit the anatomical information to guide the noise predictor in the diffusion model towards generating more precise predictions. (3) The proposed DiffDP is extensively evaluated on a clinical dataset consisting of 130 rectum cancer patients, and the results demonstrate that our approach outperforms other state-of-the-art methods."
DiffDP: Radiotherapy Dose Prediction via a Diffusion Model,2,Methodology,"An overview of the proposed diffDP model is illustrated in Fig. 2, containing two Markov chain processes: a forward process and a reverse process. An image set of cancer patient is defined as {x, y}, where x ∈ R H ×W ×(2+o) represents the structure images, ""2"" signifies the CT image and the segmentation mask of the PTV, and o denotes the total number of segmentation mask of OARs. Meanwhile, y ∈ R H ×W ×1 is the corresponding dose distribution map for x. Concretely, the forward process produces a sequence of noisy images {y 0 , y 1 , . . . , y T }, y 0 = y by gradually adding a small amount of noise to y in T steps with the noise increased at each step and a noise predictor f is constructed to predict the noise added to y t-1 by treating y t , anatomic information from x and embedding of step t as input. To obtain the anatomic information, a structure encoder g is designed to extract the crucial feature representations from the structure images. Then, in the reverse process, the model progressively deduces the dose distribution map by iteratively denoising from y T using the well-trained noise predictor. "
DiffDP: Radiotherapy Dose Prediction via a Diffusion Model,2.1,Diffusion Model,"The framework of DiffDP is designed following the Denoising Diffusion Probabilistic Models (DDPM) [25] which contains a forward process and a reverse process. By utilizing both processes, the DiffDP model can progressively transform the Gaussian noise into complex data distribution.Forward Process. In the forward process, the DiffDP model employs the Markov chain to progressively add noise to the initial dose distribution map y 0 ∼ q(y 0 ) until the final disturbed image y T becomes completely Gaussian noise which is represented as y T ∼ N (y T | 0, I ). This forward process can be formulated as:where α t is the unlearnable standard deviation of the noise added to y t-1 . Herein, the α t (t = 1, . . . , T ) could accumulate during the forward process, which can be treated as the noise intensity γ t = t i=1 α i . Based on this, we can directly obtain the distribution of y t at any step t from y 0 through the following formula:where the disturbed image y t is sampled using:in which ε t ∼ N (0, I ) is random noise sampled from normal Gaussian distribution.Reverse Process. The reverse process also harnesses the Markov chain to progressively convert the latent variable distribution p θ (y T ) into distribution p θ (y 0 ) parameterized by θ . Corresponding to the forward process, the reverse one is a denoising transformation under the guidance of structure images x that begins with a standard Gaussian distribution y T ∼ N (y T | 0, I ). This reverse inference process can be formulated as:where μ θ (x, y t , t) is a learned mean, and σ t is a unlearnable standard deviation. Following the idea of [16], we parameterize the mean of μ θ as:where ε t,θ is a function approximator intended to predict ε t from the input x, y t and γ t . Consequently, the reverse inference at two adjacent steps can be expressed as:where z t ∼ N (0, I ) is a random noise sampled from normal Gaussian distribution. More derivation processes can be found in the original paper of diffusion model [25]."
DiffDP: Radiotherapy Dose Prediction via a Diffusion Model,2.2,Structure Encoder,"Vanilla diffusion model has difficulty preserving essential structural information and produce unstable results when predicting dose distribution maps directly from noise with a simple condition mechanism. To address this, we design a structure encoder g that effectively extracts the anatomical information from the structure images guiding the noise predictor to generate more accurate dose maps by incorporating extracted structural knowledge. Concretely, the structure encoder includes five operation steps, each with a residual block (ResBlock) and a Down block, except for the last one. The ResBlock consists of two convolutional blocks (ConvBlock), each containing a 3 × 3 convolutional (Conv) layer, a GroupNorm (GN) layer, and a Swish activation function.The residual connections are reserved for preventing gradient vanishment in the training.The Down block includes a 3 × 3 Conv layer with a stride of 2. It takes structure image x as input, which includes the CT image and segmentation masks of PTV and OARs, and evacuates the compact feature representation in different levels to improve the accuracy of dose prediction. The structure encoder is pre-trained by L 1 loss and the corresponding feature representation x e = g(x) is then fed into the noise predictor."
DiffDP: Radiotherapy Dose Prediction via a Diffusion Model,2.3,Noise Predictor,"The purpose of the noise predictor f (x e , y t , γ t ) is to predict the noise added on the distribution map y t with the guidance of the feature representation x e extracted from the structure images x and current noise intensity γ t in each step t. Inspired by the great achievements of UNet [15], we employ a six-level UNet to construct the noise predictor. Specifically, the encoder holds the similar architecture with the structure encoder while the decoder comprises five deconvolution blocks to fulfill the up-sampling operation, and each contains an Up block and two ResBlock, except for the last one which discards the UP block. In each Up block, the Nearest neighbor up-sampling and a Conv layer with a kernel size of 1 are used. A bottleneck with two Resblocks and a self-attention module is embedded between the encoder and decoder.In the encoding procedure, to guide the noise predictor with essential anatomical structure, the feature representations respectively extracted from the structure images x and noisy image y t are simultaneously fed into the noise predictor. Firstly, y t is encoded into feature maps through a convolutional layer. Then, these two feature maps are fused by element-wise addition, allowing the structure information in x to be transferred to the noise predictor. The following two down-sampling operations retain the addition operation to complete information fusion, while the last three use a cross-attention mechanism to gain similarity-based structure guidance at deeper levels.In the decoding procedure, the noise predictor restores the feature representations captured by the encoder to the final output, i.e., the noise ε t,θ = f (x e , y t , γ t ) in step t. The skip connections between the encoder and decoder are reserved for multi-level feature reuse and aggregation."
DiffDP: Radiotherapy Dose Prediction via a Diffusion Model,2.4,Objective Function,"The main purpose of the DiffDP model is to train the noise predictor f and structure encoder g, so that the predicted noise ε t,θ = f (g(x), y t , γ t ) in the reverse process can approximate the added noise ε t in the forward process. To achieve this, we define the objective function as:For a clearer understanding, the training procedure is summarized in Algorithm 1."
DiffDP: Radiotherapy Dose Prediction via a Diffusion Model,,Algorithm 1:,"Training procedure 1: Input: Input image pairs where is the structure image and is the corresponding dose distribution map, the total number of diffusion steps 2: Initialize: Randomly initialize the noise predictor and pre-trained structure encoder 3: Repeat 4: Sample 5: Sample 6: Perform the gradient step on Equation (9) 7: until converged"
DiffDP: Radiotherapy Dose Prediction via a Diffusion Model,2.5,Training Details,"We accomplish the proposed network in the PyTorch framework. All of our experiments are conducted through one NVIDIA RTX 3090 GPU with 24 GB memory and a batch size of 16 with an Adaptive moment estimation (Adam) optimizer. We train the whole model for 1500 epochs (about 1.5M training steps) where the learning rate is initialized to 1e-4 and reset to 5e-5 after 1200 epochs. The parameter T is set to 1000. Additionally, the noise intensity is initialized to 1e-2 and decayed to 1e-4 linearly along with the increase of steps."
DiffDP: Radiotherapy Dose Prediction via a Diffusion Model,3,Experiments and Results,"Dataset and Evaluations. We measure the performance of our model on an in-house rectum cancer dataset which contains 130 patients who underwent volumetric modulated arc therapy (VMAT) treatment at West China Hospital. Concretely, for every patient, the CT images, PTV segmentation, OARs segmentations, and the clinically planned dose distribution are included. Additionally, there are four OARs of rectum cancer containing the bladder, femoral head R, femoral head L, and small intestine. We randomly select 98 patients for model training, 10 patients for validation, and the remaining 22 patients for test. The thickness of the CTs is 3 mm and all the images are resized to the resolution of 256 × 256 before the training procedure.We measure the performance of our proposed model with multiple metrics. Considering Dm represents the minimal absorbed dose covering m% percentage volume of PTV, we involve D 98 , D 2 , maximum dose (D max ), and mean dose (D mean ) as metrics. Besides, the heterogeneity index (HI) is used to quantify dose heterogeneity [26]. To quantify performance more directly, we calculate the difference ( ) of these metrics between the ground truth and the predicted results. More intuitively, we involve the dose volume histogram (DVH) [27] as another essential metric of dose prediction performance. When the DVH curves of the predictions are closer to the ground truth, we can infer higher prediction accuracy.Comparison with State-of-the-Art Methods. To verify the superior accuracy of our proposed model, we select multiple state-of-the-art (SOTA) models in dose prediction, containing UNet (2017) [7], GAN (2018) [12], deepLabV3+ (2020) [11], C3D (2021) [9], and PRUNet (2022) [10], for comparison. The quantitative comparison results are listed in Table . 1 where our method outperforms the existing SOTAs in terms of all metrics. Specifically, compared with deepLabV3+ with the second-best accuracy in HI (0.0448) and D 98 (0.0416), the results generated by the proposed are 0.0035 and 0.0014 lower, respectively. As for D 2 and D max , our method gains overwhelming performance with 0.0008 and 0.0005, respectively. Moreover, the paired t-test is conducted to investigate the significance of the results. The p-values between the proposed and other SOTAs are almost all less than 0.05, indicating that the enhancement of performance is statistically meaningful.Besides the quantitative results, we also present the DVH curves derived by compared methods in Fig. 3. The results are compared on PTV as well as two OARs: bladder and small intestine. Compared with other methods, the disparity between the DVH curves of   our method and the ground truth is the smallest, demonstrating the superior performance of the proposed. Furthermore, we display the visualization comparison in Fig. 4. As we can see, the proposed model achieves the best visual quality with clearer and sharper high-frequency details (as indicated by red arrows). Furthermore, the error map of the proposed is the darkest, suggesting the least disparity compared with the ground truth.Ablation Study. To study the contributions of key components of the proposed method, we conduct the ablation experiments by 1) removing the structure encoder from the proposed method and concatenating the anatomical images x and noisy image y t together as the original input for diffusion model (denoted as Baseline); 2) the proposed DiffDP model. The quantitative results are given in Table 2. We can clearly see the performance for all metrics is enhanced with the structure encoder, demonstrating its effectiveness in the proposed model. "
DiffDP: Radiotherapy Dose Prediction via a Diffusion Model,4,Conclusion,"In this paper, we introduce a novel diffusion-based dose prediction (DiffDP) model for predicting the radiotherapy dose distribution of cancer patients. The proposed method involves a forward and a reverse process to generate accurate prediction by progressively transferring the Gaussian noise into a dose distribution map. Moreover, we propose a structure encoder to extract anatomical information from patient anatomy images and enable the model to concentrate on the dose constraints within several essential organs. Extensive experiments on an in-house dataset with 130 rectum cancer patients demonstrate the superiority of our method."
DiffDP: Radiotherapy Dose Prediction via a Diffusion Model,,Fig. 1 .,
DiffDP: Radiotherapy Dose Prediction via a Diffusion Model,,Fig. 2 .,
DiffDP: Radiotherapy Dose Prediction via a Diffusion Model,,Fig. 3 .,
DiffDP: Radiotherapy Dose Prediction via a Diffusion Model,,Fig. 4 .,
DiffDP: Radiotherapy Dose Prediction via a Diffusion Model,,Table 1 .,
DiffDP: Radiotherapy Dose Prediction via a Diffusion Model,,Table 2 .,
Coupling Bracket Segmentation and Tooth Surface Reconstruction on 3D Dental Models,1,Introduction,"With the rapid development of 3D scanning devices, the application of computeraided diagnosis in orthodontics has gradually developed. In orthodontic treatment, an essential step for patients is to wear retainers. Orthodontists need to remove the brackets, utilize the intra-oral scanners to acquire the digital models, and then print models to fabricate retainers. However, there is a long waiting time for patients and the position of teeth would change, which affects the effectiveness of orthodontic treatment. As shown in Fig. 1, removing the brackets on 3D dental models and reconstructing the tooth surface can enable orthodontists to pre-make retainers. Patients can get retainers immediately after removing brackets which reduces the waiting time. When the retainer is lost or broken, a new one can be easily fabricated by using the archived digital model. The whole process helps to maintain the long-term stability of orthodontic treatment. However, it takes approximately 20 min for an orthodontist to precisely delineate brackets on a digital model. Besides that, the bracket segmentation requires a very high level of precision otherwise it will affect the reconstruction step. Therefore, an efficient and precise bracket segmentation method is crucial in orthodontic treatment. Orthodontists usually use CAD software (i.e., Geomagic Studio) to reconstruct the tooth surface after removing the brackets due to the clinical applicability of its reconstructed results. However, it requires interactive manual operation and automatic segmentation algorithms cannot be deployed to the software. To automate the entire process, a 3D mesh reconstruction algorithm is also needed. Integrating segmentation and reconstruction into a unified framework can serve as an effective tool to assist orthodontic treatment.Since 3D dental models can be transformed into point clouds, methods proposed for point cloud segmentation would provide guidance. Point cloud segmentation methods can be mainly summarized as MLP-based [10,12,13], graphbased [7,14], convolution-based [16], and attention-based [4]. Besides that, several methods have been proposed for computer-aided orthodontic processes such as tooth segmentation [3,9,17], landmark localization [5,6,15], and tooth completion [11,19] on 3D dental models. A two-stream graph-based network TSGCNet [17] is proposed for tooth segmentation. ToothCR [19] is proposed to recover the missing tooth which consists of point completion and surface reconstruction. The holes formed by the removal of the brackets are simple and regular polygons located on each tooth surface. An effective and fast method should be proposed to fill these characteristic holes and reconstruct the tooth surface. Different from the network [8] proposed for single-tooth bracket separation, our work focuses on the more challenging task of segmenting entire dental models. To the best of our knowledge, there exists no work proposed to integrate the segmentation and reconstruction of 3D dental models to automate the overall process.As graph-based network [6,9,15,17,18] shows its superiority on various tasks on 3D dental models, we analyze the performance of different local operations and modules in the graph network. Based on these analyses, we propose a network named BSegNet for bracket segmentation. After segmenting the brackets, we adopt a simple yet effective projection-based method to reconstruct the tooth surface, which converts 3D holes into a 2D plane and triangulates the projected polygons on the 2D plane. We then estimate the z coordinate of the new vertices through the neighbor information and transform the vertices to the original space. The proposed method can better recover the surface of teeth.The contributions of this paper are as follows: 1) We propose a graph-based network named BSegNet for bracket segmentation on 3D dental models which can reduce the burden on orthodontists; 2) An effective method is proposed to reconstruct the tooth surface where brackets are removed; 3) The low reconstruction error of the automatically processed models suggests that the framework integrating the segmentation and reconstruction can be used as a powerful tool to assist orthodontists."
Coupling Bracket Segmentation and Tooth Surface Reconstruction on 3D Dental Models,2,Method,"We propose a network named BSegNet for bracket segmentation on 3D dental models. BSegNet regards each mesh cell as a graph node and updates the nodewise feature via several local modules. For dental models with brackets removed, a simple yet effective reconstruction method is adopted to reconstruct the tooth surface. We will describe the network architecture of BSegNet (Fig. 2) and the process of tooth surface reconstruction in detail."
Coupling Bracket Segmentation and Tooth Surface Reconstruction on 3D Dental Models,2.1,Bracket Segmentation Network,"Compared to point clouds, more geometric spatial information can be obtained from 3D mesh data. Different input features will have different effects on the subsequent network training. In this paper, we use a 24-dimension vector as the initial input. The 24-dimensional input vector corresponds to the coordinates of three vertices, the normal vectors of three vertices, the normal vectors of the mesh cell, and the coordinates of the mesh cell centroid.We use a MLP to map the input featureThen a transform net is adopted to transform the feature F 1 into a canonical space to improve the robustness. The transformed feature is fed to several local modules which are designed to encode the local features of each mesh cell in semantic space. In each local module, the first step is to construct the graph G(V, E) where V = {c 1 , c 2 , ..., c N } denotes the set of mesh cells and E represents the After building the graph, we construct the local feature of each cell and update the feature through the graph convolution layers. Let the f l i ∈ R 64 denote the feature vector of the mesh cell c i in the l-th layer anddenotes the edge feature which is used to capture the geometric relationship between each cell and its neighbors. Then the local feature f local = ( f l ij ⊕ f l i ) goes through two Conv2D layers to further encode the feature. The update process of the mesh cell features (graph node) is defined aswhere h ϑ denotes the Conv2D layers and R(.) stands for the feature aggregation function. To avoid the over-smoothing problem in the graph network, we use the residual connection in each local module the same as DeepGCN [7]. We concatenate the features of each local module and use an MLP layer to form a global high-dimension feature. Then the high-dimension global feature is concatenated with local features, forming the node-wise feature. Finally, several projection layers and one classifier layer are used to predict an N ×2 probability matrix.As the prediction results of the network may have some isolated labeled mesh cells, we use the graph-cut method to refine the results. The post-processing stage minimizes an energy function by combining the probability term and the smoothness term. The energy function to be optimized is defined aswhere p i (l i ) denotes the probability belongs to the l i , is the minimal probability threshold, and λ denotes the smooth parameter. The local smoothness term is defined aswhere θ ij denotes the dihedral angle of two adjacent facets and d ij denotes the distance between the centroids of two adjacent facets."
Coupling Bracket Segmentation and Tooth Surface Reconstruction on 3D Dental Models,2.2,Tooth Surface Reconstruction,"First, we identify all the holes and extract their boundaries. Then we project the vertices of the boundary into the 2D plane. We triangulate the projected polygon without inserting new vertices. If the line segment inside the polygon exceeds the preset length, it should be n-equally divided. To get more uniform triangles, we use the optimal delaunay triangulation algorithm [1] to iteratively optimize the position of vertices. The optimization process of vertices is as followswhere p is the vertex needs to be optimized, Ω(p) denotes the set of first-ring neighborhood mesh cells of p, |T j | denotes the area of mesh cell T j , c j is the circumcentre of T j . After optimizing the positions of the vertices, the triangles have almost the same angles and the distribution is closer to the original models. After the polygon triangulation, the x-y coordinates are determined and a layer-by-layer procedure is employed to estimate the corresponding z-values. To avoid the vertices at the gingiva, we only use the information of the first-ring neighborhoods of the boundary vertices. We first compute the slopes of the boundary vertices which are defined aswhere N 1 (i) denotes the set of first-ring neighborhood vertices of the boundary point b i . Then the calculation of the z-coordinate value is denoted aswhere N b (i) denotes the set of adjacent boundary points and k b j denotes the slope of boundary vertex b j . Then we regard the added points as new boundary vertices and repeat the above process until the z-values of all vertices are calculated.Before transforming back to the original space, the extreme z values are removed by median filtering. Then we use a rotation matrix to obtain the coordinates in the original space. Finally, we employ Laplacian smoothing to enhance the smoothness of the reconstructed surface. To make the reconstructed surface blend better with the boundary, we need to reduce the effect of smoothing on the first-ring neighborhood of the boundary. The calculation equation is as followswhere p b and p s denote the vertex before and after smoothing, d b is the average distance between the vertex and the adjacent boundary vertices, and d h is the average length of the hole boundary."
Coupling Bracket Segmentation and Tooth Surface Reconstruction on 3D Dental Models,3,Experiments and Results,
Coupling Bracket Segmentation and Tooth Surface Reconstruction on 3D Dental Models,3.1,Datasets and Implementation Details,"We collect 80 dental mesh models in STL format from different patients. The number of mesh cells in each dental model is approximately 100,000 and all the dental models are down-sampled to nearly 24,000 mesh cells. The ground truth segmentations are annotated by professional orthodontists on the downsampled dental models. We divide the dataset into a training, validation, and test set which consists of 45, 14, and 21 subjects, respectively. The performance of bracket segmentation is evaluated by mean Intersection-over-Union (mIoU) and Overall Accuracy (OA). The performance of reconstruction is evaluated by the Mean Distance (MD) and Standard Deviation (SD) of the distance between the models reconstructed by our method and by Geomagic Studio. We also evaluate the reconstruction error of models processed by the automatic framework and manually by orthodontists. We train all the networks by minimizing the crossentropy loss for 400 epochs except for MeshSegNet which minimizes the dice loss. We use the Adam optimizer and set the mini-batch as 5. The initial learning rate is 0.001, and we anneal the learning rate using the cosine functions. "
Coupling Bracket Segmentation and Tooth Surface Reconstruction on 3D Dental Models,3.2,Experimental Evaluation,"Comparison Results. We compare our method with SOTA point cloud segmentation methods and teeth segmentation methods. All the results are shown in Table 1 and BSegNet achieves the best performance. Since PointNet lacks the feature encoding of local regions, it performs worst among all the methods. The performance of PointNet++ and PointConv are close and outperform PointNet by a large margin. Although PointMLP has the best performance among all the compared methods, the mIoU of BSegNet is higher (94.60 vs. 93.15). We also compare our method with the attention-based method PCT and the mIoU of our method is higher than PCT (94.60 vs. 92.99). The tooth segmentation network on 3D dental models performs worse than several point segmentation methods in the bracket segmentation task, especially for TSGCNet which uses a two-stream network to encode the coordinates and normal vectors respectively. MeshSegNet performs better than TSGCNet but the proposed graph-constrained learning modules cause high computation complexity. Our method is based on the DGCNN but the mIoU of our method is much higher (94.60 vs. 92.42).As shown in Table 2, the reconstruction error of the models predicted by BSegNet is significantly lower than PointMLP. When reconstructing the models processed by doctors, our reconstruction method achieves low values of SD and MD which reveals it can replace the interactive reconstruction operation to some extent. We also compare our method with another reconstruction method Meshfix and our method has a lower value of SD (0.032 vs. 0.049). Compared to the manual process by doctors, the reconstruction error of our automatic framework is clinically acceptable and it can assist in orthodontic treatment. Figure 3 displays the segmentation and tooth surface reconstruction results.Ablation Study and Analysis. In this section, we analyze different operations and modules in the BSegNet. As shown in Table 3, the performance of the 24-dimension input is better than the 15-dimension input which suggests the   "
Coupling Bracket Segmentation and Tooth Surface Reconstruction on 3D Dental Models,4,Conclusion,"In this paper, we propose a network named BSegNet for bracket segmentation on 3D dental models which can reduce the burden on orthodontists. BSegNet is a graph-based network that employs dynamic dilated neighborhood construction and residual connections to improve segmentation results. With label optimization, the segmentation results can be further refined. Experimental results on a clinical dataset demonstrate our method significantly outperforms related stateof-the-art methods. We also propose a simple yet effective method to reconstruct the tooth surface which can better recover the feature of the teeth. The whole framework achieves a low reconstruction error and can be used as a powerful tool to assist doctors in orthodontic diagnosis."
Coupling Bracket Segmentation and Tooth Surface Reconstruction on 3D Dental Models,,Fig. 1 .,
Coupling Bracket Segmentation and Tooth Surface Reconstruction on 3D Dental Models,,Fig. 2 .,
Coupling Bracket Segmentation and Tooth Surface Reconstruction on 3D Dental Models,,Fig. 3 .,
Coupling Bracket Segmentation and Tooth Surface Reconstruction on 3D Dental Models,,Table 1 .,
Coupling Bracket Segmentation and Tooth Surface Reconstruction on 3D Dental Models,,Table 2 .,SD(mm) MD+(mm) MD-(mm) Aver.(mm) normal
Coupling Bracket Segmentation and Tooth Surface Reconstruction on 3D Dental Models,,Table 3 .,
Coupling Bracket Segmentation and Tooth Surface Reconstruction on 3D Dental Models,,Table 4 .,
Coupling Bracket Segmentation and Tooth Surface Reconstruction on 3D Dental Models,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 40.
Whole-Heart Reconstruction with Explicit Topology Integrated Learning,1,Introduction,"With the advent of advanced medical imaging technologies, such as computed tomography (CT) and magnetic resonance (MR), non-invasive visualizations of various human organs and tissues become feasible and are widely utilized in clinical practice [8,22]. Cardiac CT imaging and MR imaging play important roles in the understanding of cardiac anatomy, diagnosis of cardiac diseases [20] and multimodal visualizations [9]. Potential applications range from patient-specific treatment planning, virtual surgery, morphology assessment to biomedical simulations [2,17]. However, in traditional procedures, visualizing human organs usually requires significant expert efforts and could take up to dozens of hours depending on the specific organs of interest [7,24], which makes large-cohort studies prohibitive and limits clinical applications [16].Empowered by the great feature extraction ability of deep neural networks (DNNs) and the strong parallel computing power of graph processing units (GPUs), automated visualizations of cardiac organs have been extensively explored in recent years [18,24]. These methods typically follow a common processing flow that requires a series of post-processing steps to produce acceptable reconstruction results. Specifically, the organs of interest are first segmented from medical imaging data. After that, an isosurface generation algorithm, such as marching cubes [15], is utilized to create 3D visualizations typically with staircase appearance, followed by smoothing filters to create smooth meshes. Finally, manual corrections or connected component analyses [11] are applied to remove artifacts and improve topological correctness. The entire flow is not optimized in an end-to-end fashion, which might introduce and accumulate multi-step errors or still demand non-trivial manual efforts.In such context, automated approaches that can directly and efficiently generate cardiac shapes from medical imaging data are highly desired. Recently, various DNN works [1,3,4,[12][13][14]19] delve into this topic and achieve promising outcomes. In particular, the method depicted in [1] performs predictions of cardiac ventricles using both cine MR and patient metadata based on statistical shape modeling (SSM). Similarly, built on SSM, [4] uses 2D cine MR slices to generate five cardiac meshes. Another approach proposed in [19] employs distortion energy to produce meshes of the aortic valves. Inspiringly, graph neural network (GNN) based methods [12][13][14] are shown to be capable of simultaneously reconstructing seven cardiac organs in a single pass, producing whole-heart meshes that are suitable for computational simulations of cardiac functioning. The training processes for these aforementioned methods are usually optimized via the Chamfer distance (CD) loss, a point cloud based evaluation metric. Such type of point cloud based losses is first calculated for each individual vertex, followed by an average across all vertices, which nonetheless does not take the overall mesh topology into consideration. This could result in suboptimal or even incorrect topology in the reconstructed mesh, which is undesirable.To solve this issue, we introduce a novel surface loss that inherently considers the topology of the two to-be-compared meshes in the loss function, with a goal of optimizing the anatomical topology of the reconstructed mesh. The surface loss is defined by a computable norm on currents [6] and is originally introduced in [23] for diffeomorphic surface registration, which has extensive applicability in shape analysis and disease diagnosis [5,21]. Motivated by its inherent ability to characterize and quantify a mesh's topology, we make use of it to minimize the topology-considered overall difference between a reconstructed mesh and its corresponding ground truth mesh. Such currents guided supervision ensures effec-tive and efficient whole-heart mesh reconstructions of seven cardiac organs, with high reconstruction accuracy and correct anatomical topology being attained."
Whole-Heart Reconstruction with Explicit Topology Integrated Learning,2,Methodology,"Figure 1 illustrates the proposed end-to-end pipeline, consisting of a voxel feature extraction module (top panel) and a deformation module (middle panel). The inputs contain a CT or MR volume accompanied by seven initial spherical meshes. To be noted, the seven initial spherical meshes are the same for all training and testing cases. A volume encoder followed by a decoder is employed as the voxel feature extraction module, which is supervised by a segmentation loss comprising binary cross entropy (BCE) and Dice. This ensures that the extracted features explicitly encode the characteristics of the regions of interest (ROIs). For the deformation module, a GNN is utilized to map coordinates of the mesh vertices, combine and map trilinearly-interpolated voxel features indexed at each mesh vertex, extract mesh features, and deform the initial meshes to reconstruct the whole-heart meshes. There are three deformation blocks that progressively deform the initial meshes. Each deformation block is optimized on three types of losses: a surface loss for both accuracy and topology correctness purposes, a point cloud loss for an accuracy purpose, and three regularization losses for a smoothness purpose. The network structure details of the two modules are detailed in the supplementary material.For an input CT or MR volume, it passes into the voxel feature extraction module to predict binary segmentation for the to-be-reconstructed ROIs. Meanwhile, the initial spherical meshes enter into the first deformation block along with the trilinearly-interpolated voxel features to predict the vertex-wise displacements of the initial meshes. Then, the updated meshes go through the following blocks for subsequent deformations. The third deformation block finally outputs the reconstructed whole-heart meshes. The three deformation blocks follow the same process, except for the meshes they deform and the trilinearlyinterpolated voxel features they operate on. In the first deformation block, we use high-level voxel features, f 3 and f 4 , obtained from the deepest layers of the volume encoder. In the second deformation block, the middle-level voxel features, f 1 and f 2 , are employed. As for the last deformation block, its input meshes are usually quite accurate and only need to be locally refined. Thus, low-level voxel features are employed to supervise this refining process.Surface Representation as Currents. Keeping in line with [23], we employ a generalized distribution from geometric measure theory, namely currents [6], to represent surfaces. Specifically, surfaces are represented as objects in a linear space equipped with a computable norm. Given a triangular mesh S embedded in R 3 , it can be associated with a linear functional on the space of 2-form via the following equationwhere for each x ∈ S u 1 x and u 2x form an orthonormal basis of the tangent plane at x. ω(x) is a skew-symmetric bilinear function on R 3 . dσ(x) represents the basic element of surface area. Subsequently, a surface can be represented as currents in the following expressionwhere S(ω) denotes the currents representation of the surface. f denotes each face of S and σ f is the surface measure on f . ω(x) is the vectorial representation of ω(x), with • and × respectively representing dot product and cross product.After the currents representation is established, an approximation of ω over each face can be obtained by using its value at the face center.Letis the center of the face and N (f ) = 1  2 (e 2 × e 3 ) is the normal vector of the face with its length being equal to the face area. Then, ω can be approximated over the face by its value at the face center, resulting in S(ω) ≈ f ω(c(f ))•N (f ). In fact, the approximation is a sum of linear evaluation functionals C(S) = f δ N (f ) c(f ) associated with a Reproducing Kernel Hilbert Space (RKHS) under the constraints presented elsewhere [23].Thus, S ε , the discrepancy between two surfaces S and T , can be approximately calculated via the RKHS as belowwhere W * is the dual space of a Hilbert space (W, •, • W ) of differential 2-forms and || || 2 is l 2 -norm. () T denotes the transpose operator. f , g index the faces of S and q, r index the faces of T . k W is an isometry between W * and W , and we have. The first and third terms enforce the structural integrity of the two surfaces, while the middle term penalizes the geometric and spatial discrepancies between them. With this preferable property, Eq. 3 fulfills the topology correctness purpose, the key of this proposed pipeline."
Whole-Heart Reconstruction with Explicit Topology Integrated Learning,,Surface Loss.,"As in [23], we choose a Gaussian kernel as the instance of), where x and y are the centers of two faces and σ W is a scale controlling parameter that controls the affecting scale between the two faces. Therefore, the surface loss can be expressed aswhere t 1 , t 2 , t and p 1 , p 2 , p respectively index faces on the reconstructed surfaces S R and those on the corresponding ground truth surfaces S T . L surf ace not only considers each face on the surfaces but also its corresponding direction. When the reconstructed surfaces are exactly the same as the ground truth, the surface loss L surf ace should be 0. Otherwise, L surf ace is a bounded positive value [23].Minimizing L surf ace enforces the reconstructed surfaces to be progressively close to the ground truth as the training procedure develops. Figure 2 illustrates how σ W controls the affecting scale of a face on a surface. The three surfaces are identical meshes of a left atrium structure except for the affecting scale (shown in different colors) on them. There are three colored circles (red, blue, and green) respectively representing the centers of three faces on the surfaces, and the arrowed vectors on these circles denote the corresponding face normals. The color bar ranges from 0 to 1, with 0 representing no effect and 1 representing the most significant effect. From Fig. 2, the distance between the blue circle and the red one is closer than that between the blue circle and the green one, and the effect between the red circle and the blue one is accordingly larger than that between the red circle and the green one. With σ W varying from a large value to a small one, the effects between the red face and other remaining faces become increasingly small. In this way, we are able to control the acting scale of the surface loss via changing the value of σ W . Assigning σ W a value that covers the entire surface results in a global topology encoding of the surface, while assigning a small value that only covers neighbors shall result in a topology encoding that focuses on local geometries.Loss Function. In addition to the surface loss we introduce above, we also involve two segmentation losses L BCE and L Dice , one point cloud loss L CD , and three regularization losses L laplace , L edge , and L normal that comply with [13]. The total loss function can be expressed as:where w s is the weight for the segmentation loss, and w 1 , w 2 , w 3 and w 4 are respectively the weights for the surface loss, the Chamfer distance, the Laplace loss, and the edge loss. The geometric mean is adopted to combine the five individual mesh losses to accommodate their different magnitudes.L seg ensures useful feature learning of the ROIs. L surf ace enforces the integrity of the reconstructed meshes and makes them topologically similar to the ground truth. L CD makes the point cloud representation of the reconstructed meshes to be close to that of the ground truth. Additionally, L laplace , L edge , and L normal are employed for the smoothness consideration of the reconstructed meshes."
Whole-Heart Reconstruction with Explicit Topology Integrated Learning,3,Experiments,"Datasets and Preprocessing. We evaluate and validate our method on a publicly-accessible dataset MM-WHS (multi-modality whole heart segmentation) [24], which contains 3D cardiac images of both CT and MR modalities. 20 cardiac CT volumes and 20 cardiac MR volumes are provided in the training set. 40 held-out cardiac CT volumes and 40 held-out cardiac MR volumes are offered in the testing set. All training and testing cases are accompanied by expertlabeled segmentation of seven heart structures: the left ventricle (LV), the right ventricle (RV), the left atrium (LA), the right atrium (RA), the myocardium of the LV (Myo), the ascending aorta (Ao) and the pulmonary artery (PA). For preprocessing, we follow [13] to perform resizing, intensity normalization, and data augmentation (random rotation, scaling, shearing, and elastic warping) for each training case. Data characteristics and preprocessing details are summarised in the supplementary material.Evaluation Metrics. In order to compare with existing state-of-the-art (SOTA) methods, four metrics as in [13] are employed for evaluation, including Dice, Jaccard, average symmetric surface distance (ASSD), and Hausdorff distance (HD). Furthermore, intersected mesh facets are detected by TetGen [10] and used for quantifying self-intersection (SI).Table 1. Comparisons with two SOTA methods on the MM-WHS CT test data. The MeshDeform [13] results are obtained from our self-reimplementation, while the Voxel2Mesh results are directly copied from [13]  Results. We compare our method with two SOTA methods on the five evaluation metrics. Ours and MeshDeform [13] are trained on the same dataset consisting of 16 CT and 16 MR data that are randomly selected from the MM-WHS training set with 60 augmentations for each, and the remaining 4 CT and 4 MR are used for validation. Evaluations are performed on the encrypted testing set with the officially provided executables. We reimplement MeshDeform [13] with Pytorch according to the publicly available Tensorflow version. Please note the Voxel2Mesh results are directly obtained from [13] since its code has not been open sourced yet. Training settings are detailed in the supplementary material.Table 1 shows evaluation results on the seven heart structures and the whole heart of the MM-WHS CT testing set. Our method achieves the best results in most entries. For SI, Voxel2Mesh holds the best results in most entries because of its unpooling operations in each deformation procedure, in which topological information is additionally used. However, as described in [13], Voxel2Mesh may easily encounter out-of-memory errors for its increasing vertices along the reconstruction process. More results for the MR data can be found in the supplementary material. Figure 3 shows the best and the worst CT results for MeshDeform with respect to Dice and our results on the same cases. Noticeably, the best case for MeshDeform is not the best for our method. For that best case of MeshDeform, we can see obvious folded areas on the mesh of PA, while our method yields more satisfactory visualization results. As for the worst case, both methods obtain unsatisfactory visualizations. However, the two structures (PA and RV) obtained from MeshDeform intersect with each other, leading to significant topological errors. Our method does not have such topology issues.Ablation Study. For the ablation study, we train a model without the surface loss while keeping the rest the same. Table 2 shows the ablation analysis results on the CT data, which apparently validates the effectiveness of the surface loss. "
Whole-Heart Reconstruction with Explicit Topology Integrated Learning,4,Conclusion,"In this work, we propose and validate a whole-heart mesh reconstruction method incorporating a novel surface loss. Due to the intrinsic and favorable property of the currents representation, our method is able to generate accurate meshes with the correct topology."
Whole-Heart Reconstruction with Explicit Topology Integrated Learning,,Fig. 1 .,
Whole-Heart Reconstruction with Explicit Topology Integrated Learning,,Fig. 2 .,
Whole-Heart Reconstruction with Explicit Topology Integrated Learning,,Fig. 3 .,
Whole-Heart Reconstruction with Explicit Topology Integrated Learning,,,
Whole-Heart Reconstruction with Explicit Topology Integrated Learning,,Table 2 .,
Whole-Heart Reconstruction with Explicit Topology Integrated Learning,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 11.
Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning,1,Introduction,"In the past few years, the development of histopathological whole slide image (WSI) analysis methods has dramatically contributed to the intelligent cancer diagnosis [4,10,15]. However, due to the limitation of hardware resources, it is difficult to directly process gigapixel WSIs in an end-to-end framework. Recent studies usually divide the WSI analysis into multiple stages.Generally, multiple instance learning (MIL) is one of the most popular solutions for WSI analysis [14,17,18]. MIL methods regard WSI recognition as a weakly supervised learning problem and focus on how to effectively and efficiently aggregate histopathological local features into a global representation. Several studies introduced attention mechanisms [9], recurrent neural networks [2] and graph neural network [8] to enhance the capacity of MIL in structural information mining. More recently, Transformer-based structures [13,19] are proposed to aggregate long-term relationships of tissue regions, especially for large-scale WSIs. These Transformer-based models achieved state-of-the-art performance in sub-type classification, survival prediction, gene mutant prediction, etc. However, these methods still rely on at least patient-level annotations. In the networkbased consultation and communication platforms, there is a vast quantity of unlabeled WSIs not effectively utilized. These WSIs are usually without any annotations or definite diagnosis descriptions but are available for unsupervised learning. In this case, self-supervised learning (SSL) is gradually introduced into the MIL-based framework and is becoming a new paradigm for WSI analysis [1,11,16]. Typically, Chen et al. [5] explored and posed a new challenge referred to as slide-level self-learning and proposed HIPT, which leveraged the hierarchical structure inherent in WSIs and constructed multiple levels of the self-supervised learning framework to learn high-resolution image representations. This approach enables MIL-based frameworks to take advantage of abundant unlabeled WSIs, further improving the accuracy and robustness of tumor recognition.However, HIPT is a hierarchical learning framework based on a greedy training strategy. The bias and error generated in each level of the representation model will accumulate in the final decision model. Moreover, the ViT [6] backbone used in HIPT is originally designed for nature sense images in fixed sizes whose positional information is consistent. However, histopathological WSIs are scale-varying and isotropic. The positional embedding strategy of ViT will bring ambiguity into the structural modeling. To relieve this problem, KAT [19] built hierarchical masks based on local anchors to maintain multi-scale relative distance information in the training. But these masks are manually defined which is not trainable and lacked orientation information. The current embedding strategy for WSI structural description is not complete.In this paper, we propose a novel whole slide image representation learning framework named position-aware masked autoencoder (PAMA), which achieves slide-level representation learning by reconstructing the local representations of the WSI in the patch feature space. PAMA can be trained end-to-end from the local features to the WSI-level representation. Moreover, we designed a position-aware cross-attention mechanism to guarantee the correlation of localto-global information in the WSIs while saving computational resources. The proposed approach was evaluated on a public TCGA-Lung dataset and an in-house Endometrial dataset and compared with 6 state-of-the-art methods. The results have demonstrated the effectiveness of the proposed method.The contribution of this paper can be summarized into three aspects. (1) We propose a novel whole slide image representation learning framework named position-aware masked autoencoder (PAMA). PAMA can make full use of abundant unlabeled WSIs to learn discriminative WSI representations. (2) We propose a position-aware cross-attention (PACA) module with a kernel reorientation (KRO) strategy, which makes the framework able to maintain the spatial integrity and semantic enrichment of slide representation during the selfsupervised training. (3) The experiments on two datasets show our PAMA can achieve competitive performance compared with SOTA MIL methods and SSL methods. "
Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning,2,Methods,
Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning,2.1,Problem Formulation and Data Preparation,"MAE [7] is a successful SSL framework that learns image presentations by reconstructing the masked image in the original pixel space. We introduced this paradigm to WSI-level representation learning. The flowchart of the proposed work is illustrated in Fig. 1. First, we divided WSIs into non-overlapping image patches and meanwhile removed the background without tissue regions based on a threshold (as shown in Fig. 1(I)). Then, we applied the self-supervised learning framework DINO [3] for patch feature learning and extraction. Afterward, the features for a WSI are represented as X ∈ R np×d f , where d f is the dimension of the feature and n p is the number of patches in the WSI. Inspired by KAT [19], we extracted multiple anchors by clustering the location coordinates of patches for the auxiliary description of the WSI structure. We assigned trainable representations for these anchors, which are formulated as K ∈ R n k ×d f , where n k is the number of anchors in the WSI. Here, we regard each anchor as an observation point of the tissue and assess the relative distance and orientation from the patch positions to the anchor positions. Specifically, a polar coordinate system is built on each anchor position, and the polar coordinates of all the patches on the system are recorded. Finally, a relative distance matrix D ∈ N n k ×np and relative polar angle matrix P ∈ N n k ×np are obtained, where D ij ∈ D and P ij ∈ P respectively represent the distance and polar angle of the i-th patch in the polar coordinate system that takes the position of the j-th anchor as the pole. Then, we can formulate a WSI as S = {X, K, D, P}."
Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning,2.2,Masked WSI Representation Autoencoder,"Figure 1(II) illustrates the procedure of WSI representation learning. Referring to MAE [7], we random mask patch tokens with a high masking ratio (i.e. 75% in our experiments). The remaining tokens (as shown in Fig. 1(b)) are fed into the encoder. Each encoder block sequentially consists of LayerNorm, PACA module, LayerNorm, and multilayer perceptron (MLP), as shown in Fig. 1(c). Then, masked tokens are appended into encoded tokens to conduct the full set of tokens, which is shown in Fig. 1(d). Next, the decoder reconstructs the slide representation in feature space. Finally, mean squared error (MSE) loss is built between the reconstructed patch features and the original patch features. Referring to MAE [7], a trainable token is appended to the patch tokens to extract the global representation. After training, the pre-trained encoder will be employed as the backbone for various downstream tasks."
Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning,2.3,Position-Aware Cross-Attention,"To preserve the structure information of the tissue, we propose the positionaware cross-attention (PACA) module, which is the core of the encoder and decoder blocks. The structure of PACA is shown in Fig. 1"
Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning,,(III).,"The message passing between the anchors and patches is achieved by a bidirectional cross-attention between the patches and anchors. First, the anchors collect the local information from the patches, which is formulated aswhere W l ∈ R d f ×de , l = q, k, v are learnable parameters with d e denoting the dimension of the head output, σ represents the softmax function, and ϕ d and ϕ p are the embedding functions that respectively take the distance and polar angle as input and output the corresponding trainable embedding values. Symmetrically, each patch token catches the information of all anchors into their own local representations by the equationsThe two-way communication makes the patches and anchors timely transmit local information and perceive the dynamic change of global information. The embedding of relative distance and polar angle information helps the model maintain the semantic and structural integrity of the WSI and meanwhile prevents the WSI representation from collapsing to the local area throughout the training process.In terms of efficiency, the computational complexity of self-attention is O(n p 2 ) where n p is the number of patch tokens. In contrast, our proposed PACA's complexity is O(n k × n p ) where n k is the number of anchors. Notice that n k << n p , the complexity is close to O(n p ), i.e. linear correlation with the size of the WSI."
Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning,2.4,Kernel Reorientation,"As for the polar angle matrix P ∈ N n k ×np , we specify the horizontal direction of all the anchors as the initial polar axis. In natural scene images, there is natural directional conspicuousness of semantics. For instance, in the case of a church, it is most likely to find a door below the windows rather than be located above them. But histopathology images have no absolute definition of direction. The semantics of WSI will not change with rotation and flip. Namely, it is isotropic. Embedding the orientation information with a fixed polar axis will lead to ambiguities in various slides.To address this problem, we design a kernel reorientation (KRO) strategy to dynamically update the polar axis during the training. As shown in Fig. 1(IV), we equally divide the polar coordinate system into N bins and calculate the sum of the attention scores from each bin. Then, the orientation with the highest score is recognized as the new polar axis for the anchor. Based on the updated polar axis, we can then amend P (n) to P (n+1) . The detailed algorithm is described in Algorithm 1."
Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning,3,Experiments and Results,
Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning,3.1,Datasets,"We evaluated the proposed method on two datasets, the public TCGA-Lung and the in-house Endometrial dataset, which are introduced as follows.Algorithm 1: Kernel Reorientation algorithm.Input: P (n) ∈ N H×n k ×np : The relative polar angle matrix of n-th block, where H is the head number of multi-head attention, n k is the number of anchors in the WSI, np is the number of patches in the WSI; A (n) ∈ R H×n k ×np : The attention matrix from anchors to patches, defined asD score : A dictionary taking the angle as KEY for storing attention scores; Output: P (n+1) ∈ R H×n k ×np : The updated polar angle matrix.h,i,max = arg max D score ; // Find the orientation that has the highest attention score. for j in np do Ph,i,max ;// Reorientation."
Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning,,end end end,"TCGA-Lung dataset is collected from The Cancer Genome Atlas (TCGA) Data Portal. The dataset includes a total of 3,064 WSIs, which consist of three categories, namely Tumor-free (Normal), Lung Adenocarcinoma (LUAD), and Lung Squamous Cancer (LUSC), Endometrial dataset includes 3,654 WSIs of endometrial pathology, which includes 8 categories, namely Well/Moderately/Low-differentiated endometrioid adenocarcinoma, Squamous differentiation carcinoma, Plasmacytoid carcinoma, Clear cell carcinoma, Mixed-cell adenocarcinoma, and benign tumor.Each dataset was randomly divided into training, validation and test sets according to 6:1:3 while keeping each category of data proportionally. We conducted WSI multi-type classification experiments on the two datasets. The validation set was used to perform an early stop. The results of the test set were reported for comparison."
Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning,3.2,Implementation Details,"The WSI representation pre-training stage uses all training data and does not involve any supervised information. During the downstream classification task,  the pre-trained encoder is utilized as the slide representation extractor, and the [CLS ] token is fed into the following classifier consisting of a multilayer perceptron (MLP) and a fully connected layer. Following the protocol in selfsupervised learning [7], we evaluated the quality of pre-training with the two approaches: 1) Fine-tuning is to train the whole network parameters, including WSI encoder and classifier; 2) Linear probing is to freeze the encoder and only train the classifier. The usage of [CLS ] token refers to the MAE [7] framework, which was concatenated with patch tokens. During pre-training, the [CLS ] token is not involved in loss computation, but it continuously interacts with kernels and receives global information. After pre-training, the pre-trained parameters of the [CLS ] token will be loaded for fine-tuning and linear probing.To ensure the uniformity of patch features, we choose DINO [3] to extract patch features on the magnification under 20× lenses. Accuracy (ACC) and area under the ROC curve (AUC) are employed as evaluation metrics. We implemented all the models in Python 3.8 with PyTorch 1.7 and Cuda 10.2 and run the experiments on a computer with 4 GPUs of Nvidia Geforce 2080Ti."
Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning,3.3,Effectiveness of the WSI Representation Learning,"We first conducted experiments on the Endometrial dataset to verify the effectiveness of self-supervised learning for WSI analysis under label-limited conditions. The results are shown in Fig. 2, where the performance obtained with different ratios of labeled training WSIs are compared. MAE [7] based on the patch features is implemented as the baseline. Furthermore, we applied the proposed distance and polar angle embedding to the self-attention module of MAE [7], which is referred to as MAE+ in Fig. 2. Overall, PAMA consistently achieves significantly better performance across all the label ratios than MAE [7] and HIPT [5]. These results have demonstrated the effectiveness of PAMA in WSI representation pre-training. Moreover, PAMA achieves the best stability in AUCs and ACCs when the label ratios are reduced from 85% to 10%. This is of practical importance as it reduces the dependence on a large number of labeled WSIs for training robust WSI analysis models. Meanwhile, it means that we can utilize the unlabeled WSIs to improve the capacity of the models with the help of PAMA. HIPT [5] is a two-stage self-learning framework, which first leverages DINO [3] to pre-train patches (256 × 256) divided from regions (4096 × 4096) and then utilizes DINO-4k [5] to pre-train regions of WSIs. The multi-stage framework accumulated the training bias and noise, which caused an AUC gap of HIPT [5] to MAE [7] and PAMA, especially trained with only 10% labeled WSIs. We also observed a significant improvement when comparing MAE+ with MAE [7]. It indicates the proposed distance and polar angle embedding strategy is more appreciated than the positional embedding of ViT [6] to describe the structure of histopathological WSIs. Please refer to the supplementary materials for more detailed results."
Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning,3.4,Ablation Study,"Then, we conducted ablation experiments to verify the necessity of the proposed structural embedding strategy. The detailed results are shown in Table 1, where all the models were fine-tuned with 35% training WSIs. It shows that the AUC decreases by 0.019 and 0.021, respectively, when the distance or polar angle embedding is discarded. And, when removing both the distance and polar angle embedding, the AUC drops by 0.034. These results demonstrate that local and global spatial information is crucial for PAMA to learn WSI representations."
Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning,3.5,Comparison with SOTA Methods,"Finally, we additionally compared the proposed PAMA with four weaklysupervised methods, DSMIL [12], TransMIL [13], SETMIL [18] and KAT [19]. The results are shown in Table 2. Overall, PAMA consistently achieves the best performance. In comparison with the second-best methods, PAMA achieves an increase of 0.015/0.011 and 0.025/0.009 in AUCs on TCGA and Endometrial datasets, respectively, by using 35%/100% labeled WSIs. Moreover, PAMA reveals the most robust capacity when reducing the training data from 100% to 35%, with AUC decreasing slightly from 0.988 to 0.982 and from 0.851 to 0.829 on the two datasets. TransMIL [13], SETMIL [18] and KAT [19] are state-ofthe-art methods for histopathological image classification. They all considered the spatial adjacency of patches but neglected the orientation relationships of the patches. It is the main reason that the three methods cannot surpass our method even with 100% training WSIs."
Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning,4,Conclusion,"In this paper, we proposed an effective self-supervised representation learning framework for WSI analysis. The experiments on two large-scale datasets have demonstrated the effectiveness of PAMA in the condition of limited-label. The results have shown superiority to the existing weakly-supervised and selfsupervised MIL methods. Future work will focus on training the WSI representation model based on datasets across multiple organs, thus promoting the generalization ability of the model for different downstream tasks."
Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning,,Fig. 1 .,
Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning,,Fig. 2 .,
Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning,,Table 1 .,
Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning,,Table 2 .,
Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 69.
Detection-Free Pipeline for Cervical Cancer Screening of Whole Slide Images,1,Introduction,"Cervical cancer is a common and severe disease that affects millions of women globally, particularly in developing countries [9]. Early diagnosis is vital for successful treatment, which can significantly increase the cure rate [17]. In recent years, computer-aided diagnosis (CAD) methods have become an important tool in the fight against cervical cancer, as they aim to improve the accuracy and efficiency of diagnosis.Several computer-aided cervical cancer screening methods have been proposed for whole slide images (WSIs) in the literature. Most of them are detectionbased methods, which typically contain a detection model as well as some postprocessing modules in their frameworks. For instance, Zhou et al. [29] proposed a three-step framework for cervical thin-prep cytologic test (TCT) [12]. The first step involves training a RetinaNet [13] as a cell detection network to localize suspiciously abnormal cervical cells from WSIs. In the second step, the patches centered on these detected cells are processed through a classification model, to refine the judgment of whether they are positive or negative. Finally, the positive patches refined by the patch-level classification are further combined to produce an overall positive/negative diagnosis for the WSI at the sample level.Some methods improve the final classification performance by improving the detection model to identify positive cells more reliably. Cao et al. [1] improved the detection performance by incorporating clinical knowledge and attention mechanism into their cell detection model of AttFPN. Wei et al. [24] adopted the Yolo [20] architecture with a variety of convolution kernels of different sizes to accommodate diverse cell clusters. Other methods improve the classification performance by changing the post-processing modules behind the detection model. Cheng et al. [5] proposed a progressive identification method that leveraged multi-scale visual cues to identify abnormal cells and then an RNN [27] for sample-level classification. Zhang et al. [28] used GAT [23] to model the relation of the suspicious positive cells provided by detection, thus obtaining a global description of the WSI and performing sample-level classification.These methods have achieved good results through continuous improvement on the detection-based pipeline, but there are some common drawbacks. First, they are not able to get rid of their reliance on detection models, which means they have a high need for expensive detection data labeling to train the detection model. Cervical cancer cell detection datasets involve labeling individual and small bounding boxes in a large number of cells. It often requires multiple experienced pathologists to annotate [15], which is very time-consuming and labor-intensive. Second, the widely used detection-based pipeline has not fully utilized the massive information in WSIs. A WSI is typically large (sized of about 20000 × 20000 pixels). A lot of data would be wasted if only a small part of annotated images (e.g., corresponding to positive cells and bounding boxes) was used as training data. Finally, many existing methods focus on detecting and classifying individual cells. The tendency to neglect effective integration of the overall information across the entire WSI results in poor performance in sample-level classification.To address the aforementioned issues, we propose a detection-free pipeline in this paper, which does not rely on any detection model. Instead, our pipeline requires only sample-level diagnosis labels, which are naturally available in clinical scenarios and thus get rid of additional image labeling. To attain this goal, we have designed a two-stage pipeline as in Fig. 1. In the coarse-grained stage, we crop and downsample a WSI into multiple images, and conduct sample-level classification roughly based on all resized images. The coarse-grained classification yields attention scores, from which we perform attention guided selection to localize these key patches from the original WSI. Then, in the fine-grained stage, we use these key patches for fine prediction of the sample. The two stages in our pipeline adopt the same network design (i.e., encoder + pooling trans-former), which makes our solution friendly to develop and to use. We also adopt contrastive learning to effectively utilize the massive information in WSIs when training the encoder for classification. As a summary, our pipeline surpasses previous detection-based methods and achieves state-of-the-art performance with large-scale training. Our experiments show that our method becomes more effective when increasing the data size for training. Moreover, while many pathological images are also based on WSIs, our pipeline has a high potential to extend to other pathological tasks."
Detection-Free Pipeline for Cervical Cancer Screening of Whole Slide Images,2,Methodology,"Fig. 1. The overview of our proposed method. For feasibility of computation, we crop a WSI into mutiple images. The cropped images are passed through the coarse-grained and fine-grained stages, where only sample-level diagnosis labels of WSIs, instead of any additional manual labeling, are required for training."
Detection-Free Pipeline for Cervical Cancer Screening of Whole Slide Images,,Two-Stage Pipeline with Attention Guided Selection.,"The overview of our two-stage pipeline is shown in Fig. 1. The input WSI is typically too big to be directly processed by a common deep learning pipeline [18], so we crop each WSI into local images sized 1024 × 1024 pixels. The images are then processed through the coarse-grained and fine-grained stages in order to obtain the WSIlevel classification results, respectively. In general, the purpose of the coarsegrained stage is to replace the detection model and identify local images that may contain abnormal positive cells. The fine-grained stage then integrates these key regions, producing refined classification for the sample.To complete sample-level classification, both stages share basically the same network architecture. The input images are first processed by a CNN encoder to extract features. Then, we propose the pooling transformer, which is modified from the basic transformer module in Sect. 2, to integrate these features for WSI classification. Additionally, the input images for both stages are 256 × 256. In the coarse-grained stage, in order to allow the model to examine as many local images as possible, we resize the cropped local images from 1024 × 1024 to 256 × 256. In the fine-grained stage, we enlarge suspicious local abnormality and thus crop input images to 256 × 256 from 1024 × 1024.For the coarse-grained stage, after passing the resized local images through encoder and pooling transformer, we obtain a rough prediction result at the sample level. We then use the Cross-Entropy (CE) loss to minimize the difference between the predicted WSI label and the ground truth. In addition, we calculate the attention score to identify the local image inputs that are most likely to yield positive reading. We describe the attention score aswhere x 0 represents classification token (which is a commonly used setting in transformer [7,22]), and d x0 is 512 in our implementation, f represents the feature vector of a certain input local image. After calculating attention scores, we preserve top-8 (resized) local images with the highest scores from the entire WSI for subsequent fine-grained classification.Next, in the fine-grained stage, each local image that has passed attention guided selection is cropped into 16 patches of the size 256 × 256. We expect that those patches contain positive cells and are thus critical to diagnosis at the sample level. The network of the fine-grained stage is the same as that of the coarse-grained stage, but the weights of the encoder is pre-trained in an unsupervised manner (Sect. 2). The same CE loss supervised by sample-level ground truth is used for the fine-grained stage here. For inference, the output of the fine-grained stage will be treated as the final result of the test WSI. Pooling Transformer. We use a transformer network to aggregate features of multiple inputs and to derive the sample-level outcome in both coarse-grained and fine-grained stages. We have observed that different local images of the same sample often have patterns of grouped similarity (such as the first two images in the upper-right of Fig. 1). For negative samples, most of the local images are similar with each other. For positive samples, the images of abnormal cells are inclined to be grouped into several clusters.Therefore, inspired by [2,14], we propose pooling transformer that is effective to reduce the redundancy and distortion from the input images. The pooling transformer in Fig. 2 is designed to integrate all inputs toward the samplelevel diagnosis. To remove redundant features, between two transformer layers, we use the affinity propagation algorithm [8] to cluster the inputs into several classes. Within each clustered class, we average the features and aggregate a single token. Finally, the classification (CLS) token is concatenated with all tokens after clustering-based pooling, and passed through the rest of the network to obtain the classification result. In this way, we find that the similar yet redundant input features can be fused, making the network more concise and efficient to calculate the attention between pooled features."
Detection-Free Pipeline for Cervical Cancer Screening of Whole Slide Images,,Contrastive Pre-training of Encoder.,"To make full use of WSI data and provide a better feature encoder, inspired by MoCo [4,11] and other contrastive learning methods [25,26] pre-training on ImageNet [6], we also perform pretraining for fine-grained encoder on a large scale of pathology images. Generally, large-scale pre-training usually requires a massive dataset and a suitable loss function. For data, WSI naturally has the advantage of having a large amount of training data. A WSI (20000 × 20000) can be cropped into about 5000-6000 patches (256 × 256). Therefore, we only need 2,000-3,000 WSI samples to obtain a dataset that can even be compared to ImageNet in quantity. For the loss function, there are typically two ways: one is like MAE [10] to model the loss function using masks, and the other is to use contrastive learning as in MoCo and CLIP [19]. In our task, since the structural features of cells are relatively weak compared to natural images, it is not suitable to model the loss function using masks. Therefore, we adopt a contrastive learning approach.Specifically, in the same training batch, a patch (256 × 256, the same to the input size of the fine-grained stage) and its augmented patch are treated as a positive pair (note that here ""positive/negative"" is defined in the context of contrastive learning), and their features are required to be as similar as possible. Meanwhile, their features are required to be as dissimilar as possible from those of other patches. So the loss function can be described asf i and f ia represent the positive pair, and f j represents another patch negatively paired with f i . Using this method, we can pre-train a feature encoder in an unsupervised manner and initialize it into our encoder for the fine-grained stage."
Detection-Free Pipeline for Cervical Cancer Screening of Whole Slide Images,3,Experiment and Results,"Dataset and Experimental Setup. In this study, we have collected 5384 cervical cytopathological WSI by 20x lens, each with 20000 × 20000 pixels, from our collaborating hospitals. Among them, there 2853 negative samples, and 2531 positive samples (962 ASCUS, and 1569 high-level positive samples). All WSIs only have diagnosis labels at the sample level, without annotation boxes at the cell level. And all sample labels are strictly diagnosed according to the TBS [16] criterion by a pathologist with 35 years of clinical experience. We conduct the experiment with initial learning rate of 1.0 × 10 -4 , batch size of 4, and SGD optimizer [21] for 30 epochs each stage. For contrastive pre-training, we follow the settings of MoCov2 [3] and trained for 300 epochs.Comparison to SOTA Methods. In this section, we experiment to compare our method with popular state-of-the-art (SOTA) methods, which are all fully supervised and detection-based. To the best of our knowledge, there are few good methods to train cervical cancer classification models in weakly supervised or unsupervised learning ways. No methods can achieve the detection-free goal either.All the detection-based methods are evaluated in the following way. First, we label a dataset with cell-level bounding boxes to train a detection model. The detection dataset has 3761 images and 7623 cell-level annotations. After obtaining the suspicious cell patches provided by the detection model, we use the subsequent classification models used in these SOTA works to classify them and obtain the final classification results. As shown in Table 1 for fair five-fold cross-validation, our method outperforms all compared detection-based methods. While our method has a large margin with most methods in the table, the improvement against [28] (top-ranked in current detection-based methods) is relatively limited. On one hand, in [28], GAT aggregates local patches that are detected by Retinanet. And the attention mechanism of GAT is similar with the transformer used in our pipeline to certain extent. On the other hand, the result implies that our coarse-grained task has replaced the role of cell detection in early works. Thus, we conclude that a detection model trained with an expensive annotated dataset is not necessary to build a CAD pipeline for cervical abnormality.Ablation Study. In this section, we experiment to demonstrate the effectiveness of all the proposed parts in our pipeline. We divide all 5384 samples into five independent parts for five-fold cross-validation, and the results are shown in Table 2. Here, CG means the classification passes only the coarse-grained stage. As can be seen, its performance is low, in that the resized images sacrifices the resolution and thus perform poorly for image-based classification. FG refers to classifying in the fine-grained stage. It is worth noting that without the attention scores provided by the coarse-grained stage, we have no way of knowing which local images might contain suspicious positive cells. Thus, we use random selection to experiment for FG only, as exhaustively checking all local images is computationally forbidden. As can be seen, the classification result is the lowest because it lacks enough access to the key image content in WSIs. By combining the two stages for attention guided selection, it is effective to improve the classification performance compared to the two previous experiments. Here, for the cases of CG, FG and CG+FG, an original transformer network without clustering-based pooling is used. In addition, as shown in the last two rows of the table, both pooling transformer (PT) and unsupervised pretraining (CL) contribute to our pipeline. Ultimately, we combine them together to achieve the best performance."
Detection-Free Pipeline for Cervical Cancer Screening of Whole Slide Images,,Sample Numbers and Inference Time.,"In order to further demonstrate the huge potential of our method, we also perform an ablation study on the number of samples used for training and compare the time consuming of the different methods. For the experiment of sample numbers, We compare the best fully supervised detection-based method (Retinanet+GAT [28]) with ours under the sample numbers of 500, 1000, 2000, and 5384. As shown by Table 3 and  left of Fig. 3, the traditional detection-based method has quickly encountered a saturation bottleneck as the amount of data increases. Although our method initially has poorer performance, it has shown an impressive growth trend. And at our current maximum data number (5384), the proposed pipeline has already exceeded the performance of the detection-based method. The above results also demonstrate that our new pipeline method has greater potential, even though it requires no cell-level image annotation. For inference time consuming, as shown in right of Fig. 3, our method has shorter inference time and a good balance between accuracy and inference time."
Detection-Free Pipeline for Cervical Cancer Screening of Whole Slide Images,4,Conclusion and Discussion,"In this paper, we propose a novel two-stage detection-free pipeline for WSI classification of cervical abnormality. Our method does not rely on detection models and eliminates the need for expensive cell-level data annotation. By leveraging just sample-level diagnosis labels, we achieve results that are competitive with fully supervised detection-based methods. Through the use of the proposed pooling transformer and unsupervised pre-training, our method makes full use of information within WSIs, resulting in improved efficiency in the use of pathological images. Importantly, our method offers even greater advantages with increasing amounts of data. And also, by utilizing attention weights, we can calculate attention scores to visually represent the importance of each image in the sample, making it easier for doctors to make judgments. Relevant visualization results can be found on our project homepage. Admittedly, our method has some limitations, such as slow training. Accelerating the training of massive data can be our next optimization direction."
Detection-Free Pipeline for Cervical Cancer Screening of Whole Slide Images,,Fig. 2 .,
Detection-Free Pipeline for Cervical Cancer Screening of Whole Slide Images,,Fig. 3 .,
Detection-Free Pipeline for Cervical Cancer Screening of Whole Slide Images,,,
Detection-Free Pipeline for Cervical Cancer Screening of Whole Slide Images,,Table 1 .,
Detection-Free Pipeline for Cervical Cancer Screening of Whole Slide Images,,Table 2 .,
Detection-Free Pipeline for Cervical Cancer Screening of Whole Slide Images,,Table 3 .,
Democratizing Pathological Image Segmentation with Lay Annotators via Molecular-Empowered Learning,1,Introduction,"Multi-class cell segmentation is an essential technique for analyzing tissue samples in digital pathology. Accurate cell quantification assists pathologists in identifying and diagnosing diseases [5,29] as well as obtaining detailed information about the progression of the disease [23], its severity [28], and the effectiveness of treatment [15]. For example, the distribution and density of podocyte and mesangial cells in the glomerulus offer a faint signal of functional injury in renal pathology [14]. The cell-level characterization is challenging for experienced pathologists due to the decades of expensive medical training, long annotation time, large variability [30], and low accuracy, while it is impractical to hire massive experienced pathologists for cell annotation.Previous works proposed several computer vision tools to perform automated or semi-automated cell segmentation on pathological images [17], including Annota-torJ [12], NuClick [16], QuPath [2], etc. Such software is able to mark nuclei, cells, and multi-cellular structures by compiling pre-trained segmentation models [11], color deconvolution [25], or statistical analysis [22]. However, those automatic approaches still heavily rely on the morphology of cells from pathological Periodic acid-Schiff (PAS) images, thus demanding intensive human intervention for extra supervision and correction. Recently, immunofluorescence (IF) staining imaging has been widely used to visualize multiple biomolecules simultaneously in a single sample using fluorescently labeled antibodies [6,20]. Such technology can accurately serve as a guide to studying the heterogeneity of cellular populations, providing reliable information for cell annotation. Furthermore, crowd-sourcing technologies [1,13,19] were introduced generate better annotation for AI learning from multiple annotations.In this paper, we proposed a holistic molecular-empowered learning scheme that democratizes AI pathological image segmentation by employing only lay annotators (Fig. 1). The learning pipeline consists of (1) morphology-molecular multi-modality image registration, (2) molecular-informed layman annotation, and (3) molecularoriented corrective learning. The pipeline alleviates the difficulties at the R&D from the expert level (e.g., experienced pathologists) while relegating annotation to the lay annotator level (e.g., non-expert undergraduate students), all while enhancing both the accuracy and efficiency of the cell-level annotations. An efficient semi-supervised learning strategy is proposed to offset the impact of noisy label learning on lay annotations. The contribution of this paper is three-fold:• We propose a molecular-empowered learning scheme for multi-class cell segmentation using partial labels from lay annotators; • The molecular-empowered learning scheme integrates (1) Giga-pixel level molecular-morphology cross-modality registration, (2) molecular-informed annotation, and (3) molecular-oriented segmentation model to achieve statistically a significantly superior performance via lay annotators as compared with experienced pathologists; • A deep corrective learning method is proposed to further maximize the cell segmentation accuracy using partially annotated noisy annotation from lay annotators."
Democratizing Pathological Image Segmentation with Lay Annotators via Molecular-Empowered Learning,2,Methods,"The overall pipeline of the entire labeling and auto-quantification pipeline is presented in Fig. 2. Molecular images are aligned with anatomical images in order to provide accurate guidance for cell labeling by using multi-scale registration. After this registration, a functional unit segmentation model is implemented to localize the regions of glomeruli. Within those glomeruli, lay annotators label multiple cell types by using the pair-wise molecular images and anatomical images in ImageJ [12]. A partial-label learning model with a molecular-oriented corrective learning strategy is employed so as to diminish the gap between labels from lay annotators and gold standard labels."
Democratizing Pathological Image Segmentation with Lay Annotators via Molecular-Empowered Learning,2.1,Morphology-Molecular Multi-modality Registration,"Multi-modality, multi-scale registration is deployed to ensure the pixel-to-pixel correspondence (alignment) between molecular IF and PAS images at both the WSI and regional levels. To maintain the morphological characteristics of the functional unit structure, a slide-wise multi-modality registration pipeline (Map3D) [8] is employed to register the molecular images to anatomical images. The first stage is global alignment. The Map3D approach was employed to achieve reliable translation on WSIs when encountering missing tissues and staining variations. The output of this stage is a pairwise affine matrix M Map3D (t) from Eq. ( 1).To achieve a more precise pixel-level correspondence, Autograd Image Registration Laboratory (AIRLab) [27] was utilized to calibrate the registration performance at the second stage. The output of this step is M AIRLab (t) from Eq. (2). where i is the index of pixel x i in the image I, with N pixels. The two-stage registration (Map3D + AIRLab) affine matrix for each pair is presented in Eq. ( 3).In Eq. ( 1) and ( 2), A indicates the affine registration. The affine matrix M Map3D (t) from Map3D is applied to obtain pair-wise image regions. The ||.|| Af fMap3D and ||.|| Af f AIRLab in Eq. ( 1) and ( 2) indicates the different similarity metrics for two affine registrations, respectively."
Democratizing Pathological Image Segmentation with Lay Annotators via Molecular-Empowered Learning,2.2,Molecular-Informed Annotation,"After aligning molecular images with PAS images, an automatic multi-class functional units segmentation pipeline Omni-Seg [7] is deployed to locate the tuft unit on the images. With the tuft masks, the molecular images then manifest heterogeneous cells with different color signals on pathological images during the molecular-informed annotation. Each anatomical image attains a binary mask for each cell type, in the form of a partial label. Following the same process, the pathologist examines both anatomical images and molecular images to generate a gold standard for this study (Fig. 1)."
Democratizing Pathological Image Segmentation with Lay Annotators via Molecular-Empowered Learning,2.3,Molecular-Oriented Corrective Learning for Partial Label Segmentation,"The lack of molecular expertise as well as the variability in the quality of staining in molecular images can cause annotations provided by non-specialists to be unreliable and error-prone. Therefore, we propose a corrective learning strategy (in Fig. 3) to efficiently train the model with noise labels, so as to achieve the comparable performance of training the same model using the gold standard annotations.Inspired by confidence learning [21] and similarity attention [18], top-k pixel feature embeddings at the annotation regions with higher confidences from the prediction probability (W , defined as confidence score in Eq. ( 4)) are selected as critical representations for the current cell type from the decoder(in Eq. ( 5)).where k denotes the number of selected embedding features. E is the embedding map from the last layer of the decoder, while Y is the lay annotation.We then implement a cosine similarity score S between the embedding from an arbitrary pixel to those from critical embedding features as Eq. (6)."
Democratizing Pathological Image Segmentation with Lay Annotators via Molecular-Empowered Learning,,"S(e i , e top","where m denotes the channel of the feature embeddings.Since the labels from lay annotators might be noisy and erroneous, the W and S are applied in following Eq. ( 7) to highlight the regions where both the model and lay annotation agree on the current cell type, when calculating the loss function in Eq. (8)."
Democratizing Pathological Image Segmentation with Lay Annotators via Molecular-Empowered Learning,3,Data and Experiments,"Data. 11 PAS staining WSIs, including 3 injured glomerulus slides, were collected with pair-wise IF images for the process. The stained tissues were scanned at a 20× magnification. After multi-modality multi-scale registration, 1,147 patches for podocyte cells, and 789 patches for mesangial cells were generated and annotated. Each patch has 512×512 pixels."
Democratizing Pathological Image Segmentation with Lay Annotators via Molecular-Empowered Learning,,Morphology-Molecular Multi-modality Registration.,"The slide-level global translation from Map3D was deployed at a 5× magnification, which is 2 µm per pixel. The 4096×4096 pixels PAS image regions with 1024 pixels overlapping were tiled on anatomical WSIs at a 20× magnification, which is 0.5 µm per pixel.Molecular-Empowered Annotation. The automatic tuft segmentation and molecular knowledge images assisted the lay annotators with identifying glomeruli and cells. ImageJ (version v1.53t) was used throughout the entire annotation process. ""Synchronize Windows"" was used to display cursors across the modalities with spatial correlations for annotation. ""ROI Manager"" was used to store all of the cell binary masks for each cell type.Molecular-Oriented Corrective Learning. Patches were randomly split into training, validation, and testing sets -with a ratio of 6:1:3, respectively -at the WSI level. The distribution of injured glomeruli and normal glomeruli were balanced in the split.Experimental Setting. 2 experienced pathologists and 3 lay annotators without any specialized knowledge were included in the experiment. All anatomical and molecular patches of glomerular structures are extracted from WSI on a workstation equipped with a 12-core Intel Xeon W-2265 Processor, and NVIDIA RTXA6000 GPU. An 8-core AMD Ryzen 7 5800X Processor workstation with XP-PEN Artist 15.6 Pro Wacom is used for drawing the contour of each cell. Annotating 1 cell type on 1 WSI requires 9 h, while staining and scanning 24 IF WSIs (as a batch) requires 3 h. The experimental setup for the 2 experts and the 3 lay annotators is kept strictly the same to ensure a fair comparison.Evaluation Metrics. 100 patches from the testing set with a balanced number of injuries and normal glomeruli were captured by the pathologists for evaluating morphology-based annotation and molecuar-informed annotation. The annotation from one pathologist (over 20 years' experience) with both anatomical and molecular images as gold standard (Fig. 1). The balanced F-score (F1) was used as the major metric for this study. The Fleiss' kappa was used to compute the inter-rater variability between experts and lay annotators."
Democratizing Pathological Image Segmentation with Lay Annotators via Molecular-Empowered Learning,4,Results,"Figure 4, Fig. 5 and Table 1 indicate the annotation performance from the naked human eye with expert knowledge and the lay annotator with molecular-informed learning. As  shown, our learning method achieved better annotation with higher F1 scores with fewer false positive and false negative regions as compared with the pathologist's annotations. Statistically, the Fleiss' kappa test shows that the molecular-informed annotation by lay-annotators has higher annotation agreements than the morphology-based annotation by experts. This demonstrates the benefits of reducing the expertise requirement to a layman's level and improving accuracy in pathological cell annotation."
Democratizing Pathological Image Segmentation with Lay Annotators via Molecular-Empowered Learning,4.1,Performance on Multi-class Cell Segmentation,"In Table 2, we compared the proposed partial label segmentation method to baseline models, including (1) multiple individual models (U-Nets [24], DeepLabv3s [4], and Residual U-Nets [26]), (2) multi-head models (Multi-class [10], Multi-Kidney [3]), and (3) single dynamic networks with noisy label learning (Omni-Seg [7]). Our results found that the partial label paradigm shows superior performance on multi-class cell segmentation. The proposed model particularly demonstrates better quantification in the normal glomeruli, which contain large amounts of cells.To evaluate the performance of molecular-oriented corrective learning on imperfect lay annotation, we also implemented two noisy label learning strategies Confidence Learning (CL) [21] and Partial Label Loss (PLL) [18] with the proposed Molecular-oriented corrective learning (MOCL) on our proposed partial label model. As a result, the proposed molecular-oriented corrective learning alleviated the error between lay annotation and the gold standard in the learning stage, especially in the injured glomeruli that incorporate more blunders in the annotation due to the identification difficulty from morphology changing."
Democratizing Pathological Image Segmentation with Lay Annotators via Molecular-Empowered Learning,4.2,Ablation Study,"The purpose of corrective learning is to alleviate the noise and distillate the correct information, so as to improve the model performance using lay annotation. Four designs of corrective learning with different utilization of similarity losses and confidence losses were evaluated with lay annotation in Table 3. Each score is used in either an exponential function or a linear function (Eq. ( 7)), when multiplying and calculating the loss function (Eq. ( 8)). The bold configuration was selected as the final design.  "
Democratizing Pathological Image Segmentation with Lay Annotators via Molecular-Empowered Learning,5,Conclusion,"In this work, we proposed a holistic, molecular-empowered learning solution to alleviate the difficulties of developing a multi-class cell segmentation deep learning model from the expert level to the lay annotator level, enhancing the accuracy and efficiency of cell-level annotation. An efficient corrective learning strategy is proposed to offset the impact of noisy label learning from lay annotation. The results demonstrate the feasibility of democratizing the deployment of a pathology AI model while only relying on lay annotators."
Democratizing Pathological Image Segmentation with Lay Annotators via Molecular-Empowered Learning,,Fig. 1 .,
Democratizing Pathological Image Segmentation with Lay Annotators via Molecular-Empowered Learning,,Fig. 2 .,
Democratizing Pathological Image Segmentation with Lay Annotators via Molecular-Empowered Learning,,Fig. 3 .,
Democratizing Pathological Image Segmentation with Lay Annotators via Molecular-Empowered Learning,,Fig. 4 .,
Democratizing Pathological Image Segmentation with Lay Annotators via Molecular-Empowered Learning,,Fig. 5 .,
Democratizing Pathological Image Segmentation with Lay Annotators via Molecular-Empowered Learning,,Table 1 .,accuracy from only anatomical morphology and molecular-informed annotation. Average F1 scores and Fleiss' kappa between 2 experts and 3 lay annotators are reported.
Democratizing Pathological Image Segmentation with Lay Annotators via Molecular-Empowered Learning,,Table 2 .,*
Democratizing Pathological Image Segmentation with Lay Annotators via Molecular-Empowered Learning,,Table 3 .,
Diffusion-Based Hierarchical Multi-label Object Detection to Analyze Panoramic Dental X-Rays,1,Introduction,"The use of panoramic X-rays to diagnose numerous dental diseases has increased exponentially due to the demand for precise treatment planning [11]. However, visual interpretation of panoramic X-rays may consume a significant amount of essential clinical time [2] and interpreters may not always have dedicated training in reading scans as specialized radiologists have [13]. Thus, the diagnostic process can be automatized and enhanced by getting the help of Machine Learning (ML) models. For instance, an ML model that automatically detects abnormal teeth with dental enumeration and associated diagnoses would provide a tremendous advantage for dentists in making decisions quickly and saving their time. Many ML models to interpret panoramic X-rays have been developed specifically for individual tasks such as quadrant segmentation [19,29], tooth detection [6], dental enumeration [14,23], diagnosis of some abnormalities [12,30], as well as treatment planning [27]. Although many of these studies have achieved good results, three main issues still remain. (1) Multi-label detection: there has not been an end-to-end model developed that gives all the necessary information for treatment planning by detecting abnormal teeth with dental enumeration and multiple diagnoses simultaneously [1]. (2) Data availability: to train a model that performs this task with high accuracy, a large set of fully annotated data is needed [13]. Because labeling every tooth with all required classes may require expertise and take a long time, such kind of fully labeled large datasets do not always exist [24]. For instance, we structure three different available annotated data hierarchically shown in Fig. 1, using the Fédération Dentaire Internationale (FDI) system. The first data is partially labeled because it only included quadrant information. The second data is also partially labeled but contains additional enumeration information along with the quadrant. The third data is fully labeled because it includes all quadrant-enumeration-diagnosis information for each abnormal tooth. Thus, conventional object detection algorithms would not be well applicable to this kind of hierarchically and partially annotated data [21]. (3) Model performance: to the best of our knowledge, models designed to detect multiple diagnoses on panoramic X-rays have not achieved the same high level of accuracy as those specifically designed for individual tasks, such as tooth detection, dental enumeration, or detecting single abnormalities [18].To circumvent the limitations of the existing methods, we propose a novel diffusion-based hierarchical multi-label object detection method to point out each abnormal tooth with dental enumeration and associated diagnosis concurrently on panoramic X-rays, see Fig. 2. Due to the partial annotated and hierarchical characteristics of our data, we adapt a diffusion-based method [5] that formulates object detection as a denoising diffusion process from noisy boxes to object boxes. Compared to the previous object detection methods that utilize conventional weight transfer [3] or cropping strategies [22] for hierarchical learning, the denoising process enables us to propose a novel hierarchical diffusion network by utilizing the inference from the previously trained model in hierarchical order to manipulate the noisy bounding boxes as in Fig. 2. Besides, instead of pseudo labeling techniques [28] for partially annotated data, we develop a multi-label object detection method to learn efficiently from partial annotations and to give all the needed information about each abnormal tooth for treatment planning. Finally, we demonstrate the effectiveness of our multi-label detection method on partially annotated data and the efficacy of our proposed bounding box manipulation technique in diffusion networks for hierarchical data.The contributions of our work are three-fold. (1) We propose a multi-label detector to learn efficiently from partial annotations and to detect the abnormal tooth with all three necessary classes, as shown in Fig 3 for treatment planning. (2) We rely on the denoising process of diffusion models [5] and frame the detection problem as a hierarchical learning task by proposing a novel bounding box manipulation technique that outperforms conventional weight transfer as shown in Fig. 4. (3) Experimental results show that our model with bounding box manipulation and multi-label detection significantly outperforms state-of-the-art object detection methods on panoramic X-ray analysis, as shown in Table 1."
Diffusion-Based Hierarchical Multi-label Object Detection to Analyze Panoramic Dental X-Rays,2,Methods,"Figure 2 illustrates our proposed framework. We utilize the DiffusionDet [5] model, which formulates object detection as a denoising diffusion process from noisy boxes to object boxes. Unlike other state-of-the-art detection models, the denoising property of the model enables us to propose a novel manipulation technique to utilize a hierarchical learning architecture by using previously inferred boxes. Besides, to learn efficiently from partial annotations, we design a multilabel detector with adaptable classification layers based on available labels. In addition, we designed our approach to serve as a foundational baseline for the Dental Enumeration and Diagnosis on Panoramic X-rays Challenge (DENTEX), set to take place at MICCAI 2023. Remarkably, the data and annotations we utilized for our method mirror exactly those employed for DENTEX [9]."
Diffusion-Based Hierarchical Multi-label Object Detection to Analyze Panoramic Dental X-Rays,2.1,Base Model,"Our method employs the DiffusionDet [5] that comprises two essential components, an image encoder that extracts high-level features from the raw image and a detection decoder that refines the box predictions from the noisy boxes using those features. The set of initial noisy bounding boxes is defined as:where z 0 represents the input bounding box b, and b ∈ R N ×4 is a set of bounding boxes, z t represents the latent noisy boxes, and ᾱt represents the noise variance schedule. The DiffusionDet model [5] f θ (z t , t, x), is trained to predict the final bounding boxes defined as b i = (c i x , c i y , w i , h i ) where (c i x , c i y ) are the center coordinates of the bounding box and (w i , h i ) are the width and height of the bounding boxes and category labels defined as y i for objects."
Diffusion-Based Hierarchical Multi-label Object Detection to Analyze Panoramic Dental X-Rays,2.2,Proposed Framework,"To improve computational efficiency during the denoising process, Diffusion-Det [5] is divided into two parts: an image encoder and a detection decoder. Iterative denoising is applied only for the detection decoder, using the outputs of the image encoder as a condition. Our method employs this approach with several adjustments, including multi-label detection and bounding box manipulation. Finally, we utilize conventional transfer learning for comparison.Image Encoder. Our method utilizes a Swin-transformer [17] backbone pretrained on the ImageNet-22k [7] with a Feature Pyramid Network (FPN) architecture [15] as it was shown to outperform convolutional neural network-based models such as ResNet50 [10]. We also apply pre-training to the image encoder using our unlabeled data, as it is not trained during the training process. We utilize SimMIM [26] that uses masked image modeling to finetune the encoder. Detection Decoder. Our method employs a detection decoder that inputs noisy initial boxes to extract Region of Interest (RoI) features from the encodergenerated feature map and predicts box coordinates and classifications using a detection head. However, our detection decoder has several differences from DiffusionDet [5]. Our proposed detection decoder (1) has three classification heads instead of one, which allows us to train the same model with partially annotated data by freezing the heads according to the unlabeled classes, (2) employs manipulated bounding boxes to extract RoI features, and (3) leverages transfer learning from previous training steps.Multi-label Detection. We utilize three classification heads as quadrantenumeration-diagnosis for each bounding box and freeze the heads for the unlabeled classes, shown in Fig. 2. Our model denoted by f θ is trained to predict:where y i q , y i e , and y i d represent the bounding box classifications for quadrant, enumeration, and diagnosis, respectively, and h q , h e , and h d represent binary indicators of whether the labels are present in the training dataset. By adapting this approach, we leverage the full range of available information and improve our ability to handle partially labeled data. This stands in contrast to conventional object detection methods, which rely on a single classification head for each bounding box [25] and may not capture the full complexity of the underlying data. Besides, this approach enables the model to detect abnormal teeth with all three necessary classes for clinicians to plan the treatment, as seen in Fig. 3. Bounding Box Manipulation. Instead of completely noisy boxes, we use manipulated bounding boxes to extract RoI features from the encoder-generated feature map and to learn efficiently from hierarchical annotations as shown in Fig. 2. Specifically, to train the model (b) in Eq. ( 2), we concatenate the noisy boxes described in Eq. ( 1) with the boxes inferred from the model (a) in Eq. ( 2) with a score greater than 0.5. Similarly, we manipulate the denoising process during the training of the model (c) in Eq. ( 2) by concatenating the noisy boxes with boxes inferred from the model (b) in Eq. ( 2) with a score greater than 0. "
Diffusion-Based Hierarchical Multi-label Object Detection to Analyze Panoramic Dental X-Rays,3,Experiments and Results,"We evaluate models' performances using a combination of Average Recall (AR) and Average Precision (AP) scores with various Intersection over Union (IoU) thresholds. This included AP [0.5,0.95] , AP 50 , AP 75 , and separate AP scores for large objects (AP l ), and medium objects (AP m ).Data. All panoramic X-rays were acquired from patients above 12 years of age using the VistaPano S X-ray unit (Durr Dental, Germany). To ensure patient privacy and confidentiality, panoramic X-rays were randomly selected from the hospital's database without considering any personal information.To effectively utilize FDI system [8], three distinct types of data are organized hierarchically as in Fig. 1 (a) 693 X-rays labeled only for quadrant detection, (b) 634 X-rays labeled for tooth detection with both quadrant and tooth enumeration classifications, and (c) 1005 X-rays fully labeled for diseased tooth detection with quadrant, tooth enumeration, and diagnosis classifications. In the diagnosis, there are four specific classes corresponding to four different diagnoses: caries, deep caries, periapical lesions, and impacted teeth. The remaining 1571 unlabeled X-rays are used for pre-training. All necessary permissions were obtained from the ethics committee.Experimental Design. To evaluate our proposed method, we conduct two experiments: (1) Comparison with state-of-the-art object detection models, including DETR [4], Faster R-CNN [20], RetinaNet [16], and DiffusionDet [5] in Table 1. (2) A comprehensive ablation study to assess the effect of our modifications to DiffusionDet in hierarchical detection performance in Fig. 4.Evaluation. Fig. 3 presents the output prediction of the final trained model. As depicted in the figure, the model effectively assigns three distinct classes to each well-defined bounding box. Our approach that utilizes novel box manipulation and multi-label detection, significantly outperforms state-of-the-art methods. The box manipulation approach specifically leads to significantly higher AP and AR scores compared to other state-of-the-art methods, including RetinaNet, Faster-R-CNN, DETR, and DiffusionDet. Although the impact of conventional transfer learning on these scores can vary depending on the data, our bounding box manipulation outperforms it. Specifically, the bounding box manipulation approach is the sole factor that improves the accuracy of the model, while weight transfer does not improve the overall accuracy, as shown in Fig. 4.  [20] 0.588 29.5 48.6 33.0 39.9 29.5 DETR [4] 0.659 39.1 60.5 47.6 55.0 39.1 Base (DiffusionDet) [5] 0 Ablation Study. Our ablation study results, shown in Fig. 4 and Table 1, indicate that our approaches have a synergistic impact on the detection model's accuracy, with the highest increase seen through bounding box manipulation. We systematically remove every combination of bounding box manipulation and weight transfer, to demonstrate the efficacy of our methodology. Conventional transfer learning does not positively affect the models' performances compared to the bounding box manipulation, especially for enumeration and diagnosis. "
Diffusion-Based Hierarchical Multi-label Object Detection to Analyze Panoramic Dental X-Rays,4,Discussion and Conclusion,"In this paper, we introduce a novel diffusion-based multi-label object detection framework to overcome one of the significant obstacles to the clinical application of ML models for medical and dental diagnosis, which is the difficulty in getting a large volume of fully labeled data. Specifically, we propose a novel bounding box manipulation technique during the denoising process of the diffusion networks with the inference from the previously trained model to take advantage of hierarchical data. Moreover, we utilize a multi-label detector to learn efficiently from partial annotations and to assign all necessary classes to each box for treatment planning. Our framework outperforms state-of-the-art object detection models for training with hierarchical and partially annotated panoramic X-ray data.From the clinical perspective, we develop a novel framework that simultaneously points out abnormal teeth with dental enumeration and associated diagnosis on panoramic dental X-rays with the help of our novel diffusion-based hierarchical multi-label object detection method. With some limits due to partially annotated and limited amount of data, our model that provides three necessary classes for treatment planning has a wide range of applications in the real world, from being a clinical decision support system to being a guide for dentistry students."
Diffusion-Based Hierarchical Multi-label Object Detection to Analyze Panoramic Dental X-Rays,,Fig. 1 .,
Diffusion-Based Hierarchical Multi-label Object Detection to Analyze Panoramic Dental X-Rays,,Fig. 2 .,
Diffusion-Based Hierarchical Multi-label Object Detection to Analyze Panoramic Dental X-Rays,,Fig. 3 .,
Diffusion-Based Hierarchical Multi-label Object Detection to Analyze Panoramic Dental X-Rays,,5 .,
Diffusion-Based Hierarchical Multi-label Object Detection to Analyze Panoramic Dental X-Rays,,Fig. 4 .,
Diffusion-Based Hierarchical Multi-label Object Detection to Analyze Panoramic Dental X-Rays,,Table 1 .,
Diffusion-Based Hierarchical Multi-label Object Detection to Analyze Panoramic Dental X-Rays,,.4 61.3 47.9 49.7 39.5,
Diffusion-Based Hierarchical Multi-label Object Detection to Analyze Panoramic Dental X-Rays,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 38.
MulHiST: Multiple Histological Staining for Thick Biological Samples via Unsupervised Image-to-Image Translation,1,Introduction,"Histological staining is regarded as the standard protocol in clinical pathological examination, which is used to label biological structures and morphological changes in tissues [1]. The most frequently used histological staining is H&E stain for the inspection of cell nuclei and the extracellular matrix, in addition to some special stains to complement specific biomarkers and particular structures, such as MT stain used for connective tissues and PAS stain used for mucopolysaccharides [2]. However, multiple tissue sections are required if special stains are desired since the same section cannot be stained several times in conventional pathology workflow. In general, pathologists need to check the H&E-stained images firstly for a basic examination, and then decide whether to prepare additional sections and perform special stains, which will increase the time for diagnosis. More importantly, the abovementioned traditional histochemical staining techniques can only be performed on thin sections of 2-10 µm. Therefore, sample preparation steps, including paraffin embedding, tissue slicing, and chemical dewaxing, will result in long turnaround times and high laboratory infrastructure demands.The rapidly emerging field of digital virtual staining has shown great promise to revolutionize the decade-old staining workflow. Zhang et al. [3] have done pioneering works on multi-stain translation from unstained thin sections, and Yang et al. [4] also tried to achieve multiple stains generation from label-free tissue images. Both used an image registration to prepare pixel-level matched source-unstained and target-stained image pairs for supervised model training. However, obtaining such pixel-wise aligned data is not accessible for thick tissues as the traditional histological staining can only be performed on thin sections. Even though we can collect the surface cut from the thick specimen and then stain it with chemical reagents, there is still a huge morphological difference due to multiple-layer information captured by a slide-free microscope. Therefore, the virtual staining of thick tissues has to rely on unsupervised methods. There were some primary investigations on virtual staining of thick tissues that use slide-free imaging systems, such as MUSE [5], CHAMP [6], and UV-PAM [7]. However, those methods can only produce virtual H&E-stained images instead of multi-stained images. The emergency of starGAN opens new possibilities for multi-domain image translation [8], and they achieve flexible facial attribute transfer with the proposed domain label. [9,10] employ the idea of the domain label to represent different staining for multiple histological staining generations. However, those models use H&E staining as input and transfer H&E staining into other stains, which still require laborious tissue embedding and slide sectioning process. [11] focus on the unsupervised multiple virtual staining from autofluorescence images, yet the input should be images obtained from thin slides, which is not ideal for thick tissues.In this paper, we propose MulHiST, a novel Multiple Histological Staining model for Thick biological tissues, which is not feasible in the traditional histochemical staining workflow. To our knowledge, this is the first attempt to achieve multiple histological staining generations for thick tissues. 1 . Our key contributions are: (1) we propose MulHiST: a generative adversarial network (GAN)-based multi-domain image translation model capable of mapping a given light-sheet (LS) image of thick tissue into its histologically stained (HS) versions. We utilize unsupervised learning and do not require paired images, tailored for the virtual staining from slide-free imaging techniques; (2) we verify that the multi-domain translation can capture more reliable histopathological features for generating high-quality images, eliminating the ambiguity brought by multiple layers of information in thick tissue images; (3) both qualitative and quantitative results on H&E/PAS/MT staining generations show the superiority and efficiency of the proposed MulHiST over other baseline models."
MulHiST: Multiple Histological Staining for Thick Biological Samples via Unsupervised Image-to-Image Translation,2,Methodology,"To represent different staining types, we follow the idea of the domain-specific attribute vector proposed in [8], aiming to use a one-hot vector c to indicate unstained and various stained domains. During the training, the generator aims to transfer the image style of input image x to the desired domain c while keeping the content of x : G (x, c) → y, which means y has the same pathological context as x but with a different image style. When the training is finished, we use the well-trained generator to achieve multiple histological stain generations.In general, the image of thick tissues contains several-layer information, and information from different layers interferes with each other, which will lead to ambiguity in the determination of pathological features, e.g., cell boundary and tissue content. We believe that all the image domains, i.e., LS images and H&E/PAS/MT-stained images, share some domain-invariant features that can facilitate the model training. Moreover, our generator learns the mapping between every two domains. Then, a reconstruction loss can be employed in the single-generator model (orange dashed arrows in Fig. 1). We only need to input the LS image for the inference to get its corresponding histological stained versions (right part in Fig. 1).Unlike starGAN, we add the style code into the model with the Adaptive Instance Normalization (AdaIN) Layer [12] instead of concatenating the style code with the input image along the channel dimension. We only add the AdaIN to the decoder module, which will not affect the image feature extraction and encoding of the model input. The parameters of AdaIN are dynamically generated by a multilayer perceptron (MLP) based on the style code. The discriminator used in this work follows the one in [13], providing adversarial feedback to guide the learning of the generator. However, our discriminator will further classify images into different domains, not limited to real or fake signals.During the training, we incorporate multiple domain images and shuffle the ensembled dataset. A style code c can be generated randomly to indicate the target domain for the generator. The generator will transfer the input image into an image with the target style indicated by c, and the semantic content of the original input will not change. The model forward can be expressed by:where c is the style code that is randomly generated during the training. R is residual block. Enc and Dec are the encoder and decoder in the generator, respectively. The AdaIN layer in the decoder can be computed as:where x here is the feature map results of the previous layer, and s is the output of MLP. AdaIN aims to align the mean and variance of the input to match those of the desired style. The overall loss formulation of the generator is:where the L G adv is the adversarial loss and L rec is the image reconstruction loss. The c trg is the generated target style code and c org is the style code of the original input images."
MulHiST: Multiple Histological Staining for Thick Biological Samples via Unsupervised Image-to-Image Translation,3,Experimental Results,
MulHiST: Multiple Histological Staining for Thick Biological Samples via Unsupervised Image-to-Image Translation,3.1,Dataset and Implementation Details,"In this work, we prepared six thick tissue slabs and obtained scanned images of ∼15,000 × 15,000 pixels using an open-top LS microscope with an excitation wavelength of 266 nm. After imaging, the specimens were sectioned and histologically processed with the standard protocol to obtain HS images for the model training. We chose one set of scanned images as training data, and the others were used for the testing. For the training, we extracted small image patches with the size of 128 × 128 randomly. During testing, we divided the tested whole-slide image into 256 × 256 patches with 16 pixels overlap to avoid artifacts. There are 11,643 extracted image patches in the testing set.Our model was implemented in PyTorch on a single NVIDIA GeForce RTX 3090 GPU. We trained our model with the Adam optimizer (with β 1 = 0.5 and β 2 = 0.999) [14]. The initial learning rate was set to 1×10 -4 for both generator and discriminator with a linear decay scheduled after 50,000 iterations. The batch size was set to 16. The λ 1 in (3) was set to 5 and λ 2 was set to 10."
MulHiST: Multiple Histological Staining for Thick Biological Samples via Unsupervised Image-to-Image Translation,3.2,Evaluation Metrics,"We quantitatively evaluate our model and results with Kernel Inception Distance (KID) [15] and Fréchet Inception Distance (FID) [16], which are prevalent for evaluating the quality of digitally synthesized images. Unlike natural image generation, biomedical image generation not only require these indicators that reflect the image quality but also require some more convincing metrics with clinical values. As shown in Fig. 3 (5 th column, cycleGAN), the image style of generated results is similar to that of the real staining. However, the MT staining generated by cycleGAN is incorrect since the background and tissue content is reversed, which means that sometimes the model can produce target images with high fidelity, but it is difficult to keep correct semantic content or targeted biomarkers. It is not easy to identify pathological features from the LS image, therefore, we need a ground truth for a more reliable comparison. However, it is infeasible to obtain the well-matched ground truth of thick tissues.In this work, we observed that LS images of thick tissues share a similar style as fluorescence images of thin sections i.e., the cell nuclei are highlighted with positive contrast for both thin and thick tissues with the help of fluorescent labels. In this case, we used the model trained on LS images of thick tissues to test the fluorescence images of thin sections. Then, we could prepare the ground truth of thin sections for comprehensive comparison, as well as quantitative indicators, such as mean square error (MSE), structural similarity (SSIM), and peak signalto-noise ratio (PSNR). It is worth mentioning that when tested on the thinsection images, the model trained with thick-tissue images would underperform the model trained with thin-section images. Therefore, for virtual staining on thin slices, it is better to train on thin-section images as there are still some differences in details between data from thick tissues and thin sections. Here, we use the thin-section data only for model validation."
MulHiST: Multiple Histological Staining for Thick Biological Samples via Unsupervised Image-to-Image Translation,3.3,Quantitative and Qualitative Results,"In this paper, we select cycleGAN [17], MUNIT [18], and starGAN [8] as baseline models. As shown in Fig. 2, our model surpasses all comparison methods on visual results. From the virtually stained PAS (top zoomed-in regions, yellow square), we can distinguish two different convoluted tubules according to the PAS-positive/negative patterns (yellow/green arrows) in our results. In general, the glomerular and tubular basement membrane, as well as the brush border of the proximal tubules, can be visualized by the PAS staining with pink color, whereas the interior of the distal tubule will not be stained [19]. There is something pink inside the tubules pointed by yellow arrows in our results and no pink area inside the tubules indicated by green arrows. However, from other model results, it is hard to recognize corresponding histopathological features. Here we train cycleGAN multiple times for every pair of source/target domains so that those three cycleGAN models are independent. For such single-domain image translation, it is hard to satisfy all domains with a correct transformation. Specifically, we can see that the H&E-stained images of cycleGAN are correct, whereas the model reverses the background and cell nuclei in the MT domain. As shown in Fig. 2, neither MUNIT nor starGAN can perform well in this task.Moreover, we quantitatively evaluate the model performance with FID and KID scores, as shown in Table 1 (top part). We can observe that our model outperforms the other baseline models significantly in three different image domains. Although the H&E staining of cycleGAN also achieves good FID and KID scores, the corresponding PAS and MT staining results are much worse than ours, which also agrees with the qualitative analysis shown in Fig. 2.As no ground truth can be provided for the virtual staining of thick tissue. We collected 2 sets of thin mouse kidney sections for the model validation. We used the model trained with LS images of thick tissues to test the scanned images of prepared thin sections. In this situation, we can perform traditional histological staining to obtain ground truth for further comparison. Figure 3 confirms that our MulHiST can generate the correct pathological features, i.e., the PAS-positive proximal convoluted tubules indicated by yellow arrows and PAS-negative distal tubules pointed by green arrows, which are consistent with the ground truth. The same pathological representation can also be observed in the PAS result of cycleGAN, whereas the MT staining generated by cycleGAN presents an obvious error between the background and tissue content. Meanwhile, the quantitative evaluation in Table 1 (bottom part) also shows that our proposed model outperforms other baseline models in various evaluation metrics."
MulHiST: Multiple Histological Staining for Thick Biological Samples via Unsupervised Image-to-Image Translation,3.4,Ablation Analysis,"In this paper, we have two main hypotheses, one is that the model can benefit from data fusion from multiple domains due to the domain-invariant features, and the other is that the AdaIN-based style transfer is better in source image feature extraction compared with channel-wise style code concatenation.  We first verify the importance of domain-invariant features shared by multiple domains. The starGAN [8] also reached a similar conclusion that different domains will share the same facial context, which is beneficial to facial expression synthesis. Similarly, there are also some shared features in our multi-domain dataset, such as cell nuclei and cytoplasm membranes. We claim that the model training can benefit from incorporating multiple data domains. From Fig. 4, the synthetic image quality improves with the increase of input data domains. For single-domain translation, the model is sensitive to different domains, where only H&E staining is correct and the other two cannot be determined. This also agrees with the performance of cycleGAN in Fig. 2,3. The main reason is that the different models are built independently for single-domain image translation, resulting in the inability to share effective information among domains. When adding another domain, the model can correctly translate the background and tissue content. This can be attributed to the feature-sharing between different data domains. In addition, the triple-domain model is superior to the dualdomain ones, where all H&E, PAS, and MT show clear and natural structures.  Next, we compare the AdaIN-based method and channel-wise concatenation (Fig. 5). We can observe that both ways can achieve correct style transfer, but from the PAS staining, the AdaIN-based method can produce plausible pathological patterns that another model fails (yellow arrows). The PAS should be stained inside the tubules, and the results from the channel-wise concatenation method will cause ambiguity in the judgment by pathologists."
MulHiST: Multiple Histological Staining for Thick Biological Samples via Unsupervised Image-to-Image Translation,4,Conclusion,"This paper proposes a multiple histological image generation model for thick biological samples. This is undesirable in the traditional histopathology workflow as the chemical histological staining should be performed on the thin sections, and the same section cannot be stained with various stains simultaneously. We use slide-free microscopy to capture the thick tissues and translate the scanned images into multiple stained-versions. The model is optimized in an unsupervised manner, fitting the issue of large morphological mismatches between the scanned thick tissue and histologically stained images. Experiment results demonstrated the superiority and the great promise of the proposed method in developing a slide-free, cost-effective, and chemical-free histopathology staining pipeline."
MulHiST: Multiple Histological Staining for Thick Biological Samples via Unsupervised Image-to-Image Translation,,Fig. 1 .,
MulHiST: Multiple Histological Staining for Thick Biological Samples via Unsupervised Image-to-Image Translation,,Fig. 2 .,
MulHiST: Multiple Histological Staining for Thick Biological Samples via Unsupervised Image-to-Image Translation,,Fig. 3 .,
MulHiST: Multiple Histological Staining for Thick Biological Samples via Unsupervised Image-to-Image Translation,,Fig. 4 .,
MulHiST: Multiple Histological Staining for Thick Biological Samples via Unsupervised Image-to-Image Translation,,Fig. 5 .,
MulHiST: Multiple Histological Staining for Thick Biological Samples via Unsupervised Image-to-Image Translation,,Table 1 .,
MulHiST: Multiple Histological Staining for Thick Biological Samples via Unsupervised Image-to-Image Translation,,90 0.039 82.49 0.048 84.44 0.055 1217.36 0.6475 17.50,
Instance-Aware Diffusion Model for Gland Segmentation in Colon Histology Images,1,Introduction,"Colorectal cancer is a prevalent form of cancer characterized by colorectal adenocarcinoma, which develops in the colon or rectum's inner lining and exhibits glandular structures [5]. These glands play a critical role in protein and carbohydrate secretion across various organ systems. Histological examinations using Hematoxylin and Eosin staining are commonly conducted by pathologists to evaluate the differentiation of colorectal adenocarcinoma [15]. The extent of gland formation is a crucial factor in determining tumor grade and differentiation. Accurate segmentation of glandular instances on histological images is essential for evaluating glandular morphology and assessing colorectal adenocarcinoma malignancy. However, manual annotation of glandular instances is a time-consuming and expertise-demanding process. Hence, automated methods for glandular instance segmentation hold significant value in clinical practice. Automated segmentation has been explored using deep learning techniques [21,33], including U-Net [17], FCN [13], Siamese network [10,11] and their variations for semantic segmentation [31]. There are also methods that combine information bottleneck for detection and segmentation [23]. Additionally, twostage instance segmentation methods like Mask R-CNN [7] and BlendMask [3] have been utilized, combining object detection and segmentation sub-networks. However, these methods may face difficulties in capturing different cell shapes and distinguishing tightly positioned gland boundaries. Limitations arise from image scaling and cropping, leading to information loss or distortion, resulting in ineffective boundary recognition and over-/under-segmentation. To overcome these limitations, we aim to perform gland instance segmentation to accurately identify the target location and prevent misclassification of background tissue.Recently, diffusion model [9] has gained popularity as efficient generative models [16]. In the task of image synthesis, diffusion model has evolved to achieve state-of-the-art performance in terms of quality and mode coverage compared with GAN [32]. Furthermore, diffusion model has been applied to various other tasks [18]. DiffusionDet [4] treats the object detection task as a generative task on the bounding box space in images to handle projection detection. Several studies have explored the feasibility of using diffusion model in image segmentation [26]. These methods generate segmentation maps from noisy images and demonstrate better representation of segmentation details compared to previous deep learning methods.In this paper, we propose a new method for gland instance segmentation based on the diffusion model. (1) Our method utilizes a diffusion model to perform denoising and tackle the task of gland instance segmentation in histology images. The noise boxes are generated from Gaussian noise, and the predicted ground truth (GT) boxes and segmentation masks are performed during the diffusion process. (2) To improve segmentation, we use instance-aware techniques to recover lost details during denoising. This includes employing a filter and a multi-scale Mask Branch to create a global mask and refine finer segmentation details. (3) To enhance object-background differentiation, we utilize Conditional Encoding to augment intermediate features with the original image encoding. This method effectively integrates the abundant information from the original image, thereby enhancing the distinction between the objects and the surrounding background. Our proposed method was trained and tested on the 2015 MIC-CAI Gland Segmentation (GlaS) Challenge dataset [20] and Colorectal Adenocarcinoma Gland (CRAG) dataset [6] (as shown in Fig. 1), and the experiment results demonstrate the efficacy of the method. To preserve multi-scale information, we introduce a Mask Branch that operates on F mask . By applying convolutions with weights assigned from filters to F mask , we obtain instance masks."
Instance-Aware Diffusion Model for Gland Segmentation in Colon Histology Images,2,Method,"In this section, we present the architecture of our proposed method, which includes an Image Encoder, an Image Decoder, and a Mask Branch. The network structure is shown in Fig. 2."
Instance-Aware Diffusion Model for Gland Segmentation in Colon Histology Images,2.1,Image Encoder,"We propose to perform subsequent operations on the features of the original image, so we use an Image Encoder for advanced feature extraction. The Image Encoder takes the original image as input and we use a convolutional neural network such as ResNet [8] for feature extraction and a Feaure Pyramid Network (FPN) [12] is used to generate a multi-scale feature map for ResNet backbone following.The input image is x and the output is a high-level feature F R .where F is the ResNet. The Image Encoder operates only once and uses the F R as condition to progressively refine and generate predictions from the noisy boxes."
Instance-Aware Diffusion Model for Gland Segmentation in Colon Histology Images,2.2,Image Decoder,"We designed our model based on the diffusion model [30], which typically uses two Markov chains divided into two phases: a forward diffusion process and a reverse denoising process. The components of diffusion model are a learning reverse process called p θ (z t-1 |z t ) that creates samples by converting noise into samples from q(z 0 ) and a forward diffusion process called q(z t |z t-1 ) that gradually corrupts data from some target distribution into a normal distribution. The forward diffusion process is defined as:A variance schedule β t ∈ (0, 1), t ∈ {1, ..., T } determines the amount of noise that is introduced at each stage. Alternatively, we can obtain a sample of z t from direct z 0 as follows:where ᾱt = t s=0 (1β s ), ∼ N (0, I). Our Image Decoder is based on diffusion model, which can be viewed as a noise-to-GT denoising process. In this setting, the data samples consist of a set of bounding boxes represented as z 0 , where z 0 is a set of N boxes.The neural network f θ (z t , t) is trained to predict z 0 from the z t based on the corresponding image x. In addition, to achieve complementary information by integrating the segmentation information from z t into the original image encoding, we introduce Conditional Encoding, which uses the encoding features of the current step to enhance its intermediate features.where D represent the decoder, E represent the encoder and m ∈ {1, ..., T }. We use Instance Aware Filters (IAF) during iterative sampling, which allows sharing parameters between steps.where F t f is the output feature of the filter."
Instance-Aware Diffusion Model for Gland Segmentation in Colon Histology Images,2.3,Mask Branch,"We have also utilized dynamic mask head [22] to predict masks in our study.In this stage, we use the Mask Branch to fuse the different scale information of the FPN and output the mask feature F mask . The diffusion process decodes RoI features into local masks, and multi-scale features can be supplemented with more detailed information for predicting global masks to compensate for the detail lost in the diffusion process, and we believe that instance masks require a larger perceptual domain because of the higher demands on instance edges. Specifically, the instance mask can be generated by convolving the feature map F mask from the Mask Branch and F t f from the IAF , which is calculated as follows:where the predicted instance mask is denoted by s ∈ R H×W . The Mask FCN Head, denoted by MF H, is comprised of three 1 × 1 convolutional layers. We enhance our loss function by incorporating two components, L d and L s , and utilize the γ parameter to optimize the balance between these two losses.where the L s in our model represents the measure of overlap between the predicted instance mask and the ground truth s GT [14], and the L d is the loss of DiffusionDet. The optimal value for the parameter γ is usually determined based on achieving the best overall performance on the validation set. In this work, we chose γ = 5 to balance these two losses."
Instance-Aware Diffusion Model for Gland Segmentation in Colon Histology Images,3,Experiments and Results,"We presented the segmentation results of our model compared to the ground truth in Fig. 3, and provided both qualitative and quantitative evaluations that validate the effectiveness of our proposed network for gland instance segmentation."
Instance-Aware Diffusion Model for Gland Segmentation in Colon Histology Images,,Data and Evaluation Metrics:,"We evaluated the effectiveness of the proposed model on two datasets: the GlaS dataset and the CRAG dataset. The GlaS dataset comprises 85 training and 80 testing images, divided into 60 images in Test A and 20 images in Test B. The CRAG dataset consists of 173 training and 40 testing images. We have adopted Vahadane method for stain normalization [1]. Furthermore, to enhance the training dataset and mitigate the risk of overfitting, we employed random combinations of image flipping, translation, Gaussian blur, brightness variation, and other augmentation techniques.We assessed the segmentation results using three metrics from the GlaS Challenge: (1) Object F1, which measures the accuracy of detecting individual glands, (2) Object Dice, which evaluates the volume-based accuracy of gland segmentation, and (3) Object Hausdorff, which assesses the shape similarity between the segmentation result and the ground truth. We assigned each method three ranking numbers based on these metrics and computed their sum to determine the final ranking for each method's overall performance."
Instance-Aware Diffusion Model for Gland Segmentation in Colon Histology Images,,Implementation Details:,"In our experiments, we choose the ResNet-50 with FPN as the backbone in the proposed method. The backbone is pretrained on ImageNet. Image decoder, Mask Branch and Mask FCN Head are trained end-toend. We trained on the GlaS and CRAG datasets in a Python 3.8.3 environment on Ubuntu 18.04, using PyTorch 1.10 and CUDA 11.4. During training, we utilized an SGD optimizer with a learning rate of 2.5 × 10 -5 and the weight decay as 10 -4 . We set diffusion timesteps T = 1000 and chose a linear schedule from β 1 = 10 -4 to β T = 0.02. Training was performed on A100 GPU with a batch size of 2."
Instance-Aware Diffusion Model for Gland Segmentation in Colon Histology Images,,Results on the GlaS Challenge Dataset:,"We conducted experiments to evaluate the performance of our proposed model by comparing it with the DSE model [27], the DMCN [28], the DCAN [2], the SPL-Net [29], the DoubleU-Net [24], the MILD-Net [6], the GCSBA-Net [25], and the MPCNN [19]. Table 1 provides an overview of the average performance of these models.Our proposed model demonstrated a enhancement in performance, surpassing the second-best method on both Test A and Test B datasets. Specifically, on Test A, we observed an improvement of 0.006, 0.01, and 1.793 in Object F1, Object Dice, and Object Hausdorf. Similarly, on Test B, resulting in an improvement of 0.022, 0.014 and 3.694 in Object F1, Object Dice, and Object Hausdorf, respectively. Although Test B presented a more challenging task due to the presence of complex morphology in the images, our proposed model demonstrated accurate segmentation in all cases. The experimental results highlighted the effectiveness of our approach in improving the accuracy of gland instance segmentation."
Instance-Aware Diffusion Model for Gland Segmentation in Colon Histology Images,,Results on the CRAG Dataset:,"The proposed model was additionally evaluated on the CRAG dataset by comparing it against the GCSBA-Net, DoubleU-Net, DSE model, MILD-Net, and DCAN. The average performance of these models is shown in Table 2. Our experimental results demonstrate that our proposed method achieves superior performance, with improvements of 0.017, 0.012, and 4.026 for Object F1, Object Dice, and Object Hausdorff, respectively, compared to the second-best method. These results demonstrate the effectiveness of our method in segmenting different datasets.Ablation Studies: Our network utilizes the Mask Branch and Conditional Encoding to enhance performance and segmentation quality. Ablation studies on the GlaS and CRAG datasets confirm the effectiveness of these modules (Table 3). The Mask Branch is responsible for multi-scale feature extraction and fusion with the backbone network, as well as refining the Image Decoder's output. Without the Mask Branch, direct usage of original image features lacks multi-scale information and results in less accurate segmentation. Conditional Encoding is employed to establish a connection between input image features  "
Instance-Aware Diffusion Model for Gland Segmentation in Colon Histology Images,4,Conclusion and Discussion,"In this paper, we propose a diffusion model based method for gland instance segmentation. By considering instance segmentation as a denoising process based on diffusion model. Our model contains three main parts: Image Encoder, Image Decoder, and Mask Branch. By utilizing a diffusion model with Conditional Encoding for denoising, we are able to improve the precision of instance localization while compensating for the missing details in the diffusion model. By incorporating multi-scale information fusion, our approach results in more accurate segmentation outcomes. Experimental results on the GlaS dataset and CRAG dataset show that our method surpasses state-of-the-art approach, demonstrating its effectiveness. Although our method demonstrates excellent performance in gland instance segmentation, challenges arise in certain scenarios characterized by irregular shapes, flattening, and overlapping. In such cases, our network tends to classify multiple small targets with unclear boundaries as a single object, indicating limitations in segmentation accuracy when dealing with high aggregation or overlap. This limitation may stem from the difficulty in accurately distinguishing fine details between instances and the incorrect identification of boundaries.To address these limitations, future work will focus on improving segmentation performance in challenging scenarios by specifically targeting three identified limitations: (1) Incorporate random noise during training to reduce reliance on bounding box information for denoising; (2) Explore more efficient methods for cross-step denoising in the diffusion model to improve processing time without compromising segmentation accuracy; and (3) Develop a more effective Conditional Encoding method to provide accurate instance context for noise filtering in discriminative tasks like nuclear segmentation."
Instance-Aware Diffusion Model for Gland Segmentation in Colon Histology Images,,Fig. 1 .,
Instance-Aware Diffusion Model for Gland Segmentation in Colon Histology Images,,Fig. 2 .,
Instance-Aware Diffusion Model for Gland Segmentation in Colon Histology Images,,Fig. 3 .,
Instance-Aware Diffusion Model for Gland Segmentation in Colon Histology Images,,Table 1 .,
Instance-Aware Diffusion Model for Gland Segmentation in Colon Histology Images,,Table 2 .,
Instance-Aware Diffusion Model for Gland Segmentation in Colon Histology Images,,Table 3 .,
Adaptive Supervised PatchNCE Loss for Learning H&E-to-IHC Stain Translation with Inconsistent Groundtruth Image Pairs,1,Introduction,"Immunohistochemical (IHC) staining is a widely used technique in pathology for visualizing abnormal cells that are often found in tumors. IHC chromogens highlight the presence of certain antigens or proteins by staining their corresponding antibodies. For instance, the HER2 (human epidermal growth factor receptor 2) biomarker is associated with aggressive breast tumor development and is essential in forming a precise treatment plan. Despite its capability to provide highly valuable diagnostic information, the process of IHC staining is very labor-intensive, time-consuming and requires specialized histotechnologists and laboratory equipments [2]. Such factors hinder the general availability of IHC staining in histopathological applications.At the other end of the spectrum, H&E (Hematoxylin and Eosin) staining, as the gold standard in histological staining, highlights the tissue structures and cell morphology. In routine diagnostics, on account of its much lower cost, an H&E-stained slide is prepared by pathologists in order to determine whether or not to also apply the IHC stains for a more precise assessment of the disease. Therefore, it is of great interest to have an algorithm that can automatically translate an H&E-stained slide into one that could be considered to have been stained with IHC while accurately predicting the target expression levels.To that end, researchers have recently proposed to use GAN-based Image-to-Image Translation (I2IT) algorithms for transforming H&E-stained slides into IHC. Despite the progress, the outstanding challenge in training such I2IT frameworks is the lack of aligned H&E-IHC image pairs, or in other words, the inconsistencies in the H&E-IHC groundtruth pairs. To explain, since re-staining a slice is physically infeasible, a matching pair of H&E-IHC slices are taken from two depth-wise consecutive cuts of the same tissue then stained and scanned separately. This inevitably prevents pixel-perfect image correspondences due to the slice-to-slice changes in cell morphology, staining-induced degradation (e.g. tissue-tearing), imaging artifacts that may vary among slices (e.g. camera out-offocus) and multi-slice registration errors. An example pair of patches is shown in Fig. 1 and another pair with significant inconsistencies is shown in Fig. 2(a)(c). In the latter, comparing the groundtruth IHC image to the input H&E image, one can clearly see the inconsistencies -nearly the entire left half of the tissue present in the H&E image is missing.As a result, recent advances in H&E-to-IHC I2IT have mostly avoided using the inconsistent GT pairs and instead have imposed the cycle-consistency constraint [6,8,13]. Moreover, existing approaches have also exploited using expert annotations such as per-cell labels [9], semantic masks [8] and patch-level labels [8,13]. As for the prior works that directly utilize the H&E-IHC pairs for supervision, a variant of Pix2Pix [4] that uses a Gaussian Pyramid based reconstruction loss to accommodate the noisy GT is proposed in [7]. However, the robustness of such approaches that punish absolute errors in the generated image to dealing with GT inconsistencies remains unclear.In this paper, we argue that the IHC slides, despite the disparities vis-a-vis their H&E counterparts, can still serve as useful targets for stain translation. The work we present in this paper is based on the important realization that even when pairs of consecutive tissue slices do not yield images that are pixel-perfect aligned, it is highly likely that the corresponding patches in the two stains share the same diagnostic label. For example, if the levels of expression in a region of the HER2 slide are high, the corresponding region in the H&E slide is highly likely to contain a high density of cancerous cells. Therefore, we set our goal to meaningfully leverage such correlations to benefit the H&E-to-IHC I2IT while being resilient to any inconsistencies.Toward this goal, we propose a supervised patchwise contrastive loss named the Adaptive Supervised PatchNCE (ASP) loss. Our formulation of this loss was inspired by the recent research findings that contrastive loss benefits model robustness under label noise [3,12]. Furthermore, based on the observation that any dissimilarity between the patch embeddings at corresponding locations in the generated and groundtruth IHC images is indicative to the level of inconsistency of the GT at that location, we employ an adaptive weighting scheme in ASP. By down-weighting the contrastive loss at locations with low similarities, i.e. high inconsistencies, our proposed ASP loss helps the network learn more robustly.Lastly, to support further research in virtual IHC-restaining, we present the Multi-IHC Stain Translation (MIST) as a new public dataset. The MIST dataset contains 4k+ training and 1k testing aligned H&E-IHC patches for each of the following IHC stains that are critical for breast cancer diagnostics: HER2, Ki67, ER (Estrogen Receptor) and PR (Progesterone Receptor). We evaluated existing I2IT methods and ours for multiple IHC stains and demonstrate the superior performance achieved by our method both qualitatively and quantitatively. "
Adaptive Supervised PatchNCE Loss for Learning H&E-to-IHC Stain Translation with Inconsistent Groundtruth Image Pairs,2,Method Description,
Adaptive Supervised PatchNCE Loss for Learning H&E-to-IHC Stain Translation with Inconsistent Groundtruth Image Pairs,2.1,The Supervised PatchNCE (SP) Loss,"Before getting to our ASP loss, we need to first introduce the SP loss as a robust means to learning from inconsistent GT image pairs. The SP loss was inspired by the findings in recent literature that demonstrate the positive effect of contrastive learning on boosting model robustness against label noise [3,12,14]. It takes the same form as the PatchNCE loss as introduced in [11], except that it is applied on the generated-GT image pair (instead of the input-generated pair).The goal of the PatchNCE loss is to ensure the content is consistent across translation by maximizing the mutual information between the input and the corresponding output. It does so by minimizing a patch-based InfoNCE loss [10], which encourages the network to associate the corresponding patches with each other in the learned embedding space, while disassociating them from the noncorresponding ones. Mathematically, the InfoNCE loss takes the form:where v, v + and v -are the embeddings of the anchor, positive and negative samples, respectively. With InfoNCE, the PatchNCE loss is set up as follows:given the anchor embedding ẑY of a patch in the output image, the positive z X is the embedding of the corresponding patch from the input image, while the negatives z X are embeddings of the non-corresponding ones, i.e.As for the SP loss, given the embedding of an output patch ẑY as anchor, we now designate the embedding of the corresponding patch in the groundtruth image z Y as the positive and the embeddings of the non-corresponding ones z Y as the negatives. We then use the same InfoNCE-based contrastive learning objective, i.e. L SP = L InfoNCE (ẑ Y , z Y , z Y ). A depiction of both the PatchNCE loss and the SP loss is given in Fig. 1. It is worth noting that, despite the fact that a similar patchwise constrastive loss was proposed in [1] for supervised I2IT, it is one of our contributions in this paper to explicitly exploit the robustness of this contrastive loss in the context of H&E-to-IHC translation where the GT pairs can be highly inconsistent for reasons mentioned previously. We think that the key factor behind the robustness of L SP to inconsistent GT compared to, say, the MSE loss, is its relativeness. Instead of using an absolute loss term that may not work well on inconsistent groundtruth pairs, L SP punishes dissimilarities between the anchor and the positive in the learned latent space, relative to those between the anchor and the negatives."
Adaptive Supervised PatchNCE Loss for Learning H&E-to-IHC Stain Translation with Inconsistent Groundtruth Image Pairs,2.2,The Adaptive Supervised PatchNCE (ASP) Loss,"To learn selectively from more consistent groundtruth locations, we further propose to augment the Supervised PatchNCE loss in an adaptive manner. The key idea here is to automatically recognize patch locations that are inconsistent and adapt the SP loss so that the severely inconsistent patch locations will have lesser effects on training. To measure the consistency at a given patch location, we use the cosine similarity between the embeddings of the generated IHC patch and the corresponding GT patch. In Fig. 2, we show an example pair of generated vs GT IHC images that contain significant inconsistencies and their anchor-positive similarity heat map. For pairs of embeddings produced by a trained network, a high similarity value indicates good correspondence between the groundtruth patches while a low similarity value indicates inconsistencies.Directly motivated by this observation, we first propose a weighting scheme for the SP loss. More specifically, we assign lower weights to patch locations that have low anchor-positive similarity values to alleviate the negative impacts the inconsistent targets may have on training. At training time t, the weight is a function of the anchor-positive cosine similarity. Examples of the weight function h(•) are shown in Fig. 3(b). The weight functions are monotonic increasing so that the more confident patch locations are always treated with more importance.In order to make the weighting scheme work in practice, we must also account for the phase of training. The intuition is that, during the initial phase of training, the network is not going to be able to discriminate between consistent patch locations from those that are inconsistent. Additionally, as shown in Fig. 3(a), the histograms of the anchor-positive similarity evolve rather slowly over the training epochs. Therefore, it would not make sense to reinforce the weighting function in the beginning of the training as much as near the end of the training.To that end, we further augment the weight so that it is also a function of the training iterations. Such scheduling of the weights is done so that in the beginning of the training, the weights are uniform in order not to wrongly bias the network when the embeddings are still indiscriminative. And as training progresses, the selective weighting scheme is gradually enforced so that the inconsistent patch locations are treated with reduced weights. We call this gradual process of shifting the learning focus weight scheduling. Let t denote the current iteration and T the total number of training iterations. Then weight scheduling is achieved by using a scheduling function g( t T ). Various options of g(•) are shown in Fig. 3(c). Subsequently, combining the weighting function with the scheduling function, we can write the following formula for the final weight:We refer to the new augmented Supervised PatchNCE loss as the Adaptive Supervised PatchNCE (ASP) loss, which can be expressed as:   where W l t = s w l,s t is a normalization factor that maintains the total magnitude of the loss after applying the weights. Finally, the overall learning objective for our generator is as follows:where L GP is the Gaussian Pyramid based reconstruction loss from [7]. "
Adaptive Supervised PatchNCE Loss for Learning H&E-to-IHC Stain Translation with Inconsistent Groundtruth Image Pairs,3,Experiments,"Datasets. The following datasets are used in our experiments: the Breast Cancer Immunohistochemical (BCI) challenge dataset [7] and our own MIST dataset that is now in the public domain. The publicly available portion of BCI contains 3396 H H&E-HER2 patches for training and 500 of the same for testing. Note that we have additionally normalized the brightness levels of all BCI images to the same level. Due to the page limit, from the MIST dataset, here we only present detailed results on HER2 and ER. For MIST HER2 , we extracted 4642 paired patches for training and 1000 for testing from 64 WSIs. And for MIST ER , we extracted 4153 patches for training, and 1000 for testing from 56 WSIs. All WSIs were taken at 20× magnification. All patches are of size 1024 × 1024 and non-overlapping. Additional results on MIST Ki67 and MIST PR are provided in the Supplementary Materials.Implementation Details. For all of our models, we used ResNet-6Blocks as the generator and a 5-layer PatchGAN as the discriminator. We trained our networks with random 512 × 512 crops and a batch size of one. The Adam optimizer [5] was used with a linear decay scheduler (as shown in Fig. 3(c)) and an initial learning rate of 2 × 10 -4 . The hyperparameters in Eq. ( 4) are set as: λ PatchNCE = 10.0, λ ASP = 10.0 and λ GP = 10.0.Evaluation Metrics. We compare the methods using both paired and unpaired evaluation metrics. To compare a pair of images, generated and groundtruth, we use the standard SSIM (Structural Similarity Index Measure) and PHV (Perceptual Hash Value) as described in [8]. As for the unpaired metrics, we use the FID (Fréchet Inception Distance) and the KID (Kernel Inception Distance).Qualitative Evaluations. In Fig. 4, we compare visually the generated IHC images by our framework. It can be observed that by using either L SP or L ASP , the pathological representations in the generated images are significantly more accurate. And by using L ASP , such representations appear to be more consistent. Quantitative Evaluations. The full results comparing existing I2IT methods to ours are tabulated in Tab. 1. Overall, it can be observed that the proposed framework with the ASP loss consistently outperforms existing methods across all three datasets. For those methods, Fig. 5 visually illustrates the extent of hallucinations which we believe is the reason for their poor quantitative performance. Subsequently, in Tab.  "
Adaptive Supervised PatchNCE Loss for Learning H&E-to-IHC Stain Translation with Inconsistent Groundtruth Image Pairs,4,Conclusion,"In this paper, we have proposed the Adaptive Supervised PatchNCE (ASP) loss for learning H&E-to-IHC stain translation with inconsistent GT image pairs. The adaptive logic in ASP is based on the intuition that inconsistent patch locations should contribute less to learning. We demonstrated that our proposed framework is able to achieve significant improvements both qualitatively and quantitatively over the existing approaches for translations to multiple IHC stains. Finally, we have made public our Multi-IHC Stain Translation dataset with the hope to assist further research towards accurate H&E-to-IHC stain translation."
Adaptive Supervised PatchNCE Loss for Learning H&E-to-IHC Stain Translation with Inconsistent Groundtruth Image Pairs,,Fig. 1 .,
Adaptive Supervised PatchNCE Loss for Learning H&E-to-IHC Stain Translation with Inconsistent Groundtruth Image Pairs,,Fig. 2 .,
Adaptive Supervised PatchNCE Loss for Learning H&E-to-IHC Stain Translation with Inconsistent Groundtruth Image Pairs,,Fig. 3 .,
Adaptive Supervised PatchNCE Loss for Learning H&E-to-IHC Stain Translation with Inconsistent Groundtruth Image Pairs,,Fig. 4 .,
Adaptive Supervised PatchNCE Loss for Learning H&E-to-IHC Stain Translation with Inconsistent Groundtruth Image Pairs,,Fig. 5 .,
Adaptive Supervised PatchNCE Loss for Learning H&E-to-IHC Stain Translation with Inconsistent Groundtruth Image Pairs,,Table 1 .,
Adaptive Supervised PatchNCE Loss for Learning H&E-to-IHC Stain Translation with Inconsistent Groundtruth Image Pairs,,Table 2 .,
Adaptive Supervised PatchNCE Loss for Learning H&E-to-IHC Stain Translation with Inconsistent Groundtruth Image Pairs,,,
Adaptive Supervised PatchNCE Loss for Learning H&E-to-IHC Stain Translation with Inconsistent Groundtruth Image Pairs,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 61.
Gradient and Feature Conformity-Steered Medical Image Classification with Noisy Labels,1,Introduction,"Deep neural networks (DNNs) have achieved remarkable success in medical image classification. However, the great success of DNNs relies on a large amount of training data with high-quality annotations, which is practically infeasible. The annotation of medical images requires expert domain knowledge, and suffers from large intra-and inter-observer variability even among experts, thus noisy annotations are inevitable in clinical practice. Due to the strong memorization ability, DNNs can easily over-fit the corrupted labels and degrade performance [1,2], thus it is crucial to train DNNs that are robust to noisy labels.An effective paradigm in learning with noisy labels (LNL) first selects clean samples, then formulates the LNL problem as semi-supervised learning (SSL) task by regarding the clean samples as a labeled set and noisy samples as an unlabeled set [3]. However, both the clean data selection and SSL stages in existing methods have some drawbacks. In the clean data selection stage, most existing studies rely on the small-loss [3,4] or high-confidence criteria [5] of individual samples, but neglect the global contextual information and high-order topological correlations among samples, thus unavoidably resulting in confirmation bias [6]. Besides, the above criteria in the output space are directly supervised and easily affected by corrupted labels [7]. Previous studies indicate that optimization dynamics (characterized by sample gradients) can reflect the true class information [8] and feature space is more robust to noisy labels, thus can provide more robust criteria for clean data selection [6,7,9]. Therefore, we aim to achieve more accurate clean data selection by exploring the topological correlation and contextual information in the robust gradient and feature spaces.In the SSL stage, most existing studies [3,10] estimate pseudo labels for all samples and employ Mixup [11,12] to linearly interpolate the input samples and their pseudo labels for model training. Compared with previous methods that train DNNs by reweighting samples [13] or utilizing clean data only [4,14], the Mixup [12] operation can effectively augment the dataset and regularize the model from over-fitting. However, as the pseudo labels of noisy datasets cannot be always reliable, the traditional Mixup method which randomly chooses the mixup partner for each sample may accumulate errors in pseudo labels. Therefore, it is highly desirable to design a novel Mixup method that can select reliable mixup partners and mitigate the interference of unreliable pseudo labels.In this paper, we present a novel two-stage framework to combat noisy labels in medical image classification. In the clean data selection stage, we propose a gradient and feature conformity-based method to identify the samples with clean labels. Specifically, the Gradient Conformity-based Selection (GCS) criterion selects clean samples that show higher conformity with the principal gradient of its labeled class. The Feature Conformity-based Selection (FCS) criterion identifies clean samples that show better alignment with the feature eigenvector of its labeled class. In the SSL stage, we propose a Sample Reliability-based Mixup (SRMix) to augment the training data without aggravating the error accumulation of pseudo labels. Specifically, SRMix interpolates each sample with reliable mixup partners which are selected based on their spatial reliability, temporal stability, and prediction confidence. Our main contributions are as follows: -We devise two novel criteria (i.e., GCS and FCS) to improve clean data selection by exploring the topological correlation and contextual information in the gradient and feature spaces. -We propose a novel SRMix method that selects reliable mixup partners to mitigate the error accumulation of pseudo labels and improve model training.-Extensive experiments show that our proposed framework is effective in combating label noise and outperforms state-of-the-art methods on two medical datasets with both synthetic and real-world label noise."
Gradient and Feature Conformity-Steered Medical Image Classification with Noisy Labels,2,Method,"An overview of our proposed two-stage framework is shown in Fig. 1. The training dataset is denoted as, where the given label y i could be noisy or clean. In the clean data selection stage, we propose a gradient and feature conformity-based method to distinguish clean samples from the noisy dataset. As shown in Fig. 1 (a), the GCS computes the principal gradient of each class (i.e., g 1 , g 2 , g 3 ) to represent its optimization dynamics, and measures the label quality of each sample by its gradient conformity with the class-wise principal gradient. The FCS computes the principal feature eigenvector of each class to reflect its contextual information, and measures the label quality of each sample by its feature conformity with the class-wise feature eigenvector. Based on the integration of these two criteria, the training data is divided into a noisy set D noisy and a clean set D clean . In the SSL stage (see Fig. 1 (b)), our SRMix module interpolates each sample (x i , y i ) with a reliable mixup partner (x j , y j ), which is selected based on its spatial reliability, temporal stability, and prediction confidence. The mixed samples are used for model training."
Gradient and Feature Conformity-Steered Medical Image Classification with Noisy Labels,2.1,Gradient and Feature Conformity-Based Clean Data Selection,"Inspired by previous studies that optimization dynamics in the gradient space reflects the true class information [8,15] and contextual information in the feature space is more robust to noisy labels [7], we devise the novel GCS and FCS criteria to measure label quality in the gradient and feature spaces."
Gradient and Feature Conformity-Steered Medical Image Classification with Noisy Labels,,Gradient Conformity-Based Selection (GCS).,"The GCS aims to distinguish clean samples from noisy ones by exploring their optimization dynamics in the gradient space. Since training samples from the same class usually exhibit similar optimization dynamics [15], the gradient of a sample should be similar to the principal gradient of its true class, thus we use the gradient conformity as a criterion to evaluate the quality of its given label. Specifically, for each sample x i , its gradient g(x i ) is computed as:where f (x i ) is the feature vector of the sample x i , and x j denotes the K-Nearest Neighbors (KNN) of x i . p (x i ) is the probability of the most likely true class predicted by its KNN neighbors. Therefore, the gradient g(x i ) is very likely to reflect the true class information and optimization dynamics of x i . For each class, we select α% samples with the smallest loss as an anchor set A c , which is depicted in the shaded areas of the GCS in Fig. 1 (a). Then, the principal gradient of the c-th class is computed as:which is the average gradient of all samples in the anchor set A c of the c-th class. Then, we can measure the similarity between the gradient of the sample x i and the principal gradient of class y i with the cosine similarity s g (x i ) = cos < g(x i ), g yi >. For the sample x i , if y i is a noisy label, g(x i ) should be consistent with the principal gradient of its true class and diverge from g yi , thus yielding small s g (x i ). By fitting Gaussian mixture models (GMM) on the similarity score s g (x i ), we can get c g (x i ) = GM M (s g (x i )), which represents the clean probability of the sample x i decided by the GCS criterion. To the best of our knowledge, this is the first work that explores gradient conformity for clean data selection.Feature Conformity-Based Selection (FCS). Since feature space is more robust to noisy labels than the output space [7], our FCS criterion explores high-order topological information in the feature space and utilizes the feature conformity with class-wise principal eigenvectors as a criterion to select clean samples. Specifically, for each class, we compute the gram matrix as:where f (x i ) denotes the feature vector of the sample x i in the anchor set A c of the c-th class. Then, we perform eigen-decomposition on the gram matrix:, where U c is the eigenvector matrix and Σ c is a diagonal matrix composed of eigenvalues. The principal eigenvector u c of U c is utilized to represent the distribution and contextual information of the c-th class. Then, for each sample x i , we measure its label quality based on the conformity of its feature f (x i ) with the principal eigenvector u yi of its given label: s f (x i ) = cos < f (x i ), u yi >. Samples that better align with the principal eigenvectors of their labeled class are more likely to be clean. According to the FCS criterion, the clean probability of the sample x i is obtained byCompared with existing methods that utilize classwise average features to represent contextual information [7], the eigenvectors in our method can better explore the high-order topological information among samples and are less affected by noisy features.Integration of GCS and FCS. Finally, we average the clean probabilities estimated by the GCS and FCS criteria to identify clean data. As shown in Fig. 1 (a), for a dataset with the noise rate of r%, we divide all samples into a clean set D clean (i.e., (1 -r%) samples with higher clean probabilities) and a noisy set D noisy (i.e., r% samples with lower clean probabilities)."
Gradient and Feature Conformity-Steered Medical Image Classification with Noisy Labels,2.2,Sample Reliability-Based Mixup (SRMix),"By regarding D clean as a labeled set and D noisy as an unlabeled set, we can formulate the LNL task into an SSL problem and employ Mixup [12] to generate mixed samples for model training [3]. In the traditional Mixup [12], each sample (x i , y i ) is linearly interpolated with another sample (x j , y j ) randomly chosen from the mini-batch. However, the pseudo labels of noisy datasets cannot be always reliable and the Mixup operation will aggravate the error accumulation of pseudo labels. To mitigate the error accumulation of pseudo labels, we propose a Sample Reliability-based Mixup (SRMix) method, which selects mixup partners based on their spatial reliability, temporal stability, and prediction confidence.Intuitively, samples with reliable pseudo labels should have consistent predictions with their neighboring samples, stable predictions along sequential training epochs, and high prediction confidence. As shown in Fig. 1 (b), we select reliable mixup partners for each sample based on the triple criteria. First, for each sample x j , we define the spatial reliability as:where p(x j ) and p(x k ) are the pseudo labels of sample x j and its neighbor x k . Normalize() denotes the min-max normalization over all samples in each batch. If the pseudo label of a sample is more consistent with its neighbors, a higher R spatial (x j ) will be assigned, and vice versa. Second, for each sample x j , we keep the historical sequence of its predictions in the past T epochs, e.g., the prediction sequence at the t-th epoch is defined as P t (x j ) = [p t-T +1 (x j ), ..., p t-1 (x j ), p t (x j )]. The temporal stability of x j can be defined as:where p(x j ) is the average prediction of the historical sequence. According to Eq. ( 5), a sample with smaller variance or fluctuation over time will be assigned with a larger R temporal (x j ), and vice versa. Finally, the overall sample reliability is defined as: R(, where max(p(x j )) denotes the prediction confidence of the pseudo label of the sample x j . The possibility of x j being chosen as a mixup partner is set aswhere τ R is a predefined threshold to filter out unreliable mixup partners. For each sample x i , we select a mixup partner x j with the probability p m (x j ) defined in Eq. ( 6), and linearly interpolate their inputs and pseudo labels to generate a mixed sample (x, y). The mixed sample is fed into the network and trained with cross-entropy loss L CE (x, y). Considering the estimation of sample reliability might be inaccurate in the initial training stage, we employ the traditional Mixup in the first 5 epochs and utilize our proposed SRMix for the rest training epochs.Compared with the traditional Mixup, the SRMix can effectively mitigate error accumulation of the pseudo labels and promote model training."
Gradient and Feature Conformity-Steered Medical Image Classification with Noisy Labels,3,Experiments,
Gradient and Feature Conformity-Steered Medical Image Classification with Noisy Labels,3.1,Datasets and Implementation Details,"WCE Dataset with Synthetic Label Noise. The Wireless Capsule Endoscopy (WCE) dataset [16] contains 1,812 images, including 600 normal images, 605 vascular lesions, and 607 inflammatory frames. We perform 5-fold cross-validation to evaluate our method. Following the common practice in the LNL community [3,4,10], we employ symmetric and pairflip label noise with diverse settings on the training set to simulate errors in the annotation process.The symmetric noise rate is set as 20%, 40%, 50%, and the pairflip noise rate is set as 40%. The model performance is measured by the average Accuracy (ACC) and Area Under the Curve (AUC) on the 5-fold test data.Histopathology Dataset with Real-World Label Noise. The histopathology image dataset is collected from Chaoyang Hospital [10] and is annotated by 3 professional pathologists. There are 1,816 normal, 1,163 serrated, 2,244 adenocarcinoma, and 937 adenoma samples of colon slides in total. The samples with the consensus of 3 pathologists are selected as the test set, including 705 normal, 321 serrated, 840 adenocarcinoma, and 273 adenoma samples. The rest samples are utilized to construct the training set, with randomly selected opinions from one of the three doctors used as the noisy labels. The model performance is measured by the average Accuracy (ACC), F1 Score (F1), Precision, and Recall on 3 independent runs. Implementation Details. Our method follows the baseline framework of DivideMix [3] and adopts the pre-trained ResNet-50 [17] for feature extraction. We implement our method and all comparison methods on NVIDIA RTX 2080ti GPU using PyTorch [18]. For the WCE dataset, our method is trained for 40 epochs with an initial learning rate set to 0.0001 and divided by 10 after 20 epochs. For the histopathology dataset, the network is trained for 20 epochs with the learning rate set to 0.0001. For both datasets, the network is trained by Adam optimizer with β 1 = 0.9 and β 2 = 0.999, and batch size of 16. The number of neighbors K is set as 10 in Eq. ( 1) and Eq. ( 4). Length T of the historical sequence is set as 3. The reliability threshold τ R is set as 0.2 for the WCE dataset and 0.05 for the histopathology dataset. "
Gradient and Feature Conformity-Steered Medical Image Classification with Noisy Labels,3.2,Experimental Results,"Comparison with State-of-the-Art Methods. We first evaluate our method on the WCE dataset under diverse synthetic noise settings and show the results in Table 1. We compare with three well-known LNL methods (i.e., Co-teaching [4], Coteaching+ [14], and DivideMix [3]) and three state-of-the-art LNL methods (i.e., EHN-NSHE [10], SFT [19], and TSCSI [7]). As shown in Table 1, our method outperforms existing methods under all noise settings, and the performance gain is more significant under severe noise settings (e.g., noise rate ≥ 40%). Under the four settings, our method outperforms the second-best model by 0.67%, 2.65%, 4.19%, and 4.08% in accuracy. These results indicate the effectiveness of our method.We then evaluate our method on the histopathology dataset with real-world label noise. As shown in Table 2, our method outperforms existing state-of-theart methods, indicating the capability of our method in dealing with complex real-world label noise.Ablation Study. To quantitatively analyze the contribution of the proposed components (i.e., GCS, FCS, and SRMix) in combating label noise, we perform an ablation study on the WCE dataset under 40% symmetric and pairflip noise. As shown in Table 3, compared with the DivideMix baseline (line 1) [3], replacing the small-loss criterion by GCS or FCS both improve the model performance significantly (lines 2-3), and their combination leads to further performance gains (line 5). Furthermore, better performance can be achieved by replacing the traditional Mixup with our proposed SRMix method (line 1 vs. line 4, line 5 vs. line 6). These results indicate that filtering out unreliable mixup partners can effectively improve the model's capacity in combating label noise.More comprehensive analysis of the GCS and FCS criteria is provided in the supplementary material. Figure S1 demonstrates that compared with the normalized loss [3], the GCS and FCS criteria are more distinguishable between the clean and noisy data. This is consistent with the improvement of clean data selection accuracy in Fig. S2. As shown in Fig. S3, both the feature and gradient of each sample are aligned with the center of its true class, further validating the rationality of using gradient and feature conformity for clean data selection."
Gradient and Feature Conformity-Steered Medical Image Classification with Noisy Labels,4,Conclusion,"In this paper, we present a two-stage framework to combat label noise in medical image classification tasks. In the first stage, we propose two novel criteria (i.e., GCS and FCS) that select clean data based on their conformity with the class-wise principal gradients and feature eigenvectors. By exploring contextual information and high-order topological correlations in the gradient space and feature space, our GCS and FCS criteria enable more accurate clean data selection and benefit LNL tasks. In the second stage, to mitigate the error accumulation of pseudo labels, we propose an SRMix method that interpolates input samples with reliable mixup partners which are selected based on their spatial reliability, temporal stability, and prediction confidence. Extensive experiments on two datasets with both diverse synthetic and real-world label noise indicate the effectiveness of our method."
Gradient and Feature Conformity-Steered Medical Image Classification with Noisy Labels,,Fig. 1 .,
Gradient and Feature Conformity-Steered Medical Image Classification with Noisy Labels,,Table 1 .,
Gradient and Feature Conformity-Steered Medical Image Classification with Noisy Labels,,Table 2 .,
Gradient and Feature Conformity-Steered Medical Image Classification with Noisy Labels,,Table 3 .,
Gradient and Feature Conformity-Steered Medical Image Classification with Noisy Labels,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2_8.
Forensic Histopathological Recognition via a Context-Aware MIL Network Powered by Self-supervised Contrastive Learning,1,Introduction,"Computational pathology powered by artificial intelligence (AI) shows promising applications in various clinical studies [14,18], significantly easing the workload and promoting the development of clinical pathology. Inspired by such exciting progress, let's think step by step, so why not leverage advanced AI techniques to boost the research and applications in another important discipline, i.e., forensic pathology? Forensic pathology focuses on investigating the cause, manner, and time of (non-natural) deaths based on histopathological examinations of postmortem organ tissues [5]. As an indispensable part of the medicolegal autopsy, it provides critical evidence from the microscopic aspect to confirm, perfect, or refute macroscopic findings, establishing a reliable factual basis for future inferences [4]. Histopathological analysis in forensic pathology is challenging and time-consuming, since postmortem changes (e.g., putrefaction and autolysis) severely destroy tissues' typical image appearance, even making the manual differentiation between the tissues of different organs very difficult.Although diverse deep-learning approaches have been proposed in clinical studies to process and analyze histopathological images [14,18], no similar work has yet in the forensic pathology community. The main reason could be threefold. 1) Forensic and clinical pathology have distinct purposes. The former case analyzes the tissue images from multiple organs concurrently. In contrast, clinical diagnosis/prognosis usually focuses on one tissue type in one task [6]. 2) Due to postmortem changes, histopathological images in forensic pathology have atypical appearances and more complex distributions than in clinical pathology, bringing additional challenges to deep representation learning [21,25]. 3) Data in forensic pathology are more difficult to obtain and have relatively lower quality. Therefore, to deploy a reliable computational pathology system for forensic investigation, fine-grained discriminative representation learning from complex postmortem histopathological images is a very precondition.In this paper, we introduce a deep computational pathology framework (dubbed as FPath) for forensic histopathological analysis. As shown in Fig. 1, FPath leverages the idea of self-supervised contrastive learning and multiple instance learning (MIL) to learn discriminative histopathological representations. Specifically, we propose a self-supervised contrastive learning strategy to learn a double-tier backbone network for fine-grained feature embedding of local image patches (i.e., instances in MIL). After that, a context-aware MIL block is designed, which adopts a self-attention mechanism to refine instancelevel representations by aggregating contextual information, and then applies an adaptive-pooling operation to produce a holistic image-level representation for prediction. Our FPath performs efficient predictions without the need for tedious pre-processing (e.g., foreground extraction/segmentation). To the best of our knowledge, this paper is the first attempt that shows promising appli-cations of advanced AI techniques (e.g., self-supervised contrastive learning) to forensic pathology.The main technical contributions of our work are:1) We design a double-tier backbone and a dedicated self-supervised learning strategy to capture discriminative instance-level histopathological patterns of postmortem organ tissues. The double-tier backbone combines CNN and transformer for local and non-local information fusion. To effectively train such a backbone to handle images acquired with varying microscopic magnifications, the dedicated self-supervised learning strategy leverages multiple complementary contrastive losses and regularization terms to concurrently maximize global and spatially fine-grained similarities between different views of the same instances/patches in an informative representation space. 2) We design a context-aware MIL branch to produce the bag-level discriminative representations for accurate and efficient postmortem histopathological recognition. Our MIL branch first refines instance embedding by leveraging a self-attention mechanism integrating positional embedding to model crosspatch associations for contextual information enhancement. Thereafter, an adaptive pooling operation is designed to learn deformable spatial attention to distill from contextually enhanced patch-level representations a holistic image-level representation for recognition. 3) Our FPath was applied to recognize postmortem organ tissues, a fundamental task in forensic pathology. To this end, we established a relatively largescale multi-domain database consisting of an experimental rat postmortem dataset and a real-world human decedent dataset, each with 19, 607 and 3, 378 images acquired at a specific microscopic magnification (e.g., 5×, 10×, 20×, and 40×), respectively. On such a multi-domain database, our FPath led to promising cross-domain generalization and state-of-the-art accuracy in recognizing seven different postmortem organs. "
Forensic Histopathological Recognition via a Context-Aware MIL Network Powered by Self-supervised Contrastive Learning,2,Method,"The schematic diagram of our FPath is shown in Fig. 1, which consists of two steps: 1) Self-supervised contrastive learning of a double-tier backbone, and 2) Context-aware multiple instance learning for postmortem tissue recognition.2.1 Self-supervised Contrastive Patch Embedding Double-Tier Backbone. Given patches from a postmortem histopathological image acquired at a specific magnification (i.e., 5×, 10×, 20×, or 40×), we adopt a backbone with a local branch (LB) and a global branch (GB) for instance/patch feature embedding. The LB is a ResNet50 [10] consisting of 16 successive bottlenecks, each with three convolutional layers with the kernel size of 1 × 1, 3 × 3, and 1 × 1, respectively. The GB is a Swin Transformer [17] that contains of a series of 12 window-based multi-head self-attention modules. Let an input patch be X ∈ R H×W ×3 . The corresponding feature embedding produced by the double-tier backbone will be M = M LB ⊕ M GB (∈ R h×w×C ), where M LB and M GB denotes the representations from the LB and GB branch, respectively, and ⊕ stands for the channel-wise concatenation operation.Self-supervised Contrastive Learning Strategy. We leverage the idea of self-supervised representation learning to establish the double-tier backbone.Referring to MoCo [9], our self-supervised learning is constructed by a teacher branch and a student branch. The student branch consists of six components, including a double-tier backbone (i.e., f θ (•)), three projection layers (i.e., g sg (•), g so (•), g sp (•)), and two prediction layers (i.e., p sg (•) and p so (•)). The teacher branch contains four components, including a double-tier backbone f η (•), and three projection layers (i.e., g tg (•), g to (•), and g tp (•)). By feeding the two branches with different views of same patches, f θ (•) in the student branch (i.e., parameterized by θ) is trained via back-propagation to update f η (•) in the teacher branch (i.e., parameterized by η) in a momentum-based moving average fashion, such aswhere m = 0.99 is the momentum parameter.Another key issue that determines the quality of the embedding from such a self-supervised strategy is the formulation of respective contrastive loss functions and regularization terms. Accordingly, we design a thorough contrastive learning strategy to capture fine-grained discriminative patterns of postmortem tissues under varying microscopic magnifications. That is, let X s and X t be two different views of an image patch X generated by a random data augmentation process. Our contrastive learning strategy concurrently encourages the global similarity and spatially fine-grained similarity between the corresponding feature embedding M s = f θ (X s ) and M t = f η (X t ) (∈ R h×w×C ). Also, two regularization terms are applied as auxiliary guidance to protect the informativeness and avoid collapses of the embedding learned by the backbone. Specifically, the global similarity between M s and M t is encouraged by minimizing a general cosine contrastive loss, such aswhere z g s = p sg (g sg (GAP(M s ))) and z g t = g tg (GAP(M t )), with GAP(•) standing for the global average pooling that produces feature vectors.In practice, forensic pathologists typically infer postmortem tissue type by evaluating the cellular compositions in multiple local regions. Accordingly, inspired by cross-view learning [11], we design a spatially fine-grained contrastive loss to explicitly encourage multi-parts similarity between M s and M t . Assume M s and M t are two (h • w) × C tensors flattened from M s and M t across the spatial dimension, respectively. They are further processed by g so (•) and g to (•) (followed by softmax normalization), respectively, to produce two (h • w) × K attention matrices, i.e., A s = g so (M s ) and A t = g to (M t ), where K denotes the predefined number of parts. Thereafter, we aggregate the backbone representations in terms of the attention matrices to deduce multi-parts representations, i.e., Z o s = p so (g sp (A T s ⊗ M s )) and, where ⊗ denotes tensor multiplication. Finally, the spatially fine-grained contrastive loss is quantified aswhereBesides, two additional regularization terms are further included to stabilize contrastive representation learning. Following [1], we penalize small changes between the global representations of different image patches across each feature dimension. Also, we encourage the global representations to be diverse/orthogonal across different feature dimensions. Let Z g s be a set of feature representations for an input mini-batch of patches in the student branch, and Z g s and Z g s denote their channel-wise variation and mean. The regularization terms are defined aswhere is a small scalar to stabilize numerical computation, Z g s [d] denotes the dth dimension of Z g s , andth element in such a covariance matrix. According to [1], Eqs. ( 3) and (4) jointly encourage the diversity across patches and feature dimensions, thus protecting the informativeness and avoid collapse of self-supervised contrastive learning.Overall, we combine Eqs. ( 1) to (4) as the final loss function to train the double-tier backbone, such as L all = L global + L parts + γL var + λL cov , where γ and λ are two tuning parameters balancing different terms."
Forensic Histopathological Recognition via a Context-Aware MIL Network Powered by Self-supervised Contrastive Learning,2.2,Context-Aware MIL,"Given the patch/instance-level representations of a histopathological image from the double-tier backbone, we further design a context-aware MIL framework to aggregate their information for postmortem tissue recognition. Given patch embeddings of a Microscope image, our context-aware MIL part contains two main steps, i.e., a multi-head self-attention to refine each patch's feature and an adaptive pooling step to distill all patches' information.In detail, we first adopt a multi-head self-attention (MSA) mechanism [19] integrating relative positional embedding to explicitly model cross-patch associations for contextual enhancement of the instance representations from the backbone. Let Z = {z i } I i=1 be a set of the contextually enhanced instance embedding from an image. Thereafter, inspired by Deformable DETR [28], we further design an adaptive pooling operation, which is simple but effective to distill from Z a bag-level holistic representation for the classification purpose. Specifically, the bag-level holistic representation determined by the adaptive pooling iswhere h ω1 (•) and h ω2 (•) are two linear projections with the same number of output units, symbol • denotes the Hadamard product between two tensors, and sof tmax(•) is performed across different instances to filter out uninformative patches and preserve discriminative patches in quantifying z bag for classification."
Forensic Histopathological Recognition via a Context-Aware MIL Network Powered by Self-supervised Contrastive Learning,3,Experiments,
Forensic Histopathological Recognition via a Context-Aware MIL Network Powered by Self-supervised Contrastive Learning,3.1,Data and Experimental Setup,"Rat Postmortem Histopathology Dataset. Ninety Sprague-Dawley adult male rats were executed by the spinal cord dislocation and placed in a constant temperature and humidity environment for 6-8 h. The animal experiments were approved by the Laboratory Animal Care Committee of the anonymous institution. Seven organs, i.e., brain, heart, kidney, liver, lung, pancreas, and spleen, were removed and placed in the formalin solution. Briefly, paraffin sections of these organ tissues were stained with the H&E solution. The H&E-stained sections were then analyzed by three forensic pathologists, who used Lercai LAS EZ microscopes to record the areas according to their expertise. Overall, five to ten images were recorded from a section at each magnification (i.e., 5×, 10×, 20×, and 40×). Finally, we split the 90 rats as training, validation, and test sets of 60, 10, and 20 rats, respectively, each with 13, 137, 2, 235, and 4, 325 images.Human Forensic Histopathology Dataset. The real forensic images were provided by the Forensic Judicial Expertise Center of the anonymous institution, after getting the informed consent of relatives. All procedures followed the requirements of local laws and institutional guidelines, and were approved and supervised by the Ethics Committee. A total of 32 decedents participated in this study. Four to six images were recorded at each of three magnifications (5×, 10×, and 20×) per H&E stained section. Similar to the rat dataset, the human dataset was selected from the same seven organs. Finally, the training, validation and test sets contain 1, 691 images, 628, and 1059 images, corresponding to 16, 6, and 10 different decedents, respectively.Experimental Details. Notably, the double-tier backbone was self-supervised and learned on the rat training set for 100 epochs by setting the mini-batch size as 1024, with the parameters initialized by the ImageNet pre-trained models. The training data were augmented by a histopathology-oriented strategy by combining different kinds of staining jitters, random affine transformation, Gaussian blurring, resizing, etc. The image(patch) dimension in our implementation was 224*224. The tuning parameters γ and λ in L all were set as 5 and 0.005, respectively. Thereafter, the MIL blocks on two different datasets were both trained by minimizing the cross-entropy loss for 20 epochs with the minibatch size setting as 32. The experiments were conducted on three PCs with twenty NVIDIA GEFORCE RTX 3090 GPUs."
Forensic Histopathological Recognition via a Context-Aware MIL Network Powered by Self-supervised Contrastive Learning,3.2,Results of Self-supervised Contrastive Learning,"Our self-supervised double-tier backbone was compared with other state-ofthe-art self-supervised learning approaches, including balow twins [27], swin transformer (SSL) [26], TransPath [23], CTransPath [24],RetCCL [22] and MOCOV3 [3]. To evaluate the discriminative power of these competing methods, we adopted GAP to aggregate their instance representations from a whole image to train simple linear classifiers for the recognition of seven different organ tissues on both the rat and human datasets, with the test performance quantified in terms of four general classification metrics (i.e., ACC, F1 score, MCC(Matthews Correlation Coefficient), and Precision). The corresponding results are summarized in Table 1, from which we can have two observations. First, our self-supervised double-tier backbone consistently outperformed all other competing methods in terms of all metrics on two datasets. Second, our method led to better generalization, as the backbone trained on the rat dataset shows promising performance on the challenging real-world human dataset (e.g., resulting in an ACC higher than 90%). These results suggest the effectiveness of our self-supervised learning strategy. For a more detailed evaluation, we further conducted a series of ablation studies to evaluate the contributions of the contrastive losses (i.e., L global and L parts ) and regularization strategy (i.e., L var + L cov ). The corresponding results are summarized in Table 2, from which we can see that, given the baseline of L global , both the inclusion of the spatially fine-grained contrastive loss (i.e., L parts ) and informativeness regularization (i.e., L var and L cov ) led to respective performance gains. These results further justify our self-supervised design. "
Forensic Histopathological Recognition via a Context-Aware MIL Network Powered by Self-supervised Contrastive Learning,3.3,Results of Multiple-Instance Learning,"Based upon the double-tier backbone learned on the rat training set, we compared our context-aware MIL with other MIL methods, including the gated attention-based approach (i.e., AB-MIL [12], DSMIL [15], Transmil [19] and MSA [2,16]) approaches with/without different positional embedding strategies, i.e., relative position embedding (MSA-RP [17]), learnable position embedding (MSA-LP [7]), and 2D sine-cosine position embedding (MSA-SP [8]). Notably, our approach used MSA-RP as the baseline, based on which an adaptive pooling operation is designed to produce the final bag-level representation. To check the efficacy of adaptive pool, we further conducted a corresponding set of ablation studies by replacing it with other operations, including max pool, and soft pool [20]. These comparison and ablations results are shown in Table 3, from which we can observe that our method led to the best results on both datasets, with relatively more significant improvements on the challenging human dataset. Also, compared with other pooling operations, the adaptive pool design brought consistent performance gains. These results suggest the efficacy of our contextaware MIL for postmortem tissue recognition.In addition, we conducted LayerCAM-based analysis [13] to check the explainability and reliability of our postmortem histopathological recognition results. From the representative examples shown in Fig. 2, we can have an interesting observation that our method tends to focus on tissue-specific postmortem patterns at different microscopic scales. For example, the spatial attention maps reliably highlighted the meningeal structures of the brain tissue, the glomeruli in the kidney cortex, and the central vein area between the liver lobules. On  the other hand, based on the pancreas example, we can see that our network can sensitively localize the pancreas glandular structure while filtering out the uninformative background in an end-to-end fashion, without the need for any pre-processing to segment first the foreground. These observations support our assumption that the proposed method is reliable and efficient in learning discriminative histopathological representations of postmortem organ tissues."
Forensic Histopathological Recognition via a Context-Aware MIL Network Powered by Self-supervised Contrastive Learning,4,Conclusion,"In this study, we have proposed a context-aware MIL framework powered by self-supervised contrastive learning to learn fine-grained discriminative representations for postmortem histopathological recognition. The dedicated selfsupervised learning strategy concurrently maximizes multiple contrastive losses and regularization terms to deduce informative and discriminative instance embedding. Thereafter, the context-aware MIL framework adopts MSA followed by an adaptive pooling operation to distill from all instances a holistic bag/image-level representation. The experimental results on a relatively largescale database suggest the state-of-the-art postmortem recognition performance of our method."
Forensic Histopathological Recognition via a Context-Aware MIL Network Powered by Self-supervised Contrastive Learning,,Fig. 1 .,
Forensic Histopathological Recognition via a Context-Aware MIL Network Powered by Self-supervised Contrastive Learning,,Fig. 2 .,
Forensic Histopathological Recognition via a Context-Aware MIL Network Powered by Self-supervised Contrastive Learning,,Table 1 .,
Forensic Histopathological Recognition via a Context-Aware MIL Network Powered by Self-supervised Contrastive Learning,,Table 2 .,
Forensic Histopathological Recognition via a Context-Aware MIL Network Powered by Self-supervised Contrastive Learning,,Table 3 .,
ProtoASNet: Dynamic Prototypes for Inherently Interpretable and Uncertainty-Aware Aortic Stenosis Classification in Echocardiography,1,Introduction,"Aortic stenosis (AS) is a common heart valve disease characterized by the calcification of the aortic valve (AV) and the restriction of its movement. It affects 5% of individuals aged 65 or older [2] and can progress rapidly from mild or moderate to severe, reducing life expectancy to 2 to 3 years [20]. Echocardiography (echo) is the primary diagnostic modality for AS. This technique measures Doppler-derived clinical markers [16] and captures valve motion from the parasternal long (PLAX) and short axis (PSAX) cross-section views. However, obtaining and interpreting Doppler measurements requires specialized training and is subject to significant inter-observer variability [14,15].To alleviate this issue, deep neural network (DNN) models have been proposed for automatic assessment of AS directly from two-dimensional B-mode echo, a modality more commonly used in point-of-care settings. Huang et al. [9,10] proposed a multitask model to classify the severity of AS using echo images. Ginsberg et al. [6] proposed an ordinal regression-based method that predicts the severity of AS and provides an estimate of aleatoric uncertainty due to uncertainty in training labels. However, these works utilized black-box DNNs, which could not provide an explanation of their prediction process.Explainable AI (XAI) methods can provide explanations of a DNN's decision making process and can generally be categorized into two classes. Post-hoc XAI methods explain the decisions of trained black-box DNNs. For example, gradientbased saliency maps [18,19] show where a model pays attention to, but these methods do not necessarily explain why one class is chosen over another [17], and at times result in misleading explanations [1]. Ante-hoc XAI methods are explicitly designed to be explainable. For instance, prototype-based models [4,8,11,12,22,23], which the contributions of our paper fall under, analyze a given input based on its similarity to learned discriminative features (or ""prototypes"") for each class. Both the learned prototypes and salient image patches of the input can be visualized for users to validate the model's decision making.There are two limitations to applying current prototype-based methods to the task of classifying AS severity from echo cine series. First, prototypes should be spatio-temporal instead of only spatial, since AS assessment requires attention to small anatomical regions in echo (such as the AV) at a particular phase of the heart rhythm (mid-systole). Second, user variability in cardiac view acquisition and poor image quality can complicate AV visualization in standard PLAX and PSAX views. The insufficient information in such cases can lead to more plausible diagnoses than one. Therefore, a robust solution should avoid direct prediction and notify the user. These issues have been largely unaddressed in previous work.We propose ProtoASNet (Fig. 1), a prototype-based model for classifying AS severity from echo cine series. ProtoASNet discovers dynamic prototypes that describe shape-and movement-based phenomena relevant to AS severity, outperforming existing models that only utilize image-based prototypes. Additionally, our model can detect ambiguous decision-making scenarios based on similarity with less informative samples in the training set. This similarity is expressed as a measure of aleatoric uncertainty. To the best of our knowledge, the only prior work for dynamic prototypes published to-date is [7]. ProtoASNet is the first work to use dynamic prototypes in medical imaging and the first to incorporate aleatoric uncertainty estimation with prototype-based networks."
ProtoASNet: Dynamic Prototypes for Inherently Interpretable and Uncertainty-Aware Aortic Stenosis Classification in Echocardiography,2,Methods,
ProtoASNet: Dynamic Prototypes for Inherently Interpretable and Uncertainty-Aware Aortic Stenosis Classification in Echocardiography,2.1,Background: Prototype-Based Models,"Prototype-based models explicitly make their decisions using similarities to cases in the training set. These models generally consist of three key components structured as h(g(f (x))). Firstly, f (.) is a feature encoder such as a ConvNet that maps images x ∈ R Ho×Wo×3 to f (x) ∈ R H×W ×D , where H, W , and D correspond to the height, width, and feature depth of the ConvNet's intermediate layer, respectively. Secondly, g(.) ∈ R H×W ×D → R P is a prototype pooling function that computes the similarity of encoded features f (x) to P prototype vectors. There are K learnable prototypes defined for each of C classes, denoted as p c k . Finally, h(.) ∈ R P → R C is a fully-connected layer that learns to weigh the input-prototype similarities against each other to produce a prediction score for each class. To ensure that the prototypes p Such models are inherently interpretable since they are enforced to first search for similar cases in the training set and then to compute how these similarities contribute to the classification. As a result, they offer a powerful approach for identifying and classifying similar patterns in data."
ProtoASNet: Dynamic Prototypes for Inherently Interpretable and Uncertainty-Aware Aortic Stenosis Classification in Echocardiography,2.2,ProtoASNet,"Feature Extraction. The overall structure of ProtoASNet is shown in Fig. 1.The feature extraction layer consists of a convolutional backbone, in our case the first three blocks of a pre-trained R(2+1)D-18 [21] model, followed by two branches of feature and region of interest (ROI) modules made up of two and three convolutional layers respectively. In both modules, the convolutional layers have ReLU activation function, except the last layers which have linear activations. Given an input video x ∈ R Ho×Wo×To×3 with T o frames, the first branch learns a feature F (x) ∈ R H×W ×T ×D , where each D-dimensional vector in F (x) corresponds to a specific spatio-temporal region in the video. The second branch generates P regions of interest, M p c k (x) ∈ R H×W ×T , that specify which regions of F (x) are relevant for comparing with each prototype p c k . The features from different spatio-temporal regions must be pooled before being compared to prototypes. As in [12], we perform a weighted average pooling with the learned regions of interest as follows:where |.| is the absolute value and • is the Hadamard product.Prototype Pooling. The similarity score of a feature vector f p c k and prototype p c k is calculated using cosine similarity, which is then shifted to [0, 1]:).(Prototypes for Aleatoric Uncertainty Estimation. In Fig. 1, trainable uncertainty prototypes (denoted p u k ) are added to capture regions in the data distribution that are inherently ambiguous (Fig. 1.B). We use similarity between f p u k (x) and p u k to quantify aleatoric uncertainty, denoted α ∈ [0, 1]. We use an ""abstention loss"" (Eq. ( 6)) method inspired by [5] to learn α and thereby p u k . In this loss, α is used to interpolate between the ground truth and prediction, pushing the model to ""abstain"" from its own answer at a penalty.where σ denotes Softmax normalization in the output of h(.), y and ŷ are the ground truth and the predicted probabilities, respectively, and λ abs is a regularization constant.When projecting p u k to the nearest extracted feature from training examples, we relax the requirement in Eq. ( 1) allowing the uncertainty prototypes to be pushed to data with the ground truth of any AS severity class.Class-Wise Similarity Score. The fully connected (FC) layer h(.) is a dense mapping from prototype similarity scores to prediction logits. Its weights, w h , are initialized to be 1 between class c and the corresponding prototypes and 0 otherwise to enforce the process to resemble positive reasoning. h(.) produces a score for membership in each class and for α.Loss Function. As in previous prototype-based methods [4,12], the following losses are introduced to improve performance: 1) Clustering and separation losses (Eq. ( 7)), which encourage clustering based on class, where P y denotes the set of prototypes belonging to class y. Due to lack of ground truth uncertainties, these losses are only measured on p c k , not p u k ; 2) Orthogonality loss (Eq. ( 8)), which encourages prototypes to be more diverse; 3) Transformation loss L trns (described in [12]), which regularizes the consistency of the predicted occurrence regions under random affine transformations; 4) Finally, L norm (described in [4]) regularizes w h to be close to its initialization and penalizes relying on similarity to one class to influence the logits of other classes. Equation ( 9) describes the overall loss function where λ represent regularization coefficients for each loss term. The network is trained end-to-end. We conduct a ""push"" stage (see Eq. ( 1)) every 5 epochs to ensure that the learned prototypes are consistent with the embeddings from real examples.3 Experiments and Results"
ProtoASNet: Dynamic Prototypes for Inherently Interpretable and Uncertainty-Aware Aortic Stenosis Classification in Echocardiography,3.1,Datasets,"We conducted experiments on a private AS dataset and the public TMED-2 dataset [10]. The private dataset was extracted from an echo study database of a tertiary care hospital with institutional review ethics board approval. Videos were acquired with Philips iE33, Vivid i, and Vivid E9 ultrasound machines. For each study, the AS severity was classified using clinically standard Doppler echo guidelines [3] by a level III echocardiographer, keeping only cases with concordant Doppler measurements. PLAX and PSAX view cines were extracted from each study using a view-detection algorithm [13], and subsequently screened by a level III echocardiographer to remove misclassified cines. For each cine, the echo beam area was isolated and image annotations were removed. The dataset consists of 5055 PLAX and 4062 PSAX view cines, with a total of 2572 studies. These studies were divided into training, validation, and test sets, ensuring patient exclusivity and following an 80-10-10 ratio. We performed randomized augmentations including resized cropping and rotation. The TMED-2 dataset [10] consists of 599 fully labeled echo studies containing 17270 images in total. Each study consists of 2D echo images with clinicianannotated view labels (PLAX/PSAX/Other) and Doppler-derived study-level AS severity labels (no AS/early AS/significant AS). Though the dataset includes an unlabeled portion, we trained on the labeled set only. We performed data augmentation similar to the private dataset without time-domain operations."
ProtoASNet: Dynamic Prototypes for Inherently Interpretable and Uncertainty-Aware Aortic Stenosis Classification in Echocardiography,3.2,Implementation Details,"To better compare the results with TMED-2 dataset, we adopted their labeling scheme of no AS (normal), early AS (mild), and significant AS (moderate and severe) in our private dataset. We split longer cines into 32-frame clips which are approximately one heart cycle long. In both layers of the feature module, we used D convolutional filters, while the three layers in the ROI module had D, D  2 , and P convolutional filters, preventing an abrupt reduction of channels to the relatively low value of P . In both modules, we used kernel size of 1×1×1. We set D = 256 and K = 10 for AS class and aleatoric uncertainty prototypes. Derived from the hyperparameter selection of ProtoPNet [4], we assigned the values of 0.8, 0.08, and 10 -4 to λ clst , λ sep , and λ norm respectively. Through a search across five values of 0.1, 0.3, 0.5, 0.9, and 1.0, we found the optimal λ abs to be 0.3 based on the mean F1 score of the validation set. Additionally, we found λ orth and λ trns to be empirically better as 10 -2 and 10 -3 respectively. We implemented our framework in PyTorch and trained the model end-to-end on one 16 GB NVIDIA Tesla V100 GPU."
ProtoASNet: Dynamic Prototypes for Inherently Interpretable and Uncertainty-Aware Aortic Stenosis Classification in Echocardiography,3.3,Evaluations on Private Dataset,"Quantitative Assessment. In Table 1, we report the performance of ProtoAS-Net in AS severity classification against the black-box baselines for image (Huang et al. [9]), video (Ginsberg et al. [6]), as well as other prototypical methods, i.e. ProtoPNet [4] and XProtoNet [12]. In particular, for ProtoASNet, ProtoPNet [4], and XProtoNet [12], we conduct both image-based and video-based experiments with ResNet-18 and R(2+1)D-18 backbones respectively. We apply softmax to normalize the ProtoASNet output scores, including α, to obtain class probabilities that account for the presence of aleatoric uncertainty. We aggregate model predictions by averaging their probabilities from the image-(or clip-) level to obtain cine-and study-level predictions. We believe the uncertainty probabilities reduce the effect of less informative datapoints on final aggregated results. Additionally, the video-based models perform better than the image-based ones because the learnt prototypes can also capture AV motion which is an indicator of AS severity. These two factors may explain why our proposed method, ProtoASNet, outperforms all other methods for study-level classification.Table 1. Quantitative results on the test set of our private dataset in terms of balanced accuracy (bACC), mean F1 score, and balanced mean absolute error (bMAE). bMAE is the average of the MAE of each class, assuming labels of 0, 1, 2 for no AS, early AS and significant AS respectively. Study-level results were calculated by averaging the prediction probabilities over all cines of each study. Results are shown as""mean(std)"" calculated across five repetitions for each experiment. Best results are in bold. Qualitative Assessment. The interpretable reasoning process of ProtoASNet for a video example is shown in Fig. 2. We observe that ProtoASNet places significant importance on prototypes corresponding to thickened AV leaflets due to calcification, which is a characteristic of both early and significant AS. Additionally, prototypes mostly capture the part of the heart cycle that aligns with the opening of the AV, providing a clinical indication of how well the valve opens up to be able to pump blood to the rest of the body. This makes ProtoASNet's reasoning process interpretable for the user. Note how the uncertainty prototypes focusing on AV regions where the valve leaflets are not visible, are contributing to the uncertainty measure, resulting in the case being flagged as uncertain."
ProtoASNet: Dynamic Prototypes for Inherently Interpretable and Uncertainty-Aware Aortic Stenosis Classification in Echocardiography,,Method,"Ablation Study. We assessed the effect of removing distinct components of our design: uncertainty prototypes (L abs , p u k ), clustering and separation (L clst , L sep ), and push mechanism. As shown in Table 2, keeping all the aforementioned components results in superior performance in terms of bACC and bMAE. We evaluated whether the model is capable of detecting its own misclassification using the value of α (or entropy of the class predictions in the case without L abs , p u k ). This is measured by the AUROC of detecting (y = ŷ). Learning p u k may benefit accuracy by mitigating the overfitting of p c k to poor-quality videos. Furthermore, α seems to be a stronger indicator for misclassification than entropy. Moreover, we measured prototype quality using diversity and sparsity [8], normalized by the total number of prototypes. Ideally, each prediction can be explained by a low number of prototypes (low s spars ) but different predictions are explained with different prototypes (high Diversity). When L clst and L sep are removed, the protoypes are less constrained, which contributes to stronger misclassification detection and more diversity, but reduce accuracy and cause explanations to be less sparse. Finally, the push mechanism improves performance, countering the intuition of an interpretability-performance trade-off. "
ProtoASNet: Dynamic Prototypes for Inherently Interpretable and Uncertainty-Aware Aortic Stenosis Classification in Echocardiography,3.4,"Evaluation on TMED-2, a Public Dataset","We also applied our method to TMED-2, a public image-based dataset for AS diagnosis. Consistent with [10], images were fed to a WideResNet-based prototype model with two output branches. The view classifier branch used averagepooling of patches followed by a fully connected layer. However, the AS diagnosis branch used the prototype setup outlined in Methods. A diagram of the overall architecture is available in the supplementary material. We trained the model end-to-end with images from all views. During inference, images with high entropy in the predicted view and high aleatoric uncertainty for AS classification were discarded. Then, probabilities for PLAX and PSAX were used for weighted averaging to determine the study-level prediction. Addition of the prototypical layer and thresholding on predicted uncertainty achieves 79.7% accuracy for AS severity, outperforming existing black-box method [10] at 74.6%. "
ProtoASNet: Dynamic Prototypes for Inherently Interpretable and Uncertainty-Aware Aortic Stenosis Classification in Echocardiography,4,Conclusion,"We introduce ProtoASNet, an interpretable method for classifying AS severity using B-mode echo that outperforms existing black-box methods. ProtoASNet identifies clinically relevant spatio-temporal prototypes that can be visualized to improve algorithmic transparency. In addition, we introduce prototypes for estimating aleatoric uncertainty, which help flag difficult-to-diagnose scenarios, such as videos with poor visual quality. Future work will investigate methods to optimize the number of prototypes, or explore out-of-distribution detection using prototype-based methods."
ProtoASNet: Dynamic Prototypes for Inherently Interpretable and Uncertainty-Aware Aortic Stenosis Classification in Echocardiography,,Fig. 1 .,
ProtoASNet: Dynamic Prototypes for Inherently Interpretable and Uncertainty-Aware Aortic Stenosis Classification in Echocardiography,,,
ProtoASNet: Dynamic Prototypes for Inherently Interpretable and Uncertainty-Aware Aortic Stenosis Classification in Echocardiography,,Fig. 2 .,
ProtoASNet: Dynamic Prototypes for Inherently Interpretable and Uncertainty-Aware Aortic Stenosis Classification in Echocardiography,,Table 2 .,
ProtoASNet: Dynamic Prototypes for Inherently Interpretable and Uncertainty-Aware Aortic Stenosis Classification in Echocardiography,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 36.
HIGT: Hierarchical Interaction Graph-Transformer for Whole Slide Image Analysis,1,Introduction,"Histopathology is considered the gold standard for diagnosing and treating many cancers [19]. The tissue slices are usually scanned into Whole Slide Images (WSIs) and serve as important references for pathologists. Unlike natural images, WSIs typically contain billions of pixels and also have a pyramid structure, as shown in Fig. 1. Such gigapixel resolution and expensive pixel-wise annotation efforts pose unique challenges to constructing effective and accurate models for WSI analysis. To overcome these challenges, Multiple Instance Learning (MIL) has become a popular paradigm for WSI analysis. Typically, MIL-based WSI analysis methods have three steps: (1) crop the huge WSI into numerous image patches; (2) extract instance features from the cropped patches; and (3) aggregate instance features together to obtain slide-level prediction results. Many advanced MIL models emerged in the past few years. For instance, ABMIL [9] and DeepAttnMIL [18] incorporated attention mechanisms into the aggregation step and achieved promising results. Recently, Graph-Transformer architecture [17] has been proposed to learn short-range local features through GNN and long-range global features through Transformer simultaneously. Such Graph-Transformer architecture has also been introduced into WSI analysis [15,20] to mine the thorough global and local correlations between different image patches. However, current Graph-Transformer-based WSI analysis models only consider the representation learning under one specific magnification, thus ignoring the rich multi-resolution information from the WSI pyramids.Different resolution levels in the WSI pyramids contain different and complementary information [3]. The images at a high-resolution level contain cellularlevel information, such as the nucleus and chromatin morphology features [10]. At a low-resolution level, tissue-related information like the extent of tumorimmune localization can be found [1], while the whole WSI describes the entire tissue microenvironment, such as intra-tumoral heterogeneity and tumor invasion [3]. Therefore, analyzing from only a single resolution would lead to an incomplete picture of WSIs. Some very recent works proposed to characterize and analyze WSIs in a pyramidal structure. H2-MIL [7] formulated WSI as a hierarchical heterogeneous graph and HIPT [3] proposed an inheritable ViT framework to model WSI at different resolutions. Whereas these methods only characterize local or global correlations within the WSI pyramids and use only unidirectional interaction between different resolutions, leading to insufficient capability to model the rich multi-resolution information of the WSI pyramids.In this paper, we present a novel Hierarchical Interaction Graph-Transformer framework (i.e., HIGT) to simultaneously capture both local and global information from WSI pyramids with a novel Bidirectional Interaction module. Specifically, we abstract the multi-resolution WSI pyramid as a heterogeneous hierarchical graph and devise a Hierarchical Interaction Graph-Transformer architecture to learn both short-range and long-range correlations among different image patches within different resolutions. Considering that the information from different resolutions is complementary and can benefit each other, we specially design a Bidirectional Interaction block in our Hierarchical Interaction ViT mod- ule to establish communication between different resolution levels. Moreover, a Fusion block is proposed to aggregate features learned from the different levels for slide-level prediction. To reduce the tremendous computation and memory cost, we further adopt the efficient pooling operation after the hierarchical GNN part to reduce the number of tokens and introduce the Separable Self-Attention Mechanism in Hierarchical Interaction ViT modules to reduce the computation burden. The extensive experiments with promising results on two public WSI datasets from TCGA projects, i.e., kidney carcinoma (KICA) and esophageal carcinoma (ESCA), validate the effectiveness and efficiency of our framework on both tumor subtyping and staging tasks. The codes are available at https:// github.com/HKU-MedAI/HIGT."
HIGT: Hierarchical Interaction Graph-Transformer for Whole Slide Image Analysis,2,Methodology,"Figure 1 depicts the pipeline of HIGT framework for better exploring the multiscale information in hierarchical WSI pyramids. First, we abstract each WSI as a hierarchical graph, where the feature embeddings extracted from multiresolution patches serve as nodes and the edge denotes the spatial and scaling relationships of patches within and across different resolution levels. Then, we feed the constructed graph into several hierarchical graph convolution blocks to learn the short-range relationship among graph nodes, following pooling operations to aggregate local context and reduce the number of nodes. We further devise a Separable Self-Attention-based Hierarchical Interaction Transformer architecture equipped with a novel Bidirectional Interaction block to learn the long-range relationship among graph nodes. Finally, we design a fusion block to aggregate the features learned from the different levels of WSI pyramids for final slide-level prediction."
HIGT: Hierarchical Interaction Graph-Transformer for Whole Slide Image Analysis,2.1,Graph Construction,"As shown in Fig. 1, a WSI is cropped into numerous non-overlapping 512 × 512 image patches under different magnifications (i.e., ×5, ×10) by using a sliding window strategy, where the OTSU algorithm [4] is used to filter out the background patches. Afterwards, we employ a pre-trained KimiaNet [16] to extract the feature embedding of each image patch. The feature embeddings of the slide-level T (Thumbnail), region-level R (×5), and the patch-level P (×10) can be represented as,where t, r i , p i,j ∈ R 1×C correspond to the feature embeddings of each patch in thumbnail, region, and patch levels, respectively. N is the total number of the region nodes and M is the number of patch nodes belonging to a certain region node, and C denotes the dimension of feature embedding (1,024 in our experiments). Based on the extracted feature embeddings, we construct a hierarchical graph to characterize the WSI, following previous H 2 -MIL work [7]. Specifically, the cropped patches serve as the nodes of the graph and we employ the extracted feature embedding as the node embeddings. There are two kinds of edges in the graph: spatial edges to denote the 8-adjacent spatial relationships among different patches in the same levels, and scaling edges to denote the relationship between patches across different levels at the same location."
HIGT: Hierarchical Interaction Graph-Transformer for Whole Slide Image Analysis,2.2,Hierarchical Graph Neural Network,"To learn the short-range relationship among different patches within the WSI pyramid, we propose a new hierarchical graph message propagation operation, called RAConv+. Specifically, for any source node j in the hierarchical graph, we define the set of it all neighboring nodes at resolution k as N k and k ∈ K.Here K means all resolutions. And the h k is the mean embedding of the node j's neighboring nodes in resolution k. And h j is the embedding of the neighboring nodes of node j in resolution k and h j ∈ N k . The formula for calculating the attention score of node j in resolution-level and node-level:where α j,j is the attention score of the node j to node j and h j is the source node j embedding. And U , V , a and b are four learnable layers. The main difference from H2-MIL [6] is that we pose the non-linear LeakyReLU between a and U , b and V , to generate a more distinct attention score matrix which increases the feature differences between different types of nodes [2]. Therefore, the layer-wise graph message propagation can be represented as:where A represents the attention score matrix, and the attention score for the j-th row and j -th column of the matrix is given by Eq. ( 2). At the end of the hierarchical GNN part, we use the IHPool [6] progressively aggregate the hierarchical graph."
HIGT: Hierarchical Interaction Graph-Transformer for Whole Slide Image Analysis,2.3,Hierarchical Interaction ViT,"We further propose a Hierarchical Interaction ViT (HIViT) to learn long-range correlation within the WSI pyramids, which includes three key components: Patch-level (PL) blocks, Bidirectional Interaction (BI) blocks, and Region-level (RL) blocks."
HIGT: Hierarchical Interaction Graph-Transformer for Whole Slide Image Analysis,,Patch-Level Block.,"Given the patch-level feature set P = N i=1 P i , the PL block learns long-term relationships within the patch level:where l = 1, 2, ..., L is the index of the HIViT block. P L(•) includes a Separable Self Attention (SSA) [13], 1×1 Convolution, and Layer Normalization in sequence. Note that here we introduced SSA into the PL block to reduce the computation complexity of attention calculation from quadratic to linear while maintaining the performance [13].Bidirectional Interaction Block. We propose a Bidirectional Interaction (BI) block to establish communication between different levels within the WSI pyramids. The BI block performs bidirectional interaction, and the interaction progress from region nodes to patch nodes is:where the SE(•) means the Sequeeze-and-Excite layer [8] and the r l i means the i-th region node in R l , and pl+1 i,k is the k-th patch node linked to the i-th region node after the interaction. Besides, another direction of the interaction is,where the MEAN(•) is the operation to get the mean value of patch nodes set P l+1 i associated with the i-th region node and P l+1 1 ∈ R 1×C and the C is the feature channel of nodes, and Rl+1 is the region nodes set after interaction.Region-Level Block. The final part of this module is to learn the long-range correlations of the interacted region-level nodes:where l = 1, 2, ..., L is the index of the HIViT module,and RL(•) has a similar structure to P L(•)."
HIGT: Hierarchical Interaction Graph-Transformer for Whole Slide Image Analysis,2.4,Slide-Level Prediction,"In the final stage of our framework, we design a Fusion block to combine the coarse-grained and fine-grained features learned from the WSI pyramids. Specifically, we use an element-wise summation operation to fuse the coarse-grained thumbnail feature and patch-level features from the Hierarchical Interaction GNN part, and then further fuse the fine-grained patch-level features from the HIViT part with a concatenation operation. Finally, a 1 × 1 convolution and mean operation followed by a linear projection are employed to produce the slide-level prediction."
HIGT: Hierarchical Interaction Graph-Transformer for Whole Slide Image Analysis,3,Experiments,"Datasets and Evaluation Metrics. We assess the efficacy of the proposed HIGT framework by testing it on two publicly available datasets (KICA and ESCA) from The Cancer Genome Atlas (TCGA) repository. The datasets are described below in more detail:-KICA dataset. The KICA dataset consists of 371 cases of kidney carcinoma, of which 279 are classified as early-stage and 92 as late-stage. For the tumor typing task, 259 cases are diagnosed as kidney renal papillary cell carcinoma, while 112 cases are diagnosed as kidney chromophobe. -ESCA dataset. The ESCA dataset comprises 161 cases of esophageal carcinoma, with 96 cases classified as early-stage and 65 as late-stage. For the tumor typing task, there are 67 squamous cell carcinoma cases and 94 adenocarcinoma cases.Experimental Setup. The proposed framework was implemented by PyTorch [14] and PyTorch Geometric [5]. All experiments were conducted on a workstation with eight NVIDIA GeForce RTX 3090 (24 GB) GPUs. The shape of all nodes' features extracted by KimiaNet is set to 1 × 1024. All methods are trained with a batch size of 8 for 50 epochs. The learning rate was set as 0.0005, with Adam optimizer. The accuracy (ACC) and area under the curve (AUC) are used as the evaluation metric. All approaches were evaluated with five-fold cross-validations (5-fold CVs) from five different initializations. Comparison with State-of-the-Art Methods. We first compared our proposed HIGT framework with two groups of state-of-the-art WSI analysis methods: (1) non-hierarchical methods including: ABMIL [9], CLAM-SB [12], Deep-AttnMIL [18], DS-MIL [11], LA-MIL [15], and (2) hierarchical methods including: H2-MIL [7], HIPT [3]. For LA-MIL [15] method, it was introduced with a single-scale Graph-Transformer architecture. For H2-MIL [7] and HIPT [3], they were introduced with a hierarchical Graph Neural Network and hierarchical Transformer architecture, respectively. The results for ESCA and KICA datasets are summarized in Table 1 and Table 2, respectively. Overall, our model achieves a content result both in AUC and ACC of classifying the WSI, and especially in predicting the more complex task (i.e. Staging) compared with the SOTA approaches. Even for the non-hierarchical Graph-Transformer baseline LA-MIL and hierarchical transformer model HIPT, our model approaches at least around 3% and 2% improvement on AUC and ACC in the classification of the Staging of the KICA dataset. Therefore we believe that our model benefits a lot from its used modules and mechanisms.Ablation Analysis. We further conduct an ablation study to demonstrate the effectiveness of the proposed components. The results are shown in Table 3. In its first row, we replace the RAConv+ with the original version of this operation. And in the second row, we replace the Separable Self Attention with a canonical transformer block. The third row changes the bidirectional interaction mechanism into just one direction from region-level to patch-level. And the last row, we remove the fusion block from our model. Finally, the ablation analysis results show that all of these modules we used actually improved the prediction effect of the model to a certain extent. Computation Cost Analysis. We analyze the computation cost during the experiments to compare the efficiency between our methods and existing state-ofthe-art approaches. Besides we visualized the model size (MB) and the training memory allocation of GPU (GB) v.s. performance in KICA's typing and staging task plots in Fig. 2. All results demonstrate that our model is able to maintain the promising prediction result while reducing the computational cost and model size effectively."
HIGT: Hierarchical Interaction Graph-Transformer for Whole Slide Image Analysis,4,Conclusion,"In this paper, we propose HIGT, a framework that simultaneously and effectively captures local and global information from the hierarchical WSI. Firstly, the constructed hierarchical data structure of the multi-resolution WSI is able to offer multi-scale information to the later model. Moreover, the redesigned H2-MIL and HIViT capture the short-range and long-range correlations among varying magnifications of WSI separately. And the bidirectional interaction mechanism and fusion block can facilitate communication between different levels in the Transformer part. We use IHPool and apply the Separable Self Attention to deal with the inherently high computational cost of the Graph-Transformer model. Extensive experimentation on two public WSI datasets demonstrates the effectiveness and efficiency of our designed framework, yielding promising results. In the future, we will evaluate on other complex tasks such as survival prediction and investigate other techniques to improve the efficiency of our framework."
HIGT: Hierarchical Interaction Graph-Transformer for Whole Slide Image Analysis,,Fig. 1 .,
HIGT: Hierarchical Interaction Graph-Transformer for Whole Slide Image Analysis,,Table 1 .,
HIGT: Hierarchical Interaction Graph-Transformer for Whole Slide Image Analysis,,Table 2 .,
HIGT: Hierarchical Interaction Graph-Transformer for Whole Slide Image Analysis,,Table 3 .,
HIGT: Hierarchical Interaction Graph-Transformer for Whole Slide Image Analysis,,.90 ± 0.60 97.90 ± 1.40 Fig,". 2. Computational analysis of our framework and some selected SOTA methods. From left to right are scatter plots of Typing AUC v.s. GPU Memory Allocation, Staging AUC v.s. GPU Memory Allocation, Typing AUC v.s. Model Size, Staging AUC v.s. Model Size."
HIGT: Hierarchical Interaction Graph-Transformer for Whole Slide Image Analysis,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 73.
Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,1,Introduction,"Image classification is a significant challenge in medical image analysis. Although some classification methods achieve promising performance on balanced and clean medical datasets, balanced datasets with high-accuracy annotations are time-consuming and expensive. Besides, pruning clean and balanced datasets require a large amount of crucial clinical data, which is insufficient for large-scale deep learning. Therefore, we focus on a more practical yet unexplored setting for handling imbalanced medical data with noisy labels, utilizing all available lowcost data with possible noisy annotations. Noisy imbalanced datasets arise due to the lack of high-quality annotations [11] and skewed data distributions [18] where the number of instances largely varies across different classes. Besides, the class hardness problem where classification difficulties vary for different categories presents another challenge in removing label noise. Due to differences in disease epidemicity and collection difficulty, rare anomalies or anatomical features render diseases with low epidemicity easier to detect. However, existing techniques [12,23,24] fail to jointly address these scenarios, leading to inadequate classification outcomes. Therefore, noisy-labeled, imbalanced datasets with various class hardness remain a persistent challenge in medical classification.Existing approaches for non-ideal medical image classification can be summarized into noisy classification, imbalanced recognition, and noisy imbalanced identification. Noisy classification approaches [3,7,23] conduct noise-invariant learning depending on the big-loss hypothesis, where classifiers trained with clean data with lower empirical loss aid with de-noising identification. However, imbalanced data creates different confidence distributions of clean and noisy data in the majority class and minority class as shown in Fig. 1, which invalidates the big-loss assumption [3,4]. Imbalanced recognition approaches [9,15,21] utilize augmented embeddings and imbalance-invariant training loss to re-balance the long-tailed medical data artificially, but the disturbance from noisy labels leads to uncasual feature learning, impeding the recognition of tail classes. Noisy longtailed identification technique [25] has achieved promising results by addressing noise and imbalance concerns sequentially. However, the class hardness problem leads to vague decision boundaries that hinders accurate' noise identification.In this work, we propose a multi-stage noise removal framework to address these concerns jointly. The main contributions of our work include: 1) We decompose the negative effects in practical medical image classification, 2) We minimize the invariant risk to tackle noise identification influenced by multiple factors, enabling the classifier to learn causal features and be distribution-invariant, 3) A re-scaling class-aware Gaussian Mixture Modeling (CGMM) approach is proposed to distinguish noise labels under various class hardness, 4) We evaluate our method on two medical image datasets, and conduct thorough ablation studies to demonstrate our approach's effectiveness."
Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,2,Method,
Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,2.1,Problem Formulation,"In the noisy imbalanced classification setting, we denote a medical dataset as {(x i , y i )} N i=1 where y i is the corresponding label of data x i and N is the total amount of instances. Here y i may be noisy. Further, we split the dataset according to class categories. Then, we have {D j } M j=1 , where M is the number of classes; D j denotes the subset for class j. In each subset containing N j samples, the data pairs are expressed as {(x j i , y j i )} Nj i=1 . Without loss of generality, we order the classes as N 1 > N 2 > ... > N M -1 > N M . Further, we denote the backbone as H(•; θ), X → Z mapping data manifold to the latent manifold, the classifier head as G(•; γ), Z → C linking latent space to the category logit space, and the identifier as F(•; φ), Z → C. We aim to train a robust medical image classification model composed of a representation backbone and a classifier head on label noise and imbalance distribution, resulting in a minimized loss on the testing dataset:"
Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,2.2,Mapping Correction Decomposition,"We decompose the non-linear mapping p(y = c|x) as a product of two space mappings p G (y = c|z) • p H (z|x). Given that backbone mapping is independent of noisy imbalanced effects, we conduct further disentanglement by defining e as the negative effects and P as constant for fixed probability mappings:The induction derives from the assumption that the incorrect mapping p G (y = c|z, e) conditions on both pure latent to logits mapping p G (y = c|z) and adverse effects p G (y = c|e). By Bayes theorem, we decompose the effect into imbalance, noise, and mode (hardness), where the noise effect depends on skew distribution and hardness effect; and the hardness effect is noise-invariant. Currently, noise removal methods only address pure noise effects (p G (e n |y = c)), while imbalance recognition methods can only resolve imbalanced distribution, which hinders the co-removal of adverse influences. Furthermore, the impact of hardness effects has not been considered in previous studies, which adds an extra dimension to noise removal. In essence, the fundamental idea of noisy classification involves utilizing clean data for classifier training, which determines the importance of noise identification and removal. To address these issues, we propose a mapping correction approach that combines independent noise detection and removal techniques to identify and remove noise effectively."
Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,2.3,Minimizing Invariant Risk Across Multi-distributions,"Traditional learning with noisy label methods mainly minimize empirical risk on training data. However, they fail to consider the influence of imbalanced distributions, which might cause a biased gradient direction on the optimization subspace. Following [25], we minimize the invariant risk [2] across multi-environment for independent detector learning. By assuming that the robust classifier performs well on every data distribution, we solve the optimizing object by finding the optima to reduce the averaged distance for gradient shift: minwhere ε represents an environment (distribution) for classifier F φ and backbone H θ ; and L denotes the empirical loss for classification. Since the incorrect mapping is not caused by feature representation, the backbone H θ is fixed during the optimization. By transferring the constraints into a penalty in the optimizing object, we solve this problem by learning the constraint scale ω [2]:Ideally, the noise removal process is distribution-invariant if data is uniformly distributed w.r.t. classes. By the law of large numbers, all constructed distributions should be symmetric according to the balanced distribution to obtain a uniform expectation. To simplify this assumption, we construct three different data distributions [25] composed of one uniform distribution and two symmetric skewed distributions instead of theoretical settings. In practice, all environments are established from the training set with the same class categories."
Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,2.4,Rescaling Class-Aware Gaussian Mixture,"Existing noise labels learning methods [1,13] cluster all sample loss or confidence scores with Beta Mixture Model or Gaussian Mixture Model into noisy and clean distributions. From the perspective of clustering, definite and immense gaps between two congregate groups contribute to more accurate decisions. However, in medical image analysis, an overlooked mismatch exists between class hardness and difficulty in noise identification. This results in ineffectiveness of global cluster methods in detecting label noises across all categories. To resolve the challenge, we propose a novel method called rescaling class-aware Gaussian Mixture Modeling (RCGM) which clusters each category data independently by fitting confidence scores q ij from ith class into two Gaussian distributions as p n i (x n |μ n , Σ n ) and p c i (x c |μ c , Σ c ). The mixed Gaussian p M i (•) is obtained by linear combinations α ik for each distribution:which produces more accurate and independent measurements of label quality. Rather than relying on the assumption that confidence distributions of training samples depend solely on their label quality, RCGM solves the effect of class hardness in noisy detection by individually clustering the scores in each category. This overcomes the limitations of global clustering methods and significantly enhances the accuracy of noise identification even when class hardness varies.Instead of assigning a hard label to the potential noisy data as [8] which also employs a class-specific GMM to cluster the uncertainty, we further re-scale the confidence score of class-wise noisy data. Let x ij be the jth in class i, then its probability of having a clean label is:which is then multiplied by a hyperparameter s if the instance is predicted as noise to reduce its weight in the finetuning. With a pre-defined noise selection threshold as τ , we have the final clean score as:"
Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,2.5,Overall Learning Framework for Imbalanced and Noisy Data,"In contrast to two-stage noise removal and imbalance classification techniques, our approach applies a multi-stage protocol: warm-up phases, noise removal phases, and fine-tuning phases as shown in Fig. 2. In the warm-up stage, we train backbone H and classifier G a few epochs by assuming that G only remembers clean images with less empirical loss. In the noise removal phases, we learn classinvariant probability distributions of noisy-label effect with MER and remove class hardness impact with RCGM. Finally, in the fine-tuning phases, we apply MixUp technique [13,25,26] to rebuild a hybrid distribution from noisy pairs and clean pairs by:where α kl := v(x k ) v(x l ) denotes the balanced scale; and {(x kl , ŷkl )} are the mixed clean data for classifier fine-tuning. Sqrt sampler is applied to re-balance the data, and cross-stage KL [12] and CE loss are the fine-tuning loss functions."
Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,3,Experiment,
Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,3.1,Dataset and Evaluation Metric,"We evaluated our approach on two medical image datasets with imbalanced class distributions and noisy labels. The first dataset, HAM10000 [22], is a dermatoscopic image dataset for skin-lesion classification with 10,015 images divided into seven categories. It contains a training set with 7,007 images, a validation set with 1,003 images, and a testing set with 2,005 images. Following the previous noisy label settings [25], we add 20% noise to its training set by randomly flipping labels. The second dataset, CHAOYANG [29], is a histopathology image dataset manually annotated into four cancer categories by three pathological experts, with 40% of training samples having inconsistent annotations from the experts. To emulate imbalanced scenarios, we prune the class sizes of the training set into an imbalanced distribution as [5]. Consequently, CHAOYANG dataset consists of a training set with 2,181 images, a validation set with 713 images, and a testing set with 1,426 images, where the validation and testing sets have clean labels. The imbalanced ratios [12] of HAM10000 and CHAOYANG are 59 and 20, respectively. The evaluation metrics are Macro-F1, B-ACC, and MCC."
Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,3.2,Implementation Details,"We mainly follow the training settings of FCD [12]. ResNet-18 pretrained on the ImageNet is the backbone. The batch size is 48. Learning rates are 0.06, 0.001, 0.06 and 0.006 with the cosine schedule for four stages, respectively. We train our models by SGD optimizer with sharpness-aware term [6] for 90, 90, 90, and 20 epochs. The size of input image is 224 × 224. The scale and threshold in RCGM are 0.6 and 0.1, respectively."
Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,3.3,Comparison with State-of-the-Art Methods,"We compare our model with state-of-the-art methods which contain noisy methods (including DivideMix [13], NL [16], GCE [27], Co-Learning [19]), imbalance methods (including Focal Loss [14], Sqrt-RS [17], PG-RS [10], CB-Focal [5], EQL [21], EQL V2 [20], CECE [5], CLAS [28], FCD [12]), and noisy imbalanced classification methods (including H2E [25], NL+Sqrt-RS, GCE+Sqrt-RS, GCE+Focal). We train all approaches under the same data augmentations and network architecture.  "
Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,3.4,Ablation Studies,"As shown in Fig. 3, we evaluate the effectiveness of the components in our method by decomposing them on extensive experiments. We choose the first stage of FCD [12] as our baseline. Figure 3a and 3b show that only using MER or RCGM achieves better performance than our strong baseline on both datasets. For example, MER achieves 5.37% and 1.15% improvements on HAM10000 and CHAOYANG, respectively, demonstrating the effectiveness of our noise removal techniques. Further, our multi-stage noise removal technique outperforms single MER and RCGM, revealing that the decomposition for noise effect and hardness effect works on noisy imbalanced datasets. We find that the combination of MER and RCGM improves more on CHAOYANG dataset. This is because CHAOYANG has more possible label noise than HAM10000 caused by the high annotating procedure. From Fig. 3c, we observe the accuracy trends are as the scale increases and achieve the peak around 0.6. It indicates the re-scaling process for noise weight deduction contributes to balancing the feature learning and classification boundary disturbance from the mixture of noisy and clean data. Furthermore, similar performance trends reveal the robustness of scale s."
Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,4,Conclusion and Discussion,"We propose a multi-step framework for noisy long-imbalanced medical image classification. We address three practical adverse effects including data noise, imbalanced distribution, and class hardness. To solve these difficulties, we conduct Multi-Environment Risk Minimization (MER) and rescaling class-aware Gaussian Mixture Modeling (RCGM) together for robust feature learning.Extensive results on two public medical image datasets have verified that our framework works on the noisy imbalanced classification problem. The main limitation of our work is the manually designed multi-stage training protocol which lacks simplicity compared to end-to-end training and warrants future simplification."
Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,,Fig. 1 .,
Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,,Fig. 2 .,
Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,,Fig. 3 .,
Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,,Table 1 .,
Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,,,
Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 30.
MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,1,Introduction,"Colorectal cancer is the third most common malignant tumor, and nearly half of all patients with colorectal cancer develop liver metastasis during the course of the disease [6,16]. Liver metastases after surgery of colorectal cancer is the major cause of disease-related death. Colorectal cancer liver metastases (CRLM) have therefore become one of the major focuses in the medical field. Patients with colorectal cancer typically undergo contrast-enhanced computed tomography (CECT) scans multiple times during follow-up visits after surgery for early detection of CRLM, generating a 5D dataset. In addition to the axial, sagittal, and coronal planes in 3D CT scans, the data comprises contrast-enhanced multiple phases as its 4th dimension, along with different timestamps as its 5th dimension. Radiologists heavily rely on this data to detect the CRLM in the very early stage [15].Extensive existing works have demonstrated the power of deep learning on various spatial-temporal data, and can potentially be applied towards the problem of CRLM. For example, originally designed for natural data, several mainstream models such as E3D-LSTM [12], ConvLSTM [11] and PredRNN [13] use Convolutional Neural Networks (CNN) to capture spatial features and Long Short-Term Memory (LSTM) to process temporal features. Some other models, such as SimVP [4], replace LSTMs with CNNs but still have the capability of processing spatiotemporal information. These models can be adapted for classification tasks with the use of proper classification head.However, all these methods have only demonstrated their effectiveness towards 3D/4D data (i.e., time-series 2D/3D images), and it is not clear how to best extend them to work with the 5D CECT data. Part of the reason is due to the lack of public availability of such data. When extending these models towards 5D CECT data, some decisions need to be made, for example: 1) What is the most effective way to incorporate the phase information? Simply concatenating different phases together may not be the optimal choice, because the positional information of the same CT slice in different phases would be lost.2) Shall we use uni-directional LSTM or bi-direction LSTM? E3D-LSTM [12] shows uni-directional LSTM works well on natural videos while several other works show bi-directional LSTM is needed in certain medical image segmentation tasks [2,7].In this paper, we investigate how state-of-art deep learning models can be applied to the CRLM prediction task using our 5D CECT dataset. We evaluate the effectiveness of bi-directional LSTM and explore the possible method of incorporating different phases in the CECT dataset. Specifically, we show that the best prediction accuracy can be achieved by enhancing E3D-LSTM [12] with a bi-directional LSTM and a multi-plane structure.  When patients undergo CECT scans to detect CRLM, typically three phases are captured: the unenhanced plain scan phase (P), the portal venous phase (V), and the arterial phase (A). The P phase provides the basic shape of the liver tissue, while the V and A phases provide additional information on the liver's normal and abnormal blood vessel patterns, respectively [10]. Professional radiologists often combine the A and V phases to determine the existence of metastases since blood in the liver is supplied by both portal venous and arterial routes."
MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,2,Dataset and Methodology,
MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,2.1,Dataset,"Our dataset follows specific inclusion criteria:-No tumor appears on the CT scans. That means patients have not been diagnosed as CRLM when they took the scans.-Patients were previously diagnosed with colorectal cancer TNM stage I to stage III, and recovered from colorectal radical surgery. -Patients have two or more times of CECT scans.-We already determined whether or not the patients had liver metastases within 2 years after the surgery, and manually labeled the dataset based on this. -No potential focal infection in the liver before the colorectal radical surgery.-No metastases in other organs before the liver metastases.-No other malignant tumors.Our retrospective dataset includes two cohorts from two hospitals. The first cohort consists of 201 patients and the second cohort includes 68 patients. Each scan contains three phases and 100 to 200 CT slices with a resolution of 512×512. Patients may have different numbers of CT scans, ranging from 2 to 6, depending on the number of follow-up visits. CT images are collected with the following acquisition parameters: window width 150, window level 50, radiation dose 120 kV, slice thickness 1 mm, and slice gap 0.8 mm. All images underwent manual quality control to exclude any scans with noticeable artifacts or blurriness and to verify the completeness of all slices. Additional statistics on our dataset are presented in Table 1 and examples of representative images are shown in Fig. 1. The dataset is available upon request. "
MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,2.2,Methods,"Numerous state-of-the-art deep learning models are available to effectively process 4D data. In this paper, we will evaluate some of the most popular ones:1) SaConvLSTM, introduced by Lin et al. [9], incorporates the self-attention mechanism into the ConvLSTM [11] structure, which improves the ability to capture spatiotemporal correlations compared to traditional LSTM. 2) E3D-LSTM, introduced by Wang et al. [12], integrates 3D CNNs into LSTM cells to capture both short-and long-term temporal relations. They used 3D-CNNs to handle the 3D data at each timestamp and LSTMs to compute information at different timestamps. 3) PredRNN-V2, introduced by Wang et al. [13,14], uses Spatiotemporal LSTM (ST-LSTM) by stacking multiple ConvLSTM units and connecting them in a zigzag pattern to handle spatiotemporal data of 4 dimensions. 4) SimVP [4], introduced by Gao et al., uses CNN as the translator instead of LSTM.All of these models need to be modified to handle 5D CECT datasets. A straightforward way to extend them is simply concatenating the A phase and V phase together, thus collapsing the 5D dataset to 4D. However, such an extension may not be the best way to incorporate the 5D spatiotemporal information, because the positional information of the same CT slice in different phases would be lost. Below we explore an alternative modification multi-plane bi-directional LSTM (MPBD-LSTM), based on E3D-LSTM, to handle the 5D data."
MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,,MPBD-LSTM.,"The most basic building block in MPBD-LSTM is the 3D-LSTM modules. Each 3D-LSTM module is composed of two E3D-LSTM cells [12]. Additionally, inspired by the bi-directional LSTM used in medical image segmentation task [2], we replace the uni-directional connections with bidirectional connections by using the backward pass in the 2nd E3D-LSTM cell in each 3D-LSTM module. This allows us to further jointly compute information from different timestamps and gives us more accurate modeling of temporal dynamics. The inner structure of one such module is shown in Fig. 2(b). Aside from the two E3D-LSTM cells, it also includes an output gate σ. Each 3D-LSTM module will generate an output y v,t , which can be calculated as [3]:where -→ h v,t and ←h v,t are the output hidden state of the forward pass and backward pass of phase v at timestamp t, and σ is the function which is used to combine these two outputs, which we choose to use a summation function to get the summation product of these two hidden states. Therefore, the output of the bi-directional LSTM module presented in Fig. 2(b) can be represented as:in which ⊕ stands for summation. After this, the output y v,t0 is passed into the bi-directional LSTM module in the next layer and viewed as input for this module.Figure 2(a) illustrates how MPBD-LSTM uses these 3D-LSTM building blocks to handle the multiple phases in our CT scan dataset. We use two planes, one for the A phase and one for the V phase, each of which is based on a backbone of E3D-LSTM [12] with the same hyperparameters. We first use three 3D-CNN encoders (not displayed in Fig. 2(a)) as introduced in E3D-LSTM to extract the features. Each encoder is followed by a 3D-LSTM stack (the ""columns"") that processes the spatiotemporal data for each timestamp. The stacks are bidirectionally connected, as we described earlier, and consist of two layers of 3D-LSTM modules that are connected by their hidden states. When the spatiotemporal dataset enters the model, it is divided into smaller groups based on timestamps and phases. The 3D-LSTM stacks process these groups in parallel, ensuring that the CT slices from different phases are processed independently and in order, preserving the positional information. After the computation of the 3D-LSTM modules in each plane, we use an average function to combine the output hidden states from both planes.An alternative approach is to additionally connect two planes by combining the hidden states of 3D-LSTM modules and taking their average if a module receives two inputs. However, we found that such design actually resulted in a worse performance. This issue will be demonstrated and discussed later in the ablation study.In summary, the MPBD-LSTM model comprises two planes, each of which contains three 3D-LSTM stacks with two modules in each stack. It modifies E3D-LSTM by using bi-directional connected LSTMs to enhance communication between different timestamps, and a multi-plane structure to simultaneously process multiple phases."
MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,3,Experiments,
MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,3.1,Data Augmentation and Selection,"We selected 170 patients who underwent three or more CECT scans from our original dataset, and cropped the images to only include the liver area, as shown in Fig. 1. Among these cases, we identified 49 positive cases and 121 negative cases. To handle the imbalanced training dataset, we selected and duplicated 60% of positive cases and 20% of negative cases by applying Standard Scale Jittering (SSJ) [5]. For data augmentation, we randomly rotated the images from -30 • to 30 • and employed mixup [17]. We applied the same augmentation technique consistently to all phases and timestamps of each patient's data. We also used Spline Interpolated Zoom (SIZ) [18] to uniformly select 64 slices. For each slice, the dimension was 256 × 256 after cropping. We used the A and V phases of CECT for our CRLM prediction task since the P phase is only relevant when tumors are significantly present, which is not the case in our dataset. The dimension of our final input is (3 × 2 × 64 × 64 × 64), representing (T × P × D × H × W ), where T is the number of timestamps, P is the number of different phases, D is the slice depth, H is the height, and W is the width."
MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,3.2,Experiment Setup,"As the data size is limited, 10-fold cross-validation is adopted, and the ratio of training and testing dataset is 0.9 and 0.1, respectively. Adam optimizer [8] and Binary Cross Entropy loss function [1] are used for network training. For MPBD-LSTM, due to GPU memory constraints, we set the batch size to one and the number of hidden units in LSTM cells to 16, and trained the model till converge with a learning rate of 5e-4. Each training process required approximately 23 GB of memory and took about 20 h on an Nvidia Titan RTX GPU. We ran the 10 folds in parallel on five separate GPUs, which allowed us to complete the entire training process in approximately 40 h. We also evaluated E3D-LSTM [12], PredRNN-V2 [14], SaConvLSTM [9], and SimVP [4]. As this is a classification task, we evaluate all models' performance by their AUC scores. Table 2 shows the AUC scores of all models tested on our dataset. Additional data on accuracy, sensitivity specificity, etc. can be found in the supplementary material. The MPBD-LSTM model outperforms all other models with an AUC score of 0.790. Notably, SimVP [4] is the only CNN-based model we tested, while all other models are LSTM-based. Our results suggest that LSTM networks are more effective in handling temporal features for our problem compared with CNN-based models. Furthermore, PredRNN-V2 [14], which passes memory flow in a zigzag manner of bi-directional hierarchies, outperforms the uni-directional LSTM-based SaConvLSTM [9]. Although the architecture of PredRNN-V2 is different from MPBD-LSTM, it potentially supports the efficacy of jointly computing spatiotemporal relations in different timestamps. Ablation Study on Model Structures. As shown in Table 3, to evaluate the effectiveness of multi-plane and bi-directional connections, we performed ablation studies on both structures. First, we removed the multi-plane structure and concatenated the A and V phases as input. This produced a one-dimensional bi-directional LSTM (Fig. 2(a), without the gray plane) with an input dimension of 3 × 128 × 64 × 64, which is the same as we used on other models. The resulting AUC score of 0.774 is lower than the original model's score of 0.790, indicating that computing two phases in parallel is more effective than simply concatenating them. After this, we performed an ablation study to assess the effectiveness of the bi-directional connection. By replacing the bi-directional connection with a uni-directional connection, the MPBD-LSTM model's performance decreased to 0.768 on the original dataset. This result indicates that the bi-directional connection is crucial for computing temporal information effectively, and its inclusion is essential for achieving high performance in MPBD-LSTM. Also, as mentioned previously, we initially connected the 3D-LSTM modules in two planes with their hidden states. However, as shown in Table 3, we observed that inter-plane connections actually decreased our AUC score to 0.786 compared to 0.790 without the connections. This may be due to the fact that when taking CT scans with contrast, different phases have a distinct focus, showing different blood vessels as seen in Fig. 1. Connecting them with hidden states in the early layers could disrupt feature extraction for the current phase. Therefore, we removed the inter-plane connections in the early stage, since their hidden states are still added together and averaged after they are processed by the LSTM layers. Ablation Study on Timestamps and Phases. We conducted ablation studies using CT images from different timestamps and phases to evaluate the effectiveness of time-series data and multi-phase data. The results, as shown in Table 4, indicate that MPBD-LSTM achieves AUC scores of 0.660, 0.676, and 0.709 if only images from timestamps T0, T1, and T2 are used, respectively. These scores suggest that predicting CRLM at earlier stages is more challenging since the features about potential metastases in CT images get more significant over time. However, all of these scores are significantly lower than the result using CT images from all timestamps. This confirms the effectiveness of using a time-series predictive model. Additionally, MPBD-LSTM obtains AUC scores of 0.653 and 0.752 on single A and V phases, respectively. These results suggest that the V phase is more effective when predicting CRLM, which is consistent with medical knowledge [15]. However, both of these scores are lower than the result of combining two phases, indicating that a multi-phase approach is more useful."
MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,4,Results and Discussion,"Error Analysis. In Fig. 1, Patients B and C are diagnosed with positive CRLM later. MPBD-LSTM correctly yields a positive prediction for Patient B with a confidence of 0.82, but incorrectly yields a negative prediction for Patient C with a confidence of 0.77. With similar confidence in the two cases, the error is likely due to the relatively smaller liver size of Patient C. Beyond this case, we find that small liver size is also present in most of the false negative cases. A possible explanation would be that smaller liver may provide less information for accurate prediction of CRLM. How to effectively address inter-patient variability in the dataset, perhaps by better fusing the 5D features, requires further research from the community in the future."
MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,5,Conclusion,"In this paper, we put forward a 5D CECT dataset for CRLM prediction. Based on the popular E3D-LSTM model, we established MPBD-LSTM model by replacing the uni-directional connection with the bi-directional connection to better capture the temporal information in the CECT dataset. Moreover, we used a multiplane structure to incorporate the additional phase dimension. MPBD-LSTM achieves the highest AUC score of 0.790 among state-of-the-art approaches. Further research is still needed to improve the AUC."
MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,,Fig. 1 .,
MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,,Fig. 2 .,
MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,,Table 1 .,
MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,,Table 2 .,
MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,,Table 3 .,
MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,,Table 4 .,
MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 37.
Deep Cellular Embeddings: An Explainable Plug and Play Improvement for Feature Representation in Histopathology,1,Introduction,"Accurate diagnosis plays an important role in achieving the best treatment outcomes for people with cancer [1]. Identification of cancer biomarkers permits more granular classification of tumors, leading to better diagnosis, prognosis, and treatment decisions [2,3]. For many cancers, clinically reliable genomic, molecular, or imaging biomarkers have not been identified and biomarker identification techniques (e.g., fluorescence in situ hybridization) have limitations that can restrict their clinical use. On the other hand, histological analysis of hematoxylin and eosin (H&E)-stained pathology slides is widely used in cancer diagnosis and prognosis. However, visual examination of H&E-stained slides is insufficient for classification of some tumors because identifying morphological differences between molecularly defined subtypes is beyond the limit of human detection.The introduction of digital pathology (DP) has enabled application of machine learning approaches to extract otherwise inaccessible diagnostic and prognostic information from H&E-stained whole slide images (WSIs) [4,5]. Current deep learning approaches to WSI analysis typically operate at three different histopathological scales: whole slidelevel, region-level, and cell-level [4]. Although cell-level analysis has the potential to produce more detailed and explainable data, it can be limited by the unavailability of sufficiently annotated training data. To overcome this problem, weakly supervised and multiple instance learning (MIL) based approaches have been applied to numerous WSI classification tasks [6][7][8][9][10]. However, many of these models use embeddings derived from tiles extracted using pretrained networks, and these often fail to capture useful information from individual cells. Here we describe a new embedding extraction method that combines tile-level embeddings with a cell-level embedding summary. Our new method achieved better performance on WSI classification tasks and had a greater level of explainability than models that used only tile-level embeddings."
Deep Cellular Embeddings: An Explainable Plug and Play Improvement for Feature Representation in Histopathology,2,Embedding Extraction Scheme,"Transfer learning using backbones pretrained on natural images is a common method that addresses the challenge of using data sets that largely lack annotation. However, using backbones pretrained on natural images is not optimal for classification of clinical images [11]. Therefore, to enable the use of large unlabeled clinical imaging data sets, as the backbone of our neural network we used a ResNet50 model [12]. The backbone was trained with the bootstrap your own latent (BYOL) method [13] using four publicly available data sets from The Cancer Genome Atlas (TCGA) and three data sets from private vendors that included healthy and malignant tissue from a range of organs [14]."
Deep Cellular Embeddings: An Explainable Plug and Play Improvement for Feature Representation in Histopathology,2.1,Tile-Level Embeddings,"Following standard practice, we extracted tiles with dimensions of 256 × 256 pixels from WSIs (digitized at 40 × magnification) on a spatial grid without overlap. Extracted tiles that contained artifacts were discarded (e.g., tiles that had an overlap of >10% with background artifacts such blurred areas or pen markers). We normalized the tiles for stain color using a U-Net model for stain normalization [15] that was trained on a subset of data from one of the medical centers in the CAMELYON17 data set to ensure homogeneity of staining [16].To create the tile-level embeddings, we used the method proposed by [17] to summarize the convolutional neural network (CNN) features with nonnegative matrix factorization (NMF) for K = 2 factors. We observed that the feature activations within the last layer of the network were not aligned with the cellular content. Although these features may still have been predictive, they were less interpretable, and it was more difficult to know what kind of information they captured. Conversely, we observed that the self-supervised network captured cellular content and highlighted cells within the tiles (Fig. 1). Therefore, the tile-level embeddings were extracted after dropping the last layer (i.e., dropping three bottleneck blocks in ResNet50) from the pretrained model. "
Deep Cellular Embeddings: An Explainable Plug and Play Improvement for Feature Representation in Histopathology,2.2,Cell-Level Embeddings,"Tiles extracted from WSIs may contain different types of cells, as well as noncellular tissue such as stroma and blood vessels and nonbiological features (e.g., glass). Celllevel embeddings may be able to extract useful information, based on the morphological appearance of individual cells, that is valuable for downstream classification tasks but would otherwise be masked by more dominant features within tile-level embeddings.We extracted deep cell-level embeddings by first detecting individual cellular boundaries using StarDist [18] and extracting 32 × 32-pixel image crops centered around each segmented nucleus to create cell-patch images. We then used the pre-trained ResNet50 model to extract cell-level embeddings in a similar manner to the extraction of the tilelevel embeddings. Since ResNet50 has a spatial reduction factor of 32 in the output of the CNN, the 32 × 32-pixel image had a 1:1 spatial resolution in the output. To ensure the cell-level embeddings contained features relevant to the cells, prior to the mean pooling in ResNet50 we increased the spatial image resolution to 16 × 16 pixels in the output from the CNN by enlarging the 32 × 32-pixel cell-patch images to 128 × 128 pixels and skipping the last 4-layers in the network.Because of heterogeneity in the size of cells detected, each 32 × 32-pixel cellpatch image contained different proportions of cellular and noncellular features. Higher proportions of noncellular features in an image may cause the resultant embeddings to be dominated by noncellular tissue features or other background features. Therefore, to limit the information used to create the cell-level embeddings to only cellular features, we removed portions of the cell-patch images that were outside of the segmented nuclei by setting their pixel values to black (RGB 0, 0, 0). Finally, to prevent the size of individual nuclei or amount of background in each cell-patch image from dominating over the celllevel features, we modified the ResNet50 Global Average Pooling layer to only average the features inside the boundary of the segmented nuclei, rather than averaging across the whole output tensor from the CNN."
Deep Cellular Embeddings: An Explainable Plug and Play Improvement for Feature Representation in Histopathology,2.3,Combined Embeddings,"To create a combined representation of the tile-level and cell-level embeddings, we first applied a nuclei segmentation network to each tile. Only tiles with ≥ 10 cells per tile, excluding any cells which overlapped the tile border, were included for embedding extraction. For the included tiles, we extracted the tile-level embeddings as described in Sect. 2.1 and for each detected cell we extracted the cell-level embeddings as described in Sect. 2.2. We then calculated the mean and standard deviation of the vectors of the cell-level embeddings for each tile and concatenated those to each tile-level embedding. This resulted in a combined embedding representation with a total size of 1536 pixels (1024 + 256 + 256).In addition to the WSI classification results presented in the next sections, we also performed experiments to compare the ability of combined embeddings and tile-level embeddings to predict nuclei-related features that were manually extracted from the images and to identify tiles where nuclei had been ablated. The details and results of these experiments are available in supplementary materials and provide further evidence of the improved ability to capture cell-level information when using combined embeddings compared with tile-level embeddings alone."
Deep Cellular Embeddings: An Explainable Plug and Play Improvement for Feature Representation in Histopathology,3,WSI Classification Tasks,"For each classification task we compared different combinations of tile-level and celllevel embeddings using a MIL framework. We also compared two different MIL architectures to aggregate the embeddings for WSI-level prediction.The first architecture used an attention-MIL (A-MIL) network [19] (the code was adapted from a publicly available implementation [20]). We trained the network with a 0.001 learning rate and tuned the batch size (48 or 96) and bag sample size (512, 1024, or 2048) for each classification task separately. When comparing the combined embedding extraction method with the tile-level only embeddings, parameters were fixed to demonstrate differences in performance without additional parameter tuning.Transformer (Xformer) was used as the second MIL architecture [21]. We used three Xformer layers, each with eight attention heads, 512 parameters per token, and 256 parameters in the multi-layer perceptron layers. The space complexity of the Xformer was quadratic with the number of tokens. While some WSIs had up to 100,000 tiles, we found, in practice, that we could not fit more than 6000 tokens in the memory. Consequently, we used the Nyströformer Xformer variant [22] since it consumes less memory (the code was adapted from a publicly available implementation [23]). This Xformer has two outputs, was trained with the Adam optimizer [24] with default parameters, and the loss was weighted with median frequency balancing [25] to assign a higher weight to the less frequent class. Like A-MIL, the batch and bag sample sizes were fixed for each classification task. During testing a maximum of 30,000 tiles per slide were used. The complete flow for WSI classification is shown in Fig. 2. The models were selected using a validation set, that was a random sample of 20% of the training data. All training was done using PyTorch version 1.12.1 (pytorch.org) on 8 NVIDIA Tesla V100 GPUs with Cuda version 10.2. "
Deep Cellular Embeddings: An Explainable Plug and Play Improvement for Feature Representation in Histopathology,3.1,Data,"We tested our feature representation method in several classification tasks involving WSIs of H&E-stained histopathology slides. The number of slides per class for each classification task are shown in Fig. 3. For breast cancer human epidermal growth factor receptor 2 (HER2) prediction, we used data from the HEROHE Challenge data set [26]. To enable comparison with previous results we used the same test data set that was used in the challenge [27]. For prediction of estrogen receptor (ER) status, we used images from the TCGA-Breast Invasive Carcinoma (TCGA-BRCA) data set [28] for which the ER status was known.For these two tasks we used artifact-free tiles from tumor regions detected with an in-house tumor detection model.For breast cancer metastasis detection in lymph node tissue, we used WSIs of H&Estained healthy lymph node tissue and lymph node tissue with breast cancer metastases from the publicly available CAMELYON16 challenge data set [16,29]. All artifact-free tissue tiles were used.For cell of origin (COO) prediction of activated B-cell like (ABC) or germinal center B-cell like (GCB) tumors in diffuse large B-cell lymphoma (DLBCL), we used data from the phase 3 GOYA (NCT01287741) and phase 2 CAVALLI (NCT02055820) clinical trials, hereafter referred to as CT1 and CT2, respectively. All slides were H&E-stained and scanned using Ventana DP200 scanners at 40× magnification. CT1 was used for training and testing the classifier and CT2 was used only as an independent holdout data set. For these data sets we used artifact-free tiles from regions annotated by expert pathologists to contain tumor tissue."
Deep Cellular Embeddings: An Explainable Plug and Play Improvement for Feature Representation in Histopathology,4,Model Classification Performance,"For the HER2 prediction, ER prediction, and metastasis detection classification tasks, combined embeddings outperformed tile-level only embeddings irrespective of the downstream classifier architecture used (Fig. 4). In fact, for the HER2 classification task, combined embeddings obtained using the Xformer architecture achieved, to our knowledge, the best performance yet reported on the HEROHE Challenge data set (area under the receiver operating characteristic curve [AUC], 90%; F1 score, 82%).For COO classification in DLBCL, not only did the combined embeddings achieve better performance than the tile-level only embeddings with both the Xformer and A-MIL architectures (Fig. 5) on the CT1 test set and CT2 holdout data set, but they also had a significant advantage versus tile-only level embeddings in respect of the additional insights they provided through cell-level model explainability (Sect. 4.1). "
Deep Cellular Embeddings: An Explainable Plug and Play Improvement for Feature Representation in Histopathology,4.1,Model Explainability,"Tile-based approaches in DP often use explainability methods such as Gradient-weighted Class Activation Mapping [30] to highlight parts of the image that correspond with certain category outputs. While the backbone of our model was able to highlight individual cells, there was no guaranteed correspondence between the model activations and the cells. To gain insights into cell-level patterns that were very difficult or impossible to obtain from tile-level only embeddings, we applied an explainability method that assigned attention weights to the cellular average part of the embedding."
Deep Cellular Embeddings: An Explainable Plug and Play Improvement for Feature Representation in Histopathology,,Cellular Explainability Method. The cellular average embedding is,"where e ij ∈ R 256 is the cellular embedding extracted from every detected cell in the tile j i ∈ 1, 2, . . . , N j where N j is the number of cells in the tile j. This can be rewritten as a weighted average of the cellular embeddingswhere w i ∈ R 256 are the per cell attention weights that if initialized to 0 result in the original cellular average embedding. The re-formulation does not change the result of the forward pass since w i are not all equal. Note that the weights are not learned through training but calculated per cell at inference time to get the per cell contribution. We computed the gradient of the output category (of the classification method applied on top of the computed embedding) with respect to the attention weights w i : grad i = ∂Score i /∂w i and visualized cells that received positive and negative gradients using different colors.Visual Example Results. Examples of our cellular explainability method applied to weakly supervised tumor detection on WSIs from the CAMELYON16 data set using A-MIL are shown in Fig. 6. Cells with positive attention gradients shifted the output towards a classification of tumor and are labeled green. Cells with negative attention gradients are labeled red. When reviewed by a trained pathologist, cells with positive gradients had characteristics previously associated with breast cancer tumors (e.g., larger nuclei, more visible nucleoli, differences in size and shape). Conversely, negative cells had denser chromatin and resembled other cell types (e.g., lymphocytes). These repeatable findings demonstrate the benefit of using cell-level embeddings and our explainability method to gain a cell-level understanding of both correct and incorrect slide-level model predictions (Fig. 6). We also applied our explainability method to COO prediction in DLBCL.In this case, cells with positive attention gradients that shifted the output towards a classification of GCB were labeled green and cells with negative attention gradients that shifted the classification towards ABC were labeled red. Cells with positive attention gradients were mostly smaller lymphoid cells with low grade morphology or were normal lymphocytes, whereas cells with negative attention gradients were more frequently larger lymphoid cells with high grade morphology (Fig. 6). "
Deep Cellular Embeddings: An Explainable Plug and Play Improvement for Feature Representation in Histopathology,5,Conclusions,"We describe a method to capture both cellular and texture feature representations from WSIs that can be plugged into any MIL architecture (e.g., CNN or Xformer-based), as well as into fully supervised models (e.g., tile classification models). Our method is more flexible than other methods (e.g., Hierarchical Image Pyramid Transformer) that usually capture the hierarchical structure in WSIs by aggregating features at multiple levels in a complex set of steps to perform the final classification task. In addition, we describe a method to explain the output of the classification model that evaluates the contributions of histologically identifiable cells to the slide-level classification. Tilelevel embeddings result in good performance for detection of tumor metastases in lymph nodes. However, introducing more cell-level information, using combined embeddings, resulted in improved classification performance. In HER2 and ER prediction tasks for breast cancer we demonstrate that addition of a cell-level embedding summary to tilelevel embeddings can boost model performance by up to 8%. Finally, for COO prediction in DLBCL and breast cancer metastasis detection in lymph nodes, we demonstrated the potential of our explainability method to gain insights into previously unknown associations between cellular morphology and disease biology."
Deep Cellular Embeddings: An Explainable Plug and Play Improvement for Feature Representation in Histopathology,,Fig. 1 .,
Deep Cellular Embeddings: An Explainable Plug and Play Improvement for Feature Representation in Histopathology,,Fig. 2 .,
Deep Cellular Embeddings: An Explainable Plug and Play Improvement for Feature Representation in Histopathology,,Fig. 3 .,
Deep Cellular Embeddings: An Explainable Plug and Play Improvement for Feature Representation in Histopathology,,Fig. 4 .,
Deep Cellular Embeddings: An Explainable Plug and Play Improvement for Feature Representation in Histopathology,,Fig. 5 .,
Deep Cellular Embeddings: An Explainable Plug and Play Improvement for Feature Representation in Histopathology,,Fig. 6 .,
HC-Net: Hybrid Classification Network for Automatic Periodontal Disease Diagnosis,1,Introduction,"Periodontal disease is a set of inflammatory gum infections damaging the soft tissues in the oral cavity, and one of the most common issues for oral health [4].If not diagnosed and treated promptly, it can develop into irreversible loss of the bone and tissue that support the teeth, eventually causing tooth loosening or even falling out. Thus, it is of great significance to accurately classify periodontal disease in an early stage. However, in clinics, dentists have to measure the clinical attachment loss (CAL) of each tooth by manual probing, and eventually determine the severity and progression of periodontal disease mainly based on the most severe area [14]. This is excessively time-consuming, laborious, and over-dependent on the clinical experience of experts. Therefore, it is essential to develop an efficient automatic method for accurate periodontal disease diagnosis from radiography, i.e., panoramic X-ray images.With the development of computer techniques, computer-aided diagnosis has been widely applied for lesion detection [8,9] and pathological classification [10] in medical image analysis. However, periodontal disease diagnosis from panoramic X-ray images is a very challenging task. While reliable diagnosis can only be provided from 3D probing measurements of each tooth (i.e. clinical golden standard), evidence is highly subtle to be recognized from radiographic images. Panoramic X-ray images make it even more difficult with only 2D information, along with severe tooth occlusion and distortion. Moreover, due to this reason, it is extremely hard to provide confident and consistent radiographic annotations on these images, even for the most experienced experts. Many researchers have already attempted to directly measure radiographic bone loss from panoramic X-ray images for periodontal disease diagnosis. Chang et al. [2] employ a multi-task framework to simulate clinical probing, by detecting bone level, cementoenamel junction (CEJ) level, and tooth long axis. Jiang et al. [7] propose a two-stage network to calculate radiographic bone loss with tooth segmentation and keypoint object detection. Although these methods provide feasible strategies, they still rely heavily on radiographic annotations that are actually not convincing. These manually-labeled landmarks are hard to accurately delineate and usually inconsistent with clinical diagnosis by probing measurements. For this reason, the post-estimated radiographic bone loss is easily affected by prediction errors and noises, which can lead to incorrect and unstable diagnosis.To address the aforementioned challenges and limitations of previous methods, we propose HC-Net, a novel hybrid classification framework for automatic periodontal disease diagnosis from panoramic X-ray images, which significantly learns from clinical probing measurements instead of any radiographic manual annotations. The framework learns upon both tooth-level and patient-level with three major components, including tooth-level classification, patient-level classification, and an adaptive noisy-OR gate. Specifically, tooth-level classification first applies tooth instance segmentation, then extracts features from each tooth and predicts a tooth-wise score. Meanwhile, patient-level classification provides patient-wise prediction with a multi-task strategy, simultaneously learning a classification activation map (CAM) to show the confidence of local lesion areas upon the panoramic X-ray image. Most importantly, a learnable adaptive noisy-OR gate is designed to integrate information from both levels, with the tooth-level scores and patient-level CAM. Note that our classification is only supervised by the clinical golden standard, i.e., probing measurements. We provide comprehensive learning and integration on both tooth-level and patient-level classification, eventually contributing to confident and stable diagnosis. Our proposed HC-Net is validated on the dataset from real-world clinics. Experiments have demonstrated the outstanding performance of our hybrid structure for periodontal disease diagnosis compared to state-of-the-art methods."
HC-Net: Hybrid Classification Network for Automatic Periodontal Disease Diagnosis,2,Method,"An overview of our proposed framework, HC-Net, is shown in Fig. 1. We first formulate our task (Sect. 2.1), and then elaborate the details of tooth-level classification (Sect. 2.2), patient-level classification (Sect. 2.3), and adaptive noisy-OR gate (Sect. 2.4), respectively."
HC-Net: Hybrid Classification Network for Automatic Periodontal Disease Diagnosis,2.1,Task Formulation and Method Overview,"In this paper, we aim to classify each patient into seriously periodontal (including periodontitis stage II-IV) or not (including health, gingivitis, and periodontitis stage I), abbreviated below as 'positive' or 'negative'. We collect a set of panoramic X-ray images Xwhere Y i ∈ {0, 1} indicates whether the i-th patient is negative (0) or positive (1). For the i-th patient, we acquire corresponding tooth-level labels T i = {T 1 i , T 2 i , . . . , T Ki i } from clinical golden standard, where K i denotes the number of teeth, and T j i ∈ {0, 1} indicates whether the j-th tooth of the i-th patient is positive or negative.Briefly, our goal is to build a learning-based framework to predict the probability P i ∈ [0, 1] of the i-th patient from panoramic X-ray image. An intuitive solution is to directly perform patient-level classification upon panoramic X-ray images. However, it fails to achieve stable and satisfying results (See Sect. 3.2), mainly for the following two reasons. Firstly, evidence is subtle to be recognized in the large-scale panoramic X-ray image (notice that clinical diagnosis relies on tedious probing around each tooth). Secondly, as we supervise the classification with clinical golden standard (i.e., probing measurements), a mapping should be well designed and established from radiography to this standard, since the extracted discriminative features based on radiography may not be well consistent with the golden standard. Therefore, as shown in Fig. 1, we propose a novel hybrid classification framework to learn upon both tooth-level and patient-level, and a learnable adaptive noisy-OR gate that integrates the predictions from both labels and returns the final classification (i.e., positive or negative)."
HC-Net: Hybrid Classification Network for Automatic Periodontal Disease Diagnosis,2.2,Tooth-Level Classification,"Given the panoramic X-ray image of the i-th patient, we propose a two-stage structure for tooth-level classification, which first captures each tooth with tooth instance segmentation, and then predicts the classification of each tooth. Tooth instance segmentation aims to efficiently detect each tooth with its centroid, bounding box, and mask, which are later used to enhance tooth-level learning. It introduces a detection network with Hourglass [12] as the backbone, followed by three branches, including tooth center regression, bounding box regression, and tooth semantic segmentation. Specifically, the first branch generates tooth center heatmap H. We obtain the filtered heatmap H to get center points for each tooth, by a kernel that retains the peak value for every 8-adjacent, described aswhere we denote p = {p c + e i } 8 i=1 as the set of 8-adjacent, where p c is the center point and {e i } 8 i=1 is the set of direction vectors. The second branch then uses the center points and image features generated by the backbone to regress the bounding box offsets. The third branch utilizes each bounding box to crop the original panoramic X-ray image and segment each tooth. Eventually, with the image patch and corresponding mask A j i for the j-th tooth of the i-th patient, we employ a classification network (i.e., feature extractor and MLP) to predict the probability T j i , if the tooth being positive. To train the tooth-level framework, we design a multi-term objective function to supervise the learning process. Specifically, for tooth center regression, we employ the focal loss of [16] to calculate the heatmap error, denoted as L ctr . For bounding box regression, we utilize L1 loss to calculate the regression error, denoted as L bbx . For tooth semantic segmentation, we jointly compute the cross-entropy loss and dice loss, denoted as L seg = 0.5 × (L segCE + L segDice ). We finally supervise the tooth-level classification with a cross-entropy loss, denoted as L clst . Therefore, the total loss of the tooth-level classification is formulated as L tooth = L ctr +0.1×L bbx +L seg +L clst ."
HC-Net: Hybrid Classification Network for Automatic Periodontal Disease Diagnosis,2.3,Patient-Level Classification,"As described in Sect. 2.1, although patient-level diagnosis is our final goal, direct classification is not a satisfying solution, and thus we propose a hybrid classification network on both tooth-level and patient-level. Additionally, to enhance patient-level classification, we introduce a multi-task strategy that simultaneously predicts the patient-level classification and a classification activation map (CAM). The patient-level framework first utilizes a backbone network to extract image features for its following two branches. One branch directly determines whether the patient is positive or negative through an MLP, which makes the extracted image features more discriminative. We mainly rely on the other branch, which transforms the image features into CAM to provide local confidence upon the panoramic X-ray image.Specifically, for the i-th patient, with the predicted area {A j i } Ki j=1 of each tooth and the CAM M i , the intensity I of the j-th tooth can be obtained, described aswhere C(•, * ) denotes the operation that crops • with the area of * . To supervise the CAM, we generate a distance map upon the panoramic X-ray image, based on Euclidean Distance Transform with areas of positive tooth masks. In this way, we train patient-level classification in a multi-task scheme, jointly with direct classification and CAM regression, which increases the focus on possible local areas of lesions and contributes to accurate classification. We introduce two terms to train the patient-level framework, including a cross-entropy loss L clsp to supervise the classification, and a mean squared loss L CAM to supervise the regression for CAM. Eventually, the total loss of the patient-level classification is"
HC-Net: Hybrid Classification Network for Automatic Periodontal Disease Diagnosis,2.4,Learnable Adaptive Noisy-OR Gate,"We finally present a learnable adaptive noisy-OR gate [13] to integrate toothlevel classification and patient-level classification. To further specify the confidence of local lesion areas on CAM, we propose to learn dummy probabilities D j i for each tooth with its intensitywhere Φ denotes the pooling operation.In this way, as shown in Fig. 1, we have obtained tooth-wise probabilities predicted from both tooth-level (i.e., probabilities { T j i } Ki j=1 ) and patient-level (i.e., dummy probabilities {D j i } Ki j=1 ). We then formulate the final diagnosis as hybrid classification, by designing a novel learnable adaptive noisy-OR gate to aggregate these probabilities, described aswhere Ỹi is the final prediction of the i-th patient, G i is the subset of tooth numbers. We employ the binary cross entropy loss L gate to supervise the learning of adaptive noisy-OR Gate. Eventually, the total loss L of our complete hybrid classification framework is formulated as3 Experiments"
HC-Net: Hybrid Classification Network for Automatic Periodontal Disease Diagnosis,3.1,Dataset and Evaluation Metrics,"To evaluate our framework, we collect 426 panoramic X-ray images of different patients from real-world clinics, with the same size of 2903 × 1536. Each patient has corresponding clinical records of golden standard, measured and diagnosed by experienced experts. We randomly split these 426 scans into three sets, including 300 for training, 45 for validation, and 81 for testing. To quantitatively evaluate the classification performance of our method, we report the following metrics, including accuracy, F1 score, and AUROC. Accuracy directly reflects the performance of classification. F1 score further supports the accuracy with the harmonic mean of precision and recall. AUROC additionally summarizes the performance over all possible classification thresholds."
HC-Net: Hybrid Classification Network for Automatic Periodontal Disease Diagnosis,3.2,Comparison with Other Methods,"We mainly compare our proposed HC-Net with several state-of-the-art classification networks, which can be adapted for periodontal disease diagnosis.ResNet [5], DenseNet [6], and vision transformer [3] are three of the most representative classification methods, which are used to perform patient-level classification as competing methods. We implement TC-Net as an approach for toothlevel classification, which extracts features respectively from all tooth patches, and all features are concatenated together to directly predict the diagnosis. Moreover, we notice the impressive performance of the multi-task strategy in medical imaging classification tasks [15], and thus adopt MTL [1] to perform multi-task learning scheme. Note that we do not include [2,7] in our comparisons, as they do not consider the supervision by golden standard and heavily rely on unconvincing radiographic manual annotations, which actually cannot be applied in clinics. We employed the well-studied CenterNet [16] for tooth instance segmentation, achieving promising detection (mAP50 of 93%) and segmentation (DICE of 91%) accuracy. As shown in Table 1, our HC-Net outperforms all other methods by a large margin. Compared to the patient-level classification methods (such as, ResNet [5], DenseNet [6] and transformer-based xViTCOS [11]) and the toothlevel classification method (TC-Net), MTL [1] achieves better performance and robustness in terms of all metrics, showing the significance of learning from both levels with multi-task strategy. Compared to MTL, we exploit the multi-task strategy with CAM in the patient-level, and design an effective adaptive noisy-OR gate to integrate both levels. Although the DeLong test doesn't show a significant difference, the boosting of all metrics (e.g., accuracy increase from 87.65% to 92.59%) demonstrates the contributions of our better designs that can aggregate both levels more effectively. "
HC-Net: Hybrid Classification Network for Automatic Periodontal Disease Diagnosis,3.3,Ablation Studies,"We conduct ablative experiments to validate the effectiveness of each module in HC-Net, including patient-level multi-task strategy with classification activation map (CAM) and hybrid classification with adaptive noisy-OR gate. We first define the baseline network, called B-Net, with only patient-level classification. Then, we enhance B-Net with the multi-task strategy, denoted as M-Net, which involves CAM for joint learning on the patient level. Eventually, we extend B-Net to our full framework HC-Net, introducing the tooth-level classification and the adaptive noisy-OR gate.Effectiveness of Multi-task Strategy with CAM. We mainly compare M-Net to B-Net to validate the multi-task strategy with CAM. We show the classification activation area of both methods as the qualitative results in Fig. 2. Obviously, the activation area of B-Net is almost evenly distributed, while M-Net concentrates more on the tooth area. It shows great potential in locating evidence on local areas of the large-scale panoramic X-ray image, which discriminates the features to support classification. Eventually, it contributes to more accurate qualitative results, as shown in Table 2 and Fig. 3.  Effectiveness of Hybrid Classification with Noisy-OR Gate. We eventually utilize hybrid classification with adaptive noisy-OR Gate, comparing our full framework HC-Net to M-Net. In Table 2 and Fig. 3, we observe that all metrics are dramatically improved. Specifically, the accuracy and F1 score are boosted from 90.12% and 91.67%, to 92.59% and 93.61%, respectively. Note that the AUROC is also significantly increased to 95.81%, which verifies that hybrid classification with noisy-OR gate can improve both the accuracy and robustness of our framework."
HC-Net: Hybrid Classification Network for Automatic Periodontal Disease Diagnosis,3.4,Implementation Details,"Our framework is implemented based on the PyTorch platform and is trained with a total of 200 epochs on the NVIDIA A100 GPU with 80GB memory. The feature extractors are based on DenseNet [6]. We use the Adam optimizer with the initial learning rate of 0.001, which is divided by 10 every 50 epochs. Note that in the learnable noisy-or gate, we utilize the probabilities of the top 3 teeth to make predictions for the final outcome."
HC-Net: Hybrid Classification Network for Automatic Periodontal Disease Diagnosis,4,Conclusion,"We propose a hybrid classification network, HC-Net, for automatic periodontal disease diagnosis from panoramic X-ray images. In tooth-level, we introduce instance segmentation to help extract features for tooth-level classification. In patient-level, we adopt the multi-task strategy that jointly learns the patientlevel classification and CAM. Eventually, a novel learnable adaptable noisy-OR gate integrates both levels to return the final diagnosis. Notice that we significantly utilize the clinical golden standard instead of unconvincing radiographic annotations. Extensive experiments have demonstrated the effectiveness of our proposed HC-Net, indicating the potential to be applied in real-world clinics."
HC-Net: Hybrid Classification Network for Automatic Periodontal Disease Diagnosis,,Fig. 1 .,
HC-Net: Hybrid Classification Network for Automatic Periodontal Disease Diagnosis,,Fig. 2 .,
HC-Net: Hybrid Classification Network for Automatic Periodontal Disease Diagnosis,,Fig. 3 .,
HC-Net: Hybrid Classification Network for Automatic Periodontal Disease Diagnosis,,Table 1 .,
HC-Net: Hybrid Classification Network for Automatic Periodontal Disease Diagnosis,,Table 2 .,Method Accuracy (%) F1 Score (%) AUROC (%)
Anatomical Landmark Detection Using a Multiresolution Learning Approach with a Hybrid Transformer-CNN Model,1,Introduction,"Anatomical landmark detection has been used successfully in parametric modeling [19], registration [22], and quantification of various anatomical abnormalities [6,21]. To detect landmarks automatically and accurately, advanced artificial intelligence technologies, including deep learning with convolutional neural net-work (CNN)-based [13], transformer-based [7], and graph-convolution methods [8], have been developed and have attracted great interest from both academia and industry.Generally, deep learning-based anatomical landmark detection is based on heatmap regression approaches [2,3], which decode the predicted landmark coordinates from the heatmap corresponding to the landmarks. In previous studies, the networks mostly generate only one resolution of the heatmap to decode the landmark coordinates. However, deriving the landmark coordinate from a highresolution heatmap exhibits high bias and low variance, whereas the landmark coordinates obtained from a low-resolution heatmap demonstrate low bias and high variance. Typically, the heatmap regression-based detectors generate highresolution heatmaps by utilizing the high-resolution coarse feature. However, this process results in the loss of specific landmark-related features, including crucial information regarding the geometric relationship between landmarks. Consequently, this impacts the network's ability to accurately localize landmarks.In this study, we propose a multiresolution heatmap learning strategy that derives the predicted landmark coordinate from multiresolution heatmaps to balance the bias and variance of the predicted landmarks. Moreover, leveraging multiresolution feature representations to generate the heatmap can effectively increase the localization accuracy of the deep learning network.Typically, the existing methods of anatomical landmark detection are formulated using CNN-based or transformer-based encoder-decoder architecture [16]. The convolution operation collects information by layer, which focus on the local feature information. Meanwhile, the vision transformer has the ability to encode global representations. To combine the advantages of CNNs and transformers, we introduce a novel hierarchical hybrid transformer and CNN architecture called the hybrid transformer-CNN (HTC). HTC introduces a stack of convolutional and transformer modules, which are applied to all stages of the encoder for extracting global information, and local information. Furthermore, we propose a lightweight positional-encoding-free transformer module. Instead of using multihead attention, we introduce the bilinear pooling operation to capture secondorder statistics of features and generate global representations. Moreover, general transformer encoders suffer from the fixed resolution of positional encoding, which results in decreased accuracy when interpolating the positional encoding during testing with resolutions different from the training data. To alleviate this problem, we remove the positional encoding from the transformer modules and employ a 3 ˆ3 convolutional operation as the patch embedding to capture location information and generate low-resolution fine features for the hierarchical encoder architecture design.The main contributions of this paper are as follows:-Introduction of a multiresolution heatmaps learning strategy, which increases the detection ability of the network by leveraging multiresolution information to derive the predicted landmark. -Development of a hierarchical hybrid transformer and CNN architecture named the HTC, which sequentially combines transformer and convolutional modules.-Our proposed HTC model trained with a multiresolution heatmap learning approach clearly outperforms previous state-of-the-art models on three datasets: XCAT 2D projections of head CBCT volumes, X-ray dataset from ISBI2023 Challenge [1], and hand X-ray dataset [13]."
Anatomical Landmark Detection Using a Multiresolution Learning Approach with a Hybrid Transformer-CNN Model,2,Methods,"In this section, we introduce our anatomical landmark detector, which consists of the proposed multiresolution heatmap learning method and the HTC backbone network, as shown in Fig. 1."
Anatomical Landmark Detection Using a Multiresolution Learning Approach with a Hybrid Transformer-CNN Model,2.1,Multiresolution Heatmap Learning,"As depicted in Fig.  where Conv 1 and Conv 3 are 1ˆ1 and 3ˆ3 convolutional operations, respectively. In addition, K is the number of landmarks.During training, we use mean squared error (MSE) as the loss function between the predicted and ground-truth heatmaps for each resolution. Moreover, we enforce the detector to learn global and local information from the heatmap generated by the high-resolution coarse feature and the low-resolution fine-grained features, through the weighted summation of the heatmap loss from each resolution heatmap as follows:where L H1 and L H2 are the respective losses calculated from the predicted heatmap H 1 and ground truth heatmap of size H 4 ˆW 4 ˆK as well as the predicted heatmap H 2 and ground truth heatmap of size H 2 ˆW 2 ˆK. Additionally, λ is the loss weight, which is set as three in all the experiments. During inference, we calculate the output landmark prediction coordinates of the model by averaging the corresponding coordinates decoded from the heatmap at each resolution."
Anatomical Landmark Detection Using a Multiresolution Learning Approach with a Hybrid Transformer-CNN Model,2.2,Hybrid Transformer-CNN (HTC),"We introduce a novel hierarchical encoder architecture named HTC, which consists of four stages of a stack of the convolutional ConvU i and transformer mod-ules T ransU i to generate multi-level feature representation. The overall architecture design of the HTC is shown in Fig. 1. At each stage, the transformer module captures global information such as the geometric relation between landmarks, while the convolutional module extracts local information. The architecture of the transformer and convolutional module are shown in Fig. 2 (a) and (b), respectively. In this study, we proposed a lightweight positional-encoding-free transformer module. Instead of using multi-head attention, we introduce the bilinear pooling operation to capture second-order statistics of features and generate global representations. Moreover, we eliminate the usage of positional encoding to address the challenge of reduced model performance when testing the model with resolutions that differ from the training data. This modification aims to mitigate the issue and improve the overall performance under varying resolution scenarios.In the first stage, an input image of size H ˆW ˆ3 is fed to the patch embedding, which is a convolutional 3 ˆ3 operation to obtain a patch token of size H 2 ˆW 2 ˆC1 . Then, the patch token is passed through the bilinear pooling attention module, which requires three inputs: a query Q, a key K, and a value V , which are the patch token that passes through the 1 ˆ1 convolutional operations, separately. Then, Q, K, and V are flattened to size HW 2 2 ˆC1 . The key and query are then fed to bilinear pooling [9], which is an effective way of gathering the key features and capturing the global representations of the images as follows:where N is the set of spatial locations (combinations of rows and columns).We further applied the softmax function to the output of bilinear pooling F to generate the attention weighting vector. Thereafter, matrix multiplication was performed between F and V , and the output of the bilinear pooling attention module was passed through to the feed-forward layer [23]. Then, the output of the feed-forward layer was reshaped as feature representations G i of size H 2 ˆW 2 Ĉ1 . In addition, the output of the transformer module T ransU i was fed to the convolutional module ConvU i , comprising 3ˆ3 convolutional layers with dilated rates equal to one and two (Conv 3,d1 and Conv 3,d2 ), a batch normalization operation (Norm), and a rectified linear unit (ReLU ) activation function as follows:(3) Similarly, using the feature representations from the previous stages as inputs, we obtained E 2 , E 3 , and E 4 with spatial reduction ratios of 4, 8, and 16 pixels, respectively, with respect to the input image."
Anatomical Landmark Detection Using a Multiresolution Learning Approach with a Hybrid Transformer-CNN Model,3,Experiments,
Anatomical Landmark Detection Using a Multiresolution Learning Approach with a Hybrid Transformer-CNN Model,3.1,Dataset,"To evaluate our method, we conducted experiments on a total of three datasets, including one 4D XCAT phantom CT dataset and two public X-ray datasets. Here, we generated head models from the 4D XCAT dataset [17] for 27 patients with varying anatomical sizes and genders. We manually labeled 13 cephalometric landmarks on CT phantom volumes. Moreover, we perform forward projection on both the 3D phantom CT volumes and landmarks at 360 angles per patient to obtain 2D images and landmark labels. We randomly selected 70% of the patients' CT scans as the training dataset (18 patients) and the remaining 30% as the test dataset (9 patients). The size of each image was 620ˆ480, with a pixel spacing of 0.66 mm.We also evaluated our method on the public X-ray dataset from the IEEE ISBI 2023 Challenge [1]. A total of 29 ground truth landmarks were labeled by two experts. The image sizes and pixel spacings vary over patients. We randomly selected 75% of the X-ray images of the provided training dataset as the network training dataset(525 images) and the remaining 25% as the test dataset.Finally, we performed experiments on a public hand dataset containing Xray images from 895 patients and having 37 landmarks [13]. The sizes of these images are not all the same, so we resized the images to 1024ˆ1216. Owing to their lack of physical pixel resolution, we calculated the pixel spacing based on the assumption that the distance between two landmarks at both endpoints of the wrist is approximately 50 mm."
Anatomical Landmark Detection Using a Multiresolution Learning Approach with a Hybrid Transformer-CNN Model,3.2,Implementation Details,"In our experiments, we implemented our framework using MMPose [4], an opensource toolbox for pose estimation based on PyTorch. For XCAT CT landmark detection, our HTC with multiresolution heatmap learning was trained for 50 epochs using the AdamW optimizer with the initial learning rate set to 0.0003. Furthermore, we trained the ISBI2023 landmark detection method for 180 epochs using the AdamW optimizer, with an initial learning rate of 0.00045. In addition, for hand landmark detection, we trained for 300 epochs using the AdamW optimizer at an initial learning rate of 0.0004. For all the experiments, the evaluation metrics are the mean radial error (MRE, mm) and successful detection rate (SDR, %) under 2 mm, 2.5 mm, 3 mm, and 4 mm conditions."
Anatomical Landmark Detection Using a Multiresolution Learning Approach with a Hybrid Transformer-CNN Model,3.3,Performance Evaluation,"In this section, we compare the performances of the proposed and state-of-theart methods as well as analyze ablation studies on the proposed method. In each of the tables below, the metrics showing the best and second-best performances are indicated by boldface and underlined, respectively.  2 presents the performance comparison between the proposed and existing methods on the hand dataset. Our method significantly outperformed the best performance for all metrics on the hand dataset, with 0.56 mm MRE and 0.58 mm standard deviation of MRE. Additionally, qualitative comparisons of the images of the detection results are shown in Fig. 3.  "
Anatomical Landmark Detection Using a Multiresolution Learning Approach with a Hybrid Transformer-CNN Model,,Ablation Study:,"We conducted an additional study to observe the effects of the proposed multiresolution heatmap learning and HTC. For this study, we compared the HTC with Conformer [15], which is a representative hybrid transformer and CNN architecture. As shown in Table 3, the HTC outperforms Conformer with MRE values of 3.03 and 1.11 mm for the XCAT CT and ISBI2023 datasets, respectively. Furthermore, the implementation of multiresolution heatmap learning can enhance the MRE of 0.15 and 0.03 mm for the XCAT CT and ISBI2023 datasets, respectively."
Anatomical Landmark Detection Using a Multiresolution Learning Approach with a Hybrid Transformer-CNN Model,4,Conclusion,"This study presents a new feature extraction architecture, referred to as the hybrid transformer-CNN (HTC), along with multiresolution heatmap learning for automatic anatomical landmark detection. The HTC architecture comprises of multiple stages of stacked transformer modules, which incorporate a bilinear pooling attention module to capture the global information of images and convolutional modules to extract local and specific feature representations relevant to landmarks. Additionally, we introduced multiresolution heatmap learning to improve the network's ability to capture global and local representations more accurately than learning from a single heatmap resolution, thereby enhancing network localization. Our experimental evaluations on three benchmark datasets demonstrate that the proposed method surpasses state-of-the-art approaches across various modalities and anatomical regions. These findings highlight the potential of our method for automatic anatomical landmark detection in various medical applications."
Anatomical Landmark Detection Using a Multiresolution Learning Approach with a Hybrid Transformer-CNN Model,,Fig. 1 .,
Anatomical Landmark Detection Using a Multiresolution Learning Approach with a Hybrid Transformer-CNN Model,,Fig. 2 .,
Anatomical Landmark Detection Using a Multiresolution Learning Approach with a Hybrid Transformer-CNN Model,,Fig. 3 .,
Anatomical Landmark Detection Using a Multiresolution Learning Approach with a Hybrid Transformer-CNN Model,,,"16 Ŵ 16 ˆC4 is passed through 1 ˆ1 convolutional operation, and the output feature representation D 1 has size H 16 ˆW 16 ˆCd . Then, D 1 is upsampled using 3ˆ3 deconvolution operations and aggregated with the encoder feature representations E 3 to generate the next stage feature representations D 2 having size H 8 ˆW 8 ˆCd ."
Anatomical Landmark Detection Using a Multiresolution Learning Approach with a Hybrid Transformer-CNN Model,,Table 1 .,
Anatomical Landmark Detection Using a Multiresolution Learning Approach with a Hybrid Transformer-CNN Model,,Table 2 .,
Anatomical Landmark Detection Using a Multiresolution Learning Approach with a Hybrid Transformer-CNN Model,,.56(0.58) 96.84 99.63 100 Comparisons with State-of-the-art Methods:,
Anatomical Landmark Detection Using a Multiresolution Learning Approach with a Hybrid Transformer-CNN Model,,Table 3 .,
Anatomical Landmark Detection Using a Multiresolution Learning Approach with a Hybrid Transformer-CNN Model,,Acknowledgements,". This research was partly supported by the BK21 FOUR (Fostering Outstanding Universities for Research) funded by the Ministry of Education (MOE, Korea) and National Research Foundation of Korea (NRF-5199990614253); by the Technology development Program of MSS [S3146559]; by the National Research Foundation of Korea (NRF-2022R1A2C1092072); and Institute of Information and communications Technology Planning and Evaluation (IITP) grant funded by the Korean government (MSIT) (No. RS-2022-00155966, Artificial Intelligence Convergence Innovation Human Resources Development (Ewha Womans University))."
Transformer-Based End-to-End Classification of Variable-Length Volumetric Data,1,Introduction,"Volumetric medical scans allow for comprehensive diagnosis, but their manual interpretation is time consuming and error prone [19,21]. Deep learning methods have shown exceptional performance in automating this task [27], often at medical expert levels [6]. However, their application in the clinical practice is still limited, partially because they require rigid acquisition settings. In particular, variable volume length, i.e. number of slices, is common for imaging modalities such as computed tomography, magnetic resonance imaging or optical coherence tomography (OCT). Despite the advantages of having data diversity in terms of quality and size, automated classification of dense scans with variable input size is a challenge. Furthermore, the 3D nature of medical volumes results in a memory-intensive training procedure when processing the entire volume. To account for this constraint and make the input size uniform, volumes are usually subsampled, ignoring and potentially hiding relevant diagnostic information.Among approaches for handling variable input size, Multiple Instance Learning (MIL) is commonly used. There, a model classifies each slice or subgroup of slices individually, and the final prediction is determined by aggregating subdecisions via maximum or average pooling [16,17,23], or other more sophisticated fusion approaches [20,24]. However, they often do not take advantage of the 3D aspect of the data. The same problem occurs when stacking slice-wise embeddings [4,11,22], applying self-attention [5] for feature aggregation, or using principal component analysis (PCA) [9] to reduce the variable number of embeddings to a fixed size. As an alternative, recurrent neural networks (RNNs) [18] consider the volume as a sequence of arranged slices or the corresponding embeddings. However, their performance is overshadowed by arduous training and lack of parallelization.Vision Transformers (ViTs) [8], on the other hand, allow parallel computation and effective analysis of longer sequences by benefiting from multi-head self-attention (MSA) and positional encoding. These components allow to model both local and global dependencies, playing a pivotal role for 3D medical tasks where the order of slices is important [13,14,26]. Moreover, ViTs are more flexible regarding input size. Ignoring slice positional information (bag-of-slices) or using sinusoidal positional encoding enables them to process sequences of arbitrary length with respect to computational resources. However, ViTs with learnable positional embeddings (PEs) have shown better performance [8]. In this case, the only restriction in processing variable length sequences is the number of PEs. Although interpolating the PE sequence helps overcome this restriction, the resultant sequence will not model the exact positional information of the corresponding slices in the input sequence, affecting ViTs performance [2]. Notably, Flexible ViT [2] (FlexiViT) handles patch sequences of variable sizes by randomizing the patch size during training and, accordingly, resizing the embedding weights and parameters corresponding to PEs.Despite the merits of the aforementioned approaches, three fundamental challenges still remain. First, the model should be able to process inputs with variable volume resolutions, where throughout the paper we refer to the resolution in the dimension across slices (number of slices), and simultaneously capture the size-independent characteristics of the volume and similarities among the constituent slices. The second challenge is the scalability and the ability of the model to adapt to unseen volume-wise resolutions at inference time. Lastly, the training of deep learning models with high resolution volumes is both computationally expensive and memory-consuming.In this paper, we propose a late fusion Transformer-based end-to-end framework for 3D volume classification whose local-similarity-aware PEs not only improve the model performance, but also make it more robust to interpolation of PEs sequence. We first embed each slice by a spatial feature extractor and then aggregate the corresponding sequence of slice-wise embeddings with a Feature Aggregator Transformer (FAT) module to capture 3D intrinsic characteristics of the volume and produce a volume-level representation. To enable the model to process volumes with variable resolutions, we propose a novel training strategy, Variable Length FAT (VLFAT), that enables FAT module to process volumes with different resolutions both at training and test times. VLFAT can be trained with a proportionally few #slices, an efficient trait in case of training time/memory constraints. Consequently, even with drastic slice subsampling during training, the model will be robust against extreme PEs interpolation for high-resolution volumes at the test time. The proposed approach is modelagnostic and can be deployed with Transformer-based backbones. VLFAT beats the state-of-the-art performance in retinal OCT volume classification on a private dataset with nine disease classes, and achieves competitive performance on a two-class public dataset."
Transformer-Based End-to-End Classification of Variable-Length Volumetric Data,2,Methods,"Our end-to-end Transformer-based volume classification framework (Fig. 1) has three main components: 1) Slice feature extractor (SFE) to extract spatial biomarkers and create a representation of the corresponding slice; 2) Volume feature aggregator (VFA) to combine the slice-level representations into a volumelevel representation, and 3) Volume classification. Trained with the proposed strategy, our approach is capable of processing and classifying volumes with varying volume-wise resolutions. Let's consider a full volume v ∈ R (N ×W ×H) , where (N, H, W) are the #slices, its width and height respectively. The input to the network is a subsampled volume by randomly selecting n slices."
Transformer-Based End-to-End Classification of Variable-Length Volumetric Data,,Slice Feature Extractor (SFE).,"To obtain the slice representations, we use ViT as our SFE due to its recent success in medical interpretation tasks [10]. ViT mines crucial details from each slice and, using MSA and PE, accumulates the collected information in a learnable classification token, constituting the slicewise embedding. For each slice token, we then add a learnable 1D PE [8] to retain the position of each slice in the volume.Volume Feature Aggregator (VFA). The output of the previous step is a sequence of slice-wise embeddings, to which we append a learnable volumelevel classification token [7]. The resulting sequence of embedding vectors is then processed by the FAT module to produce a volume-level embedding. In particular, we propose VLFAT, a FAT with enhanced learnable PEs, inspired on FlexiViT [2], where we modify #slices per input volume instead of patch sizes and correspondingly apply PEs interpolation. This allows handling arbitrary volume resolutions, which generally would not be possible except for an ensemble of models of different scales. Specifically, at initialization we set a fixed value, n, for #slices, resulting in PEs sequence with size (n + 1, dim), where an extra PE is assigned to the classification token and dim is the dimension of the slice representation. In each training step, we then randomly sample a new value for n from a predefined set and, accordingly, linearly interpolate the PEs sequence (Fig. 1), using the known adjacent PEs [3]. This allows to preserve the similarity between neighboring slices in the volume, sharing biomarkers in terms of locality, and propagating the corresponding positional information. The new PEs are then normalized according to a truncated normal distribution.Volume Classification. Finally, the volume-level classification token is fed to a Fully Connected (FC) layer, which produces individual class scores. As a loss function, we employ the weighted cross-entropy."
Transformer-Based End-to-End Classification of Variable-Length Volumetric Data,3,Experiments,"We tested our model for volume classification of macula-centered retinal OCT scans, where large variation in volume resolution (#B-scans) between samples is very common. For multiclass classification performance metrics, we relied on Balanced Accuracy (BAcc) and one-vs-all Area Under the Receiver Operating Curve (AUROC). The source code is available at: github.com/marziehoghbaie/VLFAT. Comparison to State-of-the-Art Methods. We compared the performance of the proposed method with two state-of-the-art video ViTs (ViViT) [1], originally designed for natural video classification: 1) factorized encoder (FE) ViViT, that models the spatial and temporal dimensions separately; 2) factorised selfattention (FSA) ViViT, that simultaneously computes spatial and temporal interactions. We selected FE and FSA ViViTs as baselines to understand the importance of separate feature extractors and late fusion in our approach. FE ViViT, similar to ours, utilizes late fusion, while FSA ViViT is a slow-fusion model and processes spatiotemporal patches as tokens.Ablation Studies. To investigate the contribution of SFE module, we deployed ViT and ResNet18 [26], a standard 2D convolutional neural network (CNN) in medical image analysis, with pooling methods as VFA where the quality of slicewise features is more influential. For VFA, we explored average pooling (AP), max pooling (MP), and 1D convolution (1DConv). As MIL-based baselines, pooling methods can be viable alternatives to VLFAT for processing variable volume resolutions. In addition to learnable PE, we deployed sinusoidal PE (sinPE) and bag-of-slices (noPE) for FAT to examine the effect of positional information.Robustness Analysis. We investigate the robustness of VFLAT and FAT to PEs sequence interpolation at inference time by changing the volume resolution.To process inputs with volume resolutions different from FAT's and VLFAT's input size, we linearly interpolate the sequence of PEs at the test time. For 9C dataset, we only assess samples with minimum #slices of 128 to better examine the PE's scalability to higher resolutions.Implementation Details. The volume input size was 25 × 224 × 224 for all experiments except for FSA ViViT where #slices was set to 24 based on the corresponding tublet size of 2 × 16 × 16. During VLFAT training, the #slices varied between {5, 10, 15, 20, 25}, specified according to memory constraints. We randomly selected slices using a normal distribution with its mean at the central slice position, thus promoting the inclusion of the region near the fovea, essential for the diagnosis of macular diseases. Our ViT configuration is based on ViT-Base [8] with patch size 16 × 16, and 12 Transformer blocks and heads. For FAT and VLFAT, we set the number of Transformer blocks to 12 and heads to 3. The slice-wise and volume-wise embedding dimension were set to 768. The configuration of ViViT baselines was set according to the original papers [1]. Training was performed using AdamW [12] optimizer with learning rate of 6×10 -6 with cosine annealing. All models were trained for 600 epochs with a batch size of 8. Data augmentation included random brightness enhancing, motion blur, salt/pepper noise, rotation, and random erasing [28]. The best model was selected based on the highest BAcc on the validation set. All experiments were performed using Pytorch 1.13.0+cu117 and timm library [25] on a server with 1 TB RAM, and NVIDIA RTX A6000 (48 GB VRAM)."
Transformer-Based End-to-End Classification of Variable-Length Volumetric Data,4,Results and Discussion,"In particular, on large 9C dataset our VLFAT achieved 21.4% and 22.51% BAcc improvement compared to FE ViViT and FSA ViViT, respectively. Incorporating our training strategy, VLFAT, improved FAT's performance by 16.12% on 9C, and 8.79% on OLIVES, which verifies the ability of VLFAT in learning more location-aware PEs, something that is also reflected in the increase of AUROCs (0.96→ 0.98 on 9C dataset and 0.95→ 0.97 on OLIVES). Per-class AUROCs are shown in Table 2. The results show that for most of the classes, our VLFAT has better diagnostic ability and collects more disease-specific clues from the volume.  The ablation study (Table 1) showed that each introduced component in the proposed model contributed to the performance improvement. In particular, ViT was shown as a better slice feature extractor compared to ResNet18, particularly on 9C dataset where the differences between disease-related biomarkers are more subtle. Additionally, the poor performance of the pooling methods as compared to FAT and 1DConv, emphasizes the importance of contextual volumetric information, the necessity of a learnable VFA, and the superiority of Transformers over 1DConv. Although, on OLIVES, less complicated VFAs (pooling/1DConv) and FAT (noPE) also achieved comparable results, which can be attributed primarily to DR vs. DME [15] being an easier classification task compared to the diverse disease severity in the 9C dataset. In addition, the competitive advantage of VLFAT in handling different resolutions was not fully exploited in OLIVES since the large majority of cases had the same #slices. On 9C, however, the comparison of positional encoding strategies demonstrated that although ignoring PEs and sinusoidal approach provide deterministic predictions, the importance of learnable PEs in modeling the anatomical order of slices in the volume is crucial. The robustness analysis is shown in Fig. 3. VLFAT was observed to have more scalable and robust PEs when the volume-wise resolutions at the test time deviated from those used during training. This finding highlights the VLFAT's potential for resource-efficient training and inference."
Transformer-Based End-to-End Classification of Variable-Length Volumetric Data,5,Conclusions,"In this paper, we propose an end-to-end framework for 3D volume classification of variable-length scans, benefiting from ViT to process volume slices and FAT to capture 3D information. Furthermore, we enhance the capacity of PE in FAT to capture sequential dependencies along volumes with variable resolutions. Our proposed approach, VLFAT, is more scalable and robust than vanilla FAT at classifying OCT volumes of different resolutions. On a large-scale retinal OCT datasets, our results indicate that this effective method performs in the majority of cases better than other common methods for volume classification. Besides its applicability for volumetric medical data analysis, our VFLAT has potential to be applied on other medical tasks including video analysis (e.g. ultrasound videos) and high-resolution imaging, as is the case in histopathology. Future work would include adapting VLFAT to ViViT models to make them less computationally expensive. Furthermore, PEs in VLFAT could be leveraged for improving the visual interpretation of decision models by collecting positional information about the adjacent slices sharing anatomical similarities."
Transformer-Based End-to-End Classification of Variable-Length Volumetric Data,,Fig. 1 .,
Transformer-Based End-to-End Classification of Variable-Length Volumetric Data,,Fig. 2 .,
Transformer-Based End-to-End Classification of Variable-Length Volumetric Data,,Fig. 3 .,
Transformer-Based End-to-End Classification of Variable-Length Volumetric Data,,Table 1 .,Legend: * #slices at the test time; + the input length is fixed in both training and test time
Transformer-Based End-to-End Classification of Variable-Length Volumetric Data,,Table 2 .,
Progressive Attention Guidance for Whole Slide Vulvovaginal Candidiasis Screening,1,Introduction,"Vulvovaginal candidiasis (VVC) is a type of fungal infection caused by candida, which results in discomforting symptoms, including itching and burning in the genital area [4,18]. It is the most prevalent human candidal infection, estimated to afflict approximately 75% of all women at least once in their lifetime [1,20], resulting in huge consumption of medical resources. Currently, thin-layer cytology (TCT) [6] is one of the main tools for screening cervical abnormalities. Manual reading upon whole slide image (WSI) of TCT is time-consuming and labor-intensive, which limits the efficiency and scale of disease screening. Therefore, automatic computer-aided screening for candida would be a valuable asset, which is low-cost and effective in the fight against infection.Previous studies for computer-aided VVC diagnosis were mainly based on pap smears rather than WSIs. For example, Momenzadeh et al. [11] implemented automatic diagnosis based on machine learning. Peng et al. [13] compared different CNN models on VVC classification. Some works also applied deep learning to classify candida in other body parts [2,24]. In recent years, TCT has become mainstream in cervical disease screening compared to pap smear [8]. Many systems of automatic computer-aided WSI screening have been designed for cytopathology [22,23], and histopathology [17,21]. However, partially due to the limited data and annotation, screening for candidiasis is mostly understudied.Computer-aided diagnosis for candidiasis through WSI is highly challenging (see examples in Fig. 1). (1) Candida is hard to localize in a large WSI, especially due to its long-stripe shape, low-contrast appearance, and often occlusion with respect to nearby cells. The representation of candida is easily dominated by other objects in deep layers of a network. (2) In addition to occupying only a small image space for each candida, the overall candida quantity in WSIs is also low compared to the number of other cells. The class imbalance makes it difficult to conduct discriminative learning and to find candida. (3) The staining of different samples leads to the huge style gap between WSIs. While collecting more candida data may contribute to a more robust network, such efforts are dwarfed by the inhomogeneity of WSIs, which adds to the risk of overfitting. All of the above issues make it difficult for diagnostic models to focus on candida, thus resulting in poor classification performance and generalization capability.In this paper, we find that the attention for a deep network to focus on candida is the key to the high performance of the screening task. And we propose a series of strategies to make the model focus on candida progressively. Our contributions are summarized into three parts: (1) We use a detection task to pre-train the encoder of the classification model, moving the network's attention away from individual cells and onto candida-like objects; (2) We propose skip self-attention (SSA) to take into account multi-scale semantic and texture fea- tures, improving network attention to the candida that is severely occluded or with long hyphae; (3) Contrastive learning [3] is applied to alleviate the overfitting risk caused by the style gap and to improve the ability to discern candida."
Progressive Attention Guidance for Whole Slide Vulvovaginal Candidiasis Screening,2,Method,"We use a hierarchical framework for cervical candida screening, concerning the huge size of WSI and the infeasibility of handling a WSI scan in one shot. The overall pipeline of our framework is presented in Fig. 2. Given a WSI, we first crop it into multiple images, each of which is sized 1024×1024. For each cropped image, we conduct image-level classification to find out whether it suffers from suspicious candida infection. The image-level classifier produces a score and feature representation of the image under consideration. Then scores and features from all cropped images are reorganized and aggregated by a transformer for final classification by a fully connected (FC) layer."
Progressive Attention Guidance for Whole Slide Vulvovaginal Candidiasis Screening,2.1,Detection Task for Pre-training,"We use a pre-trained detection model as prior to initialize the classification model. In experimental exploration, we find that, if we train the detection network directly, the bounding-box annotation indicates the location of candida and can rapidly establish a rough understanding of the morphology of candida. However, the positioning task coming with detection lacks enough granularity, resulting in relatively low precision to discern cell edges or folding from candida. Meanwhile, directly training a classification model is usually easier to converge. However, in such a task, as candida occupies only a few pixels in an image, it is difficult for the classifier to focus on the target. That, the attention of the classifier may spread across the entire image, leading to overfitted training quickly.Therefore, we argue that the detection and classification tasks are complementary to solve our problem. Particularly, we pre-train a detector and inherit its advantages in the classifier. We use Retinanet [10], which is composed of a backbone attached with FPN (Feature Pyramid Network, FPN, [9]) and a detection head, as shown in (Fig. 3). We chose the same encoder architecture (Resnet [5]) for the detection and classification networks. To train the encoder with the detection task (Fig. 3), we use bounding-box annotations to supervise Fig. 3. Attention Guided Image-level Classification (corresponding to the classification model in Fig. 2). The same parameters are used between modules marked ""shared"".Retinanet. We then initialize the classification network by directly loading the encoder parameters and freezing the first few layers during the training of the classification network. Note that pre-training not only discards the complex positioning task but also makes it easier for the classification network to converge especially in the early stage of training."
Progressive Attention Guidance for Whole Slide Vulvovaginal Candidiasis Screening,2.2,Transformer with Skip Self-Attention (SSA),"We design a novel skip self-attention (SSA) module to fuse discriminative features of candida from different scales. At a fine-grained level, the hyphae and spores of candida are usually the basis for judging. Yet we need to distinguish them from easily distorting factors such as contaminants in WSIs and edges of nearby cells. At a coarse-grained level, there is the phenomenon that a candida usually links multiple host cells and yields a string of them. Thus it is necessary to combine long-range visual cues that span several cells to derive the decision related to candida.CNN-based methods have achieved excellent performance in computer-aided diagnosis including cervical cancer [22]. However, the unique shape and appearance of the candidate incur troubles for CNN-based classifiers, whose spatial field of view can be relatively limited. In recent years, vision transformer (ViT) has been widely used in visual tasks for its global attention mechanism [14], sensitivity to shape information in images [19], and robustness to occlusion [12]. Nevertheless, such a transformer can be hard to train for our task, due to the large image size, huge network parameters, and huge demand for training data. Therefore, to adapt to the shape and appearance of candida, we propose the SSA module and apply it to ViT for efficient learning. Specifically, we use the pre-trained CNN-based encoder to extract features for each cropped image. The feature maps extracted after the first layer is considered low-level, which contains fine-grained texture information. On the contrary, the feature maps extracted from the last layer are high-level, which represents semantics regarding candida. To combine the low-and high-level features, we regard the low-level features as queries (Q), and the high-level features as keys (K) and values (V). For each patch in the ViT scheme, the transformer decoder computes the attention between low-and high-level features and combines them. The class token 'CLS' is used for the final classification. The combined feature maps can offer more representative information so that the classifier focuses more on different scales to long-range candida. Meanwhile, the extra SSA structure is simple, which causes a low computation burden."
Progressive Attention Guidance for Whole Slide Vulvovaginal Candidiasis Screening,2.3,Contrastive Learning,"As mentioned in Sect. 1, the style gap is another problem, which makes overfitting more severe. In this part, we adopt the strategy of contrastive learning to alleviate such problems and further optimize the attention of the network. Our approach has two key goals: (1) to ensure that the features from the original image remain consistent after undergoing various image augmentations, and (2) to construct an image without the region of candida, resulting in highly dissimilar features compared to the original.Inspired by a weakly supervised learning segmentation method [7], we construct a contrastive learning method, which will be described in detail in the following sections, as shown in Fig. 3.To achieve this, we use augmentation and the attention map generated during the training process to construct three types of images and apply contrastive learning to the features extracted from them. For a given image I, we use image augmentation to generate I aug and use the encoder attached with SSA to extract feature, F c aug . The attention map A is transformed from F c aug by an attention extractor. The attention extractor uses FC (the same params as that of the classifier) to reduce the channels of features (except the class token) to 2 (candida and others), then reshape the features representing candida to a feature map, and applies bilinear interpolation to upsample it to the same size of I. Equation 1 normalizes A to the interval [0,1], obtaining M to represent the likelihood of candida distribution. We get the masked image I masked by subtracting M from I.where σ and s are used to adjust the range of values, set to 0.5 and 10.As shown in Fig. 3, the features F , F aug and F masked from the three types of images I, I aug and I masked by the shared classifier. In our task, we hope that the style gap does not affect the feature extraction of the image, so the distance between F aug and F should be attracted. At the same time, we hope that F masked should not contain the characteristics of candida, which is repelled from F aug . To achieve our goal, we introduce triplet loss [15] for contrastive learning as shown in the first part of Eq. 2.In addition, we use two constraints, leading to more stable and robust training. If our network has effective attention, the masked image should not contain any candida, so the score of the Candida category after the mask S(I masked ) should be minimized. We use the attention mining loss to handle this, as shown in the second part of Eq. 2. Additionally, we need the attention to cover only the partial area around candida, without false positive regions. Otherwise, attention maps that cover the whole image can also result in low L tri . We take the average grayscale of attention map M as a restriction, as shown in the last part of Eq. 2. L tri ,L am , and L focus are combined as L cl to constrain each other and take full advantage of contrastive learning, as shown in Eq. 2.Finally, we use the cross-entropy loss L ce to calculate the classification loss with labels. The total loss during training can be expressed as shown in Eq. 3. α is a hyper-parameter, set to 0.1.(3)"
Progressive Attention Guidance for Whole Slide Vulvovaginal Candidiasis Screening,2.4,Aggregated Classification for WSI,"With the strategies above, we have built the classifier for all cropped images in a WSI. Then we can finish the pipeline of WSI-level classification, which is shown as part of Fig. 2. Specifically, for each cropped image, we conduct image-level classification to find out whether it suffers from suspicious candida infection.The image-level classification also produces a score, as well as the feature representation of the image under consideration. Then, we reorganize features from all images by ranking their scores and preserving that with top-k scores. We complete the aggregation of the top-k features by the transformer and make the WSI-level decision by an FC layer in the final."
Progressive Attention Guidance for Whole Slide Vulvovaginal Candidiasis Screening,3,Experimental Results,"Datasets and Experimental Setup. Our samples were collected by a collaborating clinical institute in 2021. Each sample is scanned into a WSI following standard cytology protocol, which can be further cropped to 500 images sized 1024×1024.For pre-training the detector, we prepare 1467 images with the size of 1024×1024 pixels, all of which have bounding-box annotations. The ratio of training and validation is 4:1.For training of the image-level classification model, we use 1940 positive images (1467 of which are used in detector pre-training) and 2093 negative images. All images used to pre-train the detector are categorized as training data here. The rest 473 images are split in 5-fold cross-validation, from which we collect experimental results and report later. The ratio of training, validation, and testing is 3:1:1. At the WSI level, we use two datasets. Dataset-Small is balanced with 100 positive WSIs and 100 negative WSIs. We conduct a 5-fold cross-validation, and the ratio of training, validation, and testing is 3:1:1. We further validate upon an imbalanced Dataset-Large of 7654 WSIs. There are only 140 positive WSIs in this dataset, which is closer to real world. These two WSI-level datasets have no overlay with the data used to train the above detection and classification tasks.For implementation details, the models are implemented by PyTorch and trained on 4 Nvidia Tesla V100S GPUs. All parameters are optimized by Adam for 100 epochs with the initial learning rate of 3 × 10 -4 . The batch sizes of the detection task, image-level classification, and WSI-level classification are 8, 8, 16, respectively. To aggregate WSI classification, we use top-10 cropped images and their features. We report the performance using five common metrics: area under the receiver operating characteristic curve (AUC), accuracy (ACC), sensitivity (Sen), specificity (Spe), and F1-score.Comparisons for Image-Level Classification. We conduct an ablation study to evaluate the contribution of pre-training (PT), skip self-attention (SSA), and contrastive learning (CL) for the image-level classification, as shown in Table 1. It is observed that with all our proposed components, the network reaches the highest AUC 97.20, which is 11.49% higher than the baseline. PT shows improvement in all situations, as a reasonable initial focus provides a solid foundation. SSA and CL can bring 2.89% and 6.38% improvement respectively compared to the method without each of them. It shows that SSA and CL can perform better when the model already has the basic ability to localize candida, i.e., after PT.To verify whether our model focuses on important regions of the input image for accurate classification, we visualize the model's attention using Grad-CAM [16]. We present two examples in Fig. 4. We can see in Fig. 4(b) that the baseline's attention is very scattered spatially. After PT, the model can focus on the candida area, edges of cells, and folds that resemble candida, as shown in Fig. 4(c). After adding the SSA module, more texture information is used to distinguish with cells, as shown in Fig. 4(d). Finally, CL helps the model better narrow its attention, focusing on the most important part as shown in Fig. 4(e). These comparisons demonstrate that our proposed method effectively guides and corrects the model's attention.Comparisons for WSI-Level Classification. We compare our proposed method to other methods in the whole slide of cervical disease screening. To save computation, we did not verify the performance of the methods that performed too poorly on Dataset-Small. The detection-based method [23] uses a detection network to get suspicious candida and classify WSIs with average confidence. Resnet trained without our method is the same as the baseline in Table 1. At the WSI level, we compare our method with traditional classifiers and a multiinstance learning method TransMIL [17]. We both considered the original Trans-MIL with pre-trained Resnet-50 and the modified version with our image-level encoder. Table 2 shows that our method reaches the highest AUC of 95.78% and is the most stable. Our attention-based method brings 6% improvement of accuracy on Data-Small compared to other methods with the same WSI-level method 'Threshold'. Transformer shows a better capacity of feature aggregation than other WSI-level classifiers, raising the AUC on Dataset-Large to 84.18%."
Progressive Attention Guidance for Whole Slide Vulvovaginal Candidiasis Screening,4,Conclusion,"We introduced a novel attention-guided method for VVC screening, which can progressively correct the attention of the model. We pre-train a detection task for the initialization, then add SSA to fuse features from coarse and fine-grained, and finally narrower attention with contrastive learning. After obtaining accurate attention and good generalization for the image-level classifier, we reorganized and ensemble features from slices, and make a diagnosis. Both numerical metrics and visualization results show the effectiveness of our model. In the future, we would like to explore the method of weakly supervised learning to make use of a huge number of unlabeled images and jointly train the image-level and WSI-level models."
Progressive Attention Guidance for Whole Slide Vulvovaginal Candidiasis Screening,,Fig. 1 .,
Progressive Attention Guidance for Whole Slide Vulvovaginal Candidiasis Screening,,Fig. 2 .,
Progressive Attention Guidance for Whole Slide Vulvovaginal Candidiasis Screening,,Fig. 4 .,
Progressive Attention Guidance for Whole Slide Vulvovaginal Candidiasis Screening,,,
Progressive Attention Guidance for Whole Slide Vulvovaginal Candidiasis Screening,,Table 1 .,
Progressive Attention Guidance for Whole Slide Vulvovaginal Candidiasis Screening,,Table 2 .,
Frequency-Mixed Single-Source Domain Generalization for Medical Image Segmentation,1,Introduction,"Due to the superiority in image representation, tremendous success has been achieved in medical image segmentation through recent advancements of deep learning [10]. Nevertheless, sufficient labeled training data is necessary for deep learning to learn state-of-the-art segmentation networks, resulting in the burden of costly and labor-intensive pixel-accurate annotations [2]. Consequently, annotation scarcity has become a pervasive bottleneck for clinically deploying deep networks, and existing similar datasets have been resorted to alleviate the annotation burden. However, networks trained on a single-source dataset may suffer performance dropping when applied to clinical datasets, since neural networks are sensitive to domain shifts.Consequently, domain adaptation (DA) and DG [14] have been leveraged to mitigate the impact of domain shifts between source and target domains/datasets. Unfortunately, DA relies on a strong assumption that source and target data are simultaneously accessible [4], which does not always hold in practice. Thereby, DG has been introduced to overcome the absence of target data, which learns a robust model from distinct source domains to generalize to any target domain. To efficiently transfer domain knowledge across various source domains, FACT [12] has been designed to adapt the domains by swapping the low-frequency spectrum of one with the other. Considering privacy protection in medical scenarios, federated learning and continuous frequency space interpolation were combined to achieve DG on medical image segmentation [5]. More recently, single-source domain generalization (SDG) [9] has been proposed to implement DG without accessing multi-source domains. Based on global intensity non-linear augmentation (GIN) and interventional pseudocorrelation augmentation (IPA), a causality-inspired SDG was designed in [7]. Although DG has boosted the clinical practice of deep neural networks, troublesome challenges still remain in clinical deployment. 1) Data from multi-source domains are commonly required to implement DG, which is costly and even impractical to collect in clinics. 2) Medical data sharing is highly concerned, accessing multi-source domains exacerbates the risk of data breaching. 3) Additional generative networks may constrain algorithms' efficiency and versatility, negatively impacting clinical deployment.To circumvent the above challenges, a frequency-mixed single-source domain generalization strategy, called FreeSDG, is proposed in this paper to learn generalizable segmentation models from a single-source domain. Specifically, the impact of frequency on domain discrepancy is first explored to test our hypotheses on domain augmentation. Then based on the hypotheses, diverse frequency views are extracted from medical images and mixed to augment the single-source domain. Simultaneously, a self-supervised task is posed from frequency views to learn robust context-aware representations. Such that the representations are injected into the vanilla segmentation task to train segmentation networks for out-of-domain inference. Our main contributions are summarised as follows: -We design an efficient SDG algorithm named FreeSDG for medical image segmentation by exploring the impact of frequency on domain discrepancy and mixing frequency views for domain augmentation. -Through identifying the frequency factor for domain discrepancy, a frequencymixed domain augmentation (FMAug) is proposed to extend the margin of the single-source domain. -A self-supervised task is tailored with FMAug to learn robust context-aware representations, which are injected into the segmentation task. -Experiments on various medical image modalities demonstrate the effectiveness of the proposed approach, by which data dependency is alleviated and superior performance is presented when compared with state-of-the-art DG algorithms in medical image segmentation."
Frequency-Mixed Single-Source Domain Generalization for Medical Image Segmentation,2,Methodology,"Aiming to robustly counter clinical data from unknown domains, an SDG algorithm for medical image segmentation is proposed, as shown in Fig. 1. A generalizable segmentation network is attempted to be produced from a single-source domain (x, m) ∼ D(x, m), where m ∈ R H×W is the segmentation mask for the image x ∈ R H×W ×3 . By mixing frequency spectrums, FMAug is executed to augment the single-source domain, and self-supervision is simultaneously acquired to learn context-aware representations. Thus a medical image segmentation network capable of out-of-domain generalization is implemented from a single-source domain. "
Frequency-Mixed Single-Source Domain Generalization for Medical Image Segmentation,2.1,Frequency-controlled Domain Discrepancy,"Generalizable algorithms have been developed using out-of-domain knowledge to circumvent the clinical performance dropping caused by domain shifts. Nevertheless, extra data dependency is often inevitable in developing the generalizable algorithms, limiting their clinical deployment. To alleviate the data dependency, a single source generalization strategy is designed inspired by the Fourier domain adaption [13] and generalization [12].According to [12,13], the domain shifts between the source and target could be reduced by swapping/integrating the low-frequency spectrum (LFS) of one with the other. Thus we post two hypotheses:1) uniformly removing the LFS reduces inter-and inner-domain shifts; 2) discriminatively removing the LFS from a single domain increases innerdomain discrepancy.Various frequency views are thus extracted from medical images with changing parameters to verify the above hypotheses. Denote the frequency filter with parameters θ n as F n (•), where n ∈ R N +1 refers to the index of parameters. Following [3,4], a frequency view acquired with θ n from an image x is given by xn = F n (x) = x -x * g(r n , σ n ), where g(r n , σ n ) denotes a Gaussian filter with radius r n ∈ [5,50] and spatial constant σ n ∈ [2,22]. Then the frequency views are converted to vectors by a pre-trained ResNet-18 and t-SNE is employed to demonstrate the domain discrepancy controlled by the low-frequency spectrum.As shown in Fig. 2, compared to the raw images, the distribution of various datasets is more clustered after the uniform LFS removement, which indicates domain shift reduction. While the domain discrepancy in DRIVE is increased by discriminatively removing the LFS. Accordingly, these hypotheses can be leveraged to implement SDG."
Frequency-Mixed Single-Source Domain Generalization for Medical Image Segmentation,2.2,Frequency-mixed Domain Augmentation,"Motivated by the hypotheses, domain augmentation is implemented by F n (•) with perturbed parameters. Moreover, the local-frequency-mix is executed to further extend the domain margin, as shown in Fig. 2 (d). As exhibited in the blue block of Fig. 1, random patches are cut from a frequency view and mixed with diverse ones to conduct FMAug, which is given bywhere M ∈ 0, 1 W ×H is a binary mask controlling where to drop out and fill in from two images, and is element-wise multiplication. k = (i -1) × N + (j -1) denotes the index of the augmentation outcomes, where i, j ∈ R N , i = j. Notably, self-supervision is simultaneously acquired from FMAug, where only patches from N frequency views xn , n ∈ R N are mixed, and the rest one x0 is cast as a specific view to be reconstructed from the mixed ones, where (r n , σ n ) = (27, 9). Under the self-supervision, an objective function for learning contextaware representations from view reconstruction is defined as(where xk refers to the view reconstructed from xk , K = N × (N -1). Consequently, FMAug not only extends the domain discrepancy and margin, but also poses a self-supervised pretext task to learn generalizable context-aware representations from view reconstruction."
Frequency-Mixed Single-Source Domain Generalization for Medical Image Segmentation,2.3,Coupled Segmentation Network,"As the FMAug promises domain-augmented training data and generalizable context-aware representations, a segmentation model capable of out-of-domain inference is waiting to be learned. To inject the context-aware representations into the segmentation model seamlessly, a coupled network is designed with attention mechanisms (shown in the purple block of Fig. 1), which utilize the most relevant parts of representation in a flexible manner.Concretely, the network comprises an encoder E and two decoders D sel , D seg , where skip connection bridges E and D sel while D sel marries D seg using attention mechanisms. For the above pretext task, E and D sel compose a U-Net architecture to reconstruct x0 from xk with the objective function given in Eq. 2. On the other hand, the segmentation task shares E with the pretext task, and introduces representations from D sel to D seg . The features outcomes from the l-th layer of D seg are given bywhere f l sel refers to the features from the l-th layer of D sel . Additionally, attention modules are implemented to properly couple the features from D sel and D seg . D l seg imports and concatenates f l-1 seg and f l-1 sel as a tensor. Subsequently, the efficient channel and spatial attention modules proposed by [11] are executed to couple the representations learned from the pretext and segmentation task.Then convolutional layers are used to generate the final outcome f l seg . Accordingly, denote the segmentation result from xk as mk , the objective function for segmentation task is given bywhere m denotes the ground-truth segmentation mask corresponding to the original source sample x. Therefore, the overall objective function for the network is defined aswhere α is the hyper-parameter to balance L sel and L seg ."
Frequency-Mixed Single-Source Domain Generalization for Medical Image Segmentation,3,Experiments,"Implementation: Five image datasets of three modalities were collected to conduct segmentation experiments on fundus vessels and articular cartilage. For fundus vessels, training was based on 1) DRIVE The image data were resized to 512 × 512, the training batch size was 2, and Adam optimizer was used. The model was trained according to an earlystop mechanism, which means the optimal parameter on the validation set was selected in the total 200 epochs, where the learning rate is 0.001 in the first 80 epochs and decreases linearly to 0 in the last 120 epochs. The encoder and two decoders are constructed based on the U-net architecture with 8 layers. The comparisons were conducted with the same setting and were quantified by DICE and Matthews's correlation coefficient (Mcc)."
Frequency-Mixed Single-Source Domain Generalization for Medical Image Segmentation,,Comparison and Ablation Study:,"The effectiveness of the proposed algorithm is demonstrated in comparison with state-of-the-art methods and an ablation study. The Fourior-based DG methods FACT [12], FedDG [5], and the whitening-based DG method SAN-SAW [8], as well as the SGD method GIN-IPA [7] were compared, where CE-Net [1] and CS-Net [6] were served as the base models cooperated with FACT [12]. Then in the ablation study, FMAug, selfsupervised learning (SSL), and attention mechanisms (ATT) were respectively removed from the proposed algorithm.(1) Comparison. Quantified comparison of our algorithm with the competing methods is summarized in Table 1, where segmentation results in three modalities and data dependency are exhibited. Due to the domain shifts between DRIVE and LES-AV as well as IOSTAR, interior performance are presented by CE-Net [1] and CS-Net [6], which are only learned from DRIVE without DG. Due to the substantial domain discrepancy, EyePACS were treated as multiple source domains to implement DG. FACT [12] boosts the generalization by transferring LFS across the multi-source domains, and efficiently promotes the performance of CE-Net [1] and CS-Net [6]. FedDG [5] were then respectively trained using DRIVE perturbed by EyePACS. As SAN-SAW [8] was designed for region structure segmentation, it appears redundant in the vessel structure task. Thanks to coupling federated learning and contrastive learning, reasonable performance are provided by FedDG [5]. GIN-IPA [7] and our FreeSDG were learned based on the single source domain of DRIVE. Through augmenting the source domain with intensity variance and consistency constraint, GIN-IPA [7] performs decently on out-of-domain inference. The proposed FreeSDG allows for learning efficient segmentation models only from DRIVE. Therefore, our FreeSDG outperforms the state-of-the-art methods without extra data dependency. Additionally, an iden- tical situation is observed from the results of ultrasound data, further validating the effectiveness of our algorithm.Visualized comparison is shown in Fig. 3. Uneven brightness in LES-AV impacts the segmentation performance, vessels in the highlight box are ignored by most algorithms. Cooperating with FACT [12], CE-Net [1] achieves impressive performance. The remarkable performance of GIN-IPA [7] indicates that SDG is a promising paradigm for generalizable segmentation. In the cross-modality segmentation in IOSTAR, CE-Net [1] married with FACT [12] and GIN-IPA [7] still performs outstandingly. In addition, decent segmentation is also observed from FedDG [5] via DG with multi-source domains. FreeSDG efficiently recognizes the variational vessels in LES-AV and IOSTAR, indicating its robustness and generalizability in the quantitative comparison. Furthermore, FreeSDG outperforms the competing methods in accurately segmenting low-contrast cartilage of ultrasound images. In nutshell, our SDG strategy promises FreeSDG prominent performance without extra data dependency.(2) Ablation Study. According to Table 1, the ablation study also validates the effectiveness of the three designed modules. Through FMAug, an augmented source domain with adequate discrepancy is constructed for training generalizable models. Robust context-aware representations are extracted from self-supervised learning, boosting the downstream segmentation task. Attention mechanisms seamlessly inject the context-aware representations into segmentation, further improving the proposed algorithm. Therefore, a promising segmentation model for medical images is learned from a single-source domain."
Frequency-Mixed Single-Source Domain Generalization for Medical Image Segmentation,4,Conclusion,"Pixel-accurate annotations have long been a common bottleneck for developing medical image segmentation networks. Segmentation models learned from a single-source dataset always suffer performance dropping on out-of-domain data. Leveraging DG solutions bring extra data dependency, limiting the deployment of segmentation models. In this paper, we proposed a novel SDG strategy called FreeSDG that leverages a frequency-based domain augmentation technique to extend the single-source domain discrepancy and injects robust representations learned from self-supervision into the network to boost segmentation performance. Our experimental results demonstrated that the proposed algorithm outperforms state-of-the-art methods without requiring extra data dependencies, providing a promising solution for developing accurate and generalizable medical image segmentation models. Overall, our approach enables the development of accurate and generalizable segmentation models from a single-source dataset, presenting the potential to be deployed in real-world clinical scenarios."
Frequency-Mixed Single-Source Domain Generalization for Medical Image Segmentation,,Fig. 1 .,
Frequency-Mixed Single-Source Domain Generalization for Medical Image Segmentation,,Fig. 2 .,
Frequency-Mixed Single-Source Domain Generalization for Medical Image Segmentation,,Fig. 3 .,
Frequency-Mixed Single-Source Domain Generalization for Medical Image Segmentation,,Table 1 .,
Vision Transformer Based Multi-class Lesion Detection in IVOCT,1,Introduction,"Despite the rapid development of new detection and treatment methods, the prevalence of cardiovascular disease continues to increase [1]. It is still reported to be the most prevalent and deadly disease worldwide, with more than 1 million people diagnosed with acute coronary syndrome (ACS) in the U.S. in 2016. The average cost of hospital discharge for ACS patients is as high as $63,578 [2], which significantly increasing the financial burden on society and patients.Optical coherence tomography (OCT) [3] is a new biomedical imaging technique born in the 19901990ss. Intravascular optical coherence tomography (IVOCT) [4] has a higher resolution compared with other imaging modalities in the vasculature and is considered to be the best imaging tool for plaque rupture, plaque erosion, and calcified nodules [5]. Therefore, most existing work on IVOCT images focuses on identifying vulnerable plaques in the vasculature [6][7][8][9], while neglecting other characteristic manifestations of atherosclerotic plaques in IVOCT images, such as macrophage infiltration and thrombus formation. These lesions are closely related to the development of plaque changes [10]. Studies have shown that atherosclerosis is an inflammatory disease dominated by macrophages and T lymphocytes, that a high density of macrophages usually represents a higher risk, and that thrombosis due to plaque rupture is a common cause of acute myocardial infarction [11,12]. In addition, some spontaneous coronary artery dissection (SCAD) can be detected in IVOCT images. The presence of the dissection predisposes to coronary occlusion, rupture, and even death [13,14]. These lesions are inextricably linked to ACS. All three types of features observed through IVOCT images are valuable for clinical treatment, as shown in Fig. 1. These lesions are inextricably linked to ACS and should be considered in clinical management. Achieving multi-class lesion detection in IVOCT images faces two challenges: 1) There is no public IVOCT dataset specifically designed for multi-class lesion detection. Most IVOCT datasets only focus on a single lesion, and research on the specific types of lesions in the cardiovascular system is still in its early stage.2) It is difficult to distinguish between different lesions, even for senior radiologists. This is because these lesions vary in size and appearance within the same class, and some of them do not have regular form, as shown in Fig. 1. In clinical diagnosis, radiologists usually combine different pathological manifestations, lesion size, and the continuous range before and after in the IVOCT image to design accurate treatment strategies for patients. Unfortunately, most existing works ignore such information and do not consider the continuity of lesions in the 3D dimension. To address the above issues, we collaborated with the Cardiovascular Research Center of Sichuan Provincial People's Hospital to collect an IVOCT dataset and introduce a novel detection model that leverages the information from consecutive IVOCT images.Overall, the contribution of this work can be summarized as follows: 1) We propose a new IVOCT dataset that is the first multi-class IVOCT dataset with bounding box annotations for macrophages, cavities/dissections, and thrombi. 2) We design a multi-class lesion detection model with a novel self-attention module that exploits the relationship between adjacent frames in IVOCT, resulting in improved performance. 3) We explore different data augmentation strategies for this task. 4) Through extensive experiments, we demonstrate the effectiveness of our proposed model."
Vision Transformer Based Multi-class Lesion Detection in IVOCT,2,Dataset,"We collected and annotated a new IVOCT dataset consisting of 2,988 IVOCT images, including 2,811 macrophages, 812 cavities and dissections, and 1,111 thrombi. The collected data from 69 patients are divided into training/validation/test sets in a 55:7:7 ratio, respectively. Each split contains 2359/290/339 IVOCT frames. In this section, we will describe the data collection and annotation process in detail."
Vision Transformer Based Multi-class Lesion Detection in IVOCT,2.1,Data Collection,"We collaborated with the Cardiovascular and Cerebrovascular Research Center of Sichuan Provincial People's Hospital, which provided us with IVOCT data collected between 2019 and 2022. The data include OCT examinations of primary patients and post-coronary stenting scenarios. Since DICOM is the most widely-used data format in medical image analysis, the collecting procedure was exported to DICOM, and the patient's name and other private information contained in DICOM were desensitized at the same time. Finally, the 69 DICOM format data were converted into PNG images with a size of 575 × 575 pixels. It is worth noting that the conversion from DICOM to PNG did not involve any downsampling operations to preserve as much information as possible."
Vision Transformer Based Multi-class Lesion Detection in IVOCT,2.2,Data Annotation,"In order to label the lesions as accurately as possible, we designed a two-step annotation procedure. The first round was annotated by two expert physicians using the one-stop medical image labeling software Pair. Annotations of the two physicians may be different. Therefore, we asked them to discuss and reach agreement on each annotation. Next, the annotated data was sent to senior doctors to review. The review starts with one physician handling the labeling, including labeling error correction, labeling range modification, and adding missing labels. After that, another physician would continue to check and review the previous round's results to complete the final labeling. Through the above two steps, 2,988 IVOCT images with 4,734 valid annotations are collected."
Vision Transformer Based Multi-class Lesion Detection in IVOCT,3,Methodology,"Recently, object detection models based on Vision Transformers have achieved state-of-the-art (SOTA) results on various object detection datasets, such as the MS-COCO dataset. Among them, the Swin Transformer [19] model is one of the best-performing models. Swin Transformer uses a self-attention mechanism within local windows to ensure computational efficiency. Moreover, its sliding window mechanism allows for global modeling by enabling self-attention computation between adjacent windows. Its hierarchical structure allows flexible modeling of information at different scales and is suitable for various downstream tasks, such as object detection."
Vision Transformer Based Multi-class Lesion Detection in IVOCT,3.1,G-Swin Transformer,"In traditional object detection datasets such as the MS-COCO dataset, the images are typically isolated from each other without any correlation. However, in our proposed IVOCT dataset, each IVOCT scan contains around 370 frames with a strong inter-frame correlation. Specifically, for example, if a macrophage lesion is detected at the [x, y, w, h] position in frame F i of a certain IVOCT scan, it is highly likely that there is also a macrophage lesion near the [x, y, w, h] position in frame F i-1 or F i+1 , due to the imaging and pathogenesis principles of IVOCT and ACS. Doctors also rely on the adjacent frames for diagnosis rather than a single frame when interpreting IVOCT scans. But, the design of the Swin-Transformer did not consider the utilization of inter-frame information. Though global modeling is enabled by using the sliding window mechanism. In the temporal dimension, it still has a locality because the model did not see adjacent frames.Based  feature pyramid network (FPN) for fusion of features at different resolutions. The RPN Head is then applied to obtain candidate boxes, and finally, the ROI Head is used for classification and refinement of candidate boxes to obtain class and bbox (bounding box) predictions. The inter-frame feature fusing is happend in the attention block, introduced in the next subsection."
Vision Transformer Based Multi-class Lesion Detection in IVOCT,3.2,Grid Attention,"To better utilize information from previous and future frames and perform feature fusion, we propose a self-attention calculation mode called ""Grid Attention"". The structure shown in Fig. 3 is an application of Grid Attention. The input of the block is 3 feature maps respectively from frames 0, 1, and 2. (Here we use k = 3.) Before entering the W-MSA module for multi-head self-attention calculation, the feature maps from different frames are fused together.Based on the feature map of the key frame (orange color), the feature maps of the previous (blue) and next (green) frames first do a dimensional reduction from [H, W, C] to [H, W, C/2]. Then they are down-sampled and a grid-like feature map are reserved. The grid-like feature map are then added to key-frame feature map, and the fusion progress finishes. In the W-MSA module, the self-attention within the local window and that between adjacent local windows are calculated, and the inter-frame information is fully used. The local window of key-frame has contained information from other frames, and self-attention calculation happens in inter-frames. The frame-level feature modeling can thus be achieved, simulating the way that doctors view IVOCT by combining information from previous and next frames. During feature fusion with Grid Attention, the feature maps from different frames are fused together in a grid-like pattern (as shown in the figure). The purpose of this is to ensure that when dividing windows, half of the grid cells within a window come from the current frame, and the other half come from other frames. If the number of channels in the feature map is C, and the number of frames being fused is 3 (current frame + previous frame + next frame), then the first C/2 channels will be fused between the current frame and the previous frame, and the last C/2 channels will be fused between the current frame and the next frame. Therefore, the final feature map consists of 1/4 of the previous frame, 1/2 of the current frame, and 1/4 of the next frame. The impact of the current frame on the new feature map remains the largest, as the current frame is the most critical frame."
Vision Transformer Based Multi-class Lesion Detection in IVOCT,4,Experiments,"Baseline Methods and Evaluation Metrics. The baseline is based on a PyTorch implementation of the open-source object detection toolbox MMDetection. We compare our proposed approach with Swin Transformer and four CNNbased network models including Faster-RCNN [15], YOLOv3 [16], YOLOv5 [17], Retinanet [18]. All the baseline model is pre-trained on the ImageNet dataset.To ensure objective comparison, all experiments were conducted in the MMdetection framework. The metric we used is the AP /AR for each lesion and the mAP , based on the COCO metric and the COCO API (the default evaluation method in the MMdetection framework). We trained the model for 60 epochs with an AdamW optimizer following Swin Transformer. The learning  rate and weight decay is set to be 1e-4 and 1e-2, respectively. The batch size is set to be 2.Quantitative and Qualitative Results. All experiments are conducted on our newly-collected dataset. Each model is trained on the training set, selected based on the performance of the validation set, and the reported results are obtained on the test set. Table 1 shows the comparison between the baseline methods and the G-Swin Transformer method. Methods based on Swin Transformer outperformed the four baseline methods in terms of precision and recall, and our proposed G-Swin Transformer outperforms the baseline method Swin Transformer by 2.15% in mAP. Figure 4 compares some results of our method and baselines. The first row is the detection of the macrophage. Our method's prediction is the most closed to the ground truth. The second row is the detection of cavities/dissections and thrombi. Only our method gets the right prediction. The YOLOv3, YOLOv5, Faster-RCNN and RetinaNet model failed to detect all lesions, while RetinaNet model even produced some false positive lesions.  Effect of Different Data Augmentation Methods. We compared the impact of different data augmentation strategies in our task. As shown in Table 2, Random Resize and Random Crop had a significant impact on performance improvement. Resize had the greatest impact on the model's performance because different-sized OCT images were generated after data augmentation, and the lesions were also enlarged or reduced proportionally. Since the sizes of lesions in different images are usually different, different-sized lesions produced through data augmentation are advantageous for the model to utilize multi-scale features for learning.Effect of Different Hyper-parameters. Table 3 shows the impact of hyperparameters on the performance of the G-Swin Transformer model. The best mAP was achieved when using a 3-layer image input. Using the upper and lower 5 layers of image input not only increased the training/inference time, but also may not provide more valuable information since frame 0 and frame 4 are too far away from the key frame. The fusion strategy indicates how the feature map from other frames are combined with the key-frame feature map. We can find add them up gets better result then simply replacement. We think this is because by this way, the 1 × 1 convolutional layer can learn a residual weights, keeps more detail of the key-frame.Effect of Fusion Methods. In addition to Grid Attention, there are other methods of feature fusion. The first method is like 2.5D convolution, in which multiple frames of images are mapped into 96-dimensional feature maps directly through convolution in the Linear Embedding layer. This method is the simplest, but since the features are fused only once at the initial stage of the network, the use of adjacent frame features is very limited. The second method is to weight and sum the feature maps of different frames before each Attention Block, giving higher weight to the current frame and lower weight to the reference frames. Table 4 shows the impact of other feature fusion methods on performance. Our method gets better mAP and AR."
Vision Transformer Based Multi-class Lesion Detection in IVOCT,5,Conclusion,"In this work, we have presented the first multi-class lesion detection dataset of IVOCT scans. We have also proposed a Vision Transformer-based model, called G-Swin Transformer, which uses adjacent frames as input and leverages the temporary dimensional information inherent in IVOCT data. Our method outperforms traditional detection models in terms of accuracy. Clinical evaluation shows that our model's predictions provide significant value in assisting the diagnosis of acute coronary syndrome (ACS)."
Vision Transformer Based Multi-class Lesion Detection in IVOCT,,Fig. 1 .,
Vision Transformer Based Multi-class Lesion Detection in IVOCT,,,
Vision Transformer Based Multi-class Lesion Detection in IVOCT,,Fig. 2 .,
Vision Transformer Based Multi-class Lesion Detection in IVOCT,,Fig. 3 .,
Vision Transformer Based Multi-class Lesion Detection in IVOCT,,Fig. 4 .,
Vision Transformer Based Multi-class Lesion Detection in IVOCT,,Table 1 .,
Vision Transformer Based Multi-class Lesion Detection in IVOCT,,Table 2 .,
Vision Transformer Based Multi-class Lesion Detection in IVOCT,,Table 3 .,
Vision Transformer Based Multi-class Lesion Detection in IVOCT,,Table 4 .,
Vision Transformer Based Multi-class Lesion Detection in IVOCT,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 32.
CellGAN: Conditional Cervical Cell Synthesis for Augmenting Cytopathological Image Classification,1,Introduction,"Cervical cancer accounts for 6.6% of the total cancer deaths in females worldwide, making it a global threat to healthcare [6]. Early cytology screening is highly effective for the prevention and timely treatment of cervical cancer [23].Nowadays, thin-prep cytologic test (TCT) [1] is widely used to screen cervical cancers according to the Bethesda system (TBS) rules [21]. Typically there are five types of cervical squamous cells under TCT examinations [5], including normal class or negative for intraepithelial malignancy (NILM), atypical squamous cells of undetermined significance (ASC-US), low-grade squamous intraepithelial lesion (LSIL), atypical squamous cells that cannot exclude HSIL (ASC-H), and high-grade squamous intraepithelial lesion (HSIL). The NILM cells have no cytological abnormalities while the others are manifestations of cervical abnormality to a different extent. By observing cellular features (e.g., nucleus-cytoplasm ratio) and judging cell types, pathologists can provide a diagnosis that is critical to the clinical management of cervical abnormality.After scanning whole-slide images (WSIs) from TCT samples, automatic TCT screening is highly desired due to the large population versus the limited number of pathologists. As the WSI data per sample has a huge size, the idea of identifying abnormal cells in a hierarchical manner has been proposed and investigated by several studies using deep learning [3,27,31]. In general, these solutions start with the extraction of suspicious cell patches and then conduct patch-level classification. The promising performance of cell classification at the patch level is critical, which contributes to sample-level diagnosis after integrating outcomes from many patches in a WSI. However, such a patchlevel classification task requires a large number of annotated training data. And the efforts in collecting reliably annotated data can hardly be negligible, which requires high expertise due to the intrinsic difficulty of visually reading WSIs.To alleviate the shortage of sufficient data to supervise classification, one may adopt traditional data augmentation techniques, which yet may bring little improvement due to scarcely expanded data diversity [26]. Thus, synthesizing cytopathological images for cervical cells is highly desired to effectively augment training data. Existing literature on pathological image synthesis has explored the generation of histopathological images [10,28]. In cytopathological images, on the contrary, cervical cells can be spatially isolated from each other, or are highly squeezed and even overlapped. The spatial relationship of individual cells is complex, adding diversity to the image appearance of color, morphology, texture, etc. In addition, the differences between cell types are mainly related to nuanced cellular attributes, thus requiring fine granularity in modulating synthesized images toward the expected cell types. Therefore, the task to synthesize realistic cytopathological images becomes very challenging.Aiming at augmenting the performance of cervical abnormality screening, we develop a novel conditional generative adversarial network in this paper, namely CellGAN, to synthesize cytopathological images for various cell types. We leverage FastGAN [16] as the backbone for the sake of training stability and computational efficiency. To inject cell type for fine-grained conditioning, a non-linear mapping network embeds the class labels to perform layer-wise feature modulation in the generator. Meanwhile, we introduce the Skip-layer Global Context (SGC) module to capture the long-range dependency of cells for precisely modeling their spatial relationship. We adopt an adversarial learning scheme, where the discriminator is modified in a projection-based way [20] for matching condi- tional data distribution. To the best of our knowledge, our proposed CellGAN is the first generative model with the capability to synthesize realistic cytopathological images for various cervical cell types. The experimental results validate the visual plausibility of CellGAN synthesized images, as well as demonstrate their data augmentation effectiveness on patch-level cell classification."
CellGAN: Conditional Cervical Cell Synthesis for Augmenting Cytopathological Image Classification,2,Method,"The dilemma of medical image synthesis lies in the conflict between the limited availability of medical image data and the high demand for data amount to train reliable generative models. To ensure the synthesized image quality given relatively limited training samples, the proposed CellGAN is built upon FastGAN [16] towards stabilized and fast training for few-shot image synthesis. By working in a class-conditional manner, CellGAN can explicitly control the cervical squamous cell types in the synthesized cytopathological images, which is critical to augment the downstream classification task. The overall architecture of CellGAN is presented in Fig. 1, and more detailed structures of the key components are displayed in Supplementary Materials."
CellGAN: Conditional Cervical Cell Synthesis for Augmenting Cytopathological Image Classification,2.1,Architecture of the Generator,"The generator of CellGAN has two input vectors. The first input of the class label y, which adopts one-hot encoding, provides class-conditional information to indicate the expected cervical cell type in the synthesized image I syn . The second input of the 128-dimensional latent vector z represents the remaining image information, from which I syn is gradually expanded. We stack six UpBlocks to form the main branch of the generator.To inject cell class label y into each UpBlock, we follow a similar design to StyleGAN [13]. Specifically, the class label y is first projected to a class embedding c via a non-linear mapping network, which is implemented using four groups of fully connected layers and LeakyReLU activations. We set the dimensions of class embedding c to the same as the latent vector z. Then, we pass c through learnable affine transformations, such that the class embedding is specialized to the scaling and bias parameters controlling Adaptive Instance Normalization (AdaIN) [13] in each UpBlock. The motivation for the design above comes from our hypothesis that the class-conditional information mainly encodes cellular attributes related to cell types, rather than common image appearance. Therefore, by modulating the feature maps at multiple scales, the input class label can better control the generation of cellular attributes.We further introduce the Skip-layer Global Context (SGC) module into the generator (see Fig. 2 in Supplementary Materials), to better handle the diversity of the spatial relationship of the cells. Our SGC module reformulates the idea of GCNet [4] with the design of SLE module from FastGAN [16]. It first performs global context modeling on the low-resolution feature maps, then transforms global context to capture channel-wise dependency, and finally merges the transformed features into high-resolution feature maps. In this way, the proposed SGC module learns a global understanding of the cell-to-cell spatial relationship and injects it into image generation via computationally efficient modeling of long-range dependency."
CellGAN: Conditional Cervical Cell Synthesis for Augmenting Cytopathological Image Classification,2.2,Discriminator and Adversarial Training,"In an adversarial training setting, the discriminator forces the generator to faithfully match the conditional data distribution of real cervical cytopathological images, thus prompting the generator to produce visually and semantically realistic images. For training stability, the discriminator is trained as a feature encoder with two extra decoders. In particular, five ResNet-like [7] Down-Blocks are employed to convert the input image into an 8 × 8 × 512 feature map. Two simple decoders reconstruct downscaled and randomly cropped versions of input images I crop and I resize from 8 2 and 16 2 feature maps, respectively. These decoders are optimized together with the discriminator by using a reconstruction loss L recon that is represented below:where T denotes the image processing (i.e., 1 2 downsampling and 1 4 random cropping) on real image I real , f is the processed intermediate feature map from the discriminator Dis, and Dec stands for the reconstruction decoder. This simple self-supervised technique provides a strong regularization in forcing the discriminator to extract a good image representation.To provide more detailed feedback from the discriminator, PatchGAN [12] architecture is adopted to output an 8 × 8 logit map by using a 1 × 1 convolution on the last feature map. By penalizing image content at the scale of patches, the color fidelity of synthesized images is guaranteed as illustrated in our ablation study (see Fig. 3). To align the class-conditional fake and real data distributions in the adversarial setting, the discriminator directly incorporates class labels as additional inputs in the manner of projection discriminator [20]. The class label is projected to a learned 512-dimensional class embedding and takes innerproduct at every spatial position of the 8 × 8 × 512 feature map. The resulting 8×8 feature map is then added to the aforementioned 8×8 logit map, composing the final output of the discriminator.For the objective function, we use the hinge version [15] of the standard adversarial loss L adv . We also employ R 1 regularization L reg [17] as a slight gradient penalty for the discriminator. Combining all the loss functions above, the total objective L total to train the proposed CellGAN in an adversarial manner can be expressed as:where λ reg is empirically set to 0.01 in our experiments.3 Experimental Results"
CellGAN: Conditional Cervical Cell Synthesis for Augmenting Cytopathological Image Classification,3.1,Dataset and Experimental Setup,"Dataset. In this study, we collect 14,477 images with 256 × 256 pixels from three collaborative clinical centers. All the images are manually inspected to contain different cervical squamous cell types. In total, there are 7,662 NILM, 2,275 ASC-US, 2,480 LSIL, 1,638 ASC-H, and 422 HSIL images. All the 256×256 images with their class labels are selected as the training data.Implementation Details. We use the learning rate of 2.5 × 10 -4 , batch size of 64, and Adam optimizer [14] to train both the generator and the discriminator for 100k iterations. Spectral normalization [19], differentiable augmentation [30] and exponential-moving-average optimization [29] are included in the training process. Fréchet Inception Distance (FID) [8] is used to measure the overall semantic realism of the synthesized images. All the experiments are conducted using an NVIDIA GeForce RTX 3090 GPU with PyTorch [22]."
CellGAN: Conditional Cervical Cell Synthesis for Augmenting Cytopathological Image Classification,3.2,Evaluation of Image Synthesis Quality,"We compare CellGAN with the state-of-the-art generative models for classconditional image synthesis, i.e., BigGAN [2] from cGANs [18] and Latent Diffusion Model (LDM) [25] from diffusion models [9]. As shown in Fig. 2, BigGAN  To verify the effects of key components in the proposed CellGAN, we conduct an ablation study on four model settings in Table 2 and Fig. 3. We denote the models in Fig. 3 from left to right as Model i, Model ii, Model iii, and CellGAN. The visual results of Model i suffer from severe color distortions while the other models do not, indicating that the PatchGAN-based discriminator can guarantee color fidelity by patch-level image content penalty.  The abnormal cells generated by Model i and Model ii tend to have highly similar cellular features. In contrast, Model iii and CellGAN can accurately capture the morphological characteristics of different cell types. This phenomenon suggests that the implementation of the class mapping network facilitates more distinguishable feature representations for different cell types. By comparing the synthesized images from Model iii with CellGAN, it is observed that adopting SGC modules can yield more clear cell boundaries, which demonstrates the capability of SGC module in modeling complicated cell-to-cell relationships in image space. The quantitative results further state the effects of the components above."
CellGAN: Conditional Cervical Cell Synthesis for Augmenting Cytopathological Image Classification,3.3,Evaluation of Augmentation Effectiveness,"To validate the data augmentation capacity of the proposed CellGAN, we conduct 5-fold cross-validations on the cell classification performances of two classi- In each fold, one group is selected as the testing data while the other four are used for training. For different data settings, we synthesize 2,000 images for each cell type using the corresponding generative method, and add them to the training data of each fold. We use the learning rate of 1.0 × 10 -4 , batch size of 64, and SGD optimizer [24] to train all the classifiers for 30 epochs. Random flip is applied to all data settings since it is reasonable to use traditional data augmentation techniques simultaneously in practice.The experimental accuracy, precision, recall, and F1 score are listed in Table 3. It is shown that both the classifiers achieve the best scores in all metrics using the additional synthesized data from CellGAN. Compared with the baselines, the accuracy values of ResNet-34 and DenseNet-121 are improved by 5.25% and 4.05%, respectively. Meanwhile, the scores of other metrics are all improved by more than 4%, indicating that our synthesized data can significantly enhance the overall classification performance. Thanks to the visually plausible and semantically realistic synthesized data, CellGAN is conducive to the improvement of cell classification, thus serving as an efficient tool for augmenting automatic abnormal cervical cell screening."
CellGAN: Conditional Cervical Cell Synthesis for Augmenting Cytopathological Image Classification,4,Conclusion and Discussion,"In this paper, we propose CellGAN for class-conditional cytopathological image synthesis of different cervical cell types. Built upon FastGAN for training stability and computational efficiency, incorporating class-conditional information of cell types via non-linear mapping can better represent distinguishable cellular features. The proposed SGC module provides the global contexts of cell spatial relationships by capturing long-range dependencies. We have also found that the PatchGAN-based discriminator can prevent potential color distortion. Qualitative and quantitative experiments validate the semantic realism as well as the data augmentation effectiveness of the synthesized images from CellGAN.Meanwhile, our current CellGAN still has several limitations. First, we cannot explicitly control the detailed attributes of the synthesized cell type, e.g., nucleus size, and nucleus-cytoplasm ratio. Second, in this paper, the synthesized image size is limited to 256×256. It is worth conducting more studies for expanding synthesized image size to contain much more cells, such that the potential applications can be extended to other clinical scenes (e.g., interactively training pathologists) in the future."
CellGAN: Conditional Cervical Cell Synthesis for Augmenting Cytopathological Image Classification,,Fig. 1 .,
CellGAN: Conditional Cervical Cell Synthesis for Augmenting Cytopathological Image Classification,,Fig. 2 .,
CellGAN: Conditional Cervical Cell Synthesis for Augmenting Cytopathological Image Classification,,Fig. 3 .,
CellGAN: Conditional Cervical Cell Synthesis for Augmenting Cytopathological Image Classification,,Table 1 .,comparison between state-of-the-art generative models and the proposed CellGAN (↓: Lower is better).
CellGAN: Conditional Cervical Cell Synthesis for Augmenting Cytopathological Image Classification,,Table 3 .,
CellGAN: Conditional Cervical Cell Synthesis for Augmenting Cytopathological Image Classification,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2_47.
An Anti-biased TBSRTC-Category Aware Nuclei Segmentation Framework with a Multi-label Thyroid Cytology Benchmark,1,Introduction,"Thyroid cancer is the most common cancer of the endocrine system, accounting for 2.1% of all malignant cancers [1]. Clinically, pathologists rely on the six-category of ""The Bethesda System for Reporting Thyroid Cytopathology"" (TBSRTC) [2,3] to distinguish the cell morphology in the stained cytopathologic sections. The emergence of computational pathology allows automatic diagnosis of thyroid cancer, and nuclei segmentation becomes one of the most critical diagnostic tasks [4,5], as the shapes of nuclei, whether round, oval, or elongated, can provide valuable information for further analysis [6]. For example, small and scattered thyroid cells with a light hue and relatively low cell density are usually low-grade and indicative of early-stage cancer; whereas large and dark cells with extreme-dense agglomeration are usually middle-or late-grade [3]. Correspondingly, accurate location of cell boundaries is essential for both pathologists and computer-aided diagnosis (CAD) systems to assist decision [7].However, nuclei segmentation in thyroid cytopathology is still challenged by the varying cellularity of images from different TBSRTC categories [3,8]. For example, benign cells (I & II) present high sparsity and are difficult to be distinguished from background tissues, thus may account for a relatively small proportion when equal images are involved in a training set [3]. By contrast, high-grade cells (V & VI) are densely packed and severely clustered, thus much more are presented in a training set. In this way, an unbalanced distribution across different categories resulted, correspondingly, the training leads to biased models with lower accuracy [9,10]. Such distinct morphological differences can be characterized by the TBSRTC category, which thus inspires us to utilize the handy image-wise grading labels to guide the nuclei segmentation model learning from unbalanced datasets. We also noticed that another challenge for accurate nuclei identification is the heavy reliance on large-scale high-quality annotations [11]. Moreover, amongst multiple annotation paradigms [12], pixellevel labeling is the most time-consuming and laborious, whereas the image-wise diagnostic labels, i.e. TBSRTC categories, are comparatively simpler. Despite the labeling intensity, prevalent nuclei segmentation methods, e.g., CIA-Net [13], CA 2.5 -Net [14], and ClusterSeg [15], are limited to pixel-wise annotations, where the potential benefits of integrating accessible image-wise labels are unaware.To narrow the gap discussed, we propose a novel TBSRTC-category-aware nuclei segmentation framework. Our contributions are three-fold. (1) We propose a cytopathology nuclei segmentation network named TCSegNet, to provide supplementary guidance to facilitate the learning of nuclei boundaries. Innovatively, our approach can help reduce bias in the learning process of the segmentation model with the routine unbalanced training set. (2) We expand TCSegNet to Semi-TCSegNet to leverage image-wise labels in a semi-supervised learning manner, which significantly reduces the reliance on annotation-intensive pixel-wise labels. Additionally, an HSV-intensity noise is designed specifically for cytopathology images to boost the generalization ability. (3) We establish a dataset of thyroid cytopathology image patches of 224 × 224, where 4,965 image labels are provided following TBSRTC, and 1,473 of them are densely annotated [3] (to be on GitHub upon acceptance). To the best of our knowledge, it is the first publicized thyroid cytopathology dataset of both image-wise and pixel-wise labels. The annotated dataset well alleviates the insufficiency of an open cytopathology dataset for computer-assisted analysis (Fig. 1). "
An Anti-biased TBSRTC-Category Aware Nuclei Segmentation Framework with a Multi-label Thyroid Cytology Benchmark,2,Methodology,"Overview. We propose a novel TBSRTC-category Aware Segmentation Network (TCSegNet) to segment nuclei boundaries in cytopathology images, which is guided by TBSRTC-category label to learn from unbalanced data. Our model uses a CNN and Transformer dual-path U-shape architecture, where the CNN captures the local features, and the Transformer extracts the global features for a more comprehensive representation of nuclei allocation [16]. Considering the spatial distributions of thyroid cells in cytopathology images, our design provides extended global information for more accurate segmentation. Our approach employs short connections to allow effective communication of local and global representations [17]. Formally, the overall segmentation loss L seg to train our model is a combination of the binary cross-entropy loss (BCE), i.e.where ŷ is the prediction from TCSegNet and y is and pixel-wise annotation, respectively. Subscript ni and nb denote the nuclei area and boundary, respectively. Superscripts cnn and trans write for the CNN branch and Transformer branch, respectively. We set the balancing coefficient γ ni to 1 and γ nb to 10. Additionally, to ensure the consistency between the two branches, we impose a dice consistency loss (L cons ) between the nuclei instance predictions from the CNN branch and the Transformer branch, namelyTBSRTC-Category Label Guidance Block. In TCSegNet, we introduce a TBSRTC-category label guidance block to address the learning issue from unbalanced routine datasets. This block consists of two learnable fully connected layers that process the feature extracted by the CNN and Transformer branches separately, which obtains image-wise TBSRTC-category prediction denoted as ŷcnn cls and ŷtrans cls . Correspondingly, to train this block, we use a cross-entropy loss function (CE) that provides an extra supervision signal to help the network learn from unbalanced datasets, defined as follows:where y cls is the image-wise TBSRTC-category label, and the balancing coefficient γ cls is set to 3, as the global feature captured by the Transformer branch is tightly correlated with the image-level classification tag. Finally, the overall loss for TCSegNet becomesExtension to Semi-supervised Learning. To leverage images that only have image-wise labels, we extend to a semi-supervised mean teacher [18] framework called Semi-TCSegNet. In this framework, both the student and teacher share the same full-supervised nuclei segmentation architecture of TCSegNet. The weights of the teacher θ t are updated with the exponential moving average (EMA) of the weights of student θ s , and smoothing coefficient α = 0.99, following the previous work [19]. Formally, the weights of the teacher at e-th epoch are updated by where e max is the maximum epoch number.HSV-Intensity Noise. The traditional method of integrating Gaussian noise in the mean teacher [18] may be problematic when working with cytopathology images that have an imbalanced color distribution. To address this issue, we generate a novel intensity-based noise, which can adaptively behave stronger in the dark nuclei areas and weaker in bright cytoplasm or background regions. We first sample η from a Gaussian distribution N 0, σ 2 , where σ is the standard deviation computed from the pixel values of the V channel in HSV space. The Gaussian noise η serves as the basis for generating the intensity-based noise, which is obtained bySpecifically, X v is the pixel value of the image's V channel in HSV space, and hyper-parameter λ v is set as 0.5 to control the amplitude of the intensity-based noise. Finally, the value of the obtained noise is clamped to [-0.2, 0.2] before being added to the images."
An Anti-biased TBSRTC-Category Aware Nuclei Segmentation Framework with a Multi-label Thyroid Cytology Benchmark,3,Experiments,"Image Dataset. We construct a clinical thyroid cytopathology dataset with images of both image-wise and pixel-wise labels as a benchmark (appear in GitHub upon acceptance) Some representative images are presented in Fig. 2, together with the profile of the dataset. The dataset comprises 4,965 H&E stained image patches and labels of TBSRTC, where a subset of 1,473 images was densely annotated for nuclei boundaries by three experienced cytopathologists and reached a total number of 31,064 elaborately annotated nuclei. Patient-level images were partitioned first for training and test images, and patch-level curation was performed. We divided the dataset with image-wise labels into 80% training samples and the remaining 20% testing samples. Our collection of thyroid cytopathology images was granted with an Ethics Approval document. Table 1. Quantitative comparisons in both fully-supervised and semi-supervised manners. The best performance is highlighted in bold, where we can observe that both TCSegNet and its semi-supervised extension outperform state-of-the-art."
An Anti-biased TBSRTC-Category Aware Nuclei Segmentation Framework with a Multi-label Thyroid Cytology Benchmark,,Method Dice IoU,"Fully-supervised Mask R-CNN [20] 0.657 0.500 Swin-Unet [21] 0.671 0.516 SegNet [22] 0.676 0.587 UNet++ [23] 0.784 0.691 Ca 2.5 -Net [14] 0.838 0.732 CIA-Net [13] 0.854 0.775 ClusterSeg [15] 0.857 0.761 TCSegNet (Ours) 0.877 0.788Semi-supervised PseudoSeg [24] 0.734 0.612 Cross Pseudo Seg [25] 0.737 0.618 Cross Teaching [26] 0.795 0.704 PS-ClusterSeg [15] 0.866 0.775 MTMT-Net [27] 0.878 0.789 Semi-TCSegNet (Ours) 0.889 0.805 Implementations. The proposed method and compared methods are implemented on a single NVIDIA GeForce RTX 3090 GPU card. We employ a conformer [16] with 12 Transformer layers and 5 CNN blocks as the encoder in TCSegNet. Both TCSegNet and Semi-TCSegNet use SGD optimizer with a momentum of 0.9 and a weight decay of 10 -4 . The initial learning rate lr 0 is set to 5 × 10 -3 , and the learning rate for e th epoch is determined by the poly strategy [28], i.e., lr e = lr 0 × (1 -e/e max ) 0.9 , where e max = 280 is the total epoch number. We set the batch size for TCSegNet to 8, and for Semi-TCSegNet to 10, i.e. 8 fully-annotated images and 2 partially-annotated images per batch.Compared Methods and Evaluation Metrics. We compared TCSegNet with the fully-supervised counterparts, including method specific for segmentation in general image [20,22], medical image [21,23], and nuclei [13][14][15]. We also compared Semi-TCSegNet with semi-supervised methods [15,[24][25][26][27]. We used the officially released code published along with their papers for all the compared methods. Intersection over Union (IoU) and Dice score were applied as the evaluation metrics, where a higher value indicated a better semantic segmentation performance. Experimental Results. The results in Table 1 indicated that TCSegNet can achieve the highest performance by a Dice score of 87.7% and an IoU of 78.8%. The performance values in the challenging regions are highlighted with red boxes in Fig. 3, together with the line charts in Fig. 4 (A,B). Our approach is capable to address the current issue in the recognition and segmentation of small isolated cells graded in the I category, which is always ignored by the unbalanced pixel-wise cell morphology with other approaches. Also, it yields that the incorporation of TBSRTC-category can contribute to a partial alleviation of a biased model, resulting in more satisfying segmentation performance experimentally. Furthermore, the fact that the TBSRTC-category label is easy to obtain endows the applicability of our model to various circumstances that nuclei in various sizes, shapes, and dyeing styles can be accurately recognized and segmented. Consequently, it can serve as a guarantee for the validity and accuracy of the subsequent analysis in real clinical practice. Moreover, with the semi-supervised learning, Semi-TCSegNet can further boost the performance to an 88.9% Dice score, and 80.5% IoU, by leveraging additional data with image-wise TBSRTCcategory labels solely. The performance improvement of 1.2% Dice, 1.7% IoU, together with the general improvement is shown in the boxplot in Fig. 4 (C,D), as a demonstration of the advantage using full data resources with Semi-TCSegNet. Ablation Study. To evaluate the effectiveness of each functional block and demonstrate the functionality of semi-supervised learning, we illustrate the ablation study in Table 2. The results indicate that performance improvement is accumulated with increasing data size. Besides, training with a classificationlearning block alone can increase the nuclei segmentation performance by 1.7% and 2.6% in the Dice score and IoU, respectively. Meanwhile, trained with specially designed HSV-Intensity noise can also increase the performance by 0.9% Dice and 1.4% IoU, showing its potential for generation ability improvement. Importantly, the benefits from the two blocks are orthonormal, where Semi-TCSegNet achieves the optimal performance with the utilization of both. "
An Anti-biased TBSRTC-Category Aware Nuclei Segmentation Framework with a Multi-label Thyroid Cytology Benchmark,4,Conclusion,"In this paper, we propose a TBSRTC-category aware nuclei segmentation framework TCSegNet, that leverages easy-to-obtain image-wise diagnostic category to facilitate nuclei segmentation. Importantly, it addresses the challenge of distinguishing nuclei across different cell scales in an unbalanced dataset. We also extend the framework to a semi-supervised learning fashion to overcome the issue of lacking annotated training samples. Moreover, we construct the first thyroid cytopathology dataset with both image-wise and pixel-wise labels, which we believe can it facilitate future research in this field. As the spatial distribution, shape, and area information from nuclear segmentation is supportive of diagnostic decisions, we will further leverage the segmentation result for malignancy analysis and also explore the potential of spatial information for unlabeled data exploration in the future."
An Anti-biased TBSRTC-Category Aware Nuclei Segmentation Framework with a Multi-label Thyroid Cytology Benchmark,,Fig. 1 .,
An Anti-biased TBSRTC-Category Aware Nuclei Segmentation Framework with a Multi-label Thyroid Cytology Benchmark,,),
An Anti-biased TBSRTC-Category Aware Nuclei Segmentation Framework with a Multi-label Thyroid Cytology Benchmark,,Fig. 2 .,
An Anti-biased TBSRTC-Category Aware Nuclei Segmentation Framework with a Multi-label Thyroid Cytology Benchmark,,Fig. 3 .,
An Anti-biased TBSRTC-Category Aware Nuclei Segmentation Framework with a Multi-label Thyroid Cytology Benchmark,,Fig. 4 .,
An Anti-biased TBSRTC-Category Aware Nuclei Segmentation Framework with a Multi-label Thyroid Cytology Benchmark,,Table 2 .,
Zero-Shot Nuclei Detection via Visual-Language Pre-trained Models,1,Introduction,"In the field of medical image processing, nuclei detection on Hematoxylin and Eosin (H&E)-stained images plays a crucial role in various areas of biomedical research and clinical applications [8]. While fully-supervised methods have been proposed for this task [9,20,28], the annotation remains labor-intensive and expensive. To address the aforementioned issue, several unsupervised methods have been proposed, including thresh-holding-based methods [12,22], selfsupervised-based methods [26], and domain adaptation-based methods [14]. Among these, domain adaptation methods are mainstream and have demonstrated favorable performance by achieving adaptation through aligning the source and target domains [14]. However, current unsupervised methods exhibit strong empirical design and introduce subjective biases during the model design process, thus current unsupervised methods may lead to suboptimal results.Yet, newly developed large-scale visual-language pre-trained models (VLPMs) have provided another possible unsupervised learning paradigm [11,24]. VLPM learns aligned text and image features from massive text-image pairs acquired from the internet, making the learned visual features semantic-rich, general, and transferable. Zero-shot learning methods based on VLPM for downstream tasks such as text-driven image manipulation [23], image captioning [15], view synthesis [10], and object detection [16], have achieved excellent results.Among VLPMs, Grounded Language-Image Pre-training (GLIP) model [16], pre-trained at the object level, can even rival fully-supervised counterparts in zero-shot object detection and phrase grounding tasks. Although VLPM has been utilized for object detection in natural scenes, zero-shot nuclei detection on H&E images via VLPM remains underexplored. The significant domain differences between medical H&E images and the natural images used for pre-training make this task challenging. It is wondered whether VLPM, with its rich semantic information, can facilitate direct prediction of nuclei detection through semanticdriven prompts, establishing an elegant, concise, clear but more efficient and transferable unsupervised system for label-free nuclei detection.Building upon this concept, our goal is to establish a zero-shot nuclei detection framework based on VLPM. However, directly applying VLPM for this task poses two challenges. (1) Due to the gap between medical images and the web-originated text-image pairs used for pre-training, the text-encoder may lack prior knowledge of medical concept words, thus making the prompt design for zero-shot detection a challenging task. (2) Different from the objects in natural images, the high density and specialized morphology of nuclei in H&E stained images may lead to missed detection, false detection, and overlapping during zero-shot transfer solely with prompts.To address the first challenge, Yamada et al. have analyzed and revealed that under the pre-training of vast text-image pairs, VLPM establishes a strong association binding between the object and its semantic attributes regardless of image domain, i.e., associated attribute text can fully describe the corresponding objects in an image through VLPM [27]. Therefore, it is feasible for VLPM to detect unseen medical objects in a label-free manner by constructing appropriate attribute texts. Manual prompting is a cumbersome and subjective process, which may lead to considerable bias. Yet, the VLPM network BLIP [15] has the capability to generate automatic descriptions for images. Therefore, we first use BLIP to automatically generate attribute words to describe the unseen nuclei object. This approach avoids the empirical manual prompt design and fully leverages the text-to-image aligning trait of VLPMs. We subsequently integrate these attribute words with medical nouns, i.e. ""[shape][color][noun]"", to create detection prompts. These prompts are then inputted into GLIP to realize zero-shot detection of nuclei. Our proposed automatic prompt designing method fully utilizes the text-to-image alignment of VLPM, and enables the automatic generation of the most suitable attribute text words describing the corresponding domain. Our approach offers excellent interpretability.Through GLIP's strong object retrieval performance, we can obtain preliminary boxes. The precision of these preliminary boxes is relatively high, but there is still considerable room for improvement in recall. Therefore, we further establish a self-training framework. We use the preliminary boxes generated by GLIP as pseudo labels for further training YOLOX [7], to refine and polish the predicted boxes in an iterative manner. Together with the self-training strategy, the resulting model achieves a remarkable performance for label-free nuclei detection, surpassing other comparison methods. We demonstrate that VLPM, which is pre-trained on natural image-text pairs, also exhibits astonishing potential for downstream tasks in the medical field.The contributions of this paper are threefold. (1) A novel zero-shot labelfree nuclei detection framework is proposed based on VLPMs. Our method outperforms all existing unsupervised methods and demonstrates excellent transferability. (2) We leverage GLIP, which places more emphasis on object-level representation learning and generates more high-quality language-aware visual representations compared to Contrastive Language-Image Pre-training (CLIP) model, to achieve better nuclei retrieval. (3) An automatic prompt design process is established based on the association binding trait of VLPM to avoid non-trivial empirical manual prompt engineering."
Zero-Shot Nuclei Detection via Visual-Language Pre-trained Models,2,Method,Our approach aims to establish a zero-shot nuclei detection framework based on VLPMs by directly using text prompts. We utilize GLIP for better object-level representation extraction. The overview of our framework is shown in Fig. 1.
Zero-Shot Nuclei Detection via Visual-Language Pre-trained Models,2.1,Object-Level VLPM -GLIP,"Recently, large VLPMs such as CLIP [24] and ALIGN [11] have made great progress in generic visual representation learning and demonstrated the enormous potential of utilizing prompts for zero-shot transfer.A typical VLPM framework comprises two encoders: the text encoder, which encodes text prompts to semantic-rich text embeddings, and the image encoder, which encodes images to visual embeddings. These VLPMs use a vast amount of web-originated text-image pairs {(X, T )} i to learn the text-to-image alignment through contrastive loss over text and visual embeddings. Denoting the image encoder as E I , the text encoder as E T , the cosine similarity function as cos(•), and assuming that K text-image pairs are used in each training epoch, the objective of the contrastive learning can be formulated as:Through this aligning contrastive learning, VLPM aligns text and image in a common feature space, allowing one to directly transfer a trained VLPM to downstream tasks via manipulating text, i.e., prompt engineering. The visual representations of input images are semantic-rich and interpretability-friendly with the help of aligned text-image pairs. However, the conventional VLPM pre-training process only aligns the image with the text from a whole perspective, which results in a lack of emphasis on object-level representations. Therefore, Li et al. proposed GLIP [16], whose image encoder generates visual embeddings for each object of different regions in the image to align with object words present in the text. Moreover, GLIP utilizes web-originated phrase grounding text-image pairs to extract novel object word entities, expanding the object concept in the text encoder. Additionally, unlike CLIP, which only aligns embeddings at the end of the model, GLIP builds a deep cross-modality fusion based on cross-attention [3] for multi-level alignment. As shown in Fig. 1(a), GLIP leverages DyheadModules [4] and BERTLayer [5] as the image and text encoding layers, respectively. With text embedding represented as R and visual embedding as P , the deep fusion process can be represented as:where L is the total number of layers, R 0 denotes the visual features from swintransformer-large [19], and P 0 denotes the token features from BERT [5]. X-MHA represents cross-attention. This architecture enables GLIP to attain superior object-level performance and semantic aggregation. Consequently, GLIP is better suited for object-level zero-shot transfer than conventional VLPMs. Thus, we adopt GLIP to extract better object-level representations for nuclei detection."
Zero-Shot Nuclei Detection via Visual-Language Pre-trained Models,2.2,Automatic Prompt Design,"The text input, known as the prompt, plays a crucial role in the zero-shot transfer of VLPM for downstream tasks. GLIP originally uses concatenated object nouns such as ""object noun 1. Object noun 2..."" as default text prompts for detection, and also allows for manual engineering to improve performance [16]. However, manual prompt engineering is a non-trivial challenge, demanding substantial effort and expertise. Furthermore, a notable disparity exists between the weboriginated pre-training text-image pairs and medical images. Thus simple noun concatenation is insufficient for GLIP to retrieve nuclei. We note that Yamada et al. prove that the pre-training on extensive textimage pairs has allowed VLPMs to establish a strong association binding between objects and their semantic attributes [27]. Thus, through VLPM, the associated attribute text can accurately depict the corresponding object. Based on this research, we propose that VLPM has the potential to detect unlabelled medical objects in a zero-shot manner by constructing relevant attribute text.As shown in Fig. 1(a), we first use the image captioning VLPM BLIP [15] to automatically generate attribute words to describe the unseen nuclei object. BLIP allows us to avoid manual attribute design and generates attribute vocabulary that conforms to the text-to-image alignment of VLPM. This process involves three steps. (1) Directly input target medical nouns into GLIP for coarse box prediction. (2) Use the coarse boxes to squarely crop the image, the cropped objects are fed into a frozen BLIP to automatically generate attribute words that describe the object. (3) Word frequency statistics and classification are adopted to find the top M words that describe the object's shape and color, respectively, for that ""shape"" and ""color"" are the most relative two attributes that depict nuclei. For a thorough description, we augment the attribute words with synonyms retrieved by a pre-trained language model [25]. All these attribute words are combined with those medical nouns to automatically generate a triplet detection prompt of ""[shape][color][noun]"". Finally, all generated triplets were put into GLIP for refined detection. This method avoids the empirical manual prompt design and fully utilizes the text-to-image aligning trait of VLPMs.Our automatic prompt design leverages the powerful text-to-image aligning capabilities of GLIP and BLIP. This approach also enables the automatic generation of the most appropriate attribute words for the specific domain, embodying excellent interpretability."
Zero-Shot Nuclei Detection via Visual-Language Pre-trained Models,2.3,Self-training Boosting,"Leveraging the strong object retrieval performance of GLIP, we obtain preliminary detection boxes with high precision but low recall. These boxes suffer from missed detection, false detection, and overlapping. To fully exploit the zeroshot potential of GLIP, a self-training framework is established. The automatic prompts are inputted into GLIP to generate the initial results which served as pseudo labels for training YOLOX [7]. Then, the converged YOLOX is used as a teacher to generate new pseudo labels, and iteratively trains students YOLOX. As self-training is based on the EM optimization algorithm [21], it propels our system to continuously refine the predicted boxes and achieve a better optimum."
Zero-Shot Nuclei Detection via Visual-Language Pre-trained Models,3,Experiments,
Zero-Shot Nuclei Detection via Visual-Language Pre-trained Models,3.1,Dataset and Implementation,"The dataset used in this study is the MoNuSeg dataset [13], which consists of 30 nuclei images of size 1000 × 1000, with an average of 658 nuclei per image. Following Kumar et al. [13], the dataset was split into training and testing sets with a ratio of 16:14. 16 training images served as inputs of GLIP to generate pseudo-labels for self-training, with 4 images randomly selected for validation. Annotations were solely employed for evaluation purposes on the test images. 16 overlapped image patches of size 256 × 256 were extracted from each image and randomly cropped into 224 × 224 as inputs.In terms of experimental settings, four Nvidia RTX 3090 GPUs were utilized, each with 24 GB of memory. In the automatic prompt generating process, the VQA weights of BLIP finetuned on ViT-B and CapFilt-L [15] were used to generate [shape] and [color] attributes. These attribute words are augmented with synonyms by GPT [25], i.e. attribute augmentation. The target medical noun list was first set to [""nuclei""] straightforwardly, and was also augmented by GPT to [""nuclei"", ""nucleus"", ""cyteblast"", ""karyon""], i.e. noun augmentation. Attribute words were subsequently combined with the target medical nouns to ""[shape][color][noun]"" format as inputs of GLIP to generate bounding boxes as pseudo labels. The weights used for GLIP is GLIP-L. For self-training refinement, we used the default setting of YOLOX and followed the standard self-training methodology described in [6].Table 1. Comparison results on MoNuSeg [13]. The best results of unsupervised methods are marked in bold. "
Zero-Shot Nuclei Detection via Visual-Language Pre-trained Models,3.2,Comparison,"Our proposed method was compared with the representative fully-supervised method YOLOX [7], as well as the current state-of-the-art (SOTA) methods in unsupervised object detection, including SSNS [26], SOP [14], and VLDet [17]. Among them, the fully-supervised method YOLOX represents the current SOTA on natural images, and VLDet is a newly proposed zero-shot object detection method based on CLIP. For evaluation, mAP, AP50, AP75, and AR are chosen as metrics, following COCO [18]. The final results are shown in Table 1.Referring to the table, it is evident that our GLIP-based approach outperforms all unsupervised techniques, including domain adaptation-based and clipbased methods. Figure 2 depicts the visualization of the detection results."
Zero-Shot Nuclei Detection via Visual-Language Pre-trained Models,3.3,Ablation Studies,"Automatic Prompt Design. Firstly, we conducted an ablation study specifically targeting the prompt while ensuring that other conditions remained constant, the results are presented in Table 2.The first row of the table displays the default noun-concatenation prompt GLIP originally adopted, i.e. ""nuclei. nucleus. cyteblast. karyon"". The second row represents the same set of nouns with some manual property descriptions added, like ""Nuclei. Nucleus. cyteblast. karyon, which are round or oval, and purple or magenta"". It is noteworthy that this manual approach is empirically  subjective and therefore prone to significant biases. The subsequent rows in the table demonstrate the combination of attributes generated by our automatic prompt design method. In these experiments, M was set to 3.It is worth noting that the predictions generated by the prompts shown in the first and second rows also employed the self-training strategy until convergence. However, the results of the first row contain a significant amount of noise, implying that the intrinsic gap between medical nouns and natural nouns impedes the directly zero-shot transfer of GLIP. The second row improves obviously, indicating the effectiveness of attribute description. But manual design is empirical and tedious. As for the automatically generated prompts, it is evident that as the description of attributes becomes comprehensive, from only including shape or color solely to encompassing both, GLIP's performance improves gradually. Furthermore, the second-to-last row indicates that even without nouns, attribute words alone can achieve good results, which also demonstrates the ability of BLIP-generated attribute words to effectively describe the target nuclei. Through the utilization of VLPM's text-to-image alignment capabilities, the proposed automatic prompt design method generates the most suitable attribute words for a given domain automatically, with a high degree of interpretability. Please refer to the supplement for a detailed list of [shape] and [color] attributes.We further looked into the effect of word augmentation. The results are shown in Table 3. Without and with noun augmentation, the noun lists were [""nuclei""] and [""nuclei"", ""nucleus"", ""cyteblast"", ""karyon""], respectively. The first row of Table 3 uses non-augmented ""[shape][color][noun]"", while the first row of Table 2 uses noun-augmented concatenation. It is intriguing that applying noun augmentation may lead to suboptimum results compared with the counterparts using the straightforward [""nuclei""]. This is most likely because the augmented new synonym nouns are uncommon medical words and did not appear in the pretraining data that GLIP used. However, applying attribute word augmentation is generally effective because augmented attribute words are also common descriptions for natural scenes. These results suggest a general approach for leveraging the VLPM's potential in downstream medical tasks, that is identifying common attribute words that can be used for describing the target nouns."
Zero-Shot Nuclei Detection via Visual-Language Pre-trained Models,,Self-training and YOLOX.,"The box optimization process of the self-training stage is recorded, and the corresponding results are presented in the supplement. YOLOX and self-training are not the essential reasons for the superior performance of our method. The true key is the utilization of semantic-information-rich VLPMs. To illustrate this point, we employed another commonly used unsupervised detection method, superpixels [1], to generate pseudo labels in a zero-shot manner for a fair comparison. These pseudo labels were then fed into the selftraining framework based on the YOLOX segmentation architecture, keeping the settings consistent with our approach except for the pseudo label generation. Additionally, we also used DETR [2] instead of YOLOX in our method. The results are shown in the supplement and demonstrate that the high performance of our method lies in the effective utilization of the knowledge from VLPMs rather than YOLOX or self-training."
Zero-Shot Nuclei Detection via Visual-Language Pre-trained Models,4,Conclusion,"In this work, we propose to use the object-level VLPM, GLIP, to realize zeroshot nuclei detection on H&E images. An automatic prompt design pipeline is proposed to avoid empirical manual prompt design. It fully utilizes the text-toimage alignment of BLIP and GLIP, and enables the automatic generation of the most suitable attribute describing words, offering excellent interpretability. Furthermore, we utilize the self-training strategy to polish the predicted boxes in an iterative manner. Our method achieves a remarkable performance for labelfree nuclei detection, surpassing other comparison methods. We demonstrate that VLPMs pre-trained on natural image-text pairs still exhibit astonishing potential for downstream tasks in the medical field."
Zero-Shot Nuclei Detection via Visual-Language Pre-trained Models,,Fig. 1 .,
Zero-Shot Nuclei Detection via Visual-Language Pre-trained Models,,Fig. 2 .,
Zero-Shot Nuclei Detection via Visual-Language Pre-trained Models,,Table 2 .,
Zero-Shot Nuclei Detection via Visual-Language Pre-trained Models,,Table 3 .,
Zero-Shot Nuclei Detection via Visual-Language Pre-trained Models,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 67.
A Semantic-Guided and Knowledge-Based Generative Framework for Orthodontic Visual Outcome Preview,1,Introduction,"Orthodontic treatment aims to correct misaligned teeth and restore normal occlusion. Patients are required to wear dental braces or clear aligners for a duration of one to three years, reported by [21], with only a vague expectation of the treatment result. Therefore, a generative framework is needed to enable patients to preview treatment outcomes and assist those considering orthodontic treatment in making decisions. Such framework may involve multiple research fields, such as semantic segmentation, 3D reconstruction, and image generation.Deep learning methods have achieved great success in image-related tasks. In the field of tooth semantic segmentation, there exist plenty of studies targeting on different data modalities, such as dental mesh scanning [31], point cloud [26,30], cone beam CT image [5,6], panoramic dental X-ray image [28], and 2D natural image [32]. Regarding image generation, the family of generative adversarial networks (GAN) [8,[13][14][15]23] and the emerging diffusion models [11,22,25] can generate diverse high-fidelity images. Although the diffusion models can overcome the mode collapse problem and excel in image diversity compared to GAN [7], their repeated reverse process at inference stage prolongs the execution time excessively, limiting their application in real-time situations.When it comes to 3D teeth reconstruction, both template-based and deeplearning-based frameworks offer unique benefits. Wu et al. employed a templatebased approach by adapting their pre-designed teeth template to match teeth contours extracted from a set of images [29]. Similarly, Wirtz et al. proposed an optimization-based pipeline that uses five intra-oral photos to restore the 3D arrangement of teeth [27]. Liang et al. restored 3D teeth using convolution neural networks (CNN) from a single panoramic radiograph [18]. However, while deep CNNs have a strong generalization ability compared to template-based methods, it often struggles to precisely and reasonably restore occluded objects.Predicting the smiling portrait after orthodontic treatment has recently gained much attention. Yang et al. developed three deep neural networks to extract teeth contours from smiling images, arrange 3D teeth models, and generate images of post-treatment teeth arrangement, respectively [19]. However, their framework requires a single frontal smiling image and the corresponding unaligned 3D teeth model from dental scanning, which may be difficult for general users to obtain. In contrast, Chen et al. proposed a StyleGAN generator with a latent space editing method that utilizes GAN inversion to discover the optimal aligned teeth appearance from a single image [3]. Although their method takes only a frontal image as input and manipulates the teeth structure and appearance implicitly in image space, it may overestimate the treatment effect and result in inaccurate visual outcomes.In this study, we propose an explainable generative framework, which is semantic-guided and knowledge-based, to predict teeth alignment after orthodontic treatment. Previous works have either required 3D teeth model as additional input and predicted its alignment using neural networks [19], or directly utilized an end-to-end StyleGAN to predict the final orthodontic outcome [3]. In contrast, our approach requires only a single frontal image as input, restores the 3D teeth model through a template-based algorithm, and explicitly incorporates orthodontists' experience, resulting in a more observable and explainable process. Our contributions are therefore three-fold: 1) we introduce a region-boundary feature fusion module to enhance the 2D tooth semantic segmentation results; 2) we employ statistical priors and reconstruct 3D teeth models from teeth semantic boundaries extracted in a single frontal image; 3) by incorporating an orthodontic simulation algorithm and a pSpGAN style encoder [23], we can yield more realistic and explainable visualization of the post-treatment teeth appearance.  "
A Semantic-Guided and Knowledge-Based Generative Framework for Orthodontic Visual Outcome Preview,2,Method,"The proposed generative framework (Fig. 1) consists of four parts: semantic segmentation in frontal images, template-based 3D teeth reconstruction, orthodontic treatment simulation, and semantic-guided image generation of mouth cavity."
A Semantic-Guided and Knowledge-Based Generative Framework for Orthodontic Visual Outcome Preview,2.1,Semantic Segmentation in Frontal Images,"The tooth areas and mouth cavity are our region of interest for semantic segmentation in each frontal image. As a preprocessing step, the rectangle mouth area is firstly extracted from frontal images by the minimal bounding box that encloses the facial key points around the mouth, which are detected by dlib toolbox [17]. We then use two separate segmentation models to handle these mouth images, considering one-hot encoding and the integrity of mouth cavity mask. A standard U-Net [24] is trained with soft dice loss [20] to predict mouth cavity.The tooth semantic segmentation model (Fig. 2) is a dual-branch U-Net3+ based network that predicts tooth regions and contours simultaneously. We employ a standard U-Net3+ [12] encoder and two identical U-Net3+ decoders for tooth region and contour segmentation. Such inter-related multi-task learning can enhance the performance of each task and mitigate overfitting. The teeth are manually labeled using FDI World Dental Federation notation, resulting in a total of 33 classes, including background.To generate a more precise tooth segmentation map, we introduce the regionboundary feature fusion module, which merges the tooth region and boundary information, i.e., the last hidden feature maps of the two decoders. The module is constructed as a stack of convolutional layers, which incorporates an improved atrous spatial pyramid pooling (ASPP) module [4]. This ASPP module employs atrous separable convolution and global pooling to capture long-range information. The integration of ASPP has a dilation rate of 6, and the number of filters in each convolutional layer, except the output layer, is set to 64. These three outputs are supervised by soft dice loss [20] during training. Some post-process techniques are added to filter segmented regions and obtain smoother tooth contours. The predicted binary contours are dilated and used to divide the semantic segmentation map into multiple connected regions. The pixels in each region are classified by its dominant tooth label. Background components are ignored and small ones are removed. Once two regions have duplicate labels, a drop-or-relabel strategy is performed on the region away from the center of central incisors. The connected regions are processed sequentially from central incisors to the third molars. Figure 3 shows the changes of tooth region prediction before and after the post process."
A Semantic-Guided and Knowledge-Based Generative Framework for Orthodontic Visual Outcome Preview,2.2,Template-Based 3D Teeth Reconstruction,"To achieve 3D tooth reconstruction from a single frontal image, we deform the parametric templates of the upper and lower tooth rows to match the projected contours with the extracted teeth contours in semantic segmentation.The parametric tooth-row template is a statistical model that characterizes the shape, scale, and pose of each tooth in a tooth row. To describe the shape of each tooth, we construct a group of morphable shape models [1]. We model the pose (orientation and position) of each tooth as a multivariate normal distribution as Wu et al. [29] did. Additionally, we suppose that the scales of all teeth follows a multivariate normal distribution. The mean shape and average of the tooth scales and poses are used to generate a standard tooth-row template.The optimization-based 3D teeth reconstruction following [29] is an iterative process alternating between searching for point correspondences between the projected and segmented teeth contours and updating the parameters of the tooth-row templates. The parameters that require estimation are the camera parameters, the relative pose between the upper and lower tooth rows, and the scales, poses, and shape parameters of each tooth.The point correspondences are established by Eq. ( 1) considering the semantic information in teeth contours, where c τ i and n τ i are the position and normal of the detected contour point i of tooth τ in image space, ĉj τ and nj τ are those of the projected contour point j, < •, • > denotes inner product, and σ angle = 0.3 is a fine-tuned hyper parameter in [29].We use L as the objective function to minimize, expressed in Eq. ( 2), which comprises an image-space contour loss [29] and a regularization term L prior described by Mahalanobis distance in probability space, where N is the number of detected contour points, λ n = 50 and λ p = 25 are the fine-tuned weights.The regularization term L prior in Eq. ( 3) is the negative log likelihood of the distributions of the vector of tooth scales, denoted by s, the pose vector of tooth τ , denoted by p τ , and the shape vector of tooth τ , denoted by b τ . The covariance matrices Σ s and Σ τ p are obtained in building tooth-row templates.During optimization, we first optimize the camera parameters and the relative pose of tooth rows for 10 iterations and optimize all parameters for 20 iterations. Afterward, we use Poisson surface reconstruction [16] to transform the surface point clouds into 3D meshes."
A Semantic-Guided and Knowledge-Based Generative Framework for Orthodontic Visual Outcome Preview,2.3,Orthodontic Treatment Simulation,"We implement a naive teeth alignment algorithm to mimic orthodontic treatment. The symmetrical beta function (Eq. ( 4)) is used to approximate the dental arch curve of a tooth row [2]. Its parameters W and D can be estimated through linear regression by fitting the positions of tooth landmarks [2].We assume that the established dental arch curves are parallel to the occlusal plane. Each reconstructed tooth is translated towards its expected position in the dental arch and rotated to its standard orientation while preserving its shape. The teeth gaps are then reduced along the dental arch curve with collision detection performed. The relative pose between tooth rows is re-calculated to achieve a normal occlusion. Finally, the aligned 3D teeth models are projected with the same camera parameters to generate the semantic image output of the simulation."
A Semantic-Guided and Knowledge-Based Generative Framework for Orthodontic Visual Outcome Preview,2.4,Semantic-Guided Image Generation,"The idea behind the semantic-guided image generation model is to decompose an image into an orthogonal representation of its style and structure. By manipulating the structural or style information, we can control the characteristics of the generated image. Improving upon [19], we replace the naive style encoder in [19] with the PixelStylePixel style encoder [23] to capture the multi-level style features and use semantic teeth image instead of teeth contours as input to better guide the generation process. The architecture is illustrated in Fig. 4. At training stage, the model learns the style and structural encoding of teeth images and attempts to restore the original image. Gradient penalty [9] and path length regularization [15] are applied to stabilize the training process. We use the same loss function as [19] did and take a standard UNet encoder connected with dense layers as the discriminator. At inference stage, the semantic teeth image output from orthodontic simulation is used to control the generated teeth structure. To remove the boundary artifacts, we dilate the input mouth cavity map and use Gaussian filtering to smooth the sharp edges after image patching."
A Semantic-Guided and Knowledge-Based Generative Framework for Orthodontic Visual Outcome Preview,3,Experiments and Results,
A Semantic-Guided and Knowledge-Based Generative Framework for Orthodontic Visual Outcome Preview,3.1,Dataset and Implementation Details,"We collected 225 digital dental scans with labelled teeth and their intra-oral photos, as well as 5610 frontal intra-oral images, of which 3300 were labelled, and 4330 smiling images, of which 2000 were labelled, from our partner hospitals. The digital dental scans were divided into two groups, 130 scans for building morphable shape models and tooth-row templates and the remaining 95 scans for 3D teeth reconstruction evaluation. The labelled 3300 intra-oral images and 2000 smiling images were randomly split into training (90%) and labelled test (10%) datasets. The segmentation accuracy was computed on the labelled test data, and the synthetic image quality was evaluated on the unlabelled test data. All the models were trained and evaluated on an NVIDIA GeForce RTX 3090 GPU. We trained the segmentation models for 100 epochs with a batch size of 4, and trained the image generation models for 300 epochs with a batch size of 8. The input and output size of the image segmentation and generation models are 256 × 256. The training was started from scratch and the learning rate was set to 10 -4 . We saved the models with the minimal loss on the labelled test data. At the inference stage, our method takes approximately 15 s to run a single case on average, with the 3D reconstruction stage accounting for the majority of the execution time, tests performed solely on an Intel 12700H CPU."
A Semantic-Guided and Knowledge-Based Generative Framework for Orthodontic Visual Outcome Preview,3.2,Evaluation,"Ablation Study of Tooth Segmentation Model. We conduct an ablation study to explore the improvement of segmentation accuracy brought by the dualbranch network architecture and the region-boundary feature fusion module. Segmentation accuracy is measured by the mean intersection over union (mIoU)  metric for different groups of tooth labels. The results listed in Table 1 show that the proposed region-boundary feature fusion module assisted with dual-branch architecture can further enhance the segmentation accuracy for UNet and its variant. Our tooth segmentation model can predict quite accurately the region of the frontal teeth with a mIoU of 0.834.Accuracy of 3D Teeth Reconstruction. We reconstruct the 3D teeth models of the 95 test cases from their intra-oral photos. The restored teeth models are aligned with their ground truth by global similarity registration. We compare the reconstruction error using different metrics, shown in Table 2, with the method of [27] that reconstructs teeth models from five intra-oral photos and the nearest retrieval that selects the most similar teeth mesh in the 135 teeth meshes for building tooth-row templates. The results show that our teeth reconstruction method significantly outperforms the method of [27] and nearest retrieval.Image Generation Quality. We use Fréchet inception distance (FID) [10] to evaluate the quality of images generated different generators on the unlabelled test data, results listed in Table 3. The multi-level style features captured by pSpGAN improve greatly the image quality from the quantitative comparison (Table 3) and the visual perception (Fig. 5). Our semantic-guided pSpGAN that takes semantic teeth image as input can further increase the constrast of different teeth and yield sharper boundaries. We test our framework on some images in Flickr-Faces-HQ dataset [14] to visualize virtual teeth alignment, shown in Fig. 6."
A Semantic-Guided and Knowledge-Based Generative Framework for Orthodontic Visual Outcome Preview,4,Conclusion,"In conclusion, we develop a semantic-guided generative framework to predict the orthodontic treatment visual outcome. It comprises tooth semantic segmentation, template-based 3D teeth reconstruction, orthodontic treatment simulation, and semantic-guided mouth cavity generation. The results of quantitative tests show that the proposed framework has a potential for orthodontic application."
A Semantic-Guided and Knowledge-Based Generative Framework for Orthodontic Visual Outcome Preview,,Fig. 1 .,
A Semantic-Guided and Knowledge-Based Generative Framework for Orthodontic Visual Outcome Preview,,Fig. 2 .,
A Semantic-Guided and Knowledge-Based Generative Framework for Orthodontic Visual Outcome Preview,,Fig. 3 .,
A Semantic-Guided and Knowledge-Based Generative Framework for Orthodontic Visual Outcome Preview,,Fig. 4 .,
A Semantic-Guided and Knowledge-Based Generative Framework for Orthodontic Visual Outcome Preview,,Fig. 5 .Fig. 6 .,
A Semantic-Guided and Knowledge-Based Generative Framework for Orthodontic Visual Outcome Preview,,Table 1 .,
A Semantic-Guided and Knowledge-Based Generative Framework for Orthodontic Visual Outcome Preview,,Table 2 .,
A Semantic-Guided and Knowledge-Based Generative Framework for Orthodontic Visual Outcome Preview,,Table 3 .,
A Semantic-Guided and Knowledge-Based Generative Framework for Orthodontic Visual Outcome Preview,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 14.
Joint Segmentation and Sub-pixel Localization in Structured Light Laryngoscopy,1,Introduction,"The voice is an essential aspect of human communication and plays a critical role in expressing emotions, conveying information, and establishing personal connections. An impaired function of the voice can have significant negative impacts on an individual. Malign changes of human vocal folds are conventionally observed by the use of (high-speed) video endoscopy that measures their 2D deformation in image space. However, it was shown that their dynamics contain a significant vertical deformation. This led to the development of varying methods for the 3D reconstruction of human vocal folds during phonation. In these works, especially active reconstruction systems have been researched that project a pattern onto the surface of vocal folds [10,16,19,20]. All these systems need to deal with issues introduced by the structured light system, in particular a) specular reflections from the endoscopic light source which occlude the structured light pattern (cp. Fig. 1), b) a dilation of the structured light pattern through subsurface scattering effects in mucosal tissue, and c) vasts amount of data generated by high-speed cameras recording with up to 4000 frames per second. Furthermore, the introduction of a structured light source, e.g. a laser projection unit (LPU), increases the form-factor of the endoscope, which makes recording uncomfortable for the patient. In current systems, the video processing happens offline, which means that often unnecessarily long footage is recorded to be sure that an appropriate sequence is contained, or-even worse-that a patient has to show up again, because the recorded sequence is not of sufficient quality. Ideally, recording should thus happen in a very short time (seconds) and provide immediate feedback to the operator, which is only possible if the segmentation and pattern detection happen close to real time. To address all these practical issues, we present a novel method for the highly efficient and accurate segmentation, localization, and tracking of human vocal folds and projection patterns in laser-supported high-speed video endoscopy. An overview of our pipeline is shown in Fig. 2. It is based on two stages: First, a convolutional neural network predicts a segmentation of the vocal folds, the glottal area, and projected laser dots. Secondly, we compute sub-pixel accurate 2D point locations based on the pixel-level laser dot class probabilities in a weighted least-squares manner that further increase prediction accuracy. This approach can provide immediate feedback about the success of the recording to the physician, e.g. in form of the number of successfully tracked laser dots. Furthermore, this method can not only be used in vocal fold 3D reconstruction pipelines but also allows for the analysis of clinically relevant 2D features."
Joint Segmentation and Sub-pixel Localization in Structured Light Laryngoscopy,2,Related Work,"Our work can properly be assumed to be simultaneously a heatmap regression as well as a semantic segmentation approach. Deep learning based medical semantic segmentation has been extensively studied in recent years with novel architectures, loss-functions, regularizations, data augmentations, holistic training and optimization approaches [23]. Here, we will focus on the specific application of semantic segmentation in laryngoscopy. Deep Learning in Laryngoscopy. In the realm of laryngoscopy works involving deep learning are few and far between. Most of these works focus on the segmentation of the glottal gap over time. The glottal dynamics give information about the patients underlying conditions, all the while being an easily detectable feature. Fehling et al. were the first to propose a CNN-based method that also infers a segmentation of the human vocal folds itself [8]. Their method uses a general U-Net architecture extended with Long Short-Term Memory cells to also take temporal information into account. Pedersen et al. [17]  Keypoint Detection can generally be separated into regression-based approaches that infer keypoint positions directly from the images [5,22,26] and heatmap-based approaches that model the likelihood of the existence of a keypoint, i.e. landmark, via channel-wise 2D Gaussians and determining channelwise global maxima via argmax(). This leads to obvious quantization errors, that are addressed in recent works [1,7,25]. Most related to our approach is the work by Sharan et al. that proposes a method for determining the position of sutures by reformulating a single-channel binary segmentation to find local maxima and calculating their positions through general center of gravity estimation [21]. In case of human vocal folds, there have been works regarding laser dot detection in structured light laryngoscopy. However, these suffer from either a manual labeling step [14,16,19,20] or work only on a per-image basis [10], thus being susceptible to artifacts introduced through specular highlights. In a similar vein, none of the mentioned methods apply promising deep learning techniques. "
Joint Segmentation and Sub-pixel Localization in Structured Light Laryngoscopy,3,Method,"Given an isotropic light source and a material having subsurface scattering properties, the energy density of the penetrating light follows an exponential falloff [12]. In case of laser-based structured light endoscopy, this means that we can observe a bleeding effect, i.e. diffusion, of the respective collimated laser beams in mucosal tissue. Note that the energy density of laser beams is Gaussian distributed as well, amplifying this effect. Our method is now based on the assumption, that a pixel-level (binary-or multi-class) classifier will follow this exponential falloff in its predictions, and we can estimate sub-pixel accurate point positions through Gaussian fitting (cp. Fig. 3). This allows us to model the keypoint detection as a semantic segmentation task, such that we can jointly estimate the human vocal folds, the glottal gap, as well as the laserpoints' positions using only a single inference step. The method can be properly divided into two parts. At first, an arbitrary hourglass-style CNN (in our case we use the U-Net architecture) estimates semantic segmentations of the glottal gap, vocal folds and the 2D laserdots position for a fixed videosequence in a supervised manner. Next, we extract these local maxima lying above a certain threshold and use weighted least squares to fit a Gaussian function into the windowed regions. In this way, we can estimate semantic segmentations as well as the position of keypoints in a single inference pass, which significantly speeds up computation.Feature Extraction. Current heatmap regression approaches use the argmax(.) or topk(.) functions to estimate channel-wise global maxima, i.e. a single point per channel. In case of an 31 by 31 LPU this would necessitate such an approach to estimate 961 channels; easily exceeding maintainable memory usage. Thus our method needs to allow multiple points to be on a single channel. However, this makes taking the argmax(.) or topk(.) functions to extract local maxima infeasible, or outright impossible. Thus, to extract an arbitrary amount of local maxima adhering to a certain quality, we use dilation filtering on a thresholded and Gaussian blurred image. More precisely we calculatewhere T (x, y) depicts a basic thresholding operation, G a Gaussian kernel, B a general box kernel with a 0 at the center, and ⊕ the dilation operator. Finally we can easily retrieve local maxima by just extracting every non-zero element of I T . We then span a window I ij of size k ∈ N around the non-zero discrete points p i . For improved comprehensibility, we are dropping the subscripts in further explanations, and explain the algorithm for a single point p. Next, we need to estimate a Gaussian function. Note that, a Gaussian function is of the form f (x) = Ae -(x-μ) 2 /2σ 2 , where x = μ is the peak, A the peaks height, and σ defines the functions width. Gaussian fitting approaches can generally be separated into two types: the first ones employ non-linear least squares optimization techniques [24], while others use the result of Caruana et al. in that the logarithm of a Gaussian is a polynomial equation [2].Caruanas Algorithm. As stated previously, Caruanas algorithm is based on the observation that by taking the logarithm of the Gaussian function, we generate a polynomial equation (Eq. 1)."
Joint Segmentation and Sub-pixel Localization in Structured Light Laryngoscopy,,ln(f (x)) = ln(,"Note that the last equation is in polynomial form ln(y) = ax 2 + bx + c, with c = ln(A) --μ 2 2σ 2 , b = μ σ 2 and a = -1 2σ 2 . By defining the error function δ = ln(f (x)) -(ax 2 + bx + c) and differentiating the sum of residuals gives a linear system of equations (Eq. 2). ⎡After solving the linear system, we can finally retrieve μ, σ and A with μ = -b/2c, σ = -1/2c, and A = e a-b 2 /4c ."
Joint Segmentation and Sub-pixel Localization in Structured Light Laryngoscopy,,Guos Algorithm.,"Due to the logarithmic nature of Caruanas algorithm, it is very susceptible towards outliers. Guo [9] addresses these problems through a weighted least-squares regimen, by introducing an additive noise term η and reformulating the cost function to (Eq. 3). Similarly, by differentiating the sum of 2 , we retrieve a linear system of the form given in Eq. 4."
Joint Segmentation and Sub-pixel Localization in Structured Light Laryngoscopy,,= y[ln(y,"The parameters of the Gaussian function μ, σ and A can be calculated similar to Caruanas algorithm. Recall that polynomial regression has an analytical solution, where the vector of estimated polynomial regression coefficients is β = (X T X) -1 X T ŷ. This formulation is easily parallelizable on a GPU and fully differentiable. Hence, we can efficiently compute the Gaussian coefficients necessary for determining the subpixel position of local maxima. Finally, we can calculate the points position p by simple addition using Eq. 5."
Joint Segmentation and Sub-pixel Localization in Structured Light Laryngoscopy,4,Evaluation,"We implemented our code in Python (3.10.6), using the PyTorch (1.12.1) [15] and kornia (0.6.8) [6] libraries. We evaluate our code as well as the comparison methods on an Nvidia RTX 3080 GPU. We follow the respective methods closely for training. For data augmentation, we use vertical and horizontal flipping, affine and perspective transformations, as well as gamma correction and brightness modulation. We opted to use data augmentation strategies that mimic data that is likely to occur in laryngoscopy. We evaluate all approaches using a kfold cross validation scheme with k = 5, on an expanded version of the publicly available Human-Laser Endoscopic (HLE) dataset [10] that also includes segmentations of the human vocal fold itself. We evaluate on subjects that were not contained in the training sets. The data generation scheme for the human vocal fold segmentation is described further below. For our baseline, we use dilation filtering and the moment method on images segmented using the ground-truth labels similar to [10,19,20]. We opt to use the ground-truth labels to show how good these methods may become given perfect information. We train our approach via Stochastic Gradient Descent, with a learning rate of 10 -1 for 100 epochs and update the learning rate via a polynomial learning rate scheduler similar to nnU-Net [11]. For estimating the keypoints we use a window size of 7, a Gaussian blur of size 5 and set the keypoint threshold to 0.7."
Joint Segmentation and Sub-pixel Localization in Structured Light Laryngoscopy,,HLE++.,"The HLE dataset is a publicly available dataset consisting of 10 labeled in-vivo recordings of human vocal folds during phonation [10], where each recording contains one healthy subject. The labels include segmentations of the glottal gap, the glottal mid-and outline as well as the 2D image space positions of the laser dots projected onto the superior surface of the vocal folds. We expand the dataset to also include segmentation labels for the vocal folds itself. Due to the high framerate of the recordings, motion stemming from the manually recording physician is minimal and can be assumed to be linear. Thus, to generate the vocal fold segmentation masks, we generate a segmentation mask of the vocal fold region manually and calculate its centroid in the first and last frame of each video. Then, we linearly interpolate between the measured centroids. To account for the glottal gap inside each frame, we set, where F i is the vocal fold segmentation at frame i and G i the glottal segmentation, respectively.Quantitative and Qualitative Evaluation. Table 1 shows a quantitative evaluation of different neural network architectures that have been used in laryngoscopy [8,18] or medical keypoint detection tasks [21]. In general, we could reformulate this segmentation task as a 3D segmentation task, in which we model the width and height of the images as first and second dimension, and lastly time as our third dimension. However, due to the poor optimization of 3D Convolutions on GPUs [13] and the large amounts of data generated in high-speed video  1). However, frame jumps can be seen inbetween sequences. Since HLE was generated with a single recording unit, we assume that a properly trained 2D-CNN can infer occluded point positions based on the surrounding laser points as well as the observed topology of the vocal folds itself. However, we believe that it generalizes less well to arbitrary point patterns than a network architecture including temporal information. For the segmentation tasks, we measure the foreground IoU and DICE scores. For the keypoints, we evaluate the precision and the F1-score. We count a keypoint prediction as true positive, when its distance to its closest ground-truth point does not exceed 2 pixels. In Fig. 4 an example of a prediction over 5 frames is given, showing that neural networks can infer proper point positions even in case of occlusions. A further qualitative assessment of point predictions, vocal fold and glottal gap segmentations is given in Fig. 5."
Joint Segmentation and Sub-pixel Localization in Structured Light Laryngoscopy,5,Conclusion,"We presented a method for the simultaneous segmentation and laser dot localization in structured light high-speed video laryngoscopy. The general idea is that we can dedicate one channel of the output of a general multiclass segmentation model to learn Gaussian heatmaps depicting the locations of multiple unlabeled keypoints. To robustly handle noise, we propose to use Gaussian regression on a per local-maxima basis that estimates sub-pixel accurate keypoints with negligible computational overhead. Our pipeline is very accurate and robust, and can give feedback about the success of a recording within a fraction of a second. Additionally, we extended the publicly available HLE Dataset to include segmentations of the human vocal fold itself. For future work, it would be beneficial to investigate how this method generalizes to arbitrary projection patterns and non-healthy subjects."
Joint Segmentation and Sub-pixel Localization in Structured Light Laryngoscopy,,Fig. 1 .,
Joint Segmentation and Sub-pixel Localization in Structured Light Laryngoscopy,,Fig. 2 .,
Joint Segmentation and Sub-pixel Localization in Structured Light Laryngoscopy,,,
Joint Segmentation and Sub-pixel Localization in Structured Light Laryngoscopy,,Fig. 3 .,
Joint Segmentation and Sub-pixel Localization in Structured Light Laryngoscopy,,Fig. 4 .,
Joint Segmentation and Sub-pixel Localization in Structured Light Laryngoscopy,,Fig. 5 .,
Joint Segmentation and Sub-pixel Localization in Structured Light Laryngoscopy,,Table 1 .,
Joint Segmentation and Sub-pixel Localization in Structured Light Laryngoscopy,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 4.
Multi-modal Pathological Pre-training via Masked Autoencoders for Breast Cancer Diagnosis,1,Introduction,"Breast cancer (BC) is one of the most common malignant tumors in women worldwide and it causes nearly 0.7 million deaths in 2020 [26]. The pathological process is usually the golden standard approach for BC diagnosis, which relies on leveraging diverse complementary information from multi-modal data. In addition to obtaining the histological characteristics of tumors from hematoxylin and eosin (H&E) staining images, immunohistochemical (IHC) staining images are also widely used for pathological diagnoses, such as the human epidermal growth factor receptor 2 (HER2), the estrogen receptor (ER), and the progesterone receptor (PR) [22]. With the development of deep learning, there are a lot of multi-modal fusion methods for cancer diagnosis [6,7,20,21].Recently, with the development of Transformer, multi-modal pre-training has achieved great success in the fields of computer vision (CV) and natural language processing (NLP). According to the data format, there are two main multi-modal pre-training approaches, as shown in Fig. 1. One is based on isomorphic data, such as vision-language pre-training [5] and vision-speech-text pre-training [3]. The other is based on heterogeneous data. Bachmann et al. [2] proposed Multi-MAE to pre-train models with intensity images, depth images, and segmentation maps. In the field of medical image analysis, it is widely recognized that using multi-modal data can produce more accurate diagnoses than using single-modal data. However, the development of multi-modal pre-training methods has been limited due to the scarcity of paired multi-modal data. Most methods focus on chest X-ray vision-language pre-training [8,11]. To our best knowledge, there is no work for multi-modal pre-training based on pathological heterogeneous data.In this paper, we propose a multi-modal pre-training method based on masked autoencoders for BC downstream tasks. Our model consists of three parts, i.e., the modal-fusion encoder, the mixed attention, and the modal-specific decoder. We choose paired H&E and IHC (only HER2) staining images, which are cropped into non-overlapped patches as the input of our model. We randomly mask some patches by a ratio and feed the remaining patches into the modalfusion encoder to get corresponding tokens. Then the mixed attention module is used to take the intra-modal and inter-modal correlation into account. Finally, we use modal-specific decoders to reconstruct the original H&E and IHC staining images respectively. Our contributions are summarized as follows:We propose a Multi-Modal Pre-training via Masked AutoEncoders MMP-MAE for BC diagnosis. To our best knowledge, this is the first pre-training work based on multi-modal pathological data. We evaluate the proposed method on two public datasets as HEROHE Challenge and BCI challenge, which shows that our method achieves state-of-theart performance. i=1 and {yi} λ 2 N i=1 into the modal-fusion encoder to extract the patch tokens {fi} λ 1 N i=1 and {gi} λ 2 N i=1 . Then we use intra-modal attention and inter-modal attention to take patch correlation into account. X and Y are reconstructed by modal-specific decoders respectively."
Multi-modal Pathological Pre-training via Masked Autoencoders for Breast Cancer Diagnosis,2,Method,
Multi-modal Pathological Pre-training via Masked Autoencoders for Breast Cancer Diagnosis,2.1,Overview,"The proposed MMP-MAE consists of three modules, i.e., the modal-fusion encoder, the mixed attention, and the modal-specific decoder, as shown in Fig. 2. A pair of H&E and HER2 images are cropped into regular non-overlapping patches. We mask some of the patches of two modalities with a ratio. The remained patches are fed into the modal-fusion encoder to get the corresponding tokens. Then we use the mixed attention module to extract intra-modal and inter-modal complementary information. Finally, the modal-specific tokens are fed into the modal-specific decoders to reconstruct the original H&E and HER2 images. The pre-trained modal-fusion encoder could be used for downstream tasks (e.g., HER2 status prediction and HER2 image generation based on H&E images)."
Multi-modal Pathological Pre-training via Masked Autoencoders for Breast Cancer Diagnosis,2.2,MMP-MAE,"Modal-Fusion Encoder. We use ViT-base [12] as the backbone of the modalfusion encoder, which contains a linear projection, 12 transformer blocks, and a Multi-Layer Perceptron (MLP) head. We remove the MLP head and use the remained part to extract patch tokens. An image is cropped into several nonoverlapping patches, and these patches are mapped to D dimension tokens with the linear projection and added position embeddings to retain positional information. Each transformer block consists of a multi-head self-attention layer (MHSA) Mixed Attention. The mixed attention module contains intra-modal attention and inter-modal attention, as shown in Fig. 3. The intra-modal attention is the original transformer block, which consists of MHSA, MLP, LNs, and residual connections. It is defined asAlgorithm 1. Transformer processing flow.F l ← FFN(LN(F l )) + F l 10: end for Output: Class token and patch tokens F l ∈ R (N +1)×D Fig. 4. Workflow of two downstream tasks. In the HER2 staining image generation task, we remain the structure of GAN and replace the generator with our pre-trained model. In the HER2 status prediction task, we replace the feature extractor with our pre-trained model to obtain representations with HER2 semantics.In inter-modal attention, we replace MHSA with the multi-head cross-attention (MHCA) module. We use MHCA to leverage diverse complementary information between two modalities.Modal-Specific Decoder. Each modal-specific decoder is a shallow block with two transformer layers. Different from the transformer encoder, the target of the transformer decoder is used to reconstruct the original image.Reconstruction Loss. Given a pair of H&E image X and HER2 image Y , which is cut into 16 × 16 non-overlapping patches {x i } N i=1 and {y i } N i=1 . We mask some of the patches randomly with the ratio λ 1 and λ 2 (λ 1 + λ 2 = 1). The remained patches are fed into the modal-fusion encoder and the output is corresponding patch tokens {f i } λ1N i=1 and {g i } λ2N i=1 . We randomly generate masked patch tokens {e x j }(1-λ1)N j=1and {e y j }(1-λ2)N j=1, which are learnable vectors for masked patch prediction. The input of the mixed attention module is the full set of tokens {f i , eand {g i , e y j }"
Multi-modal Pathological Pre-training via Masked Autoencoders for Breast Cancer Diagnosis,,"i=λ2N,j=(1-λ2)N i=1",", which include both the remaining patch tokens and the masked patch tokens. After the process of the mixed attention module, H&E and HER2 patch tokens are fed into the modal-specific decoders respectively to reconstruct the original H&E image X and HER2 image Y . The reconstruction loss is computed by the mean squared error between the original images X, Y and the generative images X , Y , which is computed asWe use an adjustable hyperparameter θ to balance the losses of two modalities. The final loss L is defined as"
Multi-modal Pathological Pre-training via Masked Autoencoders for Breast Cancer Diagnosis,2.3,Downstream Tasks,"The pre-trained encoder could be used for downstream tasks, as shown in Fig. 4. We choose two relevant tasks: HER2 image generation based on H&E images and HER2 status prediction. In the HER2 generation task, we replace the generator of Pyramid Pix2pix, a generative adversarial network (GAN) in [16], with our pre-trained encoder and a light-weight decoder. The weights of the pre-trained encoder are fixed, and the light-weight decoder in the generator and the discriminator are learnable. We use pairs of H&E and IHC images for GAN training.In the HER2 status prediction task, we replace the universal extractor ResNet-50 [14] with our pre-trained encoder. We use CLAM-MIL [19] as the aggregator in our training process.3 Experimental Results"
Multi-modal Pathological Pre-training via Masked Autoencoders for Breast Cancer Diagnosis,3.1,Datasets,"ACROBAT Challenge. The AutomatiC Registration Of Breast cAncer Tissue (ACROBAT) Challenge [27] provides H&E WSIs and matched IHC WSIs (ER, PR, HER2, and KI67), which consists of 750 training cases, 100 validation cases, and 300 testing cases. We choose paired H&E and HER2 WSIs for pre-training. We extract the key points and descriptors from paired WSIs using SIFT [18] and SuperPoint [10]. Then the extracted key points and descriptors are matched using RANSAC [13] and SuperGlue [25]. We repeat this procedure several times on the rotated, downsampled, or transformed moving WSI to fetch the best transformation based on mean squared error (MSE) loss between source and target WSIs' descriptors. After that, the selected transformation is optimized across different levels of WSIs by gradient descent with local normalized cross-correlation (NCC) as its cost function. In the final phase of nonrigid registration, we use the optimized transformation to get the initial displacement field, which is optimized across different levels of WSIs by gradient update. The loss function of which is the weighted sum of NCC and diffusive regularization. We resize the displacement field and apply it to the original moving WSI. After all the WSI pairs are well registered, we convert the padded H&E image to grayscale and apply median blur to it. Next, the Otsu threshold is applied to extract the foreground area, which is cropped into non-overlapping 256 × 256 images. Finally, all the chosen images (around 0.35 million) from WSI in the same pair are saved for MMP-MAE pre-training.BCI Challenge. Breast Cancer Immunohistochemical Image Generation Challenge [16] consists of 3896 pairs of images for training and 977 pairs for testing, which are used to generate HER2 images based on H&E images. "
Multi-modal Pathological Pre-training via Masked Autoencoders for Breast Cancer Diagnosis,3.2,Experimental Setup,"Experiments are implemented in PyTorch [24] and with 4 NVIDIA A100 Tensor Core GPUs. We pre-train our MMP-MAE on the ACROBAT dataset with AdamW [17] and the learning rate of 1e -4 . The batch size of pre-training is 1024 and it takes about 30 h for 100 epochs. We use warmup for the first 10 epochs and the learning rate is set to 1e -6 .In the HER2 staining image generation task, we use 2 GPUs with a batch size of 4. The learning rate is 2e -4 and the optimizer is Adam. We use the learning rate decay strategy for stable training. Peak Signal to Noise Ratio (PSNR) and Structural Similarity (SSIM) are used as the evaluation indicators for the quality of the HER2 generated images.In the HER2 status prediction task, we use 1 GPU with a batch size of 1 (WSI level). The learning rate is 1e -4 and the Adam optimizer is used. Four standard metrics are used to measure the HER2 status prediction results, including the area under the receiver operator characteristic curve (AUC), Precision, Recall, and F1-score."
Multi-modal Pathological Pre-training via Masked Autoencoders for Breast Cancer Diagnosis,3.3,Method Comparison,"HER2 Staining Image Generation. Three methods on BCI datasets are compared in our experiments, as shown in Table 1. CycleGAN is a representative unsupervised method, which doesn't need paired images for training. So  cycleGAN focuses more on style transformation, and it is difficult to match the cell-level information in detail. Pix2pix and Pyramid pix2pix use paired data, which obtain better results than cycleGAN. Pyramid pix2pix uses the multiscale constraint, which performs better than pix2pix. Our method is based on the framework of Pyramid pix2pix and we replace the generator with our pretrained encoder and a lightweight decoder. Our MMP-MAE further improves the performance, which achieves higher PSNR by 1.60, and SSIM by 0.007. The visualization on the ACROBAT dataset also shows our model could learn the modality-related information, as shown in Fig. 5.HER2 Status Prediction. We compare our method with the top five methods reported in HEROHE challenge review [9]. Most of these methods use the multinetwork ensemble strategy and extra datasets. Team Macaroon uses the CAME-LYON dataset [4] for tumor classification. Team MITEL uses BACH dataset [1] for tumor classification. Team Piaz and Dratur both use a multi-network ensemble strategy to improve their performances. Team IRISAI first segment the tumor area and then predict the HER2 status. MMP-MAE still achieves competitive results by using a single pre-trained model, which is shown in Table 2. Our model improves and F1-Score by 6%. The results show our model pre-training has the ability to predict status from one modality."
Multi-modal Pathological Pre-training via Masked Autoencoders for Breast Cancer Diagnosis,4,Conclusion,"In this paper, we propose a novel multi-modal pre-training framework, MMP-MAE for BC diagnosis. MMP-MAE use paired H&E and HER2 staining images for pre-training, which could be used for several downstream tasks such as HER2 staining image generation and HER2 status prediction only by H&E modality. Both the experiment results on BCI and HEROHE datasets show our pretrained MMP-MAE demonstrates strong transfer ability. Our future work will expand our work to more modalities."
Multi-modal Pathological Pre-training via Masked Autoencoders for Breast Cancer Diagnosis,,Fig. 1 .,
Multi-modal Pathological Pre-training via Masked Autoencoders for Breast Cancer Diagnosis,,Fig. 2 .,
Multi-modal Pathological Pre-training via Masked Autoencoders for Breast Cancer Diagnosis,,Fig. 3 .,
Multi-modal Pathological Pre-training via Masked Autoencoders for Breast Cancer Diagnosis,,Fig. 5 .,
Multi-modal Pathological Pre-training via Masked Autoencoders for Breast Cancer Diagnosis,,Table 1 .,
Multi-modal Pathological Pre-training via Masked Autoencoders for Breast Cancer Diagnosis,,Table 2 .,
Accurate and Robust Patient Height and Weight Estimation in Clinical Imaging Using a Depth Camera,1,Introduction,"Many clinical imaging workflows require the patient's height and weight to be estimated in the beginning of the workflow. This information is essential for patient's safety and scan optimizations across modalities and workflows. It is used for accurate prediction of the Specific Absorption Rate (SAR) in Magnetic Resonance Imaging (MRI), contrast dose calculations in Computed Tomography (CT), and drug dose computations in Emergency Room (ER) workflows.Contrary to its importance, there are no widely established methods for estimating the patient's height and weight. Measuring these values using an actual scale is not a common clinical practice since: 1) a measurement scale is not available in every scan room, 2) manual measurements add an overhead to the clinical workflow, and 3) manual measurements may not be feasibly for some patients with limited mobility. Alternative methods such as the Lorenz formulae [1] or the Crandall formulae [2] need additional body measurements (e.g. mid-arm circumference, waist circumference and/or hip circumference) and are neither very accurate nor simple. Consequently, clinical staff usually relies either on previously recorded patient information or their own experience in estimating the patient's height and weight, where the estimated values may significantly deviate from the actual values in both cases.In this paper we present a deep-learning based method for accurately and robustly estimating the patient's height and weight in challenging and unrestricted clinical environments using depth images from a 3-dimensional (3D) camera. We aim to cover the patient demographics in common diagnostic imaging workflows. Our method is trained and validated on a very large dataset of more than 1850 volunteers and/or patients, captured in more than 7500 clinical scenarios, and consists of nearly 170k depth images. We achieve a PH5 (percentage of the height estimates within 5% error) of 98.4% and a PH15 of 99.9% for height estimation, and a PW10 (percentage of the weight estimates withing 10% error) of 95.6% and a PW20 of 99.8% for weight estimation, making the proposed method state-of-the-art in clinical setting.In addition to the clinical significance, our method has the following primary technical novelties: 1) we formulate the problem as an end-to-end single-value regression problem given only depth images as input (i.e. no error-prone intermediate stages such as volume computations), 2) we present a multi-stage training approach to ensure robustness in training (i.e. no need for hyper-parameter tunings at any stage), and 3) we evaluate our method on a very large dataset of both volunteers and patients, using 23-fold cross validation to ensure field generalization."
Accurate and Robust Patient Height and Weight Estimation in Clinical Imaging Using a Depth Camera,2,Related Work,"A large number of previous methods have been proposed and independently evaluated for patient height and weight estimation. Since patient height estimation is considered to be an easier problem, the primary focus of the previous work has been on patient weight estimation.Most of the existing work in patient weight estimation are formulae-based approaches where one or more anthropometric measurements are used for estimating the patient's weight. The Mercy method uses the humeral length and the mid-arm circumference (MAC) for estimating the paediatric body weight [4]. PAWPER XL-MAC is another height-based (in combination with MAC) method for estimating the body weight in paediatric patients [3]. Broca index [6] and Kokong formula [8] do not take into account a person's body habitus and estimates the ""ideal weight"" using only the body height information. Buckley method [7], Lorenz formulae [1], Crandall formulae [2] provide gender-specific weight estimation formulae given some anthropometric measurements such as the abdominal circumference, tight circumference, and MAC.Formulae-based approaches are independently evaluated in numerous studies, both for paediatric patients [3] and adult patients [9,10]. A common conclusion of these studies is that the formulae-based methods usually perform poorly in the clinical setting with PW10 values below 70%.More recently several methods have been proposed that leverage 3D camera input for estimating the patient's weight. In [11] an RGB-D camera and a thermal camera are used for precisely segmenting the patients and then extracting volume based features. These features are then fed into an artificial neural network (ANN) for patient weight estimation. In [12], first a 3D patient avatar is fitted to the acquired depth images, which is then used for part-volume based weight estimation. Both of these approaches require a number of additional algorithmic steps and the challenges of the clinical setup (such as heavy occlusions due to covers and/or additional devices like coils during an MRI examination) may affect the accuracy of the results.Estimation of the patient weight by the clinical staff remains to be the most common approach in the clinical workflow. In [15], the performance of clinical staff is determined as PW10 of 78% for nurses and PW10 of 59% for physicians. In [16] the performance of the clinical staff is determined as PW10 of 66% for both nurses and physicians.As a clinical acceptance criteria, Wells et al. [3] proposes a minimum accuracy for patient weight estimation as PW10 greater than 70% and PW20 greater than 95%. Overview of the proposed method is illustrated in Fig. 1. Our method takes in ""normalized"" depth images as input. This normalization covers two aspects: 1) normalization with respect to the view-point of the depth camera, and 2) normalization with respect to the variations in the patient tables. Input normalized depth images are then fed into a common feature extraction encoder network. This encoder network is trained using landmark localization as an auxiliary task. In the last stage we train and utilize two separate single-value regression decoder networks for estimating the patient's height and weight. These steps are explained in-detail in the following sub-sections."
Accurate and Robust Patient Height and Weight Estimation in Clinical Imaging Using a Depth Camera,3,Approach,
Accurate and Robust Patient Height and Weight Estimation in Clinical Imaging Using a Depth Camera,3.1,"Obtaining ""Normalized"" Depth Images","When training deep neural networks it is more data efficient to eliminate as much of the foreseen variances in the problem as possible in the preprocessing steps. Conceptually this can be thought as reducing the ""dimensionality"" of the problem before the model training even starts.Camera view-point is a good example when model inputs are images. With a known system calibration, a ""virtual camera"" may be placed in a consistent place in the scene (e.g. with respect to the patient table) and the ""re-projected"" depth images from this virtual camera may be used instead of the original depth images. This way the network training does not need to learn an invariance to the camera view-point, since this variance will be eliminated in the preprocessing. This process forms the first step in our depth image normalization. Figure 2 presents some examples. In the second step, we consider the back-surface of a patient which is not visible to the camera. In a lying down pose, the soft-tissue deforms and the backsurface of the patient takes the form of the table surface. Since there are a variety of patient tables (curved or flat) we eliminate this variance by performing a ""table subtraction"" from the view-point normalized depth images. This is especially important for accurate patient weight estimation across different systems.For table subtraction, top surfaces extracted from 3D models of the corresponding patient tables are used. For a given input image, it is assumed that the corresponding patient table is known since this information is readily available in an integrated system. Even though we leveraged the actual 3D models of the patient tables, since the proposed approach only requires the top surface, this information may also be obtained during calibration by taking a depth image of the empty patient table. Figure 3 presents some examples of the table subtraction process. "
Accurate and Robust Patient Height and Weight Estimation in Clinical Imaging Using a Depth Camera,3.2,Learning Accurate Low-Level Features,"Single-value regression problems require more attention during the training since the limited feedback provided through the loss function may result in the collapse of some of the features in the lower levels of the network, especially if a larger model is being trained.In order to ensure that the learned low-level features are high-quality, we start with the training of a standard image-in image-out encoder-decoder network using the landmark localization as an auxiliary task. With this task, the network is expected to both capture local features (for getting precise landmark locations) and holistic features (for getting globally consistent landmark locations) better than the case where the network was to asked only to regress a single-value such as the height or the weight of the patient.Once the encoder-decoder network is trained, we disregard the decoder part and use the encoder part as the pre-trained feature-extractor."
Accurate and Robust Patient Height and Weight Estimation in Clinical Imaging Using a Depth Camera,3.3,Task-Specific Decoders for Height and Weight Estimation,"The final stage of our approach is the training of two separate single-value regression networks, one for the estimation of the patient's height and the other for the weight.In this stage, we attach untrained decoder heads to the pre-trained encoder bases. During the training we allow all parameters of the model, including the pre-trained encoder, to be fine-tuned. The motivation for a complete fine-tuning comes from two primary reasons: 1) pre-training of the encoder part allows the network to start with already good low-level features, so it is less-likely to turn these good low-level features to degenerate features during the fine-tuning, and 2) by still allowing changes in the low-level features we have the potential to squeeze further performance from the networks."
Accurate and Robust Patient Height and Weight Estimation in Clinical Imaging Using a Depth Camera,4,Experiments,
Accurate and Robust Patient Height and Weight Estimation in Clinical Imaging Using a Depth Camera,4.1,Dataset,"For training and validation of our method we have collected a very large dataset of 1899 volunteers and/or patients captured in 7620 clinical workflows, corresponding to nearly 170k depth images. Within this large dataset, we did not have the patient table information for 909 patients and the corresponding 909 workflows, so this subset was used only for the training of the height estimation network. Some example depth snapshots of this extensive dataset is provided in Fig. 2.This dataset is collected from multiple sites in multiple countries, over a span of several years. The target clinical workflows such as a variety of coils, a variety of positioning equipment, unrestricted covers (light and heavy blankets), and occlusions by technicians, are covered in this dataset. Due to volunteer and patient consents, this dataset cannot be made publicly available.We consider the following inclusion criteria for the training and validation: patient weight between 45 kg to 120 kg, patient height between 140 cm to 200 cm, and patient body mass index (BMI) between 18.5 to 34.9. The distribution of the samples in our dataset, together with the above inclusion criteria, is illustrated in Fig. 4."
Accurate and Robust Patient Height and Weight Estimation in Clinical Imaging Using a Depth Camera,4.2,Training,"Training of the feature extraction network is performed using a subset of nearly 2000 workflows. For these workflows, we also acquired the corresponding fullbody 3D medical volumes (MRI acquisitions). A set of 10 anatomical landmarks corresponding to major joints (i.e. knees, elbows, shoulders, ankles, and wrists) are manually annotated by a group of experts in these 3D medical volumes. These 3D annotations are transferred to depth image coordinates for training using the known calibration information.We use a modified version of ResNet [13] as our base feature extraction network. This is a smaller version compared to the originally proposed ResNet18, where the number of features in each block is kept constant at 32. We use only bottleneck blocks instead of the basic blocks used for the original ResNet18.Training of the feature extraction network is done using the ADAM optimizer [14] with default parameters. Landmark locations are represented as 2D Once the feature extraction network is trained, we retain the encoder part and attach task-specific heads to form two separate networks, one for patient height estimation and the other one for patient weight estimation. Similar to the feature extraction network, we also train these networks using the ADAM optimizer with default parameters. As the loss function we use the symmetric mean absolute percentage error (SMAPE):where A t is the actual value and P t is the predicted value. We train for 250 epochs with a relatively large patience of 50 epochs.For height estimation we omitted the second step of depth normalization through table subtraction since the patient back-surface does not affect the height estimation significantly."
Accurate and Robust Patient Height and Weight Estimation in Clinical Imaging Using a Depth Camera,4.3,Results,"We evaluate our model using 23-fold cross-validation. Since we have multiple workflows and depth images corresponding to the same volunteer or patient (e.g. same volunteer captured both in a ""knee-scan"" acquisition and a ""hip-scan""  Table 1 provides our quantitative results. Our method achieved a PH5 of 98.4% and a PH15 of 99.9% for height estimation, and a PW10 of 95.6% and a PW20 of 99.8% for weight estimation, making the proposed method state-of-theart in clinical setting. Figures 5 and6 show the estimation scatter plots and the corresponding error histograms for height and weight estimation, respectively.We also investigated the performance of our method for weight estimation for patient BMI groups outside our inclusion criteria. For patients with BMI < 18.5, our method achieved a PW10 of 89.2% and a PW20 of 98.9%. For patients with BMI > 34.9, our method achieved a PW10 of 96.2% and a PW20 of 99.8%. Even though the performance drops a little bit for underweight population, the main reason for keeping these populations outside the inclusion criteria is not the performance, but rather the limited support in the training dataset. Accurate and rapid estimation of the patient's height and weight in clinical imaging workflows is essential for both the patient's safety and the possible patient-specific optimizations of the acquisition. In this paper, we present a deeplearning based method trained on a very large dataset of volunteer and patient depth images captured in unrestricted clinical workflows. Our method achieves a PH5 of 98.4% and a PH15 of 99.9% for height estimation, and a PW10 of 95.6% and a PW20 of 99.8% for weight estimation. These results out-perform all alternative methods to the best of our knowledge, including family estimates and clinical staff estimates.Disclaimer. The concepts and information presented in this paper are based on research results that are not commercially available. Future availability cannot be guaranteed."
Accurate and Robust Patient Height and Weight Estimation in Clinical Imaging Using a Depth Camera,,Fig. 1 .,
Accurate and Robust Patient Height and Weight Estimation in Clinical Imaging Using a Depth Camera,,Fig. 2 .,
Accurate and Robust Patient Height and Weight Estimation in Clinical Imaging Using a Depth Camera,,Fig. 3 .,
Accurate and Robust Patient Height and Weight Estimation in Clinical Imaging Using a Depth Camera,,Fig. 4 .,
Accurate and Robust Patient Height and Weight Estimation in Clinical Imaging Using a Depth Camera,,Fig. 5 .,
Accurate and Robust Patient Height and Weight Estimation in Clinical Imaging Using a Depth Camera,,Fig. 6 .,
Accurate and Robust Patient Height and Weight Estimation in Clinical Imaging Using a Depth Camera,,Table 1 .,
Segment Membranes and Nuclei from Histopathological Images via Nuclei Point-Level Supervision,1,Introduction,"Accurate quantification of immunohistochemistry (IHC) membrane staining images is a crucial aspect of disease assessment [14,15]. In clinical diagnosis, pathologists typically grade diseases by manually estimating the proportion of stained membrane area [14] or evaluating the completeness of stained membrane [23]. However, this manual approach is tedious, time-consuming [22], and error-prone [13]. Therefore, there is an urgent need for precise automatic IHC membrane analysis methods to provide objective quantitative evidence and improve diagnostic efficiency. Despite numerous deep learning methods have been proposed for detecting cell nuclei [11,20] from hematoxylin-eosin (H&E) staining images, little attention has focused on analyzing cell membranes from IHC images. Currently, only a few fully supervised IHC membrane segmentation methods have been proposed [9,19], demonstrating the superiority of deep learning-based membrane segmentation. However, full supervision requires substantial time and effort for pixel-level annotations of cell membranes and nuclei. In contrast, as annotating the centers of nuclei requires much fewer efforts, weakly supervised learning has been studied for nuclei segmentation [16,21]. Nevertheless, how to utilize point annotations to supervise cell membrane segmentation is still under investigation.This study proposes a novel point-based cell membrane segmentation method, which can significantly reduce the cost of pixel-level annotation required in conventional methods, as shown in Fig. 1. In this study, the category of the point annotation is used to describe the staining state of the cell membrane (e.g., complete-stained, incomplete-stained, and unstained). We employ a network named Seg-Net to segment the nuclei and membranes separately, followed by a Trans-Net to convert the segmentation results into semantic points. Since the accuracy of semantic points is directly related to the segmentation results, the segmentation quality can be implicitly supervised by the loss between the semantic points and the point annotations, as shown in Fig. 2 To the best of our knowledge, this is the first study that using point-level supervision for membrane segmentation from IHC images, which could significantly advance future membrane analysis. Additionally, our method is the first to employ point annotations to simultaneously supervise the segmentation of two objects. Extensive experiments confirm the efficacy of the proposed method, attaining performance that is comparable to models trained with fully supervised data."
Segment Membranes and Nuclei from Histopathological Images via Nuclei Point-Level Supervision,2,Related Works,"Deep learning-based segmentation methods have been widely developed for cell nuclei segmentation from H&E images in recent years, ranging from convolutional neural networks [5,12,24] to Transformer-based architectures [8], resulting in continuously improved accuracy in nuclei segmentation.For the task of analyzing IHC membrane-stained images, due to the challenge of pixel-level annotation, existing methods mostly adopt traditional unsupervised algorithms, such as watershed [17,26], active contour [3,25], and color deconvolution [2,4]. These traditional methods perform inadequately and are vulnerable to the effects of differences in staining intensity and abnormal staining impurities. In recent years, a few fully supervised cell membrane segmentation methods also have emerged [9,19], but the high cost of data annotation limits their applicability to various membrane staining image analysis tasks.To reduce the annotation cost of nuclei segmentation in histopathological images, weakly supervised segmentation training methods have received attention, including: 1) Unsupervised cell nuclei segmentation methods represented by adversarial-based methods [6,7]. However, unsupervised methods are challenged by the difficulty of constraining the search space of model parameters, making it hard for the model to handle visually complex pathology images, such as H&E or IHC; 2) Weakly supervised cell nucleus segmentation algorithms with point annotation [16,21]. Because the cell nucleus shape in H&E images is almost elliptical, point annotation combined with Voronoi diagram [1] were used to generate pseudo-annotations for iterative model training and refinement. Although these methods can perform weakly supervised segmentation of cell nuclei from IHC membrane-stained images, they are usually ineffective in segmenting messy cell membranes.Therefore, this paper proposes a novel point-supervised cell membrane segmentation method, addressing a major challenge in the field. The paper also explores the feasibility of point supervision for the segmentation of two types of objects (cell nuclei and cell membranes) for the first time."
Segment Membranes and Nuclei from Histopathological Images via Nuclei Point-Level Supervision,3,Method,"This study aims to explore how to perform membrane and nucleus segmentation in IHC membrane-stained images using only point-level supervision. Nuclei segmentation is performed for cell localization and counting, while membrane quantification provides clinical evidence for diagnosis. "
Segment Membranes and Nuclei from Histopathological Images via Nuclei Point-Level Supervision,3.1,Formulation of the Point-Level Supervised Segmentation Problem,"Given an input cell image set {I i } N i=1 , where N is the number of images in this set, I i ∈ R H×W ×3 with H, W representating the height and width of the image, respectively, and 3 being the number of channels of the image. Our goal is to obtain the mask of membranes (, where F θ is a segmentation network (Seg-Net) and with trainable parameters θ, and σ is the sigmoid activation function. We have point annotations P i ∈ R H×W ×(c+1) for image I i , in which c is the number of semantic categories used to describe the states of membrane staining.In order to train F θ to segment membranes M i and nuclei S i using point annotations P i in image I i , we need to establish the relationship from input to segmentation, and then to point annotation, as shown in Eq. ( 1) and Fig. 3.where G ω is the transition network (Tran-Net) with parameters ω. G ω transforms M i , S i to semantic points P i ∈ R H×W ×(c+1) , it should be noted that M i and S i respectively provide the semantic and spatial information to G ω for semantic points prediction, so that the segmentation performance is crucial for G ω . So that, by fitting P i to P i ( P i ∼= P i ), the segmentation can be supervised."
Segment Membranes and Nuclei from Histopathological Images via Nuclei Point-Level Supervision,3.2,Network Architecture,"We adopt the U-Net [18] architecture for both F θ and G ω networks. The architecture of F θ is conventional and follows the original U-Net [18]. However, in the decoder of G ω , we replace the size of the last convolution kernel from 1 × 1 to 9 × 9. This is because G ω is utilized to predict the category of semantic points, which are the center points of cells and related to the membrane. Therefore, a larger receptive field is required for the convolution to improve the accuracy of category prediction. During inference, only F θ is needed, and G ω is discarded."
Segment Membranes and Nuclei from Histopathological Images via Nuclei Point-Level Supervision,3.3,Decouple the Membranes and Nuclei Segmentation,"Our goal is to use Seg-Net to generate masks for both membranes ( M i ∈ R H×W ×1 ) and nuclei ( S i ∈ R H×W ×1 ),i.e., M i , S i = σ(F θ (I i )). However, the two Seg-Net channels are interdependent, which can result in nuclei and membranes being inseparably segmented. To overcome this issue, we enforce one channel to output the nuclei segmentation using a supervised mask S i ∈ R H×W ×1 . In this study, we create S i by expanding each point annotation to a circle with a radius of 20. Thus, to provide semantic information to Tran-Net for predicting semantic points, the other channel must contain information describing the staining status of the membrane, which in turn decouples membrane segmentation. Because both S i and S i are single-channel, we employ the naive L 1 loss to supervise the segmentation of the nuclei, as shown in Eq. ( 2)(2)"
Segment Membranes and Nuclei from Histopathological Images via Nuclei Point-Level Supervision,3.4,Constraints for Membranes Segmentation,"As there are no annotations available for pixel-level membrane segmentation, the network could result in unwanted over-segmentation of membranes. This oversegmentation can take two forms: (1) segmentation of stained impurities, which can restrict the network's generalization performance by learning simple color features, and(2) segmentation of nuclei. To address these issues, we propose a normalization loss term, norm i , which is an L 1 normalization and is defined in Eq. ( 3). The purpose of this loss term is to encourage the network to learn a smoother membrane segmentation that does not capture small stained regions or nuclei.However, relying solely on the norm i normalization term might lead to a trivial solution, as it only minimizes the average value of the prediction result, potentially resulting in a minimal maximum confidence for the cell membrane segmentation (e.g., 0.03). To prevent this issue, we introduce a hinge-loss-like loss function, presented in Eq. ( 4), to constrain the distribution of membrane segmentation results. The hyper-parameter τ in the hinge-loss function determines the expected value of the result, where a larger τ corresponds to a smaller expected value. For instance, if τ is set to 1 or 2, the expected values of the result would be 1 or 0.5, respectively. By selecting an appropriate value for τ , we can mitigate the negative impact of norm i .(4)"
Segment Membranes and Nuclei from Histopathological Images via Nuclei Point-Level Supervision,3.5,Point-Level Supervision,"Using G ω to detect the central point of the cells is a typical semantic points detection task. The difference is that the input of G ω is the output of F θ rather than the image. Nevertheless, we can also employ the cross-entropy function for point-level supervision:To enhance the spatial supervision information of the point annotations P i , it is worth noting that we extended each point annotation to a Gaussian circle with a radius of 5 pixels."
Segment Membranes and Nuclei from Histopathological Images via Nuclei Point-Level Supervision,3.6,Total Loss,"By leveraging the advantages of the above items, we can obtain the total loss as follows:where norm i and hinge i are antagonistic, and their values are close to 0.5 in the ideal optimal state, which can be achieved at τ = 1 in hinge i ."
Segment Membranes and Nuclei from Histopathological Images via Nuclei Point-Level Supervision,4,Experiments,"In order to comprehensively verify the proposed method, we conduct extensive experiments on two IHC membrane-stained data sets, namely the PDL1 (Programmed cell depth 1 light 1) and HER2 (human epidermal growth factor receiver 2) datasets. The HER2 experiment is dedicated to validate segmentation performance, while the PDL1 experiment is utilized to verify the effectiveness of converting segmentation results into clinically relevant indicators."
Segment Membranes and Nuclei from Histopathological Images via Nuclei Point-Level Supervision,4.1,Dataset,"We collected 648 HER2 and 1076 PDL1 images at 40x magnification with a resolution of 1024 × 1024 from WSIs. The PDL1 data only has point-level annotations, where pathologists categorize cells as positive or negative based on membrane staining. The HER2 data has both pixel-level and point-level annotations, where pathologists delineate the nuclei and membranes for pixel-level annotations and categorize cells based on membrane staining for point-level annotations. Pixel-level annotations are used for testing and fully supervised experiments only. We split the data into training and test sets in a 1:1 ratio and do not use a validation set since our method is trained without pixel-level annotations."
Segment Membranes and Nuclei from Histopathological Images via Nuclei Point-Level Supervision,4.2,Implementation Details and Evaluation Metric,"We totally train the networks 50 epochs and employ Adam optimizer [10] with the initial learning rate of 5×10 -4 and the momentum of 0.9. Images are randomly cropped to 512 × 512, and data augmentations such as random rotation and flip were employed during model training. The hyper-parameter τ in hinge i is set to 1.0. We employ the Intersection over Union (IoU) segmentation metric and pixel-level F 1 score to validate the performance of the proposed method. However, only point-level annotations are equipped for the PDL1 dataset, we evaluate the segmentation performance at the point-level by converting the segmentation into key point predictions, and the conversion process details are available in the supplementary materials."
Segment Membranes and Nuclei from Histopathological Images via Nuclei Point-Level Supervision,4.3,Result Analysis,"HER2 Results. Table 1 shows the cell segmentation results of the proposed method and six comparison methods on the dataset HER2. Our proposed method can segment both cell nuclei and membranes simultaneously, outperforming both unsupervised methods and other point-level methods, with an IoU score of 0.5774 and an F 1 score of 0.6899 for membranes, and an IoU score of 0.5242 and an F 1 score of 0.6795 for nuclei. Furthermore, our ablation study shows that the hinge loss and normalization loss play important roles in improving the segmentation performance. Notably, other point-level methods not only fail to segment the cell membranes but also have limited performance in segmenting cell nuclei due to over-segmentation and under-segmentation errors, as shown in Fig. 4."
Segment Membranes and Nuclei from Histopathological Images via Nuclei Point-Level Supervision,,PDL1 Results,". We chose to compare our method with unsupervised segmentation methods because existing point-supervised segmentation methods are unable to segment cell membranes. We present their results in Table 2, which illustrates that the proposed method outperforms other methods, achieving F 1 scores of 0.7131 and 0.7064 for negative and positive-stained cells, respectively. Among the unsupervised methods, color deconvolution [4] shows the best performance with F 1 scores of 0.5984 and 0.6136 for negative and positive cells, respectively. However, our proposed method significantly outperformed it. Besides, qualitative experimental results can be found in the supplementary materials.   "
Segment Membranes and Nuclei from Histopathological Images via Nuclei Point-Level Supervision,5,Conclusion,"In this paper, we present a novel method for precise segmentation of cell membranes and nuclei in immunohistochemical (IHC) membrane staining images using only point-level supervision. Our method achieves comparable performance to fully supervised pixellevel annotation methods while significantly reducing annotation costs, only requiring one-tenth of the cost of pixel-level annotation. This approach effectively reduces the expenses involved in developing, deploying, and adapting IHC membrane-stained image analysis algorithms. In the future, we plan to further optimize the segmentation results of cell nuclei to further boost the performance of the proposed method, and extend it to the whole slide images (WSIs)."
Segment Membranes and Nuclei from Histopathological Images via Nuclei Point-Level Supervision,,Fig. 1 .,
Segment Membranes and Nuclei from Histopathological Images via Nuclei Point-Level Supervision,,Fig. 2 .,
Segment Membranes and Nuclei from Histopathological Images via Nuclei Point-Level Supervision,,Fig. 3 .,
Segment Membranes and Nuclei from Histopathological Images via Nuclei Point-Level Supervision,,Fig. 4 .,
Segment Membranes and Nuclei from Histopathological Images via Nuclei Point-Level Supervision,,Table 1 .,
Segment Membranes and Nuclei from Histopathological Images via Nuclei Point-Level Supervision,,Table 2 .,
Segment Membranes and Nuclei from Histopathological Images via Nuclei Point-Level Supervision,,Acknowledgements,". This work is supported by the National Natural Science Foundation of China (NSFC Grant No. 62073260, No.62106198 and No.62276052), and the Natural Science Foundation of Shaanxi Province of China (2021JQ-461), the General Project of Education Department of Shaanxi Provincial Government under Grant 21JK0927. Medical writing support is provided by AstraZeneca China. The technical and equipment support is provided by HangZhou DiYingJia Technology Co., Ltd (DeepInformatics++). The authors would like to thank the medical team at AstraZeneca China and techinical team at DeepInformatics++ for their scientific comments on this study."
Segment Membranes and Nuclei from Histopathological Images via Nuclei Point-Level Supervision,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2_52.
Cross-View Deformable Transformer for Non-displaced Hip Fracture Classification from Frontal-Lateral X-Ray Pair,1,Introduction,"Hip fractures represent a life-changing event and carry a substantial risk of decreased functional status and death, especially in elderly patients [17]. Usually, they are diagnosed from X-ray images in clinical practice. Currently, proper X-ray fracture identification relies on the manual observation of board-certified radiologists, which leads to increased workload pressures to radiologists. However, accurate and timely diagnosis of hip fractures is critical, especially in emergency situations such as non-displaced hip fractures [13]. Therefore, automated X-ray image classification is of great significance to support the clinical assistant diagnosis.Recently, Deep Learning (DL) methods for radiography analysis have gained popularity and shown promising results [4,7,14,18], which aims to distinguish normal radiography or prioritize urgent/critical cases with the goal of reducing the radiologist workload or improving the reporting time. For example, a triaging pipeline based on the urgency of exams has been proposed in [1] and Tang et al. [21] compared different DL models applied to several public chest radiography datasets for distinguishing abnormal cases. However, these works only focus on single-view radiography analysis. When the fracture displacement in the X-ray image is not apparent,i.e., a non-displaced hip fracture as shown in Fig. 1, these methods may fail in extracting enough fracture features represented by a ridge [20] in the image and result in misdiagnosis. Therefore, it is necessary to develop cross-view learning approaches to diagnose fracture from paired views (Frontal/Lateral-images), which have been demonstrated to provide complementary features to promote the diagnostic performance [3,10]. Recent studies have been investigated for cross-view learning of X-ray images, which aims to exploit the value of paired X-ray images and fuse them to get a comprehensive anatomical representation for diagnosis [2,5,15,19,22]. However, these methods do not consider cross-view feature relations which is a quite important issue for accurate cross-view feature fusion.Since the introduction of vision transformer models [9], more researches have been developed in the tokenization process and relation modeling among tokens in an image [8,16]. Recently, deformable self-attention has been proposed to refine visual tokens [6,23,25], which is powerful in focusing on relevant regions and capturing more informative features. Motivated by this, we propose a novel cross-view deformable transformer framework for hip fracture detection from cross-view X-ray images. Firstly, deformable self-attention modules are utilized to localize reliable task-related features of each view. Secondly, the dominatedview characteristics are used to explore informative representations in the other view for effective feature fusion of cross-view X-ray images. Specifically, our contributions are three folds:1. We propose a cross-view deformable transformer framework for non-displaced hip fracture classification, in which we take advantage of discriminative features of Frontal-view as a guidance to localize informative representations of Lateral-view for cross-view feature fusion. 2. For each view, we adopt the deformable self-attention module to select pivotal tokens in a data-dependent way. 3. We build a new non-displaced hip fracture X-ray dataset which includes both Frontal and Lateral views for each case to valid the proposed method. Our approach surpasses the state of the art in accuracy by over 1.5%."
Cross-View Deformable Transformer for Non-displaced Hip Fracture Classification from Frontal-Lateral X-Ray Pair,2,Method,
Cross-View Deformable Transformer for Non-displaced Hip Fracture Classification from Frontal-Lateral X-Ray Pair,2.1,Overview,"The detailed architecture of the proposed cross-view deformable attention framework is shown in Fig. 2. To model the relations among features across different views, the framework is designed as a joint of two view-specific deformable transformer branches with four stages. For each view-specific branch, the input image is firstly processed by shifted window attention modules presented in the left of Fig. 2 to aggregate information locally, followed by the last two stages to model the global relations among the locally augmented tokens with deformable self-attention modules. In the last two stages, the query features of the Frontalview are adopted as the guidance to detect the relations among the Lateral-view tokens. The detailed design of each component is introduced below."
Cross-View Deformable Transformer for Non-displaced Hip Fracture Classification from Frontal-Lateral X-Ray Pair,2.2,View-Specific Deformable Transformer Network,"To discover the task-related regions of each view, the view-specific branch is designed as a deformable transformer network consisting of four stages. In each branch, the first two stages explore the local representations of the input images with shift-window attention modules, followed by the last two stages exploit local tokens relation using deformable self-attention modules. Specifically, our framework takes an image of size H×W ×3 as input. After the first two stages, the input image will be embedded into feature mapswhere the C denotes the channel number. The f layer3 will be passed to a query projection network W q , which is a light network to obtain the query feature maps f layer3;q . Moreover, a uniform grid p original ∈ R H/4×W/4×2 is generated as a position reference of points in f layer3 . The values of p original are linearly spaced and normalized to 2D coordinates range in) refers to the horizontal and vertical coordinates for reference points respectively. In the meanwhile, reference points offset p offsets ∈ R H/4×W/4×2 are generated from the f layer3;q by a light offset network consisted of two convolutional layers followed normalization layer, which are also normalized into (-4/H, -4/W ), ). Then the deformed features of each point are sampled at the shifted position, which could be denoted as f = S(f, p), where S represents a bilinear interpolation function. Therefore, the deformed multi-head self-attention module with M heads can be described as:where σ(•) denotes the softmax function, and d is the dimension of each head. z (m) is the embedding output from the m-th attention head, and {q (m) , k(m) , v(m) } ∈ R N ×d represents query, deformed key and value embeddings, respectively. Also, W q , W k , W v , W o are the projection networks. Features passed to the 4th stage are conducted a same operation as in 3rd stage with different feature dimensions."
Cross-View Deformable Transformer for Non-displaced Hip Fracture Classification from Frontal-Lateral X-Ray Pair,2.3,Cross-View Deformable Transformer Framework,"The proposed cross-view framework is consisted of two joint view-specific branches, with a pair of X-ray images (Frontal-view and Lateral-view) from the same patient taken as the input of two individual view-specific branches, respectively. These input images will be embedded into primary representations in the first stage of view-specific network, then these primary features will be sent to the second stage to get representations with larger receptive field. To observe correlations between Frontal-view and Lateral-view, we opt for a simple solution to share queries from the Frontal-view to model token relations of Lateral-view in a self-attention manner as the Frontal-view contains dominated diagnosis features [24]. In this way, the focused regions of the Lateral-view are determined by the discriminative features of the Frontal-view. So for the Lateral-view branch, the multi-head self-attention can be denoted as:in which (•) fr and (•) la represent the features of Frontal-view and Lateral-view, respectively. While for the Frontal-view branch, the multi-head self-attention can be denoted as:It is worth noting that view-specific reference points offset are derived from corresponding view-specific query feature maps which contain global view-specific position relations. By taking query feature maps from the Frontal-view as an informative clue, it makes sense to search relevant task-related features in the Lateral-view deformed values and keys embedding which are also discriminative features of Lateral-view. In this way, the cross-view transformer framework manages to localize task-related features in both views while exploring the crossview related representations for feature aggregation. Then the final output can be denoted as outputs =MLP(Concat(f fr , f la )), where the f fr and f la represent the output features of the last layer of the Frontal-view and Lateral-view branches, respectively. The Concat is a concatenation operation and MLP is a projection head consisted of two fully connected layers to generate logit predictions."
Cross-View Deformable Transformer for Non-displaced Hip Fracture Classification from Frontal-Lateral X-Ray Pair,2.4,Technical Details,"The view-specific model shares a similar pyramid structure with DTA-T [23].The first stage consists of one shift-window block whose head number is set as 3, followed the second stage with one shift-window block whose head number is set as 6. We adopt three deformable attention block with 12 heads in the 3rd stage and one deformable attention block with 24 heads in the 4th stage. To optimize the whole framework, we calculate the cross entropy loss between the label and final output of the cross-view deformable transformer for training."
Cross-View Deformable Transformer for Non-displaced Hip Fracture Classification from Frontal-Lateral X-Ray Pair,3,Experiment,
Cross-View Deformable Transformer for Non-displaced Hip Fracture Classification from Frontal-Lateral X-Ray Pair,3.1,Experiment Setup,"Dataset. The dataset used in this study includes 768 paired hip X-ray images (329 non-displaced fractures, 439 normal hips) from 4 different manufacturers of radiologic data sources: GE Healthcare, Philips Medical Systems, Kodak and Canon. All the hip radiographs are collected and labeled by experts with nondisplaced fractures or normal for classification task."
Cross-View Deformable Transformer for Non-displaced Hip Fracture Classification from Frontal-Lateral X-Ray Pair,,Implementation Details.,"For experiments of our dataset, we manually locate the hip region and crop a 224 × 224 image that is centered on the original hip region whose size is 400 × 600. The learning rate is set as 3e-3 for the endto-end training of the framework with a batch size of 32. We adopt a 10-fold cross-validation and report the average performance of 10 folds. For each fold, we further divide the data (the other 9 folds) into a training set (90%) and a validation set (10%) and take the best model on the validation part for testing.Evaluation Metric. We evaluate our method with Accuracy (Acc), Precision, Recall and F1 score. The Precision and Recall are calculated with one-classversus-all-other-classes and then calculate F1 score F 1 = 2•P recision•Recall P recision+Recall ."
Cross-View Deformable Transformer for Non-displaced Hip Fracture Classification from Frontal-Lateral X-Ray Pair,3.2,Experimental Results,"Comparisons with the State of the Art. The proposed method is also compared with other cross-view fusion methods; results are reported in Table 1. 1) MVC-NET: a network with back projection transposition branch to explicitly incorporate the spatial information from two views at the feature level.2) DualNet: an ensemble of two DenseNet-121 [12] networks followed a global average pooling operation of the final convolutional layer before a fully connected layer to simultaneously process multi-view images. 3) Auloss: a DualNet regularized by auxiliary view-specific classification losses. 4) ResNet18-dual: an ensemble of two ResNet18 [11] networks, and the predicted results are generated by concatenating logit outputs from each ResNet18 network. 5) Densenet-dual: an ensemble of two DenseNet networks, and the predicted results are generated by concatenating logit outputs from each DenseNet network. 6) Swin-dual: an ensemble of two swin-transformer networks [16], and the predicted results are generated by concatenating logit outputs from each swin-transformer network.As shown in Table 1, we compare our method to different dual frameworks. It can be observed that the proposed method achieves better performance than others (compare Ours with ResNet18-dual, Densenet-dual and Swin-dual), which demonstrates that the accuracy boost is due to the deformable transformer network and the feature interaction design not the increased backbone size. In addition, the MVC-NET shares a similar feature-level interaction motivation with Ours, and the 19.1% accuracy improvement indicates that our cross-view deformable attention gains better performance. Otherwise, we demonstrate the effectiveness of the deformable transformer network by comparing the Our w/o q to Swin-dual, as the only difference between these two frameworks is that the Our w/o q change the last two stages of Swin-dual to deformable transformer modules.Ablation Study. We also conduct ablation experiments to validate the design of our proposed different components. We compare the following different settings. 1) Frontal: take the Frontal image as input of the view-specific deformable transformer network to generate the prediction. 2) Frontal swin: take the Frontal image as input of the swin-transformer network to generate the prediction.3) Lateral: take the Lateral image as input of the view-specific deformable transformer network to generate the prediction. 4) Lateral swin: take the Lateral image as input of the swin-transformer network to generate the prediction. 5) Ours w/o q: the proposed framework without cross-view deformable attention.Table 1 shows the ablation results. It is observed from Ours w/o q and Ours that the proposed method improves the performance of classification by adopting the proposed cross-view deformable attention, which demonstrates that the query of Frontal-view has a positive effect on mining the discrimination features of Lateral representations. Especially, cross-view learning contributes a minimum accuracy improvement of 3% compared Ours to Frontal and Lateral as discriminative features between different views can be complementary. Moreover, we  present the performance of different view-specific networks by comparing Frontal to Frontal swin, the results show that the deformable transformer network gains higher accuracy with about 1.7% increment. For the Lateral-view, the deformable transformer network also has comparable performance to Swin-transformer.Visualization Results. To verify the effectiveness of the proposed framework, we visualize the interest regions of the model as shown in Fig. 3. It shows that the model could concentrate on the interested region of the diagnosis as labeled by expert. In addition, for the diagnosis of non-displaced hip fracture, the smoothness of the bone edge is a very important reference. As shown in Fig. 3, our model is also very good at focusing on bone smoothness in the same area from different perspectives in the same patient, indicating that the features from the Frontal view actually have a guidance to feature selection of the Lateral view."
Cross-View Deformable Transformer for Non-displaced Hip Fracture Classification from Frontal-Lateral X-Ray Pair,4,Conclusion,"This paper innovatively introduces a cross-view deformable transformer framework for non-displaced hip fracture classification from paired hip X-ray images.We adopt the deformable self-attention module to locate the interested regions of each view, while exploring feature relations among Lateral-view with the guidance of Frontal-view characteristics. In addition, the proposed deformable crossview learning method is general and has great potential to boost the performance of detecting other complicated disease. Our future work will focus on more effective training strategies and extend our framework to other cross-view medical image analysis problems."
Cross-View Deformable Transformer for Non-displaced Hip Fracture Classification from Frontal-Lateral X-Ray Pair,,Fig. 1 .,
Cross-View Deformable Transformer for Non-displaced Hip Fracture Classification from Frontal-Lateral X-Ray Pair,,Fig. 2 .,
Cross-View Deformable Transformer for Non-displaced Hip Fracture Classification from Frontal-Lateral X-Ray Pair,,Fig. 3 .,
Cross-View Deformable Transformer for Non-displaced Hip Fracture Classification from Frontal-Lateral X-Ray Pair,,Table 1 .,Frontal 93.36±2.24 93.19±5.34 91.34±3.94 92.11±2.76 Ours w/o q 95.83±1.59 96.10±4.82 93.96±3.57 96.36±1.31 Ours 96.48±1.83 95.65±4.31 96.22±3.69 95.83±2.26
Cross-View Deformable Transformer for Non-displaced Hip Fracture Classification from Frontal-Lateral X-Ray Pair,,Computational Pathology,
Semi-supervised Pathological Image Segmentation via Cross Distillation of Multiple Attentions,1,Introduction,"Automatic segmentation of tumor lesions from pathological images plays an important role in accurate diagnosis and quantitative evaluation of cancers. Recently, deep learning has achieved remarkable performance in pathological image segmentation when trained with a large and well-annotated dataset [6,13,20]. However, obtaining dense annotations for pathological images is challenging and time-consuming, due to the extremely large image size (e.g., 10000 × 10000 pixels), scattered spatial distribution, and complex shape of lesions.Semi-Supervised Learning (SSL) is a potential technique to reduce the annotation cost via learning from a limited number of labeled data along with a large amount of unlabeled data. Existing SSL methods can be roughly divided into two categories: consistency-based [9,14,23] and pseudo label-based [2] methods. The consistency-based methods impose consistency constraints on the predictions of an unlabeled image under some perturbations. For example, Mean Teacher (MT)-based methods [14,23] encourage consistent predictions between a teacher and a student model with noises added to the input. Xie et al. [21] introduced a pairwise relation network to exploit semantic consistency between each pair of images in the feature space. Luo et al. [9] proposed an uncertainty rectified pyramid consistency between multi-scale predictions. Jin et al. [7] proposed to encourage the predictions of auxiliary decoders and a main decoder to be consistent under perturbed hierarchical features. Pseudo label-based methods typically generate pseudo labels for labeled images to supervise the network [4]. Since using a model's prediction to supervise itself may over-fit its bias, Chen et al. [2] proposed Cross Pseudo Supervision (CPS) where two networks learn from each other's pseudo labels generated by argmax of the output prediction. MC-Net+ [19] utilized multiple decoders with different upsampling strategies to obtain slightly different outputs, and each decoder's probability output was sharpened to serve as pseudo labels to supervise the others. However, the pseudo labels are not accurate and contain a lot of noise, using argmax or sharpening operation will lead to over-confidence of potentially wrong predictions, which limits the performance of the models. Additionally, some related works advocated the entropy-minimization methods. Typical entropy Minimization (EM) [15] that aims to reduce the uncertainty or entropy in a system. Wu et al. [17] directly applied entropy minimization to the segmentation results.In this work, we propose a novel and efficient method based on Cross Distillation with Multiple Attentions (CDMA) for semi-supervised pathological image segmentation. Firstly, a Multi-attention Tri-branch Network (MTNet) is proposed to efficiently obtain diverse outputs for a given input. Unlike MC-Net+ [19] that is based on different upsampling strategies, our MTNet uses different attention mechanisms in three decoder branches that calibrate features in different aspects to obtain diverse and complementary outputs. Secondly, inspired by the observation that smoothed labels are more effective for noise-robust learning found in recent studies [10,22], we propose a Cross Decoder Knowledge Distillation (CDKD) strategy to better leverage the diverse predictions of unlabeled images. In CDKD, each branch serves as a teacher of the other two branches using soft label supervision, which reduces the effect of noise for more robust learning from inaccurate pseudo labels than argmax [2] and sharpening-based [19] pseudo supervision in existing methods. Differently from typical Knowledge Distillation (KD) methods [5,24] that require a pre-trained teacher to generate soft predictions, our method efficiently obtains the teacher and student's soft predictions simultaneously in a single forward pass. In addition, we apply an uncertainty minimization-based regularization to the average probability prediction across the decoders, which not only increases the network's confidence, but also improves the inter-decoder consistency for leveraging labeled images.The contribution of this work is three-fold: 1) A novel framework named CDMA based on MTNet is introduced for semi-supervised pathological image segmentation, which leverages different attention mechanisms for generating diverse and complementary predictions for unlabeled images; 2) A Cross Decoder Knowledge Distillation method is proposed for robust and efficient learning from noisy pseudo labels, which is combined with an average prediction-based uncertainty minimization to improve the model's performance; 3) Experimental results show that the proposed CDMA outperforms eight state-of-the-art SSL methods on the public DigestPath dataset [3]. "
Semi-supervised Pathological Image Segmentation via Cross Distillation of Multiple Attentions,2,Methods,"As illustrated in Fig. 1, the proposed Cross Distillation of Multiple Attentions (CDMA) framework for semi-supervised pathological image segmentation consists of three core modules: 1) a tri-branch network MTNet that uses three different attention mechanisms to obtain diverse outputs, 2) a Cross Decoder Knowledge Distillation (CDKD) module to reduce the effect of noisy pseudo labels based on soft supervision, and 3) an average prediction-based uncertainty minimization loss to further regularize the predictions on unlabeled images."
Semi-supervised Pathological Image Segmentation via Cross Distillation of Multiple Attentions,2.1,Multi-attention Tri-Branch Network (MTNet),"Attention is an effective network structure design in fully supervised image segmentation [12,16]. It can calibrate the feature maps for better performance by paying more attention to the important spatial positions or channels with only a few extra parameters. However, it has been rarely investigated in semi-supervised segmentation tasks. To more effectively exploit attention mechanisms for semisupervised pathological image segmentation, our proposed MTNet consists of a shared encoder and three decoder branches that are based on Channel Attention (CA), Spatial Attention (SA) and simultaneous Channel and Spatial Attention (CSA), respectively. The encoder consists of multiple convolutional blocks that are sequentially connected to a down-sampling layer, and each decoder has multiple convolutional blocks that are sequentially connected by an up-sampling layer. For a certain decoder, it uses CA, SA or SCA at the convolutional block at each resolution level to calibrate the features.CA branch uses channel attention blocks to calibrate the features in the first decoder. A channel attention block highlights important channels in a feature map and it is formulated as:where F represents an input feature map. P ool S avg and P ool S max represent average pooling and max-pooling across the spatial dimension, respectively. MLP and σ denote multi-layer perception and the sigmoid activation function respectively. F c is the output feature map calibrated by channel attention.SA branch leverages spatial attention to highlight the most relevant spatial positions and suppress the irrelevant regions in a feature map. An SA block is:where Conv denotes a convolutional layer. P ool C avg and P ool C max are average and max-pooling across the channel dimension, respectively. ⊕ means concatenation.CSA branch calibrates the feature maps using a CSA block for each convolutional block. A CSA block consists of a CA block followed by an SA block, taking advantage of channel and spatial attention simultaneously.Due to the different attention mechanisms, the three decoder branches pay attention to different aspects of feature maps and lead to different outputs. To further improve the diversity of the outputs and alleviate over-fitting, we add a dropout layer and a feature noise layer η [11] before each of the three decoders. For an input image, the logit predictions obtained by the three branches are denoted as Z CA , Z SA and Z CSA , respectively. After using a standard Softmax operation, their corresponding probability prediction maps are denoted as P CA , P SA and P CSA , respectively."
Semi-supervised Pathological Image Segmentation via Cross Distillation of Multiple Attentions,2.2,Cross Decoder Knowledge Distillation (CDKD),"Since the three branches have different decision boundaries, using the predictions from one branch as pseudo labels to supervise the others would avoid each branch over-fitting its bias. However, as the predictions for unlabeled training images are noisy and inaccurate, using hard or sharpened pseudo labels [2,19] would strengthen the confidence on incorrect predictions, leading the model to overfit the noise [10,22]. To address this problem, we introduce CDKD to enhance the ability of our MTNet to leverage unlabeled images and eliminate the negative impact of noisy pseudo labels. It forces each decoder to be supervised by the other two decoders' soft predictions. Following the practice of KD [5], a temperature calibrated Softmax (T-Softmax) is used to soften the probability maps:where z c represents the logit prediction for class c of a pixel, and pc is the soft probability value for class c. Temperature T is a parameter to control the softness of the output probability. Note that T = 1 corresponds to a standard Softmax function, and a larger T value leads to a softer probability distribution with higher entropy. When T < 1, Eq. 3 is a sharpening function.Let PCA , PSA and PCSA represent the soft probability map obtained by T-Softmax for the three branches, respectively. With the other two branches being the teachers, the KD loss for the CSA branch is:where KL() is the Kullback-Leibler divergence function. Note that the gradient of L CSA kd is only back-propagated to the CSA branch, so that the knowledge is distilled from the teachers to the student. Similarly, the KD losses for the CA and SA branches are denoted as L CA kd and L SA kd , respectively. Then, the total distillation loss is defined as:"
Semi-supervised Pathological Image Segmentation via Cross Distillation of Multiple Attentions,2.3,Average Prediction-Based Uncertainty Minimization,"Minimizing the uncertainty (e.g., entropy) [15] has been shown to be an effective regularization for predictions on unlabeled images, which increases the model's confidence on its predictions. However, applying uncertainty minimization to each branch independently may lead to inconsistent predictions between the decoders where each of them is very confident, e.g., two branches predict the foreground probability of a pixel as 0.0 and 1.0 respectively. To avoid this problem and further encourage inter-decoder consistency for regularization, we propose an average prediction-based uncertainty minimization:where P = (P CSA + P CA + P SA )/3 is the average probability map. C and N are the class number and pixel number respectively. P c i is the average probability for class c at pixel i. Note that when L um for a pixel is close to zero, the average probability for class c of that pixel is close to 0.0 (1.0), which drives all the decoders to predict it as 0.0 (1.0) and encourages inter-decoder consistency.Finally, the overall loss function for our CDMA is:where L sup = (L CSA sup + L CA sup + L SA sup )/3 is the average supervised learning loss for the three branches on the labeled training images, and the supervised loss for each branch calculates the Dice loss and cross entropy loss between the probability prediction (P CSA , P CA and P SA ) and the ground truth label. λ 1 and λ 2 are the weights of L cdkd and L um respectively. Note that L cdkd and L um are applied on both labeled and unlabeled training images. "
Semi-supervised Pathological Image Segmentation via Cross Distillation of Multiple Attentions,3,Experiments and Results,"Dataset and Implementation Details. We used the public DigestPath dataset [3] for binary segmentation of colonoscopy tumor lesions from Whole Slide Images (WSI) in the experiment. The WSIs were collected from four medical institutions of ×20 magnification (0.475 μm/pixel) with an average size of 5000 × 5000. We randomly split 130 malignant WSIs into 100, 10, and 20 for training, validation and testing, respectively. For SSL, we investigated two annotation ratios: 5% and 10%, where only 5 and 10 WSIs in the training set were taken as annotated respectively. Labeled WSIs were randomly selected. For computational feasibility, we cropped the WSIs into patches with a size of 256 × 256.At inference time for segmenting a WSI, we used a sliding window of size 256×256 with a stride of 192 × 192.The CDMA framework was implemented in PyTorch, and all experiments were performed on one NVIDIA 2080Ti GPU. MTNet was implemented by extending DeepLabv3+ [1] into a tri-branch network, where the three decoders were equipped with CA, SA and CSA blocks respectively. The encoder used a backbone of ResNet50 pre-trained on ImageNet. The kernel size of Conv in the SA block is 7 × 7. SGD optimizer was used for training, with weight decay 5 × 10 -4 , momentum 0.9 and epoch number 150. The learning rate was initialized to 10 -3 and decayed by 0.1 every 50 epochs. The hyper-parameter setting was λ 1 = λ 2 = 0.1, T = 10 based on the best results on the validation set. The batch size was 16 (8 labeled and 8 unlabeled patches). For data augmentation, we adopted random flipping, random rotation, and random Gaussian noise. For inference, only the CSA branch was used due to the similar performance of the three branches after converge and the increased inference time of their ensemble, and no post-processing was used. Dice Similarity Coefficient (DSC) and Jaccard Index (JI) were used for quantitative evaluation. Comparison with State-of-the-Art Methods. Our CDMA was compared with eight existing SSL methods: 1) Entropy Minimization (EM) [15]; 2) Mean Teacher (MT) [14]; 3) Uncertaitny-Aware Mean Teacher (UAMT) [23]; 4) R-Drop [18] that introduces a dropout-based consistency regularization between two networks; 5) CPS [2]; 6) Hierarchical Consistency Enforcement (HCE) [7]; 7) CNN&Transformer [8] that introduces cross-supervision between CNN and Transformer; 8) MC-Net+ [19] that imposes mutual consistency between multiple slightly different decoders. They were also compared with the lower bound of Supervised Learning (SL) that only learns from the labeled images. All these methods used the same backbone of DeepLabv3+ [1] for a fair comparison.Quantitative evaluation of these methods is shown in Table 1. In the existing methods, MC-Net+ [19] and CPS [2] showed the best performance for both of the two annotation ratios. Our proposed CDMA achieved a better performance than all the existing methods, with a DSC score of 69.72% and 72.24% when the annotation ratio was 5% and 10%, respectively. Figure 2 shows a qualitative comparison between different methods. It can be observed that our CDMA yields less mis-segmentation compared with CPS [2] and MC-Net+ [19]. Ablation Study. For ablation study, we set the baseline as using the proposed MTNet with three different decoders for supervised learning from labeled images only. It obtained an average DSC of 65.02% and 68.61% under the two annotation ratios respectively. The proposed L cdkd was compared with two variants: L cdkd (argmax) and L cdkd (T =1) that represent using hard pseudo labels and standard probability output obtained by Softmax for CDKD respectively. Table 2 shows that our L cdkd obtained an average DSC of 68.84% and 71.49% under the two annotation ratios respectively, and it outperformed L cdkd (argmax) and L cdkd (T =1), demonstrating that our CDKD based on softened probability prediction is more effective in dealing with noisy pseudo labels. By introducing our average prediction-based uncertainty minimization L um , the DSC was further improved to 69.72% and 72.24% under the two annotation ratios respectively. In addition, replacing our L um by applying entropy minimization to each branch respectively (L um ) led to a DSC drop by around 0.65%. Then, we compared different MTNet variants: 1) MTNet(dual) means a dualbranch structure (removing the CSA branch); 2) MTNet(csa×3) means all the three branches use CSA blocks; 3) MTNet(-atten) means no attention block is used in all the branches; and 4) MTNet(ensb) means using an ensemble of the three branches for inference. Note that all these variants were trained with L cdkd and L um . The results in the second section of Table 2 show that using the same structures for different branches, i.e., MTNet(-atten) and MTNet(csa×3), had a lower performance than using different attention blocks, and using three attention branches outperformed just using two attention branches. It can also be found that using CSA branch for inference had a very close performance to MTNet(ensb), and it is more efficient than the later."
Semi-supervised Pathological Image Segmentation via Cross Distillation of Multiple Attentions,4,Conclusion,"We have presented a novel semi-supervised framework based on Cross Distillation of Multiple Attentions (CDMA) for pathological image segmentation. It employs a Multi-attention Tri-branch network to generate diverse predictions based on channel attention, spatial attention, and simultaneous channel and spatial attention, respectively. Different attention-based decoder branches focus on various aspects of feature maps, resulting in disparate outputs, which is beneficial to semi-supervised learning. To eliminate the negative impact of incorrect pseudo labels in training, we employ a Cross Decoder Knowledge Distillation (CDKD) to enforce each branch to learn from soft labels generated by the other two branches. Experimental results on a colonoscopy tissue segmentation dataset demonstrated that our CDMA outperformed eight state-of-the-art SSL methods. In the future, it is of interest to apply our method to multi-class segmentation tasks and pathological images from different organs."
Semi-supervised Pathological Image Segmentation via Cross Distillation of Multiple Attentions,,Fig. 1 .,
Semi-supervised Pathological Image Segmentation via Cross Distillation of Multiple Attentions,,Fig. 2 .,
Semi-supervised Pathological Image Segmentation via Cross Distillation of Multiple Attentions,,Table 1 .,* 68.32±21.18 * 52.35±21.53 * 53.62±20.32 * EM [15] 67.09±24.28 * 70.01±22.24 * 54.55±22.40 * 56.96±21.70 * MT [14] 67.46±23.10 * 70.19±21.72 * 54.68±21.27 * 56.38±21.21 * * 70.09±22.07 * 55.40±22.54 * 57.64±21.80 * Ours (CSA branch)
Semi-supervised Pathological Image Segmentation via Cross Distillation of Multiple Attentions,,69.72±22.06 72.24±21.21 57.09±21.23 60.17±21.98,
Semi-supervised Pathological Image Segmentation via Cross Distillation of Multiple Attentions,,Table 2 .,
Towards Novel Class Discovery: A Study in Novel Skin Lesions Clustering,1,Introduction,"Automatic identification of lesions from dermoscopic images is of great importance for the diagnosis of skin cancer [16,22]. Currently, deep learning mod-els, especially those based on deep convolution neural networks, have achieved remarkable success in this task [17,18,22]. However, this comes at the cost of a large amount of labeled data that needs to be collected for each class. To alleviate the labeling burden, semi-supervised learning has been proposed to exploit a large amount of unlabeled data to improve performance in the case of limited labeled data [10,15,19]. However, it still requires a small amount of labeled data for each class, which is often impossible in real practice. For example, there are roughly more than 2000 named dermatological diseases today, of which more than 200 are common, and new dermatological diseases are still emerging, making it impractical to annotate data from scratch for each new disease category [20]. However, since there is a correlation between new and known diseases, a priori knowledge from known diseases is expected to help automatically identify new diseases [9].One approach to address the above problem is novel class discovery (NCD) [7,9,24], which aims to transfer knowledge from known classes to discover new semantic classes. Most NCD methods follow a two-stage scheme: 1) a stage of fully supervised training on known category data and 2) a stage of clustering on unknown categories [7,9,24]. For example, Han et al. [9] further introduced self-supervised learning in the first stage to learn general feature representations. They also used ranking statistics to compute pairwise similarity for clustering. Zhong et al. [24] proposed OpenMix based on the mixup strategy [21] to further exploit the information from known classes to improve the performance of unsupervised clustering. Fini et al. [7] proposed UNO, which unifies multiple objective functions into a holistic framework to achieve better interaction of information between known and unknown classes. Zhong et al. [23] used neighborhood information in the embedding space to learn more discriminative representations. However, most of these methods require the construction of a pairwise similarity prediction task to perform clustering based on pairwise similarity pseudo labels between samples. In this process, the generated pseudo labels are usually noisy, which may affect the clustering process and cause error accumulation. In addition, they only consider the global alignment of samples to the category center, ignoring the local inter-sample alignment thus leading to poor clustering performance.In this paper, we propose a new novel class discovery framework to automatically discover novel disease categories. Specifically, we first use contrastive learning to pretrain the model based on all data from known and unknown categories to learn a robust and general semantic feature representation. Then, we propose an uncertainty-aware multi-view cross-pseudo-supervision strategy to perform clustering. It first uses a self-labeling strategy to generate pseudo-labels for unknown categories, which can be treated homogeneously with ground truth labels. The cross-pseudo-supervision strategy is then used to force the model to maintain consistent prediction outputs for different views of unlabeled images.In addition, we propose to use prediction uncertainty to adaptively adjust the contribution of the pseudo labels to mitigate the effects of noisy pseudo labels. Finally, to encourage local neighborhood alignment and further refine the pseudo labels, we propose a local information aggregation module to aggregate the information of the neighborhood samples to boost the clustering performance. We conducted extensive experiments on the dermoscopy dataset ISIC 2019, and the experimental results show that our method outperforms other state-of-the-art comparison algorithms by a large margin. In addition, we also validated the effectiveness of different components through extensive ablation experiments."
Towards Novel Class Discovery: A Study in Novel Skin Lesions Clustering,2,Methodology,"Given an unlabeled dataset {x u i } N u i=1 with N u images, where x u i is the ith unlabeled image. Our goal is to automatically cluster the unlabeled data into C u clusters. In addition, we also have access to a labeled dataset {x l i , y l i } N l i=1 with N l images, where x l i is the ith labeled image and y l i ∈ Y = 1, . . . , C l is its corresponding label. In the novel class discovery task, the known and unknown classes are disjoint, i.e., C l ∩ C u = ∅. However, the known and unknown classes are similar, and we aim to use the knowledge of the known classes to help the clustering of the unknown classes. The overall framework of our proposed novel class discovery algorithm is shown in Fig. 1. Specifically, we first learn general and robust feature representations through contrastive learning. Then, the uncertainty-aware multi-view cross-pseudo-supervision strategy is used for joint training on all category data. Finally, the local information aggregation module benefits the NCD by aggregating the useful information of the neighborhood samples.Contrastive Learning. To achieve a robust feature representation for the NCD task, we first use noise contrastive learning [8] to pretrain the feature extractor network, which effectively avoids model over-fitting to known categories. Specifically, we use x i and x i to represent different augmented versions of the same image in a mini-batch. The unsupervised contrastive loss can be formulated as:where z i = E(x i ) is the deep feature representation of the image x i , E is the feature extractor network, and τ is the temperature value. 1 is the indicator function.In addition, to help the feature extractor learn semantically meaningful feature representations, we introduce supervised contrastive learning [12] for labeled known category data, which can be denoted as:where N (i) represents the sample set with the same label as x i in a mini-batch data. |N (i)| represents the number of samples. The overall contrastive loss can be expressed as:, where μ denotes the balance coefficient. B l is the labeled subset of mini-batch data.Uncertainty-Aware Multi-view Cross-Pseudo-Supervision. We now describe how to train uniformly on known and unknown categories using the uncertainty-aware multi-view cross-pseudo-supervision strategy. Specifically, we construct two parallel classification models M 1 and M 2 , both of them composed of a feature extractor and two category classification heads, using different initialization parameters. For an original image x i , we generate two augmented versions of x i , x v1 i and x v2 i . We then feed these two augmented images into M 1 and M 2 to obtain the predictions for x v1 i and x v2 i :The prediction outputs are obtained by concatenating the outputs of the two classification heads and then passing a softmax layer [7]. Then, we can compute the ensemble predicted output of M 1 and M 2 :Next, we need to obtain training targets for all data. For an input image x i , if x i is from the known category, we construct the training target as one hot vector, where the first C l elements are ground truth labels and the last C u elements are 0. If x i is from the unknown category, we set the first C l elements to 0 and use pseudo labels for the remaining C u elements.We follow the self-labeling method in [1,3] to generate pseudo labels. Specifically, the parameters in the unknown category classification head can be viewed as prototypes of each category, and our training goal is to distribute a set of samples uniformly to each prototype while maximizing the similarity between samples and prototypes [1]. Let P = p u 1 ; . . . ; p u Bu ∈ R Bu×C u denotes the ensemble prediction of data of unknown categories in a mini-batch, where B u represents the number of samples. Here we only consider the output of the unknown categories head due to the samples coming from unknown categories [7]. We obtain the pseudo label by optimizing the following objective:where Y = y u 1 ; . . . ; y u Bu ∈ R Bu×C u will assign B u unknown category samples to C u category prototypes uniformly, i.e., each category prototype will be selected B u /C u times on average. S is the search space. H is the entropy function used to control the smoothness of Y. δ is the hyperparameter. The solution to this objective can be calculated by the Sinkhorn-Knopp algorithm [6]. After generating the pseudo-labels, we can combine them with the ground truth labels of known categories as training targets for uniform training.To mitigate the effect of noisy pseudo labels, we propose to use prediction uncertainty [14] to adaptively adjust the weights of pseudo labels. Specifically, we first compute the variance of the predicted outputs of the models for the different augmented images via KL-divergence:where E represents the expected value. If the variance of the model's predictions for different augmented images is large, the pseudo label may be of low quality, and vice versa. Then, based on the prediction variance of the two models, the multi-view cross-pseudo supervision loss can be formulated as:where L ce denotes the cross-entropy loss. y v1 and y v2 are the training targets.Local Information Aggregation. After the cross-pseudo-supervision training described above, we are able to assign the instances to their corresponding clustering centers. However, it ignores the alignment between local neighborhood samples, i.e., the samples are susceptible to interference from some irrelevant semantic factors such as background and color. Here, we propose a local information aggregation to enhance the alignment of local samples. Specifically, as shown in Fig. 1, we maintain a first-in-first-out memory bankduring the training process, which contains the features of N m most recent samples and their pseudo labels. For each sample in the current batch, we compute the similarity between its features and the features of each sample in the memory bank:Then based on this feature similarity, we obtain the final pseudo labels as:k , where ρ is the balance coefficient. By aggregating the information of the neighborhood samples, we are able to ensure consistency between local samples, which further improves the clustering performance."
Towards Novel Class Discovery: A Study in Novel Skin Lesions Clustering,3,Experiments,"Dataset. To validate the effectiveness of the proposed algorithm, we conduct experiments on the widely used public dermoscopy challenge dataset ISIC 2019 [4,5]. The dataset contains a total of 25,331 dermoscopic images from eight categories: Melanoma (MEL), Melanocytic Nevus (NV), Basal Cell Carcinoma (BCC), Actinic Keratosis (AK), Benign Keratosis (BKL), Dermatofibroma (DF), Vascular Lesion (VASC), and Squamous Cell Carcinoma (SCC). Since the dataset suffers from severe category imbalance, we randomly sampled 500 samples from those major categories (MEL, NV, BCC, BKL) to maintain category balance. Then, we construct the NCD task where we treat 50% of the categories (AK, MEL, NV, BCC) as known categories and the remaining 50% of the categories (BKL, SCC, DF, VASC) as unknown categories. We also swap the known and unknown categories to form a second NCD task. For task 1 and task 2, we report the average performance of 5 runs. Implementation Details. We used ResNet-18 [11] as the backbone of the classification model. The known category classification head is an l2 -normalized linear classifier with C l output units. The unknown category classification head consists of a projection layer with 128 output units, followed by an l2 -normalized linear classifier with C u output units. In the first contrastive learning pre-training step, we used SGD optimizer to train the model for 200 epochs and gradually decay the learning rate starting from 0.1 and dividing it by 5 at the epochs 60, 120, and 180. μ is set to 0.5, τ is set to 0.5. In the joint training phase, we fix the parameters of the previous feature extractor and only fine-tune the parameters of the classification head. We use the SGD optimizer to train the model for 200 epochs with linear warm-up and cosine annealing (lr base = 0.1, lr min = 0.001), and the weight decay is set to 1.5 × 10 -4 . For data augmentation, we use random horizontal/vertical flipping, color jitter, and Gaussian blurring following [7]. For pseudo label, we use the Sinkhorn-Knopp algorithm with hyperparameters inherited from [7]: δ = 0.05 and the number of iterations is 3. We use a memory bank M of size 100 and the hyperparameter ρ is set to 0.6. The batch size in all experiments is 512. In the inference phase, we only use the output of the unknown category classification head of M 1 [9]. Following [9,23,24], we report the clustering performance on the unlabeled unknown category dataset. We assume that the number of unknown categories is known and it can also be obtained by the category number estimation method proposed in [9].  [9] 0.5652 0.2571 0.2203 0.4284 0.1164 0.1023 RankStats+ [9] 0.5845 0.2633 0.2374 0.4362 0.1382 0.1184 OpenMix [24] 0.6083 0.2863 0.2512 0.4684 0.1519 0.1488 NCL [23] 0.5941 0.2802 0.2475 0.4762 0.1635 0.1573 UNO [7] 0.6131 0.3016 0.2763 0.4947 0.1692 0.1796 Ours 0.6654 0.3372 0.3018 0.5271 0.1826 0.2033Following [2,9], we use the average clustering accuracy (ACC), normalized mutual information (NMI) and adjusted rand index (ARI) to evaluate the clustering performance of different algorithms. Specifically, we first match the clustering assignment and ground truth labels by the Hungarian algorithm [13]. After the optimal assignment is determined, we then compute each metric. We implement all algorithms based on the PyTorch framework and conduct experiments on 8 RTX 3090 GPUs.Comparison with State-of-the-Art Methods. We compare our algorithms with some state-of-the-art NCD methods, including RankStats [9], RankStats+ (RankStats with incremental learning) [9], OpenMix [24], NCL [23], UNO [7]. we also compare with the benchmark method (Baseline), which first trains a model using known category data and then performs clustering on unknown category data. Table 1 shows the clustering performance of each comparison algorithm on different NCD tasks. It can be seen that the clustering performance of the benchmark method is poor, which indicates that the model pre-trained using only the known category data does not provide a good clustering of the unknown category. Moreover, the state-of-the-art NCD methods can improve the clustering performance, which demonstrates the effectiveness of the currently popular two-stage solution. However, our method outperforms them, mainly due to the fact that they need to generate pairwise similarity pseudo labels through features obtained based on self-supervised learning, while ignoring the effect of noisy pseudo labels. Compared with the best comparison algorithm UNO, our method yields 5.23% ACC improvement, 3.56% NMI improvement, and 2.55% ARI improvement on Task1, and 3.24% ACC improvement, 1.34% NMI improvement, and 2.37% ARI improvement on Task2, which shows that our method is able to provide more reliable pseudo labels for NCD.Ablation Study of Each Key Component. We performed ablation experiments to verify the effectiveness of each component. As shown in Table 2, CL is contrastive learning, UMCPS is uncertainty-aware multi-view cross-pseudosupervision, and LIA is the local information aggregation module. It can be observed that CL brings a significant performance gain, which indicates that contrastive learning helps to learn a general and robust feature representation for NCD. In addition, UMCPS also improves the clustering performance of the model, which indicates that unified training helps to the category information interaction. LIA further improves the clustering performance, which indicates that local information aggregation helps to provide better pseudo labels. Finally, our algorithm incorporates each component to achieve the best performance.Ablation Study of Contrastive Learning. We further examined the effectiveness of each component in contrastive learning. Recall that the contrastive learning strategy includes supervised contrastive learning for the labeled known category data and unsupervised contrastive learning for all data. As shown in Table 3, it can be observed that both components improve the clustering performance of the model, which indicates that SCL helps the model to learn semantically meaningful feature representations, while UCL makes the model learn robust unbiased feature representations and avoid its overfitting to known categories.Uncertainty-Aware Multi-view Cross-Pseudo-Supervision. We also examine the effectiveness of uncertainty-aware multi-view cross-pseudosupervision. We compare it with 1) w/o CPS, which does not use cross-pseudosupervision, and 2) CPS, which uses cross-pseudo-supervision but not the uncertainty to control the contribution of the pseudo label. As shown in Table 3, it can be seen that CPS outperforms w/o CPS, which indicates that CPS encourages the model to maintain consistent predictions for different augmented versions of the input images, and enhances the generalization performance of the model. UMCPS achieves the best clustering performance, which shows its ability to use uncertainty to alleviate the effect of noisy pseudo labels and avoid causing error accumulation."
Towards Novel Class Discovery: A Study in Novel Skin Lesions Clustering,4,Conclusion,"In this paper, we propose a novel class discovery framework for discovering new dermatological classes. Our approach consists of three key designs. First, contrastive learning is used to learn a robust feature representation. Second, uncertainty-aware multi-view cross-pseudo-supervision strategy is trained uniformly on data from all categories, while prediction uncertainty is used to alleviate the effect of noisy pseudo labels. Finally, the local information aggregation module further refines the pseudo label by aggregating the neighborhood information to improve the clustering performance. Extensive experimental results validate the effectiveness of our approach. Future work will be to apply this framework to other medical image analysis tasks."
Towards Novel Class Discovery: A Study in Novel Skin Lesions Clustering,,Fig. 1 .,
Towards Novel Class Discovery: A Study in Novel Skin Lesions Clustering,,Table 1 .,
Towards Novel Class Discovery: A Study in Novel Skin Lesions Clustering,,Table 2 .,
Towards Novel Class Discovery: A Study in Novel Skin Lesions Clustering,,Table 3 .,
Towards Novel Class Discovery: A Study in Novel Skin Lesions Clustering,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 3.
Enhancing Automatic Placenta Analysis Through Distributional Feature Recomposition in Vision-Language Contrastive Learning,1,Introduction,"World Bank data from 2020 suggests that while the infant mortality rate in high-income countries is as low as 0.4%, the number is over ten times higher in low-income countries (approximately 4.7%). This stark contrast underlines the necessity for accessible healthcare. The placenta, as a vital organ connecting the fetus to the mother, has discernable features such as meconium staining, infections, and inflammation. These can serve as indicators of adverse pregnancy outcomes, including preterm delivery, growth restriction, respiratory or neurodevelopmental conditions, and even neonatal deaths [9].In a clinical context, these adverse outcomes are often signaled by morphological changes in the placenta, identifiable through pathological analysis [19]. Timely conducted placental pathology can reduce the risks of serious consequences of pregnancy-related infections and distress, ultimately improving the well-being of newborns and their families. Unfortunately, traditional placenta pathology examination is resource-intensive, requiring specialized equipment and expertise. It is also a time-consuming task, where a full exam can easily take several days, limiting its widespread applications even in developed countries. To overcome these challenges, researchers have been exploring the use of automatic placenta analysis tools that rely on photographic images. By enabling broader and more timely placental analysis, these tools could help reduce infant fatalities and improve the quality of life for families with newborns. [17,20,23] and classifying [1,8,10,13,15,21,26] placenta images using histopathological, ultrasound, or MRI data. However, these methods are dependent on expensive and bulky equipment, restricting the accessibility of reproductive healthcare. Only limited research has been conducted on the gross analysis of post-birth placenta photographs, which have a lower equipment barrier. AI-PLAX [4] combines handcrafted features and deep learning, and a more recent study [29] relies on deep learning and domain adaptation. Unfortunately, both are constrained by issues such as data scarcity and single modality, which hinder their robustness and generalizability. To address these, Pan et al. [16] incorporated vision-andlanguage contrastive learning (VLC) using pathology reports. However, their method struggles with variable-length reports and is computationally demanding, making it impractical for low-resource communities."
Enhancing Automatic Placenta Analysis Through Distributional Feature Recomposition in Vision-Language Contrastive Learning,,Related Work. Considerable progress has been made in segmenting,"With growing research in vision-and-language and contrastive learning [18,28], recent research has focused on improving the performance and efficiency of VLC approaches. They propose new model architectures [2,24], better visual representation [7,27], loss function design [14,16], or sampling strategies [5,12].However, these methods are still not suitable for variable-length reports and are inefficient in low-resource settings.Our Contributions. We propose a novel framework for more accurate and efficient computer-aided placenta analysis. Our framework introduces two key enhancements: Pathology Report Feature Recomposition, a first in the medical VLC domain that captures features from pathology reports of variable lengths, and Distributional Feature Recomposition, which provides a more robust, distribution-aware representation. We demonstrate that our approach improves representational power and surpasses previous methods by a significant performance margin, without additional data. Furthermore, we boost training and testing efficiency by eliminating the large language model (LLM) from the training process and incorporating more efficient encoders. To the best of our knowledge, this is the first study to improve both the efficiency and performance of VLC training techniques for placenta analysis."
Enhancing Automatic Placenta Analysis Through Distributional Feature Recomposition in Vision-Language Contrastive Learning,2,Dataset,"We use the exact dataset from Pan et al. [16] collected using a professional photography instrument in the pathology department of the Northwestern Memorial Hospital (Chicago) from 2014 to 2018 and an iPad in 2021. There are three parts of the dataset: 1) the pre-training dataset, containing 10,193 image-and-text pairs; 2) the primary fine-tuning dataset, comprising 2,811 images labeled for five tasks: meconium, fetal inflammatory response (FIR), maternal inflammatory response (MIR), and histologic chorioamnionitis, and neonatal sepsis; and 3) the iPad evaluation dataset, consisting of 52 images from an iPad labeled for MIR and clinical chorioamnionitis. As with the original study, we assess the effectiveness of our method on the primary dataset, while utilizing iPad images to evaluate the robustness against distribution shifts. All images contain the fetal side of a placenta, the cord, and a ruler for scale. The pre-training data is also accompanied by a corresponding text sequence for the image containing a part of the corresponding pathology report as shown in Fig. 1. A detailed breakdown of the images is provided in the supplementary materials."
Enhancing Automatic Placenta Analysis Through Distributional Feature Recomposition in Vision-Language Contrastive Learning,3,Method,"This section aims to provide an introduction to the background, intuition, and specifics of the proposed methods. An overview is given in Fig. 1."
Enhancing Automatic Placenta Analysis Through Distributional Feature Recomposition in Vision-Language Contrastive Learning,3.1,Problem Formulation,"Our tasks are to train an encoder to produce placenta features and a classifier to classify them. Formally, we aim to learn a function f v using a learned function f u , such that for any pair of input (x i , t i ) and a similarity function sim, we have where sim(u, v) represents the cosine similarity between the two feature vectors u = f u (x), v = f v (t). The objective function for achieving inequality (1) is:where τ is the temperature hyper-parameter and N is the mini-batch size.To train a classifier, we aim to learn a function f c t using the learned function"
Enhancing Automatic Placenta Analysis Through Distributional Feature Recomposition in Vision-Language Contrastive Learning,3.2,Pathology Report Feature Recomposition,"Traditional VLC approaches for medical image and text analysis, such as Con-VIRT [28], encode the entire natural language medical report or electronic health record (EHR) associated with each patient into a single vector representation using a language model. However, solely relying on a pre-trained language model presents two significant challenges. First, the encoding process can result in suppression of important features in the report as the encoder is allowed to ignore certain placental features to minimize loss, leading to a single dominant feature influencing the objective (1), rather than the consideration of all relevant features in the report. Second, the length of the pathology report may exceed the capacity of the text encoder, causing truncation (e.g., a BERT [6] usually allows 512 sub-word tokens during training). Moreover, recent LLMs may handle text length but not feature suppression. Our method seeks to address both challenges simultaneously.Our approach addresses the limitations of traditional VLC methods in the medical domain by first decomposing the placenta pathology report into set T of arbitrary size, where each t i ∈ T represents a distinct placental feature; the individual items depicted in the pathology report in Fig. 1 correspond to distinct placental features. Since the order of items in a pathology report does not impact its integrity, we obtain the set of vector representations of the features V using an expert language model f v , where v i = f v (t i ) for v i ∈ V. These resulting vectors are weighted equally to recompose the global representation (see Fig. 1), v = v∈V v, which is subsequently used to calculate the cosine similarity sim(u, v) with the image representation u. The recomposition of feature vectors from full medical text enables the use of pathology reports or EHRs of any length and ensures that all placental features are captured and equally weighted, thereby improving feature representation. Additionally, our approach reduces computational resources by precomputing text features, eliminating the need for an LLM in training. Moreover, it is adaptable to any language model."
Enhancing Automatic Placenta Analysis Through Distributional Feature Recomposition in Vision-Language Contrastive Learning,3.3,Distributional Feature Recomposition,"Since our pathology reports are decomposed and encoded as a set of feature vectors, to ensure an accurate representation, it is necessary to consider potential limitations associated with vector operations. In the context of vector summation, we anticipate similar representations when two sets differ only slightly. However, even minor changes in individual features within the set can significantly alter the overall representation. This is evident in the substantial difference between v1 and v2 in Fig. 2, despite V 1 and V 2 differing by only one vector magnitude. On the other hand, two distinct sets may result in the same representation, as shown by v1 and v3 in Fig. 2, even when the individual feature vectors have drastically different meanings. Consequently, it is crucial to develop a method that ensures sim( To address these limitations, we extend the feature recomposition in Sect. 3.2 to Distributional Feature Recomposition that estimates a stable high-dimensional vector space defined by each set of features. We suggest utilizing the distribution N (μ(V), σ(V)) of the feature vectors V, instead of point estimates (single vector sum) as a more comprehensive representation, where μ(V) and σ(V) denote the mean and standard deviation, respectively. As shown by the shaded area in Fig. 2, the proposed distributional feature recomposition is more stable and representative than the point estimate sum of vector:Implementation-wise, we employ bootstrapping to estimate the distribution of the mean vector. We assume that the vectors adhere to a normal distribution with zero covariance between dimensions. During each training iteration, we randomly generate a new bootstrapped sample set Ṽ from the estimated normal distribution N (μ(V), σ(V)). Note that a slightly different sample set is generated in each training epoch to cover the variations in the feature distribution. We can therefore represent this distribution by the vector ṽ = v∈ Ṽ v, the sum of the sampled vectors, which captures the mean feature distribution in its values and carries the feature variation through epochs. By leveraging a sufficient amount of training data and running multiple epochs, we anticipate achieving a reliable estimation. The distributional feature recomposition not only inherits the scalability and efficiency of the traditional sum of vector approach but also provides a more robust estimate of the distribution of the mean vector, resulting in improved representational power and better generalizability."
Enhancing Automatic Placenta Analysis Through Distributional Feature Recomposition in Vision-Language Contrastive Learning,3.4,Efficient Neural Networks,"Efficient models, which are smaller and faster neural networks, facilitate easy deployment across a variety of devices, making them beneficial for low-resource communities. EfficientNet [22] and MobileNetV3 [11] are two notable examples of such networks. These models achieve comparable or better performance than state-of-the-art ResNet on ImageNet. However, efficient models generally have shallower network layers and can underperform when the features are more difficult to learn, particularly in medical applications [25]. To further demonstrate the representation power of our proposed method and expedite the diagnosis process, we experimentally substitute our image backbone with two efficient models, EfficientNet-B0 and MobileNetV3-Large-1.0, both of which exhibit highly competitive performance on ImageNet when compared to the original ResNet50. This evaluation serves two purposes: First, to test the applicability of our proposed method across different models, and second, to provide a more efficient and accessible placenta analysis model."
Enhancing Automatic Placenta Analysis Through Distributional Feature Recomposition in Vision-Language Contrastive Learning,4,Experiments,
Enhancing Automatic Placenta Analysis Through Distributional Feature Recomposition in Vision-Language Contrastive Learning,4.1,Implementation,"We implemented the proposed methods and baselines using the Python/PyTorch framework and deployed the system on a computing server. For input images, we used PlacentaNet [3] for segmentation and applied random augmentations such as random rotation and color jittering. We used a pre-trained BERT1  [6] as our text encoder. EfficientNet-B0 and MobileNetV3-Large-1.0 followed official PyTorch implementations. All models and baselines were trained for 400 epochs.The encoder in the last epoch was saved and evaluated on their task-specific performance on the test set, measured by the AUC-ROC scores (area under the ROC curve). To ensure the reliability of the results, each evaluation experiment was repeated five times using different fine-tuning dataset random splits. The same testing procedure was adopted for all our methods. We masked all iPad images using the provided manual segmentation masks. For more information, please refer to the supplementary material."
Enhancing Automatic Placenta Analysis Through Distributional Feature Recomposition in Vision-Language Contrastive Learning,4.2,Results,"We compare our proposed methods (Ours) with three strong baselines: a ResNet-50 classification network, the ConVIRT [28] Medical VLC framework, and Pan et al. The mean results and confidence intervals (CIs) reported for each of the experiments on the two datasets are shown in Table 1. Some qualitative examples are in the supplementary material. Our performance-optimized method with the ResNet backbone consistently outperforms all other methods in all placental analysis tasks. These results confirm the effectiveness of our approach in reducing feature suppression and enhancing representational power. Moreover, compared to Pan et al., our method generally has lower variation across different random splits, indicating that our training method can improve the stability of learned representations. Furthermore, the qualitative examples provided in the supplementary material show that incorrect predictions are often associated with incorrect salient locations.Table 2 shows the speed improvements of our method. Since the efficiency of Pan et al. and ConVIRT is the same, we only present one of them for brevity. By removing the LLM during training, our method reduces the training time by a factor of 2.0. Moreover, the efficient version (e.g., MobileNet encoder) of our method has 2.4 to 4.1 times the throughput of the original model while still outperforming the traditional baseline approaches in most of the tasks, as shown "
Enhancing Automatic Placenta Analysis Through Distributional Feature Recomposition in Vision-Language Contrastive Learning,4.3,Ablation,"To better understand the improvements, we conduct a component-wise ablation study. We use the ConVIRT method (instead of Pan et al.) as the starting point to keep the loss function the same. We report the mean AUC-ROC across all tasks to minimize the effects of randomness. As shown in Table 3, the text feature recomposition resulted in a significant improvement in performance since it treats all placental features equally to reduce the feature suppression problem. Moreover, applying distributional feature recomposition further improved performance, indicating that using a distribution to represent a set produces a more robust representation than a simple sum. Additionally, even the efficient version of our approach outperformed the performance version that was trained using the traditional VLC method. These improvements demonstrate the effectiveness of the proposed methods across different model architectures. However, we observed that the additional improvement from the distributional method was relatively small compared to that from the recomposition method. This may be due to the fact that the feature suppression problem is more prevalent than the misleading representation problem, or that the improvements may not be linearly proportional to the effectiveness-it may be more challenging to improve a better-performing model."
Enhancing Automatic Placenta Analysis Through Distributional Feature Recomposition in Vision-Language Contrastive Learning,5,Conclusions and Future Work,"We presented a novel automatic placenta analysis framework that achieves improved performance and efficiency. Additionally, our framework can accommodate architectures of different sizes, resulting in better-performing models that are faster and smaller, thereby enabling a wider range of applications. The framework demonstrated clear performance advantages over previous work without requiring additional data, while significantly reducing the model size and computational cost. These improvements have the potential to promote the clinical deployment of automated placenta analysis, which is particularly beneficial for resource-constrained communities.Nonetheless, we acknowledge the large variance and performance drop when evaluating the iPad images. Hence, further research is required to enhance the model's robustness, and a larger external validation dataset is essential. Moreover, the performance of the image encoder is heavily reliant on the pre-trained language model, and our framework does not support online training of the language model. We aim to address these limitations in our future work."
Enhancing Automatic Placenta Analysis Through Distributional Feature Recomposition in Vision-Language Contrastive Learning,,Fig. 1 .,
Enhancing Automatic Placenta Analysis Through Distributional Feature Recomposition in Vision-Language Contrastive Learning,,Fig. 2 .,
Enhancing Automatic Placenta Analysis Through Distributional Feature Recomposition in Vision-Language Contrastive Learning,,Table 1 .,
Enhancing Automatic Placenta Analysis Through Distributional Feature Recomposition in Vision-Language Contrastive Learning,,Table 2 .,
Enhancing Automatic Placenta Analysis Through Distributional Feature Recomposition in Vision-Language Contrastive Learning,,Table 1 .,
Enhancing Automatic Placenta Analysis Through Distributional Feature Recomposition in Vision-Language Contrastive Learning,,Table 3 .,
Enhancing Automatic Placenta Analysis Through Distributional Feature Recomposition in Vision-Language Contrastive Learning,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 12.
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,1,Introduction,"Now that the hardware performance has achieved a certain level, 3D rendering of biomedical volumes is becoming very popular, above all, with the younger generation of clinicians that uses to use the forefront technologies in their everyday life. 3D representation has proven useful for faster comprehension of traumas in areas of high anatomic complexity, for surgical planning, simulation, and training [2,9,15,25], [18,28]. Also, it improves communication with patients, which understand the diagnosis much better if illustrated in 3D [23].The most popular techniques for volume data rendering are maximum-intensity projection (MIP), iso-surface rendering, and direct volume rendering (DVR). These techniques have their pros and cons, but essentially, they suffer from a lack of photo realism.Our novel Advanced Realistic Rendering Technique (AR 2 T) is based on Monte-Carlo path tracing (MCPT). Historically, MCPT is thought of as a technique suited to (iso)surfaces rendering [14,19]. Applied to biomedical volumes, this technique has received relatively little attention, probably because, if naively implemented, it does not allow interaction with the data in real-time [7]. However, due to continuous hardware improvement, the problems that can not be resolved in real-time today, will be resolved in real-time tomorrow. For this reason, we have audaciously decided to apply MCPT to the biomedical volumes in attempt to increases the realism and the level of detail in data representation.In this paper, we present a practical framework that includes different visualization techniques, including AR 2 T. Our framework allows the user to interact with the data in high quality for the deterministic algorithms (iso-surface, MIP, DVR), and in low quality for the stochastic AR 2 T. Moreover, the framework supports a mixed modality that works as follows. By default, the data is rendered by DVR. It allows to interact with the data, adjust the transfer function, and apply clip planes. However, a high-quality AR 2 T image can be generated at any moment by the user request without explicitly switching between rendering algorithms. The quality improves progressively, and the process can be stopped as soon as the desired quality is achieved. As an alternative, the improvement stops automatically, when the algorithm converged. The framework permits to compare different rendering techniques directly, i.e., using the same view/light position and transfer functions. It, therefore, promotes further research on the importance of realism in visualising biomedical volumes, providing medical experts with an immediate one-to-one visual comparison between different data representations. Related Work. Various deterministic approaches were applied in an attempt to increase the realism of volume rendering. Above all, the direct volume rendering technique has been enriched with local and global illumination, combined with ambient occlusion and shadowing [13,21,24]. However, these approaches are not able to produce photo-realistic images, being based on a very simplified and far-fetched model.One interesting technique for improved DVR, which includes realistic effects and physically based lighting, was proposed by Kroes et al. in 2012 [17]. They were the first who demonstrated that, if properly optimized, ray tracing of volumetric data can be done interactively. However, their method is based on single scattering and consequently does not produce photo-realistic images. Despite that, this approach still arouses interest [5,30]. In fact, as far as we know, since then no one has presented any different reproducible technique on photo-realistic rendering of biomedical volumetric data.Recently, the cinematic rendering (CR) 3D technique was introduced [4,10]. It is available as a part of commercial closed-source software, and the implementation details are not publicly available. Several studies compared DVR and CR images, produced by different software [6][7][8]29]. However, the one-to-one comparison is difficult, because it is not practically possible to align the data by means of visual settings and, above all, positioning."
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,2,Methods,"The entire framework is written from scratch in C++ using Qt. The whole rendering runs on GPU and is implemented in OpenGL Shading Language (GLSL). In the following subsections, we describe the techniques we used within the framework, giving the details only for AR 2 T for the sake of brevity. The quality of the images produced with the different techniques is difficult to assess with a numerical index; following [4,[6][7][8]29], we carried out a survey, based on the visual comparison of the proposed methods (see Results)."
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,2.1,Deterministic Rendering Algorithms,"First of all, we implemented the most popular rendering techniques for biomedical volumetric data: iso-surface, MIP, and DVR. They gave us the basis for the direct comparison of proposed methods. Then, we enriched our DVR model with local and global illumination, applying various approaches [11,16,20] in an attempt to improve realism and receive feedback from clinicians (see Fig. 1). It was immediately clear that despite these techniques can improve realism by introducing deep shadows, they are not suitable for the visualization of biomedical volumes because hide information in the shadowed areas without increasing anyhow the level of detail."
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,2.2,Advanced Realistic Rendering Technique,"There are two modalities of AR 2 T visualization: pure AR 2 T and mixed DVR-AR 2 T. When pure AR 2 T is active, the interactivity is achieved by the execution of just one iteration of the algorithm. When the interaction is finished (e.g., the mouse button is released), 10 iterations of AR 2 T are executed. To improve the quality, Gaussian blur filter [1] is applied during the interactions, so the overall image is understandable. On request, the iterative algorithm improves the quality until the user stops the process or the convergence is achieved (see subsection Convergence). In mixed modality, the interactions are in DVR, and the AR 2 T runs on request. When the interaction restarts, the visualization automatically switches to DVR.Our AR 2 T is inspired by MCPT applied to analytically generated surfaces and isotropic volumes [12,26]. In our model, we provide advanced camera settings (aperture, focal distance, see Fig. 2) and support an unlimited number of light sources of any shape. For practical reasons, we limit the number of ray scatters to 10 (in our experiments, we did not see any improvement in realism for a larger number of scatters). In the following subsections, we step-by-step describe the implementation details of AR 2 T, to allow the reproducibility of our results.GPU Implementation. To be independent in the choice of hardware to run our framework, we implement AR 2 T in GLSL. Unfortunately, there are two main issues to resolve for Monte-Carlo path tracing in OpenGL: 1. recursion, and 2. random number generation.Recursion. As GLSL memory model does not allow for recursive function calls, which are essential for MCPT, we simulated the recursion by exploiting multiple render targets feature of modern GPUs. This feature allows the rendering pipeline to render images to multiple render target textures at once. Indeed, the information we need after every scatter of a ray is the resulting colour, the position where the scatter occurred, and the direction in which the ray scatters. Therefore, three target textures are necessary for every rendering step. Moreover, two frame buffers are used in a ping pong blending manner (as described in [16]) to enable the reading of textures filled on the previous step. Thus, in the first step, the ray origin and direction are calculated according to the camera properties and position. In the subsequent steps, the ray origin and direction are read from the corresponding textures.On any step, three situations are possible: (1) The ray does not hit the volume or the light source. In this case, a zero-length vector is saved to direction render target -it indicates that the ray scattering finishes here, and the resulting colour components are set to zeros (for ulterior speed up, and considering that the usual background for biomedical visualization is black, we do not model the Cornel box outside the volume). ( 2) The ray hits the light source. Then, the resulting colour, accumulated until this moment, is attenuated by the light's colour, and, again, the ray scattering finishes here. (3) The volume is hit. The scattering continues, until the ray encounters any of the stopping conditions, or the scatter number limit is achieved. In this case, the resulting colour components are set to zeros."
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,,Random Number Generation.,"To provide the uniformly distributed random numbers on the fragment stage of the OpenGL pipeline, we generate a pool of 50 additional two-dimensional textures of viewport size and fill them with uniformly distributed random numbers generated with std::uniform int distribution. In each step, we randomly choose four textures from the pool to provide random numbers: two for advanced Woodcock tracking, one for scattering, and one for sampling direction. Every 100 iterations of the algorithm, we regenerate the pool of random numbers to avoid the quality improvement stuck. On Intel(R) Core(TM) i5-7600K CPU @ 3.80 GHz, the generation of the pool takes ∼ 1600 ms, for viewport size 1727 × 822. It occupies ∼270 Mb of RAM. Advanced Woodcock Tracking. For volume sampling, we implemented the advanced Woodcock tracking technique, described in [3,27]. When the maximum volume density is much larger than the typical density in the volume, the Woodcock tracking can be improved by breaking the original volume into subvolumes, each having a small density variation. For this aim, every time the transfer function changes, we construct a voxel grid that has the local maxima of the density in its nodes. Then, the voxel grid is straightforwardly applied on Woodcock tracking, giving up to 5x speed-up respect to the basic implementation. On NVIDIA GeForce GTX 1060, the voxelization process, implemented in GLSL and executed on a fragment stage, takes up to 500 ms for a voxel grid node size s = 4, and depending on the volume size. The additional memory needed for the voxel grid storage is 1/s 3 of the original volume size.Phase Functions. In AR 2 T, we use four well-known phase functions, described in [26]: Lambertian, metal, dielectric, and isotropic. Every time the ray hits a volume voxel, we choose which phase function to apply based on the density of the voxel: if the density is less than some threshold t 0 , it causes dielectric scatter; when it is higher than some threshold t 1 , it causes metal scatter; otherwise, as proposed in [26], we randomly decide if to sample toward the light sources or to pick direction according to the voxel reflection (mixture probability density function). When we decide to sample according to the hit voxel direction, we choose between surface and volumetric scattering. Following [17], we switch between Lambertian and isotropic phase functions, basing not only on the voxel density but also on the local gradient. Thus, Lambertian is chosen with the following probability:where α( -→ v ) ∈ [t 0 , t 1 ] is the voxel density (or opacity), m( -→ v ) is the normalized gradient magnitude, and s is the hybrid scattering factor. Image Generation. When the first iteration of AR 2 T is completed, the result contained in the colour texture (see Recursion for rehearse) is blit into the output rendering frame buffer to be immediately displayed. Moreover, it is saved locally to be summed with the results of the next iterations. On the next iterations, the accumulated result is saved into the local buffer, and then the medium (e.g. the sum divided by the iterations number) is blit into the output frame buffer and displayed.Convergence. As a convergence criterion, we use mean square displacement (MSD) between the iterations [22]. After each iteration, the square displacement between the current and the previous pixel colour components is calculated directly on a fragment stage. When MSD becomes less than = 5 • 10 -7 , the iterations stop, and the method is considered converged (see Fig. 5). In our experiments, the convergence was achieved within 800 iterations for all images, and it took up to 100 s."
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,3,Results,"In our experiments, we have used spiral CT (Computed Tomography), MRI (Magnetic Resonance Imaging), and CBCT (Cone-Beam Computed Tomography) publicly available data sets. The CBCT data sets were acquired by SeeFactorCT3 TM (human) and Vimago TM GT30 (vet) Multimodal Medical Imaging Platforms in our layout and are available on https://kaggle.com/ datasets/imaginar2t/cbctdata. To validate the superiority of the AR 2 T over the other methods implemented in our platform, we ask a group of clinicians to participate into the survey. 22 participants (7 orthopedic surgeons, 1 trauma surgeon, 1 neurosurgeon, 7 interventional radiologists, 6 veterinaries) evaluated the data sets on Fig. 6, visualized  using MIP, iso-surface, DVR with local illumination, DVR with local and global illumination (linear lighting), and AR 2 T, voting the best overall image, the most realistic one, the more diagnostic (if any), and the more valuable in their practice.According to Table 1, the AR 2 T provides the best overall, the most realistic, diagnostic, and valuable images. Participants commented that the images produced with AR 2 T ""provide better resolution, amazing clarity with less artifacts and noise, better sharpness and contrast, are the closest in colour to real tissue and the most similar to a dissected body deprived of blood, help to understand the anatomy and the pathology of the district, have excellent qualities in general surgery for the definition of the splanchnic organs, for diagnosis and preoperative study"". Meanwhile the others are ""either glossy, or too colorized or not as sharp, and seem artificial"". Some participants stated that DVR images are the sharpest and seem to be more detailed (""peritoneal meso is better detected, subxiphoid is more visible""), but also it was mentioned that ""too much sharpening causes misleading images creating artificial findings"". Some participants noted the diagnostic usefulness of MIP for vascular issues. Participants who stated that none of the images were diagnostic or useful admitted that they did not deal with the presented anatomical structures in their practice or had never used 3D rendering and could not assess its practical application. "
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,4,Conclusions,"The main result of our research is the novel advanced realistic rendering technique-AR 2 T (see Fig. 2, 3, 4), implemented within a practical framework, that allows to compare different rendering techniques directly (Fig. 6).Despite our model supports any number of light sources, all images (except Fig. 1), presented in this paper, were generated with a single spherical light source, placed right in front of the volume. We plan to dedicate extra time to find the best light configuration from the clinical point of view. Moreover, our future research will be focused on the ulterior improvement of AR 2 T speed and quality, and the comparison metrics.At the moment of writing this paper, we are evaluating free access to our framework's executable to share our results, facilitate the comparison with other approaches, and stimulate further research on the usefulness of photo-realistic 3D images in medicine."
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,,Fig. 1 .,
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,,Fig. 2 .,
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,,Fig. 3 .,
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,,Fig. 4 .,
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,,Fig. 5 .,
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,,Fig. 6 .,
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,,Table 1 .,
Interpretable Deep Biomarker for Serial Monitoring of Carotid Atherosclerosis Based on Three-Dimensional Ultrasound Imaging,1,Introduction,"Although cardiovascular events are highly prevalent worldwide, it was estimated 75-80% of cardiovascular events in high-risk patients could be prevented through lifestyle changes and medical/dietary interventions [22]. The opportunity to prevent cardiovascular events calls for the development of sensitive and cost-effective tools to identify high-risk patients and monitor serial changes in response to therapies. As carotid atherosclerosis is a major source of ischemic stroke and a major indicator of systematic atherosclerosis [9], the carotid artery has long served as a major imaging target for assessment of atherosclerotic diseases.Carotid intima media thickness (IMT) is an early imaging biomarker measured from two-dimensional ultrasound (2DUS) images. The use of IMT in serial monitoring is limited by the small annual change (∼0.015 mm) [4], which does not allow treatment effects to be measured in a clinically affordable timeframe. As plaques grow 2.4 times faster along the arteries than they thickens [3] and change circumferentially as well, volume measurements, such as total plaque volume (TPV) and vessel wall volume (VWV), afforded by 3DUS imaging techniques are more sensitive to treatment effects [1,12]. Biomarkers derived from plaque textural features extracted from 3DUS were also shown to be sensitive to medical [2] and dietary treatments [5,14]. However, few studies consider both volume and textural features, and the handcrafted textural features extracted in previous studies are independent of the subsequent biomarker generation. To address these issues, we propose an end-to-end Siamese change biomarker generation network (SCBG-Net) to extract features from the baseline and follow-up images for generating a biomarker, AutoVT, quantifying the degree of change in volume and texture automatically. Although deep networks have been proposed for carotid plaque composition characterization [13], plaque echogenicity classification [16,21], and plaque recognition [15,17], SCBG-Net is the first deep network developed for serial monitoring of carotid atherosclerosis.A convolutional neural network (CNN) is typically represented as a blackbox function that maps images to an output. However, a biomarker should be interpretable for it to be trusted by clinicians. One approach to promote the interpretability of the biomarker is to allow the visualization of regions that have a prominent effect on the biomarker. Class activation map (CAM) [27] and its variant [19,24] highlight regions having a strong contribution to classification results. Interpretability is not only desired in classification networks but also in networks focusing on quantifying the similarity of images, such as person re-identification [20,25]. The ranking activation map (RAM) [25] and its variant, such as gradient-weighted ranking activation map (CG-RAM) [20], highlight regions contributing to the similarity between a reference image and other images. For our application, there is a need to develop a technique to generate activation maps localizing regions with a prominent effect on the novel biomarker. Another contribution of this paper is the development of such an approach to generate change biomarker activation maps (CBAM)."
Interpretable Deep Biomarker for Serial Monitoring of Carotid Atherosclerosis Based on Three-Dimensional Ultrasound Imaging,2,Materials and Methods,
Interpretable Deep Biomarker for Serial Monitoring of Carotid Atherosclerosis Based on Three-Dimensional Ultrasound Imaging,2.1,3DUS Imaging and Preprocessing,"In this work, we assessed the sensitivity of the proposed biomarker in evaluating the effect of pomegranate juice and tablets. Pomegranate is anti-oxidative, and previous studies have established that plaque texture features [5], the weighted average change of vessel-wall-plus-plaque thickness [26] and local vessel wall and plaque volume change [6] are able to detect the effect of pomegranate in the same cohort investigated in this study. Subjects were recruited by Stroke Prevention & Atherosclerosis Research Centre at Robarts Research Institute (London, Ontario, Canada) for a clinical trial (ISRCTN30768139). A total of 120 subjects involved in this study were randomized into two groups. 66 subjects received pomegranate extract and 54 subjects were given a placebo once daily for a year. There is no significant difference in the baseline characteristics between the two groups [26]. 3DUS images were acquired for participants at the baseline and a follow-up session, ranging from 283 to 428 days after the baseline scan. The reason for quantifying changes based on only two time points (i.e., baseline and followup) is that the rate of change of carotid plaque has been established as linear between the age of 50 to 75 in two studies involving over 6000 patients [11,23]. The 3DUS images were obtained by translating an ultrasound transducer (L12-5, Philips, Bothell, WA, USA) mounted on a mechanical assembly at a uniform speed of 3 mm/s along the neck for about 4 cm. Ultrasound frames acquired using an ultrasound machine (ATL HDI 5000, Philips) were digitized at the rate of 30 Hz and reconstructed into a 3D image. The input image to SCBG-Net was obtained by masking the ultrasound image with the manually segmented boundaries (Supplementary Fig. 1). Each 3DUS image was resliced into a stack of 2D axial images with a 1mm interslice distance (ISD) as described in [7]."
Interpretable Deep Biomarker for Serial Monitoring of Carotid Atherosclerosis Based on Three-Dimensional Ultrasound Imaging,2.2,Siamese Change Biomarker Generation Network (SCBG-Net),"Network Architecture. Figure 1 shows a schematic of the SCBG-Net. The baseline and follow-up image stacks consist of 48 axial images each, 24 from each of the left and right arteries. We denote the baseline (bl) and follow-up (fu) image stacks by X bl = {x bl i } 47 i=0 and X fu = {x fu i } 47 i=0 , respectively, where x bl i and x fu i are axial slices with size 300 × 180. The baseline and follow-up images are processed by a Siamese architecture. The volume-texture feature extraction module utilizes an identical ResNet50 to extract features, resulting in 2048-dimensional vectors for each axial image. The vectors are then reduced to 64 dimensions using a fully connected (FC) layer with a rectified linear unit (ReLU) activation function, and these outputs are denoted as z bl i = f (x bl i ) and z fu i = f (x fu i ) for the two respective streams. The neighborhood slice smoothing module was designed to reduce the effect of potential image misalignment. Although the baseline and follow-up 3DUS images were aligned according to the bifurcation position and the manually identified common carotid artery (CCA) axis as previously described [7], the internal carotid artery (ICA) of the two images may still be misaligned due to different bifurcation angles and non-linear transformations resulting from different head orientations [18]. To reduce the effect of potential image misalignment, the features of three neighboring slices in the baseline and follow-up images are averaged and denoted by zblj=3i z fu j , for i = 0, 1, . . . , 15. The slice-wise cosine ""dissimilarity"" for a baseline-follow-up slice pair was defined by d c (•, •) = 1-cos(•, •). To represent disease progression and regression, the sign of the slice-wise vessel wall volume change ΔV ol i from baseline to follow-up was used to determine the sign of the slice-wise score. ΔV ol i of each smoothed slice pair was computed by averaging vessel wall volume change (i.e., area change × 1mm ISD) for a group of three neighbouring slices involved in the smoothing operation. The slice-wise score was obtained by:where sgn represents the signed function. The use of ReLU in FC layers results in non-negative z i , thereby limiting d c ( zi bl , zfu i ) and s(z bl i , zfu i ) to the range of [0, 1] and [-1, 1], respectively. Defined as such, s i integrates vessel-wall-plusplaque volume change with textural features extracted by the network. Finally, the AutoVT biomarker was obtained by averaging 16 slice-wise scores (i.e., AutoV T = 1 16 15 i=0 s i ).Loss Functions. We developed a treatment label contrastive loss (TCL) to promote discrimination between the pomegranate and placebo groups and a plaque-focus (PF) constraint that considers slice-based volume change.(i) Treatment Label Contrastive Loss. The contrastive loss [8] maps similar pairs to nearby points and dissimilar pairs to distant points. In our biomarker learning problem, instead of separating similar and dissimilar pairs, we aim to discriminate baseline-follow-up image pairs of the pomegranate and placebo subjects. As changes occur in all patients, the baseline-follow-up image pairs are in general dissimilar for both groups. However, pomegranate subjects tend to experience a smaller plaque progression or even regression, whereas the placebo subjects have a larger progression [26]. As such, our focus is more on differentiating the two groups based on the signed difference between the baseline and follow-up images. We designed a treatment label contrastive loss (TCL) specifically for our biomarker learning problem:where y is the group label of the input subject (pomegranate = 1, placebo = 0). For pomegranate subjects, instead of assigning a penalty based on the squared distance as in [8] (i.e., AutoV T 2 in our context), in which the penalty applies to both positive or negative AutoVT, we penalize only positive AutoVT since it is expected that some pomegranate subjects would have a larger regression, which would be represented by a negative AutoVT. For placebo subjects, the penalty is applied only if AutoVT is smaller than m.(ii) Plaque-focus Constraint. We observe that slice pairs with high volume change are typically associated with a large plaque change (Supplementary Fig. 2). The overall loss L is a weighted combination of L tcl and L pf (i.e., L = L tcl +wL pf , where w is the weight)."
Interpretable Deep Biomarker for Serial Monitoring of Carotid Atherosclerosis Based on Three-Dimensional Ultrasound Imaging,2.3,Change Biomarker Activation Map (CBAM),"Figure 2 shows a schematic of the proposed CBAM developed for visualizing important regions contributing to the AutoVT scoring. Like previous CAM methods, CBAM generates activation maps by linearly weighting feature maps at different levels of a network. However, the weights associated with the attention maps in CBAM are novel and tailored for the proposed SCBG-Net. The importance of a feature map is determined by how much it affects the absolute value of slice-wise score s (Eq. 1). The reason for focusing on |s| is that we would highlight regions that contribute to both positive and negative changes. We denote A p,k L,i as the kth channel of feature maps from the inner convolution layer L of an image slice x p i , where p ∈ {bl, f u}. The importance of A p,k L,i towards the slice-wise score is defined in a channel-wise pair-associated manner by:with M p,k L,i = Norm(Up(A p,k L,i )), • representing the Hadamard product and (p, q) ∈ {(bl, f u), (fu, bl)}. Up(•) upsamples A p,k L,i into the size of x p i , and Norm(•) is a min-max normalization function mapping each element in the matrix into [0, 1].A p,k L,i is first upsampled and normalized to M p,k L,i , which serves as an activation map to highlight regions in the input image. The importance of A p,k L,i to the slicewise score s is quantified by the cosine dissimilarity between the feature vectors generated by SCBG-Net for the highlighted input image and the corresponding image slice in the baseline-follow-up image pair. If the input image is a baseline image, the corresponding slice would be from the follow-up image, and vice versa.For each slice x p i , the activation map from the convolutional layer L was generated as"
Interpretable Deep Biomarker for Serial Monitoring of Carotid Atherosclerosis Based on Three-Dimensional Ultrasound Imaging,3,Experiments and Results,"Statistical Evaluation. The discriminative power of biomarkers was evaluated by p-values from two-sample t-tests for normally distributed measurements or Mann-Whitney U tests for non-normally distributed measurements. P-values quantify the ability of each biomarker to discriminate the change exhibited in the pomegranate and placebo groups.Experimental Settings. Our model was developed using Keras on a computer with an Intel Core i7-6850K CPU and an NVIDIA RTX 1080Ti GPU. The ResNet50 was initialized by the ImageNet pretrained weights. The SGD optimizer was applied with an initial learning rate of 3 × 10 -3 . An exponential decay learning rate scheduler was utilized to reduce the learning rate by 0.8 every 10 epochs. We set the number of slices with top/last |ΔV ol i | in the definition of the PF constraint as K l = K s = 3. All models were evaluated by three-fold cross-validation with 80 labeled subjects and 40 test subjects. Labeled subjects are further partitioned into training and validation sets with 60 and 20 subjects, respectively. For the proposed SCBG-Net, the margin m and loss function weight w were tuned using the validation set. In all three trials, the optimized m and w were 0.8 and 0.15, respectively."
Interpretable Deep Biomarker for Serial Monitoring of Carotid Atherosclerosis Based on Three-Dimensional Ultrasound Imaging,,Comparison with Traditional Biomarkers. Table 1 shows p-values for,"AutoVT and traditional biomarkers. The proposed biomarker based on the overall loss function L, AutoVT(L), was the most sensitive to the effect of pomegranate with the lowest p-value. This biomarker has learned the volumetric information from the input images, as demonstrated by the correlation coefficient of 0.84 between AutoVT(L) and ΔV W V . AutoVT(L) has also learned the texture information, as demonstrated in Fig. 3. While the slice pairs in Fig. 3 zp i where p ∈ {bl, f u} and then generates a biomarker by bio aver = sgn(ΔV W V )d c (z bl aver , z fu aver ), where V W V in the baseline/follow-up session was computed by summing the vessel wall areas at each axial image of the left and right arteries and multipled by the 1mm ISD. ΔV W V is the difference between VWV obtained from the baseline and follow-up images. In contrast, Atten-Net computes a weighted is the attention weight for Slice i. The biomarker generated by Atten-Net is bio atten = sgn(ΔV W V )d c (z bl atten , z fu atten ). Aver-Net and Atten-Net do not involve slice-by-slice comparison, whereas slice-by-slice comparison was involved in two components of SCBG-Net: (i) the slice-wise score s i (Eq. 1) and (ii) the PF constraint (Eq. 3). In this section, we focus on investigating the effect of Component (i) and that of Component (ii) will be studied in the next section focusing on loss functions. For this reason, the three models compared in this section were driven only by L tcl (Eq. 2) for a fair comparison. Table 1 shows that SCBG-Net is the most sensitive to treatment effects among the three models.Comparison with Different Losses. We compared our proposed loss with another two losses, including cross-entropy loss and bi-direction contrastive loss. Cross-entropy loss is expressed as, where σ(•) is a sigmoid function. The bi-direction contrastive loss is a symmetric version of L tcl , expressed as L bd = y max(m + AutoV T, 0) 2 + (1y) max(m -AutoV T, 0) 2 . The margin m in L bd was tuned in the same way as the proposed L, with m = 0.4 being the optimized parameter in all three cross-validation trials. Table 1 shows p-values for different losses. Our proposed loss L tcl has a higher sensitivity than L ce and L bd , with further improvement attained by the incorporation of L pf . Pomegranate, as a dietary supplement, confers a weaker beneficial effect than intensive medical treatment [5,26]. L tcl was designed to better model the weak benefit by not forcing the AutoVTs of pomegranate patients to get too negative; the AutoVTs of pomegranate patients would not be penalized as long as it is smaller than 0. In contrast, L ce and L bd promote more negative AutoVTs for pomegranate patients. L tcl was designed to account for the weak beneficial effect of pomegranate, which may not lead to significant plaque regression in pomegranate patients compared to high-dose atorvastatin. Moreover, L pf improves the discriminative power of AutoVT by using the ranking of |ΔV V ol i | among different axial images of the same patient. whereas CG-RAM is less successful in highlighting regions with plaque changes. A possible reason for this observation is that CG-RAM is driven by gradient and may be adversely affected by gradient saturation issues, whereas CBAM is gradient-free and not affected by artifacts associated with gradient saturation."
Interpretable Deep Biomarker for Serial Monitoring of Carotid Atherosclerosis Based on Three-Dimensional Ultrasound Imaging,,Comparison with Other Activation Maps,
Interpretable Deep Biomarker for Serial Monitoring of Carotid Atherosclerosis Based on Three-Dimensional Ultrasound Imaging,4,Conclusion,"We, for the first time, developed a deep biomarker to quantify the serial change of carotid atherosclerosis by integrating the vessel wall and plaque volume change and the change of textural features extracted by a CNN. We showed that the proposed biomarker, AutoVT, is more sensitive to treatment effect than vessel wall and plaque volume measurements. SCBG-Net involves slice-based comparison of textural features and vessel wall volume (Eq. 1) and we showed that this architecture results in a biomarker that is more sensitive than Aver-Net and Atten-Net that quantify global change for the left and right arteries. This result is expected as atherosclerosis is a focal disease with plaques predominantly occurring at the bifurcation. For the same reason, PF constraint that involves local slice-based assessment further improves the sensitivity of AutoVT in detecting treatment effects. We developed a technique to generate activation maps highlighting regions with a strong influence on AutoVT. The improvement in the interpretability of AutoVT afforded by the activation maps will help promote clinical acceptance of AutoVT ."
Interpretable Deep Biomarker for Serial Monitoring of Carotid Atherosclerosis Based on Three-Dimensional Ultrasound Imaging,,Fig. 1 .,
Interpretable Deep Biomarker for Serial Monitoring of Carotid Atherosclerosis Based on Three-Dimensional Ultrasound Imaging,,Fig. 2 .,
Interpretable Deep Biomarker for Serial Monitoring of Carotid Atherosclerosis Based on Three-Dimensional Ultrasound Imaging,,Fig. 3 .,
Interpretable Deep Biomarker for Serial Monitoring of Carotid Atherosclerosis Based on Three-Dimensional Ultrasound Imaging,,,
Interpretable Deep Biomarker for Serial Monitoring of Carotid Atherosclerosis Based on Three-Dimensional Ultrasound Imaging,,Fig. 4 .,
Interpretable Deep Biomarker for Serial Monitoring of Carotid Atherosclerosis Based on Three-Dimensional Ultrasound Imaging,,,
Interpretable Deep Biomarker for Serial Monitoring of Carotid Atherosclerosis Based on Three-Dimensional Ultrasound Imaging,,Table 1 .,
Interpretable Deep Biomarker for Serial Monitoring of Carotid Atherosclerosis Based on Three-Dimensional Ultrasound Imaging,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 29.
Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,1,Introduction,"Cancers are a group of heterogeneous diseases reflecting deep interactions between pathological and genomics variants in tumor tissue environments [24]. Different cancer genotypes are translated into pathological phenotypes that could be assessed by pathologists [24]. High-resolution pathological images have proven their unique benefits for improving prognostic biomarkers prediction via exploring the tissue microenvironmental features [1,10,12,13,18,25]. Meanwhile, genomics data (e.g., mRNA-sequence) display a high relevance to regulate cancer progression [3,29]. For instance, genome-wide molecular portraits are crucial for cancer prognostic stratification and targeted therapy [16]. Despite their importance, seldom efforts jointly exploit the multimodal value between cancer image morphology and molecular biomarkers. In a broader context, assessing cancer prognosis is essentially a multimodal task in association with pathological and genomics findings. Therefore, synergizing multimodal data could deepen a crossscale understanding towards improved patient prognostication.The major goal of multimodal data learning is to extract complementary contextual information across modalities [4]. Supervised studies [5][6][7] have allowed multimodal data fusion among image and non-image biomarkers. For instance, the Kronecker product is able to capture the interactions between WSIs and genomic features for survival outcome prediction [5,7]. Alternatively, the coattention transformer [6] could capture the genotype-phenotype interactions for prognostic prediction. Yet these supervised approaches are limited by feature generalizability and have a high dependency on data labeling. To alleviate label requirement, unsupervised learning evaluates the intrinsic similarity among multimodal representations for data fusion. For example, integrating image, genomics, and clinical information can be achieved via a predefined unsupervised similarity evaluation [4]. To broaden the data utility, the study [28] leverages the pathology and genomic knowledge from the teacher model to guide the pathology-only student model for glioma grading. From these analyses, it is increasingly recognized that the lack of flexibility on model finetuning limits the data utility of multimodal learning. Meanwhile, the size of multimodal medical datasets is not as large as natural vision-language datasets, which necessitates the need for data-efficient analytics to address the training difficulty.To tackle above challenges, we propose a pathology-and-genomics multimodal framework (i.e., PathOmics) for survival prediction (Fig. 1). We summarized our contributions as follows. (1) Unsupervised multimodal data fusion. Our unsupervised pretraining exploits the intrinsic interaction between morphological and molecular biomarkers (Fig. 1a). To overcome the gap of modality heterogeneity between images and genomics, we project the multimodal embeddings into the same latent space by evaluating the similarity among them. Particularly, the pretrained model offers a unique means by using similarity-guided modality fusion for extracting cross-modal patterns. (2) Flexible modality finetuning. A key contribution of our multimodal framework is that it combines benefits from both unsupervised pretraining and supervised finetuning data fusion (Fig. 1b). As a result, the task-specific finetuning broadens the dataset usage (Fig 1b andc), which is not limited by data modality (e.g., both singleand multi-modal data). (3) Data efficiency with limited data size. Our approach could achieve comparable performance even with fewer finetuned data (e.g., only use 50% of the finetuned data) when compared with using the entire finetuning dataset."
Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,2,Methodology,"Overview. Figure 1 illustrates our multimodal transformer framework. Our method includes an unsupervised multimodal data fusion pretraining and a supervised flexible-modal finetuning. From Fig. 1a, in the pretraining, our unsupervised data fusion aims to capture the interaction pattern of image and genomics features. Overall, we formulate the objective of multimodal feature learning by converting image patches and tabular genomics data into groupwise embeddings, and then extracting multimodal patient-wise embeddings. More specifically, we construct group-wise representations for both image and genomics modalities. For image feature representation, we randomly divide image patches into groups; Meanwhile, for each type of genomics data, we construct groups of genes depending on their clinical relevance [22]. Next, as seen in Fig. 1b andc, our approach enables three types of finetuning modal modes (i.e., multimodal, image-only, and genomics-only) towards prognostic prediction, expanding the downstream data utility from the pretrained model. Group-Wise Image and Genomics Embedding. We define the group-wise genomics representation by referring to N = 8 major functional groups obtained from [22]. Each group contains a list of well-defined molecular features related to cancer biology, including transcription factors, tumor suppression, cytokines and growth factors, cell differentiation markers, homeodomain proteins, translocated cancer genes, and protein kinases. The group-wise genomics representation is defined as G n ∈ R 1×dg , where n ∈ N , d g is the attribute dimension in each group which could be various. To better extract high-dimensional group-wise genomics representation, we use a Self-Normalizing Network (SNN) together with scaled exponential linear units (SeLU) and Alpha Dropout for feature extraction to generate the group-wise embedding G n ∈ R 1×256 for each group.For group-wise WSIs representation, we first cropped all tissue-region image tiles from the entire WSI and extracted CNN-based (e.g., ResNet50) d idimensional features for each image tile k as h k ∈ R 1×di , where d i = 1, 024, k ∈ K and K is the number of image patches. We construct the group-wise WSIs representation by randomly splitting image tile features into N groups (i.e., the same number as genomics categories). Therefore, group-wise image representation could be defined as I n ∈ R kn×1024 , where n ∈ N and k n represents tile k in group n. Then we apply an attention-based refiner (ABR) [17], which is able to weight the feature embeddings in the group, together with a dimension deduction (e.g., fully-connected layers) to achieve the group-wise embedding. The ABR and the group-wise embedding I n ∈ R 1×256 are defined as:where w,V1 and V2 are the learnable parameters.Patient-Wise Multimodal Feature Embedding. To aggregate patient-wise multimodal feature embedding from the group-wise representations, as shown in Fig. 1a, we propose a pathology-and-genomics multimodal model containing two model streams, including a pathological image and a genomics data stream.In each stream, we use the same architecture with different weights, which is updated separately in each modality stream. In the pathological image stream, the patient-wise image representation is aggregated by N group representations as, where p ∈ P and P is the number of patients. Similarly, the patient-wise genomics representation is aggregated as G p ∈ R N ×256 . After generating patient-wise representation, we utilize two transformer layers [27] to extract feature embeddings for each modality as follows:where MSA denotes Multi-head Self-attention [27] (see Appendix 1), l denotes the layer index of the transformer, and H p could either be I p or G p . Then, we construct global attention poolings [17] as Eq. 1 to adaptively compute a weighted sum of each modality feature embeddings to finally construct patientwise embedding as I p embedding ∈ R 1×256 and G p embedding ∈ R 1×256 in each modality.Multimodal Fusion in Pretraining and Finetuning. Due to the domain gap between image and molecular feature heterogeneity, a proper design of multimodal fusion is crucial to advance integrative analysis. In the pretraining stage, we develop an unsupervised data fusion strategy by decreasing the mean square error (MSE) loss to map images and genomics embeddings into the same space. Ideally, the image and genomics embeddings belonging to the same patient should have a higher relevance between each other. MSE measures the average squared difference between multimodal embeddings. In this way, the pretrained model is trained to map the paired image and genomics embeddings to be closer in the latent space, leading to strengthen the interaction between different modalities.In the single modality finetuning, even if we use image-only data, the model is able to produce genomic-related image feature embedding due to the multimodal knowledge aggregation already obtained from the model pretraining. As a result, our cross-modal information aggregation relaxes the modality requirement in the finetuning stage. As shown in Fig. 1b, for multimodal finetuning, we deploy a concatenation layer to obtain the fused multimodal feature representation and implement a risk classifier (FC layer) to achieve the final survival stratification (see Appendix 2). As for single-modality finetuning mode in Fig. 1c, we simply feed I p embedding or G p embedding into risk classifier for the final prognosis prediction. During the finetuning, we update the model parameters using a log-likelihood loss for the discrete-time survival model training [6](see Appendix 2)."
Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,3,Experiments and Results,"Datasets. All image and genomics data are publicly available. We collected WSIs from The Cancer Genome Atlas Colon Adenocarcinoma (TCGA-COAD) dataset (CC-BY-3.0) [8,21] and Rectum Adenocarcinoma (TCGA-READ) dataset (CC-BY-3.0) [8,20], which contain 440 and 153 patients. We cropped each WSI into 512 × 512 non-overlapped patches. We also collected the corresponding tabular genomics data (e.g., mRNA sequence, copy number alteration, and methylation) with overall survival (OS) times and censorship statuses from Cbioportal [2,14]. We removed the samples without the corresponding genomics data or ground truth of survival outcomes. Finally, we included 426 patients of TCGA-COAD and 145 patients of TCGA-READ.Experimental Settings and Implementations. We implement two types of settings that involve internal and external datasets for model pretraining and finetuning. As shown in Fig 2a, we pretrain and finetune the model on the same dataset (i.e., internal setting). We split TCGA-COAD into training (80%) and holdout testing set (20%). Then, we implement four-fold cross-validation on the training set for pretraining, finetuning, and hyperparameter-tuning. The test set is only used for evaluating the best finetuned models from each cross-validation split. For the external setting, we implement pretraining and finetuning on the different datasets, as shown in Fig 2b ; we use TCGA-COAD for pretraining; Then, we only use TCGA-READ for finetuning and final evaluation. We implement a five-fold cross-validation for pretraining, and the best pretrained models are used for finetuning. We split TCGA-READ into finetuning (60%), validation (20%), and evaluation set (20%). For all experiments, we calculate the average performance on the evaluation set across the best models.The number of epochs for pretraining and finetuning are 25, the batch size is 1, the optimizer is Adam [19], and the learning rate is 1e-4 for pretraining and 5e-5 for finetuning. We used one 32GB Tesla V100 SXM2 GPU and Pytorch. The concordance index (C-index) is used to measure the survival prediction performance. We followed the previous studies [5][6][7] to partition the overall survival (OS) months into four non-overlapping intervals by using the quartiles of event times of uncensored patients for discretized-survival C-index calculation (see Appendix 2). For each experiment, we reported the average C-index among three-times repeated experiments. Conceptionally, our method shares a similar idea to multiple instance learning (MIL) [9,23]. Therefore, we include two types of baseline models, including the MIL-based models (DeepSet [30], AB-MIL [17], and TransMIL [26]) and MIL multimodal-based models (MCAT [6], PORPOISE [7]). We follow the same data split and processing, as well as the identical training hyperparameters and supervised fusion as above. Notably, there is no need for supervised finetuning for the baselines when using TCGA-COAD (Table 1), because the supervised pretraining is already applied to the training set."
Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,,Results.,"In Table 1, our approach shows improved survival prediction performance on both TCGA-COAD and TCGA-READ datasets. Compared with supervised baselines, our unsupervised data fusion is able to extract the phenotype-genotype interaction features, leading to achieving a flexible finetuning for different data settings. With the multimodal pretraining and finetuning, our method outperforms state-of-the-art models by about 2% on TCGA-COAD and 4% TCGA-READ. We recognize that the combination of image and mRNA sequencing data leads to reflecting distinguishing survival outcomes. Remarkably, our model achieved positive results even using a single-modal finetuning when compared with baselines (more results in Appendix 3.1). In the meantime, on the TCGA-READ, our single-modality finetuned model achieves a better performance than multimodal finetuned baseline models (e.g., with model pretraining via image and methylation data, we have only used the image data for finetuning and achieved a C-index of 74.85%, which is about 4% higher than the best baseline models). We show that with a single-modal finetuning strategy, the model could generate meaningful embedding to combine image-and genomicrelated patterns. In addition, our model reflects its efficiency on the limited finetuning data (e.g., 75 patients are used for finetuning on TCGA-READ, which are only 22% of TCGA-COAD finetuning data). In Table 1, our method could yield better performance compared with baselines on the small dataset across the combination of images and multiple types of genomics data. approach broadens the scope of dataset inclusion, particularly for model finetuning and evaluation, while enhancing model efficiency on analyzing multimodal clinical data in real-world settings. In addition, the use of synthetic data and developing a foundation model training will be helpful to improve the robustness of multimodal data fusion [11,15]."
Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,,Fig. 1 .,
Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,,Fig. 2 .,
Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,,Table 1 .,
Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,,Supplementary Information,"The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 60. Ablation Analysis. We verify the model efficiency by using fewer amounts of finetuning data in finetuning. For TCGA-COAD dataset, we include 50%, 25%, and 10% of the finetuning data. For the TCGA-READ dataset, as the number of uncensored patients is limited, we use 75%, 50%, and 25% of the finetuning data to allow at least one uncensored patient to be included for finetuning. As shown in Fig. 3a, by using 50% of TCGA-COAD finetuning data, our approach achieves the C-index of 64.80%, which is higher than the average performance of baselines in several modalities. Similarly, in Fig. 3b, our model retains a good performance by using 50% or 75% of TCGA-READ finetuning data compared with the average of C-index across baselines (e.g., 72.32% versus 64.23%). For evaluating the effect of cross-modality information extraction in the pretraining, we kept supervised model training (i.e., the finetuning stage) while removing the unsupervised pretraining. The performance is lower 2%-10% than ours on multi-and single-modality data. For evaluating the genomics data usage, we designed two settings: (1) combining all types of genomics data and categorizing them by groups; (2) removing category information while keeping using different types of genomics data separately. Our approach outperforms the above ablation studies by 3%-7% on TCGA-READ and performs similarly on TCGA-COAD. In addition, we replaced our unsupervised loss with cosine similarity loss; our approach outperforms the setting of using cosine similarity loss by 3%-6%."
Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,4,Conclusion,"Developing data-efficient multimodal learning is crucial to advance the survival assessment of cancer patients in a variety of clinical data scenarios. We demonstrated that the proposed PathOmics framework is useful for improving the survival prediction of colon and rectum cancer patients. Importantly, our approach opens up perspectives for exploring the key insights of intrinsic genotypephenotype interactions in complex cancer data across modalities. Our finetuning"
Improving Pathology Localization: Multi-series Joint Attention Takes the Lead,1,Introduction,"MRI is an essential diagnostic and investigative tool in clinical and research settings. Expert radiologists rely on multiple MRI series of varying acquisition parameters and orientations to capture different aspects of the underlying anatomy and diagnose any defect or pathology that may be present. For a knee study, it is typical to acquire MRI series with coronal, sagittal, and axial orientations using proton density (PD), proton density fat suppressed (PDFS) or T2-weighted fat suppressed series (T2FS) for each study. When series are analyzed in concert, a radiologist can make a more effective diagnosis and mark down the location of any corresponding defect in each series. The defect location is typically represented as a single point [3] regardless of the defect size as a balance of effectiveness and efficiency.In recent years, convolutional neural networks (CNNs) have achieved promising results in pathology localization. Many approaches rely on generating a multi-variate Gaussian heatmap, where the peak of the distribution represents the pathology localization. Hourglass [11,16], an encoder-decoder style architecture [9], is a mainstream model to generate a Gaussian heatmap. It uses a series of convolutional and pooling layers to extract features from the input image followed by upsampling and convolutional layers to generate the Gaussian heatmap. However, Hourglass-based methods can be overly resource-intensive when applied to 3D volumes [11]. To overcome this, regression-based models are becoming popular for detecting defects wherein a fully-connected layer is used on top of the encoder blocks to directly predict the location. These methods also alleviate the need for heatmap generation and post-processing methods to compute the location. Recently, transformer-based models have emerged as a promising trend in localization [4,6,14], and their performance has exceeded that of encoder-decoder based methods on single MRI volumes [4,7]. With the availability of multiple series, we propose a framework that imitates a clinical workflow, by simultaneously analyzing multiple series and paying attention to the location that corresponds to a pathology across multiple series.To do this, we design a framework that utilizes self-attention across multiple series and we further add a mask to allow the model to focus on relevant areas, which we term as Masked Self-Attention (MSA). To predict the pathology location, we use a transformer decoder with an encoder-based initialization of the reference points. This approach provides a strong initial guess of the pathology location, improving the accuracy of the model's predictions. Overall, our framework leverages the strengths of both self-attention and encoder-decoder architectures to enhance the performance of pathology localization.Specifically, our contributions are:-We introduce a framework that enables the simultaneous use of multiple series from an MRI study, allowing for the sharing of pathology information across different series through Masked Self-Attention. -We design a transformer-based decoder model to predict consistent locations across series in an MRI study, which reduces the network's parameters compared to standard heatmap-based approaches. -Through extensive experiments on three knee pathologies, we demonstrate the effectiveness and efficiency of our framework, showing the benefits of Masked Self-Attention and a Pathology localization decoder to accurately predict pathology locations.Overall, our framework represents a promising step towards more consistent and accurate localization, which could have important applications in medical diagnosis and treatment.Fig. 1. Overview. More than 1 series are passed to encoders that have shared parameters. ""Stem"", ""layer1"", ""layer2"", ""layer3"" and ""layer4"" follows the ResNet [12] architecture convention. We perform Masked Self-Attention starting from layer 2. The Pathology localization decoder accepts feature maps from layer 2 to layer 4 and uses a query for each series to perform deformable cross attention to generate pathological landmarks."
Improving Pathology Localization: Multi-series Joint Attention Takes the Lead,2,Methods,
Improving Pathology Localization: Multi-series Joint Attention Takes the Lead,2.1,Our Architecture,"We aim to produce a reliable pathology location for each series in a given study if a location is available for that series. More formally, we assume that we are given a dataset, D = {X i , Y i } N i=1 , with N denoting the total number of studies in the dataset, X i and Y i denoting the set of series and corresponding location for each series. Due to different acquisition protocols, the number of series in each X i can vary. Similarly, each Y i can have a different number of location. Our goal is to predict a pathology location for each series and its corresponding confidence score. Figure 1 outlines our framework which can accept multiple series to generate a more accurate locations for each series."
Improving Pathology Localization: Multi-series Joint Attention Takes the Lead,2.2,Backbone,"Our framework contains a backbone, which is responsible for generating multilevel feature maps. The multi-level feature maps are then fed into the pathology localization decoder. We use a 3D ResNet50 [12], which accepts the volume as the input and generates multiple feature maps. Each series has its own backbone with the weights been shared. Given an input X k i ∈ R d×w×h , denoting a series k from the study i, we extract multiple feature maps of resolutionsfor each series k. We adhere to common standards by initializing the 3D ResNet50 backbone with pretrained weights. Prior work fine-tunes weights from the ImageNet dataset, but this may not be optimal if the target dataset has different characteristics. Our pretrained model for medical image analysis is based on ConVIRT [15], which uses visual representations and descriptive text from our internal dataset that contains 35433 image and text pairs."
Improving Pathology Localization: Multi-series Joint Attention Takes the Lead,2.3,Masked Self-attention,"To explore the complementary information between different series, we use Masked Self-Attention inspired from [2] which we call MSA, a powerful tool commonly used in multi-modality [8,10] models that enable to capture longrange dependencies between features. More formally, we denote the latent feature maps R l = {F j l } J j=1 , where j and l represents j th series and l th layer,representing the number of channels, d representing the depth dimension, and w and h representing the width and height dimensions, respectively. We concatenate the features F j l along the depth dimension d and add position embedding on the concatenated features. The transformer uses a linear projection for computing the set of queries, keys and values Q, K and V respectively. We adhere to the naming conventions used in [8].whereThe self-attention is calculated by taking the dot products between Q and K and then aggregating the values for each query,where, the attention mask M l-1 ∈ {0, 1} is a binarized output (thresholded at δ t ) of the the resized mask prediction of the previous (l -1)-th layer. δ t is empirically set to 0.15. The attention mask ignores the features that are not relevant to the pathology and attends to pathological features. B is a mask to handle missing series and it shares the same equation as 3.Finally, the transformer uses a non-linear transformation to calculate the output features, R l+1 , which shares the same resolution as that of R l .The transformer applies the attention mechanism 3 L times to generate a deep representation learning among the features. This approach allows the transformer model to effectively capture the relationships between different input positions."
Improving Pathology Localization: Multi-series Joint Attention Takes the Lead,2.4,Pathology Localization Decoder,"The localization decoder follows the transformer decoder paradigm, using a query, reference points, and input feature maps to predict a location and corresponding score. The decoder has N identical layers, each consisting of crossattention and feed forward networks (FFNs). The query Q ∈ R 1×256 and reference points R ∈ R 3 go through each layer, generating an updated Q as input for the next layer. Unlike Deformable DETR [17], the decoder initializes reference points by taking the last layer of the backbone feature map and applying Global Average Pooling, followed by a fully connected layer to generate the initial reference point. The localization refinement stage outputs location and scores for each layer N i , similar to Deformable DETR, providing fast convergence."
Improving Pathology Localization: Multi-series Joint Attention Takes the Lead,2.5,Loss Functions,"The model generates a single location ŷl ∈ R 3 , score y s and auxiliary heatmap outputs H for each series in a given study. The goal of our framework is to generate one reasonable location and its corresponding score for each series. Since there may be multiple locations annotated for a series, we use the Hungarian Matching function [17] to find optimal matching with the prediction to one of many ground truth locations. This is similar to the approach used in DETR. The Masked Self-Attention in our framework uses heatmaps generated from the previous layers. To ensure accurate heatmap generation, we apply an auxiliary heatmap loss using Mean Square Error (MSE) between the generated heatmap and the ground truth Gaussian heatmap, where the loss is defined as,where K is the number of intermediate heatmaps generated, x and h i are ground truth heatmap and predicted heatmap. To penalize the predicted location, we use the Huber loss defined as,where δ is empirically set to 0.3. The distance of a pathology does not differ more than λ (which can be calculated from the dataset) across series. With this information, we enforce proximity between the world coordinates which can be converted from the predicted volume coordinates across different series. We employ a Margin L1 loss, which penalizes the distance between points if they exceed the margin. Formally,where N is the number of series in a given study, wc ŷl is the world coordinates converted from volume coordinates.We then formulate the confidence score loss by considering the sum over the series of the binary cross entropy between the ground truth confidence score and predicted confidence score, formally defined as,Overall, the entire loss for a given study is formulated as,We set the hyper parameter w 1 , w 2 , w 3 and w 4 as 10, 1, 0.1, 1 respectively. These values are empirically set based on the validation loss."
Improving Pathology Localization: Multi-series Joint Attention Takes the Lead,3,Experiment,
Improving Pathology Localization: Multi-series Joint Attention Takes the Lead,3.1,"Implementation Details, Datasets and Evaluation Protocols","Implementation Details. Our model was implemented in Pytorch 1.13.1 on a NVIDIA A6000 GPU. We used an AdamW [5] optimizer with a weight decay of 10 -4 . The initial learning rate for encoder was empirically set as 10 -5 and 10 -4 for all other modules. Before running the pathology detection, we perform a preprocessing step similar to [3] and resize the volume to 28×128×128. Furthermore, we clip the intensity of the images at the 1st and 99th percentile, followed by an intensity normalization to ensure a mean of 0 and standard deviation of 1.Other hyper-parameters are mentioned in the supplementary paper.Datasets. The study is limited to secondary use of existing HIPPA-based deidentified data. No IRB required. We primarily conduct our experiments using knee MRI datasets, with a specific focus on MM tear, MM displaced fragment flap (DF), and MCC defect. Studies were collected at over 25 different institutions, and differed in scanner manufacturers, magnetic field strengths, and imaging protocols. The pathological locations were annotated by American Board certified sub-specialists radiologists. The most common series types included fat-suppressed (FS) sagittal (Sag), coronal (Cor) and axial (Ax) orientations, using either T2-weighted (T2) or proton-density (PD) protocols. For pathology detection, we use CorFS, SagFS, and SagPD. The dataset statistics that we use for training, validation and test are shown in Table 1. Evaluation Protocols. A useful pathology detection device should point the user to the correct location of a pathology. For model evaluation, we use the L1 distance between the predicted location to any annotation of the same pathology, labeled on the same series. To evaluate the pathology localization in a given study, we use the predicted pathology localization mask, which is obtained by thresholding the confidence score. However, this alone does not provide a complete picture of the model's performance. To evaluate our confidence score's performance, we analyze the specificity and sensitivity of the confidence scores. We report the mean over all series in the test studies in Table 2 Table 2. Quantitative results. We show the L1 distance measured in (mm), Sensitivity (Sn), and Specificity (Sp) score for different models. ""*"" refers to the models that were trained with different hyper-parameters from their mentioned ones. The results are evaluated on the test dataset. "
Improving Pathology Localization: Multi-series Joint Attention Takes the Lead,3.2,Comparison with SOTA Methods,"Heatmap-Based Architectures. The proposed architecture was compared to two other models, the Gaussian Ball approach [3] which utilizes a UNet architecture to generate a heatmap and KNEEL [11], which uses an hourglass network architecture to predict the Gaussian heatmap. Two variants of UNet were compared, one with MSA and one without. The threshold was set for each model which balanced sensitivity and specificity on the validation data. The comparison revealed that the sensitivity and specificity of the proposed MOAT model were 14 to 27% and 15 to 17% higher, respectively, than those of the other models. Additionally, the L1 distance of the heatmap-based model was approximately 5.4 to 8.0 mm higher than that of MOAT for all true positives. Overall, the results suggest that MOAT outperforms the other models in terms of sensitivity, specificity, and L1 distance.Regression-Based Architectures. We compared our proposed architecture with several other methods: 1) a simple regression method that removes the pathology localization decoder and uses a fully connected layer to predict the pathology locations, 2) DETR, 3) deformable DETR [17], and 4) Poseur [6], which uses Residual Log estimation. We adopt our ConVIRT pretrained encoder and add MSA to all the regression models to ensure a fair comparison. MOAT, which has 63.4G FLOPs, is highly efficient when compared to State-Of-The-Art (SOTA) regression models and has L1 distance lower than other models (4.7 mm) and the highest sensitivity and specificity among the models. We attach the standard deviation scores for each model in the supplementary section."
Improving Pathology Localization: Multi-series Joint Attention Takes the Lead,3.3,Ablation Study,"We first analyze the importance of MSA to our framework by training models with and without MSA. As MSA is a variant of self-attention, we also experiment with self-attention and with an attention mechanism [13] that was popular prior to self-attention. Table 4 shows the L1 distance for Medial Meniscus Tear (MM Tear) pathology, where our MSA which is a variant of self-attention is able to achieve the lowest L1 distance. Similarly, we analyze the weight factor for consistency loss, as different weight factor yields different results. From Table 3, we can see that the lowest L1 distance was obtained when the weight factor was 0.1. All the ablation studies were performed on the MM Tear validation dataset. "
Improving Pathology Localization: Multi-series Joint Attention Takes the Lead,4,Conclusion,"We propose MOAT, a framework for performing localization in multi-series MRI studies which benefits from the ability to share relevant information across series via a novel application of self-attention. We increase the efficiency of the MOAT model by using a pathology localization decoder which is a variant of deformable decoder and initializes the reference points from the backbone of the model. We evaluate the effectiveness of our proposed framework (MOAT) on three challenging pathologies from knee MRI and find that it represents a significant improvement over several SOTA localization techniques. Moving forward, we aim to apply our framework to pathologies from other body parts with multiple series."
Improving Pathology Localization: Multi-series Joint Attention Takes the Lead,,Table 1 .,
Improving Pathology Localization: Multi-series Joint Attention Takes the Lead,,Table 3 .,
Improving Pathology Localization: Multi-series Joint Attention Takes the Lead,,Table 4 .,
Combat Long-Tails in Medical Classification with Relation-Aware Consistency and Virtual Features Compensation,1,Introduction,"Recent years have witnessed the great success of deep learning techniques in various applications on computer-aided diagnosis [5,6,9,23]. However, the challenge of class imbalance inherently exists in medical datasets due to the scarcity of target diseases [7], where normal samples are significantly more than diseased samples. This challenge leads the model training biased to the majority categories [13] and severely impairs the performance of diagnostic models in real-world scenarios [8,18]. Therefore, it is urgent to improve the performance of diagnostic models in clinical applications, especially to achieve balanced recognition of minority categories.Technically, the issue of class imbalance is formulated as a long-tailed problem in existing works [10,12,15], where a few head classes contain numerous samples while the tail classes comprise only a few instances [28]. To address this issue, most of the previous methods have typically attempted to rebalance the data distribution through under-sampling the head classes [2], over-sampling the tail classes [21], or reweighting the contribution of different classes during the optimization process [4,8]. Nevertheless, these resampling methods can encounter a decrease in performance on certain datasets since the total information volume of the dataset is either unchanged or even reduced [29]. Recent advantages in longtailed medical image classification have been achieved by two-stage methods, which first train the model on the entire dataset and then fine-tune the classifier in the second stage using rebalancing techniques to counteract the class imbalance [12,14,15,19]. By decoupling the training of encoders and classifiers, the two-stage methods can recalibrate the biased classifiers and utilize all of the training samples to enhance representation learning for the encoder.Although the aforementioned decoupling methods [14,15] have somewhat alleviated the long-tails, the classification performance degradation in the minority classes remains unsolved, which can be attributed to two challenges. First, in the first stage, the decoupling methods train the model on the imbalanced dataset, which is insufficient for representation learning in the rare classes due to the scarcity of samples [17]. To this end, improving the first-stage training strategy to render effective supervision on representation learning is in great demand. The second problem lies in the second stage, where decoupling methods freeze the pre-trained encoder and fine-tune the classifier [14,15]. Traditional rebalancing techniques, such as resampling and reweighting, are used by the decoupling methods to eliminate the bias in the classifier. However, these rebalancing strategies have intrinsic drawbacks, e.g., resampling-based methods discard the samples of head classes, and reweighting cannot eliminate the imbalance with simple coefficients [26]. Thus, a novel approach that can perform balanced classifier training by generating abundant features is desired to recalibrate the classifier and preserve the representation quality of the encoder.To address the above two challenges, we propose the MRC-VFC framework that adopts the decoupling strategy to enhance the first-stage representation learning with Multi-view Relation-aware Consistency (MRC) and recalibrate the classifier using Virtual Features Compensation (VFC). Specifically, in the first stage, to boost the representation learning under limited samples, we build a twostream architecture to perform representation learning with the MRC module, which encourages the model to capture semantic information from images under different data perturbations. In the second stage, to recalibrate the classifier, we propose to generate virtual features from multivariate Gaussian distribution with the expectation-maximization algorithm, which can compensate for tail classes and preserves the correlations among features. In this way, the proposed MRC-VFC framework can rectify the biases in the encoder and classifier, and construct a balanced and representative feature space to improve the performance for rare diseases. Experiments on two public dermoscopic datasets prove that our MRC-VFC framework outperforms state-of-the-art methods for long-tailed diagnosis."
Combat Long-Tails in Medical Classification with Relation-Aware Consistency and Virtual Features Compensation,2,Methodology,"As illustrated in Fig. 1, our MRC-VFC framework follows the decoupling strategy [14,31] to combat the long-tailed challenges in two stages. In the first stage, we introduce the Multi-view Relation-aware Consistency (MRC) to boost representation learning for the encoder g. In the second stage, the proposed Virtual Features Compensation (VFC) recalibrates the classifier f by generating massive balanced virtual features, which compensates the tails classes without dropping the samples of the head classes. By enhancing the encoder with MRC and recalibrating the classifier with VFC, our MRC-VFC framework can perform effective and balanced training on long-tailed medical datasets."
Combat Long-Tails in Medical Classification with Relation-Aware Consistency and Virtual Features Compensation,2.1,Multi-view Relation-Aware Consistency,"The representation learning towards the decoupling models is insufficient [28,29]. To boost the representation learning, we propose the Multi-view Relationaware Consistency to encourage the encoder to apprehend the inherent semantic features of the input images under different data augmentations. Specifically, we build a student neural network f • g for the strong augmented input x s and duplicate a teacher model f • g for the weak augmented input x w . The two models are constrained by the MRC module to promote the consistency for different perturbations of the same input. The parameters of the teacher model are updated via an exponential moving average of the student parameters [24].To motivate the student model to learn from the data representations but the ill distributions, we propose multi-view constraints on the consistency of two models at various phases. A straightforward solution is to encourage identical predictions for different augmentations of the same input image, as follows:where KL(•, •) refers to the Kullback-Leibler divergence to measure the difference between two outputs. As this loss function calculates the variance of classifier output, the supervision for the encoders is less effective. To this end, the proposed MRC measures the sample-wise and channel-wise similarity between the feature maps of two encoders to regularize the consistency of the encoders. We first define the correlations of individuals and feature channels as indicates the similarities across feature channels. Thus, the consistency between the feature maps of two models can be defined as:Furthermore, we also adopt the cross-entropy loss, where y denotes the ground truth, between the predictions and ground truth to ensure that the optimization will not be misled to a trivial solution. The overall loss function is summarized aswhere λ 1 , λ 2 and λ 3 are coefficients to control the trade-off of each loss term. By introducing extra semantic constraints, the MRC can enhance the representation capacity of encoders. The feature space generated by the encoders is more balanced with abundant semantics, thereby facilitating the MRC-VFC framework to combat long-tails in medical diagnosis."
Combat Long-Tails in Medical Classification with Relation-Aware Consistency and Virtual Features Compensation,2.2,Virtual Features Compensation,"Recalling the introduction of decoupling methods, the two-stage methods [14] decouple the training of the encoder and classifier to eliminate the bias in the classifier while retaining the representation learning of the encoder. However, most existing decoupling approaches [12,15] employ the resampling strategy in the second stage to rebalance the data class distribution, causing the intrinsic drawbacks of the resampling of discarding the head class samples. To handle this issue, we propose Virtual Features Compensation, which generates virtual features z k ∈ R N k ×C for each class k under multivariate Gaussian distribution [1] to combat the long-tailed problem. Different from existing resampling methods [2], the feature vectors produced by the VFC module preserve the correlations among classes and the semantic information from the encoder. Given the k-th class, we first calculate the class-wise Gaussian distribution with mean μ k and covariance Σ k , as follows:where X k denotes the set of all samples in the k-th class, and g I (•) denotes the encoder trained in the first stage on the imbalanced dataset and N k is the sample number of the k-th class. We then randomly sample R feature vectors for each category from the corresponding Gaussian distribution N (μ k , Σ k ) to build the unbiased feature space, as {V k ∈ R R×C } K k=1 . We re-initialize the classifier and then calibrated it under cross-entropy loss, as follows:where K is the number of categories in the dataset. As the Gaussian distribution is calculated according to the statistics from the first-stage feature space, to further alleviate the potential bias, we employ the expectation-maximization algorithm [20] to iteratively fine-tune the classifier and encoder. At the expectation step, we freeze the classifier and supervise the encoder with extra balancing constraints to avoid being re-contaminated by the long-tailed label space. Thus, we adopt the generalized cross-entropy (GCE) loss [30] for the expectation step as follows:where q is a hyper-parameter to control the trade-off between the imbalance calibration and the classification task. At the maximization step, we freeze the encoder and train the classifier on the impartial feature space. By enriching the semantic features with balanced virtual features, our MRC-VFC framework can improve the classification performance in long-tailed datasets, especially the performance of minority categories. "
Combat Long-Tails in Medical Classification with Relation-Aware Consistency and Virtual Features Compensation,3,Experiments,
Combat Long-Tails in Medical Classification with Relation-Aware Consistency and Virtual Features Compensation,3.1,Datasets,"To evaluate the performance on long-tailed medical image classification, we construct two dermatology datasets from ISIC1  [25] following [12]. In particular, we construct the ISIC-2019-LT dataset as the long-tailed version of ISIC 2019 challenge2 , which includes 8 diagnostic categories of dermoscopic images. We sample the subset from Pareto distribution [8] as, where the imbalance factor r = N 0 /N k-1 is defined as the sample number of the head class N 0 divided by the tail one N k-1 . We adopt three imbalance factors for ISIC-2019-LT, as r = {100, 200, 500}. Furthermore, the ISIC-Archive-LT dataset [12] is sampled from ISIC Archive with a larger imbalance factor r ≈ 1000 and contains dermoscopic images of 14 classes. We randomly split these two datasets into train, validation and test sets as 7:1:2."
Combat Long-Tails in Medical Classification with Relation-Aware Consistency and Virtual Features Compensation,3.2,Implementation Details,"We implement the proposed MRC-VFC framework with the PyTorch library [22], and employ ResNet-18 [11] as the encoder for both long-tailed datasets. All the experiments are done on four NVIDIA GeForce GTX 1080 Ti GPUs with a batch size of 128. All images are resized to 224 × 224 pixels. In the first stage of MRC-VFC, we train the model using Stochastic Gradient Descent (SGD) with a learning rate of 0.01. For the strong augmentation [3], we utilize the random flip, blur, rotate, distortion, color jitter, grid dropout, and normalization, and adopt the random flip and the same normalization for the weak augmentation. In the second stage, we use SGD with a learning rate of 1 × 10 -5 for optimizing the classifier and 1 × 10 -6 for optimizing the encoder, respectively. The loss weights λ 1 , λ 2 and λ 3 in the first stage are set as 10, 10 and 5, and the q in the second stage is set as 0.8. We set training epochs as 100 for the first stage and 500 for the second stage. The source code is available at https://github.com/jhonP-Li/ MRC VFC."
Combat Long-Tails in Medical Classification with Relation-Aware Consistency and Virtual Features Compensation,3.3,Comparison on ISIC-2019-LT Dataset,"We evaluate the performance of our MRC-VFC framework with state-of-the-art methods for long-tailed medical image classification, including (i) baselines: finetuning classification models with cross-entropy loss (CE), random data resampling methods (RS), and MixUp [27]; (ii) recent loss reweighting methods: Generalized Cross-Entropy with Sparse Regularization (GCE+SR) [32], Seesaw loss [26], focal loss [16], and Class-Balancing (CB) loss [8]; (iii) recent works for longtailed medical image classification: Flat-aware Cross-stage Distillation (FCD) [15], and Flexible Sampling (FS) [12]. As illustrated in Table 1, we compare our MRC-VFC framework with the aforementioned methods on the ISIC-2019-LT dataset under different imbalance factors. Among these methods, our MRC-VFC framework achieves the best performance with an accuracy of 77.41%, 75.98%, and 74.62% under the imbalance factor of 100, 200, and 500, respectively. Noticeably, compared with the state-ofthe-art decoupling method FCD [15] on long-tailed medical image classification, our MRC-VFC framework surpasses it by a large margin of 11.03% accuracy when the imbalance factor is 500, demonstrating the effectiveness of representation learning and virtual features compensation in our framework. Furthermore, our MRC-VFC framework outperforms FS [12], which improves the resampling strategy and achieves the best performance on the ISIC-2019-LT dataset, with an accuracy increase of 9.4% under imbalance factor = 500. These experimental results demonstrate the superiority of our MRC-VFC framework over existing approaches in long-tailed medical image classification tasks. Ablation Study. We perform the ablation study to validate the effectiveness of our proposed MRC and VFC modules on two long-tailed datasets. As shown in Table 1 and 2, both MRC and VFC modules remarkably improve the performance over the baselines. In particular, we apply two ablative baselines of the proposed MRC-VFC framework by disabling the MRC (denoted as Ours w/o MRC) and the VFC (denoted as Ours w/o VFC) individually. In detail, as shown in Table 1, when the imbalance factor is 500, the accuracy increases by 4.49% and 7.14% for MRC and VFC, respectively. In addition, as illustrated in Table 2, the mean accuracy of all classes in the ISIC-Archive-LT shows an improvement of 2.40% and 2.92% for MRC and VFC correspondingly. The ablation study verifies the effectiveness of our MRC and VFC modules."
Combat Long-Tails in Medical Classification with Relation-Aware Consistency and Virtual Features Compensation,3.4,Comparison on ISIC-Archive-LT Dataset,"To comprehensively evaluate our MRC-VFC framework, we further perform the comparison with state-of-the-art algorithms on a more challenging ISIC-Archive-LT dataset for long-tailed diagnosis. As illustrated in Table 2, our MRC-VFC framework achieves the best overall performance with an accuracy of 67.84% among state-of-the-art algorithms, and results in a balanced performance over different classes, i.e., 69.71% for head classes and 70.34% for tail classes. Compared with the advanced decoupling method [15] for medical image diagnosis, our MRC-VFC framework significantly improves the accuracy with 4.73% in medium classes and 8.87% in tail classes, respectively.Performance Analysis on Head/Tail Classes. We further present the performance of several head and tail classes in Fig. 2. Our MRC-VFC framework outperforms FS [12] on both tail and head classes, and significantly promotes the performance of tail classes, thereby effectively alleviating the affect of long-tailed problems on medical image diagnosis. These comparisons confirm the advantage of our MRC-VFC framework in more challenging long-tailed scenarios."
Combat Long-Tails in Medical Classification with Relation-Aware Consistency and Virtual Features Compensation,4,Conclusion,"To address the long-tails in computer-aided diagnosis, we propose the MRC-VFC framework to improve medical image classification with balanced perfor-mance in two stages. In the first stage, we design the MRC to facilitate the representation learning of the encoder by introducing multi-view relation-aware consistency. In the second stage, to recalibrate the classifier, we propose the VFC to train an unbias classifier for the MRC-VFC framework by generating massive virtual features. Extensive experiments on the two long-tailed dermatology datasets demonstrate the effectiveness of the proposed MRC-VFC framework, which outperforms state-of-the-art algorithms remarkably."
Combat Long-Tails in Medical Classification with Relation-Aware Consistency and Virtual Features Compensation,,Fig. 1 .,
Combat Long-Tails in Medical Classification with Relation-Aware Consistency and Virtual Features Compensation,,,
Combat Long-Tails in Medical Classification with Relation-Aware Consistency and Virtual Features Compensation,,Fig. 2 .,
Combat Long-Tails in Medical Classification with Relation-Aware Consistency and Virtual Features Compensation,,Table 1 .,
Combat Long-Tails in Medical Classification with Relation-Aware Consistency and Virtual Features Compensation,,Table 2 .,
A Style Transfer-Based Augmentation Framework for Improving Segmentation and Classification Performance Across Different Sources in Ultrasound Images,1,Introduction,"Classification and segmentation are two common tasks that use deep learning techniques to solve clinical problems [1,2]. However, training deep learning models reliably usually requires a large amount of data samples. Models trained with limited data are susceptible to overfitting and possible variations due to small Fig. 1. Limitations of previous studies and our improvements for multi-source ultrasound data.sample size, which can lead to poor performance across different sources. Different sources refer to the same modality collected from different scanners. In medical imaging, one of the main reasons for poor performance is the variation in the imaging process, such as the type of scanner, the settings, the protocol, etc. [3]. This can cause changes in the intensity distributions of the images [4,5]. While training deep learning models with a large number of high-quality data could potentially address this problem, this approach is often challenging due to limited resources and difficulties in collecting medical images, as well as the manual annotation required by experienced radiologists or experts with professional domain knowledge. Thus, limited labeled data is commonly used to model the classification and segmentation network.To prevent overfitting and improve generalization, data augmentation [3,[6][7][8][9][10] has been proposed to generate more similar but different samples for the training dataset. Very often, this can be done by applying various transformations to the training images to create new images that reflect natural variations within each class. However, the model's performance across different sources heavily depends on the augmentation strategies. Another popular technique is style transfer [11], which adapts the style of test images to match the selected reference images (standard distributions) [4,5,12,13]. However, these methods have a limitation that their performance depends on the quality of the reference images. Moreover, these methods tend to transfer the style of the whole images, which may introduce irrelevant distribution information for medical imaging applications, as shown in Fig. 1. This problem is more severe in ultrasound images due to the presence of acoustic shadow.To address the above challenges, we propose a novel framework that combines the advantages of data augmentation and style transfer to enhance the model's segmentation and classification performance on ultrasound images from different sources. Our contributions (Fig. 1) are: 1) a mixed style augmentation strategy that integrates the information from different sources to improve the model's generalizability. 2) A feature-based augmentation that shifts the style at the feature level rather than the image level to better account for the potential variations. 3) a mask-based style augmentation strategy that avoids the influence of the irrelevant style information on ultrasound images during the style transfer."
A Style Transfer-Based Augmentation Framework for Improving Segmentation and Classification Performance Across Different Sources in Ultrasound Images,2,Methods,"Our proposed framework for ultrasonic image style augmentation consists of three stages, as illustrated in Fig.  "
A Style Transfer-Based Augmentation Framework for Improving Segmentation and Classification Performance Across Different Sources in Ultrasound Images,2.1,Mixed Style Augmentation (MixStyleAug),"To improve the performance of the multi-task network, we design MixStyleAug, combining traditional transformations and style transfer to incorporate image information from target sources during training (Fig. 2A). In this method, the content and the style images are sampled from training and target sources, respectively. Firstly, the traditional augmentation is applied to transform the content image, which can prevent overfitting. The traditional augmentation includes rotation, translation, scaling, and deformation transformations. Next, we translate the style of the augmented content image to that of the style image using the WCT 2 [14] style transfer network, generating a stylized content image. Finally, inspired by AugMix [15], we mix the stylized and augmented content images using random weights to create a style-augmented image that includes information from the training source. MixStyleAug allows the augmented training dataset to implicitly contain information from multiple sources, improving the model's performance across different sources. However, this method requires a large number of available images as reference styles for style augmentation, making it impractical for small-sized datasets."
A Style Transfer-Based Augmentation Framework for Improving Segmentation and Classification Performance Across Different Sources in Ultrasound Images,2.2,Network Architecture and Feature Augmentation (FeatAug),"To address the limitation of MixStyleAug in small-size medical datasets, FeatAug is applied for augmenting image styles at the feature level during the network training (Fig. 2B). In this work, we design a simple multi-task network for simultaneous segmentation and classification, and FeatAug is applied to the feature maps for feature augmentation.The architecture of our designed multi-task network (Fig. S1 in the Supplementary Materials) includes four encoders, four decoders, and a classification head. Each encoder includes two 3 × 3 convolutional layers with padding that are used to fuse the features. Each convolutional layer is followed by a rectified linear unit (ReLU) and a batch normalization (BN) [16]. Max-pooling layer is used to downsample the feature maps for dimension reduction. Through these encoders, the feature maps are generated and fed into the decoders and classification head to generate segmentation and classification results, respectively. Each decoder consists of three 3 × 3 convolutional layers with padding, three BN layers, three ReLUs, and a max-unpooling layer. In the classification head, the feature maps from the encoders are reduced to 128 channels by using a 3 × 3 convolutional layer with padding followed by ReLU and BN layer. Then, a global average pooling is used to downsample the feature maps. Finally, the features are fed into a fully connected layer followed by a sigmoid layer to output the classification result.Previous studies reported that changing the mean and standard deviation of the feature maps could lead to different image styles [17,18]. Thus, we design a module to randomly alter these values to augment the styles at the feature level. To avoid over-augmentation at the feature level, this module is randomly applied with a 50% probability after the residual connection in each encoder. The module is defined as follows:where A indicates the feature map, A indicates the augmented feature map, μ A indicates the mean of feature map A, σ A indicates the standard deviation of feature map A, and N (μ, σ) indicates a value randomly generated from a normal distribution with mean μ and standard deviation σ. In this study, the μ and σ of the normal distribution were empirically set to 0 and 0.1 according to preliminary experimental results, respectively."
A Style Transfer-Based Augmentation Framework for Improving Segmentation and Classification Performance Across Different Sources in Ultrasound Images,2.3,Mask-Based Style Augmentation (MaskAug),"In general, the style transfer uses the style information of the entire image, but this approach may not be ideal when the regions outside of the ROIs contain conflicting style information as compared to the regions within the ROIs, as illustrated in Fig. 1. To mitigate the impact of irrelevant or even adverse style information, we propose a mask-based augmentation technique (MaskAug) that emphasize the ROIs in the ultrasound image during style transfer network training.Figure 2C shows the pipeline of MaskAug and the steps are: 1) Content and style images are randomly chosen from training and target sources, respectively. 2) A trained multi-task network, which has been trained for several epochs and will be updated in the later epochs, is used to automatically generate ROIs of these images. 3) The content image, style image and their ROIs are input to the style transfer network. 4) During the style transfer, the intensity distribution of the ROI in the content image is changed to that of the style image. 5) Finally, mask-based style augmented images are produced and these images are then input to the multi-task network for further training."
A Style Transfer-Based Augmentation Framework for Improving Segmentation and Classification Performance Across Different Sources in Ultrasound Images,2.4,Loss Function and Implementation Details,"We utilized cross-entropy (CE) as the primary loss function for segmentation and classification during the training stage. Additionally, Dice loss [19] was computed as an auxiliary loss for segmentation. These loss functions are defined as:where L CE denotes CE loss, L Dice denotes Dice loss, L m denotes the loss for the multi-task network optimization, L Seg denotes the loss computed from the segmentation result, and L Cls denotes the loss computed from the classification result. We adopted Pytorch to implement the proposed framework, and the multitask network was trained on Nvidia RTX 3070 with 8 GB memory. During training, the batch size was set to 16, the maximum epoch number was 300, and the initial learning rate was set to 0.0005. We decayed the learning rate with cosine annealing [20] for each epoch, and the minimum learning rate was set to 0.000001. The restart epoch of cosine annealing was set to 300, ensuring that the learning rate monotonically decreased during the training process. For optimization, we used the AdamW optimizer [21] in our experiments. The whole training takes about 6 h and the inference time for a sample is about 0.2 s."
A Style Transfer-Based Augmentation Framework for Improving Segmentation and Classification Performance Across Different Sources in Ultrasound Images,3,Experimental Results and Discussion,"Datasets and Evaluation Metrics. We evaluated our framework on five ultrasound datasets (each representing a source) collected from multiple centers using different ultrasound scanners, including three liver datasets and two thyroid nodules datasets. A detailed description of the collected datasets is provided in Table S1 of the Supplementary Materials. We used the dataset with the largest sample size as the training source to prevent overfitting, while the other datasets were the target sources. For each datasets, we randomly split 20% of the samples for test, and used the remaining 80% for training the network. All the results in this study are based on the test set. In the training set, 20% data was randomly selected as validation set. In the data preprocessing, the input images were resized to 224×224 and were normalized by dividing 255.AUROC is used to evaluate the classification performance. DSC is used to assess the performance of the segmentation. The DSC is defined as:where T P refers to the pixels where both the predicted results and the gold standard are positive, F P refers to the pixels where the predicted results are positive and the gold standard are negative, and F N refers to the pixels where the predicted results are negative and the gold standard are positive. Ablation Study. We evaluated the effects of MixStyleAug, FeatAug, and MaskAug by training a multi-task network with different combinations of these augmentation strategies. Table 1 shows that MixStyleAug improves the segmentation and classification performance on the target sources compared to traditional augmentation. Furthermore, The combination of FeatAug and MixStyleAug improves the classification performance slightly in the liver datasets and significantly in the thyroid nodule datasets. This improvement is due to the style transfer at the feature level, which make the augmented features more similar to the target sources. Using MaskAug improved both segmentation and classification performance on both training and target sources, compared to the combination of FeatAug and MixStyleAug. This resulted in excellent performance. Figure 3 shows that the mask-based stylized content image has a more similar distribution to the style image than the other images, which helps the model perform better on both training and target sources.Comparison with Previous Studies. We compared our proposed method with BigAug [3], the style augmentation method by Hesse et al. [8], AutoAug [10], and UDA [22] on our collected datasets. Table 2 shows that our method performs excellently on both training and target sources. Unlike BigAug [3], our method uses style augmentation instead of intensity transformations, which avoids a drop in classification performance. Hesse et al. [8] only uses training sources for style  augmentation, which fail to improve performance on target sources, especially in classification tasks, when using a small-sized, single-source training dataset. Our method outperforms AutoAug [10], which relies on large samples to obtain the optimal augmentation strategy. UDA [22] is hard to train with a small-sized dataset due to overfitting and the complex adversarial training."
A Style Transfer-Based Augmentation Framework for Improving Segmentation and Classification Performance Across Different Sources in Ultrasound Images,4,Conclusion,"We proposed an augmentation framework based on style transfer method to improve the segmentation and classification performance of the network on ultrasound images from multiple sources. Our framework consists of MixStyleAug, FeatAug, and MaskAug. MixStyleAug integrates the image information from various sources for well generalization, while FeatAug increases the number of styles at the feature level to compensate for potential style variations. MaskAug uses the segmentation results to guide the network to focus on the style information of the ROI in the ultrasound image. We evaluated our framework on five datasets from various sources, and the results showed that our framework improved the segmentation and classification performance across different sources."
A Style Transfer-Based Augmentation Framework for Improving Segmentation and Classification Performance Across Different Sources in Ultrasound Images,,2 .,
A Style Transfer-Based Augmentation Framework for Improving Segmentation and Classification Performance Across Different Sources in Ultrasound Images,,Fig. 2 .,
A Style Transfer-Based Augmentation Framework for Improving Segmentation and Classification Performance Across Different Sources in Ultrasound Images,,Fig. 3 .,
A Style Transfer-Based Augmentation Framework for Improving Segmentation and Classification Performance Across Different Sources in Ultrasound Images,,,
A Style Transfer-Based Augmentation Framework for Improving Segmentation and Classification Performance Across Different Sources in Ultrasound Images,,Table 1 .,
A Style Transfer-Based Augmentation Framework for Improving Segmentation and Classification Performance Across Different Sources in Ultrasound Images,,Table 2 .,
Robust Cervical Abnormal Cell Detection via Distillation from Local-Scale Consistency Refinement,1,Introduction,"Cervical cancer is the second most common cancer among adult women. If diagnosed early, it can be effectively treated and cured [19]. Nevertheless, delayed diagnosis of cervical cancer until an advanced stage will have a negative impact on patient prognosis and consume medical resources. Currently, early screening of cervical cancer is recommended worldwide as an effective method to prevent and treat cervical cancer. Thin-prep cytologic test (TCT) is the most common and effective screening method for detecting cervical abnormal and premalignant cervical lesions [5]. Conventionally it is performed by visually examining the stained cells collected through smearing on a glass slide, and generating a diagnosis report using the descriptive diagnosis method of the Bethesda system (TBS) [15]. Although TCT has been widely used in clinical applications and has significantly reduced the mortality rates caused by cervical cancer, it is still unavailable for population-wide screening [18]. This is partly due to its laborintensive, time-consuming, and high cost [1]. Therefore, there is a high demand for automated cervical abnormality screening to facilitate efficient and accurate identification of cervical abnormalities.With the development of deep learning [10], several attempts have been made to identify cervical abnormal cells using convolutional neural networks (CNNs). For example, Cao et al. [2] developed an attention feature pyramid network (AttFPN) for automatic abnormal cervical cell detection in cervical cytopathological images to assist pathologists in making more accurate diagnoses. Chen et al. [3] proposed a new framework that decomposes tasks and compares cells for cervical lesion cell detection. Liang et al. [11] proposed to explore contextual relationships to boost the performance of cervical abnormal cell detection. Lin et al. [22] presented an automatic cervical cell detection approach based on the Dense-Cascade R-CNN. It is worth mentioning that all of the aforementioned detection methods inevitably produce false positive results, which should be further refined by pathologists for manual checking or classification models established for automatic screening. To solve this problem, Zhou et al. [23] proposed a three-stage method including cell-level detection, image-level classification, and case-level diagnosis obtained by an SVM classifier. Zhu et al. [24] developed an artificial intelligence assistive diagnostic solution, which integrated YOLOv3 [16] for detection, Xception, and Patch-based models to boost classification.Although the above-mentioned attempts can improve the screening performance significantly, there are several issues that need to be addressed: 1) Object detection methods often require accurate annotated data to guarantee performance with robustness and generalization. However, due to legal limitations, the scarcity of positive samples, and especially the subjectivity differences between cytopathologists for manual annotations [20], it is likely to generate noisy samples that affect the performance of the detection model. 2) Conventional object detection methods intend to directly extract the feature from the object area to locate and classify the object simultaneously. However, in clinical practice pathologists usually examine the target cells by comparing them to the surrounding cells to determine whether they are abnormal. Therefore, the visual feature correlations between the target cells and their surroundings can provide valuable information to aid the screening process, which also needs to be utilized when designing the cervical abnormal cell detection network.To address these issues, we propose a novel method for cervical abnormal cell detection using distillation from local-scale consistency refinement. Inspired by knowledge distillation, we construct a pre-trained Patch Correction Network (PCN), which is designed to exploit the supervised information from the PCN to reduce the impact of noisy labels and utilize the contextual relationships between cells. In our approach, we begin by utilizing RetinaNet [12] to locate suspicious cells and crop the top-K suspicious cells into patches. Then we feed them into the PCN to obtain classification scores and propose a ranking loss to refine the classifier of the detection network by correcting the score of the detection model. In addition, we propose an ROI-Correlation Consistency (RCC) loss between ROI features and local-scale features from the PCN, which encourages the detector to explore the feature correlations of the suspicious cells. Our proposed method achieves improved performance during inference without changing the detector structure."
Robust Cervical Abnormal Cell Detection via Distillation from Local-Scale Consistency Refinement,2,Method,"The proposed framework is shown in Fig. 1, which includes cervical abnormal cell detection and the PCN. Concerning the huge size of the Whole Slide Image (WSI) and the infeasibility to handle a WSI scan for detection, we crop the WSI into images with the size of 1024 × 1024 as input to the detection. Firstly, We choose RetinaNet as our cervical abnormal cell detection, which uses a Feature Pyramid Network (FPN) backbone and attaches two subnetworks to obtain bounding boxes and classification scores. We implement the detection to locate the suspicious lesion cervical cells and extract the top K patches from the original image. Besides, we add the ROI Align layer [17] to the output of the FPN and generate ROI features. Then these patches are fed into the PCN to obtain refined scores and local-scale features. Subsequently, our ranking loss is employed to correct the score of the detection, followed by the RCC loss to capture the contextual relationships between the extracted cells for further optimizing the detection model. The distillation process involves leveraging the learned knowledge and expertise from the PCN to refine the detection results of RetinaNet."
Robust Cervical Abnormal Cell Detection via Distillation from Local-Scale Consistency Refinement,2.1,Patch Correction Network(PCN),"In Fig. 1, the detection can automatically locate the suspicious cervical abnormal cells by providing their bounding boxes with the confidence scores. Due to the intrinsic architecture limitation of the detection and incomplete annotations, the confidence scores output by the RetinaNet may not be accurate, so we need another classification model to regrade the representative patches. Our framework leverages a local-scale classification refinement mechanism to guide the training of the detection model. We adopt SE-ResNext-50 [8] as the PCN, which has demonstrated its effectiveness in this field. The PCN is employed to refine and enhance the RetinaNet proposal classifier, which is trained from a large number of patches collected in advance with more excellent classification performance.More specifically, the input image is processed by the base detector F d (•) firstly to obtain the primary proposal information. The proposed PCN F c (•) takes the top-K patches as inputs, which are cropped from original images according to the proposal location, denoted as I p = Cr(I, p), where Cr(•) denotes the crop function, I and p denote input image and proposal boxes predicted by F d (•), respectively. Similar to the RetinaNet proposal classifier in F d (•), the PCN F c (•) outputs a classification distribution vector s c . Therefore, the proposed PCN F c (•) can be represented as:(The key idea is to augment the base detector F d (•) with the PCN F c (•) in parallel to enhance the proposal classification capability."
Robust Cervical Abnormal Cell Detection via Distillation from Local-Scale Consistency Refinement,2.2,Classification Ranking Loss,"Due to the inaccurate confidence scores output by RetinaNet, false positive cells are inevitable after detection. Hence, a good correction network is required to generate more precise scores. In this work, the suspicious ranking of the detected patches is updated by applying PCN to them. The detector is optimized by interscale pairwise ranking loss. Specifically, the ranking loss is given by:where s c is the classification refinement score and s d is the detection score, which enforces s d > s c + margin in training. We set margin = 0.05. Such a design can enable RetinaNet to take the prediction score as references, and utilize refined scores from PCN to obtain more confident predictions. The ranking loss optimizes the detection to generate higher confidence scores than the previous prediction, thereby suppressing false positives and enabling the detection network to better distinguish between positive and negative cells."
Robust Cervical Abnormal Cell Detection via Distillation from Local-Scale Consistency Refinement,2.3,ROI-Correlation Consistency (RCC) Learning,"In order to solve the problem of mismatched inputs to the detection and classification models, we add the ROI Align layer to the output of the FPN. However, for cervical abnormal cell detection, normal and abnormal cells may have very similar appearances, which might not be sufficient for conducting effective differentiation. In clinical practice, to determine whether a cervical cell is normal or abnormal, cytopathologists usually compare it to the surrounding reference cells. Therefore, we studied the correlation between the top K ROIs to help more accurate classification of abnormal cells.Based on the consistency strategy [14], which enhances the consistency of the intrinsic relation among different models, we propose ROI-correlation consistency, which regularizes the network to maintain the consistency of the semantic relation between patches under ROI features and local-scale features, and thereby encourage the detector to explore the feature interaction between cells from the extracted patches to improve the network performance.We model the structured relation among different patches with a case-level Gram Matrix [6]. Given an input mini-batch with B samples, where B denotes the batch size. And each sample undergoes the ROI Align layer to obtain the top K ROIs, we denote the activation map of ROIs as F R ∈ R B×K×H×W ×C , where H and W are the spatial dimension of the feature map, and C is the channel number. We set K = 10, H = 7, W = 7, C = 256. We average pooling the feature map F R along the spatial dimension and reshape it into A R ∈ R BK×C , and then the Case-wise Gram Matrix G R ∈ R BK×BK is computed as:where G ij is the inner product between the vectorized activation map A R i and A R j , whose intuitive meaning is the similarity between the activations of i th ROI and j th ROI within the input mini-batch. The final ROI relation matrix R R is obtained by conducting the L2 normalization for each row G R i of G R , which is expressed as:The proposed PCN F c (•) takes the B×K proposals of box regressor as inputs, we denote the local-scale feature map by PCN as F C ∈ R B×K×H ×W ×C , and set H = 56, W = 56. We perform average pooling on the feature map F C across the spatial dimension and then reshape it into A C ∈ R BK×HW C , the Case-wise Gram Matrix G C ∈ R BK×BK and the final relation matrix R C are computed as:The RCC requires the correlation matrix to be stable under ROI features and local-scale features to preserve the semantic relation between patches. We then define the proposed RCC loss as:where X is the proposals from the sampled mini-batch, R C (X) and R R (X) are the correlation matrices computed on X under different network. By minimizing L RCC during the training process, the network could be enhanced to capture the intrinsic relation between patches, thus helping to extract additional semantic information from cells."
Robust Cervical Abnormal Cell Detection via Distillation from Local-Scale Consistency Refinement,2.4,Optimization,"To better optimize the Retinanet detector in a reinforced way, we take the following training strategy, which consists of three major stages. In the first stage, we collect images with doctors' labels for training and initialized the detection net. In the second stage, we train PCN with cross-entropy loss until convergence.In the last stage, we freeze the PCN and optimize the detector. The detector is optimized using the total objective function, which is written as follows:where L cls and L reg are the ordinary detection loss for each detection head in RetinaNet. L cls is a Cross-Entropy loss for classification and L reg is a Smooth-L 1 loss for bounding box regression. L Rank is the classification ranking loss,L RRC is the RCC loss. α and β are hyper-parameters that denote the different weights of loss. During inference, only the optimized detector is used to output the final detection results without any additional modules.3 Experimental Results"
Robust Cervical Abnormal Cell Detection via Distillation from Local-Scale Consistency Refinement,3.1,Dataset and Experimental Setup,"Dataset. For cervical cell detection, our dataset includes 3761 images of 1024 × 1024 pixels cropped from WSIs. Our private dataset was collected and qualitycontrolled according to a standard protocol involving three pathologists: A, B, and C. Pathologist A had 33 years of experience in reading cervical cytology images, while pathologists B and C had 10 years of experience each. Initially, the images were randomly assigned to pathologist B or C for initial labeling. Later, the assigned pathologist's annotations were reviewed and verified by the other pathologist. Any discrepancies found were checked and re-labeled by pathologist A. These images were divided into the training set and the testing set according to the ratio of 9:1. We also collect a new dataset of 5000 positive and negative 224 × 224 cell patches to train the PCN.Implementation Details. The backbone of the suspicious cell detection network is RetinaNet with ResNet-50 [7]. The backbone of the pre-trained patch classification network is SE-ResNeXt-50. All parameters are optimized by Adam [9] with an initial learning rate of 4 × 10 -5 . We set α to 0.25 and β to 1 during training. The model is implemented by PyTorch on 2 Nvidia Tesla P100 GPUs. We conduct a quantitative evaluation using two metrics: the COCO-style [13] average precision (AP) and average recall (AR). We calculate the average AP over multiple IoU thresholds from 0.5 to 0.95 with a step size of 0.05, and individually evaluated AP at the IoU thresholds of 0.5 and 0.75 (denoted as AP.5 and AP.75), respectively."
Robust Cervical Abnormal Cell Detection via Distillation from Local-Scale Consistency Refinement,3.2,Evaluation of Cervical Abnormal Cell Detection,"Comparison with SOTA Methods. We compare the performance of our proposed method against known methods for cervical lesion detection as well as representative methods for object detection. Table 1 presents the results, from which several observations can be drawn. (1) Among the models for object detection, Retinanet is generally superior to the other models. (2) Based on Retinanet, our method improves the detection performance significantly, especially AP.5 shows great performance improvement. This confirms the necessity and effectiveness of introducing the classification ranking and ROI-correlation consistency schemes for cervical lesion detection.Ablation Study. We also perform an ablation study to further evaluate the contributions of each part in our method.   In addition, to further show the effectiveness of our method, we visualize the feature maps of Retinanet and the proposed method in Fig. 1. Those feature maps are from the Conv3 stages of the class-subnet backbone. Specifically, we sum and average the features in the channel dimension, and upsample them to the original image size. As shown in Fig. 2, our method can really learn better feature representations for abnormal cells, with the help of our proposed classification ranking refinement and ROI-correlation consistency learning. By model learning, our method can gradually enhance the features of abnormal cell regions while repressing noise or other suspicious but non-lesion regions."
Robust Cervical Abnormal Cell Detection via Distillation from Local-Scale Consistency Refinement,4,Conclusion,"In this paper, we integrate a distillation strategy that uses the knowledge learned from the pre-trained PCN to guide the training of the detection model to minimize the effects of noisy labels and explore the feature interaction between cells. Our method constructs RetinaNet with the PCN module which provides the refined scores and local-scale features of extracted patches. Specifically, we propose the ranking loss by utilizing refined scores to optimize the RetinaNet proposal classifier by reducing the impact of noisy labels. In addition, the ROI features generated by the detector and local-scale features from the PCN are used for correlation consistency learning, which explores the extracted cells' relationship. Our work can achieve better performance without adding new modules during inference. Experiments demonstrate the effectiveness and robustness of our method on the task of cervical abnormal cell detection."
Robust Cervical Abnormal Cell Detection via Distillation from Local-Scale Consistency Refinement,,Fig. 1 .,
Robust Cervical Abnormal Cell Detection via Distillation from Local-Scale Consistency Refinement,,Fig. 2 .,
Robust Cervical Abnormal Cell Detection via Distillation from Local-Scale Consistency Refinement,,Table 1 .,
Robust Cervical Abnormal Cell Detection via Distillation from Local-Scale Consistency Refinement,,Table 2,"reports the detailed ablation results, from which several observations can be drawn.(1) Compared with the baseline model, Retinanet, our classification ranking loss achieves considerably"
Robust Cervical Abnormal Cell Detection via Distillation from Local-Scale Consistency Refinement,,Table 2 .,
StainDiff: Transfer Stain Styles of Histology Images with Denoising Diffusion Probabilistic Models and Self-ensemble,1,Introduction,"Staining is a vital process in preparing tissue samples for histology studies. Specifically, with dyes such as Hematoxylin and Eosin, transparent tissue elements can be transformed into distinguishable features [1]. However, stain styles can vary significantly across different pathology labs or institutions. These variations can be due to the difference in staining materials, protocols, or processes among different pathologists or digital scanners [16,23]. Yet, the stain variations can cause inconsistencies between human domain experts [11]; and also hinder the performance of computer-aided diagnostic (CAD) systems [5,7]. Moreover, experiments have shown that stain variations can lead to a significant decrease in the accuracy and reproducibility of deep learning algorithms in histology analysis. Consequently, it is crucial to minimize staining variations to ensure reliable, consistent, and accurate CAD systems.To address the issue of stain variations between different domains, stain style transfer has been proposed. While the conventional color matching [22] and stain separation methods [19] used to be popular; learning-based approaches have become increasingly dominant, because they eliminate the need for challenging manual selection of the template images. For example, Stain-to-Stain Translation (STST) [25] approaches stain style transfer within a fully supervised 'pix2pix' framework [12]. Another approach, called StainGAN [26], improves on STST by tailoring a CycleGAN [34] to get rid of the dependence on learning from paired histology images and enable an unsupervised learning manner. These methods have shown promising results in reducing staining variations. Existing learning-based methods for stain style transfer are primarily confined to Generative Adversarial Networks (GANs) [6] and AutoEncoder (AE) [2], as depicted in Fig. 1(a) and (b) respectively. However, GAN approaches and AE suffer from the training of extra discriminators and challenging alignment of the posterior distributions, respectively [27]. In contrast, diffusion models, such as the prevalent denoising diffusion probabilistic model (DDPM) [9], have emerged as an alternative approach that can achieve competitive performance in various image-related tasks, such as image generation, inpainting, super-resolution, and etc [3]. Importantly, diffusion models offer several advantages over GANs and AEs, including tractable probabilistic parameterization, stable training procedures, and theoretical guarantees [3]. Additionally, they can avoid some of the challenges encountered by GANs and AEs, such as the alignment of posterior distributions or training extra discriminators, leading to a simpler model and training process. However, the applicability of diffusion models to histology stain style transfer remains unexplored. While the current diffusion models focus on image synthesis [9] or supervised image-to-image transaction [24], they are not applicable to our circumstance, as obtaining paired histology slides with different stain styles is not feasible in real clinical practice [27]. Therefore, we design an innovative cycle-consistent diffusion model that allows the transfer of representations between latent spaces at different time steps with the same morphological structure preserved in an unsupervised manner, as shown in Fig. 1(c).The major contributions are three-fold, summarized as follows.(1) We propose StainDiff, which is the first attempt at a pure denoising diffusion probabilistic model for stain transfer. More innovatively, unlike existing diffusion models, StainDiff is capable of learning from unpaired histology images, making it a more flexible and practical solution. The model is superior to GAN-based methods as the training of additional discriminators is free, and also spares for the difficulty in the alignment of posterior probabilities in AE-based approaches.(2) We also propose a self-ensemble scheme to further improve and stabilize the style transfer performance in StainDiff. This scheme utilizes the stochastic property of the diffusion model to generate multiple slightly different outputs from one input at the inference stage. (3) A broad range of histology tasks, such as stain normalization between multiple clients, can be conveniently achieved with minor adjustment to the loss in StainDiff."
StainDiff: Transfer Stain Styles of Histology Images with Denoising Diffusion Probabilistic Models and Self-ensemble,2,Methods,"Overview. The goal of this work is to design a diffusion model [9] to transfer the stain style between two domains, i.e., X A , X B . However, the traditional training paradigm of conditional DDPMs with paired images (x A 0 , x B 0 ) ∈ X A × X B is not feasible, as they are unavailable in the context of histology. To overcome this limitation, we design an innovative diffusion framework for stain style transfer, named StainDiff, which leverages the success of CycleGAN [34] and Style-GAN [26] and thus can be trained in an unsupervised manner with a novel cycle-consistency constraint. Specifically, StainDiff comprises two forward processes that perturb the histology image of two stain style domains to noise respectively, and two corresponding reverse processes that attempt to reconstruct noise back to original images from the perturbed ones. The overall training process is depicted in Fig. 2. Forward Process. Parameterized by the Markov chain, the forward process in StainDiff follows the vanilla DDPM by perturbing the histology images gradually with Gaussian noise, until all structures and morphological context information are lost. Formally, given a histology image x A 0 with respect to the stain style domain A, a transition kernel q progressively generates a sequence ofT thorough the following equation:where N (•) denotes the Gaussian distribution, I is the identity matrix. The hyper-parameters β t s follow a linear rule as defined in DDPM [9] to guarantee ) learns to reverse the Eq. ( 1) and generate images characterized by stain style A and B respectively, by gradually removing the noise initialized from Gaussian prior. To ensure conservative outputs [24], L1-norm denoising objective L d [4] is leveraged to train the denoising networks in the transition kernels. Due to the absence of pixel-topixel paired histology of both stain styles, it is infeasible to learn the interplay between them in a supervised manner as in most previous works. Consequently, a pair of auxiliary transform networksare designed to learn the transfer between the latent variables across the two domains in an unsupervised fashion, using a novel cycle-consistency constraint. Formally, this constraint ensures that two cycles as depicted in Fig. 2(b), derive an identity mapping, i.e.,where • denotes the composition of operations. It follows the cycle-consistency constraint formulated bywhere xA t+1 and xB t+1 are defined by Eq. ( 2); E denotes the expectation; • is the L1-norm. Finally, the overall loss function is L = L d + γL c , balanced by the coefficient γ. Inference Process and Self-ensemble. We describe the inference stage of StainDiff by transferring the histology images from stain style A to B; while the inverse, namely from B to A, is similar. Given a histology image input x A 0 characterized by stain style A, we begin by perturbing it s steps with Eq. (1) to derive x A s . Choosing the optimal value for s is important, as a large s (e.g., s = T ) leads to the loss of the contextual and structural information; while a small valued s (e.g., s = 1) fails to inject sufficient noise for StainDiff to transfer style. An ideal range for s is a small subset from [1, T ] that is centered by xB 0,i . The graphical model for the inference process and the proposed self-ensemble scheme are summarized in Fig. 3.Extension to Stain Normalization. The stain transfer primarily addresses the domain gap between two stain styles, which is mathematically formulated as a one-to-one mapping. Meanwhile, in some clinical settings, multiple institutions or hospitals are involved, where stain normalization is usually employed for multiple stain styles to one style alignment. The proposed symmetric StainDiff structure can be easily adapted to support stain normalization, with minimal change to the loss in Eq. ( 3). Concretely, we assume that domain A comprises multiple stain styles and domain B identifies the targeted stain style. By discarding the second term in Eq. ( 3), StainDiff becomes asymmetric and focuses specifically on the transfer from domain A to B. This modification allows us to use StainDiff for stain normalization without any other adjustments to the inference process."
StainDiff: Transfer Stain Styles of Histology Images with Denoising Diffusion Probabilistic Models and Self-ensemble,3,Experiments,
StainDiff: Transfer Stain Styles of Histology Images with Denoising Diffusion Probabilistic Models and Self-ensemble,,Datasets. Evaluations of StainDiff are conducted on two datasets. (1),"Dataset-A: MITOS-ATYPIA 14 Challenge1 . This dataset aims to measure the style transfer performance on 284 histology frames. Each slide is digitized by two different scanners, resulting in stain style variations. For a fair comparison, we follow the settings in previous work [26] by using 10,000 unpaired patches randomly cropped from the first 184 slides of both scanners as the training set. Meanwhile, 500 paired patches are generated from the remaining 100 slides as the test set, where we use Pearson correlation coefficient (PC), Structural Similarity index (SSIM) [31] and Feature Similarity Index for Image Quality Assessment (FSIM) [33] as the evaluation metrics. ( 2) Dataset-B: The Cancer Genome Atlas (TCGA). This dataset evaluates the performance of stain normalization quantified by the downstream nine-category tissue structure classification accuracy [27]. Domain A contains histology of multiple stain styles, which are collected from 186 WSIs from TCGA-COAD and NCT-CRC-HE-100K [14]; and domain B is the target style, curated from 25 WSIs in CRC-VAL-HE-7K [14]."
StainDiff: Transfer Stain Styles of Histology Images with Denoising Diffusion Probabilistic Models and Self-ensemble,,Implementations.,"All experiments are implemented in Python 3.8.13 with Pytorch 1.12.1 on two NVIDIA GeForce RTX 3090 GPU cards with 24GiB of memory each in parallel. We leverage the Adam optimizer with a learning rate of 2 × 10 -4 , and a batch size of 4. The learning scheme follows previous work [18], where the training process continues for 100 epochs if the overall loss did not decrease to the average loss of the previous 20 epochs. For StainDiff, we set the diffusion time T = 1000, the balancing coefficient γ = 1, and ensemble number m = 10. All experiments are repeated for 7 runs with different fixed random seeds i.e., {0, 1, 2, 3, 4, 5, 6}; and metrics are reported in the form of mean±standard deviation. Ablation Study. Table 1 and 2 show that incorporating a self-ensemble scheme can both boost the performance of StainDiff, and bring down the variations, demonstrating its effectiveness in stabilizing the stain transfer and normalization. To further investigate the effect of ensemble number m, we conduct ablation on Dataset-A. Experimentally, the FSIM when m = 1, 5, 10, 15, 20, 50 are 0.742, 0.749, 0.753, 0.756, 0.759, 0.759 respectively. While a slight performance gain can be achieved with higher m values than 10, the ensemble becomes more timeconsuming, as the cost time is linear to m. It implies an optimal m should be selected as a trade-off between the performance and computational time, such as 10 in this work."
StainDiff: Transfer Stain Styles of Histology Images with Denoising Diffusion Probabilistic Models and Self-ensemble,4,Conclusion,"In this paper, we propose StainDiff, a denoising diffusion model for histological stain style transfer, hence a model can get rid of the challenging issues in mainstream networks, such as the mode collapses in GANs or alignment between posterior distributions in AEs. Innovatively, by imposing a cycle-consistent constraint imposed on latent spaces, StainDiff enables learning from unpaired histology images, making it widely applicable to real clinical settings. One future work will explore efficient sampling diffusion models, e.g., DDIM [28], to address the long sampling time issue as inherited from DDPM. Another direction is to investigate other formulations of the diffusion model in the context of stain transfer, such as score-based or score-SDE diffusion models [32]. These extensions will fully expand the scope of our work, hence further advancing towards a comprehensive solution of stain style transfer in histology images."
StainDiff: Transfer Stain Styles of Histology Images with Denoising Diffusion Probabilistic Models and Self-ensemble,,Fig. 1 .,
StainDiff: Transfer Stain Styles of Histology Images with Denoising Diffusion Probabilistic Models and Self-ensemble,,Fig. 2 .,
StainDiff: Transfer Stain Styles of Histology Images with Denoising Diffusion Probabilistic Models and Self-ensemble,,Fig. 3 .,
StainDiff: Transfer Stain Styles of Histology Images with Denoising Diffusion Probabilistic Models and Self-ensemble,,Table 1 .Fig. 4 .,
StainDiff: Transfer Stain Styles of Histology Images with Denoising Diffusion Probabilistic Models and Self-ensemble,,Reverse Process and Cycle-Consistency Constraint.,"A T ; 0, I). Identically, we can progress the latent variablesx B 1 , x B 2 , • • • , x BT for the histology image x B 0 from the stain style domain B in the same fashion as Eq. (1)."
StainDiff: Transfer Stain Styles of Histology Images with Denoising Diffusion Probabilistic Models and Self-ensemble,,,"2 = 3 5 T . Afterwards, the latent variable x A s is transferred into the corresponding latent space with respect to stain style B with auxiliary transform network G B , which gives us xB s = G B (x A s ). Next, we use the p-sample [9] iteratively to denoise the xB s and obtain the transferred image xB 0 . As the sampling is a stochastic process, different"
StainDiff: Transfer Stain Styles of Histology Images with Denoising Diffusion Probabilistic Models and Self-ensemble,,Table 2 .,
MixUp-MIL: Novel Data Augmentation for Multiple Instance Learning and a Study on Thyroid Cancer Diagnosis,1,Motivation,"Whole slide imaging is capable of effectively digitizing specimen slides, showing both the microscopic detail and the larger context, without any significant manual effort. Due to the enormous resolution of the whole slide images (WSIs), a classification based on straight-forward convolutional neural network architectures is not feasible. Multiple instance learning [8,10,13,18,20] (MIL) represents a methodology (with a high momentum indicated by a large number of recent publications) to deal with these huge images corresponding to single (global) labels. In the MIL setting, WSIs correspond to labeled bags, whereas extracted patches correspond to unlabeled bag instances. MIL approaches typically consist of a feature extraction stage, a MIL pooling stage and a following downstream classification. State-of-the-art approaches mainly rely on convolutional neural network architectures for feature extraction, often in combination with attention [10,11] or self-attention [12]. For training the feature extraction stage, classical supervised and self-supervised learning is employed [10,11]. While the majority of methods rely on separate learning stages, also end-to-end approaches have been proposed [3,14]. In spite of the large amount of data, the number of labeled samples in MIL (represented by the number of individual, globally labelled WSIs) is often small and/or imbalanced [6]. General data augmentation strategies, such as rotations, flipping, stain augmentation and normalization and affine transformations, are applicable to increase the amount of data [15]. All of these methods are performed in the image domain.Here, we consider feature-level data augmentation directly applied to the representation extracted using a convolutional neural network. These methods can be easily combined with image-based augmentation and show the advantage of a high computational efficiency (since operations are efficient and pre-computed features can be used) [10,11]. For example, Li et al. [11] proposed an augmentation strategy based on sampling the patch-descriptors to generate several bags for an individual WSI. In this paper, we focus on the interpolations of patch descriptors based on the idea of Zhang et al [21], which is referred to as MixUp. This method was originally proposed as data agnostic approach which also shows good results if applied to image data [2,4,16]. Variations were proposed, to be applied to latent representations [17] as well as to balance data sets [6]. Due to the structure of MIL training data, we identified several options to perform interpolation-based data augmentation.The main contribution of this work is a set of novel data augmentation strategies for MIL, based on the interpolation of patch descriptors. Inspired by the (linear) MixUp approach [21], we investigated several ways to translate this idea to the MIL setting. Beyond linear interpolation, we also defined a more flexible and novel multilinear approach. For evaluation, a large experimental study was conducted, including 2 histological data sets, 5 deep learning configurations for MIL, 3 common data augmentation strategies and 4 MixUp settings. We investigated the classification of WSIs containing thyroid cancer tissues [1,5]. To obtain an improved understanding of reasons behind the experimental results, we also investigate the feature distributions."
MixUp-MIL: Novel Data Augmentation for Multiple Instance Learning and a Study on Thyroid Cancer Diagnosis,2,Methods,"In this paper, we consider MIL approaches relying on separately trained feature extraction and classification stages [9,10,12]. The proposed augmentation methods are applied to the patch descriptors obtained after the feature extraction stage. This strategy is highly efficient during training since the features are only computed once (per patch) and for augmentation only simple arithmetic operations are applied to the (smaller) feature vectors. Image-based data augmentation strategies (such as stain-augmentation, rotations or deformations) can be combined easily with the feature-based approaches but require individual feature extraction during training. However, to avoid the curse of meta-parameters and thereby experiments these methods are not considered here.In the original MixUp formulation of Zhang et al. [21], synthetic samples x are generated such that x = α • x i + (1 -α) • x j , where x i and x j are randomly sampled raw input feature vectors. Corresponding labels y are generated such that y = α • y i + (1 -α) • y j , where y i and y j are the corresponding one-hot label encodings. The weight α is drawn from a uniform distribution between 0 and 1.A single input (corresponding to a WSI) of a MIL approach with a separate feature extraction stage [10] can be expressed as a P-tupel X = (x 1 , ..., x P ) with x i being the feature vector of an individual patch and P being the number of patches per WSI. The method proposed by Zhang et al. cannot directly be applied to these tupels. However, there are several options to adapt the basic idea to the changed setting. "
MixUp-MIL: Novel Data Augmentation for Multiple Instance Learning and a Study on Thyroid Cancer Diagnosis,2.1,Inter-MixUp and Intra-MixUp,"Inter-MixUp refers to the generation of synthetic feature vectors by linearly combining feature vectors of a pair of WSIs (see Fig. 1 (a)). All features of a WSI with index w can be represented by X (w) , such that X (w) = (x (w ) 1 , ... , x (w ) P ) . To generate a new synthetic sample X (u) based on two samples X (w) and X (v) , we introduce the operationwith α being a uniformly sampled random weight (α ∈ [0, 1]). The WSI indexes v and w are uniformly sampled from the set of indexes. The index u ranges from the 1 to the number of extracted WSI descriptors. Since the new synthetic descriptors are individually generated in each epoch, there is no benefit if the number of extracted WSI descriptors is increased. We fix this number to the number of WSIs in the training data set, in order to keep the number of training iterations per epoch consistent.Two different configurations are considered. Firstly, we investigate the interpolation between WSIs of the same class (V1). Secondly, interpolation between all WSIs is performed, which also includes the interpolation between the labels (V2). In the case of V2, also the one-hot-encoded label vectors are linearly combined, such that yThe random values, α, v and w are selected individually for each individual WSI and each epoch. Before applying the MixUp operation, the vector tupel is randomly shuffled (as performed in all experiments).Intra-WSI combinations (Intra-MixUp) refers to the generation of synthetic descriptors by combining feature vectors within an individual WSI (see Fig. 1 (b)). A new synthetic patch descriptor x k is created based on the randomly selected descriptors x i and x j , such that x k = α • x i + (1 -α) • x j , with i and j being random indices (uniformly sampled from {1, 2, ..., P }) and α being a uniformly sampled random value a (α ∈ [0, 1]). The index k ranges from 1 to the number of extracted descriptors per patch. This number was kept stable (1024) during all experiments. The thereby obtained vector tupel (x 1 , ..., x P ) finally represents the synthetic WSI-based image descriptor. Besides performing combinations for each WSI during training, selective interpolation can be useful to keep real samples within the training data. This can be easily achieved by choosing (x 1 , ..., x P ) with a chance of β and (x 1 , ..., x P ) otherwise. While the Intra-MixUp method described before represents a linear interpolation method, we also investigated a multilinear approach by computing x k such that x k = α•x i +(1-α)•x j with α being a random vector and • being the element-wise product. This element-wise linear (multilinear) approach enables even higher variability in the generated samples."
MixUp-MIL: Novel Data Augmentation for Multiple Instance Learning and a Study on Thyroid Cancer Diagnosis,2.2,Experimental Setting,"As experimental architecture, use the dual-stream MIL approach proposed by Li et al [10]. Since this model combines both, embedding-based and an instance-based encoding, the effect of both paths can be individually investigated without changing any other architectural details. Since the method represents a state-of-the-art approach, it further serves as well-performing baseline. In instance-based MIL, the information per patch is first condensed to a single scalar value, representing the classification per patch. Finally, all of these patch-based values are aggregated. In embedding-based MIL, the information per patch is translated into a feature vector. All feature vectors from a WSI are then aggregated followed by a classification. In the investigated model [10] an instance-and an embedding-based pathway are employed in parallel and are merged in the end by weighted addition. The embedding-based pathway contains an attention mechanism, to higher weight patches that are similar to the so-called critical instance. The model makes use of an individual feature extraction stage. Due to the limited number of WSIs, we did not train the feature extraction stage [7], but utilize a pre-trained network instead. Specifically, we applied a ResNet18 pre-trained on the image-net challenge data, due to the high performance in previous work on similar data [5]. ResNet18 was assessed as particularly appropriate due to the rather low dimensional output (512 dimensions). We actively decided not to use a self-supervised contrastive learning approach [10] as feature extraction stage since invariant features could interfere with the effect of data augmentation. We investigated various settings consisting of instancebased only (INST), embedding-based only (EMB) and the dual-stream approach with weightings 3/1, 2/2 (balanced) and 1/3 for the instance and the embedding-based pathways.As comparison, several other augmentation methods on feature level are investigated including random sampling, selective random sampling and random noise. Random sampling corresponds to the random selection of patches (feature vectors) from each WSI. Thereby the amount of investigated data per WSI is reduced with the benefit of increasing the variability of the data. In the experiments, we adjust the sample ratio q between the patch-based features for training and testing. A q of 50 % indicates that 512 descriptors are used for training while for testing always a fixed number of 1024 is used. Selective random sampling corresponds to the random sampling strategy, with the difference that the ratio of features is not fixed but drawn from a uniform random distribution (U (q, 100 %)). Here, a q of 50 % indicates that for each WSI, between 512 and 1024 feature vectors are selected. In the case of the random noise setting, to each feature vector x i , a random noise vector r is added (x i = x i + r). The elements of r are randomly sampled (individually for each x i ) from a normal distribution N (0, σ ).To incorporate for the fact that the feature dimensions show different magnitudes, σ is computed as the product of the meta parameter σ and the standard deviation of the respective feature dimension.In this work, we aimed at distinguishing different nodular lesions of the thyroid, focusing especially on benign follicular nodules (FN) and papillary carcinomas (PC). This differentiation is crucial, due to the different treatment options, in particular with respect to the extent of surgical resection of the thyroid gland [19]. The data set utilized in the experiments consists of 80 WSIs overall. One half (40) of the data set consists of frozen and the other half (40) of paraffin sections [5]), representing the different modalities. All images were acquired during clinical routine at the Kardinal Schwarzenberg Hospital. Procedures were approved by the ethics committee of the county of Salzburg (No. 1088/2021). The mean and median age of patients at the date of dissection was 47 and 50 years, respectively. The data set comprised 13 male and 27 female patients, corresponding to a slight gender imbalance. They were labeled by an expert pathologist with over 20 years experience. A total of 42 (21 per modality) slides were labeled as papillary carcinoma while 38 (19 per modality) were labeled as benign follicular nodule. For the frozen sections, fresh tissue was frozen at -15 • Celsius, slides were cut (thickness 5 µm) and stained immediately with hematoxylin and eosin. For the paraffin sections, tissue was fixed in 4 % phosphate-buffered formalin for 24 h. Subsequently formalin fixed paraffin embedded tissue was cut (thickness 2 µm) and stained with hematoxylin and eosin. The images were digitized with an Olympus VS120-LD100 slide loader system. Overviews at a 2x magnification were generated to manually define scan areas, focus points were automatically defined and adapted if needed. Scans were performed with a 20x objective (corresponding to a resolution of 344.57 nm/pixel). The image files were stored in the Oympus vsi format based on lossless compression.   q q q q q q q q 0 25 % 50 % 75 % 100 % 0.4 The data set was randomly separated into training (80 %) and test data (20 %). The whole pipeline, including the separation, was repeated 32 times to achieve representative scores. Due to the almost balanced setting, the overall classification accuracy (mean and standard deviation) is finally reported. Adam was used as optimizer. The models were trained for 200 epochs with an initial learning rate of 0.0002. Random shuffling of the vector tupels (shuffling within the WSIs) was applied for all experiments.The patches were randomly extracted from the WSI, based on uniform sampling. For each patch, we checked that at least 75 % of the area was covered with tissue (green color channel) in order to exclude empty areas [5]. To obtain a representation independent of the WSI size, we extracted 1024 patches with a size of 256 × 256 pixel per WSI, resulting in 1024 patch-descriptors per WSI [5]. For feature extraction, a ResNet18 network, pretrained on the image-net challenge was deployed [10]. Data and source code are publicly accessible via https://gitlab.com/mgadermayr/mixupmil. We use the reference implementation of the dual-stream MIL approach [10]. To obtain further insight into the feature distribution, we randomly selected patch descriptor pairs and computed the Euclidean distances. In detail, we selected 10,000 pairs (a) from different classes, (b) from different WSIs (similar and dissimilar classes), (c,d) from the same class and different WSIs, and (e) from the same WSI."
MixUp-MIL: Novel Data Augmentation for Multiple Instance Learning and a Study on Thyroid Cancer Diagnosis,3,Results,"Figure 2 shows the mean overall classification accuracy and standard deviations obtained with each individual combination. The columns represent the frozen (left) and paraffin data set (right). The top row (a) shows the baseline scores of embedding-based, instance-based and the 3 combinations. Subfigure (b) show the scores obtained with baseline data augmentation for embedding-based and dual-stream MIL. Subfigure (c) shows the scores obtained with interpolation between patches between (Inter-MixUp) and within WSIs (Intra-MixUp). Without data augmentation, scores between 0.49 and 0.72 were obtained for frozen and scores between 0.41 and 0.81 for the paraffin data set. To limit the number of figures and due to the fact that instance-based MIL showed weak scores only, in the following part the focus is on embedding-based and combined-MIL (2/2) only. With baseline data augmentation, scores between 0.69 and 0.73 were achieved for the frozen and between 0.78 and 0.83 for the paraffin data set. Inter-MixUp exhibited scores up to 0.71 for the frozen and up to 0.79 for the paraffin data set. Intra-MixUp showed average accuracy up to 0.78 for the frozen and up to 0.84 for the paraffin data set. The best scores were obtained with the multilinear setting. In Fig. 3, the distributions of the descriptor (Euclidean) distances between (a-d) patches from different different WSIs (inter-WSI) and (e) patches within a single WSI (intra-WSI) are provided. The mean distances range from 171.3 to 177.8 for the inter-WSI settings. In the intra-WSI setting, a mean distance of 134.8 was obtained. Based on the used common box plot variation (whiskers length is less than 1.5× the interquartile range), a large number of data points was identified as outliers. However, these points are not considered as real outliers, but occur due to the asymmetrical data distribution (as indicated by the violin plot in the background). "
MixUp-MIL: Novel Data Augmentation for Multiple Instance Learning and a Study on Thyroid Cancer Diagnosis,4,Discussion,"In this work, we proposed and examined novel data augmentation strategies based on the idea of interpolations of feature vectors in the MIL setting. Instance-based MIL did not show any competitive scores. Obviously the model reducing each patch to a single value is not adequate for the classification of frozen or paraffin sections from thyroid cancer tissues. The considered dual-stream approach, including an embedding and instance-based stream, exhibited slightly improved average scores, compared to embedding-based MIL only. In our analysis, we focused on the embedding-based configuration and on the balanced combined approach (referred to as 2/2). With the baseline data augmentation approaches, the maximum improvements were 0.03, and 0.02 for the frozen, and 0.01, and 0.05 for the paraffin data set. The Inter-MixUp approach did not show any systematic improvements. Independently of the chosen strategy (V1, V2), concerning the combination within or between classes, we did not notice any positive trend. The multilinear Intra-MixUp method, however, exhibited the best scores for 3 out of 4 combinations and the best overall mean accuracy for both, the frozen and the paraffin data set. Also a clear trend with increasing scores in the case of an increasing ratio of augmented data (β) is visible. The linear method showed a similar, but less pronounced trend. Obviously, the straightforward application of the MixUp scheme (as in case of the Inter-MixUp approach), is inappropriate for the considered setting. An inhibiting factor could be a high inter-WSI variability leading to incompatible feature vectors (which are too far away from realistic samples in the feature space). To particularly investigate this effect, we performed 2 different Inter-MixUp settings (V1 & V2), with the goal of identifying the effect of mixed (and thereby more dissimilar) or similar classes during interpolation. The analysis of the distance distributions between patch representations confirmed that, the variability between WSIs is clearly larger than the variability within WSIs. In addition, the results showed that the variability between classes is, on patch-level, not clearly larger than the variability within a class. Obviously variability due to the acquisition outweigh any disease specific variability. This could provide an explanation for the effectiveness of Intra-MixUp approach compared to the (similarly) poorly performing Inter-MixUp settings. We expect that stain normalization methods (but not stain augmentation) could be utilized to align the different WSIs to provide a more appropriate basis for inter-WSI interpolation. With regard to the different data sets, we noticed a stronger, positive effect in case of the frozen section data set. This is supposed to be due to the clearly higher variability of the frozen sections corresponding with a need for a higher variability in the training data. We also noticed a stronger effect of the solely embedding-based architecture (also showing the best overall scores). We suppose that this is due to the fact that the additional loss of the dual-stream architecture exhibits a valuable regularization tool to reduce the amount of needed training data. With the proposed Intra-MixUp augmentation strategy, this effect diminishes, since the amount and quality of training data is increased.To conclude, we proposed novel data augmentation strategies based on the idea of interpolations of image descriptors in the MIL setting. Based on the experimental results, the multilinear Intra-MixUp setting proved to be highly effective, while the Inter-MixUp method showed inferior scores compared to a state-of-the-art baseline. We learned that there is a clear difference between combinations within and between WSIs with a noticeable effect on the final classification accuracy. This is supposedly due to the high variability between the WSIs compared to a rather low variability within the WSIs. In the future, additional experiments will be conducted including stain normalization methods and larger benchmark data sets to provide further insights."
MixUp-MIL: Novel Data Augmentation for Multiple Instance Learning and a Study on Thyroid Cancer Diagnosis,,Fig. 1 .,
MixUp-MIL: Novel Data Augmentation for Multiple Instance Learning and a Study on Thyroid Cancer Diagnosis,,Fig. 2 .,
MixUp-MIL: Novel Data Augmentation for Multiple Instance Learning and a Study on Thyroid Cancer Diagnosis,,Fig. 3 .,
Transfer Learning-Assisted Survival Analysis of Breast Cancer Relying on the Spatial Interaction Between Tumor-Infiltrating Lymphocytes and Tumors,1,Introduction,"Breast cancer (BC) is the most common cancer diagnosed among females and the second leading cause of cancer death among women after lung cancer [1]. The BC differs greatly in clinical behavior, ranging from carcinoma in site to aggressive metastatic disease [2,3]. Thus, effective and accurate prognosis of BC as well as stratifying cancer patients into different subgroups for personalized cancer management has attracted more attention than ever before.Among different types of imaging biomarkers, histopathological images are generally considered the golden standard for BC prognosis since they can confer important cell-level information that can reflect the aggressiveness of BC [4]. Recently, with the availability of digitalized whole-slide pathological images (WSIs), many computational models have been employed for the prognosis prediction of various subtypes of BC. For instance, Lu et al [5] presented a novel approach for predicting the prognosis of ER-Positive BC patients by quantifying nuclear shape and orientation from histopathological images. Liu et al [6] developed a gradient boosting algorithm to predict the disease progression for various subtypes of BC. However, due to the high-cost of collecting survival information from the patients, it is still a challenge to build effective machine learning models for specific BC subtypes with limited annotation data.To deal with the above challenges, several researchers began to design domain adaption algorithms, which utilize the labeled data from a related cancer subtype to help predict the patients' survival in the target domain. Specifically, Alirezazadeh et al [7] presented a new representation learning-based unsupervised domain adaption method to predict the clinical outcome of cancer patients on the target domain. Zhang et al [8] proposed a collaborative unsupervised domain adaptation algorithm, which conducts transferability-aware adaptation and conquers label noise in a collaborative way. Other studies include Xu et al [9] developed graph neural networks for unsupervised domain adaptation in histopathological image analysis, based on a backbone for embedding input images into a feature space, and a graph neural layer for propagating the supervision signals of images with labels.Although much progress has been achieved, most of the existing studies applied the feature alignment strategy to reduce the distribution difference between source and target domains. However, such transfer learning methods neglected to take the interaction among different types of tissues into consideration. For example, it is widely recognized that tumor-infiltrating lymphocytes (TILs) and its correlation with tumors reveal a similar role in the prognosis of different BRCA subtypes. For instance, Kurozumi et al [10] revealed that high TILs expression was correlated with negative estrogen receptor (ER) expression and high histological grade (P < 0.001). Lu et al [11] utilized the TILs spatial pattern for survival analysis in different breast cancer subtypes including ER-negative, ER-positive, and triple-negative. It can be expected that better prognosis performance can be achieved if we leveraged the TILs-Tumor interaction information to resolve the survival analysis task on the target domain.Based on the above considerations, in this paper, we proposed a TILs-Tumor interactions guided unsupervised domain adaptation (T2UDA) algorithm to predict the patients' survival on the target BC subtype. Specifically, T2UDA first applied the graph attention network (GATs) to learn node embeddings and the spatial interactions between tumor and TILs patches in WSI. In order to preserve the node-level and interaction-level similarities across different domains, we not only aligned the embedding for different types of nodes but also designed a novel Tumor-TILs interaction alignment (TTIA) module to ensure that the distribution of the interaction weights are similar in both domains. We evaluated the performance of our method on the Breast Invasive Carcinoma (BRCA) cohort derived from the Cancer Genome Atlas (TCGA), and the experimental results indicated that T2UDA outperforms other domain adaption methods for predicting patients' clinical outcomes."
Transfer Learning-Assisted Survival Analysis of Breast Cancer Relying on the Spatial Interaction Between Tumor-Infiltrating Lymphocytes and Tumors,2,Method,"We summarized the proposed T2UDA network in Fig. 1, which consists of three parts, i.e., Graph Attention Network-based Framework, Feature Alignment(FA), and TILs-Tumor interaction alignment(TTIA). Next, we will introduce each part in detail. Data Pre-processing. We obtained valid patches of 512 × 512 pixels from pathological images and segment the TILs and tumor tissues using a pre-trained U-Net++ model. Then we calculated the tumor and TILs area ratios in each patch and selected 300 patches with the largest ratios of each tissue type. Based on the selected patches, we constructed a graph G = (V, E) for each WSI."
Transfer Learning-Assisted Survival Analysis of Breast Cancer Relying on the Spatial Interaction Between Tumor-Infiltrating Lymphocytes and Tumors,,Source Graph Construction,"Here, given patches as nodes V , we first calculated the pairwise distance among different nodes, and select the top 10 percent connections with the smallest distance values as edges E. For each node in V , we followed the study in [12], which applied the ResNet-101 model to extract node features. Then, the principal component analysis (PCA) is implemented to reduce dimensionalities of the node features to 128."
Transfer Learning-Assisted Survival Analysis of Breast Cancer Relying on the Spatial Interaction Between Tumor-Infiltrating Lymphocytes and Tumors,,Calculating TILs-Tumor Interaction via Graph Attention Networks(GATs).,"To characterize the interaction between different TILs and tumor patches, we employed GAT [13], which has been proven to be useful in describing the spatial interaction between different tissues across WSIs. Our GAT-based framework consisted of 3 GAT layers interleaved with 3 graph pooling layers [14](shown in Fig. 1). The input of the GAT layer are we calculated the attention coefficients among different nodes, which can be formulated as:Furthermore, a softmax function was then adopted to normalize the attention coefficients e ij :where N i represents all neighbors of node i. The new feature vector v i for node i was calculated via a weighted sum:Finally, the output features of each GAT layer were aggregated in the readout layer. We fed the generated output features from each readout layer into the Cox hazard proportional regression model for the final prognosis predictions.Feature Alignment. In the proposed GAT-based transfer learning framework, the feature alignment component was employed on its first two layers. Then, for the node embeddings with different types (TILs and Tumor) in both the source and target domain, we performed a mean pooling operation to obtain their aggregated features. Next, we aligned the aggregated tumor or TILs features from the two domains separately using Maximum Mean Discrepancy(MMD) [15].Here, we adopted MMD for feature alignment due to its ability to measure the distance between two distributions without explicit assumptions on the data distribution, we showed the objective function of MMD in our method as follows:where H is a Hilbert space, f represents the features from the source, f represents the feature from the target, r represents the layer number, k ∈ {L, T } referred to TILs or tumor node. In addition, n denotes the number of source samples, while m refers to the number of target samples.  TILs-Tumor Interaction Alignment. To accurately characterize the interaction between TILs and tumors, we further analyzed the extracted interaction weights by dividing them into 10 intervals (i.e., bins). For each interval, we calculated the sum of all source domain interaction weights as i s k and the sum of all target domain interaction weights as i t k , where k represents the k-th interval. Consequently, we obtained two vectors and applied softmax on each of them for normalization that can be denoted as. In order to measure the dissimilarity between p i and q i , the Kullback-Leibler (KL) divergence is adapted on the third layer of GAT, which can be formulated as:According to Eq.( 5), we can ensure that the weight distributions for the TIL-Tumor interaction are consistent in the source and target domain, which will benefit the following survival analysis. It is beneficial for the target domain."
Transfer Learning-Assisted Survival Analysis of Breast Cancer Relying on the Spatial Interaction Between Tumor-Infiltrating Lymphocytes and Tumors,,Prognosis Prediction by the Cox Proportional Hazard Model.,"The Cox proportional hazard model was applied to predict the patients' clinical outcome [16], and its negative log partial likelihood function can be formulated as:where x i represents the output of the last layer for the prognosis task and R (t i ) is the risk set at time t i , which represents the set of patients that are still under risk before time t. In addition, δ i is an indicator variable. Sample i refers to censored patient ifOverall Objective. To achieve domain-adaptive prognosis prediction, the final loss function included the Cox loss, FA loss, and TTIA loss as the following formula:where α and β represent the weights assigned to the importance of FA component and TTIA component respectively."
Transfer Learning-Assisted Survival Analysis of Breast Cancer Relying on the Spatial Interaction Between Tumor-Infiltrating Lymphocytes and Tumors,3,Experiments and Results,"Datasets. We conducted our experiments on the breast invasive carcinoma (BRCA) dataset from The Cancer Genome Atlas (TCGA). Specifically, the BRCA dataset includes 661 patients with hematoxylin and eosin (HE)-stained pathological imaging and corresponding survival information. Among the collected BRCA patients in TCGA, the number of ER positive(ER+) and ER negative(ER-) patients are 515 and 146, respectively. We hope to investigate if the proposed T2UDA could be used to help improve the prognosis performance of (ER+) or (ER-) with the aid of the survival information on its counterpart."
Transfer Learning-Assisted Survival Analysis of Breast Cancer Relying on the Spatial Interaction Between Tumor-Infiltrating Lymphocytes and Tumors,3.1,Implementation Details and Evaluation Metrics,"The dimension of intermediate layers in GAT was 256. The pooling ratio in SagPool was set to 0.7. α and β were tuned from {0.01, 0.1}. During training, the model was trained for 150 epochs for both the main experiment and all comparative experiments. We used the Adam optimizer with a learning rate tuned from {1e -5, 1e -4}. We evaluated the performance of our model using the Concordance Index (CI) and Area Under the Curve (AUC) as performance metrics. Both CI and AUC range from 0 to 1, with larger values indicating better prediction performance and vice versa [1].  "
Transfer Learning-Assisted Survival Analysis of Breast Cancer Relying on the Spatial Interaction Between Tumor-Infiltrating Lymphocytes and Tumors,3.2,Result and Discussion,"In this study, we compared the performance of our proposed model with several existing domain adaptation methods, including 1) DDC [17]: Utilize the Maximum Mean Discrepancy (MMD) to calculate the domain difference loss between source and target data and optimize both classification loss and disparity loss. 2) DANN [18]: An adversarial learning method that used gradient backpropagation to extract domain-independent features. 3) MDD [19]: An adversarial training method that combined metric learning and domain adaptation. 4) DeepJDOT [20]: An Unsupervised Domain Adaptation method based on optimal transport that simultaneously learns features and optimizes classifiers by measuring joint feature/label differences. 5) Source only: it was trained on the source domain and applied directly to the target domain. 6) T2UDA-v1: it was a variant of T2UDA which didn't use TTIA. The experimental results were presented in Table 1.The results presented in Table 1 revealed several key observations. First, our proposed method outperformed feature alignment-based methods such as DDC and DeepJDOT in terms of both CI and AUC values. The reason lies in that these methods only transferred the knowledge at the feature level and neglected the inter-relationship between TILs and tumors. Second, our method outperformed adversarial-based methods such as DANN and MDD, as the high heterogeneity between the target and source domains results in negative transfer through adversarial training. Instead of directly aligning regions, our proposed method focused on similar TILs-Tumor interactions and aligning patches of the same tissue.We also evaluated the contributions of the key components of our framework and found that T2UDA performed better than Source only and T2UDA-v1, which shows the advantage of minimizing differences in TILs-Tumor interaction weights.In addition, we also evaluated the patient stratification performance of different methods. As shown in Fig. 3, our proposed T2UDA outperformed feature alignment-based methods (such as DDC and DeepJDOT), adversarial-based methods (such as DANN and MDD), and T2UDA-v1 in stratification performance, proving that considering the interaction between TILs and tumors as migration knowledge leads to better prognostic results.We also examined the consistency of important edges in each group of stratified patients based on the TILs-Tumor interaction weights calculated by the GAT-based framework in the source and target domains. As seen in Fig. 4(a), for both the source and target domains, the proportion of edges that connect TILs and tumor regions in the low-risk group was higher than that in the highrisk group, showing that the interaction between TILs and tumors played a critical role in prognostic prediction in different BC subtypes. Furthermore, as shown in Fig. 4(b), the weights of the edges connecting tumor and TILs regions were higher for patients in the low survival risk group in both source and target domains. This was consistent with our knowledge that brisk interaction between TILs and tumor regions indicates a better clinical outcome and demonstrates the transferability of this knowledge."
Transfer Learning-Assisted Survival Analysis of Breast Cancer Relying on the Spatial Interaction Between Tumor-Infiltrating Lymphocytes and Tumors,4,Conclusion,"In this paper, we presented an unsupervised domain adaptation algorithm that leverages TILs-Tumor interactions to predict patients' survival in a target BC subtype(T2UDA). Our results demonstrated that the relationship between TILs and tumors is transferable and can be effectively used to improve the accuracy of survival prediction models. To the best of our knowledge, T2UDA was the first method to successfully achieve interrelationship transfer between TILs and tumors across different cancer subtypes for prognosis tasks."
Transfer Learning-Assisted Survival Analysis of Breast Cancer Relying on the Spatial Interaction Between Tumor-Infiltrating Lymphocytes and Tumors,,Fig. 1 .,
Transfer Learning-Assisted Survival Analysis of Breast Cancer Relying on the Spatial Interaction Between Tumor-Infiltrating Lymphocytes and Tumors,,Fig. 2 .,
Transfer Learning-Assisted Survival Analysis of Breast Cancer Relying on the Spatial Interaction Between Tumor-Infiltrating Lymphocytes and Tumors,,Fig. 3 .,
Transfer Learning-Assisted Survival Analysis of Breast Cancer Relying on the Spatial Interaction Between Tumor-Infiltrating Lymphocytes and Tumors,,Fig. 4 .,
Transfer Learning-Assisted Survival Analysis of Breast Cancer Relying on the Spatial Interaction Between Tumor-Infiltrating Lymphocytes and Tumors,,,
Transfer Learning-Assisted Survival Analysis of Breast Cancer Relying on the Spatial Interaction Between Tumor-Infiltrating Lymphocytes and Tumors,,Table 1 .,
Multi-modality Contrastive Learning for Sarcopenia Screening from Hip X-rays and Clinical Information,1,Introduction,"Sarcopenia is a progressive and skeletal muscle disorder associated with loss of muscle mass, strength, and function [1,5,8]. The presence of sarcopenia increases the risk of hospitalization and the cost of care during hospitalization. A systematic analysis of the world's population showed that the prevalence of sarcopenia is approximately 10% in healthy adults over the age of 60 [5,13]. However, the development of sarcopenia is insidious, without overt symptoms in the early stages, which means that the potential number of patients at risk for adverse outcomes is very high. Thus, early identification, screening, and diagnosis are of great necessity to improve treatment outcomes, especially for elderly people.The development of effective, reproducible, and cost-effective algorithms for reliable quantification of muscle mass is critical for diagnosing sarcopenia. However, automatically identifying sarcopenia is a challenging task due to several reasons. First, the subtle contrast between muscle and fat mass in the leg region makes it difficult to recognize sarcopenia from X-ray images. Second, although previous clinical studies [7,11] show that patient information, such as age, gender, education level, smoking and drinking status, physical activity (PA), and body mass index (BMI), is crucial for correct sarcopenia diagnosis, there is no generalizable standard. It is of great importance to develop a computerized predictive model that can fuse and mine diagnostic features from heterogeneous hip X-rays and tabular data containing patient information. Third, the number of previous works on sarcopenia diagnosis is limited, resulting in limited usable data.Deep learning attracted intensive research interests in various medical diagnosis domains [17,18]. For instance, Zhang et al. [19] proposed an attention residual learning CNN model (ARLNet) for skin lesion classification to leverage multiple ARL blocks to tackle the challenge of data insufficiency, inter-class similarities, and intra-class variations. For multi-modality based deep learning, PathomicFusion (PF) [3] fused multimodal histology images and genomic (mutations, CNV, and RNA-Seq) features for survival outcome prediction in an end-to-end manner. Based on PF [3], Braman et al. [2] proposed a deep orthogonal fusion model to combine information from multiparametric MRI exams, biopsy-based modalities, and clinical variables into a comprehensive multimodal risk score. Despite the recent success in various medical imaging analysis tasks [2,3,19], sarcopenia diagnosis by deep learning based algorithms is still under study. To the best of our knowledge, recent work by Ryu et al. [12] is the most relevant to our proposed method. Ryu et al. [12] first used three ensembled deep learning models to test appendicular lean mass (ALM), handgrip strength (HGS), and chair rise test (CRT) performance using chest X-ray images. Then they built machine learning models to aggregate predicted ALM, HGS, and CRT performance values along with basic tabular features to diagnose sarcopenia. However, the major drawback of their work lies in the complex two-stage workflow and the tedious ensemble training. Besides, since sarcopenia is defined by low appendicular muscle mass, measuring muscle wasting through hip X-ray images, which have the greatest proportion of muscle mass, is much more appropriate for screening sarcopenia.In this work, we propose a multi-modality contrastive learning (MM-CL)1 model for sarcopenia diagnosis from hip X-rays and clinical information. Different from Ryu et al.'s model [12], our MM-CL can process multi-modality images and clinical data and screen sarcopenia in an end-to-end fashion. The overall framework is given in Fig. 1. The major components include Non-local CAM Enhancement (NLC), Visual-text Feature Fusion (VFF), and Auxiliary contrastive representation (ACR) modules. Non-local CAM Enhancement enables the network to capture global long-range information and assists the network to concentrate on semantically important regions generated by class activation maps (CAM). Visual-text Feature Fusion encourages the network to improve the multi-modality feature representation ability. Auxiliary contrastive representation utilizes unsupervised learning and thus improves its ability for discriminative representation in the high-level latent space. The main contributions of this paper are summarized as follows. First, we propose a multi-modality contrastive learning model, which enhances the feature representation ability via integrating extra global knowledge, fusing multi-modality information, and joint unsupervised and supervised learning. Second, to address the absence of multi-modality datasets for sarcopenia screening, we select 1,176 patients from the Taipei Municipal Wanfang Hospital. To the best of our knowledge, our dataset is the largest for automated sarcopenia diagnosis from images and tabular information to date. Third, we experimentally show the superiority of the proposed method for predicting sarcopenia from hip X-rays and clinical information."
Multi-modality Contrastive Learning for Sarcopenia Screening from Hip X-rays and Clinical Information,2,Data Collection,"In this retrospective study, we collected anonymized data from patients who underwent sarcopenia examinations at the Taipei Municipal Wanfang Hospital. The data collection was approved by an institutional review board. The demographic and clinical characteristics of this dataset are shown in Table 1. 490 of 1,176 eligible patients who had developed sarcopenia were annotated as positive, while the remaining 686 patients were labeled as negative. The pixel resolution of these images varies from 2266 × 2033 to 3408 × 3408. Each patient's information was collected from a standardized questionnaire, including age, gender, height, weight, BMI, appendicular skeletal muscle index (ASMI), total lean mass, total fat, leg lean mass, and leg fat. We use 5 numerical variables including age, gender, height, weight, and BMI as clinical information for boosting learning as suggested by the surgeon. To the best of our knowledge, this is the largest dataset for automated sarcopenia diagnosis from images and tabular information to date. "
Multi-modality Contrastive Learning for Sarcopenia Screening from Hip X-rays and Clinical Information,3,Methodology,"As shown in Fig. 1, MM-CL consists of three major components. The Non-local CAM Enhancement module is proposed to force the network to learn from attentional spatial regions learned from class activation map (CAM) [20] to enhance the global feature representation ability. Then, we fuse the heterogeneous images and tabular data by integrating clinical variables through a Visual-text Feature Fusion module. Finally, we present an unsupervised contrastive representation learning strategy to assist the supervised screening by Auxiliary contrastive representation."
Multi-modality Contrastive Learning for Sarcopenia Screening from Hip X-rays and Clinical Information,3.1,Non-local CAM Enhancement,"Considering the large proportion of muscle regions in hip X-ray images, capturing longrange dependencies is of great importance for sarcopenia screening. In this work, we adopt the non-local module [16] (NLM) and propose using coarse CAM localization maps as extra information to accelerate learning. We have two hypotheses. First, the long-range dependency of the left and right legs should be well captured; Second, the CAM may highlight part of muscle regions, providing weak supervision to accelerate the convergence of the network. Figure 1(a) shows the overall structure of the Non-local CAM Enhancement.CAM Enhancement: First, each training image X ∈ R 3×H×W is sent to the CAM generator as shown in Fig. 1(a) to generate coarse localization map X m ∈ R 1×H×W . We use the Smooth Grad-CAM++ [10] technique to generate CAM via the ResNet18 [9] architecture. After the corresponding CAM is generated, the training image X is enhanced by its coarse localization map X m via smooth attention to the downstream precise prediction network. The output image X f is obtained as:where sigmoid denotes the Sigmoid function. The downstream main encoder is identical to ResNet18."
Multi-modality Contrastive Learning for Sarcopenia Screening from Hip X-rays and Clinical Information,,Non-local Module:,"Given a hip X-ray image X and the corresponding CAM map X m , we apply the backbone of ResNet18 to extract the high-level feature maps x ∈ R C×H ×W . The feature maps are then treated as inputs for the non-local module. For output xi from position index i, we havewhere concat denotes concatenation, w f is a weight vector that projects the concatenated vector to a scalar, ReLU is the ReLU function, a ij denotes the non-local feature attention that represents correlations between the features at two locations (i.e., x i and x j ), θ, φ, and g are mapping functions as shown in Fig. 1(a)."
Multi-modality Contrastive Learning for Sarcopenia Screening from Hip X-rays and Clinical Information,3.2,Visual-Text Feature Fusion,"After capturing the global information, we aim to fuse the visual and text features in the high-level latent space. We hypothesize that the clinical data may have a positive effect to boost the visual prediction performance. The overall structure of this strategy is given in Fig. 1(b). We extract the clinical features using a simple network, termed as TextNet. Finally, we propose a visual-text fusion module inspired by self-attention [15] to fuse the concatenated visual-text features."
Multi-modality Contrastive Learning for Sarcopenia Screening from Hip X-rays and Clinical Information,,Visual-Text Fusion Module:,"In order to learn from clinical data, we first encode 5 numerical variables as a vector and send it to TextNet. As shown in Fig. 1(b), TextNet consists of two linear layers, a batch normalization layer, and a sigmoid linear unit (SiLU) layer. We then expand and reshape the output feature xt ∈ R C t of TextNet to fit the size of C × H × W . The text and visual representations are then concatenated as xvt = concat(x, reshape(x t )) before sending it to the visual-text fusion module, where x ∈ R C×H ×W denotes the output features from Non-local CAM Enhancement. Feature vector xvt i ∈ R C vt encodes information about the combination of a specific location i in image and text features with C vt = C + C t . The visual-text self-attention module first produces a set of query, key, and value by 1 × 1 convolutional transformations asand W v are part of the model parameters to be learned. We compute the visual-text self-attentive feature ẑvt i at position i asThe softmax operation indicates the attention across each visual and text pair in the multi-modality feature."
Multi-modality Contrastive Learning for Sarcopenia Screening from Hip X-rays and Clinical Information,3.3,Auxiliary Contrastive Representation,"Inspired by unsupervised representation learning [4], we present a contrastive representation learning strategy that encourages the supervised model to pull similar data samples close to each other and push the different data samples away in the high-level embedding space. By such means, the feature representation ability in the embedding space could be further improved.During the training stage, given N samples in a mini-batch, we obtain 2N samples by applying different augmentations (AutoAugment [6]) on each sample. Two augmented samples from the same sample are regarded as positive pairs, and others are treated as negative pairs. Thus, we have a positive sample and 2N -2 negative samples for each patch. We apply global average pooling and linear transformations (Projection Head in Fig. 1(c)) to the visual-text embeddings ẑvt in sequence, and obtain transformed features ôvt . Let ôvt+ and ôvtdenote the positive and negative embeddings of ôvt , the formula of contrastive loss is defined aswhere N is the set of negative counterparts of ôvt , the sim(•, •) is the cosine similarity between two representations, and τ is the temperature scaling parameter. Note that all the visual-text embeddings in the loss function are 2 -normalized. Finally, we integrate the auxiliary contrastive learning branch into the main Classification Head as shown in Fig. 1(c), which is a set of linear layers. We use weighted cross-entropy loss L cls as our classification loss. The overall loss function is calculated as L total = L cls + βL vtcl , where β is a weight factor."
Multi-modality Contrastive Learning for Sarcopenia Screening from Hip X-rays and Clinical Information,4,Experiments and Results,
Multi-modality Contrastive Learning for Sarcopenia Screening from Hip X-rays and Clinical Information,4.1,Implementation Details and Evaluation Measures,"Our method is implemented in PyTorch using an NVIDIA RTX 3090 graphic card. We set the batch size to 32. Adam optimizer is used with a polynomial learning rate policy, where the initial learning rate 2.5 × 10 -4 is multiplied by 1 -epoch total_epoch power with power as 0.9. The total number of training epochs is set to 100, and early stopping is adopted to avoid overfitting. Weight factor β is set to 0.01. The temperature constant τ is set to 0.5. Visual images are cropped to 4/5 of the original height and resized to 224 × 224 after different online augmentation. The backbone is initialized with the weights pretrained on ImageNet.Extensive 5-fold cross-validation is conducted for sarcopenia diagnosis. We report the diagnosis performance using comprehensive quantitative metrics including area under the receiver operating characteristic curve (AUC), F1 score (F1), accuracy (ACC), sensitivity (SEN), specificity (SPC), and precision (PRE)."
Multi-modality Contrastive Learning for Sarcopenia Screening from Hip X-rays and Clinical Information,4.2,Quantitative and Qualitative Comparison,"We implement several state-of-the-art single-modality (ResNet, ARLNet, MaxNet [2], Support vector machine (SVM), and K-nearest neighbors(KNN)) and multi-modality methods (PF [3]) to demonstrate the effectiveness of our MM-CL. For a fair comparison, we use the same training settings.  When compared to the single-modality models, MM-CL outperforms state-of-the-art approaches by at least 6% on ACC. Among all the single-modality models, MaxNet [2], SVM, and KNN gain better results than image-only models. When compared to the multi-modality models, MM-CL also performs better than these methods by a large margin, which proves the effectiveness of our proposed modules.We further visualize the AUC-ROC and Precision-Recall curves to intuitively show the improved performance. As shown in Fig. 2, the MM-CL achieves the best AUC and average precision (AP), which demonstrates the effectiveness of the proposed MM-CL.We have three observations: (1) Multi-modality based models outperform singlemodality based methods, and we explain this finding that multiple modalities complement each other with useful information. (2) MaxNet [2] gains worse results than traditional machine learning methods. One primary reason is that MaxNet contains a large number of parameters to be learned, the tabular information only includes 5 factors, which could result in overfitting. (3) With the help of NLC, VFF, and ACR, our MM-CL achieves substantial improvement over all the other methods. We also conduct ablation studies to validate each proposed component i.e., NLC, VFF, and ACR. CAM/NLM of NLC denotes the CAM enhancement/non-local module. Results are shown in Table 3. Utilizing CAM in the network as an enhancement for optimization improves 0.93% for average ACC, when compared to the baseline model (ResNet18). Meanwhile, capturing long-range dependencies via NLM brings improvement on AUC, ACC, F1, and SEN. Equipped with the text information via VFF, our method can lead to significant performance gains on ACC compared with image-only experiments, e.g., 79.16% vs. 73.80%. Lastly, applying ACR to the network improves the average ACC score from 79.16% to 79.93%. We also visualize the ability of feature representation in the high-level semantic latent feature space before the final classification via t-SNE [14]. As can be seen in Fig. 3, by gradually adding the proposed modules, the feature representation ability of our model becomes more and more powerful, and the high-level features are better clustered."
Multi-modality Contrastive Learning for Sarcopenia Screening from Hip X-rays and Clinical Information,4.3,Ablation Study of the Proposed Method,"Our first finding is that fusing visual and text knowledge brings significant improvement, which demonstrates that the extra tabular information could help substantially in learning. Second, incorporating unsupervised contrastive learning in the supervised learning framework could also improve the feature representation ability of the model."
Multi-modality Contrastive Learning for Sarcopenia Screening from Hip X-rays and Clinical Information,5,Conclusions,"In conclusion, we propose a multi-modality contrastive learning model for sarcopenia screening using hip X-ray images and clinical information. The proposed model consists of a Non-local CAM Enhancement module, a Visual-text Feature Fusion module, and an Auxiliary contrastive representation for improving the feature representation ability of the network. Moreover, we collect a large dataset for screening sarcopenia from heterogeneous data. Comprehensive experiments and explanations demonstrate the superiority of the proposed method. Our future work includes the extension of our approach to other multi-modality diagnosis tasks in the medical imaging domain."
Multi-modality Contrastive Learning for Sarcopenia Screening from Hip X-rays and Clinical Information,,Fig. 1 .,
Multi-modality Contrastive Learning for Sarcopenia Screening from Hip X-rays and Clinical Information,,Fig. 2 .,
Multi-modality Contrastive Learning for Sarcopenia Screening from Hip X-rays and Clinical Information,,Fig. 3 .,
Multi-modality Contrastive Learning for Sarcopenia Screening from Hip X-rays and Clinical Information,,Table 1 .,
Multi-modality Contrastive Learning for Sarcopenia Screening from Hip X-rays and Clinical Information,,Table 2 .,
Multi-modality Contrastive Learning for Sarcopenia Screening from Hip X-rays and Clinical Information,,Table 2 outlines,
Multi-modality Contrastive Learning for Sarcopenia Screening from Hip X-rays and Clinical Information,,Table 3 .,
Gene-Induced Multimodal Pre-training for Image-Omic Classification,1,Introduction,"Pathological image-omic analysis is the cornerstone of modern medicine and demonstrates promise in a variety of different tasks such as cancer diagnosis and prognosis [12]. With the recent advance of digital pathology and sequencing technologies, modern cancer screening has jointly incorporated genomics and histology analysis of whole slide images (WSIs).Though deep learning techniques have revolutionized medical imaging, designing a task-specific algorithm for image-omic multi-modality analysis is challenging. (1) The gigapixel WSIs, which generally yield 15,000 foreground patches during pre-processing, make attention-based backbones [6] hard to extract precise image (WSI)-level representations. (2) Learning features from genomics data which have tens of thousands of genes make models such as Transformer [16] impractical to use due to its quadratic computation complexity. (3) Image-omic feature fusion [2,3] may fail to model high-order relevance and the inherent structural characteristics of each modality, making the fusion less effective.Specifically, to our knowledge, most multi-modality techniques have been designed for modalities such as chest X-ray and reports [1,17,23], CT and X-ray [18], CT and MRI [21], H&E cross-staining [22] via global feature, local feature or multi-granularity alignment. But, none of these works considers the challenges in WSIs and genes processing. Besides, vision-language models in the computer vision community stand out for their remarkable versatility [13,14]. Nevertheless, constrained by computing resources, the most commonly used multimodal representation learning strategy, contrastive learning, which relies on a large number of negative samples to avoid model collapse [8], is not affordable for gigapixel WSIs analysis. A big domain gap also hampers their usage in leveraging the structural characteristic of tumor micro-environment and genomic assay. Recently, the literature corpus has proposed some methods for accomplishing specific image-omic tasks via Kronecker Product fusion [2] or co-attention mapping between WSIs and genomics data [3]. But, the Kronecker product overly concerns feature interactions between modalities while ignoring high-order relevance, w.r.t. decision boundaries across multiple samples, which is critical to classification tasks. As for the co-attention module, it is unidirectional and cannot localize significant regions from genetic data with a large amount of information.In this paper, we propose a task-specific framework dubbed Gene-induced Multimodal Pre-training (GiMP) for image-omic classification. Concretely, we first propose a transformer-based gene encoder, Group Multi-head Self Attention (GroupMSA), to capture global structured features in gene expression cohorts. Next, we design a pre-training paradigm for WSIs, Masked Patch Modeling (MPM), masking random patch embeddings from a fixed-length contiguous subsequence of a WSI. We assume that one patch-level feature embedding can be reconstructed by its adjacent patches, and this process enhances the learning ability for pathological characteristics of different tissues. Our MPM only needs to recover the masked patch embeddings in a fixed-length subsequence rather than processing all patches from WSIs. Furthermore, to model the high-order relevance of the two modalities, we combine CLS tokens of paired image and genomic data to form unified representations and propose a triplet learning module to differentiate patient-level positive and negative samples in a mini-batch. It is worth mentioning that although our unified representation fuses features from the whole gene expression cohort and partial WSIs in a mini-batch, we can still learn high-order relevance and discriminative patient-level information between these two modalities in pre-training thanks to the triplet learning module. In addition, note that our proposed method is different from self-supervised pre-training. Specifically, we focus not only on superior representation learning capability, but also category-related feature distributions, w.r.t. intra-and inter-class variation. With the training process going on, complete information from WSIs can be integrated and the fused multimodal representations with high discrimination will make it easier for the classifier to find the classification hyperplane. Experimental results demonstrate that our GiMP achieves significant improvement in accuracy than other image-omic competitors, and our multimodal framework shows competitive performance even without pre-training."
Gene-Induced Multimodal Pre-training for Image-Omic Classification,2,Method,"Given a multimodal dataset D consisting of pairs of WSI pathological images and genomic data (X I , X G ), our GiMP learns feature representations via accomplishing masked patch modeling and triplets learning. As shown in Fig. 1, the overall framework consists of three parts: 1) group-based genetic encoder GroupMSA (Sect. 2.1), 2) efficient patch aggregator (Sect. 2.2) and 3) gene-induced multimodal fusion (Sect. 2.3). In the subsequent sections, we will introduce each part of our proposed framework in detail."
Gene-Induced Multimodal Pre-training for Image-Omic Classification,2.1,Group Multi-head Self Attention,"In this section, we propose Group Multi-head Self Attention (GroupMSA), a specialized gene encoder to capture structured features in genomic data cohorts. Specifically, inspired by tokenisation techniques in natural language processing [16], the input expression cohort X G ∈ R Nge is partitioned into N f nonoverlapping fragments, and we then use a linear projection head to acquire fragment features H f ∈ R N f ×d , where d is the hidden dimension. Next, we introduce an intra-and-inter attention module to capture local and global information in H f . Firstly, the fragment features are divided into groups and there are N gr learnable group tokens linked to each group resulting in (N f /N gr + 1) tokens per group. Then the prepared tokens are fed to a vanilla multi-head self-attention (MSA) block to extract intra-group information. After that, we model cross-group interactions by another MSA layer on the global scale with the locally learned group tokens and a final classification token CLS ge ∈ R d . Finally, GroupMSA could learn dense semantics from the genomic data cohort."
Gene-Induced Multimodal Pre-training for Image-Omic Classification,2.2,Patch Aggregator with Efficient Attention Operation,"Let's denote the whole slide pathological image with H×W spatial resolution and C channels by X I ∈ R H×W ×C . We follow the preprocessing strategy of CLAM [11] to acquire patch-level embedding sequence, i.e., each foreground patch with 256×256 pixels is fed into an ImageNet-pretrained ResNet50 and the background region is discarded. Let H p = h j | h j ∈ R 1024 Np j=1 denote the sequence of patch embeddings corresponding to WSI X I and note that the total patch number N p is image-specific. Since the quadratic computational complexity of the standard self-attention mechanism is usually unaffordable in WSI analysis due to its long instances sequence, we employ Nystrom-based attention algorithm [20] to aggregate patch embeddings and yield image-level predictions. Specifically, the input sequence H p is first embedded into a d-dimensional feature space and combined with a classification token CLS img , yielding H 0 p ∈ R (Np+1)×d . Then we perform different projection operations on H 0 p :are downsampling matrices obtained from clustering tokens in Q l and K l for layer l ∈ {0, 1}."
Gene-Induced Multimodal Pre-training for Image-Omic Classification,2.3,Gene-Induced Multimodal Fusion,"In this section, we first describe the formulation of masked patch modeling. Then we introduce the overall pipeline of our pre-training framework and illustrate how to apply it to downstream classification tasks.Masked Patch Modeling. In WSIs, the foreground patches are spatially contiguous, which means the adjacent patches have similar feature embeddings. Thus, we propose a Masked Patch Modeling (MPM) pre-training strategy that masks random patch embeddings from a fixed-length contiguous subsequencej=i in H p and reconstruct the invisible information. The fixed subsequence length L is empirically set to 6,000 and the sequences shorter than L are duplicated to build mini batches. Besides, the masking ratio is set to 50% and the set of masked subscripts is denoted as M ∈ R 0.5L . Next, a two-layer Nystrom-based patch aggregator followed by a lightweight reconstruction decoder are adopted to process the masked sequence H mpm and the reconstructed sequence is denoted as. Note that we reconstruct the missing feature embeddings rather than the raw pixels of the masked areas, which is different from traditional MIM methods like SimMIM [19] and MAE [5]. In this way, the model could consider latent pathological characteristics of different tissues, which makes the pretext task more challenging. The reconstruction L 1 loss is computed by:where 1[•] is the indicator function.Gene-Induced Triplet Learning. The transformer-based backbones in the classification task require the CLS token to be able to extract accurate global information, which is even more important yet difficult in WSIs due to the long sequence challenge. In addition, in order to construct the mini-batch, the subsequences we intercept in the MPM pre-training phase may not be sufficiently representative of the image-level characteristics. To overcome these issues, we further propose a gene-induced triplet learning module, which uses pathological images and genomic data as input and extracts high-order and discriminative features via CLS tokens. Firstly, we pre-train the GroupMSA module by patientlevel annotations in advance and froze it in the following iterations. Next, a learnable CLS token CLS img for WSIs is added to the input masked sequence H mpm . After extracting the input patch embeddings and gene sequence separately, we concatenate CLS img and CLS ge as CLS pat ∈ R 2d to represent patient-level characteristics.Suppose we obtain a triplet list {x, x + , x -} during current iteration, where x, x + , x -are concatenated tokens of anchor CLS pat , positive CLS pat , and negative CLS pat , respectively. To enhance the global modeling capability, i.e., extracting more precise patient-level features, we expect that the distance between the anchor and the positive sample gets closer, while the negative sample is farther away. The loss function for optimizing triplet learning is computed by:δ indicates a threshold, e.g., δ = 0.8. Finally, the loss function for GiMP pretraining is:Multimodal Fine-Tuning. Applying the pre-trained backbone to image-omic classification task is straightforward, since GiMP pre-training allows it to learn representative patient-level features. We use a simple Multi-Layer Perceptron (MLP) head to map CLS pat to the final class predictions P , which can be written as P = softmax(MLP(CLS pat )).3 Experiments"
Gene-Induced Multimodal Pre-training for Image-Omic Classification,3.1,Experimental Setup,"Datasets. We verify the effectiveness of our method on The Caner Genome Atlas (TCGA) non-small cell lung cancer (NSCLC) dataset, which contains two cancer subtypes, i.e., Lung Squamous Cell Carcinoma (LUSC) and Lung Adenocarcinoma (LUAD). After pre-processing [11], the patch number extracted from WSIs at 20× magnification varies from 485 to 148,569. We collect corresponding RNA-seq FPKM data for each patient and the length of the input genomic sequence is 60,480. Among 946 image-omic pairs, 470 of them belong to LUAD and 476 cases are LUSC. We randomly split the data into 567 for training, 189 for validation and 190 for testing.Implementation Details. The pre-training process of all algorithms is conducted on the training set, without any extra data augmentation. Note that our genetic encoder, GroupMSA, is fully supervised pre-trained on unimodal genetic data to accelerate convergence and it is frozen during GiMP training process. The maximum pre-training epoch for all methods is set to 100 and we finetune the models at the last epoch. During fine-tuning, we evaluate the model on the validation set after every epoch, and save the parameters when it performs the best. AdamW [10] is used as our optimizer and the learning rate is 10 -4 with cosine decline strategy. The maximum number of fine-tune epoch is 70. At last, we measure the performance on the test set. Training configurations are consistent throughout the fine-tuning process to ensure fair comparisons. All experiments are conducted on a single NVIDIA GeForce RTX 3090."
Gene-Induced Multimodal Pre-training for Image-Omic Classification,3.2,Comparison Between GiMP and Other Methods,"We conduct comparisons between GiMP and three competitors under different settings. Firstly, we compare our proposed patch aggregator with the current state-of-the-art deep MIL models on unimodal TCGA-NSCLC dataset, i.e., only pathological WSIs are included as input. As shown in Table 1, our proposed patch aggregator outperforms all the compared attention based multiple instance learning baselines in classification accuracy. In particular, 1.6% higher than the ABMIL [6] 0.7737 DSMIL [9] 0.7566 CLAM-SB [11] 0.8519 CLAM-MB [11] 0.8889 TransMIL [15] 0.8836 Pathology w/o pre-train GiMP (w/o GroupMSA) 0.8995 PORPOISE [4] 0.9524 Pathomic Fusion [2] 0.9684 MCAT [3] 0.9632 w/o pre-train GiMP (ours) 0.9737 MGCA [17] 0.9105 BioViL [1] 0.9316 REFERS [23] 0 second best compared method TransMIL [15]. We then explore the superiority of GiMP by comparing to state-of-the-art medical multi-modal approaches. We particularly compare our method to BioViL [1], MGCA [17] and REFERS [23], three popular multimodal pre-training algorithms in medical text-image classification task. We can observe in the table that, our GiMP raises ACC from 91.05% to 99.47% on TCGA-NSCLC dataset. Even without pre-training stage, GiMP shows competitive performance compared to PORPOISE [4], Pathomic Fusion [2], and MCAT [3], three influential image-omic classification architectures. We further explore why GiMP works by insightful interpretation of the proposed method with t-SNE visualisation. Figure 2 shows the feature mixtureness of pre-trained CLS pat extracting global information on training set. Compari- son between Fig. 2 (a) and (b) indicates that the addition of the genomic data is indispensable in increasing the inter-class distance and reducing the intra-class distance, which confirms our motivation that gene-induced multimodal fusion could model high-order relevance and yield more discriminative representations. Moreover, compared to the mentioned self-supervised methods BioViL [1] and MGCA [17] in Fig. 2 (c) and (d), CLS pat with GiMP pre-trained are well separated between LUAD and LUSC, i.e., GiMP pays more attention to the categoryrelated feature distribution and could extract more discriminative patient-level features during triplet learning."
Gene-Induced Multimodal Pre-training for Image-Omic Classification,3.3,Ablation Study,"Table 2 summarizes the results of ablation study. We first evaluate the effectiveness of the proposed GroupMSA. In the first two rows, GroupMSA achieves 0.53% improvement compared to SNN [7], a popular genetic encoders used in PORPOISE [4] and Pathomic Fusion [2]. We then analyze the effect of adding genetic modality during pre-training. The evaluation protocol is first pre-training, and then fine-tuning on downstream multimodal classification task. ""Aggregator + MPM"" means GiMP only uses WSIs as input and reconstructs the missing patch embeddings during the pre-training phase. Since the fixed subsequence length L = 6000 is used in our setting, it is sometimes smaller than the original patch number, e.g., the maximum size 148,569, the pre-trained model without genetic guidance may be not aware of sufficiently accurate patientlevel characteristics, i.e., ineffectively focused on normal tissues. ""Aggregator + Triplet"" indicates using unimodal image features to build triplets. We can likewise find that the lack of precise global representation leads to worse performance. Finally, we evaluate the necessity of the MPM module. ""Aggregator + GroupMSA + Triplet"" means GiMP only combines the CLS tokens of each modality and calculates triplet loss during pre-training. We can observe a performance drop without MPM module, e.g., from 99.47% to 95.26%, which demonstrates that local pathological information is equally critical as high-order relevance."
Gene-Induced Multimodal Pre-training for Image-Omic Classification,4,Conclusion,"In this paper, we propose a novel multimodal pre-training method to exploit the complementary relationship of genomic data and pathological images. Concretely, we introduce a genetic encoder with structured learning capabilities and an effective gene-induced multimodal fusion module which combines two pretraining objectives, triplet learning and masked patch modeling. Experimental results demonstrate the superior performance of the proposed GiMP compared to other state-of-the-art methods. The contribution of each proposed component of GiMP is also demonstrated in the experiments."
Gene-Induced Multimodal Pre-training for Image-Omic Classification,,Fig. 1 .,
Gene-Induced Multimodal Pre-training for Image-Omic Classification,,Fig. 2 .,
Gene-Induced Multimodal Pre-training for Image-Omic Classification,,Table 1 .,
Gene-Induced Multimodal Pre-training for Image-Omic Classification,,Table 2 .,
Gene-Induced Multimodal Pre-training for Image-Omic Classification,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2_49.
Histopathology Image Classification Using Deep Manifold Contrastive Learning,1,Introduction,"Whole slide image (WSI) classification is a crucial process to diagnose diseases in digital pathology. Owing to the huge size of a WSI, the conventional WSI classification process consists of patch decomposition and per-patch classification, followed by the aggregation of per-patch results using multiple instance learning (MIL) for the final per-slide decision [7]. MIL constructs bag-of-features (BoF) that effectively handles imperfect patch labels, allowing weakly supervised learning using per-slide labels for WSI classification. Although MIL does not require perfect per-patch label assignment, it is important to construct good feature vectors that are easily separated into different classes to make the classification more accurate. Therefore, extensive research has been conducted on metric and representation learning [11,13] aimed at developing improved feature representation. Recently, contrastive learning has demonstrated its robustness in the representational ability of the feature extractor, which employs self-supervised learning with a contrastive loss that forces samples from the same class to stay closer in the feature space (and vice versa). SimCLR [3] introduced the utilization of data augmentation and a learnable nonlinear transformation between the feature embedding and the contrastive loss to generally improve the quality of feature embedding. MoCo [6] employed a dynamic dictionary along with a momentum encoder in the contrastive learning model to serve as an alternative to the supervised pre-trained ImageNet model in various computer vision tasks. PCL [9] and HCSC [5] integrated the k-means clustering and contrastive learning model by introducing prototypes as latent variables and assigning each sample to multiple prototypes to learn the hierarchical semantic structure of the dataset. These prior works used cosine distance as their distance measurement, which computes the angle between two feature vectors as shown in Fig. 1(a). Although cosine distance is a commonly used distance metric in contrastive learning, we observed that the cosine distance approximates the difference between local neighbors and is insufficient to represent the distance between far-away points on a complicated, nonlinear manifold.The main motivation of this work is to extend the current contrastive learning to represent the nonlinear feature manifold inspired by manifold learning. Owing to the manifold distribution hypothesis [8], the relative distance between high-dimensional data is preserved on a low-dimensional manifold. ISOMAP [12] is a well-known manifold learning approach that represents the manifold structure by using geodesic distance (i.e., the shortest path length between points on the manifold). There are several previous works that use manifold learning for image classification and reconstruction tasks, such as Lu et al. [10] and Zhu et al. [14]. However, the use of geodesic distance on the feature manifold for image classification is a recent development. Aziere et al. [2] applied the random walk algorithm on the nearest neighbor graph to compute the pairwise geodesic distance and proposed the N-pair loss to maximize the similarity between samples from the same class for image retrieval and clustering applications. Gong et al. [4] employed the geodesic distance computed using the Dijkstra algorithm on the knearest neighbor graph to measure the correlation between the original samples and then further divided each class into sub-classes to deal with the problems of high spectral dimension and channel redundancy in the hyperspectral images. However, this method captured the nonlinear data manifold structure on the original data (not on the feature vectors) only once at the beginning stage, which is not updated in the further training process.In this study, we propose a hybrid method that combines manifold learning and contrastive learning to generate a good feature extractor (encoder) for histopathology image classification. Our method uses the sub-classes and prototypes as in conventional contrastive learning, but we propose the use of geodesic distance in generating the sub-classes to represent the non-linear feature manifold more accurately. By doing this, we achieve better separation between features with large margins, resulting in improved MIL classification performance. The main contributions of our work can be summarized as follows:-We introduce a novel integration of manifold geodesic distance in contrastive learning, which results in better feature representation for the non-linear feature manifold. We demonstrate that the proposed method outperforms conventional cosine-distance-based contrastive learning methods. -We propose a geodesic-distance-based feature clustering for efficient contrastive loss evaluation using prototypes without brute-force pairwise feature similarity comparison while approximating the overall manifold geometry well, which results in reduced computation. -We demonstrate that the proposed method outperforms other state-of-theart (SOTA) methods with a much smaller number of sub-classes without complicated prototype assignment (e.g., hierarchical clustering).To the best of our knowledge, this work is the first attempt to leverage manifold geodesic distance in contrastive learning for histopathology WSI classification."
Histopathology Image Classification Using Deep Manifold Contrastive Learning,2,Method,The overview of our proposed model is illustrated in Fig. 2. It is composed of two stages: (1) train the feature extractor using deep manifold embedding learning and (2) train the WSI classifier using the deep manifold embedding extracted from the first stage. The input WSIs are pre-processed to extract 256 × 256 × 3 dimensional patches from the tumor area at a 10× magnification level. Patches with less than 50% tissue coverage are excluded from the experiment.
Histopathology Image Classification Using Deep Manifold Contrastive Learning,2.1,Deep Manifold Embedding Learning,"As illustrated in Fig. 2(a), we first feed the patches into a feature extractor f , which is composed of an encoder, a pooling layer, and a multi-perceptron layer.The output is then passed through two different paths, namely, deep manifold and softmax paths. Loss Functions. For the deep manifold training, we adopted two losses: (1) intra-subclass loss L intra and (2) inter-subclass loss L inter . The main idea in intra-subclass loss is to make the samples from the same sub-class stay near their respective sub-class prototype. L intra is formulated as follows:where x i j is the i-th patch in the j-th batch, J represents the total number of batches, I represents the total number of patches per batch, f (•) is the feature extractor, and p + indicates the positive prototype of the patch (i.e., the prototype of the subclass containing x i j ). The prototype of each sub-class is computed by simply taking the mean of all the patch features that belong to each sub-class. Inter-subclass loss L inter is proposed to make the sub-classes from a different class far apart from one another. The formulation of L inter is as shown below:where Another path via softmax is simply trained on outputs from the feature extractor with the ground truth slide-level labels y by the cross-entropy loss L CE , which is defined as follows:where y is the ground truth slide-level label and ŷ is predicted label. Finally, the total loss for the first stage is defined as follows:"
Histopathology Image Classification Using Deep Manifold Contrastive Learning,2.2,MIL Classification,"As illustrated in Fig. 2(b), in the second stage, the pre-trained feature extractor from the previous stage is then deployed to extract features for bag generation. A total of 50 bags are generated for each WSI, in which each bag is composed of the concatenation of the features from 100 patches in 512 dimensions. These bags are fed into a classifier with two layers of multiple perceptron layers (512 neurons) and a Softmax layer and then trained with a binary cross-entropy loss.After the classification, majority voting is applied to the predicted labels of the bags to derive the final predicted label for each WSI."
Histopathology Image Classification Using Deep Manifold Contrastive Learning,3,Result,
Histopathology Image Classification Using Deep Manifold Contrastive Learning,3.1,Datasets,"We tested our proposed method on two different tasks: (1) intrahepatic cholangiocarcinomas(IHCCs) subtype classification and (2) liver cancer type classification. The dataset for the former task was collected from 168 patients with 332 WSIs from Seoul National University hospital. IHCCs can be further categorized into small duct type (SDT) and large duct type (LDT). Using gene mutation information as prior knowledge, we collected WSIs with wild KRAS and mutated IDH genes for use as training samples in SDT, and WSIs with mutated KRAS and wild IDH genes for use in LDT. The rest of the WSIs were used as testing samples. The liver cancer dataset for the latter task was composed of 323 WSIs, in which the WSIs can be further classified into hepatocellular carcinomas (HCCs) (collected from Pathology AI Platform [1]) and IHCCs. We collected 121 WSIs for the training set, and the remaining WSIs were used as the testing set."
Histopathology Image Classification Using Deep Manifold Contrastive Learning,3.2,Implementation Detail,"We used a pre-trained VGG16 with ImageNet as the initial encoder, which was further modified via deep manifold model training using the proposed manifold and cross-entropy loss functions. The number of nearest neighbors k and the number of sub-classes n were set to 5 and 10, respectively. In the deep manifold embedding learning model, the learning rates were set to 1e-4 with a decay rate of 1e-6 for the IHCCs subtype classification and to 1e-5 with a decay rate of 1e-8 for the liver cancer type classification. The k-nearest neighbors graph and the geodesic distance matrix are updated once every five training epochs, which is empirically chosen to balance running time and accuracy. To train the MIL classifier, we set the learning rate to 1e-3 and the decay rate to 1e-6. We used batch sizes 64 and 4 for training the deep manifold embedding learning model and the MIL classification model, respectively. The number of epochs for the deep manifold embedding learning model was 50, while 50 and 200 epochs for the IHCCs subtype classification and liver cancer type classification, respectively. As for the optimizer, we used stochastic gradient decay for both stages. The result shown in the tables is the average result from 10 iterations of the MIL classification model."
Histopathology Image Classification Using Deep Manifold Contrastive Learning,3.3,Experimental Results,"The performance of different models from two different datasets is reported in this section. For the baseline model, we chose the pre-trained VGG16 feature extractor with an MIL classifier, which is the same as our proposed model except that the encoder is retrained using the proposed loss. Two SOTA methods using contrastive learning and clustering, PCL [9] and HCSC [5], are compared with our method in this study. The MIL classification result of the IHCCs subtype classification is shown in Table 1. Our proposed method outperformed the baseline CNN by about 4% increment in accuracy, precision, recall, and F1 score. Note that our method only used 20 sub-classes but outperformed PCL (using 2300 sub-classes) by 4% and HCSC (using 112 sub-classes) by 5% in accuracy. The result of liver cancer type classification is also shown in Table 1. Our method achieved about 5% improvement in accuracy against the baseline and 1% to 2% improvement in accuracy against the SOTA methods. Moreover, it outperformed the SOTA methods with far fewer prototypes and without complicated hierarchical prototype assignments. To further evaluate the effect of prototypes, we conducted an ablation study for different prototype assignment strategies as shown in Table 2. Here, global prototypes imply assigning a single prototype per class while local prototypes imply assigning multiple prototypes per class (one per sub-class). When both are used together, it implies a hierarchical prototype assignment where local prototypes interact with the corresponding global prototype. As shown in this result, the model with local prototypes only performed about 4% higher than did the model with global prototypes only. Meanwhile, the combination of both prototypes achieved a similar performance to that of the model with local prototypes only. Since the hierarchical (global + local) assignment did not show a significant improvement but instead increased computation, we used only local sub-class prototypes in our final experiment setting.  1)-( 4) are the patches from SDT and ( 5)-( 8) are the patches from LDT.Since one of our contributions is the use of geodesic distance, we assessed the efficacy of the method by comparing it with the performance using cosine distance, as shown in Table 3. To measure the performance of the cosine-distancebased method, we simply replaced our proposed manifold loss with NT-Xent loss [3], which uses cosine distance in their feature similarity measurement. Two cosine distance experiments were conducted as follows: (1) use only their groundtruth class without further dividing the samples into sub-classes (i.e., global prototypes) and (2) divide the samples from each class into 10 sub-classes by using k-means clustering (i.e., local prototypes). As shown in Table 3, using multiple local prototypes shows slightly better performance compared to using global prototypes. By switching the NT-Xent loss with our geodesic-based manifold loss, the overall performance is increased by about 2%. Figure 3 visually compares the effect of the geodesic and cosine distance-based losses. Two scatter plots are t-SNE projections of feature vectors from the encoders trained using geodesic distance and cosine distance, respectively. Red dots represent SDT samples and blue dots represent LDT samples from the IHCCs dataset (corresponding histology thumbnail images are shown on the right). In this example, all eight cases are correctly classified by the method using geodesic distance while all cases are incorrectly classified by the method using cosine distance. It is clearly shown that geodesic distance can correctly measure the feature distance (similarity) on the manifold so that SDT and LDT groups are located far away in the t-SNE plot, whereas cosine distance failed to separate these groups and they are located nearby in the plot."
Histopathology Image Classification Using Deep Manifold Contrastive Learning,3.4,Conclusion and Future Work,"In this paper, we proposed a novel geodesic-distance-based contrastive learning for histopathology image classification. Unlike conventional cosine-distancebased contrastive learning methods, our method can represent nonlinear feature manifold better and generate better discriminative features. One limitation of the proposed method is the extra computation time for graph generation and pairwise distance computation using the Dijkstra algorithm. In the future, we plan to optimize the algorithm and apply our method to other datasets and tasks, such as multi-class classification problems and natural image datasets."
Histopathology Image Classification Using Deep Manifold Contrastive Learning,,Fig. 1 .,
Histopathology Image Classification Using Deep Manifold Contrastive Learning,,Fig. 2 .,
Histopathology Image Classification Using Deep Manifold Contrastive Learning,,,
Histopathology Image Classification Using Deep Manifold Contrastive Learning,,Table 1 .,
Histopathology Image Classification Using Deep Manifold Contrastive Learning,,Table 2 .,
Histopathology Image Classification Using Deep Manifold Contrastive Learning,,Table 3 .,Fig. 3. Comparison of geodesic and cosine distance in feature space.(
Histopathology Image Classification Using Deep Manifold Contrastive Learning,,Acknowledgements,". This study was approved by the institutional review board of Seoul National University Hospital (IRB NO.H-1011-046-339). This work was partially supported by the National Research Foundation of Korea (NRF-2019M3E5D2A01063819, NRF-2021R1A6A1A13044830), the Institute for Information & Communications Technology Planning & Evaluation (IITP-2023-2020-0-01819), the Korea Health Industry Development Institute (HI18C0316), the Korea Institute of Science and Technology (KIST) Institutional Program (2E32210 and 2E32211) and a Korea University Grant."
Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,1,Introduction,"Prostate cancer (PCa) diagnosis and grading rely on histopathology analysis of biopsy slides [1]. However, prostate biopsies are known to have sampling error as PCa is heterogenous and commonly multifocal, meaning cancer legions can be missed during the biopsy procedure [2]. If significant PCa is detected on biopsies and the patient has organ-confined cancer with no contraindications, radical prostatectomy (RP) is the standard of care [3,4]. Following RP, the prostate is processed and slices are mounted onto slides for analysis. Radical prostatectomy histopathology samples are essential for validating the biopsydetermined grade group [5,6]. Analysis of whole-mount slides, meaning slides that include slices of the entire prostate, provide more precise tumor boundary detection, identification of various tumor foci, and increased tissue for identifying morphological patterns not visible on biopsy due to a larger field of view.Field effect refers to the spread of genetic and epigenetic alterations from a primary tumor site to surrounding normal tissues, leading to the formation of secondary tumors. Understanding field effect is essential for cancer research as it provides insights into the mechanisms underlying tumor development and progression. Tumor-associated stroma, which consists of various cell types, such as fibroblasts, smooth muscle cells, and nerve cells, is an integral component of the tumor microenvironment that plays a critical role in tumor development and progression. Reactive stroma, a distinct phenotype of stromal cells, arises in response to signaling pathways from cancerous cells and is characterized by altered stromal cells and increased extracellular matrix components [7,8]. Reactive stroma is often associated with tumor-associated stroma and is thought to be a result of field effects in prostate cancer. Altered stroma can create a pro-tumorigenic environment by producing a multitude of chemokines, growth factors, and releasing reactive oxygen species [9,10], which can lead to tumor development and aggressiveness [11]. Therefore, investigating the histological characterization of tumor-associated stroma is crucial in gaining insights into the field effect and tumor progression of prostate cancer.Manual review for tumor-associated stroma is time-consuming and lacks quantitative metrics [12,13]. Several automated methods have been applied to analyze the tumor-stroma relationship; however, most of them focus on identifying a tumor-stroma ratio rather than finding reactive stroma tissue or require pathologist input. Machine learning algorithms have been used to quantify the percentage of tumor to stroma in bladder cancer patients, but required dichotomizing patients based on a threshold [14]. Software has been used to segment tumor and stroma tissue in breast cancer patient samples, but the method required constant supervision by a pathologist [15]. Similarly, Akoya Biosciences Inform software was used to quantify reactive stroma in PCa, but this method required substantial pathologist input to train the software [16]. Fully automated deep-learning methods have been developed to identify tumor-associated stroma in breast cancer biopsies, achieving an AUC of 0.962 in predicting invasive ductal cancer [13]. However, identifying tumor-associated stroma in prostate biopsies and whole-mount histopathology slides remains challenging.Analyzing tumor-associated stroma in prostate cancer requires combining whole-mount and biopsy histopathology slides. Biopsy slides provide information on the presence of PCa, while whole-mount slides provide information on the extent and distribution of PCa, including more information on tumor-associated stroma. Combining the information from both modalities can provide a more accurate understanding of the tumor microenvironment. In this work, we explore the field effect in prostate cancer by analyzing tumor-associated stroma in multimodal histopathological images. Our main contributions can be summarized as follows:-To the best of our knowledge, we present the first deep-learning approach to characterize prostate tumor-associated stroma by integrating histological image analysis from both whole-mount and biopsy slides. Our research offers a promising computational framework for in-depth exploration of the field effect and cancer progression in prostate cancer. -We proposed a novel approach for stroma classification with spatial graphs modeling, which enable more accurate and efficient analysis of tumor microenvironment in prostate cancer pathology. Given the spatial nature of cancer field effect and tumor microenvironment, our graph-based method offers valuable insights into stroma region analysis. -We developed a comprehensive pipeline for constructing tumor-associated stroma datasets across multiple data sources, and employed adversarial training and neighborhood consistency regularization techniques to learn robust multimodal-invariant image representations."
Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,2,Method,
Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,2.1,Stroma Tissue Segmentation,"Accurately analyzing tumor-associated stroma requires a critical pre-processing step of segmenting stromal tissue from the background, including epithelial tissue. This segmentation task is challenging due to the complex and heterogeneous appearance of the stroma. To address this, we propose utilizing the PointRend model [17], which can handle complex shapes and appearances and produce smooth and accurate segmentations through iterative object boundary refinement. Moreover, the model's efficiency and ability to process large images quickly make it suitable for analyzing whole-mount slides. By leveraging the PointRend model, we can generate stromal segmentation masks for more precise downstream analysis."
Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,2.2,Stroma Classification with Spatial Patch Graphs,"To capture the spatial nature of field effect and analyze tumor-associated stroma, modeling spatial relationships between stroma patches is essential. The spatial relationship can reveal valuable information about the tumor microenvironment, and neighboring stroma cells can undergo similar phenotypic changes in response to cancer. Therefore, we propose using a spatial patch graph to capture the highorder relationship among stroma tissue regions. We construct the stroma patch graph using a K-nearest neighbor (KNN) graph and neighbor sampling. The KNN graph connects each stroma patch to its K nearest neighboring patches. Given a central stroma patch, we iteratively add neighboring patches to construct  the patch graph until we reach a specified layer number L to control the subgraph size. This process results in a tree-like subgraph with each layer representing a different level of spatial proximity to the central patch. The use of neighbor sampling enables efficient processing of large images and allows for stochastic training of the model.To predict tumor-associated binary labels of stroma patches, we employ a message-passing approach that propagates patch features in the spatial graph. To achieve this, we use Graph Convolutional Networks with attention, also known as Graph Attention Networks (GATs) [18]. The GAT uses an attention mechanism on node features to construct a weighting kernel that determines the importance of nodes in the message-passing process. In our case, the patch graph G is constructed using the stroma patches as vertices, and we connect the nodes with edges based on their spatial proximity. Each vertex v i is associated with a feature vector h vi ∈ R N , which is first extracted by Resnet-50 model [19]. The GAT layer is defined aswhere W ∈ R M ×N is a learnable matrix transforming N -dimensional features to M -dimensional features. N E vi is the neighborhood of the node v i connected by E in G. GAT uses attention mechanism to construct the weighting coefficients as:where T represents transposition, is the concatenation operation, and ρ is LeakyReLU function. The final output of GAT module is the tumor-associated probability of the input patch. And the module was optimized using the crossentropy loss L GAT in an end-to-end fashion."
Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,2.3,Neighbor Consistency Regularization for Noisy Labels,"The labeling of tumor-associated stroma can be affected by various factors, which can result in noisy labels. One of the reasons for noisy labels is the irregular distribution of the field effect, which makes it challenging to define a clear boundary between the tumor-associated and normal stroma regions. Additionally, the presence of tumor heterogeneity and the varied distribution of tumor foci can further complicate the labeling process.To address this issue, we propose applying Neighbor Consistency Regularization (NCR) [20] to prevent the model from overfitting to incorrect labels. The assumption is that overfitting happens to a lesser degree before the final classifier, and this is supported by MOIT [21], which suggests that feature representations are capable of distinguishing between noisy and clean examples during model training. Based on this assumption, NCR introduces a neighbor consistency loss to encourage similar predictions of stroma patches that are similar in feature space. This loss penalizes the divergence of a patch prediction from a weighted combination of its neighbors' predictions in feature space, where the weights are determined by their feature similarity. Specifically, the loss function is designed as follows:where D KL is the KL-divergence loss to quantify the discrepancy between two probability distributions, T represents the temperature and NN k (v i ) is the set of k nearest neighbors of v i in the feature space."
Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,2.4,Adversarial Multi-modal Learning,"Biopsy and whole-mount slides provide complementary multi-modal information on the tumor microenvironment, and combining them can provide a more comprehensive understanding of tumor-associated stroma. However, using data from multiple modalities can introduce systematic shifts, which can impact the performance of a deep learning model. Specifically, whole-mount slides typically contain larger tissue sections and are processed using different protocols than biopsy slides, which can result in differences in image quality, brightness, and contrast. These technical differences can affect the pixel intensity distributions of the images, leading to systematic shifts in the features that the deep learning model learns to associate with tumor-associated stroma. For instance, a model trained on whole-mount slides only may not generalize well to biopsy slides due to systematic shifts, hindering model performance in the clinical application scenario.To address the above issues, we propose an Adversarial Multi-modal Learning (AML) module to force the feature extractor to produce multimodal-invariant representations on multiple source images. Specifically, we incorporate a source discriminator adversarial neural network as auxiliary classifier. The module takes the stroma embedding as an input and predicts the source of the image (biopsy or whole-mount) using Multilayer Perceptron (MLP) with cross-entropy loss function L AML . The overall loss function of the entire model is computed as:where hyper-parameters α and β control the impact of each loss term. All modules were concurrently optimized in an end-to-end manner. The stroma classifier and source discriminator are trained simultaneously, aiming to effectively classify tumor-associated stroma while impeding accurate source prediction by the discriminator. The optimization process aims to achieve a balance between these two goals, resulting in an embedding space that encodes as much information as possible about tumor-associated stroma identification while not encoding any information on the data source. By adopting the adversarial learning strategy, our model can maintain the correlated information and shared characteristics between two modalities, which will enhance the model's generalization and robustness."
Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,3,Experiment,
Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,3.1,Dataset,"In our study, we utilized three datasets for tumor-associated stroma analysis.(1) Dataset A comprises 513 tiles extracted from the whole mount slides of 40 patients, sourced from the archives of the Pathology Department at Cedars-Sinai Medical Center (IRB# Pro00029960). It combines two sets of tiles: 224 images from 20 patients featuring stroma, normal glands, low-grade and highgrade cancer [22], along with 289 images from 20 patients with dense high-grade cancer (Gleason grades 4 and 5) and cribriform/non-cribriform glands [23]. Each tile measures 1200×1200 pixels and is extracted from whole slide images captured at 20x magnification (0.5 microns per pixel). The tiles were annotated at the pixel-level by expert pathologists to generate stroma tissue segmentation masks and were cross-evaluated and normalized to account for stain variability.(2) Dataset B included 97 whole mount slides with an average size of over 174,000×142,000 pixels at 40x magnification. The prostate tissue within these slides had an average tumor area proportion of 9%, with an average tumor area of 77 square mm. An expert pathologist annotated the tumor region boundaries at the region-level, providing exhaustive annotations for all tumor foci. (3) Dataset C comprised 6134 negative biopsy slides obtained from 262 patients' biopsy procedures, where all samples were diagnosed as negative. These slides are presumed to contain predominantly normal stroma tissues without phenotypic alterations in response to cancer. Dataset A was utilized for training the stroma segmentation model. Extensive data augmentation techniques, such as image scaling and staining perturbation, were employed during the training process. The model achieved an average test Dice score of 95.57 ± 0.29 through 5-fold cross-validation. This model was then applied to generate stroma masks for all slides in Datasets B and C. To precisely isolate stroma tissues and avoid data bleeding from epithelial tissues, we only extracted patches where over 99.5% of the regions were identified as stroma at 40X magnification to construct the stroma classification dataset.For positive tumor-associated stroma patches, we sampled patches near tumor glands within annotated tumor region boundaries, as we presumed that tumor regions represent zones in which the greatest amount of damage has progressed. For negative stroma patches, we calculated the tumor distance for each patch by measuring the Euclidean distance from the patch center to the nearest edge of the labeled tumor regions. Negative stroma patches were then sampled from whole mount slides with a Gleason Group smaller than 3 and a tumor distance larger than 5 mm. This approach aims to minimize the risk of mislabeling tumor-associated stroma as normal tissue. Setting a 5mm threshold accounts for the typically minimal inflammatory responses induced by prostate cancers, particularly in lower-grade cases. To incorporate multi-modal information, we randomly sampled negative stroma patches from all biopsy slides in Dataset C. Overall, we selected over 1.1 million stroma patches of size 256×256 pixels at 40x magnification for experiments. During model training and testing, we performed stain normalization and standard image augmentation methods."
Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,3.2,Model Training and Evaluation,"For constructing KNN-based patch graphs, we limited the graph size by setting K = 4 and layer number L = 3. We controlled the strength of the NCR and AML terms by setting α = 0.25 and β = 0.5, respectively. The Adam optimizer with a learning rate of 0.0005 was used for model training. All models were implemented using PyTorch on a single Tesla V100 GPU. To evaluate the model performance, we perform 5-fold cross-validation, where all slides are stratified by source origin and divided into 5 subsets. In each cross-validation trial, one subset was taken as the test set while the remaining subsets constituted the training set. We measure the prediction performance using the area under the receiver operating characteristic (AUROC), F1 score, precision, and recall. To evaluate the effectiveness of our proposed method, we conducted an ablation study by comparing the performance of different model variants presented in Table 1. Specifically, the base model is the ResNet-50 feature extractor for tumor-associated stroma classification. Each model variant included a different combination of modules presented in method sections. We systematically add one or more modules to the base model to evaluate their performance contribution. The results show that the full model outperforms the base model by a large margin with 10.04% in AUROC and 10.97% in F1 score, and each module contributes to the overall performance. Compared to the base model, the addition of the GAT module resulted in a significant improvement in all metrics, suggesting spatial information captured by the patch graph was valuable for stroma classification. The most notable performance improvement was achieved by the AML module, with a 5.72% increase in AUROC and 5.55% increase in Recall. This improvement indicates that AML helps the model better capture the multimodal-invariant features that are associated with tumor-associated stroma while reducing the false negative prediction by eliminating the influence of systematic shift cross modalities. Finally, the addition of the NCR module further increased the average model performance and improved the model robustness across 5 folds. This suggests that NCR was effective in handling noisy labels and improving model's generalization ability."
Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,4,Results and Discussions,"In conclusion, our study introduced a deep learning approach to accurately characterize the tumor-associated stroma in multi-modal prostate histopathology slides. Our experimental results demonstrate the feasibility of using deep learning algorithms to identify and quantify subtle stromal alterations, offering a promising tool for discovering new diagnostic and prognostic biomarkers of prostate cancer. Through exploring field effect in prostate cancer, our work provides a computational system for further analysis of tumor development and progression. Future research can focus on validating our approach on larger and more diverse datasets and expanding the method to a patient-level prediction system, ultimately improving prostate cancer diagnosis and treatment."
Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,,Fig. 1 .,
Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,,Fig. 2 .,
Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,,Table 1 .,
Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,,53 ± 0.38 79.26 ± 0.36 78.45 ± 0.49 80.10 ± 0.32,
Multi-scope Analysis Driven Hierarchical Graph Transformer for Whole Slide Image Based Cancer Survival Prediction,1,Introduction,"The ability to predict the future risk of patients with cancer can significantly assist clinical management decisions, such as treatment and monitoring [21]. Generally, pathologists need to manually assess the pathological images obtained by whole-slide scanning systems for clinical decision-making, e.g., cancer diagnosis and prognosis [20]. However, due to the complex morphology and structure of human tissues and the continuum of histologic features phenotyped across the diagnostic spectrum, it is a tedious and time-consuming task to manually assess the whole slide image (WSI) [12]. Moreover, unlike cancer diagnosis and subtyping tasks, survival prediction is a future state prediction task with higher difficulty. Therefore, automated WSI analysis method for survival prediction task is highly demanded yet challenging in clinical practice.Over the years, deep learning has greatly promoted the development of computational pathology, including WSI analysis [9,17,24]. Due to the huge size, WSIs are generally cropped to numerous patches with a fixed size and encoded to patch features by a CNN encoder (e.g., Imagenet pretrained ResNet50 [11]) for further analysis. The attention-dominated learning frameworks (e.g., ABMIL [13], CLAM [18], DSMIL [16], TransMIL [19], SCL-WC [25], HIPT [4], NAGCN [9]) mainly aim to find the key instances (e.g., patches and tissues) for WSI representation and decision-making, which prefers the needle-ina-haystack tasks, e.g., cancer diagnosis, cancer subtyping, etc. To handle cancer survival prediction, some researchers integrated some attribute priors into the network design [5,26]. For example, Patch-GCN [5] treated the WSI as point cloud data, and the patch-level adjacent relationship of WSI is learned by a graph convolutional network (GCN). However, the fixed-size patches cropped from WSI mainly contain single-level biological entities (e.g., cells), resulting in limited structural information. DeepAttnMISL [26] extracted the phenotype patterns of the patient via a clustering algorithm, which provides meaningful medical prior to guide the aggregation of patch features. However, this cluster analysis strategy only focuses on a single sample, which cannot describe the whole picture of the pathological components specific to the cancer type. Additionally, existing learning frameworks often ignore the capture of contextual interactions of pathological components (e.g., tumor, stroma, lymphocyte, etc.), which is considered as important evidence for cancer survival prediction tasks [1,6]. Therefore, WSI-based cancer survival prediction still remains a challenging task.In summary, to better capture the prognosis-related information in WSI, two technical key points should be fully investigated: (1) an analysis strategy to mine more comprehensive and in-depth prior of WSIs, and (2) a promising learning network to explore the contextual interactions of pathological components. To this end, this paper presents a novel multi-scope analysis driven learning framework, called Hierarchical Graph Transformer (HGT), to pertinently resolve the above technical key points for more reliable and interpretable W. Hou and Y. He-Contributed equally to this work. WSI-based survival prediction. First, to mine more comprehensive and in-depth attribute priors of WSI, we propose a multi-scope analysis strategy consisting of in-slide superpixels and cross-slide clustering, which can not only extract the spatial prior but also identify the semantic prior of WSIs. Second, to explore the contextual interactions of pathological components, we design a novel learning network, i.e., HGT, which consists of a hierarchical graph convolution layer and a Transformer-based prediction head. Specifically, based on the extracted spatial topology, the hierarchical graph convolution layer in HGT progressively aggregate the patch-level features to the tissue-level features, so as to learn the topological features of variant microenvironments ranging from fine-grained (e.g., cell) to coarse-grained (e.g., necrosis, epithelium, etc.). Then, under the guidance of the identified semantic prior, the tissue-level features are further sorted and assigned to form the feature embedding of pathological components. Furthermore, the contextual interactions of pathological components are captured with the Transformer-based prediction head, leading to reliable survival prediction and richer interpretability. Extensive experiments on three cancer cohorts (i.e., CRC, TCGA-LIHC and TCGA-KIRC) demonstrates the effectiveness and interpretability of our framework. Our codes are available at https:// github.com/Baeksweety/superpixel transformer."
Multi-scope Analysis Driven Hierarchical Graph Transformer for Whole Slide Image Based Cancer Survival Prediction,2,Methodology,"Figure 1 illustrates the pipeline of the proposed framework. Due to the huge size, WSIs are generally cropped to numerous patches with a fixed size (i.e., 256×256) and encoded to patch features V patch ∈ R n×d in the embedding space D by a CNN encoder (i.e., ImageNet pretrained ResNet50 [11]) for further analysis, where n is the number of patches, d = 1024 is the feature dimension. The goal of WSI-based cancer survival prediction is to learn the feature embedding of V in a supervised manner and output the survival risk O ∈ R 1 .However, conventional patch-level analysis cannot model complex pathological patterns (e.g., tumor lymphocyte infiltration, immune cell composition, etc.), resulting in limited cancer survival prediction performance. To this end, we proposed a novel learning network, i.e., HGT, which utilized the spatial and semantic priors mined by a multi-scope analysis strategy (i.e., in-slide superpixel and cross-slide clustering) to represent and capture the contextual interaction of pathological components. Our framework consists two modules: a hierarchical graph convolutional network and a Transformer-based prediction head."
Multi-scope Analysis Driven Hierarchical Graph Transformer for Whole Slide Image Based Cancer Survival Prediction,2.1,Hierarchical Graph Convolutional Network,"Unlike cancer diagnosis and subtyping, cancer survival prediction is a quite more challenging task, as it is a future event prediction task which needs to consider complex pathological structures [20]. However, the conventional patch-level analysis is difficult to meet this requirement. Therefore, it is essential to extract and combine higher-level topology information for better WSI representation. In-slide Superpixel. As shown in Fig . 1, we first employ a Simple Linear Iterative Clustering (SLIC) [2] algorithm to detect non-overlapping homogeneous tissues of the foreground of WSI at a low magnification, which can be served as the spatial prior to mine the hierarchical topology of WSI. Intuitively, the cropped patches and segmented tissues in a WSI can be considered as hierarchical entities ranging from fine-grained level (e.g., cell) to coarse-grained level (e.g., necrosis, epithelium, etc.). Based on the in-slide superpixel, the tissue adjacency matrix E tissue ∈ R m×m can be obtained, where m denote the number of superpixels. Then, the patches in each superpixel are further connected in an 8-adjacent manner, thus generating patch adjacency matrix E patch ∈ R n×n . The spatial assignment matrix between cropped patches and segmented tissues is denoted asPatch Graph Convolutional Layer. Based on the spatial topology extracted by in-slide superpixel, the patch graph convolutional layer (Patch GCL) is designed to learn the feature of the fine-grained microenvironment (e.g., cell) through the message passing between adjacent patches, which can be represented as:where σ(•) denotes the activation function, such as ReLU. GraphConv denotes the graph convolutional operation, e.g., GCNConv [15], GraphSAGE [10], etc.Tissue Graph Convolutional Layer. Third, based on the spatial assignment matrix A spa , the learned patch-level features can be aggregated to the tissue-level features which contain the information of necrosis, epithelium, etc.where [•] T denote the matrix transpose operation. The tissue graph convolutional layer (Tissue GCL) is further designed to learn the feature of this coarse-grained microenvironment, which can be represented as:"
Multi-scope Analysis Driven Hierarchical Graph Transformer for Whole Slide Image Based Cancer Survival Prediction,2.2,Transformer-Based Prediction Head,"Clinical studies have shown that cancer survival prediction requires considering not only the biological morphology but also the contextual interactions of tumor and surrounding tissues [1]. However, existing analysis frameworks for WSI often ignore the capture of contextual interactions of pathological components (e.g., tumor, stroma, lymphocyte, etc.), resulting in limited performance and interpretability. Therefore, it is necessary to determine the feature embedding of pathological components and investigate their contextual interactions for more reliable predictions.Cross-Slide Clustering. As shown in Fig. 1, we perform the k-means algorithm on the encoded patch features of all training WSIs to generate k pathological components P ∈ R k×d in the embedding space D. P represents different pathological properties specific to the cancer type. Formally, the feature embedding of each tissue in space D is defined as the mean feature embeddings of the patches within the tissue. And then, the pathological component label of each tissue is determined as the component closest to the Euclidean distance of the tissue in space D. The semantic assignment matrix between segmented tissues and pathological components is denoted as A sem ∈ R m×k .Transformer Architecture. Under the guidance of the semantic prior identified by cross-slide clustering, the learned tissue features V tissue can be further aggregated, forming a series meaningful component embeddings P specific to the cancer type.Then we employed a Transformer [22] architecture to mine the contextual interactions of P and output the predicted survival risk. As shown in Fig. 1, P is concatenated with an extra learnable regression token R and attached with positional embeddings E P os , which are processed by:where P out is the output of Transformer, MHSA is the Multi-Headed Self-Attention [22], LN is Layer Normalization and MLP is Multilayer Perceptron. Finally, the representation of the regression token at the output layer of the Transformer, i.e., [P out ] 0 , is served as the predicted survival risk O."
Multi-scope Analysis Driven Hierarchical Graph Transformer for Whole Slide Image Based Cancer Survival Prediction,,Loss Function and Training Strategy.,"For the network training, Cox loss [26] is adopted for the survival prediction task, which is defined as:where δ i denote the censorship of i-th patient, O(i) and O(j) denote the survival output of i-th and j-th patient in a batch, respectively. GPUs. Our graph convolutional model is implemented by Pytorch Geometric [7]."
Multi-scope Analysis Driven Hierarchical Graph Transformer for Whole Slide Image Based Cancer Survival Prediction,3,Experiments,"The initial number of superpixels of SLIC algorithm is set to {600, 700, 600}, and the number of clusters of k-means algorithm is set to {16, 16, 16} for CRC, TCGA-LIHC and TCGA-KICA cohorts. The non linearity of GCN is ReLU. The number of Transformer heads is 8, and the attention scores of all heads are averaged to produce the heatmap of contextual interactions. HGT is trained with a mini-batch size of 16, and a learning rate of 1e -5 with Adam optimizer for 30 epochs.Evaluation Metric. The concordance index (CI) [23] is used to measure the fraction of all pairs of patients whose survival risks are correctly ordered. CI ranges from 0 to 1, where a larger CI indicates better performance. Moreover, to evaluate the ability of patients stratification, the Kaplan-Meier (KM) analysis is used [23]. In this study, we conduct a 5-fold evaluation procedure with 5 runs to evaluate the survival prediction performance for each method. The result of mean ± std is reported."
Multi-scope Analysis Driven Hierarchical Graph Transformer for Whole Slide Image Based Cancer Survival Prediction,3.2,Comparative Results,"We compared seven state-of-the-art methods (SOTAs), i.e., DeepSets [27], ABMIL [13], DeepAttnMISL [26], CLAM [18], DSMIL [16], PatchGCN [5], and TransMIL [19]. We also compared three baselines of our method, i.e., w/o Patch GCL, w/o Tissue GCL and w/o Transformer. For fair comparison, same CNN extractor (i.e. ImageNet pretrained Resnet50 [11]), and survival prediction loss (i.e. Cox loss [26]) is adopt for all methods. Table 1 and Fig. 2 show the results of CI and KM-analysis of each method, respectively. Generally, most MIL methods, i.e., DeepSets, ABMIL, DSMIL, TransMIL mainly focus on a few key instances for prediction, but they do not have significant advantages in cancer prognosis. Furthermore, due to the large size of CRC dataset and relatively high model complexity, Patch-GCN and TransMIL encountered a memory overflow when processing the CRC dataset, which limits their clinical application. DeepAttnMISL has a certain semantic perception ability for patch, which achieves better performance in LIHC cohort. PatchGCN is capable to capture the local contextual interactions between patch, which also achieves satisfied performance in KIRC cohort. As our method has potential to explore the contextual interactions of pathological components, which more in line with the thinking of pathologists for cancer prognosis. Our method achieves higher CI and relatively low P-Value (< 0.05) of KM analysis on both three cancer cohorts, which consistently outperform the SOTAs and baselines. In addition, the feature aggregation of the lower levels (i.e., patch and tissue) are guided by the priors, and the MHSA is only executed on pathological components, resulting in high efficiency even on the CRC dataset.  [13] 0.580 ± 0.005 0.634 ± 0.005 0.617 ± 0.094 DeepAttnMISL [26] 0.570 ± 0.001 0.644 ± 0.009 0.584 ± 0.019 CLAM [18] 0.575 ± 0.010 0.641 ± 0.002 0.635 ± 0.006 DSMIL [16] 0.550 ± 0.016 0.626 ± 0.005 0.603 ± 0.022 PatchGCN [ "
Multi-scope Analysis Driven Hierarchical Graph Transformer for Whole Slide Image Based Cancer Survival Prediction,3.3,Interpretability of the Proposed Framework,"We selected the CRC dataset for further interpretable analysis, as it is one of the leading causes of mortality in industrialized countries, and its prognosis-related factors have been widely studied [3,8]. We trained an encoded feature based classification model (i.e., a MLP) on a open-source colorectal cancer dataset (i.e., NCT-CRC-HE-100K [14]), which is annotated with 9 classes, including: adipose tissue (ADI); background (BACK); debris (DEB); lymphocytes (LYM); mucus (MUC); muscle (MUS); normal colon mucosa (NORM); stroma (STR); tumor (TUM). The trained classification model can be used to determine the biological semantics of the pathological components extracted by our model with a major voting rule. Figure 3 shows the original image, spatial topology, proportion and biological meaning of pathological components, and its contextual interactions of a typical case from CRC cohort. It can be seen that the interaction between component 1 (TUM) and component 9 (STR) has gained the highest attention of the network, which is consistent with the existing knowledge [3,8]. Moreover, there is also concentration of interaction in some other interactions, which may potentially imply some new biomarkers."
Multi-scope Analysis Driven Hierarchical Graph Transformer for Whole Slide Image Based Cancer Survival Prediction,4,Conclusion,"In this paper, we propose a novel learning framework, i.e., multi-scope analysis driven HGT, to effectively represent and capture the contextual interaction of pathological components for improving the effectiveness and interpretability of WSI-based cancer survival prediction. Experimental results on three clinical cancer cohorts demonstrated our model achieves better performance and richer interpretability over the existing models. In the future, we will evaluate our framework on more tasks and further statistically analyze the interpretability of our model to find more pathological biomarkers related to cancer prognosis."
Multi-scope Analysis Driven Hierarchical Graph Transformer for Whole Slide Image Based Cancer Survival Prediction,,Fig. 1 .,
Multi-scope Analysis Driven Hierarchical Graph Transformer for Whole Slide Image Based Cancer Survival Prediction,,Fig. 2 .Fig. 3 .,
Multi-scope Analysis Driven Hierarchical Graph Transformer for Whole Slide Image Based Cancer Survival Prediction,,Table 1 .,
TCEIP: Text Condition Embedded Regression Network for Dental Implant Position Prediction,1,Introduction,"According to a systematic research study [2], periodontal disease is the world's 11th most prevalent oral condition, which potentially causes tooth loss in adults, especially the aged [8]. One of the most appropriate treatments for such a defect/dentition loss is prosthesis implanting, in which the surgical guide is usually used. However, dentists must load the Cone-beam computed tomography (CBCT) data into the surgical guide design software to estimate the implant position, which is tedious and inefficient. In contrast, deep learning-based methods show great potential to efficiently assist the dentist in locating the implant position [7].Recently, deep learning-based methods have achieved great success in the task of implant position estimation. Kurt et al. [4] and Widiasri et al. [11] utilized the convolutional neural network (CNN) to locate the oral bone, e.g., the alveolar bone, maxillary sinus and jaw bone, which determines the implant position indirectly. Different from these implant depth measuring methods, Yang et al. [13] developed a transformer-based implant position regression network (Implant-Former), which directly predicts the implant position on the 2D axial view of tooth crown images and projects the prediction results back to the tooth root by the space transform algorithm. However, these methods generally consider simple situations, in which only one missing tooth is available. When confronting some special cases, such as multiple missing teeth and sparse teeth disturbance in Fig. 1(a), the above methods may fail to determine the correct implant position. In contrast, clinically, dentists have a subjective expertise about where the implant should be planted, which motivates us that, additional indications or conditions from dentists may help predict an accurate implant position.In recent years, great success has been witnessed in Vision-Language Pretraining (VLP). For example, Radford [9] proposed Contrastive Language-Image Pretraining (CLIP) to learn diverse visual concepts from 400 million image-text pairs automatically, which can be used for vision tasks like object detection [17] and segmentation [12]. In this paper, we found that CLIP has the ability to learn the position relationship among instances. We showcase examples in Fig. 1(b) that the image-text pair with the word 'left' get a higher matching score than others, as the position of baby is on the left of the billboard.Motivated by the above observation in dental implant and the property of CLIP, in this paper, we integrate a text condition from the CLIP to assist the implant position regression. According to the natural distribution, we divide teeth regions into three categories in Fig. 1(c),i.e., left, middle, and right. Specifically, during training, one of the text prompts, i.e., 'right', 'middle', and 'left' is paired with the crown image as input, in which the text prompt works as a guidance or condition. The crown image is processed by an encoder-decoder network for final location regression. In addition, to facilitate the interaction between features in two modalities, a cross-modal interaction that consists of cross-modal attention (CMA) and knowledge alignment module (KAM), is devised. The CMA module fuses conditional information, i.e., text prompt, to the encoderdecoder. This brings additional indications or conditions from the dentist to help the implant position regression. However, a knowledge gap may exist between our encoder-decoder and CLIP. To mitigate the problem, the KAM is proposed to distill the encoded-decoded features of crown images to the space of CLIP, which brings significant localization improvements. In inference, given an image, the dentist just simply gives a conditioning text like ""let's implant a prosthesis on the left"", the network will preferentially seek a suitable location on the left for implant prosthesis.Main contributions of this paper can be summarized as follows: 1) To the best of our knowledge, the proposed TCEIP is the first text condition embedded implant position regression network that integrates a text embedding of CLIP to guide the prediction of implant position. (2) A cross-modal interaction that consists of a cross-modal attention (CMA) and knowledge alignment module (KAM) is devised to facilitate the interaction between features that representing image and text. ( 3) Extensive experiments on a dental implant dataset demonstrated the proposed TCEIP achieves superior performance than the existing methods, especially for patients with multiple missing teeth or sparse teeth."
TCEIP: Text Condition Embedded Regression Network for Dental Implant Position Prediction,2,Method,"Given a tooth crown image with single or multiple implant regions, the proposed TCEIP aims to give a precise implant location conditioned by text indications from the dentist, i.e., a description of position like 'left', 'right', or 'middle'. An overview of TCEIP is presented in Fig. 2. It mainly consists of four parts: i) Encoder and Decoder, ii) Conditional Text Embedding, iii) Cross-Modal Interaction Module, and iv) Heatmap Regression Network. After obtaining the predicted coordinates of the implant at the tooth crown, we adopt the space transformation algorithm [13] to fit a centerline of implant to project the coordinates to the tooth root, where the real implant location can be acquired. Next, we will introduce these modules in detail. "
TCEIP: Text Condition Embedded Regression Network for Dental Implant Position Prediction,2.1,Encoder and Decoder,We employ the widely used ResNet [3]  
TCEIP: Text Condition Embedded Regression Network for Dental Implant Position Prediction,2.2,Conditional Text Embedding,"To integrate the text condition provided by a dentist, we utilize the CLIP to extract the text embedding. Specifically, additional input of text, e.g., 'left', 'middle', or 'right', is processed by the CLIP Text Encoder to obtain a conditional text embedding v t ∈ R 1×D . As shown in Fig. 2, to interact with the image features from ResNet layers, a series of transformation f (•) and g(•) over v t are performed as follow:where f (•) repeats text embedding v t from R 1×D to R 1×HW and g(•) then reshapes it to R H×W ×1 . This operation ensures better interaction between image and text in the same feature space. "
TCEIP: Text Condition Embedded Regression Network for Dental Implant Position Prediction,2.3,Cross-Modal Interaction,"High-resolution features from the aforementioned decoder can be directly used to regress the implant position. However, it cannot work well in situations of multiple teeth loss or sparse teeth disturbance. In addition, although we have extracted the conditional text embedding from the CLIP to assist the network regression, there exists a big difference with the feature of encoder-decoder in the feature space. To tackle these issues, we propose cross-modal interaction, including i) Cross-Modal Attention and ii) Knowledge Alignment module, to integrate the text condition provided by the dentist.  C+1) . The CMA module can be formulated as follows:"
TCEIP: Text Condition Embedded Regression Network for Dental Implant Position Prediction,,Cross-Modal Attention,"where four independent 1×1 convolutions α, β, θ, and γ are used to map image and fusion features to the space for cross-modal attention. At first, M and F are passed into θ(•), β(•) and α(•) for channel transformation, respectively. Following the transformed feature, M θ and F β perform multiplication via a Softmax activation function to take a cross-attention with F α . In the end, the output feature of the cross-attention via γ(•) for feature smoothing is added with F. Given the above operations, the cross-modal features O et and O dt are obtained and passed to the next layer.Knowledge Alignment Module. The above operations only consider the interaction between features in two modalities. A problem is that text embeddings from pre-trained text encoder of CLIP are not well aligned with the image features initialized by ImageNet pre-training. This knowledge shift potentially weakens the proposed cross-modal interaction to assist the prediction of implant position. To mitigate this problem, we propose the knowledge alignment module (KAM) to gradually align image features to the feature space of pre-trained CLIP. Motivated by knowledge distillation [10], we formulate the proposed knowledge alignment as follows:where m e 4 ∈ R 1×D is the transformed feature of M e 4 after attention pooling operation [1] and dimension reduction with convolution. m clip ∈ R 1×D is the image embedding extracted by the CLIP Image Encoder. Using this criteria, the encoder of TCEIP approximates the CLIP image encoder and consequently aligns the image features of the encoder with the CLIP text embeddings."
TCEIP: Text Condition Embedded Regression Network for Dental Implant Position Prediction,2.4,Heatmap Regression Network,"The heatmap regression network is used for locating the implant position, which consists of the heatmap and the offset head. The output of the heatmap head is the center localization of implant position, which is formed as a heatmap G ∈ [0, 1] H×W . Following [5], given coordinate of the ground truth implant location ( tx , ty ), we apply a 2D Gaussian kernel to get the target heatmap:where σ is an object size-adaptive standard deviation. The predicted heatmap is optimized by the focal loss [6]:where λ and ϕ are the hyper-parameters of the focal loss, Ĝ is the predicted heatmap and N is the number of implant annotation in image. The offset head computes the discretization error caused by the downsampling operation, which is used to further refine the predicted location. The local offset loss L o is optimized by the L1 loss. The overall training loss of network is:"
TCEIP: Text Condition Embedded Regression Network for Dental Implant Position Prediction,2.5,Coordinate Projection,"The output of TCEIP is the coordinate of implant at the tooth crown. To obtain the real implant location at the tooth root, we fit a centerline of implant using the predicted implant position of TCEIP and then extend the centerline to the root area, which is identical as [13]. By this means, the intersections of implant centerline with 2D slices of root image, i.e. the implant position at the tooth root area, can be obtained. 3 Experiments and Results"
TCEIP: Text Condition Embedded Regression Network for Dental Implant Position Prediction,3.1,Dataset and Implementation Details,"The dental implant dataset was collected by [13], which contains 3045 2D slices of tooth crown images. The implant position annotations are annotated by three experienced dentists. The input image size of network is set as 512 × 512. We use a batch size of 8, Adam optimizer and a learning rate of 0.001 for the network training. Total training epochs is 80 and the learning rate is divided by 10 when epoch = {40, 60}. The same data augmentation methods in [13] was employed."
TCEIP: Text Condition Embedded Regression Network for Dental Implant Position Prediction,3.2,Performance Analysis,"We use the same evaluation criteria in [13], i.e., average precision (AP) to evaluate the performance of our network. As high accurate position prediction is required in clinical practice, the IOU threshold is set as 0.75. Five-fold crossvalidation was performed for all our experiments. Transformer-based ImplantFormer ViT-Base-ResNet-50 13.7 ± 0.2045 Deformable DETR [19] 12.8 ± 0.1417 CNN-based CenterNet [18] ResNet-50 10.9 ± 0.2457 ATSS [16] 12.1 ± 0.2694 VFNet [15] 11.8 ± 0.8734 RepPoints [14] 11.2 ± 0.1858 TCEIP 17.8 ± 0.3956 Ablation Studies. To evaluate the effectiveness of the proposed network, we conduct ablation experiments to investigate the effect of each component in Table 1. We can observe from the second row of the table that the introduction of text condition improves the performance by 3.7%, demonstrating the validity of using text condition to assist the implant position prediction. When combining both text condition and KAM, the improvement reaches 4.8%. As shown in the table's last three rows, both feature fusion operation and CAM improve AP value by 1.4% and 0.8%, respectively. When combining all these components, the improvement reaches 6.9%."
TCEIP: Text Condition Embedded Regression Network for Dental Implant Position Prediction,,Comparison to the Mainstream Detectors.,"To demonstrate the superior performance of the proposed TCEIP, we compare the AP value with the mainstream detectors in Table 2. Only the anchor-free detector is used for comparison, due to the reason that no useful texture is available around the center of the implant. As the teeth are missing, the anchor-based detectors can not regress the implant position successfully. From the table we can observe that, the transformer-based methods perform better than the CNN-based networks (e.g., ImplantFormer achieved 13.7% AP, which is 1.6% higher than the best-performed anchor-free network -ATSS). The proposed TCEIP achieves the best AP value -17.8%, among all benchmarks, which surpasses the Implant-Former with a large gap. The experimental results proved the effectiveness of our method. In Fig. 4, we choose two best-performed detectors from the CNN-based (e.g., ATSS) and transformer-based (e.g., ImplantFormer) methods for visual comparison, to further demonstrate the superiority of TCEIP in the implant position prediction. The first row of the figure is a patient with sparse teeth, and the second and third rows are a patient with two missing teeth. We can observe from the figure that both the ATSS and ImplantFormer generate false positive detection, except for the TCEIP. Moreover, the implant position predicted by the TCEIP is more accurate. These visual results demonstrated the effectiveness of using text condition to assist the implant position prediction."
TCEIP: Text Condition Embedded Regression Network for Dental Implant Position Prediction,4,Conclusions,"In this paper, we introduce TCEIP, a text condition embedded implant position regression network, which integrate additional condition from the CLIP to guide the prediction of implant position. A cross-modal attention (CMA) and knowledge alignment module (KAM) is devised to facilitate the interaction between features in two modalities. Extensive experiments on a dental implant dataset through five-fold cross-validation demonstrated that the proposed TCEIP achieves superior performance than the existing methods."
TCEIP: Text Condition Embedded Regression Network for Dental Implant Position Prediction,,Fig. 1 .,
TCEIP: Text Condition Embedded Regression Network for Dental Implant Position Prediction,,Fig. 2 .,
TCEIP: Text Condition Embedded Regression Network for Dental Implant Position Prediction,,,
TCEIP: Text Condition Embedded Regression Network for Dental Implant Position Prediction,,Fig. 3 .,
TCEIP: Text Condition Embedded Regression Network for Dental Implant Position Prediction,,Fig. 4 .,
TCEIP: Text Condition Embedded Regression Network for Dental Implant Position Prediction,,,Module.
TCEIP: Text Condition Embedded Regression Network for Dental Implant Position Prediction,,Table 1 .,
TCEIP: Text Condition Embedded Regression Network for Dental Implant Position Prediction,,Table 2 .,
TCEIP: Text Condition Embedded Regression Network for Dental Implant Position Prediction,,Acknowledgments,. This work was supported by the National Natural Science Foundation of China under Grant 82261138629; Guangdong Basic and Applied Basic Research Foundation under Grant 2023A1515010688 and 2021A1515220072; Shenzhen Municipal Science and Technology Innovation Council under Grant JCYJ2022053110141 2030 and JCYJ20220530155811025.
TCEIP: Text Condition Embedded Regression Network for Dental Implant Position Prediction,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 31.
Mining Negative Temporal Contexts for False Positive Suppression in Real-Time Ultrasound Lesion Detection,1,Introduction,"Ultrasound is a widely-used imaging modality for clinical cancer screening. Deep Learning has recently emerged as a promising approach for ultrasound lesion detection. While previous works focused on lesion detection in still images [25] and offline videos [9,11,22], this paper explores real-time ultrasound video lesion detection. Real-time lesion prompts can assist radiologists during scanning, thus being more helpful to improve the accuracy of diagnosis. This task requires the model to infer faster than 30 frames per second (FPS) [19] and only previous frames are available for current frame processing.Previous general-purpose detectors [1,2] report simple and obvious FPs when applied to ultrasound videos, e.g. the red box in Fig. 1(a). These FPs, attributable to non-lesion anatomies, can mislead junior readers. These anatomies appear like lesions in certain frames, but typically show negative symptoms in adjacent frames when scanned from different positions. So experienced radiologists will refer to corresponding regions in previous frames, denoted as temporal contexts (TC), to help restrain FPs. If TC of a lesion-like region exhibit negative symptoms, denoted as negative temporal contexts (NTC), radiologists are less likely to report it as a lesion [15]. Although important, the utilization of NTC remains unexplored. In natural videos, as transitions from non-objects to objects are implausible, previous works [1,2,20] only consider inter-object relationships. As shown in Sect. 4.4, the inability to utilize NTC is a key issue leading to the FPs reported by general-purpose detectors.To address this issue, we propose a novel UltraDet model to leverage NTC. For each Region of Interest (RoI) R proposed by a basic detector, we extract temporal contexts from previous frames. To compensate for inter-frame motion, we generate deformed grids by applying inverse optical flow to the original regular RoI grids, illustrated in Fig. 1. Then we extract the RoI features from the deformed grids in previous frames and aggregate them into R. We call the overall process Negative Temporal Context Aggregation (NTCA). The NTCA module leverages RoI-level NTC which are crucial for radiologists but ignored in previous works, thereby effectively improving the detection performance in a reliable and interpretable way. We plug the NTCA module into a basic real-time detector to form UltraDet. Experiments on CVA-BUS dataset [9] demonstrate that Ultra-Det, with real-time inference speed, significantly outperforms previous works, reducing about 50% FPs at a recall rate of 0.90.Our contributions are four-fold. (1) We identify that the failure of generalpurpose detectors on ultrasound videos derives from their incapability of utilizing negative temporal contexts. (2) We propose a novel UltraDet model, incorpo-rating an NTCA module that effectively leverages NTC for FP suppression. (3) We conduct extensive experiments to demonstrate the proposed UltraDet significantly outperforms the previous state-of-the-arts. (4) We release high-quality labels of the CVA-BUS dataset [9] to facilitate future research."
Mining Negative Temporal Contexts for False Positive Suppression in Real-Time Ultrasound Lesion Detection,2,Related Works,"Real-Time Video Object Detection is typically achieved by single-frame detectors, often with temporal information aggregation modules. One-stage detectors [5,8,16,21] use only intra-frame information, DETR-based detectors [20,26] and Faster R-CNN-based detectors [1,2,7,14,23,28] are also widely utilized in video object detection. They aggregate temporal information by mining inter-object relationships without considering NTC.Ultrasound Lesion Detection [10] can assist radiologists in clinical practice. Previous works have explored lesion detection in still images [25] and offline videos [9,11,22]. Real-time video lesion detection is underexplored. In previous works, YOLO series [17,24] and knowledge distillation [19] are used to speed up inference. However, these works use single-frame detectors or post-process methods while learnable inter-frame aggregation modules are not adopted. Thus their performances are far from satisfactory.Optical Flow [3] is used to guide ultrasound segmentation [12], motion estimation [4] and elastography [13]. For the first time, we use inverse optical flow to guide temporal context information extraction. In real-time video lesion detection, given the current frame I t and a sequence of T previous frames as {I τ } t-1 τ =t-T , the goal is to detect lesions in I t by exploiting the temporal information in previous frames as illustrated in Fig. 2."
Mining Negative Temporal Contexts for False Positive Suppression in Real-Time Ultrasound Lesion Detection,3,Method,
Mining Negative Temporal Contexts for False Positive Suppression in Real-Time Ultrasound Lesion Detection,3.1,Basic Real-Time Detector,"The basic real-time detector comprises three main components: a lightweight backbone (e.g. ResNet34 [6]), a Region Proposal Network (RPN) [14], and a Temporal Relation head [2]. The backbone is responsible for extracting feature map F τ of frame I τ . The RPN generates proposals consisting of boxes B τ and proposal features Q τ using RoI Align and average pooling:whereTo aggregate temporal information, proposals from all T + 1 frames are fed into the Temporal Relation head and updated with inter-lesion information extracted via a relation operation [7]:where l = 1, • • • , L represent layer indices, B and Q are the concatenation of all B τ and Q τ , and Q 0 = Q. We call this basic real-time detector BasicDet. The BasicDet is conceptually similar to RDN [2] but does not incorporate relation distillation since the number of lesions and proposals in this study is much smaller than in natural videos."
Mining Negative Temporal Contexts for False Positive Suppression in Real-Time Ultrasound Lesion Detection,3.2,Negative Temporal Context Aggregation,"In this section, we present the Negative Temporal Context Aggregation (NTCA) module. We sample T ctxt context frames from T previous frames, then extract temporal contexts (TC) from context frames and aggregate them into proposals. We illustrate the NTCA module in Fig. 3 and elaborate on details as follows. Inverse Optical Flow Align. We propose the Inverse Optical Flow Align (IOF Align) to extract TC features. For the current frame I t and a sampled context frame I τ with τ < t, we extract TC features from the context feature map F τ with the corresponding regions. We use inverse optical flowto transform the RoIs from frame t to τ : O t→τ = FlowNet(I t , I τ ) where H, W represent height and width of feature maps. The FlowNet(I t , I τ ) is a fixed network [3] to predict optical flow from I t to I τ . We refer to O t→τ as inverse optical flow because it represents the optical flow in inverse chronological order from t to τ . We conduct IOF Align and average pooling to extract C t,τ :where IOFAlign(F τ , B t , O t→τ ) extracts context features in F τ from deformed grids generated by applying offsets O t→τ to the original regular grids in B t , which is illustrated in the Fig. 1(b).Temporal Aggregation. We concatenate C t,τ in all T ctxt context frames to form C t and enhance proposal features by fusing C t into Q t :where [18]. We refer to the concatenation of all TC-enhanced proposal features in T + 1 frames as Q ctxt . To extract consistent TC, the context frames of T previous frames are shared with the current frame."
Mining Negative Temporal Contexts for False Positive Suppression in Real-Time Ultrasound Lesion Detection,3.3,UltraDet for Real-Time Lesion Detection,"We integrate the NTCA module into the BasicDet introduced in Sect. 3.1 to form the UltraDet model, which is illustrated in Fig. 2. The head of UltraDet consists of stacked NTCA and relation modules:During training, we apply regression and classification losses L = L reg + L cls to the current frame. To improve training efficiency, we apply auxiliary losses L aux = L to all previous T frames. During inference, the UltraDet model uses the current frame and T previous frames as inputs and generates predictions only for the current frame. This design endows the UltraDet with the ability to perform real-time lesion detection."
Mining Negative Temporal Contexts for False Positive Suppression in Real-Time Ultrasound Lesion Detection,4,Experiments,
Mining Negative Temporal Contexts for False Positive Suppression in Real-Time Ultrasound Lesion Detection,4.1,Dateset,"CVA-BUS Dateset. We use the open source CVA-BUS dataset that consists of 186 valid videos, which is proposed in CVA-Net [9]. We split the dataset into train-val (154 videos) and test (32 videos) sets. In the train-val split, there are 21423 frames with 170 lesions. In the test split, there are 3849 frames with 32 lesions. We focus on the lesion detection task and do not utilize the benign/malignant classification labels provided in the original dataset.High-Quality Labels. The bounding box labels provided in the original CVA-BUS dataset are unsteady and sometimes inaccurate, leading to jiggling and inaccurate model predictions. We provide a new version of high-quality labels that are re-annotated by experienced radiologists. We reproduce all baselines using our high-quality labels to ensure a fair comparison. Visual comparisons of two versions of labels are available in supplementary materials. To facilitate future research, we will release these high-quality labels. "
Mining Negative Temporal Contexts for False Positive Suppression in Real-Time Ultrasound Lesion Detection,4.2,Evaluation Metrics,"Pr80, Pr90. In clinical applications, it is important for detection models to be sensitive. So we provide frame-level precision values with high recall rates of 0.80 and 0.90, which we denote as Pr80 and Pr90, respectively. FP80, FP90. We further report lesion-level FP rates as critical metrics. Framelevel FPs are linked by IoU scores to form FP sequences [24]. The number of FP sequences per minute at recall rates of 0.80 and 0.90 are reported as FP80 and FP90, respectively. The unit of lesion-level FP rates is seq/min. AP50. We provide AP50 instead of mAP or AP75 because the IoU threshold of 0.50 is sufficient for lesion localization in clinical practice. Higher thresholds like 0.75 or 0.90 are impractical due to the presence of blurred lesion edges.R@16. To evaluate the highest achievable sensitivity, we report the frame-level average recall rates of Top-16 proposals, denoted as R@16."
Mining Negative Temporal Contexts for False Positive Suppression in Real-Time Ultrasound Lesion Detection,4.3,Implementation Details,"UltraDet Settings. We use FlowNetS [3] as the fixed FlowNet in IOF Align and share the same finding with previous works [4,12,13] that the FlowNet trained on natural datasets generalizes well on ultrasound datasets. We set the pooling stride in the FlowNet to 4, the number of UltraDet head layers L = 2, the number of previous frames T = 15 and T ctxt = 2, and the number of proposals is 16. We cached intermediate results of previous frames and reuse them to speed up inference. Other hyper-parameters are listed in supplementary materials.Shared Settings. All models are built in PyTorch framework and trained using eight NVIDIA GeForce RTX 3090 GPUs. We use ResNet34 [6] as backbones and set the number of training iterations to 10,000. We set the feature dimensions of detection heads to 256 and baselines are re-implemented to utilize only previous frames. We refer to our code for more details."
Mining Negative Temporal Contexts for False Positive Suppression in Real-Time Ultrasound Lesion Detection,4.4,Main Results,"Quantitative Results. We compare performances of real-time detectors with the UltraDet in Table 1. We perform 4-fold cross-validation and report the mean values and standard errors on the test set to mitigate fluctuations. The UltraDet outperforms all previous state-of-the-art in terms of precision and FP rates.Especially, the Pr90 of UltraDet achieves 90.8%, representing a 5.4% absolute improvement over the best competitor, PTSEFormer [20]. Moreover, the FP90 of UltraDet is 5.7 seq/min, reducing about 50% FPs of the best competitor, PTSEFormer. Although CVA-Net [9] achieve comparable AP50 with our method, we significantly improve precision and FP rates over the CVA-Net [9]. Importance of NTC. In Fig. 4(a), we illustrate the FP ratios that can be suppressed by using NTC. The determination of whether FPs can be inhibited by NTC is based on manual judgments of experienced radiologists. We find that about 50%-70% FPs of previous methods are suppressible. However, by utilizing NTC in our UltraDet, we are able to effectively prevent this type of FPs.Inference Speed. We run inference using one NVIDIA GeForce RTX 3090 GPU and report the inference speed in Table 1. The UltraDet achieves an inference speed of 30.4 FPS and already meets the 30 FPS requirement. Using Ten-sorRT, we further optimize the speed to 35.2 FPS, which is sufficient for clinical applications [19].Qualitative Results.  Effectiveness of Each Sub-module. We ablate the effectiveness of each submodule of the NTCA module in Table 2. Specifically, we replace the IOF Align with an RoI Align and the Temporal Aggregation with a simple average pooling in the temporal dimension. The results demonstrate that both IOF Align and Temporal Aggregation are crucial, as removing either of them leads to a noticeable drop in performance. Design of the NTCA Module. Besides RoI-level TC aggregation in UltraDet, feature-level aggregation is also feasible. We plug the optical flow feature warping proposed in FGFA [28] into the BasicDet and report the results in Table 3."
Mining Negative Temporal Contexts for False Positive Suppression in Real-Time Ultrasound Lesion Detection,4.5,Ablation Study,"We find RoI-level aggregation is more effective than feature-level, and bothlevel aggregation provides no performance gains. This conclusion agrees with radiologists' skills to focus more on local regions instead of global information."
Mining Negative Temporal Contexts for False Positive Suppression in Real-Time Ultrasound Lesion Detection,5,Conclusion,"In this paper, we address the clinical challenge of real-time ultrasound lesion detection. We propose a novel Negative Temporal Context Aggregation (NTCA) module, imitating radiologists' diagnosis processes to suppress FPs. The NTCA module leverages negative temporal contexts that are essential for FP suppression but ignored in previous works, thereby being more effective in suppressing FPs. We plug the NTCA module into a BasicDet to form the UltraDet model, which significantly improves the precision and FP rates over previous state-ofthe-arts while achieving real-time inference speed. The UltraDet has the potential to become a real-time lesion detection application and assist radiologists in more accurate cancer diagnosis in clinical practice."
Mining Negative Temporal Contexts for False Positive Suppression in Real-Time Ultrasound Lesion Detection,,Fig. 1 .,
Mining Negative Temporal Contexts for False Positive Suppression in Real-Time Ultrasound Lesion Detection,,Fig. 2 .,
Mining Negative Temporal Contexts for False Positive Suppression in Real-Time Ultrasound Lesion Detection,,Fig. 3 .,
Mining Negative Temporal Contexts for False Positive Suppression in Real-Time Ultrasound Lesion Detection,,Fig. 4 .,
Mining Negative Temporal Contexts for False Positive Suppression in Real-Time Ultrasound Lesion Detection,,Figure 4 (,
Mining Negative Temporal Contexts for False Positive Suppression in Real-Time Ultrasound Lesion Detection,,Table 1 .,
Mining Negative Temporal Contexts for False Positive Suppression in Real-Time Ultrasound Lesion Detection,,Table 2 .,
Mining Negative Temporal Contexts for False Positive Suppression in Real-Time Ultrasound Lesion Detection,,Table 3 .,
Mining Negative Temporal Contexts for False Positive Suppression in Real-Time Ultrasound Lesion Detection,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2_1.
Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,1,Introduction,"Whole slide scanning is increasingly used in disease diagnosis and pathological research to visualize tissue samples. Compared to traditional microscopebased observation, whole slide scanning converts glass slides into gigapixel digital images that can be conveniently stored and analyzed. However, the high resolution of WSIs also makes their automated classification challenging [15]. Patchbased classification is a common solution to this problem [3,8,24]. It predicts the slide-level label by first predicting the labels of small, tiled patches in a WSI. This approach allows for the direct application of existing image classification models, but requires additional patch-level labeling. Unfortunately, patch-level labeling by histopathology experts is expensive and time-consuming. Therefore, many weakly-supervised [8,24] and semi-supervised [3,5] methods have been proposed to generate patch-level pseudo labels at a lower cost. However, the lack of reliable supervision directly hinders the performance of these methods, and serious class-imbalance problems could arise, as tumor patches may only account for a small portion of the entire WSI [12].In contrast, MIL-based methods have become increasingly preferred due to their only demand for slide-level labels [18]. The typical pipeline of MIL methods is shown in Fig. 1, where WSIs are treated as bags, and tiled patches are considered as instances. The aim is to predict whether there are positive instances, such as tumor patches, in a bag, and if so, the bag is considered positive as well. In practice, a fixed ImageNet pre-trained feature extractor g(•) is usually used to convert the tiled patches in a WSI into feature maps due to limited GPU memory. These instance features are then aggregated by a(•) into a slide-level feature vector to be sent to the bag-level classifier f (•) for MIL training. Due to the high computational cost, end-to-end training of the feature extractor and bag classifier is prohibitive, especially for high-resolution WSIs. As a result, many methods focus solely on improving a(•) or f (•), leaving g(•) untrained on the WSI dataset (as shown in Fig. 2(b)). However, the domain shift between WSI and natural images may lead to sub-optimal representations, so recently there have been methods proposed to fine-tune g(•) using self-supervised techniques [4,12,21] or weakly-supervised techniques [10,13,23] (as shown in Fig. 2(c)). Nevertheless, since these two processes are still trained separately with different supervision signals, they lack joint optimization and may still leads to inconsistency within the entire MIL pipeline. To address the challenges mentioned above, we propose a novel MIL framework called ICMIL, which can iteratively couple the patch feature embedding process with the bag-level classification process to enhance the effectiveness of MIL training (as illustrated in Fig. 2(d)). Unlike previous works that mainly focused on designing sophisticated instance aggregators a(•) [12,14,20] or bag classifiers f (•) [9,16,25], we aim to bridge the loss back-propagation process from f (•) to g(•) to improve g(•)'s ability to perceive slide-level labels. Specifically, we propose to use the bag-level classifier f (•) to initialize an instance-level classifier f (•), enabling f (•) to use the category knowledge learned from bag-level features to determine each instance's category. In this regard, we further propose a teacher-student [7] approach to effectively generate pseudo labels and simultaneously fine-tune g(•). After fine-tuning, the domain shift problem is alleviated in g(•), leading to better patch representations. The new representations can be used to train a better bag-level classifier in return for the next round of iteration.In summary, our contributions are: (1) We propose ICMIL which bridges the loss propagation from the bag classifier to the patch embedder by iteratively coupling them during training. This framework fine-tunes the patch embedder based on the bag-level classifier, and the refined embeddings, in turn, help train a more accurate bag-level classifier. (2) We propose a teacher-student approach to achieve effective and robust knowledge transfer from the bag-level classifier f (•) to the instance-level representation embedder g(•). (3) We conduct extensive experiments on two datasets using three different backbones and demonstrate the effectiveness of our proposed framework."
Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,2,Methodology,
Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,2.1,Iterative Coupling of Embedder and Bag Classifier in ICMIL,"The general idea of ICMIL is shown in Fig. 3, which is inspired by the Expectation-Maximization (EM) algorithm. EM has been used with MIL in some previous works [13,17,22], but it was only treated as an assisting tool for aiding the training of either g(•) or f (•) in the traditional MIL pipelines. In contrast, we are the first to consider the optimization of the entire MIL pipeline as an EM alike problem, utilizing EM for coupling g(•) and f (•) together iteratively. To begin with, we first employ a traditional approach to train a bag-level classifier f (•) on a given dataset, with patch embeddings generated by a fixed ResNet50 [6] pre-trained on ImageNet [19] (step 1 in Fig. 3). Subsequently, this f (•) is considered as the initialization of a hidden instance classifer f (•), generating pseudo-labels for each instance-level representation. This operation is feasible when the bag-level representations aggregated by a(•) are in the same hidden space as the instance representations, and most aggregation methods (e.g., max pooling, attention-based) satisfy this condition since they essentially make linear combinations of instance-level representations.Next, we freeze the weights of f (•) and fine-tune g(•) with the generated pseudo-labels (step 2 in Fig. 3), of which the detailed implementation is presented in Sect. 2.3. After this, g(•) is fine-tuned for the specific WSI dataset, which allows it to generate improved representations for each instance, thereby enhancing the performance of f (•). Moreover, with a better f (•), we can use the iterative coupling technique again, resulting in further performance gains and mitigation to the distribution inconsistencies between instance-and bag-level embeddings."
Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,2.2,Instance Aggregation Method in ICMIL,"Although most instance aggregators are compatible with ICMIL, they still have an impact on the efficiency and effectiveness of ICMIL. In addition to that a(•) has to project the bag representations to the same hidden space as the instance representations, it also should avoid being over-complicated. Otherwise, a(•) may lead to larger difference between the decision boundaries of bag-level classifer f (•) and instance-level classifier f (•), which may cause ICMIL taking more time to converge.Therefore, in our experiments, we choose to use the attention-based instance aggregation method [9] which has been widely used in many of the existing MIL frameworks [9,16,25]. For a bag that contains K instances, attention-based aggregation method firstly learns an attention score for each instance. Then, the aggregated bag-level representation H is defined as:where a k is the attention score for the k-th instance h k in the bag. Obviously, H and h k remains in the same hidden space, satisfying the prerequisite of ICMIL."
Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,2.3,Label Propagation from Bag Classifier to Embedder,"We propose a novel teacher-student model for accurate and robust label propagation from f (•) to g(•). The model's architecture is depicted in Fig. 4. In contrast to the conventional approach of generating all pseudo labels and retraining g(•) from scratch, our proposed method can simultaneously process the pseudo label generation and g(•) fine-tuning tasks, making it more flexible. Moreover, incorporating augmented inputs in the training process allows for the better utilization of supervision signals, resulting in a more robust g(•). We also introduce a learnable f (•) to self-adaptively modifying the instance-level decision boundary for more effective fine-tuning of the embedder. Specifically, we freeze the weights of g(•) and f (•) and set them as the teacher. We then train a student patch embedding network, g (•), to learn category knowledge from the teacher. For a given patch input x, the teacher generates the corresponding pseudo label, while the student receives an augmented image x and attempts to generate a similar prediction to that of the teacher through a consistency loss L c . This loss function is defined as:where f (•) and f (•) are teacher classifer and student classifier respectively, f (•) c indicates the c-th channel of f (•), and C is the total number of channels. Additionally, during training, a learnable instance-level classifier is used on the student to back-propagate the gradients to g (•). The initial weights of f (•) are the same as those of f (•), as the differences in the instance-and bag-level classification boundaries is expected to be minor. To make f (•) not so different from f (•) during training, a weight similarity loss, L w , is further imposed to constrain it by drawing closer their each layer's outputs under the same input. By applying L w , the patch embeddings from g (•) can still suit the bag-level classification task well, rather than being tailored solely for the instance-level classifier f (•). L w is defined as:where f (•) l c indicates the c-th channel of l-th layer's output in f (•). The overall loss function for this step is L c + αL w , with α set to 0.5 in our experiments."
Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,3,Experiments,
Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,3.1,Datasets,"Our experiments utilized two datasets, with the first being the publicly available breast cancer dataset, Camelyon16 [1]. This dataset consists of a total of 399 WSIs, with 159 normal and 111 metastasis WSIs for the training set, and the remaining 129 for test. Although patch-level labels are officially provided in Camelyon16, they were not used in our experiments.The second dataset is a private hepatocellular carcinoma (HCC) dataset collected from Sir Run Run Shaw Hospital, Hangzhou, China. This dataset comprises a total of 1140 valid tumor WSIs scanned at 40× magnification, and the objective is to identify the severity of each case based on the Edmondson-Steiner (ES) grading. The ground truth labels are binary classes of low risk and high risk, which were provided by experienced pathologists."
Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,3.2,Implementation Details,"For Camelyon16, we tiled the WSIs into 256×256 patches on 20× magnification using the official code of [25], while for the HCC dataset the patches are 384×384 on 40× magnification following the pathologists' advice. For both datasets, we used an ImageNet pre-trained ResNet50 to initialize g(•). The instance embedding process was the same of [16], which means for each patch, it would be "
Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,,Ours,"(w/ AB-MIL) 90.0 firstly embedded into a 1024-dimension vector, and then be projected to a 512dimension hidden space for further bag-level training. For the training of bag classifier f (•), we used an initial learning rate of 2e-4 with Adam [11] optimizer for 200 epochs with batch size being 1. Camelyon16 results are reported on the official test split, while the HCC dataset used a 7:1:2 split for training, validation and test. For the training of patch embedder g(•), we used an initial learning rate of 1e-5 with Adam [11] optimizer with the batch size being 100. Three metrics were used for evaluation. Namely, area under curve (AUC), F1 score, and slide-level accuracy (Acc). Experiments were all conducted on a Nvidia Tesla M40 (12GB)."
Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,3.3,Experimental Results,"Ablation Study. The results of ablation studies are presented in Table 1. From Table 1(a), we can learn that as the number of ICMIL iteration increases, the performance will also go up until reaching a stable point. Since the number of instances is very large in WSI datasets, we empirically recommend to choose to run ICMIL one iteration for fine-tuning g(•) to achieve the balance between performance gain and time consumption. From Table 1(b), it is shown that our teacher-student-based method outperforms the naïve ""pseudo label generation"" method for fine-tuning g(•), which demontrates the effectiveness of introducing the learnable instance-level classifier f (•).Comparison with Other Methods. Experimental results are presented in Table 2. As shown, our ICMIL framework consistently improves the performance of three different MIL baselines (i.e., Max Pool, AB-MIL, and DTFD-MIL), demonstrating the effectiveness of bridging the loss back-propagation from bag calssifier to embedder. It proves that a more suitable patch embedding can greatly enhance the overall MIL classification framework. When used with the state-of-the-art MIL method DTFD-MIL, ICMIL further increases its performance on Camelyon16 by 0.5% AUC, 2.1% F1, and 1.6% Acc. Results on the HCC dataset also proves the effectiveness of ICMIL, despite the minor difference on the relative performance of baseline methods. Mean Pooling performs better on this dataset due to the large area of tumor in the WSIs (about 60% patches are tumor patches), which mitigates the impact of average pooling on instances. Also, the performance differences among different vanilla MIL methods tends to be smaller on this dataset since risk grading is a harder task than Camelyon16. In this situation, the quality of instance representations plays a crucial role in generating more separable bag-level representations. As a result, after applying ICMIL on the MIL baselines, these methods all gain great performance boost on the HCC dataset. Furthermore, Fig. 5 displays the instance-level and bag-level representations of Camelyon16 dataset before and after applying ICMIL on AB-MIL backbone.The results indicate that one iteration of g(•) fine-tuning in ICMIL significantly improves the instance-level representations, leading to a better aggregated baglevel representation naturally. Besides, the bag-level representations are also more closely aligned with the instance representations, proving that ICMIL can reduce the inconsistencies between g(•) and f (•) by coupling them together for training, resulting in a better separability."
Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,4,Conclusion,"In this work, we propose ICMIL, a novel framework that iteratively couples the feature extraction and bag classification stages to improve the accuracy of MIL models. ICMIL leverages the category knowledge in the bag classifier as pseudo supervision for embedder fine-tuning, bridging the loss propagation from classifier to embedder. We also design a two-stream model to efficiently facilitate such knowledge transfer in ICMIL. The fine-tuned patch embedder can provide more accurate instance embeddings, in return benefiting the bag classifier. The experimental results show that our method brings consistent improvement to existing MIL backbones."
Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,,Fig. 1 .,
Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,,Fig. 2 .,
Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,,Fig. 3 .,
Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,,Fig. 4 .,
Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,,Fig. 5 .,
Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,,Table 1 .,
Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,,Table 2 .,
Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,,Acknowledgements,". This work was supported by the National Key Research and Development Project (No. 2022YFC2504605), National Natural Science Foundation of China (No. 62202403) and Hong Kong Innovation and Technology Fund (No. PRP/034/22FX). It was also supported in part by the Grant in Aid for Scientific Research from the Japanese Ministry for Education, Science, Culture and Sports (MEXT) under the Grant No. 20KK0234, 21H03470."
Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 45.
TSegFormer: 3D Tooth Segmentation in Intraoral Scans with Geometry Guided Transformer,1,Introduction,"Deep learning is becoming increasingly popular in modern orthodontic treatments for tooth segmentation in intraoral scans (IOS), cone-beam CT (CBCT) and panoramic X-ray [9,15]. Accurate tooth segmentation in 3D IOS dental models is crucial for orthodontics treatment such as diagnosis, tooth crown-root analysis and treatment simulation [10,25]. Specifically, tooth segmentation classifies each triangular face of a 3D IOS tooth model with about 100,000 to 400,000 faces and a spatial resolution of 0.008-0.02mm into teeth and gingiva categories, following the Federation Dentaire Internationale (FDI) standard [8].There are two main categories for tooth segmentation in IOS: conventional methods that handle 2D image projections [10,21,24] or directly operate on 3D IOS meshes [17,22,25,29], and deep learning methods that operate on meshes or point clouds [1,3,6,7,11,12,14,18,23,27,30]. However, many challenges persist. Complicated morphological topology or dental diseases (e.g. crowded or erupted teeth) can lead to unsatisfactory segmentation performance [6]. Additionally, current methods often fail to recognize mesh faces between adjacent teeth or the tooth and gingiva, requiring time-consuming post-processing to refine the noisy boundary segmentation [6,12,23]. Moreover, the state-of-the-art works such as MeshSegNet [12], TSGCNet [27] and DCNet [6] have only been evaluated with a limited amount of data samples and the clinical applicability need to be evaluated with large-scale dataset or in real-world scenarios.Inspired by the success of transformers in various tasks [2,4,5,13,19,28], we propose a novel 3D transformer framework, named TSegFormer, to address the aforementioned challenges. In particular, the tooth segmentation task on 3D IOSs is formulated as a semantic segmentation task on point clouds sampled from raw IOS meshes. We design the 3D transformer with tailored self-attention layers to capture long-range dependencies among different teeth, learning expressive representations from inherently sophisticated structures across IOSs. In addition, we design a multi-task learning paradigm where another auxiliary segmentation head is introduced to assist in delimiting teeth and gingiva. Furthermore, in view of the confusing boundary segmentation, we devise a novel geometry guided loss based on a newly-defined point curvature to help learn accurate boundaries. The network is trained in an end-to-end manner and requires no complicated post-processing during inference, making it appealing to practical applications.We collect a large-scale, high-resolution and heterogeneous 3D IOS dataset with 16,000 dental models where each contains over 100,000 triangular faces. To the best of our knowledge, it is the largest IOS dataset to date. Experimental results show that TSegFormer has reached 97.97% accuracy, 94.34% mean intersection over union (mIoU) and 96.01% dice similarity coefficient (DSC) on the large-scale dataset, outperforming previous works by a significant margin. To summarize, our main contributions are:-We design a novel framework for 3D tooth segmentation with a tailed 3D transformer and a multi-task learning paradigm, aiming at distinguishing the permanent teeth with divergent anatomical structures and noisy boundaries.-We design a geometry guided loss based on a novel point curvature for endto-end boundary refinement, getting rid of the two-stage and time-consuming post-processing for boundary smoothing. -We collect the largest ever 3D IOS dataset for compelling evaluation. Extensive experiments, ablation analysis and clinical applicability test demonstrate the superiority of our method, which is appealing in real-world applications. "
TSegFormer: 3D Tooth Segmentation in Intraoral Scans with Geometry Guided Transformer,2.1,Overview,"The overall pipeline is illustrated in Fig. 1. The original mesh M is converted to a point cloud P by taking the gravity center point of each mesh face. We downsample a point cloud P with N = 10, 000 points from P , and extract the input feature matrix h in ∈ R N ×8 as defined below. The network first employs a point embedding module to capture abundant local structure information h pe from h in . Thereafter, we design the 3D transformer encoder with self-attention layers to capture high-level semantic representations h a . With h a , the main segmentation head produces prediction scores ŷseg ∈ R N ×33 (32 permanant teeth and the gingiva), while the auxiliary head generates prediction scores ŷaux ∈ R N ×2 to assist distinguishing the tooth-gingiva boundary. Furthermore, we devise a geometry guided loss L geo , which is integrated with the main segmentation loss L seg and the auxiliary loss L aux to attain superior performance. During inference, we will extract the features h in ∈ R N ×8 for all points in P , process P into multiple sub-point clouds each with N points, then generate predictions for each point with N N rounds of inference, and map them back to raw mesh M ."
TSegFormer: 3D Tooth Segmentation in Intraoral Scans with Geometry Guided Transformer,2.2,TSegFormer Network Architecture,"Feature Extraction. We first transform input meshes to point clouds as directly handling meshes with deep nets is computationally expensive, especially for high-resolution IOSs. To compensate for potential topology loss, we extract 8-dimensional feature vectors h in ∈ R N ×8 /R N ×8 for each point to preserve sufficient geometric information, including the point's 3D Cartesian coordinates, 3-dimensional normal vector of mesh face, the Gaussian curvature and a novel point ""curvature"" m i . The m i is defined as, where n i is the i-th point's normal vector, K(i) is the second-order neighborhood of the i-th point, |K(i)| is the number of points in K(i), and θ(•, •) denotes the angle in radians between two vectors. By definition, the curvature of a point reflects how much the local geometric structure around this point is curved, i.e., the local geometry on 3D tooth point clouds. Backbone Network. Delineating complicated tooth-tooth or tooth-gingiva boundaries requires decent knowledge of local geometry in IOS. Hence, we first learn local dependencies from the input h in . In particular, we design a point embedding module composed of two linear layers and two EdgeConv layers [20], which takes h in as input and learn local features h pe ∈ R N ×de . The point embedding module enriches point representations with local topological information, with ablation results in the Supplementary Material (SM ) Table 1.Meantime, in view of the inherently sophisticated and inconsistent shapes and structures of the teeth, and the ability of attention mechanism to capture long-range dependencies and suitability for handling unordered point cloud data [2,4,13,19,28], we build an encoder module based on it. The encoder module, composed of four successive self-attention layers and a linear transformation, further yields the high-level point feature maps h p . To avoid misjudging jaw categories, an extra 2D category vector V is fed as input to help distinguish the maxillary and mandible and obtain the global feature maps h g . Specifically, h g = σ(V ) ⊕ MP (h p ) ⊕ AP (h p ), where ""MP"" and ""AP"" respectively denote the max and average pooling; ⊕ denotes concatenation and σ(•) is a linear layer. Finally, we obtain feature maps h a for all points, where h a = h p ⊕ h g . Segmentation Heads. To improve the network's ability to recognize different tooth and gingiva categories, we design two segmentation heads. The main segmentation head, an MLP (MLP seg ), generates point classification scores for 33 classes ŷseg = MLP seg (h a ) ∈ R N ×33 for tooth segmentation. Meanwhile, considering the prevalence of incorrect prediction of tooth-gingiva boundaries, we design an auxiliary segmentation head MLP aux to provide binary classification scores for each point belonging to either tooth or gingiva, i.e., ŷaux = MLP aux (h a ) ∈ R N ×2 . Experimental results indicate that the cooperation with MLP aux can refine tooth-gingiva segmentation boundary.Geometry Guided Loss. Previous methods are usually unsatisfactory to delineate the complicated tooth-tooth boundaries. Observing that points with high point curvatures often lie on the upper sharp ends of tooth crowns and the teeth boundaries (Fig. 2(c)), where mispredictions usually occur, we define the novel geometry guided loss L geo . L geo encourages TSegFormer to adaptively focus more on error-prone points with higher point curvatures with negligible extra computations. Specifically, we define it aswhere γ is the modulating factor (empirically set to 2 in experiments); y S(r)i ∈ R 33 represents the gold label of the i-th point in the point set S(r); pgeo ic denotes the predicted probability of the i-th point belonging to the c-th class, and Φ(y i , c) is an indicator function which outputs 1 if y i = c and 0 otherwise. Concretely, S(r) is a set of points whose point curvatures m i are among the top r • 100% (0 < r ≤ 1) of all N points, i.e., S(rThe experimental results (Fig. 2(d)) on a dataset of 2,000 cases indicate that L geo is more effective with our point curvature over traditional mean and Gaussian curvatures, even they are worst than no curvature. This is because our point curvature provides more clear tooth-tooth and tooth-gingiva boundary indications (Fig. 2(a)-2(c)), thus avoiding misleading the model to focus too much on unimportant non-boundary points.We employ the cross entropy loss as the loss of main segmentation head (L seg ) and the loss of auxiliary segmentation head (L aux ). The total loss L total is computed by combining L seg , L aux for all points and L geo for hard points: L total = L seg +ω geo •L geo +ω aux •L aux . We set the weights ω geo = 0.001, ω aux = 1 and the ratio r = 0.4, and detailed hyperparameter search results in SM Fig. 1 indicate that the performance is stable across different hyperparameter settings."
TSegFormer: 3D Tooth Segmentation in Intraoral Scans with Geometry Guided Transformer,3,Experiments,
TSegFormer: 3D Tooth Segmentation in Intraoral Scans with Geometry Guided Transformer,3.1,Dataset and Experimental Setup,"We construct a large-scale 3D IOS dataset consisting of 16,000 IOS meshes with full arches (each with 100,000 to 350,000 triangular faces) collected between 2018-2021 in China, with evenly distributed maxillary and mandible scans labeled by human experts. Detailed data statistics are presented in SM Table 2, and 39.8% of the data have third-molars, 16.8% suffer from missing teeth, which all reveal the complexity of the dataset. The dataset is randomly split into training (12,000 IOSs), validation (2,000 IOSs) and test sets (2,000 IOSs). Furthermore, we collect an external dataset with 200 complex cases (disease statistics shown in SM Table 3) to evaluate the real-world clinical applicability of TSegFormer. Detailed training and architecture settings are in SM Tables 4 and5."
TSegFormer: 3D Tooth Segmentation in Intraoral Scans with Geometry Guided Transformer,3.2,Main Results on Tooth Segmentation,"To our best knowledge, there has been no prior work on Transformer-based segmentation on non-Euclidean 3D tooth point clouds/meshes. Hence, we compare our TSegFormer to seven representative and state-of-the-art baselines from three categories: 1) neural networks for point clouds, including PointNet++ [16] and DGCNN [20]; 2) transformers for point clouds, including point transformer [28] and PVT [26]; 3) domain-specific architectures for 3D tooth segmentation, including MeshSegNet [12], TSGCNet [27] and DC-Net [6]. For fair comparison, baselines that cannot achieve raw-resolution mesh prediction followed the same inference protocols in 2.1, while the rest kept their original inference schemes. We can firstly observe that TSegFormer outperforms existing best-performing point transformer model [28] by 0.16% in accuracy, 1.04% in mIoU and 0.71% in DSC (Table 1). Such an improvement is surely significant considering the complicated real-world cases in our large-scale dataset and the relatively high performance of point transformer with an mIoU of 93.30%. Moreover, TSeg-Former consistently surpassed all baselines on both mandible and maxillary in terms of all metrics, demonstrating its universal effectiveness.It is important to integrate advanced architectures with domain-specific design for superior performance. We can notice that though MeshSegNet, TSGC-Net, and DCNet are all domain-specific 3D tooth segmentation models, their performance, though on par with PVT and DGCNN, is worse than the point transformer. This is also consistent with the superior performance of transformerbased models on standard point cloud processing tasks, which could be mainly attributed to the larger dataset and powerful attention mechanism that better capture global dependencies. Hence, though models like MeshSegNet adopt some task-specific designs to achieve good performance, they still lag behind point transformer when a huge amount of data samples are available. In contrast, our TSegFormer employs the attention mechanism for point representation learning, and meanwhile, adopted task-specific architectures and geometry guided loss to further boost the performance. More statistical results are in SM Table 2. "
TSegFormer: 3D Tooth Segmentation in Intraoral Scans with Geometry Guided Transformer,3.3,Ablation Studies,"Effectiveness of Geometry Guided Loss. Table 2 shows that introducing the geometry guided loss can improve the performance under all three metrics, e.g., around 0.4% improvement in mIoU. Besides, we show the universal effectiveness of the geometry guided loss by adding it to DCNet [6]. The performance of DCNet is also enhanced by 1.43% in mIoU (Table 3) with this additional loss.Effectiveness of the Auxiliary Segmentation Head. The auxiliary segmentation head is designed to rectify the inaccuracy brought by mislabeling teeth and gingiva near their boundaries. Adding a loss for the auxiliary branch leads to about 0.4% mIoU performance improvement (Table 2).   "
TSegFormer: 3D Tooth Segmentation in Intraoral Scans with Geometry Guided Transformer,,Effectiveness on,
TSegFormer: 3D Tooth Segmentation in Intraoral Scans with Geometry Guided Transformer,3.4,Clinically Applicability Test and Visualization,"To show the effectiveness of TSegFormer in real-world scenarios, we conducted a clinical applicability test (Table 5) on a dataset with 200 complex IOS scans, whose diseases statistics are in SM Table 3. The segmentation with five different models were evaluated by a committee of dentists with more than 5-year experience. We can notice that TSegFormer significantly outperforms the other models regarding the clinical error rate. The feedback from dentists indicates that models such as TSGCNet cannot meet the requirement when dealing with complicated boundaries, while TSeg-Former apparently handles them better. The point transformer and DCNet also showed promising performance, but they are yet far behind our TSegFormer. As for the number of parameters and inference time, though TSegFormer has the second most parameters among all methods we tested, it is the second fastest method that only takes around 23 s to complete inference for 200 cases, which is certainly acceptable in real-world clinical scenarios.By visualization, we show the superiority of TSegFormer on various complicated dental diseases in Fig. 3. The baselines unavoidably produce false predictions or even fail to identify an entire third-molar, while TSegFormer can yield more accurate segmentation and smoother boundaries (see SM Fig. 2 for details), corroborating great potential for clinical applications. Specifically, SM Fig. 3 shows that with our geometry guided loss and auxiliary head, the isolated mispredictions and boundary errors are greatly reduced. However, TSegFormer fails in some complex samples, e.g. the missing tooth, the erupted wisdom tooth and sunken gingiva and alveolar bone regions, as illustrated in SM Fig. 4, which needs to be further studied."
TSegFormer: 3D Tooth Segmentation in Intraoral Scans with Geometry Guided Transformer,4,Conclusion,"We propose TSegFormer, a 3D transformer-based model for high-resolution IOS segmentation. It combines a point embedding module and attention mechanism to effectively capture local and global features, and introduces a geometry guided loss based on a novel point curvature to handle boundary errors and multi-task segmentation heads for boundary refinement. Results of comprehensive experiments on a large-scale dataset and clinical applicability tests demonstrate TSeg-Former's state-of-the-art performance and its great potential in digital dentistry."
TSegFormer: 3D Tooth Segmentation in Intraoral Scans with Geometry Guided Transformer,,Fig. 1 .,
TSegFormer: 3D Tooth Segmentation in Intraoral Scans with Geometry Guided Transformer,,Fig. 2 .,
TSegFormer: 3D Tooth Segmentation in Intraoral Scans with Geometry Guided Transformer,,Fig. 3 .,
TSegFormer: 3D Tooth Segmentation in Intraoral Scans with Geometry Guided Transformer,,Table 1 .,
TSegFormer: 3D Tooth Segmentation in Intraoral Scans with Geometry Guided Transformer,,Table 2 .,
TSegFormer: 3D Tooth Segmentation in Intraoral Scans with Geometry Guided Transformer,,Table 3 .,
TSegFormer: 3D Tooth Segmentation in Intraoral Scans with Geometry Guided Transformer,,Table 4 .,
TSegFormer: 3D Tooth Segmentation in Intraoral Scans with Geometry Guided Transformer,,,
TSegFormer: 3D Tooth Segmentation in Intraoral Scans with Geometry Guided Transformer,,Table 5 .,#param: number of parameters in the network. Inf-T: inference time for 200 cases. Model #success ↑ #fail ↓ clinical error rate (%) ↓ #param ↓ Inf-T(s) ↓
TSegFormer: 3D Tooth Segmentation in Intraoral Scans with Geometry Guided Transformer,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2_41.
Artifact Restoration in Histology Images with Diffusion Probabilistic Models,1,Introduction,"Histology is critical for accurately diagnosing all cancers in modern medical imaging analysis. However, the complex scanning procedure for histological wholeslide images (WSIs) digitization may result in the alteration of tissue structures, due to improper removal, fixation, tissue processing, embedding, and storage [11]. Typically, these changes in tissue details can be caused by various extraneous factors such as bubbles, tissue folds, uneven illumination, pen marks, altered staining, and etc [13]. Formally, the changes in tissue structures are known as artifacts. The presence of artifacts not only makes the analysis more challenging for pathologists but also increases the risk of misdiagnosis for Computer-Aided Diagnosis (CAD) systems [14]. Particularly, deep learning models, which have become increasingly prevalent in histology analysis, have shown vulnerability to the artifact, resulting in a two-times increase in diagnosis errors [18].  [19] formulates the artifact restoration as an image-to-image transfer problem. It leverages two pairs of the generator and discriminator to learn the transfer between the artifact and artifactfree image domains. (b) Diffusion probabilistic model [5] (ours) formulates artifact restoration as a regional denoising process.In real clinical practice, rescanning the WSIs that contain artifacts can partially address this issue. However, it may require multiple attempts before obtaining a satisfactory WSI, which can lead to a waste of time, medical resources, and deplete tissue samples. Discarding the local region with artifacts for deep learning models is another solution, but it may result in the loss of critical contextual information. Therefore, learning-based artifact restoration approaches have gained increasing attention. For example, CycleGAN [19] formulates the artifact restoration as an image-to-image transfer problem by learning the transfer between the artifact and artifact-free image domains from unpaired images, as depicted in Fig. 1(a). However, existing artifact restoration solutions are confined to Generative Adversarial Networks (GANs) [2], which are difficult to train due to the mode collapse and are prone to suffer from unexpected stain style mistransfer. To address these issues, we make the first attempt at a diffusion probabilistic model for artifact restoration approach [5], as shown in Fig. 1(b). Innovatively, our framework formulates the artifact restoration as a regional denoising process, which thus can to the most extent preserve the stain style and avoid the loss of contextual information in the non-artifact region. Furthermore, our approach is trained solely with artifact-free images, which reduces the difficulty in data collection.The major contributions are two-fold. (1) We make the first attempt at a denoising diffusion probabilistic model for artifact removal, called ArtiFusion. This approach differs from GAN-based methods that require either paired or unpaired artifacts and artifact-free images, as our ArtiFusion relies solely on artifact-free images, resulting in a simplified training process. (2) To capture the local-global correlations in the gradual regional artifact restoration process, we innovatively propose a Swin-Transformer denoising architecture to replace the commonly-used U-Net and a time token scheme for optimal Swin-Transformer denoising. Extensive evaluations on real-world histology datasets and downstream tasks demonstrate the superiority of our framework in artifact removal performance, which can generate reliable restored images while preserving the stain style. "
Artifact Restoration in Histology Images with Diffusion Probabilistic Models,2,Methodology,"Overall Pipeline. The proposed histology artifact restoration diffusion model ArtiFusion, comprises two stages, namely the training, and inference. During the training stage, ArtiFusion learns to generate regional histology tissue structures based on the contextual information from artifact-free images. In the inference stage, ArtiFusion formulates the artifact restoration as a gradual denoising process. Specifically, it first replaces the artifact regions with Gaussian noise, and then gradually restores them to artifact-free images using the contextual information from nearby regions.Diffusion Training Stage. The proposed ArtiFusion learns the capability of generating local tissue representation from contextual information during the training stage. To achieve this, we follow the formulations of DDPM [5], which involve a forward process that gradually injects Gaussian noise into an artifact-free image and a reverse process that aims to reconstruct images from noise. During the forward process, we can obtain a noisy version of x t for arbitrary timestep t ∈ N[0, T ] using a Gaussian transition kernel q, where β t ∈ (0, 1) are predefined hyper-parameters [5]. Simultaneously, the reverse process trains a denoising network p θ (x t-1 |x in t ), which is parameterized by θ, to reverse the forward process q(x t |x t-1 ). The overall training objective L is defined as the variational lower bound of the negative log-likelihood, given by:This formulation is extended in DDPM [5] to be further written as:where Artifact Restoration in Inference Stage. During the inference stage, we first use a threshold method to detect the artifact region in the input image x 0 . Then, unlike the conventional diffusion models [5] that aim to generate the entire image, ArtiFusion selectively performs denoising resampling only in the artifact region to maximally preserve the original morphology and stain style in the artifact-free region, as shown in Fig. 2. Specifically, we represent the artifactfree region and the artifact region in the input image as x 0 (1-m) and x 0 m, respectively [10], where m is a Boolean mask indicating the artifact region and is the pixel-wise multiplication operator. To perform the denoising resampling, we write the input image x in t at each reverse step from t to t -1 as the sum of the diffused artifact-free region and the denoised artifact region, i.e.,where ). Consequently, the final restored image is obtained asSwin-Transformer Denoising Network. To capture the local-global correlation and enable the denoising network to effectively restore the artifact regions, we propose a novel Swin-Transformer-basedr [9] denoising network for ArtiFusion. As shown in Fig. 3, our network follows a U-shape architecture, where the encoder, bottleneck, and decoder modules all employ Swin-Transformer as the basic building block. Additionally, we introduce an innovative auxiliary time token to inject the time information. In an arbitrary time step t during the training process, to obtain a time token, we first embed the scalar t by learnable linear layers, with weights that are specific to each Swin-Transformer block. In contrast to existing U-Net based denoising networks [5], we propose a better interaction between hidden features and time information by concatenating the time token to feature tokens before passing them to the attention layers. The resulting tokens are then processed by the attention layers, and the auxiliary time token is discarded to retain the original feature dimension to fit the Swin-Transformer block design after the attention layers."
Artifact Restoration in Histology Images with Diffusion Probabilistic Models,3,Experiments,"Dataset. To evaluate the performance of artifact restoration, a training set is curated from a subset of Camelyon17 [8] 1 . It comprises a total number of 2445 artifact-free images and another 2547 images with artifacts, where all histological images are scaled to the resolution of 256 × 256 pixels at the magnitude of 20×. The test set uses another public histology image dataset [6] with 462 artifact-free images 2 , where we obtain the paired artifact images by the manually-synthesized artifacts [18]. Fig. 4. Artifact restoration on five real-world artifact images. We observe that ArtiFusion can successfully overcome the drawback of stain style mistransfer in CycleGAN. We also illustrate the gradual denoising process in the artifact region by ArtiFusion, at time step t = 0, 50, 100, 150. It highlights the ability of ArtiFusion to progressively remove artifacts from the histology image, resulting in a final restored image that is both visually pleasing and scientifically accurate.Implementations. We implement the proposed ArtiFusion and its counterpart in Python 3.8.10 and PyTorch 1.10.0. All experiments are carried out in parallel on two NVIDIA RTX A4000 GPU cards with 16 GiB memory. Hyperparameters are as follows: a learning rate of 10 -4 with Adam optimizer, the total timesteps is set to 250.Compared Methods and Evaluation Metrics. As a proof-of-concept attempt at a generative-models-based artifact restoration framework in the histology domain, currently, there are limited available literature works and opensourced codes for comparison. Consequently, we leverage the prevalent Cycle-GAN [19] as the baseline for comparison, because of its excellent performance in the image transfer, and also its nature that requires no paired data can fit our circumstance. Unlike CycleGAN which requires both artifact-free images and artifact images, ArtiFusion only relies on artifact-free images, leading to a size of the training set that is half that of CycleGAN. For a fair compaison, we train the CycleGAN with two configurations, namely (#1) using the entire dataset, and (#2) using only half the dataset, where the latter uses the same number of the training samples as ArtiFusion. Regarding the ablation, we compare the proposed Swin-Transformer denoising network with the conventional U-Net [5] (denoted as 'U-Net'), and the time token scheme with the direct summation scheme (denoted as 'Add'). We use the following metrics: L 2 distance (L2) with respect to the artifact region, the mean-squared error (MSE) over the whole image, structural similarity index (SSIM) [15], Peak signal-tonoise ratio (PSNR) [1], Feature-based similarity index (FSIM) [17] and Signal to reconstruction error ratio (SRE) [7].Table 1. Quantitative comparison of ArtiFusion with CycleGAN on artifact restoration performance. The ↓ indicates the smaller value, the better performance; and vice versa.  1, where some exemplary images are illustrated in Fig. 4. Our results demonstrate the superiority of ArtiFusion over GAN in the context of artifact restoration, with a large margin observed in all evaluation metrics. For instance, ArtiFusion can reduce the L2 and MSE by more than 50%, namely from 1 × 10 4 to 0.5 × 10 4 and from 0.55 to 0.25 respectively. It implying that our method can to the large extent restore the artifact regions using the global information. In addition, ArtiFusion can improve other metrics, including SSIM, PSNR, FSIM and SRE by 0.0204, 5.72, 0.1028 and 4.02 respectively, indicating that it can preserve the stain style during the restoration process. Moreover, our ablation study shows that the Swin-Transformer denoising network can outperform the conventional U-Net, highlighting the significance of capturing global correlation for local artifact restoration. Finally, the concatenating time token with feature tokens can bring an improvement in terms of all evaluation matrices, making it a better fit for the transformer architecture than the direct summation scheme in U-Net [5]. In summary, our ablations confirm the effectiveness of all the components in our method. Evaluations by Downstream Classification Task. We further evaluate the proposed artifact restoration framework on a downstream tissue classification task. To this end, we use the public dataset NCT-CRC-HE-100K for training and CRC-VAL-HE-7K for testing, which together contains 100, 000 training samples and 7, 180 test samples. We consider the performance on the original unprocessed data, denoted as 'Clean', as the upper bound. Then, we manually synthesize the artifact (denoted as 'Artifact') and evaluate the classification performance with restoration approaches CycleGAN and ArtiFusion. In Table 3, comparisons show that the presence of artifacts can result in a significant performance decline of over 5% across all five network architectures. Importantly, the classification accuracy on images restored with ArtiFusion is consistently higher than those restored with CycleGAN, demonstrating the superiority of our model. These results highlight the effectiveness of ArtiFusion as a practical pre-processing method for histology analysis."
Artifact Restoration in Histology Images with Diffusion Probabilistic Models,4,Conclusion,"In this paper, we propose ArtiFusion, the first attempt at a diffusion-based artifact restoration framework for histology images. With a novel Swin-Transformer denoising backbone, ArtiFusion is able to restore regional artifacts using the context information, while preserving the tissue structures in artifact-free regions as well as the stain style. Experimental results on a public histological dataset demonstrate the superiority of our proposed method over the state-of-the-art GAN counterpart. Consequently, we believe that our proposed method has the potential to benefit the medical community by enabling more accurate diagnosis or treatment planning as a pre-processing method for histology analysis. Future work includes investigating the extension of ArtiFusion to more advanced diffusion models such as score-based or score-SDE models [16]."
Artifact Restoration in Histology Images with Diffusion Probabilistic Models,,Fig. 1 .,
Artifact Restoration in Histology Images with Diffusion Probabilistic Models,,Fig. 2 .,
Artifact Restoration in Histology Images with Diffusion Probabilistic Models,,Fig. 3 .,
Artifact Restoration in Histology Images with Diffusion Probabilistic Models,,,
Artifact Restoration in Histology Images with Diffusion Probabilistic Models,,,"sample t o (1-m) is artifact-free region diffused for t times using the Gaussian transition kernel i.e. x sample t ∼ N ( √ ᾱt x 0 , (1-ᾱt I)) with ᾱt = t i=1 (1-β i ); and x"
Artifact Restoration in Histology Images with Diffusion Probabilistic Models,,Table 2 .,
Artifact Restoration in Histology Images with Diffusion Probabilistic Models,,Table 3 .,
STAR-Echo: A Novel Biomarker for Prognosis of MACE in Chronic Kidney Disease Patients Using Spatiotemporal Analysis and Transformer-Based Radiomics Models,1,Introduction,"Cardiovascular disease (CVD) is the most common cause of mortality among patients with Chronic Kidney disease (CKD), with CVD accounting for 40-50% deaths in patients with acute CKD [1]. Echocardiography (Echo), a non-invasive imaging modality, provides a quick critical assessment of cardiac structure and function for cardiovascular disease diagnosis. Despite its usefulness, due to its nature, echo data is noisy and have poor resolution, which presents challenges in effectively interpreting and analyzing them. Measurements like Left ventricle (LV) volume captured by ejection fraction (EF), LV mass and LV geometry have been standard biomarkers for diagnosing and predicting severity of CVD [2][3][4][5][6][7]. With the availability of large-scale public dataset like EchoNet [7], various works have even assimilated recent deep learning advances like graphs CNNs [8] and transformers [9] for EF estimation using automated LV segmentation from echo videos. However, recent studies on CKD patients demonstrated that standard echo measurements based on static LV volume and morphology, such as EF, may have limited prognostic value beyond baseline clinical characteristics, as adverse outcomes are to the common occurrence inspite of a preserved EF [10][11][12]. As a result, studies have utilized machine learning to provide more detailed links to cardiac structure and function using echo data (measurements, images and videos) [10][11][12][13][14]. LV Wall (LVW) alterations have been reported as non-traditional biomarker of CVD due to pathophysiological changes, in CKD patients [15,16]. Abnormalities of the LVW motion are known prognostic marker for MACE prediction in CVD patients. [17][18][19]. Longitudinal dysfunction common in CKD patients, is reflected in LVW with change in morphology [13]. This dysfunction has been associated with CKD progression [10,12] and abnormalities are evident even in early stages of CKD [13]. Thus, an investigation of LVW morphology and longitudinal changes is warranted [10].Radiomic based interpretable features have been used to model cardiac morphology for predicting disease outcomes [2]. Deep learning based transformer architectures [20] have recently been popular for modelling of spatiotemporal changes in LV [9]. Though they lack interpretability, they are able to extract novel features to model the data using attention [20]. Thus, in CKD patients, a combination of radiomic and transformer based spatiotemporal models could associate LVW changes over time with progression of CVD and potentially provide some insight into the factors implicated for MACE outcomes."
STAR-Echo: A Novel Biomarker for Prognosis of MACE in Chronic Kidney Disease Patients Using Spatiotemporal Analysis and Transformer-Based Radiomics Models,2,Prior Work and Novel Contributions,"The relationship between Chronic Kidney disease (CKD) and Cardiovascular disease (CVD) is well established [1,21]. Echo provides a noisy yet effective modality for CVD prediction. Echo data (image, video or clinical measurements) has been extensively analyzed using machine learning and deep learning based techniques, to help in diagnosing heart conditions, predicting their severity, and identifying cardiac disease states [2]. Several studies have employed machine learning to predict diagnostic measurements and global longitudinal strain [22,23]. In recent years, deep learning-based approaches have been successfully used to tackle echocardiographic segmentation, view classification and phase detection tasks [24,25]. With advent of EchoNet [7], a plethora of studies estimating EF and predicting heart failure have come up [8,9] while few others have used deep learning for prognostic prediction using cardiac measurements [23] and to model wall motion abnormalities [18].Echo coupled with machine learning, has been a potent tool for analysis of CVD in patients with CKD [13]. Machine learning based analysis has revealed poor prognostic value of EF and LV based biomarkers for CVD risk assessment in CKD patients. [10,12]. Thus, techniques developed using EchoNet [7] dataset are potentially less useful for modeling CVD outcomes in CKD patients. The need for a novel approach investigating the longitudinal changes in cardiac structure and function has been recommended for analyzing CVD in patients with CKD [10,13].In this study, for the first time, we employ echo video based spatiotemporal analysis of LVW to prognosticate CVD outcomes in kidney disease patients. Our code is available here -https://github.com/rohand24/STAR Echo Key contributions of our work are as follows: * STAR-Echo, a novel biomarker that combines spatiotemporal radiomics and transformer-based models to capture morphological differences in shape and texture of LVW and their longitudinal evolution over time. * Unique interpretable features: spatiotemporal evolution of sphericity and perimeter (shape-based) and LongRunHighGrayLevelEmphasis (texturebased), prognostic for CVD risk in CKD patients * Demonstrated the superiority of STAR-Echo in the prognosis of CVD in CKD patients, compared to individual spatiotemporal models and clinical biomarkers. * An end-to-end automated pipeline for echo videos that can identify heartbeat cycles, segment the LVW, and predict a prognostic risk score for CVD in CKD patients."
STAR-Echo: A Novel Biomarker for Prognosis of MACE in Chronic Kidney Disease Patients Using Spatiotemporal Analysis and Transformer-Based Radiomics Models,3,STAR-ECHO: Methodological Description,
STAR-Echo: A Novel Biomarker for Prognosis of MACE in Chronic Kidney Disease Patients Using Spatiotemporal Analysis and Transformer-Based Radiomics Models,3.1,Notation,"We denote a patient's echo video as a series of two-dimensional frames, represented as V = [I 1 , I 2 , ..., I N ], where I 1 , I 2 , ..., I N represent the N phases (frames) "
STAR-Echo: A Novel Biomarker for Prognosis of MACE in Chronic Kidney Disease Patients Using Spatiotemporal Analysis and Transformer-Based Radiomics Models,3.3,Systolic Phase Extraction,"The heart-beat cycle is captured using the consecutive end-diastolic phases identified by the CNN+LSTM based multi-beat echo phase detection model [25].The heart-beat cycle frame lengths vary with individual patients. To achieve a standard frame length of 30 frames, the videos are processed with the following strategy -in videos with extra frames, random frame dropping based on normal distribution is performed, while in videos with fewer frames, a video-frame interpolation model [27] is utilized to generate frames between randomly selected adjacent frames."
STAR-Echo: A Novel Biomarker for Prognosis of MACE in Chronic Kidney Disease Patients Using Spatiotemporal Analysis and Transformer-Based Radiomics Models,3.4,Automated LVW Segmentation,"Since LVW annotations were only available for the end-diastolic phase in the dataset, a weakly supervised segmentation approach is employed to obtain annotations for all phases of the echo video. Combining the available annotations with the publicly available CAMUS dataset [28], we trained a nnUNet-based U-Net [29] model for LVW segmentation. This model was then used to predict LVW for all phases, which were then checked and corrected by two expert annotators.Masking the input echo video with LVW mask provides the echo video V , a 30-frame sequence of masked LVW image frames I N , to be input to the spatiotemporal models M R and M T (Fig. 1). Additionally, each image frame I N is center-cropped and resized to equal k × k dimensions. Thus the input to transformer model M T is an echo video V R 30×k×k ."
STAR-Echo: A Novel Biomarker for Prognosis of MACE in Chronic Kidney Disease Patients Using Spatiotemporal Analysis and Transformer-Based Radiomics Models,3.5,Spatiotemporal Feature Representation,"Radiomic Time-Series Model: To build this model, we employ a two-stage feature extraction process."
STAR-Echo: A Novel Biomarker for Prognosis of MACE in Chronic Kidney Disease Patients Using Spatiotemporal Analysis and Transformer-Based Radiomics Models,,Radiomic Feature Extraction:,"In first stage, radiomic feature R(I N ) is extracted on each I N of the LVW for each phase (frame) of the echo video, V as given in (1). Radiomic features comprising of shape (10), texture (68) and first-order statistics (19) of the LVW in each phase of the echo video are extracted using pyradiomics [30] python package on the input echo phase. A total of 97 radiomic features are extracted per echo phase (frame) image I N ."
STAR-Echo: A Novel Biomarker for Prognosis of MACE in Chronic Kidney Disease Patients Using Spatiotemporal Analysis and Transformer-Based Radiomics Models,,Time Series Feature Extraction:,"In the second stage, to model the temporal LVW motion, we consider the sequence of radiomic features from each phase of one heartbeat cycle as individual time-series. Thus, a radiomic feature timeseries t R (V ) is given by (2). Time-series feature T R , as given by (3), is extracted for each radiomic feature time-series using the TSFresh library [31]."
STAR-Echo: A Novel Biomarker for Prognosis of MACE in Chronic Kidney Disease Patients Using Spatiotemporal Analysis and Transformer-Based Radiomics Models,,R(I,"Thus, the radiomic time-series model M R is trained on time-series features T R (V ) obtained for each V of the patient to predict the outcome O T ."
STAR-Echo: A Novel Biomarker for Prognosis of MACE in Chronic Kidney Disease Patients Using Spatiotemporal Analysis and Transformer-Based Radiomics Models,,Video Transformer Model:,"The Video Vision Transformer (ViViT) model [26] with factorised encoder is employed for this task. This architecture has two transformer encoders in series. The first encoder captures the spatial relationships between tokens from the same frame and generates a hidden representation for each frame. The second encoder captures the temporal relationships between frames, resulting in a ""late fusion"" of temporal and spatial information. The transformer model M T is trained with input echo video V masked with LVW to predict the patient outcome O T . Supplementary table S3 gives the important hyperparameters used for training the model M T ."
STAR-Echo: A Novel Biomarker for Prognosis of MACE in Chronic Kidney Disease Patients Using Spatiotemporal Analysis and Transformer-Based Radiomics Models,3.6,Model Fusion,"The predictions from radiomic time-series model, M R and video transformer model, M T are fused at the junction D j linear discriminant analysis (LDA) based fusion, with the output prediction probability P given by ( 4)where ω k , ω k0 are the learned parameters of the model and input X i for patient i is the linear combination of the predictions of M R and M T given as in (5),"
STAR-Echo: A Novel Biomarker for Prognosis of MACE in Chronic Kidney Disease Patients Using Spatiotemporal Analysis and Transformer-Based Radiomics Models,4,Experimental Results and Discussion,
STAR-Echo: A Novel Biomarker for Prognosis of MACE in Chronic Kidney Disease Patients Using Spatiotemporal Analysis and Transformer-Based Radiomics Models,4.1,Dataset Description,"The dataset consisted of echo videos from patients with CKD (N = 150) and their composite CVD outcomes and clinical biomarker measurements for Ejection Fraction (EF), B-type natriuretic peptide (BNP) and N-terminal pro-B-type natriuretic peptide (NT-proBNP). It was stratified into 70% training (S t = 101) and 30% holdout set (S v = 44) The reported cardiovascular outcome, O T , is composite of following cardiovascular events -chronic heart failure, myocardial infarction and stroke. The patients were participants of the Chronic Renal Insufficiency cohort (CRIC) [21] and their data was retrospectively made available for this work. The CRIC study [21] is on individuals with mild to moderate CKD, not on dialysis. Diabetes is the main reported comorbidity for the study.The study followed the patients for CVD outcomes. Additional details along with inclusion-exclusion criteria of the dataset are included in the supplementary materials. In the dataset, a total of 46 patients experienced the composite event outcome. The dataset also included survival time information about the patients. The median survival time for patients, O T was 6.5 years (median O - = 6.7 years, median O + = 3.5 years)."
STAR-Echo: A Novel Biomarker for Prognosis of MACE in Chronic Kidney Disease Patients Using Spatiotemporal Analysis and Transformer-Based Radiomics Models,4.2,Statistical and Survival Analysis,"Statistical Feature Selection -Extracted times-series features (T R ) are normalized and multicollinear features are excluded. Analysis of Variance (ANOVA) based stable feature selection (p < 0.002) and Brouta [32] based all-relevant feature selection (α = 0.05, threshold = 0.98) is applied sequentially. A Random forest classifier was trained on the training set using 5-fold cross-validation across 100 iterations, and the best model was selected based on AUC. The selected model was applied to the holdout set, and performance metrics were reported.Survival Analysis -KM plots with log-rank test analyzed the prognostic ability of models. The curves showed survival time (in years) on the horizontal axis and probability of survival on the vertical axis, with each point representing survival probability of patients. Hazard ratios and p-values were reported for significance between low-risk and high-risk cohorts."
STAR-Echo: A Novel Biomarker for Prognosis of MACE in Chronic Kidney Disease Patients Using Spatiotemporal Analysis and Transformer-Based Radiomics Models,4.3,Evaluating Ability of STAR-ECHO to MACE Prognosis,"MACE prognosis is a necessary task in CKD patients management due to high CVD events in CKD patients [33]. Figure 2 shows the KM curves for each indi- vidual models on the holdout set (S v = 44). Significant separation(p < 0.05) is observed for models M R and STAR-Echo. None of the clinical biomarker models have significant separation. Thus, the models trained on EchoNet [7], relying on EF, would not provide a prognostic prediction. Thus, the radiomics features aid in prognosis of MACE risk in CKD patients. The combination model, STAR-Echo, outperforms all the individual models with better risk stratification (p = 0.037 and HR = 2.98) than individual spatiotemporal models(M R : p = 0.042; M T : p = 0.069) and shows high accuracy in prediction compared to M R and M T models. Thus we are able to showcase that saptiotemporal changes in the shape and texture of LVW are prognostic markers for MACE outcomes in CKD patients. Accuracy, area under the ROC curve (AUC), sensitivity, specificity were the prediction metrics and p-value and hazard ratios (HR) were the prognosis metrics observed (Table 1) for all the models, with the model STAR-Echo achieving the highest Accuracy of 70.45% with a significant p-value(= 0.0372) and high HR(= 2.98). No statistically significant difference is observed in AUC performances of the different spatiotemporal models, indicating that the video transformer and the radiomic time-series models are capturing similar spatiotemporal changes in the shape and intensities of the LVW. The individual clinical biomarker based models performed poorly consistent with observations in CKD patients [10,12]. Thus EchoNet [7] based models would perform poorly in predicting the MACE outcomes as well.The top radiomic features (Fig. 3-A) include longitudinal changes in perimeter and sphericity of the LVW shape along with intensity changes in the LVW texture. The texture feature changes in systolic function and the shape differences over different frames can be observed in Fig. 3-B. Changes in the LVW shape can indicate left ventricular hypertrophy (LVH), a common complication of CKD linked to a higher risk of cardiovascular events. LVH can stem from various factors that modify the ventricle's structure and geometry [15]. Texture changes in LV may also reflect alterations in collagen content or fibrosis which increase the risk of adverse events associated with cardiac remodeling [15]."
STAR-Echo: A Novel Biomarker for Prognosis of MACE in Chronic Kidney Disease Patients Using Spatiotemporal Analysis and Transformer-Based Radiomics Models,5,Conclusion,"In this study, we introduced STAR-Echo, a novel biomarker combining radiomics and video transformer-based descriptors to evaluate spatiotemporal changes in LVW morphology. STAR-Echo identifies novel features based on longitudinal changes in LVW shape (perimeter and sphericity) and texture (intensity variations) over a heartbeat cycle, similar to recent clinical pathophysiological findings of CVD in CKD [15]. Results show that STAR-Echo significantly improves CVD prognosis in CKD patients (AU C = 0.71, p = 0.0372, HR = 2.98) compared to clinical biomarkers, potentially outperforming EchoNet [7] based LV volumebased approaches. Future research will validate STAR-Echo in a larger patient population and incorporate clinical data for improved management of CVD and CKD."
STAR-Echo: A Novel Biomarker for Prognosis of MACE in Chronic Kidney Disease Patients Using Spatiotemporal Analysis and Transformer-Based Radiomics Models,,Fig. 1 .,
STAR-Echo: A Novel Biomarker for Prognosis of MACE in Chronic Kidney Disease Patients Using Spatiotemporal Analysis and Transformer-Based Radiomics Models,,Fig. 2 .,
STAR-Echo: A Novel Biomarker for Prognosis of MACE in Chronic Kidney Disease Patients Using Spatiotemporal Analysis and Transformer-Based Radiomics Models,,Fig. 3 .,
STAR-Echo: A Novel Biomarker for Prognosis of MACE in Chronic Kidney Disease Patients Using Spatiotemporal Analysis and Transformer-Based Radiomics Models,,Table 1 .,
Thinking Like Sonographers: A Deep CNN Model for Diagnosing Gout from Musculoskeletal Ultrasound,1,Introduction,"Gout is the most common inflammatory arthritis and musculoskeletal ultrasound (MSKUS) scanning is recommended to diagnose gout due to the non-ionizing radiation, fast imaging speed, and non-invasive characteristics of MSKUS [7]. However, misdiagnosis of gout can occur frequently when a patient's clinical characteristics are atypical. Traditional MSKUS diagnosis relies on the experience of the radiologist which is time-consuming and labor-intensive. Although convolutional neural networks (CNNs) based ultrasound classification models have been successfully used for diseases such as thyroid nodules and breast cancer, conspicuously absent from these successful applications is the use of CNNs for gout diagnosis from MSKUS images. There are significant challenges in CNN based gout diagnosis. Firstly, the gout-characteristics contain various types including double contour sign, synovial hypertrophy, synovial effusion, synovial dislocation and bone erosion, and these gout-characteristics are small and difficult to localize in MSKUS. Secondly, the surrounding fascial tissues such as the muscle, sarcolemma and articular capsule have similar visual traits with gout-characteristics, and we found the existing CNN models can't accurately pay attention to the gout-characteristics that radiologist doctors pay attention to during the diagnosis process (as shown in Fig. 1). Due to these issues, SOTA CNN models often fail to learn the gouty MSKUS features which are key factors for sonographers' decision.In medical image analysis, recent works have attempted to inject the recorded gaze information of clinicians into deep CNN models for helping the models to predict correctly based on lesion area. Mall et al. [9,10] modeled the visual search behavior of radiologists for breast cancer using CNN and injected human visual attention into CNN to detect missing cancer in mammography. Wang et al. [15] demonstrated that the eye movement of radiologists can be a new supervision form to train the CNN model. Cai et al. [3,4] developed the SonoNet [1] model, which integrates eye-gaze data of sonographers and used Generative Adversarial Networks to address the lack of eye-gaze data. Patra et al. [11] proposed the use of a teacher-student knowledge transfer framework for US image analysis, which combines doctor's eye-gaze data with US images as input to a large teacher model, whose outputs and intermediate feature maps are used to condition a student model. Although these methods have led to promising results, they can be difficult to implement due to the need to collect doctors' eye movement data for each image, along with certain restrictions on the network structure.Different from the existing studies, we propose a novel framework to adjust the general CNNs to ""think like sonographers"" from three different levels. (1) Where to adjust: Modeling sonographers' gaze map to emphasize the region that needs adjust; (2) What to adjust: Classify the instances to systemically detect predictions made based on unreasonable/biased reasoning and adjust; (3) How to adjust: Developing a training mechanism to strike the balance between gout prediction accuracy and attention reasonability."
Thinking Like Sonographers: A Deep CNN Model for Diagnosing Gout from Musculoskeletal Ultrasound,2,Method,"Fig. 2. The overall framework of the proposed method.Figure 2 presents the overall framework, which controls CNNs to ""think like sonographers"" for gout diagnosis from three levels. 1) Where to adjust: we model the sonographers' gaze map to emphasize the region that needs control. This part learns the eye gaze information of the sonographers which is collected by the Eye-Tracker. 2) What to adjust: we divide instances into four categories to reflect whether the model prediction given to the instance is reasonable and precise. 3) How to adjust: a training mechanism is developed to strike the balance between gout diagnosis and attention accuracy for improving CNN."
Thinking Like Sonographers: A Deep CNN Model for Diagnosing Gout from Musculoskeletal Ultrasound,2.1,Where to Adjust,"It is essential to obtain the gaze map corresponding to each MSKUS to emphasize the region where gouty features are obvious. Inspired by studies of saliency model [8], we integrate transformer into CNNs to capture multi-scale and longrange contextual visual information for modeling sonographers' gaze map. This gaze map learns the eye gaze information, collected by the Eye-Tracker, of the sonographers when they perform diagnosis. As shown in Fig. 2, this part consists of a CNN encoder for extracting multi-scale feature, a transformer-encoder for capturing long-range dependency, and a CNN decoder for predicting gaze map.The MSKUS image I 0 ∈ R H×W ×3 is first input into CNN encoder that contains five convolution blocks. The output feature maps from the deeper last three convolution blocks are denoted as F 0 , F 1 , F 2 and are respectively fed into transformer encoders to enhance the long-range and contextual information. During the transformer-encoder, we first flatten the feature maps produced by the CNN encoder into a 1D sequence. Considering that flatten operation leads to losing the spatial information, the absolute position encoding [14] is combined with the flatten feature map via element-wise addition to form the input of the transformer layer. The transformer layer contains the standard Multi-head Self-Attention (MSA) and Multi-layer Perceptron (MLP) blocks. Layer Normalization (LN) and residual connection are applied before and after each block respectively.In the CNN decoder part, a pure CNN architecture progressively up-samples the feature maps into the original image resolution and implements pixel-wise prediction for modeling sonographers' gaze map. The CNN decoder part includes five convolution blocks. In each block, 3 × 3 convolution operation, Batch normalization (BN), RELU activation function, and 2-scale upsampling that adopts nearest-neighbor interpolation is performed. In addition, the transformer's output is fused with the feature map from the decoding process by an element-wise product operation to further enhance the long-range and multi-scale visual information. After five CNN blocks, a 3 × 3 convolution operation and Sigmoid activation is performed to output the predicted sonographers' gaze map. We use the eye gaze information of the sonographers which is collected by the Eye-Tracker to restrain the predicted sonographers' gaze map. The loss function is the sum of the Normalized Scanpath Saliency (NSS), the Linear Correlation Coefficient (CC), Kullback-Leibler divergence (KLD) and Similarity (SIM) [2]."
Thinking Like Sonographers: A Deep CNN Model for Diagnosing Gout from Musculoskeletal Ultrasound,2.2,What to Adjust,"Common CNN classification models for gout diagnosis often fail to learn the gouty MSKUS features including the double contour sign, tophus, and snowstorm which are key factors for sonographers' decision. A CAM for a particular category indicates the discriminative regions used by the CNN to identify that category. Inspired by CAM technique, it is needed to decide whether the attention region given to an CNN model is reasonable for diagnosis of gout. We firstly use the Grad-CAM technique [12] to acquire the salient attention region S CAM that CNN model perceives for differential diagnosis of gout. To ensure the scale of the attention region S CAM is the same as the sonographers' gaze map S sono which is modeled by saliency model, we normalize S CAM to the values between 0 and 1, get S CAM . Then we make bit-wise intersection over union(IoU) operations with the S sono and S CAM to measure how well the two maps overlap. Note that we only calculate the part of S CAM that is greater than 0.5. For instances whose IoU is less than 50%, we consider that the model's prediction for that instance is unreasonable. As shown in Fig. 3, when CNN do prediction, we can divide the instances into four categories: RP: Reasonable Precise: The attention region focusses on the gouty features which are important for sonographers' decision, and the diagnosis is precise. RIP: Reasonable Imprecise: Although attention region focusses on the gouty features, while the diagnosis result is imprecise. UP: Unreasonable Precise: Although the gout diagnosis is precise, amount of attention is given to irrelevant feature of MSKUS image. UIP: Unreasonable Imprecise: The attention region focusses on irrelevant features, and the diagnosis is imprecise.Our target of adjustment is to reduce imprecise and unreasonable predictions. In this way, CNNs not only finish correct gout diagnosis, but also acquire the attention region that agreements with the sonographers' gaze map."
Thinking Like Sonographers: A Deep CNN Model for Diagnosing Gout from Musculoskeletal Ultrasound,2.3,How to Adjust,"We proposed a training mechanism (Algorithm 1) which can strike the balance between the gout diagnosis error and the reasonability error of attention region to promote the CNNs to ""think like sonographers"". In addition to reducing the diagnosis error, we also want to minimize the difference between sonographers' gaze map S sono and normalized salient attention region S CAM , which directly leads to our target: The total loss function can be expressed as the weighted sum the gout diagnosis error and the reasonability error, as follows:The gout diagnosis error L diagnosis is calculated by the Cross-entropy loss, and the reasonability is calculated by the L1-loss. This training mechanism uses the quadrant of instances to identify whether samples' attention needs to adjusted. For MSKUS sample in the quadrant of UP, α can be set 0.2 to control the CNN pay more attention to reasonability. Correspondingly, for sample in RIP, α can be set 0.8 to make CNN pay more attention to precise. For sample in RP and UIP, α can be set 0.5 to strike the balance between accuracy and reasonability. Gaze Data Collection. We collected the eye movement data with the Tobii 4C eye-tracker operating at 90 Hz. The MSKUS images were displayed on a 1920 × 1080 27-inch LCD screen. The eye tracker was attached beneath the screen with a magnetic mounting bracket. Sonographers were seated in front of the screen and free to adjust the chair's height and the display's inclination. Binary maps of the same size as the corresponding MSKUS images were generated using the gaze data, with the pixel corresponding to the point of gaze marked with a'1' and the other pixels marked with a'0'. A sonographer gaze map S was generated for each binary map by convolving it with a truncated Gaussian Kernel G(σ x,y ), where G has 299 pixels along x dimension, and 119 pixels along y dimension."
Thinking Like Sonographers: A Deep CNN Model for Diagnosing Gout from Musculoskeletal Ultrasound,3,Experiments,
Thinking Like Sonographers: A Deep CNN Model for Diagnosing Gout from Musculoskeletal Ultrasound,,MSKUS,"Evaluation Metrics. Five metrics were used to evaluate model performance: Accuracy (ACC), Area Under Curve (AUC), Correlation Coefficient (CC), Similarity (SIM) and Kullback-Leibler divergence (KLD) [2]. ACC and AUC were implemented to assess the gout classification performance of each model, while CC, SIM, and KLD were used to evaluate the similarity of the areas that the model and sonographers focus on during diagnoses.Evaluation of ""Thinking like Sonographers"" Mechanism. To evaluate the effectiveness of our proposed mechanism of ""Thinking like Sonographers"" (TLS) that combines ""where to adjust"", ""what to adjust"" and ""how to adjust"", we compared the gout diagnosis results of several classic CNN classification [5,6,13] models without/with our TLS mechanism. The results, shown in Table 1, revealed that using our TLS mechanism led to a significant improvement in all metrics. Specifically, for ACC and AUC, the model with our TLS mechanism achieved better results than the model without it. Resnet34 with TLS acquired the highest improvement in ACC with a 4.41% increase, and Resnet18 with TLS had a 0.027 boost in AUC. Our TLS mechanism consistently performed well in improving the gout classification performance of the CNN models. More comparison results were shown in Appendix Fig. A1 and Fig. A2. The CC, SIM, and KLD metrics were utilized to assess the similarity between the CAMs of classification models and the collected gaze maps, providing an indication of whether the model was able to ""think"" like a sonographer. Table 1 showed that the models with our TLS mechanism achieved significantly better results in terms of CC and SIM (i.e., higher is better), as well as a decline of more than 1.50 in KLD (lower is better), when compared to the original models. This indicated that the models with TLS focused on the areas shown to be similar to the actual sonographers. Furthermore, Fig. 4 illustrated the qualitative results of CAMs of models with and without TLS mechanism. The original models without TLS paid more attention to noise, textures, and artifacts, resulting in unreasonable gout diagnosis. With TLS, however, models could focus on the crucial areas in lesions, allowing them to think like sonographers."
Thinking Like Sonographers: A Deep CNN Model for Diagnosing Gout from Musculoskeletal Ultrasound,,Stability Under Different Gaze Maps via t-Test.,"To evaluate the prediction's stability under the predicted gaze map from the generation model in ""Where to adjust"", we conducted three t-test studies. Specifically, we trained two classification models (M C and M P ), using the actual collected gaze maps, and the predicted maps from the generation model, respectively. During the testing, we used the collected maps as input for M C and M P to get classification results R CC and R P C . Similarly, we used the predicted maps as input for M C andM P As shown in Table 2, the p-values of t-test (1)( 2) and ( 3) are all greater than 0.005, suggesting that no significant difference was observed between the classification results obtained from different generative strategies. This implied that our training mechanism was model-insensitive. Consequently, it was possible to use predicted gaze maps for both the training and testing phases of the classification models without any notable performance decrease. This removed the need to collect eye movement maps during the training and testing phases, significantly lightening the workload of data collection. Therefore, our TLS mechanism, which involved predicting the gaze maps, could potentially be used in clinical environments. This would allow us to bypass the need to collect the real gaze maps of the doctors while classifying newly acquired US images, and thus improved the clinical implications of our mechanism, ""Thinking like Sonographers"". "
Thinking Like Sonographers: A Deep CNN Model for Diagnosing Gout from Musculoskeletal Ultrasound,4,Conclusion,"In this study, we propose a framework to adjust CNNs to ""think like sonographers"", and diagnose gout from MSKUS images. The mechanism of ""thinking like sonographers"" contains three levels: where to adjust, what to adjust, and how to adjust. The proposed design not only steers CNN models as we intended, but also helps the CNN classifier focus on the crucial gout features. Extensive experiments show that our framework, combined with the mechanism of ""thinking like sonographers"" improves performance over the baseline deep classification architectures. Additionally, we can bypass the need to collect the real gaze maps of the doctors during the classification of newly acquired MSKUS images, thus our method has good clinical application values."
Thinking Like Sonographers: A Deep CNN Model for Diagnosing Gout from Musculoskeletal Ultrasound,,Fig. 1 .,
Thinking Like Sonographers: A Deep CNN Model for Diagnosing Gout from Musculoskeletal Ultrasound,,Fig. 3 .,
Thinking Like Sonographers: A Deep CNN Model for Diagnosing Gout from Musculoskeletal Ultrasound,,Algorithm 1 :,
Thinking Like Sonographers: A Deep CNN Model for Diagnosing Gout from Musculoskeletal Ultrasound,,,
Thinking Like Sonographers: A Deep CNN Model for Diagnosing Gout from Musculoskeletal Ultrasound,,Fig. 4 .,
Thinking Like Sonographers: A Deep CNN Model for Diagnosing Gout from Musculoskeletal Ultrasound,,,
Thinking Like Sonographers: A Deep CNN Model for Diagnosing Gout from Musculoskeletal Ultrasound,,Table 1 .,
Thinking Like Sonographers: A Deep CNN Model for Diagnosing Gout from Musculoskeletal Ultrasound,,Table 2 .,
Thinking Like Sonographers: A Deep CNN Model for Diagnosing Gout from Musculoskeletal Ultrasound,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 16.
A Novel Multi-task Model Imitating Dermatologists for Accurate Differential Diagnosis of Skin Diseases in Clinical Images,1,Introduction,"As the largest organ in the human body, the skin is an important barrier protecting the internal organs and tissues from harmful external substances, such as sun exposure, pollution, and microorganisms [8,10]. In recent years, the increasing number of deaths by skin diseases has aroused widespread public concern [16,17]. Due to the complexity of skin diseases and the shortage of dermatological expertise resources, developing an automatic and accurate skin disease diagnosis framework is of great necessity.Among non-invasive skin imaging techniques, dermoscopy is currently widely used in the diagnosis of many skin diseases [1,7], but it is technically demanding and not necessary for many common skin diseases. Clinical images, on the contrary, can be easily acquired through consumer-grade cameras, increasingly utilized in teledermatology, but their diagnostic value is underestimated. Recently, deep learning-based methods have received great attention in clinical skin disease image recognition and achieved promising results [3,5,11,18,20,23,25,26]. Sun et al. [18] released a clinical image dataset of skin diseases, namely SD-198, containing 6,584 images from 198 different categories. The results demonstrate that deep features from convolutional neural networks (CNNs) outperform handcrafted features in exploiting structural and semantic information. Gupta et al. [5] proposed a dual stream network that employs class activation maps to localize discriminative regions of the skin disease and exploit local features from detected regions to improve classification performance.Although these approaches have achieved impressive results, most of them neglect the domain knowledge of dermatology and lack interpretability in diagnosis basis and results. In a typical inspection, dermatologists give an initial evaluation with the consideration of both global information, e.g. body part, and local information, e.g. the attributes of skin lesions, and further information including the patient's medical history or additional examination is required to draw a diagnostic conclusion from several possible skin diseases. Recognizing skin diseases from clinical images presents various challenges that can be summarized as follows: (1) Clinical images taken by portable electronic devices (e.g. mobile phones) often have cluttered backgrounds, posing difficulty in accurately locating lesions. (2) Skin diseases exhibit high intra-class variability in lesion appearance, but low inter-class variability, thereby making discrimination challenging. (3) The diagnostic reasoning of dermatologists is empirical and complicated, which makes it hard to simulate and model.To tackle the above issues and leverage the domain knowledge of dermatology, we propose a novel multi-task model, namely DermImitFormer. The model is designed to imitate the diagnostic process of dermatologists (as shown in Fig. 1), by employing three distinct modules or strategies. Firstly, the multi-task learning strategy provides extra body parts and lesion attributes predictions, which enhances the differential diagnosis accuracy with the additional correlation from multiple predictions and improves the interpretability of diagnosis with more supporting information. Secondly, a lesion selection module is designed to imitate dermatologists' zoom-in action, effectively highlighting the local lesion features from noisy backgrounds. Thirdly, a cross-interaction module explicitly models the complicated diagnostic reasoning between body parts, lesion attributes, and diseases, increasing the feature alignments and decreasing gradient conflicts from different tasks. Last but not least, we build a new dataset containing 57,246 clinical images. The dataset includes 49 most common skin diseases, covering 80% of the consultation scenarios, 15 body parts, and 27 lesion attributes, following the International League of Dermatological Societies (ILDS) guideline [13]. The main contributions can be summarized as follows: (1) A novel multitask model DermImitFormer is proposed to imitate dermatologists' diagnostic processes, providing outputs of diseases, body parts, and lesion attributes for improved clinical interpretability and accuracy. (2) A lesion selection module is presented to encourage the model to learn more distinctive lesion features. A cross-interaction module is designed to effectively fuse three different feature representations. (3) A large-scale clinical image dataset of skin diseases is established, containing significantly more cases than existing datasets, and closer to the real data distribution of clinical routine. More importantly, our proposed approach achieves the leading recognition performance on three different datasets."
A Novel Multi-task Model Imitating Dermatologists for Accurate Differential Diagnosis of Skin Diseases in Clinical Images,2,Method,"The architecture of the proposed multi-task model DermImitFormer is shown in Fig. 2. It takes the clinical image as input and outputs the classification results of skin diseases, body parts, and attributes in an end-to-end manner. During diagnostic processes, dermatologists consider local and global contextual features of the entire clinical image, including shape, size, distribution, texture, location, etc. To effectively capture these visual features, we use the vision transformer (ViT) [4] as the shared backbone. Three separate task-specific heads are then utilized to predict diseases, body parts, and attributes, respectively, with each head containing two independent ViT layers. In particular, in the task-specific heads of diseases and attributes, the extracted features of each layer are separated into the image features and the patch features. These two groups of features are fed into the lesion selection module (LSM), to select the most informative lesion tokens. Finally, the feature representations of diseases, body parts, and attributes are delivered to the cross-interaction module (CIM) to generate a more comprehensive representation for the final differential diagnosis. Shared Backbone. Following the ViT model, an input image X is divided to N p squared patches {x n , n ∈ {1, 2, ..., N p }}, where N p = (H × W )/P 2 , P is the side length of a squared patch, H and W are the height and width of the image, respectively. Then, the patches are flattened and linearly projected into patch tokens with a learnable position embedding, denoted as t n , n ∈ {1, 2, ..., N p }. Together with an extra class token t 0 , the network inputs are represented as t n ∈ R D , n ∈ {0, 1, ..., N p } with a dimension of D. Finally, the tokens are fed to L consecutive transformer layers to obtain the preliminary image features.Lesion Selection Module. As introduced above, skin diseases have high variability in lesion appearance and distribution. Thus, it requires the model to concentrate on lesion patches so as to describe the attributes and associated diseases precisely. The multi-head self-attention (MHSA) block in ViT generates global attention, weighing the informativeness of each token. Inspired by [19], we introduce a lesion selection module (LSM), which guides the transformer encoder to select the tokens that are most relevant to lesions at different levels. Specifically, for each attention head in MHSA blocks, we compute the attention matrix Np+1) , where m ∈ {1, 2, ..., N h }, N h denoting the number of heads, Q and K the Query and Key representations of the block inputs, respectively. The first row calculates the similarities between the class token and each patch token. As the class token is utilized for classification, the higher the value, the more informative each token is. We apply softmax to the first row and the first column of A m , denoted as a m 0,n and a m n,0 , n ∈ {1, 2, ..., N p }, representing the attention scores between the class token and other tokens:The mutual attention score s n is calculated across all attention heads. Thereafter, we select the top K tokens according to s n for two task heads as l k d and l k a , k ∈ {1, 2, ..., K}. "
A Novel Multi-task Model Imitating Dermatologists for Accurate Differential Diagnosis of Skin Diseases in Clinical Images,,Cross-Interaction Module.,"A diagnostic process of skin diseases takes multiple visual information into account, which is relatively complicated and difficult to model in an analytical way. Simple fusion operations such as concatenation are insufficient to simulate the diagnostic logic. Thus, partially inspired by [21], the CIM is designed to learn complicated correlations between disease, body part, and attribute. The detailed module schematic is shown in Fig. 3. Firstly, the features of body-part and disease are integrated to enhance global representations by a cross-attention block. For example, the fusion between the class token of disease and patch tokens of body-part is:where g B , g D are the class token, p i b , p i d , i ∈ {1, 2, ..., N p } the corresponding patch tokens. GAP and LN denote the global average pooling and layer normalization, respectively. W Q BD , W K BD , W V BD ∈ R F ×F denote learnable parameters. F denotes the dimension of features. g D B is computed from the patch tokens of disease and the class token of body-part in the same fashion. Similarly, we can obtain the fused class tokens (g D A and g A D ) and the fused local class tokens (l D A and l A D ) between attribute and disease. Note that the disease class token g D is replaced by g B D in the later computations, and local class tokens l A and l D in Fig. 3 are generated by GAP on selected local patch tokens from LSM. Finally, these mutually enhanced features from CIM are concatenated together to generate more accurate predictions of diseases, body parts, and attributes.Learning and Optimization. We argue that joint training can enhance the feature representation for each task. Thus, we define a multi-task loss as follows: where N s denotes the number of samples, n x , n h the number of classes for each task, and p ij , y ij the prediction and label, respectively. Notably, body parts and attributes are defined as multi-label classification tasks, optimized with the binary cross-entropy loss, as shown in Eq. 6. The correspondence of x and h is shown in Fig. 2."
A Novel Multi-task Model Imitating Dermatologists for Accurate Differential Diagnosis of Skin Diseases in Clinical Images,3,Experiment,"Datasets. The proposed DermImitFormer is evaluated on three different clinical skin image datasets including an in-house dataset and two public benchmarks.(1) Derm-49: We establish a large-scale clinical image dataset of skin diseases, collected from three cooperative hospitals and a teledermatology platform. The 57,246 images in the dataset were annotated with the diagnostic ground truth of skin disease, body parts, and lesions attributes from the patient records. We clean up the ground truth into 49 skin diseases, 15 body parts, and 27 lesion attributes following the ILDS guidelines [13].(2) SD-198 [18]: It is one of the largest publicly available datasets in this field containing 198 skin diseases and 6,584 clinical images collected through digital cameras or mobile phones. ( 3) PAD-UFES-20 [15]: The dataset contains 2,298 samples of 6 skin diseases.Each sample contains a clinical image and a set of metadata with labels such as diseases and body parts.Implementation Details. The DermImitFormer is initialized with the pretrained ViT-B/16 backbone and optimized with SGD method (initial learning rate 0.003, momentum 0.95, and weight decay 10 -5 ) for 100 epochs on 4 NVIDIA Tesla V100 GPUs with a batch size of 96. We define the input size i.e. H = W = 384 that produces a total of 576 spatial tokens i.e. N p = 576 for a ViT-B backbone. K = 24 in the LSM module. For data augmentation, we employed the Cutmix [24] with a probability of 0.5 and Beta(0.3, 0.3) during optimization. We adopt precision, recall, F1-score, and accuracy as the evaluation metrics.    [14,15], and transformer-based methods [4,12], achieving the state-of-the-art classification performance. In particular, the performance of Der-mImitFormer is better than that of DermImitFormer-ST in Single-Task mode (w/o CIM), which further indicates the effectiveness of the multi-task learning strategy and CIM."
A Novel Multi-task Model Imitating Dermatologists for Accurate Differential Diagnosis of Skin Diseases in Clinical Images,4,Conclusion,"In this work, DermImitFormer, a multi-task model, has been proposed to better utilize dermatologists' domain knowledge by mimicking their subjective diagnostic procedures. Extensive experiments demonstrate that our approach achieves state-of-the-art recognition performance in two public benchmarks and a largescale in-house dataset, which highlights the potential of our approach to be employed in real clinical environments and showcases the value of leveraging domain knowledge in the development of machine learning models."
A Novel Multi-task Model Imitating Dermatologists for Accurate Differential Diagnosis of Skin Diseases in Clinical Images,,Fig. 1 .,
A Novel Multi-task Model Imitating Dermatologists for Accurate Differential Diagnosis of Skin Diseases in Clinical Images,,Fig. 2 .,
A Novel Multi-task Model Imitating Dermatologists for Accurate Differential Diagnosis of Skin Diseases in Clinical Images,,Fig. 3 .,
A Novel Multi-task Model Imitating Dermatologists for Accurate Differential Diagnosis of Skin Diseases in Clinical Images,,Fig. 4 .,
A Novel Multi-task Model Imitating Dermatologists for Accurate Differential Diagnosis of Skin Diseases in Clinical Images,,Table 1 .,
A Novel Multi-task Model Imitating Dermatologists for Accurate Differential Diagnosis of Skin Diseases in Clinical Images,,,
A Novel Multi-task Model Imitating Dermatologists for Accurate Differential Diagnosis of Skin Diseases in Clinical Images,,Table 2 .,Results.
