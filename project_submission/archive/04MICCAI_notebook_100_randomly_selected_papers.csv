,Unnamed: 0,title,header_no,header_title,text,volume
0,5,Anatomy-Driven Pathology Detection on Chest X-rays,3.4,Dataset,"training dataset. we train on the chest imagenome dataset [4,21,22]1 ,
consisting of roughly 240 000 frontal chest x-ray images with corresponding
scene graphs automatically constructed from free-text radiology reports. it is
derived from the mimic-cxr dataset [9,10], which is based on imaging studies
from 65 079 patients performed at beth israel deaconess medical center in
boston, us. amongst other information, each scene graph contains bounding boxes
for 29 unique anatomical regions with annotated attributes, where we consider
positive anatomical finding and disease attributes as positive labels for
pathologies, leading to binary anatomy-level annotations for 55 unique
pathologies. we consider the image-level label for a pathology to be positive if
any region is positively labeled with that pathology.we use the provided
jpg-images [11] 2 and follow the official mimic-cxr training split but only keep
samples containing a scene graph with at least five valid region bounding boxes,
resulting in a total of 234 307 training samples.during training, we use random
resized cropping with size 224 × 224, apply contrast and brightness jittering,
random affine augmentations, and gaussian blurring.evaluation dataset and class
mapping. we evaluate our method on the subset of 882 chest x-ray images with
pathology bounding boxes, annotated by radiologists, from the nih chestxray-8
(cxr8) dataset [20] 3 from the national institutes of health clinical center in
the us. we use 50% for validation and keep the other 50% as a held-out test set.
note that for evaluation only pathology bounding boxes are required (to compute
the metrics), while during training only anatomical region bounding boxes
(without considering pathologies) are required. all images are center-cropped
and resized to 224 × 224.the dataset contains bounding boxes for 8 unique
pathologies. while partly overlapping with the training classes, a one-to-one
correspondence is not possible for all classes. for some evaluation classes, we
therefore use a many-to-one mapping where the class probability is computed as
the mean over several training classes. we refer to the supp. material for a
detailed study on class mappings.",1
1,18,Self-supervised Learning for Physiologically-Based Pharmacokinetic Modeling in Dynamic PET,2.4,Dataset,"the dataset is composed of 23 oncological patients with different tumor types.
dpet data was acquired on a biograph vision quadra for 65 min, over 62 frames.
the exposure duration of the frames were 2 × 10 s, 30 × 2 s, 4 × 10 s, 8 × 30 s,
4 × 60 s, 5 × 120 s and 9 × 300 s. the pet volumes were reconstructed with an
isotropic voxel size of 1.6 mm. the dataset included the label maps of 7 organs
(bones, lungs, heart, liver, kidneys, spleen, aorta) and one image-derived input
function a(t) [bq/ml] from the descending aorta per patient. further details on
the dataset are presented elsewhere [16].the pet frames and the label map were
resampled to an isotropic voxel size of 2.5 mm. then, the dataset was split
patient-wise into training, validation, and test set, with 10, 4, and 9 patients
respectively. details on the dataset split are available in the supplementary
material (table 1). the training set consisted of 750 slices and the validation
consisted of 300. in both cases, 75 axial slices per patient were extracted in a
pre-defined patient-specific range from the lungs to the bladder (included) and
were cropped to size 112 × 112 pixels.",1
2,20,Self-supervised Learning for Physiologically-Based Pharmacokinetic Modeling in Dynamic PET,0.0,(Color figure online),"the most important design choice is the selection of the final activation
function. indeed, the multi-clamp final activation function was proven to be the
best both in terms of cs (exp 4.1: cs = 0.78 ± 0.05) and mae (exp 4.2: mae =
3.27 ± 2.01). compared to the other final activation functions, when the
multi-clamp is used the impact of the max-pooling design is negligible also in
terms of mae. for the rest of the experiments, the selected configuration is the
one from exp. 4.1 (see table 1).figure 2 shows the kps for four selected organs
as computed with the proposed dnn (kp dnn ), as computed with curve fit using
only the 9 patients of the test set (kp cf ) and using all 23 patients (kp ref
cf ) [16]. the voxel-wise kps predicted by the dnn were averaged over the
available organ masks.in terms of run-time, the dnn needed ≈ 1 min to predict
the kps of the a whole-body scan (≈ 400 slices), whereas curve fit took 8.7 min
for a single slice: the time reduction of the dnn is expected to be ≈ 3.500
times.",1
3,21,Self-supervised Learning for Physiologically-Based Pharmacokinetic Modeling in Dynamic PET,4.0,Discussion,"even though the choice of the final activation function has a greater impact,
the selection of the kernel design is important. using spatial and temporal
convo- lution results in an increase in the performance (+0.01 in cs) and
reduces the number of trainable parameters (from 2.1 m to 8.6 k), as pointed out
by [10]. therefore, the convergence is reached faster. moreover, the use of two
separate kernels in time and space is especially meaningful. pixel counts for a
given exposure are affected by the neighboring count measurements due to the
limited resolution of the pet scanner [20]. the temporally previous or following
counts are independent. in general, there is good agreement between kp dnn , kp
cf and kp ref cf . the dnn prediction of k 1 and k 2 in the spleen and k 3 in
the lungs is outside the confidence interval of the results published by sari et
al. [16].an analysis per slice of the metrics shows that the cs between tac i
and tac i changes substantially depending on the region: cs max = 0.87 within
the liver boundaries and cs min = 0.71 in the region corresponding to the heart
and lungs (see fig. 3a). this can be explained by the fact that v b is
underestimated for the heart and aorta. the proposed network predicts v heart b
= 0.376 ± 0.133 and v aorta b = 0.622 ± 0.238 while values of nearly 1 are to be
expected. this is likely due to breathing and heartbeat motion artifacts, which
cannot be modeled properly with a 2tc km that assumes no motion between
frames.figure 3b-e shows the central coronal slice of the four kpis in an
exemplary patient. as expected, k 1 is high in the heart, liver, and kidney.
similarly, the blood fraction volume v b is higher in the heart, blood vessels,
and lungs.the kp dnn are more homogeneous than kp cf , as can be seen in the
exemplary k 1 axial slice shown in fig. 4. a quantitative evaluation of the
smoothness of the images is reported in the supplementary material (fig. 1).
moreover, the distribution in the liver is more realistic in kp dnn , where the
gallbladder can be seen as an ellipsoid between the right and left liver lobes.
high k 1 regions are mainly within the liver, spleen, and kidney for kp dnn ,
while they also appear in unexpected areas in the kp cf (e.g., next to the spine
or in the region of the stomach).the major limitation of this work is the lack
of ground truth and a canonical method to evaluate quantitatively its
performance. this limitation is inherent to pbpk modeling and results in the
need for qualitative analyses based on expected physiological processes. a
possible way to leverage this would be to work on simulated data, yet the
validity of such evaluations strongly depends on how realistic the underlying
simulation models are. as seen in fig. 3a, motion (gross, respiratory, or
cardiac) has a major impact on the estimation quality. registering different
dpet frames has been shown to improve conventional pbpk models [8] and would
possibly have a positive impact on our approach.",1
4,32,AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on MRI Brain Tumor,5.0,Conclusion,"in this work, we propose a brain tumor segmentation method for mri images using
only class labels, based on an attentive multiple-exit class activation mapping
(ame-cam). our approach extracts activation maps from different exits of the
network to capture information from multiple resolutions. we then use an
attention model to hierarchically aggregate these activation maps, learning
pixel-wise weighted sums.experimental results on the four modalities of the 2021
brats dataset demonstrate the superiority of our approach compared with other
cam-based weakly-supervised segmentation methods. specifically, ame-cam achieves
the highest dice score for all patients in all datasets and modalities. these
results indicate the effectiveness of our proposed approach in accurately
segmenting brain tumors from mri images using only class labels.",1
9,59,Tracking Adaptation to Improve SuperPoint for 3D Reconstruction in Endoscopy,1.0,Introduction,"endoscopy is an important medical procedure with many applications, from routine
screening to detection of early signs of cancer and minimally invasive
treatment. automatic analysis and understanding of these videos raises many
opportunities for novel assistive and automatization tasks on endoscopy
procedures. obtaining 3d models from the intracorporeal scenes captured in
endoscopies is an essential step to enable these novel tasks and build
applications, for example, for improved monitoring of existing patients or
augmented reality during training or real explorations.3d reconstruction
strategies have been studied for long, and one crucial step in these strategies
is feature detection and matching which serves as input for structure from
motion (sfm) pipelines. endoscopic images are a challenging case for feature
detection and matching, due to several well known challenges for these tasks,
such as lack of texture, or the presence of frequent artifacts, like specular
reflections. these problems are accentuated when all the elements in the scene
are deformable, as it is the case in most endoscopy scenarios, and in particular
in the real use case studied in our work, the lower gastrointestinal tract
explored with colonoscopies. existing 3d reconstruction pipelines are able to
build small 3d models out of short clips from real and complete recordings [1].
one of the current bottle-necks to obtain better 3d models is the lack of more
abundant and higher quality correspondences in real data.this work introduces
superpoint-e, a new model to extract interest points from endoscopic images. we
build on the well known superpoint architecture [5], a seminal work that
delivers state-of-the-art results when coupled with downstream tasks1 . our main
contribution is a novel supervision strategy to train the model. we propose to
automatically generate reliable training data from video sequences by tracking
feature points from existing detection methods, which do not require training.
we select good features with the colmap sfm pipeline [21], generating training
examples with feature points that can be tracked across several images according
to colmap result. when used to train superpoint, our approach yields a
self-supervised method outperforming current ones.",1
11,75,Self-Supervised Domain Adaptive Segmentation of Breast Cancer via Test-Time Fine-Tuning,1.0,Introduction,"in recent years, deep learning (dl) methods have demonstrated remarkable
performance in detecting and localizing tumors on ultrasound images [2,27].
compared with conventional image processing methods, dl methods provide an
accurate feature extraction capability on ultrasound images, despite their low
resolution and noise disturbance, leading to superior segmentation accuracy
[2,5,14]. however, there are some limitations in developing a dl model in a
source domain and deploying it in an unseen target domain. the primary
limitation is that dl models require a large number of training samples to
achieve accurate predictions [8,24]. yet, acquiring large training datasets and
their corresponding labels, especially from a cohort of patients, can be costly
or even infeasible, which poses a significant challenge in developing a dl model
with high performance [7]. second, even when large-scale datasets are available
through collaborative research from multiple sites, dl models trained on such
datasets may yield sub-optimal solutions due to domain gaps caused by
differences in images acquired from different sites [20]. third, due to the
small number of datasets from each domain, the images for each individual domain
may not capture representative features, limiting the ability of dl models to
generalize across domains [3].domain adaptation (da) has been extensively
studied to alleviate the aforementioned limitations, the goal of which is to
reduce the domain gap caused by the diversity of datasets from different domains
[12,20,26,29,33]. example solutions include transfer learning-and style
transfer-based methods. nonetheless, unlike natural images, generating labels
can be a challenging task, making it difficult to apply general da methods; thus
bridging domain gaps by da methods remains limited [26,33]. this is due to
sensitive privacy issues in patients' data, particularly in collaborative
research, which restricts access to labels from different domains. as a result,
conventional da methods cannot be easily applied [10]. more recently,
unsupervised domain adaptation (uda) has been introduced to address this issue
[16,33], aiming to generate semi-predictions (pseudo-labels) in target domains
first, followed by producing accurate predictions using the pseudo-labels. one
critical limitation of pseudo-label-based uda is the possibility of error
accumulation due to mispredicted pseudo-labels. this can lead to significant
degradation of the performance of dl models, as errors can compound and become
more pronounced over time [17,25].to alleviate the problem of pseudo-label-based
uda, in this work, we propose an advanced uda framework based on self-supervised
da with a test-time finetuning network. test-time adaptation methods have been
developed [4,11,13,23] to improve the learning of knowledge in target domains.
the distinctive feature of our test-time self-supervised da is that it enables
the dl network (i) to learn knowledge about the features of target domains by
fine-tuning the network itself during the test-time phase, rather than
generating pseudo-labels and then (ii) to provide precise predictions on images
in target domains, by using the fine-tuned network. specifically, we adopt
self-supervised learning and verify the model via thorough mathematical
analysis. our framework was tested on the task of breast cancer segmentation in
ultrasound images, but it could also be applied to other lesion segmentation
tasks.to summarize, our contributions are three-fold:• we design a
self-supervised da framework that includes a parameter search method and provide
a mathematical justification for it. with our framework, we are able to identify
the best-performing parameters that result in improved performance in da tasks.
• our framework is effective at preserving privacy, since it carries out da
using only pre-trained network parameters, without transferring any patient
data. • we applied our framework to the task of segmenting breast cancer from
ultrasound imaging data, demonstrating its superior performance over competing
uda methods.our results indicate that our framework is effective in improving
the accuracy of breast cancer segmentation from ultrasound images, which could
have potential implications for improving the diagnosis and treatment of breast
cancer. sample batches of (t, ?) ∼ t",1
12,98,3D Arterial Segmentation via Single 2D Projections and Depth Supervision in Contrast-Enhanced CT Images,4.0,Experimental Design,"dataset. we use an in-house dataset of contrast-enhanced abdominal computed
tomography images (cts) in the arterial phase to segment the peripancreatic
arteries [6]. the cohort consists of 141 patients with pancreatic ductal
adenocarcinoma, of an equal ratio of male to female patients. given a 3d
arterial ct of the abdominal area, we automatically extract the vertebrae
[15,18] and semi-automatically extract the ribs, which have similar intensities
as arteries in arterial cts and would otherwise occlude the vessels. in order to
remove as much of the cluttering surrounding tissue and increase the visibility
of the vessels in the projections, the input is windowed so that the vessels
appear hyperintense. details of the exact preprocessing steps can be found in
table 2 of the supplementary material. the dataset contains binary 3d
annotations of the peripancreatic arteries carried out by two radiologists, each
having annotated half of the dataset. the 2d annotations we use in our
experiments are projections of these 3d annotations. for more information about
the dataset, see [6].image augmentation and transformation. as the annotations
lie on a 2d plane, 3d spatial augmentation cannot be used due to the information
sparsity in the ground truth. instead, we apply an invertible transformation t
to the input volume and apply the inverse transformation t -1 to the network
output before applying the loss, such that the ground truth need not be altered.
a detailed description of the augmentations and transformations used can be
found in table 1 in the supplementary material.training and evaluation. we use a
3d u-net [17] with four layers as our backbone, together with xavier
initialization [10]. a diagram of the network architecture can be found in fig.
2 in the supplementary material. the loss weight α is tuned at 0.5, as this
empirically yields the best performance. our experiments are averaged over
5-fold cross-validation with 80 train samples, 20 validation samples, and a
fixed test set of 41 samples. the network initialization is different for each
fold but kept consistent across different experiments run on the same fold. this
way, both data variance and initialization variance are accounted for through
cross-validation. to measure the performance of our models, we use the dice
score, precision, recall, and mean surface distance (msd). we also compute the
skeleton recall as the percentage of the ground truth skeleton pixels which are
present in the prediction.",1
14,133,Infusing Physically Inspired Known Operators in Deep Models of Ultrasound Elastography,2.4,Dataset and Quantitative Metrics,"we use publicly available data collected from a breast phantom (model 059, cirs:
tissue simulation & phantom technology, norfolk, va) using an alpinion e-cube
r12 research us machine (bothell, wa, usa). the center frequency was 8 mhz and
the sampling frequency was 40 mhz. the young's modulus of the experimental
phantom was 20 kpa and contains several inclusions with young's modulus of
higher than 40 kpa. this data is available online at http://code.sonography.ai
in [16].in vivo data was collected at johns hopkins hospital from patients with
liver cancer during open-surgical rf thermal ablation by a research antares
siemens system using a vf 10-5 linear array with the sampling frequency of 40
mhz and the center frequency of 6.67 mhz. the institutional review board
approved the study with the consent of the patients. we selected 600 rf frame
pairs of this dataset for the training of the networks.two well-known metrics of
contrast to noise ratio (cnr) and strain ratio (sr) are utilized to evaluate the
compared methods. two regions of interest (roi) are selected to compute these
metrics and they can be defined as [10]:where the subscript t and b denote the
target and background rois. the sr is only sensitive to the mean (s x ), while
cnr depends on both the mean and the standard deviation (σ x ) of rois. for
stiff inclusions as the target, higher cnr correlates with better target
visibility, and lower sr translates to a higher difference between the target
and background strains.",1
17,158,Many Tasks Make Light Work: Learning to Localise Medical Anomalies from Multiple Synthetic Tasks,1.0,Introduction,"in recent years, the workload of radiologists has grown drastically, quadrupling
from 2006 to 2020 in western europe [4]. this huge increase in pressure has led
to long patient-waiting times and fatigued radiologists who make more mistakes
[3]. the most common of these errors is underreading and missing anomalies
(42%); followed by missing additional anomalies when concluding their search
after an initial finding (22%) [10]. interestingly, despite the challenging work
environment, only 9% of errors reviewed in [10] were due to mistakes in the
clinicians' reasoning. therefore, there is a need for automated second-reader
capabilities, which brings any kind of anomalies to the attention of
radiologists. for such a tool to be useful, its ability to detect rare or
unusual cases is particularly important. traditional supervised models would not
be appropriate, as acquiring sufficient training data to identify such a broad
range of pathologies is not feasible. unsupervised or self-supervised methods to
model an expected feature distribution, e.g., of healthy tissue, is therefore a
more natural path, as they are geared towards identifying any deviation from the
normal distribution of samples, rather than a particular type of pathology.there
has been rising interest in using end-to-end self-supervised methods for anomaly
detection. their success is most evident at the miccai medical
outof-distribution analysis (mood) challenge [31], where all winning methods
have followed this paradigm so far (2020-2022). these methods use the variation
within normal samples to generate diverse anomalies through sample mixing
[7,[23][24][25]. however all these methods lack a key component: structured
validation. this creates uncertainty around the choice of hyperparameters for
training. for example, selecting the right training duration is crucial to avoid
overfitting to proxy tasks. yet, in practice, training time is often chosen
arbitrarily, reducing reproducibility and potentially sacrificing generalisation
to real anomalies.",1
18,179,VesselVAE: Recursive Variational Autoencoders for 3D Blood Vessel Synthesis,3.0,Experimental Setup,"materials. we trained our networks using a subset of the open-access intra
dataset1 published by yang et al. in 2020 [32]. this subset consisted of 1694
healthy vessel segments reconstructed from 2d mra images of patients. we
converted 3d meshes into a binary tree representation and used the network
extraction script from the vmtk toolkit2 to extract the centerline coordinates
of each vessel model. the centerline points were determined based on the ratio
between the sphere step and the local maximum radius, which was computed using
the advancement ratio specified by the user. the radius of the blood vessel
conduit at each centerline sample was determined using the computed
crosssections assuming a maximal circular shape (see fig. 2). to improve
computational efficiency during recursive tree traversal, we implemented an
algorithm that balances each tree by identifying a new root. we additionally
trimmed trees to a depth of ten in our experiments. this decision reflects a
balance between the computational demands of depth-first tree traversal in each
training step and the complexity of the training meshes. we excluded from our
study trees that exhibited greater depth, nodes with more than two children, or
with loops. however, non-binary trees can be converted into binary trees and it
is possible to train with deeper trees at the expense of higher computational
costs. ultimately, we were able to obtain 700 binary trees from the original
meshes using this approach.implementation details. for the centerline
extraction, we set the advancement ratio in the vmtk script to 1.05. the script
can sometimes produce multiple cross-sections at centerline bifurcations. in
those cases, we selected the sample with the lowest radius, which ensures proper
alignment with the centerline principal direction. all attributes were
normalized to a range of [0, 1]. for the mesh reconstruction we used 4
iterations of catmull-clark subdivision algorithm. the data pre-processing
pipeline and network code were implemented in python and pytorch
framework.training. in all stages, we set the batch size to 10 and used the adam
optimizer with β 1 = 0.9, β 2 = 0.999, and a learning rate of 1 × 10 -4 . we set
α = .3 and γ = .001 for eq. 1 in our experiments. to enhance computation speed,
we implemented dynamic batching [16], which groups together operations involving
input trees of dissimilar shapes and different nodes within a single input
graph. it takes approximately 12 h to train our models on a workstation equipped
with an nvidia a100 gpu, 80 gb vram, and 256 gb ram. however, the memory
footprint during training is very small (≤1 gb) due to the use of a lightweight
tree representation. this means that the amount of memory required to store and
manipulate our training data structures is minimal. during training, we ensure
that the reconstructed tree aligns with the original structure, rather than
relying solely on the classifier's predictions. we train the classifier using a
crossentropy loss that compares its predictions to the actual values from the
original tree. since the number of nodes in each class is unbalanced, we scale
the weight given to each class in the cross-entropy loss using the inverse of
each class count. during preliminary experiments, we observed that accurately
classifying nodes closer to the tree root is critical. this is because a
miss-classification of top nodes has a cascading effect on all subsequent nodes
in the tree (i.e. skip reconstructing a branch). to account for this, we
introduce a weighting scheme that for each node, assigns a weight to the
cross-entropy loss based on the number of total child nodes. the weight is
normalized by the total number of nodes in the tree.metrics. we defined a set of
metrics to evaluate our trained network's performance. by using these metrics,
we can determine how well the generated 3d models of blood vessels match the
original dataset distribution, as well as the diversity of the generated output.
the chosen metrics have been widely used in the field of blood vessel 3d
modeling, and have shown to provide reliable and accurate quantification of
blood vessels main characteristics [3,13]. we analyzed tortuosity per branch,
the vessel centerline total length, and the average radius of the tree.
tortuosity distance metric [4] is a widely used metric in the field of blood
vessel analysis, mainly because of its clinical importance. it measures the
amount of twistiness in each branch of the vessel. vessel's total length and
average radius were used in previous work to distinguish healthy vasculature
from cancerous malformations. finally, in order to measure the distance across
distributions for each metric, we compute the cosine similarity.",1
19,197,DAS-MIL: Distilling Across Scales for MIL Classification of Histological WSIs,1.0,Introduction,"modern microscopes allow the digitalization of conventional glass slides into
gigapixel whole-slide images (wsis) [18], facilitating their preservation and
fig. 1. overview of our proposed framework, das-mil. the features extracted at
different scales are connected (8-connectivity) by means of different graphs.
the nodes of both graphs are later fused into a third one, respecting the rule
""part of"". the contextualized features are then passed to distinct
attention-based mil modules that extract bag labels. furthermore, a knowledge
distillation mechanism encourages the agreement between the predictions
delivered by different scales.retrieval, but also introducing multiple
challenges. on the one hand, annotating wsis requires strong medical expertise,
is expensive, time-consuming, and labels are usually provided at the slide or
patient level. on the other hand, feeding modern neural networks with the entire
gigapixel image is not a feasible approach, forcing to crop data into small
patches and use them for training. this process is usually performed considering
a single resolution/scale among those provided by the wsi image.recently,
multi-instance learning (mil) emerged to cope with these limitations. mil
approaches consider the image slide as a bag composed of many patches, called
instances; afterwards, to provide a classification score for the entire bag,
they weigh the instances through attention mechanisms and aggregate them into a
single representation. it is noted that these approaches are intrinsically flat
and disregard the pyramidal information provided by the wsi [15], which have
been proven to be more effective than single-resolution [4,13,15,19]. however,
to the best of our knowledge, none of the existing proposals leverage the full
potential of the wsi pyramidal structure. indeed, the flat concatenation of
features [19] extracted at different resolutions does not consider the
substantial difference in the informative content they provide. a proficient
learning approach should instead consider the heterogeneity between global
structures and local cellular regions, thus allowing the information to flow
effectively across the image scales.to profit from the multi-resolution
structure of wsi, we propose a pyramidal graph neural network (gnn) framework
combined with (self) knowledge distillation (kd), called das-mil (distilling
across scales). a visual representation of the proposed approach is depicted in
fig. 1. distinct gnns provide contextualized features, which are fed to distinct
attention-based mil modules that compute bag-level predictions. through
knowledge distillation, we encour-age agreement across the predictions delivered
at different resolutions, while individual scale features are learned in
isolation to preserve the diversity in terms of information content. by
transferring knowledge across scales, we observe that the classifier
self-improves as information flows during training. our proposal has proven its
effectiveness on two well-known histological datasets, camelyon16 and tcga lung
cancer, obtaining state-of-the-art results on wsi classification.",1
20,238,Spectral Adversarial MixUp for Few-Shot Unsupervised Domain Adaptation,1.0,Introduction,"a common challenge for deploying deep learning to clinical problems is the
discrepancy between data distributions across different clinical sites
[6,15,20,28,29]. this discrepancy, which results from vendor or protocol
differences, can cause a significant performance drop when models are deployed
to a new site [2,21,23]. to solve this problem, many unsupervised domain
adaptation (uda) methods [6] have been developed for adapting a model to a new
site with only unlabeled data (target domain) by transferring the knowledge
learned from the original dataset (source domain). however, most uda methods
require sufficient target samples, which are scarce in medical imaging due to
the limited accessibility to patient data. this motivates a new problem of
few-shot unsupervised domain adaptation (fsuda), where only a few unlabeled
target samples are available for training.few approaches [11,22] have been
proposed to tackle the problem of fsuda. luo et. al [11] introduced adversarial
style mining (asm), which uses a pretrained style-transfer module to generate
augmented images via an adversarial process. however, this module requires extra
style images [9] for pre-training. such images are scarce in clinical settings,
and style differences across sites are subtle. this hampers the applicability of
asm to medical image analysis. sm-ppm [22] trains a style-mixing model for
semantic segmentation by augmenting source domain features to a fictitious
domain through random interpolation with target domain features. however, sm-ppm
is specifically designed for segmentation tasks and cannot be easily adapted to
other tasks. also, with limited target domain samples in fsuda, the random
feature interpolation is ineffective in improving the model's
generalizability.in a different direction, numerous uda methods have shown high
performance in various tasks [4,[16][17][18]. however, their direct application
to fsuda can result in severe overfitting due to the limited target domain
samples [22]. previous studies [7,10,24,25] have demonstrated that transferring
the amplitude spectrum of target domain images to a source domain can
effectively convey image style information and diversify training dataset. to
tackle the overfitting issue of existing uda methods, we propose a novel
approach called sensitivityguided spectral adversarial mixup (samix) to augment
training samples. this approach uses an adversarial mixing scheme and a spectral
sensitivity map that reveals model generalizability weaknesses to generate
hard-to-learn images with limited target samples efficiently. samix focuses on
two key aspects. 1) model generalizability weaknesses: spectral sensitivity
analysis methods have been applied in different works [26] to quantify the
model's spectral weaknesses to image amplitude corruptions. zhang et al. [27]
demonstrated that using a spectral sensitivity map to weigh the amplitude
perturbation is an effective data augmentation. however, existing sensitivity
maps only use single-domain labeled data and cannot leverage target domain
information. to this end, we introduce a domain-distance-modulated spectral
sensitivity (dodiss) map to analyze the model's weaknesses in the target domain
and guide our spectral augmentation. 2) sample hardness: existing studies
[11,19] have shown that mining hard-to-learn samples in model training can
enhance the efficiency of data augmentation and improve model generalization
performances. therefore, to maximize the use of the limited target domain data,
we incorporate an adversarial approach into the spectral mixing process to
generate the most challenging data augmentations. this paper has three major
contributions. 1) we propose samix, a novel approach for augmenting target-style
samples by using an adversarial spectral mixing scheme. samix enables
high-performance uda methods to adapt easily to fsuda problems. 2) we introduce
dodiss to characterize a model's generalizability weaknesses in the target
domain. 3) we conduct thorough empirical analyses to demonstrate the
effectiveness and efficiency of samix as a plug-in module for various uda
methods across different tasks.",1
21,252,Gall Bladder Cancer Detection from US Images with only Image Level Labels,1.0,Introduction,"gbc is a deadly disease that is difficult to detect at an early stage [12,15].
early diagnosis can significantly improve the survival rate [14]. non-ionizing
radiation, low cost, and accessibility make us a popular non-invasive diagnostic
modality for patients with suspected gall bladder (gb) afflictions. however,
identifying signs of gbc from routine us imaging is challenging for radiologists
[11]. in recent years, automated gbc detection from us images has drawn
increased interest [3,5] due to its potential for improving diagnosis and
treatment outcomes. many of these works formulate the problem as an object
detection, since training a image classification model for gbc detection seems
challenging due to the reasons outlined in the abstract (also see fig. 1).
recently, gbcnet [3], a cnn-based model, achieved sota performance on
classifying malignant gb from us images. gbcnet uses a two-stage pipeline
consisting of object detection followed by classification, and requires bounding
box annotations for gb as well as malignant regions for training. such bounding
box annotations surrounding the pathological regions are time-consuming and
require an expert radiologist for annotation. this makes it expensive and
non-viable for curating large datasets for training large dnn models. in another
recent work, [5] has exploited additional unlabeled video data for learning good
representations for downstream gbc classification and obtained performance
similar to [3] using a resnet50 [13] classifier. the reliance of both sota
techniques on additional annotations or data, limits their applicability. on the
other hand, the image-level malignancy label is usually available at a low cost,
as it can be obtained readily from the diagnostic report of a patient without
additional effort from clinicians.instead of training a classification pipeline,
we propose to solve an object detection problem, which involves predicting a
bounding box for the malignancy. the motivation is that, running a classifier on
a focused attention/ proposal region in an object detection pipeline would help
tackle the low inter-class and high intra-class variations. however, since we
only have image-level labels available, we formulate the problem as a weakly
supervised object detection (wsod) problem. as transformers are increasingly
outshining cnns due to their ability to aggregate focused cues from a large area
[6,9], we choose to use transformers in our model. however, in our initial
experiments sota wsod methods for transformers failed miserably. these methods
primarily rely on training a classification pipeline and later generating
activation heatmaps using attention and drawing a bounding box circumscribing
the heatmaps [2,10] to show localization. however, for gbc detection, this line
of work is not helpful as we discussed earlier.inspired by the success of the
multiple instance learning (mil) paradigm for weakly supervised training on
medical imaging tasks [20,22], we train a detection transformer, detr, using the
mil paradigm for weakly supervised malignant region detection. in this, one
generates region proposals for images, and then considers the images as bags and
region proposals as instances to solve the instance classification (object
detection) under the mil constraints [8]. at inference, we use the predicted
instance labels to predict the bag labels. our experiments validate the utility
of this approach in circumventing the challenges in us images and detecting gbc
accurately from us images using only image-level labels.",1
22,254,Gall Bladder Cancer Detection from US Images with only Image Level Labels,2.0,Datasets,"gallbladder cancer detection in ultrasound images: we use the public gbc us
dataset [3] consisting of 1255 image samples from 218 patients. the dataset
contains 990 non-malignant (171 patients) and 265 malignant (47 patients) gb
images (see fig. 2 for some sample images). the dataset contains image labels as
well as bounding box annotations showing the malignant regions.note that, we use
only the image labels for training. we report results on 5-fold
cross-validation. we did the cross-validation splits at the patient level, and
all images of any patient appeared either in the train or validation split.
polyp detection in colonoscopy images: we use the publicly available kvasir-seg
[17] dataset consisting of 1000 white light colonoscopy images showing polyps
(see fig. 2). since kvasir-seg does not contain any control images, we add 600
non-polyp images randomly sampled from the polypgen [1] dataset.since the
patient information is not available with the data, we use random stratified
splitting for 5-fold cross-validation.",1
23,262,Structured State Space Models for Multiple Instance Learning in Digital Pathology,1.0,Introduction,"precision medicine efforts are shifting cancer care standards by providing novel
personalised treatment plans with promising outcomes. patient selection for such
treatment regimes is based principally on the assessment of tissue biopsies and
the characterisation of the tumor microenvironment. this is typically performed
by experienced pathologists, who closely inspect chemically stained
histopathological whole slide images (wsis). increasingly, clinical centers are
investing in the digitisation of such tissue slides to enable both automatic
processing as well as research studies to elucidate the underlying biological
processes of cancer. the resulting images are of gigapixel size, rendering their
computational analysis challenging. to deal with this issue, multiple instance
learning (mil) schemes based on weakly supervised training are used for wsi
classification tasks. in such schemes, the wsi is typically divided into a grid
of patches, with general purpose features derived from pretrained imagenet [18]
networks extracted for each patch. these representations are subsequently pooled
together using different aggregation functions and attention-based operators for
a final slide-level prediction.state space models are designed to efficiently
model long sequences, such as the sequences of patches that arise in wsi mil. in
this paper, we present the first use of state space models for wsi mil.
extensive experiments on three publicly available datasets show the potential of
such models for the processing of gigapixel-sized images, under both weakly and
multi-task schemes. moreover, comparisons with other commonly used mil schemes
highlight their robust performance, while we demonstrate empirically the
superiority of state space models in processing the longest of wsi sequences
with respect to commonly used mil methods.",1
24,269,Structured State Space Models for Multiple Instance Learning in Digital Pathology,4.1,Data,"camelyon16 [16] is a dataset that consists of resections of lymph nodes, where
each wsi is annotated with a binary label indicating the presence of tumour
tissue in the slide, and all slides containing tumors have a pixel-level
annotation indicating the metastatic region. in multitask experiments, we use
this annotation to give each patch a label indicating local tumour presence.
there are 270 wsis in the training/validation set, and 130 wsis in the
predefined test set. in our experiments, the average patch sequence length
arising from camelyon16 is 6129 (ranging from 127 to 27444).tcga-luad is a tcga
lung adenocarcinoma dataset that contains 541 wsis along with genetic
information about each patient. we obtained genetic information for this cohort
using xena browser [7]. as a mil task, we chose the task of predicting the
patient mutation status of tp53, a tumor suppressor gene that is highly relevant
in oncology studies. the average sequence length is 10557 (ranging from 85 to
34560).tcga-rcc is a tcga dataset for three kidney cancer subtypes (denoted
kich, kirc, and kirp). it consists of 936 wsis (121 kich, 518 kirc, and 297
kirp). the average sequence length is 12234 (ranging from 319 to 62235).",1
25,274,Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision,1.0,Introduction,"longitudinal lesion or tumor tracking is a fundamental task in treatment
monitoring workflows, and for planning of re-treatments in radiation therapy.
based on longitudinal imaging for a given patient it requires establishing which
lesions are corresponding (i.e., same lesion, observed at different timepoints),
which lesions have disappeared and which are new compared to prior scanning.
this information can be leveraged to assess treatment response, e.g., by
analyzing the evolution of size and morphology for a given tumor [1], but also
for adaptation of (re-)treatment radiotherapy plans that take into account new
tumors.in practice, the development of automatic and reliable lesion tracking
solutions is hindered by the complexity of the data (over different modalities),
the absence of large, annotated datasets, and the difficulties associated with
lesion identification (i.e., varying sizes, poses, shapes, and sparsely
distributed locations). in this work, we present a multi-scale self-supervised
learning solution for lesion tracking in longitudinal studies using the
capabilities of contrastive learning [9]. inspired by the pixel-wise contrastive
learning strategy introduced in [5], we choose to learn pixel-wise feature
representations that embed consistent anatomical information from unlabeled
(i.e., without lesion-related annotations) and unpaired (i.e., without the use
of longitudinal scans) data, overcoming barriers to data collection. to increase
the system robustness and emulate the clinician's reading strategies, we propose
to use multi-scale embeddings to enable the system to progressively refine the
fine-grained location. in addition, as imaging offers contextual information
about the human body that is naturally consistent, we design the model to
benefit from biologically-meaningful points (i.e., anatomical landmarks). the
reasoning behind this strategy is that simple data augmentation methods cannot
faithfully model inter-subject variability or possible organ deformations.
hence, we ensure the spatial coherence of the tracked lesion location using
well-defined anatomical landmarks.our proposed method brings two elements of
novelty from a technical point of view: (1) the multi-scale approach for the
anatomical embedding learning and (2) a positive sampling approach that
incorporates anatomically significant landmarks across different subjects. with
these two strategies, the goal is to ensure a high degree of robustness in the
computation of the lesion matching across different lesion sizes and varying
anatomies. furthermore, a significant focus and contribution of our research is
the experimental study at a very large scale: we (1) train a pixel-wise
self-supervised system using a very large and diverse dataset of 52,487 ct
volumes and (2) evaluate on two publicly available datasets. notably, one of the
datasets, nlst, presents challenging cases with 68% of lesions being very small
(i.e., radius < 5 mm).",1
26,279,Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision,4.1,Datasets and Setup,"datasets: we train the universal and fine-grained anatomical point matching
model using an in-house ct dataset (variousct). the training dataset contains
52,487 unlabeled 3d ct volumes capturing various anatomies, including chest,
head, abdomen, pelvis, and more.the evaluation is based on two datasets, the
publicly released deep longitudinal study (dls) dataset [8] and the national
lung screening trial (nlst) dataset [12]. the dls dataset is a subset of the
deeplesion [11] medical imaging dataset, containing 3891 pairs of lesions with
information on their location and size. the dataset covers various types of
lesions across different organs. we follow the official data split for dls
dataset and perform evaluation on the testing dataset which comprises 480 lesion
pairs. for nlst, we randomly selected a subset of 1045 test images coming from
420 patients with up to 3 studies. a certified radiologist annotated the testing
data by identifying the location and size of the pulmonary nodules, resulting in
a total of 825 paired annotations. we evaluate lesion tracking in both
directions, from baseline to follow-up and from follow-up to baseline [8]. this
results in a total of 960 and 1650 testing lesion pairs in dls and nlst test
sets, respectively. the isotropic resolution of all ct volumes is adjusted to
2mm through bilinear interpolation.system training: our learning model is
implemented in pytorch and uses the torchio library [13] for medical data
manipulation and augmentation.we employ a u-net-based encoder-decoder
architecture [2] that utilizes an inflated 3d resnet-18 [3,4] as its encoder,
which extends all 2d convolutions table 1. comparison between the proposed
solution and several state-of-the-art approaches (reference results are from
[8]). the exact same test set was used to compute the performance of each
approach listed in the table; however, we retrained only sam. in the standard
resnet to 3d convolutions and allows the use of pre-trained imagenet weights.
the multi-scale embedding model employs s = 5 scales, and the embedding length
is fixed at l = 128 for each scale. convolution with a stride of (2, 2, 2) is
used to reduce the feature map size at the first and fifth levels, while a
stride of (1, 2, 2) is employed for intermediary levels 2 to 4. the u-net
decoder uses a convolution layer with a 3 × 3 × 3 kernel after every up-sampling
layer to generate the final cascade of feature embeddings. the model is trained
with adamw optimizer [6] for 64 epochs using an early stopping strategy with a
patience of 5 epochs, a batch size of 8 augmented 3d paired patches of 32 × 96 ×
96, and a learning rate of 0.0001.",1
27,284,Geometry-Invariant Abnormality Detection,1.0,Introduction,"the use of machine learning for anomaly detection in medical imaging analysis
has gained a great deal of traction over previous years. most recent approaches
have focused on improvements in performance rather than flexibility, thus
limiting approaches to specific input types -little research has been carried
out to generate models unhindered by variations in data geometries. often,
research assumes certain similarities in data acquisition parameters, from image
dimensions to voxel dimensions and fields-of-view (fov). these restrictions are
then carried forward during inference [5,25]. this strong assumption can often
be complex to maintain in the real-world and although image pre-processing steps
can mitigate some of this complexity, test error often largely increases as new
data variations arise. this can include variances in scanner quality and
resolution, in addition to the fov selected during patient scans. usually
training data, especially when acquired from differing sources, undergoes
significant preprocessing such that data showcases the same fov and has the same
input dimensions, e.g. by registering data to a population atlas. whilst making
the model design simpler, these pre-processing approaches can result in poor
generalisation in addition to adding significant pre-processing times
[11,13,26]. given this, the task of generating an anomaly detection model that
works on inputs with a varying resolution, dimension and fov is a topic of
importance and the main focus of this research.unsupervised methods have become
an increasingly prominent field for automatic anomaly detection by eliminating
the necessity of acquiring accurately labelled data [4,7] therefore relaxing the
stringent data requirements of medical imaging. this approach consists of
training generative models on healthy data, and defining anomalies as deviations
from the defined model of normality during inference. until recently, the
variational autoencoder (vae) and its variants held the state-of-the-art for the
unsupervised approach. however, novel unsupervised anomaly detectors based on
autoregressive transformers coupled with vector-quantized variational
autoencoders (vq-vae) have overcome issues associated with autoencoder-only
methods [21,22]. in [22], the authors explore the advantage of tractably
maximizing the likelihood of the normal data to model the long-range
dependencies of the training data. the work in [21] takes this method a step
further through multiple samplings from the transformer to generate a
non-parametric kernel density estimation (kde) anomaly map.even though these
methods are state-of-the-art, they have stringent data requirements, such as
having a consistent geometry of the input data, e.g., in a whole-body imaging
scenario, it is not possible to crop a region of interest and feed it to the
algorithm, as this cropped region will be wrongly detected as an anomaly. this
would happen even in the case that a scan's original fov was restricted [17].as
such, we propose a geometric-invariant approach to anomaly detection, and apply
it to cancer detection in whole-body pet via an unsupervised anomaly detection
method with minimal spatial labelling. through adapting the vq-vae transformer
approach in [21], we showcase that we can train our model on data with varying
fields of view, orientations and resolutions by adding spatial conditioning in
both the vq-vae and transformer. furthermore, we show that the performance of
our model with spatial conditioning is at least equivalent to, and sometimes
better, than a model trained on whole-body data in all testing scenarios, with
the added flexibility of a ""one model fits all data"" approach. we greatly reduce
the pre-processing requirements for generating a model (as visualised in fig.
1), demonstrating the potential use cases of our model in more flexible
environments with no compromises on performance.",1
28,293,Geometry-Invariant Abnormality Detection,5.0,Conclusion,"detection and segmentation of anomalous regions, particularly for cancer
patients, is essential for staging, treatment and intervention planning.
generally, the variation scanners and acquisition protocols can cause failures
in models trained on data from single sources. in this study, we proposed a
system for anomaly detection that is robust to variances in geometry. not only
does the proposed model showcase strong and statistically-significant
performance improvements on varying image resolutions and fov, but also on
whole-body data. through this, we demonstrate that one can improve the
adaptability and flexibility to varying data geometries while also improving
performance. such flexibility also increases the pool of potential training
data, as they dont require the same fov. we hope this work serves as a
foundation for further exploration into geometry-invariant deep-learning methods
for medical-imaging.",1
29,298,Interpretable Medical Image Classification Using Prototype Learning and Privileged Information,3.0,Experiments,"data. the proposed approach is evaluated using the publicly available lidc-idri
dataset consisting of 1018 clinical thoracic ct scans from patients with
non-small cell lung cancer (nsclc) [2,3]. each lung nodule with a minimum size
of 3 mm was segmented and annotated with a malignancy score ranging from
1-highly unlikely to 5-highly suspicious by one to four expert raters. nodules
were also scored according to their characteristics with respect to predefined
attributes, namely subtlety (difficulty of detection, 1-extremely subtle,
5-obvious), internal structure (1-soft tissue, 4-air ), pattern of calcification
(1popcorn, 6-absent), sphericity (1-linear, 5-round ), margin (1-poorly defined,
5sharp), lobulation (1-no lobulation, 5-marked lobulation), spiculation (1-no
spiculation, 5-marked spiculation), and texture (1-non-solid, 5-solid ). the
pylidc framework [7] is used to access and process the data. the mean attribute
annotation and the mean and standard deviation of the malignancy annotations are
calculated. the latter was used to fit a gaussian distribution, which serves as
the ground truth label for optimization. samples with a mean expert malignancy
score of 3-indeterminate or annotations from fewer than three experts were
excluded in consistency with the literature [8,9,11].experiment designs. to
ensure comparability with previous work [8,9,11], the main metric used is
within-1-accuracy, where a prediction within one score is considered correct.
five-fold stratified cross-validation was performed using 10 % of the training
data for validation and the best run of three is reported.the algorithm was
implemented using the pytorch framework version 1.13 and cuda version 11.6. a
learning rate of 0.5 was chosen for the prototype vectors and 0.02 for the other
learnable parameters. the batch size was set to 128 and the optimizer was adam
[10]. with a maximum of 1000 epochs, but stopping early if there was no
improvement in target accuracy within 100 epochs, the experiments lasted an
average of three hours on a geforce rtx 3090 graphics card. the code is publicly
available at https://github.com/xrad-ulm/proto-caps.besides pure performance,
the effect of reduced availability of attribute annotations was investigated.
this was done by using attribute information only for a randomly selected
fraction of the nodules during the training.to investigate the effect of
prototypes on the network performance, an ablation study was performed. three
networks were compared: proto-caps (proposed) including learning and applying
prototypes during inference, proto-caps w/o use where prototypes are only
learned but ignored for inference, and proto-caps w/o learn using the proposed
architecture without any prototypes.",2
30,343,Longitudinal Multimodal Transformer Integrating Imaging and Latent Clinical Signatures from Routine EHRs for Pulmonary Nodule Classification,3.0,Experimental Setup,"datasets. this study used an imaging-only cohort from the nlst [28] and three
multimodal cohorts from our home institution with irb approval (table 1). for
the nlst cohort (https://cdas.cancer.gov/nlst/), we identified cases who had a
biopsy-confirmed diagnosis of lung malignancy and controls who had a positive
screening result for an spn but no lung malignancy. we randomly sampled from the
control group to obtain a 4:6 case control ratio. next, ehr-pulmonary was the
unlabeled dataset used to learn clinical signatures in an unsupervised manner.
we searched all records in our ehr archives for patients who had billing codes
from a broad set of pulmonary conditions, intending to capture pulmonary
conditions beyond just malignancy. additionally, image-ehr was a labeled dataset
with paired imaging and ehrs. we searched our institution's imaging archive for
patients with three chest cts within five years. in the ehr-image cohort,
malignant cases were labeled as those with a billing code for lung malignancy
and no cancer of any type prior. importantly, this case criteria includes
metastasis from cancer in non-lung locations. benign controls were those who did
not meet this criterion. finally, image-ehr-spn was a subset of image-ehr with
the inclusion criteria that subjects had a billing code for an spn and no cancer
of any type prior to the spn. we labeled malignant cases as those with a lung
malignancy billing code occurring within three years after any scan and only
used data collected before the lung malignancy code. all data within the
five-year period were used for controls. we removed all billing codes relating
to lung malignancy. a description of the billing codes used to define spn and
lung cancer events are provided in supplementary 1.2. training and validation.
all models were pretrained with the nlst cohort after which we froze the
convolutional embedding layer. while this was the only pretraining step for
image-only models (csimage and tdimage), the multimodal models underwent another
stage of pretraining using the image-ehr cohort with subjects from image-ehr-spn
subtracted. in this stage, we randomly selected one scan and the corresponding
clinical signature expressions for each subject and each training epoch. models
were trained until the running mean over 100 global steps of the validation loss
increased by more than 0.2. for evaluation, we performed five-fold
cross-validation with image-ehr-spn, using up to three of the most recent scans
in the longitudinal models. we report the mean auc and 95% confidence interval
from 1000 bootstrapped samples, sampling with replacement from the pooled
predictions across all test folds. a two-sided wilcoxon signed-rank test was
used to test if differences in mean auc between models were
significant.reclassification analysis. we performed a reclassification analysis
of low, medium, and high-risk tiers separated by thresholds of 0.05 and 0.65,
which are the cutoffs used to guide clinical management. given a baseline
comparison, our approach reclassifies a subject correctly if it predicts a
higher risk tier than the baseline in cases, or a lower risk tier than the
baseline in controls (fig. 2).",2
31,355,Centroid-Aware Feature Recalibration for Cancer Grading in Pathology Images,1.0,Introduction,"globally, cancer is a leading cause of death and the burden of cancer incidence
and mortality is rapidly growing [1]. in cancer diagnosis, treatment, and
management, pathologydriven information plays a pivotal role. cancer grade is,
in particular, one of the major factors that determine the treatment options and
life expectancy. however, the current pathology workflow is sub-optimal and
low-throughput since it is, by and large, manually conducted, and the large
volume of workloads can result in dysfunction or errors in cancer grading, which
have an adversarial effect on patient care and safety [2]. therefore, there is a
high demand to automate and expedite the current pathology workflow and to
improve the overall accuracy and robustness of cancer grading.recently, many
computational tools have shown to be effective in analyzing pathology images
[3]. these are mainly built based upon deep convolutional neural networks
(dcnns). for instance, [4] used dccns for prostate cancer detection and grading,
[5] classified gliomas into three different cancer grades, and [6] utilized an
ensemble of dcnns for breast cancer classification. to further improve the
efficiency and effectiveness of dcnns in pathology image analysis, advanced
methods that are tailored to pathology images have been proposed. for example,
[7] proposed to incorporate both local and global contexts through the
aggregation learning of multiple context blocks for colorectal cancer
classification; [8] extracted and utilized multi-scale patterns for cancer
grading in prostate and colorectal tissues; [9] proposed to re-formulate cancer
classification in pathology images as both categorical and ordinal
classification problems. built based upon a shared feature extractor, a
categorical classification branch, and an ordinal classification branch, it
simultaneously conducts both categorical and ordinal learning for colorectal and
prostate cancer grading; a hybrid method that combines dccns with hand-crafted
features was developed for mitosis detection in breast cancer [10]. moreover,
attention mechanisms have been utilized for an improved pathology image
analysis. for instance, [11] proposed a two-step framework for glioma sub-type
classification in the brain, which consists of a contrastive learning framework
for robust feature extractor training and a sparse-attention block for
meaningful multiple instance feature aggregation. such attention mechanisms have
been usually utilized in a multiple instance learning framework or as
self-attention for feature representations. to the best of our knowledge,
attention mechanisms have not been used for feature representations of class
centroids.in this study, we propose a centroid-aware feature recalibration
network (cafenet) for accurate and robust cancer grading in pathology images.
cafenet is built based upon three major components: 1) a feature extractor, 2) a
centroid update (cup) module, and 3) a centroid-aware feature recalibration
(cafe) module. the feature extractor is utilized to obtain the feature
representation of pathology images. cup module obtains and updates the centroids
of class labels, i.e., cancer grades. cafe module adjusts the input embedding
vectors with respect to the class centroids (i.e., training data distribution).
assuming that the classes are well separated in the feature space, the centroid
embedding vectors can serve as reference points to represent the data
distribution of the training data. this indicates that the centroid embedding
vectors can be used to recalibrate the input embedding vectors of pathology
images. during inference, we fix the centroid embedding vectors so that the
recalibrated embedding vectors do not vary much compared to the input embedding
vectors even though the data distribution substantially changes, leading to
improved stability and robustness of the feature representation. in this manner,
the feature representations of the input pathology images are re-calibrated and
stabilized for a reliable cancer classification. the experimental results
demonstrate that cafenet achieves the state-of-the-art cancer grading
performance in colorectal cancer grading datasets. the source code of cafenet is
available at https://github.com/col in19950703/cafenet.",2
32,370,Explaining Massive-Training Artificial Neural Networks in Medical Image Analysis Task Through Visualizing Functions Within the Models,3.1,Dynamic Contrast-Enhanced Liver CT,"our xai technique was applied to explain the mtann model's decision in a liver
tumor segmentation task [20]. dynamic contrast-enhanced liver ct scans
consisting of 42 patients with 194 liver tumors in the portal venous phase from
the lits database [21] were used in this study. each slice of the ct volumes in
the dataset has a matrix size of 512 × 512 pixels, with in-plane pixel sizes of
0.60-1.00 mm and thicknesses of 0.20-0.70 mm. the dataset consists of the
original hepatic ct image with the liver mask and the ""gold-standard"" liver
tumor region manually segmented by a radiologist, as illustrated in fig. 2.
firstly, to have the same physical scale on spatial coordinates, bicubic
interpolation was applied on the original hepatic ct images together with the
corresponding liver mask and ""gold-standard"" tumor segmentation to obtain
isotropic images with a voxel size of 0.60 × 0.60 × 0.60 mm 3 . then, to unify
the image size into the same size, the isotropic image was cropped to obtain the
liver region volume of interest (voi) with an in-plane matrix size of 512 × 512.
an anisotropic diffusion filter was applied to reduce the quantum noise, which
could substantially reduce the noise while major structures such as tumors and
vessels maintained [22]. finally, a z-score normalization was applied to unify
complex histograms of tumors in different cases. the final pre-processed ct
images were used as the input images.in addition, since most liver tumors' shape
is ellipsoidal, the liver tumors can also be enhanced by the hessian-based
method and utilized in the model to improve the performance [23,24]. hence, the
model consisted of these two input channels: segmented liver ct image and its
hessian-enhanced image. also, the patches were extracted from input images from
both channels: a 5 × 5 × 5 sized patch in the same spatial position was
extracted to form a training patch with a size of 2 × 5 × 5 × 5 pixels.seven
cases and 24 cases in the dynamic contrast-enhanced ct scans dataset were used
for training and testing, respectively. 10,000 patches were randomly selected
from the liver mask region in each case, summing up to a total of 70,000
training samples for training. the number of input units in the mtann model with
one hidden layer was 250. the structure optimization process started with 80
hidden units in the hidden layer. the binary cross-entropy (bce) loss function
was used to train the model. the mtann model classified the input patches into
tumor or non-tumor classes, and the output pixels represented the probability of
being a tumor class. during the structure optimization process, the f1 score on
the training patches and the dice coefficient on the training images were also
calculated as the reference to select a suitable compact model that performed
equivalently to the original large model.as observed in the four evaluation
metric curves in fig. 3, as the number of hidden units was reduced from 80 to 9,
the performance of the model fluctuated up and down, and after it was reduced
below 9, the performance of the model dropped dramatically. therefore, we chose
a number of hidden units of 9 as the optimized structure.then, we applied the
unsupervised hierarchical clustering algorithm to the weighted function maps
from the optimized compact model with 9 hidden units. figure 4 shows that the 9
hidden units are clearly divided into 3 different groups. we denote hidden units
3, 4, and 7 as group a, hidden units 2, 6, 1, and 8 as group b, and hidden units
0 and 5 as group c. the hidden units in the same group should have a similar
function, and the function maps from each group should show the function of the
group. as illustrated in fig. 5, the low-intensity areas in the function maps of
hidden units 0 and 5 in group c match the high-intensity areas in the
hessian-enhanced input image, which means they suppress the high-intensity
areas. likewise, group a enhances the liver area, and group b suppresses the
non-tumor area. we also understood that groups a and b worked together to
enhance the tumor area, and group c suppressed the liver's boundary as well as
reduced the false enhancements inside the liver. thus, our xai method was able
to reveal the learned functions of groups of neurons in the neural network,
which we call ""functional explanations"" and define as the explanations of the
model behavior by a combination of functions. our method is a post-hoc method
that offers both instance-based and model-based functional explanations.",2
36,407,Explainable Image Classification with Improved Trustworthiness for Tissue Characterisation,3.0,Experiments and Analysis,"dataset. the developed explainability framework has been validated on an in vivo
and ex vivo pcle dataset of meningioma, glioblastoma and metastases of an
invasive ductal carcinoma (idc). all studies on human subjects were performed
according to the requirements of the local ethic committee and in agreement with
the declaration of helsinki (no. cle-001 nr: 2014480). the cellvizio c by mauna
kea technologies, paris, france has been used in combination with the mini laser
probe cystoflex c uhd-r. the distinguishing characteristic of the meningioma is
the psammoma body with concentric circles that show various degrees of
calcification. regarding glioblastomas, the pcle images allow for the
visualization of the characteristic hypercellularity, evidence of irregular
nuclei with mitotic activities or multinuclear appearance with irregular cell
shape. when examining metastases of an idc, the tumor presents as egg-shaped
cells with uniform evenly spaced nuclei. our dataset includes 38 meningioma
videos, 24 glioblastoma and 6 idc. each pcle video represents one tumour type
and corresponds to a different patient. the data has been curated to remove
noisy images and similar frames. this resulted in a training dataset of 2500
frames per class (7500 frames in total) and a testing dataset of the same size.
the dataset is split into a training and testing subset, with the division done
on the patient level.implementation. to implement the dl models we use the
open-source framework pytorch [25] and a nvidia geforce rtx 3090 graphics card
for parallel computation. to show our method generalises we trained two
lightweight models: resnet-18 [17] with a learning rate of 0.01 and mobilenetv2
[29] with a learning rate of 0.001. both were trained using the adam-w [22]
optimiser with a weight decay of 0.01 and dropout probability 0.1. we report the
model's top-1 accuracy for resnet18 as 94.0% and for mobilenet as 86.6%. at test
time, we set t = 100 to create a fair distribution of pa maps. pa methods were
implemented with the help of torchcam [9] and reciprocam was implemented using
the authors' source code.evaluation metrics. evaluating a pa method is not a
trivial task because a pa map may not need to be inline with what a human deems
""reasonable"" [1]. segmentation scores like intersection over union (iou) may be
used with caution to compare thresholded pa maps to ground truth maps with
annotated salient regions. by doing so, we can measure how informed the model is
about a particular class. to quantify how misinformed a model is, we can
estimate at its average drop [6]:where, x = x f s ( ŷ (x). the above equation
measures the effect on the output score of the classification model if we only
include the pixels which the pa method scored highly. a minimum average drop is
desired.as average drop was found to not be sufficient on its own, the unified
method adcc [27] was introduced which is the harmonic mean of average drop,
coherency and complexity, defined as:coherency is the pearson correlation
coefficient which ensures that the remaining pixels after dropping are still
important, defined as:where, cov(., .) is the covariance and σ is the standard
deviation. a higher coherency is better. complexity is the l1 norm of the output
pa map.complexity is used to measure how cluttered a pa map is. for a good pa
map, complexity should be a minimum. as it has been shown in the literature, the
metrics in eqs. ( 3), ( 5) and ( 6), can not be used individually to evaluate a
pa method [27]. adcc combined with computation time gives us a reliable overall
metric of how a pa method is performing. performance evaluation. the proposed
method has been compared to combinations of resnet18 and mobilenetv2 with sota
pa methods. at test time, dropout is not enabled for these standard methods, it
is only enabled for our method. in table 1, we show that our method outperforms
all the compared cnn-pa method combinations on adcc. the dropout version of
scorecam is too computationally expensive and therefore is not included in our
comparison. we believe that the better performance of our method is because of
the random dropping of features taking place during dropout at test time which
helps to suppress noise in the estimated enhanced pa map. the combination of
recipro-cam with our proposed method improves performance (increases adcc) at
the expense of increasing the computational complexity. we believe that this
could be reduced using a batched implementation of recipro-cam. we attribute
slow down in smoothgradcam++ when dropout is applied during test time to the
perturbations it adds on top of the pa method. our validation study shows that
grad-cam, grad-cam++ and recipro-cam are often leading in terms of speed as
expected from the literature. in fig. 1, we can see our proposed method reduces
noise in the pa map around the salient region. the distinguishing characteristic
of the meningioma is the psammoma body which is highlighted by all the pa
methods. risk estimations from eq. ( 2) are also displayed and provide an added
visualisation for a surgeon to trust the model. as it can be seen, areas of low
cv match the areas of high pa values which verifies the trustworthiness of our
method. we believe that the proposed explainability method could be used to
support the surgeon intraoperatively in diagnosis and decision making during
tumour resection. the enhanced pa map extracted with our method highlights the
areas which were the most important to the model's prediction. when these areas
correlate with clinically relevant areas, it shows that the model has learned to
robustly classify the different tissue classes. hence, it can be trusted by the
surgeon for diagnosis.",2
37,447,Localized Region Contrast for Enhancing Self-supervised Learning in Medical Image Segmentation,3.2,Fine-Tuning Datasets,"during the fine-tuning stage, we perform extensive experiments on three datasets
with respect to different regions of the human body. abd-110 is an abdomen
dataset from [25] that contains 110 ct images from patients with various
abdominal tumors and these ct images were taken during the treatment planning
stage. we report the average dsc on 11 abdominal organs (large bowel, duodenum,
spinal cord, liver, spleen, small bowel, pancreas, left kidney, right kidney,
stomach and gallbladder).thorax-85 is a thorax dataset from [5] that contains 85
thoracic ct images. we report the average dsc on 6 thoracic organs (esophagus,
trachea, spinal cord, left lung, right lung, and heart).han is from [24] and
contains 120 ct images covering the head and neck region. we report the average
dsc on 9 organs (brainstem, mandible, optical chiasm, left optical nerve, right
optical nerve, left parotid, right parotid, left submandibular gland, and right
submandibular gland).",2
38,463,Faithful Synthesis of Low-Dose Contrast-Enhanced Brain MRI Scans Using Noise-Preserving Conditional GANs,1.0,Introduction,"magnetic resonance imaging (mri) of the brain is an essential imaging modality
to accurately diagnose various neurological diseases ranging from inflammatory
t. pinetz and a. effland-are funded the german research foundation under
germany's excellence strategy -exc-2047/1 -390685813 and -exc2151 -390873048 and
r. haase is funded by a research grant (bonfor; o-194.0002.1). t. pinetz and e.
kobler-contributed equally to this work. lesions to brain tumors and metastases.
for accurate depictions of said pathologies, gadolinium-based contrast agents
(gbca) are injected intravenously to highlight brain-blood barrier dysfunctions.
however, these contrast agents are expensive and may cause nephrogenic systemic
fibrosis in patients with severely reduced kidney function [31]. moreover, [17]
reported that gadolinium accumulates inside patients with unclear health
consequences, especially after repeated application. the american college of
radiology recommends administering the lowest gbca dose to obtain the needed
clinical information [1].driven by this recommendation, several research groups
have recently published dose-reduction techniques focusing on maintaining image
quality. complementary to the development of higher relaxivity contrast agents
[28], virtual contrast [3,8] -replacing a large fraction of the gbca dose by
deep learninghas been proposed. these approaches typically acquire a
contrast-enhanced (ce) scan with a lower gbca dose along with non-ce scans,
e.g., t1w, t2w, flair, or adc. these input images are then processed by a deep
neural network (dnn) to replicate the corresponding standard-dose scan. while
promising, virtual contrast techniques have not been integrated into clinical
practice yet due to falsepositive signals or missed small lesions [3,23]. as
with all deep learning-based approaches, the availability of large datasets is
essential, which is problematic in the considered case since the additional ce
low-dose scan is not acquired in clinical routine exams. hence, there are no
public datasets to easily benchmark and compare different algorithms or evaluate
their performance. in general, the enhancement behavior of pathological tissues
at various gbca dosages has barely been researched due to a lack of data [12].in
recent years, generative models have been used to overcome data scarcity in the
computer vision and medical imaging community. frequently, generative
adversarial networks (gans) [9] are applied as state-of-the-art in image
generation [30] or semantic translation/interpolation [5,18,21]. in a nutshell,
the gan framework trains two competing dnns -the generator and the
discriminator. the generator learns a non-linear transformation of a predefined
noise distribution to fit the distribution of a target dataset, while the
discriminator provides feedback by simultaneously approximating a distance or
divergence between the generated and the target distribution. the choice of this
distance leads to the well-known different gan algorithms, e.g., wasserstein
gans [4,10], least squares gans [24], or non-saturating gans [9]. however, lucic
et al. [22] showed that this choice has only a minor impact on the
performance.learning conditional distributions between images can be
accomplished by additionally feeding a condition (additional scans, dose level,
etc.) into both the generator and discriminator. in particular, for
image-to-image translation tasks, these conditional gans have been successfully
applied using paired [14,25,27] and unpaired training data [35]. within these
methods, an additional content (cycle) loss typically penalizes pixel-wise
deviations (e.g., 1 ) from a corresponding reference to enforce structural
similarity, whereas a local adversarial loss (discriminator with local receptive
field) controls textural similarity. in addition, embeddings have been used to
inject metadata [7,18]. to study the gbca accumulation behavior, we collected
453 ce scans with non-standard gbca doses in the set of {10%, 20%, 33%} along
with the corresponding standard-dose (0.1 mmol/kg) scan after applying the
remaining contrast agent. using this dataset, we aim at the semantic
interpolation of the gbca signal at various fractional dose levels. to this end,
we use gans to learn the contrast enhancement behavior from the dataset
collective and thereby enable the synthesis of contrast signals at various dose
levels for individual cases. further, to minimize the smoothing effect [19] of
typical content losses (e.g., 1 or perceptual [16]), we develop a
noise-preserving content loss function based on the wasserstein distance between
paired image patches calculated using a sinkhornstyle algorithm. this novel loss
enables a faithful generation of noise, which is important for the
identification of enhancing pathologies and their usability as additional
training data.with this in mind, the contributions of this work are as
follows:-synthesis of gbca behavior at various doses using conditional gans,
-loss enabling interpolation of dose levels present in training data,
-noise-preserving content loss function to generate realistic synthetic images.",2
39,501,SFusion: Self-attention Based N-to-One Multimodal Fusion Block,1.0,Introduction,"people perceive the world with signals from different modalities, which often
carry complementary information about varying aspects of an object or event of
interest. therefore, collecting and utilizing multimodal information is crucial
for artificial intelligence to understand the world around us. data collected
from various sensors (e.g., microphones, cameras, motion controllers) are used
to identify human activity [4]. moreover, multimodal medical images obtained
from different scanning protocols (e.g., computed tomography, magnetic resonance
imaging) are employed for disease diagnosis [12]. satisfactory performances have
been achieved with these multimodal data.in practical application, however,
modality missing is a common scenario. wirelessly connected sensors may
occasionally disconnect and temporarily be unable to send any data [3]. medical
images may be missing due to artifacts and diverse patient conditions [11]. in
these unexpected situations, any combinatorial subset of available modalities
can be given as input. to handle this, one intuitive solution is to train a
dedicated model on all possible subsets of available modalities [6,14,23].
however, these methods are ineffective and timeconsuming. another way is to
predict missing modalities and perform with the completed modalities [20]. but,
these approaches also require additional prediction networks for each missing
situation, and the quality of the recovered data directly affects the
performance, especially when there are only a few available modalities.
recently, fusing the available modalities into a shared representation received
wide attention. however, it is particularly challenging due to the varying
number of input modalities, which results in the n-to-one fusion
problem.currently, existing fusion strategies to tackle this challenge can be
broadly grouped into three categories: the arithmetic strategy, the selection
strategy and the convolution strategy. as shown in fig. 1(a), in the arithmetic
strategy, feature representations of available modalities are merged by an
arithmetic function, such as averaging, computing the first and second moments
or other designed formulas [10,13,17]. for the selection strategy, as shown in
fig. 1(b), each value of fused representation is selected from the values at the
corresponding position of the inputs. the selection rule can be defined as max,
min or probabilitybased [2,8,19]. although the above two fusion strategies are
easily scalable to various data missing situations, their fusion operation is
hard-coded. all available modalities contribute equally and their latent
correlations are neglected. unlike hard-coding the fusion operation, in the
convolution strategy, the convolutional fusion network automatically learns how
to fuse these feature representations, which is beneficial to exploiting the
correlation between multiple modalities. however, as shown in fig. 1(c), this
fusion strategy needs a constant number of data to meet the requirements of the
input channels in the convolutional network. therefore, it has to simulate
missing data by crudely zero-padding or replacing it with similar modalities,
which inevitably introduces a bias in computation and causes performance
degradation [5,18,25].transformer has achieved success in the field of computer
vision, demonstrating that self-attention mechanism has the ability to capture
the latent correlation of image tokens. however, no work has explored the
effectiveness of self-attention mechanism on the n-to-one fusion, where n is
variable during training, rather than fixed. furthermore, the calculation of
self-attention does not require a fixed number of tokens as input, which
represents a potential for handling missing data. therefore, we propose a
self-attention based fusion block (sfusion) to tackle the problems of the above
fusion strategies. as shown in fig. 1(d), sfusion can handle any number of input
data instead of fixing its number. in addition, sfusion is a learning-based
fusion strategy that consists of two components: the correlation extraction (ce)
module and the modal attention (ma) module. in the ce module, feature
representations extracted from available modalities are projected as tokens and
fed into the self-attention layers to learn multimodal correlations. based on
these correlations, a modal softmax function is proposed to generate weight maps
in the ma module. finally, it builds a shared feature representation by fusing
the varying inputs with the weight maps.the contributions of this work are:-we
propose sfusion, which is a data-dependent fusion strategy without impersonating
missing modalities. it can learn the latent correlations between different
modalities and builds a shared representation adaptively. -the sfusion is not
limited to specific deep learning architectures. it takes inputs from any kind
of upstream processing model and serves as the input of the downstream decision
model, which enables applying the sfusion to various backbone networks for
different tasks. -we provide qualitative and quantitative performance
evaluations on activity recognition with the shl [22] dataset and brain tumor
segmentation with the brats2020 [1] dataset. the results show the superiority of
sfusion over competing fusion strategies.",2
40,507,SFusion: Self-attention Based N-to-One Multimodal Fusion Block,0.0,GFF.,"in the experiments on brain tumor segmentation, we compare sfusion with a gated
feature fusion block (gff) [5], which belongs to the convolution strategy (shown
in fig. 1(c)). as shown in fig. 4 (b), a feature disentanglement architecture is
employed. multimodal medical images are decomposed into the modality-invariant
content and the modality-specific appearance code by encoders e c and e a ,
respectively. the content codes (e.g., c 2 and c 3 , shown in fig. 4 (b)) of
missing modalities are simulated with zero values. then, all content codes are
fused into a shared representation c s by gff. given c s , the tumor
segmentation results are generated by the decoder d s . for a fair comparison,
we adopt the same encoders (e c i and e a i ) and decoders (d s and d r i ) as
used in [5]. we obtain the performance of our fusion strategy by replacing gff
with sfusion and removing the zero-padding operation. the training max_epoch is
set to 200. following [5] setting, the batch size is set to 1. adam [16] is
utilized with a learning rate of 1 × 10 -4 and progressively multiplies it by (1
-epoch / max_epoch) 0.9 . losses of l kl , l rec and l seg are employed as [5].
during training, to simulate real missing modalities scenarios, each training
patient's data is fixed to one of 15 possible missing cases. for a comprehensive
evaluation, we test the performance of all 15 cases for each test patient.our
implementations are on an nvidia rtx 3090(24g) with pytorch 1.8.1.",2
41,566,Unpaired Cross-Modal Interaction Learning for COVID-19 Segmentation on Limited CT Images,2.2,Knowledge Condensation and Interaction,"knowledge condensation. it is difficult to directly learn cross-modal
dependencies using the features obtained by the encoder because ct and x-ray
data were collected from different patients. this means that the data may not
have a direct correspondence between two modalities, making it challenging to
capture their relationship. as shown in fig. 1(a), we design a knowledge
condensation (kc) module by introducing a momentum-updated prototype learning
strategy to condensate valuable knowledge in each modality from the learned
features. for the x-ray modality, given its prototypes p cxr = {p cxr 1 , p cxr
2 , ..., p cxr k } initialized randomly and the feature sequence f cxr , kc
module first reduces the spatial resolution of f cxr and groups the reduced f
cxr into k prototypes by calculating the distance between each feature point and
prototypes, shown as followswhere c cxr i suggests the feature points closing to
the i-th prototype. σ(•) represents a linear projection to reduce the feature
sequence length to relieve the computational burden. then we introduce a
momentum learning function to update the prototypes with c cxr i , which means
that the updates at each iteration not only depend on the current c cxr i but
also consider the direction and magnitude of the previous updates, defined
aswhere λ is the momentum factor, which controls the influence of the previous
update on the current update. similarly, the prototypes p ct for ct modality can
be calculated and updated with the feature set f ct . the prototypes effectively
integrate the informative features of each modality and can be considered
modality-specific knowledge to improve the subsequent cross-modal interaction
learning. the momentum term allows prototypes to move more smoothly and
consistently towards the optimal position, even in the presence of noise or
other factors that might cause the prototypes to fluctuate. this can result in a
more stable learning process and more accurate prototypes, thus contributing to
condensate the knowledge of each modality better.knowledge-guided interaction.
the knowledge-guided interaction (ki) module is proposed for unpaired
cross-modality learning, which accepts the learned prototypes from one modality
and features from another modality as inputs. as shown in fig. 1(b), the ki
module contains two multi-head attention (mha) blocks. take ct features f ct and
x-ray prototypes p cxr as input example, the first block considers p cxr as the
query and reduced f ct as the key and value of the attention. it embeds the
x-ray prototypes through the calculated affinity map between f ct and p cxr ,
resulting in the adapted prototype p cxr . the first block can be seen as a
warm-up to make the prototype adapt better to the features from another
modality. the second block treats f ct as the query and the concatenation of
reduced f ct and p cxr as the key and value, improving the f ct through the
adapted prototypes. similarly, for the f cxr and p ct as inputs, the ki module
is also used to boost the x-ray representations. inspired by the knowledge
prototypes, ki modules boost the interaction between the two modalities and
allow for the learning of strong representations for covid-19 segmentation and
x-ray classification tasks.",3
42,568,Unpaired Cross-Modal Interaction Learning for COVID-19 Segmentation on Limited CT Images,3.1,Materials,"we used the public covid-19 segmentation benchmark [15] to verify the proposed
uci. it is collected from two public resources [5,8] on chest ct images
available on the cancer imaging archive (tcia) [4]. all ct images were acquired
without intravenous contrast enhancement from patients with positive reverse
transcription polymerase chain reaction (rt-pcr) for sars-cov-2. in total, we
used 199 ct images including 149 training images and 50 test images. we also
used two chest x-ray-based classification datasets including chestx-ray14 [18]
and chestxr [1] to assist the uci training. the chestx-ray14 dataset comprises
112,120 x-ray images showing positive cases from 30,805 patients, encompassing
14 disease image labels pertaining to thoracic and lung ailments. an image may
contain multiple or no labels. the chestxr dataset consists of 21,390 samples,
with each sample classified as healthy, pneumonia, or covid-19.",3
44,644,Guiding the Guidance: A Comparative Analysis of User Guidance Signals for Interactive Segmentation of Volumetric Images,2.2,Model Backbone and Datasets,"we use the deepedit [11] model with a u-net backbone [15] and simulate a fixed
number of clicks n during training and evaluation. for each volume, n clicks are
iteratively sampled from over-and undersegmented predictions of the model as in
[16] and represented as foreground and background guidance signals. we
implemented our experiments with monai label [23] and will release our code.we
trained and evaluated all of our models on the openly available autopet [1] and
msd spleen [2] datasets. msd spleen [2] contains 41 ct volumes with voxel size
0.79×0.79×5.00mm 3 and average resolution of 512×512×89 voxels with dense
annotations of the spleen. autopet [1] consists of 1014 pet/ct volumes with
annotated tumor lesions of melanoma, lung cancer, or lymphoma. we discard the
513 tumor-free patients, leaving us with 501 volumes. we also only use pet data
for our experiments. the pet volumes have a voxel size of 2.0 × 2.0 × 2.0mm 3
and an average resolution of 400 × 400 × 352 voxels.",3
45,680,Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,1.0,Introduction,"lung cancer is the main cause of cancer death worldwide [18]. pulmonary nodules
and masses are both features present in computed tomography images that aid in
the diagnosis of lung cancer. the primary difference is that a nodule is smaller
than 30 mm in diameter, while a mass is larger than 30 mm [22]. early detection
of these features is crucial to aid physicians in making a diagnosis of z. li
and j. yang-equal contributions. visualization on results of four large-scale
mass segmentation given by nnu-net baseline [7]. compared with the ground-truth
segmentation, the recall rate for these four samples is 46.29%, 58.34%, 79.51%,
and 68.51%, respectively. this is significantly lower than the mean value of
81.68%. (b): statistics of the number of nodules at different scales in three
datasets. the range of nodule diameter corresponding to micro, small, medium,
and mass is (0, 10], (10,20], (20,30], [30, ∞), respectively. (c) : the
distribution of recall rate with respect to the nodule size. existing methods
have low recall rates for the segmentation of large scale nodules and
masses.benign or malignant tumors [27] and determining follow-up treatment.
lesion segmentation can be utilized to evaluate two important factors: the
volume of the lesion and its growth rate [5,6,8,12]. furthermore, obtaining
accurate information regarding the nodule can assist in determining the
appropriate resection method and surgical margin required to preserve as much
lung function as possible. [14,17].segmenting nodules is a tedious task that
requires significant human labor. computer aided diagnosis (cad) systems can
significantly reduce such heavy workloads. the accuracy of the existing nodule
detection model reaches 96.1% [9] accuracy. however, the accuracy of the 3d
nodule segmentation model is prone to significantly decline in the application,
regardless of whether its structure is based on cnn or transformer [2]. as shown
in fig. 1(a-c), the recall rate of the large-scale nodule and mass is usually
lower than the average level. the main reason is that the lesion scale in the
two public datasets are relatively small, which matches the fact few patients
have very large nodule or mass. this makes the pulmonary nodule and mass
segmentation task resemble a long-tail problem rather than a mere large scale
span problem. this leads to unsatisfactory results when segmenting large lesions
that require more accurate delineation [26].several studies have proposed
solutions to tackle the large scale span challenges at both the input and
feature level. for instance, some approaches adopt multi-scale inputs [4], where
the input images are resized to different resolu-tion ratios. some other methods
leverage multi-scale feature maps to capture information from different scales,
such as cross-scale feature fusion [19] or using multi-scale convolutional
filters [3]. furthermore, the attention mechanisms [23] has also been utilized
to emphasize the features that are more relevant for segmentation. though these
methods have achieved impressive performance, they still struggle to accurately
segment the extremely imbalanced multi-scale lesions.recently, some click-based
lesion segmentation methods [19][20][21] introduce the click at the input or
feature level and modify the network accordingly, resulting in higher accuracy
results. yet, the click input does not provide the scale information of lesions
for the network.in this paper, we propose a scale-aware test-time click
adaptation (sattca) method, which simply utilizes easily obtainable lesion click
(i.e., the center detected nodule) to adjust the parameters of the network
normalization layers [24] during testing. note that we do not need to exploit
any data from the training set. specifically, we expand the click into an
ellipsoid mask, which supervises the test-time adaptation. this helps to improve
the segmentation performance of large-scale nodules and masses. additionally, we
also propose a multi-scale input encoder to further address the problem of
imbalanced lesion scales. experimental results on two public datasets and one
in-house dataset demonstrate that the proposed method outperforms existing
methods with different backbones.",3
46,706,Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos,3.3,Location-Based Contrastive Loss,"note that the breast lesion locations of neighboring ultrasound video frames are
close, while the breast lesion location distance is large for different
ultrasound videos, which are often obtained from different patients. motivated
by this, we further devise a location-based contrastive loss to make the breast
lesion locations at the same video to be close, while pushing the lesion
locations of frames from different videos away. by doing so, we can enhance the
breast lesion location prediction in the localization branch. hence, we devise a
location-based contrastive loss based on a triplet loss [15], and the definition
is given by:where α is a margin that is enforced between positive and negative
pairs. h t and h t-1 are predicted heatmaps of neighboring frames from the same
video. n t denotes the heatmap of the breast lesion from a frame from another
ultrasound video. hence, the total loss l total of our network is computed
by:where g h t and g s t denote the ground truth of the breast lesion
segmentation and the breast lesion localization. we empirically set weights",3
47,735,Asymmetric Contour Uncertainty Estimation for Medical Image Segmentation,3.1,Data,"camus. the camus dataset [20] contains cardiac ultrasounds from 500 patients,
for which two-chamber and four-chamber sequences were acquired.manual
annotations for the endocardium and epicardium borders of the left ventricle
(lv) and the left atrium were obtained from a cardiologist for the end-diastolic
(ed) and end-systolic (es) frames. the dataset is split into 400 training
patients, 50 validation patients, and 50 testing patients. contour points were
extracted by finding the basal points of the endocardium and epicardium and then
the apex as the farthest points along the edge. each contour contains 21
points.private cardiac us. this is a proprietary multi-site multi-vendor dataset
containing 2d echocardiograms of apical two and four chambers from 890 patients.
data comes from patients diagnosed with coronary artery disease, covid, or
healthy volunteers. the dataset is split into a training/validation set (80/20)
and an independent test set from different sites, comprised of 994
echocardiograms from 684 patients and 368 echocardiograms from 206 patients,
respectively. the endocardium contour was labeled by experts who labeled a
minimum of 7 points based on anatomical landmarks and add as many other points
as necessary to define the contour. we resampled 21 points equally along the
contour.jsrt. the japanese society of radiological technology (jsrt) dataset
consists of 247 chest x-rays [26]. we used the 120 points for the lungs and
heart annotation made available by [10]. the set of points contains specific
anatomical points for each structure (4 for the right lung, 5 for the left lung,
and 4 for the heart) and equally spaced points between each anatomical point. we
reconstructed the segmentation map with 3 classes (background, lungs, heart)
with these points and used the same train-val-test split of 70%-10%-20% as [10].",3
49,776,Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels,3.1,Datasets,"qubiq is a recent challenge held at miccai 2020 and 2021, specifically designed
to evaluate the inter-rater variability in medical imaging. following [23,41],
we use qubiq 2020, which contains 7 segmentation tasks in 4 different ct and mr
datasets: prostate (55 cases, 2 tasks, 6 raters), brain growth (39 cases, 1
task, 7 raters), brain tumor (32 cases, 3 tasks, 3 raters), and kidney (24
cases, 1 task, 3 raters). for each dataset, we calculate the average dice score
between each rater and the majority votes in table 1. in some datasets, such as
brain tumor t2, the inter-rater disagreement can be quite substantial. in line
with [23], we resize all images to 256 × 256. lits contains 201 high-quality ct
scans of liver tumors. out of these, 131 cases are designated for training and
70 for testing. as the ground-truth labels for the test set are not publicly
accessible, we only use the training set. following [36], all images are resized
to 512×512 and the hu values of ct images are windowed to the range of [-60,
140]. kits includes 210 annotated ct scans of kidney tumors from different
patients. in accordance with [36], all images are resized to 512 × 512 and the
hu values of ct images are windowed to the range of [-200, 300].",3
50,791,Semi-supervised Domain Adaptive Medical Image Segmentation Through Consistency Regularized Disentangled Contrastive Learning,3.0,Experiments and Results,"datasets: we evaluate our work on two different da tasks to evaluate its
generalizability: (1) polyp segmentation from colonoscopy images in kvasir-seg
[11] and cvc-endoscene still [20], and (2) brain tumor segmentation in mri
images from brats2018 [16]. kvasir and cvc contain 1000 and 912 images
respectively and were split into 4 : 1 training-testing sets following [10].
brats consists of brain mris from 285 patients with t1, t2, t1ce, and flair
scans. the data was split into 4 : 1 train-test ratio, following [14].
source→target: we perform experiments on cv c → kvasir and kvasir → cv c for
polyp segmentation, and t 2 → {t 1, t 1ce, f lair} for tumor segmentation. the
ssda accesses 10 -50% and 1 -5 labels from the target domain for the two tasks,
respectively. for uda, only s is used for l sup , whereas t 1 ∪ t 2 is used for
l reg . implementation details: implementation is done in a pytorch environment
using a tesla v100 gpu with 32gb ram. we use u-net [17] backbone for the
encoder-decoder structure, and the projection heads g s and g c are shallow fc
layers. the model is trained for 300 epochs for pre-training and 500 epochs for
fine-tuning using an adam optimizer with a batch size of 4 and a learning rate
of 1e -4. λ1, λ2, λ3, and t h are set to 0.75, 0.75, 0.5, 0.6, respectively by
validation, τ, α are set to 0.07, 0.999 following [9]. augmentations include
random rotation and translation. metrics: segmentation performance is evaluated
using dice similarity score (dsc) and hausdorff distance (hd).",4
51,803,BerDiff: Conditional Bernoulli Diffusion Model for Medical Image Segmentation,3.1,Experimental Setup,"dataset and preprocessing. the data used in this experiment are obtained from
lidc-idri [2,7] and brats 2021 [4] datasets. lidc-idri contains 1,018 lung ct
scans with plausible segmentation masks annotated by four radiologists. we adopt
a standard preprocessing pipeline for lung ct scans and the trainvalidation-test
partition as in previous work [5,15,23]. brats 2021 consists of four different
sequence (t1, t2, flair, t1ce) mri images for each patient. all 3d scans are
sliced into axial slices and discarded the bottom 80 and top 26 slices. note
that we treat the original four types of brain tumors as one type following
previous work [25], converting the multi-target segmentation problem into
binary. our training set includes 55,174 2d images scanned from 1,126 patients,
and the test set comprises 3,991 2d images scanned from 125 patients. finally,
the sizes of images from lidc-idri and brast 2021 are resized to a resolution of
128 × 128 and 224 × 224, respectively. implementation details. we implement all
the methods with the pytorch library and train the models on nvidia v100 gpus.
all the networks are trained using the adamw [19] optimizer with a mini-batch
size of 32. the initial learning rate is set to 1 × 10 -4 for brats 2021 and 5 ×
10 -5 for lidc-idri. the bernoulli noise estimation u-net network in fig. 1 of
our berdiff is the same as previous diffusion-based models [20]. we employ a
linear noise schedule for t = 1000 timesteps for all the diffusion models. and
we use the sub-sequence sampling strategy of ddim to accelerate the segmentation
process. during minibatch training of lidc-idri, our berdiff learns diverse
expertise by randomly sampling one from four annotated segmentation masks for
each image. four metrics are used for performance evaluation, including
generalized energy distance (ged), hungarian-matched intersection over union
(hm-iou), soft-dice and dice coefficient. we compute ged using varying numbers
of segmentation samples (1, 4, 8, and 16), hm-iou and soft-dice using 16
samples.",4
52,825,Multimodal CT and MR Segmentation of Head and Neck Organs-at-Risk,0.0,Related Work.,"a literature review by zhang et al. [24] divides deep learning (dl)-based
multimodal segmentation methods into three fusion strategy groups: early, late
and hybrid (also named layer ) fusion. the first two groups of methods are most
commonly applied; early fusion comprises simple concatenation of modalities
along the channel dimension before feeding them into the deep neural network.
additionally, concatenating feature maps (fms) from separate modality encoders
can also be considered as early fusion [7]. late fusion, on the other hand,
employs separate branches for each input modality and then fuses the output
features by either plain concatenation or by weighing the contributions of
separate branches at the decision level. for example, zhang et al. [23] proposed
an attention mechanism to fuse fms from two separate u-nets that accepted
contrast-enhanced arterial and venous phase ct images. the third group, hybrid
fusion, aims to combine the strengths of early and late fusion [24] by employing
two or more separate encoders (i.e. one for each modality) and a single decoder,
where features from different resolution levels of the encoder are fused and fed
into the decoder that produces the final full-resolution segmentation. such
hybrid or multi-level fusion along with the adaptive fusion method represents
the current trend in computer vision [24], with the self-supervised model
adaptation method as a prime example [18]. one important aspect is also the
missing modality scenario, meaning that the multimodal model should produce
satisfactory results even if only one input modality is available. nevertheless,
the optimal fusion strategy remains an open question in need of further
exploration. similar conclusions were reached in a review of multimodal
segmentation methods in the medical imaging community by zhou et al. [25]. most
methods implement either early or late fusion, however, the layer fusion
strategy was identified as a better choice, since dense connections among layers
can exploit more complex and complementary information to enhance training. the
highlight is hyperdensenet, a dual-path 3d network proposed by dolz et al. [4]
that employs dense connections between two convolutional paths, and achieves
improvements compared to other fusion strategies and single modality variants.
however, other studies have shown that the best fusion strategy depends on the
specific nature of the problem, e.g. yan et al. [22] demonstrated that the late
fusion outperforms the other two approaches for the longitudinal detection of
diabetic retinopathy. relevant to the field of multimodal segmentation are also
developments on unpaired multimodal segmentation, where cross-modality learning
is employed to take advantage of different image modalities covering the same
anatomy, but without the constraint to collect images from the same patients
[5,10,19]. although the methodologies comprising cyclegans and/or multiple
segmentation networks [10,19] seem promising, they can be excessively complex
for the task of han oar segmentation where both ct and mr image modalities from
the same patient are often available. consequently, our primary focus is the
paired multimodal segmentation problem, including the missing modality
scenario.motivation. when segmenting oars in the han region for the purpose of
rt planning, a multimodal segmentation model that can leverage the information
from ct and mr images of the same patient might be beneficial compared to
separate single-modal models. firstly, as intuition suggests, such a model would
rely on the ct image for bone structures and on the mr image for soft tissues,
and therefore improve the overall segmentation quality by exploiting the
complementary information from both modalities. secondly, a multimodal model
would facilitate cross-modality learning by extracting knowledge from one and
applying that knowledge to the other modality, potentially improving the
segmentation accuracy. several studies indicated that such an approach is
feasible, for example, for improving video classification by training a model on
an auxiliary audio reconstruction task [12], or for audio-based detection by
using the multimodal knowledge distillation concept, where teacher networks
trained on rgb, depth and thermal images improve a student network trained only
on audio data [20]. finally, from the dl infrastructure maintenance perspective,
it is easier to maintain a single model that can handle both modalities than two
separate models for each modality. however, clinical practice differs
considerably from theory, meaning that a number of considerations must be taken
into account. firstly, although mr image acquisition is recommended, it is not
always feasible due to time constraints, scanner occupancy and financial
aspects. consequently, automatic oar multimodal segmentation is required to
handle the missing modality scenario, and provide a similar segmentation quality
as a single-modality system. secondly, because ct and mr images are not acquired
simultaneously and with the same acquisition parameters (e.g. resolution), there
is an inherent misalignment between both modalities. this can be mitigated with
image registration, but not completely, mainly due to different patient
positioning that especially affects the deformation of soft tissues, and various
modality-specific artifacts (e.g. motion, implants, partial volume effect,
etc.).",4
53,829,Multimodal CT and MR Segmentation of Head and Neck Organs-at-Risk,3.0,Experiments and Results,"image datasets. the proposed methodology was evaluated on two publicly available
datasets: our recently released han-seg dataset [14] and the pddca dataset [15].
the han-seg dataset comprises ct and t1-weighted mr images of 56 patients, which
were deformably registered with the simpleelastix registration tool, and
corresponding curated manual delineations of 30 oars (for details, please refer
to [14]). although only a subset of images is publicly available1 due to the
ongoing han-seg challenge2 , both the publicly available training as well as the
privately withheld test images were used in our 4-fold cross-validation
experiments. on the other hand, to evaluate the generalization ability of our
method, we also conducted experiments on the ct-only pddca dataset (for details,
please refer to [15]), from which we collected 15 images from the offand on-site
test sets of the corresponding challenge for our evaluation. as this dataset is
widely used for evaluating the performance of automatic han oar segmentation
methods, it serves as a valuable benchmark for comparison with other
state-of-the-art methods. note that none of the images from the ct-only pddca
dataset were used for training, and as our model expects two inputs, we
substituted the missing mr modality with an empty matrix (i.e.
zeros).implementation details. all models were trained for all oars using the 3d
fullres configuration of nnu-net, with the only modification that we reduced
rotation around the axial axis and disabled image flipping along the sagittal
plane, which eliminated segmentation errors that were previously observed for
the paired (left and right) oars. the same modification was also used with the
maml model. to ensure a fair model comparison, we set the number of filters in
the encoder of the single modality baseline model to match the number of filters
of the entry-level concatenation encoder. we also halved the number of filters
in networks that have separate encoders so that the overall number of parameters
in the proposed model and the baselines remains approximately the same
(excluding the parameters in the localization part of mfm block). note that the
maml model, which is composed of two u-nets, had a considerably higher number of
parameters. to address the challenge of a relatively small dataset, we adopted a
4-fold cross-validation strategy without using any external training images. all
models were trained until convergence, i.e. when the validation loss plateaued,
and we selected the model with the best validation loss for inference.results.
the quality of the obtained oar segmentation masks was evaluated by computing
the dice similarity coefficient (dsc) and the 95 th -percentile hausdorff
distance (hd 95 ) against reference manual delineations, and the results for all
oars are presented in figs. 2 and3, respectively. since not all images contain
all 30 oars (due to a different field-of-view), we first calculated the mean
metric for each oar and then the overall mean across all oars to ensure that the
contributions were equally weighted. we also performed analysis of statistical
significance by applying paired sample t-tests with the bonferroni correction,
presented with bars on top of the box plots (non-significant: ns (p > 0.05),
significant: * (0.01 < p < 0.05), * * (0.001 < p < 0.01), * * * (0.0001 < p <
0.001) and * * * * (p < 0.0001)).",4
54,844,Breast Ultrasound Tumor Classification Using a Hybrid Multitask CNN-Transformer Network,3.3,Implementation Details,"the proposed approach was implemented with keras and tensorflow libraries.all
experiments were performed on a machine with nvidia quadro rtx 8000 gpus and two
intel xeon silver 4210r cpus (2.40ghz) with 512 gb of ram. all bus images in the
dataset were zero-padded and reshaped to form square images. to avoid data
leakage and bias, we selected the train, test, and validation sets based on the
cases, i.e., the images from one case (patient) were assigned to only one of the
training, validation, and test sets. furthermore, we employed horizontal flip,
height shift (20%), width shift (20%), and rotation (20 • c) for data
augmentation. the proposed approach utilizes the building blocks of resnet50 and
swin-transformer-v2, pretrained on imagenet dataset. namely, mt-estan uses
pretrained resnet50 as a base model for the five encoder blocks (the
implementation details of mt-estan can be found in [3]). the encoder with aaa
blocks uses the swintransformer v2 base 256 pretrained model as a backbone. for
the composite loss function, we adopted a weight coefficient w 1 = 3, and in the
focal loss α = 0.5 and γ = 2. for model training we utilized adam optimizer with
a learning rate of 10 -5 and mini batch size of 4 images.",4
55,857,SimPLe: Similarity-Aware Propagation Learning for Weakly-Supervised Breast Cancer Segmentation in DCE-MRI,1.0,Introduction,"breast cancer is the most common cause of cancer-related deaths among women all
around the world [8]. early diagnosis and treatment is beneficial to improve the
survival rate and prognosis of breast cancer patients. mammography,
ultrasonography, and magnetic resonance imaging (mri) are routine imaging
modalities for breast examinations [15]. recent clinical studies have proven
that dynamic contrast-enhanced (dce)-mri has the capability to reflect tumor
morphology, texture, and kinetic heterogeneity [14], and is with the highest
sensitivity for breast cancer screening and diagnosis among current clinical
imaging modalities [17]. the basis for dce-mri is a dynamic t1-weighted contrast
enhanced sequence (fig. 1). t1-weighted acquisition depicts enhancing
abnormalities after contrast material administration, that is, the cancer
screening is performed by using the post-contrast images. radiologists will
analyze features such as texture, morphology, and then make the treatment plan
or prognosis assessment. computer-aided feature quantification and diagnosis
algorithms have recently been exploited to facilitate radiologists analyze
breast dce-mri [12,22], in which automatic cancer segmentation is the very first
and important step.to better support the radiologists with breast cancer
diagnosis, various segmentation algorithms have been developed [20]. early
studies focused on image processing based approaches by conducting graph-cut
segmentation [29] or analyzing low-level hand-crafted features [1,11,19]. these
methods may encounter the issue of high computational complexity when analyzing
volumetric data, and most of them require manual interactions. recently,
deep-learning-based methods have been applied to analyze breast mri. zhang et
al. [28] proposed a mask-guided hierarchical learning framework for breast tumor
segmentation via convolutional neural networks (cnns), in which breast masks
were also required to train one of cnns. this framework achieved a mean dice
value of 72% on 48 testing t1-weighted scans. li et al. [16] developed a
multi-stream fusion mechanism to analyze t1/t2-weighted scans, and obtained a
dice result of 77% on 313 subjects. gao et al. [7] proposed a 2d cnn
architecture with designed attention modules, and got a dice result of 81% on 87
testing samples. zhou et al. [30] employed a 3d affinity learning based
multi-branch ensemble network for the segmentation refinement and generated 78%
dice on 90 testing subjects. wang et al. [24] integrated a combined 2d and 3d
cnn and a contextual pyramid into u-net to obtain a dice result of 76% on 90
subjects. wang et al. [25] proposed a tumor-sensitive synthesis module to reduce
false segmentation and obtained 78% dice value. to reduce the huge annotation
burden for the segmentation task, zeng et al. [27] presented a semi-supervised
strategy to segment the manually cropped dce-mri scans, and attained a dice
value of 78%.although [27] has been proposed to alleviate the annotation effort,
to acquire the voxel-level segmentation masks is still time-consuming and
laborious, see fig. 1(c). weakly-supervised learning strategies such as extreme
points [5,21], bounding box [6] and scribbles [4] can be promising solutions.
roth et al. [21] utilized extreme points to generate scribbles to supervise the
training of the segmentation network. based on [21], dorent et al. [5]
introduced a regularized loss [4] derived from a conditional random field (crf)
formulation to encourage the prediction consistency over homogeneous regions. du
et al. [6] employed bounding boxes to train the segmentation network for organs.
however, the geometric prior used in [6] can not be an appropriate strategy for
the segmentation of lesions with various shapes. to our knowledge, currently
only one weakly-supervised work [18] has been proposed for breast mass
segmentation in dce-mri. this method employed three partial annotation methods
including single-slice, orthogonal-slice (i.e., 3 slices) and interval-slice (∼6
slices) to alleviate the annotation cost, and then constrained segmentation by
estimated volume using the partial annotation. the method obtained a dice value
of 83% using the interval-slice annotation, on a testing dataset containing only
28 patients.in this study, we propose a simple yet effective weakly-supervised
strategy, by using extreme points as annotations (see fig. 1(d)) to segment
breast cancer. specifically, we attempt to optimize the segmentation network via
the conventional trainfine-tuneretrain process. the initial training is
supervised by a contrastive loss to pull close positive voxels in feature space.
the fine-tune is conducted by using a similarity-aware propagation learning
(simple) strategy to update the pseudo-masks for the subsequent retrain. we
evaluate our method on a collected dce-mri dataset containing 206 subjects.
experimental results show our method achieves competitive performance compared
with fully supervision, demonstrating the efficacy of the proposed simple
strategy.",4
58,925,Edge-Aware Multi-task Network for Integrating Quantification Segmentation and Uncertainty Prediction of Liver Tumor on Multi-modality Non-contrast MRI,1.0,Introduction,"simultaneous multi-index quantification (i.e., max diameter (md), center point
coordinates (x o , y o ), and area), segmentation, and uncertainty prediction of
liver tumor have essential significance for the prognosis and treatment of
patients [6,16]. in clinical settings, segmentation and quantitation are
manually performed by the clinicians through visually analyzing the
contrast-enhanced mri images (cemri) [9,10,18]. however, as shown in fig. 1(b),
contrast-enhanced fig. 1. our method integrates segmentation and quantification
of liver tumor using multi-modality ncmri, which has the advantages of avoiding
contrast agent injection, mutual promotion of multi-task, and reliability and
stability. mri (cemri) has the drawbacks of being toxic, expensive, and
time-consuming due to the need for contrast agents (ca) to be injected [2,4].
moreover, manually annotating medical images is a laborious and tedious process
that requires human expertise, making it manpower-intensive, subjective, and
prone to variation [14]. therefore, it is desirable to provide a reliable and
stable tool for simultaneous segmentation, quantification, and uncertainty
analysis, without requiring the use of contrast agents, as shown in fig.
1(a).recently, an increasing number of works have been attempted on liver tumor
segmentation or quantification [25,26,28,30]. as shown in fig. 1(c), the work
[26] attempted to use the t2fs for liver tumor segmentation, while it ignored
the complementary information between multi-modality ncmri of t2fs and dwi. in
particular, there is evidence that diffusion-weighted imaging (dwi) helps to
improve the detection sensitivity of focal lesions as these lesions typically
have higher cell density and microstructure heterogeneity [20]. the study in
[25,30] attempted to quantify the multi-index of liver tumor, however, the
approach is limited to using multi-phase cemri that requires the injection of
ca. in addition, all these works are limited to a single task and ignore the
constraints and mutual promotion between multi-tasks. available evidence
suggests that uncertainty information regarding segmentation results is
important as it guides clinical decisions and helps understand the reliability
of the provided segmentation. however, current research on liver tumors tends to
overlook this vital task.to the best of our knowledge, although many works focus
on the simultaneous quantization, segmentation, and uncertainty in medical
images (i.e., heart [3,5,11,27], kidney [17], polyp [13]). no attempt has been
made to automatically liver tumor multi-task via integrating multi-modality
ncmri due to the following challenges: (1) the lack of an effective
multi-modality mri fusion mechanism. because the imaging characteristics between
t2fs and dwi have significant differences (i.e., t2fs is good at anatomy
structure information while dwi is good at location information of lesions
[29]). (2) the lack of strategy for capturing the accurate boundary information
of liver tumors. due to the lack of contrast agent injection, the boundary of
the lesion may appear blurred or even invisible in a single ncmri, making it
challenging to accurately capture tumor boundaries [29]. (3) the lack of an
associated multi-task framework. because segmentation and uncertainty involve
pixel-level classification, whereas quantification tasks involve image-level
regression [11]. this makes it challenging to integrate and optimize the
complementary information between multi-tasks.in this study, we propose an
edge-aware multi-task network (eamtnet) that integrates the multi-index
quantification (i.e., center point, max-diameter (md), and area), segmentation,
and uncertainty. our basic assumption is that the model should capture the
long-range dependency of features between multimodality and enhance the boundary
information for quantification, segmentation, and uncertainty of liver tumors.
the two parallel cnn encoders first extract local feature maps of multi-modality
ncmri. meanwhile, to enhance the weight of tumor boundary information, the sobel
filters are employed to extract edge maps that are fed into edge-aware feature
aggregation (eafa) as prior knowledge. then, the eafa module is designed to
select and fuse the information of multi-modality, making our eamtnet edge-aware
by capturing the long-range dependency of features maps and edge maps. lastly,
the proposed method estimates segmentation, uncertainty prediction, and
multi-index quantification simultaneously by combining multi-task and cross-task
joint loss.the contributions of this work mainly include: (1) for the first
time, multiindex quantification, segmentation, and uncertainty of the liver
tumor on multimodality ncmri are achieved simultaneously, providing a
time-saving, reliable, and stable clinical tool. (2) the edge information
extracted by the sobel filter enhances the weight of the tumor boundary by
connecting the local feature as prior knowledge. (3) the novel eafa module makes
our eamtnet edge-aware by capturing the long-range dependency of features maps
and edge maps for feature fusion. the source code will be available on the
author's website.",4
60,955,Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data,1.0,Introduction,"prostate segmentation from magnetic resonance imaging (mri) is a crucial step
for diagnosis and treatment planning of prostate cancer. recently, deep
learningbased approaches have greatly improved the accuracy and efficiency of
automatic prostate mri segmentation [7,8]. yet, their success usually requires a
large amount of labeled medical data, which is expensive and expertise-demanding
in practice. in this regard, semi-supervised learning (ssl) has emerged as an
attractive option as it can leverage both limited labeled data and abundant
unlabeled data [3,[9][10][11]15,16,[21][22][23][24][25][26]28]. nevertheless,
the effectiveness of ssl is heavily dependent on the quantity and quality of the
unlabeled data.regarding quantity , the abundance of unlabeled data serves as a
way to regularize the model and alleviate overfitting to the limited labeled
data. unfortunately, such ""abundance"" may be unobtainable in practice, i.e., the
local unlabeled pool is also limited due to restricted image collection
capabilities or scarce patient samples. as a specific case shown in table 1,
there are only limited prostate scans available per center. taking c1 as a case
study, if the amount of local unlabeled data is limited, existing ssl methods
may still suffer from inferior performance when generalizing to unseen test data
(fig. 1). to efficiently enrich the unlabeled pool, seeking support from other
centers is a viable solution, as illustrated in fig. 1. yet, due to differences
in imaging protocols and variations in patient demographics, this solution
usually introduces data heterogeneity, lead-ing to a quality problem. such
heterogeneity may impede the performance of ssl which typically assumes that the
distributions of labeled data and unlabeled data are independent and identically
distributed (i.i.d.) [16]. thus, proper mechanisms are called for this practical
but challenging ssl scenario.here, we define this new ssl scenario as multi-site
semi-supervised learning (ms-ssl), allowing to enrich the unlabeled pool with
multi-site heterogeneous images. being an under-explored scenario, few efforts
have been made. to our best knowledge, the most relevant work is ahdc [2].
however, it only deals with additional unlabeled data from a specific source
rather than multiple arbitrary sources. thus, it intuitively utilizes
image-level mapping to minimize dual-distribution discrepancy. yet, their
adversarial min-max optimization often leads to instability and it is difficult
to align multiple external sources with the local source using a single image
mapping network.in this work, we propose a more generalized framework called
categorylevel regularized unlabeled-to-labeled (cu2l) learning, as depicted in
fig. 2, to achieve robust ms-ssl for prostate mri segmentation. specifically,
cu2l is built upon the teacher-student architecture with customized learning
strategies for local and external unlabeled data: (i) recognizing the importance
of supervised learning in data distribution fitting (which leads to the failure
of cps [3] in ms-ssl as elaborated in sec. 3), the local unlabeled data is
involved into pseudolabel supervised-like learning to reinforce fitting of the
local data distribution; (ii) considering that intra-class variance hinders
effective ms-ssl, we introduce a non-parametric unlabeled-to-labeled learning
scheme, which takes advantage of the scarce expert labels to explicitly
constrain the prototype-propagated predictions, to help the model exploit
discriminative and domain-insensitive features from heterogeneous multi-site
data to support the local center. yet, observing that such scheme is challenging
when significant shifts and various distributions are present, we further
propose category-level regularization, which advocates prototype alignment, to
regularize the distribution of intra-class features from arbitrary external data
to be closer to the local distribution; (iii) based on the fact that
perturbations (e.g., gaussian noises [15]) can be regarded as a simulation of
heterogeneity, perturbed stability learning is incorporated to enhance the
robustness of the model. our method is evaluated on prostate mri data from six
different clinical centers and shows promising performance on tackling ms-ssl
compared to other semi-supervised methods.",4
61,958,Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data,2.3,Category-Level Regularized Unlabeled-to-Labeled Learning,"unlabeled-to-labeled learning. inherently, the challenge of ms-ssl stems from
intra-class variation, which results from different imaging protocols, disease
progress and patient demographics. inspired by prototypical networks [13,19,25]
that compare class prototypes with pixel features to perform segmentation, here,
we introduce a non-parametric unlabeled-to-labeled (u2l) learning scheme that
utilizes expert labels to explicitly constrain the prototype-propagated
predictions. such design is based on two considerations: (i) a good
prototypepropagated prediction requires both compact feature and discriminative
prototypes, thus enhancing this prediction can encourage the model to learn in a
variation-insensitive manner and focus on the most informative clues; (ii) using
expert labels as final guidance can prevent error propagation from pseudo
labels. specifically, we denote the feature map of the external unlabeled image
x u e before the penultimate convolution in the teacher model as f u,t e . note
that f u,t e has been upsampled to the same size of x u e via bilinear
interpolation but with l channels. with the argmax pseudo label ŷ u,t e and the
predicted probability map p u,t e , the object prototype from the external
unlabeled data can be computed via confidence-weighted masked average pooling:
c.likewise, the background prototype c u(bg) e can also be obtained. considering
the possible unbalanced sampling of prostate-containing slices, ema strategy
across training steps (with a decay rate of 0.9) is applied for prototype
update. then, as shown in fig. 2 , where we use cosine similarity for sim(•, •)
and empirically set the temperature t to 0.05 [19]. note that a similar
procedure can also be applied to the local unlabeled data x u local , and thus
we can obtain another prototype-propagated unlabeledto-labeled prediction p u2l
local for x l local . as such, given the accurate expert label y l local , the
unlabeled-to-labeled supervision can be computed as:category-level
regularization. being a challenging scheme itself, the above u2l learning can
only handle minor intra-class variation. thus, proper mechanisms are needed to
alleviate the negative impact of significant shift and multiple distributions.
specifically, we introduce category-level regularization, which advocates class
prototype alignment between local and external data, to regularize the
distribution of intra-class features from arbitrary external data to be closer
to the local one, thus reducing the difficulty of u2l learning. in u2l, we have
obtained prototypes from local unlabeled data {c where mean squared error is
adopted as the distance function d(•, •). the weight of background prototype
alignment is smaller due to less relevant contexts.",4
62,960,Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data,3.0,Experiments and Results,"materials. we utilize prostate t2-weighted mr images from six different clinical
centers (c1-6) [1,4,5] to perform a retrospective evaluation. rizes the
characteristics of the six data sources, following [7,8], where [7,8] also
reveal the severity of inter-center heterogeneity here through extensive
experiments. the heterogeneity comes from the differences in scanners, field
strengths, coil types, disease and in-plane/through-plane resolution. compared
to c1 and c2, scans from c3 to c6 are taken from patients with prostate cancer,
either for detection or staging purposes, which can cause inherent semantic
differences in the prostate region to further aggravate heterogeneity. following
[7,8], we crop each scan to preserve the slices with the prostate region only
and then resize and normalize it to 384 × 384 px in the axial plane with zero
mean and unit variance. we take c1 or c2 as the local target center and randomly
divide their 30 scans into 18, 3, and 9 samples as training, validation, and
test sets, respectively.implementation and evaluation metrics. the framework is
implemented on pytorch using an nvidia geforce rtx 3090 gpu. considering the
large variance in slice thickness among different centers, we adopt the 2d
architecture. specifically, 2d u-net [12] is adopted as our backbone. consists
of the cross-entropy loss and the k-regional dice loss [6]. the maximum
consistency weight w max is set to 0.1 [20,26]. t max is set to 20,000. k is
empirically set to 2. the network is trained using the sgd optimizer and the
learning rate is initialized as 0.01 and decayed by multiplication with (1.0t/t
max ) 0.9 . data augmentation is applied, including random flip and rotation. we
adopt the dice similarity coefficient (dsc) and jaccard as the evaluation
metrics and the results are the average over three runs with different
seeds.comparison study. table 2 presents the quantitative results with either c1
or c2 as the local target center, wherein only 6 or 8 local scans are annotated.
besides the supervised-only baselines, we include recent top-performing ssl
methods [2,3,11,14,15,17,20,25,26] for comparison. all methods are implemented
with the same backbone and training protocols to ensure fairness. as observed,
compared to the supervised-only baselines, our cu2l with {6, 8} local labeled
scans achieves {19.15%, 17.42%} and {9.1%, 6.44%} dsc improvements in {c1, c2},
showing its effectiveness in leveraging multi-site unlabeled data. despite the
violation of the assumption of i.i.d. data, existing ssl methods can still
benefit from the external unlabeled data to some extent compared to the results
using local data only as shown in fig. 1, revealing that the quantity of
unlabeled data has a significant impact. however, due to the lack of proper
mechanisms for learning from heterogeneous data, limited improvement can be
achieved by them, especially for cps [3] and fixmatch [14] in c2. particularly,
cps relies on cross-modal pseudo labeling which exploits all the unlabeled data
in a supervised-like fashion. we attribute its degradation to the fact that
supervised learning is crucial for distribution fitting, which supports our
motivation of performing pseudo-label learning on local unlabeled data only. as
a result, its models struggle to determine which distribution to prioritize.
meanwhile, the most relevant ahdc [2] is mediocre in ms-ssl, mainly due to the
instability of adversarial training and the difficulty of aligning multiple
distributions to the local distribution via a single image-mapping network. in
contrast, with specialized mechanisms for simultaneously learning informative
representations from multi-site data and handling heterogeneity, our cu2l
obtains the best performance over the recent ssl methods. figure 3(a) further
shows that the predictions of our method fit more accurately with the ground
truth.ablation study. to evaluate the effectiveness of each component, we
conduct an ablation study under the setting with 6 local labeled scans, as shown
in fig. 2(b). firstly, when we remove l u p l (cu2l-1), the performance drops by
{5.69% (c1), 3.05%(c2)} in dsc, showing that reinforcing confirmation on local
distribution is critical. cu2l-2 represents the removal of both l u2l and l cr ,
and it can be observed that such an unlabeled-to-labeled learning approach
combined with class-level regularization is crucial for exploring multi-site
data. if we remove l cr which accompanies with l u2l (cu2l-3), the performance
degrades, which justifies the necessity of this regularization to reduce the
difficulty of unlabeled-to-labeled learning process. cu2l-4 denotes the removal
of l u sta . as observed, such a typical stability loss [15] can further improve
the performance by introducing hand-crafted noises to enhance the robustness to
real-world heterogeneity.",4
73,1031,H-DenseFormer: An Efficient Hybrid Densely Connected Transformer for Multimodal Tumor Segmentation,3.1,Dataset and Metrics,"to validate the effectiveness of our proposed method, we performed extensive
experiments on hecktor21 [1] and pi-cai221 . hecktor21 is a dualmodality dataset
for head and neck tumor segmentation, containing 224 pet-ct image pairs. each
pet-ct pair is registered and cropped to a fixed size of (144,144,144). pi-cai22
provides multimodal mr images of 220 patients with prostate cancer, including
t2-weighted imaging (t2w), high b-value diffusion-weighted imaging (dwi), and
apparent diffusion coefficient (adc) maps. after standard resampling and center
cropping, all images have a size of (24,384,384). we randomly select 180 samples
for each dataset as the training set and the rest as the independent test set
(44 cases for hecktor21 and 40 cases for pi-cai22). specifically, the training
set is further randomly divided into five folds for cross-validation. for
quantitative analysis, we use the dice similarity coefficient (dsc), the jaccard
index (ji), and the 95% hausdorff distance (hd95) as evaluation metrics for
segmentation performance. a better segmentation will have a smaller hd95 and
larger values for dsc and ji. we also conduct holistic t-tests of the overall
performance for our method and all baseline models with the two-tailed p < 0.05.",4
75,1063,Diffusion Kinetic Model for Breast Cancer Segmentation in Incomplete DCE-MRI,1.0,Introduction,"dynamic contrast-enhanced magnetic resonance imaging (dce-mri) revealing tumor
hemodynamics information is often applied to early diagnosis and treatment of
breast cancer [1]. in particular, automatically and accurately segmenting tumor
regions in dce-mri is vital for computer-aided diagnosis (cad) and various
clinical tasks such as surgical planning. for the sake of promoting segmentation
performance, recent methods utilize the dynamic mr sequence and exploit its
temporal correlations to acquire powerful representations [2][3][4]. more
recently, a handful of approaches take advantage of hemodynamic knowledge and
time intensity curve (tic) to improve segmentation accuracy [5,6]. however, the
aforementioned methods require the complete dce-mri sequences and overlook the
difficulty in assessing complete temporal sequences and the missing time point
problem, especially post-contrast phase, due to the privacy protection and
patient conditions. hence, these breast cancer segmentation models cannot be
deployed directly in clinical practice.recently, denoising diffusion
probabilistic model (ddpm) [7,8] has produced a tremendous impact on image
generation field due to its impressive performance. diffusion model is composed
of a forward diffusion process that add noise to images, along with a reverse
generation process that generates realistic images from the noisy input [8].
based on this, several methods investigate the potential of ddpm for natural
image segmentation [9] and medical image segmentation [10][11][12].
specifically, baranchuk et al. [9] explores the intermediate activations from
the networks that perform the markov step of the reverse diffusion process and
find these activations can capture semantic information for segmentation.
however, the applicability of ddpm to medical image segmentation are still
limited. in addition, existing ddpm-based segmentation networks are generic and
are not optimized for specific applications. in particular, a core question for
dce-mri segmentation is how to optimally exploit hemodynamic priors.based on the
above observations, we innovatively consider the underlying relation between
hemodynamic response function (hrf) and denoising diffusion process (ddp). as
shown in fig. 1, during hrf process, only tumor lesions are enhanced and other
non-tumor regions remain unchanged. by designing a network architecture to
effectively transmute pre-contrast images into post-contrast images, the network
should acquire hemodynamic inherent in hrf that can be used to improve
segmentation performance. inspired by the fact that ddpm generates images from
noisy input provided by the parameterized gaussian process, this work aims to
exploit implicit hemodynamic information by a diffusion process that predict
post-contrast images from noisy pre-contrast images. specifically, given the
pre-contrast and post-contrast images, the latent kinetic code is learned using
a score function of ddpm, which contains sufficient hemodynamic characteristics
to facilitate segmentation performance.once the diffusion module is pretrained,
the latent kinetic code can be easily generated with only pre-contrast images,
which is fed into a segmentation module to annotate cancers. to verify the
effectiveness of the latent kinetic code, the sm adopts a simple u-net-like
structure, with an encoder to simultaneously conduct semantic feature encoding
and kinetic code fusion, along with a decoder to obtain voxel-level
classification. in this manner, our latent kinetic code can be interpreted to
provide tic information and hemodynamic characteristics for accurate cancer
segmentation.we verify the effectiveness of our proposed diffusion kinetic model
(dkm) on dce-mri-based breast cancer segmentation using breast-mri-nact-pilot
dataset [13]. compared to the existing state-of-the-art approaches with complete
sequences, our method yields higher segmentation performance even with
precontrast images. in summary, the main contributions of this work are listed
as follows:• we propose a diffusion kinetic model that implicitly exploits
hemodynamic priors in dce-mri and effectively generates high-quality
segmentation maps only requiring pre-contrast images. • we first consider the
underlying relation between hemodynamic response function and denoising
diffusion process and provide a ddpm-based solution to capture a latent kinetic
code for hemodynamic knowledge. • compared to the existing approaches with
complete sequences, the proposed method yields higher cancer segmentation
performance even with pre-contrast images.",4
76,1068,Diffusion Kinetic Model for Breast Cancer Segmentation in Incomplete DCE-MRI,3.0,Experiments,"dataset: to demonstrate the effectiveness of our proposed dkm, we evaluate our
method on 4d dce-mri breast cancer segmentation using the breast-mri-nact-pilot
dataset [13], which contains a total of 64 patients with the contrastenhanced
mri protocol: a pre-contrast scan, followed by 2 consecutive postcontrast time
points (as shown in fig. 3). each mr volume consists of 60 slices and the size
of each slice is 256 × 256. regarding preprocessing, we conduct zeromean
unit-variance intensity normalization for the whole volume. we divided the
original dataset into training (70%) and test set (30%) based on the scans.
ground truth segmentations of the data are provided in the dataset for tumor
annotation. no data augmentation techniques are used to ensure fairness.",4
78,1089,EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,1.0,Introduction,"medical image analysis has greatly benefited from advances in ai [1] yet some
improvements still remain to be addressed, importantly in areas that allow both
algorithmic performance and fairness [2], and in certain medical applications
that promise to significantly lessen morbidity and mortality. early detection of
skin lesions is such an endeavor as it can aid in identifying infectious
diseases with cutaneous manifestations. lyme disease is an example of that with
a potentially diagnostic skin lesion [3]-which is caused by the bacterium
borrelia burgdorferi and leads to nearly 476,000 cases per annum during
2010-2018 [4]. the earliest and most treatable phase of lyme disease is
manifested via a red concentric lesion at the site of a tick bite, called
erythema migrans (em) [5]. while the em pattern may appear simple to recognize,
its diagnosis can be challenging for those with or without a medical background
alike, as only 20% of united states patients have the stereotypical bull's eye
lesion [6]. when skin lesions are atypical they can be mistaken for other
diseases such as tinea corporis (tc) or herpes zoster (hz), two other diseases
acting as confusers for lyme, considered herein. this has increased interest in
medical applications of deep learning (dl), and using deep convolutional neural
networks (cnns), to assist clinicians in timely and accurate diagnosis of
conditions including lyme disease, tc and hz [7][8][9].one important diagnosis
task is to segment lyme lesion, particularly the em pattern, from benign skins.
such dl-assisted segmentation not only helps clinicians in pre-screening
patients but also improves downstream tasks such as lesion classification.
however, while lyme disease lesion segmentation is intuitively simple, it is
challenging due to the following reasons. first, there lacks of a well-segmented
dataset with manual labels on lyme disease. on one hand, some datasets-such as
ham10000 [10] and isbi challenges [11]-have manual annotated segmentations for
diseases like melanoma, but they do not have lyme disease lesions. on the other
hand, some datasets-such as groh et al. [12]-have lyme disease and skin tone and
classification labels, but not segmentation.second, the segmentation of lyme
lesion is itself challenging due to the nature of em pattern. specifically, a
typical lyme lesion exhibits a bull's eye pattern with one central redness and
one outer circle, which is different from darkness lesion in cancer-related skin
disease like melanoma. furthermore, clinical data collected for training is
usually imbalanced in some properties, e.g., more samples with light skins
compared with dark skins. therefore, existing skin disease segmentation [13] as
well as existing general segmentation works, such as u-net [14], polar training
[15], vit-adapter [16], and mfsnet [17], usually suffer from relatively low
performance and reduced fairness [2,18,19].in this paper, we present the first
lyme disease dataset that contains labeled segmentation and skin tones. our lyme
disease dataset contains two parts: (i) a classification dataset, composed of
more than 3,000 diseased skin images that are either obtained from public
resources or clinicians with patient-informed consent, and (ii) a segmentation
dataset containing 185 samples that are manually annotated for three
regions-i.e., background, skin (light vs. dark), and lesionconducted under
clinician supervision and institutional review boards (irb) approval. our
dataset with manual labels is available at this url [20].secondly, we design a
simple yet novel data preprocessing and alternation method, called edgemixup, to
improve lyme disease segmentation and diagnosis fairness on samples with
different skin-tones. the key insight is to alter a skin image with a linear
combination of the source image and a detected lesion boundary so that the
lesion structure is preserved while minimizing skin tone information. such an
improvement is an iterative process that gradually improves lesion edge
detection and segmentation fairness until convergence. then, the detected,
converged edge in the first step also helps classification of lyme diseases via
mixup with improved fairness. our source code is available at this url [20].we
evaluate edgemixup for skin disease segmentation and classification tasks. our
results show that edgemixup is able to increase segmentation utility and improve
fairness. we also show that the improved segmentation further improves
classification fairness as well as joint fairness-utility metrics compared to
existing debiasing methods, e.g., ad [21] and st-debias [22].",4
79,1102,EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,4.0,Datasets,"we present two datasets: (i) a dataset collected and annotated by us (called
skin), and (ii) a subset of sd-198 [23] with our annotation (called sd-sub).
first, we collect and annotate a dataset with 3,027 images containing three
types of disease/lesions, i.e., tinea corporis (tc), herpes zoster (hz), and
erythema migrans (em). all skin images are either collected from publicly
available sources or from clinicians with patient informed consent. then, a
medical technician and a clinician in our team manually annotate each image. for
the segmentation task, we annotate skin images into three classes: background,
skin, and lesion; then, for the classification task, we annotate skin images by
classifying them into four classes: no disease (no), tc, hz, and em. we name it
as skin-class for later reference. second, we select five classes from sd-198
[23], a benchmark dataset for skin disease classification, as another dataset
for both segmentation and classification tasks. note that due to the amount of
manual work involved in annotation, we select those classes based on the number
of samples in each class. the selected classes are dermatofibroma (df),
keratoacanthoma (ka), pyogenic granuloma (pg), tinea corporis (tc), and tinea
faciale (tf). we choose 30 samples in each class for segmentation task, and we
split them into 0.7, 0.1, and 0.2 ratio for training, validation, and testing,
respectively.table 1 show the characteristics of these two datasets for both
classification and segmentation tasks broken down by the disease type and skin
tone, as calculated by the individual typology angle (ita) [24]. specifically,
we consider tan2, tan1, and dark as dark skin (ds) and others as light skin
(ls). compared to other skin tone classification schemas such as fitzpartick
scale [25], we divide ita scores into more detailed categories (eight). one
prominent observation is that ls images are more abundant than ds images due to
a disparity in the availability of ds imagery found from either public sources
or from clinicians with patient consent.",4
81,1115,Graph-Theoretic Automatic Lesion Tracking and Detection of Patterns of Lesion Changes in Longitudinal CT Studies,1.0,Introduction,"the periodic acquisition and analysis of volumetric ct and mri scans of oncology
patients is essential for the evaluation of the disease status, the selection of
the treatment, and the response to treatment. currently, scans are acquired
every 2-12 months according to the patient's characteristics, disease stage, and
treatment regime. the scan interpretation consists of identifying lesions
(primary tumors, metastases) in the affected organs and characterizing their
changes over time. lesion changes include changes in the size of existing
lesions, the appearance of new lesions, the disappearance of existing lesions,
and complex lesion changes, e.g., the formation of conglomerate lesions. as
treatments improve and patients live longer, the number of scans in longitudinal
studies increases and their interpretation is more challenging and
time-consuming.radiological follow-up requires the quantitative analysis of
lesions and patterns of lesion changes in subsequent scans. it differs from
diagnostic reading since the goal is to find and quantify the differences
between the scans, rather than to find abnormalities in a single scan. in
current practice, quantification of lesion changes is partial and approximate.
the recist 1.1 guidelines call for finding new lesions (if any), identifying up
to the five largest lesions in each scan in the ct slice where they appear
largest, manually measuring their diameters, and comparing their difference [1].
while volumetric measures of individual lesions and of all lesions (tumor
burden) have long been established as more accurate and reliable than partial
linear measurements, they are not used clinically because they require manual
lesion delineation and lesion matching in unregistered scans, which is usually
time-consuming and subject to variability [2].in a previous paper, we presented
an automatic pipeline for the detection and quantification of lesion changes in
pairs of ct liver scans [3]. this paper describes a graph-based lesion tracking
method for the comprehensive analysis of lesion changes and their patterns at
the lesion level. the tasks are formalized as graph-theoretic problems (fig. 1).
complex lesion changes include merged lesions, which occurs when at least two
lesions grow and merge into one (possible disease progression), split lesions,
which occurs when a lesion shrinks and cleaves into several parts (possible
response to treatment) and conglomeration of lesions, which occurs when clusters
of lesions coalesce. while some of these lesion changes have been observed [4],
they have been poorly studied. comprehensive quantitative analysis of lesion
changes and patterns is of clinical importance, since response to treatment may
vary among lesions, so the analysis of a few lesions may not be
representative.the novelties of this paper are: 1) identification and
formalization of longitudinal lesion matching and patterns of lesion changes in
ct in a graph-theoretic framework; 2) new classification and detection of
changes of individual lesions and lesion patterns based on the properties of the
lesion changes graph and its connected components; 3) a simultaneous lesion
matching method with more than two scans; 4) graph-based methods for the
detection of changes in individual lesions and patterns of lesion changes.
experimental results on lung (83 cts, 19 patients) and liver (77 cects, 18
patients) datasets show that our method yields high classification accuracy.to
the best of our knowledge, ours is the first method to perform longitudinal
lesion matching and lesion changes pattern detection. only a few papers address
lesion matching in pairs of ct/mri scans [5][6][7][8][9][10][11][12][13] -none
performs simultaneous matching of all lesions in more than two scans. also, very
few methods [3,14] handle matching of split/merged lesions. although many
methods exist for object tracking in optical images and videos [15][16][17],
they are unsuited for analyzing lesion changes since they assume many
consecutive 2d images where objects have very similar appearance and undergo
small changes between images. overlap-based methods pair two lesions in
registered scans when their segmentations overlap, with a reported accuracy of
66-98% [3,[5][6][7][8][9][10][11]18]. these methods assume that organs and
lesions undergo minor changes, are very sensitive to registration errors, and
cannot handle complex lesion changes. similarity-based methods pair two lesions
with similar features, e.g., intensity, shape, location [13][14][15][16] with an
84-96% accuracy on the deeplesion dataset [14]. they are susceptible to major
changes in the lesion appearance and do not handle complex lesion changes.
split-andmerge matching methods are used for cell tracking in fluorescence
microscopy [19]. they are limited to 2d images, assume registration between
images, and do not handle conglomerate changes.",5
82,1117,Graph-Theoretic Automatic Lesion Tracking and Detection of Patterns of Lesion Changes in Longitudinal CT Studies,2.1,Problem Formalization,"let s = s 1 , . . . , s n be a series of n ≥ 2 consecutive patient scans
acquired at timesis a set of vertices v i j corresponding to the lesions
associated with the lesion segmentation masks l i = l i 1 , l i 2 , . . . , l i
n i , where n i ≥ 0 is the number of lesions in scan s i at time t i . by
definition, any two lesionsj,l indicates that the lesions corresponding to
vertices v i j , v k l are the same lesion, i.e., that the lesion appears in
scans s i , s k in the same location. edges of consecutive scans s i , s i+1 are
called consecutive edges; edges of non-consecutive scans, s i , s k , i < k -1,
are called non-consecutive edges. the in-and out-degree of a vertex v i j , d in
(v i j ) and d out (v i j ), are the number of incoming and outcoming edges,
respectively.let cc = {cc m } m m=1 be the set of connected components of the
undirected graph version of g, where m is the number of connected components
andby definition, for each 1 ≤ m ≤ m , the sets v m , e m are mutually disjoint
and their unions are v , e, respectively. in a connected component cc m , there
is an undirected path between any two vertices v i j , v k l consisting of a
sequence of undirected edges in e m . . in this setup, connected components
correspond to matched lesions and their pattern of evolution over time (fig.
1d).we define seven mutually exclusive individual lesion change labels for
lesion v i j in scan s i based on the vertex in-and out-degrees (fig. 2). in the
following definitions we refer to the indices: 1 ≤ k < i < l ≤ n ; 1) lone: a
lesion present in scan s i and absent in all previous scans s k and subsequent
scans s l ; 2) new: a lesion present in scan s i and absent in all previous
scans s k ; 3) disappeared: a lesion present in scan s i and absent in all
subsequent scans s l ; 4) unique: a lesion present in scan s i and present as a
single lesion in a previous scan s k and/or in a subsequent scan s l ; 5)
merged: a lesion present in scan s i and present as two or more lesions in a
previous scan s k ; 6) split: a lesion present in scan s i and present as two or
more lesions in a subsequent scan s l ; 7) complex: a lesion present as two or
more lesions in at least one previous scan s k and at least one subsequent scan
s l . we also define as existing a lesion present in scan s i and present in at
least one previous scan s k and one subsequent scan s l , (d in (v i j ) ≥ 1, d
out (v i j ) ≥ 1). for the first and current scans s 1 and s n , we set d in (v
1 j ) = 1, d out (v n j ) = 1, i.e., the lesion existed before the first scan or
remains after the last scan. thus, lesions in the first (last) scan can only be
unique, disappeared or split (unique, new or merged). finally, when lesion v i j
is merged and d out (v i j ) = 0, i < n , it is also labeled disappeared; when
it is split and d in (v i j ) = 0, i > 1, it is also labeled new. we define five
patterns of lesion changes based on the properties of the connected components
cc m of g and on the labels of lesion changes: 1) single_p: a connected
component cc m = v i j consisting of a single lesion labeled as lone, new,
disappeared; 2) linear_p: a connected component consisting of a single earliest
vertex v the changes in individual lesions and the detection and classification
of patterns of lesion changes consist of constructing a graph whose vertices are
the corresponding lesion in the scans, computing the graph consecutive and
non-consecutive edges that correspond to lesion matchings, computing the
connected components of the resulting graph, and assigning an individual lesion
change label to each vertex and a lesion change pattern label to each connected
component according to the categories above.",5
83,1119,Graph-Theoretic Automatic Lesion Tracking and Detection of Patterns of Lesion Changes in Longitudinal CT Studies,3.0,Experimental Results,"we evaluated our method with two studies on retrospectively collected patient
datasets that were manually annotated by an expert radiologist.dataset: lung and
liver ct studies were retrospectively obtained from two medical centers
(hadassah univ hosp jerusalem israel) during the routine clinical examination of
patients with metastatic disease. each patient study consists of at least 3
scans.dlung consists of 83 chest ct scans from 19 patients with a mean 4.4 ± 2.0
scans/patient, a mean time interval between consecutive scans of 125.9 ± 81.3
days, and voxel sizes of 0.6-1.0 × 0.6-1.0 × 1.0-3.0 mm 3 . dliver consists of
77 abdominal cect scans from 18 patients with a mean 4.3 ± 2.0 scans/patient, a
mean time interval between consecutive scans of 109.7 ± 93.5 days, and voxel
sizes of 0.6-1.0 × 0.6-1.0 × 0.8-5.0 mm 3 .lesions in both datasets were
annotated by an expert radiologist, yielding a total of 1,178 lung and 800 liver
lesions, with a mean of 14.2 ± 19.1 and 10.4 ± 7.9 lesions/scan (lesions with
<20 voxels were excluded). ground-truth lesion matching graphs and lesion
changes labeling were produced by running the method on the datasets and then
having the radiologist review and correct the resulting node labels and
edges.study 1: lesion changes labeling, lesion matching, evaluation of patterns
of lesion changes. we ran our method on the dlungs and dliver lesion
segmentations. the settings of the parameters were: dilation distance d = 1 mm,
overlap percentage p = 10%, number of iterations r = 5 and 7, and centroid
maximum distance δ = 17 and 23 mm for the lungs and liver lesions,
respectively.we compared the computed and ground-truth lesion changes graphs
with two metrics: 1) lesion changes classification accuracy, which is the % of
correct computed labels from the ground truth labels; 2) lesion matching
precision and recall based on the presence/absence of computed vs. ground truth
edges. the precision and recall definitions were adapted so that wrong or missed
non-consecutive edges are counted as true positive when there is a path between
their vertices in either the ground-truth or the computed graph. table 1
summarizes the results. the distribution of lesion changes labels for dlungs
(1,178 lesions) is unique 785 (67%), new 215 (18%), lone 109 (9%), disappeared
51 (4%), merged 12 (1%), split 6 (1%), complex 0 (0%) with class accuracy ≥ 96%
for all except split (66%). for dliver (800 lesions) it is unique 450 (56%), new
185 (23%), lone 45 (6%), disappeared 77 (10%), merged 27 (3%), split 18 (2%),
complex 1 (0.05%) with class accuracy ≥ 81% for all except disappeared (71%) and
split (67%).for the patterns of lesion changes, we compared the computed and
ground truth patterns of lesion changes. the accuracy is the % of identical
connected components in each category. table 1 summarizes the results. note that
the split_p, merged_p and complex_p patterns jointly account for 3% and 8% of
the cases. these patterns are hard to detect manually but their correct
classification and tracking are crucial for the proper application of the recist
1.1 follow-up protocol [1]. study 2: detection of missed lesions in the ground
truth. the expert radiologist was asked to examine non-consecutive edges and
lesions labeled as lone in the lesion changes graph and determine if lesions
were unseen or undetected (actual or presumed false negative) in the skipped or
contiguous scans (fig. 1d). for each non-consecutive edge connecting lesions v i
j , v k l , he analyzed the corresponding region in the skipped scans s j at t j
∈ ]t i , t k [ for possible missed lesions. for the dlungs dataset, 25 visible
and 5 faintly visible or surmised to be present unmarked lesions were found for
27 nonconsecutive edges. for the dliver dataset, 20 visible and 21 faintly
visible or surmised to be present unmarked lesions were found for 25
non-consecutive edges.after reviewing the 42 and 37 lesions labeled as lone in
dlungs and dliver with > 5mm diameter, the radiologist determined that 1 and 8
of them had been wrongly identified as a cancerous lesion. moreover, he found
that 14 and 16 lesions initially labeled as lone, had been wrongly classified:
for these lesions he found 15 and 21 previously unmarked matching lesions in the
next or previous scans. in total, 45 and 62 missing lesions were added to the
ground truth dlungs and dliver datasets, respectively. these hard-to-find
ground-truth false negatives (3.7%, 7.2% of all lesions) may change the
radiological interpretation and the disease status. see the supplemental
material for examples of these scenarios.",5
84,1120,Graph-Theoretic Automatic Lesion Tracking and Detection of Patterns of Lesion Changes in Longitudinal CT Studies,4.0,Conclusion,"the use of graph-based methods for lesion tracking and detection of patterns of
lesion changes was shown to achieve high accuracy in classifying changes in
individual lesion and identifying patterns of lesion changes in liver and lung
longitudinal ct studies of patients with metastatic disease. this approach has
proven to be useful in detecting missed, faint, and surmised to be present
lesions, otherwise hardly detectable by examining the scans separately or in
pairs, leveraging the added information provided by evaluating all patient's
scans simultaneously using the labels from the lesion changes graph and
non-consecutive edges.",5
85,1122,Robust Exclusive Adaptive Sparse Feature Selection for Biomarker Discovery and Early Diagnosis of Neuropsychiatric Systemic Lupus Erythematosus,1.0,Introduction,"neuropsychiatric systemic lupus erythematosus (npsle) refers to a complex
autoimmune disease that damages the brain nervous system of patients. the
clinical symptoms of npsle include cognitive disorder, epilepsy, mental illness,
etc., and patients with npsle have a nine-fold increased mortality compared to
the general population [11]. since the pathogenesis and mature treatment of
npsle have not yet been found, it is extremely important to detect npsle at its
early stage and put better clinical interventions and treatments to prevent its
progression. however, the high overlap of clinical symptoms with other
psychiatric disorders and the absence of early non-invasive biomarkers make
accurate diagnosis difficult and time-consuming [3].although conventional
magnetic resonance imaging (mri) tools are widely used to detect brain injuries
and neuronal lesions, around 50% of patients with npsle present no brain
abnormalities in structural mri [17]. in fact, metabolic changes in many brain
diseases precede pathomorphological changes, which indicates proton magnetic
resonance spectroscopy ( 1 h-mrs) to be a more effective way to reflect the
early appearance of npsle. 1 h-mrs is a non-invasive neuroimaging technology
that can quantitatively analyze the concentration of metabolites and detect
abnormal metabolism of the nervous system to reveal brain lesions. however, the
complex noise caused by overlapping metabolite peaks, incomplete information on
background components, and low signal-tonoise ratio (snr) disturb the analysis
results of this spectroscopic method [15]. meanwhile, the individual differences
in metabolism and the interaction between metabolites under low sample size make
it difficult for traditional learning methods to distinguish npsle. figure 1
shows spectra images of four participants including healthy controls (hc) and
patients with npsle. it can be seen that the visual differences between patients
with npsle and hcs in the spectra of the volumes are subtle. therefore, it is
crucial to develop effective learning algorithms to discover metabolic
biomarkers and accurately diagnose npsle. the machine learning application for
biomarker analysis and early diagnosis of npsle is at a nascent stage [4]. most
studies focus on the analysis of mr images using statistical or machine learning
algorithms, such as mann-whitney u test [8], support vector machine (svm)
[7,24], ensemble model [16,22], etc. generally, machine learning algorithms
based on the minimum mean square error (mmse) criterion heavily rely on the
assumption that noise is of gaussian distribution. however, measurement-induced
non-gaussian noise in 1 h-mrs data undoubtedly limits the performance of
mmse-based machine learning methods.on the other hand, for the discovery task of
potential biomarkers, sparse codingbased methods (e.g., 2,1 norm, 2,0 norm,
etc.) force row elements to zero that remove some valuable features [12,21].
more importantly, different brain regions have different functions and
metabolite concentrations, which implies that the metabolic features for each
brain region have different sparsity levels. therefore, applying the same
sparsity constraint to the metabolic features of all brain regions may not
contribute to the improvement of the diagnostic performance of npsle.in light of
this, we propose a robust exclusive adaptive sparse feature selection (reasfs)
algorithm to jointly address the aforementioned problems in biomarker discovery
and early diagnosis of npsle. specifically, we first extend our feature learning
through generalized correntropic loss to handle data with complex non-gaussian
noise and outliers. we also present the mathematical analysis of the adaptive
weighting mechanism of generalized correntropy. then, we propose a novel
regularization called generalized correntropy-induced exclusive 2,1 to
adaptively accommodate various sparsity levels and preserve informative
features. the experimental results on a benchmark npsle dataset demonstrate the
proposed method outperforms comparing methods in terms of early noninvasive
biomarker discovery and early diagnosis.",5
86,1123,Robust Exclusive Adaptive Sparse Feature Selection for Biomarker Discovery and Early Diagnosis of Neuropsychiatric Systemic Lupus Erythematosus,2.0,Method,"dataset and preprocessing: the t2-weighted mr images of 39 participants
including 23 patients with npsle and 16 hcs were gathered from our affiliated
hospital. all images were acquired at an average age of 30.6 years on a signa
3.0t scanner with an eight-channel standard head coil. then, the mr images were
transformed into spectroscopy by multi-voxel 1 h-mrs based on a point-resolved
spectral sequence (press) with a two-dimensional multi-voxel technique. the
collected spectroscopy data were preprocessed by a sage software package to
correct the phase and frequency. an lcmodel software was used to fit the
spectra, correct the baseline, relaxation, and partial-volume effects, and
quantify the concentration of metabolites. finally, we used the absolute naa
concentration in single-voxel mrs as the standard to gain the absolute
concentration of metabolites, and the naa concentration of the corresponding
voxel of multi-voxel 1 h-mrs was collected consistently. the spectra would be
accepted if the snr is greater than or equal to 10 and the metabolite
concentration with standard deviations (sd) is less than or equal to 20%. the
absolute metabolic concentrations, the corresponding ratio, and the linear
combination of the spectra were extracted from different brain regions: rpcg,
lpcg, rdt, ldt, rln, lln, ri, rpwm, and lpwm. a total of 117 metabolic features
were extracted, and each brain region contained 13 metabolic features: cr,
phosphocreatine (pcr), cr+pcr, naa, naag, naa+naag, naa+naag/cr+pcr, mi,
mi/cr+pcr, cho+phosphocholine (pch), cho+pch/cr+pcr, glu+gln, and
glu+gln/cr+pcr.",5
87,1127,Robust Exclusive Adaptive Sparse Feature Selection for Biomarker Discovery and Early Diagnosis of Neuropsychiatric Systemic Lupus Erythematosus,3.0,Experimental Results and Conclusion,"experimental settings: the parameters α and λ 1 are are set to 1, while β and λ
2 are searched form {0.5, 1, 2, 5} and {0.1, 0.5, 1}, respectively. we use adam
optimizer and the learning rate is 0.001. to evaluate the performance of
classification, we employ a support vector machine as the basic classifier,
where the kernel is set as the radial basis function (rbf) and parameter c is
set to 1. we average the 3-fold cross-validation results.results and discussion:
we compare the classification accuracy of the proposed reasfs with several sota
baselines, including two filter methods: maximal information coefficient (mic)
[5], gini [23], and four sparse coding-based methods: multi-task feature
learning via 2,1 norm [6,12], discriminative feature selection via 2,0 norm
[21], feature selection via 1,2 norm [10] and exclusive 2,1 [9]. the proposed
reasfs is expected to have better robustness and flexibility. it can be seen
from fig. 2 that the sparse coding-based methods achieve better performance than
filter methods under most conditions, where ""0%"" represents no noise
contamination. the highest accuracy of our reasfs demonstrates the effectiveness
and flexibility of the proposed gcie 2,1 . generally speaking, the probability
of samples being contaminated by random noise is equal. therefore, we randomly
select features from the training set and replace the selected features with
pulse noise. the number of noisy attributes is denoted by the ratio between the
numbers of selected features and total features, such as 15% and 30%. the
classification performances of the npsle dataset contaminated by attribute noise
are shown in fig. 3(a) and fig. 3(b), where one clearly perceives that our
reasfs achieves the highest accuracy under all conditions. besides, it is
unreasonable to apply the same level of sparse regularization to noise features
and uncontaminated features, and our gcie 2,1 can adaptively increase the sparse
level of noise features to remove redundant information, and vice versa. for
label noise, we randomly select samples from the training set and replace
classification labels of the selected samples with opposite values, i.e., 0 → 1
and 1 → 0. the results are shown in fig. 3(c) and fig. 3(d), where the proposed
reasfs is superior to other baselines. it can be seen from fig. 3 that our
reasfs achieves the highest accuracy in different noisy environments, which
demonstrates the robustness of generalized correntropic loss. for non-invasive
biomarkers, our method shows that some metabolic features contribute greatly to
the early diagnosis of npsle, i.e., naag, mi/cr+pcr, and glu+gln/cr+pcr in rpcg;
cr+pcr, naa+naag, naa+naag/cr+pcr, mi/cr+pcr and glu+gln in lpcg; naa, naag, and
cho+pch in ldt; pcr, cr+pcr, cho+pch, cho+pch/cr+pcr and glu+gln/cr+pcr in rln;
mi/cr+pcr, cho+pch and cho+pch/cr+pcr in lln; naa+naag/cr+pcr and cho+pch in ri;
cho+pch/cr+pcr and glu+gln/cr+pcr in rpwm; and pcr, naag and naa+naag/cr+pcr in
lpwm. moreover, we use isometric feature mapping (isomap) [19] to analyze these
metabolic features and find that this feature subset is essentially a
low-dimensional manifold. meanwhile, by combining the proposed reasfs and
isomap, we can achieve 99% accuracy in the early diagnosis of npsle. in
metabolite analysis, some studies have shown that the decrease in naa
concentration is related to chronic inflammation, damage, and tumors in the
brain [18]. in the normal white matter area, different degrees of npsle disease
is accompanied by different degrees of naa decline, but structural mri is not
abnormal, suggesting that naa may indicate the progress of npsle. we also found
that glu+gln/cr+pcr in ri decreased, which indicates that the excitatory
neurotransmitter glu in the brain of patients with npsle may have lower
activity. to sum up, the proposed method provides a shortcut for revealing the
pathological mechanism of npsle and early detection.",5
88,1135,CARL: Cross-Aligned Representation Learning for Multi-view Lung Cancer Histology Classification,3.1,Dataset,"our dataset nsclc-tcia for lung cancer histological subtype classification is
sourced from two online resources of the cancer imaging archive (tcia) [5]:
nsclc radiomics [1] and nsclc radiogenomics [2]. exclusion criteria involves
patients diagnosed with large cell carcinoma or not otherwise specified, along
with cases that have contouring inaccuracies or lacked tumor delineation [9,13].
finally, a total of 325 available cases (146 adc cases and 179 scc cases) are
used for our study. we evaluate the performance of nsclc classification in
five-fold cross validation on the nsclc-tcia dataset, and measure accuracy
(acc), sensitivity (sen), specificity (spe), and the area under the receiver
operating characteristic (roc) curve (auc) as evaluation metrics. we also
conduct analysis including standard deviations and 95% ci, and delong
statistical test for further auc comparison.for preprocessing, given that the ct
data from nsclc-tcia has an in-plane resolution of 1 mm × 1 mm and a slice
thickness of 0.7-3.0 mm, we resample the ct images using trilinear interpolation
to a common resolution of 1mm × 1mm × 1mm. then one 128 × 128 pixel slice is
cropped from each view as input based on the center of the tumor. finally
following [7], we clip the intensities of the input patches to the interval
(-1000, 400 hounsfield unit) and normalize them to the range of [0, 1].",5
89,1138,CARL: Cross-Aligned Representation Learning for Multi-view Lung Cancer Histology Classification,4.0,Conclusion,"in summary, we propose a novel multi-view method called cross-aligned
representation learning (carl) for accurately distinguishing between adc and scc
using multi-view ct images of nsclc patients. it is designed with a cross-view
representation alignment learning network which effectively generates
discriminative view-invariant representations in the common subspace to reduce
the discrepancies among multi-view images. in addition, we leverage a
view-specific representation learning network to acquire viewspecific
representations as a necessary complement. the generated view-invariant and
-specific representations together offer a holistic and disentangled perspective
of the multi-view ct images for histological subtype classification of nsclc.
the experimental result on nsclc-tcia demonstrates that carl reaches 0.817 auc,
76.8% acc, 73.2% sen, and 79.7% spe and surpasses other relative approaches,
confirming the effectiveness of the proposed carl method.",5
90,1139,Self- and Semi-supervised Learning for Gastroscopic Lesion Detection,1.0,Introduction,"gastroscopic lesion detection (gld) plays a key role in computer-assisted
diagnostic procedures. although deep neural network-based object detectors
achieve tremendous success within the domain of natural images, directly
training generic object detectors on gld datasets performs below expectations
for two reasons: 1) the scale of labeled data in gld datasets is limited in
comparison to natural images due to the annotation costs. though gastroscopic
images are abundant, those containing lesions are rare, which necessitates
extensive image review for lesion annotation. 2) the characteristic of
gastroscopic images exhibits distinct differences from the natural images
[18,19,21] and is often of high similarity in global but high diversity in
local. specifically, each type of lesion may have diverse appearances though
gastroscopic images look quite similar. some appearances of lesions are quite
rare and can only be observed in a few patients. generic self-supervised
backbone pre-training or semi-supervised detector training methods can solve the
first challenge for natural images but its effectiveness is undermined for
gastroscopic images due to the second challenge.self-supervised backbone
pre-training methods enhance object detection performance by learning
high-quality feature representations from massive unlabelled data for the
backbone. the mainstream self-supervised backbone pretraining methods adopt
self-supervised contrast learning [3,4,7,9,10] or masked fig. 1. pipeline of
self-and semi-supervised learning (ssl) for gld. ssl consists of a hybrid
self-supervised learning (hsl) method and a prototype-based pseudo-label
generation (ppg) method. hsl combines patch reconstruction with dense
contrastive learning. ppg generates pseudo-labels for potential lesions based on
the similarity to the prototype feature vectors.image modeling [8,15].
self-supervised contrastive learning methods [3,4,7,9] can learn discriminative
global feature representations, and [10] can further learn discriminative local
feature representations by extending contrastive learning to dense paradigm.
however, these methods usually cannot grasp enough local detailed information.
on the other hand, masked image modeling is expert in extracting local detailed
information but is weak in preserving the discriminability of feature
representation. therefore, both types of methods have their own weakness for gld
tasks.semi-supervised object detection methods [12,14,16,17,20,22,23] first use
detectors trained with labeled data to generate pseudo-labels for unlabeled data
and then enhance object detection performance by regarding these unlabeled data
with pseudo-labels as labeled data to train the detector. current pseudolabel
generation methods rely on the objectiveness score threshold to generate
pseudo-labels, which makes them perform below expectations on gld, because the
characteristic of gastroscopic lesions makes it difficult to set a suitable
threshold to discover potential lesions meanwhile avoiding introducing much
noise.the motivation of this paper is to explore how to enhance gld performance
using massive unlabeled gastroscopic images to overcome the labeled data
shortage problem. the main challenge for this goal is the characteristic of
gastroscopic lesions. intuitively, such a challenge requires local feature
representations to contain enough detailed information, meanwhile preserving
discriminability. enlightened by this, we propose the self-and semi-supervised
learning (ssl) framework tailored to address challenges in daily clinical
practice and use massive unlabeled data to enhance gld performance. ssl
overcomes the challenges of gld by leveraging a large volume of unlabeled
gastroscopic images using self-supervised learning for improved feature
representations and semi-supervised learning to discover and utilize potential
lesions to enhance performance. specifically, it consists of a hybrid
self-supervised learning (hsl) method for self-supervised backbone pre-training
and a prototype-based pseudo-label generation (ppg) method for semi-supervised
detector training. the hsl combines the dense contrastive learning [10] with the
patch reconstruction to inherit the advantages of discriminative feature
learning and grasp the detailed information that is important for gld tasks. the
ppg generates pseudo-labels based on the similarity to the prototype feature
vectors (formulated from the feature vectors in its memory module) to discover
potential lesions from unlabeled data, and avoid introducing much noise at the
same time. moreover, we propose the first large-scale gld datasets (lgldd),
which contains 10,083 gastroscopic images with 12,292 well-annotated lesion
bounding boxes of four categories of lesions (polyp, ulcer, cancer, and
sub-mucosal tumor). we evaluate ssl with multiple detectors on lgldd and ssl
brings significant improvement compared with baseline methods (centernet [6]:
+2.7ap, faster rcnn [13]: +2.0ap). in summary, our contributions include:-a
self-and semi-supervise learning (ssl) framework to leverage massive unlabeled
data to enhance gld performance. -a large-scale gastroscopic lesion detection
datasets (lgldd) -experiments on lgldd demonstrate that ssl can bring
significant enhancement compared with baseline methods.",5
91,1143,Self- and Semi-supervised Learning for Gastroscopic Lesion Detection,3.0,Datasets,"we contribute the first large-scale gstroscopic lesion detection datasets
(lgldd) in the literature. collection : lgmdd collects about 1m+ gastroscopic
images from 2 hospitals of about 500 patients and their diagnosis reports. after
consulting some senior doctors and surveying gastroscopic diagnosis papers [1],
we select to annotate 4-category lesions: polyp(pol), ulcer(ulc), cancer(can)
and sub-mucosal tumor(smt). we invite 10 senior doctors to annotate them from
the unlabeled endoscopic images. to preserve the annotation quality, doctors can
refer to the diagnosis reports, and each lesion is annotated by a doctor and
checked by another. finally, they annotates 12,292 lesion boxes in 10,083 images
after going through about 120,000 images. the polyp, ulcer, cancer, and
sub-mucosal tumor numbers are 7,779, 2,171, 1,164and 1,178, respectively. the
train/val split of lgmdd is 8,076/2,007. the other data serves as unlabeled
data.evaluation metrics : we use standard object detection metrics to evaluate
the gld performance, which computes the average precision (ap) under multiple
intersection-of-union (iou) thresholds and then evaluate the performance using
the mean of aps (map) and the ap of some specific iou threshold. for map, we
follow the popular object detection datasets coco [11] and calculate the mean of
11 aps of iou from 0.5 to 0.95 with stepsize 0.05 (map @[.5:.05:.95]).we also
report ap under some specific iou threshold (ap 50 for .5, ap 75 for .75) and ap
of different scale lesions (ap s , ap m , ap l ) like coco [11].",5
92,1152,"Joint Prediction of Response to Therapy, Molecular Traits, and Spatial Organisation in Colorectal Cancer Biopsies",1.0,Introduction,"in the uk, approximately 11,500 patients are diagnosed with rectal cancer each
year [19]. a common form of treatment for such patients is neoadjuvant therapy,
including chemotherapy and radiotherapy, which can be given to patients with
locally advanced rectal cancer to shrink the tumour prior to surgery. recent
evidence suggests that 10-20% of patients will have a complete pathological
response to neoadjuvant therapy and can therefore avoid surgery altogether
[2,5]. however, one third of patients do not benefit from radiotherapy treatment
prior to surgery [8], hence it is important to determine how a patient will
respond to radiotherapy with a personalized approach in order to avoid
overtreatment.histology-based digital biomarkers enable the possibility to
predict a patient's response to therapy. the consensus molecular subtypes (cms)
classification system derived from gene expressions [9] has been developed to
provide biological insight into metastatic colorectal cancer. it has been shown
that these four cms classes can be predicted directly from the standard
haematoxylin and eosin (h&e) stained slide images using deep learning [18].
various studies have investigated the link between cms and patient outcomes,
suggesting that patients with tumour classified as cms4, which features stromal
invasion [9] and shows significantly higher stroma content [15], have worse
survival rates compared to the other cms classes [5]. increased stromal content
has independently been shown to be a predictor for increased risk of recurrence
in early rectal cancer [10], and tumour immune infiltrate evaluated with
immunoscore is a useful prognostic marker [3]. the spatial organisation of the
cancerous tissue has been identified as a biomarker for aggressiveness or
recurrence [12], and qi et al. [15] found that the features they developed
representing spatial organisation reflected characteristics of the four cms
classes. interactions between the epithelial tissue (cellular tissue lining) and
other prevalent tissue types in the tumour microenvironment are also indicators
of prognosis [15], since progression of colorectal cancer is dependent on both
the epithelial and stromal tissues [20]. other work has looked at predicting
chemoradiotherapy response in rectal cancer patients from h&e images using
different approaches, but without providing contextual interpretations
[19,22].as opposed to predicting response to radiotherapy alone, we aim to
analyse this prediction in the context of the overall tissue architecture and
the tumour biology as captured by cms. input to our model is a standard h&e
whole slide image (wsi) which is split into smaller patches to overcome the
memory limitations of existing gpus. to achieve our goal we need to capture the
heterogeneity at the slide level, which is why applying full or semi-supervised
approaches on individual tiles followed by a slide aggregation method is not
suitable. instead, we build on recent graph neural network (gnn) approaches that
allow us to model the entire wsi as a graph. as local cell communities form the
nodes of such a graph it can effectively model the micro-anatomy of the tissue.
at the same time it is possible to make predictions at the node-, graph-, and
slide-level. related work. to predict the grading of colorectal cancer (crc),
both cellbased and patch-based graphs have been used in separate works [16,23],
setting the nodes of the graph as either cell nuclei or square patches, defining
the node features as either handcrafted or learned features, and then applying a
gnn for outcome prediction. another patch-based gnn approach to predicting
genetic mutations in crc from h&e slides found their model trained on colon
cancer generalised well to rectal cancer. for other cancers, the slidegraph
pipeline clusters nuclei for the graph nodes, and provides node-level
predictions to make their model more interpretable [13]. other approaches to
setting the graph nodes include using subgraphs to represent regions [14], and
creating superpatches by combining patches [11]. edges between the nodes are
usually defined by a spatial distance metric, which helps model the spatial
organisation of the tissue. common choices for gnns include a graph isomorphism
network (gin) with jumping connectivity [7,13,14], as we use in this
research.our methodology proposes a novel and disease relevant approach to a
more interpretable model that effectively supports a diagnostic task.
pathologists and oncologists can use this information to inspect the validity of
the prediction result and interrogate key aspects of the spatial biology that is
critical for patient management. ultimately, this type of information that is
not available today will help to characterise interactions between the tumour
and the host tissue and therefore help to support choice of therapy. the
developed framework combines self-supervised training of a vision transformer
(vit) to extract morphological features, a superpixel algorithm for determining
nodes of a graph, and a gnn for predictions. we achieve 0.82 auc predicting
complete response to radiotherapy using deep learning on wsis for crc patients,
whilst providing novel interpretability of the results.",5
93,1153,"Joint Prediction of Response to Therapy, Molecular Traits, and Spatial Organisation in Colorectal Cancer Biopsies",2.0,Methods,"in this section we present the patch-level feature extraction, provide the
detail of the superpixel segmentation of the wsi, and illustrate the resulting
graph presentation. a gnn with three branches for our output predictions is used
to simultaneously make the three different predictions as shown in fig.
1.pipeline. for computational reasons, all images are split into patches of size
256 × 256 pixels. in order to have a common feature set all the way up to the
last layer of the gnn, individual patches should be represented by morphological
features that are label-agnostic. this last layer of the gnn then splits into
three branches to predict response to radiotherapy, the cms4 subtype
classification for crc, and epithelial tissue regions. this way we can guarantee
the common latent features and derivation across branches, maintaining the
contextual importance of each branch. the dino framework [4] uses a
self-distillation training approach, using data augmentation to locally crop the
patches and train with a local-global student-teacher approach. we use the dino
framework to train a vit in a self-supervised manner on our h&e slides [6],
representing each patch with 384 features. we use only the training set to train
this model, and use the image patches at 20x magnification. we extract
patch-level features from each wsi using selfsupervised dino training with a vit
model [4]. the slic superpixel algorithm segments the entire slide into smaller
regions [1]. we calculate the mean patch features for these superpixel regions,
and use the superpixel features and centers as our graph nodes, applying
delaunay triangulation to generate the edges of the graph. a gnn consisting of
ginconv layers is trained on these fixed graphs, and the final layer splits into
three separate mlp branches to provide predictions of three different outcomes,
complete response (cr) to radiotherapy (rt), cms4 classification, and epithelial
tissue. an example output is visualized in fig. 2.to find the nodes of the wsi
graphs, we apply the slic superpixel algorithm [1] on the wsis at 5x
magnification to segment the tissue to capture cellular neighbourhoods that are
roughly between 80-100 µm 2 /pixels in size. it can be seen that the superpixel
boundaries consistently align with the boundaries of tissue compartments.the
superpixels centers are used as the nodes of the graph, and the node features
are the weighted mean of the corresponding patch features which overlap with the
superpixel region. the edges of the graph are determined by nearest neighbours
from delaunay triangulation, as in slidegraph [13].building on the ideas
introduced by slidegraph [13] we use ginconv layers [21], adding tempering to
avoid overfitting, and replace their logistic regression scaler with a simple
sigmoid function. we add three branches to the final layer of the gnn, in the
form of three separate multilayer perceptrons (mlps). two of these mlps return a
graph-level prediction, for the response to rt and cms4 predictions, and the
final branch returns node-level predictions, predicting whether each node is
epithelial tissue or not. our loss function is defined aswhere bce is the binary
cross entropy loss, ŷrt ∈ r is the slide-level prediction of response to
radiotherapy, ŷcms4 ∈ r is the slide-level prediction of cms4, ŷepi ∈ r ni are
the node-level predictions of epithelial tissue and n i is the number of nodes
in the i th wsi graph.for each prediction branch, we can visualize the
individual node predictions from the wsi graph, overlaid on the wsi itself, to
get an idea of how the node predictions vary across the different tissue
regions. each graph-level prediction is derived from the corresponding branch
node predictions, by applying pooling and dropout.data. we train and validate
our methods on two retrospective rectal cancer datasets, grampian and aristotle.
both cohorts received standard chemoradiotherapy of pelvic irradiation
(45-50.4gy in 25 fractions over 5 weeks) with capecitabine 900mg/m 2 . the
pre-treatment biopsy slides were all sectioned and stained in the same
laboratory, and scanned at 20x magnification (0.5 µm 2 /pixel) on an aperio
scanner. pathological complete response, which we use as a target outcome here,
was derived from histopathological assessment from posttreatment resections.the
cms labels for this data are derived from three different transcriptomic
versions (single cohort, combined cohort correcting batch effects and combined
cohort including 2036 cases run with the same platform), in order to generate
robust classifications. in all cases the cms call was calculated using the
cmsclassifier random forest and single sample predictor [9]. final cms calls are
based on matching calls between the three transcriptomic versions. despite our
efforts to minimise the noise from rna sequencing, we still expect a certain
level of noise in our ground truth data, which we discuss in the results
section.the epithelial labels for each graph node are calculated from epithelial
masks for each wsi. these epithelial segmentation masks were generated at 10x
magnification (1 µm 2 /pixel) with a u-net [17] which was trained and validated
on 666 full tissue sections belonging to 362 patients from the focus cohort
[18]. the ground truth annotations for the training of this model were generated
by vk.for consistency the tumour regions were marked up by an expert
pathologist. we use these masks in our analysis to filter out background and
irrelevant tissue from the images. grampian and aristotle are used in both
training and validation, with a 70/30% training-validation split, keeping any
wsis from a single patient in the same dataset. we predict complete response to
radiotherapy against all other responses, such as partial response and no
response. the datasets are unbalanced, since in grampian only 61/244 slides have
complete response, and in aristotle only 24/121 slides have complete response.
they are even more unbalanced for cms4, since only 28/244 slides in grampian and
17/121 slides in aristotle are labelled with cms4. we address this imbalance in
the supplementary materials. there are 365 slides total in our dataset, from 249
patients.",5
94,1155,"Joint Prediction of Response to Therapy, Molecular Traits, and Spatial Organisation in Colorectal Cancer Biopsies",0.0,Response branch,"response results. despite the noise in our reference data used for training, our
model achieves good performance in terms of mean auc scores on all three
prediction branches of our model, predicting complete response to radiotherapy
(rt) with 0.819 auc, cms4 with 0.819 auc and epithelial tissue at the node level
with 0.760 auc across folds. further metrics are provided in table 1. the
prediction performance of the model could be improved by utilising a larger
training dataset and performing more exhaustive parameter searches, however the
current performance of the model is sufficient to demonstrate the impact of this
approach.the predicted response to radiotherapy can now be viewed in the context
of disease biology as captured by cms4. for example, the model demonstrates that
cms4 patients are less likely to respond to radiotherapy. in addition, it is now
possible to view the spatial distribution of cms4 active regions in the tissue
architecture context as shown in fig. 2. additional samples are presented in the
supplementary materials. an example of our proposed prediction maps on two
slides can be seen in fig. 2, with further slides in the supplementary
materials. a pathologist reviewing these maps assesses that the observed
patterns fit the known interplay of response to therapy, cms4 activation, and
the spatial localisation of these signals. in the top slide, we observe high
cms4 activation in stromal rich regions, and interestingly also high cms4
activation in the bottom center, dissociating from the response to rt activation
map. this could be explained by the lymphocyte content, supported by the higher
epithelial map activations in the same location. expert pathologists highlight a
similar pattern in certain regions of the maps for the bottom slide. different
from the slide above, the cms4 and response to rt maps have some overlap with
moderate activations here, encouraging discovery into tumour-host interactions.
ultimately, a pathologist confirmed that these maps support an interpretable and
trustworthy prediction in the context of response to radiotherapy. while we
cannot present a more extensive interpretation of these results due to space
limitations, these examples already indicate that the proposed approach enables
a level of analysis that has not been possible before. we find that predicting
these outcomes individually in a single branch model, particularly with response
to radiotherapy, can result in slightly higher auc scores, but we consciously
make this trade-off in order to provide better interpretability of the model
predictions. the focus of this research is not to achieve the best possible
metrics, but to develop robust methods which can add context and explanation to
clinical black box deep learning model predictions, with the view to ease
clinical translation of such models.to explore the effects of the noisy cms4
ground truth labels, we remove from our dataset any wsis classified as
'unmatched' for the cms call, which for the main results of this paper we
defined as 'not cms4'. removing this data and rerunning our analysis improved
our predictions for cms4 by +0.06 auc, and reduced our response to radiotherapy
and epithelial predictions by -0.02 and -0.01 respectively. the results can be
found in the supplementary materials. these small changes indicate that the
noise in our data does not degrade the performance of our classifier,
reinforcing it as a robust and accurate model.",5
95,1156,"Joint Prediction of Response to Therapy, Molecular Traits, and Spatial Organisation in Colorectal Cancer Biopsies",4.0,Conclusion,"by setting the prediction of response to therapy in context with disease biology
and spatial organisation of the tissue we are providing a novel approach for
enhancing the interpretablity of complex prediction tasks. these results do not
only enhance the interpretability, they also provide new ways to utilise large
retrospective clinical trial cohorts for which no additional molecular data is
available. extending the amount of training data and improving model training
will improve model performance, which is already impressive.we argue that this
work also advances the state of the art in feature representation and analysis.
our prediction maps derive from the same graph model, and hence they share
underlying graph features. the prediction branches only diverge at the final
stage of translating these graph features into outcome predictions for our three
clinically relevant outcomes. importantly, this level of visualisation is not
only accessible to pathologists, this joint prediction model also enhances the
communication between pathologists and oncologists which is critical for patient
management. by cross-referencing these prediction maps with our prior
understanding of cancer biology, this approach can help to establish trust in
the prediction model and also help to identify potential failure cases.this work
relies on access to well annotated clinical trial samples which will limit our
ability to include more data for training and testing. in future, we plan to use
these methods to help better characterise tumour-stromal interactions of the
tissue. we also plan to use a denser graph with less connectivity to be able to
better predict the heterogeneous epithelial tissue.the aristotle trial was
funded by cancer research uk (cruk/08/032). the funders played no role in the
analyses performed or the results presented. financial support: rw -epsrc center
for doctoral training in health data science (ep/s02428x/1), oxford cruk cancer
centre; vhk -promedica foundation (f-87701-41-01) and swiss national science
foundation (p2skp3_168322/1, p2skp3_168322/2); tsm -s:cort (see above); jr, ks
-oxford nihr national oxford biomedical research centre and the pathlake
consortium (innovateuk). the computational aspects of this research were funded
from the nihr oxford brc with additional support from the wellcome trust core
award grant number 203141/z/16/z. the views expressed are those of the author(s)
and not necessarily those of the nhs, the nihr or the department of health.",5
96,1158,Automatic Bleeding Risk Rating System of Gastric Varices,1.0,Introduction,"esophagogastric varices are one of the common manifestations in patients with
liver cirrhosis and portal hypertension and occur in about 50 percent of
patients with liver cirrhosis [3,6]. the occurrence of esophagogastric variceal
bleeding is the most serious adverse event in patients with cirrhosis, with a
6-week acute bleeding mortality rate as high as 15%-20% percent [14]. it is
crucial to identify high-risk patients and offer prophylactic treatment at the
appropriate time. regular endoscopy examinations have been proven an effective
clinical approach to promptly detect esophagogastric varices with a high risk of
bleeding [7]. different from the grading of esophageal varices (ev) that is
relatively complete [1], the bleeding risk grading of gastric varices (gv)
involves complex variables including the diameter, shapes, colors, and
locations. several rating systems have been proposed to describe gv based on the
anatomical area. sarin et al. [16] described and divided gv into 2 groups
according to their locations and extensions. hashizume et al. [10] published a
more detailed examination describing the form, location, and color. although the
existing rating systems tried to identify the risk from different perspectives,
they still lack clear quantification standard and heavily rely on the
endoscopists' subjective judgment. this may cause inconsistency or even
misdiagnosis due to the variant experience of endoscopists in different
hospitals. therefore, we aim to build an automatic gv bleeding risk rating
method that can learn a stable and robust standard from multiple experienced
endoscopists.recent works have proven the effectiveness and superiority of deep
learning (dl) technologies in handling esophagogastroduodenoscopy (egd) tasks,
such as the detection of gastric cancer and neoplasia [4]. it is even
demonstrated that ai can detect neoplasia in barrett's esophagus at a higher
accuracy than endoscopists [8]. intuitively we may regard the gv bleeding risk
rating as an image classification task and apply typical classification
architectures (e.g., resnet [12]) or state-of-the-art gastric lesion
classification methods to it. however, they may raise poor performance due to
the large intra-class variation between gv with the same bleeding risk and small
inter-class variation between gv and normal tissue or gv with different bleeding
risks. first, the gv area may look like regular stomach rugae as it is caused by
the blood vessels bulging and crumpling up the stomach (see fig. 1). also, since
the gv images are taken from different distances and angles, the number of
pixels of the gv area may not reflect its actual size. consequently, the model
may fail to focus on the important gv areas for prediction as shown in fig. 3.
to encourage the model to learn more robust representations, we constructively
introduce segmentation into the classification framework. with the segmentation
information, we further propose a region-constraint module (rcm) and a
cross-region attention module (cram) for better feature localization and
utilization. specifically, in rcm, we utilize the segmentation results to
constrain the cam heatmaps of the feature maps extracted by the classification
backbone, avoiding the model making predictions based on incorrect areas. in
cram, the varices features are extracted using the segmentation results and
combined with an attention mechanism to learn the intra-class correlation and
cross-region correlation between the target area and the context.to learn from
experienced endoscopists, gv datasets with bleeding risks annotation is needed.
while most works and public datasets focus on colonoscopy [13,15] and esophagus
[5,9], with a lack of study on gastroscopy images. in the public dataset of
endocv challenge [2], the majority are colonoscopies while only few are
gastroscopy images. in this work, we collect a gv bleeding risks rating dataset
(gvbleed) that contains 1678 gastroscopy images from 411 patients with different
levels of gv bleeding risks. three senior clinical endoscopists are invited to
grade the bleeding risk of the retrospective data in three levels and annotated
the corresponding segmentation masks of gv areas.in sum, the contributions of
this paper are: 1) a novel gv bleeding risk rating framework that constructively
introduces segmentation to enhance the robustness of representation learning; 2)
a region-constraint module for better feature localization and a cross-region
attention module to learn the correlation of target gv with its context; 3) a gv
bleeding risk rating dataset (gvbleed) with high-quality annotation from
multiple experienced endoscopists. baseline methods have been evaluated on the
newly collected gvbleed dataset. experimental results demonstrate the
effectiveness of our proposed framework and modules, where we improve the
accuracy by nearly 5% compared to the baseline model.",5
97,1164,Automatic Bleeding Risk Rating System of Gastric Varices,3.0,GVBleed Dataset,"data collection and annotation. the gvbleed dataset contains 1678 endoscopic
images with gastric varices from 527 cases. all of these cases are collected
from 411 patients in a grade-iii class-a hospital during the period from 2017 to
2022. in the current version, images from patients with ages elder than 18 are
retained 1 . the images are selected from the raw endoscopic videos and frames.
to maximize the variations, non-consecutive frames with larger angle differences
are selected. to ensure the quality of our dataset, senior endoscopists are
invited to remove duplicates, blurs, active bleeding, chromoendoscopy, and nbi
pictures.criterion of gv bleeding risk level rating. based on the clinical
experience in practice, the gv bleeding risks in our dataset are rated into
three levels, i.e., mild, moderate, and severe. the detailed rating standard is
as follows: 1) mild: low risk of bleeding, and regular follow-up is sufficient
(usually with a diameter less than or equal to 5 mm). 2) moderate: moderate risk
of bleeding, and endoscopic treatment is necessary, with relatively low
endoscopic treatment difficulty (usually with a diameter between 5 mm and 10
mm). 3) severe: high risk of bleeding and endoscopic treatment is necessary,
with high endoscopic treatment difficulty. the varices are thicker (usually with
a diameter greater than 10 mm) or less than 10mm but with positive red signs.
note that the diameter is only one reference for the final risk rating since the
gv is with 1 please refer to the supplementary material for more detailed
information about our dataset. various 3d shapes and locations. the other facts
are more subjectively evaluated based on the experience of endoscopists. to
ensure the accuracy of our annotation, three senior endoscopists with more than
10 years of clinical experience are invited to jointly label each sample in our
dataset. if three endoscopists have inconsistent ratings for a sample, the final
decision is judged by voting. a sample is selected and labeled with a specific
bleeding risk level only when two or more endoscopists reach a consensus on it.
the gvbleed dataset is partitioned into training and testing sets for
evaluation, where the training set contains 1337 images and the testing set has
341 images. the detailed statistics of the three levels of gv bleeding risk in
each set are shown in table 1. the dataset is planned to be released in the
future.",5
108,1186,Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,0.0,abbreviated as PARE.,"at the intra-level, the contextual information of the nodules provides clues
about their shape, size, and surroundings, and the integration of this
information can facilitate a more reliable diagnosis of whether they are benign
or malignant. motivated by this, we first segment the context structure, i.e.,
nodule and its surroundings, and then aggregate the context information to the
nodule representation via the attention-based dependency modeling, allowing for
a more comprehensive understanding of the nodule itself. at the inter-level, we
hypothesize that the diagnosis process does not have to rely solely on the
current nodule itself, but can also find clues from past learned cases. this is
similar to how radiologists rely on their accumulated experience in clinical
practice. thus, the model is expected to have the ability to store and recall
knowledge, i.e., the knowledge learned can be recorded in time and then recalled
as a reference for comparative analysis. to achieve this, we condense the
learned nodule knowledge in the form of prototypes, and recall them to explore
potential inter-level clues as an additional discriminant criterion for the new
case. to fulfill both ldct and ncct screening needs, we curate a large-scale
lung nodule dataset with pathology-or follow-up-confirmed benign/malignant
labels. for the ldct, we annotate more than 12,852 nodules from 8,271 patients
from the nlst dataset [14]. for the ncct, we annotate over 4,029 nodules from
over 2,565 patients from our collaborating hospital. experimental results on
several datasets demonstrate that our method achieves outstanding performance on
both ldct and ncct screening scenarios.our contributions are summarized as
follows: (1) we propose context parsing to extract and aggregate rich contextual
information for each nodule. (2) we condense the diagnostic knowledge from the
learned nodules into the prototypes and use them as a reference to assist in
diagnosing new nodules. (3) we curate the largest-scale lung nodule dataset with
high-quality benign/malignant labels to fulfill both ldct and ncct screening
needs. (4) our method achieves advanced malignancy prediction performance in
both screening scenarios (0.931 auc), and exhibits strong generalization in
external validation, setting a new state of the art on lungx (0.801 auc).",5
109,1193,Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,3.1,Datasets and Implementation Details,"data collection and curation: nlst is the first large-scale ldct dataset for
low-dose ct lung cancer screening purpose [14]. there are 8,271 patients
enrolled in this study. an experienced radiologist chose the last ct scan of
each for l = 1, ..., l do 12:cross prototype attention 13:end for 15:",5
110,1195,Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,0.0,17:,"j ← seg loss(m, s) + 3 i=1 cls loss(y, p i ) update loss18: end for patient, and
localized and labeled the nodules in the scan as benign or malignant based on
the rough candidate nodule location and whether the patient develops lung cancer
provided by nlst metadata. the nodules with a diameter smaller than 4mm were
excluded. the in-house cohort was retrospectively collected from 2,565 patients
at our collaborating hospital between 2019 and 2022. unlike nlst, this dataset
is noncontrast chest ct, which is used for routine clinical care. segmentation
annotation: we provide the segmentation mask for our in-house data, but not for
the nlst data considering its high cost of pixel-level labeling. the nodule mask
of each in-house data was manually annotated with the assistance of ct labeler
[20] by our radiologists, while other contextual masks such as lung, vessel, and
trachea were generated using the totalsegmentator [21].",5
111,1196,Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,0.0,Train-Val-Test:,"the training set contains 9,910 (9,413 benign and 497 malignant) nodules from
6,366 patients at nlst, and 2,592 (843 benign and 1,749 malignant) nodules from
2,113 patients at the in-house cohort. the validation set contains 1,499 (1,426
benign and 73 malignant) nodules from 964 patients at nlst. the nlst test set
has 1,443 (1,370 benign and 73 malignant) nodules from 941 patients. the
in-house test set has 1,437 (1,298 benign and 139 malignant) nodules from 452
patients. we additionally evaluate our method on the lungx [2] challenge
dataset, which is usually used for external validation in previous work
[6,11,24]. lungx contains 83 (42 benign and 41 malignant) nodules, part of which
(13 scans) were contrast-enhanced. segmentation: we also evaluate the
segmentation performance of our method on the public nodule segmentation dataset
lidc-idri [3], which has 2,630 nodules with nodule segmentation mask. evaluation
metrics: the area under the receiver operating characteristic curve (auc) is
used to evaluate the malignancy prediction performance.implementation: all
experiments in this work were implemented based on the nnunet framework [8],
with the input size of 32 × 48 × 48, batch size of 64, and total training
iterations of 10k. in the context patch embedding, each patch token is generated
from a window of 8 × 8 × 8. the hyper-parameters of pare are empirically set
based on the ablation experiments on the validation set. for example, the
transformer layer is set to 4 in both sca and cpa modules, and the number of
prototypes is fixed to 40 by default. more details can be found in the ablation.
due to the lack of manual annotation of nodule masks for the nlst dataset, we
can only optimize the segmentation task using our in-house dataset, which has
manual nodule masks.",5
114,1215,SHISRCNet: Super-Resolution and Classification Network for Low-Resolution Breast Cancer Histopathology Image,1.0,Introduction,"breast cancer is one of the high-mortality cancers among women in the 21st
century. every year, 1.2 million women around the world suffer from breast
cancer and about 0.5 million die of it [3]. accurate identification of cancer
types will make a correct assessment of the patient's risk and improve the
chances of survival. however, the traditional analysis method is time-consuming,
as it mainly depends on the experience and skills of the doctors. therefore, it
is essential to develop computer-aided diagnosis (cadx) for assisting doctors to
realize the rapid detection and classification.due to being collected by various
devices, the resolution of histopathological images extracted may not always be
high. low-resolution (lr) images lack of lots of details, which will have an
important impact on doctors' diagnosis. considering the improvement of
histopathological images' acquisition equipment will cost lots of money while
significantly increasing patients' expense of detection. the super-resolution
(sr) algorithms that improve the resolution of lr images at a small cost can be
a practical solution to assist doctors in diagnosis. at present, most single
super-resolution methods only have fixed receptive fields [7,10,11,18]. these
models cannot capture multi-scale features and do not solve the problems caused
by lr in various magnification factors well. mrc-net [6] adopted lstm [9] and
multi-scale refined context to improve the effect of reconstructing
histopathological images. it considered the problem of multi-scale, but only
fused two scales features. this limits its performance in the scenarios with
various magnification factors. therefore, designing an appropriate feature
extraction block for sr of the histopathological images is still a challenging
task.in recent years, a series of deep learning methods have been proposed to
solve the breast cancer histopathological image classification issue by the
highresolution (hr) histopathological images. [12,21,22] improved the specific
model structure to classify breast histopathology images, which showed a
significant improvement in recognition accuracy compared with the previous works
[1,20]. ssca [24] considered the problem of multi-scale feature extraction which
utilized feature pyramid network (fpn) [15] and attention mechanism to extract
discriminative features from complex backgrounds. however, it only concatenates
multi-scale features and does not consider the problem of feature fusion. so it
is still worth to explore the potential of extraction and fusion of multi-scale
features for breast images classification.to tackle the problem of lr breast
cancer histopathological images reconstruction and diagnosis, we propose the
single histopathological image super-resolution classification network
(shisrcnet) integrating super-resolution (sr) and classification (cf) modules.
the main contributions of this paper can be described as follows:(1) in the sr
module, we design a new block called multi-features extraction block (mfeblock)
as the backbone. mfeblock adopts multi-scale receptive fields to obtain
multi-scale features. in order to better fuse multi-scale features, a new fusion
method named multi-scale selective fusion (msf) is used for multi-scale
features. these make mfeblock reconstruct lr images into sr images well.(2) the
cf module completes the task of image classification by utilizing the sr images.
like sr module, it also needs to extract multi-scale features. the difference is
that the cf module can use the method of downsampling to capture multi-scale
features. so we combine the multi-scale receptive fields (sknet) [13] with the
feature pyramid network (fpn) to achieve the feature extraction of this module.
in fpn, we design a cross-scale selective fusion block (csfblock) to fuse
features of different scales.(3) through the joint training of these two
designed modules, the superresolution and classification of low-resolution
histopathological images are integrated into our model. for improving the
performance of cf module and reducing the error caused by the reconstructed sr
images, we introduce hr images to cf module in the training stage. the
experimental results demonstrate that the effects of our method are close to
those of sota methods that take hr breast cancer histopathological images as
inputs.",5
115,1228,Text-Guided Foundation Model Adaptation for Pathological Image Classification,4.0,Experimental Settings,"dataset. we adopt the patchgastric [25] dataset, which includes
histopathological image patches extracted from h&e stained whole slide images
(wsi) of stomach adenocarcinoma endoscopic biopsy specimens. there are 262,777
patches of size 300 × 300 extracted from 991 wsis at x20 magnification. the
dataset contains 9 subtypes of gastric adenocarcinoma. we choose 3 major
subtypes including ""well differentiated tubular adenocarcinoma"", ""moderately
differentiated tubular adenocarcinoma"", and ""poorly differentiated
adenocarcinoma"" to form a 3-class grading-like classification task with 179,285
patches from 693 wsis. we randomly split the wsis into train (20%) and
validation (80%) subsets for measuring the model performance. to extend our
evaluation into the real-world setting with insufficient data, we additionally
choose 1, 2, 4, 8, or 16 wsis with the largest numbers of patches from each
class as the training set.the evaluation metric is patient-wise accuracy, where
the prediction of a wsi is obtained by a soft vote over the patches, and
accuracy is averaged class-wise.implementation. we use clip vit-b/16 [5] as the
visual backbone, with input image size 224 × 224, patch size 16 × 16, and
embedding dimension d v = 512. we adopt biolinkbert-large [11] as the biomedical
language model, with embedding dimension d l = 1, 024. to show the extensibility
of our approach, we additionally test on vision encoders including imagenet-21k
vit-b/16 [24,26] and intern vit-b/16 [6], and biomedical language model
biobert-large [10].our implementation is based on clip 1 , huggingface2 and
mmclassification3 .training details. prompt length p is set to 1. we resize the
images to 224×224 to fit the model and follow the original data pipeline in
patchgastric [25]. a class-balanced sampling strategy is adopted by choosing one
image from each class in turn. training is done with 1,000 iterations of
stochastic gradient descent (sgd), and the mini-batch size is 128, requiring
11.6 gb of gpu memory and 11 min on two nvidia geforce rtx 2080 ti gpus. all our
experiment results are averaged on 3 random seeds unless otherwise specified.",5
118,1271,MUVF-YOLOX: A Multi-modal Ultrasound Video Fusion Network for Renal Tumor Diagnosis,1.0,Introduction,"renal cancer is the most lethal malignant tumor of the urinary system, and the
incidence is steadily rising [13]. conventional b-mode ultrasound (us) is a good
screening tool but can be limited in its ability to characterize complicated
renal lesions. contrast-enhanced ultrasound (ceus) can provide information on
microcirculatory perfusion. compared with ct and mri, ceus is radiation-free,
cost-effective, and safe in patients with renal dysfunction. due to these
benefits, ceus is becoming increasingly popular in diagnosing renal lesions.
however, recognizing important diagnostic features from ceus videos to diagnose
lesions as benign or malignant is non-trivial and requires lots of experience.to
improve diagnostic efficiency and accuracy, many computational methods were
proposed to analyze renal us images and could assist radiologists in making
clinical decisions [6]. however, most of these methods only focused on
conventional b-mode images. in recent years, there has been increasing interest
in multi-modal medical image fusion [1]. directly concatenation and addition
were the most common methods, such as [3,4,12]. these simple operations might
not highlight essential information from different modalities. weight-based
fusion methods generally used an importance prediction module to learn the
weight of each modality and then performed sum, replacement, or exchange based
on the weights [7,16,17,19]. although effective, these methods did not allow
direct interaction between multi-modal information. to address this,
attention-based methods were proposed. they utilized cross-attention to
establish the feature correlation of different modalities and self-attention to
focus on global feature modeling [9,18]. nevertheless, we prove in our
experiments that these attentionbased methods may have the potential risks of
entangling features of different modalities.in practice, experienced
radiologists usually utilize dynamic information on tumors' blood supply in ceus
videos to make diagnoses [8]. previous researches have proved that temporal
information is effective in improving the performance of deep learning models.
lin et al. [11] proposed a network for breast lesion detection in us videos by
aggregating temporal features, which outperformed other image-based methods.
chen et al. [2] showed that ceus videos can provide more detailed blood supply
information of tumors allowing a more accurate breast lesion diagnosis than
static us images.in this work, we propose a novel multi-modal us video fusion
network (muvf-yolox) based on ceus videos for renal tumor diagnosis. our main
contributions are fourfold. (1) to the best of our knowledge, this is the first
deep learning-based multi-modal framework that integrates both b-mode and
ceusmode information for renal tumor diagnosis using us videos. (2) we propose
an attention-based multi-modal fusion (amf) module consisting of cross-attention
and self-attention blocks to capture modality-invariant and modality-specific
features in parallel. (3) we design an object-level temporal aggregation (ota)
module to make video-based diagnostic decisions based on the information from
multi-frames. (4) we build the first multi-modal us video datatset containing
b-mode and ceus-mode videos for renal tumor diagnosis. experimental results show
that the proposed framework outperforms single-modal, single-frame, and other
state-of-the-art methods in renal tumor diagnosis.",5
119,1279,Patients and Slides are Equal: A Multi-level Multi-instance Learning Framework for Pathological Image Analysis,1.0,Introduction,"pathological image analysis is a vital area of research within medical image
analysis, focused on utilizing computer technology to aid doctors in diagnosing
and treating diseases by analyzing pathological tissue slide images [5].
advancements in pathological image analysis have been made in early cancer
diagnosis, tumor localization, and grading, and treatment planning [3,10].
multi-instance learning [2] is the primary analysis method used, which involves
analyzing tasks based on slide labels and patches. despite this, the clinical
pathological analysis presents certain challenges and complexities, with the
ultimate diagnosis relying on patients rather than slides.specifically, in
clinical problems of pathological image analysis, doctors usually summarize
patient-level labels based on slide labels as the diagnostic results [1,6]. for
example, for the pathological discrimination diagnosis task of intestinal
tuberculosis(itb) and crohn's desease(cd), the categories of postoperative
slides are divided into three types (normal, cd, itb), and doctors will
summarize the binary results of patients (itb or cd) based on slide-level labels
[6]. similar situations exist in other tasks, such as the classification of
breast cancer metastases in lymph nodes, where slide categories may have
different classifications, and the corresponding diagnosis of the same patient
is whether the cancer has spread to the regional lymph nodes (n-stage) [1].
therefore, as shown in fig. 1, actual pathological image analysis involves the
relationships of patches, slides, and patients, which is called a multi-level
multi-instance learning (ml-mil) problem. among them, for patients and slides,
patients are bags while slides are instances, and for slides and patches, slides
are bags while patches are instances.there are generally two methods to solve
the ml-mil problem. the first method is to directly average the prediction
values of slides or take the maximum prediction value [9]. this method is
relatively simple, but the information exchange between slides is not fully
utilized, which may lead to errors in the summary result. the second method is
to treat slide-patient as a new mil problem according to the traditional mil
thinking, where slides are regarded as instances and patient labels as bags.
although this method seems reasonable, the number of patients is usually
relatively small, and deep learning models usually require a large amount of
data for training. therefore, the insufficient number of samples at the
slide-patient level may make it difficult for the model to learn enough
information.to address the multi-level multi-instance learning (ml-mil) problem
in medical field, we propose a novel framework called patients and slides are
equal (p&sre). inspired by the iterative labeling process in medical diagnosis,
this framework treats patients and slides as instances at the same level and
uses transformers and attention mechanisms to build connections between them.
this simple yet effective method allows for interaction between patient-level
and slidelevel information to correct their respective features and improve
classification performance. our framework consists of two steps: first, at the
patch-slide level, a common mil framework is used to train a mil neural network
and obtain slide-level feature vectors; then, at the slide-patient level, we use
self-attention mechanisms to combine the slides of the same patient into
patient-level feature vectors, and treat these patient-level feature vectors
together with all slide-level feature vectors of the same patient as instances
at the same level, which are inputted into transformers for feature interaction
and prediction of patient-and slide-level labels. our method can effectively
solve the problem of difficult training due to the scarcity of samples at the
highest level in ml-mil, and can be integrated into two state-of-the-art methods
to further improve performance. we conducted rigorous experiments on two
datasets and demonstrated the effectiveness of our method. our contributions
include:1) proposing a novel general framework to address the unique
""patch-slidepatient"" ml-mil problem in the medical field. before this, no other
framework had directly tackled this specific problem, making our proposal a
ground-breaking step in the application of ml-mil in healthcare; 2) proposing a
simple yet highly effective method that leverages self-attention mechanisms and
transformer models to enhance the interaction between slide and patient
information. this innovative approach not only improves the classification
performance at the patient level but also at the slide level, showcasing its
effectiveness and versatility; 3) conducting extensive experiments on two
separate datasets. our method was seamlessly integrated with two prior
state-of-the-art methods, demonstrating its compatibility and adaptability. the
experiments resulted in improved performance, indicating that our method
enhances the efficacy of these existing approaches.",5
120,1280,Patients and Slides are Equal: A Multi-level Multi-instance Learning Framework for Pathological Image Analysis,2.1,Overview,"our proposed method p&sre is illustrated in fig. 2. specifically, the framework
consists of two parts. the first part is the slide-patch level mil based on a
state-of-the-art mil method. the second part is the patient-slide level mil,
which generates patient-level features using attention mechanism and interacts
the features with transformer. to enhance readability, we first provide the
following symbolization for ml-mil: for a patient x i , it has a patient-level
classification label y i . for patient x i , there may exist n i slides s i ={s
j |j=1 to n i }, where the classification label for each slide s j is denoted as
z j . for each slide s j , it may be divided into m j patches p j ={p k |k=1 to
m j }. here, i,j, and k are indices for patient, slide, and patch levels,
respectively. our proposed framework has strong scalability as it can be based
on any attention-based mil method. therefore, we directly use the
state-of-the-art (sota) mil methods, abmil [8] and dsmil [9] for the slide-patch
stage. these two methods differ in their attention computing approach for each
patch.for abmil, the attention of each patch is computed by an mlp.
specifically, for m j patches p k , an encoder is applied to obtain the patch
feature matrix f i , where,f i ∈ r mj ×1024 . then, f i is passed through an fc
layer followed by a tanh activation and another fc layer followed by a sigmoid
activation to obtain two feature matrices, f i and f i , both ∈ r mj ×128 .
these matrices are elementwise multiplied and then passed through an fc layer to
obtain the weight of each patch, ω k .for dsmil, the attention of each patch is
based on the cosine distance between instances and key instances. first, an fc
layer is applied to the patch feature matrix f i to obtain the importance score
θ k for each patch. the patch with the highest score is selected as the key
instance. then, the feature matrix f i is mapped to a matrix q i ∈ r mj ×128 and
the cosine similarity between all instances and the key instance is computed as
the weight of each patch, ω k .although abmil and dsmil compute attention
differently, both methods compute the attention-weighted sum of patch instances
features as the bag representation of the slide. therefore, the slide feature
output by both methods can be generalized as:finally, we obtain the feature
vector set h i ={h j |j=1 to n i } for all slides {s j } of patientx i through
patch-slide mil.",5
121,1281,Patients and Slides are Equal: A Multi-level Multi-instance Learning Framework for Pathological Image Analysis,2.3,Patient-Slide Level MIL,"after performing patch-slide level mil, we move on to patient-slide level mil.
in general mil algorithms, the patient is regarded as the bag and the slide as
the instance. however, considering the diagnostic process in clinical practice,
we propose to treat both patients and slides as instances at the same level.
specifically, our p&sre framework for patient-slide level consists of two parts:
patient-level feature generation based on self-attention and patient-slide
feature interaction based on transformer [11].patient-level feature generation
based on self-attention. doctors usually select certain key slides for careful
observation and information aggregation during diagnosis, similar to the
self-attention mechanism. therefore, we directly use a fully connected (fc)
layer to integrate the feature-level features into patient-level features v i
through attention mechanism, serving as patient instances. specifically, given
the feature vector collection {h j } from multiple slides in the previous step,
we input it to the fc layer and apply the sigmoid activation function to output
the weight α j for each h j . then, we perform a weighted average of the vectors
based on this weight to obtain the patient feature v i :patient-slide feature
interaction based on transformer. this process is where our framework shines.
after doctors summarize the patient-level results, they typically review the
slides to double-check the diagnosis results. this patient-slide feature
interaction (psfi) naturally lends itself to the construction of a transformer,
and information exchange and integration between slides and patient level are
bidirectional. thus, self-attention is more ideal for this purpose than other
kinds of attention (such as cross-attention or doctors' attention). by using the
self-attention-based transformer structure, each input token is treated equally
(i.e., viewed as the same instance level), and tokens can interact extensively
with each other, enabling mutual correction between patients and slides and even
between slides. specifically, we merge the slide feature set {h j } and the
patient feature v i into the input tokensand then input them into a multi-layer
transformer through self-attention and feed-forward neural network layers to
obtain the interaction information between slides and output tokens t out i
:where d is the dimension of the token, and t k and t l come from t in i . β k,l
is multi-head attention matrix, and w q , w k , and w v are weight matrices of
query, key, and value, respectively. w r and w o are transformation matrices. b
1 and b 2 are bias vectors. this update procedure is repeated for l layers,
where the t k are fed to the successive transformer layer. finally, we obtain
the output tokensthen, all output tokens are input into a shared fc layer, and
the patient's predicted logits y i and the predicted classification logits {z j
|j = 1 to n i } for each slide are output.training progress and loss function.
during training, we sampled one patient at a time and pre-extracted their
batch-level features for all slides, in order to save gpu memory. due to the
issue of class imbalance in both slide level and patient level, we use the lade
[7] loss function.",5
122,1282,Patients and Slides are Equal: A Multi-level Multi-instance Learning Framework for Pathological Image Analysis,3.1,Dataset and Evaluation,"cd-itb dataset. cd-itb is a private dataset consisting of 853 slides from 163
patients, with binary patient-level labels of cd or itb in a ratio of 103:60 and
tri-class slide-level labels of cd, itb, and normal slides in a ratio of
436:121:296, respectively. on average, there were 5 slides per patient. the
slides were scanned at a magnification of 40× (0.25 µm/px), and annotations were
curated by experienced pathologists. we adopted a patient-level stratification
approach for 5-fold cross-validation, with 20% of the training set randomly
assigned as the validation set for each fold. the dataset comprises an average
of 2.3k instances per bag, with the largest bag containing over 16k
instances.camelyon17 dataset. camelyon17 [1] is a publicly dataset, and its
training set comprises 500 slides from 100 breast cancer patients with lymph
node metastases. the slides are classified into four distinct categories, namely
negative, itc, micro, and macro, in proportions of 318:36:59:87, respectively.
there were 5 slides per patient on average. the patients are divided into two
groups based on their pn stage, namely lymph node positive and lymph node
negative, in proportions of 24:76, respectively. the data folding method is the
same as the cd-itb dataset. the average number of instances per bag is
approximately 6.1k, and the largest bag contains over 23k instances.metrics. we
report class-wise weighted accuracy (acc), precision(pre), recall, and f1-score
(f1). to avoid randomness, we run all experiments five times and report the
averaged metrics.",5
123,1283,Patients and Slides are Equal: A Multi-level Multi-instance Learning Framework for Pathological Image Analysis,3.2,Implementation Details,"we utilized resnet50, which was pre-trained on imagenet1k, to extract features
from patches. each patch was of size 512 × 512 pixels. for both abmil and dsmil
networks, we kept the original parameters for the number of channels at each
layer. following the reference [4], we employed a transformer with 8 heads and 8
layers in the patient-slide feature interactions. all networks are implemented
using pytorch and trained on a nvidia rtx titan gpu with 24 gb memory. we
employed two adam optimizers with a maximum learning rate of 1e-4 and a cosine
annealing update strategy that gradually decreased the learning rate to 1e-12
over 300 epochs.",5
124,1284,Patients and Slides are Equal: A Multi-level Multi-instance Learning Framework for Pathological Image Analysis,3.3,Comparisons and Results,"we compared our strategy with two state-of-the-art mil methods to evaluate its
performance. to investigate the impact of self-attention and transformers on
slide-level and case-level results, we conducted ablation experiments: ""abmil +
p&sre (with/without psfi)"" and ""dsmil + p&sre (with/without psfi)"",
respectively. for slide-level classification, we used mean pooling and max
pooling to pool feature vectors of patches into a representative vector for the
slide, which was then fed into a fully connected layer for classification. at
the patient level, we used two approaches for prediction: maxs, where the
feature of the instance that achieves the maximum positive probability from the
slide-level mil model is selected to patient-level model, and maxmins, where the
mean value of features of the maximum and minimum positive probability from the
slide-level mil model is selected to patient-level model.the results of 5-fold
cv at the slide and patient levels are reported in table 1 and table 2,
respectively. our p&sre framework improves both abmil and dsmil methods at both
levels. abmil with p&sre improves the f1 score from 0.565 to 0.579 for the
cd-itb dataset and from 0.529 to 0.571 for the camelyon17 dataset at the
slide-level, and improves the f1 score from 0.522 to 0.599 for the cd-itb
dataset and from 0.842 to 0.861 for the camelyon17 dataset at the patient-level.
therefore, the ablation experiments demonstrate the effectiveness of p&sre in
enhancing the classification performance at both the slide and patient levels.",5
125,1285,Patients and Slides are Equal: A Multi-level Multi-instance Learning Framework for Pathological Image Analysis,4.0,Limitations,"our study has some limitations that should be addressed. for instance, we did
not explore the possibility of treating patches as an equivalent level to slides
and patients. the primary reason is that the vast number of patches required for
analysis is significantly larger than that of slides and patients, which
presents a computational challenge for training. as a result, we have not yet
explored this avenue. in the future, we plan to leverage clustering and active
learning methods to reduce the number of patches and enable the interaction of
all three levels with the transformer, which would further enhance the accuracy
and efficiency of our proposed method.",5
126,1286,Patients and Slides are Equal: A Multi-level Multi-instance Learning Framework for Pathological Image Analysis,5.0,Conclusion,"this study proposes a highly scalable and versatile framework to address m-mil
problems. we first classify the process from patch to slide to the patient in
medical pathology diagnosis as a multi-level mil problem. based on existing
state-of-the-art mil methods, we then extend the framework to p&sre, which
conducts feature extraction and interaction at the slide-patient level. by
introducing a transformer, the framework enables iterative interaction and
correction of information between patients and slides, resulting in better
performance at both the patient level and slide level compared to existing
state-of-the-art algorithms on two validation datasets.",5
128,1302,Improved Prognostic Prediction of Pancreatic Cancer Using Multi-phase CT by Integrating Neural Distance and Texture-Aware Transformer,1.0,Introduction,"pancreatic ductal adenocarcinoma (pdac) is one of the deadliest forms of human
cancer, with a 5-year survival rate of only 9% [16]. neoadjuvant chemotherapy
can increase the likelihood of achieving a margin-negative resection and avoid
unnecessary surgery in patients with aggressive tumor types [23]. providing
accurate and objective preoperative biomarkers is crucial for triaging patients
who are most likely to benefit from neoadjuvant chemotherapy. however, current
clinical markers such as larger tumor size and high carbohydrate antigen (ca)
19-9 level may not be sufficient to accurately tailor neoadjuvant treatment for
patients [19]. therefore, multi-phase contrast-enhanced ct has a great potential
to enable personalized prognostic prediction for pdac, leveraging its ability to
provide a wealth of texture information that can aid in the development of
accurate and effective prognostic models [2,10].previous studies have utilized
image texture analysis with hand-crafted features to predict the survival of
patients with pdacs [1], but the representational fig. 1. two examples of
spatial information between vessel (orange region) and tumor (green region). the
minimum distance, which refers to the closest distance between the superior
mesenteric artery (sma) and the pdac tumor region, is almost identical in these
two cases. we define the surface-to-surface distance based on point-to-surface
distance (weighted-average of red lines from ♦ to ) instead of point-to-point
distance (blue lines) to better capture the relationship between the tumor and
the perivascular tissue.here ♦ and are points sampled from subset vc and pc
defined in eq. power of these features may be limited. in recent years, deep
learning-based methods have shown promising results in prognosis models
[3,6,12]. however, pdacs differ significantly from the tumors in these studies.
a clinical investigation based on contrast-enhanced ct has revealed a dynamic
correlation between the internal stromal fractions of pdacs and their
surrounding vasculature [14]. therefore, focusing solely on the texture
information of the tumor itself may not be effective for the prognostic
prediction of pdac. it is necessary to incorporate tumor-vascular involvement
into the feature extraction process of the prognostic model. although some
studies have investigated tumor-vascular relationships [21,22], these methods
may not be sufficiently capable of capturing the complex dynamics between the
tumor and its environment.we propose a novel approach for measuring the relative
position relationship between the tumor and the vessel by explicitly using the
distance between them. typically, chamfer distance [7], hausdorff distance [8],
or other surfaceawareness metrics are used. however, as shown in fig. 1, these
point-to-point distances cannot differentiate the degree of tumor-vascular
invasion [18]. to address this limitation, we propose a learnable neural
distance that considers all relevant points on different surfaces and uses an
attention mechanism to compute a combined distance that is more suitable for
determining the degree of invasion. furthermore, to capture the tumor
enhancement patterns across multi-phase ct images, we are the first to combine
convolutional neural networks (cnn) and transformer [4] modules for extracting
the dynamic texture patterns of pdac and its surroundings. this approach takes
advantage of the visual transformer's adeptness in capturing long-distance
information compared to the cnn-onlybased framework in the original approach. by
incorporating texture information between pdac, pancreas, and peripancreatic
vessels, as well as the local tumor information captured by cnn, we aim to
improve the accuracy of our prognostic prediction model.in this study, we make
the following contributions: (1) we propose a novel approach for aiding survival
prediction in pdac by introducing a learnable neural distance that explicitly
evaluates the degree of vascular invasion between the tumor and its surrounding
vessels. (2) we introduce a texture-aware transformer block to enhance the
feature extraction approach, combining local and global information for
comprehensive texture information. we validate that the cross-attention is
utilized to capture cross-modality information and integrate it with in-modality
information, resulting in a more accurate and robust prognostic prediction model
for pdac. (3) through extensive evaluation and statistical analysis, we
demonstrate the effectiveness of our proposed method. the signature built from
our model remains statistically significant in multivariable analysis after
adjusting for established clinical predictors. our proposed model has the
potential to be used in combination with clinical factors for risk
stratification and treatment decisions for patients with pdac.",5
129,1305,Improved Prognostic Prediction of Pancreatic Cancer Using Multi-phase CT by Integrating Neural Distance and Texture-Aware Transformer,2.2,Neural Distance: Positional and Structural Information,"between pdac and vesselsthe vascular involvement in patients with pdac affects
the resectability and treatment planning [5]. in this study, we investigate four
important vessels: portal vein and splenic vein (pvsv), superior mesenteric
artery (sma), superior mesenteric vein (smv), and truncus coeliacus (tc). we
used a semi-supervised nnunet model to segment pdac and the surrounding vessels,
following recent work [11,21]. we define a general distance between the surface
boundaries of pdac (p) and the aforementioned four types of vessels (v) as d(v,
p), which can be derived as follows:where v ∈ v and p ∈ p are points on the
surfaces of blood vessels and pdac, respectively. the point-to-surface distance
d ps (v, p) is the distance from a point v on v to p, defined as d ps (v, p) =
min p∈p v -p 2 2 , and vice versa. to numerically calculate the integrals in the
previous equation, we uniformly sample from the surfaces v and p to obtain the
sets v and p consisting of n v points and n p points, respectively. the distance
is then calculated between the two sets using the following equation:however,
the above distance treats all points equally and may not be flexible enough to
adapt to individualized prognostic predictions. therefore, we improve the above
equation in two ways. firstly, we focus on the sub-sets vc and pc of v and p,
respectively, which only contain the k closest points to the opposite surfaces p
and v, respectively. the sub-sets are defined as:secondly, we regard the entire
sets vc and pc as sequences and calculate the distance using a 2-way
cross-attention block (similar to eq. 1) to build a neural distance based on the
3d spatial coordinates of each point:neural distance allows for the flexible
assignment of weights to different points and is able to find positional
information that is more suitable for pdac prognosis prediction. in addition to
neural distance, we use the 3d-cnn model introduced in [22] to extract the
structural relationship between pdac and the vessels. specifically, we
concatenate each pdac-vessel pair x v s ∈ r 2×h×w ×d , where v ∈{pvsv, smv, sma,
tc} and obtain the structure feature f s ∈ r cs .finally, we concatenate the
features extracted from the two components and apply a fully-connected layer to
predict the survival outcome, denoted as o os , which is a value between 0 and
1. to optimize the proposed model, we use the negative log partial likelihood as
the survival loss [9].",5
130,1306,Improved Prognostic Prediction of Pancreatic Cancer Using Multi-phase CT by Integrating Neural Distance and Texture-Aware Transformer,3.0,Experiments,"dataset. in this study, we used data from shengjing hospital to train our method
with 892 patients, and data from three other centers, including guangdong
provincial people's hospital, tianjin medical university and sun yatsen
university cancer center for independent testing with 178 patients. the
contrast-enhanced ct protocol included non-contrast, pancreatic, and portal
venous phases. pdac masks for 340 patients were manually labeled by a
radiologist from shengjing hospital with 18 years of experience in pancreatic
cancer, while the rest were predicted using self-learning models [11,24] and
checked by the same annotator. other vessel masks were generated using the same
semisupervised segmentation models. c-index was used as our primary evaluation
metric for survival prediction. we also reported the survival auc, which
estimates the cumulative area under the roc curve for the first 36
months.implementation details: we used nested 5-fold cross-validation and
augmented the training data by rotating volumetric tumors in the axial direction
and randomly selecting cropped regions with random shifts. we also set the
output feature dimensions to c t = 64 for the texture-aware transformer, c s =
64 for the structure extraction and k = 32 for the neural distance. the batch
size was 16 and the maximum iteration was set to 1000 epochs, and we selected
the model with the best performance on the validation set during training for
testing. we implemented our experiments using pytorch 1.11 and trained the
models on a single nvidia 32g-v100 gpu.ablation study. we first evaluated the
performance of our proposed textureaware transformer (tat) by comparing it with
the resnet18 cnn backbone and vit transformer backbone, as shown in table 1. our
model leverages the strengths of both local and global information in the
pancreas and achieved the best result. next, we compared different methods for
multi-phase stages, including lstm, early fusion (fusion), and cross-attention
(cross) in our method. cross-attention is more effective and lightweight than
lstm. moreover, we separated texture features into in-phase features and
cross-phase features, which is more reasonable than early fusion.secondly, we
evaluated each component in our proposed method, as shown in fig. 2, and
presented the results in table 1. combining the texture-aware transformer and
regular structure information improved the results from 0.630 to 0.648, as tumor
invasion strongly affects the survival of pdac patients. we also employed a
simple 4-variable regression model that used only the chamfer distance of the
tumor and the four vessels for prognostic prediction. the resulting c-index of
0.611 confirmed the correlation of the distance with the survival, which is
consistent with clinical findings [18]. explicitly adding the distance measure
further improved the results. our proposed neural distance metric outperformed
traditional surface distance metrics like chamfer distance, indicating its
suitability for distinguishing the severity of pdac.",5
131,1307,Improved Prognostic Prediction of Pancreatic Cancer Using Multi-phase CT by Integrating Neural Distance and Texture-Aware Transformer,0.0,Comparisons.,"to further evaluate the performance of our proposed model, we compared it with
recent deep prediction methods [17,21] and report the results in table 2. we
modified baseline deep learning models [12,17] and used their network
architectures to take a single pancreatic phase or all three phases as inputs.
deepct-pdac [21] is the most recent method that considers both tumor-related and
tumor-vascular relationships using 3d cnns. our proposed method, which uses the
transformer and structure-aware blocks to capture tumor enhancement patterns and
tumor-vascular involvement, demonstrated its effectiveness with better
performance in both nested 5-fold cross-validation and the multi-center
independent test set.in table 3, we used univariate and multivariate cox
proportional-hazards models to evaluate our signature and other
clinicopathologic factors in the independent test set. the proposed risk
stratification was a significant prognostic factor, along with other factors
like pathological tnm stages. after selecting significant variables (p < 0.05)
in univariate analysis, our proposed staging remained strong in multivariable
analysis after adjusting for important prognostic markers like pt and resection
margins. notably, our proposed marker remained the strongest among all
pre-operative markers, such as tumor size and ca 19-9.neoadjuvant therapy
selection. to demonstrate the added value of our signature as a tool to select
patients for neoadjuvant treatment before surgery, we plotted kaplan-meier
survival curves in fig. 3. we further stratify patients by our signature after
grouping them by tumor size and ca19-9, two clinically used preoperative
criteria for selection, and also age. our signature could significantly stratify
patients in all cases and those in the high-risk group had worse outcomes and
might be considered as potential neoadjuvant treatment candidates (e.g. 33
high-risk patients with larger tumor size and high ca19-9).",5
132,1308,Improved Prognostic Prediction of Pancreatic Cancer Using Multi-phase CT by Integrating Neural Distance and Texture-Aware Transformer,4.0,Conclusion,"in our paper, we propose a multi-branch transformer-based framework for
predicting cancer survival. our framework includes a texture-aware transformer
that captures both local and global information about the pdac and pancreas. we
also introduce a neural distance to calculate a more reasonable distance between
pdac and vessels, which is highly correlated with pdac survival. we have
extensively evaluated and statistically analyzed our proposed method,
demonstrating its effectiveness. furthermore, our model can be combined with
established high-risk features to aid in the patient selections who might
benefit from neoadjuvant therapy before surgery.",5
133,1320,Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans,1.0,Introduction,"gastric cancer (gc) is the third leading cause of cancer-related deaths
worldwide [19]. the five-year survival rate for gc is approximately 33% [16],
which is mainly attributed to patients being diagnosed with advanced-stage
disease harboring unresectable tumors. this is often due to the latent and
nonspecific signs and symptoms of early-stage gc. however, patients with
early-stage disease have a substantially higher five-year survival rate of
around 72% [16]. therefore, early detection of resectable/curable gastric
cancers, preferably before the onset of symptoms, presents a promising strategy
to reduce associated mortality. unfortunately, current guidelines do not
recommend any screening tests for gc [22]. while several screening tools have
been developed, such as barium-meal gastric photofluorography [5], upper
endoscopy [4,7,9], and serum pepsinogen levels [15], they are challenging to
apply to the general population due to their invasiveness, moderate
sensitivity/specificity, high cost, or side effects. therefore, there is an
urgent need for novel screening methods that are noninvasive, highly accurate,
low-cost, and ready to distribute.non-contrast ct is a commonly used imaging
protocol for various clinical purposes. it is a non-invasive, relatively
low-cost, and safe procedure that exposes patients to less radiation dose and
does not require the use of contrast injection that may cause serious side
effects (compared to multi-phase contrastenhanced ct). with recent advances in
ai, opportunistic screening of diseases using non-contrast ct during routine
clinical care performed for other clinical indications, such as lung and
colorectal cancer screening, presents an attractive approach to early detect
treatable and preventable diseases [17]. however, whether early detection of
gastric cancer using non-contrast ct scans is possible remains unknown. this is
because early-stage gastric tumors may only invade the mucosal and muscularis
layers, which are difficult to identify without the help of stomach preparation
and contrast injection. additionally, the poor contrast between the tumor and
normal stomach wall/tissues on non-contrast ct scans and various shape
alterations of gastric cancer, further exacerbates this challenge.in this paper,
we propose a novel approach for detecting gastric cancer on non-contrast ct
scans. unlike the conventional ""segmentation for classification"" methods that
directly employ segmentation networks, we developed a clusterinduced mask
transformer that performs segmentation and global classification simultaneously.
given the high variability in shape and texture of gastric cancer, we encode
these features into learnable clusters and utilize cluster analysis during
inference. by incorporating self-attention layers for global context modeling,
our model can leverage both local and global cues for accurate detection. in our
experiments, the proposed approach outperforms nnunet [8] by 0.032 in auc, 5.0%
in sensitivity, and 4.1% in specificity. these results demonstrate the potential
of our approach for opportunistic screening of gastric cancer in asymptomatic
patients using non-contrast ct scans.",5
134,1321,Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans,2.0,Related Work,"automated cancer detection. researchers have explored automated tumor detection
techniques on endoscopic [13,14], pathological images [20], and the prediction
of cancer prognosis [12]. recent developments in deep learning have
significantly improved the segmentation of gastric tumors [11], which is
critical for their detection. however, our framework is specifically designed
for noncontrast ct scans, which is beneficial for asymptomatic patients. while
previous studies have successfully detected pancreatic [25] and esophageal [26]
cancers on non-contrast ct, identifying gastric cancer presents a unique
challenge due to its subtle texture changes, various shape alterations, and
complex background, e.g., irregular gastric wall; liquid and contents in the
stomach.",5
135,1324,Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans,0.0,Knowledge Transfer from Contrast-Enhanced to Non-contrast CT.,"to address difficulties with tumor annotation on non-contrast cts, the
radiologists start by annotating a voxel-wise tumor mask on the
contrast-enhanced ct, referring to clinical and endoscopy reports as needed.
deeds [6] registration is then performed to align the contrast-enhanced ct with
the non-contrast ct and the resulting deformation field is applied to the
annotated mask. any misaligned ones are revised manually. in this manner (fig.
1d), a relatively coarse yet highly reliable tumor mask can be obtained for the
non-contrast ct image. cluster-induced classification with mask transformers.
segmentation for classification is widely used in tumor detection [25,26,32]. we
first train a unet [8,18] to segment the stomach and tumor regions using the
masks from the previous step. this unet considers local information and can only
extract stomach rois well during testing. however, local textures are inadequate
for accurate gastric tumor detection on non-contrast cts, so we need a network
of both local sensitivity to textures and global awareness of the organ-tumor
morphology. mask transformer [3,24] is a well-suited approach to boost the cnn
backbone with stand-alone transformer blocks. recent studies [27,28] suggest
interpreting object queries as cluster centers, which naturally exhibit
intra-cluster similarity and inter-class discrepancy. inspired by this, we
further develop a deep classification model on top of learnable cluster
representations.specifically, given image x ∈ r h×w ×d , annotation y ∈ r k×hw d
, and patient class p ∈ l, our model consists of three components: 1) a cnn
backbone to extract its pixel-wise features f ∈ r c×hw d (fig. 1a), 2) a
transformer module (fig. 1b), and 3) a multi-task cluster inference module (fig.
1c). the transformer module gradually updates a set of randomly initialized
object queries c ∈ r n ×c , i.e., to meaningful mask embedding vectors through
cross-attention between object queries and multi-scale pixel features,where c
and p stand for query and pixel features, q c , k p , v p represent linearly
projected query, key, and value. we adopt cluster-wise argmax from kmax-deeplab
[28] to substitute spatial-wise softmax in the original settings.we further
interpret the object queries as cluster centers from a cluster analysis
perspective. all the pixels in the convolutional feature map are assigned to
different clusters based on these centers. the assignment of clusters (a.k.a.
mask prediction) m ∈ r n ×hw d is computed as the cluster-wise softmax function
over the matrix product between the cluster centers c and pixel-wise feature
matrix f, i.e.,the final segmentation logits z ∈ r k×hw d are obtained by
aggregating the pixels within each cluster according to cluster-wise
classification, which treats pixels within a cluster as a whole. the aggregation
of pixels is achieved by z = c k m, where the cluster-wise classification c k is
represented by an mlp that projects the cluster centers c to k channels (the
number of segmentation classes).the learned cluster centers possess high-level
semantics with both intercluster discrepancy and intra-cluster similarity for
effective classification. rather than directly classifying the final feature
map, we first generate the clusterpath feature vector by taking the channel-wise
average of cluster centers c =additionally, to enhance the consistency between
the segmentation and classification outputs, we apply global max pooling to
cluster assignments r to obtain the pixel-path feature vector r ∈ r n . this
establishes a direct connection between classification features and segmentation
predictions. finally, we concatenate these two feature vectors to obtain the
final feature and project it onto the classification prediction p ∈ r 2 via a
two-layer mlp.the overall training objective is formulated as,where the
segmentation loss l seg (•, •) is a combination of dice and cross entropy
losses, and the classification loss l cls (•, •) is cross entropy loss.",5
136,1325,Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans,4.1,Experimental Setup,"dataset and ground truth. our study analyzed a dataset of ct scans collected
from guangdong province people's hospital between years 2018 and 2020, with
2,139 patients consisting of 787 gastric cancer and 1,352 normal cases. we used
the latest patients in the second half of 2020 as a hold-out test set, resulting
in a training set of 687 gastric cancer and 1,204 normal cases, and a test set
of 100 gastric cancer and 148 normal cases. we randomly selected 20% of the
training data as an internal validation set. to further evaluate specificity in
a larger population, we collected an external test set of 903 normal cases from
shengjing hospital. cancer cases were confirmed through endoscopy (and
pathology) reports, while normal cases were confirmed by radiology reports and a
two-year follow-up. all patients underwent multi-phase cts with a median spacing
of 0.75 × 0.75 × 5.0 mm and an average size of (512, 512, 108) voxel. tumors
were annotated on the venous phase by an experienced radiologist specializing in
gastric imaging using ctlabeler [23], while the stomach was automatically
annotated using a self-learning model [31].implementation details. we resampled
each ct volume to the median spacing while normalizing it to have zero mean and
unit variance. during training, we cropped the 3d bounding box of the stomach
and added a small margin of (32,32,4). we used nnunet [8] as the backbone, with
four transformer decoders, each taking pixel features with output strides of 32,
16, 8, and 4. we set the number of object queries n to 8, with each having a
dimension of 128, and included an eight-head self-attention layer in each block.
the patch size used during training and inference is (192, 224, 40) voxel. we
followed [8] to augment data. we trained the model with radam using a learning
rate of 10 -4 and a (backbone) learning rate multiplier of 0.1 for 1000 epochs,
with a frozen backbone of the pretrained nnunet [8] for the first 50 epochs. to
enhance performance, we added deep supervision by aligning the cross-attention
map with the final segmentation map, as per kmax-deeplab [27]. the hidden layer
dimension in the two-layer mlp is 128. we also trained a standard unet [8,18] to
localize the stomach region in the entire image in the testing phase.",5
137,1326,Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans,0.0,Evaluation Metrics and Reader Study.,"for the binary classification, model performance is evaluated using area under
roc curve (auc), sensitivity (sens.), and specificity (spec.). and successful
localization of the tumors is considered when the overlap between the
segmentation mask generated by the model and the ground truth is greater than
0.01, measured by the dice score. a reader study was conducted with two
experienced radiologists, one from guangdong province people's hospital with 20
years of experience and the other from the first affiliated hospital of zhejiang
university with 9 years of experience in gastric imaging. the readers were given
248 non-contrast ct scans from the test set and asked to provide a binary
decision for each scan, indicating whether the scan showed gastric cancer. no
patient information or records were provided to the readers. readers were
informed that the dataset might contain more tumor cases than the standard
prevalence observed in screening, but the proportion of case types was not
disclosed. readers used itk-snap [30] to interpret the ct scans without any time
constraints. 1 presents a comparative analysis of our proposed method with three
baselines. the first two approaches belong to ""segmentation for classification""
(s4c) [26,32], using nnunet [8] and transunet [2]. a case is classified as
positive if the segmented tumor volume exceeds a threshold that maximizes the
sum of sensitivity and specificity on the validation set. the third baseline
(denoted as ""nnunet-joint"") integrates a cnn classification head into unet [8]
and trained end-to-end. we obtain the 95% confidence interval of auc,
sensitivity, and specificity values from 1000 bootstrap replicas of the test
dataset for statistical analysis. for statistical significance, we conduct a
delong test between two aucs (ours vs. compared method) and a permutation test
between two sensitivities or specificities (ours vs. compared method and
radiologists).",5
138,1329,Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans,0.0,Subgroup Analysis.,"in table 2, we report the performance of patient-level detection and tumor-level
localization stratified by tumor (t) stage. we compare our model's performance
with that of both radiologists. the results show that our model performs better
in detecting early stage tumors (t1, t2) and provides more precise tumor
localization. specifically, our model detects 60.0% (6/10) t1 cancers, and 77.8%
(7/9) t2 cancers, surpassing the best performing expert (50% t1, 55.6% t2).
meanwhile, our model maintains a reliable detection rate and credible
localization accuracy for t3 and t4 tumors (2 of 34 t3 tumors missed).comparison
with established screening tools. our method surpasses or performs on par with
established screening tools [4,7,10] in terms of sensitivity for gastric cancer
detection at a similar specificity level with a relatively large testing patient
size (n = 1151 by integrating the internal and external test sets), as shown in
table 3. this finding sheds light on the opportunity to employ automated ai
systems to screen gastric cancer using non-contrast ct scans.",5
142,1383,Liver Tumor Screening and Diagnosis in CT with Pixel-Lesion-Patient Network,1.0,Introduction,"liver cancer is the third leading cause of cancer death world-wide in 2020 [14].
early detection and accurate diagnosis of liver tumors may improve overall
patient outcomes, in which imaging plays a key role [11]. computed tomography
(ct) is one of the most important imaging modalities for liver tumors. dynamic
contrast-enhanced (dce) ct is widely used for diagnostics, but it requires
iodine contrast injection which can cause reaction and potential risks in
patients. recently, non-contrast (nc) ct scans are gaining attention as they are
cheaper and safer to acquire, thus can be potential tools for opportunistic
tumor screening [18,20]. meanwhile, finding and diagnosing tumors in nc cts is
also extremely challenging because of the poor contrast between tumors and
normal tissues compared to those in dce cts. prior works on pancreas [18] and
esophagus [20] have shown that latest deep learning techniques can detect subtle
texture and shape changes in nc ct that even human eyes may miss. thus, we aim
to investigate the performance of liver tumor segmentation and classification in
nc cts. such an approach will be helpful to discover asymptomatic incidental
tumors [12] from routine nc ct scans indicated for general diagnostic purposes
at no additional cost and radiation exposure. after an incidental tumor is
found, the patient may undergo further imaging examination such as a multi-phase
dce ct for differential diagnosis [11], which can provide useful discriminative
information such as the vascularity of lesions and the pattern of contrast agent
enhancement [19]. liver is largest solid organ in body and is the site of many
tumor types [11]. therefore, accurate tumor type classification is important for
the decision of treatment plans and prognosis.many researchers have developed
algorithms to automatically segment [1,9,13,15,23] or classify [19,21,25] liver
tumors in ct to help radiologists improve their accuracy and efficiency. for
example, public datasets such as the liver tumor segmentation benchmark (lits)
[1] fostered a series of works aiming to segment liver tumors with improved
convolutional neural network (cnn) backbones [9,13] and lesion edge information
[15]. lits only has single-phase cts (venous phase). several studies
investigated methods to exploit multi-phase ct by methods such as hetero-phase
fusion [5] and modality-aware mutual learning [23]. there are few work
discussing liver tumor analysis in nc ct [5]. besides lesion segmentation,
cnn-based lesion classification algorithms have been studied to distinguish
common lesion types [19,21,25].in this paper, we build a comprehensive framework
to address both tumor screening and diagnosis. (1) tumor screening involves
finding tumor patients in a large pool of healthy subjects and patients. most
existing works in tumor segmentation and detection did not explicitly consider
it since their training and testing images are all tumor patients. such models
may generate false positives in real-world screening scenario when facing
diverse tumor-free images. we collect a large-scale dataset with both tumor and
non-tumor subjects, where the non-tumor subjects includes not only healthy ones,
but also patients with various diffuse liver diseases such as steatosis and
hepatitis to improve the robustness of the algorithm. (2) most works studied
liver tumor segmentation alone without differentiating tumor types, while a few
works classify liver tumors on cropped tumor patches [19,21,25]. meanwhile, we
learn tumor segmentation and classification with one network using an instance
segmentation framework [3]. we train two networks for nc and multi-phase dce
cts, respectively. (3) for evaluation, previous segmentation works typically use
pixel-level metrics such as dice coefficient. such metrics cannot reflect the
lesion-level accuracy (how many lesion instances are correctly detected and
classified) and may bias to large lesions when a patient has multiple tumors.
patient-level metrics (e.g. classifying whether a subject has malignant tumors)
are also useful for treatment recommendation in clinical practice [18,20].
therefore, we assess our algorithm thoroughly with pixel, lesion, and
patient-level metrics.algorithms for liver tumor segmentation have focused on
improving the feature extraction backbone of a fully-convolutional cnn
[9,13,15,23]. the pixelwise segmentation architectures may not be optimal for
lesion and patient-level evaluation metrics since they cannot consider a lesion
or an image holistically. recently, a series of mask transformer algorithms
[3,4,17] have emerged in the computer vision community and achieved the
state-of-the-art performance in instance segmentation tasks. in brief, they use
object queries to interact with image feature maps and with each other to
produce mask and class predictions for each instance. inspired by them, we
propose a novel end-to-end framework named pixel-lesion-patient network (plan)
for lesion segmentation and classification, as well as patient classification.
it contains three branches with bottomup cooperation: the segmentation map from
the pixel branch helps to initialize the lesion branch, which is an improved
mask transformer aiming to segment and classify each lesion; the patient branch
aggregates information from the whole image and predicts image-level labels of
each lesion type, with regularization terms to encourage consistency with the
lesion branch.we collected a large-scale multi-phase dataset containing 810
non-tumor subjects and 939 tumor patients. 4010 tumor instances of eight types
are extensively annotated based on pathological reports. on the non-contrast
tumor screening and diagnosis task, plan achieves 95.0%, 96.4%, and 0.965 in
patient-level sensitivity, specificity, and average auc for malignant and benign
patients, in contrast to 94.4%, 93.7%, and 0.889 for the widely-used nnu-net
[8]. on multi-phase dce ct, our lesion-level detection precision, recall, and
classification accuracy are 92.2%, 89.0%, 85.9%, outperforming nnu-net [8] and
mask2former [3]. we further conduct a reader study on a holdout set of 250
cases. our algorithm is on par with a senior radiologist (16 yrs experience),
showing the clinical significance of our results. our codes will be made public
upon institutional approval.",5
143,1385,Liver Tumor Screening and Diagnosis in CT with Pixel-Lesion-Patient Network,2.2,Pixel-Lesion-Patient Network (PLAN),"our goal is to segment the mask and classify the type of each tumor in a liver
ct. we also hope to make patient-level diagnoses for each ct scan. plan is
inspired by mask2former [3] with three key improvements: (1) a pixel branch is
added to provide anchor queries to the lesion branch. (2) the lesion branch is
composed of the transformer decoder in mask2former, and we improve its
segmentation loss to enhance recall of small lesions. (3) a patient branch is
attached to make dedicated image-level predictions with a proposed
lesion-patient consistency loss. our framework is shown in fig. 1.pixel branch
and anchor queries. the pixel branch is a convolutional layer after the pixel
decoder and learns to predict pixel-wise segmentation maps similar to
traditional segmentators. we do cc analysis to the predicted mask to extract
lesion instances, and then average the pixel embeddings inside each predicted
lesion to obtain a feature vector. the feature vectors are regarded as anchor
queries and work the same way as the randomly initialized queries in the lesion
branch. compared to the random queries in the original mask2former, the anchor
queries contain prior information of the lesions to be segmented, helping the
lesion branch to match with the lesion targets more easily [10].lesion branch
and foreground-enhanced sampling loss. similar to mask2former, the lesion branch
predicts a binary mask and a class label for each query, see fig. 1. mask2former
calculates its segmentation loss on k sampled pixels instead of on the whole
image, which is shown to both improve accuracy and reduce gpu memory usage [3].
however, in lesion segmentation, some tumors are very small compared to the
whole 3d image. the importance sampling strategy [3] can hardly select any
foreground pixels in such cases, so the loss only contains background pixels,
degrading the segmentation recall of small lesions. we propose a simple approach
to remedy this issue by sampling an extra n foreground pixels for each
lesion.patient branch. a patient-level diagnosis is useful for triage. for
example, diagnosing the subject as normal, benign, or malignant will result in
completely different treatments [24]. intuitively, we can also infer
patient-level labels from segmentation results by checking if there is any
lesion in the predicted mask. however, certain tumors are often related to signs
outside the tumor, e.g. hepatocellular carcinoma and cirrhosis,
cholangiocarcinoma and bile duct dilatation, etc. we equip plan with a dedicated
patient branch to aggregate such global information to make better patient-level
prediction. since one patient can have multiple liver tumors of different types,
in our problem, we give each image several hierarchical binary labels. the first
label classifies normal and tumor subjects (whether the image contains any
tumor); the second and third labels indicate the existence of respectively
benign and malignant tumors; the rest c labels suggest the existence of c
fine-grained types of tumors. we employ the dual-path transformer block [17] to
fuse multi-scale features from the pixel encoder and decoder to generate a
feature map, followed by global average pooling and a linear classification
layer to predict the c + 3 labels.a lesion-patient consistency loss is further
proposed to encourage coherence of the lesion and patient-level predictions.
inspired by multi-instance learning [6], we compute a pseudo patient-level
prediction c ∈ r c from the lesion-level predictions by max-pooling the class
probability of each class across all lesion queries (discarding the no-object
class). we also have the probability vector from the patient branch p ∈ r c
corresponding to the c fine-grained classes. then, we compute the l2 loss
between them:the overall loss of plan is listed in eq. 1, where l pixel is the
combined crossentropy (ce) and dice loss for the pixel branch as in nnu-net [8];
l lesion-class is the ce loss [3] for lesion classification in the lesion
branch; l lesion-mask is the combined ce and dice loss [3] for binary lesion
segmentation in the lesion branch with the foreground-enhanced sampling
strategy; l patient is the binary ce loss for the multi-label classification
task in the patient branch.",5
144,1386,Liver Tumor Screening and Diagnosis in CT with Pixel-Lesion-Patient Network,3.0,Experiments,"data. our dataset contains 810 normal subjects and 939 patients with liver
tumors. each normal subject has a non-contrast (nc) ct, while each patient has a
dynamic contrast-enhanced (dce) ct scan with nc, arterial, and venous phases. we
use deeds [7] to register nc and arterial phases to the venous phase, and then
invite a senior radiologist with 10 years of experience to annotate on the
multi-phase cts using ct labeler [16]. the 3d mask and the type of all liver
tumors are annotated based on pathological reports and magnetic resonance scans
if necessary. eight tumor types are considered in our study: hepatocellular
carcinoma (hcc), intrahepatic cholangiocarcinoma (icc), metastasis (meta),
hepatoblastoma (hepato), hemangioma (heman), focal nodular hyperplasia (fnh),
cyst, and others (all other tumor types). if a lesion's type cannot be
determined according to image signs [11] and pathology, it will be marked as
""unknown"" and ignored in training and evaluation. in total, 4010 tumor instances
are annotated, whose volumes range from 11 to 3.7×10 6 mm 3 . detailed
statistics and examples of the lesions are shown in the supplementary material.
we train two separate networks for nc and dce cts. in the former setting, both
normal and patient data are used and randomly split into 1149 training, 100
validation, and 500 testing. in the latter one, only patient data are used with
641 training, 100 validation, and 200 testing. another hold-out set of 150
patients and 100 normal cts are used for reader study to compare our accuracy
with two radiologists. implementation details. each ct is resampled to
0.7×0.7×5mm in spacing. we first train an nnu-net on public datasets to segment
liver and surrounding organs (gallbladder, hepatic vein, spleen, stomach, and
pancreas), and then crop the liver region to train plan. to help plan
differentiate liver tumors and other organs, we train the network to segment
both tumors and organs using the predicted organ labels. plan is built on top of
the nnu-net framework [8]. its pixel encoder is a u-net encoder, whereas its
pixel decoder is a light-weight feature pyramid network [3]. the lesion branch
incorporates three transformer decoder blocks with masked attention [3] which
use feature maps of strides 16, 8, 4 from the pixel decoder. the number of
random queries is q = 20; the embedding dimension is m = 64; the number of
sampled pixels is k = 12544 [3], foreground pixels n = 3; the loss weight is 0.1
for the no-object class while 1 for other classes in the lesion branch [3]. the
weights in eq. 1 arewe use the radam optimizer with an initial learning rate of
0.0001. each training batch contains two patches of size 256 × 256 × 24. for dce
ct, the three phases form a 3-channel image as the network input. extensive data
augmentation is applied including random cropping, scaling, flipping, elastic
deformation, and brightness adjustment [8].during training, we first pretrain
the backbone and the pixel branch for 500 epochs, and then train the whole
network for another 500 epochs.patient-level results. this paper has three major
goals: tumor screening in nc ct (classifying a subject as normal or tumor),
preliminary diagnosis in nc ct (predicting the existence of malignant and benign
tumors), and fine-grained diagnosis in dce ct (predicting the existence of 8
tumor types). among the 8 tumor types, hcc, icc, meta, and hepato are malignant;
heman, fnh, and cyst are benign. ""others"" can be either malignant or benign,
thus are excluded in the preliminary diagnosis task. the nc test set contains
198 tumor cases, 202 completely normal cases, and 100 ""hard"" non-tumor cases
which may have larger image noise, artifact, ascites, diffuse liver diseases
such as hepatitis and steatosis. these cases are used to test the robustness of
the model in real-world screening scenario with diverse tumor-free images. we
compare plan with a widely-used strong baseline, nnu-net [8]. the recent mask
transformer, mask2former [3], is also adapted to 3d for comparison. for the
baselines, patient-level labels are inferred from their predicted masks by
counting lesion pixels. as displayed in table 1, plan achieves the best accuracy
on all tasks, especially in nc preliminary diagnosis tasks, which demonstrates
the effectiveness of its dedicated patient branch that can explicitly aggregate
features from the whole image.lesion and pixel-level results. in lesion-level
evaluation, we treat a prediction as a true positive if its overlap with a
ground-truth lesion is >0.2 in dice. lesions smaller than 3 mm in radius are
ignored. as shown in table 2, the pixellevel accuracy of nnu-net and plan are
comparable, but plan's lesion-level accuracy is consistently higher than
nnu-net. in this work, we focus more on patient and lesion-level metrics.
although nc images have low contrast, they can still be used to segment and
classify lesions with ∼ 80% precision, recall, and classification accuracy. it
implies the potential of nc ct, which has been understudied in previous works.
mask2former has higher precision but lower recall in nc ct, especially for small
lesions, while plan achieves the best recall using the foreground-enhanced
sampling loss. both plan and mask2former achieve better classification accuracy,
which illustrates the mask transformer architecture is good at lesion-level
classification.",5
145,1387,Liver Tumor Screening and Diagnosis in CT with Pixel-Lesion-Patient Network,0.0,Comparison with Radiologists.,"in the reader study, we invited a senior radiologist with 16 years of experience
in liver imaging, and a junior radiologist with 2 years of experience. they
first read the nc ct of all subjects and provided a diagnosis of normal, benign,
or malignant. then, they read the dce scans and provided a diagnosis of the 8
tumor types. we consider patients with only one tumor type in this study. their
reading process is without time constraint. in table 3 and fig. 2, all methods
get good specificity probably because the normal subjects are completely
healthy. our model achieves comparable accuracy with the senior radiologist but
outperforms the junior one by a large margin in sensitivity and classification
accuracy. an ablation study for our method is shown in table 4. it can be seen
that our proposed anchor queries produced by the pixel branch, fes loss, and
lesionpatient consistency loss are useful for the final performance. the
efficacy of the lesion and patient branches has been analyzed above based on the
lesion and patient-level results. due to space limit, we will show the accuracy
for each tumor type and more qualitative examples in the supplementary
material.comparison with literature. in the pixel level, we obtain dice scores
of 77.2% and 84.2% using nc and dce cts, respectively. the current state of the
art (sota) of lits [1] achieved 82.2% in dice using cts in venous phase; [23]
achieved 81.3% in dice using dce ct of two phases. in the lesion level, our
precision and recall are 80.1% and 81.9% for nc ct, 92.2% and 89.0% for dce ct,
at 20% overlap. [25] achieved 83% and 93% for dce ct. sota of lits achieved
49.7% and 46.3% at 50% overlap. [21] classified lesions into 5 classes,
achieving 84% accuracy for dce and 49% for nc ct. we classify lesions into 8
classes with 85.9% accuracy for dce and 78.5% for nc ct. in the patient level,
[5] achieved auc=0.75 in nc ct tumor screening, while our auc is 0.985. in
summary, our results are superior or comparable to existing works.",5
148,1398,Merging-Diverging Hybrid Transformer Networks for Survival Prediction in Head and Neck Cancer,1.0,Introduction,"head and neck (h&n) cancer refers to malignant tumors in h&n regions, which is
among the most common cancers worldwide [1]. survival prediction, a regression
task that models the survival outcomes of patients, is crucial for h&n cancer
patients: it provides early prognostic information to guide treatment planning
and potentially improves the overall survival outcomes of patients [2].
multi-modality imaging of positron emission tomography -computed tomography
(pet-ct) has been shown to benefit survival prediction as it offers both
anatomical (ct) and metabolic (pet) information about tumors [3,4]. therefore,
survival prediction from pet-ct images in h&n cancer has attracted wide
attention and serves as a key research area. for instance, head and neck tumor
segmentation and outcome prediction challenges (hecktor) have been held for the
last three years to facilitate the development of new algorithms for survival
prediction from pet-ct images in h&n cancer [5][6][7].traditional survival
prediction methods are usually based on radiomics [8], where handcrafted
radiomics features are extracted from pre-segmented tumor regions and then are
modeled by statistical survival models, such as the cox proportional hazard
(coxph) model [9]. in addition, deep survival models based on deep learning have
been proposed to perform end-to-end survival prediction from medical images,
where pre-segmented tumor masks are often unrequired [10]. deep survival models
usually adopt convolutional neural networks (cnns) to extract image features,
and recently visual transformers (vit) have been adopted for its capabilities to
capture long-range dependency within images [11,12]. these deep survival models
have shown the potential to outperform traditional survival prediction methods
[13]. for survival prediction in h&n cancer, deep survival models have achieved
top performance in the hecktor 2021/2022 and are regarded as state-of-the-art
[14][15][16]. nevertheless, we identified that existing deep survival models
still have two main limitations.firstly, existing deep survival models are
underdeveloped in utilizing complementary multi-modality information, such as
the metabolic and anatomical information in pet and ct images. for survival
prediction in h&n cancer, existing methods usually use single imaging modality
[17,18] or rely on early fusion (i.e., concatenating multi-modality images as
multi-channel inputs) to combine multi-modality information [11,[14][15][16]19].
in addition, late fusion has been used for survival prediction in other diseases
such as gliomas and tuberculosis [20,21], where multi-modality features were
extracted by multiple independent encoders with resultant features fused.
however, early fusion has difficulties in extracting intra-modality information
due to entangled (concatenated) images for feature extraction, while late fusion
has difficulties in extracting inter-modality information due to fully
independent feature extraction. recently, tang et al. [22] attempted to address
this limitation by proposing a multi-scale non-local attention fusion (mnaf)
block for survival prediction of glioma patients, in which multi-modality
features were fused via non-local attention mechanism [23] at multiple scales.
however, the performance of this method heavily relies on using tumor
segmentation masks as inputs, which limits its generalizability.secondly,
although deep survival models have advantages in performing end-to-end survival
prediction without requiring tumor masks, this also incurs difficulties in
extracting region-specific information, such as the prognostic information in
primary tumor (pt) and metastatic lymph node (mln) regions. to address this
limitation, recent deep survival models adopted multi-task learning for joint
tumor segmentation and survival prediction, to implicitly guide the model to
extract features related to tumor regions [11,16,[24][25][26]. however, most of
them only considered pt segmentation and ignored the prognostic information in
mln regions [11,[24][25][26]. meng et al. [16] performed survival prediction
with joint pt-mln segmentation and achieved one of the top performances in
hecktor 2022. however, this method extracted entangled features related to both
pt and mln regions, which incurs difficulties in discovering the prognostic
information in pt-/mln-only regions.in this study, we design an x-shape
merging-diverging hybrid transformer network (named xsurv, fig. 1) for survival
prediction in h&n cancer. our xsurv has a merging encoder to fuse complementary
anatomical and metabolic information in pet and ct images and has a diverging
decoder to extract region-specific prognostic information in pt and mln regions.
our technical contributions in xsurv are three folds: (i) we propose a
merging-diverging learning framework for survival prediction. this framework is
specialized in leveraging multi-modality images and extracting regionspecific
information, which potentially could be applied to many survival prediction
tasks with multi-modality imaging. (ii) we propose a hybrid parallel
cross-attention (hpca) block for multi-modality feature learning, where both
local intra-modality and global inter-modality features are learned via parallel
convolutional layers and crossattention transformers. (iii) we propose a
region-specific attention gate (rag) block for region-specific feature
extraction, which screens out the features related to lesion regions. extensive
experiments on the public dataset of hecktor 2022 [7] demonstrate that our xsurv
outperforms state-of-the-art survival prediction methods, including the
top-performing methods in hecktor 2022.",6
149,1404,Merging-Diverging Hybrid Transformer Networks for Survival Prediction in Head and Neck Cancer,3.1,Dataset and Preprocessing,"we adopted the training dataset of hecktor 2022 (refer to
https://hecktor.grand-cha llenge.org/), including 488 h&n cancer patients
acquired from seven medical centers [7], while the testing dataset was excluded
as its ground-truth labels are not released. each patient underwent pretreatment
pet/ct and has clinical indicators. we present the distributions of all clinical
indicators in the supplementary materials. recurrence-free survival (rfs),
including time-to-event in days and censored-or-not status, was provided as
ground truth for survival prediction, while pt and mln annotations were provided
for segmentation. the patients from two centers (chum and chuv) were used for
testing and other patients for training, which split the data into 386/102
patients in training/testing sets. we trained and validated models using 5-fold
cross-validation within the training set and evaluated them in the testing
set.we resampled pet-ct images into isotropic voxels where 1 voxel corresponds
to 1 mm 3 . each image was cropped to 160 × 160 × 160 voxels with the tumor
located in the center. pet images were standardized using z-score normalization,
while ct images were clipped to [-1024, 1024] and then mapped to [-1, 1]. in
addition, we performed univariate and multivariate cox analyses on the clinical
indicators to screen out the prognostic indicators with significant relevance to
rfs (p < 0.05).",6
150,1409,ALL-IN: A Local GLobal Graph-Based DIstillatioN Model for Representation Learning of Gigapixel Histopathology Images With Application In Cancer Risk Assessment,1.0,Introduction,"the examination of tissue and cells using microscope (referred to as histology)
has been a key component of cancer diagnosis and prognostication since more than
a hundred years ago. histological features allow visual readout of cancer
biology as they represent the overall impact of genetic changes on cells
[20].the great rise of deep learning in the past decade and our ability to
digitize histopathology slides using high-throughput slide scanners have fueled
interests in the applications of deep learning in histopathology image analysis.
the majority of the efforts, so far, focus on the deployment of these models for
diagnosis and classification [27]. as such, there is a paucity of efforts that
embark on utilizing machine learning models for patient prognostication and
survival analysis (for example, predicting risk of cancer recurrence or expected
patient survival). while prognostication and survival analysis offer invaluable
insights for patient management, biological studies and drug development
efforts, they require careful tracking of patients for a lengthy period of time;
rendering this as a task that requires a significant amount of effort and
funding.in the machine learning domain, patient prognostication can be treated
as a weakly supervised problem, which a model would predict the outcome (e.g.,
time to cancer recurrence) based on the histopathology images. their majority
have utilized multiple instance learning (mil) [8] that is a two-step learning
method. first, representation maps for a set of patches (i.e., small fields of
view), called a bag of instances, are extracted. then, a second pooling model is
applied to the feature maps for the final prediction. different mil variations
have shown superior performances in grading or subtype classification in
comparison to outcome prediction [10]. this is perhaps due to the fact that
mil-based technique do not incorporate patch locations and interactions as well
as tissue heterogeneity which can potentially have a vital role in defining
clinical outcomes [4,26].to address this issue, graph neural networks (gnn) have
recently received more attention in histology. they can model patch relations
[17] by utilizing message passing mechanism via edges connecting the nodes
(i.e., small patches in our case). however, most gnn-based models suffer from
over smoothing [22] which limits nodes' receptive fields [3]. while local
contexts mainly capture cell-cell interactions, global patterns such as immune
cell infiltration patterns and tumor invasion in normal tissue structures (e.g.,
depth of invasion through myometrium in endometrial cancer [1]) could capture
critical information about outcome [10]. hence, locally focused methods are
unable to benefit from the coarse properties of slides due to their high
dimensions which may lead to poor performance.this paper aims to investigate the
potential of extracting fine and coarse features from histopathology slides and
integrating them for risk stratification in cancer patients. therefore, the
contributions of this work can be summarized as: 1) a novel graph-based model
for predicting survival that extracts both local and global properties by
identifying morphological super-nodes; 2) introducing a fine-coarse feature
distillation module with 3 various strategies to aggregate interactions at
different scales; 3) outperforming sota approaches in both risk prediction and
patient stratification scenarios on two datasets; 4) publishing two large and
rare prostate cancer datasets containing more than 220 graphs for active
surveillance and 240 graphs for brachytherapy cases. the code and graph
embeddings are publicly available at https://github.com/pazadimo/all-in 2
related works",6
151,1413,ALL-IN: A Local GLobal Graph-Based DIstillatioN Model for Representation Learning of Gigapixel Histopathology Images With Application In Cancer Risk Assessment,3.1,Problem Formulation,"for p n , which is the n-th patient, a set of patches {patch j } m j=1 is
extracted from the related whole slide images. in addition, a latent vector z j
∈ r 1×d is extracted from patch j using our encoder network (described in sect.
3.2) that results in feature matrix z n ∈ r m ×d for p n . finally, a specific
graph (g n ) for the n-th patient (p n ) can be constructed by assuming patches
as nodes. also, edges are connected based on the patches' k-nearest neighbour in
the spatial domain resulting in an adjacency matrix a n . therefore, for each
patient such as p n , we have a graph defined by adjacency matrix a n with size
m × m and features matrix z n (g n = graph(z n , a n )). we estimate k
super-nodes as matrix s n ∈ r k×d representing groups of local nodes with
similar properties as coarse features for p n 's slides. the final model ( θ )
with parameters θ utilizes g n and s n to predict the risk associated with this
patient:",6
152,1414,ALL-IN: A Local GLobal Graph-Based DIstillatioN Model for Representation Learning of Gigapixel Histopathology Images With Application In Cancer Risk Assessment,3.2,Self-supervised Encoder,"due to computational limits and large number of patches available for each
patient, we utilize a self-supervised approach to train an encoder to reduce the
inputs' feature space size. therefore, we use dino [9], a knowledge distillation
model (kdm), with vision transformer (vit) [13] as the backbone. it utilizes
global and local augmentations of the input patch j and passes them to the
student (s θ1,v it ) and teacher (t θ2,v it ) models to find their respective
representations without any labels. then, by using distillation loss, it makes
the representations' distribution similar to each other. finally, the fixed
weights of the teacher model are utilized in order to encode the input patches.",6
153,1416,ALL-IN: A Local GLobal Graph-Based DIstillatioN Model for Representation Learning of Gigapixel Histopathology Images With Application In Cancer Risk Assessment,3.4,Super-Nodes Extractor,"in order to find the coarse histo-morphological patterns disguised in the local
graph, we propose extracting k super-nodes, which each represents a weighted
cluster of further processed local features. intuitively, the number of
super-nodes k should not be very large or small, as the former encourages them
to only represent local clusters and the latter leads to larger clusters and
loses subtle details. we exploit the mincut [5] idea to extract super-nodes in a
differentiable process after an auxiliary ginconv to focus more on large-scale
interactions and to finally learn the most global correlated super-nodes.
inspired by the relaxation form of the known k-way mincut problem, we create a
continuous cluster matrix c n ∈ r m ×k using mlp layers and can finally estimate
the super-nodes features (s n ∈ r m ×d ) as:where w 1 , w 2 are mlps' weights.
hence, the extracted nodes are directly dependent on the final survival-specific
loss. in addition, two additional unsupervised weighted regularization terms are
optimized to improve the process:mincut regularizer. this term is motivated by
the original mincut problem and intends to solve it for the the patients' graph.
it is defined as:where d n is the diagonal degree matrix for a n . also, t r(.)
represents the trace of matrix and a n,norm is the normalized adjacency matrix.
r mincu t 's minimum value happens when t r(therefore, minimizing r mincu t
causes assigning strongly similar nodes to a same super-node and prevent their
association with others.orthogonality regularizer. r mincu t is non-convex and
potent to local minima such as assigning all vertexes to a super-node or having
multiple super-nodes with only a single vertex. r orthogonal penalizes such
solutions and helps the model to distribute the graph's features between
super-nodes. it can be formulated as:where ||.|| f is the frobenius norm, and i
is the identity matrix. this term pushes the model's parameters to find coarse
features that are orthogonal to each other resulting in having the most useful
global features. overall, utilizing these two terms encourages the model to
extract supernodes by leaning more towards the strongly associated vertexes and
keeping them against weakly connected ones [5], while the main survival loss
still controls the global extraction process.",6
154,1417,ALL-IN: A Local GLobal Graph-Based DIstillatioN Model for Representation Learning of Gigapixel Histopathology Images With Application In Cancer Risk Assessment,3.5,Fine-Coarse Distillation,"we propose our fine-coarse morphological feature distillation module to leverage
all-scale interactions in the final prediction by finding a local and a global
patientlevel representations ( ĥl,n , ĥg,n ). assume that x n ∈ r m ×d and s n ∈
r k×d are the feature matrices taken from local gnn (sect. 3.3) and super-nodes
for p n , respectively. we explore 3 different attention-based feature
distillation strategies for this task, including:-dual attention (da): two gated
self-attention modules for local and global properties with separate weights (w
φ,l , w φ,g , w k,l , w k,g , w q,l , w q,g ) are utilized to find patches
scores α l ∈ r 1×m and α g ∈ r 1×k and the final features ( ĥl,n , ĥg,n ) as:)
where x n,i and s n,i are rows of x n and s n , respectively, and the final
representation ( ĥ) is generated as ĥ = cat( ĥl , ĥg ).-mixed guided attention
(mga): in the first strategy, the information flows from local and global
features to the final representations in parallel without mixing any knowledge.
the purpose of this policy is the heavy fusion of fine and coarse knowledge by
exploiting shared weights (w φ,shared , w k,shared , w q,shared , w v,shared )
in both routes and benefiting from the guidance of local representation on
learning the global one by modifying eq. ( 7) to:-mixed co-attention (mca):
while the first strategy allows the extreme separation of two paths, the second
one has the highest level of mixing information. here, we take a balanced policy
between the independence and knowledge mixture of the two routes by only sharing
the weights without using any guidance.",6
155,1418,ALL-IN: A Local GLobal Graph-Based DIstillatioN Model for Representation Learning of Gigapixel Histopathology Images With Application In Cancer Risk Assessment,4.1,Dataset,"we utilize two prostate cancer (pca) datasets to evaluate the performance of our
proposed model. the first set (pca-as) includes 179 pca patients who were
managed with active surveillance (as). radical therapy is considered
overtreatment in these patients, so they are instead monitored with regular
serum prostate-specific antigen (psa) measurements, physical examinations,
sequential biopsies, and magnetic resonance imaging [23]. however, as may be
over-or under-utilized in low-and intermediate-risk pca due to the uncertainty
of current methods to distinguish indolent from aggressive cancers [11].
although majority of patients in our cohort are classified as low-risk based on
nccn guidelines [21], a significant subset of them experienced disease upgrade
that triggered definitive therapy (range: 6.2 to 224 months after diagnosis).the
second dataset (pca-bt) includes 105 pca patients with low to high risk disease
who went through brachytherapy. this treatment involves placing a radioactive
material inside the body to safely deliver larger dose of radiation at one time
[25]. the recorded endpoint for this set is biochemical recurrence with time to
recurrence ranging from 11.7 to 56.1 months. we also utilized the prostate
cancer grade assessment (panda) challenge dataset [7] that includes more than
10,000 pca needle biopsy slides (no outcome data) as an external dataset for
training the encoder of our model.",6
156,1419,ALL-IN: A Local GLobal Graph-Based DIstillatioN Model for Representation Learning of Gigapixel Histopathology Images With Application In Cancer Risk Assessment,4.2,Experiments,"we evaluate the models' performance in two scenarios utilizing several objective
metrics. implementation details are available in supplementary material.hazard
(risk) prediction. we utilize concordance-index (c-index) that measures the
relative ordering of patients with observed events and un-censored cases
relative to censored instances [2]. using c-index, we compare the quality of
hazard ranking against multiple methods including two mil (deepset [31], amil
[14]) and graph-based (dgc [17] and patch-gcn [10]) models that were utilized
recently for histopathology risk assessment. c-index values are available in
table 1. the proposed model with all strategies outperforms baselines across all
sets and is able to achieve 0.639 and 0.600 on pca-as and pca-bt, while the
baselines, at best, obtain 0.555, and 0.572, respectively. statistical tests
(paired t-test) on c-indices also show that our model is statistically better
than all baselines in pca-as and also superior to all models, except dgc, in
pca-bt. superior performance of our mca policy implies that balanced
exploitation of fine and coarse features with shared weights may provide more
robust contextual information compared to using mixed guided information or
utilizing them independently.patient stratification. the capacity of stratifying
patients into risk groups (e.g., low and high risk) is another criterion that we
employ to assess the utility of models in clinical practice. we evaluate model
performances via kaplan-meier curve [15] (cut-off set as the ratio of patients
with recurrence within 3 years of therapy initiation for pca-bt and the ratio of
upgraded cases for pca-as), logrank test [6] (with 0.05 as significance level),
and median outcome associated with risk groups (table 1 and fig. 2). our model
stratified pca-as patients into high-and low-risk groups with median time to
progression of 36.5 and 131.7 months, respectively. moreover, pca-bt cases
assigned to high-and low-risk groups have median recurrence time of 21.86 and
35.7 months. while none of the baselines are capable of assigning patients into
risk groups with statistical significance, our distillation policies achieve
significant separation in both pca-as and pca-bt datasets; suggesting that
global histo-morphological properties improve patient stratification
performance. furthermore, our findings have significant clinical implications as
they identify, for the first time, highrisk prostate cancer patients who are
otherwise known to be low-risk based on clinico-pathological parameters. this
group should be managed differently from the rest of the low-risk prostate
cancer patients in the clinic. therefore, providing evidence of the predictive
(as opposed to prognostic) clinical information that our model provides. while a
prognostic biomarker provides information about a patient's outcome (without
specific recommendation on the next course of action), a predictive biomarker
gives insights about the effect of a therapeutic intervention and potential
actions that can be taken.ablation study. we perform ablation study (table 2) on
various components of our framework including local nodes, self-supervised
vit-based encoder, and most importantly, super-nodes in addition to fine-coarse
distillation module. although our local-only model is still showing superior
results compared to baselines, this analysis demonstrates that all modules are
essential for learning the most effective representations. we also assess the
impact of our vit on the baselines (full-results in appendix), showing that it
can, on average, improve their performance by an increase of ∼ 0.03 in c-index
for pca-as. however, the best baseline with vit still has poorer performance
compared to our model in both datasets, while the number of parameters (reported
for vit embeddings' size in table 1) in our full-model is about half of this
baseline. achieving higher c-indices in our all model versions indicates the
important role of coarse features and global context in patient risk estimation
in addition to local patterns.",6
158,1432,An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,1.0,Introduction,"accurate spatial characterization of tumor immune microenvironment is critical
for precise therapeutic stratification of cancer patients (e.g. via
immunotherapy). currently, this characterization is done manually by individual
pathologists on standard hematoxylin-and-eosin (h&e) or singleplex
immunohistochemistry (ihc) stained images. however, this results in high
interobserver variability among pathologists, primarily due to the large (> 50%)
disagreement among pathologists for immune cell phenotyping [10]. this is also a
big cause of concern for publicly available h&e/ihc cell segmentation datasets
with immune cell annotations from single pathologists. multiplex staining
resolves this issue by allowing different tumor and immune cell markers to be
stained on the same tissue section, avoiding any phenotyping guesswork from
pathologists. multiplex staining can be performed using expensive multiplex
immunofluorescence (mif) or via cheaper multiplex immunohistochemistry (mihc)
assays. mif staining (requiring expensive scanners and highly skilled lab
technicians) allows multiple markers to be stained/expressed on the same tissue
section (no co-registration needed) while also providing the utility to turn
on/off individual markers as needed. in contrast, current brightfield mihc
staining protocols relying on dab (3,3'-diaminobenzidine) alcohol-insoluble
chromogen, even though easily implementable with current clinical staining
protocols, suffer from occlusion of signal from sequential staining of
additional markers. to this effect, we introduce a new brightfield mihc staining
protocol using alcoholsoluble aminoethyl carbazole (aec) chromogen which allows
repeated stripping, restaining, and scanning of the same tissue section with
multiple markers. this requires only affine registration to align the digitized
restained images to obtain non-occluded signal intensity profiles for all the
markers, similar to mif staining/scanning.in this paper, we introduce a new
dataset that can be readily used out-ofthe-box with any artificial intelligence
(ai)/deep learning algorithms for spatial characterization of tumor immune
microenvironment and several other use cases.to date, only two denovo stained
datasets have been released publicly: bci h&e and singleplex ihc her2 dataset
[7] and deepliif singleplex ihc ki67 and mif dataset [2], both without any
immune or tumor markers. in contrast, we release the first denovo mif/mihc
stained dataset with tumor and immune markers for more accurate characterization
of tumor immune microenvironment. we also demonstrate several interesting use
cases: (1) ihc quantification of cd3/cd8 tumor-infiltrating lymphocytes (tils)
via style transfer, (2) virtual translation of cheap mihc stains to more
expensive mif stains, and (3) virtual tumor/immune cellular phenotyping on
standard hematoxylin images.",6
159,1433,An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,2.0,Dataset,"the complete staining protocols for this dataset are given in the accompanying
supplementary material. images were acquired at 20× magnification at moffitt
cancer center. the demographics and other relevant information for all eight
head-and-neck squamous cell carcinoma patients is given in table 1.",6
160,1434,An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,2.1,Region-of-Interest Selection and Image Registration,"after scanning the full images at low resolution, nine regions of interest
(rois) from each slide were chosen by an experienced pathologist on both mif and
mihc images: three in the tumor core (tc), three at the tumor margin (tm), and
three outside in the adjacent stroma (s) area. the size of the rois was
standardized at 1356×1012 pixels with a resolution of 0.5 µm/pixel for a total
surface area of 0.343 mm 2 . hematoxylin-stained rois were first used to align
all the mihc marker images in the open source fiji software using affine
registration. after that, hematoxylin-and dapi-stained rois were used as
references to align mihc and mif rois again using fiji and subdivided into
512×512 patches, resulting in total of 268 co-registered mihc and mif patches
(∼33 co-registered mif/mihc images per patient).",6
161,1438,An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,0.0,(b) Style Transfer:,"this sub-network creates the stylized ihc image using an attention module, given
(1) the input hematoxylin and the mif marker images and (2) the style and its
corresponding marker images. for synthetically generating stylized ihc images,
we follow the approach outlined in adaattn [8]. we use a pre-trained vgg-19
network [12] as an encoder to extract multi-level feature maps and a decoder
with a symmetric structure of vgg-19. we then use both shallow and deep level
features by using adaattn modules on multiple layers of vgg. this sub-network is
used to create a stylized image using the structure of the given hematoxylin
image while transferring the overall color distribution of the style image to
the final stylized image. the generated marker image from the first sub-network
is used for a more accurate colorization of the positive cells against the blue
hematoxylin counterstain/background; not defining loss functions based on the
markers generated by the first sub-network leads to discrepancy in the final
brown dab channel synthesis.for the stylized ihc images with ground truth
cd3/cd8 marker images, we also segmented corresponding dapi images using our
interactive deep learning impartial [9] tool
https://github.com/nadeemlab/impartial and then classified the segmented masks
using the corresponding cd3/cd8 channel intensities, as shown in fig. 4. we
extracted 268 tiles of size 512×512 from this final segmented and co-registered
dataset. for the purpose of training and testing all the models, we extract four
images of size 256 × 256 from each tile due to the size of the external ihc
images, resulting in a total of 1072 images. we randomly extracted tiles from
the lyon19 challenge dataset [14] to use as style ihc images. using these
images, we created a dataset of synthetically generated ihc images from the
hematoxylin and its marker image as shown in fig. 3.we evaluated the
effectiveness of our synthetically generated dataset (stylized ihc images and
corresponding segmented/classified masks) using our generated dataset with the
nuclick training dataset (containing manually segmented cd3/cd8 cells) [6]. we
randomly selected 840 and 230 patches of size 256 × 256 from the created dataset
for training and validation, respectively. nuclick training and validation sets
[6] comprise 671 and 200 patches, respectively, of size 256 × 256 extracted from
lyon19 dataset [14]. lyon19 ihc cd3/cd8 images are taken from breast, colon, and
prostate cancer patients. we split their training set into training and
validation sets, containing 553 and 118 images, respectively, and use their
validation set for testing our trained models. we trained three models including
unet [11], fpn [5], unet++ [15] with the backbone of resnet50 for 200 epochs and
early stopping on validation score with patience of 30 epochs, using binary
cross entropy loss and adam optimizer with learning rate of 0.0001. as shown in
table 2, models trained with our synthetic training set outperform those trained
solely with nuclick data in all metrics.we also tested the trained models on
1,500 randomly selected images from the training set of the lymphocyte
assessment hackathon (lysto) [1], containing image patches of size 299 × 299
obtained at a magnification of 40× from breast, prostate, and colon cancer whole
slide images stained with cd3 and cd8 markers. only the total number of
lymphocytes in each image patch are reported in this dataset. to evaluate the
performance of trained models on this dataset, we counted the total number of
marked lymphocytes in a predicted mask and calculated the difference between the
reported number of lymphocytes in each image with the total number of
lymphocytes in the predicted mask by the model. in table 2, the average
difference value (diffcount) of lymphocyte number for the whole dataset is
reported for each model. as seen, the trained models on our dataset outperform
the models trained solely on nuclick data.",6
162,1441,An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,4.0,Conclusions and Future Work,"we have released the first ai-ready restained and co-registered mif and mihc
dataset for head-and-neck squamous cell carcinoma patients. this dataset can be
used for virtual phenotyping given standard clinical hematoxylin images, virtual
clinical ihc dab generation with ground truth segmentations (to train
highquality segmentation models across multiple cancer types) created from
cleaner mif images, as well as for generating standardized clean mif images from
neighboring h&e and ihc sections for registration and 3d reconstruction of
tissue specimens. in the future, we will release similar datasets for additional
cancer types as well as release for this dataset corresponding whole-cell
segmentations via impartial https://github.com/nadeemlab/impartial.",6
165,1468,Multi-scale Prototypical Transformer for Whole Slide Image Classification,2.1,MIL Problem Formulation,"mil is a typical weakly supervised learning method, where the training data
consists of a set of bags, and each bag contains multiple instances. the goal of
mil is to learn a classifier that can predict the label of a bag based on the
instances in it. in binary classification, a bag can be marked as negative if
all in-stances in the bag are negative, otherwise, the bag is labeled as
positive with at least one positive instance. in the mil setting, a wsi is
considered as a bag and the numerous cropped patches in wsi are regarded as
instances in the bag. a wsi dataset t can be defined as:where x i denotes a
patient, y i the label of x i , i j i is the j-th instance of x i , n is the
number of patients and n is the number of instances.",6
166,1484,Thyroid Nodule Diagnosis in Dynamic Contrast-Enhanced Ultrasound via Microvessel Infiltration Awareness,3.0,Experiments,"dataset. our dataset contained 282 consecutive patients who underwent thyroid
nodule examination at nanjing drum tower hospital. all patients performed
dynamic ceus examination by an experienced sonographer using an iu22 scanner
(philips healthcare, bothell, wa) equipped with a linear transducer l9-3 probe.
these 282 cases included 147 malignant nodules and 135 benign nodules. on the
one hand, the percutaneous biopsy based pathological examination was implemented
to determine the ground-truth of malignant and benign. on the other hand, a
sonographer with more than 10 years of experience manually annotated the nodule
lesion mask to obtain the pixel-level groundtruth of thyroid nodules
segmentation. all data were approved by the institutional review board of
nanjing drum tower hospital, and all patients signed the informed consent before
enrollment into the study.implementation details. our network was implemented
using pytorch framework with the single 12 gb gpu of nvidia rtx 3060. during
training, we first pre-trained the talr backbone via dice loss for 30 epochs and
used adam optimizer with learning rate of 0.0001. then, we loaded the
pre-trained weights to train the whole model for 100 epochs and used adam
optimizer with learning rate of 0.0001. here, we set batch-size to 4 during the
entire training process the ceus consisted the full wash-in and wash-out phases,
and the resolution of each frame was (600 × 800). in addition, we carried out
data augmentation, including random rotation and cropping, and we resize the
resolution of input frames to (224 × 224). we adopted 5-fold cross-validation to
achieve quantitative evaluation. three indexes including dice, recall, and iou,
were used to evaluate the lesion recognition task, while five indexes, namely
average accuracy (acc), sensitivity (se), specificity (sp), f1-score (f1), and
auc, were used to evaluate the diagnosis task.experimental results. as in table
1, we compared our method with sota method including v-net, unet3d, transunet.
for the task of identifying lesions, the index of recall is important, because
information in irrelevant regions can be discarded, but it will be disastrous to
lose any lesion information. v-net achieved the highest recall scores compared
to others; thus, it was chosen as the backbone of tlar. table 1 revealed that
the modules (tpa, saf, and ipo) used in the network greatly improved the
segmentation performance compared to baseline, increasing dice and recall scores
by 7.60% and 7.23%, respectively. for the lesion area recognition task, our
method achieved the highest dice of 85.54% and recall of 90.40%, and the
visualized results were shown in fig. 3. to evaluate the effectiveness of the
baseline of lightweight c3d, we compared the results with sota video
classification methods including c3d, r3d, r2plus1d and convlstm. for fair
comparison, all methods used the manually annotated lesion mask to assist the
diagnosis. experimental results in table 2 revealed that our baseline network
could be useful for the diagnosis. with the effective baseline, the introduced
modules including tlar, saf and ipo further improved the diagnosis accuracy,
increasing the accuracy by 9.5%. the awareness of microvascular infiltration
using saf and ipo unit was helpful for ceus-based diagnosis, as it could improve
the diagnosis accuracy by 7.69% (as in table 2). as in appendix fig. a1,
although sota method fails to focus on lesion areas, our method can pinpoint
discriminating lesion areas. influence of α values. the value of α in saf is
associated with simulating microvessel infiltration. figure 3 (c) showed that
the diagnosis accuracy increased along with the increment of α and then tended
to become stable when α was close to 9. therefore, for balancing the efficiency
and performance, the number of ipo was set as n = 3 and α was set as α = {1, 5,
9} to generate a group of confidence maps that can simulate the process of
microvessel infiltration. (more details about the setting of n is in appendix
fig. a4 of the supplementary material.)",6
167,1495,Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,1.0,Introduction,"the tumor microenvironment (tme) is comprised of cancer, immune (e.g. b
lymphocytes, and t lymphocytes), stromal, and other cells together with
noncellular tissue components [3,5,24,30]. it is well acknowledged that tumors
evolve in close interaction with their microenvironment. quantitatively
characterizing tme has the potential to predict tumor aggressiveness and
treatment response [3,23,24,30]. different types of lymphocytes such as cd4+
(helper t cells), cd8+ (cytotoxic t cells), cd20+ (b cells), within the tme
naturally interact with tumor and stromal cells. studies [5,9] have shown that
quantifying spatial interplay of these different cell families within the tme
can provide more prognostic/predictive value compared to only measuring the
density of a single biomarker such as tumor-infiltrating lymphocytes (tils)
[3,24]. immunotherapy (io) is the standard treatment for patients with advanced
non-small cell lung cancer (nsclc) [19] but only 27-45% of patients respond to
this treatment [21]. therefore, better algorithms and improved biomarkers are
essential for identifying which cancer patients are most likely to respond to io
in advance of treatment. quantitative features that relate to the complex
spatial interplay between different types of b-and t-cells in the tme might
unlock attributes that are associated with io response. in this study, we
introduce a novel approach called triangular analysis of geographical interplay
of lymphocytes (triangil), representing a unique and interpretable way to
characterize the distribution, and higher-order interaction of various cell
families (e.g., cancerous cells, stromal cells, lymphocyte subtypes) across
digital histopathology slides. we demonstrate the efficacy of triaangil for
characterizing tme in the context of predicting 1) response to io with immune
checkpoint inhibitors (ici), 2) overall survival (os), in patients with nsclc,
and 3) providing novel insights into the spatial interplay between different
immune cell subtype. triangil source code is publicly available at
http://github.com/sarayar/triangil.",6
168,1496,Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,2.0,Previous Related Work and Novel Contributions,"many studies have only looked at the density of a single biomarker (e.g. tils),
to show that a high density of tils is associated with improved patient survival
and treatment response in nsclc [3,24]. other works have attempted to
characterize the spatial arrangement of cells in tme using computational
graphbased approaches. these approaches include methods that connect cells
regardless of their type (1) using global graphs (gg) such as voronoi that
connect all nuclei [2,14], or (2) using cell cluster graphs (ccg) [16] to create
multiple nuclear subgraphs based on cell-to-cell proximity to predict tumor
aggressiveness and patient outcome [16]. others have explored (3) the spatial
interplay between two different cell types [5].one example approach is spatial
architecture of til (spatil) [9] which attempted to characterize the interplay
between immune and cancer cells and has proven to be helpful in predicting the
recurrence in early-stage nsclc. all of these approaches point to overwhelming
evidence that spatial architecture of cells in tme is critical in predicting
cancer outcome. however, these approaches have not been able to exploit
higher-order interactions and dependencies between multiple cell types (> 2),
relationships that might provide additional actionable insights. the
contributions of this work include:(1) triangil is a computational framework
that characterizes the architecture and relationships of different cell types
simultaneously. instead of measuring only simple two-by-two relations between
cells, it seeks to identify triadic spatial relations (hyperedges [18,20] have
shown great capabilities in solving complex problems in the biomedical field,
these tend to be black-box in nature. a key consideration in cancer immunology
is the need for actionable insights into the spatial relationships between
different types of immune cells. not only does triangil provide predictions that
are on par or superior compared to dl approaches, but also provides a way to
glean insights into the spatial interplay of different immune cell types. these
complex interactions enhance our understanding of the tme and will help pave the
way for new therapeutic strategies that leverage these insights.",6
169,1499,Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,4.1,Dataset,"the cohort employed in this study was composed of pre-treatment tumor biopsy
specimens from patients with nsclc from five centers (two centers for training
(s t ) and three centers for independent validation (s v )). the entire analysis
was carried out using 122 patients in experiment 1 (73 in s t , and 49 in s v )
and 135 patients in experiment 2 (81 in s t , and 54 in s v ). specimens were
analyzed with a multiplexed quantitative immunofluorescence (qif) panel using
the method described in [22]. from each whole slide image, 7 representative
tiles were obtained and used to train the software inform to define background,
tumor and stromal compartments. then, individual cells were segmented based on
nuclear dapi staining and the segmentation performance was controlled by direct
visualization of samples by a trained observer. next, the software was trained
to identify cell subtypes based on marker expression (cd8, cd4, cd20, ck for
tumor epithelial cells and absence of these markers for stromal cells).",6
170,1501,Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,0.0,TIL density (DenTIL):,"for every patient, multiple density measures including the number of different
cells types and their ratios are calculated [3,24] (supplemental table 2).",6
171,1502,Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,0.0,"GG: A Delaunay triangulation, a Voronoi diagram, and a Minimum Spanning","tree were constructed [2,14] on all nuclei regardless of their type.
architectural features (e.g., perimeter, triangle area, edge length) were then
calculated on these global graphs for each patient.",6
172,1503,Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,0.0,CCG:,"for every patient, subgraphs are built on nuclei regardless of their type and
only based on their euclidean distance. local graph metrics (e.g. clustering
coefficient) [16] are then calculated from these subgraphs. spatil: for each
patient, first, subgraphs are built on individual cell types based on a distance
parameter. the convex hulls are then constructed on these subgraphs. after
selecting every two cell types, features are extracted from their convex hulls
(e.g. the number of clusters of each cell type, area intersected between
clusters [9]; complete list of combinations in supplemental table 3).",6
173,1505,Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,4.3,Experiment 1: Immunotherapy Response Prediction in Lung Cancer,"design: triangil was also trained to differentiate between patients who
responded to io and those who did not. for our study, the responders to io were
identified as those patients with complete response, partial response, and
stable disease, and non-responders were patients with progressive disease. a
linear discriminant analysis (lda) classifier was trained on s t to predict
which patients would respond to io. for creating the model, the minimum
redundancy maximum relevance (mrmr) method [1] was used to select the top
features. the same procedure using mrmr and lda was performed for the
comparative hand-crafted approaches. the ability to identify responders post-io
was assessed by the area under the receiver operating characteristic curve (auc)
in s v .",6
174,1506,Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,0.0,Results:,"the two top predictive triangil features were found to be the number of edges
between stroma and cd4+ cells, and the number of edges between stroma and tumor
cells with more interactions between stromal cells and both cd4+ and tumor cells
being associated with response to io. this finding is concordant with other
studies [13,17,22,27] that stromal tils were significantly associated with
improved os. therefore, triangil approach is not only predictive of treatment
response but more critically it enables biological interpretations that a dl
model might not be able to provide. in s v , this lda classifier was able to
distinguish responders from non-responders to io with au c t ri =0. design: s t
was used to construct a least absolute shrinkage and selection operator (lasso)
[28] regularized cox proportional hazards model [6] using the triangil features,
to obtain risk score for each patient. lasso features are listed in supplemental
table 4. the median risk score in s t was used as a threshold in both s t and s
v to dichotomize patients into low-risk/high-risk categories. kaplan-meier (km)
survival curves [26] were plotted and the model performance was summarized by
hazard ratio (hr), with corresponding (95% confidence intervals (ci)) using the
log-rank test, and harrell's concordance index (c-index) on s v . the c-index
evaluates the correlation between risk predictions and survival times, aiming to
maximize the discrimination between high-risk and low-risk patients [11]. os is
the time between the initiation of io to the death of the patient. the patients
were censored if the date of death was unknown.result: figure 2 presents some
triangil features in a field of view for a patient with long-term survival and
another with short-term survival. more triangular relationships, shorter
triangle edges, and smaller triangles with smaller perimeters are found in the
long-term survival case when analyzing the triadic interactions within
tumor-stroma-cd4, thereby suggesting higher relative presence and closer
interaction of these cell families. figure 3 illustrates the km plots for the
six approaches. we also calculated the concordance index (c-index) for the two
prognostic approaches in s v . the c-index for triangil and gnn methods were
0.64, and 0.63 respectively. therefore, overall triangil worked marginally
better than gnn, with much higher biological interpretability.",6
175,1507,Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,5.0,Concluding Remarks,"we presented a new approach, triangular analysis of geographical interplay of
lymphocytes (triangil), to quantitatively chartacterize the spatial arrangement
and relative geographical interplay of multiple cell families across
pathological images. compared to previous spatial graph-based methods, triangil
quantifies the spatial interplay between multiple cell families, providing a
more comprehensive portrait of the tumor microenvironment. triangil was
predictive of response after io (n = 122) and also demonstrated a strong
correlation with os in nsclc patients treated with io (n = 135). triangil
outperformed other graph-and dl-based approaches, with the added benefit of
provoding interpretability with regard to the spatial interplay between cell
families. for instance, triangil yielded the insight that more interactions
between stromal cells and both cd4+ and tumor cells appears to be associated
with better response to io. although five cell families were studies in this
work, triangil is flexible and could include other cell types (e.g.,
macrophages). future work will entail larger validation studies and also
evaluation on other use cases.",6
176,1522,DiffDP: Radiotherapy Dose Prediction via a Diffusion Model,1.0,Introduction,"radiotherapy, one of the mainstream treatments for cancer patients, has gained
notable advancements in past decades. for promising curative effect, a
high-quality radiotherapy plan is demanded to distribute sufficient dose of
radiation to the planning target volume (ptv) while minimizing the radiation
hazard to organs at risk (oars). to achieve this, radiotherapy plans need to be
manually adjusted by the dosimetrists in a trial-and-error manner, which is
extremely labor-intensive and time-consuming [1,2]. additionally, the quality of
treatment plans might be variable among radiologists due to their different
expertise and experience [3]. consequently, it is essential to develop a robust
methodology to automatically predict the dose distribution for cancer patients,
relieving the burden on dosimetrists and accelerating the radiotherapy
procedure.recently, the blossom of deep learning (dl) has promoted the automatic
medical image processing tasks [4][5][6], especially for dose prediction
[7][8][9][10][11][12][13][14]. for example, nguyen et al. [7] modified the
traditional 2d unet [15] to predict the dose of prostate cancer patients. wang
et al. [10] utilized a progressive refinement unet (prunet) to refine the
predictions from low resolution to high resolution. besides the above unetbased
frameworks, song et al. [11] employed the deeplabv3+ [16] to excavate contextual
information from different scales, thus obtaining accuracy improvements in the
dose prediction of rectum cancer. mahmood et al. [12] utilized a generative
adversarial network (gan)-based method to predict the dose maps of oropharyngeal
cancer. furthermore, zhan et al. [13] designed a multi-organ constraint loss to
enforce the deep model to better consider the dose requirements of different
organs. following the idea of multi-task learning, tan et al. [8] utilized
isodose line and gradient information to promote the performance of dose
prediction of rectum cancer. to ease the burden on the delineation of ptv and
oars, li et al. [17] constructed an additional segmentation task to provide the
dose prediction task with essential anatomical knowledge.although the above
methods have achieved good performance in predicting dose distribution, they
suffer from the over-smoothing problem. these dl-based dose prediction methods
always apply the l 1 or l 2 loss to guide the model optimization which
calculates a posterior mean of the joint distribution between the predictions
and the ground truth [17,18], leading to the over-smoothed predicted images
without important high-frequency details [19]. we display predicted dose maps
from multiple deep models in fig. 1. as shown, compared with the ground truth,
i.e., (5) in fig. 1, the predictions from (1) to (3) are blurred with fewer
high-frequency details, such as ray shapes. these high-frequency features formed
by ray penetration reveal the ray directions and dose attenuation with the aim
of killing the cancer cells while protecting the oars as much as possible, which
are critical for radiotherapy. consequently, exploring an automatic method to
generate high-quality predictions with rich high-frequency information is
important to improve the performance of dose prediction. currently, diffusion
model [20] has verified its remarkable potential in modeling complex image
distributions in some vision tasks [21][22][23]. unlike other dl models, the
diffusion model is trained without any extra assumption about target data
distribution, thus evading the average effect and alleviating the over-smoothing
problem [24]. figure 1 (4) provides an example in which the diffusion-based
model predicts a dose map with shaper and clearer boundaries of ray-penetrated
areas. therefore, introducing a diffusion model to the dose prediction task is a
worthwhile endeavor.in this paper, we investigate the feasibility of applying a
diffusion model to the dose prediction task and propose a diffusion-based model,
called diffdp, to automatically predict the clinically acceptable dose
distribution for rectum cancer patients. specifically, the diffdp consists of a
forward process and a reverse process. in the forward process, the model employs
a markov chain to gradually transform dose distribution maps with complex
distribution into gaussian distribution by progressively adding pre-defined
noise. then, in the reverse process, given a pure gaussian noise, the model
gradually removes the noise in multiple steps and finally outputs the predicted
dose map. in this procedure, a noise predictor is trained to predict the noise
added in the corresponding step of the forward process. to further ensure the
accuracy of the predicted dose distribution for both the ptv and oars, we design
a dl-based structure encoder to extract the anatomical information from the ct
image and the segmentation masks of the ptv and oars. such anatomical
information can indicate the structure and relative position of organs. by
incorporating the anatomical information, the noise predictor can be aware of
the dose constraints among ptv and oars, thus distributing more appropriate dose
to them and generating more accurate dose distribution maps.overall, the
contributions of this paper can be concluded as follows: (1) we propose a novel
diffusion-based model for dose prediction in cancer radiotherapy to address the
over-smoothing issue commonly encountered in existing dl-based dose prediction
methods. to the best of our knowledge, we are the first to introduce the
diffusion model for this task. (2) we introduce a structure encoder to extract
the anatomical information available in the ct images and organ segmentation
masks, and exploit the anatomical information to guide the noise predictor in
the diffusion model towards generating more precise predictions. (3) the
proposed diffdp is extensively evaluated on a clinical dataset consisting of 130
rectum cancer patients, and the results demonstrate that our approach
outperforms other state-of-the-art methods.",6
177,1523,DiffDP: Radiotherapy Dose Prediction via a Diffusion Model,2.0,Methodology,"an overview of the proposed diffdp model is illustrated in fig. 2, containing
two markov chain processes: a forward process and a reverse process. an image
set of cancer patient is defined as {x, y}, where x ∈ r h ×w ×(2+o) represents
the structure images, ""2"" signifies the ct image and the segmentation mask of
the ptv, and o denotes the total number of segmentation mask of oars. meanwhile,
y ∈ r h ×w ×1 is the corresponding dose distribution map for x. concretely, the
forward process produces a sequence of noisy images {y 0 , y 1 , . . . , y t },
y 0 = y by gradually adding a small amount of noise to y in t steps with the
noise increased at each step and a noise predictor f is constructed to predict
the noise added to y t-1 by treating y t , anatomic information from x and
embedding of step t as input. to obtain the anatomic information, a structure
encoder g is designed to extract the crucial feature representations from the
structure images. then, in the reverse process, the model progressively deduces
the dose distribution map by iteratively denoising from y t using the
well-trained noise predictor.",6
178,1530,DiffDP: Radiotherapy Dose Prediction via a Diffusion Model,3.0,Experiments and Results,"dataset and evaluations. we measure the performance of our model on an in-house
rectum cancer dataset which contains 130 patients who underwent volumetric
modulated arc therapy (vmat) treatment at west china hospital. concretely, for
every patient, the ct images, ptv segmentation, oars segmentations, and the
clinically planned dose distribution are included. additionally, there are four
oars of rectum cancer containing the bladder, femoral head r, femoral head l,
and small intestine. we randomly select 98 patients for model training, 10
patients for validation, and the remaining 22 patients for test. the thickness
of the cts is 3 mm and all the images are resized to the resolution of 256 × 256
before the training procedure.we measure the performance of our proposed model
with multiple metrics. considering dm represents the minimal absorbed dose
covering m% percentage volume of ptv, we involve d 98 , d 2 , maximum dose (d
max ), and mean dose (d mean ) as metrics. besides, the heterogeneity index (hi)
is used to quantify dose heterogeneity [26]. to quantify performance more
directly, we calculate the difference ( ) of these metrics between the ground
truth and the predicted results. more intuitively, we involve the dose volume
histogram (dvh) [27] as another essential metric of dose prediction performance.
when the dvh curves of the predictions are closer to the ground truth, we can
infer higher prediction accuracy.comparison with state-of-the-art methods. to
verify the superior accuracy of our proposed model, we select multiple
state-of-the-art (sota) models in dose prediction, containing unet (2017) [7],
gan (2018) [12], deeplabv3+ (2020) [11], c3d (2021) [9], and prunet (2022) [10],
for comparison. the quantitative comparison results are listed in table . 1
where our method outperforms the existing sotas in terms of all metrics.
specifically, compared with deeplabv3+ with the second-best accuracy in hi
(0.0448) and d 98 (0.0416), the results generated by the proposed are 0.0035 and
0.0014 lower, respectively. as for d 2 and d max , our method gains overwhelming
performance with 0.0008 and 0.0005, respectively. moreover, the paired t-test is
conducted to investigate the significance of the results. the p-values between
the proposed and other sotas are almost all less than 0.05, indicating that the
enhancement of performance is statistically meaningful.besides the quantitative
results, we also present the dvh curves derived by compared methods in fig. 3.
the results are compared on ptv as well as two oars: bladder and small
intestine. compared with other methods, the disparity between the dvh curves of
our method and the ground truth is the smallest, demonstrating the superior
performance of the proposed. furthermore, we display the visualization
comparison in fig. 4. as we can see, the proposed model achieves the best visual
quality with clearer and sharper high-frequency details (as indicated by red
arrows). furthermore, the error map of the proposed is the darkest, suggesting
the least disparity compared with the ground truth.ablation study. to study the
contributions of key components of the proposed method, we conduct the ablation
experiments by 1) removing the structure encoder from the proposed method and
concatenating the anatomical images x and noisy image y t together as the
original input for diffusion model (denoted as baseline); 2) the proposed diffdp
model. the quantitative results are given in table 2. we can clearly see the
performance for all metrics is enhanced with the structure encoder,
demonstrating its effectiveness in the proposed model.",6
179,1531,DiffDP: Radiotherapy Dose Prediction via a Diffusion Model,4.0,Conclusion,"in this paper, we introduce a novel diffusion-based dose prediction (diffdp)
model for predicting the radiotherapy dose distribution of cancer patients. the
proposed method involves a forward and a reverse process to generate accurate
prediction by progressively transferring the gaussian noise into a dose
distribution map. moreover, we propose a structure encoder to extract anatomical
information from patient anatomy images and enable the model to concentrate on
the dose constraints within several essential organs. extensive experiments on
an in-house dataset with 130 rectum cancer patients demonstrate the superiority
of our method.",6
186,1658,Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,1.0,Introduction,"cancers are a group of heterogeneous diseases reflecting deep interactions
between pathological and genomics variants in tumor tissue environments [24].
different cancer genotypes are translated into pathological phenotypes that
could be assessed by pathologists [24]. high-resolution pathological images have
proven their unique benefits for improving prognostic biomarkers prediction via
exploring the tissue microenvironmental features [1,10,12,13,18,25]. meanwhile,
genomics data (e.g., mrna-sequence) display a high relevance to regulate cancer
progression [3,29]. for instance, genome-wide molecular portraits are crucial
for cancer prognostic stratification and targeted therapy [16]. despite their
importance, seldom efforts jointly exploit the multimodal value between cancer
image morphology and molecular biomarkers. in a broader context, assessing
cancer prognosis is essentially a multimodal task in association with
pathological and genomics findings. therefore, synergizing multimodal data could
deepen a crossscale understanding towards improved patient prognostication.the
major goal of multimodal data learning is to extract complementary contextual
information across modalities [4]. supervised studies [5][6][7] have allowed
multimodal data fusion among image and non-image biomarkers. for instance, the
kronecker product is able to capture the interactions between wsis and genomic
features for survival outcome prediction [5,7]. alternatively, the coattention
transformer [6] could capture the genotype-phenotype interactions for prognostic
prediction. yet these supervised approaches are limited by feature
generalizability and have a high dependency on data labeling. to alleviate label
requirement, unsupervised learning evaluates the intrinsic similarity among
multimodal representations for data fusion. for example, integrating image,
genomics, and clinical information can be achieved via a predefined unsupervised
similarity evaluation [4]. to broaden the data utility, the study [28] leverages
the pathology and genomic knowledge from the teacher model to guide the
pathology-only student model for glioma grading. from these analyses, it is
increasingly recognized that the lack of flexibility on model finetuning limits
the data utility of multimodal learning. meanwhile, the size of multimodal
medical datasets is not as large as natural vision-language datasets, which
necessitates the need for data-efficient analytics to address the training
difficulty.to tackle above challenges, we propose a pathology-and-genomics
multimodal framework (i.e., pathomics) for survival prediction (fig. 1). we
summarized our contributions as follows. (1) unsupervised multimodal data
fusion. our unsupervised pretraining exploits the intrinsic interaction between
morphological and molecular biomarkers (fig. 1a). to overcome the gap of
modality heterogeneity between images and genomics, we project the multimodal
embeddings into the same latent space by evaluating the similarity among them.
particularly, the pretrained model offers a unique means by using
similarity-guided modality fusion for extracting cross-modal patterns. (2)
flexible modality finetuning. a key contribution of our multimodal framework is
that it combines benefits from both unsupervised pretraining and supervised
finetuning data fusion (fig. 1b). as a result, the task-specific finetuning
broadens the dataset usage (fig 1b andc), which is not limited by data modality
(e.g., both singleand multi-modal data). (3) data efficiency with limited data
size. our approach could achieve comparable performance even with fewer
finetuned data (e.g., only use 50% of the finetuned data) when compared with
using the entire finetuning dataset.",6
187,1659,Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,2.0,Methodology,"overview. figure 1 illustrates our multimodal transformer framework. our method
includes an unsupervised multimodal data fusion pretraining and a supervised
flexible-modal finetuning. from fig. 1a, in the pretraining, our unsupervised
data fusion aims to capture the interaction pattern of image and genomics
features. overall, we formulate the objective of multimodal feature learning by
converting image patches and tabular genomics data into groupwise embeddings,
and then extracting multimodal patient-wise embeddings. more specifically, we
construct group-wise representations for both image and genomics modalities. for
image feature representation, we randomly divide image patches into groups;
meanwhile, for each type of genomics data, we construct groups of genes
depending on their clinical relevance [22]. next, as seen in fig. 1b andc, our
approach enables three types of finetuning modal modes (i.e., multimodal,
image-only, and genomics-only) towards prognostic prediction, expanding the
downstream data utility from the pretrained model. group-wise image and genomics
embedding. we define the group-wise genomics representation by referring to n =
8 major functional groups obtained from [22]. each group contains a list of
well-defined molecular features related to cancer biology, including
transcription factors, tumor suppression, cytokines and growth factors, cell
differentiation markers, homeodomain proteins, translocated cancer genes, and
protein kinases. the group-wise genomics representation is defined as g n ∈ r
1×dg , where n ∈ n , d g is the attribute dimension in each group which could be
various. to better extract high-dimensional group-wise genomics representation,
we use a self-normalizing network (snn) together with scaled exponential linear
units (selu) and alpha dropout for feature extraction to generate the group-wise
embedding g n ∈ r 1×256 for each group.for group-wise wsis representation, we
first cropped all tissue-region image tiles from the entire wsi and extracted
cnn-based (e.g., resnet50) d idimensional features for each image tile k as h k
∈ r 1×di , where d i = 1, 024, k ∈ k and k is the number of image patches. we
construct the group-wise wsis representation by randomly splitting image tile
features into n groups (i.e., the same number as genomics categories).
therefore, group-wise image representation could be defined as i n ∈ r kn×1024 ,
where n ∈ n and k n represents tile k in group n. then we apply an
attention-based refiner (abr) [17], which is able to weight the feature
embeddings in the group, together with a dimension deduction (e.g.,
fully-connected layers) to achieve the group-wise embedding. the abr and the
group-wise embedding i n ∈ r 1×256 are defined as:where w,v1 and v2 are the
learnable parameters.patient-wise multimodal feature embedding. to aggregate
patient-wise multimodal feature embedding from the group-wise representations,
as shown in fig. 1a, we propose a pathology-and-genomics multimodal model
containing two model streams, including a pathological image and a genomics data
stream.in each stream, we use the same architecture with different weights,
which is updated separately in each modality stream. in the pathological image
stream, the patient-wise image representation is aggregated by n group
representations as, where p ∈ p and p is the number of patients. similarly, the
patient-wise genomics representation is aggregated as g p ∈ r n ×256 . after
generating patient-wise representation, we utilize two transformer layers [27]
to extract feature embeddings for each modality as follows:where msa denotes
multi-head self-attention [27] (see appendix 1), l denotes the layer index of
the transformer, and h p could either be i p or g p . then, we construct global
attention poolings [17] as eq. 1 to adaptively compute a weighted sum of each
modality feature embeddings to finally construct patientwise embedding as i p
embedding ∈ r 1×256 and g p embedding ∈ r 1×256 in each modality.multimodal
fusion in pretraining and finetuning. due to the domain gap between image and
molecular feature heterogeneity, a proper design of multimodal fusion is crucial
to advance integrative analysis. in the pretraining stage, we develop an
unsupervised data fusion strategy by decreasing the mean square error (mse) loss
to map images and genomics embeddings into the same space. ideally, the image
and genomics embeddings belonging to the same patient should have a higher
relevance between each other. mse measures the average squared difference
between multimodal embeddings. in this way, the pretrained model is trained to
map the paired image and genomics embeddings to be closer in the latent space,
leading to strengthen the interaction between different modalities.in the single
modality finetuning, even if we use image-only data, the model is able to
produce genomic-related image feature embedding due to the multimodal knowledge
aggregation already obtained from the model pretraining. as a result, our
cross-modal information aggregation relaxes the modality requirement in the
finetuning stage. as shown in fig. 1b, for multimodal finetuning, we deploy a
concatenation layer to obtain the fused multimodal feature representation and
implement a risk classifier (fc layer) to achieve the final survival
stratification (see appendix 2). as for single-modality finetuning mode in fig.
1c, we simply feed i p embedding or g p embedding into risk classifier for the
final prognosis prediction. during the finetuning, we update the model
parameters using a log-likelihood loss for the discrete-time survival model
training [6](see appendix 2).",6
188,1660,Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,3.0,Experiments and Results,"datasets. all image and genomics data are publicly available. we collected wsis
from the cancer genome atlas colon adenocarcinoma (tcga-coad) dataset
(cc-by-3.0) [8,21] and rectum adenocarcinoma (tcga-read) dataset (cc-by-3.0)
[8,20], which contain 440 and 153 patients. we cropped each wsi into 512 × 512
non-overlapped patches. we also collected the corresponding tabular genomics
data (e.g., mrna sequence, copy number alteration, and methylation) with overall
survival (os) times and censorship statuses from cbioportal [2,14]. we removed
the samples without the corresponding genomics data or ground truth of survival
outcomes. finally, we included 426 patients of tcga-coad and 145 patients of
tcga-read.experimental settings and implementations. we implement two types of
settings that involve internal and external datasets for model pretraining and
finetuning. as shown in fig 2a, we pretrain and finetune the model on the same
dataset (i.e., internal setting). we split tcga-coad into training (80%) and
holdout testing set (20%). then, we implement four-fold cross-validation on the
training set for pretraining, finetuning, and hyperparameter-tuning. the test
set is only used for evaluating the best finetuned models from each
cross-validation split. for the external setting, we implement pretraining and
finetuning on the different datasets, as shown in fig 2b ; we use tcga-coad for
pretraining; then, we only use tcga-read for finetuning and final evaluation. we
implement a five-fold cross-validation for pretraining, and the best pretrained
models are used for finetuning. we split tcga-read into finetuning (60%),
validation (20%), and evaluation set (20%). for all experiments, we calculate
the average performance on the evaluation set across the best models.the number
of epochs for pretraining and finetuning are 25, the batch size is 1, the
optimizer is adam [19], and the learning rate is 1e-4 for pretraining and 5e-5
for finetuning. we used one 32gb tesla v100 sxm2 gpu and pytorch. the
concordance index (c-index) is used to measure the survival prediction
performance. we followed the previous studies [5][6][7] to partition the overall
survival (os) months into four non-overlapping intervals by using the quartiles
of event times of uncensored patients for discretized-survival c-index
calculation (see appendix 2). for each experiment, we reported the average
c-index among three-times repeated experiments. conceptionally, our method
shares a similar idea to multiple instance learning (mil) [9,23]. therefore, we
include two types of baseline models, including the mil-based models (deepset
[30], ab-mil [17], and transmil [26]) and mil multimodal-based models (mcat [6],
porpoise [7]). we follow the same data split and processing, as well as the
identical training hyperparameters and supervised fusion as above. notably,
there is no need for supervised finetuning for the baselines when using
tcga-coad (table 1), because the supervised pretraining is already applied to
the training set.",6
189,1661,Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,0.0,Results.,"in table 1, our approach shows improved survival prediction performance on both
tcga-coad and tcga-read datasets. compared with supervised baselines, our
unsupervised data fusion is able to extract the phenotype-genotype interaction
features, leading to achieving a flexible finetuning for different data
settings. with the multimodal pretraining and finetuning, our method outperforms
state-of-the-art models by about 2% on tcga-coad and 4% tcga-read. we recognize
that the combination of image and mrna sequencing data leads to reflecting
distinguishing survival outcomes. remarkably, our model achieved positive
results even using a single-modal finetuning when compared with baselines (more
results in appendix 3.1). in the meantime, on the tcga-read, our single-modality
finetuned model achieves a better performance than multimodal finetuned baseline
models (e.g., with model pretraining via image and methylation data, we have
only used the image data for finetuning and achieved a c-index of 74.85%, which
is about 4% higher than the best baseline models). we show that with a
single-modal finetuning strategy, the model could generate meaningful embedding
to combine image-and genomicrelated patterns. in addition, our model reflects
its efficiency on the limited finetuning data (e.g., 75 patients are used for
finetuning on tcga-read, which are only 22% of tcga-coad finetuning data). in
table 1, our method could yield better performance compared with baselines on
the small dataset across the combination of images and multiple types of
genomics data. approach broadens the scope of dataset inclusion, particularly
for model finetuning and evaluation, while enhancing model efficiency on
analyzing multimodal clinical data in real-world settings. in addition, the use
of synthetic data and developing a foundation model training will be helpful to
improve the robustness of multimodal data fusion [11,15].",6
190,1662,Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,0.0,Supplementary Information,"the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43987-2 60. ablation analysis. we verify the
model efficiency by using fewer amounts of finetuning data in finetuning. for
tcga-coad dataset, we include 50%, 25%, and 10% of the finetuning data. for the
tcga-read dataset, as the number of uncensored patients is limited, we use 75%,
50%, and 25% of the finetuning data to allow at least one uncensored patient to
be included for finetuning. as shown in fig. 3a, by using 50% of tcga-coad
finetuning data, our approach achieves the c-index of 64.80%, which is higher
than the average performance of baselines in several modalities. similarly, in
fig. 3b, our model retains a good performance by using 50% or 75% of tcga-read
finetuning data compared with the average of c-index across baselines (e.g.,
72.32% versus 64.23%). for evaluating the effect of cross-modality information
extraction in the pretraining, we kept supervised model training (i.e., the
finetuning stage) while removing the unsupervised pretraining. the performance
is lower 2%-10% than ours on multi-and single-modality data. for evaluating the
genomics data usage, we designed two settings: (1) combining all types of
genomics data and categorizing them by groups; (2) removing category information
while keeping using different types of genomics data separately. our approach
outperforms the above ablation studies by 3%-7% on tcga-read and performs
similarly on tcga-coad. in addition, we replaced our unsupervised loss with
cosine similarity loss; our approach outperforms the setting of using cosine
similarity loss by 3%-6%.",6
191,1663,Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,4.0,Conclusion,"developing data-efficient multimodal learning is crucial to advance the survival
assessment of cancer patients in a variety of clinical data scenarios. we
demonstrated that the proposed pathomics framework is useful for improving the
survival prediction of colon and rectum cancer patients. importantly, our
approach opens up perspectives for exploring the key insights of intrinsic
genotypephenotype interactions in complex cancer data across modalities. our
finetuning",6
199,1696,Gene-Induced Multimodal Pre-training for Image-Omic Classification,1.0,Introduction,"pathological image-omic analysis is the cornerstone of modern medicine and
demonstrates promise in a variety of different tasks such as cancer diagnosis
and prognosis [12]. with the recent advance of digital pathology and sequencing
technologies, modern cancer screening has jointly incorporated genomics and
histology analysis of whole slide images (wsis).though deep learning techniques
have revolutionized medical imaging, designing a task-specific algorithm for
image-omic multi-modality analysis is challenging. (1) the gigapixel wsis, which
generally yield 15,000 foreground patches during pre-processing, make
attention-based backbones [6] hard to extract precise image (wsi)-level
representations. (2) learning features from genomics data which have tens of
thousands of genes make models such as transformer [16] impractical to use due
to its quadratic computation complexity. (3) image-omic feature fusion [2,3] may
fail to model high-order relevance and the inherent structural characteristics
of each modality, making the fusion less effective.specifically, to our
knowledge, most multi-modality techniques have been designed for modalities such
as chest x-ray and reports [1,17,23], ct and x-ray [18], ct and mri [21], h&e
cross-staining [22] via global feature, local feature or multi-granularity
alignment. but, none of these works considers the challenges in wsis and genes
processing. besides, vision-language models in the computer vision community
stand out for their remarkable versatility [13,14]. nevertheless, constrained by
computing resources, the most commonly used multimodal representation learning
strategy, contrastive learning, which relies on a large number of negative
samples to avoid model collapse [8], is not affordable for gigapixel wsis
analysis. a big domain gap also hampers their usage in leveraging the structural
characteristic of tumor micro-environment and genomic assay. recently, the
literature corpus has proposed some methods for accomplishing specific
image-omic tasks via kronecker product fusion [2] or co-attention mapping
between wsis and genomics data [3]. but, the kronecker product overly concerns
feature interactions between modalities while ignoring high-order relevance,
w.r.t. decision boundaries across multiple samples, which is critical to
classification tasks. as for the co-attention module, it is unidirectional and
cannot localize significant regions from genetic data with a large amount of
information.in this paper, we propose a task-specific framework dubbed
gene-induced multimodal pre-training (gimp) for image-omic classification.
concretely, we first propose a transformer-based gene encoder, group multi-head
self attention (groupmsa), to capture global structured features in gene
expression cohorts. next, we design a pre-training paradigm for wsis, masked
patch modeling (mpm), masking random patch embeddings from a fixed-length
contiguous subsequence of a wsi. we assume that one patch-level feature
embedding can be reconstructed by its adjacent patches, and this process
enhances the learning ability for pathological characteristics of different
tissues. our mpm only needs to recover the masked patch embeddings in a
fixed-length subsequence rather than processing all patches from wsis.
furthermore, to model the high-order relevance of the two modalities, we combine
cls tokens of paired image and genomic data to form unified representations and
propose a triplet learning module to differentiate patient-level positive and
negative samples in a mini-batch. it is worth mentioning that although our
unified representation fuses features from the whole gene expression cohort and
partial wsis in a mini-batch, we can still learn high-order relevance and
discriminative patient-level information between these two modalities in
pre-training thanks to the triplet learning module. in addition, note that our
proposed method is different from self-supervised pre-training. specifically, we
focus not only on superior representation learning capability, but also
category-related feature distributions, w.r.t. intra-and inter-class variation.
with the training process going on, complete information from wsis can be
integrated and the fused multimodal representations with high discrimination
will make it easier for the classifier to find the classification hyperplane.
experimental results demonstrate that our gimp achieves significant improvement
in accuracy than other image-omic competitors, and our multimodal framework
shows competitive performance even without pre-training.",6
200,1700,Gene-Induced Multimodal Pre-training for Image-Omic Classification,2.3,Gene-Induced Multimodal Fusion,"in this section, we first describe the formulation of masked patch modeling.
then we introduce the overall pipeline of our pre-training framework and
illustrate how to apply it to downstream classification tasks.masked patch
modeling. in wsis, the foreground patches are spatially contiguous, which means
the adjacent patches have similar feature embeddings. thus, we propose a masked
patch modeling (mpm) pre-training strategy that masks random patch embeddings
from a fixed-length contiguous subsequencej=i in h p and reconstruct the
invisible information. the fixed subsequence length l is empirically set to
6,000 and the sequences shorter than l are duplicated to build mini batches.
besides, the masking ratio is set to 50% and the set of masked subscripts is
denoted as m ∈ r 0.5l . next, a two-layer nystrom-based patch aggregator
followed by a lightweight reconstruction decoder are adopted to process the
masked sequence h mpm and the reconstructed sequence is denoted as. note that we
reconstruct the missing feature embeddings rather than the raw pixels of the
masked areas, which is different from traditional mim methods like simmim [19]
and mae [5]. in this way, the model could consider latent pathological
characteristics of different tissues, which makes the pretext task more
challenging. the reconstruction l 1 loss is computed by:where 1[•] is the
indicator function.gene-induced triplet learning. the transformer-based
backbones in the classification task require the cls token to be able to extract
accurate global information, which is even more important yet difficult in wsis
due to the long sequence challenge. in addition, in order to construct the
mini-batch, the subsequences we intercept in the mpm pre-training phase may not
be sufficiently representative of the image-level characteristics. to overcome
these issues, we further propose a gene-induced triplet learning module, which
uses pathological images and genomic data as input and extracts high-order and
discriminative features via cls tokens. firstly, we pre-train the groupmsa
module by patientlevel annotations in advance and froze it in the following
iterations. next, a learnable cls token cls img for wsis is added to the input
masked sequence h mpm . after extracting the input patch embeddings and gene
sequence separately, we concatenate cls img and cls ge as cls pat ∈ r 2d to
represent patient-level characteristics.suppose we obtain a triplet list {x, x +
, x -} during current iteration, where x, x + , x -are concatenated tokens of
anchor cls pat , positive cls pat , and negative cls pat , respectively. to
enhance the global modeling capability, i.e., extracting more precise
patient-level features, we expect that the distance between the anchor and the
positive sample gets closer, while the negative sample is farther away. the loss
function for optimizing triplet learning is computed by:δ indicates a threshold,
e.g., δ = 0.8. finally, the loss function for gimp pretraining is:multimodal
fine-tuning. applying the pre-trained backbone to image-omic classification task
is straightforward, since gimp pre-training allows it to learn representative
patient-level features. we use a simple multi-layer perceptron (mlp) head to map
cls pat to the final class predictions p , which can be written as p =
softmax(mlp(cls pat )).3 experiments",6
201,1701,Gene-Induced Multimodal Pre-training for Image-Omic Classification,3.1,Experimental Setup,"datasets. we verify the effectiveness of our method on the caner genome atlas
(tcga) non-small cell lung cancer (nsclc) dataset, which contains two cancer
subtypes, i.e., lung squamous cell carcinoma (lusc) and lung adenocarcinoma
(luad). after pre-processing [11], the patch number extracted from wsis at 20×
magnification varies from 485 to 148,569. we collect corresponding rna-seq fpkm
data for each patient and the length of the input genomic sequence is 60,480.
among 946 image-omic pairs, 470 of them belong to luad and 476 cases are lusc.
we randomly split the data into 567 for training, 189 for validation and 190 for
testing.implementation details. the pre-training process of all algorithms is
conducted on the training set, without any extra data augmentation. note that
our genetic encoder, groupmsa, is fully supervised pre-trained on unimodal
genetic data to accelerate convergence and it is frozen during gimp training
process. the maximum pre-training epoch for all methods is set to 100 and we
finetune the models at the last epoch. during fine-tuning, we evaluate the model
on the validation set after every epoch, and save the parameters when it
performs the best. adamw [10] is used as our optimizer and the learning rate is
10 -4 with cosine decline strategy. the maximum number of fine-tune epoch is 70.
at last, we measure the performance on the test set. training configurations are
consistent throughout the fine-tuning process to ensure fair comparisons. all
experiments are conducted on a single nvidia geforce rtx 3090.",6
202,1702,Gene-Induced Multimodal Pre-training for Image-Omic Classification,3.2,Comparison Between GiMP and Other Methods,"we conduct comparisons between gimp and three competitors under different
settings. firstly, we compare our proposed patch aggregator with the current
state-of-the-art deep mil models on unimodal tcga-nsclc dataset, i.e., only
pathological wsis are included as input. as shown in table 1, our proposed patch
aggregator outperforms all the compared attention based multiple instance
learning baselines in classification accuracy. in particular, 1.6% higher than
the abmil [6] 0.7737 dsmil [9] 0.7566 clam-sb [11] 0.8519 clam-mb [11] 0.8889
transmil [15] 0.8836 pathology w/o pre-train gimp (w/o groupmsa) 0.8995 porpoise
[4] 0.9524 pathomic fusion [2] 0.9684 mcat [3] 0.9632 w/o pre-train gimp (ours)
0.9737 mgca [17] 0.9105 biovil [1] 0.9316 refers [23] 0 second best compared
method transmil [15]. we then explore the superiority of gimp by comparing to
state-of-the-art medical multi-modal approaches. we particularly compare our
method to biovil [1], mgca [17] and refers [23], three popular multimodal
pre-training algorithms in medical text-image classification task. we can
observe in the table that, our gimp raises acc from 91.05% to 99.47% on
tcga-nsclc dataset. even without pre-training stage, gimp shows competitive
performance compared to porpoise [4], pathomic fusion [2], and mcat [3], three
influential image-omic classification architectures. we further explore why gimp
works by insightful interpretation of the proposed method with t-sne
visualisation. figure 2 shows the feature mixtureness of pre-trained cls pat
extracting global information on training set. compari- son between fig. 2 (a)
and (b) indicates that the addition of the genomic data is indispensable in
increasing the inter-class distance and reducing the intra-class distance, which
confirms our motivation that gene-induced multimodal fusion could model
high-order relevance and yield more discriminative representations. moreover,
compared to the mentioned self-supervised methods biovil [1] and mgca [17] in
fig. 2 (c) and (d), cls pat with gimp pre-trained are well separated between
luad and lusc, i.e., gimp pays more attention to the categoryrelated feature
distribution and could extract more discriminative patient-level features during
triplet learning.",6
203,1703,Gene-Induced Multimodal Pre-training for Image-Omic Classification,3.3,Ablation Study,"table 2 summarizes the results of ablation study. we first evaluate the
effectiveness of the proposed groupmsa. in the first two rows, groupmsa achieves
0.53% improvement compared to snn [7], a popular genetic encoders used in
porpoise [4] and pathomic fusion [2]. we then analyze the effect of adding
genetic modality during pre-training. the evaluation protocol is first
pre-training, and then fine-tuning on downstream multimodal classification task.
""aggregator + mpm"" means gimp only uses wsis as input and reconstructs the
missing patch embeddings during the pre-training phase. since the fixed
subsequence length l = 6000 is used in our setting, it is sometimes smaller than
the original patch number, e.g., the maximum size 148,569, the pre-trained model
without genetic guidance may be not aware of sufficiently accurate patientlevel
characteristics, i.e., ineffectively focused on normal tissues. ""aggregator +
triplet"" indicates using unimodal image features to build triplets. we can
likewise find that the lack of precise global representation leads to worse
performance. finally, we evaluate the necessity of the mpm module. ""aggregator +
groupmsa + triplet"" means gimp only combines the cls tokens of each modality and
calculates triplet loss during pre-training. we can observe a performance drop
without mpm module, e.g., from 99.47% to 95.26%, which demonstrates that local
pathological information is equally critical as high-order relevance.",6
204,1710,Histopathology Image Classification Using Deep Manifold Contrastive Learning,3.1,Datasets,"we tested our proposed method on two different tasks: (1) intrahepatic
cholangiocarcinomas(ihccs) subtype classification and (2) liver cancer type
classification. the dataset for the former task was collected from 168 patients
with 332 wsis from seoul national university hospital. ihccs can be further
categorized into small duct type (sdt) and large duct type (ldt). using gene
mutation information as prior knowledge, we collected wsis with wild kras and
mutated idh genes for use as training samples in sdt, and wsis with mutated kras
and wild idh genes for use in ldt. the rest of the wsis were used as testing
samples. the liver cancer dataset for the latter task was composed of 323 wsis,
in which the wsis can be further classified into hepatocellular carcinomas
(hccs) (collected from pathology ai platform [1]) and ihccs. we collected 121
wsis for the training set, and the remaining wsis were used as the testing set.",6
205,1716,Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,1.0,Introduction,"prostate cancer (pca) diagnosis and grading rely on histopathology analysis of
biopsy slides [1]. however, prostate biopsies are known to have sampling error
as pca is heterogenous and commonly multifocal, meaning cancer legions can be
missed during the biopsy procedure [2]. if significant pca is detected on
biopsies and the patient has organ-confined cancer with no contraindications,
radical prostatectomy (rp) is the standard of care [3,4]. following rp, the
prostate is processed and slices are mounted onto slides for analysis. radical
prostatectomy histopathology samples are essential for validating the
biopsydetermined grade group [5,6]. analysis of whole-mount slides, meaning
slides that include slices of the entire prostate, provide more precise tumor
boundary detection, identification of various tumor foci, and increased tissue
for identifying morphological patterns not visible on biopsy due to a larger
field of view.field effect refers to the spread of genetic and epigenetic
alterations from a primary tumor site to surrounding normal tissues, leading to
the formation of secondary tumors. understanding field effect is essential for
cancer research as it provides insights into the mechanisms underlying tumor
development and progression. tumor-associated stroma, which consists of various
cell types, such as fibroblasts, smooth muscle cells, and nerve cells, is an
integral component of the tumor microenvironment that plays a critical role in
tumor development and progression. reactive stroma, a distinct phenotype of
stromal cells, arises in response to signaling pathways from cancerous cells and
is characterized by altered stromal cells and increased extracellular matrix
components [7,8]. reactive stroma is often associated with tumor-associated
stroma and is thought to be a result of field effects in prostate cancer.
altered stroma can create a pro-tumorigenic environment by producing a multitude
of chemokines, growth factors, and releasing reactive oxygen species [9,10],
which can lead to tumor development and aggressiveness [11]. therefore,
investigating the histological characterization of tumor-associated stroma is
crucial in gaining insights into the field effect and tumor progression of
prostate cancer.manual review for tumor-associated stroma is time-consuming and
lacks quantitative metrics [12,13]. several automated methods have been applied
to analyze the tumor-stroma relationship; however, most of them focus on
identifying a tumor-stroma ratio rather than finding reactive stroma tissue or
require pathologist input. machine learning algorithms have been used to
quantify the percentage of tumor to stroma in bladder cancer patients, but
required dichotomizing patients based on a threshold [14]. software has been
used to segment tumor and stroma tissue in breast cancer patient samples, but
the method required constant supervision by a pathologist [15]. similarly, akoya
biosciences inform software was used to quantify reactive stroma in pca, but
this method required substantial pathologist input to train the software [16].
fully automated deep-learning methods have been developed to identify
tumor-associated stroma in breast cancer biopsies, achieving an auc of 0.962 in
predicting invasive ductal cancer [13]. however, identifying tumor-associated
stroma in prostate biopsies and whole-mount histopathology slides remains
challenging.analyzing tumor-associated stroma in prostate cancer requires
combining whole-mount and biopsy histopathology slides. biopsy slides provide
information on the presence of pca, while whole-mount slides provide information
on the extent and distribution of pca, including more information on
tumor-associated stroma. combining the information from both modalities can
provide a more accurate understanding of the tumor microenvironment. in this
work, we explore the field effect in prostate cancer by analyzing
tumor-associated stroma in multimodal histopathological images. our main
contributions can be summarized as follows:-to the best of our knowledge, we
present the first deep-learning approach to characterize prostate
tumor-associated stroma by integrating histological image analysis from both
whole-mount and biopsy slides. our research offers a promising computational
framework for in-depth exploration of the field effect and cancer progression in
prostate cancer. -we proposed a novel approach for stroma classification with
spatial graphs modeling, which enable more accurate and efficient analysis of
tumor microenvironment in prostate cancer pathology. given the spatial nature of
cancer field effect and tumor microenvironment, our graph-based method offers
valuable insights into stroma region analysis. -we developed a comprehensive
pipeline for constructing tumor-associated stroma datasets across multiple data
sources, and employed adversarial training and neighborhood consistency
regularization techniques to learn robust multimodal-invariant image
representations.",6
206,1721,Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,3.1,Dataset,"in our study, we utilized three datasets for tumor-associated stroma
analysis.(1) dataset a comprises 513 tiles extracted from the whole mount slides
of 40 patients, sourced from the archives of the pathology department at
cedars-sinai medical center (irb# pro00029960). it combines two sets of tiles:
224 images from 20 patients featuring stroma, normal glands, low-grade and
highgrade cancer [22], along with 289 images from 20 patients with dense
high-grade cancer (gleason grades 4 and 5) and cribriform/non-cribriform glands
[23]. each tile measures 1200×1200 pixels and is extracted from whole slide
images captured at 20x magnification (0.5 microns per pixel). the tiles were
annotated at the pixel-level by expert pathologists to generate stroma tissue
segmentation masks and were cross-evaluated and normalized to account for stain
variability.(2) dataset b included 97 whole mount slides with an average size of
over 174,000×142,000 pixels at 40x magnification. the prostate tissue within
these slides had an average tumor area proportion of 9%, with an average tumor
area of 77 square mm. an expert pathologist annotated the tumor region
boundaries at the region-level, providing exhaustive annotations for all tumor
foci. (3) dataset c comprised 6134 negative biopsy slides obtained from 262
patients' biopsy procedures, where all samples were diagnosed as negative. these
slides are presumed to contain predominantly normal stroma tissues without
phenotypic alterations in response to cancer. dataset a was utilized for
training the stroma segmentation model. extensive data augmentation techniques,
such as image scaling and staining perturbation, were employed during the
training process. the model achieved an average test dice score of 95.57 ± 0.29
through 5-fold cross-validation. this model was then applied to generate stroma
masks for all slides in datasets b and c. to precisely isolate stroma tissues
and avoid data bleeding from epithelial tissues, we only extracted patches where
over 99.5% of the regions were identified as stroma at 40x magnification to
construct the stroma classification dataset.for positive tumor-associated stroma
patches, we sampled patches near tumor glands within annotated tumor region
boundaries, as we presumed that tumor regions represent zones in which the
greatest amount of damage has progressed. for negative stroma patches, we
calculated the tumor distance for each patch by measuring the euclidean distance
from the patch center to the nearest edge of the labeled tumor regions. negative
stroma patches were then sampled from whole mount slides with a gleason group
smaller than 3 and a tumor distance larger than 5 mm. this approach aims to
minimize the risk of mislabeling tumor-associated stroma as normal tissue.
setting a 5mm threshold accounts for the typically minimal inflammatory
responses induced by prostate cancers, particularly in lower-grade cases. to
incorporate multi-modal information, we randomly sampled negative stroma patches
from all biopsy slides in dataset c. overall, we selected over 1.1 million
stroma patches of size 256×256 pixels at 40x magnification for experiments.
during model training and testing, we performed stain normalization and standard
image augmentation methods.",6
207,1723,Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,4.0,Results and Discussions,"in conclusion, our study introduced a deep learning approach to accurately
characterize the tumor-associated stroma in multi-modal prostate histopathology
slides. our experimental results demonstrate the feasibility of using deep
learning algorithms to identify and quantify subtle stromal alterations,
offering a promising tool for discovering new diagnostic and prognostic
biomarkers of prostate cancer. through exploring field effect in prostate
cancer, our work provides a computational system for further analysis of tumor
development and progression. future research can focus on validating our
approach on larger and more diverse datasets and expanding the method to a
patient-level prediction system, ultimately improving prostate cancer diagnosis
and treatment.",6
211,1761,Thinking Like Sonographers: A Deep CNN Model for Diagnosing Gout from Musculoskeletal Ultrasound,1.0,Introduction,"gout is the most common inflammatory arthritis and musculoskeletal ultrasound
(mskus) scanning is recommended to diagnose gout due to the non-ionizing
radiation, fast imaging speed, and non-invasive characteristics of mskus [7].
however, misdiagnosis of gout can occur frequently when a patient's clinical
characteristics are atypical. traditional mskus diagnosis relies on the
experience of the radiologist which is time-consuming and labor-intensive.
although convolutional neural networks (cnns) based ultrasound classification
models have been successfully used for diseases such as thyroid nodules and
breast cancer, conspicuously absent from these successful applications is the
use of cnns for gout diagnosis from mskus images. there are significant
challenges in cnn based gout diagnosis. firstly, the gout-characteristics
contain various types including double contour sign, synovial hypertrophy,
synovial effusion, synovial dislocation and bone erosion, and these
gout-characteristics are small and difficult to localize in mskus. secondly, the
surrounding fascial tissues such as the muscle, sarcolemma and articular capsule
have similar visual traits with gout-characteristics, and we found the existing
cnn models can't accurately pay attention to the gout-characteristics that
radiologist doctors pay attention to during the diagnosis process (as shown in
fig. 1). due to these issues, sota cnn models often fail to learn the gouty
mskus features which are key factors for sonographers' decision.in medical image
analysis, recent works have attempted to inject the recorded gaze information of
clinicians into deep cnn models for helping the models to predict correctly
based on lesion area. mall et al. [9,10] modeled the visual search behavior of
radiologists for breast cancer using cnn and injected human visual attention
into cnn to detect missing cancer in mammography. wang et al. [15] demonstrated
that the eye movement of radiologists can be a new supervision form to train the
cnn model. cai et al. [3,4] developed the sononet [1] model, which integrates
eye-gaze data of sonographers and used generative adversarial networks to
address the lack of eye-gaze data. patra et al. [11] proposed the use of a
teacher-student knowledge transfer framework for us image analysis, which
combines doctor's eye-gaze data with us images as input to a large teacher
model, whose outputs and intermediate feature maps are used to condition a
student model. although these methods have led to promising results, they can be
difficult to implement due to the need to collect doctors' eye movement data for
each image, along with certain restrictions on the network structure.different
from the existing studies, we propose a novel framework to adjust the general
cnns to ""think like sonographers"" from three different levels. (1) where to
adjust: modeling sonographers' gaze map to emphasize the region that needs
adjust; (2) what to adjust: classify the instances to systemically detect
predictions made based on unreasonable/biased reasoning and adjust; (3) how to
adjust: developing a training mechanism to strike the balance between gout
prediction accuracy and attention reasonability.",6
215,1793,CTFlow: Mitigating Effects of Computed Tomography Acquisition and Reconstruction with Normalizing Flows,2.1,Datasets,"this study used two unique datasets: (1) the ucla low-dose chest ct dataset, a
collection of 186 exams acquired using siemens ct scanners at an equivalent dose
of 2 mgy following an institutional review board-approved protocol. the raw
projection data of scans were exported, and poisson noise was introduced, as
described in zabic et al. [15], at levels equivalent to 10% of the original
dose. projection data were then reconstructed into an image size of 512 × 512
using three reconstruction kernels (smooth, medium, sharp) at 1.0 mm slice
thickness. the dataset was split into 80 scans for training, 20 for validation,
and 86 for testing. (2) aapm-mayo clinic low-dose ct ""grand challenge"" dataset,
a publicly available grand challenge dataset consisting of 5,936 abdominal ct
images from 10 patient cases reconstructed at 1.0 mm slice thickness. each case
consists of a paired 100% ""normal dose"" scan and a simulated 25% ""low dose""
scan. images from eight patient cases were used for training, and two cases were
reserved for validation. all images were randomly cropped into patches of 128 ×
128 pixels, generated from regions containing the body. this dataset was only
used for evaluating image quality against other harmonization techniques.",7
216,1820,DisAsymNet: Disentanglement of Asymmetrical Abnormality on Bilateral Mammograms Using Self-adversarial Learning,3.1,Datasets,"this study reports experiments on four mammography datasets. the inbreast
dataset [7] consists of 115 exams with bi-rads labels and pixel-wise
anno-tations, comprising a total of 87 normal (bi-rads = 1) and 342 abnormal
(bi-rads = 1) images. the ddsm dataset [3] consists of 2,620 cases, encompassing
6,406 normal and 4,042 (benign and malignant) images with outlines generated by
an experienced mammographer. the vindr-mammo dataset [8] includes 5,000 cases
with bi-rads assessments and bounding box annotations, consisting of 13,404
normal (bi-rads = 1) and 6,580 abnormal (bi-rads = 1) images. the in-house
dataset comprises 43,258 mammography exams from 10,670 women between 2004-2020,
collected from a hospital with irb approvals. in this study, we randomly select
20% women of the full dataset, comprising 6,000 normal (bi-rads = 1) and 28,732
abnormal (bi-rads = 1) images. due to a lack of annotations, the in-house
dataset is only utilized for classification tasks. each dataset is randomly
split into training, validation, and testing sets at the patient level in an
8:1:1 ratio, respectively (except for that inbreast which is split with a ratio
of 6:2:2, to keep enough normal samples for the test).table 1. comparison of
asymmetric and abnormal classification tasks on four mammogram datasets. we
report the auc results with 95% ci. note that, when ablating the ""asyc "", we
only drop the ""asyt"" and keep the encoders and classifiers.",7
217,1825,Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,1.0,Introduction,"over 430,000 new cases of renal cancer were reported in 2020 in the world [1]
and this number is expected to rise [22]. when the tumor size is large (greater
than 7 cm) often the whole kidney is removed, however, when the tumor size is
small (less than 4 cm), partial nephrectomy is the preferred treatment [20] as
it could preserve kidney's function. thus, early detection of kidney tumors can
help to improve patient's prognosis. however, early-stage renal cancers are
usually asymptomatic, therefore they are often incidentally found during other
examinations [19], which includes non-contrast ct (ncct) scans.segmentation of
kidney tumors on ncct images adds challenges compared to contrast-enhanced ct
(cect) images, due to low contrast and lack of multiphase images. on cect
images, the kidney tumors have different intensity values compared to the normal
tissues. there are several works that demonstrated successful segmentation of
kidney tumors with high precision [13,21]. however, on ncct images, as shown in
fig. 1b, some tumors called isodensity tumors, have similar intensity values to
the surrounding normal tissues. to detect such tumors, one must compare the
kidney shape with tumors to the kidney shape without the tumors so that one can
recognize regions with protuberance.3d u-net [3] is the go-to network for
segmenting kidney tumors on cect images. however, convolutional neural networks
(cnns) are biased towards texture features [5]. therefore, without any
intervention, they may fail to capture the protuberance caused by isodensity
tumors on ncct images.in this work, we present a novel framework that is capable
of capturing the protuberances in the kidneys. our goal is to segment kidney
tumors including isodensity types on ncct images. to achieve this goal, we
create a synthetic dataset, which has separate annotations for normal kidneys
and protruded regions, and train a segmentation network to separate the
protruded regions from the normal kidney regions. in order to segment whole
tumors, our framework consists of three networks. the first is a base network,
which extracts kidneys and an initial tumor region masks. the second
protuberance detection network receives the kidney region mask as its input and
predicts a protruded region mask. the last fusion network receives the initial
tumor mask and the protruded region mask to predict a final tumor mask. this
proposed framework enables a better segmentation of isodensity tumors and boosts
the performance of segmentation of kidney tumors on ncct images. the
contribution of this work is summarized as follows:1. present a pioneering work
for segmentation of kidney tumors on ncct images. 2. propose a novel framework
that explicitly captures protuberances in a kidney to enable a better
segmentation of tumors including isodensity types on ncct images. this framework
can be extended to other organs (e.g. adrenal gland, liver, pancreas). 3. verify
that the proposed framework achieves a higher dice score compared to the
standard 3d u-net using a publicly available dataset.",7
218,1826,Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,2.0,Related Work,"the release of two public ct image datasets with kidney and tumor masks from the
2019/2021 kidney and kidney tumor segmentation challenge [8] (kits19, kits21)
attracted researchers to develop various methods for segmentation.looking at the
top 3 teams from each challenge [6,11,13,17,21], all teams utilized 3d u-net [3]
or v-net [16], which bears a similar architecture. the winner of kits19 [13]
added residual blocks [7] to 3d u-net and predicted kidney and tumor regions
directly. however, the paper notes that modifying the architecture resulted in
only slight improvement. the other 5 teams took a similar approach to nnu-net's
coarse-to-fine cascaded network [12], where it predicts from a low-resolution
image in the first stage and then predicts kidneys and tumors from a
high-resolution image in the second stage. thus, although other attempts were
made, using 3d u-net is the go-to method for predicting kidneys and tumors. in
our work, we also make use of 3d u-net, but using this network alone fails to
learn some isodensity tumors. to overcome this issue, we developed a framework
that specifically incorporates protuberances in kidneys, allowing for an
effective segmentation of tumors on ncct images.in terms of focusing on
protruded regions in kidneys, our work is close to [14,15]. [14] developed a
computer-aided diagnosis system to detect exophytic kidney tumors on ncct images
using belief propagation and manifold diffusion to search for protuberances. an
exophytic tumor is located on the outer surface of the kidney that creates a
protrusion. while this method demonstrated high sensitivity (95%), its false
positives per patient remained high (15 false positives per patient). in our
work, we will not only segment protruded tumors but also other tumors as well.
the first base network is responsible for predicting kidney and tumor region
masks. our architecture is based on 3d u-net, which has an encoder-decoder style
architecture, with few modifications. to reduce the required size of gpu memory,
we only use the encoder that has only 16 channels at the first resolution, but
instead we make the architecture deeper by having 1 strided convolution and 4
max-pooling layers. in the decoder, we replace the up-convolution layers with a
bilinear up-sampling layer and a convolution layer. in addition, by only having
a single convolution layer instead of two in the original architecture at each
resolution, we keep the decoder relatively small. throughout this paper, we
refer this architecture as our 3d u-net.the second protuberance detection
network is the same as the base network except it starts from 8 channels instead
of 16. we train this network using synthetic datasets. the details of the
dataset and training procedures are described in sect. 3.2.the last fusion
network combines the outputs from the base network and the protuberance
detection network and makes the final tumor prediction. in detail, we perform a
summation of the initial tumor mask and the protruded region mask, and then
concatenate the result with the input image. this is the input of the last
fusion network, which also has the same architecture as the base network with an
exception of having two input channels. this fusion network do not just combine
the outputs but also is responsible for removing false positives from the base
network and the protuberance detection network.our combined three network is
fully differentiable, however, to train efficiently, we train the model in 3
steps.",7
220,1846,Anatomy-Informed Data Augmentation for Enhanced Prostate Cancer Detection,1.0,Introduction,"data augmentation (da) is a key factor in the success of deep neural networks
(dnn) as it artificially enlarges the training set to increase their
generalization ability as well as robustness [22]. it plays a crucial role in
medical image analysis [8] where annotated datasets are only available with
limited size. dnns have already successfully supported radiologists in the
interpretation of magnetic resonance images (mri) for prostate cancer (pca)
diagnosis [3]. however, the da scheme received less attention, despite its
potential to leverage the data characteristic and address overfitting as the
root of generalization problems.state-of-the-art approaches still rely on
simplistic spatial transformations, like translation, rotation, cropping, and
scaling by globally augmenting the mri sequences [12,20]. they exclude random
elastic deformations, which can change the lesion outline but might alter the
underlying label and thus produce counterproductive examples for training [22].
however, soft tissue deformations, which are currently missing from the da
schemes, are known to significantly affect the image morphology and therefore
play a critical role in accurate diagnosis [6].both lesion and prostate shape
geometrical appearance influence the clinical assessment of prostate
imaging-reporting and data system (pi-rads) [24]. the prostate constantly
undergoes soft tissue deformation dependent on muscle contractions, respiration,
and more importantly variable filling of the adjacent organs, namely the bladder
and the rectum. among these sources, the rectum has the largest influence on the
prostate and lesion shape variability due to its large motion [4] and the fact
that the majority of the lesions are located in the adjacent peripheral prostate
zone [1]. however, only one snapshot of all these functional states is captured
within each mri examination, and almost never will be exactly the same on any
repeat or subsequent examination. ignoring these deformations in the da scheme
can potentially limit model performance.model-driven transformations attempting
to simulate organ functions -like respiration, urinary excretion,
cardiovascular-and digestion mechanics -offer a high degree of diversity while
also providing realistic transformations. currently, the finite element method
(fem) is the standard for modeling biomechanics [13]. however, their computation
is overly complex [10] and therefore does not scale to on-the-fly da [7]. recent
motion models rely on dnns using either a fem model [15] or complex training
with population-based models [18]. motion models have not been integrated into
any deep learning framework as an online data augmentation yet, thereby leaving
the high potential of inducing applicationspecific knowledge into the training
procedure unexploited.in this work we propose an anatomy-informed spatial
augmentation, which leverages information from adjacent organs to mimic typical
deformations of the prostate. due to its lightweight computational requirements,
it can be easily integrated into common da frameworks. this technique allows us
to simulate different physiological states during the training and enrich our
dataset with a wider range of organ and lesion shapes. inducing this kind of
soft tissue deformation ultimately led to improved model performance in
patient-and lesion-level pca detection on an independent test set.",7
221,1848,Anatomy-Informed Data Augmentation for Enhanced Prostate Cancer Detection,2.2,Experimental Setting,"we evaluate our anatomy-informed da qualitatively as well as
quantitatively.first, we visually inspect whether our assumptions in sect. 2.1
regarding pelvic biomechanics resulted in realistic transformations. we apply
either our proposed transformation to the rectum or the bladder, random
deformable or no transformation in randomly selected exams and conduct a strict
turing test with clinicians having different levels of radiology expertise (a
freshly graduated clinician (c.e.) and resident radiologists (c.m., k.s.z.), 1.5
-3 years of experience in prostate mri) to determine if they can notice the
artificial deformation.finally, we quantify the effect of our proposed
transformation on the clinical task of patient-level pca diagnosis and
lesion-level pca detection. we derive the diagnosis through semantic
segmentation of the malignant lesions following previous studies
[5,11,12,20,21]. semantic segmentation provides interpretable predictions that
are sensitive to spatial transformations, making it appropriate for testing
spatial das. to compare the performance of the trained models to radiologists,
we calculate their performance using the clinical pi-rads scores and
histopathological ground truths. to consider clinically informative results, we
use the partial area under the receiver operating characteristic (pauroc) for
patient-level evaluation with the sensitivity threshold of 78.75%, which is 90%
of the sensitivity of radiologists for pi-rads ≥ 4. additionally, we calculate
the f 1 -score at the sensitivity of pi-rads ≥ 4. afterward, we evaluate model
performances on object-level using the free-response receiver operating
characteristic (froc) and the number of detections at the radiologists' lesion
level performance for pi-rads ≥ 4, at 0.32 average number of false positives per
scan. objects were derived by applying a threshold of 0.5 to the softmax outputs
followed by connected component analysis to identify connected regions in the
segmentation maps. predictions with an intersection over union of 0.1 with a
ground truth object were considered true positives. to systematically compare
the effect of our proposed anatomy-informed da with the commonly used settings,
we create three main da schemes:1. basic da setting of nnu-net [8], which is an
extensive augmentation pipeline containing simple spatial transformations,
namely translation, rotation and scaling. this setting is our reference da
scheme. 2. random deformable transformations as implemented in the nnu-net [8]
da pipeline extending the basic da scheme (1) to test its presence in the
medical domain. our hypothesis is that it will produce counterproductive
examples, resulting in inferior performance compared to our proposed da.3.
proposed anatomy-informed transformation in addition to the simple da scheme
(1). we define two variants of it: (a) deforming only the rectum, as rectal
distension has the highest influence among the organs on the shapes of the
prostate lesions [4]. (b) deforming the bladder in addition to the rectum, as
bladder deformations also have an influence on lesions, although smaller.",7
222,1849,Anatomy-Informed Data Augmentation for Enhanced Prostate Cancer Detection,2.3,Prostate MRI Data,"774 consecutive bi-parametric prostate mri examinations are included in this
study, which were acquired in-house during the clinical routine. the ethics
committee of the medical faculty heidelberg approved the study (s-164/2019) and
waived informed consent to enable analysis of a consecutive cohort. all
experiments were performed in accordance with the declaration of helsinki [2]
and relevant data privacy regulations. for every exam, pi-rads v2 [24]
interpretation was performed by a board-certified radiologist. every patient
underwent extended systematic and targeted mri trans-rectal ultrasound-fusion
transperineal biopsy. malignancy of the segmented lesions was determined from a
systematic-enhanced lesion ground-truth histopathological assessment, which has
demonstrated reliable ground-truth assessment with sensitivity comparable to
radical prostatectomy [17]. the samples were evaluated according to the
international society of urological pathology (isup) standards under the
supervision of a dedicated uropathologist. clinically significant prostate
cancer (cspca) was defined as isup grade 2 or higher. based on the biopsy
results, every cspca lesion was segmented on the t2-weighted sequences
retrospectively by multiple in-house investigators under the supervision of a
board-certified radiologist. in addition to the lesions, the rectum and the
bladder segmentations were automatically predicted by a model built upon nnu-net
[8] trained iteratively on an in-house cohort initially containing a small
portion of our cohort. multiple radiologists confirmed the quality of the
predicted segmentations.",7
223,1851,Anatomy-Informed Data Augmentation for Enhanced Prostate Cancer Detection,3.0,Results,"the anatomy-informed transformation produced highly realistic soft tissue
deformations. figure 2 shows an example of the transformation simulating rectum
distensions with prostate lesions at different distances from the rectum. 92% of
the rectum and 93% of the bladder deformation from the randomly picked exams
became so realistic that our freshly graduated clinician did not detect them,
but our residents noticed 87.5% of the rectum and 25% of the bladder
deformations based on small transformation artifacts and their expert intuition.
irregularities resulted from the random elastic deformations can be easily
detected, in contrast to our method being challenging to detect its artificial
nature. in table 1 we summarize the patient-level pauroc and f 1 -scores; and
lesion-level froc results on the independent test set showing the advantage of
using anatomy-informed da. to further highlight the practical advantage of the
proposed augmentation, we compare the performance of the trained models to the
radiologists' diagnostic performance for pi-rads ≥ 4, which locate the most
informative performance point clinically on the roc diagram, see fig. 3.
extending the basic da scheme with the proposed anatomy-informed deformation not
only increased the sensitivity closely matching the radiologists' patient-level
diagnostic performance but also improved the detection of pca on a lesion level.
interestingly, while the use of random deformable transformation also improved
lesion-level performance, it did not approach the diagnostic performance of the
radiologists, unlike the anatomy-informed da.at the selected patient-and
object-level working points, the model with the proposed rectum-and
bladder-informed da scheme reached the best results with significant
improvements (p < 0.05) compared to the model with the basic da setting by
increasing the f 1 -score with 5.11% and identifying 4 more lesions (5.3%) from
the 76 lesions in our test set.the time overhead introduced by anatomy-informed
augmentation caused no increase in the training time, the gpu remained the main
bottleneck.",7
224,1857,Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,2.1,Data Curation,"ex-vivo: data is collected from fresh breast tissue samples from the patients
referred to bcs at kingston health sciences center over two years. the study is
approved by the institutional research ethics board and patients consent to be
included. peri-operatively, a pathologist guides and annotates the ex-vivo
pointburns, referred to as spectra, from normal or cancerous breast tissue
immediately after excision. in addition to spectral data, clinicopathological
details such as the status of hormone receptors is also provided
post-surgically. in total 51 cancer and 149 normal spectra are collected and
stratified into five folds (4 for cross validation and 1 prospectively) with
each patient restricted to one fold only.",7
225,1862,Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,0.0,Ex-vivo Evaluation:,"the performance of the proposed network is compared with 3 baseline models
including gtn, graph convolution network [14], and non-graph convolution
network. four-fold cross validation is used for comparison of the different
approaches, to increase the generalizability (3 folds for train/validation, test
on remaining unseen fold, report average test performance). separate ablation
studies are performed for the baseline models to fine tune their structural
parameters. all experiments are implemented using pytorch with adam optimizer,
learning rate of 10 -4 , batch size of 32, and early stopping based on
validation loss. to demonstrate the robustness of the model and ensure it is not
overfitting, we also report the performance of the ensemble model from the
4-fold cross validation study on the 5th unseen prospective test fold.clinical
relevance: hormone receptor status plays an important role in determining breast
cancer prognosis and tailoring treatment plans for patients [6]. here, we
explore the correlation of the attention maps generated by egt with the status
of her2 and pr hormones associated with each spectrum. these hormones are
involved in different types of signaling that the cell depends on [5].",7
226,1867,Radiomics-Informed Deep Learning for Classification of Atrial Fibrillation Sub-Types from Left-Atrium CT Volumes,1.0,Introduction,"atrial fibrillation (af) is a cardiac disease characterized by rapid, irregular
heartbeats [4]. the disease can lead to stroke and heart failure, and has a
mortal-ity rate of almost 20% [5,10,13]. af is classified as either persistent
atrial fibrillation (peaf), where abnormal heart rhythms occur continuously for
more than seven days, or paroxysmal atrial fibrillation (paaf), where the heart
rhythm returns to normal within seven days. although af can be treated through a
procedure called catheter ablation, peaf cases have high recurrence rates and
often require re-intervention [8]. accurate knowledge of the disease type is
therefore highly valuable for treatment planning and has high prognostic value
[22].clinical studies have discovered a strong relationship between af and
epicardial adipose tissue (eat), a fat depot layer on the surface of the
myocardium that can cause inflammation and disrupt cardiac function [3,15].
recent works have shown that automatic classification of af sub-types can be
done using ct volumes of the left atrium and surrounding eat, which can be used
to screen for patients with high risk of peaf. huber et al. [7] showed that eat
volume, approximated from left-atrium ct images, can be used as a predictor for
af recurrence. yang et al. [22] trained a random forest model to classify af
subtype based on radiomic features and volume measurements, achieving 85.3% auc.
although these methods demonstrate the usefulness of radiomic features for af
sub-type classification, such features are generic and not specific to the task,
which can limit model performance [12]. radiomic features also rely on summary
statistics such as entropy or homogeneity to obtain global descriptors, and
these have limited effectiveness when capturing local feature variations
[16].deep learning has achieved outstanding results on medical imaging analysis
tasks, largely due to its ability to learn task-specific features and complex
relations between them [17]. naïvely using deep neural networks (dnns) to
predict af sub-types from ct volumes yields poor results however due to
over-fitting on high-dimensional volume inputs (see results for dnn in table 1).
existing works have attempted to combine deep and radiomic features through
methods such as direct concatenation [2,19], attention modules [14], or
contrastive learning between feature types [24]. although these methods propose
different ways of using both approaches, they do not explicitly address the
limitations of either approach or explore ways to combine their complementary
advantages.in this work, we propose a novel approach to atrial fibrillation
sub-type classification from ct volumes by integrating radiomic and deep
learning methods. we note that textural radiomic features identified by feature
selection methods can serve as an information prior to supplement low-level
features from dnns, since they are designed to capture low-level context and
have predictive power [23]. to this end, we locally calculate radiomic features
based on patches surrounding each voxel, and perform feature fusion with
low-level dnn features. this provides the dnn with pre-defined features known to
be relevant to the task to reduce over-fitting, and also allows spatial
relations between radiomic features to be learned. furthermore, we encourage the
dnn to learn features complementary to radiomic features to obtain more
comprehensive signals and design a novel feature de-correlation loss. the
overall framework, which we term radiomics-informed deep learning (ridl), is
illustrated in fig. 1. unlike existing works, our method is designed to directly
addresses the limitations of both deep learning and radiomic approaches and
achieves state-of-the-art performance on af sub-type classification. to
summarize our key contributions: -we propose a novel radiomics-informed deep
learning (ridl) method for af sub-type classification from ct volumes, which
achieves state-of-the-art results and can be used to screen for patients with
high risk of peaf. -our method uses a novel approach of fusing locally computed
radiomic features with low-level dnn features to improve capturing of local
context. -furthermore, we enforce feature de-correlation using a novel
feature-bank design to ensure complementary deep and radiomic features are
extracted.",7
227,1872,Radiomics-Informed Deep Learning for Classification of Atrial Fibrillation Sub-Types from Left-Atrium CT Volumes,3.1,Implementation Details,"dataset. we use a dataset of 172 patients containing 94 paaf and 78 peaf cases
collected from the sun yat-sen memorial hospital in china. ct volumes are
centered on the left atrium and normalized to between -1 and 1. roi masks for
eat are obtained through hounsfield value thresholding between -250 and 0.
volumes are resized to the same aspect ratio to ensure consistent dimensions
across samples. we use an input size of 96 × 128 × 128 voxels and apply zero
padding for smaller volumes. we use five-fold cross-validation and report
average test performance across folds. cross-validation is implemented by
splitting the dataset into five equal subsets and using three subsets for
training, one subset for validation, and one subset for testing. a rolling
scheme is used such that different validation and test subsets are used for each
of the five folds. data acquisition procedures and statistics are given in the
supplementary materials.setup. we use the pyradiomic package [18] to extract
radiomic features from the input volumes and masks. using the cross-validation
splits, we perform feature selection and classification using lasso regularized
logistic regression. lasso regularization consistently selects four radiomic
features as the ones with the most significant predictive power: maximum 3d
diameter, maximum 2d diameter, maximum voxel value, and normalized inverse
difference of glcm (glcm idn). the texture feature glcm idn is calculated
locally for p ∈ {1, 2, 5, 10} to obtain local radiomic features r l i ∈ r
4×96×128×128 . for our dnn network, we use a modified 3d u-net [1] (abbreviated
as m3dunet) with skip connections between the encoder and decoder removed to
enhance bottle-neck feature compression. bottle-neck features are averaged
across spatial dimensions for classification, whilst decoder outputs are used
for self-reconstruction regularization. the model is trained using the adam
optimizer with learning rate 10 -4 for 100 epochs and 0.1 decay at 30 epochs. we
use batch size b = 1, feature bank size n k = 25, and warm-up period of one
epoch. we use w corr = 2 for de-correlation loss weighting, which was chosen
based on the validation splits. mean and standard deviation of ten runs are
reported. additional experiments and details are included in the supplementary
materials.",7
228,1876,Radiomics-Informed Deep Learning for Classification of Atrial Fibrillation Sub-Types from Left-Atrium CT Volumes,4.0,Conclusion,"in this work, we propose a new approach to atrial fibrillation sub-type
classification from ct volumes by integrating radiomic and deep learning
approaches through a radiomics-informed deep learning method, ridl. our method
is based on two key ideas: feature fusion of locally computed radiomic features
with lowlevel dnn features to improve local context, and encouraging
complementary deep and radiomic features through feature de-correlation. unlike
existing hybrid approaches, our method specifically addresses the advantages and
limitations of both techniques to improve feature extraction. we achieve
state-of-the-art results on af sub-type classification and outperform existing
radiomic, deep learning, and hybrid methods.future improvements to ridl can be
made by introducing more sophisticated local radiomic features selection
methods, given the large set features to choose from. experiments on larger
datasets or alternative tasks can also be done to provide more empirical
support, since current results show only slight improvements over baseline.
these issues may be addressed in future works. overall, our method is a novel
way of combining radiomic and deep learning approaches, and can be used to
improve accuracy of peaf screening from ct volumes for better preventive care of
high-risk patients.",7
229,1879,Synthesis of Contrast-Enhanced Breast MRI Using T1- and Multi-b-Value DWI-Based Hierarchical Fusion Network with Attention Mechanism,1.0,Introduction,"breast cancer is the most common cancer and the leading cause of cancer death in
women [18]. early detection of breast cancer allows patients to receive timely
treatment, which may have less burden and a higher probability of survival [6].
among current clinical imaging modalities, magnetic resonance imaging (mri) has
the highest sensitivity for breast cancer detection [12]. especially,
contrastenhanced mri (ce-mri) can identify tumors well and has become an
indispensable technique for detecting and defining cancer [13]. however, the use
of gadolinium-based contrast agents (gbca) requires iv-cannulation, which is a
burden to patients, time consuming and cumbersome in a screening situation.
moreover, contrast administration can lead to allergic reactions and finaly
ce-mri may be associated with nephrogenic systemic fibrosis and lead to
bioaccumulation in the brain, posing a potential risk to human health
[4,9,[14][15][16]. in 2017, the european medicines agency concluded its review
of gbca, confirming recommendations to restrict the use of certain linear gbca
used in mri body scans and to suspend the authorization of other contrast
agents, albeit macrocyclic agents can still be freely used [10].with the
development of computer technology, artificial intelligence-based methods have
shown potential in image generation and have received extensive attention. some
studies have shown that some generative models can effectively perform mutual
synthesis between mr, ct, and pet [19]. among them, synthesis of ce-mri is very
important as mentioned above, but few studies have been done by researchers in
this area due to its challenging nature. li et al. analyzed and studied the
feasibility of using t1-weighted mri and t2-weighted mri to synthesize ce-mri
based on deep learning model [11]. their results showed that the model they
developed could potentially synthesize ce-mri and outperform other cohort
models. however, mri source data of too few sequences (only t1 and t2) may not
provide enough valuable informative to effectively synthesize ce-mri. in another
study, chung et al. investigated the feasibility of using deep learning (a
simple u-net structure) to simulate contrast-enhanced breast mri of invasive
breast cancer, using source data including t1-weighted non-fatsuppressed mri,
t1-weighted fat-suppressed mri, t2-weighted fat-suppressed mri, dwi, and
apparent diffusion coefficient [5]. however, obtaining a complete mri sequence
makes the examination costly and time-consuming. on the other hand, the
information provided by multi-sequences may be redundant and may not contain the
relevant information of ce-mri. therefore, it is necessary to focus on the most
promising sequences to synthesize ce-mri.diffusion-weighted imaging (dwi) is
emerging as a key imaging technique to complement breast ce-mri [3]. dwi can
provide information on cell density and tissue microstructure based on the
diffusion of tissue water. studies have shown that dwi could be used to detect
lesions, distinguish malignant from benign breast lesions, predict patient
prognosis, etc [1,3,7,8,17]. in particular, dwi can capture the dynamic
diffusion state of water molecules to estimate the vascular distribution in
tissues, which is closely related to the contrast-enhanced regions in ce-mri.
dwi may be a valuable alternative in breast cancer detection in patients with
contraindications to gbca [3]. inspired by this, we develop a multi-sequence
fusion network based on t1-weighted mri and multi-b-value dwi to synthesize
ce-mri. our contributions are as follows:i from the perspective of method, we
innovatively proposed a multi-sequence fusion model, designed for combining
t1-weighted imaging and multi-b-value dwi to synthesize ce-mri for the first
time. ii we invented hierarchical fusion module, weighted difference module and
multi-sequence attention module to enhance the fusion at different scale, to
control the contribution of different sequence and maximising the usage of the
information within and across sequences. iii from the perspective of clinical
application, our proposed model can be used to synthesize ce-mri, which is
expected to reduce the use of gbca.",7
230,1880,Synthesis of Contrast-Enhanced Breast MRI Using T1- and Multi-b-Value DWI-Based Hierarchical Fusion Network with Attention Mechanism,2.1,Patient Collection and Pre-processing,"this study was approved by institutional review board of our cancer institute
with a waiver of informed consent. we retrospectively collected 765 patients
with breast cancer presenting at our cancer institute from january 2015 to
november 2020, all patients had biopsy-proven breast cancers (all cancers
included in this study were invasive breast cancers, and ductal carcinoma in
situ had been excluded). the mris were acquired with philips ingenia all mris
were resampled to 1 mm isotropic voxels and uniformly sized, resulting in
volumes of 352 × 352 pixel images with 176 slices per mri, and subsequent
registration was performed based on advanced normalization tools (ants) [2].",7
231,1887,Synthesis of Contrast-Enhanced Breast MRI Using T1- and Multi-b-Value DWI-Based Hierarchical Fusion Network with Attention Mechanism,4.0,Conclusion,"we have developed a multi-sequence fusion network based on multi-b-value dwi to
synthesize ce-mri, using source data including dwis and t1-weighted
fatsuppressed mri. compared to existing methods, we avoid the challenges of
using full-sequence mri and aim to be selective on valuable source data dwi.
hierarchical fusion generation module, weighted difference module, and
multisequence attention module have all been shown to improve the performance of
synthesizing target images by addressing the problems of synthesis at different
scales, leveraging differentiable information within and across sequences. given
that current research on synthetic ce-mri is relatively sparse and challenging,
our study provides a novel approach that may be instructive for future research
based on dwis. our further work will be to conduct reader studies to verify the
clinical value of our research in downstream applications, such as helping
radiologists on detecting tumors. in addition, synthesizing dynamic
contrastenhanced mri at multiple time points will also be our future research
direction. our proposed model can potentially be used to synthesize ce-mri,
which is expected to reduce or avoid the use of gbca, thereby optimizing
logistics and minimizing potential risks to patients.",7
241,1899,Automated CT Lung Cancer Screening Workflow Using 3D Camera,1.0,Introduction,"lung cancer is the leading cause of cancer death in the united states, and early
detection is key to improving survival rates. ct lung cancer screening is a
lowdose ct (ldct) scan of the chest that can detect lung cancer at an early
stage, when it is most treatable. however, the current workflow for performing
ct lung scans still requires an experienced technician to manually perform
pre-scanning steps, which greatly decreases the throughput of this high volume
procedure. while recent advances in human body modeling [4,5,12,13,15] have
allowed for automation of patient positioning, scout scans are still required as
they are used by automatic exposure control system in the ct scanners to compute
the dose to be delivered in order to maintain constant image quality [3].since
ldct scans are obtained in a single breath-hold and do not require any contrast
medium to be injected, the scout scan consumes a significant portion of the
scanning workflow time. it is further increased by the fact that tube rotation
has to be adjusted between the scout and actual ct scan. furthermore, any
patient movement during the time between the two scans may cause misalignment
and incorrect dose profile, which could ultimately result in a repeat of the
entire process. finally, while minimal, the radiation dose administered to the
patient is further increased by a scout scan.we introduce a novel method for
estimating patient scanning parameters from non-ionizing 3d camera images to
eliminate the need for scout scans during pre-scanning. for ldct lung cancer
screening, our framework automatically estimates the patient's lung position
(which serves as a reference point to start the scan), the patient's isocenter
(which is used to determine the table height for scanning), and an estimate of
patient's water equivalent diameter (wed) profiles along the craniocaudal
direction which is a well established method for defining size specific dose
estimate (ssde) in ct imaging [8,9,11,18]. additionally, we introduce a novel
approach for updating the estimated wed in real-time, which allows for
refinement of the scan parameters during acquisition, thus increasing accuracy.
we present a method for automatically aborting the scan if the predicted wed
deviates from real-time acquired data beyond the clinical limit. we trained our
models on a large collection of ct scans acquired from over 60, 000 patients
from over 15 sites across north america, europe and asia. the contributions of
this work can be summarized as follows:-a novel workflow for automated ct lung
cancer screening without the need for scout scan -a clinically relevant method
meeting iec 62985:2019 requirements on wed estimation. -a generative model of
patient wed trained on over 60, 000 patients.-a novel method for real-time
refinement of wed, which can be used for dose modulation",7
242,1900,Automated CT Lung Cancer Screening Workflow Using 3D Camera,2.0,Method,"water equivalent diameter (wed) is a robust patient-size descriptor [17] used
for ct dose planning. it represents the diameter of a cylinder of water having
the same averaged absorbed dose as the material contained in an axial plane at a
given craniocaudal position z [2]. the wed of a patient is thus a function
taking as input a craniocaudal coordinate and outputting the wed of the patient
at that given position. as wed is defined in an axial plane, the diameter needs
to be known on both the anterior-posterior (ap) and lateral (left-right) axes
noted respectively w ed ap (z) and w ed l (z). as our focus here is on lung
cancer screening, we define 'wed profile' to be the 1d curve obtained by
uniformly sampling the wed function along the craniocaudal axis within the lung
region.our method jointly predicts the ap and lateral wed profiles. while wed
can be derived from ct images, paired ct scans and camera images are rarely
available, making direct regression through supervised learning challenging. we
propose a semi-supervised approach to estimate wed from depth images. first, we
train a wed generative model on a large collection of ct scans. we then train an
encoder network to map the patient depth image to the wed manifold. finally, we
propose a novel method to refine the prediction using real-time scan data.",7
243,1901,Automated CT Lung Cancer Screening Workflow Using 3D Camera,2.1,WED Latent Space Training,"we use an autodecoder [10] to learn the wed latent space. our model is a fully
connected network with 8 layers of 128 neurons each. we used layer normalization
and relu activation after each layer except the last one. our network takes as
input a latent vector together with a craniocaudal coordinate z and outputs w ed
ap (z) and w ed l (z), the values of the ap and lateral wed at the given
coordinate. in this approach, our latent vector represents the encoding of a
patient in the latent space. this way, a single autodecoder can learn
patient-specific continuous wed functions. since our network only takes the
craniocaudal coordinate and the latent vector as input, it can be trained on
partial scans of different sizes. the training consists of a joint optimization
of the autodecoder and the latent vector: the autodecoder is learning a
realistic representation of the wed function while the latent vector is updated
to fit the data.during training, we initialize our latent space to a unit
gaussian distribution as we want it to be compact and continuous. we then
randomly sample points along the craniocaudal axis and minimize the l1 loss
between the prediction and the ground truth wed. we also apply l2-regularization
on the latent vector as part of the optimization process.",7
244,1903,Automated CT Lung Cancer Screening Workflow Using 3D Camera,2.3,Real-Time WED Refinement,"while the depth image provides critical information on the patient anatomy, it
may not always be sufficient to accurately predict the wed profiles. for
example, some patients may have implants or other medical devices that cannot be
guessed solely from the depth image. additionally, since the encoder is trained
on a smaller data collection, it may not be able to perfectly project the depth
image to the wed manifold. to meet the strict safety criteria defined by the
iec, we propose to dynamically update the predicted wed profiles at inference
time using real-time scan data. first, we use our encoder network to initialize
the latent vector to a point in the manifold that is close to the current
patient. then, we use our autodecoder to generate initial wed profiles. as the
table moves and the patient gets scanned, ct data is being acquired and ground
truth wed can be computed for portion of the body that has been scanned, along
with the corresponding craniocaudal coordinate. we can then use this data to
optimize the latent vector by freezing the autodecoder and minimizing the l1
loss between the predicted and ground truth wed profiles through gradient
descent. we can then feed the updated latent vector to our autodecoder to
estimate the wed for the remaining portions of the body that have not yet been
scanned and repeat the process.in addition to improving the accuracy of the wed
profiles prediction, this approach can also help detect deviation from real
data. after the latent vector has been optimized to fit the previously scanned
data, a large deviation between the optimized prediction and the ground truth
profiles may indicate that our approach is not able to find a point in the
manifold that is close to the data. in this case, we may abort the scan, which
further reduces safety risks. overall flowchart of the proposed approach is
shown in fig. 1.",7
245,1904,Automated CT Lung Cancer Screening Workflow Using 3D Camera,3.1,Data,"our ct scan dataset consists of 62, 420 patients from 16 different sites across
north america, asia and europe. our 3d camera dataset consists of 2, 742 pairs
of depth image and ct scan from 2, 742 patients from 6 different sites across
north america and europe acquired using a ceiling-mounted kinect 2 camera. our
evaluation set consists of 110 pairs of depth image and ct scan from 110
patients from a separate site in europe.",7
246,1905,Automated CT Lung Cancer Screening Workflow Using 3D Camera,3.2,Patient Preparation,"patient positioning is the first step in lung cancer screening workflow. we
first need to estimate the table position and the starting point of the scan. we
propose to estimate the table position by regressing the patient isocenter and
the starting point of the scan by estimating the location of the patient's lung
top.starting position. we define the starting position of the scan as the
location of the patient's lung top. we trained a denseunet [7] taking the camera
depth image as input and outputting a gaussian heatmap centered at the patient's
lung top location. we used 4 dense blocks of 4 convolutional layers for the
encoder and 4 dense blocks of 4 convolutional layers for the decoder. each
convolutional layer (except the last one) is followed by a batch normalization
layer and a relu activation. we trained our model on 2, 742 patients using
adaloss [14] and the adam [6] optimizer with a learning rate of 0.001 and a
batch size of 32 for 400 epochs. our model achieves a mean error of 12.74 mm and
a 95 th percentile error of 28.32 mm. to ensure the lung is fully visible in the
ct image, we added a 2 cm offset on our prediction towards the outside of the
lung. we then defined the accuracy as whether the lung is fully visible in the
ct image when using the offset prediction. we report an accuracy of 100% on our
evaluation set of 110 patients. third and fourth columns show the performance of
our model with real-time refinement every 5 cm and 2 cm respectively. ground
truth is depicted in green and our prediction is depicted in red. while the
original prediction was off towards the center of the lung, the real-time
refinement was able to correct the error.isocenter. the patient isocenter is
defined as the centerline of the patient's body. we trained a densenet [1]
taking the camera depth image as input and outputting the patient isocenter. our
model is made of 4 dense blocks of 3 convolutional layers. each convolutional
layer (except the last one) is followed by a batch normalization layer and a
relu activation. we trained our model on 2, 742 patients using adadelta [16]
with a batch size of 64 for 300 epochs. on our evaluation set, our model
outperforms the technician's estimates with a mean error of 5.42 mm and a 95 th
percentile error of 8.56 mm compared to 6.75 mm and 27.17 mm respectively.
results can be seen in fig. 2.",7
247,1906,Automated CT Lung Cancer Screening Workflow Using 3D Camera,3.3,Water Equivalent Diameter,"we trained our autodecoder model on our unpaired ct scan dataset of 62, 420
patients with a latent vector of size 32. the encoder was trained on our paired
ct scan and depth image dataset of 2, 742 patients. we first compared our method
against a simple direct regression model. we trained a denseunet [7] taking the
camera depth image as input and outputting the water equivalent diameter
profile. we trained this baseline model on 2, 742 patients using the adadelta
[6] optimizer with a learning rate of 0.001 and a batch size of 32. we table 1.
wed profile errors on our testing set (in mm). 'w' corresponds to the portion
size of the body that gets scanned before updating the prediction (in cm). top
of the table corresponds to lateral wed profile, bottom corresponds to ap wed
profile. updating the prediction every 20 mm produces the best results.",7
248,1907,Automated CT Lung Cancer Screening Workflow Using 3D Camera,0.0,Method (lateral),"mean then measured the performance of our model before and after different
degrees of real-time refinement, using the same optimizer and learning rate. we
report the comparative results in table 1.we observed that our method largely
outperforms the direct regression baseline with a mean lateral error 40% lower
and a 90 th percentile lateral error over 30% lower. bringing in real-time
refinement greatly improves the results with a mean lateral error over 40% and a
90 th percentile lateral error over 20% lower than before refinement. ap
profiles show similar results with a mean ap error improvement of nearly 40% and
a 90 th percentile ap error improvement close to 30%. when using our proposed
method with a 20 mm window refinement, our proposed approach outperforms the
direct regression baseline by over 60% for lateral profile and nearly 80% for
ap.figures 3 highlights the benefits of using real-time refinement. overall, our
approach shows best results with an update frequency of 20 mm, with a mean
lateral error of 15.93 mm and a mean ap error of 10.40 mm. figure 4 presents a
qualitative evaluation on patients with different body morphology.finally, we
evaluated the clinical relevancy of our approach by computing the relative error
as described in the international electrotechnical commission (iec) standard iec
62985:2019 on methods for calculating size specific dose estimates (ssde) for
computed tomography [2]. the δ rel metric is defined as:where:-ŵ ed(z) is the
predicted water equivalent diameter -w ed(z) is the ground truth water
equivalent diameter z is the position along the craniocaudal axis of the
patient. iec standard states the median value of the set of δ rel (z) along the
craniocaudal axis (noted δ rel ) should be below 0.1. our method achieved a mean
lateral δ rel error of 0.0426 and a mean ap δ rel error of 0.0428, falling well
within the acceptance criteria.",7
249,1908,Automated CT Lung Cancer Screening Workflow Using 3D Camera,4.0,Conclusion,"we presented a novel 3d camera based approach for automating ct lung cancer
screening workflow without the need for a scout scan. our approach effectively
estimates start of scan, isocenter and water equivalent diameter from depth
images and meets the iec acceptance criteria of relative wed error. while this
approach can be used for other thorax scan protocols, it may not be applicable
to trauma (e.g. with large lung resections) and inpatient settings, as the
deviation in predicted and actual wed would likely be much higher. in future, we
plan to establish the feasibility as well as the utility of this approach for
other scan protocols and body regions.1",7
257,1932,Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,1.0,Introduction,"esophageal cancer is a significant contributor to cancer-related deaths globally
[3,15]. one effective treatment option is radiotherapy (rt), which utilizes
high-energy radiation to target cancerous cells [4]. to ensure optimal treatment
outcomes, both the cancerous region and the adjacent organ-at-risk (oar) must be
accurately delineated, to focus the high-energy radiation solely on the
cancerous area while protecting the oars from any harm. gross tumor volume (gtv)
represents the area of the tumor that can be identified with a high degree of
certainty and is of paramount importance in clinical practice.in the clinical
setting, patients may undergo a second round of rt treatment to achieve complete
tumor control when initial treatment fails to completely eradicate cancer [16].
however, the precise delineation of the gtv is laborintensive, and is restricted
to specialized hospitals with highly skilled rt experts. the automatic
identification of the esophagus presents inherent challenges due to its
elongated soft structure and ambiguous boundaries between it and adjacent organs
[12]. moreover, the automatic delineation of the gtv in the esophagus poses a
significant difficulty, primarily attributable to the low contrast between the
esophageal gtv and the neighboring tissue, as well as the limited
datasets.recently, advances in deep learning [21] have promoted research in
automatic esophageal gtv segmentation from computed tomography (ct) [18,19].
since the task is challenging, jin et al. [9,10] improve the segmentation
accuracy by incorporating additional information from paired positron emission
tomography (pet). nevertheless, such approaches require several imaging
modalities, which can be both costly and time-consuming, while disregarding any
knowledge from previous treatment or anatomical understanding. moreover, the
correlation between the first and second courses of rt is rarely investigated,
where detailed prior tumor information naturally exists in the previous rt
planning.in this paper, we present a comprehensive study on accurate gtv
delineation for the second course rt. we proposed a novel prior anatomy and rt
information enhanced second-course esophageal gtv segmentation network (artseg).
a region-preserving attention module (ram) is designed to effectively capture
the long-range prior knowledge in the esophageal structure, while preserving
regional tumor patterns. to the best of our knowledge, we are the first to
reveal the domain gap between the first and second courses for gtv segmentation,
and explicitly leverage prior information from the first course to improve gtv
segmentation performance in the second course.the medical images are labeled
sparsely, which are isolated by different tasks [20]. meanwhile, an ideal method
for automatic esophageal gtv segmentation in the second course of rt should
consider three key aspects: 1) changes in tumor volume after the first course of
rt, 2) the proliferation of cancerous cells from a tumor to neighboring healthy
cells, and 3) the anatomical-dependent our training approach leverages
multi-center datasets containing relevant annotations, that challenges the
network to retrieve information from e1 using the features from e2. the decoder
d utilizes the prior knowledge obtained from i1 and g1 to generate the mask
prediction. our training strategy leverages three datasets that introduce prior
knowledge to the network of the following three key aspects: 1) tumor volume
variation, 2) cancer cell proliferation, and 3) reliance of gtv on esophageal
anatomy.nature of gtv on esophageal locations. to achieve this, we efficiently
exploit knowledge from multi-center datasets that are not tailored for
second-course gtv segmentation. our training strategy does not specific to any
tasks but challenges the network to retrieve information from another encoder
with augmented inputs, which enables the network to learn from the above three
aspects. extensive quantitative and qualitative experiments validate our
designs.",7
258,1933,Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,2.0,Network Architecture,"in the first course of rt, a ct image denoted as i 1 is utilized to manually
delineate the esophageal gtv, g 1 . during the second course of rt, a ct image i
2 of the same patient is acquired. however, i 2 is not aligned with i 1 due to
soft tissue movement and changes in tumor volume that occurred during the first
course of treatment. both images i 1/2 have the spatial shape of h × w × d.our
objective is to predict the esophageal gtv g 2 of the second course. it would be
advantageous to leverage insights from the first course, as it comprises
comprehensive information pertaining to the tumor in its preceding phase.
therefore, the input to encoder e 1 consists of the concatenation of i 1 and g 1
to encode the prior information (features f d 1 ) from the first course, while
encoder e 2 embeds both low-and high-level features f d 2 of the local pattern
of i 2 (fig. 1),where the spatial shape of, with 2 d+4 channels.
region-preserving attention module. to effectively learn the prior knowledge in
the elongated esophagus, we design a region-preserving attention module (ram),
as shown in fig. 1. the multi-head attention (mha) [17] is employed to gather
long-range informative values in f d 1 with f d 2 as queries and f d 1 as keys.
the features f d 1/2 are reshaped to hw d 2 3d × c before passed to the mha,
where c is the channel dimension. the attentive features f d a can be formulated
as:since mha perturbs the positional information, we preserve the tumor local
patterns by concatenating original features to the attentive features at the
channel dimension, followed by a 1 × 1 × 1 bottleneck convolution ξ 1×1×1 to
squeeze the channel features (named as ram), as shown in the following
equations,where the lower-level features from both encoders are fused by
concatenation.the decoder d generates a probabilistic prediction) with skip
connections (fig. 1). we utilize the 3d dice [14] loss function,",7
259,1938,Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,4.1,Experimental Setup,"datasets. the paired first-second course dataset, s p , is collected from sun
yat-sen university cancer center (ethics approval number: b2023-107-01),
comprising paired ct scans of 69 distinct patients from south china. we
collected the gtv dataset s v from medmind technology co., ltd., which has ct
scans from 179 patients. for both s p and s v , physicians annotated the
esophageal cancer gtv in each ct. the gtv volume statistics (cm 3 , mean ± std.)
in s v is 40.60 ± 29.75, and is 83.70 ± 55.97/71.66 ± 49.36 for the first/second
course rt in s p respectively. additionally, we collect s e from segthor [12],
consisting of ct scans and esophagus annotations from 40 patients who did not
implementation details. the ct volumes from the first and second course in s p
are aligned based on the center of the lung mask [8]. the ct volumes are applied
with a windowing of [-100, 300] hu, and resampled to 128 3 , with a voxel size
of 1.2 × 1.2 × 3 mm 3 . the augmentations p 1/2 involve a combination of random
3d resized cropping, flipping, rotation in the transverse plane, and gaussian
noise. we employ the adam [11] optimizer with (β 1 , β 2 , lr) = (0.9, 0.999,
0.001) for training for 500 epoches. the network is implemented using pytorch
[2] and monai [1], and detailed configurations are in the supplementary
material. experiments are performed on an nvidia rtx 3090 gpu with 24gb
memory.performance metrics. dice score (dsc), averaged surface distance (asd)
and hausdorff distance (hsd) are used as metrics for evaluation. the wilcoxon
signed-rank test is used to compare the performance of different methods.",7
260,1939,Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,4.2,Domain Gap Between the First and Second Course,"as previously mentioned, the volume of the tumors changes after the first course
of rt. to demonstrate the presence of a domain gap between the first and second
courses, we train sota methods with datasets s train p and s v , by feeding the
data sequentially into the network. we then evaluate the models on s test p .
the results presented in table 1 indicate a performance gap between gtv
segmentation in the first and second courses, with the latter being more
challenging. notably, the paired first-second course dataset s test p pertains
to the same group of patients, thereby ensuring that any performance drop can be
attributed solely to differences in courses of rt, rather than variations across
different patients.figure 2 illustrates the reduction in the gtv area after the
initial course of rt, where the transverse plane is taken from the same location
relative to the vertebrae (yellow lines). the blue arrows indicate that the
networks failed to track these changes and produced false predictions in the
second course of rt. this suggests that deep learning-based approaches may not
rely solely on the identification of malignant tissue patterns, as doctors do,
but rather predict highrisk areas statistically. therefore, for accurate
second-course gtv segmentation, we need to explicitly propagate prior
information from the first course using dual encoders in artseg, and incorporate
learning about tumor changes.",7
261,1948,CLIP-Lung: Textual Knowledge-Guided Lung Nodule Malignancy Prediction,3.1,Dataset and Implementation Details,"dataset. lidc-idri [1] is a dataset for pulmonary nodule classification or
detection based on low-dose ct, which involves 1,010 patients. according to the
annotations, we extracted 2, 026 nodules, and all of them were labeled with
scores from 1 to 5, indicating the malignancy progression. we cropped all the
nodules with a square shape of a doubled equivalent diameter at the annotated
center, then resized them to the volume of 32 × 32 × 32. following [9,11], we
modified the first layer of the image encoder to be with 32 channels. according
to existing works [11,18], we regard a nodule with an average score between 2.5
and 3.5 as unsure nodules, benign and malignant categories are those with scores
lower than 2.5 and larger than 3.5, respectively. in this paper, we construct
three sub-datasets: lidc-a contains three classes of nodules both in training
and test sets; according to [11], we construct the lidc-b, which contains three
classes of nodules only in the training set, and the test set contains benign
and malignant nodules; lidc-c includes benign and malignant nodules both in
training and test sets.experimental settings. in this paper, we apply the clip
pre-trained vit-b/16 as the text encoder for clip-lung, and the image encoder we
used is resnet-18 [6] due to the relatively smaller scale of training data. the
image encoder is initialized randomly. note that for the text branch, we froze
the parameters of the text encoder and updated the learnable tokens l and l
during training. the learning rate is 0.001 following the cosine decay, while
the optimizer is stochastic gradient descent with momentum 0.9 and weight decay
0.00005. the temperature τ is initialized as 0.07 and updated during training.
all of our experiments are implemented with pytorch [15] and trained with nvidia
a100 gpus. the experimental results are reported with average values through
five-fold cross-validation. we report the recall and f1-score values for
different classes and use ""±"" to indicate standard deviation.",7
262,1964,Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,1.0,Introduction,"computer-aided diagnosis (cad) systems have achieved success in many clinical
tasks [5,6,12,17]. most cad studies were developed on regular and selected
datasets in the laboratory environment, which avoided the problems (data noise,
missing data, etc.) in the clinical scenarios [3,6,9,13,18]. in a real clinical
scenario, the clinicians generally synthesize all aspects of information, and
conduct consultations with multidisciplinary team (mdt), to accurately diagnose
and plan the treatment [9,10,13]. real-world studies have received increasing
attention [11,16], and it is challenging for the cad in the real-world scenarios
as: 1) consistent with the clinical workflow, cad needs to consider
multidisciplinary information to obtain multidimensional diagnosis; 2) due to
information collection, storage and manual evaluation, there are missing and
noisy medical data. this phenomenon is especially common in rare tumors like
pancreatic neuroendocrine neoplasms (pnens).in order to overcome above
challenges, some studies [3,9,13,18] used multilabel method because of the
following advantages: 1) the input of the model is only a single modality such
as images, which is easy to apply clinically; 2) the model learns multi-label
and multi-disciplinary knowledge, which is consistent with clinical logic; 3)
multi-label simultaneous prediction, which meets the need of clinical
multi-dimensional description of patients. for the above advantages, multi-label
technology is suitable for real-world cad. the previous multi-label cad studies
were designed based on simple parameter sharing methods [9,15,20] or graph
neural network (gnn) method [2]. the former implicitly interacts with
multi-label information, making it difficult to fully utilize the correlation
among labels; and the latter requires the use of word embeddings pre-trained on
public databases, which is not friendly to many medical domain proper nouns. the
generalizability of previous multi-label cad studies is poor due to these
disadvantages. in addition, none of the current multi-label cad studies have
considered the problem of missing labels and noisy labels.considering these
real-world challenges, we propose a multi-label model named self-feedback
transformer (sft), and validate our method on a realworld pnens dataset. the
main contributions of this work are listed: 1) a transformer multi-label model
based on self-feedback mechanism was proposed, which provided a novel method for
multi-label tasks in real-world medical application; 2) the structure is
flexibility and interactivity to meet the needs of realworld clinical
application by using four inference modes, such as expert-machine combination
mode, etc.; 3) sft has good noise resistance, and can maintain good performance
under noisy label input in expert-assisted mode.",7
263,1969,Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,3.1,Dataset and Evaluation,"real-world pnens dataset. we validated our method on a real-world pnens dataset
from two centers. all patients with arterial phase computed tomography (ct)
images were included. the dataset contained 264 and 28 patients in center 1 and
center 2, and a senior radiologist annotated the bounding boxes for all 408 and
28 lesions. we extracted 37 labels from clinical reports, including survival,
immunohistochemical (ihc), ct findings, etc. among them, 1)recist drug response
(rs), 2)tumor shrink (ts), 3)durable clinical benefit (dcb), 4)progression-free
survival (pfs), 5)overall survival (os), 6)grade (gd), 7)somatostatin receptor
subtype 2(sstr2), 8)vascular endothelial growth factor receptor 2 (vefgr2),
9)o6-methylguanine methyltransferase (mgmt), 10)metastatic foci (mtf), and
11)surgical recurrence (rt) are main tasks, and the remaining are auxiliary
tasks. 143 and 28 lesions were segmented by radiologists, and the radiomics
features of them were extracted, of which 162 features were selected and
binarized as auxiliary tasks because of its statistically significant
correlation with the main labels. the label distribution and the overlap ratio
(jaccard index) of lesions between pairs of labels are shown in fig. 3. it is
obvious that the real-world dataset has a large number of labels with randomly
missing data, thus, we used an adjusted 5-fold cross-validation. taking a
patient as a sample, we chose the dataset from center 1 as the internal dataset,
of which the samples with most of the main labels were used as dataset 1 (219
lesions) and was split into 5 folds, and the remaining samples are randomly
divided into the training set dataset 2 (138 lesions) and the validation set
dataset 3 (51 lesions), the training set and the validation set of the
corresponding folds were added during cross-validation, respectively. all
samples in center 2 left as external test set. details of each dataset are in
the supplementary material. dataset evaluation metrics. we evaluate the
performance of our method on the 10 main tasks for internal dataset, and due to
missing labels and too few sstr2 labels, only the performance of predicting rt,
pfs, os, gd, mtf are evaluated for external dataset. we employ accuracy (acc),
sensitivity (sen), specificity (spc), f1-score (f1) and area under the receiver
operating characteristic (auc) for each task, and compute the mean value of them
(e.g. mauc).",7
264,1990,Physics-Informed Conditional Autoencoder Approach for Robust Metabolic CEST MRI at 7T,2.0,Methods,"data measurements. cest imaging was performed in seven subjects, including two
glioblastoma patients, after written informed consent was obtained to
investigate the dependence of cest effects on b 1 in brain tissue. the local
ethics committee approved the study. all volunteers were measured at three b 1
field strengths 0.72 μt, 1.0 μt, and 1.5 μt. a method as described by mennecke
et al. [9] was used to acquire cest data on a 7t whole-body mri system
(magne-tom terra, siemens healthcare gmbh, erlangen, germany). saturated images
were obtained for 54 non-equidistant frequency offsets ranging from -100 ppm to
+100 ppm. the acquisition time per b 1 level was 6:42 min. the acquisition of
the b 1 map required an additional 1:06 min.conditional autoencoder. we
developed a conditional autoencoder (cae) to solve the b 1 inhomogeneity
problem, which is essential for the generation of metabolic cest contrast maps
at 7t. the left part of fig. 2 describes the cae. the encoding network of cae
took the raw cest-spectrum and the corresponding effective b 1 value as input
and generate a latent space that was concatenated once with the same b 1 input
value and passed to the decoder that reconstruct the uncorrected b 1
cest-spectrum, and another time the latent space was concatenated with the
desired/specific effective b 1 value to reconstruct the cest-spectrum at a
specific b 1 saturation amplitude. both decoders shared the weights (cf. fig.
2). for the development of the cae networks, we used the well-known fully
concatenated (fc) layers with leaky relu activations except for the last layer
of decoder, which had a linear activation. the encoder and decoder both
consisted of 4 layers, where the layers of the encoder successively contain 128,
128, 64, 32 neurons, while the layers of the decoder successively contain 32,
64, 128, 128 neurons. the input, latent space and output layers had 55, 17 and
54 neurons respectively.physics-informed autoencoder. the lorentzian model and
its b 1dispersion can be derived from the underlying spin physics described by
the bloch-mcconnell equation system [8]. the physics-informed autoencoder (piae)
utilized fully connected nn as encoder and lorentzian distribution generator as
a decoder to perform the pixel-wise 5-pool lorentzian curve fit to the
cest-spectrum (water, amide, amine, noe, mt) [14]. the 5-pool model was
described aswhere l denotes the lorentz function. the direct saturation pool
(water) was defined as(the remaining other four pools were defined as) 2 , i ∈
amide, amine, rn oe, ssm t .(the right part of fig. 2 describes the piae. the
encoder of piae mapped the cest-spectrum to the amplitudes a i , the full width
half maximum (fwhm) τ i , and the water peak position δ ds of the 5-pool
lorentzian model. its encoder consisted of four fc layers, each with 128 neurons
with leaky relu activations. it had three so-called fc latent space layers with
linear activation for position and exponential activations for fwhm and
amplitudes of 5-pool lorentzian model. the positions of amide, rnoe, ssmt, and
amine were fixed at 3.5 ppm, -3.5 ppm, -3 ppm, and 2 ppm, respectively, and
shifted with respect to the predicted position of the water peak. the decoder of
piae consisted of a lorentzian distribution generator (cf. fig. 2). it generated
samples of the 5-pool distributions exactly at the offsets δω (i.e. between -100
ppm and 100 ppm) where the input cest-spectrum was sampled, and combined them
according to eq. 1 to generate the input cest spectrum with or without b 0
correction.bound loss. the peak positions δ i and widths τ i of the pools had to
be within certain bounds so that certain neurons in the latent space layer of
piae would not be exchanged and provide the same pool parameters for all
samples. we developed a simple cost function along the lines of the hinge loss
[12], called the bound loss. mathematically, it is defined as followsthe bound
loss increases linearly as the output of the latent space neurons of piae
exceeds or recede from the boundaries. the lower and upper limits for positions
and widths are given in table 1 of the supplementary material.training and
evaluation. four healthy volunteers formed the training and validation sets. the
test set consisted of the two tumor patients and one healthy subject. to ensure
that the outcomes were exclusively based on the cest-spectrum and not influenced
by spatial position, the training was carried out voxel-by-voxel. consequently,
there were approximately one million cestspectra for the training process. cae
was first trained with mse loss. in this step, the cae encoder was fed with the
cest-spectrum of a specific b 1 saturation amplitude, and it generated two
cest-spectra, one for the input b 1 saturation level and the other for the b 1
level injected into the latent space (cf. fig. 2). later, it was trained with a
combination of mse loss and perception loss (mse loss between the latent space
of the cest-spectra at two different b 1 levels). to incorporate perception
loss, we used two forward passes with two different b 1 cest-spectra and used
perception loss to generate a latent space that is independent of b 1 saturation
amplitude. the following equation describes the loss of the second step.piae, on
the other hand, was trained with a combination of mse loss and bound loss. the
piae loss was described as followsfor evaluation we input the uncorrected
cest-spectrum acquired at 1μt and generated corrected cest-spectra at b 1 0.5,
0.72, 1.0, 1.3, 1.5 μt. piae encoder yielded the amplitudes of 5-pool for b 1
corrected cest-spectrum. its decoder reconstructed the b 1 b 0 fitted
cest-spectrum. the b 0 correction simply refers to the shift of the position of
the water peak to 0 ppm.cest quantification. the multi-b 1 cest-spectra allow
quantification of cest effects (amide, rnoe, amine) [14,15] down to the exchange
rate and concentration. the amplitudes of the cest contrasts were expressed
according to the definition in [15] as followswhere f i , k i , and r 2i express
the concentrations, exchange rates, and relaxation rates of the pools. z ref
defines the sum of all 5 distributions at the resonance frequency of the
specific pool in b 1 b 0 corrected cest-spectrum and w 1 is the frequency of the
oscillating field.the amplitudes of cest contrasts in the lorentzian function
have the b 1 dispersion function given by the labeling efficiency α (eq. 7). the
exchange rate occurs here separately from the concentration, which allows their
quantification via the b 1 dispersion. concentration and exchange rate were
fitted as a product and denoted as z 1 (quantified maps), and k(k+r 2 ) was also
fitted with the single term z 2 using trust-region reflective least squares
[10].",8
265,1991,Physics-Informed Conditional Autoencoder Approach for Robust Metabolic CEST MRI at 7T,3.0,Results,"the comparison of picae with the conventional method [9,14] is shown in columns
1 and 2 of fig. 3. the top image in column 3 shows the t 1 -weighted reference
image enhanced with the exogenous contrast agent gadolinium (gd-t 1 w), and the
bottom image shows the b 1 -map. the tumor shows a typical so called gadolinium
ring enhancement indicated by the arrow (a 15 ), which is also visible in the
non-invasive and gadolinium-free cest contrast maps (columns 1 and 2). the
picae-cest maps showed better visualization of this tumor feature compared to
the conventional method. the proposed method yielded at least 25% increase in
the structural similarity index (ssim) with the gd-t 1 w image for the ring
enhancement region. the contrast maps also appear less noisy and more
homogeneous over the whole brain compared to the lorentzian fit on the
interpolated-corrected b 1 cest-spectra [14]. to further evaluate the
performance of piae and cae, we b1-corrected the data using cae and fitted it
with the least squares method (cae-lorentzian fit). the comparison of the cest
maps produced by the conventional lorentzian fit, the cae-lorentzian fit, and
picae is shown in table 1 using ssim and gradient cross correlation (gcc) [11]
for the tumor ring region. both the cae-lorentzian fit and picae were better
than the conventional method. cae-lorentzian fit even outperformed picae for
rnoe metabolic map and has similar performance for amide, but it has much lower
performance for amine. the ability of picae to produce b 1 -robust cest maps at
arbitrary levels is shown in fig. 4, where different b 1 levels reveal different
features of the heterogenous tumor. quantification of chemical exchange rates
and concentration, i.e., z 1 = f•k, is shown in column 4. z 1 (quantified maps)
further improve the visualization of the ring enhancement area. column 5 shows
the z 2 maps, which are combination of the exchange rate k and the relaxation
rate r 2 . quantified maps of amide, rnoe and amine for another tumor patient is
shown in supplementary fig. 1. the accuracy of the cae to generate particular b
1 cestspectra is depicted using absolute error for acquisition at different b 1
levels (see supplementary fig. 2). the performance was lower for b 1 0.72 μt,
and 1.5 μt compared to 1 μt.",8
267,2013,A Texture Neural Network to Predict the Abnormal Brachial Plexus from Routine Magnetic Resonance Imaging,1.0,Introduction,"brachial plexopathy is a form of peripheral neuropathy [1]. it occurs when there
is damage to the brachial plexus (bp) which is a complex nerve network under the
skin of the shoulder. there is a wide range of disease that may cause a brachial
plexopathy.radiation fibrosis, primary and metastatic lung cancer, and
metastatic breast cancer account for almost three-fourths of causes [2].
brachial plexus syndrome occurs not infrequently in patients with malignant
disease. it is due to compression or direct invasion of the nerves by tumor
which will bring many serious symptoms [3]. our research focuses on the brachial
plexopathy caused by metastatic breast cancers.magnetic resonance imaging (mri)
and ultrasound of the brachial plexus have become two reliable diagnostic tools
for brachial plexopathy [4]. automatic identification of the bp in mri and
ultrasound images has become a hot topic. currently, most of relevant research
in this field are focusing on ultrasound modality [5][6][7][8]. compared with
ultrasound, mri has become the primary imaging technique in the evaluation of
brachial plexus pathology [9]. however, to our knowledge, radiomics related bp
studies utilizing mri have not been reported previously.many radiomics studies
have experimentally demonstrated that image texture has great potential for
differentiation of different tissue types and pathologies [10]. in the past
several decades, many state-of-the-art methods have been proposed to extract
texture patterns [11,12]. however, how to most effectively combine texture
features with deep learning, called deep texture, is still an open area of
research. one prior approach, termed glcm-cnn, was proposed to carry out a polyp
differentiation task [13]. however, how to arrange these glcms to form the 3d
volume to optimize the performance is a major challenge.with the goal of
classifying normal from abnormal bp, we explored the approach of deep texture
learning. this paper constructed a bp dataset with the most commonly used bp
mris in our clinical practice. considering the shortcoming of traditional
patterns, triple point pattern (tpp) is proposed for the quantitative
representation of the heterogeneity of abnormal bp's. in contrast to glcm-cnn,
tppnet is designed to train models by feeding tpp matrices as the input with a
huge number of channels. finally, we analyze the model's performance in the
experimental section. the major contributions of this study include 1) directed
triangle construction idea for tpp, 2) huge number of tpp matrices as the
heterogeneity representations of bp, 3) tppnet with 15 layers and huge number of
channels, 4) the bp dataset containing mr images and their corresponding roi
masks.",8
268,2014,A Texture Neural Network to Predict the Abnormal Brachial Plexus from Routine Magnetic Resonance Imaging,2.1,Dataset Preparation and Preprocessing,"following irb approval for this study, we search for patients with metastatic
breast cancer who had a breast cancer mri performed between 2010 and 2020 and
had morphologically positive bp on the mri report from our electronic medical
records (emr) in * hospital. totally, we collect approximate 807 series which
include 274 t2, 254 t1 and 279 post-gadolinium. since some scans are seriously
degraded due to motion artifacts. therefore, each case underwent several
essential image adjustments such as multi-series splitting, two-series merging,
slice swapping, artifact checking and boundary corrections. to yield the roi,
firstly, we randomly sampled -40% of the sequences including both normal and
abnormal ones that were manually segmented with itk-snap by two skilled trainees
[14,15]. then, the manual segmentations were utilized to train a 3d nnunet model
which was utilized to train the model which was used to predict rois for the
rest series [16]. the predicted segmentations were manually divided into three
groups, i.e. good, fair and poor. good cases were added to the training set.
this process was repeated until no improvements in the predictions for the
remaining sequences was seen. the final dataset for radiomic analysis was
constructed by merging the datasets for each sequence type. only patients that
had all three sequences segmented (t2, t1 and post-gadolinium) were included in
the dataset. table 1 shows a breakdown of the final dataset.",8
269,2065,Prompt-Based Grouping Transformer for Nucleus Detection and Classification,3.1,Datasets and Implementation Details,"consep 1 [10] is a colorectal nuclear dataset with three types, consisting of 41
h&e stained image tiles from 16 colorectal adenocarcinoma whole-slide images
(wsis). the wsis are at 20× magnification and the size of the slides is 500 ×
500. we split them following the official partition [1,10].is a breast cancer
dataset with three types and consists of 120 image tiles from 113 patients. the
wsis are at 20× magnification and the size of the slides ranges from 465 × 465
to 504 × 504. we follow the work [1] to apply the slic [2] algorithm to generate
superpixels as instances and split them into 80/10/30 slides for
training/validation/testing.lizard 3 [9] has 291 histology images of colon
tissue from six datasets, containing nearly half a million labeled nuclei in h&e
stained colon tissue. the wsis are at 20× magnification with an average size of
1,016 × 917 pixels. our implementation and the setting of hyper-parameters are
based on mmdetection [5]. the number of grouping prompts g is 64. random crop,
flipping, and scaling are used for data augmentation. our method is trained with
pytorch on a 48 gb gpu (nvidia a100) for 12-24 h (depending on the dataset
size). more details are listed in the supplementary material.",8
272,2095,TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,0.0,Table 3 .,"works on unregistered tractography from neonate brains (much smaller than adult
brains). in the challenging btp (tumor patients) dataset, tractcloud reg-free
obtains significantly lower tda values than sota methods and comparable
performance to tractcloud regist . as shown in table2, our registration-free
framework is much faster than other compared methods.",8
273,2105,Relaxation-Diffusion Spectrum Imaging for Probing Tissue Microarchitecture,3.3,In Vivo Data: Neurite Morphology,"figure 4(a) shows the relaxation times of the restricted compartment in white
matter lesions, indicating that relaxation times are longer in gliomas than
normal white matter tissue. the higher t 2 in grade 4 glioma is associated with
changes in metabolite compositions, resulting in remarkable changes in neurite
morphology in lesioned tissues (fig. 4(c-d)), consistent with previous
observations [12,23]. the rate of longitudinal relaxation time has been shown to
be positively correlated with myelin content. our results indicate that mte dmri
is more sensitive to neurite morphology than ste dmri (fig. 4(b)).figures 4(c-d)
show that the estimated mean nr in the gray matter is approximately in the range
of 10 µm, which is in good agreement with the sizes of somas in human brains,
i.e., 11 ± 7 µm [26]. rdsi improves the detection of small metastases,
delineation of tumor extent, and characterization of the intratumoral
microenvironment when compared to conventional microstructure models (fig.
4(c)). our studies suggest that rdsi provides useful information on
microvascularity and necrosis helpful for facilitating early stratification of
patients with gliomas (fig. 4(d)).",8
277,2139,Single-subject Multi-contrast MRI Super-resolution via Implicit Neural Representations,1.0,Introduction,"in clinical practice, magnetic resonance imaging (mri) provides important
information for diagnosing and monitoring patient conditions [4,16]. to capture
the complex pathophysiological aspects during disease progression,
multiparametric mri (such as t1w, t2w, dir, flair) is routinely acquired. image
acquisition inherently poses a trade-off between scan time, resolution, and
signalto-noise ratio (snr) [19]. to maximize the source of information within a
reasonable time budget, clinical protocol often combines anisotropic 2d scans of
different contrasts in complementary viewing directions. although acquired 2d
scans offer an excellent in-plane resolution, they lack important details in the
orthogonal out-of-plane. for a reliable pathological assessment, radiologists
often resort to a second scan of a different contrast in the orthogonal viewing
direction. furthermore, poor out-of-plane resolution significantly affects the
accuracy of volumetric downstream image analysis, such as radiomics and lesion
volume estimation, which usually require isotropic 3d scans. as multi-parametric
isotropic 3d scans are not always feasible to acquire due to time-constraints
[19], motion [9], and patient's condition [10], super-resolution offers a
convenient alternative to obtain the same from anisotropic 2d scans. recently,
it has been shown that acquiring three complementary 2d views of the same
contrast may yield higher snr at reduced scan time [19,29]. however, it remains
under-explored if orthogonal anisotropic 2d views of different contrasts can
benefit from each other based on the underlying anatomical consistency.
additionally, whether such strategies can further decrease scan times while
preserving similar resolution and snr remains unanswered. moreover, unlike
conventional super-resolution models trained on a cohort, a personalized model
is of clinical relevance to avoid the danger of potential misdiagnosis caused by
cohort-learned biases. in this work, we mitigate these gaps by proposing a novel
multi-contrast super-resolution framework that only requires the
patient-specific low-resolution mr scans of different sequences (and views) as
supervision. as shown in various settings, our approach is not limited to
specific contrasts or views but provides a generic framework for
super-resolution. the contributions in this paper are three-fold: 1. to the best
of our knowledge, our work is the first to enable subject-specific
multi-contrast super-resolution from low-resolution scans without needing any
high-resolution training data. we demonstrate that implicit neural
representations (inr) are good candidates to learn from complementary views of
multi-parametric sequences and can efficiently fuse low-resolution images into
anatomically faithful super-resolution. 2. we introduce mutual information (mi)
[26] as an evaluation metric and find that our method preserves the mi between
high-resolution ground truths in its predictions. further observation of its
convergence to the ground truth value during training motivates us to use mi as
an early stopping criterion. 3. we extensively evaluate our method on multiple
brain mri datasets and show that it achieves high visual quality for different
contrasts and views and preserves pathological details, highlighting its
potential clinical usage.related work. single-image super-resolution (sisr) aims
at restoring a highresolution (hr) image from a low-resolution (lr) input from a
single sequence and targets applications such as low-field mr upsampling or
optimization of mri acquisition [3]. recent methods [3,8] incorporate priors
learned from a training set [3], which is later combined with generative models
[2]. on the other hand, multi-image super-resolution (misr) relies on the
information from complementary views of the same sequence [29] and is especially
relevant to capturing temporal redundancy in motion-corrupted low-resolution mri
[9,27]. multi-contrast super-resolution (mcsr) targets using inter-contrast
priors [20]. in conventional settings [15], an isotropic hr image of another
contrast is used to guide the reconstruction of an anisotropic lr image. zeng et
al. [30] use a two-stage architecture for both sisr and mcsr. utilizing a
feature extraction network, lyu et al. [14] learn multi-contrast information in
a joint feature space. later, multi-stage integration networks [6], separatable
attention [7] and transformers [13] have been used to enhance joint feature
space learning. however, all current mcsr approaches are limited by their need
for a large training dataset. consequently, this constrains their usage to
specific resolutions and further harbors the danger of hallucination of features
(e.g., lesions, artifacts) present in the training set and does not generalize
well to unseen data.originating from shape reconstruction [18] and multi-view
scene representations [17], implicit neural representations (inr) have achieved
state-of-the-art results by modeling a continuous function on a space from
discrete measurements. key reasons behind inr's success can be attributed to
overcoming the low-frequency bias of multi-layer perceptrons (mlp) [21,24,25].
although mri is a discrete measurement, the underlying anatomy is a continuous
space. we find inr to be a good fit to model a continuous intensity function on
the anatomical space. once learned, it can be sampled at an arbitrary resolution
to obtain the super-resolved mri. following this spirit, inrs have recently been
successfully employed in medical imaging applications ranging from k-space
reconstruction [11] to sisr [29]. unlike [22,29], which learn anatomical priors
in single contrasts, and [1,28], which leverage inr with latent embeddings
learned over a cohort, we focus on employing inr in subject-specific,
multi-contrast settings.",8
278,2140,Single-subject Multi-contrast MRI Super-resolution via Implicit Neural Representations,2.0,Methods,"in this section, we first formally introduce the problem of joint
super-resolution of multi-contrast mri from only one image per contrast per
patient. next, we describe strategies for embedding information from two
contrasts in a shared space. subsequently, we detail our model architecture and
training configuration.problem statement. we denote the collection of all 3d
coordinates of interest in this anatomical space as ω = {(x, y, z)} with
anatomical function q : ω → a. the image intensities are a function of the
underlying anatomical properties a. two contrasts c 1 and c 2 can be scanned in
a low-resolution subspace ω 1 , ω 2 ⊂ ω. let us consider g 1 , g 2 : a → r that
map from anatomical properties to contrast intensities c 1 and c 2 ,
respectively. we obtain sparse observations, where f i is composition of g i and
q. however, one can easily obtain the global anatomical space ω by knowing ω 1
and ω 2 , e.g., by rigid registration between the two images. in this paper, we
aim to estimate f 1 , f 2 : ω → r given i 1 and i 2 .joint multi-contrast
modelling. since both component-functions f 1 and f 2 operate on a subset of the
same input space, we argue that it is beneficial to model them jointly as a
single function f : ω → r 2 and optimize it based on their estimation error
incurred in their respective subsets. this will enable information transfer from
one contrast to another, thus improving the estimation and preventing
over-fitting in single contrasts, bringing consistency to the prediction.to this
end, we propose to leverage inr to model a continuous multi-contrast function f
from discretely sampled sparse observations i 1 and i 2 .mcsr setup. without
loss of generalization, let us consider two lr input contrasts scanned in two
orthogonal planes p 1 and p 2 , where p 1 , p 2 ∈ {axial, sagittal, coronal}. we
assume they are aligned by rigid registration requiring no coordinate
transformation. their corresponding in-plane resolutions are (s 1 ×s 1 ) and (s
2 × s 2 ) and slice thickness is t 1 and t 2 , respectively. note that s 1 < t 1
and s 2 < t 2 imply high in-plane and low out-of-plane resolution. in the end,
we aim to sample an isotropic (s × s × s) grid for both contrasts where s ≤ s 1
, s 2 .implicit neural representations for mcsr. we intend to project the
information available in one contrast into another by embedding both in the
shared weight space of a neural network. however, a high degree of weight
sharing could hinder contrast-specific feature learning. based on this
reasoning, we aim to hit the sweet spot where maximum information exchange can
be encouraged without impeding contrast-specific expressiveness. we propose a
split-head architecture, as shown in fig. 1, where the initial layers jointly
learn the common anatomical features, and subsequently, two heads specialize in
contrast-specific information. the model takes fourier [25] features v =
[cos(2πbx), sin(2πbx)] t as input and predicts [ î1 , î2 ] = f (v), where x =
(x, y, z) and b is sampled from a gaussian distribution n (μ, σ 2 ). we use
mean-squared error loss, l mse , for training.where α and β are coefficients for
the reconstruction loss of two contrasts. note that for points {(x, y, z)} ∈ ω 2
\ ω 1 , there is no explicit supervision coming from low resolution c 1 . for
these points, one can interpret learning c 1 from the loss in c 2 , and vice
versa, to be a weakly supervised task.",8
279,2142,Single-subject Multi-contrast MRI Super-resolution via Implicit Neural Representations,3.0,Experiments and Results,"datasets. to enable fair evaluation between our predictions and the reference hr
ground truths, the in-plane snr between the lr input scan and corresponding
ground truth has to match. to synthetically create 2d lr images, it is necessary
to downsample out-of-plane in the image domain anisotropically [32] while
preserving in-plane resolution. consequently, to mimic realistic 2d clinical
protocol, which often has higher in-plane details than that of 3d scans, we use
spline interpolation to model partial volume and downsampling. we demonstrate
our network's modeling capabilities for different contrasts (t1w, t2w, flair,
dir), views (axial, coronal, sagittal), and pathologies (ms, brain tumor). we
conduct experiments on two public datasets, brats [16], and msseg [4], and an
in-house clinical ms dataset (cms). in each dataset, we select 25 patients that
fulfill the isotropic acquisition criteria for both ground truth hr scans. note
that we only use the ground truth hr for evaluation, not anywhere in training.
we optimize separate inrs for each subject with supervision from only its two lr
scans. if required, we employ skull-stripping [12] and rigid registration to the
mni152 (msseg, cms) or sri24 (brats) templates. for details, we refer to table 2
in the supplementary.metrics. we evaluate our results by employing common sr
[5,14,29] quality metrics, namely psnr and ssim. to showcase perceptual image
quality, we additionally compute the learned perceptual image patch similarity
(lpips) [31] and measure the absolute error mi in mutual information of two
upsampled images to their ground truth counterparts as follows:baselines and
ablation. to the best of our knowledge, there are no prior data-driven methods
that can perform mcsr on a single-subject basis. hence, we provide
single-subject baselines that operate solely on single contrast and demonstrate
the benefit of information transfer from other contrasts with our proposed
models. quantitative analysis. table 1 demonstrates that our proposed framework
poses a trustworthy candidate for the task of mcsr. as observed in [32], lrtv
struggles for anisotropic up-sampling while smore's overall performance is
better than cubic-spline, but slightly worse to single-contrast inr. however,
the benefit of single-contrast inr may be limited if not complemented by
additional views as in [29]. for mcsr from single-subject scans, we achieve
encouraging results across all metrics for all datasets, contrasts, and views.
since t1w and t2w both encode anatomical structures, the consistent improvement
in brats for both sequences serves as a proof-of-concept for our approach. as
flair is the go-to-sequence for ms lesions, and t1w does not encode such
information, the results are in line with the expectation that there could be a
relatively higher transfer of anatomical information to pathologically more
relevant flair than vice-versa. lastly, given their similar physical acquisition
and lesion sensitivity, we note that dir/flair benefit to the same degree in the
cms dataset.qualitative analysis. figure 2 shows the typical behavior of our
models on cms dataset, where one can qualitatively observe that the split-head
inr pre-serves the lesions and anatomical structures shown in the yellow boxes,
which other models fail to capture. while our reconstruction is not identical to
the gt hr, the coronal view confirms anatomically faithful reconstructions
despite not receiving any in-plane supervision from any contrast during
training. we refer to fig. 4 in the supplementary for similar observations on
brats and msseg.",8
280,2146,Overall Survival Time Prediction of Glioblastoma on Preoperative MRI Using Lesion Network Mapping,1.0,Introduction,"glioblastomas (gbms, known as grade iv gliomas) are the most common primary
malignant brain tumors with high spatial heterogeneity and varying degrees of
aggressiveness [22]. patients with gbm generally have a very poor survival rate;
the median overall survival time is about 14 months [17]; and the overall
survival time is affected by many factors, including patient characteristics
(e.g., age and physical status), tissue histopathology (e.g., cellular density
and nuclear atypia), and molecular pathology (e.g., mutations and gene
expression levels) [1,14,15]. although these factors, particularly molecular
information, have usually proved to be strong predictors of survival in gbm,
there remain substantial challenges and unmet clinical needs to exploit easily
accessible, noninvasive neuroimaging data acquired preoperatively to predict
overall survival time of gbm patients, which can benefit treatment planning.to
do so, magnetic resonance imaging (mri) and its derived radiomics have been
widely used to study gbm preoperative prognosis over the last few decades. for
example, anand et al. [2] first applied a forest of trees to assign an
importance value to each of the 1022 radiomic features extracted from t1 mri,
and then the 32 most important features were fed to the random forest regressor
for predicting overall survival time of a gbm patient. based on patches from
multi-modal mri images, nie et al. [19] trained a 3d convolutional neural
network (cnn) to learn the high-level semantic features, which were eventually
input to a support vector machine (svm) for classifying long-and short-term gbm
survivors. in addition, an integrated model by fusing radiomics features,
mri-based cnn features, and clinical features, was presented for gbm survival
group classification, resulting in better performance than using any single type
of features [12].although both mri and its derived radiomics features have been
demonstrated to have predictive power for survival analysis in the
aforementioned literature, they do not account for brain's functional
alternations caused by tumors, which are clinically significant as
biologically-interpretable biomarkers of recovery and therapy. these
alternations can be reflected by changes in resting-state functional mri
(fmri)-derived functional connectivities/connections (fcs) between the blood
oxygenation level-dependence (bold) time series of paired brain regions.
therefore, the use of fcs to predict overall survival time for gbm has recently
attracted increasing attention [7,16,24], and more importantly, survival-related
fc patterns or brain regions were found to guide therapeutic solutions aimed at
inhibiting tumor-brain communication.nevertheless, current fc-based survival
prediction still suffers from two main deficiencies when applied to gbm
prognosis. first, due to mass effect and physical infiltration of gbm in the
brain, fcs estimated directly from gbm patients' resting-state fmri might be
inaccurate, especially when the tumors are near or in the regions of interest.
second, resting-state fmri data are not routinely collected for gbm clinical
practices, which restricts the size of annotated datasets such that it is
infeasible to train a reliable prediction model based on deep learning for
survival prediction. in order to circumvent these issues, in this paper we
introduce a novel neuroimaging feature family, namely functional lesion network
(fln) maps that are generated by our augmented lesion network mapping (a-lnm),
for overall survival time prediction of gbm patients. our a-lnm is motivated by
lesion network mapping (lnm) [8] which can localize neurological deficits to
functional brain networks and identify regions relate to a clinical syndrome. by
embedding the lesion into a normative functional connectome and computing
functional connectivity between the lesion and the rest of the brain using fmri
of all healthy subjects in the normative cohort, lnm has been successfully
employed to the identification of the brain network underlying particular
symptoms or behavioral deficits in stoke [4,13].the details of our workflow are
described as follows.1) we first manually segment the whole tumor (regarded as
lesion in this paper) on structural mri for all gbm patients, and the resulting
lesion masks are mapped onto a reference brain template, e.g., the mni152 2mm 3
template.2) the proposed a-lnm is next used to generate fln maps for each gbm
patient by using resting-state fmri from a large cohort of healthy subjects.
specifically, for each patient, we correlate the mean bold time series of all
voxels within the lesion with the bold time series of every voxel in the whole
brain for all n subjects in the normative cohort, producing n functional
disconnection (fdc) maps of voxel-wise correlation values (transformed to
zscores). these resulting n fdc maps are partitioned into m disjoint subsets of
equal size, and m fln maps are separately obtained by averaging the fdc maps in
each of the m subsets. similar to data augmentation schemes, we can artificially
boost data volume (i.e., fln maps) up to m times through producing m fln maps
for each patient in the a-lnm, which helps to mitigate the risk of over-fitting
and improve the performance of overall survival time prediction when learning a
deep neural network from a small sized dataset.for this reason, we propose the
name ""augmented lnm (a-lnm)"", compared to the traditional lnm where only one fln
map is generated per patient by averaging all the n fdc maps. 3) finally, these
augmented fln maps are fed to a 3d resnet-based backbone network followed by the
average pooling operation and fully-connected layers for gbm survival
prediction.to our knowledge, this paper is the first to demonstrate a successful
extension of lnm for survival prediction in gbm. to evaluate the predictive
power of the fln maps generated by our a-lnm, we conduct extensive experiments
on 235 gbm patients in the training dataset of brats 2020 [18] to classify the
patients into three overall survival time groups viz. long, mid, and short.
experimental results show that our a-lnm based survival prediction framework
outperforms previous state-of-the-art methods. in addition, an explainable
analysis driven by the gradient-weighted class activation mapping (grad-cam)
[10] for survivalrelated brain regions is fulfilled.",8
281,2147,Overall Survival Time Prediction of Glioblastoma on Preoperative MRI Using Lesion Network Mapping,2.0,Materials and Methods,"2.1 materials gsp1000 processed connectome. it publicly released preprocessed
restingstate fmri data of 1000 healthy right-handed subjects with an average age
21.5 ± 2.9 years and approximately equal numbers of males and females from the
brain genomics superstruct project (gsp) [5], where the concrete image
acquisition parameters and preprocessing procedures can be found as well.
specifically, a slightly modified version of yeo's computational brain imaging
group (cbig) fmri preprocessing pipeline (https://github.com/bchcohenlab/cbig)
was employed to obtain either one or two preprocessed resting-state fmri runs of
each subject that had 120 time points per run and were spatially normalized into
the mni152 template with 2mm 3 voxel size. we downloaded and used the first-run
preprocessed resting-state fmri of each subject for the following analysis.brats
2020. it provided an open-access pre-operative imaging training dataset to
segment brain tumors of glioblastoma (gbm, belonging to high grade glioma) and
low grade glioma (lgg) patients, as well as to predict overall survival time of
gbm patients [18]. this training dataset contained 133 lgg and 236 gbm patients,
and each patient had four mri modalities, including t1, post-contrast
t1-weighted, t2-weighted, and t2 fluid attenuated inversion recovery. manual
expert segmentation delineated three tumor sub-regions, i.e., the gd-enhancing
tumor, the peritumoral edema, and the necrotic and non-enhancing tumor core.the
union of all the three tumor sub-regions was considered as the whole tumor,
which is regarded as the lesion in this paper.",8
282,2148,Overall Survival Time Prediction of Glioblastoma on Preoperative MRI Using Lesion Network Mapping,2.2,Methods,"in this paper, we propose to investigate the feasibility of the novel
neuroimaging features, i.e., fln maps, for overall survival time prediction of
gbm patients in the training dataset of the brats 2020, in which one patient
alive was excluded, and the remaining 235 patients consisted of 89 short-term
survivors (less than 10 months), 59 mid-term survivors (between 10 and 15
months), and 87 long-term survivors (more than 15 months). to this end, our
framework for the three-class survival classification is shown in fig. 1, and
the details are described as follows.lesion mapping procedures. as stated above,
the whole tumor is referred to as a lesion for each gbm patient. from the manual
expert segmentation labels of lesions in the 235 gbm patients of the brats 2020,
we co-register the lesion masks to the mni152 2mm 3 template by employing a
symmetric normalization algorithm in antspy [3].",8
283,2149,Overall Survival Time Prediction of Glioblastoma on Preoperative MRI Using Lesion Network Mapping,0.0,Augmented Lesion Network Mapping (A-LNM).,"after lesion mapping, we introduce a modified lnm (called augmented lnm (a-lnm)
in this paper) to generate fln maps for each gbm patient by using resting-state
fmri of all 1000 gsp healthy subjects, as described below. i) for each patient,
the lesion is viewed as a seed region to calculate fdc in the healthy subjects
with restingstate fmri. specifically, to compute fdc, the mean bold time series
of voxels within each lesion is correlated with the bold time series of every
voxel in the whole brain for all the 1000 healthy subjects, yielding 1000 fdc
maps of voxelwise correlation values (transformed to z-scores), where an fdc map
is actually a three-dimensional voxel-wise matrix of size 91 × 109 × 91 (spatial
resolution: 2mm 3 voxel size). ii) different from the commonly used lnm where
the resulting 1000 fdc maps are thresholded or averaged to obtain a single fln
map for each patient, the a-lnm generates many fln maps for each patient in a
manner that partitions all the 1000 fdc maps into disjoint subsets of equal size
and averages each subset to produce one fln map. one can clearly see that
similar to data augmentation schemes, we artificially boost the number of
training samples (i.e., fln maps) by our a-lnm, which helps to mitigate the risk
of over-fitting and improve the performance of overall survival time prediction
when learning a deep neural network from such a small sized training set used in
this paper. note that in sect. 3 of this paper, according to experimental
results, we divided the 1000 fdc maps into 100 subsets, and randomly chose 10
out of the resulting 100 fln maps for each patient as input to the downstream
prediction model.deep neural network for overall survival time prediction. by
taking the obtained fln maps as input, we apply a 3d resnet-based backbone
network transferred from the encoder of medicalnet [6] to extract cnn features
from each fln map. the features are then combined using the average pooling
operation and fed to a fully-connected layer with kernel size (1, 1, 1) to
classify each gbm patient into one of the three overall survival time groups
(i.e., short-term survival, mid-term survival, and long-term survival).",8
284,2150,Overall Survival Time Prediction of Glioblastoma on Preoperative MRI Using Lesion Network Mapping,3.1,Experimental Settings,"implementation details. our proposed method was implemented in pytorch 1.13.1 on
nvidia a100 tensor core gpus. the loss function was the standard cross-entropy
loss. the adam optimizer with the weight decay of 10 -5 was adopted. three 3d
resnet-based backbones with different numbers of layers (10, 50, and 101) were
performed, where the initial learning rates were set as 10 -4 , 10 -4 , and 10
-5 , respectively, and would decrease by a factor of 5 if the classification
performance is not improved within 5 epochs. the number of epochs for training
was 50, and the batch size was fixed as 64.performance evaluation. we evaluated
the classification performance of our proposed method using 235 gbm patients in
the brats 2020 training dataset, because only these 235 patients had both
overall survival time and manual expert segmentation labels of lesions. in all
experiments, we conducted five-fold crossvalidation ten times in order to reduce
the effect of sampling bias. moreover, the a-lnm was performed ten times
randomly to avoid particular data distribution and obtain more reliable results.
the classification results were reported in terms of accuracy, macro precision
(macro-p), macro recall (macro-r), and macro f1 score (macro-f1), respectively.",8
285,2152,Overall Survival Time Prediction of Glioblastoma on Preoperative MRI Using Lesion Network Mapping,3.3,Brain Regions in Relation to GBM Survival,"to identify the most discriminative brain regions associated with overall
survival time in gbm, we estimated the relative contribution of each voxel to
the classification performance in our proposed method by using the grad-cam
[10]. to obtain steady results, as shown in fig. 2(a), the voxels with top 5%
weights in the class activation maps (cams) of all candidate models were
overlapped by class, and the position covered by more than half of the models is
displayed. the cams of three classes of survivors overlapped in fig. 2(b) where
both coincident and non-coincident areas exist.the association of an increased
degree of invasion within the frontal lobe with decreased survival time can be
observed, which is in concordance with a previous study [20]. patients whose
frontal lobe is affected by tumors showed more executive dysfunction, apathy,
and disinhibition [11]. on the dominant left hemisphere, the cams of long-term
survivors and mid-term survivors overlapped at the superior temporal gyrus and
wernicke's area which are involved in the sensation of sound and language
comprehension respectively, and have been associated with decreased survival in
patients with high-grade glioma [26]. in addition, the cam of mid-term survivors
covered more areas of the middle and inferior temporal gyri which were
considered as one of the higher level ventral streams of visual processing
linked to facial recognition [25].",8
286,2153,Overall Survival Time Prediction of Glioblastoma on Preoperative MRI Using Lesion Network Mapping,4.0,Conclusion,"in this paper, we introduce a novel neuroimaging feature family, called a-lnm
derived fln maps, for overall survival time prediction of gbm patients. a-lnm
was presented to generate plenty of fln maps for each gbm patient by
partitioning the fdc maps obtained from resting-state fmri of 1000 gsp healthy
subjects into disjoint subsets of equal size and averaging each subset. we
applied a 3d resnet-based backbone network to extract features from the
generated fln maps and classify gbm patients into three overall survival time
groups. experimental results on the brats 2020 training dataset validated the
effectiveness of the a-lnm derived fln maps for gbm survival prediction.
moreover, a visualization analysis implemented by the grad-cam revealed the
brain regions associated with gbm survival. in future work, we will try to fuse
the fln maps and mri-based radiomics features to study their combined predictive
power for gbm survival analysis.",8
289,2164,Intraoperative CT Augmentation for Needle-Based Liver Interventions,1.0,Introduction,"needle-based liver tumor ablation techniques (e.g., radiofrequency, microwave,
laser, cryoablation) have a great potential for local curative tumor control
[1], with comparable results to surgery in the early stages for both primary and
secondary cancers. furthermore, as it is minimally invasive, it has a low rate
of major complications and procedure-specific mortality, and is tissue-sparing,
thus, its indications are growing exponentially and extending the limits to more
advanced tumors [3]. ct-guidance is a widely used imaging modality for placing
the needles, monitoring the treatment, and following up patients. however, it is
limited by the exposure to ionizing radiation and the need for intravenous
injection of contrast agents to visualize the intrahepatic vessels and the
target tumor(s).in standard clinical settings, the insertion of each needle
requires multiple check points during its progression, fine-tune maneuvers, and
eventual repositioning. this leads to multiple ct acquisitions to control the
progression of the needle with respect to the vessels, the target, and other
sensible structures [26]. however, intrahepatic vessels (and some tumors) are
only visible after contrast-enhancement, which has a short lifespan and
dose-related deleterious kidney effects. it makes it impossible to perform each
of the control ct acquisitions under contrast injection. a workaround to
shortcut these limitations is to perform an image fusion between previous
contrasted and intraoperative noncontrasted images. however, such a solution is
only available in a limited number of clinical settings, and the registration is
only rigid, usually deriving into bad results. in this work, we propose a method
for visualizing intrahepatic structures after organ motion and needle-induced
deformations, in non-injected images, by exploiting image features that are
generally not perceivable by the human eye in common clinical workflows.to
address this challenge, two main strategies could be considered: image fusion
and image processing techniques. image fusion typically relies on the estimation
of rigid or non-rigid transformations between 2 images, to bring into the
intraoperative image structures of interest only visible in the preoperative
data. this process is often described as an optimization problem [9,10] which
can be computationally expensive when dealing with non-linear deformations,
making their use in a clinical workflow limited. recent deep learning approaches
[11,12,14] have proved to be a successful alternative to solve image fusion
problems, even when a large non-linear mapping is required. when ground-truth
displacement fields are not known, state-of-the-art methods use unsupervised
techniques, usually an encoder-decoder architecture [7,13], to learn the unknown
displacement field between the 2 images. however, such unsupervised methods fail
at solving our problem due to lack of similar image features between the
contrasted (cct) and non-contrasted (ncct) image in the vascular tree region
(see sect. 3.3).on the other hand, deep learning techniques have proven to be
very efficient at solving image processing challenges [15]. for instance, image
segmentation [16], image style transfer [17], or contrast-enhancement to cite a
few. yet, segmenting vessels from non-contrasted images remains a challenge for
the medical imaging community [16]. style transfer aims to transfer the style of
one image to another while preserving its content [17][18][19]. however,
applying such methods to generate a contrasted intraoperative ct is not a
sufficiently accurate solution for the problem that we address.
contrast-enhancement methods could be an alternative. in the method proposed by
seo et al. [20], a deep neural network synthesizes contrast-enhanced ct from non
contrast-enhanced ct. nevertheless, results obtained by this method are not
sufficiently robust and accurate to provide an augmented intraoperative ct on
which needle-based procedures can be guided.in this paper we propose an
alternative approach, where a neural network learns local image features in a
ncct image by leveraging the known preoperative vessel tree geometry and
topology extracted from a matching (undeformed) cct. then, the augmented ct is
generated by fusing the deformed vascular tree with the non-contrasted
intraoperative ct. section 2 presents the method and its integration in the
medical workflow. section 3 presents and discusses the results, and finally we
conclude in sect. 4 and highlight some perspectives.",9
290,2169,Intraoperative CT Augmentation for Needle-Based Liver Interventions,2.4,Augmented CT,"once the network has been trained on the patient-specific preoperative data, the
next step is to augment and visualize the intraoperative ncct. this is done in 3
steps:-the dilatation operations introduced in sect. 2.1 are not reversible
(i.e. the segmented vessel tree cannot be recovered from the vm by applying the
same number of erosion operations). also, neighboring branches in the vessel
tree could end up being fused, thus changing the topology of the vessel map.
therefore, to retrieve the correct segmented (yet deformed) vascular tree, we
compute a displacement field between the pre-and intraoperative vms. this is
done with the elastix library [21,22]. the resulting displacement field is
applied on the preoperative segmentation to retrieve the intraoperative vessel
tree segmentation. this is illustrated in fig. 4. -the augmented image is
obtained by fusing the predicted intraoperative segmentation with the
intraoperative ncct image. the augmented vessels are displayed in green to
ensure the clinician is aware this is not a true cct image (see fig. 5). -it is
also possible to add anatomical labels to the intraoperative augmented ct to
further assist the clinician. to achieve this objective, we compute a graph data
structure from the preoperative segmentation. we first extract the vessel
centerlines as described in [4]. to define the associated graph structure, we
start by selecting all branches with either no parent or no children. the branch
with the highest radius is then selected as the root edge. an oriented graph is
created using a breadth first search algorithm starting from the root edge.
nodes and edges correspond respectively to vessel tree bifurcations and
branches. we use the graph structure to associate each anatomical label
(manually defined) with a strahler [6] graph ordering. the same process is
applied to the predicted intraoperative segmentation. this makes it possible to
correctly map the preoperative anatomical labels (e.g. vessel name) and display
them on the augmented image.fig. 4. this figure illustrates the different stages
of the pipeline adopted to generate the vm and show how the vessel tree topology
is retrieved from the predicted intraoperative vm by computing a displacement
field between the preoperative vm and the predicted vm. this field is applied to
the preoperative segmentation to get the intraoperative one.",9
291,2170,Intraoperative CT Augmentation for Needle-Based Liver Interventions,3.1,Dataset and Implementation Details,"to validate our approach, 4 couples of mpcect abdominal porcine images were
acquired from 4 different subjects. for a given subject, each couple corresponds
to a preoperative and an intraoperative mpcect. we recall that an mpcect
contains a set of registered ncct and cct images. these images are then cropped
and down-sampled to 256 × 256 × 256, and the voxels intensities are scaled
between 0 and 255. finally, we extract the vm from each mpcect sample and apply
3 dilation operations, which demonstrated the best performance in terms of
prediction accuracy and robustness on our data. we note that public data sets
such as deeplesion [24], 3dircadb-01 [25] and others do not fit our problem
since they do not include the ncct images. aiming at a patientspecific
prediction, we only train on a ""subject"" at a time. for a given subject, we
generate 100 displacement fields using the data augmentation strategy explained
above with 50 voxels for the control points spacing in the three spatial
directions and a standard deviation of 5 voxels for the normal distributions.
the resulting deformation is applied to the preoperative mpcect and its
corresponding vm. thus, we end up with a set of 100 triplets (ncct, cct and vm).
two out of the 100 triplets are used for each training batch, where one is
considered as the pre-operative mpcect and the other as the intraoperative one.
this makes it possible to generate up to 4950 training and validation samples.
the intraoperative mpcect of the same subject is used to test the network. our
method is implemented in tensorflow 2.4, on a geforce rtx 3090. we use an adam
optimizer (β 1 = 0.001, β 2 = 0.999) with a learning rate of 10 -4 . the
training process converges in about 1,000 epochs with a batch size of 1 and 200
steps per epoch.",9
292,2174,Intraoperative CT Augmentation for Needle-Based Liver Interventions,4.0,Conclusion,"in this paper, we proposed a method for augmenting intra-operative ncct images
as a means to improve needle ct-guided techniques while reducing the need for
contrast agent injection during tumor ablation procedures, or other needle-based
procedures. our method uses a u-net architecture to learn local vessel tree
image features in the ncct by leveraging the known vessel tree geometry and
topology extracted from a matching cct image. the augmented ct is generated by
fusing the predicted vessel tree with the ncct. our method is validated on
several porcine images, achieving an average dice score of 0.81 on the predicted
vessel tree location. in addition, it demonstrates robustness even in the
presence of large deformations between the preoperative and intraoperative
images. our future steps will essentially involve applying this method to
patient data and perform a small user study to evaluate the usefulness and
limitations of our approach.aknowledgments. this work was partially supported by
french state funds managed by the anr under reference anr-10-iahu-02 (ihu
strasbourg). the authors would like to thank paul baksic and robin enjalbert for
proofreading the manuscript.",9
293,2175,Optical Coherence Elastography Needle for Biomechanical Characterization of Deep Tissue,1.0,Introduction,"healthy and cancerous soft tissue display different elastic properties, e.g. for
breast [19], colorectal [7] and prostate cancer [4]. different imaging
modalities can be used to detect the biomechanical response to an external load
for the characterization of cancerous tissue, e.g., ultrasound, magnetic
resonance and optical coherence elastography (oce). the latter is based on
optical coherence tomography (oct), which provides excellent visualization of
microstructures and superior spatial and temporal resolution in comparison to
ultrasound or magnetic resonance elastography [8]. one common approach for
quantitative oce is to determine the elastic properties from the deformation of
the sample and the magnitude of a quasi-static, compressive load [10]. however,
due to the attenuation and scattering of the near-infrared light, imaging depth
is generally limited to approximately 1 mm in soft tissue. therefore, oce is
well suited for sampling surface tissue and commonly involves bench-top imaging
systems [26], e.g. in ophthalmology [21,22] or as an alternative to
histopathological slice examination [1,16]. handheld oce systems for
intraoperative assessment [2,23] have also been proposed. while conventional oce
probes have been demonstrated at the surface, regions of interest often lie deep
within the soft tissue, e.g., cancerous tissue in percutaneous biopsy.taking
prostate cancer as an example, biomechanical characterization could guide needle
placement for improved cancer detection rates while reducing complications
associated with increased core counts, e.g. pain and erectile dysfunction
[14,18]. however, the measurement of both the applied load and the local sample
compression is challenging. friction forces superimpose with tip forces as the
needle passes through tissue, e.g., the perineum. furthermore, the prostate is
known to display large bulk displacement caused by patient movement and needle
insertions [20,24] in addition to actual sample compression (fig. 1, left). tip
force sensing for estimating elastic properties has been proposed [5] but bulk
tissue displacement of deep tissue was not considered. in principle, compression
and tip force could be estimated by oct. yet, conventional oce probes typically
feature flat tip geometry [13,17].to perform oce in deep tissue structures, we
propose a novel bevel tip oce needle design for the biomechanical
characterization during needle insertions. we consider a dual-fiber setup with
temporal multiplexing for the combined load and compression sensing at the
needle tip. we design an experimental setup that can simulate friction forces
and bulk displacement occurring during needle biopsy (fig. 1). we consider
tissue-mimicking phantoms for surface and deep tissue indentation experiments
and compare our results with force-position curves externally measured at the
needle shaft. finally, we consider how the obtained elasticity estimates can be
used for the classification of both materials.",9
294,2187,Pelvic Fracture Segmentation Using a Multi-scale Distance-Weighted Neural Network,2.4,Data and Annotation,"although large-scale datasets on pelvic segmentation have been studied in some
research [13], to the best of our knowledge, currently there is no
well-annotated fractured pelvic dataset publicly available. therefore, we
curated a dataset of 100 preoperative ct scans covering all common types of
pelvic fractures. these data is collected from 100 patients (aged 18-74 years,
41 females) who were to undergo pelvic reduction surgery at beijing jishuitan
hospital between 2018 and 2022, under irb approval (202009-04). the ct scans
were acquired on a toshiba aquilion scanner. the average voxel spacing is 0.82 ×
0.82 × 0.94 mm 3 . the average image shape is 480 × 397 × 310.to generate
ground-truth labels for bone fragments, a pre-trained segmentation network was
used to create initial segmentations for the ilium and sacrum [13]. then, these
labels were further modified and annotated by two annotators and checked by a
senior expert.",9
295,2192,Towards Multi-modal Anatomical Landmark Detection for Ultrasound-Guided Brain Tumor Resection with Contrastive Learning,1.0,Introduction,"gliomas are the most common central nervous system (cns) tumors in adults,
accounting for 80% of primary malignant brain tumors [1]. early surgical
treatment to remove the maximum amount of cancerous tissues while preserving the
eloquent brain regions can improve the patient's survival rate and functional
outcomes of the procedure [2]. although the latest multi-modal medical imaging
(e.g., pet, diffusion/functional mri) allows more precise pre-surigcal planning,
during surgery, brain tissues can deform under multiple factors, such as
gravity, intracranial pressure change, and drug administration. the phenomenon
is referred to as brain shift, and often invalidates the pre-surgical plan by
displacing surgical targets and other vital anatomies. with high flexibility,
portability, and cost-effectiveness, intra-operative ultrasound (us) is a
popular choice to track and monitor brain shift. in conjunction with effective
mri-us registration algorithms, the tool can help update the pre-surgical plan
during surgery to ensure the accuracy and safety of the intervention.as the true
underlying deformation from brain shift is impossible to obtain and the
differences of image features between mri and us are large, quantitative
validation of automatic mri-us registration algorithms often rely on homologous
anatomical landmarks that are manually labeled between corresponding mri and
intra-operative us scans [3]. however, manual landmark identification requires
strong expertise in anatomy and is costly in labor and time. moreover, inter-and
intra-rater variability still exists. these factors make quality assessment of
brain shift correction for us-guided brain tumor resection challenging. in
addition, due to the time constraints, similar evaluation of inter-modal
registration quality during surgery is nearly impossible, but still highly
desirable. to address these needs, deep learning (dl) holds the promise to
perform efficient and automatic inter-modal anatomical landmark
detection.previously, many groups have proposed algorithms to label landmarks in
anatomical scans [4][5][6][7][8][9]. however, almost all earlier techniques were
designed for mono-modal applications, and inter-modal landmark detection, such
as for usguided brain tumor resection, has rarely been attempted. in addition,
unlike other applications, where the full anatomy is visible in the scan and all
landmarks have consistent spatial arrangements across subjects, intra-operative
us of brain tumor resection only contains local regions of the pathology with
noncanonical orientations. this results in anatomical landmarks with different
spatial distributions across cases. to address these unique challenges, we
proposed a new contrastive learning (cl) framework to detect matching landmarks
in intra-operative us with those from mri as references. specifically, the
technique leverages two convolutional neural networks (cnns) to learn features
between mri and us that distinguish the inter-modal image patches which are
centered at the matching landmarks from those that are not. our approach has two
major novel contributions to the field. first, we proposed a multi-modal
landmark detection algorithm for us-guided brain tumor resection for the first
time. second, cl is employed for the first time in inter-modal anatomical
landmark detection. we developed and validated the proposed technique with the
public resect database [10] and compared its landmark detection accuracy against
the popular scale-invariant feature transformation (sift) algorithm in 3d [11].",9
296,2201,Towards Multi-modal Anatomical Landmark Detection for Ultrasound-Guided Brain Tumor Resection with Contrastive Learning,5.0,Results,"table 1 lists the mean and standard deviation of landmark identification errors
(in mm) between the predicted position and the ground truth in intra-operative
us for each patient of the resect dataset. in the table, we also provide the
severity of brain shift for each patient. here, tissue deformation measured as
mean target registration errors (mtres) with the ground truth anatomical
landmarks is classified as small (mtre below 3 mm), median (3-6 mm), or large
(above 6 mm). the results show that our cl-based landmark selection technique
can locate the corresponding us landmarks with a mean landmark identification
error of 5.88±4.79 mm across all cases while the sift algorithm has an error
18.78±4.77 mm. with a two-sided paired-samples t-test, our method outperformed
the sift approach with statistical significance (p <1e-4). when reviewing the
mean landmark identification error using our proposed technique, we also found
that the magnitude is associated with the level of brain shift. however, no such
trend is observed when using sift features for landmark identification. when
inspecting landmark identification errors across all subjects between the cl and
sift techniques, we also noticed that our cl framework has significantly lower
standard deviations (p <1e-4), implying that our technique has a better
performance consistency.",9
301,2241,Learning Expected Appearances for Intraoperative Registration During Neurosurgery,1.0,Introduction,"we address the important problem of intraoperative patient-to-image registration
in a new way by relying on preoperative data to synthesize plausible
transformations and appearances that are expected to be found intraoperatively.
in particular, we tackle intraoperative 3d/2d registration during neurosurgery,
where preoperative mri scans need to be registered with intraoperative surgical
views of the brain surface to guide neurosurgeons towards achieving a maximal
safe tumor resection [22]. indeed, the extent of tumor removal is highly
correlated with patients' chances of survival and complete resection must be
balanced against the risk of causing new neurological deficits [5] making
accurate intraoperative registration a critical component of
neuronavigation.most existing techniques perform patient-to-image registration
using intraoperative mri [11], cbct [19] or ultrasound [9,17,20]. for 3d-3d
registration, 3d shape recovery of brain surfaces can be achieved using
near-infrared cameras [15], phase-shift 3d shape measurement [10], pattern
projections [17] or stereovision [8]. the 3d shape can subsequently be
registered with the preoperative mri using conventional point-to-point methods
such as iterative closest point (icp) or coherent point drift (cpd). most of
these methods rely on cortical vessels that bring salient information for such
tasks. for instance, in [6], cortical vessels are first segmented using a deep
neural network (dnn) and then used to constrain a 3d/2d non-rigid registration.
the method uses physics-based modeling to resolve depth ambiguities. a manual
rigid alignment is however required to initialize the optimization.
alternatively, cortical vessels have been used in [13] where sparse 3d points,
manually traced along the vessels, are matched with vessels extracted from the
preoperative scans. a model-based inverse minimization problem is solved by
estimating the model's parameters from a set of pre-computed transformations.
the idea of pre-computing data for registration was introduced by [26], who used
an atlas of pre-computed 3d shapes of the brain surface for registration. in
[7], a dnn is trained on a set of pre-generated preoperative to intraoperative
transformations. the registration uses cortical vessels, segmented using another
neural network, to find the best transformation from the pre-generated set.the
main limitation of existing intraoperative registration methods is that they
rely heavily on processing intraoperative images to extract image features (eg.,
3d surfaces, vessels centerlines, contours, or other landmarks) to drive
registration, making them subject to noise and low-resolution images that can
occur in the operating room [2,25]. outside of neurosurgery, the concept of
pregenerating data for optimizing dnns for intraoperative registration has been
investigated for ct to x-ray registration in radiotherapy where x-ray images can
be efficiently simulated from cts as digital radiographic reconstructions
[12,27]. in more general applications, case-centered training of dnns is gaining
in popularity and demonstrates remarkable results [16].",9
302,2242,Learning Expected Appearances for Intraoperative Registration During Neurosurgery,0.0,Contribution:,"we propose a novel approach for patient-to-image registration that registers the
intraoperative 2d view through the surgical microscope to preoperative mri 3d
images by learning expected appearances. as shown in fig. 1, we formulate the
problem as a camera pose estimation problem that finds the optimal 3d pose
minimizing the dissimilarity between the intraoperative 2d image and its
pre-generated expected appearance. a set of expected appearances are synthesized
from the preoperative scan and for a set of poses covering the range of
plausible 6 degrees-of-freedom (dof) transformations. this set is used to train
a patient-specific pose regressor network to obtain a model that is
texture-invariant and is cross-modality to bridge the mri and rgb camera
modalities. similar to other methods, our approach follows a monocular
singleshot registration, eliminating cumbersome and tedious calibration of
stereo cameras, the laser range finder, or optical trackers. in contrast to
previous methods, our approach does not involve processing intraoperative images
which have several advantages: it is less prone to intraoperative image
acquisition noise; it does not require pose initialization; and is
computationally fast thus supporting real-time use. we present results on both
synthetic and clinical data and show that our approach outperformed
state-of-the-art methods.",9
303,2244,Learning Expected Appearances for Intraoperative Registration During Neurosurgery,2.1,Problem Formulation,"as illustrated in fig. 1, given a 3d surface mesh of the cortical vessels m,
derived from a 3d preoperative scan, and a 2d monocular single-shot image of the
brain surface i, acquired intraoperatively by a surgical camera, we seek to
estimate the 6-dof transformation that aligns the mesh m to the image i.
assuming a set of 3d points u = {u j ∈ r 3 } ⊂ m and a set of 2d points in the
image v = {v i ∈ r 2 } ⊂ i, solving for this registration problem can be
formalized as finding the 6-dof camera pose that minimizes the reprojection
error:where r ∈ so(3) and t ∈ r 3 represent a 3d rotation and 3d translation,
respectively, and a is the camera intrinsic matrix composed of the focal length
and the principal points (center of the image) while {c i } i is a
correspondence map and is built so that if a 2d point v i corresponds to a 3d
point u j where c i = j for each point of the two sets. note that the set of 3d
points u is expressed in homogenous coordinates in the minimization of the
reprojection error.in practice, finding the correspondences set {c i } i between
u and v is nontrivial, in particular when dealing with heterogeneous
preoperative and intraoperative modality pairs (mri, rgb cameras, ultrasound,
etc.) which is often the case in surgical guidance. existing methods often rely
on feature descriptors [14], anatomical landmarks [13], or organ's contours and
segmentation [6,18] involving tedious processing of the intraoperative image
that is sensitive to the computational image noise. we alleviate these issues by
directly minimizing the dissimilarity between the image i and its expected
appearance synthesized from m.by defining a synthesize function s θ that
synthesizes a new image i given a projection of a 3d surface mesh for different
camera poses, i.e. i = s θ (a[r|t], m), the optimization problem above can be
rewritten as:argminthis new formulation is correspondence-free, meaning that it
alleviates the requirement of the explicit matching between u and v. this is one
of the major strengths of our approach. it avoids the processing of i at
run-time, which is the main source of registration error. in addition, our
method is patient-specific, centered around m, since each model is trained
specifically for a given patient. these two aspects allow us to transfer the
computational cost from the intraoperative to the preoperative stage thereby
optimizing intraoperative performance.the following describes how we build the
function s θ and how to solve eq. 1.",9
304,2245,Learning Expected Appearances for Intraoperative Registration During Neurosurgery,2.2,Expected Appearances Synthesis,"we define a synthesis network s θ : (a[r|t], m, t) → i, that will generate a new
image resembling a view of the brain surface from the 2d projection of the input
mesh m following [r|t], and a texture t. several methods can be used to optimize
θ. however, they require a large set of annotated data [3,24] or perform only on
modalities with similar sensors [12,27]. generating rgb images from mri scans is
a challenging task because it requires bridging a significant difference in
image modalities. we choose to use a neural image analogy method that combines
the texture of a source image with a high-level content representation of a
target image without the need for a large dataset [1]. this approach transfers
the texture from t to i constrained by the projection of m using a[r|t] by
minimizing the following loss function:where g l ij (t ) is the gram matrix of
texture t at the l-th convolutional layer (pre-trained vgg-19 model), and w l,c
t class are the normalization factors for each gram matrix, normalized by the
number of pixels in a label class c of t class . this allows for the
quantification of the differences between the texture image t and the generated
image i as it is being generated. importantly, computing the inner-most sum over
each label class c allows for texture comparison within each class, for
instance: the background, the parenchyma, and the cortical vessels.in practice,
we assume constant camera parameters a and first sample a set of binary images
by randomly varying the location and orientation of a virtual camera [r|t]
w.r.t. to the 3d mesh m before populating the binary images with the textures
using s θ (see fig. 2). we restrict this sampling to the upper hemisphere of the
3d mesh to remain consistent with the plausible camera positions w.r.t.
patient's head during neurosurgery.we use the l-bfgs optimizer and 5
convolutional layers of vgg-19 to generate each image following [1] to find the
resulting parameters θ. the training to synthesize for a single image typically
takes around 50 iterations to converge.",9
305,2246,Learning Expected Appearances for Intraoperative Registration During Neurosurgery,2.3,Pose Regression Network,"in order to solve eq. 1, we assume a known focal length that can be obtained
through pre-calibration. to obtain a compact representation of the rotation and
since poses are restricted to the upper hemisphere of the 3d mesh (no gimbal
lock), the euler-rodrigues representation is used. therefore, there are six
parameters to be estimated: rotations r x , r y , r z and translations t x , t y
, t z . we estimate our 6-dof pose with a regression network p ω : i → p and
optimize its weights ω to map each synthetic image i to its corresponding camera
pose p = [r x , r y , r z , t x , t y , t z ] t .the network architecture of p ω
consists of 3 blocks each composed of two convolutional layers and one relu
activation. to decrease the spatial dimension, an average pooling layer with a
stride of 2 follows each block except the last one. at the end of the last
hierarchy, we add three fully-connected layers with 128, 64, and 32 neurons and
relu activation followed by one fully-connected with 6 neurons with a linear
activation. we use the set of generated expected appearances t p = {(i i ; p i
)} i ; and optimize the following loss function over the parameters ω of the
network p ω :where t and r vec are the translation and rotation vector,
respectively. we experimentally noticed that optimizing these entities
separately leads to better results. the model is trained for each case (patient)
for 200 epochs using mini-batches of size 8 with adam optimizer and a learning
rate of 0.001 and decays exponentially to 0.0001 over the course of the
optimization. finally, at run-time, given an image i we directly predict the
corresponding 3d pose p so that: p ← p(i; ω), where ω is the resulting
parameters from the training.",9
306,2247,Learning Expected Appearances for Intraoperative Registration During Neurosurgery,3.0,Results,"dataset. we tested our method retrospectively on 6 clinical datasets from 6
patients (cases) (see fig. 5). these consisted of preoperative t1 contrast mri
scans and intraoperative images of the brain surface after dura opening.
cortical vessels around the tumors were segmented and triangulated to generate
3d meshes using 3d slicer. we generated 100 poses for each 3d mesh (i.e.: each
case) and used a total of 15 unique textures from human brain surfaces
(different from our 6 clinical datasets) for synthesis using s θ . in order to
account for potential intraoperative brain deformations [4] we augment the
textured projection with elastic deformation [21] resulting in approximately
1500 images per case. the surgical images of the brain (left image of the
stereoscopic camera) were acquired with a carl zeiss surgical microscope. the
ground-truth poses were obtained by manually aligning the 3d meshes on their
corresponding images. we evaluated the pose regressor network on both synthetic
and real data. the model training and validation were performed on the
synthesized images while the model testing was performed on the real images.
because a conventional train/validation/test split would lead to texture
contamination, we created our validation dataset so that at least one texture is
excluded from the training set. on the other hand, the test set consisted of the
real images of the brain surface acquired using the surgical camera and are
never used in the training. accuracy-threshold curves on the validation
set.metrics. we chose the average distance metric (add) as proposed in [23] for
evaluation. given a set of mesh's 3d vertices, the add computes the mean of the
pairwise distance between the 3d model points transformed using the ground truth
and estimated transformation. we also adjusted the default 5 cm-5 deg
translation and rotation error to our neurosurgical application and set the new
threshold to 3 mm-3 deg.accuracy-threshold curves. we calculated the number of
'correct' poses estimated by our model. we varied the distance threshold on the
validation sets (excluding 2 textures) in order to reveal how the model performs
w.r.t. that threshold. we plotted accuracy-threshold curves showing the
percentage of pose accuracy variation with a threshold in a range of 0 mm to 20
mm. we can see in fig. 3 that a 80.23% pose accuracy was reached within the 3
mm-3 deg threshold for all cases. this accuracy increases to 95.45% with a 5
mm-5 deg threshold.",9
307,2249,Learning Expected Appearances for Intraoperative Registration During Neurosurgery,4.0,Discussion and Conclusion,"clinical feasibility. we have shown that our method is clinically viable. our
experiments using clinical data showed that our method provides accurate
registration without manual intervention, that it is computationally efficient,
and it is invariant to the visual appearance of the cortex. our method does not
require intraoperative 3d imaging such as intraoperative mri or ultrasound,
which require expensive equipment and are disruptive during surgery. training
patient-specific models from preoperative imaging transfers computational tasks
to the preoperative stage so that patient-to-image registration can be performed
in near real-time from live images acquired from a surgical
microscope.limitations. the method presented in this paper is limited to 6-dof
pose estimation and does not account for deformation of the brain due to changes
in head position, fluid loss, or tumor resection and assumes a known focal
length. in the future, we will expand our method to model non-rigid deformations
of the 3d mesh and to accommodate expected changes in zoom and focal depth
during surgery. we will also explore how texture variability can be controlled
and adapted to the observed image to improve model accuracy.",9
308,2250,Learning Expected Appearances for Intraoperative Registration During Neurosurgery,0.0,Conclusion.,"we introduced expected appearances, a novel learning-based method for
intraoperative patient-to-image registration that uses synthesized expected
images of the operative field to register preoperative scans with intraoperative
views through the surgical microscope. we demonstrated state-ofthe-art,
real-time performance on challenging neurosurgical images using our method. our
method could be used to improve accuracy in neuronavigation and in image-guided
surgery in general.",9
309,2251,Dose Guidance for Radiotherapy-Oriented Deep Learning Segmentation,1.0,Introduction,"radiotherapy (rt) has proven effective and efficient in treating cancer
patients. however, its application depends on treatment planning involving
target lesion and radiosensitive organs-at-risk (oar) segmentation. this is
performed to guide radiation to the target and to spare oar from inappropriate
irradiation. hence, this manual segmentation step is very time-consuming and
must be performed accurately and, more importantly, must be patient-safe.
studies have shown that the manual segmentation task accounts for over 40% of
the treatment planning duration [7] and, in addition, it is also error-prone due
to expert-dependent variations [2,24]. hence, deep learning-based (dl)
segmentation is essential for reducing time-to-treatment, yielding more
consistent results, and ensuring resource-efficient clinical workflows.nowadays,
training of dl segmentation models is predominantly based on loss functions
defined by geometry-based (e.g., softdice loss [15]), distributionbased
objectives (e.g., cross-entropy), or a combination thereof [13]. the general
strategy has been to design loss functions that match their evaluation
counterpart. nonetheless, recent studies have reported general pitfalls of these
metrics [4,19] as well as a low correlation with end-clinical objectives
[11,18,22,23]. furthermore, from a robustness point of view, models trained with
these loss functions have been shown to be more prone to generalization issues.
specifically, the dice loss, allegedly the most popular segmentation loss
function, has been shown to have a tendency to yield overconfident trained
models and lack robustness in out-of-distribution scenarios [5,14]. these
studies have also reported results favoring distribution-matching losses, such
as the cross-entropy being a strictly proper scoring rule [6], providing
better-calibrated predictions and uncertainty estimates. in the field of rt
planning for brain tumor patients, the recent study of [17] shows that current
dl-based segmentation algorithms for target structures carry a significant
chance of producing false positive outliers, which can have a considerable
negative effect on applied radiation dose, and ultimately, they may impact
treatment effectiveness. in rt planning, the final objective is to produce the
best possible radiation plan that jointly targets the lesion and spares healthy
tissues and oars. therefore, we postulate that training dl-based segmentation
models for rt planning should consider this clinical objective.in this paper, we
propose an end-to-end training loss function for dl-based segmentation models
that considers dosimetric effects as a clinically-driven learning objective. our
contributions are: (i) a dosimetry-aware training loss function for dl
segmentation models, which (ii) yields improved model robustness, and (iii)
leads to improved and safer dosimetry maps. we present results on a clinical
dataset comprising fifty post-operative glioblastoma (gbm) patients. in
addition, we report results comparing the proposed loss function, called
dose-segmentation loss (doselo), with models trained with a combination of
binary cross-entropy (bce) and softdice loss functions.",9
310,2252,Dose Guidance for Radiotherapy-Oriented Deep Learning Segmentation,2.0,Methodology,"figure 1 describes the general idea of the proposed doselo. a segmentation model
(u-net [20]) is trained to output target segmentation predictions for the gross
tumor volume (gtv) based on patient mri sequences. predicted segmentations and
their corresponding ground-truth (gt) are fed into a dose predictor model, which
outputs corresponding dose predictions (denoted as d p and d p in fig. 1). a
pixel-wise mean squared error between both dose predictions is then a
segmentation model (u-net [20]) is trained to output target segmentation
predictions ( st ) for the gross tumor volume (gtv) based on patient mri
sequences imr. predicted ( st ) and ground-truth segmentations (st ) are fed
into the dose predictor model along with the ct-image (ict ), and oar
segmentation (sor). the dose predictor outputs corresponding dose predictions dp
and dp . a pixel-wise mean squared error between both dose predictions is
calculated, and combined with the binary crossentropy (bce) loss to form the
final loss, l total = lbce + λldsl. calculated and combined with the bce loss to
form the final loss. in the next sections we describe the adopted dose
prediction model [9,12], and the proposed doselo.",9
311,2253,Dose Guidance for Radiotherapy-Oriented Deep Learning Segmentation,2.1,Deep Learning-Based Dose Prediction,"recent dl methods based on cascaded u-nets have demonstrated the feasibility of
generating accurate dose distribution predictions from segmentation masks,
approximating analytical dose maps generated by rt treatment planning systems
[12]. originally proposed for head and neck cancer [12], this approach has been
recently extended for brain tumor patients [9] with levels of prediction error
below 2.5 gy, which is less than 5% of the prescribed dose. this good level of
performance, along with its ability to yield near-instant dose predictions,
enables us to create a training pipeline that guides learned features to be
dose-aware.following [12], the dose predictor model consists of a cascaded u-net
(i.e., the input to the second u-net is the output of the first concatenated
with the input to the first u-net) trained on segmentation masks, ct images, and
reference dose maps. the model's input is a normalized ct volume and
segmentation masks for target volume and oars. as output, it predicts a
continuous-valued dose map of the same dimension as the input. the model is
trained via deep supervision as a linear combination of l2-losses from the
outputs of each u-net in the cascade. we refer the reader to [9,12] for further
implementation details. we remark that the dose predictor model was also trained
with data augmentation, so imperfect segmentation masks and corresponding dose
plans are included. this allows us in this study to use the dose predictor to
model the interplay between segmentation variability and dosimetric
changes.formally, the dose prediction model m d receives as inputs:
segmentations masks for the gtv s t ∈ z w ×h and the oars s or ∈ z w ×h , the ct
image (used for tissue attenuation calculation purposes in rt) i ct ∈ r w ×h ,
and outputs m d (s t , s or , i ct ) → d p ∈ r w ×h , a predicted dose map where
each pixel value in d corresponds to the local predicted dose in gy. due to the
limited data availability, we present results using 2d-based models but remark
that their extension to 3d is straightforward. working in 2d is also feasible
from an rt point of view because the dose predictor is based on co-planar
volumetric modulated arc therapy (vmat) planning, commonly used in this clinical
scenario.",9
312,2256,Dose Guidance for Radiotherapy-Oriented Deep Learning Segmentation,0.0,Dose Prediction:,"the dose prediction model was trained on an in-house dataset comprising a total
of 50 subjects diagnosed with post-operative gbm. this includes ct imaging data,
segmentation masks of 13 oars, and the gtv. gtvs were defined according to the
estro-acrop guidelines [16]. the oars were contoured by one radiotherapist
according to [21] and verified by mutual consensus of three experienced
radiation oncology experts. each subject had a reference dose map, calculated
using a standardized clinical protocol with eclipse (varian medical systems
inc., palo alto, usa). this reference was generated on basis of a double arc
co-planar vmat plan to deliver 30 times 2 gy while maximally sparing oars. we
divided the dataset into training (35 cases), validation (5 cases), and testing
(10 cases). we refer the reader to [9] for further details.segmentation models:
to develop and test the proposed approach, we employed a separate in-house
dataset (i.e., different cases than those used to train the dose predictor
model) of 50 cases from post-operative gmb patients receiving standard rt
treatment. we divided the dataset into training (35 cases), validation (5
cases), and testing (10 cases). all cases comprise a planning ct registered to
the standard mri images (t1-post-contrast (gd), t1-weighted, t2-weighted,
flair), and gt segmentations containing oars as well as the gtv. we note that
for this first study, we decided to keep the dose prediction model fixed during
the training of the segmentation model for a simpler presentation of the concept
and modular pipeline. hence, only the parameters of the segmentation model are
updated.",9
313,2260,Dose Guidance for Radiotherapy-Oriented Deep Learning Segmentation,3.3,Results,"figure 2 shows results on the test set, sorted by their dosimetric impact. we
found an overall reduction of the relative mean absolute error (rmae) with
respect to the reference dose maps, from 0.449 ± 0.545, obtained via the
bce+softdice combo-loss, to 0.258 ± 0.201 for the proposed doselo (i.e., an
effective 42.5% reduction with λ = 1). this significant dose error reduction
shows the ability of the proposed approach to yield segmentation results in
better agreement with dose maps obtained using gt segmentations than those
obtained using the state-of-the-art bce+softdice combo-loss.table 1 shows
results for the first and most significant four cases from a rt point of view
(due to space limitations, all other cases are shown in supplementary material).
we observe the ability of the proposed approach to significantly reduce
outliers, generating a negative dosimetry impact on the dose fig. 2. relative
mean absolute dose errors/differences (rmae) between the reference dose map and
dose maps obtained using the predicted segmentations. lower is better. across
all tested cases and folds we observe a large rmae reduction for dose maps using
the proposed doselo (average rmae reduction of 42.5%).maps. we analyzed case
number 3, 4, and 5 from fig. 2 for which the standard bce+softdice was slightly
better than the proposed doselo. for case no. 3 the tumor presents a non-convex
shape alongside the skull's parietal lobe, which was not adequately modeled by
the training dataset used to train the segmentation models. indeed, we remark
that both models failed to yield acceptable segmentation quality in this area.
in case no. 4, both models failed to segment the diffuse tumor area alongside
the skull; however, as shown in fig. 2-case no. 4, the standard bce+softdice
model would yield a centrally located radiation dose, with strong negative
clinical impact to the patient. case no. 5 (shown in supplementary material) is
an interesting case called butterfly gbm, which is a rare type of gbm (around 2%
of all gbm cases [3]), characterized by bihemispheric involvement and invasion
of the corpus callosum. in this case, the training data also lacked
characterization for such cases. despite this limitation, we observed favorable
dose distributions with the proposed method.although we are aware that classical
segmentation metrics poorly correlate with dosimetric effects [18], we report
that the proposed method is more robust than the baseline bce+softdice loss
function, which yields outliers with hausdorff distances: 64.06 ± 29.84 mm vs
28.68 ± 22.25 mm (-55.2% reduction) for the proposed approach. as pointed out by
[17], segmentation outliers can have a detrimental effect on rt planning. we
also remark that the range of hd values is in range with values reported by
models trained using much more training data (see [1]), alluding to the
possibility that the problem of robustness might not be directly solvable with
more data. dice coefficients did not deviate significantly between the baseline
and the doselo models (dsc: 0.713 ± 0.203 (baseline) vs. 0.697 ± 0.216
(doselo)).table 1. comparison of dose maps and their absolute differences to the
reference dose maps (bce+softdice (bce+sd), and the proposed doselo). it can be
seen that doselo yields improved dose maps, which are in better agreement with
the reference dose maps (dose map color scale: 0 (blue) -70gy (red)).",9
314,2261,Dose Guidance for Radiotherapy-Oriented Deep Learning Segmentation,4.0,Discussion and Conclusion,"the ultimate goal of dl-based segmentation for rt planning is to provide
reliable and patient-safe segmentations for dosimetric planning and optimally
targeting tumor lesions and sparing of healthy tissues. however, current loss
functions used to train models for rt purposes rely solely on geometric
considerations that have been shown to correlate poorly with dosimetric
objectives [11,18,22,23]. in this paper, we propose a novel dosimetry-aware
training loss function, called doselo, to effectively guide the training of
segmentation models toward dosimetric-compliant segmentation results for rt
purposes. the proposed doselo uses a fast-dose map prediction model, enabling
model guidance on how dosimetry is affected by segmentation variations. we merge
this information into a simple yet effective loss function that can be combined
with existing ones. these first results on a dataset of post-operative gbm
patients show the ability of the proposed doselo to deliver improved
dosimetric-compliant segmentation results. future work includes extending our
database of gbm cases and to other anatomies, as well as verifying potential
improvements when cotraining the segmentation and dose predictor models, and
jointly segmenting gtvs and oars. with this study, we hope to promote more
research toward clinically-relevant dl training loss functions.",9
322,2273,FocalErrorNet: Uncertainty-Aware Focal Modulation Network for Inter-modal Registration Error Estimation in Ultrasound-Guided Neurosurgery,1.0,Introduction,"resection of early-stage brain tumors can greatly reduce the mortality rate of
patients. during the surgery, brain tissue deformation (called brain shift) can
occur due to various causes, such as gravity, drug administration, and pressure
change after craniotomy. while modern magnetic resonance imaging (mri)
techniques can provide rich anatomical and physiological information with
various contrasts (e.g., fmri) for more elaborate pre-surgical planning,
intra-operative mri that can track brain shift requires a complex setup and is
costly. in contrast, intra-operative ultrasound (ius) has gained popularity for
real-time imaging during surgery to monitor tissue deformation and surgical
tools because of its lower cost, portability, and flexibility [1]. accurate and
robust mri-ius registration techniques [2] can greatly enhance the value of ius
for updating pre-surgical plans and guiding the interpretation of ius, which has
an unintuitive contrast and non-standard orientations. this can greatly enhance
the safety and outcomes of the surgical procedure by allowing maximum brain
tumor removal while avoiding eloquent regions [3]. however, as the true
underlying tissue deformation is unknown due to the 3d nature of the surgical
data and the time constraint, real-time manual inspection of mri-ius
registration results is challenging and error-prone, especially for
precision-sensitive neurosurgery. therefore, algorithms that can detect and
quantify unreliable inter-modal medical image registration results are highly
beneficial.recently, automatic quality assessment for medical image registration
has attracted increasing attention [4] from the domains of big medical data
analysis and surgical interventions. with high efficiency, machine, and deep
learning techniques have been proposed to allow automatic grading and dense
estimation of medical image registration errors. early endeavors on this topic
primarily relied on hand-crafted features, including information theory-based
metrics [5][6][7][8][9][10]. more recently, deep learning (dl) techniques that
learn task-specific features have also been adopted in automatic evaluation of
medical image registration, with a primary focus on intra-contrast/modal
applications, including ct [9,10] and mri [11]. unfortunately, so far, error
grading and estimation in inter-contrast/modal registration have rarely been
explored, despite the particular demand in surgical applications. in this
direction, bierbrier et al. [12] made the first attempt using simulated ius from
mri to train 3d convolutional neural networks (cnns) to perform dense error
regression for mri-ius registration in brain tumor resection. although their
algorithm performed well in simulated cases, the results on real clinical scans
still required improvements. in this paper, we propose a novel 3d cnn to perform
patch-wise error estimation for mri-ius registration in neurosurgery, by using
focal modulation [13], a recent alternative dl technique to self-attention [14]
for encoding contextual information, and uncertainty estimation. we call our
method focalerrornet, which has three main novelties. first, we adapted the
focal modulation network [13] from 2d to 3d and employed the technique in
registration error assessment for the first time. second, we incorporated
uncertainty estimation using monte carlo (mc) dropouts [15] to offer assurance
for error regression. lastly, we developed and thoroughly evaluated our
technique against a recent baseline model [12] using real clinical data and
showed excellent results.",9
323,2274,FocalErrorNet: Uncertainty-Aware Focal Modulation Network for Inter-modal Registration Error Estimation in Ultrasound-Guided Neurosurgery,2.1,Dataset and Preprocessing,"for methodological development and assessment, we used the resect
(retro-spective evaluation of cerebral tumors) dataset [16], which has
pre-operative mri, and ius scans at different surgical stages from 23 subjects
who underwent low-grade glioma resection surgeries. as it is still challenging
to model ius scans with tissue resection, we took 22 cases with t2flair mri that
better depicts tumor boundaries and ius acquired before resection. an example of
an mri-ius pair from a patient is shown in fig. 1. we hypothesized that directly
leveraging clinical ius could help learn more realistic image features with
potentially better outcomes in clinical applications than with simulated
contrasts [9,12]. however, since the true brain shift model is impossible to
obtain, we followed the strategy of creating silver ground truths for image
alignment [9,12], upon which simulated misalignment is augmented in the ius to
build and test our dl model. to create the silver registration ground truths, we
used the homologous landmarks between mri and ius in the resect dataset to
perform landmark-based 3d b-spline nonlinear registration to register ius to the
corresponding mri for all 22 cases. to tackle the limited field of view (fov) in
ius, we cropped the t2flair mri to the same fov of the ius, which was resampled
to a 0.5 × 0.5 × 0.5 mm 3 resolution. to perform spatial misalignment
augmentation, we continued to leverage 3d b-spline transformation, similar to
earlier reports on the same topic [10,12,17]. in short, b-spline transformation
can be modeled by a grid of regularly spaced control points and the associated
parameters to allow various levels of nonlinear deformation. while the spacing
of the control points determines the levels of details in local deformation
fields, the displacement parameters control the magnitude of the deformation. to
ensure that simulated registration errors are of different varieties and sizes,
we randomly selected the number of control points and the associated
displacements (in each 3d axis) with a maximum of 20 points and 30 mm,
respectively. note that the control point grid is isotropic, and the density is
arbitrarily determined per deformation in our case. each coregistered ius scan
was deformed ten times. after misalignment augmentation on the previously
co-registered ius, matching pairs of 3d image patches of size 33 × 33 × 33
voxels were taken from both the ius volume and the corresponding mri. as ius has
limited fov and may contain no anatomical features, to ensure that the patches
we extracted contain useful information (e.g. to avoid the dark background) in
ius, we focused on acquiring patches centered around the anatomical landmark
locations available through the resect database. since b-spline transformation
offers a displacement vector at each voxel of the ius volume, we directly
considered the norm of the vector as the simulated registration error at the
associated voxel. in our design, we determined the registration error of the
image patch pair as the mean of all voxel-wise errors within the ius patch.
finally, the image patch pairs, along with corresponding registration errors
were then fed to the proposed dl algorithm for training and validation.",9
324,2276,FocalErrorNet: Uncertainty-Aware Focal Modulation Network for Inter-modal Registration Error Estimation in Ultrasound-Guided Neurosurgery,2.3,Uncertainty Quantification,"for registration error regression in surgical applications, knowledge regarding
the reliability of the automated results is instrumental for the safety and
wellbeing of the patients. uncertainty estimation has gained popularity in
probing the trustworthiness and credence of dl algorithms. although the concept
has been widely applied in image segmentation and classification, it has not
been employed for registration error estimation, especially in the case of
multi-modal situations, such as mri-ius alignment. therefore, we incorporated
uncertainty estimation in our proposed focalerrornet. for each mri-ius patch
pair, 200 regression samples were collected by random sampling from mc dropouts
[15] at test time. while the final patch registration error was obtained as the
mean of all the samples, the sample standard deviation was used as the
uncertainty metric.",9
325,2277,FocalErrorNet: Uncertainty-Aware Focal Modulation Network for Inter-modal Registration Error Estimation in Ultrasound-Guided Neurosurgery,2.4,Experimental Setup and Implementation,"from the transformation augmentation, we acquired 3380 samples of mri-ius pairs.
for our experiments, we arbitrarily split the subjects into training,
validation, and test sets with the proportion of 60%, 20%, and 20%,
respectively. to prevent information leakage, we ensured that each patient was
included in only one of the split sets. for model training, we adopted the adam
optimization with a learning rate of 5 × 10 -5 and a batch size of 64. for the
loss function, we used mean squared error (mse) to minimize the difference
between the predicted mri-ius registration error and the ground truths.
furthermore, in addition to the transformation augmentation, we also included
additional data augmentation, including random noise addition and random image
flipping on training sets to mitigate overfitting and increase the model's
generalizability. to assess our proposed focalerrornet, we compared it against a
3d cnn [9,12] (see fig. 3) that was employed for medical image registration
error regression. the two dl models were trained with the same dataset and
procedure, and their prediction accuracies, measured as the absolute error
between the predicted and ground truths mis-registration on the test set were
compared with two-sided pairedsamples t-tests to confirm the superiority of the
proposed method, in addition to correlations between their estimated and ground
truth errors. to validate the proposed uncertainty estimation method, we
calculated the correlation between the uncertainty measure and absolute error of
focalerrornet, and the correlation between the uncertainty and mutual
information between mri and ius, which is often used to measure the information
overlap in multi-modal registration. finally, to test the robustness of the
focalerrornet, we acquired additional mri-ius patch pairs from the test
subjects, by introducing random linear shifts (the max displacement from
landmark locations is 10 voxels) from the selected locations in the original
set, and evaluated the dl model performance.",9
326,2281,FocalErrorNet: Uncertainty-Aware Focal Modulation Network for Inter-modal Registration Error Estimation in Ultrasound-Guided Neurosurgery,4.0,Discussion,"in image-guided interventions, there is an urgent need for automatic assessment
of image registration quality. multi-modal registration quality evaluation poses
major challenges due to three main factors. first, dissimilar contrasts between
images require more elaborate strategies to derive relevant features for error
assessment. second, unlike segmentation or classification, the ground truths of
registration errors are difficult to obtain. finally, compared with
classification, regression tasks tend to be more error-prone for deep learning
algorithms. to tackle these challenges, we employed 3d focal modulation with
depth-wise convolution to encode contextual information for the image pair.
compared with the vit and its variants, focal modulation allows a more
lightweight setup, which could be desirable for 3d data. although we admit that
residual errors still remain after landmark-based b-spline nonlinear alignment,
this approach has been adopted in different prior studies, considering the
residual landmark registration error is fairly low (mtre of 0.0008 ± 0.0010mm).
although simulated ultrasound has been used to provide a perfect alignment with
mris, the fidelity of the simulated results is still suboptimal, and this may
explain the underperformance of the previous technique in real clinical data
[12]. to ensure the performance of our focalerrornet, we opted to regress the
mean registration error of image patches than simplistic error grades or
voxel-wise error maps. we believe that this design choice offers a more stable
performance, which is supported by our validation. we adopted uncertainty
estimation in inter-modal registration error assessment for the first time.
while other techniques exist to provide model uncertainty [18], mc dropout is
more flexible for various dl models. furthermore, the use of standard deviation
as an uncertainty measurement maintains the same unit as the regressed errors,
thus making the interpretation more intuitive. from quantitative and qualitative
evaluations using correlation coefficients and scatter plots to assess the
association of uncertainty measures with the prediction errors and image
entropy, we confirmed the validity of the proposed uncertainty estimation
approach. for our focalerrornet, we achieved a prediction error of 0.59 ± 0.57
mm, which is on par with the image resolution (0.5 mm). additionally, the
standard deviation of our results is lower than the baseline model [12]. these
signify a robust performance of the focalerrornet. one limitation of our work
lies in the limited patient data, as public ius datasets are scarce, while the
settings and properties of us scanners can vary, potentially affecting the dl
model designs. therefore, we created random deformations for patch-wise error
estimation, and will further explore data-efficient approaches for registration
error assessment.",9
327,2283,Detecting the Sensing Area of a Laparoscopic Probe in Minimally Invasive Cancer Surgery,1.0,Introduction,"cancer remains a significant public health challenge worldwide, with a new
diagnosis occurring every two minutes in the uk (cancer research uk 1 ). surgery
is one of the main curative treatment options for cancer. however, despite
substantial advances in pre-operative imaging such as ct, mri, or pet/spect to
aid diagnosis, surgeons still rely on the sense of touch and naked eye to detect
cancerous tissues and disease metastases intra-operatively due to the lack of
reliable intraoperative visualization tools. in practice, imprecise
intraoperative cancer tissue detection and visualization results in missed
cancer or the unnecessary removal of healthy tissues, which leads to increased
costs and potential harm to the patient. there is a pressing need for more
reliable and accurate intraoperative visualization tools for minimally invasive
surgery (mis) to improve surgical outcomes and enhance patient care. a recent
miniaturized cancer detection probe (i.e., 'sensei r ' developed by lightpoint
medical ltd.) leverages the cancer-targeting ability of nuclear agents typically
used in nuclear imaging to more accurately identify cancer intraoperatively from
the emitted gamma signal (see fig. 1b) [6]. however, the use of this probe
presents a visualization challenge as the probe is non-imaging and is air-gapped
from the tissue, making it challenging for the surgeon to locate the
probe-sensing area on the tissue surface.it is crucial to accurately determine
the sensing area, with positive signal potentially indicating cancer or affected
lymph nodes. geometrically, the sensing area is defined as the intersection
point between the gamma probe axis and the tissue surface in 3d space, but
projected onto the 2d laparoscopic image. however, it is not trivial to
determine this using traditional methods due to poor textural definition of
tissues and lack of per-pixel ground truth depth data. similarly, it is also
challenging to acquire the probe pose during the surgery.problem redefinition.
in this study, in order to provide sensing area visualization ground truth, we
modified a non-functional 'sensei' probe by adding a miniaturized laser module
to clearly optically indicate the sensing area on the laparoscopic images -i.e.
the 'probe axis-surface intersection'. our system consists of four main
components: a customized stereo laparoscope system for capturing stereo images,
a rotation stage for automatic phantom movement, a shutter for illumination
control, and a daq-controlled switchable laser module (see fig. 1a). with this
setup, we aim to transform the sensing area localization problem from a
geometrical issue to a high-level content inference problem in 2d. it is
noteworthy that this remains a challenging task, as ultimately we need to infer
the probe axis-surface intersection without the aid of the laser module to
realistically simulate the use of the 'sensei' probe.",9
331,2332,X2Vision: 3D CT Reconstruction from Biplanar X-Rays with Deep Structure Prior,1.0,Introduction,"tomographic imaging estimates body density using hundreds of x-ray projections,
but it's slow and harmful to patients. acquisition time may be too high for
certain applications, and each projection adds dose to the patient. a quick,
low-cost 3d estimation of internal structures using only bi-planar x-rays can
revolutionize radiology, benefiting dental imaging, orthopedics, neurology, and
more. this can improve image-guided therapies and preoperative planning,
especially for radiotherapy, which requires precise patient positioning with
minimal radiation exposure.however, this task is an ill-posed inverse problem:
x-ray measurements are the result of attenuation integration across the body,
which makes them very fig. 1. current methods vs our method. feed-forward
methods do not manage to predict a detailed and matching tomographic volume from
a few projections. iterative methods based on neural radiance fields lack prior
for good reconstruction. by learning an embedding for the possible volumes, we
can recover an accurate volume from very few projections with an optimization
based on a bayesian formulation.ambiguous. traditional reconstruction methods
require hundreds of projections to get sufficient constraints on the internal
structures. with very few projections, it is very difficult to disentangle the
structures for even coarse 3d estimation. in other words, many 3d volumes may
have generated such projections a priori.classical analytical and iterative
methods [8] fail when very few projections are available. several works have
attempted to largely decrease the number of projections needed for an accurate
volumetric reconstruction. some deep learning methods [7,12,24,25,30] predict
directly a 3d volume in a forward way from very few projections. the volume is
however not guaranteed to be consistent with the projections and it is not clear
which solution is retrieved. other recent methods have adapted nerfs [20] to
tomographic reconstruction [23,31]. these non-learning methods show good results
when the number of input projections remains higher than a dozen but fail when
very few projections are provided, as our experiments in sect. 3.3 show.as
illustrated in fig. 1, to be able to reconstruct a volume accurately given as
low as two projections only, we first learn a prior on the volume. to do this,
we leverage the potential of generative models to learn a low-dimensional
manifold of the target body part. given projections, we find by a bayesian
formulation the intermediate latent vectors conditioning the generative model
that minimize the error between synthesized projections of our reconstruction
and these input projections. our work builds on hong et al. [10]'s 3d
style-based generative model, which we extend via a more complex network and
training framework.compared to other 3d gans, it is proven to provide the best
disentanglement of the feature space related to semantic features [2].by
contrast with feed-forward methods, our approach does not require paired
projections-reconstructions, which are very tedious to acquire, and it can be
used with different numbers of projections and different projection geometries
without retraining. compared to nerf-based methods, our method exploits prior
knowledge from many patients to require only two projections. we evaluate our
method on reconstructing cancer patients' head-and-neck cts, which involves
intricate and complicated structures. we perform several experiments to compare
our method with a feed-forward-based method [30] and a recent nerf-based method
[23], which are the previous state-of-the-art methods for the very few or few
projections cases, respectively.we show that our method allows to retrieve
results with the finest reconstructions and better matching structures, for a
variety of number of projections. to summarize, our contributions are two-fold:
(i) a new paradigm for 3d reconstruction with biplanar x-rays: instead of
learning to invert the measurements, we leverage a 3d style-based generative
model to learn deep image priors of anatomic structures and optimize over the
latent space to match the input projections; (ii) a novel unsupervised method,
fast and robust to sampling ratio, source energy, angles and geometry of
projections, all of which making it general for downstream applications and
imaging systems.",10
332,2336,X2Vision: 3D CT Reconstruction from Biplanar X-Rays with Deep Structure Prior,2.3,Reconstruction from Biplanar Projections,"since our generative model provides a volume v as a function of vectors w and n,
we can reparameterize our optimization from eq. ( 1) into:note that by contrast
with [18] for example, we optimize on the noise vectors n as well: as we
discovered in our early experiments, the n are also useful to embed
high-resolution details. we take our regularization term r(w, n) as:term l w (w)
=k log n (w k |μ, σ) ensures that w lies on the same distribution as during
training. n (•|μ, σ) represents the density of the standard normal distribution
of mean μ and standard deviation σ.term l c (w) =i,j log m(θ i,j |0, κ)
encourages the w i vectors to be collinear so to keep the generation of
coarse-to-fine structures coherent. m(•; μ, κ) is the density of the von mises
distribution of mean μ and scale κ, which we take fixed, and θ i,j = arccos(
wi•wj wi wj ) is the angle between vectors w i and w j .term l n (n) =j log n (n
j |0, i ) ensures that the n j lie on the same distribution as during training,
i.e., a multivariate standard normal distribution. the λ * are fixed
weights.projection operator. in practice, we take operator a as a 3d cone beam
projection that simulates x-ray attenuation across the patient, adapted from
[21,27]. we model a realistic x-ray attenuation as a ray tracing projection
using material and spectrum awareness:with μ(m, e) the linear attenuation
coefficient of material m at energy state e that is known [11], t m the material
thickness, i 0 the intensity of the source x-ray.for materials, we consider the
bones and tissues that we separate by threshold on electron density. a inverts
the attenuation intensities i atten to generate an x-ray along few directions
successively. we make a differentiable using [21] to allow end-to-end
optimization for reconstruction.3 experiments and results",10
333,2337,X2Vision: 3D CT Reconstruction from Biplanar X-Rays with Deep Structure Prior,3.1,Dataset and Preprocessing,"manifold learning. we trained our model with a large dataset of 3500 cts of
patients with head-and-neck cancer, more exactly 2297 patients from the publicly
available the cancer imaging archive (tcia) [1,6,16,17,28,32] and 1203 from
private internal data, after obtention of ethical approbations. we split this
data into 3000 cases for training, 250 for validation, and 250 for testing. we
focused ct scans on the head and neck region above shoulders, with a resolution
of 80 × 96 × 112, and centered on the mouth after automatic segmentation using a
pre-trained u-net [22]. the cts were preprocessed by min-max normalization after
clipping between -1024 and 2000 hounsfield units (hu).3d reconstruction. to
evaluate our approach, we used an external private cohort of 80 patients who had
undergone radiotherapy for head-and-neck cancer, with their consent. planning ct
scans were obtained for dose preparation, and cbct scans were obtained at each
treatment fraction for positioning with full gantry acquisition. as can be seen
in fig. 3 and the supplementary material, all these cases are challenging as
there are large changes between the original ct scan and the cbct scans. we
identified these cases automatically by comparing the cbcts with the planning
cts. to compare our reconstruction in the calibrated hu space, we registered the
planning cts on the cbcts by deformable registration with mrf minimization [4].
we hence obtained 3d volumes as virtual cts we considered as ground truths for
our reconstructions after normalization. from these volumes, we generated
projections using the projection module described in sect. 2.3.",10
334,2339,X2Vision: 3D CT Reconstruction from Biplanar X-Rays with Deep Structure Prior,3.3,Results and Discussion,"manifold learning. we tested our model's ability to learn the low-dimensional
manifold. we used fid [9] to measure the distance between the distribution of
generated volumes and real volumes, and ms-ssim [29] to evaluate volumes'
diversity and quality. we obtained a 3d fid of 46 and a ms-ssim of 0.92. for
reference, compared to 3dstylegan [10], our model achieved half their fid score
on another brain mri dataset, with comparable ms-ssim. this may be due to a more
complex architecture, discriminator augmentation, or simpler anatomy.baselines.
we compared our method against the main feed-forward method x2ct-gan [30] and
the neural radiance fields with prior image embedding method nerp [23] meant for
modest sparsely-sampled reconstruction. recent methods like [24] and [12] were
excluded because they provide only minor improvements compared to x2ct-gan [30]
and have similar constraints to feed-forward methods. additionally, no public
implementation is available. [26] uses a flow-based generative model, but the
results are of lower quality compared to gans and similar to x2ct-gan [30].3d
reconstruction. to evaluate our method's performance with biplanar projections,
we focused on positioning imaging for radiotherapy. figure 3 compares our
reconstruction with those of the baselines from biplanar projections. our method
achieves better fitting of the patient structure, including bones, tissues, and
air separations, almost matching the real ct volume. x2ct-gan [30] produced
realistic structures, but failed to match the actual structures as it does not
enforce consistency with the projections. in some clinical procedures, an
earlier ct volume of the patient may be available and can be used as an
additional input for nerp [23]. without a previous ct volume, nerp lacks the
necessary prior to accurately solve the ill-posed problem. even when initialised
with a previous ct volume, nerp often fails to converge to the correct volume
and introduces many artifacts when few projections are used. in contrast, our
method is more versatile and produces better results. we used quantitative
metrics (psnr and ssim) to evaluate reconstruction error and human perception,
respectively. table 1 shows these metrics for our method and baselines with 1 to
8 cone beam projections. deviation from projections, as in x2ct-gan, leads to
inaccurate reconstruction. however, relying solely on projection consistency is
inadequate for this ill-posed problem. nerp matches projections but cannot
reconstruct the volume correctly. our approach balances between instant and
iterative methods by providing a reconstruction in 25 s with 100 optimization
steps, while ensuring maximal consistency. in contrast, nerp requires 7 min, and
x2ct-gan produces structures instantly but unmatching. clinical cbct acquisition
and reconstruction by fdk [3] take about 1-2 min and 10 s respectively. our
approach significantly reduces clin-ical time and radiation dose by using
instant biplanar projections, making it promising for fast 3d visualization
towards complex positioning.",10
335,2340,X2Vision: 3D CT Reconstruction from Biplanar X-Rays with Deep Structure Prior,4.0,"Conclusion, Limitations, and Future Work","we proposed a new unsupervised method for 3d reconstruction from biplanar x-rays
using a deep generative model to learn the structure manifold and retrieve the
maximum a posteriori volume with the projections, leading to stateof-the-art
reconstruction. our approach is fast, robust, and applicable to various human
body parts, making it suitable for many clinical applications, including
positioning and visualization with reduced radiation.future hardware
improvements may increase resolution, and our approach could benefit from other
generative models like latent diffusion models. this approach may provide coarse
reconstructions for patients with rare abnormalities, as most learning methods,
but a larger dataset or developing a prior including tissue abnormalities could
improve robustness.",10
336,2342,Structure-Preserving Synthesis: MaskGAN for Unpaired MR-CT Translation,1.0,Introduction,"magnetic resonance imaging (mri) and computed tomography (ct) are two commonly
used cross-sectional medical imaging techniques. mri and ct produce different
tissue contrast and are often used in tandem to provide complementary
information. while mri is useful for visualizing soft tissues (e.g. muscle, [20]
fails to preserve the smooth anatomy of the mri. (c) attentiongan [12] inflates
the head area in the synthetic ct, which is inconsistent with the original mri.
quantitative evaluations in mae (lower is better) are shown in yellow.fat), ct
is superior for visualizing bony structures. some medical procedures, such as
radiotherapy for brain tumors, craniosynostosis, and spinal surgery, typically
require both mri and ct for planning. unfortunately, ct imaging exposes patients
to ionizing radiation, which can damage dna and increase cancer risk [9],
especially in children and adolescents. given these issues, there are clear
advantages for synthesizing anatomically accurate ct data from mri.most
synthesis methods adopt supervised learning paradigms and train generative
models to synthesize ct [1][2][3]6,17]. despite the superior performance,
supervised methods require a large amount of paired data, which is prohibitively
expensive to acquire. several unsupervised mri-to-ct synthesis methods [4,6,14],
leverage cyclegan with cycle consistency supervision to eliminate the need for
paired data. unfortunately, the performance of unsupervised ct synthesis methods
[4,14,15] is inferior to supervised counterparts. due to the lack of direct
constraints on the synthetic outputs, cyclegan [20] struggles to preserve the
anatomical structure when synthesizing ct images, as shown in fig. 1(b). the
structural distortion in synthetic results exacerbates when data from the two
modalities are heavily misaligned, which usually occurs in pediatric scanning
due to the rapid growth in children.recent unsupervised methods impose
structural constraints on the synthesized ct through pixel-wise or shape-wise
consistency. pixel-wise consistency methods [8,14,15] capture and align
pixel-wise correlations between mri and synthesized ct. however, enforcing
pixel-wise consistency may introduce undesirable artifacts in the synthetic
results. this problem is particularly relevant in brain scanning, where both the
pixel-wise correlation and noise statistics in mr and ct images are different,
as a direct consequence of the signal acquisition technique. the alternative
shape-wise consistency methods [3,4,19] aim to preserve the shapes of major body
parts in the synthetic image. notably, shape-cyclegan [4] segments synthesized
ct and enforces consistency with the ground-truth mri segmentation. however,
these methods rely on segmentation annotations, which are time-consuming,
labor-intensive, and require expert radiological annotators. a recent natural
image synthesis approach, called attention-gan [12], learns attention masks to
identify discriminative structures. atten-tiongan implicitly learns prominent
structures in the image without using the ground-truth shape. unfortunately, the
lack of explicit mask supervision can lead to imprecise attention masks and, in
turn, produce inaccurate mappings of the anatomy, as shown in fig. 1(c). in this
paper, we propose maskgan, a novel unsupervised mri-to-ct synthesis method, that
preserves the anatomy under the explicit supervision of coarse masks without
using costly manual annotations. unlike segmentationbased methods [4,18],
maskgan bypasses the need for precise annotations, replacing them with standard
(unsupervised) image processing techniques, which can produce coarse anatomical
masks. such masks, although imperfect, provide sufficient cues for maskgan to
capture anatomical outlines and produce structurally consistent images. table 1
highlights our differences compared with previous shape-aware methods [4,12].
our major contributions are summarized as follows. 1) we introduce maskgan, a
novel unsupervised mri-to-ct synthesis method. maskgan is the first framework
that maintains shape consistency without relying on human-annotated
segmentation. 2) we present two new structural supervisions to enforce
consistent extraction of anatomical structures across mri and ct domains. 3)
extensive experiments show that our method outperforms state-of-the-art methods
by using automatically extracted coarse masks to effectively enhance structural
consistency.",10
337,2347,Structure-Preserving Synthesis: MaskGAN for Unpaired MR-CT Translation,3.1,Experimental Settings,"data collection. we collected 270 volumetric t1-weighted mri and 267 thinslice
ct head scans with bony reconstruction performed in pediatric patients under
routine scanning protocols1 . we targeted the age group from 6-24 months since
pediatric patients are more susceptible to ionizing radiation and experience a
greater cancer risk (up to 24% increase) from radiation exposure [7].
furthermore, surgery for craniosynostosis, a birth defect in which the skull
bones fuse too early, typically occurs during this age [5,16]. the scans were
acquired by ingenia 3.0t mri scanners and philips brilliance 64 ct scanners. we
then resampled the volumetric scans to the same resolution of 1.0 × 1.0 × 1.0 mm
3 . the dataset comprises brain mr and ct volumes from 262 subjects. 13 mri-ct
volumes from the same patients that were captured less than three months apart
are registered using rigid registration algorithms. the dataset is divided into
249, 1 and 12 subjects for training, validating and testing set. following [13],
we conducted experiments on sagittal slices. each mr and ct volume consists of
180 to 200 slices, which are resized and padded to the size of 224 × 224. the
intensity range of ct is clipped into [-1000, 2000]. all models are trained
using the adam optimizer for 100 epochs, with a learning rate of 0.0002 which
linearly decays to zero over the last 50 epochs. we use a batch size of 16 and
train on two nvidia rtx 3090 gpus.evaluation metrics. to provide a quantitative
evaluation of methods, we compute the same standard performance metrics as in
previous works [6,14] including mean absolute error (mae), peak signal-to-noise
ratio (psnr), and structural similarity (ssim) between ground-truth and
synthesized ct. the scope of the paper centers on theoretical development;
clinical evaluations such as dose calculation and treatment planning will be
conducted in future work.",10
339,2368,Co-learning Semantic-Aware Unsupervised Segmentation for Pathological Image Registration,0.0,3D Pseudo Brain MRI.,"to evaluate the performance of atlas-based registration, it is essential to have
the correct mapping of pathological regions to healthy brain regions. to create
such a mapping, we created a pseudo dataset by utilizing images from the oasis-1
and brats2020. from the resulting t1 sequences, a pseudo dataset of 300 images
was randomly selected for further analysis. appendix b provides a detailed
process for creating the pseudo dataset.real data with landmarks. brats-reg 2022
[2] provides extensive annotations of landmarks points within both the
pre-operative and the follow-up scans that have been generated by clinical
experts. a total of 140 images are provided, of which 112 are for training, and
28 for testing. comparison to pathology registration. we compared our method
(gir-net) with competitive algorithms: 1) three cutting-edge deep learning-based
unsupervised deformable registration approaches: voxelmorph [3], voxelmorph-df
[8] and symnet [14]. 2) two unsupervised deformable registration methods for
pathological images: dramms [18] and dirac [16]. dramms is an optimization-based
method that reduces the impact of non-corresponding regions. dirac jointly
estimates regions with absent correspondence and bidirectional deformation
fields and ranked first in the bratsreg2022 challenge.atlas-based registration.
after creating the pseudo dataset, we warped brain mr images without tumors to
the atlas and used the resulting deformation field as the gold standard for
evaluation. we then evaluated the mean deformation error (mde) [10], which is
calculated as the average euclidean distance between the coordinates of the
deformation field and the gold standard within specific regions of interest.
these regions include: 1) the tumor region. 2) the normal region near the tumor
(within 30 voxels). 3) the normal region far from the tumor (over 30 voxels but
within brain tissue). our results, presented in fig. 2, show that our method
with histogram matching (hm) outperforms other methods in all three regions,
particularly in the normal regions (near and far). by utilizing hm, our network
achieves an mde of less than 1 mm compared to the gold standard deformations.
these results demonstrate the effectiveness of our method in differentiating the
impact of pathology in atlas-based registration tasks. specifically, dirac is
unable to eliminate the influence of domain differences and resulting in the
largest registration error among the evaluated methods.longitudinal
registration. to perform the longitudinal registration task, we registered each
pre-operative scan to the corresponding follow-up scan of the same patient and
measured the mean target registration error (tre) of the paired landmarks using
the resulting deformation field. for this purpose, we leveraged segnet, trained
on brats2020, to segment the tumor of brat-sreg2022 and separated the landmarks
into two regions: near tumor and far from tumor. figure 3 shows the mean tre for
the various registration approaches. in our proposed framework, we replaced
regnet with cir-dm [15] (denoted as gir(cirdm)) without the need for supervised
training or pretraining, and achieved comparable performance with the
state-of-the-art method dirac. moreover, our gir approach outperforms other deep
learning-based methods and achieved accurate segmentation of pathological
images.to quantitatively evaluate the segmentation capability of our proposed
framework, we compared its performance with other unsupervised segmentation
techniques methods, including unsupervised clustering toolbox aucseg [25], joint
non-correspondence segmentation and registration method ncrnet [1], and dirac.
we used the mean dice similarity coefficient (dsc) to evaluate the similarity
between predicted masks and the ground truth. as shown in table 1, aucseg fails
to detect the lesion in t1 scans. our proposed framework achieved the highest
dsc result of 0.83, following post-processing.ablation study. we compared the
performance of the inpnet trained with histogram matching (hm) and the segnet
trained with ground truth masks (supervised). the results, shown in table 1 and
fig. 2, demonstrate that domain differences between s and t have a significant
effect on segmentation accuracy (without hm), leading to lower registration
quality overall. additionally, fig. 4 shows an example of a pseudo image. we
reconstructed the spatial correspondence by first using segnet to localize the
lesion and then using inpnet to inpaint it with the normal appearance.",10
340,2371,Fast Reconstruction for Deep Learning PET Head Motion Correction,1.0,Introduction,"positron emission tomography (pet) has been widely used in human brain imaging,
thanks to the availability of a vast array of specific radiotracers. these
compounds allow for studying various neurotransmitters and receptor dynamics for
different brain targets [11]. brain pet images are commonly used to diagnose and
monitor neurodegenerative diseases, such as alzheimer's disease, parkinson's
disease, epilepsy, and certain types of brain tumors [3]. head motion in pet
imaging reduces brain image resolution, lowers tracer distribution estimation,
and introduces attenuation correction (ac) mismatch artifacts [12].
consequently, the capability to monitor and correct head motion is of utmost
importance in brain pet studies.the first step of pet head motion correction is
motion tracking. when head motion information is acquired, either frame-based
motion correction or eventby-event (ebe) motion correction methods can be
applied in the reconstruction workflow to derive motion-free pet images. ebe
motion correction provides better results for real-time motion tracking compared
to frame-based methods, as the latter does not allow for correction of motion
that occurs within each dynamic frame [1]. currently, there are two main
categories of head motion tracking methods, hardware-based motion tracking (hmt)
and data-driven methods. for hmt, head motion is obtained from external devices.
generally, hmt systems offer accurate tracking results with high time
resolution. marker-based hmt such as polaris vicra (ndi, canada) use
light-reflecting markers on the patient's head and track the markers for motion
correction [6]. however, vicra is not routinely used in the clinic, as setup and
calibration of the tracking device can be complicated and attaching markers to
each patient increases the logistical burden of the scan. in response, some
researchers began to use markerless motion tracking systems for brain pet
[4,13]. these methods typically rely on the use of cameras and computer vision
algorithms to detect and analyze the movement of a person's head in real-time,
but these methods still require additional hardware setup. in data-driven motion
tracking methods, head motion is estimated from pet reconstructions or raw data.
with the development of commercial pet systems and technological advancements
such as time of flight (tof), data-driven head pet motion tracking has shown
promising results in reducing motion artifacts and improving image quality. for
instance, [12] developed a novel data-driven head motion detection method based
on the centroid of distribution (cod) of pet 3d point cloud image (pci). image
registration methods that seek to align two or more images offer a data-driven
solution for correcting head motion. intensity-based registration methods have
been used to track head motion using good-quality pet reconstruction frames to
achieve stable performance [14]. however, because of the dynamic change in pet
images, current registration-based methods need to split the data into several
discrete time frames, e.g., 5 min. therefore, they will introduce a cumulative
error when dealing with inter-frame motion. finally, inspired by the development
of deep learning-based registration methods, a deep learning head motion
correction (dl-hmc) network using vicra as ground truth was proposed [15]. this
study achieved accurate motion tracking on single subject testing data, but
showed less accurate motion predictions for multi-subject motion studies.
meanwhile, the input images were low-resolution pcis without tof and had large
voxel spacing, which can negatively affect motion tracking accuracy.in this
study, we proposed a new method to perform deep learning-based brain pet motion
prediction across multiple subjects by utilizing high-resolution one-second fast
reconstruction images (fris) with tof. a novel encoder and data augmentation
strategy was also applied to improve model performance. ablation studies were
conducted to assess the individual contributions of key method components.
multi-subject studies were conducted on a dataset of 20 subject and its results
were quantitatively and qualitatively evaluated by molar reconstruction studies
and corresponding brain region of interest (roi) standard uptake values (suv)
evaluation.",10
345,2425,Solving Low-Dose CT Reconstruction via GAN with Local Coherence,1.0,Introduction,"computed tomography (ct) is one of the most widely used technologies in medical
imaging, which can assist doctors for diagnosing the lesions in human internal
organs. due to harmful radiation exposure of standard-dose ct, the low dose ct
is more preferable in clinical application [4,6,34]. however, when the dose is
low together with the issues like sparse-view or limited angles, it becomes
quite challenging to reconstruct high-quality ct images. the high-quality ct
images are important to improve the performance of diagnosis in clinic [27]. in
mathematics, we model the ct imaging as the following procedure y = t (x r ) +
δ, (1) where x r ∈ r d denotes the unknown ground-truth picture, y ∈ r m denotes
the received measurement, and δ is the noise. the function t represents the
forward operator that is analogous to the radon transform, which is widely used
in medical imaging [23,28]. the problem of ct reconstruction is to recover x r
from the received y.solving the inverse problem of ( 1) is often very
challenging if there is no any additional information. if the forward operator t
is well-posed and δ is neglectable, we know that an approximate x r can be
easily obtained by directly computing t -1 (y). however, t is often ill-posed,
which means the inverse function t -1 does not exist and the inverse problem of
(1) may have multiple solutions. moreover, when the ct imaging is low-dose, the
filter backward projection (fbp) [11] can produce serious detrimental artifact.
therefore, most of existing approaches usually incorporate some prior knowledge
during the reconstruction [14,17,26]. for example, a commonly used method is
based on regularization:where • p denotes the p-norm and r(x) denotes the
penalty item from some prior knowledge.in the past years, a number of methods
have been proposed for designing the regularization r. the traditional
model-based algorithms, e.g., the ones using total variation [3,26], usually
apply the sparse gradient assumptions and run an iterative algorithm to learn
the regularizers [12,18,24,29]. another popular line for learning the
regularizers comes from deep learning [13,17]; the advantage of the deep
learning methods is that they can achieve an end-to-end recovery of the true
image x r from the measurement y [1,21]. recent researches reveal that
convolutional neural networks (cnns) are quite effective for image denoising,
e.g., the cnn based algorithms [10,34] can directly learn the reconstructed
mapping from initial measurement reconstructions (e.g., fbp) to the ground-truth
images. the dual-domain network that combines the sinograms with reconstructed
low-dose ct images were also proposed to enhance the generalizability [15,30].a
major drawback of the aforementioned reconstruction methods is that they deal
with the input ct 2d slices independently (note that the goal of ct
reconstruction is to build the 3d model of the organ). namely, the neighborhood
correlations among the 2d slices are often ignored, which may affect the
reconstruction performance in practice. in the field of computer vision,
""optical flow"" is a common technique for tracking the motion of object between
consecutive frames, which has been applied to many different tasks like video
generation [35], prediction of next frames [22] and super resolution synthesis
[5,31]. to estimate the optical flow field, existing approaches include the
traditional brightness gradient methods [2] and the deep networks [7]. the idea
of optical flow has also been used for tracking the organs movement in medical
imaging [16,20,33]. however, to the best of our knowledge, there is no work
considering gans with using optical flow to capture neighbor slices coherence
for low dose 3d ct reconstruction.in this paper, we propose a novel optical flow
based generative adversarial network for 3d ct reconstruction. our intuition is
as follows. when a patient is located in a ct equipment, a set of consecutive
cross-sectional images are generated. if the vertical axial sampling space of
transverse planes is small, the corresponding ct slices should be highly
similar. so we apply optical flow, though there exist several technical issues
waiting to solve for the design and implementation, to capture the local
coherence of adjacent ct images for reducing the artifacts in low-dose ct
reconstruction. our contributions are summarized below:1. we introduce the
""local coherence"" by characterizing the correlation of consecutive ct images,
which plays a key role for suppressing the artifacts. 2. together with the local
coherence, our proposed generative adversarial networks (gans) can yield
significant improvement for texture quality and stability of the reconstructed
images. 3. to illustrate the efficiency of our proposed approach, we conduct
rigorous experiments on several real clinical datasets; the experimental results
reveal the advantages of our approach over several state-of-the-art ct
reconstruction methods.",10
346,2429,Solving Low-Dose CT Reconstruction via GAN with Local Coherence,4.0,Experiment,"datasets. first, our proposed approaches are evaluated on the ""mayo-clinic
low-dose ct grand challenge"" (mayo-clinic) dataset of lung ct images [19].the
dataset contains 2250 two dimensional slices from 9 patients for training, and
the remaining 128 slices from 1 patient are reserved for testing. the lowdose
measurements are simulated by parallel-beam x-ray with 200 (or 150) uniform
views, i.e., n v = 200 (or n v = 150), and 400 (or 300) detectors, i.e., n d =
400 (or n d = 300). in order to further verify the denoising ability of our
approaches, we add the gaussian noise with standard deviation σ = 2.0 to the
sinograms after x-ray projection in 50% of the experiments. to evaluate the
generalization of our model, we also consider another dataset rider with
nonsmall cell lung cancer under two ct scans [36] for testing. we randomly
select 4 patients with 1827 slices from the dataset. the simulation process is
identical to that of mayo-clinic. the proposed networks were implemented in the
pytorch framework and trained on nvidia 3090 gpu with 100 epochs.baselines and
evaluation metrics. we consider several existing popular algorithms for
comparison. ( 1) fbp [11]: the classical filter backward projection on low-dose
sinograms. ( 2) fbpconvnet [10]: a direct inversion network followed by the cnn
after initial fbp reconstruction. ( 3) lpd [1]: a deep learning method based on
proximal primal-dual optimization. ( 4) uar [21]: an end-toend reconstruction
method based on learning unrolled reconstruction operators and adversarial
regularizers. our proposed method is denoted by gan-lc.we set λ pix = 1.0, λ adv
= 0.01 and λ per = 1.0 for the optimization objective in eq. ( 7) during our
training process. following most of the previous articles on 3d ct
reconstruction, we evaluate the experimental performance by two metrics: the
peak signal-to-noise ratio (psnr) and the structural similarity index (ssim)
[32]. psnr measures the pixel differences of two images, which is negatively
correlated with mean square error. ssim measures the structure similarity
between two images, which is related to the variances of the input images. for
both two measures, the higher the better.results. a similar increasing trend
with our approach across different settings but has worse reconstruction
quality. to evaluate the stability and generalization of our model and the
baselines trained on mayo-clinic dataset, we also test them on the rider
dataset. the results are shown in table 2. due to the bias in the datasets
collected from different facilities, the performances of all the models are
declined to some extents. but our proposed approach still outperforms the other
models for most testing cases.to illustrate the reconstruction performances more
clearly, we also show the reconstruction results for testing images in fig. 3.
we can see that our network can reconstruct the ct image with higher quality.
due to the space limit, the experimental results of different views n v and more
visualized results are placed in our supplementary material.",10
347,2432,Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,1.0,Introduction,"radiotherapy (rt) is one of the cornerstones of cancer patients. it utilizes
ionizing radiation to eradicate all cells of a tumor. the total radiation dose
is typically divided over 3-30 daily fractions to optimize its effect. as the
surrounding normal tissue is also sensitive to radiation, highly accurate
delivery is vital. image guided rt (igrt) is a technique to capture the anatomy
of the day using in room imaging in order to align the treatment beam with the
tumor location [1]. cone beam ct (cbct) is the most widely used imaging modality
for igrt.a major challenge especially for cbct imaging of the thorax and
upperabdomen is the respiratory motion that introduces blurring of the anatomy,
reducing the localization accuracy and the sharpness of the image.a technique
used to alleviate motion artifacts is respiratory correlated cbct (4dcbct) [16].
from the projections, it is possible to extract a respiratory signal [12], which
indicates the position of the organs within the patient during breathing. with
this, subsets of the projections can be defined to create reconstructions that
resolve the motion. however, only 20 to 60 respiratory periods are imaged. this
limits the number of projections available and results in view-aliasing [16].
additionally, the projections are affected by stochastic measurement noise
caused by the finite imaging dose used, which further degrades the quality of
the reconstruction even when all projections are used.several traditional
methods based on iterative reconstruction algorithms and motion compensation
techniques are used to reduce view-aliasing in 4dcbcts [7,10,11,14,15]. although
effective, these methods suffer from motion modeling uncertainty and prolonged
reconstruction times.deep learning has been proposed as a way to address
view-aliasing with accelerated reconstruction [6]. however, the method cannot
reduce measurement noise because it is still present in the images used as
targets during training.a different method, called noise2inverse, uses an
unsupervised approach to reduce measurement noise in the traditional ct setting
[4]. there are two ways to apply it to 4dcbct and both fail to reduce stochastic
noise effectively. the first is to apply noise2inverse to each
respiratory-correlated reconstruction. in this case, the method will struggle
because of the very low number of projections that are available. the second is
to apply noise2inverse directly to all the projections. in this case, the motion
artifacts that blur the image will appear again, as noise2inverse requires
averaging the sub-reconstructions to obtain a clean reconstruction.we propose
noise2aliasing to address these limitations. the method can be used to provably
train models to reduce both view-aliasing artifacts and stochastic noise from
4dcbcts in an unsupervised way. training deep learning models for medical
applications often needs new data. this was not the case for noise2aliasing, and
historical clinical data sufficed for training.we validated our method on
publicly available data [15] against a supervised approach [6] and applied it to
an internal clinical dataset of 30 lung cancer patients. we explore different
dataset sizes to understand their effects on the reconstructed images.",10
348,2436,Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,4.0,Experiments,"first, we used the spare varian dataset to study whether noise2aliasing can
match the performance of the supervised baseline and if it can outperform it
when adding noise to the projections. then, we use the internal dataset to
explore the requirements for the method to be applied to an existing clinical
dataset. these required around 64 gpu days on nvidia a100 gpus.training of the
model is done on 2d slices. the projections obtained during a scan are
sub-sampled according to the pseudo-average subset selection method described in
[6] and then used to obtain 3d reconstructions. in noise2aliasing these are used
for both input and target during training. given two volumes (x, y), the
training pairs (x i (k) , y i (k) ) are the same i-th slice along the k-th
dimension of each volume chosen to be the axial plane.the datasets used in this
study are two:1. the spare varian dataset was used to provide performance
results on publicly available patient data. to more closely resemble normal
respiratory motion per projection image, the 8 min scan has been used from each
patient (five such scans are available in the dataset). training is performed
over 4 patients while 1 patient is used as a test set. the hyperparameters are
optimized over the training dataset.2. an internal dataset (irb approved) of 30
lung cancer patients' 4dcbcts from 2020 to 2022, originally used for igrt, with
25 patients for training and 5 patients for testing. the scans are 4 min 205 •
scans with 120kev source and 512 × 512 sized detector, using elekta linacs. the
data were anonymized prior to analysis.projection noise was added using the
poisson distribution to the spare varian dataset to evaluate the ability of the
unsupervised method to reduce it. given a projected value of p and a photon
count π (chosen to be 2500), the rate of the poisson distribution is defined as
πe -p and given a sample q from this distribution, then the new projected value
is p =log q π .the architecture used in this work is the mixed scale dense cnn
(msd) [8], the most successful architecture from noise2inverse [4]. the msd
makes use of dilated convolutions to process features at all scales of the
image. we use the msd with depth 200 and width 1, adam optimizer, mse loss, a
batch size of 16, and a learning rate of 0.0001.the baselines we compare against
are two. the first is the traditional fdk obtained using rtk [13]. the second is
the supervised approach proposed by [6], where we replace the model with the
msd, for a fair comparison. in the supervised approach, the model is trained by
using as input reconstructions obtained from subsets defined with pseudo-average
subset selection while the targets use all of the projections available.the
metrics used in this work are the root mean squared error (rmse), peak
signal-to-noise ratio (psnr), and structural similarity index measure (ssim)
[17] all the metrics are defined between the output of the neural network and a
3d (cb)ct scan. for the spare varian dataset, we use the rois defined provided
[15] and used the 3d reconstruction using all the projections available as a
ground truth. for the internal dataset, we deformed the planning ct to each of
the phases reconstructed using the fdk algorithm and evaluate the metric over
only the 4dcbct volume boundaries.",10
349,2437,Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,5.0,Results and Discussion,"spare varian. inference speed with the nvidia a100 gpu averages 600ms per volume
made of 220 slices. from the qualitative evaluation of the methods in fig. 1,
noise2aliasing matches the visual quality of the supervised approach on the
low-noise dataset on both soft tissue and bones. the metrics in table 1 show
mean and standard deviation across all phases for a single patient. in the
lownoise setting, both supervised and noise2aliasing outperform fdk with very
similar results, often within a single standard deviation. noise2aliasing
successfully matches the performance of the supervised baseline. noisy spare
varian. from fig. 1 and table 1, the supervised approach reproduces the noise
that was seen during training, while noise2aliasing manages to remove it
consistently, outperforming the supervised approach, especially in the soft
tissue area around the lungs, where the noise affects attenuation coefficients
the most.noise2aliasing is capable of reducing the artifacts present in
reconstructions caused by stochastic noise in the projections used,
outperforming the supervised baseline.internal dataset. noise2alisting trained
on 25 patients and tested on 5 achieved mean psnr of 35.24 and ssim of 0.91,
while the clinical method achieved mean psnr of 29.97 and 0.74 ssim with p-value
of 0.048 for the psnr and 0.0015 for the ssim, so noise2aliasing was
significantly better according to both metrics. additionally, from fig. 3 we can
see how the breathing extent is matched with sharp reconstruction of the
diaphragm. overall, using more patients results in better noise reduction and
sharper reconstructions (see fig. 2), fig. 2. reconstruction using
noise2aliasing with different-sized datasets. with fewer patients, the model is
more conservative and tends to keep more noise, but also smudges the interface
between tissues and bones. with more patients, more of the view-aliasing is
addressed, and the reconstruction is sharper, however, a few small anatomical
structures tend to be suppressed by the model.especially between fat tissue and
skin and around the bones. however, the model also tends to remove small
anatomical structures as high-frequency objects that cannot be distinguished
from the noise.when applied to a clinical dataset, noise2aliasing benefits from
more patients being included in the dataset, however, qualitatively good
performance is already achieved with 5 patients. no additional data collection
was required and the method can be applied without major changes to the current
clinical practice.",10
350,2438,Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,6.0,Conclusion,"we have presented noise2aliasing, a method to provably remove both viewaliasing
and stochastic projection noise from 4dcbcts using an unsupervised deep learning
method. we have empirically demonstrated its performance on a publicly available
dataset and on an internal clinical dataset. noise2aliasing outperforms a
supervised approach when stochastic noise is present in the projections and
matches its performance on a popular benchmark. noise2aliasing can be trained on
existing historical datasets and does not require changing current clinical
practices. the method removes noise more reliably when the dataset size is
increased, however further analysis is required to establish a good quantitative
measurement of this phenomenon. as future work, we plan to study noise2aliasing
in the presence of changes in the breathing frequency and amplitude between
patients and during a scan.",10
351,2447,DULDA: Dual-Domain Unsupervised Learned Descent Algorithm for PET Image Reconstruction,3.1,Experimental Evaluations,"forty 128 × 128 × 40 3d zubal brain phantoms [24] were used in the simulation
study as ground truth, and one clinical patient brain images with different dose
level were used for the robust analysis. two tumors with different size were
added in each zubal brain phantom. the ground truth images were firstly
forward-projected to generate the noise-free sinogram with count of 10 6 for
each transverse slice and then poisson noise were introduced. 20 percent of
uniform random events were simulated. in total, 1600 (40 × 40) 2d sinograms were
generated. among them, 1320 (33 samples) were used in training, 200 (5 samples)
for testing, and 80 (2 samples) for validation. a total of 5 realizations were
simulated and each was trained/tested independently for bias and variance
calculation [15]. we used peak signal to noise ratio (psnr), structural
similarity index (ssim) and root mean square error (rmse) for overall
quantitative analysis. the contrast recovery coefficient (crc) [25] was used for
the comparison of reconstruction results in the tumor region of interest (roi)
area.",10
352,2449,DULDA: Dual-Domain Unsupervised Learned Descent Algorithm for PET Image Reconstruction,4.0,Discussion,"to test the robustness of proposed dulda, we forward-project one patient brain
image data with different dose level and reconstructed it with the trained dulda
model. the results compared with mlem are shown in fig. 3. the patient is
scanned with a ge discovery mi 5-ring pet/ct system. the real image has very
different cortex structure and some deflection compared with the training data.
it can be observed that dulda achieves excellent reconstruction results in both
details and edges across different dose level and different slices.table 2 shows
the ablation study on phase numbers and loss function for dulda. it can be
observed that the dual domain loss helps improve the performance and when the
phase number is 4, dulda achieves the best performance.",10
353,2451,Low-Dose CT Image Super-Resolution Network with Dual-Guidance Feature Distillation and Dual-Path Content Communication,1.0,Introduction,"following the ""as low as reasonably achievable"" (alara) principle [22], lowdose
computer tomography (ldct) has been widely used in various medical applications,
for example, clinical diagnosis [18] and cancer screening [28]. to balance the
high image quality and low radiation damage compared to normaldose ct (ndct),
numerous algorithms have been proposed for ldct superresolution [3,4].in the
past decades, image post-processing techniques attracted much attention from
researchers because they did not rely on the vendor-specific parameters [2] like
iterative reconstruction algorithms [1,23] and could be easily applied to
current ct workflows [29]. image post-processing super-resolution (sr) methods
could be divided into 3 categories: interpolated-based methods [16,25],
modelbased methods [13,14,24,26] and learning-based methods [7][8][9]17].
interpolatedbased methods could recover clear results in those flattened regions
but failed to reconstruct detailed textures because they equally recover
information with different frequencies. and model-based methods often involved
time-consuming optimization processes and degraded quickly when image statistics
were biased from the image prior [6].with the development of deep learning (dl),
various learning-based methods have been proposed, such as edsr [20], rcan [31],
and swinir [19]. those methods optimized their trainable parameters by
pre-degraded low-resolution (lr) and high-resolution (hr) pairs to build a
robust model with generalization and finally reconstruct sr images. however,
they were designed for known degradation (for example bicubic degradation) and
failed to deal with more complex and unknown degradation processes (such as ldct
degradation). facing more complex degradation processes, blind sr methods have
attracted attention. huang et al. [11] introduced a deep alternating network
(dan) which estimated the degradation kernels and corrected those kernels
iteratively and reconstructed results following the inverse process of the
estimated degradation. more recently, aiming at improving the quality of medical
images further, huang et al. [12] first composited degradation model proposed
for radiographs and proposed attention denoising super-resolution generative
adversarial network (aid-srgan) which could denoise and super-resolve
radiographs simultaneously. to accurately reconstruct hr ct images from lr ct
images, hou et al. [10] proposed a dual-channel joint learning framework which
could process the denoising reconstruction and sr reconstruction in parallel.the
aforementioned methods still have drawbacks: (1) they treated the regions of
interest (roi) and regions of uninterest equally, resulting in the extra cost in
computing source and inefficient use for hierarchical features. (2) most of them
extracted the features with a fixed resolution, failing to effectively leverage
multi-scale features which are essential to image restoration task [27,32].(3)
they connected the sr task and the ldct denoising task stiffly, leading to
smooth texture, residual artifacts and unclear edges.to deal with those issues,
as shown in fig. 1(a), we propose an ldct image sr network with dual-guidance
feature distillation and dual-path content com-fig. 1. architecture of our
proposed method. sam is sampling attention module. cam is channel attention
module. avg ct is the average image among adjacent ct slices of each patient.
munication. our contributions are as follows: (1) we design a dual-guidance
fusion module (dgfm) which could fuse the 3d ct information and roi guidance by
mutual attention to make full use of ct features and reconstruct clearer
textures and sharper edges. (2) we propose a sampling attention block (sab)
which consists of sampling attention module (sam), channel attention module
(cam) and elaborate multi-depth residual connection aiming at the essential
multi-scale features by up-sampling and down-sampling to leverage the features
in ct images. (3) we design a multi-supervised mechanism based on shared task
heads, which introducing the denoising head into sr task to concentrate on the
connection between the sr task and the denoising task. such design could
suppress more artifacts while decreasing the number of parameters.",10
354,2452,Low-Dose CT Image Super-Resolution Network with Dual-Guidance Feature Distillation and Dual-Path Content Communication,2.1,Overall Architecture,"the pipeline of our proposed method is shown in fig. 1(a). we first calculate
the average ct image of adjacent ct slices of each patient to provide the 3d
spatial structure information of ct volume. meanwhile, the roi mask is obtained
by a pre-trained segmentation network to guide the network to concentrate on the
focus area or tissue area. then those guidance images and the input ldct image
are fed to the dual-guidance feature distillation backbone to extract the deep
features. finally, the proposed dual-path architecture consisting of
parametershared sr heads and denoising heads leverages the deep visual features
obtained by our backbone to build the connection between the sr task and the
denoising task, resulting in noise-free and detail-clear reconstructed
results.dual-guidance feature distillation backbone. to decrease the redundant
computation and make full use of the above-mentioned extra information, we
design a dual-guidance feature distillation backbone consisting of a
dual-guidance fusion module (dgfm) and sampling attention block(sab).firstly, we
use a 3 × 3 convolutional layer to extract the shallow features of the three
input images. then, those features are fed into 10 dgfm-sab blocks to obtain the
deep visual features.especially, the dgfm-sab block is composed of dgfm
concatenated with sab. considering the indicative function of roi, we calculate
the correlation matrix between ldct and its mask and then acquire the response
matrix between the correlation matrix and the average ct image by multi-heads
attention mechanism:where, f sab i are the output of i-th sab. f mask and f av g
represent the shallow features of the input roi mask and the average ct image
respectively. meanwhile, p rj(•) is the projection function, sof tmax[•] means
the softmax function and f i are the output features of the i-th dgfm. the dgfm
helps the backbone to focus on the roi and tiny structural information by
continuously introducing additional guidance information.furthermore, to take
advantage of the multi-scale information which is essential for obtaining the
response matrix containing the connections between different levels of features,
as shown in fig. 1(b), we design the sampling attention block (sab) which
introduces the resampling features into middle connection to fuse the
multi-scale information. in the sab, the input features are up-sampled and
down-sampled simultaneously and then down-sampled and up-sampled to recover the
spatial resolution, which can effectively extract multi-scale features. in
addition, as shown in fig. 1(c), we introduce the channel attention module (cam)
to focus on those channels with high response values, leading to detailed
features with high differentiation to different regions. shared heads mechanism.
singly using the sr head that consists of pixel shuffle layer and convolution
layer fails to suppress the residual artifacts because of its poor noise removal
ability. to deal with this problem, we develop a dualpath architecture by
introducing the shared denoising head into sr task where the parameters of sr
heads and denoising heads in different paths are shared respectively. two paths
are designed to process the deep features extracted from our backbone: (1) the
sr path transfers the deep features to those with highfrequency information and
reconstructs the sr result, and (2) the denoising path migrates the deep
features to those without noise and recovers the clean result secondly.
especially, the parameters of those two paths are shared and optimized by
multiple supervised strategy simultaneously. this process could be formulated
as:where,",10
355,2459,Non-iterative Coarse-to-Fine Transformer Networks for Joint Affine and Deformable Image Registration,1.0,Introduction,"image registration is a fundamental requirement for medical image analysis and
has been an active research focus for decades [1]. it aims to find a spatial
transformation between a pair of fixed and moving images, through which the
moving image can be warped to spatially align with the fixed image. similar to
natural image registration [2], medical image registration usually requires
affine registration to eliminate rigid misalignments and then performs
additional deformable registration to address non-rigid deformations.
traditional methods usually formulate medical image registration as a
time-consuming iterative optimization problem [3,4]. recently, deep registration
methods based on deep learning have been widely adopted to perform end-to-end
registration [5,6]. deep registration methods learn a mapping from image pairs
to spatial transformations based on training data in an unsupervised manner,
which have shown advantages in registration accuracy and computational
efficiency [7][8][9][10][11][12][13][14][15][16][17][18].many deep registration
methods perform coarse-to-fine registration to improve registration accuracy,
where the registration is decoupled into multiple coarse-to-fine registration
steps that are iteratively performed by using multiple cascaded networks
[10][11][12][13] or repeatedly running a single network for multiple iterations
[14,15]. mok et al. [13] proposed a laplacian pyramid image registration network
(lapirn), where multiple networks at different pyramid levels were cascaded. shu
et al. [14] proposed to use a single network (ulae-net) to perform
coarse-to-fine registration with multiple iterations. these methods perform
iterative coarse-to-fine registration and extract image features repeatedly in
each iteration, which inevitably increases computational loads and prolongs the
registration runtime. recently, non-iterative coarse-to-fine (nice) registration
methods have been proposed to perform coarse-to-fine registration with a single
network in a single iteration [16][17][18]. for example, we previously proposed
a nice registration network (nice-net) [18,19], where multiple coarse-to-fine
registration steps are performed with a single network in a single iteration.
these nice registration methods show advantages in both registration accuracy
and runtime on the benchmark task of intra-patient brain mri registration.
nevertheless, we identified that existing nice registration methods still have
two main limitations.firstly, existing nice registration methods merely focus on
deformable coarseto-fine registration, while affine registration, a common
prerequisite, is still reliant on traditional registration methods [16,18] or
extra affine registration networks [17]. using traditional registration methods
incurs time-consuming iterative optimization, while cascading extra networks
consumes additional computational resources (e.g., extra gpu memory and
runtime). secondly, existing nice registration methods are based on convolution
neural networks (cnn) and thus are limited by the intrinsic locality (i.e.,
limited receptive field) of convolution operations. transformers have been
widely adopted in many medical applications for their capabilities to capture
long-range dependency [20]. recently, transformers have also been shown to
improve registration with conventional voxelmorph [7]-like architecture
[21][22][23]. however, the benefits of using transformers for nice registration
have not been explored.in this study, we propose a non-iterative coarse-to-fine
transformer network (nice-trans) for joint affine and deformable registration.
our technical contributions are two folds: (i) we extend the existing nice
registration framework to affine registration, where multiple steps of both
affine and deformable coarse-to-fine registration are performed with a single
network in a single iteration. (ii) we explore the benefits of transformers for
nice registration, where swin transformer [24] is embedded into the nice-trans
to model long-range relevance between fixed and moving images. this is the first
deep registration method that integrates previously separated affine and
deformable coarse-to-fine registration into a single network, and this is also
the first deep registration method that exploits transformers for nice
registration. extensive experiments with seven public datasets show that our
nice-trans outperforms state-of-the-art registration methods on both
registration accuracy and runtime.",10
356,2464,Non-iterative Coarse-to-Fine Transformer Networks for Joint Affine and Deformable Image Registration,3.1,Dataset and Preprocessing,"we evaluated the proposed nice-trans on the task of inter-patient brain mri
registration, which is a common benchmark task in medical image registration
studies [7][8][9][12][13][14][15][16][17][18]. we followed the dataset settings
in [18]: 2,656 brain mri images acquired from four public datasets (adni [27],
abide [28], adhd [29], and ixi [30]) were used for training; two public brain
mri datasets with anatomical segmentation (mindboggle [31] and buckner [32])
were used for validation and testing. the mindboggle dataset contains 100 mri
images and were randomly split into 50/50 images for validation/testing. the
buckner dataset contains 40 mri images and were used for testing only. in
addition to the original settings of [18], we adopted an additional public brain
mri dataset (lpba [33]) for testing, which contains 40 mri images.we performed
brain extraction and intensity normalization for each mri image with freesurfer
[32]. each image was placed at the same position via center of mass (com)
initialization [34], and then was cropped into 144 × 192 × 160 voxels.",10
357,2468,Non-iterative Coarse-to-Fine Transformer Networks for Joint Affine and Deformable Image Registration,4.0,Results and Discussion,"table 1 presents the registration performance of our nice-trans and all
comparison methods. the registration accuracy of all methods degraded by 1-3% in
dsc when affine registration was not performed, which demonstrates the
importance of affine registration. however, using flirt or affinenet for affine
registration incurred extra computational loads and increased the registration
runtime. our nice-trans performed joint affine and deformable registration,
which enabled it to realize affine registration with negligible additional
runtime. moreover, we suggest that integrating affine and deformable
registration into a single network also brings convenience for network training.
training two separate affine and deformable registration networks will prolong
the whole training time, while joint training will consume more gpu memory. as
for registration accuracy, the transmorph and swin-vm achieved higher dscs than
the conventional vm and difvm, but still cannot outperform the existing
cnn-based coarse-to-fine registration methods (lapirn, ulae-net, and nice-net).
our nice-trans leverages swin transformer to perform coarse-to-fine
registration, which enabled it to achieve the highest dscs among all methods.
this means that our nice-trans also has advantages on registration accuracy. we
present a qualitative comparison in the supplementary materials, which shows
that the registration result produced by our nice-trans is more consistent with
the fixed image. in addition, there usually exists a trade-off between dsc and
njd as imposing constraints on the spatial transformations limits their
flexibility, which results in degraded registration accuracy [13,18]. for
example, compared with vm, the difvm with diffeomorphic constraints achieved
better njds and worse dscs. nevertheless, our nice-trans achieved both the best
dscs and njds. we suggest that, if we set λ as 0 to maximize the registration
accuracy with the cost of transformation invertibility, our nice-trans can
achieve higher dscs and outperform the comparison methods by a larger margin
(refer to the regularization analysis in the supplementary materials).table 2
shows the results of our ablation study. swin transformer improved the
registration performance when embedded into the decoder, but had limited
benefits in the encoder. this suggests that swin transformer can benefit
registration in modeling inter-image spatial relevance while having limited
benefits in learning intra-image representations. this finding is intuitive as
image registration aims to find spatial relevance between images, instead of
finding the internal relevance within an image. under this aim, embedding
transformers in the decoder helps to capture long-range relevance between images
and improves registration performance. we noticed that previous studies gained
improvements by embedding swin transformer in the encoder [21] or leveraging a
full transformer network [22]. this is attributed to the fact that they used a
vm-like architecture that entangles image representation learning and spatial
relevance modeling throughout the whole network. our nice-trans decouples these
two parts and provides further insight on using transformers for registration:
leveraging transformers to learn intra-image relevance might not be beneficial
but merely incurs extra computational loads. it should be acknowledged that
there are a few limitations in our study. first, the experiment (table 1)
demonstrated that our nice-trans can well address the inherent misalignments
among inter-patient brain mri images, but the sensitivity of affine registration
to different degrees of misalignments is still awaiting further exploration.
second, in this study, we evaluated the nice-trans on the benchmark task of
inter-patient brain mri registration, while we believe that our nice-trans also
could apply to other image registration applications (e.g., brain tumor
registration [37]).",10
