{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective of the Notebook:\n",
    "The notebook is designed for preprocessing and extracting structured information from a collection of MICCAI 2023 XML documents. This involves transforming the raw XML data into clean, organized formats suitable for analysis and further processing. Key operations include parsing XML for headers and related text, identifying specific content like cancer-related papers, and extracting meaningful insights through structured dataframes.\n",
    "\n",
    "### Input Data Expected:\n",
    "The input for this notebook includes:\n",
    "- XML documents from MICCAI 2023, located at a predefined directory on the user's system.\n",
    "- These XML documents should already be formatted correctly and accessible to the script for reading and processing.\n",
    "\n",
    "### Output Data/Files Generated:\n",
    "Outputs generated by this notebook include:\n",
    "- Structured CSV files or dataframes containing parsed and cleaned information from the XML documents.\n",
    "- A CSV file specifically aggregating cancer-related papers extracted from the XML data.\n",
    "- Various CSV files based on further refined categorizations (like patient mentions in the cancer-related papers).\n",
    "\n",
    "### Assumptions or Important Notes:\n",
    "- The GROBID client must be running on the local machine via terminal to process the documents. This client is essential for extracting text from the XML documents.\n",
    "- XML documents are expected to follow a consistent structure that allows for automated parsing by the script.\n",
    "- Manual interventions were necessary for file renaming based on titles extracted from XML, implying the script assumes that each document can be uniquely identified by its title.\n",
    "- The script assumes the presence of specific headers and textual formats within the XML files for accurate parsing.\n",
    "- Error handling is minimal, so XML files should not contain corrupt data or unexpected formatting errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old code\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Merge all volumes into 1 dataframe\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for vol, df in cleaned_dataframes.items():\n",
    "#     volume_number = int(re.search(r'\\d+', vol).group())\n",
    "#     # Append volume information to each title to ensure uniqueness across volumes\n",
    "#     df['Paper Title'] = df['Paper Title'].astype(str) + ' (vol' + str(volume_number) + ')'\n",
    "#     df['Volume'] = volume_number\n",
    "\n",
    "# combined_df = pd.concat(cleaned_dataframes.values(), ignore_index=True)\n",
    "# # Check unique titles after appending volume information\n",
    "# unique_titles_count = len(combined_df['Paper Title'].unique())\n",
    "# print(f\"Total unique titles after enhancement: {unique_titles_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = combined_df.fillna(0)\n",
    "#combined_df.to_csv('combined_df.csv')\n",
    "\n",
    "# Lowercase all text in the 'Text' column\n",
    "combined_df['Text'] = combined_df['Text'].str.lower()\n",
    "combined_df['Text'] = combined_df['Text'].apply(wrap_text, width = 80)\n",
    "\n",
    "# Regular expression with str.replace to remove the volume information\n",
    "combined_df['Paper Title'] = combined_df['Paper Title'].str.replace(r'\\s*\\(vol\\d+\\)', '', regex=True)\n",
    "\n",
    "combined_df.rename(columns={'Paper Title': 'title', 'Header Number':'header_no', \n",
    "                                           'Header Title': 'header_title', 'Text':'text', 'Volume': 'volume'}, inplace=True)\n",
    "\n",
    "#combined_df.to_csv('refined_all_papers_extracted_w_text.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/outputs/databases/refined_all_papers_extracted_w_text.csv'\n",
    "filename = \"/Users/yasminsarkhosh/Desktop/all_papers_w_extracted_text.csv\"\n",
    "combined_df = pd.read_csv(filename, index_col=0)\n",
    "print(len(combined_df['title'].unique()))\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select papers related to cancer\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for 'cancer' in the Text column, case insensitive\n",
    "cancer_papers_mask = combined_df['text'].str.contains('cancer|tumor|tumour', case=False, na=False)\n",
    "papers_with_cancer = combined_df[cancer_papers_mask]\n",
    "\n",
    "# Get the unique titles of papers that mention 'cancer'\n",
    "unique_titles_with_cancer = papers_with_cancer['title'].unique()\n",
    "\n",
    "# Extract all headers and their related text for papers that mention 'cancer'\n",
    "extracted_info = pd.DataFrame()\n",
    "for title in unique_titles_with_cancer:\n",
    "    paper_info = combined_df[combined_df['title'] == title]\n",
    "    extracted_info = pd.concat([extracted_info, paper_info])\n",
    "\n",
    "# Reset index of the resulting DataFrame\n",
    "extracted_info.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "unique_paper_titles_with_cancer = extracted_info['title'].unique()\n",
    "print(len(unique_paper_titles_with_cancer))\n",
    "\n",
    "# Display the first few rows of the resulting DataFrame\n",
    "extracted_info\n",
    "extracted_info.to_csv(\"cancer_related_papers_w_text.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Extraction and Analysis of MICCAI 2023 XML Documents\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries \n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# The GROBID client to process and extract information from XML files\n",
    "from grobid_client.grobid_client import GrobidClient\n",
    "\n",
    "# For parsing XML files\n",
    "from xml.etree import ElementTree as et"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Text wrapping utility function that wrappes text lines within a dataframe row\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_text(text, width=80):\n",
    "    if pd.isnull(text):\n",
    "        return text\n",
    "    \n",
    "    wrapped_lines = []\n",
    "    for paragraph in text.split('\\n'):\n",
    "        line = ''\n",
    "        for word in paragraph.split():\n",
    "            if len(line) + len(word) + 1 > width:\n",
    "                wrapped_lines.append(line)\n",
    "                line = word\n",
    "            else:\n",
    "                line += ' ' + word if line else word\n",
    "        wrapped_lines.append(line)\n",
    "    return '\\n'.join(wrapped_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Interacting with the GROBID client for processing documents \n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_fulltext_document(client, process_file, output_dir):\n",
    "    client.process('processFulltextDocument', process_file, output=output_dir, force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Renaming MICCAI 2023 XML files based on their individual titles\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename XML files in a folder to the title of the paper\n",
    "def rename_xml_files_in_folder(folder_path):\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.xml'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            try:\n",
    "                tree = et.parse(file_path)\n",
    "                root = tree.getroot()\n",
    "                paper_title = find_title(root)\n",
    "                if paper_title:\n",
    "                    new_filename = paper_title.replace(\" \", \"_\") + '.xml'\n",
    "                    new_file_path = os.path.join(folder_path, new_filename)\n",
    "                    os.rename(file_path, new_file_path)\n",
    "            except et.ParseError as e:\n",
    "                print(f\"Error parsing '{filename}': {e}\")\n",
    "\n",
    "def find_title(element):\n",
    "    if 'title' in element.tag.lower() and element.text:\n",
    "        return element.text.strip()\n",
    "    for child in element:\n",
    "        title = find_title(child)\n",
    "        if title:\n",
    "            return title\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Parsing XML files to extract headers and related text into a dataframe\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_xml_and_extract_headers(file_path):\n",
    "    tree = etree.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "    ns = {'tei': 'http://www.tei-c.org/ns/1.0'}\n",
    "\n",
    "    # Extract the paper title by XPath in the XML's structure\n",
    "    paper_title_element = root.find('.//tei:title', ns)\n",
    "    paper_title = paper_title_element.text if paper_title_element is not None else \"No Title Found\"\n",
    "\n",
    "    headers = root.xpath('//tei:head', namespaces=ns)\n",
    "    print(f\"Found {len(headers)} headers in '{paper_title}'\")\n",
    "    \n",
    "    data = []\n",
    "    for header in headers:\n",
    "        # Use XPath string() function to get all text within the <p> tags, including nested elements\n",
    "        text_content = ''.join(header.getparent().xpath('.//tei:p//text()', namespaces=ns))\n",
    "        data.append({\n",
    "            'Paper Title': paper_title,\n",
    "            'Header Number': header.get('n'),\n",
    "            'Header Title': header.text,\n",
    "            'Text': text_content  # Updated to use text_content\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(data, columns=['Paper Title', 'Header Number', 'Header Title', 'Text'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Aggregating and cleaning data from multiple sources.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_xml_folder(folder_path):\n",
    "    # Aggregates data from multiple XML files in a given folder\n",
    "    all_data_frames = []\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith(\".xml\"):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            df = parse_xml_and_extract_headers(file_path)\n",
    "            all_data_frames.append(df)\n",
    "\n",
    "    if all_data_frames:\n",
    "        final_df = pd.concat(all_data_frames, ignore_index=True)\n",
    "    else:\n",
    "        final_df = pd.DataFrame()\n",
    "\n",
    "    return final_df\n",
    "\n",
    "# Clean dataframe from duplicates \n",
    "def clean_dataframe(df):\n",
    "    df_cleaned = df.dropna(subset=['Header Title', 'Text'], how='all').dropna(subset=['Text'], how='any')\n",
    "    return df_cleaned\n",
    "\n",
    "# Merge each dataframe (where 1 dataframe contains all information from volume 1 etc.) into a final dataframe with all papers from MICCAI 2023\n",
    "def merge_dataframes(cleaned_dataframes):\n",
    "    for vol, df in cleaned_dataframes.items():\n",
    "        volume_number = int(re.search(r'\\d+', vol).group())\n",
    "        df['Paper Title'] += f' (vol{volume_number})'\n",
    "        df['Volume'] = volume_number\n",
    "    return pd.concat(cleaned_dataframes.values(), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GROBID server is up and running\n"
     ]
    }
   ],
   "source": [
    "# Run GROBID in terminal before running the notebook\n",
    "# Installation and running commands\n",
    "# wget https://github.com/kermitt2/grobid/archive/0.8.0.zip\n",
    "# unzip 0.8.0.zip\n",
    "# cd grobid-0.8.0\n",
    "# ./gradlew run\n",
    "\n",
    "# GROBID library: Runs in terminal\n",
    "grobid_server = 'http://localhost:8070'\n",
    "client = GrobidClient(grobid_server=grobid_server)\n",
    "\n",
    "# MICCAI 2023 PDF files, organised by volumes into separate folders\n",
    "# 730 PDFs in total, divided into 10 folders\n",
    "process_file = '/Users/yasminsarkhosh/Documents/archive/miccai_papers/vol1'\n",
    "#process_file = '../miccai_papers'\n",
    "# Output folder for processed PDF files as XML files\n",
    "#output_dir = '../miccai_XML_documents'\n",
    "output_dir = \"/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/03MICCAI_notebook_outputs\"\n",
    "\n",
    "# Call the function to start the preprocessing and data extraction \n",
    "process_fulltext_document(client, process_file, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combined_df = process_fulltext_document(client, process_file, output_dir)\n",
    "#combined_df.to_csv('03MICCAI_notebook_all_papers_w_extracted_text.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Scope of papers #1: cancer-related medical AI's**\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Selection of papers: identifying cancer-related keywords in papers within the aggregated data.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "730\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>header_no</th>\n",
       "      <th>header_title</th>\n",
       "      <th>text</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AMAE: Adaptation of Pre-trained Masked Autoenc...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Introduction</td>\n",
       "      <td>to reduce radiologists' reading burden and mak...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AMAE: Adaptation of Pre-trained Masked Autoenc...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Method</td>\n",
       "      <td>notation. we first formally define the problem...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AMAE: Adaptation of Pre-trained Masked Autoenc...</td>\n",
       "      <td>2.1</td>\n",
       "      <td>Stage 1-Proxy Task to Detect Synthetic Anomalies</td>\n",
       "      <td>amae starts the first training stage using onl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AMAE: Adaptation of Pre-trained Masked Autoenc...</td>\n",
       "      <td>2.2</td>\n",
       "      <td>Stage 2-MAE Inter-Discrepancy Adaptation</td>\n",
       "      <td>the proposed mae adaptation scheme is inspired...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AMAE: Adaptation of Pre-trained Masked Autoenc...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Experiments</td>\n",
       "      <td>datasets. we evaluated our method on three pub...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6963</th>\n",
       "      <td>LightNeuS: Neural Surface Reconstruction in En...</td>\n",
       "      <td>3.1</td>\n",
       "      <td>Using Illumination Decline as a Depth Cue</td>\n",
       "      <td>the neus formulation of sect. 2 assumes distan...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6964</th>\n",
       "      <td>LightNeuS: Neural Surface Reconstruction in En...</td>\n",
       "      <td>3.2</td>\n",
       "      <td>Endoscope Photometric Model</td>\n",
       "      <td>apart from illumination decline, there are sev...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6965</th>\n",
       "      <td>LightNeuS: Neural Surface Reconstruction in En...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Experiments</td>\n",
       "      <td>we validate our method on the c3vd dataset [4]...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6966</th>\n",
       "      <td>LightNeuS: Neural Surface Reconstruction in En...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Conclusion</td>\n",
       "      <td>we have presented a method for 3d dense multi-...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6967</th>\n",
       "      <td>LightNeuS: Neural Surface Reconstruction in En...</td>\n",
       "      <td>0</td>\n",
       "      <td>Supplementary Information</td>\n",
       "      <td>the online version contains supplementary mate...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6968 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title header_no  \\\n",
       "0     AMAE: Adaptation of Pre-trained Masked Autoenc...       1.0   \n",
       "1     AMAE: Adaptation of Pre-trained Masked Autoenc...       2.0   \n",
       "2     AMAE: Adaptation of Pre-trained Masked Autoenc...       2.1   \n",
       "3     AMAE: Adaptation of Pre-trained Masked Autoenc...       2.2   \n",
       "4     AMAE: Adaptation of Pre-trained Masked Autoenc...       3.0   \n",
       "...                                                 ...       ...   \n",
       "6963  LightNeuS: Neural Surface Reconstruction in En...       3.1   \n",
       "6964  LightNeuS: Neural Surface Reconstruction in En...       3.2   \n",
       "6965  LightNeuS: Neural Surface Reconstruction in En...       4.0   \n",
       "6966  LightNeuS: Neural Surface Reconstruction in En...       5.0   \n",
       "6967  LightNeuS: Neural Surface Reconstruction in En...         0   \n",
       "\n",
       "                                          header_title  \\\n",
       "0                                         Introduction   \n",
       "1                                               Method   \n",
       "2     Stage 1-Proxy Task to Detect Synthetic Anomalies   \n",
       "3             Stage 2-MAE Inter-Discrepancy Adaptation   \n",
       "4                                          Experiments   \n",
       "...                                                ...   \n",
       "6963         Using Illumination Decline as a Depth Cue   \n",
       "6964                       Endoscope Photometric Model   \n",
       "6965                                       Experiments   \n",
       "6966                                        Conclusion   \n",
       "6967                         Supplementary Information   \n",
       "\n",
       "                                                   text  volume  \n",
       "0     to reduce radiologists' reading burden and mak...       1  \n",
       "1     notation. we first formally define the problem...       1  \n",
       "2     amae starts the first training stage using onl...       1  \n",
       "3     the proposed mae adaptation scheme is inspired...       1  \n",
       "4     datasets. we evaluated our method on three pub...       1  \n",
       "...                                                 ...     ...  \n",
       "6963  the neus formulation of sect. 2 assumes distan...      10  \n",
       "6964  apart from illumination decline, there are sev...      10  \n",
       "6965  we validate our method on the c3vd dataset [4]...      10  \n",
       "6966  we have presented a method for 3d dense multi-...      10  \n",
       "6967  the online version contains supplementary mate...      10  \n",
       "\n",
       "[6968 rows x 5 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notebook_name = \"03MICCAI_notebook_\"\n",
    "filename = \"03MICCAI_notebook_all_papers_w_extracted_text.csv\"\n",
    "combined_df = pd.read_csv(filename, index_col=0)\n",
    "print(len(combined_df['title'].unique()))\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263\n"
     ]
    }
   ],
   "source": [
    "# Search for 'cancer, tumor, tumour' in the text column, case insensitive\n",
    "cancer_papers_mask = combined_df['text'].str.contains('cancer|tumor|tumour', case=False, na=False)\n",
    "papers_with_cancer = combined_df[cancer_papers_mask]\n",
    "\n",
    "# Get the unique titles of papers that mention 'cancer'\n",
    "unique_titles_with_cancer = papers_with_cancer['title'].unique()\n",
    "\n",
    "# Extract all headers and their related text for papers that mention 'cancer'\n",
    "extracted_info = pd.DataFrame()\n",
    "for title in unique_titles_with_cancer:\n",
    "    paper_info = combined_df[combined_df['title'] == title]\n",
    "    extracted_info = pd.concat([extracted_info, paper_info])\n",
    "\n",
    "# Reset index of the resulting DataFrame\n",
    "extracted_info.reset_index(drop=True, inplace=True)\n",
    "\n",
    "unique_paper_titles_with_cancer = extracted_info['title'].unique()\n",
    "print(len(unique_paper_titles_with_cancer))\n",
    "\n",
    "# Save the extracted information to a CSV file\n",
    "# extracted_info.to_csv(notebook_name + 'cancer_related_papers_w_text.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Scope of papers #2: Cancer-related medical AI's wording 'patients' in their research articles**\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an experiment, I have narrowed down the 263 cancer-related medical AI's papers down to a scope of papers, working with datasets with a subgroup defined as 'patient/patients'. Mindful, that this code only selects papers by keyword-match.\n",
    "\n",
    "The total number of papers are now down to 155."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique titles for papers containing the keyword <patient/patients>: 155\n"
     ]
    }
   ],
   "source": [
    "categories = {\n",
    "    'age': ['age', 'age', 'young', 'old', 'gender'],\n",
    "    'gender': ['gender', 'sex', 'women', 'woman', 'female', 'male'],\n",
    "    'ethnicity': ['ethnicity', 'ethnicities', 'race', 'white patients', 'black patients'],\n",
    "    'location_info': ['geolocation', 'geographical', 'geographic', 'country', 'countries', \n",
    "                    'city', 'cities', 'hospital', 'hospitals', 'clinic', 'clinics', 'continent',\n",
    "                    'province', 'state', 'region', 'town', 'village', 'area', 'district'],\n",
    "    'patients': ['patient', 'patients'],\n",
    "    'dataset_info': ['dataset', 'datasets', 'data set', 'data sets', 'publicly', 'public', 'private', 'open access', 'open-access'],\n",
    "    'bias_info': ['bias', 'biases', 'fairness'],\n",
    "    \n",
    "}\n",
    "\n",
    "# Flatten the list of all keywords excluding 'patients' to avoid redundancy\n",
    "all_keywords = sum([kw for cat, kw in categories.items() if cat != 'patients'], [])\n",
    "\n",
    "# Filter papers that mention 'patient' or 'patients'\n",
    "scope_mask = extracted_info['text'].str.contains('patient|patients', case=False, na=False)\n",
    "papers_with_patients = extracted_info[scope_mask]\n",
    "\n",
    "# Prepare a list to collect paper info dictionaries\n",
    "papers_info_list = []\n",
    "\n",
    "# Iterate over unique titles in the filtered DataFrame\n",
    "for title in papers_with_patients['title'].unique():\n",
    "    paper_info = papers_with_patients[papers_with_patients['title'] == title]\n",
    "    # Initialize a dictionary for the current paper with zeros for all keywords\n",
    "    paper_keywords = dict.fromkeys(all_keywords, 0)\n",
    "    paper_keywords['title'] = title\n",
    "    # Check for each keyword in the text of the paper\n",
    "    for keyword in all_keywords:\n",
    "        if any(paper_info['text'].str.contains(keyword, case=False, na=False)):\n",
    "            paper_keywords[keyword] = 1\n",
    "    # Collect the keyword matches for the current paper\n",
    "    papers_info_list.append(paper_keywords)\n",
    "\n",
    "# Create a DataFrame from the list of dictionaries\n",
    "keywords_per_paper = pd.DataFrame(papers_info_list)\n",
    "\n",
    "# Display or work with the keywords_per_paper DataFrame\n",
    "#keywords_per_paper.to_csv('keywords_per_paper.csv')\n",
    "#papers_with_patients.to_csv(notebook_name + 'papers_with_patients.csv')\n",
    "\n",
    "print('Number of unique titles for papers containing the keyword <patient/patients>:', len(papers_with_patients['title'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old code\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Merge all volumes into 1 dataframe\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for vol, df in cleaned_dataframes.items():\n",
    "#     volume_number = int(re.search(r'\\d+', vol).group())\n",
    "#     # Append volume information to each title to ensure uniqueness across volumes\n",
    "#     df['Paper Title'] = df['Paper Title'].astype(str) + ' (vol' + str(volume_number) + ')'\n",
    "#     df['Volume'] = volume_number\n",
    "\n",
    "# combined_df = pd.concat(cleaned_dataframes.values(), ignore_index=True)\n",
    "# # Check unique titles after appending volume information\n",
    "# unique_titles_count = len(combined_df['Paper Title'].unique())\n",
    "# print(f\"Total unique titles after enhancement: {unique_titles_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = combined_df.fillna(0)\n",
    "#combined_df.to_csv('combined_df.csv')\n",
    "\n",
    "# Lowercase all text in the 'Text' column\n",
    "combined_df['Text'] = combined_df['Text'].str.lower()\n",
    "combined_df['Text'] = combined_df['Text'].apply(wrap_text, width = 80)\n",
    "\n",
    "# Regular expression with str.replace to remove the volume information\n",
    "combined_df['Paper Title'] = combined_df['Paper Title'].str.replace(r'\\s*\\(vol\\d+\\)', '', regex=True)\n",
    "\n",
    "combined_df.rename(columns={'Paper Title': 'title', 'Header Number':'header_no', \n",
    "                                           'Header Title': 'header_title', 'Text':'text', 'Volume': 'volume'}, inplace=True)\n",
    "\n",
    "#combined_df.to_csv('refined_all_papers_extracted_w_text.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/outputs/databases/refined_all_papers_extracted_w_text.csv'\n",
    "filename = \"/Users/yasminsarkhosh/Desktop/all_papers_w_extracted_text.csv\"\n",
    "combined_df = pd.read_csv(filename, index_col=0)\n",
    "print(len(combined_df['title'].unique()))\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select papers related to cancer\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for 'cancer' in the Text column, case insensitive\n",
    "cancer_papers_mask = combined_df['text'].str.contains('cancer|tumor|tumour', case=False, na=False)\n",
    "papers_with_cancer = combined_df[cancer_papers_mask]\n",
    "\n",
    "# Get the unique titles of papers that mention 'cancer'\n",
    "unique_titles_with_cancer = papers_with_cancer['title'].unique()\n",
    "\n",
    "# Extract all headers and their related text for papers that mention 'cancer'\n",
    "extracted_info = pd.DataFrame()\n",
    "for title in unique_titles_with_cancer:\n",
    "    paper_info = combined_df[combined_df['title'] == title]\n",
    "    extracted_info = pd.concat([extracted_info, paper_info])\n",
    "\n",
    "# Reset index of the resulting DataFrame\n",
    "extracted_info.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "unique_paper_titles_with_cancer = extracted_info['title'].unique()\n",
    "print(len(unique_paper_titles_with_cancer))\n",
    "\n",
    "# Display the first few rows of the resulting DataFrame\n",
    "extracted_info\n",
    "extracted_info.to_csv(\"cancer_related_papers_w_text.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old code\n",
    "#process_file = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/vol1'\n",
    "#client.process('processFulltextDocument', process_file, output=\"./vol01\", force=True)\n",
    "\n",
    "#process_file = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2'\n",
    "#client.process('processFulltextDocument', process_file, output = \"./vol02\", force=True)\n",
    "\n",
    "#process_file = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3'\n",
    "#client.process('processFulltextDocument', process_file, output=\"./vol03\", force=True)\n",
    "\n",
    "#process_file = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4'\n",
    "#client.process('processFulltextDocument', process_file, output=\"./vol04\", force=True)\n",
    "\n",
    "# process_file = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5'\n",
    "# client.process('processFulltextDocument', process_file, output=\"./vol05\", force=True)\n",
    "\n",
    "#process_file = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6'\n",
    "#client.process('processFulltextDocument', process_file, output=\"./vol06\", force=True)\n",
    "\n",
    "#process_file = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7'\n",
    "#client.process('processFulltextDocument', process_file, output=\"./vol07\", force=True)\n",
    "\n",
    "#process_file = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8'\n",
    "#client.process('processFulltextDocument', process_file, output=\"./vol08\", force=True)\n",
    "\n",
    "#process_file = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9'\n",
    "#client.process('processFulltextDocument', process_file, output=\"./vol09\", force=True)\n",
    "\n",
    "#process_file = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10'\n",
    "#client.process('processFulltextDocument', process_file, output=\"./vol10\", force=True)  \n",
    "\n",
    "# folder_path = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/vol01'\n",
    "# folder_path = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/vol02'\n",
    "# folder_path = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/vol03'\n",
    "# folder_path = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/vol04'\n",
    "# folder_path = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/vol05'\n",
    "# folder_path = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/vol06'\n",
    "# folder_path = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/vol07'\n",
    "# folder_path = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/vol08'\n",
    "# folder_path = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/vol09'\n",
    "# folder_path = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/vol10'\n",
    "\n",
    "\n",
    "#process_file = base_path + 'vol2'\n",
    "#client.process('processFulltextDocument', process_file, output = \"./vol02\", force=True)\n",
    "\n",
    "#process_file = base_path + 'vol3'\n",
    "#client.process('processFulltextDocument', process_file, output=\"./vol03\", force=True)\n",
    "\n",
    "#process_file = base_path + 'vol4'\n",
    "#client.process('processFulltextDocument', process_file, output=\"./vol04\", force=True)\n",
    "\n",
    "# process_file = base_path + 'vol5'\n",
    "# client.process('processFulltextDocument', process_file, output=\"./vol05\", force=True)\n",
    "\n",
    "#process_file = base_path + 'vol6'\n",
    "#client.process('processFulltextDocument', process_file, output=\"./vol06\", force=True)\n",
    "\n",
    "#process_file = base_path + 'vol7'\n",
    "#client.process('processFulltextDocument', process_file, output=\"./vol07\", force=True)\n",
    "\n",
    "#process_file = base_path + 'vol8'\n",
    "#client.process('processFulltextDocument', process_file, output=\"./vol08\", force=True)\n",
    "\n",
    "#process_file = base_path + 'vol9'\n",
    "#client.process('processFulltextDocument', process_file, output=\"./vol09\", force=True)\n",
    "\n",
    "#process_file = base_path + 'vol10'\n",
    "#client.process('processFulltextDocument', process_file, output=\"./vol10\", force=True)  \n",
    "\n",
    "# folder_path = '../vol01'\n",
    "# folder_path = '../vol02'\n",
    "# folder_path = '../vol03'\n",
    "# folder_path = '../vol04'\n",
    "# folder_path = '../vol05'\n",
    "# folder_path = '../vol06'\n",
    "# folder_path = '../vol07'\n",
    "# folder_path = '../vol08'\n",
    "# folder_path = '../vol09'\n",
    "# folder_path = '../vol10'\n",
    "# # The collection of MICCAI 2023 papers is stored in the following directory structure:\n",
    "# # ../miccai_papers/vol1\n",
    "# # ../miccai_papers/vol2\n",
    "# # ...\n",
    "# # ../miccai_papers/vol10\n",
    "\n",
    "# ''' \n",
    "# For each subfolder in the main folder_path directory of the MICCAI 2023 papers, \n",
    "# the GRONBID client is used to process the full text of the papers by converting the PDF files to XML files.\n",
    "# '''\n",
    "\n",
    "# base_path = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/project_submission/00_project_notebook/miccai_papers'\n",
    "\n",
    "# for folder in os.listdir(base_path):\n",
    "#     process_file = base_path + folder\n",
    "#     client.process('processFulltextDocument', process_file, output=f\"./processed_vol{folder}\", force=True)\n",
    "#     print(f\"Processing {folder}...\")\n",
    "#     print(f\"Output: {f'./{folder}'}\")\n",
    "#     print(f\"Force: True\")\n",
    "\n",
    "# folder_path = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/vol01/vol1'\n",
    "# rename_xml_files_in_folder(folder_path)\n",
    "\n",
    "\n",
    "# Manually renaming the XML files based on their title tags\n",
    "\n",
    "# Load and parse the XML file\n",
    "#file_path = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/vol09/paper_59.grobid.tei.xml'\n",
    "#file_path = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/output2/Medical_Image_Computing_and_Computer_Assisted_Intervention_–_MICCAI_2023.xml'\n",
    "\n",
    "''' Paper 44 XML file: title was too long to be used as a file name, removed '/CT Self-supervised Denoising' from the title '''\n",
    "#file_path ='/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/paper_44.grobid.tei.xml'\n",
    "\n",
    "#file_path = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/output2/paper_13.grobid.tei.xml'\n",
    "\n",
    "'''FileNotFoundError: [Errno 2] No such file or directory: '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/paper_49.grobid.tei.xml' -> \n",
    "'/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/A_Patient-Specific_Self-supervised_Model_for_Automatic_X-Ray/CT_Registration.xml'\n",
    "Solution: removed '/CT_Registration' from title\n",
    "'''\n",
    "#file_path = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/paper_49.grobid.tei.xml'\n",
    "#file_path = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/papers_xml/vol04/Medical_Image_Computing_and_Computer_Assisted_Intervention_–_MICCAI_2023.xml'\n",
    "#file_path = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/papers_xml/vol01/Medical_Image_Computing_and_Computer_Assisted_Intervention_–_MICCAI_2023.xml'\n",
    "file_path = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/papers_xml/vol02/Medical_Image_Computing_and_Computer_Assisted_Intervention_–_MICCAI_2023.xml'\n",
    "\n",
    "tree = et.parse(file_path)\n",
    "root = tree.getroot()\n",
    "\n",
    "# Since XML namespaces can complicate direct tag access, we find the title tag dynamically.\n",
    "# This approach is based on the assumption that titles are relatively unique in structure.\n",
    "\n",
    "# Attempt to extract the paper title. This might need adjustments based on the actual structure.\n",
    "title = None\n",
    "for elem in root.iter():\n",
    "    if 'title' in elem.tag.lower():\n",
    "        title = elem.text\n",
    "        break\n",
    "\n",
    "title_clean = title.strip().replace(\" \", \"_\") if title else \"Untitled_Document\"\n",
    "title_clean\n",
    "\n",
    "# Attempt a more generic search for the title, considering common patterns in scholarly articles\n",
    "# We'll look for title elements that might be nested within other elements (like \"titleStmt\" or \"fileDesc\" in TEI format)\n",
    "\n",
    "def find_title(element):\n",
    "    \"\"\"\n",
    "    Recursively search for the title element in the XML structure.\n",
    "    \"\"\"\n",
    "    if 'title' in element.tag.lower() and element.text:\n",
    "        return element.text.strip()\n",
    "    for child in element:\n",
    "        title = find_title(child)\n",
    "        if title:\n",
    "            return title\n",
    "    return None\n",
    "\n",
    "# Attempt to find the title using the recursive search\n",
    "paper_title = find_title(root)\n",
    "paper_title_clean = paper_title.replace(\" \", \"_\") if paper_title else \"Untitled_Document\"\n",
    "paper_title, paper_title_clean\n",
    "\n",
    "import os\n",
    "\n",
    "# Define the new file path with the clean title\n",
    "new_file_path = os.path.join(os.path.dirname(file_path), f\"{paper_title_clean}.xml\")\n",
    "\n",
    "# Rename the file\n",
    "os.rename(file_path, new_file_path)\n",
    "\n",
    "new_file_path\n",
    "\n",
    "# Manually renaming the XML files that are named \"Medical_Image_Computing_and_Computer_Assisted_Intervention_–_MICCAI_2023.xml\" \n",
    "\n",
    "path = \"../03MICCAI_notebook_GROBID_processed_volumes\"\n",
    "\n",
    "# Load and parse the XML file\n",
    "# file_path = path + '/vol1/Medical_Image_Computing_and_Computer_Assisted_Intervention_–_MICCAI_2023.xml'\n",
    "# file_path = path + '/vol2/Medical_Image_Computing_and_Computer_Assisted_Intervention_–_MICCAI_2023.xml'\n",
    "# file_path = '/vol4/Medical_Image_Computing_and_Computer_Assisted_Intervention_–_MICCAI_2023.xml'\n",
    "# file_path = '/vol6/Medical_Image_Computing_and_Computer_Assisted_Intervention_–_MICCAI_2023.xml'\n",
    "# file_path = '/vol7/Medical_Image_Computing_and_Computer_Assisted_Intervention_–_MICCAI_2023.xml'\n",
    "# file_path = '/vol9/Medical_Image_Computing_and_Computer_Assisted_Intervention_–_MICCAI_2023.xml'\n",
    "\n",
    "# path = \"03MICCAI_notebook_GROBID_processed_volumes\"\n",
    "\n",
    "# Load and parse the XML file\n",
    "# file_path = path + '/vol1/Medical_Image_Computing_and_Computer_Assisted_Intervention_–_MICCAI_2023.xml'\n",
    "# file_path = path + '/vol2/Medical_Image_Computing_and_Computer_Assisted_Intervention_–_MICCAI_2023.xml'\n",
    "# file_path = path + '/vol4/Medical_Image_Computing_and_Computer_Assisted_Intervention_–_MICCAI_2023.xml'\n",
    "# file_path = path + '/vol6/Medical_Image_Computing_and_Computer_Assisted_Intervention_–_MICCAI_2023.xml'\n",
    "# file_path = path + '/vol7/Medical_Image_Computing_and_Computer_Assisted_Intervention_–_MICCAI_2023.xml'\n",
    "# file_path = path + '/vol9/Medical_Image_Computing_and_Computer_Assisted_Intervention_–_MICCAI_2023.xml'\n",
    "\n",
    "# df_headers = process_xml_folder(folder_path)\n",
    "# df_headers.to_csv(\"/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/vol1_headers.csv\", index=False)\n",
    "# #df_headers\n",
    "\n",
    "# cleaned_dataframes['vol1'].to_csv(\"/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/dfs/vol1_cleaned.csv\", index=False)\n",
    "# cleaned_dataframes['vol2'].to_csv(\"/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/dfs/vol2_cleaned.csv\", index=False)\n",
    "# cleaned_dataframes['vol3'].to_csv(\"/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/dfs/vol3_cleaned.csv\", index=False)\n",
    "# cleaned_dataframes['vol4'].to_csv(\"/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/dfs/vol4_cleaned.csv\", index=False)\n",
    "# cleaned_dataframes['vol5'].to_csv(\"/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/dfs/vol5_cleaned.csv\", index=False)\n",
    "# cleaned_dataframes['vol6'].to_csv(\"/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/dfs/vol6_cleaned.csv\", index=False)\n",
    "# cleaned_dataframes['vol7'].to_csv(\"/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/dfs/vol7_cleaned.csv\", index=False)\n",
    "# cleaned_dataframes['vol8'].to_csv(\"/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/dfs/vol8_cleaned.csv\", index=False)\n",
    "# cleaned_dataframes['vol9'].to_csv(\"/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/dfs/vol9_cleaned.csv\", index=False)\n",
    "# cleaned_dataframes['vol10'].to_csv(\"/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/dfs/vol10_cleaned.csv\", index=False)\n",
    "\n",
    "# # Loop over the volume directories\n",
    "# for i in range(1, 11):\n",
    "#     vol_path = os.path.join(base_path, f'/03MICCAI_notebook_df_vol{str(i)}_headers.csv')\n",
    "#     df_headers = process_xml_folder(vol_path)\n",
    "#     df_cleaned = clean_dataframe(df_headers)\n",
    "    \n",
    "#     # Store the cleaned DataFrame in the dictionary with the volume number as the key\n",
    "#     cleaned_dataframes[f'vol{str(i)}'] = df_cleaned\n",
    "\n",
    "# Dictionary with all cleaned DataFrames\n",
    "# cleaned_dataframes['vol1'], cleaned_dataframes['vol2']\n",
    "\n",
    "# print('total papers in 1:', len(cleaned_dataframes['vol1']['Paper Title'].unique())) # 73\n",
    "# print('total papers in 2:', len(cleaned_dataframes['vol2']['Paper Title'].unique())) # 73  \n",
    "# print('total papers in 3:', len(cleaned_dataframes['vol3']['Paper Title'].unique())) # 72 \n",
    "# print('total papers in 4:', len(cleaned_dataframes['vol4']['Paper Title'].unique())) # 75 \n",
    "# print('total papers in 5:', len(cleaned_dataframes['vol5']['Paper Title'].unique())) # 76 \n",
    "# print('total papers in 6:', len(cleaned_dataframes['vol6']['Paper Title'].unique())) # 77 \n",
    "# print('total papers in 7:', len(cleaned_dataframes['vol7']['Paper Title'].unique())) # 75 \n",
    "# print('total papers in 8:', len(cleaned_dataframes['vol8']['Paper Title'].unique())) # 65 \n",
    "# print('total papers in 9:', len(cleaned_dataframes['vol9']['Paper Title'].unique())) # 70\n",
    "# print('total papers in 10:', len(cleaned_dataframes['vol10']['Paper Title'].unique())) # 74\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
