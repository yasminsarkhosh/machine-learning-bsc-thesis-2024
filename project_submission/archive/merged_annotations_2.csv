title,paper name,vol,Does the article contain any of these keywords: cancer/tumor/tumour?,notes,Study subject labelled as ‘patient/patients’ in dataset(s),age,sex/gender,ethnicity,geographical location ,dataset quantity,quantity of public datasets,quantity of private datasets,image type in datasets,organ/body part in datasets,Does the dataset contain sex-specific organ(s)?,Female and/or male organs?,location ,location as a healthcare facility,name of healthcare facility,location as other (center/department/laboratory/university/institution/online),name of location as other (center/department/laboratory/university/institution/online),location as large-scale geographical entity,name of location as large-scale geographical entity,location as subnational geographical entity,name of location as subnational geographical entity,Information found outside article (such as sup material and/or by references,links/sup material 
3D Arterial Segmentation via Single 2D Projections and Depth Supervision in Contrast-Enhanced CT Images,14,1,1,"We use an in-house dataset of contrast-enhanced abdominal computed tomography images (CTs). 

The cohort consists of 141 patients with pancreatic ductal adenocarcinoma, of an equal ratio of male to female patients. Given a 3D arterial CT of the abdominal area, we automatically extract the vertebrae [15,18] and semi-automatically extract the ribs, which have similar intensities as arteries in
arterial CTs and would otherwise occlude the vessels.

For more information about the dataset, see [6].

Info outside paper
For cross-validation, the imaging data were partitioned into four even folds yielding a size of 35 or 36 samples per subset. For each cross-validation, two subsets served as train set, one as validation set and the remaining one as test set, rotated such that each partition was used for testing and validation exactly once and twice for training. Age at diagnosis and sex were obtained for all patients using the hospital’s information system. The mean age of the patients analyzed in this study was  years and the cohort consisted in equal parts of male and female patients (71 male, 72 female).

The clinical and imaging data were collected according to the principles set in Declaration of Helsinki and Good Clinical Practice. Written conformed consent was waived and the study approved by the institutional ethics review board of the Technical University of Munich, Faculty of Medicine ",1,0,1,0,1,1,0,1,"contrast-enhanced abdominal computed tomography images, cts",abdomen ,0,,1,1,No Name,0,No Name,0,No Name,0,No Name,1,https://link.springer.com/chapter/10.1007/978-3-030-87589-3_61#Sec2
A Multi-task Network for Anatomy Identification in Endoscopic Pituitary Surgery,45,9,1,"A dataset of 64-videos (635 images) were recorded, and annotated for anatomical structures through multi-round expert consensus.

Images come from 64-videos of endoscopic pituitary surgery where the sellar phase is present [9], recorded between 30 Aug 2018 and 20 Feb 2021 from The National Hospital of Neurology and Neurosurgery, London, United Kingdom. All patients have provided informed consent, and the study was registered with the local governance committee

Info outside paper
Participant demographics collected included training grade and country of practice. The collected data regarding the surgical workflow were quantitative (whether participants agree it is complete and accurate) and qualitative (additional suggestions or comments). Summary statistics (e.g. frequencies) were generated for participants demographics.",1,0,0,0,1,1,0,0,endoscope,pituitary gland,0,,1,1,national hospital of neurology and neurosurgery,0,No Name,1,united kingdom,1,london,1,https://link.springer.com/article/10.1007/s11102-021-01162-3#Sec2
A Texture Neural Network to Predict the Abnormal Brachial Plexus from Routine Magnetic Resonance Imaging,46,8,1,"Following IRB approval for this study, we search for patients with metastatic breast cancer who had a breast cancer MRI performed between 2010 and 2020 and had morphologically positive BP on the MRI report from our electronic medical records (EMR) in * hospital. Totally, 189 patients including 141 normal patients and 41 abnormal ones are obtained. The range of the age are varying from 15 to 85 years old. The female patient number and male patient number are almost even. Their weights by kg are in the range of [40.8 kg, 145 kg]. All patient experiences three kinds of MRI sequences including T2, T1 and post-gadolinium.  Patients are required to maintain a decubitus position while scanning.",1,1,1,0,1,1,0,0,mri,breast,0,,1,1,anonymous,0,No Name,0,No Name,0,No Name,0,0
ALL-IN: A Local GLobal Graph-Based DIstillatioN Model for Representation Learning of Gigapixel Histopathology Images With Application In Cancer Risk Assessment,74,3,1,"We compared the performance of our proposed model against the stateof- the-art (SOTA) techniques in histopathology risk stratification in two cancer datasets. Our results suggest that the proposed model is capable of stratifying patients into statistically significant risk groups (p < 0.01 across the two datasets) with clinical utility while competing models fail to achieve a statistical significance endpoint (p = 0.148 − 0.494).

This paper aims to investigate the potential of extracting fine and coarse features from histopathology slides and integrating them for risk stratification in cancer patients. Therefore, the contributions of this work can be summarized as: 1) a novel graph-based model for predicting survival that extracts both local and global properties by identifying morphological super-nodes; 2) introducing a fine-coarse feature distillation module with 3 various strategies to aggregate interactions at different scales; 3) outperforming SOTA approaches in both risk prediction and patient stratification scenarios on two datasets; 4) publishing two large and rare prostate cancer datasets containing more than 220 graphs for active surveillance and 240 graphs for brachytherapy cases. The code and graph embeddings are publicly available at https://github.com/pazadimo/ALL-IN

We utilize two prostate cancer (PCa) datasets to evaluate the performance of our proposed model. The first set (PCa-AS) includes 179 PCa patients who were managed with Active Surveillance (AS). Radical therapy is considered overtreatment in these patients, so they are instead monitored with regular serum prostate-specific antigen (PSA) measurements, physical examinations, sequential biopsies, and magnetic resonance imaging [23]. However, AS may be over- or under-utilized in low- and intermediate-risk PCa due to the uncertainty of current methods to distinguish indolent from aggressive cancers [11]. Although majority of patients in our cohort are classified as low-risk based on NCCN guidelines [21], a significant subset of them experienced disease upgrade that triggered definitive therapy (range: 6.2 to 224 months after diagnosis).

The second dataset (PCa-BT) includes 105 PCa patients with low to high risk disease who went through brachytherapy. 

We also utilized the Prostate cANcer graDe Assessment (PANDA) Challenge dataset [7] that includes more than 10,000 PCa needle biopsy slides (no outcome data) as an external dataset for training the encoder of our model.",1,0,0,0,0,3,2,0,"magnetic resonance,biopsy slides",prostate,1,male,0,0,No Name,0,No Name,0,No Name,0,No Name,1,"public dataset:
https://www.kaggle.com/c/prostate-cancer-grade-assessment/data"
AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on MRI Brain Tumor,17,1,1,"*mention patient in conclusion

We evaluate our method on the Brain Tumor Segmentation challenge (BraTS) dataset [1,2,14], which contains 2,000 cases, each of which includes four 3D volumes from four different MRI modalities. The official data split divides these cases by the ratio of 8:1:1 for training, validation, and testing (5,802 positive and 1,073 negative images). In order to evaluate the performance, we use the
validation set as our test set and report statistics on it. We preprocess the data by slicing each volume along the z-axis to form a total of 193,905 2D images, following the approach of Kang et al. [10] and Dey and Hong [6]. We use the ground-truth segmentation masks only in the final evaluation, not in the training process.

Experimental results on the four modalities of the 2021 BraTS datasetdemonstrate the superiority of our approach compared with other CAM-based weakly-supervised segmentation methods. Specifically, AME-CAM achieves the highest dice score for all patients in all datasets and modalities. These results indicate the effectiveness of our proposed approach in accurately segmenting
brain tumors from MRI images using only class labels

Info outside the paper
The BraTS dataset describes a collection of brain tumor MRI scans acquired from multiple different centers under standard clinical conditions, but with different equipment and imaging protocols, resulting in a vastly heterogeneous image quality reflecting diverse clinical practice across different institutions. However, we designed the following tumor annotation protocol, in order to make it
possible to create similar ground truth delineations across various annotators.",1,0,0,0,0,1,0,0,mri,brain,0,,0,0,No Name,0,No Name,0,No Name,0,No Name,1,https://arxiv.org/pdf/1811.02629.pdf
An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,68,6,1,"For the purpose of training and testing all the models, we extract four images of size 256 × 256 from each tile due to the size of the external IHC images, resulting in a total of 1072 images. We randomly extracted tiles from the LYON19 challenge dataset [14] to use as style IHC images. Using these images, we created a dataset of synthetically generated IHC images from
the hematoxylin and its marker image as shown in Fig. 3.",1,1,1,1,1,1,1,0,"ihc, if, multiplex staining","head and neck, h&n",0,,1,0,No Name,1,moffitt cancer center,0,No Name,0,No Name,0,0
An Anti-biased TBSRTC-Category Aware Nuclei Segmentation Framework with a Multi-label Thyroid Cytology Benchmark,56,6,1,"*info about the availability of the dataset is ambigous 

To the best of our knowledge, it is the first publicized thyroid cytopathology dataset of both image-wise and pixel-wise labels.
 We construct a clinical thyroid cytopathology dataset with images of both image-wise and pixel-wise labels as a benchmark (appear in GitHub upon acceptance) Some representative images are presented in Fig. 2, together with the profile of the dataset. The dataset comprises 4,965H&E stained image patches and labels of TBSRTC, where a subset of 1,473 images was densely annotated for nuclei boundaries by three experienced cytopathologists and reached a total number of 31,064 elaborately annotated nuclei. Patient-level images were partitioned first for training and test images, and patch-level curation was performed. We divided the dataset with image-wise labels into 80% training samples and the remaining 20% testing samples. Our collection of thyroid cytopathology images was granted with an Ethics Approval document.",1,0,0,0,0,1,0,0,h&e,thyroid,0,,0,0,No Name,0,No Name,0,No Name,0,No Name,0,0
Anatomy-Driven Pathology Detection on Chest X-rays,6,1,1,"* they reference to the datasets, however do not articulate whether they are public in the section of Dataset

** ambiguity: if the chest ImaGenome data is derived from MIMIC-CXR, does it mean it's a subset of that dataset. If so, what differs from the two? 

Training Dataset.
 We train on the Chest ImaGenome dataset [4,21,22] consisting of roughly 240 000 frontal chest X-ray images with corresponding scenegraphs automatically constructed from free-text radiology reports. It is derived from the MIMIC-CXR dataset [9,10], which is based on imaging studies from 65 079 patients performed at Beth Israel Deaconess Medical Center in Boston,
US.

Evaluation Dataset and Class Mapping. 
We evaluate our method on the subset of 882 chest X-ray images with pathology bounding boxes, annotated by radiologists, from the NIH ChestXray-8 (CXR8) dataset [20]3 from the National Insti tutes of Health Clinical Center in the US.",1,0,0,0,1,3,0,0,x-ray,chest,0,,1,1,"national institutes of health clinical center, beth israel deaconess medical center",1,"national institutes of health clinical center, beth israel deaconess medical center",1,us,1,boston,1,"https://physionet.org/content/chest-imagenome/1.0.0

https://physionet.org/content/mimic-cxr-jpg/2.0.0/

https://www.kaggle.com/datasets/nih-chest-xrays/data"
Anatomy-Informed Data Augmentation for Enhanced Prostate Cancer Detection,50,7,1,"774 consecutive bi-parametric prostate MRI examinations are included in this study, which were acquired in-house during the clinical routine. The ethics committee of the Medical Faculty Heidelberg approved the study (S-164/2019) and waived informed consent to enable analysis of a consecutive cohort. All experiments were performed in accordance with the declaration of Helsinki [2] and relevant data privacy regulations. For every exam, PI-RADS v2 [24 interpretation was performed by a board-certified radiologist. Every patient underwent extended systematic and targeted MRI trans-rectal ultrasound-fusion transperineal biopsy. Malignancy of the segmented lesions was determined from a systematic-enhanced lesion ground-truth histopathological assessment, which has demonstrated reliable ground-truth assessment with sensitivity comparable to radical prostatectomy [17]. The samples were evaluated according to the International Society of Urological Pathology (ISUP) standards under the supervision of a dedicated uropathologist. Clinically significant prostate cancer (csPCa) was defined as ISUP grade 2 or higher. Based on the biopsy results, every csPCa lesion was segmented on the T2-weighted sequences retrospectively by multiple in-house investigators under the supervision of a board-certified radiologist. In addition to the lesions, the rectum and the bladder segmentations were automatically predicted by a model built upon nnU-Net [8] trained iteratively on an in-house cohort initially containing a small portion of our cohort. Multiple radiologists confirmed the quality of the predicted segmentations.

",1,0,0,0,1,1,0,1,mri,prostate,1,male,1,1,medical faculty heidelberg,0,No Name,1,0,1,heidelberg,0,0
Asymmetric Contour Uncertainty Estimation for Medical Image Segmentation,21,3,1,"Note: The Japanese Society of Radiological Technology (JSRT) - a location?
**Ambigous: they say they use three datasets, however interpreting this line from the paper might seem like there is 4 datasets in total?  
Data comes from patients diagnosed with coronary artery disease, COVID, or healthy volunteers. The dataset is split into a training/validation set (80/20) and an independent test set from different sites, comprised of 994 echocardiograms from 684 patients and 368 echocardiograms from 206 patients, respectively.

In addition to being directly interpretable, our uncertainty estimation method outperforms previous methods on three datasets using two different image modalities. Code is available at: https://github.com/ThierryJudge/contouring-uncertainty.

The CAMUS dataset [20] contains cardiac ultrasounds from 500 patients.Manual annotations for the endocardium and epicardium borders of the left ventricle (LV) and the left atrium were obtained from a cardiologist for the end-diastolic (ED) and end-systolic (ES) frames. The dataset is split into 400 training patients, 50 validation patients, and 50 testing patients. 

Private Cardiac US. This is a proprietary multi-site multi-vendor dataset containing 2D echocardiograms of apical two and four chambers from 890 patients. Data comes from patients diagnosed with coronary artery disease, COVID, or healthy volunteers. The dataset is split into a training/validation set (80/20) and an independent test set from different sites, comprised of 994 echocardiograms from 684 patients and 368 echocardiograms from 206 patients, respectively.

JSRT. The Japanese Society of Radiological Technology (JSRT) dataset consists of 247 chest X-Rays [26]. We used the 120 points for the lungs and heart annotation made available by [10]. The set of points contains specific anatomical points for each structure (4 for the right lung, 5 for the left lung, and 4 for the heart) and equally spaced points between each anatomical point. We reconstructed the segmentation map with 3 classes (background, lungs, heart) with these points and used the same train-val-test split of 70%–10%–20% as [10].

With the exception of the Private Cardiac US dataset, the skewed normal distribution model shows very similar or improved results for both correlation and mutual information compared to the univariate and bivariate models. It can be noted, however, that in specific instances, the asymmetric model performs better on Private Cardiac US dataset (c.f. column 2 and 3 in Fig. 3). This confirms that it is better capturing asymmetric errors over the region of every contour point.",1,0,0,0,1,4,0,2,"ultrasounds, 2d echocardigrams, echocardiograms, x-rays","heart, chest, lung",0,,0,0,No Name,1,different sites,0,No Name,0,No Name,0,0
atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,23,8,1,"The method is evaluated on 21 openly available healthy subjects from the Human Connectome Project and an internal dataset of ten
neurosurgical cases.

The proposed technique was tested on a healthy-subject dataset and on a dataset containing tumor cases. The first comprises 21 subjects of the human connectome project (HCP) that were used for testing the automated methods TractSeg and Classifyber [3,18].

To test the proposed method on pathological data, we used an in-house dataset containing ten presurgical scans of patients with brain tumors. ",1,0,0,0,0,2,1,1,tractography,brain,0,,0,0,No Name,0,No Name,0,No Name,0,No Name,1,https://pdf.sciencedirectassets.com/272508/1-s2.0-S1053811913X00151/1-s2.0-S105381191300551X/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjENn%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJGMEQCIFyE0kxcJCYtCVec8GIZWCo%2FqswIZKasNPPjr85bjuJZAiArUMkvOXIaakMw0%2BeL9T3uGxYejpDoZ6h%2Bzoi1xSHfpiqzBQgSEAUaDDA1OTAwMzU0Njg2NSIMBscULTCwofTIB86zKpAFwEBhFNczb8dLeGrpdhqjphPiiKWnjJWOzjFo4JqSWb4%2FOngvVUaTIOgf%2F52xr%2F83tvEbLAdkfDbbI0fQMQXFDNWjbqMtENQsx8A96dGEvxnaFeO24T%2Fxm58fkUohyQ%2FbCxipeatF53ot3j9LRFQmpETWw64njUMRz32WaKW%2FnJ%2FHSoWhEEofMrLxTZIb2x6RroS%2Br%2FUETEDQiJz9Me1sjd1W9TKJINU46xKTbBZE69jZFHGsgQf%2FN%2B9YPYBlfjctKwr6nunXJeK77icrwTzU2jAhWuNbWE3V62ePEDTXFspNT8EiAokf7p4Bw3Ty8wcv8bjYomO5kqMKcNqzEZMyiSrjiHY55ro85irJeWY45SM62WjrKE4wyInm8P2qQZNDHPJ6mTbIoQjMgArEcCBAFLtiPAbjaRyyvfOIbgnP2xg2JCZKh7XP3ctPc7wNDmnkf%2FvStkogkwYO%2FGyUaxK0YRoFCPJmGhZUmtUMDjLGDTxtFOXJjB814MRINxWBd1WKP81sLDHf2o8Uu4jrYCua92s5tl6PrGY%2FfvzUgF%2Bj8Ef3uHqdKj5YjtYJCmYfx3bZN1%2F7%2Bvfeqp28zVfYv8VmfJ7zKdevIssqsEfDzzYdh8I46hRqLvJUjNOyGmgW%2FKOlArqAkS7NN%2Bt17vNI3HUPNeY%2FCZbNnqq6t1xwWsNHeC1HdCY%2BhoJo9Qi8IQC0Wu%2FKDcJucgjgMTcgI91w49kAyMEganjm0t1EyLvQbNZ0BMKka3mVg2T6yNV60Bp8UJPP0zTIhLaAPMMv4Rw8b2iMlo%2BvI7J8hcNpYxVBQsmMAdK37Go%2BoozF2VkYYymjrN3cVb%2FkZkdQkh9DGPAc0fCdrqP6XSGxlDfN1qElIzFFX54w1%2FzTsAY6sgGyCsphQxVkTUFDnX2sUZd4XothWgMtzxvChmdsHOGswGbvtgxPhzXLAXDFa9PbUzprys8LZy%2B2etbGCZOSF4KaHxBcvOtUGTNXy4%2FX8n9r0mg%2BidusFqo0G7G2EiyWZsVqEBXFMsEjWnrZkzqyV6axcHB50xvDMb6f85wINFNlO69L77jaS1bOax2WdBlxso9EZ4t1fA0N5BngtDpf9Cx8RskafomHPMy3zJfdDMlJro2m&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20240409T100743Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTY6XY2OAIV%2F20240409%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=766015d0b8659e71d80a4c9a06d40a560c1c3b6d0856d6031f925ea9a39eabb3&hash=866c7070a37f124da0eb26592f5a19cce8ed2c119cae96ca752f40cb403042ef&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S105381191300551X&tid=spdf-279f0031-3a6e-4e05-b067-7e3cf1f45660&sid=069c60407f4132401a1b3459c59a3a378755gxrqa&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=020d585650580d5f5505&rr=8719b11a0c1d92e8&cc=dk
Automated CT Lung Cancer Screening Workflow Using 3D Camera,40,7,1,"Note: difficult to determine the quantity of datasets used

CT scan dataset, 3D camera dataset and CT scan, CT scan from seperate site in Europe

Our CT scan dataset consists of 62, 420 patients from 16 different sites across
North America, Asia and Europe. Our 3D Camera dataset consists of 2, 742 pairs of depth image and CT scan from 2, 742 patients from 6 different sites across
North America and Europe acquired using a ceiling-mounted Kinect 2 camera.
Our evaluation set consists of 110 pairs of depth image and CT scan from 110
patients from a separate site in Europe.

We trained our AutoDecoder model on our unpaired CT scan dataset of 62, 420
patients with a latent vector of size 32. The encoder was trained on our paired
CT scan and depth image dataset of 2, 742 patients.",1,0,0,0,1,3,0,0,"ct, 3d camera",lung,0,,1,0,No Name,0,No Name,1,"north america, asia, europe",1,"unknown, referred to as ""different sites""",0,0
Automatic Bleeding Risk Rating System of Gastric Varices,1,5,1,"We also collect a GV bleeding risks rating dataset (GVbleed) with 1678 gastroscopy images from 411 patients that are jointly annotated in three levels of risks by senior clinical endoscopists. The experiments on our collected dataset show that our method can improve the rating accuracy by nearly 5% compared to the baseline. Codes and dataset will be available at https://github.com/LuyueShi/gastric-varices.

The GVBleed dataset contains 1678 endoscopicimages with gastric varices from 527 cases. All of these cases are collected from 411 patients in a Grade-III Class-A hospital during the period from 2017 to 2022. In the current version, images from patients with ages elder than 18 are retained[1]. The images are selected from the raw endoscopic videos and frames. To maximize the variations, non-consecutive frames with larger angle differences are selected. To ensure the quality of our dataset, senior endoscopists are invited to remove duplicates, blurs, active bleeding, chromoendoscopy, and NBI pictures.

The GVBleed dataset is partitioned into training and testing sets for evaluation, where the training set contains 1337 images and the testing set has 341 images. The detailed statistics of the three levels of GV bleeding risk in each set are shown in Table 1. The dataset is planned to be released in the future.

Info outside paper
Dataset details such as gender and age in supplementary material (outside of paper, however references to it in paper for more dataset details)",1,1,0,0,1,1,0,1,"gastroscopy, endoscopy","stomach, abdomen",0,,1,1,grade-III class-A hospital,0,No Name,0,No Name,0,No Name,1,https://static-content.springer.com/esm/chp%3A10.1007%2F978-3-031-43904-9_1/MediaObjects/554074_1_En_1_MOESM1_ESM.pdf
BerDiff: Conditional Bernoulli Diffusion Model for Medical Image Segmentation,47,4,1,"Extensive experimental results on two medical image segmentation datasets.

The data used in this experiment are obtained from LIDC-IDRI [2,7] and BRATS 2021 [4] datasets. LIDC-IDRI contains 1,018 lung CT scans with plausible segmentation masks annotated by four radiologists.

BRATS 2021 consists of four different sequence (T1, T2, FlAIR, T1CE) MRI images for each patient.

Our training set includes 55,174 2D images scanned from 1,126 patients, and the test set comprises 3,991 2D images scanned from 125 patients",1,0,0,0,0,2,0,0,"mri, t1, t2, flair, t1ce, ct, 2d",lung,0,,0,0,No Name,0,No Name,0,No Name,0,No Name,0,0
Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,53,7,1,"Ex-vivo: Data is collected from fresh breast tissue samples from the patients referred to BCS at Kingston Health Sciences Center over two years. The study
is approved by the institutional research ethics board and patients consent to be included. Peri-operatively, a pathologist guides and annotates the ex-vivo pointburns, referred to as spectra, from normal or cancerous breast tissue immediately after excision. In addition to spectral data, clinicopathological details such as the status of hormone receptors is also provided post-surgically. In total 51 cancer and 149 normal spectra are collected and stratified into five folds (4 for cross validation and 1 prospectively) with each patient restricted to one fold only.

Intra-operative: A stream of iKnife data is collected during a BCS case (27 min) at Kingston Health Sciences Center. At the sampling rate of 1Hz, a
total of 1616 spectra are recorded. Each spectrum is then labeled based both on surgeons comments during the operation and post-operative pathology report.

",1,0,0,0,1,2,0,0,intra-operative spectrometry,breast,0,,1,0,No Name,1,BSC kingston health sciences center,0,No Name,0,No Name,0,0
CARL: Cross-Aligned Representation Learning for Multi-view Lung Cancer Histology Classification,35,5,1,"Note: dataset collected from two different online resources

Our dataset NSCLC-TCIA for lung cancer histological subtype classification is sourced from two online resources of The Cancer Imaging Archive (TCIA) [5]: NSCLC Radiomics [1] and NSCLC Radiogenomics [2]. Exclusion criteria involves patients diagnosed with large cell carcinoma or not otherwise specified, along with cases that have contouring inaccuracies or lacked tumor delineation [9, 13]. Finally, a total of 325 available cases (146 ADC cases and 179 SCC cases) are used for our study. We evaluate the performance of NSCLC classification in five-fold cross validation on the NSCLC-TCIA dataset, and measure accuracy (Acc), sensitivity (Sen), specificity (Spe), and the area under the receiver operating characteristic (ROC) curve (AUC) as evaluationmetrics.We also conduct analysis including standard deviations and 95% CI, and DeLong statistical test for further AUC comparison.

For preprocessing, given that the CT data from NSCLC-TCIA has an in-plane resolutionof 1mm×1mmand a slice thickness of 0.7–3.0 mm, we resample the CT images using trilinear interpolation to a common resolution of 1mm × 1mm × 1mm.",1,0,0,0,0,1,1,0,ct,lung,0,,1,0,No Name,1,the cancer imaging archive,0,No Name,0,No Name,0,0
Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data,1,4,1,"To efficiently enrich the unlabeled pool, seeking support from other centers is a viable solution, as illustrated in Fig. 1. Yet, due to differences in imaging protocols and variations in patient demographics, this solution usually introduces data heterogeneity, leading to a quality problem. Such heterogeneity may impede the performance of SSL which typically assumes that the distributions of labeled data and unlabeled data are independent and identically distributed (i.i.d.) [16]. Thus, proper mechanisms are called for this practical but challenging SSL scenario.

We utilize prostate T2-weighted MR images from six different clinical centers (C1–6) [1,4,5] to perform a retrospective evaluation. 
The heterogeneity comes from the differences in scanners, field strengths,
coil types, disease and in-plane/through-plane resolution. Compared to C1 and
C2, scans from C3 to C6 are taken from patients with prostate cancer, either for
detection or staging purposes, which can cause inherent semantic differences in
the prostate region to further aggravate heterogeneity.",1,0,0,0,1,6,0,6,"t2, mr",prostate,1,male,1,1,No Name,0,No Name,0,No Name,0,No Name,0,0
Centroid-Aware Feature Recalibration for Cancer Grading in Pathology Images,20,2,1,"*the article say there are two publicly available colorectal cancer datasets, however the referred article (that contain the details of dataset does not show they are public. However, the referred article talks about 2 other publicly prostate datasets)

We evaluate the proposed network using colorectal cancer datasets that were collected under different environments.

Two publicly available colorectal cancer datasets [9] were employed to evaluate the effectiveness of the proposed CaFeNet. Table 1 shows the details of the datasets. Both datasets provide colorectal pathology images with ground truth labels for cancer grading. The ground labels are benign (BN), well-differentiated (WD) cancer, moderately differentiated (MD) cancer, and poorly-differentiated (PD) cancer. The first dataset includes 1600 BN, 2322 WD, 4105 MD, and 1830 PD image patches that were collected between 2006 and 2008. This dataset is divided into a training dataset (CTrain), validation dataset (CValidation), and a test dataset (CTestI). The second dataset, designated as CTestII. These were acquired between 2016.

However, the experiments were only conducted on two public colorectal cancer datasets from a single institute

Info outside the paper
- the dataset (that is referred to in the paper) labels subjects as patients.
- location and location information from sup. material.
- data collected from kangbuk samsung hospital",0,0,0,0,1,2,2,0,"wsi, wsis,  tissue microarrays, TMAs ",colorectal ,0,,1,0,No Name,1,No Name,0,No Name,0,No Name,1,https://www.sciencedirect.com/science/article/pii/S1361841521002516#sec0006
Certification of Deep Learning Models for Medical Image Segmentation,58,4,1,"*demographic information such as age, gender and location details  were outside paper's content, and study subjects labelled as patients. 

We conduct extensive experiments on five public datasets of chest X-rays, skin lesions, and colonoscopies, and empirically show that we are able to maintain high certified Dice scores even for highly perturbed images.

Datasets: We perform experiments on 5 different publicly available datasets. All datasets were divided to 70% for training, 10% for validation, and 20% for testing. The testing set is the one used to compute certified results.

Chest X-rays Datasets: JSRT dataset [31] with annotations of lung, heart, and clavicles provided by [35] is used. This dataset contains
247 images. For lung segmentation only, we use both the Montgomery and Shenzen datasets [21]. Montgomery consists of 138 and Shenzen of 662 annotated images.

Skin Lesion: Skin images with their annotations provided by the ISIC 2018 boundary segmentation challenge [10] were used. This dataset consists of 2694 RGB dermatoscopy images.

Colonoscopy Images: CVC-ClinicDB dataset [6] containing 612 colonoscopy images in RGB together with their annotations were utilized.

Info outside paper
{chest x-rays datasets: x-rays,  ISIC 2018: rgb dermatocopy,  cvc-clinicDB dataset: colonoscopy}, {chest x-rays datasets: chest, lung, heart, ISIC 2018: skin, cvc-clinicDB dataset: colon}, {chest x-rays datasets: Shenzhen No.3 People’s Hospital, ISIC 2018: unknown, cvc-clinicDB dataset:  Hospital Clinic of Barcelona} 
Chest X-rays Datasets: JSRT dataset
Original posteroanterior chest films for this database were collected from 13 medical centers in Japan and one institution in the United States under the following conditions: only one nodule on an image for nodule cases; confirmation of presence or absence of a lung nodule by CT examination; and nodule classification as malignant based on histologic and cytologic examination or as benign based on histology, definitive isolation of a pathogenic organism, shrinkage and disappearance with the use of antibiotics, or no change observed during a follow-up period of 2 year.
Of the patients with nodules, 68 were men and 86 were women, whereas patients without nodules included 51 men and 42 women. The average age of patients with nodules was 60 years old. Two patients were 21-30 years old, seven were 31-40 years old, 24 were 41-50 years old, 37 were 51-60 years old, 53 were 61-70 years old, and 29 were 71 years old or older. Two patients' ages were unknown.

MC dataset
The MC set has been collected in collaboration with the Department of Health and Human Services, Montgomery County, Maryland, USA. The set contains 138 frontal chest X-rays from Montgomery County’s Tuberculosis screening program, of which 80 are normal cases and 58 are cases with manifestations of TB.
Each reading contains the patient’s age, gender, and abnormality seen in the lung, if any.

The Shenzhen dataset 
was collected in collaboration with Shenzhen No.3 People’s Hospital, Guangdong Medical College, Shenzhen, China. The chest X-rays are from outpatient clinics and were captured as part of the daily hospital routine within a 1-month period.
Each reading contains the patient’s age, gender, and abnormality seen in the lung, if any.

ISIC 2018:
Unknown

CVC-ClinicDB database 
We introduce in this paper the CVC-ClinicDB database built in collaboration with Hospital Clinic of Barcelona, Spain.",0,0,0,0,0,5,5,0,"x-rays, rgb dermatocopy, colonoscopy","chest, lung, heart, skin, colon


",0,,0,0,No Name,0,No Name,0,No Name,0,No Name,1,"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4256233/

http://mv.cvc.uab.es/projects/colon-qa"
Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,64,8,1,"We analyze the behavior of CDVaDE on multiple datasets and compare it to other deep clustering algorithms.

The Colored MNIST is an extension to the classic MNIST dataset [3], which contains binary images of handwritten digits. The Colored MNIST includes colored images of the same digits, where each number and background have a color assignment. We present results of the experiments with five distinct colors and five digits of MNIST (0–4). To enhance computational efficiency and expedite experiments, we utilized only 1% of the MNIST images, which were sampled at random.

HER2 Dataset. 
Human epidermal growth factor receptor 2 (HER2 or HER2/neu) is a protein involved in normal cell growth, which plays an important role in the diagnosis and treatment of breast cancer [8]. The dataset consists of 241 patches extracted from 64 digitized slides of breast cancer tissue which were stained with HER2 antibody. Each tissue slide has been digitized at three different sites using three different whole slide imaging systems, evaluated by 7 pathologists on a 0–100 scale, and following clinical practice labeled as HER2 Class 1, 2, or 3 (based on mean pathologists’ scores with cut-points at 33 and 66). 
We use a subset of this dataset consisting of 672 images (the remainder is held out for future research). Because the intended purpose is finding subgroupsin the given dataset only, a separate test set is not used.  We refer to [4,8] for more details about this dataset.
This retrospective human subject dataset has been made available to us by the authors of the prior studies [4,8], who are not associated with this paper. Appropriate ethical approval for the use of this material in research has been obtained.

We evaluate the performance and behavior of the DEC, VaDE, and CDVaDE models on the HER2 dataset. We investigate whether the models will learn to distinguish the HER2 class labels, the scanner labels, or other potentially meaningful data subgroups in a fully unsupervised fashion.

Info outside paper
Acknowledgement: The authors would like to thank Dr. Marios Gavrielides for providing access to the HER2 dataset and for helpful discussion. This project was supported n part by an appointment to the Research Participation Program at the U.S. Food and Drug Administration administered by the Oak Ridge Institute for Science and Education through an interagency agreement between the U.S. Department of Energy and the U.S. Food and Drug Administration. XS acknowledges support from the Hightech Agenda Bayern.

MNIST dataset (referenced link was not accessable)
",0,0,0,0,0,2,0,0,wsi,breast,0,,0,0,No Name,0,No Name,0,No Name,0,No Name,1,https://www.sciencedirect.com/science/article/pii/S0262885604000721?via%3Dihub
Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,16,5,1,"*location details dervied outside paper

Grand Challenge Camelyon data [16] is used for model training (770 WSIs of which 293 WSIs contain metastases) and in-domain testing (629 WSIs of which
289 WSIs contain metastases). For domain shift test data, we extracted 302 WSIs (of which 111 contain metastases) from AIDA BRLN dataset [12]. To evaluate clinically realistic domain shifts, we divided the dataset in two different ways, creating four subsets:

1a. 161 WSIs from sentinel node biopsy cases (54 WSIs with metastases): a small shift as it is the same type of lymph nodes as in Grand Challenge Camelyon data [16].
1b. 141 WSIs from axillary nodes dissection cases (57 WSIs with metastases): potentially large shift as some patients have already started neoadjuvant treatment as well as the tissue may be affected from the procedure of sentinel lymph node removal.
2a. 207 WSIs with ductal carcinoma (83 WSIs with metastases): a small shift as it is the most common type of carcinoma and relatively easy to diagnose.
2b. 68 WSIs with lobular carcinoma (28 WSIs with metastases): potentially large shift as it is a rare type of carcinoma and relatively difficult to diagnose.

Moreover, discussions with pathologists led to
the conclusion that it is clinically relevant to evaluate the performance difference between ductal and lobular carcinoma. Our method is intended to avoid requiring dedicated WSI labelling efforts. We deem that the information needed to do this type of subset divisions would be available without labelling since the patient cases in a clinical setting would already contain such information. All datasets are publicly available to be used in legal and ethical medical diagnostics research.

Info outside paper
The data set for CAMELYON17 is collected from 5 medical centres in the Netherlands. 
{CAMELYON17:netherlands},
AIDA BRLN dataset:
{AIDA BRLN dataset: department of clinical pathology} ,  {AIDA BRLN dataset: Linköping Region Östergötland}
The cases are anonymised and exported from the digital archive at the Department of Clinical Pathology in Linköping, Region Östergötland.",1,0,0,0,0,2,2,0,"wsi, wsis, h&e",lymp nodes,0,,0,0,No Name,0,No Name,0,No Name,0,No Name,1,https://camelyon17.grand-challenge.org/Data/
Detecting the Sensing Area of a Laparoscopic Probe in Minimally Invasive Cancer Surgery,25,9,1,"To further validate the proposed solution, we acquired and publicly released two datasets captured using a custom-designed, portable stereo laparoscope system.

our first newly acquired dataset, named Jerry, contains 1200 sets of images. Since it is important to report errors in 3D and in millimeters, we recorded another dataset similar to Jerry but also including ground truth depth map for all frames by using structured-lighting system [8]—namely the Coffbee dataset.

[8] https://arxiv.org/pdf/2208.08407.pdf
Provides info about 2 datasets: SCARED and LATTE, not Coffbee.",0,0,0,0,0,2,0,0,"laparoscopy, 3d, 2d",no organ mentioned,0,,0,0,No Name,0,No Name,0,No Name,0,No Name,0,0
Detection of Basal Cell Carcinoma in Whole Slide Images,26,6,1,"*they say they use on skin cancer dataset, however tested their model on the ChestMNIST and DermaMINIST subsets of MedMNISTv2

we analyze skin cancer images using the optimal network obtained by neural architecture search (NAS) on the skin cancer dataset.

The dataset, comprised of 194 skin slides acquired from the Southern Sun Pathology laboratory, includes 148 BCC cases and 46 other types (common nevus, SCC), all manually annotated by a dermatopathologist. BCC slides served as positive samples and the rest as negatives.

We tested our model’s generalization on the ChestMNIST and DermaMNIST subsets of MedMNISTv2 [25], following established protocols. As shown in Table 4, our s ResNet50 surpassed the original ResNet50 on all datasets, gaining 2.3% and 1.8% more AUC on ChestMNIST and DermaMNIST respectively, proving the model’s robust generalization.

Performance comparison on ChestMNIST and DermaMNIST datasets.",0,0,0,0,1,3,0,0,wsi,skin,0,,1,0,No Name,1,south sun pathology laboratory,0,No Name,0,No Name,0,0
Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels,46,3,1,"*location details of data collection outside the paper's content.

qubiq dataset: prostate, brain, kidney
we use QUBIQ 2020, which contains 7 segmentation tasks in 4 differen CT and MR datasets: Prostate (55 cases, 2 tasks, 6 raters), Brain Growth (39 cases, 1 task, 7 raters), Brain Tumor (32 cases, 3 tasks, 3 raters), and Kidney 24 cases, 1 task, 3 raters).

lits dataset: iver
LiTS contains 201 high-quality CT scans of liver tumors. Out of these, 131 cases are designated for training and 70 for testing. As the ground-truth labels for the test set are not publicly accessible, we only use the training set.

kits dataset: 
KiTS includes 210 annotated CT scans of kidney tumors from different patients

Our extensive experiments on the public QUBIQ, LiTS and KiTS benchmarks validate that incorporating soft labels leads to higher Dice score and lower calibration error, indicating that these losses can find wide application in diverse medical image segmentation problems.

Info outside the paper
{lits: Rechts der Isar Hospital), {lits: the Technical University of Munich, Polytechnique Montreal and CHUM Research Center,  Sheba Medical Center, the Hebrew University of Jerusalem, Hadassah University Medical Center,  IRCAD}, {lits: Germany, Netherlands, Canada, Israel, France}, 
The image data for the LiTS challenge are collected from seven clinical sites all over the world, including a) Rechts der Isar Hospital, the Technical University of Munich in Germany, b) Radboud University Medical Center, the Netherlands, c) Polytechnique Montr´eal and CHUM Research Center in Canada, d) Sheba Medical Center in Israel, e) the Hebrew University of Jerusalem in Israel, f) Hadassah University Medical Center in Israel, and g) IRCAD in France. The distribution of the number of scans per institution is described in Table 2. The LiTS benchmark dataset contains 201 computed tomography images of the abdomen, of which 194 CT scans contain lesions. All data are anonymized, and the images have been reviewed visually to preclude the presence of personal identifiers

kits dataset: public,  kidney (abdominal cts)
KiTS includes 210 annotated CT scans of kidney tumors from different patients",1,0,0,0,0,3,3,0,"ct, mr","prostate, brain, abdomen, kidney, liver",1,male,0,0,No Name,0,No Name,0,No Name,0,No Name,1,"https://qubiq.grand-challenge.org/

https://arxiv.org/pdf/2108.09987.pdf

https://arxiv.org/pdf/1901.04056.pdf

https://competitions.codalab.org/competitions/17094"
Diffusion Kinetic Model for Breast Cancer Segmentation in Incomplete DCE-MRI,10,4,1,"*age and race provided outside paper's content

We evaluate the proposed method for tumor segmentation on public breast cancer DCE-MRI dataset.

To demonstrate the effectiveness of our proposed DKM, we evaluate our method on 4D DCE-MRI breast cancer segmentation using the Breast-MRINACT-Pilot dataset [13], which contains a total of 64 patients with the contrastenhanced MRI protocol: a pre-contrast scan, followed by 2 consecutive postcontrast time points (As shown in Fig. 3). Each MR volume consists of 60 slices and the size of each slice is 256×256. Regarding preprocessing, we conduct zeromean unit-variance intensity normalization for the whole volume. We divided the original dataset into training (70%) and test set (30%) based on the scans. Ground truth segmentations of the data are provided in the dataset for tumor annotation. No data augmentation techniques are used to ensure fairness.

Info outside paper
Breast-MRI-NACT-Pilot
Recurrence-free survival (RFS) was assessed for each patient at 6-month or 1-year intervals following surgery. Other clinical and endpoint data includes patient age, lesion characteristics including pretreatment tumor size, histologic type, pathologic size, tumor subtype, and lymph node involvement. These data are included as supplemental information for the collection in the accompanying clinical information workbook.

Clinical and DFS Metadata shows:
race and age",1,0,0,0,0,1,1,0,"dce-mri, mri",breast,0,,0,0,No Name,0,No Name,0,No Name,0,No Name,1,"Clinical and DFS Metadata (XLS, 103 kB) in
https://wiki.cancerimagingarchive.net/pages/viewpage.action?pageId=22513764

https://www.kaggle.com/datasets/alexnguyen10/breast-mri-nact-pilot"
DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration,72,10,1,"*gender/sex info, location details of data collection  found outside paper 

Experiments on three different datasets demonstrate that our approach generalizes well beyond the training data, yielding a broad capture range even on unseen anatomies and modality pairs, without the need for specialized retraining. We make our training code and data publicly available.

dataset 1:
Our neural network is trained using patches from the “Gold Atlas- Male Pelvis - Gentle Radiotherapy” [14] dataset, which is comprised of 18 patients each with a CT, MR T1, and MR T2 volumes. 

Consequently, for the purpose of ensuring reproducibility, all evaluations presented in this paper exclusively pertain to the model trained solely on the public MR-CT dataset.

dataset 2:
Evaluation of Cerebral Tumors (RESECT) MICCAI challenge dataset [23]. This dataset consists of 22 pairs of pre operative brain MRs and intra-operative ultrasound volumes.

dataset 3: 
Our second application is the Abdomen MR-CT task of the Learn2Reg challenge 2021 [8]. The dataset comprises 8 sets of MR and CT volumes, both depicting the abdominal region of a single patient and exhibiting notable deformations

We are using a heterogeneous dataset of 27 cases, comprising liver cancer patients and healthy volunteers, different ultrasound machines, as well as optical

Info outside paper
Gold Atlas: public,  Magnetic resonance images (MRI) (T1w and T2w) with flat table top, and CT images of 19 male patients. The imaging was performed over the pelvic region. The patients were recruited and included from three different Swedish radiotherapy departments. Male patients with prostate or rectal cancer referred for curative radiotherapy were eligible for inclusion. Patients with locally advanced tumors (prostate cT3-4 and rectal cT4) were not included.

RESECT: public,  All medical images used for the challenge were acquired for routine clinical care at St Olavs University Hospital (Trondheim, Norway), after patients gave their informed consent.

Learn2Reg: public",1,0,0,0,0,3,1,0,"ct, mr, mr t1 volumes, mr t2 volumes, ultrasound volumes, us volumes,  intra-operative ultrasound volumes, mr-ct","brain, abdomen, pelvic region, prostate, liver",1,male,0,0,No Name,0,No Name,0,No Name,0,No Name,1,"https://zenodo.org/records/583096

https://aapm.onlinelibrary.wiley.com/doi/full/10.1002/mp.12748

https://curious2018.grand-challenge.org/Data/

https://aapm.onlinelibrary.wiley.com/doi/full/10.1002/mp.12268

https://learn2reg.grand-challenge.org/Learn2Reg2021/"
Distributionally Robust Image Classifiers for Stroke Diagnosis in Accelerated MRI,74,5,1*,"*The article contain the word 'tumor', but the main research area is brain strokes. 
We verified the effectiveness of our approach on the actual clinical scans acquired for clinical care and not just for research purposes, suggesting that the methods and findings in the current study should be generalizable to routine clinical practice conditions and potentially other types of clinical image-based diagnosis (e.g., brain tumor) as well.

Our dataset included MRI brain scans from 226 patients performed at an urban tertiary referral academic medical center that is a comprehensive stroke center. Clinical scans of adult patients aged 18–89 years with recent (acute or subacute) strokes were identified between 1/1/2013 and 1/1/2021 for inclusion in this study via a search of the Philips Performance Bridge. Scans meeting this criteria were downloaded and simultaneously anonymized to preserve patient anonymity and prevent disclosure of protected health information as part of this IRB exempt study. No patient demographic information was retained for the scans, as it was considered to represent an unnecessary risk for accidental release of protected health information. The diffusion weighted images with a gradient of B=1000 were utilized for the analysis (see the Supplement1 for information about the MRI scanner and parameter settings).

While the whole dataset includes 4,883 (74.7%) normal slices and 1,650 (25.3%) stroke slices, we further randomly split them into training/validation/test sets using the ratio 80%/10%/10%. For the training set, we implemented data augmentation strategies by rotating or flipping each slice. Finally, the training/validation/test set contains 31,356/653/654 slices, correspondingly.

We implemented DRL to both CNN and ViT models. For the CNN model, we used a ResNet-18 [9] architecture,
while for the ViT model, we first pre-trained a 4-layer ViT using a self-supervised pre-training method called Masked Autoencoder (MAE)
[8], using the T1/T2-weighted brain MR images in the IXI dataset [1].

As our dataset is unbalanced, we also considered the Area Under Precision-Recall Curve (AUPRC).We ran the experiments 3 times using different random seeds.

Info outside paper
{ixi dataset: hammersmith hospital, guy's hospital, institute of psychiatry}
{ixi dataset:  institute of psychiatry}
{ixi data: london}
",1,1,0,0,1,2,0,0,"mri, mr, t1, t2",brain,0,,1,1,No Name,0,No Name,0,No Name,0,No Name,1,https://brain-development.org/ixi-dataset/
FocalErrorNet: Uncertainty-Aware Focal Modulation Network for Inter-modal Registration Error Estimation in Ultrasound-Guided Neurosurgery,66,9,1,"*location details of data collection found outside paper's content.

Developed and validated with the public RESECT clinical database.

we used the RESECT (REtro-Spective Evaluation of Cerebral Tumors) dataset [16], MRI and iUS scans at different surgical stages from 23 subjects who underwent low-grade glioma resection surgeries. we took 22 cases with T2FLAIR MRI that better depicts tumor boundaries and iUS acquired before resection. An example of an MRI-iUS pair from a patient is shown in Fig. 1

One limitation of our work lies in the limited patient data, as public iUS datasets are scarce, while the settings and properties of US scanners can vary, potentially affecting the DL model designs.

Info outside paper
Pre-operative magnetic resonance images (MRI), and intra-operative ultrasound (US) scans were acquired from 23 patients with low-grade gliomas who underwent surgeries at St. Olavs University Hospital between 2011 and 2016",1,0,0,0,0,1,1,0,"mri, ius","brain, cerebral",0,,0,0,No Name,0,No Name,1,No Name,0,No Name,1,https://pubmed.ncbi.nlm.nih.gov/28391601/
Gene-Induced Multimodal Pre-training for Image-Omic Classification,49,6,1,"We verify the effectiveness of our method on The Caner Genome Atlas (TCGA) non-small cell lung cancer (NSCLC) dataset, which contains two cancer subtypes, i.e., Lung Squamous Cell Carcinoma (LUSC) and Lung Adeno carcinoma (LUAD).
 TCGA-NSCLC dataset, i.e., only pathological WSIs are included as input.",1,0,0,0,0,1,0,0,"wsis, wsi",lung,0,,0,0,No Name,0,No Name,0,No Name,0,No Name,0,0
Geometry-Invariant Abnormality Detection,29,1,1,"*demographics and data collection details (location) found outside paper's content

For this work we leveraged whole-body PET/CT data from different sources to explore the efficacy of our approach for varying image geometries. 211 scans from NSCLC Radiogenomics [2,3,10,16] combined with 83 scans from a proprietary dataset constitute our lower resolution dataset.

For evaluation, we use four testing sets: a lower resolution set derived from both the NSCLC and the private dataset. a testing set with random crops of the same NSCLC/private testing dataset and finally a testing set that has been rotated through 90◦ using the high resolution testing data.


Info outside paper: 
Acknowledgements: The models were trained on the NVIDIA Cambridge-1. Private dataset was obtained through King’s College London (14/LO/0220 ethics application number). 

NSCLC
{nsclc: standford university school of medicine, palo alto veterans affairs healtcare system}
Between 2008 and 2012, we collected clinical and imaging data for 211 subjects referred for surgical treatment and obtained tissue samples from the excised tumors, where available. Tissue samples were analyzed to produce molecular phenotypes using gene microarrays, RNA sequencing technology, or both, in addition to standard-of-care NSCLC mutational testing. We also collected clinical data, such as: age, gender, weight, ethnicity, smoking status, TNM stage, histopathological grade. 

The R01 cohort consisted of 162 NSCLC subjects (38 females, 124 males, age at scan: mean 68, range 42–86) from Stanford University School of Medicine (69) and Palo Alto Veterans Affairs Healthcare System (93). Subjects were recruited between April 7th, 2008 and September 15th, 2012.

",0,0,0,0,0,2,0,1,"pet, ct",whole body,0,,1,0,No Name,0,No Name,0,No Name,0,No Name,1,https://www.nature.com/articles/sdata2018202
Graph-Theoretic Automatic Lesion Tracking and Detection of Patterns of Lesion Changes in Longitudinal CT Studies,11,5,1,"We evaluated our method with two studies on retrospectively collected patient datasets
that were manually annotated by an expert radiologist.

DLIVER dataset, DLUNG dataset.

Lung and liver CT studies were retrospectively obtained from two medical
centers (Hadassah Univ Hosp Jerusalem Israel) during the routine clinical examination
of patients with metastatic disease. Each patient study consists of at least 3 scans.

",1,0,0,0,1,2,0,0,"mri, ct","lung, liver",0,,1,1,hassadah university hospital,0,No Name,1,Israel,1,Jerusalem,0,0
Guiding the Guidance: A Comparative Analysis of User Guidance Signals for Interactive Segmentation of Volumetric Images,61,3,1,"*sex, age was found outside the paper's content, and the location details of data collections.

We conduct our study on the MSD Spleen and the AutoPET datasets to explore the segmentation of both anatomy (spleen) and pathology (tumor lesions).

We trained and evaluated all of our models on the openly available AutoPET [1] and MSD Spleen [2] datasets. MSD Spleen [2] contains 41 CT volume.  AutoPET [1] consists of 1014 PET/CT volumes with annotated tumor lesions of melanoma, lung cancer, or lymphoma. We discard the 513 tumor-free patients, leaving us with 501 volumes. We also only use PET data for our experiments. 

Info outside the paper
{msd spleen: memorial sloan kettering cancer center, autopet: unknown}
MSD spleen
Task09_Spleen The spleen dataset was comprised of patients undergoing chemotherapy treatment for liver metastases at Memorial Sloan Kettering Cancer Center (New York, NY, USA) and previously reported [32]. 
AutoPet
Publication of anonymized data was approved by the institutional ethics committee of the Medical Faculty of the University of Tübingen as well as the institutional data security and privacy review board. Data from 1,014 whole-body FDG-PET/CT examinations of 900 patients acquired between 2014 and 2018 as part of a prospective registry study9 were included in this dataset.

The selection criteria for positive and negative samples were: age >18 years patient characteristics: https://www.nature.com/articles/s41597-022-01718-3/tables/2",1,0,0,0,0,2,2,0,"ct, computed tomography, pet/ct, pet, Positron Emission Tomography","spleen, lung",0,,0,0,No Name,0,No Name,0,No Name,0,No Name,1,"https://arxiv.org/pdf/1902.09063.pdf

https://www.nature.com/articles/s41597-022-01718-3

https://www.nature.com/articles/s41597-022-01718-3/tables/2"
Histopathology Image Classification Using Deep Manifold Contrastive Learning,66,6,1,"We tested our proposed method on two different tasks: (1) intrahepatic cholangiocarcinomas(
IHCCs) subtype classification and (2) liver cancer type classification. 

The dataset for the former task was collected from 168 patients with 332 WSIs from Seoul National University hospital. IHCCs can be further categorized into small duct type (SDT) and large duct type (LDT). Using gene mutation information as prior knowledge, we collected WSIs with wild KRAS and mutated IDH genes for use as training samples in SDT, and WSIs with mutated KRAS and wild IDH genes for use in LDT. The rest of the WSIs were used as testing samples. 
The liver cancer dataset for the latter task was composed of 323 WSIs, in which the WSIs can be further classified into hepatocellular carcinomas (HCCs) (collected from Pathology AI Platform [1]) and IHCCs.We collected 121 WSIs for the training set, and the remaining WSIs were used as the testing set.",1,0,0,0,1,2,0,0,"wsis, wsi",liver,0,,1,1,seoul national university hospital,0,No Name,1,south korea,1,seoul,0,http://wisepaip.org/paip
Liver Tumor Screening and Diagnosis in CT with Pixel-Lesion-Patient Network,8,5,1,"We collect a large-scale dataset with both tumor and non-tumor subjects, where the non-tumor subjects includes not only healthy ones, but also patients with various diffuse liver diseases such as steatosis and hepatitis to improve the robustness of the algorithm

Our dataset contains 810 normal subjects and 939 patients with liver tumors. Each normal subject has a non-contrast (NC) CT, while each patient has a dynamic contrast-enhanced (DCE) CT scan with NC, arterial, and venous phases.

We first train an nnU-Net on public datasets to segment liver and surrounding organs (gallbladder, hepatic vein, spleen, stomach, and pancreas), and then crop the liver region to train PLAN.

The NC test set contains 198 tumor cases, 202 completely normal cases, and 100 “hard” non-tumor cases which may have larger image noise, artifact, ascites, diffuse liver diseases such as hepatitis and steatosis. These cases are used to test the robustness of the model in real-world screening scenario with diverse tumor-free images.",1,0,0,0,0,2,1,1,"nc-ct, dce-ct, ct","liver, gallbladder, hepatic vein, spleen, stomach, pancreas",0,,0,0,No Name,0,No Name,0,No Name,0,No Name,0,0
Localized Region Contrast for Enhancing Self-supervised Learning in Medical Image Segmentation,45,2,1,"Through extensive experiments on three multi-organ segmentation datasets, we demonstrate that integrating LRC to an existing
self-supervised method in a limited annotation setting significantly improves segmentation performance.

During both global and local pre-training stages, we pre-train the encoders on the Abdomen-1K [17] dataset. It contains over one thousand CT images which equates to roughly 240,000 2D slices. The CT images have been curated from 12 medical centers and include multi-phase, multi-vendor, and multi-disease cases.Although segmentation masks for liver, kidney, spleen, and pancreas are provided.

During the fine-tuning stage, we perform extensive experiments on three datasets with respect to different regions of the human body: abd-110 dataset: abdomen, thorax-85 dataset: thorax, HaN dataset: head and neck

ABD-110 is an abdomen dataset from [25] that contains 110 CT images from patients with various abdominal tumors and these CT images were taken during the treatment planning stage. We report the average DSC on 11 abdominal organs (large bowel, duodenum, spinal cord, liver, spleen, small bowel, pancreas, left kidney, right kidney, stomach and gallbladder).
Thorax-85 is a thorax dataset from [5] that contains 85 thoracic CT images. We report the average DSC on 6 thoracic organs (esophagus, trachea, spinal cord, left lung, right lung, and heart).
HaN is from [24] and contains 120 CT images covering the head and neck region. We report the average DSC on 9 organs (brainstem, mandible, optical chiasm, left optical nerve, right optical nerve, left parotid, right parotid, left submandibular gland, and right submandibular gland).

Info outside paper
AbdomenCT-1K
Most existing abdominal organ segmentation datasets have limitations in diversity and scale. In this paper, we present a large-scale dataset that is closer to real-world applications and has more diverse abdominal CT cases. consists of 1112 3D CT scans from five existing datasets: LiTS (201 cases) [16], KiTS (300 cases) [17], MSD Spleen (61 cases) and Pancreas (420 cases) [20], NIH Pancreas (80 cases) [21], [22], [23], and a new dataset from Nanjing University (50 cases). The 50 CT scans in the Nanjing University dataset are from 20 patients with pancreas cancer, 20 patients with colon cancer, and 10 patients with liver cancer. The number of plain phase, artery phase, and portal phase scans are 18, 18, and 14 respectively

Thorax-85
The data sources included in-house CT images (540 cases) and public data derived from the Cancer Image Archive (TCIA [21], 215 cases).",1,0,0,0,1,4,0,0,ct,"abdomen, thorax, head and neck ",0,,1,1,No Name,0,No Name,0,No Name,0,No Name,1,"https://arxiv.org/pdf/2010.14808.pdf

https://arxiv.org/pdf/2012.09279.pdf

https://www.sciencedirect.com/science/article/pii/S0167814021062174"
Many Tasks Make Light Work: Learning to Localise Medical Anomalies from Multiple Synthetic Tasks,16,1,1,"*demographic info such as age, gender, details of location of data collection provided outside of paper's content. study subjects are labelled as patients outside paper's content. whether datasets are public or private was to find in the section for ""References""

We evaluate our method on T2-weighted brain MR and chest X-ray datasets to provide direct comparisons to state-of-the-art methods over a wide range of real anomalies. 
For brain MRI we train on the Human Connectome Project (HCP) dataset [28] which consists of 1113 MRI scans of healthy, young adults acquired as part of a scientific study. 
To evaluate, we use the Brain Tumor Segmentation Challenge 2017 (BraTS) dataset [1], containing 285 cases with either high or low grade glioma, and the ischemic stroke lesion segmentation challenge 2015 (ISLES) dataset [13], containing 28 cases with ischemic stroke lesions. The data from both test sets was acquired as part of clinical routine. The HCP dataset was resampled to have 1mm isotropic spacing to match the test datasets. 
For chest X-rays we use the VinDr-CXR dataset [18] including 22 differen local labels.

Info outside paper
HCP
Our decision to acquire data from twins and non-twin siblings will enable analyses of the heritability of brain circuits and will greatly increasethe power of genetic analyses. However, due to the relatively small size and localized geography of the subject population, HCP faces some extra challenges with respect to subject confidentiality and privacy, especially regarding sensitive data. One likely scenario is that the publicly released HCP dataset will include all neuroimagingdata and most behavioral data, along with subject sex and age range (e.g., 5-year grouping). Information about family relationships, ethnic and racial identity, exact age (year), and potentially sensitive behavioral measures would be restricted to qualified investigators who agree to appropriate limits on storage and distribution of sensitive data. The publicly released data could also include a dataset consisting of only one individual per family, thereby allowing analyses not confounded by unspecified family relationships.

Brats
In 2017, thanks to additional contributions to the BraTS dataset, from CBICA@UPenn and the
University of Alabama in Birmingham (UAB), a validation set was included to facilitate algorithm
fine-tuning following a ML paradigm of training, validation, and testing datasets. Notably, in 2017
the number of cases was doubled with respect to the previous year, amounting to 477 cases, which
was further increased in 2018 with 542 cases, thanks to contributions from MD Anderson Cancer
Center in Texas, the Washington University School of Medicine in St. Louis, and the Tata Memorial
Center in India

VinDr-CXR
{vin-dr cxr: HMUH, H108}
{vin-dr cxr: vietnam}
The building of VinDr-CXR dataset, as visualized in Fig. 1, is divided into three main steps: (1) data collection, (2) data filtering, and (3) data labeling. Between 2018 and 2020, we retrospectively collected more than 100,000 CXRs in DICOM format from local PACS servers of two hospitals in Vietnam, HMUH and H108. Imaging data were acquired from a wide diversity of scanners from well-known medical equipment manufacturers, including Phillips, GE, Fujifilm, Siemens, Toshiba, Canon, Samsung, and Carestream. The ethical clearance of this study was approved by the Institutional Review Boards (IRBs) of the HMUH and H108 before the study started. The need for obtaining informed patient consent was waived because this retrospective study did not impact clinical care or workflow at these two hospitals and all patient-identifiable information in the data has been removed.

dataset provides info about patient's sex, age, size, weights",0,0,0,0,0,4,0,0,"mri,  x-rays, mr","brain, chest",0,,0,0,No Name,0,No Name,0,No Name,0,No Name,1,"https://www.sciencedirect.com/science/article/pii/S1053811912001954?via%3Dihub

https://arxiv.org/abs/1811.02629

https://www.sciencedirect.com/science/article/pii/S1361841516301268?via%3Dihub

https://www.nature.com/articles/s41597-022-01498-w

https://static-content.springer.com/esm/art%3A10.1038%2Fs41597-022-01498-w/MediaObjects/41597_2022_1498_MOESM1_ESM.pdf"
Merging-Diverging Hybrid Transformer Networks for Survival Prediction in Head and Neck Cancer,39,6,1,"*demographic information provided in supplementary information.

Extensive experiments on the public dataset of HEad and neCK TumOR segmentation and outcome prediction challenge (HECKTOR
2022) demonstrate that our XSurv outperforms state-of-the-art survival prediction methods.

We adopted the training dataset of HECKTOR 2022 (refer to https://hecktor.grand-cha llenge.org/), including 488 H&N cancer patients acquired from seven medical centers [7], while the testing dataset was excluded as its ground-truth labels are not released. Each patient underwent pretreatment PET/CT and has clinical indicators. We present the distributions of all clinical indicators in the supplementary materials. Recurrence- Free Survival (RFS), including time-to-event in days and censored-or-not status, was provided as ground truth for survival prediction, while PT and MLN annotations were provided for segmentation. The patients from two centers (CHUM and CHUV) were used for testing and other patients for training, which split the data into 386/102 patients in training/testing sets. We trained and validated models using 5-fold cross-validation within the training set and evaluated them in the testing set.

Info outside paper
HECKTOR 2022
canada, switzerland, france, usa
The data used in this challenge is multi-centric (9 centers in total), including four centers in Canada [Vallières et al. 2017], two centers in Switzerland [Castelli et al. 2017; Bogowicz et al. 2017], two centers in France [Hatt et al. 2019; Legot et al. 2018], and one center in USA [Ger et al. 2019] for a total of XXX patients with annotated GTVp and GTVn.

The clinical information for each patient is contained in the hecktor2022_clinical_info_training.csv, including center, gender, age, weight, tobacco and alcohol consumption, performance status (Zubrod), HPV status, treatment (surgery and/or chemotherapy in addition to the radiotherapy that all patients underwent).  Note that some information may be missing for some patients. 

Data were collected from 9 centers :
Hôpital général juif, Montréal, CA (HGJ)
Centre hospitalier universitaire de Sherbooke, Sherbrooke, CA (CHUS)
Hôpital Maisonneuve-Rosemont, Montréal, CA (HMR)
Centre hospitalier de l’Université de Montréal, Montréal, CA (CHUM)
Centre Hospitalier Universitaire Vaudois, CH (CHUV)
Centre Hospitalier Universitaire de Poitiers, FR (CHUP)
MD Anderson Cancer Center, Houston, Texas, USA (MDA)
UniversitätsSpital Zürich, CH (USZ)
Centre Henri Becquerel, Rouen, FR (CHB)
",1,0,0,0,1,1,1,0,pet/ct,"head and neck, h&n",0,,1,1,"chum, chuv",0,No Name,0,No Name,0,No Name,1,"https://static-content.springer.com/esm/chp%3A10.1007%2F978-3-031-43987-2_39/MediaObjects/554075_1_En_39_MOESM1_ESM.pdf

https://hecktor.grand-challenge.org/"
Mesh2SSM: From Surface Meshes to Statistical Shape Models of Anatomy,59,1,1,"We use the publicly available Decath-Pancreas dataset of 273 segmentations from patients who underwent pancreatic mass resection [24].

Info outside the paper
Images were provided by Memorial Sloan Kettering Cancer Center (New York, NY, USA) and
were previously reported in radiomic applications [24, 25, 26]. Four hundred and twenty portal venous phase CT scans were obtained with the following reconstruction and acquisition parameters: pitch/table speed 0.984–1.375/39.37–27.50 mm; automatic tube current modulation range, 220–380 mA; noise index, 12.5–14;20 kVp; tube rotation speed, 0.7–0.8 ms; scan delay, 80–85 s; and axial slices reconstructed at 2.5 mm intervals. The pancreatic parenchyma and pancreatic mass (cyst or tumour) were manually segmented in each slice by an expert abdominal radiologist using the Scout application [27].

",1,0,0,0,0,1,1,0,segmentations,pancreas,0,,0,0,No Name,0,No Name,0,No Name,0,No Name,1,https://arxiv.org/abs/1902.09063
MixUp-MIL: Novel Data Augmentation for Multiple Instance Learning and a Study on Thyroid Cancer Diagnosis,46,6,1,"Based on stateof- the-art multiple instance learning architectures and two thyroid cancer data
sets, an exhaustive study was conducted considering a range of common data augmentation strategies.
In this work, we aimed at distinguishing different nodular lesions of the thyroid, focusing especially on benign follicular nodules (FN) and papillary carcinomas (PC).
The data set utilized in the experiments consists of 80 WSIs overall. One half (40) of the data set consists of frozen and the other half (40) of paraffin sections [5]), representing the different modal
ities. All images were acquired during clinical routine at the Kardinal Schwarzenberg Hospital. Procedures were approved by the ethics committee of the county of Salzburg (No. 1088/2021). The mean and median age of patients at the date of dissection was 47 and 50 years, respectively. The data set comprised 13 male and 27 female patients, corresponding to a slight gender imbalance. They were labeled by an expert pathologist
with over 20 years experience.",1,1,1,0,1,2,0,0,"wsis, wsi",thyroid,0,,1,1,kardinal schwarzenberg hospital,0,No Name,0,No Name,1,salzburg,0,0
Multi-scale Prototypical Transformer for Whole Slide Image Classification,58,6,1,"To evaluate the effectiveness ofMSPT, we conducted experiments on two public dataset, namely Camelyon16 [24] and TCGA-NSCLC. Camelyon16 is a WSI dataset for the automated detection of metastases in lymph node tissue slides. It includes 270 training
samples and 129 testing samples. 
The TCGA-NSCLC dataset includes two sub-types of lung cancer, i.e., Lung Squamous Cell Carcinoma (TGCA-LUSC) and Lung Adenocarcinoma (TCGA-LUAD). We collected a total of 854 diagnostic slides from the National Cancer Institute Data Portal (https:// portal.gdc.cancer.gov). ",0,0,0,0,1,2,2,0,wsi,"lymp nodes, lung",0,,1,0,No Name,1,national cancer institute data portal ,0,No Name,0,No Name,0,0
Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision,55,1,1,"We train our approach at large scale with more than 50,000 computed tomography (CT) scans and validate it on two different applications: 1) Tracking of generic lesions based on the DeepLesion dataset, including liver tumors, lung nodules, enlarged lymph-nodes, for which we report highest matching accuracy of 92%, with localization accuracy that is nearly 10% higher than the state-ofthe-
art; and 2) Tracking of lung nodules based on the NLST dataset for which we achieve similarly high performance.

we (1) train a pixel-wise self-supervised system using a very large and diverse dataset of 52,487 CT volumes and (2) evaluate on two publicly available datasets.

We train the universal and fine-grained anatomical point matching model using an in-house CT dataset (VariousCT). The training dataset contains 52,487 unlabeled 3D CT volumes capturing various anatomies, including chest, head, abdomen, pelvis, and more.

For NLST, we randomly selected a subset of 1045 test images coming from 420 patients with up to 3 studies

The authors thank the National Cancer Institute for access to NCI’s data collected by the National Lung Screening Trial (NLST). The statements contained herein are solely those of the authors and do not represent or imply concurrence or endorsement by NCI.",1,0,0,0,0,3,2,1,"tomography, ct","liver, lung, lymp nodes",0,,0,0,No Name,0,No Name,0,No Name,0,No Name,0,0
MUVF-YOLOX: A Multi-modal Ultrasound Video Fusion Network for Renal Tumor Diagnosis,62,5,1,"We collect a renal tumor US dataset of 179 cases from two medical centers, which is split into the training and validation sets. We further collect 36 cases from the two medical centers mentioned above (14 benign cases) and another center (Fujian Provincial Hospital, 22 malignant cases) to form the test set. Each case has a video with simultaneous imaging of B-mode and CEUS-mode.

Note: the two medical centers mentioned above are not to be found in the content of the paper (unless they mean in the section under title name, which isnt obvious)",0,0,0,0,1,1,0,1,"us, ultrasound",renal,0,,1,1,fujian provincial hospital,0,No Name,0,No Name,0,No Name,0,0
Overall Survival Time Prediction of Glioblastoma on Preoperative MRI Using Lesion Network Mapping,29,8,1,"GSP1000 Processed Connectome. It publicly released preprocessed restingstate fMRI data of 1000 healthy right-handed subjects with an average age 21.5±2.9 years and approximately equal numbers of males and females from the Brain Genomics Superstruct Project (GSP) [5], where the concrete image acquisition parameters and preprocessing procedures can be found as well. Specifically, a slightly modified version of Yeo’s Computational Brain Imaging Group (CBIG) fMRI preprocessing pipeline (https://github.com/bchcohenlab/CBIG) was employed to obtain either one or two preprocessed resting-state fMRI runs of each subject that had 120 time points per run and were spatially normalized
into the MNI152 template with 2mm3 voxel size. We downloaded and used the first-run preprocessed resting-state fMRI of each subject for the following analysis.

BraTS 2020. It provided an open-access pre-operative imaging training dataset to segment brain tumors of glioblastoma (GBM, belonging to high grade glioma) and low grade glioma (LGG) patients, as well as to predict overall survival time of GBM patients [18]. This training dataset contained 133 LGG and 236 GBM patients, and each patient had four MRI modalities, including T1, post-contrast T1-weighted, T2-weighted, and T2 Fluid Attenuated Inversion Recovery",1,1,1,0,0,2,2,0,"fmri, mri, t1, t2",brain,0,,0,0,No Name,0,No Name,0,No Name,0,No Name,0,0
Pre-trained Diffusion Models for Plug-and-Play Medical Image Enhancement,1,3,1,"Experiments on low-dose CT and heart MR datasets.

We conducted experiments on two common image enhancement tasks: denoising and SR. To mimic the real-world setting, the diffusion models weretrained on a diverse dataset, including images from different centers and scanners.
The testing set (e.g., MR images) is from a new medical center that has not appeared in the training set. Experiments show that our model can generalize to these unseen images. Specifically, the denoising task is based on the AAPM Low Dose CT Grand Challenge abdominal dataset [19], which can be also used for SR [33]. The heart MR SR task is based on three datasets: ACDC [1], M&Ms1-2 [3], and CMRxMotion [27]. Notably, the presented framework eliminates the requirement of paired data. For the CT image enhancement task, we trained a diffusion model [21] based on the full-dose dataset that contains 5351 images, and the hold-out quarter-dose images were used for testing. For the MR enhancement task, we used the whole ACDC [1] and M&Ms1-2 [3] for training the diffusion model and the CMRxMotion [27] dataset for testing. The testing images were downsampled by operator H with factors of 4× and 8× to produce low-resolution images, and the original images served as the ground truth.

testing dataset, aapm low dose ct grand challenge abdominal dataset, acdc, m&ms1-2, cmrxmotion, full-dose dataset

We also thank the organizers of AAPM Low Dose CT Grand Challenge [20], ACDC [1], M&Ms1-2 [3], and CMRxMothion [27] for making the datasets publicly available.",0,0,0,0,1,6,4,0,"mr, ct","abdomen, heart",0,,1,1,No Name,0,No Name,0,No Name,0,No Name,0,0
Prompt-Based Grouping Transformer for Nucleus Detection and Classification,55,8,1,"CoNSeP1 [10] is a colorectal nuclear dataset with three types, consisting of 41H&E stained image tiles from 16 colorectal adenocarcinoma whole-slide images (WSIs). 
BRCA-M2C2 [1] is a breast cancer dataset with three types and consists of 120 image tiles from 113 patients. 
Lizard3 [9] has 291 histology images of colon tissue from six datasets, containing nearly half a million labeled nuclei in H&E stained colon tissue. ",1,0,0,0,0,3,3,0,"h&e, wsis, wsi, image titles, stained image tiles, histology images,","colorectal, breast, colon  ",0,,0,0,No Name,0,No Name,0,No Name,0,No Name,0,"https://warwick.ac.uk/fac/cross_fac/tia/data/hovernet/.
 https://github.com/TopoXLab/Dataset-BRCA-M2C/.
 https://warwick.ac.uk/fac/cross_fac/tia/data/lizard/."
Regularized Kelvinlet Functions to Model Linear Elasticity for Image-to-Physical Registration of the Breast,33,9,1,"This dataset consists of supine breast MR images simulating surgical deformations of 11 breasts from 7 healthy volunteers. Volunteers (ages 23–57) were enrolled in a study approved by the Institutional Review Board at Vanderbilt University.

This dataset consists of supine breastMR images simulating surgical deformations from one breast cancer patient. A 71-year-old patient with invasive mammary carcinoma in the left breast was enrolled in a study approved by the Institutional Review Board at Vanderbilt University.",1,1,0,0,1,2,0,0,mr,breast,0,,1,0,No Name,0,No Name,0,No Name,0,No Name,0,0
Relaxation-Diffusion Spectrum Imaging for Probing Tissue Microarchitecture,15,8,1,"We evaluate RDSI using both ex vivo monkey and in vivo human brain MTE data.
We used an ex vivo monkey dMRI dataset[2] collected with a 7T MRI scanner [19].
One healthy subject and three patients with gliomas were scanned using a Philips Ingenia CX 3T MRI scanner with a gradient strength of 80mT/m and switching rates of 200 mT/m/ms.

Info outside paper
Data was collected from an ex vivo fixed Vervet (Chlorocebus aethiops) monkey brain, obtained from the Montreal Monkey Brain Bank. The monkey, cared for on the island of St. Kitts, had been treated in line with a protocol approved by The Caribbean Primate Center of St. Kitts. The brain had previously been stored and prepared according to Dyrby et al. [1]. The data was collected with a Bruker Biospec 70/20 7.0 T scanner (Billerica, Massachusetts, USA) using a quadrature RF coil (300 MHz). The brain was let to reach room temperature and to mechanically stabilize prior the start of the acquisition.",1,0,0,0,0,2,0,0,"mte, dmri, mri","human brain, monkey brain",0,,0,0,No Name,0,No Name,0,No Name,0,No Name,1,https://resources.drcmr.dk/MAPdata/axon-relaxation/README.txt
Reversing the Abnormal: Pseudo-Healthy Generative Networks for Anomaly Detection,29,5,1,"We trained our model using two publicly available brain T1w MRI datasets, including FastMRI+ (131 train, 15 val, 30 test) and IXI (581 train samples), to capture the healthy distribution. Performance evaluation was done on a large stroke T1-weighted MRI dataset, ATLAS v2.0 [14], containing 655 images",0,0,0,0,0,2,2,0,"mri, t1",brain,0,,0,0,No Name,0,No Name,0,No Name,0,No Name,0,0
Robust Exclusive Adaptive Sparse Feature Selection for Biomarker Discovery and Early Diagnosis of Neuropsychiatric Systemic Lupus Erythematosus,13,5,1*,"*Inmetabolite analysis, some studies have shown that the decrease in NAA concentration is related to chronic inflammation, damage, and tumors in the brain [18].

Dataset and Preprocessing: The T2-weighted MR images of 39 participants including 23 patients with NPSLE and 16 HCs were gathered from our affiliated hospital. All images were acquired at an average age of 30.6 years on a SIGNA 3.0T scanner with an eight-channel standard head coil.

",1,1,0,0,1,1,0,0,"mr, t2",no organ mentioned,0,,1,1,No Name,0,No Name,0,No Name,0,No Name,0,0
SHISRCNet: Super-Resolution and Classification Network for Low-Resolution Breast Cancer Histopathology Image,3,5,1,"* The paper does not say whether the dataset is public,  they just referenced to it

This work uses the breast cancer histopathological image database (BreaKHis)1 [20]. This dataset includes four distinct histological types of benign breast tumors: adenosis (A), fibroadenoma (F), phyllodes tumor (PT), and tubular adenona (TA); and four malignant tumors (breast cancer): carcinoma (DC), lobular carcinoma (LC), mucinous carcinoma (MC) and papillary carcinoma (PC). The original dataset is randomly divided into training set and testing set for each magnification at a ratio of 7:3 following previous work

Info outside paper
BreakHis
The BreaKHis database contains microscopic biopsy images of benign and malignant breast tumors. Images were collected through a clinical study from January 2014 to December 2014.
All patients referred to the P&D Laboratory, Brazil, during this period of time, with a clinical indication of BC were invited to participate in the study. The institutional review board approved the study and all patients  gave written informed consent. All the data were anonymized. Samples are generated from breast tissue biopsy slides, stained with hematoxylin and eosin (HE). The samples are collected by SOB, prepared for histological study, and labeled by pathologists of the P&D Lab.
",0,0,0,0,0,1,0,0,histopathology,breast,0,,0,0,No Name,0,No Name,0,No Name,0,No Name,1,https://www.inf.ufpr.br/lesoliveira/download/TBME-00608-2015-R2-preprint.pdf
SLPD: Slide-Level Prototypical Distillation for WSIs,25,1,1,"We conduct experiments on two public WSI datasets2. TCGANSCLC dataset includes two subtypes in lung cancer, Lung Squamous Cell Carcinoma and Lung Adenocarcinoma, with a total of 1,054 WSIs. TCGA-BRCA dataset includes two subtypes in breast cancer, Invasive Ductal and Invasive
Lobular Carcinoma, with a total of 1,134 WSIs",0,0,0,0,0,2,2,0,wsi,"lung, breast",0,,0,0,No Name,0,No Name,0,No Name,0,No Name,0,0
Spectral Adversarial MixUp for Few-Shot Unsupervised Domain Adaptation,69,1,1,"We evaluated SAMix on two medical image datasets. Fundus [5,14] is an optic disc and cup segmentation task. Following [21], we consider images collected from different scanners as distinct domains. The source domain contains 400 images of the REFUGE [14] training set.We took 400 images from the REFUGE validation set and 159 images of RIM-One [5] to form the target domain 1 & 2. We center crop and resize the disc region to 256 × 256 as network input. Camelyon [1] is a tumor tissue binary classification task across 5 hospitals. We use the training set of Camelyon as the source domain (302, 436 images from hospitals 1 − 3) and consider the validation set (34, 904 images from hospital 4) and test set (85, 054 images from the hospital 5) as the target domains 1 and 2, respectively. 
",0,0,0,0,1,4,0,0,segmentations,tissue,0,,1,0,No Name,0,No Name,0,No Name,0,No Name,0,0
Structure-Preserving Synthesis: MaskGAN for Unpaired MR-CT Translation,6,10,1,"We collected 270 volumetric T1-weighted MRI and 267 thinslice CT head scans with bony reconstruction performed in pediatric patients under routine scanning protocols1. We targeted the age group from 6–24 months since pediatric patients are more susceptible to ionizing radiation and experience a greater cancer risk (up to 24% increase) from radiation exposure [7].

The dataset comprises brain MR and CT volumes from 262 subjects. 13 MRICT volumes from the same patients that were captured less than three months apart are registered using rigid registration algorithms. The dataset is divided into 249, 1 and 12 subjects for training, validating and testing se",1,1,0,0,0,1,0,0,"mri, t1, ct, mr, ct volumes",brain,0,,0,0,No Name,0,No Name,0,No Name,0,No Name,0,0
Synthesis of Contrast-Enhanced Breast MRI Using T1- and Multi-b-Value DWI-Based Hierarchical Fusion Network with Attention Mechanism,8,7,1,"This study was approved by Institutional Review Board of our cancer institute with a waiver of informed consent.We retrospectively collected 765 patients with breast cancer presenting at our cancer institute from January 2015 to November 2020, all patients had biopsy-proven breast cancers (all cancers included in this study were invasive breast cancers, and ductal carcinoma in situ had been excluded). The MRIs were acquired with Philips Ingenia 3.0-T scanners, and overall, three sequences were present in the in-house dataset, including T1- weighted fat-suppressed MRI, contrast enhanced T1-weighted MRI and DWI

Based on the ratio of 8:2, the training set and independent test set of the in-house dataset have 612 and 153 cases, respectively.",1,0,0,0,1,1,0,1,"mri, t1, dwi",breast,0,,1,0,No Name,1,No Name,0,No Name,0,No Name,0,0
Thyroid Nodule Diagnosis in Dynamic Contrast-Enhanced Ultrasound via Microvessel Infiltration Awareness,17,6,1,"Our dataset contained 282 consecutive patients who underwent thyroid nodule examination at Nanjing Drum Tower Hospital. All patients performed dynamic CEUS examination by an experienced sonographer using an iU22 scanner (Philips Healthcare, Bothell, WA) equipped with a linear transducer
L9-3 probe. These 282 cases included 147 malignant nodules and 135 benign nodules. On the one hand, the percutaneous biopsy based pathological examination was implemented to determine the ground-truth of malignant and benign. On the other hand, a sonographer with more than 10 years of experience manually annotated the nodule lesion mask to obtain the pixel-level groundtruth
of thyroid nodules segmentation. All data were approved by the Institutional Review Board of Nanjing Drum Tower Hospital, and all patients signed the informed consent before enrollment into the study.",1,0,0,0,1,1,0,0,"ultrasound, ceus",thyroid,0,,1,1,nanjing drum tower hospital,0,No Name,0,No Name,0,No Name,0,0
Towards Multi-modal Anatomical Landmark Detection for Ultrasound-Guided Brain Tumor Resection with Contrastive Learning,64,9,1,"We employed the publicly available EASY-RESECT (REtroSpective Evaluation of Cerebral Tumors) dataset [10 (https://archive.sigma2.no/pages/ public/dataset Detail.jsf?id 10.11582/2020.00025) to train and evaluate our proposed method. This dataset is a deep-learning-ready version of the original RESECT database, and was released as part of the 2020 Learn2Reg Challenge [24]. Specifically, EASY-RESECT contains MRI and intra-operative US scans (before resection) of 22 subjects who have undergone low grade glioma resection surgeries. 

To train our DL model, we made subject-wise division of the entire dataset into 70%:15%:15% as the training, validation, and testing sets, respectively.

Table 1 lists the mean and standard deviation of landmark identification errors (in mm) between the predicted position and the ground truth in intra-operative US for each patient of the RESECT dataset.",1,0,0,0,0,1,1,0,"mri, intra-operative us, us, ultrasound","cerebral, brain ",0,,0,0,No Name,0,No Name,0,No Name,0,No Name,0,0
Trackerless Volume Reconstruction from Intraoperative Ultrasound Images,29,10,1,"To validate our method, six tracked sequences were acquired from an ex vivo swine liver. A manually manipulated IVUS catheter was used (8 Fr lateral firing AcuNavTM 4–10 MHz) connected to an ultrasound system (ACUSON S3000 HELX Touch, Siemens Healthineers, Germany), both commercially available. An electromagnetic tracking system (trakSTARTM, NDI, Canada) was used along with a 6 DoF sensor (Model 130) embedded close to the tip of the catheter, and the PLUS toolkit [17] along with 3D Slicer [18] were used to record the sequences.",0,0,0,0,0,1,0,0,"intra-operative us, ultrasound, intraoperative ultrasound",swine liver,0,,0,0,No Name,0,No Name,0,No Name,0,No Name,0,0
TransLiver: A Hybrid Transformer Model for Multi-phase Liver Lesion Classification,32,2,1,"Extensive experiments are conducted, in which we achieve an overall accuracy of 90.9% on an in-house dataset of four CT phases and seven liver lesion classes. 
The employed single-phase annotated dataset is collected from Sir Run Run Shaw Hospital (SRRSH), affiliated with the Zhejiang University School of Medicine, and has received the ethics approval of IRB. The collection process can be found in supplementary materials

After the pre-processing unit with window Dice threshold of 0.3, we screen 761 lesions from 444 patients with four phases of CTs, seven types of lesions (13.2% of HCC, 5.3% of HM, 11.3% of ICC, 22.6%of HH, 31.1% of HC, 8.7% of FNH, and 7.8% of HA), and totally 4820 slices. To handle the imbalance of dataset, we randomly select 586 lesions as the training and validation set with no more than 700 axial slices in each lesion type, and the rest 175 lesions constitute the test set. Lesions from the same patient are either assigned to the training and validation set or the test set, but not both.",1,0,0,0,1,1,0,1,ct,liver,0,,1,1,"sir run run shaw hospital, zhejiang university school of medicine",1, zhejiang university school of medicine,0,No Name,0,No Name,0,0
Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,77,6,1,"The cohort employed in this study was composed of pre-treatment tumor biopsy specimens from patients with NSCLC from five centers (two centers for training (St) and three centers for independent validation (Sv)). The entire analysis was carried out using 122 patients in Experiment 1 (73 in St, and 49 in Sv) and 135 patients in Experiment 2 (81 in St, and 54 in Sv). Specimens were analyzed with a multiplexed quantitative immunofluorescence (QIF) panel using the method described in [22]. From each whole slide image, 7 representative tiles were obtained and used to train the software InForm to define background, tumor and stromal compartments. Then, individual cells were segmented based on nuclear DAPI staining and the segmentation performance was controlled by direct visualization of samples by a trained observer. Next, the software was trained to identify cell subtypes based on marker expression (CD8, CD4, CD20, CK for tumor epithelial cells and absence of these markers for stromal cells).

NSCLC: non-small cell lung cancer",1,0,0,0,1,5,0,5,wsi,lung,0,,1,0,No Name,1,No Name,0,No Name,0,No Name,0,0
Uncertainty-Informed Mutual Learning for Joint Medical Image Classification and Segmentation,4,4,1,"The experiments on the public datasets demonstrate that our UML outperforms existing methods in terms of both accuracy and robustness.

We evaluate the our UML network on two datasets REFUGE [14] and ISPY-1 [13]. REFUGE contains two tasks, classification of glaucoma and segmentation of optic disc/cup in fundus images. The overall 1200 images were equally divided for training, validation, and testing.
 The tasks of ISPY-1 are the pCR prediction and the breast tumor segmentation. A total of 157 patients who suffer the breast cancer are considered - 43 achieve pCR and 114 non-pCR. 

Info outside the paper
REFUGE dataset
The REFUGE challenge database consists of 1200 retinal CFPs stored in JPEG format, with 8 bits per color channel, acquired by ophthalmologists or technicians from patients sitting upright and using one of two devices- These pictures correspond to Chinese patients (52% and 55% female in offline and online test sets, respectively) visiting eye clinics, and were retrieved retrospectively from multiple sources, including several hospitals and clinical studies. Only high-quality images were selected to ensure a proper labelling, and any personal and/or device information was removed for anonymization.

ISPY-1 dataset
Clinical and MRI data from the ISPY1 clinical trial of patients with breast cancer. 
Clinical Data as a XLS file with the following fields:

Age (Years)
Race, encoded as:
1 = Caucasian
3 = African American
4 = Asian
5 = Native Hawaiian
6 = American Indian
50 = Multiple race",1,0,0,0,0,2,2,0,"fundus, optical coherence tomography, oct,  segmentation","breast, eye",0,,0,0,No Name,0,No Name,0,No Name,0,No Name,0,0
Unpaired Cross-Modal Interaction Learning for COVID-19 Segmentation on Limited CT Images,58,3,1,"*note: paper refs to wrong dataset (look further down)
** 4 datasets, since one dataset is collected from two public datasets
We used the public COVID-19 segmentation benchmark [15] to verify the proposed UCI. It is collected from two public resources [5,8] on chest CT images available on The Cancer Imaging Archive (TCIA) [4]. All CT images were acquired without intravenous contrast enhancement from patients with positive Reverse Transcription Polymerase Chain Reaction (RT-PCR) for SARSCoV-2. In total, we used 199 CT images including 149 training images and 50 test images. We also used two chest x-ray-based classification datasets including
ChestX-ray14 [18] and ChestXR [1] to assist the UCI training. The ChestXray14 dataset comprises 112,120 X-ray images showing positive cases from 30,805 patients, encompassing 14 disease image labels pertaining to thoracic and lung ailments. An image may contain multiple or no labels. The ChestXR dataset consists of 21,390 samples, with each sample classified as healthy, pneumonia, or COVID-19.

Info outside the paper
Two public resources[5,8]
Cinical data: age, sex, race, zip, weight.
The average age in the cohort was 54.3 years old (range 19–91) with an even sex distribution (52 Male, 53 Female). The worldwide incidence is reported to be close to 1:1, with a 50% increase in hospitalizations, ICU stays, and mortality among males10. The racial characteristics of the cohort are presented in Fig. 1 in comparison to the total population of Arkansas and the current characteristics of the state-wide infected population. The average BMI in the cohort is 33.1 (18.7–64.9), well within obese range (30.0 or higher). Key Comorbidities include burns (2%), malnutrition (3%), pregnancy (4%), chronic kidney disease (11%), diabetes (21%), organ transplant (3%), and cancer (24%).
The overall ICU admission rate was 28% (29/105). The Average age among those admitted to the ICU was 58 (range 25–89), slightly higher than the average for the cohort as a whole. The racial breakdown of ICU admissions included 28% of the white patients, 25% of the black, 50% of other, and 100% of Pacific islanders. The ICU population was 66% male and 33% female and included 1 pregnant patient. Forty three percent of patients admitted to the ICU had BMI greater than 30. Of the black patients, 21% had diabetes and 29% chronic renal disease, while among white patients the highest percentage comorbidities were cancer (10%) and diabetes (10%). The ICU mortality rate was 34.4% (10/29) which is 1.5 times the national average of 23.6%11. The overall mortality rate was 10% (10/105) and all 10 patients in the mortality group were first admitted to ICU. Arkansan males were 1.95 times more likely to go to ICU (19/52 vs. 10/53). Our data shows an almost even overall mortality rate among males (5/53) and females (5/52). The data also suggest that once in the ICU, female mortality occurs at a rate 1.9 times that of males (5/19 vs. 5/10). 
COVID-19-AR image collection: Image data were extracted from the clinical PACS (Sectra AB, Linkoping, Sweden) at the University of Arkansas for Medical Sciences using Digital Imaging and Communications in Medicine (DICOM) query/retrieve software. All image data were de-identified and stored in DICOM standard format17 on TCIA as collection COVID-19-AR18. All clinical data were obtained from the Arkansas Clinical Data Repository (AR-CDR)19. The AR-CDR is a research data warehouse that provides a single and secure source of data for use in clinical and translational research; housing data for this project that are extracted from the EPIC (Epic Systems Inc, Verona, WI) electronic health record (EHR) system and legacy systems.

https://www.nature.com/articles/s41467-020-17971-2/tables/1
Patient cohorts in model development and testing: demographic values, center/location, age, gender, demographic distribution pr training, validation and testing 

TCIA
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3824915/
ChestX-ray14 and ChestXR
ChestX-ray14 refs to Chestx-ray8 in the paper and not ray14. Ray8 contains (taken from ref): In this paper, we present a new chest X-ray database, namely “ChestX-ray8”, which comprises 108,948 frontalview X-ray images of 32,717 unique patients with the textmined eight disease image labels (where each image can have multi-labels), from the associated radiological reports
using natural language processing

The link to ray 14 describes the dataset as: ChestX-ray14 is a medical imaging dataset which comprises 112,120 frontal-view X-ray images of 30,805 (collected from the year of 1992 to 2015) unique patients with the text-mined fourteen common disease labels, mined from the text radiological reports via NLP techniques. It expands on ChestX-ray8 by adding six additional thorax diseases: Edema, Emphysema, Fibrosis, Pleural Thickening and Hernia.
ChestXR ",1,0,0,0,0,4,2,0,"ct, x-rays, segmentations",chest,0,,0,0,No Name,0,No Name,0,No Name,0,No Name,0,0
X2Vision: 3D CT Reconstruction from Biplanar X-Rays with Deep Structure Prior,66,10,1,"Manifold Learning. We trained our model with a large dataset of 3500 CTs of
patients with head-and-neck cancer, more exactly 2297 patients from the publicly
available The Cancer Imaging Archive (TCIA) [1,6,16,17,28,32] and 1203 from
private internal data, after obtention of ethical approbations. We split this data
into 3000 cases for training, 250 for validation, and 250 for testing. We focused
CT scans on the head and neck region above shoulders, with a resolution of
80×96×112, and centered on the mouth after automatic segmentation using a
pre-trained U-Net [22]. The CTs were preprocessed by min-max normalization
after clipping between -1024 and 2000 Hounsfield Units (HU).

3D Reconstruction. To evaluate our approach, we used an external private cohort
of 80 patients who had undergone radiotherapy for head-and-neck cancer, with
their consent. Planning CT scans were obtained for dose preparation, and CBCT
scans were obtained at each treatment fraction for positioning with full gantry
acquisition. As can be seen in Fig. 3 and the supplementary material, all these
cases are challenging as there are large changes between the original CT scan
and the CBCT scans. We identified these cases automatically by comparing the
CBCTs with the planning CTs",1,0,0,0,0,3,1,2,"ct, mri, cbct","head, neck, brain ",0,,0,0,No Name,0,No Name,0,No Name,0,No Name,0,0
CLIP-Lung: Textual Knowledge-Guided Lung Nodule Malignancy Prediction,38,7,1,"Dataset. LIDC-IDRI [1] is a dataset for pulmonary nodule classification or detection
based on low-dose CT, which involves 1,010 patients. According to the annotations,
we extracted 2, 026 nodules, and all of them were labeled with scores from
1 to 5, indicating the malignancy progression.  
In this paper,we construct three sub-datasets: LIDC-A contains three classes of nodules both in training and test sets; according to [11], we construct the LIDC-B,whichcontains three classes ofnodules only in the training set, andthe test set contains benign and malignant nodules;LIDC-C includes benign and malignant
nodules both in training and test sets.",1,0,0,0,0,3,0,0,ct,lung,0,0,0,0,0,0,0,0,0,0,0,0,0
Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans,15,5,1,"Dataset and Ground Truth. Our study analyzed a dataset of CT scans collected
from Guangdong Province People’s Hospital between years 2018 and 2020, with 2,139 patients consisting of 787 gastric cancer and 1,352 normal cases. We used the latest patients in the second half of 2020 as a hold-out test set, resulting in a training set of 687 gastric cancer and 1,204 normal cases, and a test set of 100 gastric cancer and 148 normal cases. We randomly selected 20% of the training data as an internal validation set. To further evaluate specificity in a larger population, we collected an external test set of 903 normal cases from Shengjing Hospital. Cancer cases were confirmed through endoscopy (and pathology) reports, while normal cases were confirmed by radiology reports and a two-year follow-up. All patients underwent multi-phase CTs ",1,0,0,0,1,1,0,0,ct,stomach,0,0,1,1,"Guangdong Province People’s Hospital,  Shengjing Hospital.",0,0,0,0,0,0,0,0
Co-learning Semantic-Aware Unsupervised Segmentation for Pathological Image Registration,51,10,1,"Experimental results on multiple datasets, including Magnetic Resonance Imaging (MRI) of T1 sequences, demonstrate the efficacy of our proposed method.
For our study, we selected the ICBM 152 Nonlinear Symmetric template as our atlas [9]. We reoriented all MRI scans of the T1 sequence to the RAS orientation with a resolution of 1mm×1mm×1mm and align the images to atlas using the mri robust register tool in FreeSurfer [19]. We then cropped the resulting MRI scans to a size of 160×192×144, without any image augmentation. To evaluate our approach, we employed a 5-fold cross-validation method and divided our data into training and test sets in an 8:2 ratio.

3D Brain MRI. 
OASIS-1 [12] includes 416 cross-sectional MRI scans from individuals aged 18 to 96, with 100 of them diagnosed with mild to moderate Alzheimer’s disease. BraTS2020 [13] provides 369 expert-labeled pre-operative MRI scans of glioblastomas and low-grade gliomas, acquired from multiple institutions for routine clinical use.

3D Pseudo Brain MRI. 
To evaluate the performance of atlas-based registration, it is essential to have the correct mapping of pathological regions to healthy brain regions. To create such a mapping, we created a pseudo dataset by utilizing images from the OASIS-1 and BraTS2020. From the resulting t1 sequences, a pseudo dataset of 300 images was randomly selected for further analysis. Appendix B provides a detailed process for creating the pseudo dataset.

Real Data with Landmarks. 
BraTS-Reg 2022 [2] provides extensive annotations of landmarks points within both the pre-operative and the follow-up scans that have been generated by clinical experts. A total of 140 images are provided, of which 112 are for training, and 28 for testing.",0,1,0,0,0,3,0,0,mri,brain,0,0,0,0,0,0,0,0,0,0,0,0,0
Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,62,6,1,"The model achieved an average testing AUROC of 86.53% on a large curated dataset with over 1.1 million stroma patches.

Our experimental results indicate that stromal alterations are detectable in the presence of prostate cancer and highlight the potential for tumor-associated stroma
to serve as a diagnostic biomarker in prostate cancer.

In our study, we utilized three datasets for tumor-associated stroma analysis.
(1) Dataset A comprises 513 tiles extracted from the whole mount slides of 40
patients, sourced from the archives of the Pathology Department at Cedars-
Sinai Medical Center (IRB# Pro00029960). It combines two sets of tiles: 224
images from 20 patients featuring stroma, normal glands, low-grade and highgrade
cancer [22], along with 289 images from 20 patients with dense high-grade
cancer (Gleason grades 4 and 5) and cribriform/non-cribriform glands [23]. Each
tile measures 1200×1200 pixels and is extracted from whole slide images captured
at 20x magnification (0.5 microns per pixel). The tiles were annotated
at the pixel-level by expert pathologists to generate stroma tissue segmentation
masks and were cross-evaluated and normalized to account for stain variability.
(2) Dataset B included 97 whole mount slides with an average size of over
174,000×142,000 pixels at 40x magnification. The prostate tissue within these
slides had an average tumor area proportion of 9%, with an average tumor area of
77 square mm. An expert pathologist annotated the tumor region boundaries at
the region-level, providing exhaustive annotations for all tumor foci. (3) Dataset
C comprised 6134 negative biopsy slides obtained from 262 patients’ biopsy procedures, where all samples were diagnosed as negative. These slides are presumed to contain predominantly normal stroma tissues without phenotypic alterations in response to cancer.",1,0,0,0,1,3,0,0, whole mount slides,"stroma, prostate",1,male,1,0,0,1,"Pathology Department at Cedars-
Sinai Medical Center",0,0,0,0,0,0
DiffDP: Radiotherapy Dose Prediction via a Diffusion Model,19,6,1,"Extensive experiments on an in-house dataset with 130 rectum
cancer patients demonstrate the superiority of our method.

The proposed DiffDPis extensively evaluated on a clinical dataset
consisting of 130 rectum cancer patients, and the results demonstrate that our approach outperforms other state-of-the-art methods.

Dataset and Evaluations. We measure the performance of our model on an in-house
rectum cancer dataset which contains 130 patients who underwent volumetric modulated arc therapy (VMAT) treatment at West China Hospital. Concretely, for every patient, the CT images, PTV segmentation, OARs segmentations, and the clinically planned dose distribution are included. Additionally, there are four OARs of rectum cancer containing the bladder, femoral head R, femoral head L, and small intestine.We randomly select 98 patients for model training, 10 patients for validation, and the remaining 22 patients for test. The thickness of the CTs is 3 mm and all the images are resized to the resolutionof 256 × 256 before the training procedure. We measure the performance of our proposed",1,0,0,0,1,1,0,1,ct,"rectum, bladder, femoral head, small intestine",0,0,1,1,West China Hospital,0,0,0,0,0,0,0,0
DULDA: Dual-Domain Unsupervised Learned Descent Algorithm for PET Image Reconstruction,15,10,1,"Forty 128×128×40 3D Zubal brain phantoms [24] were used in the simulation
study as ground truth, and one clinical patient brain images with different
dose level were used for the robust analysis. Two tumors with different size
were added in each Zubal brain phantom. Among them, 1320 (33 samples) were used in training, 200 (5 samples) for testing, and 80 (2 samples) for validation. A total of 5 realizations were simulated and each was trained/tested independently for bias and variance calculation[15]. We used peak signal to noise ratio (PSNR), structural similarityindex (SSIM) and root mean square error (RMSE) for overall quantitative analysis.The contrast recovery coefficient (CRC) [25] was used for the comparison of reconstruction results in the tumor region of interest (ROI) area.",1,0,0,0,0,not clear,0,0,pet,brain,0,0,0,0,0,0,0,0,0,0,0,0,0
Edge-Aware Multi-task Network for Integrating Quantification Segmentation and Uncertainty Prediction of Liver Tumor on Multi-modality Non-contrast MRI,62,4,1,"An axial dataset includes 250 distinct subjects, each underwent initial standard clinical liver MRI protocol examinations with corresponding pre-contrast images (T2FS [4mm]) and DWI [4mm]) was collected.
The ground truth was reviewed by two abdominal radiologists with 10
and 22 years of experience in liver imaging, respectively. If any interpretations
demonstrated discrepancies between the reviewers, they would re-evaluate the
examinations together and reach a consensus. To align the paired images of T2
and DWI produced at different times. We set the T2 as the target image and
the DWI as the source image to perform the pre-processing of non-rigid registration
between T2 and DWI by using the Demons non-rigid registration method.
It has been widely used in the field of medical image registration since it was
proposed by Thirion [21]. ",0,0,0,0,0,1,0,0,mri,liver,0,0,0,0,0,0,0,0,0,0,0,0,0
Explaining Massive-Training Artificial Neural Networks in Medical Image Analysis Task Through Visualizing Functions Within the Models,68,2,1,"Our XAI technique was applied to explain the MTANN model’s decision in a liver
tumor segmentation task [20]. Dynamic contrast-enhanced liver CT scans consisting of 42 patients with 194 liver tumors in the portal venous phase from the LiTS database [21] were used in this study. Each slice of the CT volumes in the dataset has a matrix size of 512 × 512 pixels, with in-plane pixel sizes of 0.60–1.00 mm and thicknesses of 0.20–0.70 mm. The dataset consists of the original hepatic CT image with the liver mask and the “gold-standard” liver tumor region manually segmented by a radiologist, as illustrated in Fig. 2.",1,0,0,0,0,1,0,0,ct,"hepatic, liver",0,0,0,0,0,0,0,0,0,0,0,0,0
Faithful Synthesis of Low-Dose Contrast-Enhanced Brain MRI Scans Using Noise-Preserving Conditional GANs,58,2,1,"we collected 453 CE scans with non-standard GBCA doses in the set of {10%, 20%, 33%} along with the corresponding standard-dose (0.1 mmol/kg) scan after applying the remaining contrast agent. Using this dataset, we aim at the semantic interpolation of the GBCA signal at various fractional dose levels. To this end, we use GANs to
learn the contrast enhancement behavior from the dataset collective and thereby
enable the synthesis of contrast signals at various dose levels for individual cases.
Further, to minimize the smoothing effect [19] of typical content losses (e.g.,  1 or
perceptual [16]), we develop a noise-preserving content loss function based on the
Wasserstein distance between paired image patches calculated using a Sinkhornstyle algorithm. This novel loss enables a faithful generation of noise, which is important for the identification of enhancing pathologies and their usability as
additional training data.

To simplify learning of the contrast accumulation behavior, we adapt the preprocessing pipeline of BraTS [6]. Further details of the dataset and the preprocessing are in the supplementary material.

Further details of the dataset, model and training can be found in the supplementary.",0,0,0,0,0,2,0,0,"ce, mri",brain,0,0,0,0,0,0,0,0,0,0,0,1,0
Gall Bladder Cancer Detection from US Images with only Image Level Labels,20,1,1,"Gallbladder Cancer Detection in Ultrasound Images: We use the public
GBC US dataset [3] consisting of 1255 image samples from 218 patients.
The dataset contains 990 non-malignant (171 patients) and 265 malignant (47
patients) GB images (see Fig. 2 for some sample images). The dataset contains
image labels as well as bounding box annotations showing the malignant regions.
Note that, we use only the image labels for training. We report results on 5-fold
cross-validation. We did the cross-validation splits at the patient level, and all
images of any patient appeared either in the train or validation split.

Polyp Detection in Colonoscopy Images: We use the publicly available
Kvasir-SEG [17] dataset consisting of 1000 white light colonoscopy images showing
polyps (see Fig. 2). Since Kvasir-SEG does not contain any control images,
we add 600 non-polyp images randomly sampled from the PolypGen [1] dataset.
Since the patient information is not available with the data, we use random
stratified splitting for 5-fold cross-validation.",1,0,0,0,0,3,2,0,"us, colonoscopy",gallbladder,0,0,0,0,0,0,0,0,0,0,0,0,0
Improved Prognostic Prediction of Pancreatic Cancer Using Multi-phase CT by Integrating Neural Distance and Texture-Aware Transformer,24,5,1,"We extensively evaluated and compared the proposed method with existing methods in the multicenter (n=4) dataset with 1,070 patients with PDAC,

In this study, we used data from Shengjing Hospital to train our method with 892 patients, and data from three other centers, including Guangdong Provincial People’s Hospital, Tianjin Medical University and Sun Yatsen University Cancer Center for independent testing with 178 patients. The contrast-enhanced CT protocol included non-contrast, pancreatic, and portal venous phases. PDAC masks for 340 patients were manually labeled by a radiologist from Shengjing Hospital with 18 years of experience in pancreatic cancer, while the rest were predicted using self-learning models [11,24] and checked by the same annotator. Other vessel masks were generated using the same semisupervised segmentation models. C-index was used as our primary evaluation metric for survival prediction. We also reported the survival AUC, which estimates the cumulative area under the ROC curve for the first 36 months.",1,0,0,0,1,4,0,0,ct,pancreas,0,0,1,1,"Shengjing Hospita,  Guangdong Provincial People’s Hospital, ",1,Tianjin Medical University and Sun Yatsen University Cancer Center,0,0,0,0,0,0
Infusing Physically Inspired Known Operators in Deep Models of Ultrasound Elastography,45,1,1,"We use publicly available data collected from a breast phantom (Model 059,
CIRS: Tissue Simulation & Phantom Technology, Norfolk, VA) using an Alpinion
E-Cube R12 research US machine (Bothell, WA, USA). The center frequency
was 8MHz and the sampling frequency was 40MHz. The Young’s modulus
of the experimental phantom was 20 kPa and contains several inclusions
with Young’s modulus of higher than 40 kPa. This data is available online at
http://code.sonography.ai in [16].
In vivo data was collected at Johns Hopkins hospital from patients with liver
cancer during open-surgical RF thermal ablation by a research Antares Siemens
system using a VF 10-5 linear array with the sampling frequency of 40MHz and
the center frequency of 6.67 MHz. The institutional review board approved the
study with the consent of the patients. We selected",1,0,0,0,1,2,1,0,"Ultrasound Elastography, us","liver, breast",0,0,1,1,john hopkins hospital,0,0,0,0,0,0,0,0
Interpretable Medical Image Classification Using Prototype Learning and Privileged Information,42,3,1,"Evaluating the proposed solution on the LIDC-IDRI dataset shows
that it combines increased interpretability with above state-of-the-art
prediction performance.

The proposed approach is evaluated using the publicly available LIDCIDRI
dataset consisting of 1018 clinical thoracic CT scans from patients with
Non-Small Cell Lung Cancer (NSCLC) [2,3]. Each lung nodule with a minimum
size of 3mm was segmented and annotated with a malignancy score ranging
from 1-highly unlikely to 5-highly suspicious by one to four expert raters.
Nodules were also scored according to their characteristics with respect to predefined attributes, namely subtlety (difficulty of detection, 1-extremely subtle,
5-obvious), internal structure (1-soft tissue, 4-air ), pattern of calcification (1-
popcorn, 6-absent), sphericity (1-linear, 5-round), margin (1-poorly defined, 5-
sharp), lobulation (1-no lobulation, 5-marked lobulation), spiculation (1-no spiculation, 5-marked spiculation), and texture (1-non-solid, 5-solid). The pylidc
framework [7] is used to access and process the data. The mean attribute annotation
and the mean and standard deviation of the malignancy annotations are
calculated. The latter was used to fit a Gaussian distribution, which serves as
the ground truth label for optimization. Samples with a mean expert malignancy
score of 3-indeterminate or annotations from fewer than three experts
were excluded in consistency with the literature [8,9,11].
",1,0,0,0,0,1,1,0,ct,"thorax, lung",0,0,0,0,0,0,0,0,0,0,0,0,0
Longitudinal Multimodal Transformer Integrating Imaging and Latent Clinical Signatures from Routine EHRs for Pulmonary Nodule Classification,62,2,1,"Our classifier is pretrained on 2,668 scans from a public dataset and 1,149 subjects with longitudinal chest CTs, billing codes, medications, and laboratory tests from EHRs of our home institution. 

This study used an imaging-only cohort from the NLST [28] and
three multimodal cohorts from our home institution with IRB approval (Table 1).
For the NLST cohort (https://cdas.cancer.gov/nlst/), we identified cases who
had a biopsy-confirmed diagnosis of lung malignancy and controls who had a
positive screening result for an SPN but no lung malignancy. We randomly
sampled from the control group to obtain a 4:6 case control ratio. Next, EHRPulmonary was the unlabeled dataset used to learn clinical signatures in an
unsupervised manner. We searched all records in our EHR archives for patients
who had billing codes from a broad set of pulmonary conditions, intending to
capture pulmonary conditions beyond just malignancy. Additionally, Image-
EHR was a labeled dataset with paired imaging and EHRs. We searched our
institution’s imaging archive for patients with three chest CTs within five years.
In the EHR-Image cohort, malignant cases were labeled as those with a billing
code for lung malignancy and no cancer of any type prior. Importantly, this case
criteria includes metastasis from cancer in non-lung locations. Benign controls
were those who did not meet this criterion. Finally, Image-EHR-SPN was a
subset of Image-EHR with the inclusion criteria that subjects had a billing code
for an SPN and no cancer of any type prior to the SPN. We labeled malignant
cases as those with a lung malignancy billing code occurring within three years
after any scan and only used data collected before the lung malignancy code. All
data within the five-year period were used for controls. We removed all billing
codes relating to lung malignancy. A description of the billing codes used to
define SPN and lung cancer events are provided in Supplementary 1.2.",1,0,0,0,1,4,1,0,"ct, ehr","lung, chest",0,0,1,0,0,1,0,0,0,0,0,0,0
Multimodal CT and MR Segmentation of Head and Neck Organs-at-Risk,71,4,1,"head and neck (HaN)

Our implementation within the nnU-Net framework shows promising results
on a dataset of CT and MR image pairs from the same patients.

Image Datasets. The proposed methodology was evaluated on two publicly
available datasets: our recently released HaN-Seg dataset [14] and the PDDCA
dataset [15]. The HaN-Seg dataset comprises CT and T1-weighted MR images of
56 patients, which were deformably registered with the SimpleElastix registration
tool, and corresponding curated manual delineations of 30 OARs (for details,
please refer to [14]). Although only a subset of images is publicly available1 due
to the ongoing HaN-Seg challenge2, both the publicly available training as well
as the privately withheld test images were used in our 4-fold cross-validation
experiments. On the other hand, to evaluate the generalization ability of our
method, we also conducted experiments on the CT-only PDDCA dataset (for
details, please refer to [15]), from which we collected 15 images from the offand
on-site test sets of the corresponding challenge for our evaluation. As this
dataset is widely used for evaluating the performance of automatic HaN OAR
segmentation methods, it serves as a valuable benchmark for comparison with
other state-of-the-art methods. Note that none of the images from the CT-only
PDDCA dataset were used for training, and as our model expects two inputs,
we substituted the missing MR modality with an empty matrix (i.e. zeros).",1,0,0,0,0,2,2,0,"ct, t1, mr, ",head and neck,0,0,0,0,0,0,0,0,0,0,0,1,0
Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,46,10,1,"We apply the method to a public dataset and to an in-house dataset
and show that it matches the performance of a supervised approach and
outperforms it when measurement noise is present in the data.

We validated our method on publicly available data [15] against a supervised
approach [6] and applied it to an internal clinical dataset of 30 lung cancer
patients. We explore different dataset sizes to understand their effects on the
reconstructed images.

First, we used the SPARE Varian dataset to study whether Noise2Aliasing can
match the performance of the supervised baseline and if it can outperform it
when adding noise to the projections. Then, we use the internal dataset to explore
the requirements for the method to be applied to an existing clinical dataset.
These required around 64 GPU days on NVIDIA A100 GPUs.
The Datasets used in this study are two:
1. The SPARE Varian dataset was used to provide performance results on publicly
available patient data. To more closely resemble normal respiratory
motion per projection image, the 8 min scan has been used from each patient
(five such scans are available in the dataset). Training is performed over 4
patients while 1 patient is used as a test set. The hyperparameters are optimized
over the training dataset.

2. An internal dataset (IRB approved) of 30 lung cancer patients’ 4DCBCTs
from 2020 to 2022, originally used for IGRT, with 25 patients for training and
5 patients for testing. 

The method removes noise more reliably when the
dataset size is increased, however further analysis is required to establish a good
quantitative measurement of this phenomenon",1,0,0,0,0,2,1,1,ct,lung,0,0,0,0,0,0,0,0,0,0,0,0,0
Non-iterative Coarse-to-Fine Transformer Networks for Joint Affine and Deformable Image Registration,71,10,1,"Extensive experiments with seven public datasets show that our NICE-Trans outperforms state-of-the-art registration methods on both registration accuracy and runtime.

We evaluated the proposed NICE-Trans on the task of inter-patient brain MRI registration, which is a common benchmark task in medical image registration studies [7–9, 12–18].We followed the dataset settings in [18]: 2,656 brainMRI images acquired from four public datasets (ADNI [27], ABIDE [28], ADHD [29], and IXI [30]) were used for training; two public brainMRI datasets with anatomical segmentation (Mindboggle [31] and Buckner [32]) were used for validation and testing. TheMindboggle dataset contains 100 MRI images and were randomly split into 50/50 images for validation/testing. The Buckner dataset contains 40 MRI images and were used for testing only. In addition to the original settings of [18], we adopted an additional public brain MRI dataset (LPBA [33]) for testing, which contains 40 MRI images. We performed brain extraction and intensity normalization for each MRI image with FreeSurfer [32]. Each image was placed at the same position via Center of Mass (CoM) initialization [34], and then was cropped into 144 × 192 × 160 voxels.",1,0,0,0,0,7,7,0,mri,brain,0,0,0,0,0,0,0,0,0,0,0,0,0
Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,20,5,1,"To meet the needs of both lowdose and noncontrast screening, we collect a large-scale dataset of 12,852 and 4,029 nodules from low-dose and noncontrast CTs respectively, each with pathology- or follow-up-confirmed labels. Experiments on several datasets demonstrate that our method achieves advanced screening performance on both low-dose and noncontrast scenarios.

For the LDCT, we annotate more than 12,852 nodules from 8,271 patients
from the NLST dataset [14]. For the NCCT, we annotate over 4,029 nodules from
over 2,565 patients from our collaborating hospital. Experimental results on several
datasets demonstrate that our method achieves outstanding performance on
both LDCT and NCCT screening scenarios.

NLST is the first large-scale LDCT dataset for low-dose CT lung cancer screening purpose [14]. There are 8,271 patients enrolled in this study. An experienced radiologist chose the last CT scan of each patient, and localized and labeled the nodules in the scan as benign or malignant based on the rough candidate nodule location and whether the patient develops lung cancer provided by NLST metadata. The nodules with a diameter smaller than 4mm were excluded. The in-house cohort was retrospectively collected from 2,565 patients at our collaborating hospital between 2019 and 2022. Unlike NLST, this dataset is noncontrast chest CT, which is used for routine clinical care. Segmentation annotation: We provide the segmentation mask for our in-house data, but not for the NLST data considering its high cost of pixel-level labeling. The nodule mask of each in-house data was manually annotated with the assistance of CT labeler [20] by our radiologists, while other contextual masks such as lung, vessel, and trachea were generated using the TotalSegmentator [21].

Train-Val-Test: The training set contains 9,910 (9,413 benign and 497 malignant)
nodules from 6,366 patients at NLST, and 2,592 (843 benign and 1,749
malignant) nodules from 2,113 patients at the in-house cohort. The validation
set contains 1,499 (1,426 benign and 73 malignant) nodules from 964 patients
at NLST. The NLST test set has 1,443 (1,370 benign and 73 malignant) nodules
from 941 patients. The in-house test set has 1,437 (1,298 benign and 139
malignant) nodules from 452 patients. We additionally evaluate our method on
the LUNGx [2] challenge dataset, which is usually used for external validation
in previous work [6,11,24]. LUNGx contains 83 (42 benign and 41 malignant)
nodules, part of which (13 scans) were contrast-enhanced. Segmentation: We
also evaluate the segmentation performance of our method on the public nodule
segmentation dataset LIDC-IDRI [3], which has 2,630 nodules with nodule
segmentation mask. Evaluation metrics: The area under the receiver operating
characteristic curve (AUC) is used to evaluate the malignancy prediction
performance.",1,0,0,0,1,4,1,1,ct,"lung, chest",0,0,1,1,0,0,0,0,0,0,0,0,0
Pelvic Fracture Segmentation Using a Multi-scale Distance-Weighted Neural Network,30,9,1,"We built a dataset containing 100 CT scans with fractured pelvis and
manually annotated the fractures. A five-fold cross-validation experiment
shows that our method outperformed max-flow segmentation and network
without distance weighting, achieving a global Dice of 99.38%, a
local Dice of 93.79%, and an Hausdorff distance of 17.12 mm. We have
made our dataset and source code publicly available and expect them to
facilitate further pelvic research, especially reduction planning.

We established a comprehensive pelvic fracture CT dataset
and provided ground-truth annotations. Our dataset and source code are publicly
available at https://github.com/YzzLiu/FracSegNet. We expect them to
facilitate further pelvis-related research, including but not limited to fracture
identification, segmentation, and subsequent reduction planning.

Therefore, we curated a dataset of
100 preoperative CT scans covering all common types of pelvic fractures. These
data is collected from 100 patients (aged 18–74 years, 41 females) who were
to undergo pelvic reduction surgery at Beijing Jishuitan Hospital between 2018
and 2022, under IRB approval (202009-04). The CT scans were acquired on a
Toshiba Aquilion scanner. The average voxel spacing is 0.82×0.82×0.94 mm3.
The average image shape is 480 × 397 × 310.
To generate ground-truth labels for bone fragments, a pre-trained segmentation
network was used to create initial segmentations for the ilium and
sacrum [13]. Then, these labels were further modified and annotated by two
annotators and checked by a senior expert.",1,1,1,1,1,1,1,0,ct,"pelvis, pelvic",0,0,1,1,Beijing Jishuitan Hospital,0,0,0,0,0,0,0,0
Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,2,7,1,"We created a synthetic mask dataset that simulates a
protuberance, and trained a segmentation network to separate the protruded
regions from the normal kidney regions.

The proposed method was evaluated on a publicly available KiTS19 dataset, which contains 108 NCCT images, and showed that our method achieved a higher
dice score of 0.615 (+0.097) and sensitivity of 0.721 (+0.103) compared
to 3D-UNet.

The release of two public CT image datasets with kidney and tumor masks from
the 2019/2021 Kidney and Kidney Tumor Segmentation challenge [8] (KiTS19,
KiTS21) attracted researchers to develop various methods for segmentation.

We used a dataset from KiTS19 [8] which contains both CECT and NCCT
images. For CECT images, there are 210 images for training and validation
and, 90 images for testing. For NCCT images, there are 108 images, which are
different series of the 210 images. The ground truth masks are only available for
the 210 CECT images. Thus, we transfer the masks to NCCT images. This is
achieved by extracting kidney masks and adjusting the height of each kidney.
The ground truth mask contains a kidney label and a kidney tumor label. Cysts
are not annotated separately and included in the kidney label on this dataset.
The data can be downloaded from The Cancer Imaging Archive (TCIA) [4,9].

During the Step2 phase of the training, where we used the synthetic dataset,
we created 10,000 masks using the method from Sect. 3.2. We applied some
augmentations during training to input masks to simulate the incoming inputs
from the base network.",1,0,0,0,0,2,1,0,"cect, ncct",kidney,0,0,1,0,0,1,The Cancer Imaging Archive (TCIA) ,0,0,0,0,0,0
Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,49,7,1,"We evaluated SFT on 11 clinical tasks using a real-world
dataset of pNENs and achieved higher performance than other state-ofthe-
art multi-label models with mAUCs of 0.68 and 0.76 on internal and
external datasets, respectively.

Real-World pNENs Dataset. We validated our method on a real-world
pNENs dataset from two centers. All patients with arterial phase Computed
Tomography (CT) images were included. The dataset contained 264 and 28
patients in center 1 and center 2, and a senior radiologist annotated the bounding
boxes for all 408 and 28 lesions. We extracted 37 labels from clinical reports,
including survival, immunohistochemical (IHC), CT findings, etc. Among them,
1)RECIST drug response (RS), 2)tumor shrink (TS), 3)durable clinical benefit
(DCB), 4)progression-free survival (PFS), 5)overall survival (OS), 6)grade (GD),
7)somatostatin receptor subtype 2(SSTR2), 8)Vascular Endothelial Growth Factor
Receptor 2 (VEFGR2), 9)O6-methylguanine methyltransferase (MGMT),
10)metastatic foci (MTF), and 11)surgical recurrence (RT) are main tasks, and
the remaining are auxiliary tasks. 143 and 28 lesions were segmented by radiologists,
and the radiomics features of them were extracted, of which 162 features
were selected and binarized as auxiliary tasks because of its statistically significant
correlation with the main labels. The label distribution and the overlap
ratio (Jaccard index) of lesions between pairs of labels are shown in Fig. 3. It is
obvious that the real-world dataset has a large number of labels with randomly
missing data, thus, we used an adjusted 5-fold cross-validation. Taking a patient
as a sample, we chose the dataset from center 1 as the internal dataset, of which
the samples with most of the main labels were used as Dataset 1 (219 lesions) and
was split into 5 folds, and the remaining samples are randomly divided into the
training set Dataset 2 (138 lesions) and the validation set Dataset 3 (51 lesions),
the training set and the validation set of the corresponding folds were added
during cross-validation, respectively. All samples in Center 2 left as external test
set. Details of each dataset are in the Supplementary Material.

Dataset Evaluation Metrics. We evaluate the performance of our method
on the 10 main tasks for internal dataset, and due to missing labels and too few
SSTR2 labels, only the performance of predicting RT, PFS, OS, GD, MTF are
evaluated for external dataset. We employ accuracy (ACC), sensitivity (SEN),
specificity (SPC), F1-score (F1) and area under the receiver operating characteristic
(AUC) for each task, and compute the mean value of them (e.g. mAUC).",1,0,0,0,1,1,0,0,ct,"arterial, auxillary, pancreas",0,0,1,1,0,1,0,0,0,0,0,1,0
SimPLe: Similarity-Aware Propagation Learning for Weakly-Supervised Breast Cancer Segmentation in DCE-MRI,54,4,1,"The proposed method is evaluated on our
collected DCE-MRI dataset containing 206 patients with biopsy-proven
breast cancers.

The method obtained a Dice value of 83% using the interval-slice annotation, on a testing dataset containing only 28 patients
We evaluate our method on a collected DCE-MRI dataset containing 206 subjects.

Dataset. We evaluated our method on an in-house breast DCE-MRI dataset
collected from the Cancer Center of Sun Yat-Sen University. In total, we
collected 206 DCE-MRI scans with biopsy-proven breast cancers. All MRI
scans were examined with 1.5T MRI scanner. The DCE-MRI sequences
(TR/TE = 4.43ms/1.50 ms, and flip angle=10◦) using gadolinium-based contrast
agent were performed with the T1-weighted gradient echo technique,
and injected 0.2ml/kg intravenously at 2.0 ml/s followed by 20ml saline. The
DCE-MRI volumes have two kinds of resolution, 0.379×0.379×1.700 mm3 and
0.511×0.511×1.000 mm3.
All cancerous regions and extreme points were manually annotated by an
experienced radiologist via ITK-SNAP [26] and further confirmed by another
radiologist. We randomly divided the dataset into 21 scans for training and the
remaining scans for testing1. Before training, we resampled all volumes into the
same target spacing 0.600×0.600×1.000 mm3 and normalized all volumes as zero
mean and unit variance.",1,0,0,0,1,1,0,1,"dce-mri, mri",breast,0,0,1,0,0,1,Cancer Center of Sun Yat-Sen University,0,0,0,0,0,0
Single-subject Multi-contrast MRI Super-resolution via Implicit Neural Representations,17,8,1,"Trained within minutes on a single commodity
GPU, our model provides realistic super-resolution across different
pairs of contrasts in our experiments with three datasets.

We extensively evaluate our method on multiple brain MRI datasets and
show that it achieves high visual quality for different contrasts and views and
preserves pathological details, highlighting its potential clinical usage.

To enable fair evaluation between our predictions and the reference
HR ground truths, the in-plane SNR between the LR input scan and corresponding
ground truth has to match. To synthetically create 2D LR images, it is
necessary to downsample out-of-plane in the image domain anisotropically [32]
while preserving in-plane resolution. Consequently, to mimic realistic 2D clinical
protocol, which often has higher in-plane details than that of 3D scans, we use
spline interpolation to model partial volume and downsampling.We demonstrate
our network’s modeling capabilities for different contrasts (T1w, T2w, FLAIR,
DIR), views (axial, coronal, sagittal), and pathologies (MS, brain tumor). We
conduct experiments on two public datasets, BraTS [16], and MSSEG [4], and an
in-house clinical MS dataset (cMS). In each dataset, we select 25 patients that
fulfill the isotropic acquisition criteria for both ground truth HR scans. Note
that we only use the ground truth HR for evaluation, not anywhere in training.
We optimize separate INRs for each subject with supervision from only its two
LR scans. If required, we employ skull-stripping [12] and rigid registration to
the MNI152 (MSSEG, cMS) or SRI24 (BraTS) templates. For details, we refer
to Table 2 in the supplementary.
",1,0,0,0,0,3,2,1,mri,brain,0,0,0,0,0,0,0,0,0,0,0,1,0
Solving Low-Dose CT Reconstruction via GAN with Local Coherence,50,10,1,"We evaluate our proposed method on real datasets and the experimental
results suggest that it can outperform existing state-of-the-art reconstruction
approaches significantly.

To illustrate the efficiency of our proposed approach, we conduct rigorous
experiments on several real clinical datasets; the experimental results reveal
the advantages of our approach over several state-of-the-art CT reconstruction
methods.

First, our proposed approaches are evaluated on the “Mayo-Clinic
low-dose CT Grand Challenge” (Mayo-Clinic) dataset of lung CT images [19].
The dataset contains 2250 two dimensional slices from 9 patients for training,
and the remaining 128 slices from 1 patient are reserved for testing.  To evaluate the
generalization of our model, we also consider another dataset RIDER with nonsmall
cell lung cancer under two CT scans [36] for testing. We randomly select 4
patients with 1827 slices from the dataset.

To evaluate the stability and generalization of our
model and the baselines trained on Mayo-Clinic dataset, we also test them on
the RIDER dataset. The results are shown in Table 2. Due to the bias in the
datasets collected from different facilities, the performances of all the models are
declined to some extents. But our proposed approach still outperforms the other
models for most testing cases.",1,0,0,0,1,2,0,0,ct,lung,0,0,1,1,mayo-clinic,0,0,0,0,0,0,0,0
Text-Guided Foundation Model Adaptation for Pathological Image Classification,27,5,1,"Through extensive experiments on the PatchGastric stomach
tumor pathological image dataset, we demonstrate that CITE achieves
leading performance compared with various baselines especially when
training data is scarce.

We adopt the PatchGastric [25] dataset, which includes histopathological
image patches extracted from H&E stained whole slide images (WSI)
of stomach adenocarcinoma endoscopic biopsy specimens. There are 262,777
patches of size 300 × 300 extracted from 991 WSIs at x20 magnification. The
dataset contains 9 subtypes of gastric adenocarcinoma. We choose 3 major subtypes including “well differentiated tubular adenocarcinoma”, “moderately differentiated tubular adenocarcinoma”, and “poorly differentiated adenocarcinoma”
to form a 3-class grading-like classification task with 179,285 patches from 693
WSIs. We randomly split the WSIs into train (20%) and validation (80%) subsets
for measuring the model performance. To extend our evaluation into the
real-world setting with insufficient data, we additionally choose 1, 2, 4, 8, or 16
WSIs with the largest numbers of patches from each class as the training set.
The evaluation metric is patient-wise accuracy, where the prediction of a WSI
is obtained by a soft vote over the patches, and accuracy is averaged class-wise.",1,0,0,0,0,1,0,0,"h&e stained whole slide images, wsi",stomach,0,0,0,0,0,0,0,0,0,0,0,0,0
Thinking Like Sonographers: A Deep CNN Model for Diagnosing Gout from Musculoskeletal Ultrasound,16,6,1,"MSKUS Dataset Collection. The MSKUS data were collected for patients
suspected of metatarsal gout in Nanjing Drum Tower Hosptial. Informed written
consent was obtained at the time of recruitment. Dataset totally contains 1127
US images from different patients including 509 gout images and 618 healthy
images. The resolution of the MSKUS images were resized to 224 × 224. During
experiments, we randomly divided 10% of the dataset into testing sets, then
the remaining data was divided equally into two parts for the different phases
of the training. We used 5-fold cross validation to divide the training sets and
validation sets.

Gaze Data Collection. We collected the eye movement data with the Tobii 4C
eye-tracker operating at 90Hz. The MSKUS images were displayed on a 1920 ×
1080 27-inch LCD screen. The eye tracker was attached beneath the screen with
a magnetic mounting bracket. Sonographers were seated in front of the screen
and free to adjust the chair’s height and the display’s inclination.
",1,0,0,0,1,2,0,0,us,"eye, Musculoskeletal",0,0,1,1,Nanjing Drum Tower Hosptial,0,0,0,0,0,0,0,0
VesselVAE: Recursive Variational Autoencoders for 3D Blood Vessel Synthesis,7,1,1,"We trained our networks using a subset of the open-access IntrA
dataset1 published by Yang et al. in 2020 [32]. This subset consisted of 1694
healthy vessel segments reconstructed from 2D MRA images of patients. We
converted 3D meshes into a binary tree representation and used the network
extraction script from the VMTK toolkit2 to extract the centerline coordinates
of each vessel model. The centerline points were determined based on the ratio
between the sphere step and the local maximum radius, which was computed
using the advancement ratio specified by the user. The radius of the blood vessel
conduit at each centerline sample was determined using the computed crosssections
assuming a maximal circular shape (See Fig. 2). To improve computational
efficiency during recursive tree traversal, we implemented an algorithm
that balances each tree by identifying a new root. We additionally trimmed trees
to a depth of ten in our experiments. This decision reflects a balance between
the computational demands of depth-first tree traversal in each training step
and the complexity of the training meshes. We excluded from our study trees",1,0,0,0,1,1,1,0,2d mra,blood vessels,0,0,0,0,0,0,0,0,0,0,0,0,0
