,title,header_no,header_title,text,volume
5,Anatomy-Driven Pathology Detection on Chest X-rays,3.4,Dataset,"training dataset. we train on the chest imagenome dataset [4,21,22]1 ,
consisting of roughly 240 000 frontal chest x-ray images with corresponding
scene graphs automatically constructed from free-text radiology reports. it is
derived from the mimic-cxr dataset [9,10], which is based on imaging studies
from 65 079 patients performed at beth israel deaconess medical center in
boston, us. amongst other information, each scene graph contains bounding boxes
for 29 unique anatomical regions with annotated attributes, where we consider
positive anatomical finding and disease attributes as positive labels for
pathologies, leading to binary anatomy-level annotations for 55 unique
pathologies. we consider the image-level label for a pathology to be positive if
any region is positively labeled with that pathology.we use the provided
jpg-images [11] 2 and follow the official mimic-cxr training split but only keep
samples containing a scene graph with at least five valid region bounding boxes,
resulting in a total of 234 307 training samples.during training, we use random
resized cropping with size 224 × 224, apply contrast and brightness jittering,
random affine augmentations, and gaussian blurring.evaluation dataset and class
mapping. we evaluate our method on the subset of 882 chest x-ray images with
pathology bounding boxes, annotated by radiologists, from the nih chestxray-8
(cxr8) dataset [20] 3 from the national institutes of health clinical center in
the us. we use 50% for validation and keep the other 50% as a held-out test set.
note that for evaluation only pathology bounding boxes are required (to compute
the metrics), while during training only anatomical region bounding boxes
(without considering pathologies) are required. all images are center-cropped
and resized to 224 × 224.the dataset contains bounding boxes for 8 unique
pathologies. while partly overlapping with the training classes, a one-to-one
correspondence is not possible for all classes. for some evaluation classes, we
therefore use a many-to-one mapping where the class probability is computed as
the mean over several training classes. we refer to the supp. material for a
detailed study on class mappings.",1
18,Self-supervised Learning for Physiologically-Based Pharmacokinetic Modeling in Dynamic PET,2.4,Dataset,"the dataset is composed of 23 oncological patients with different tumor types.
dpet data was acquired on a biograph vision quadra for 65 min, over 62 frames.
the exposure duration of the frames were 2 × 10 s, 30 × 2 s, 4 × 10 s, 8 × 30 s,
4 × 60 s, 5 × 120 s and 9 × 300 s. the pet volumes were reconstructed with an
isotropic voxel size of 1.6 mm. the dataset included the label maps of 7 organs
(bones, lungs, heart, liver, kidneys, spleen, aorta) and one image-derived input
function a(t) [bq/ml] from the descending aorta per patient. further details on
the dataset are presented elsewhere [16].the pet frames and the label map were
resampled to an isotropic voxel size of 2.5 mm. then, the dataset was split
patient-wise into training, validation, and test set, with 10, 4, and 9 patients
respectively. details on the dataset split are available in the supplementary
material (table 1). the training set consisted of 750 slices and the validation
consisted of 300. in both cases, 75 axial slices per patient were extracted in a
pre-defined patient-specific range from the lungs to the bladder (included) and
were cropped to size 112 × 112 pixels.",1
20,Self-supervised Learning for Physiologically-Based Pharmacokinetic Modeling in Dynamic PET,0,(Color figure online),"the most important design choice is the selection of the final activation
function. indeed, the multi-clamp final activation function was proven to be the
best both in terms of cs (exp 4.1: cs = 0.78 ± 0.05) and mae (exp 4.2: mae =
3.27 ± 2.01). compared to the other final activation functions, when the
multi-clamp is used the impact of the max-pooling design is negligible also in
terms of mae. for the rest of the experiments, the selected configuration is the
one from exp. 4.1 (see table 1).figure 2 shows the kps for four selected organs
as computed with the proposed dnn (kp dnn ), as computed with curve fit using
only the 9 patients of the test set (kp cf ) and using all 23 patients (kp ref
cf ) [16]. the voxel-wise kps predicted by the dnn were averaged over the
available organ masks.in terms of run-time, the dnn needed ≈ 1 min to predict
the kps of the a whole-body scan (≈ 400 slices), whereas curve fit took 8.7 min
for a single slice: the time reduction of the dnn is expected to be ≈ 3.500
times.",1
21,Self-supervised Learning for Physiologically-Based Pharmacokinetic Modeling in Dynamic PET,4.0,Discussion,"even though the choice of the final activation function has a greater impact,
the selection of the kernel design is important. using spatial and temporal
convo- lution results in an increase in the performance (+0.01 in cs) and
reduces the number of trainable parameters (from 2.1 m to 8.6 k), as pointed out
by [10]. therefore, the convergence is reached faster. moreover, the use of two
separate kernels in time and space is especially meaningful. pixel counts for a
given exposure are affected by the neighboring count measurements due to the
limited resolution of the pet scanner [20]. the temporally previous or following
counts are independent. in general, there is good agreement between kp dnn , kp
cf and kp ref cf . the dnn prediction of k 1 and k 2 in the spleen and k 3 in
the lungs is outside the confidence interval of the results published by sari et
al. [16].an analysis per slice of the metrics shows that the cs between tac i
and tac i changes substantially depending on the region: cs max = 0.87 within
the liver boundaries and cs min = 0.71 in the region corresponding to the heart
and lungs (see fig. 3a). this can be explained by the fact that v b is
underestimated for the heart and aorta. the proposed network predicts v heart b
= 0.376 ± 0.133 and v aorta b = 0.622 ± 0.238 while values of nearly 1 are to be
expected. this is likely due to breathing and heartbeat motion artifacts, which
cannot be modeled properly with a 2tc km that assumes no motion between
frames.figure 3b-e shows the central coronal slice of the four kpis in an
exemplary patient. as expected, k 1 is high in the heart, liver, and kidney.
similarly, the blood fraction volume v b is higher in the heart, blood vessels,
and lungs.the kp dnn are more homogeneous than kp cf , as can be seen in the
exemplary k 1 axial slice shown in fig. 4. a quantitative evaluation of the
smoothness of the images is reported in the supplementary material (fig. 1).
moreover, the distribution in the liver is more realistic in kp dnn , where the
gallbladder can be seen as an ellipsoid between the right and left liver lobes.
high k 1 regions are mainly within the liver, spleen, and kidney for kp dnn ,
while they also appear in unexpected areas in the kp cf (e.g., next to the spine
or in the region of the stomach).the major limitation of this work is the lack
of ground truth and a canonical method to evaluate quantitatively its
performance. this limitation is inherent to pbpk modeling and results in the
need for qualitative analyses based on expected physiological processes. a
possible way to leverage this would be to work on simulated data, yet the
validity of such evaluations strongly depends on how realistic the underlying
simulation models are. as seen in fig. 3a, motion (gross, respiratory, or
cardiac) has a major impact on the estimation quality. registering different
dpet frames has been shown to improve conventional pbpk models [8] and would
possibly have a positive impact on our approach.",1
32,AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on MRI Brain Tumor,5.0,Conclusion,"in this work, we propose a brain tumor segmentation method for mri images using
only class labels, based on an attentive multiple-exit class activation mapping
(ame-cam). our approach extracts activation maps from different exits of the
network to capture information from multiple resolutions. we then use an
attention model to hierarchically aggregate these activation maps, learning
pixel-wise weighted sums.experimental results on the four modalities of the 2021
brats dataset demonstrate the superiority of our approach compared with other
cam-based weakly-supervised segmentation methods. specifically, ame-cam achieves
the highest dice score for all patients in all datasets and modalities. these
results indicate the effectiveness of our proposed approach in accurately
segmenting brain tumors from mri images using only class labels.",1
39,Weakly-Supervised Positional Contrastive Learning: Application to Cirrhosis Classification,3.1,Datasets,"three histo . it corresponds to absent fibrosis (f0), mild fibrosis (f1),
significant fibrosis (f2), severe fibrosis (f3) and cirrhosis (f4). this score
is then binarized to indicate the absence or presence of advanced fibrosis [14]:
f0/f1/f2 (n = 28) vs. f3/f4 (n = 78). d 2 histo . this is the public lihc
dataset from the cancer genome atlas [9], which presents a histological score,
the ishak score, designated as y 2 histo , that differs from the metavir score
present in d 1 histo . this score is also distributed through five labels: no
fibrosis, portal fibrosis, fibrous speta, nodular formation and incomplete
cirrhosis and established cirrhosis. similarly to the metavir score in d 1 histo
, we also binarize the ishak score, as proposed in [16,20], which results in two
cohorts of 34 healthy and 15 pathological patients.in all datasets, we select
the slices based on the liver segmentation of the patients. to gain in
precision, we keep the top 70% most central slices with respect to liver
segmentation maps obtained manually in d radio , and automatically for d 1 histo
and d 2 histo using a u-net architecture pretrained on d radio [18]. for the
latter pretraining dataset, it presents an average slice spacing of 3.23 mm with
a standard deviation of 1.29 mm. for the x and y axis, the dimension is 0.79 mm
per voxel on average, with a standard deviation of 0.10 mm.",1
40,Weakly-Supervised Positional Contrastive Learning: Application to Cirrhosis Classification,3.2,Architecture and Optimization,"backbones. we propose to work with two different backbones in this paper:
tinynet and resnet-18 [12]. tinynet is a small encoder with 1.1m parameters,
inspired by [24], with five convolutional layers, a representation space (for
downstream tasks) of size 256 and a latent space (after a projection head of two
dense layers) of size 64. in comparison, resnet-18 has 11.2m parameters, a
representation space of dimension 512 and a latent space of dimension 128. more
details and an illustration of tinynet are available in the supplementary
material, as well as a full illustration of the algorithm flow.data
augmentation, sampling and optimization. cl methods [5,10,11] require strong
data augmentations on input images, in order to strengthen the association
between positive samples [22]. in our work, we leverage three types of
augmentations: rotations, crops and flips. data augmentations are computed on
the gpu, using the kornia library [17]. during inference, we remove the
augmentation module to only keep the original input images.for sampling,
inspired by [4], we propose a strategy well-adapted for contrastive learning in
2d medical imaging. we first sample n patients, where n is the batch size, in a
balanced way with respect to the radiological/histological classes; namely, we
roughly have the same number of subjects per class. then, we randomly select
only one slice per subject. in this way, we maximize the slice heterogeneity
within each batch. we use the same sampling strategy also for classification
baselines. for d 2 histo , which has fewer patients than the batch size, we use
a balanced sampling strategy with respect to the radiological/histological
classes with no obligation of one slice per patient in the batch. as we work
with 2d slices rather than 3d volumes, we compute the average probability per
patient of having the pathology. the evaluation results presented later are
based on the patient-level aggregated prediction.finally, we run our experiments
on a tesla v100 with 16gb of ram and a 6 cpu cores, and we used the
pytorch-lightning library to implement our models. all models share the same
data augmentation module, with a batch size of b = 64 and a fixed number of
epochs n epochs = 200. for all experiments, we fix a learning rate (lr) of α =
10 -4 and a weight decay of λ = 10 -4 . we add a cosine decay learning rate
scheduler [15] to prevent over-fitting. for byol, we initialize the moving
average decay at 0.996. evaluation protocol. we first pretrain the backbone
networks on d radio using all previously listed contrastive and non-contrastive
methods. then, we train a regularized logistic regression on the frozen
representations of the datasets d 1 histo and d 2 histo . we use a stratified
5-fold cross-validation. as a baseline, we train a classification algorithm from
scratch (supervised) for each dataset, d 1 histo and d 2 histo , using both
backbone encoders and the same 5-fold crossvalidation strategy. we also train a
regularized logistic regression on representations obtained with a random
initialization as a second baseline (random). finally, we report the
cross-validated results for each model on the aggregated dataset",1
41,Weakly-Supervised Positional Contrastive Learning: Application to Cirrhosis Classification,4.0,Results and Discussion,"we present in table 1 the results of all our experiments. for each of them, we
report whether the pretraining method integrates the weak label meta-data, the
depth spatial encoding, or both, which is the core of our method. first, we can
notice that our method outperforms all other pretraining methods in d 1 histo
and d 1+2 histo , which are the two datasets with more patients. for the latter,
the proposed method surpasses the second best pretraining method, depth-aware,
by 4%. for d 1 histo , it can be noticed that wsp (ours) provides the best auc
score whatever the backbone used. for the second dataset d 2 histo , our method
is on par with byol and supcon when using a small encoder and outperforms the
other methods when using a larger backbone.to illustrate the impact of the
proposed method, we report in fig. 2 the projections of the resnet-18
representation vectors of 10 randomly selected subjects of d 1 histo onto the
first two modes of a pca. it can be noticed that the representation space of our
method is the only one where the diagnostic label (not available during
pretraining) and the depth position are correctly integrated. indeed, there is a
clear separation between slices of different classes (healthy at the bottom and
cirrhotic cases at the top) and at the same time it seems that the depth
position has been encoded in the x-axis, from left to right. supcon performs
well on the training set of d radio (figure available in the supplementary
material), as well as d 2 histo with tinynet, but it poorly generalizes to d 1
histo and d 1+2 histo . the method depth-aware manages to correctly encode the
depth position but not the diagnostic class label.to assess the clinical
performance of the pretraining methods, we also compute the balanced accuracy
scores (bacc) of the trained classifiers, which is compared in table 2 to the
bacc achieved by radiologists who were asked to visually assess the presence or
absence of cirrhosis for the n=106 cases of d 1 histo . the reported bacc values
correspond to the best scores among those obtained with tiny and resnet
encoders.radiologists achieved a bacc of 82% with respect to the histological
reference. the two bestperforming methods surpassed this score: depth-aware and
the proposed wsp approach, improving respectively the radiologists score by 2%
and 3%, suggesting that including 3d information (depth) at the pretraining
phase was beneficial.",1
53,S 2 ME: Spatial-Spectral Mutual Teaching and Ensemble Learning for Scribble-Supervised Polyp Segmentation,3.1,Experimental Setup,"datasets. we employ the sun-seg [10] dataset with scribble annotations for
training and assessing the in-distribution performance. this dataset is based on
the sun database [16], which contains 100 different polyp video cases. to reduce
data redundancy and memory consumption, we choose the first of every five
consecutive frames in each case. we then randomly split the data into 70, 10,
and 20 cases for training, validation, and testing, leaving 6677, 1240, and 1993
frames in the respective split. for out-of-distribution evaluation, we utilize
three public datasets, namely kvasir-seg [9], cvc-clinicdb [2], and polypgen [1]
with 1000, 612, and 1537 polyp frames, respectively. these datasets are
collected from diversified patients in multiple medical centers with various
data acquisition systems. varying data shifts and corruption like motion blur
and specular reflections2 pose significant challenges to model generalization
and robustness.implementation details. we implement our method with pytorch [18]
and run the experiments on a single nvidia rtx3090 gpu. the sgd optimizer is
utilized for training 30k iterations with a momentum of 0.9, a weight decay of
0.0001, and a batch size of 16. the execution time for each experiment is
approximately 4 h. the initial learning rate is 0.03 and updated with the
polyscheduling policy [15]. the loss weighting coefficients λ mt and λ el are
empirically set the same and exponentially ramped up [3] from 0 to 5 in 25k
iterations. all the images are randomly cropped at the border with maximally 7
pixels and resized to 224×224 in width and height. besides, random horizontal
and vertical flipping are applied with a probability of 0.5, respectively. we
utilize unet [20] and ynet [6] as the respective segmentation model in the
spatial and spectral branches. the performance of the scribble-supervised model
with partial cross entropy [13] loss (scrib-pce) and the fully-supervised model
with cross entropy loss (fully-ce) are treated as the lower and upper bound,
respectively. five classical and relevant methods, including entmin [7], gcrf
[17], ustm [14], cps [3], and dmpls [15] are employed as the comparative
baselines and implemented with unet [20] as the segmentation backbone referring
to the wsl4mis3 repository. for a fair comparison, the output from the spatial
branch is taken as the final prediction and utilized in evaluation without
post-processing. in addition, statistical evaluations are conducted with
multiple seeds, and the mean and standard deviations of the results are
reported. [17] 0.656±0.019 0.541±0.022 0.690±0.017 4.983±0.089 ustm [14]
0.654±0.008 0.533±0.009 0.663±0.011 5.207±0.138 cps [3] 0.658±0.004 0.539±0.005
0.676±0.005 5.092±0.063 dmpls [15] 0.656±0.006 0.539±0.005 0.659±0.011
5.208±0.061 s 2 me (ours) 0.674±0.003 0.565±0.001 0.719±0.003 4.583±0.014",1
59,Tracking Adaptation to Improve SuperPoint for 3D Reconstruction in Endoscopy,1.0,Introduction,"endoscopy is an important medical procedure with many applications, from routine
screening to detection of early signs of cancer and minimally invasive
treatment. automatic analysis and understanding of these videos raises many
opportunities for novel assistive and automatization tasks on endoscopy
procedures. obtaining 3d models from the intracorporeal scenes captured in
endoscopies is an essential step to enable these novel tasks and build
applications, for example, for improved monitoring of existing patients or
augmented reality during training or real explorations.3d reconstruction
strategies have been studied for long, and one crucial step in these strategies
is feature detection and matching which serves as input for structure from
motion (sfm) pipelines. endoscopic images are a challenging case for feature
detection and matching, due to several well known challenges for these tasks,
such as lack of texture, or the presence of frequent artifacts, like specular
reflections. these problems are accentuated when all the elements in the scene
are deformable, as it is the case in most endoscopy scenarios, and in particular
in the real use case studied in our work, the lower gastrointestinal tract
explored with colonoscopies. existing 3d reconstruction pipelines are able to
build small 3d models out of short clips from real and complete recordings [1].
one of the current bottle-necks to obtain better 3d models is the lack of more
abundant and higher quality correspondences in real data.this work introduces
superpoint-e, a new model to extract interest points from endoscopic images. we
build on the well known superpoint architecture [5], a seminal work that
delivers state-of-the-art results when coupled with downstream tasks1 . our main
contribution is a novel supervision strategy to train the model. we propose to
automatically generate reliable training data from video sequences by tracking
feature points from existing detection methods, which do not require training.
we select good features with the colmap sfm pipeline [21], generating training
examples with feature points that can be tracked across several images according
to colmap result. when used to train superpoint, our approach yields a
self-supervised method outperforming current ones.",1
73,Black-box Domain Adaptative Cell Segmentation via Multi-source Distillation,3.0,Experiments,"dataset and setting: we collect four pathology image datasets to validate our
proposed approach. firstly, we acquire 50 images from a cohort of patients with
triple negative breast cancer (tnbc), which is released by naylor et al [18].
hou et al. [10] publish a dataset of nucleus segmentation containing 5,060
segmented slides from 10 tcga cancer types. in this work, we use 98 images from
invasive carcinoma of the breast (brca). we have also included 463 images of
kidney renal clear cell carcinoma (kirc) in our dataset, which are made publicly
available by irshad et al [11]. awan et al. [2] publicly release a dataset
containing tissue slide images and associated clinical data on colorectal cancer
(crc), from which we randomly select 200 patches for our study. in our
experiments, we transfer knowledge from three black-box models trained on
different source domains to a new target domain model (e.g.,from crc, tnbc, kirc
to brca). the backbone network for the student model and source domain black-box
predictors employ the widely adopted residual u-net [12], which is commonly used
for medical image segmentation. for each source domain network, we conduct
full-supervision training on the corresponding source domain data and directly
evaluate its performance on target domain data. the upper performance metrics
(source-only upper) are shown in the table 1. to ensure the reliability of the
results, we use the same data for training, validation, and testing, which
account for 80%, 10%, and 10% of the original data respectively. for the target
domain network, we use unsupervised and semi-supervised as our task settings
respectively. in semi-supervised domain adaptation, we only use 10% of the
target domain data as labeled data.experimental results: to validate our method,
we compare it with the following approaches: (1) cellsegssda [8], an adversarial
learning based semisupervised domain adaptation approach. (2) us-msma [13], a
multi-source model domain aggregation network. (3) sfda-dpl [5], a source-free
unsupervised domain adaptation approach. (4) bbuda [17], an unsupervised
black-box model domain adaptation framework. a point worth noting is that most
of the methods we compared with are white-box methods, which means they can
obtain more information from the source domain than us. for single-source domain
adaptation approach, cellsegssda and sfda-dpl, we employ two strategies to
ensure the fairness of the experiments: (1) single-source, i.e. performing
adaptation on each single source, where we select the best results to display in
the table 1; (2) source-combined, i.e. all source domains are combined into a
traditional single source. the table 1 and fig. 2 demonstrate that our proposed
method exhibits superior performance, even when compared to these white-box
methods, surpassing them in various evaluation metrics and visualization
results.in addition, the experimental results also show that simply combining
multiple source data into a traditional single source will result in performance
degradation in some cases, which also proves the importance of studying
multi-source domain adaptation methods.ablation study: to evaluate the impact of
our proposed methods of weighted logits(wl), pseudo-cutout label(pcl) and
maximize mutual information(mmi) on the model performance, we conduct an
ablation study. we compare the baseline model with the models that added these
three methods separately. we chose crc, kirc and brca as our source domains, and
tnbc as our target domain. the results of these experiments, presented in the
table 2, show that our proposed modules are indeed useful.",1
75,Self-Supervised Domain Adaptive Segmentation of Breast Cancer via Test-Time Fine-Tuning,1.0,Introduction,"in recent years, deep learning (dl) methods have demonstrated remarkable
performance in detecting and localizing tumors on ultrasound images [2,27].
compared with conventional image processing methods, dl methods provide an
accurate feature extraction capability on ultrasound images, despite their low
resolution and noise disturbance, leading to superior segmentation accuracy
[2,5,14]. however, there are some limitations in developing a dl model in a
source domain and deploying it in an unseen target domain. the primary
limitation is that dl models require a large number of training samples to
achieve accurate predictions [8,24]. yet, acquiring large training datasets and
their corresponding labels, especially from a cohort of patients, can be costly
or even infeasible, which poses a significant challenge in developing a dl model
with high performance [7]. second, even when large-scale datasets are available
through collaborative research from multiple sites, dl models trained on such
datasets may yield sub-optimal solutions due to domain gaps caused by
differences in images acquired from different sites [20]. third, due to the
small number of datasets from each domain, the images for each individual domain
may not capture representative features, limiting the ability of dl models to
generalize across domains [3].domain adaptation (da) has been extensively
studied to alleviate the aforementioned limitations, the goal of which is to
reduce the domain gap caused by the diversity of datasets from different domains
[12,20,26,29,33]. example solutions include transfer learning-and style
transfer-based methods. nonetheless, unlike natural images, generating labels
can be a challenging task, making it difficult to apply general da methods; thus
bridging domain gaps by da methods remains limited [26,33]. this is due to
sensitive privacy issues in patients' data, particularly in collaborative
research, which restricts access to labels from different domains. as a result,
conventional da methods cannot be easily applied [10]. more recently,
unsupervised domain adaptation (uda) has been introduced to address this issue
[16,33], aiming to generate semi-predictions (pseudo-labels) in target domains
first, followed by producing accurate predictions using the pseudo-labels. one
critical limitation of pseudo-label-based uda is the possibility of error
accumulation due to mispredicted pseudo-labels. this can lead to significant
degradation of the performance of dl models, as errors can compound and become
more pronounced over time [17,25].to alleviate the problem of pseudo-label-based
uda, in this work, we propose an advanced uda framework based on self-supervised
da with a test-time finetuning network. test-time adaptation methods have been
developed [4,11,13,23] to improve the learning of knowledge in target domains.
the distinctive feature of our test-time self-supervised da is that it enables
the dl network (i) to learn knowledge about the features of target domains by
fine-tuning the network itself during the test-time phase, rather than
generating pseudo-labels and then (ii) to provide precise predictions on images
in target domains, by using the fine-tuned network. specifically, we adopt
self-supervised learning and verify the model via thorough mathematical
analysis. our framework was tested on the task of breast cancer segmentation in
ultrasound images, but it could also be applied to other lesion segmentation
tasks.to summarize, our contributions are three-fold:• we design a
self-supervised da framework that includes a parameter search method and provide
a mathematical justification for it. with our framework, we are able to identify
the best-performing parameters that result in improved performance in da tasks.
• our framework is effective at preserving privacy, since it carries out da
using only pre-trained network parameters, without transferring any patient
data. • we applied our framework to the task of segmenting breast cancer from
ultrasound imaging data, demonstrating its superior performance over competing
uda methods.our results indicate that our framework is effective in improving
the accuracy of breast cancer segmentation from ultrasound images, which could
have potential implications for improving the diagnosis and treatment of breast
cancer. sample batches of (t, ?) ∼ t",1
98,3D Arterial Segmentation via Single 2D Projections and Depth Supervision in Contrast-Enhanced CT Images,4.0,Experimental Design,"dataset. we use an in-house dataset of contrast-enhanced abdominal computed
tomography images (cts) in the arterial phase to segment the peripancreatic
arteries [6]. the cohort consists of 141 patients with pancreatic ductal
adenocarcinoma, of an equal ratio of male to female patients. given a 3d
arterial ct of the abdominal area, we automatically extract the vertebrae
[15,18] and semi-automatically extract the ribs, which have similar intensities
as arteries in arterial cts and would otherwise occlude the vessels. in order to
remove as much of the cluttering surrounding tissue and increase the visibility
of the vessels in the projections, the input is windowed so that the vessels
appear hyperintense. details of the exact preprocessing steps can be found in
table 2 of the supplementary material. the dataset contains binary 3d
annotations of the peripancreatic arteries carried out by two radiologists, each
having annotated half of the dataset. the 2d annotations we use in our
experiments are projections of these 3d annotations. for more information about
the dataset, see [6].image augmentation and transformation. as the annotations
lie on a 2d plane, 3d spatial augmentation cannot be used due to the information
sparsity in the ground truth. instead, we apply an invertible transformation t
to the input volume and apply the inverse transformation t -1 to the network
output before applying the loss, such that the ground truth need not be altered.
a detailed description of the augmentations and transformations used can be
found in table 1 in the supplementary material.training and evaluation. we use a
3d u-net [17] with four layers as our backbone, together with xavier
initialization [10]. a diagram of the network architecture can be found in fig.
2 in the supplementary material. the loss weight α is tuned at 0.5, as this
empirically yields the best performance. our experiments are averaged over
5-fold cross-validation with 80 train samples, 20 validation samples, and a
fixed test set of 41 samples. the network initialization is different for each
fold but kept consistent across different experiments run on the same fold. this
way, both data variance and initialization variance are accounted for through
cross-validation. to measure the performance of our models, we use the dice
score, precision, recall, and mean surface distance (msd). we also compute the
skeleton recall as the percentage of the ground truth skeleton pixels which are
present in the prediction.",1
109,Mesh2SSM: From Surface Meshes to Statistical Shape Models of Anatomy,3.0,Experiments and Discussion,"dataset: we use the publicly available decath-pancreas dataset of 273
segmentations from patients who underwent pancreatic mass resection [24]. the
shapes of the pancreas are highly variable and have thin structures, making it a
good candidate for non-linear ssm analysis. the segmentations were isotropically
resampled, smoothed, centered, and converted to meshes with roughly 2000
vertices. although the dgcnn mesh autoencoder used in mesh2ssm does not require
the same number of vertices, uniformity across the dataset makes it
computationally efficient; hence, we pad the smallest mesh by randomly repeating
the vertices (akin to padding image for convolutions). the samples were randomly
divided, with 218 used for training, 26 for validation, and 27 for testing.
flowssm [15] with two templates: sphere, medoid. the color map and arrows show
the signed distance and direction from the mean shape.",1
133,Infusing Physically Inspired Known Operators in Deep Models of Ultrasound Elastography,2.4,Dataset and Quantitative Metrics,"we use publicly available data collected from a breast phantom (model 059, cirs:
tissue simulation & phantom technology, norfolk, va) using an alpinion e-cube
r12 research us machine (bothell, wa, usa). the center frequency was 8 mhz and
the sampling frequency was 40 mhz. the young's modulus of the experimental
phantom was 20 kpa and contains several inclusions with young's modulus of
higher than 40 kpa. this data is available online at http://code.sonography.ai
in [16].in vivo data was collected at johns hopkins hospital from patients with
liver cancer during open-surgical rf thermal ablation by a research antares
siemens system using a vf 10-5 linear array with the sampling frequency of 40
mhz and the center frequency of 6.67 mhz. the institutional review board
approved the study with the consent of the patients. we selected 600 rf frame
pairs of this dataset for the training of the networks.two well-known metrics of
contrast to noise ratio (cnr) and strain ratio (sr) are utilized to evaluate the
compared methods. two regions of interest (roi) are selected to compute these
metrics and they can be defined as [10]:where the subscript t and b denote the
target and background rois. the sr is only sensitive to the mean (s x ), while
cnr depends on both the mean and the standard deviation (σ x ) of rois. for
stiff inclusions as the target, higher cnr correlates with better target
visibility, and lower sr translates to a higher difference between the target
and background strains.",1
142,SLPD: Slide-Level Prototypical Distillation for WSIs,2.3,Slide-Level Clustering,"many histopathologic features have been established based on the morphologic
phenotypes of the tumor, such as tumor invasion, anaplasia, necrosis and
mitoses, which are then used for cancer diagnosis, prognosis and the estimation
of response-to-treatment in patients [3,9]. to obtain meaningful representations
of slides, we aim to explore and maintain such histopathologic features in the
latent space. clustering can reveal the representative patterns in the data and
has achieved success in the area of unsupervised representation learning
[4,5,24,26].to characterize the histopathologic features underlying the slides,
a straightforward practice is the global clustering, i.e., clustering the region
embeddings from all the wsis, as shown in the left of fig. 1(d). however, the
obtained clustering centers, i.e., the prototypes, are inclined to represent the
visual bias related to staining or scanning procedure rather than medically
relevant features [33]. meanwhile, this clustering strategy ignores the
hierarchical structure ""region→wsi→whole dataset"" underlying the data, where the
id of the wsi can be served as an extra learning signal. therefore, we first
consider the slidelevel clustering that clusters the embeddings within each wsi,
which is shown in the right of fig. 1(d). specifically, we conduct k-means
algorithm before the start of each epoch over l n region embeddings {z l n } ln
l=1 of w n to obtain m prototypes {c m n ∈ r d } m m=1 . similar operations are
applied across other slides, and then we acquire n groups of prototypes {{c m n
} m m=1 } n n=1 . each group of prototypes is expected to encode the semantic
structure (e.g., the combination of histopathologic features) of the wsi.",1
144,SLPD: Slide-Level Prototypical Distillation for WSIs,2.5,Inter-Slide Distillation,"tumors of different patients can exhibit morphological similarities in some
respects [17,21], so the correspondences across slides should be characterized
during learning. previous self-supervised learning methods applied to
histopathologic images only capture such correspondences with positive pairs at
the patchlevel [22,23], which overlooks the semantic structure of the wsi. we
rethink this problem from the perspective how to measure the similarity between
two slides accurately. due to the heterogeneity of the slides, comparing them
with the local crops or the averaged global features are both susceptible to
being one-sided. to address this, we bridge the slides with their semantic
structures and define the semantic similarity between two slides w i and w j
through an optimal bipartite matching between two sets of prototypes:where
cos(•, •) measures the cosine similarity between two vectors, and s m enumerates
the permutations of m elements. the optimal permutation σ * can be computed
efficiently with the hungarian algorithm [19]. with the proposed setto-set
distance, we can model the inter-slide correspondences conveniently and
accurately. specifically, for a region embedding z belonging to the slide w and
assigned to the prototype c, we first search the top-k nearest neighbors of w in
the dataset based on the semantic similarity, denoted as { ŵk } k k=1 . second,
we also obtain the matched prototype pairs {(c, ĉk )} k k=1 determined by the
optimal permutation, where ĉk is the prototype of ŵk . finally, we encourage z
to be closer to ĉk with the inter-slide distillation:the inter-slide
distillation can encode the sldie-level information complementary to that of
intra-slide distillation into the region embeddings.the overall learning
objective of the proposed slpd is defined as:where the loss scale is simply set
to α 1 = α 2 = 1. we believe the performance can be further improved by tuning
this.",1
158,Many Tasks Make Light Work: Learning to Localise Medical Anomalies from Multiple Synthetic Tasks,1.0,Introduction,"in recent years, the workload of radiologists has grown drastically, quadrupling
from 2006 to 2020 in western europe [4]. this huge increase in pressure has led
to long patient-waiting times and fatigued radiologists who make more mistakes
[3]. the most common of these errors is underreading and missing anomalies
(42%); followed by missing additional anomalies when concluding their search
after an initial finding (22%) [10]. interestingly, despite the challenging work
environment, only 9% of errors reviewed in [10] were due to mistakes in the
clinicians' reasoning. therefore, there is a need for automated second-reader
capabilities, which brings any kind of anomalies to the attention of
radiologists. for such a tool to be useful, its ability to detect rare or
unusual cases is particularly important. traditional supervised models would not
be appropriate, as acquiring sufficient training data to identify such a broad
range of pathologies is not feasible. unsupervised or self-supervised methods to
model an expected feature distribution, e.g., of healthy tissue, is therefore a
more natural path, as they are geared towards identifying any deviation from the
normal distribution of samples, rather than a particular type of pathology.there
has been rising interest in using end-to-end self-supervised methods for anomaly
detection. their success is most evident at the miccai medical
outof-distribution analysis (mood) challenge [31], where all winning methods
have followed this paradigm so far (2020-2022). these methods use the variation
within normal samples to generate diverse anomalies through sample mixing
[7,[23][24][25]. however all these methods lack a key component: structured
validation. this creates uncertainty around the choice of hyperparameters for
training. for example, selecting the right training duration is crucial to avoid
overfitting to proxy tasks. yet, in practice, training time is often chosen
arbitrarily, reducing reproducibility and potentially sacrificing generalisation
to real anomalies.",1
179,VesselVAE: Recursive Variational Autoencoders for 3D Blood Vessel Synthesis,3.0,Experimental Setup,"materials. we trained our networks using a subset of the open-access intra
dataset1 published by yang et al. in 2020 [32]. this subset consisted of 1694
healthy vessel segments reconstructed from 2d mra images of patients. we
converted 3d meshes into a binary tree representation and used the network
extraction script from the vmtk toolkit2 to extract the centerline coordinates
of each vessel model. the centerline points were determined based on the ratio
between the sphere step and the local maximum radius, which was computed using
the advancement ratio specified by the user. the radius of the blood vessel
conduit at each centerline sample was determined using the computed
crosssections assuming a maximal circular shape (see fig. 2). to improve
computational efficiency during recursive tree traversal, we implemented an
algorithm that balances each tree by identifying a new root. we additionally
trimmed trees to a depth of ten in our experiments. this decision reflects a
balance between the computational demands of depth-first tree traversal in each
training step and the complexity of the training meshes. we excluded from our
study trees that exhibited greater depth, nodes with more than two children, or
with loops. however, non-binary trees can be converted into binary trees and it
is possible to train with deeper trees at the expense of higher computational
costs. ultimately, we were able to obtain 700 binary trees from the original
meshes using this approach.implementation details. for the centerline
extraction, we set the advancement ratio in the vmtk script to 1.05. the script
can sometimes produce multiple cross-sections at centerline bifurcations. in
those cases, we selected the sample with the lowest radius, which ensures proper
alignment with the centerline principal direction. all attributes were
normalized to a range of [0, 1]. for the mesh reconstruction we used 4
iterations of catmull-clark subdivision algorithm. the data pre-processing
pipeline and network code were implemented in python and pytorch
framework.training. in all stages, we set the batch size to 10 and used the adam
optimizer with β 1 = 0.9, β 2 = 0.999, and a learning rate of 1 × 10 -4 . we set
α = .3 and γ = .001 for eq. 1 in our experiments. to enhance computation speed,
we implemented dynamic batching [16], which groups together operations involving
input trees of dissimilar shapes and different nodes within a single input
graph. it takes approximately 12 h to train our models on a workstation equipped
with an nvidia a100 gpu, 80 gb vram, and 256 gb ram. however, the memory
footprint during training is very small (≤1 gb) due to the use of a lightweight
tree representation. this means that the amount of memory required to store and
manipulate our training data structures is minimal. during training, we ensure
that the reconstructed tree aligns with the original structure, rather than
relying solely on the classifier's predictions. we train the classifier using a
crossentropy loss that compares its predictions to the actual values from the
original tree. since the number of nodes in each class is unbalanced, we scale
the weight given to each class in the cross-entropy loss using the inverse of
each class count. during preliminary experiments, we observed that accurately
classifying nodes closer to the tree root is critical. this is because a
miss-classification of top nodes has a cascading effect on all subsequent nodes
in the tree (i.e. skip reconstructing a branch). to account for this, we
introduce a weighting scheme that for each node, assigns a weight to the
cross-entropy loss based on the number of total child nodes. the weight is
normalized by the total number of nodes in the tree.metrics. we defined a set of
metrics to evaluate our trained network's performance. by using these metrics,
we can determine how well the generated 3d models of blood vessels match the
original dataset distribution, as well as the diversity of the generated output.
the chosen metrics have been widely used in the field of blood vessel 3d
modeling, and have shown to provide reliable and accurate quantification of
blood vessels main characteristics [3,13]. we analyzed tortuosity per branch,
the vessel centerline total length, and the average radius of the tree.
tortuosity distance metric [4] is a widely used metric in the field of blood
vessel analysis, mainly because of its clinical importance. it measures the
amount of twistiness in each branch of the vessel. vessel's total length and
average radius were used in previous work to distinguish healthy vasculature
from cancerous malformations. finally, in order to measure the distance across
distributions for each metric, we compute the cosine similarity.",1
197,DAS-MIL: Distilling Across Scales for MIL Classification of Histological WSIs,1.0,Introduction,"modern microscopes allow the digitalization of conventional glass slides into
gigapixel whole-slide images (wsis) [18], facilitating their preservation and
fig. 1. overview of our proposed framework, das-mil. the features extracted at
different scales are connected (8-connectivity) by means of different graphs.
the nodes of both graphs are later fused into a third one, respecting the rule
""part of"". the contextualized features are then passed to distinct
attention-based mil modules that extract bag labels. furthermore, a knowledge
distillation mechanism encourages the agreement between the predictions
delivered by different scales.retrieval, but also introducing multiple
challenges. on the one hand, annotating wsis requires strong medical expertise,
is expensive, time-consuming, and labels are usually provided at the slide or
patient level. on the other hand, feeding modern neural networks with the entire
gigapixel image is not a feasible approach, forcing to crop data into small
patches and use them for training. this process is usually performed considering
a single resolution/scale among those provided by the wsi image.recently,
multi-instance learning (mil) emerged to cope with these limitations. mil
approaches consider the image slide as a bag composed of many patches, called
instances; afterwards, to provide a classification score for the entire bag,
they weigh the instances through attention mechanisms and aggregate them into a
single representation. it is noted that these approaches are intrinsically flat
and disregard the pyramidal information provided by the wsi [15], which have
been proven to be more effective than single-resolution [4,13,15,19]. however,
to the best of our knowledge, none of the existing proposals leverage the full
potential of the wsi pyramidal structure. indeed, the flat concatenation of
features [19] extracted at different resolutions does not consider the
substantial difference in the informative content they provide. a proficient
learning approach should instead consider the heterogeneity between global
structures and local cellular regions, thus allowing the information to flow
effectively across the image scales.to profit from the multi-resolution
structure of wsi, we propose a pyramidal graph neural network (gnn) framework
combined with (self) knowledge distillation (kd), called das-mil (distilling
across scales). a visual representation of the proposed approach is depicted in
fig. 1. distinct gnns provide contextualized features, which are fed to distinct
attention-based mil modules that compute bag-level predictions. through
knowledge distillation, we encour-age agreement across the predictions delivered
at different resolutions, while individual scale features are learned in
isolation to preserve the diversity in terms of information content. by
transferring knowledge across scales, we observe that the classifier
self-improves as information flows during training. our proposal has proven its
effectiveness on two well-known histological datasets, camelyon16 and tcga lung
cancer, obtaining state-of-the-art results on wsi classification.",1
238,Spectral Adversarial MixUp for Few-Shot Unsupervised Domain Adaptation,1.0,Introduction,"a common challenge for deploying deep learning to clinical problems is the
discrepancy between data distributions across different clinical sites
[6,15,20,28,29]. this discrepancy, which results from vendor or protocol
differences, can cause a significant performance drop when models are deployed
to a new site [2,21,23]. to solve this problem, many unsupervised domain
adaptation (uda) methods [6] have been developed for adapting a model to a new
site with only unlabeled data (target domain) by transferring the knowledge
learned from the original dataset (source domain). however, most uda methods
require sufficient target samples, which are scarce in medical imaging due to
the limited accessibility to patient data. this motivates a new problem of
few-shot unsupervised domain adaptation (fsuda), where only a few unlabeled
target samples are available for training.few approaches [11,22] have been
proposed to tackle the problem of fsuda. luo et. al [11] introduced adversarial
style mining (asm), which uses a pretrained style-transfer module to generate
augmented images via an adversarial process. however, this module requires extra
style images [9] for pre-training. such images are scarce in clinical settings,
and style differences across sites are subtle. this hampers the applicability of
asm to medical image analysis. sm-ppm [22] trains a style-mixing model for
semantic segmentation by augmenting source domain features to a fictitious
domain through random interpolation with target domain features. however, sm-ppm
is specifically designed for segmentation tasks and cannot be easily adapted to
other tasks. also, with limited target domain samples in fsuda, the random
feature interpolation is ineffective in improving the model's
generalizability.in a different direction, numerous uda methods have shown high
performance in various tasks [4,[16][17][18]. however, their direct application
to fsuda can result in severe overfitting due to the limited target domain
samples [22]. previous studies [7,10,24,25] have demonstrated that transferring
the amplitude spectrum of target domain images to a source domain can
effectively convey image style information and diversify training dataset. to
tackle the overfitting issue of existing uda methods, we propose a novel
approach called sensitivityguided spectral adversarial mixup (samix) to augment
training samples. this approach uses an adversarial mixing scheme and a spectral
sensitivity map that reveals model generalizability weaknesses to generate
hard-to-learn images with limited target samples efficiently. samix focuses on
two key aspects. 1) model generalizability weaknesses: spectral sensitivity
analysis methods have been applied in different works [26] to quantify the
model's spectral weaknesses to image amplitude corruptions. zhang et al. [27]
demonstrated that using a spectral sensitivity map to weigh the amplitude
perturbation is an effective data augmentation. however, existing sensitivity
maps only use single-domain labeled data and cannot leverage target domain
information. to this end, we introduce a domain-distance-modulated spectral
sensitivity (dodiss) map to analyze the model's weaknesses in the target domain
and guide our spectral augmentation. 2) sample hardness: existing studies
[11,19] have shown that mining hard-to-learn samples in model training can
enhance the efficiency of data augmentation and improve model generalization
performances. therefore, to maximize the use of the limited target domain data,
we incorporate an adversarial approach into the spectral mixing process to
generate the most challenging data augmentations. this paper has three major
contributions. 1) we propose samix, a novel approach for augmenting target-style
samples by using an adversarial spectral mixing scheme. samix enables
high-performance uda methods to adapt easily to fsuda problems. 2) we introduce
dodiss to characterize a model's generalizability weaknesses in the target
domain. 3) we conduct thorough empirical analyses to demonstrate the
effectiveness and efficiency of samix as a plug-in module for various uda
methods across different tasks.",1
252,Gall Bladder Cancer Detection from US Images with only Image Level Labels,1.0,Introduction,"gbc is a deadly disease that is difficult to detect at an early stage [12,15].
early diagnosis can significantly improve the survival rate [14]. non-ionizing
radiation, low cost, and accessibility make us a popular non-invasive diagnostic
modality for patients with suspected gall bladder (gb) afflictions. however,
identifying signs of gbc from routine us imaging is challenging for radiologists
[11]. in recent years, automated gbc detection from us images has drawn
increased interest [3,5] due to its potential for improving diagnosis and
treatment outcomes. many of these works formulate the problem as an object
detection, since training a image classification model for gbc detection seems
challenging due to the reasons outlined in the abstract (also see fig. 1).
recently, gbcnet [3], a cnn-based model, achieved sota performance on
classifying malignant gb from us images. gbcnet uses a two-stage pipeline
consisting of object detection followed by classification, and requires bounding
box annotations for gb as well as malignant regions for training. such bounding
box annotations surrounding the pathological regions are time-consuming and
require an expert radiologist for annotation. this makes it expensive and
non-viable for curating large datasets for training large dnn models. in another
recent work, [5] has exploited additional unlabeled video data for learning good
representations for downstream gbc classification and obtained performance
similar to [3] using a resnet50 [13] classifier. the reliance of both sota
techniques on additional annotations or data, limits their applicability. on the
other hand, the image-level malignancy label is usually available at a low cost,
as it can be obtained readily from the diagnostic report of a patient without
additional effort from clinicians.instead of training a classification pipeline,
we propose to solve an object detection problem, which involves predicting a
bounding box for the malignancy. the motivation is that, running a classifier on
a focused attention/ proposal region in an object detection pipeline would help
tackle the low inter-class and high intra-class variations. however, since we
only have image-level labels available, we formulate the problem as a weakly
supervised object detection (wsod) problem. as transformers are increasingly
outshining cnns due to their ability to aggregate focused cues from a large area
[6,9], we choose to use transformers in our model. however, in our initial
experiments sota wsod methods for transformers failed miserably. these methods
primarily rely on training a classification pipeline and later generating
activation heatmaps using attention and drawing a bounding box circumscribing
the heatmaps [2,10] to show localization. however, for gbc detection, this line
of work is not helpful as we discussed earlier.inspired by the success of the
multiple instance learning (mil) paradigm for weakly supervised training on
medical imaging tasks [20,22], we train a detection transformer, detr, using the
mil paradigm for weakly supervised malignant region detection. in this, one
generates region proposals for images, and then considers the images as bags and
region proposals as instances to solve the instance classification (object
detection) under the mil constraints [8]. at inference, we use the predicted
instance labels to predict the bag labels. our experiments validate the utility
of this approach in circumventing the challenges in us images and detecting gbc
accurately from us images using only image-level labels.",1
254,Gall Bladder Cancer Detection from US Images with only Image Level Labels,2.0,Datasets,"gallbladder cancer detection in ultrasound images: we use the public gbc us
dataset [3] consisting of 1255 image samples from 218 patients. the dataset
contains 990 non-malignant (171 patients) and 265 malignant (47 patients) gb
images (see fig. 2 for some sample images). the dataset contains image labels as
well as bounding box annotations showing the malignant regions.note that, we use
only the image labels for training. we report results on 5-fold
cross-validation. we did the cross-validation splits at the patient level, and
all images of any patient appeared either in the train or validation split.
polyp detection in colonoscopy images: we use the publicly available kvasir-seg
[17] dataset consisting of 1000 white light colonoscopy images showing polyps
(see fig. 2). since kvasir-seg does not contain any control images, we add 600
non-polyp images randomly sampled from the polypgen [1] dataset.since the
patient information is not available with the data, we use random stratified
splitting for 5-fold cross-validation.",1
262,Structured State Space Models for Multiple Instance Learning in Digital Pathology,1.0,Introduction,"precision medicine efforts are shifting cancer care standards by providing novel
personalised treatment plans with promising outcomes. patient selection for such
treatment regimes is based principally on the assessment of tissue biopsies and
the characterisation of the tumor microenvironment. this is typically performed
by experienced pathologists, who closely inspect chemically stained
histopathological whole slide images (wsis). increasingly, clinical centers are
investing in the digitisation of such tissue slides to enable both automatic
processing as well as research studies to elucidate the underlying biological
processes of cancer. the resulting images are of gigapixel size, rendering their
computational analysis challenging. to deal with this issue, multiple instance
learning (mil) schemes based on weakly supervised training are used for wsi
classification tasks. in such schemes, the wsi is typically divided into a grid
of patches, with general purpose features derived from pretrained imagenet [18]
networks extracted for each patch. these representations are subsequently pooled
together using different aggregation functions and attention-based operators for
a final slide-level prediction.state space models are designed to efficiently
model long sequences, such as the sequences of patches that arise in wsi mil. in
this paper, we present the first use of state space models for wsi mil.
extensive experiments on three publicly available datasets show the potential of
such models for the processing of gigapixel-sized images, under both weakly and
multi-task schemes. moreover, comparisons with other commonly used mil schemes
highlight their robust performance, while we demonstrate empirically the
superiority of state space models in processing the longest of wsi sequences
with respect to commonly used mil methods.",1
269,Structured State Space Models for Multiple Instance Learning in Digital Pathology,4.1,Data,"camelyon16 [16] is a dataset that consists of resections of lymph nodes, where
each wsi is annotated with a binary label indicating the presence of tumour
tissue in the slide, and all slides containing tumors have a pixel-level
annotation indicating the metastatic region. in multitask experiments, we use
this annotation to give each patch a label indicating local tumour presence.
there are 270 wsis in the training/validation set, and 130 wsis in the
predefined test set. in our experiments, the average patch sequence length
arising from camelyon16 is 6129 (ranging from 127 to 27444).tcga-luad is a tcga
lung adenocarcinoma dataset that contains 541 wsis along with genetic
information about each patient. we obtained genetic information for this cohort
using xena browser [7]. as a mil task, we chose the task of predicting the
patient mutation status of tp53, a tumor suppressor gene that is highly relevant
in oncology studies. the average sequence length is 10557 (ranging from 85 to
34560).tcga-rcc is a tcga dataset for three kidney cancer subtypes (denoted
kich, kirc, and kirp). it consists of 936 wsis (121 kich, 518 kirc, and 297
kirp). the average sequence length is 12234 (ranging from 319 to 62235).",1
274,Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision,1.0,Introduction,"longitudinal lesion or tumor tracking is a fundamental task in treatment
monitoring workflows, and for planning of re-treatments in radiation therapy.
based on longitudinal imaging for a given patient it requires establishing which
lesions are corresponding (i.e., same lesion, observed at different timepoints),
which lesions have disappeared and which are new compared to prior scanning.
this information can be leveraged to assess treatment response, e.g., by
analyzing the evolution of size and morphology for a given tumor [1], but also
for adaptation of (re-)treatment radiotherapy plans that take into account new
tumors.in practice, the development of automatic and reliable lesion tracking
solutions is hindered by the complexity of the data (over different modalities),
the absence of large, annotated datasets, and the difficulties associated with
lesion identification (i.e., varying sizes, poses, shapes, and sparsely
distributed locations). in this work, we present a multi-scale self-supervised
learning solution for lesion tracking in longitudinal studies using the
capabilities of contrastive learning [9]. inspired by the pixel-wise contrastive
learning strategy introduced in [5], we choose to learn pixel-wise feature
representations that embed consistent anatomical information from unlabeled
(i.e., without lesion-related annotations) and unpaired (i.e., without the use
of longitudinal scans) data, overcoming barriers to data collection. to increase
the system robustness and emulate the clinician's reading strategies, we propose
to use multi-scale embeddings to enable the system to progressively refine the
fine-grained location. in addition, as imaging offers contextual information
about the human body that is naturally consistent, we design the model to
benefit from biologically-meaningful points (i.e., anatomical landmarks). the
reasoning behind this strategy is that simple data augmentation methods cannot
faithfully model inter-subject variability or possible organ deformations.
hence, we ensure the spatial coherence of the tracked lesion location using
well-defined anatomical landmarks.our proposed method brings two elements of
novelty from a technical point of view: (1) the multi-scale approach for the
anatomical embedding learning and (2) a positive sampling approach that
incorporates anatomically significant landmarks across different subjects. with
these two strategies, the goal is to ensure a high degree of robustness in the
computation of the lesion matching across different lesion sizes and varying
anatomies. furthermore, a significant focus and contribution of our research is
the experimental study at a very large scale: we (1) train a pixel-wise
self-supervised system using a very large and diverse dataset of 52,487 ct
volumes and (2) evaluate on two publicly available datasets. notably, one of the
datasets, nlst, presents challenging cases with 68% of lesions being very small
(i.e., radius < 5 mm).",1
279,Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision,4.1,Datasets and Setup,"datasets: we train the universal and fine-grained anatomical point matching
model using an in-house ct dataset (variousct). the training dataset contains
52,487 unlabeled 3d ct volumes capturing various anatomies, including chest,
head, abdomen, pelvis, and more.the evaluation is based on two datasets, the
publicly released deep longitudinal study (dls) dataset [8] and the national
lung screening trial (nlst) dataset [12]. the dls dataset is a subset of the
deeplesion [11] medical imaging dataset, containing 3891 pairs of lesions with
information on their location and size. the dataset covers various types of
lesions across different organs. we follow the official data split for dls
dataset and perform evaluation on the testing dataset which comprises 480 lesion
pairs. for nlst, we randomly selected a subset of 1045 test images coming from
420 patients with up to 3 studies. a certified radiologist annotated the testing
data by identifying the location and size of the pulmonary nodules, resulting in
a total of 825 paired annotations. we evaluate lesion tracking in both
directions, from baseline to follow-up and from follow-up to baseline [8]. this
results in a total of 960 and 1650 testing lesion pairs in dls and nlst test
sets, respectively. the isotropic resolution of all ct volumes is adjusted to
2mm through bilinear interpolation.system training: our learning model is
implemented in pytorch and uses the torchio library [13] for medical data
manipulation and augmentation.we employ a u-net-based encoder-decoder
architecture [2] that utilizes an inflated 3d resnet-18 [3,4] as its encoder,
which extends all 2d convolutions table 1. comparison between the proposed
solution and several state-of-the-art approaches (reference results are from
[8]). the exact same test set was used to compute the performance of each
approach listed in the table; however, we retrained only sam. in the standard
resnet to 3d convolutions and allows the use of pre-trained imagenet weights.
the multi-scale embedding model employs s = 5 scales, and the embedding length
is fixed at l = 128 for each scale. convolution with a stride of (2, 2, 2) is
used to reduce the feature map size at the first and fifth levels, while a
stride of (1, 2, 2) is employed for intermediary levels 2 to 4. the u-net
decoder uses a convolution layer with a 3 × 3 × 3 kernel after every up-sampling
layer to generate the final cascade of feature embeddings. the model is trained
with adamw optimizer [6] for 64 epochs using an early stopping strategy with a
patience of 5 epochs, a batch size of 8 augmented 3d paired patches of 32 × 96 ×
96, and a learning rate of 0.0001.",1
284,Geometry-Invariant Abnormality Detection,1.0,Introduction,"the use of machine learning for anomaly detection in medical imaging analysis
has gained a great deal of traction over previous years. most recent approaches
have focused on improvements in performance rather than flexibility, thus
limiting approaches to specific input types -little research has been carried
out to generate models unhindered by variations in data geometries. often,
research assumes certain similarities in data acquisition parameters, from image
dimensions to voxel dimensions and fields-of-view (fov). these restrictions are
then carried forward during inference [5,25]. this strong assumption can often
be complex to maintain in the real-world and although image pre-processing steps
can mitigate some of this complexity, test error often largely increases as new
data variations arise. this can include variances in scanner quality and
resolution, in addition to the fov selected during patient scans. usually
training data, especially when acquired from differing sources, undergoes
significant preprocessing such that data showcases the same fov and has the same
input dimensions, e.g. by registering data to a population atlas. whilst making
the model design simpler, these pre-processing approaches can result in poor
generalisation in addition to adding significant pre-processing times
[11,13,26]. given this, the task of generating an anomaly detection model that
works on inputs with a varying resolution, dimension and fov is a topic of
importance and the main focus of this research.unsupervised methods have become
an increasingly prominent field for automatic anomaly detection by eliminating
the necessity of acquiring accurately labelled data [4,7] therefore relaxing the
stringent data requirements of medical imaging. this approach consists of
training generative models on healthy data, and defining anomalies as deviations
from the defined model of normality during inference. until recently, the
variational autoencoder (vae) and its variants held the state-of-the-art for the
unsupervised approach. however, novel unsupervised anomaly detectors based on
autoregressive transformers coupled with vector-quantized variational
autoencoders (vq-vae) have overcome issues associated with autoencoder-only
methods [21,22]. in [22], the authors explore the advantage of tractably
maximizing the likelihood of the normal data to model the long-range
dependencies of the training data. the work in [21] takes this method a step
further through multiple samplings from the transformer to generate a
non-parametric kernel density estimation (kde) anomaly map.even though these
methods are state-of-the-art, they have stringent data requirements, such as
having a consistent geometry of the input data, e.g., in a whole-body imaging
scenario, it is not possible to crop a region of interest and feed it to the
algorithm, as this cropped region will be wrongly detected as an anomaly. this
would happen even in the case that a scan's original fov was restricted [17].as
such, we propose a geometric-invariant approach to anomaly detection, and apply
it to cancer detection in whole-body pet via an unsupervised anomaly detection
method with minimal spatial labelling. through adapting the vq-vae transformer
approach in [21], we showcase that we can train our model on data with varying
fields of view, orientations and resolutions by adding spatial conditioning in
both the vq-vae and transformer. furthermore, we show that the performance of
our model with spatial conditioning is at least equivalent to, and sometimes
better, than a model trained on whole-body data in all testing scenarios, with
the added flexibility of a ""one model fits all data"" approach. we greatly reduce
the pre-processing requirements for generating a model (as visualised in fig.
1), demonstrating the potential use cases of our model in more flexible
environments with no compromises on performance.",1
293,Geometry-Invariant Abnormality Detection,5.0,Conclusion,"detection and segmentation of anomalous regions, particularly for cancer
patients, is essential for staging, treatment and intervention planning.
generally, the variation scanners and acquisition protocols can cause failures
in models trained on data from single sources. in this study, we proposed a
system for anomaly detection that is robust to variances in geometry. not only
does the proposed model showcase strong and statistically-significant
performance improvements on varying image resolutions and fov, but also on
whole-body data. through this, we demonstrate that one can improve the
adaptability and flexibility to varying data geometries while also improving
performance. such flexibility also increases the pool of potential training
data, as they dont require the same fov. we hope this work serves as a
foundation for further exploration into geometry-invariant deep-learning methods
for medical-imaging.",1
298,Interpretable Medical Image Classification Using Prototype Learning and Privileged Information,3.0,Experiments,"data. the proposed approach is evaluated using the publicly available lidc-idri
dataset consisting of 1018 clinical thoracic ct scans from patients with
non-small cell lung cancer (nsclc) [2,3]. each lung nodule with a minimum size
of 3 mm was segmented and annotated with a malignancy score ranging from
1-highly unlikely to 5-highly suspicious by one to four expert raters. nodules
were also scored according to their characteristics with respect to predefined
attributes, namely subtlety (difficulty of detection, 1-extremely subtle,
5-obvious), internal structure (1-soft tissue, 4-air ), pattern of calcification
(1popcorn, 6-absent), sphericity (1-linear, 5-round ), margin (1-poorly defined,
5sharp), lobulation (1-no lobulation, 5-marked lobulation), spiculation (1-no
spiculation, 5-marked spiculation), and texture (1-non-solid, 5-solid ). the
pylidc framework [7] is used to access and process the data. the mean attribute
annotation and the mean and standard deviation of the malignancy annotations are
calculated. the latter was used to fit a gaussian distribution, which serves as
the ground truth label for optimization. samples with a mean expert malignancy
score of 3-indeterminate or annotations from fewer than three experts were
excluded in consistency with the literature [8,9,11].experiment designs. to
ensure comparability with previous work [8,9,11], the main metric used is
within-1-accuracy, where a prediction within one score is considered correct.
five-fold stratified cross-validation was performed using 10 % of the training
data for validation and the best run of three is reported.the algorithm was
implemented using the pytorch framework version 1.13 and cuda version 11.6. a
learning rate of 0.5 was chosen for the prototype vectors and 0.02 for the other
learnable parameters. the batch size was set to 128 and the optimizer was adam
[10]. with a maximum of 1000 epochs, but stopping early if there was no
improvement in target accuracy within 100 epochs, the experiments lasted an
average of three hours on a geforce rtx 3090 graphics card. the code is publicly
available at https://github.com/xrad-ulm/proto-caps.besides pure performance,
the effect of reduced availability of attribute annotations was investigated.
this was done by using attribute information only for a randomly selected
fraction of the nodules during the training.to investigate the effect of
prototypes on the network performance, an ablation study was performed. three
networks were compared: proto-caps (proposed) including learning and applying
prototypes during inference, proto-caps w/o use where prototypes are only
learned but ignored for inference, and proto-caps w/o learn using the proposed
architecture without any prototypes.",2
343,Longitudinal Multimodal Transformer Integrating Imaging and Latent Clinical Signatures from Routine EHRs for Pulmonary Nodule Classification,3.0,Experimental Setup,"datasets. this study used an imaging-only cohort from the nlst [28] and three
multimodal cohorts from our home institution with irb approval (table 1). for
the nlst cohort (https://cdas.cancer.gov/nlst/), we identified cases who had a
biopsy-confirmed diagnosis of lung malignancy and controls who had a positive
screening result for an spn but no lung malignancy. we randomly sampled from the
control group to obtain a 4:6 case control ratio. next, ehr-pulmonary was the
unlabeled dataset used to learn clinical signatures in an unsupervised manner.
we searched all records in our ehr archives for patients who had billing codes
from a broad set of pulmonary conditions, intending to capture pulmonary
conditions beyond just malignancy. additionally, image-ehr was a labeled dataset
with paired imaging and ehrs. we searched our institution's imaging archive for
patients with three chest cts within five years. in the ehr-image cohort,
malignant cases were labeled as those with a billing code for lung malignancy
and no cancer of any type prior. importantly, this case criteria includes
metastasis from cancer in non-lung locations. benign controls were those who did
not meet this criterion. finally, image-ehr-spn was a subset of image-ehr with
the inclusion criteria that subjects had a billing code for an spn and no cancer
of any type prior to the spn. we labeled malignant cases as those with a lung
malignancy billing code occurring within three years after any scan and only
used data collected before the lung malignancy code. all data within the
five-year period were used for controls. we removed all billing codes relating
to lung malignancy. a description of the billing codes used to define spn and
lung cancer events are provided in supplementary 1.2. training and validation.
all models were pretrained with the nlst cohort after which we froze the
convolutional embedding layer. while this was the only pretraining step for
image-only models (csimage and tdimage), the multimodal models underwent another
stage of pretraining using the image-ehr cohort with subjects from image-ehr-spn
subtracted. in this stage, we randomly selected one scan and the corresponding
clinical signature expressions for each subject and each training epoch. models
were trained until the running mean over 100 global steps of the validation loss
increased by more than 0.2. for evaluation, we performed five-fold
cross-validation with image-ehr-spn, using up to three of the most recent scans
in the longitudinal models. we report the mean auc and 95% confidence interval
from 1000 bootstrapped samples, sampling with replacement from the pooled
predictions across all test folds. a two-sided wilcoxon signed-rank test was
used to test if differences in mean auc between models were
significant.reclassification analysis. we performed a reclassification analysis
of low, medium, and high-risk tiers separated by thresholds of 0.05 and 0.65,
which are the cutoffs used to guide clinical management. given a baseline
comparison, our approach reclassifies a subject correctly if it predicts a
higher risk tier than the baseline in cases, or a lower risk tier than the
baseline in controls (fig. 2).",2
355,Centroid-Aware Feature Recalibration for Cancer Grading in Pathology Images,1.0,Introduction,"globally, cancer is a leading cause of death and the burden of cancer incidence
and mortality is rapidly growing [1]. in cancer diagnosis, treatment, and
management, pathologydriven information plays a pivotal role. cancer grade is,
in particular, one of the major factors that determine the treatment options and
life expectancy. however, the current pathology workflow is sub-optimal and
low-throughput since it is, by and large, manually conducted, and the large
volume of workloads can result in dysfunction or errors in cancer grading, which
have an adversarial effect on patient care and safety [2]. therefore, there is a
high demand to automate and expedite the current pathology workflow and to
improve the overall accuracy and robustness of cancer grading.recently, many
computational tools have shown to be effective in analyzing pathology images
[3]. these are mainly built based upon deep convolutional neural networks
(dcnns). for instance, [4] used dccns for prostate cancer detection and grading,
[5] classified gliomas into three different cancer grades, and [6] utilized an
ensemble of dcnns for breast cancer classification. to further improve the
efficiency and effectiveness of dcnns in pathology image analysis, advanced
methods that are tailored to pathology images have been proposed. for example,
[7] proposed to incorporate both local and global contexts through the
aggregation learning of multiple context blocks for colorectal cancer
classification; [8] extracted and utilized multi-scale patterns for cancer
grading in prostate and colorectal tissues; [9] proposed to re-formulate cancer
classification in pathology images as both categorical and ordinal
classification problems. built based upon a shared feature extractor, a
categorical classification branch, and an ordinal classification branch, it
simultaneously conducts both categorical and ordinal learning for colorectal and
prostate cancer grading; a hybrid method that combines dccns with hand-crafted
features was developed for mitosis detection in breast cancer [10]. moreover,
attention mechanisms have been utilized for an improved pathology image
analysis. for instance, [11] proposed a two-step framework for glioma sub-type
classification in the brain, which consists of a contrastive learning framework
for robust feature extractor training and a sparse-attention block for
meaningful multiple instance feature aggregation. such attention mechanisms have
been usually utilized in a multiple instance learning framework or as
self-attention for feature representations. to the best of our knowledge,
attention mechanisms have not been used for feature representations of class
centroids.in this study, we propose a centroid-aware feature recalibration
network (cafenet) for accurate and robust cancer grading in pathology images.
cafenet is built based upon three major components: 1) a feature extractor, 2) a
centroid update (cup) module, and 3) a centroid-aware feature recalibration
(cafe) module. the feature extractor is utilized to obtain the feature
representation of pathology images. cup module obtains and updates the centroids
of class labels, i.e., cancer grades. cafe module adjusts the input embedding
vectors with respect to the class centroids (i.e., training data distribution).
assuming that the classes are well separated in the feature space, the centroid
embedding vectors can serve as reference points to represent the data
distribution of the training data. this indicates that the centroid embedding
vectors can be used to recalibrate the input embedding vectors of pathology
images. during inference, we fix the centroid embedding vectors so that the
recalibrated embedding vectors do not vary much compared to the input embedding
vectors even though the data distribution substantially changes, leading to
improved stability and robustness of the feature representation. in this manner,
the feature representations of the input pathology images are re-calibrated and
stabilized for a reliable cancer classification. the experimental results
demonstrate that cafenet achieves the state-of-the-art cancer grading
performance in colorectal cancer grading datasets. the source code of cafenet is
available at https://github.com/col in19950703/cafenet.",2
370,Explaining Massive-Training Artificial Neural Networks in Medical Image Analysis Task Through Visualizing Functions Within the Models,3.1,Dynamic Contrast-Enhanced Liver CT,"our xai technique was applied to explain the mtann model's decision in a liver
tumor segmentation task [20]. dynamic contrast-enhanced liver ct scans
consisting of 42 patients with 194 liver tumors in the portal venous phase from
the lits database [21] were used in this study. each slice of the ct volumes in
the dataset has a matrix size of 512 × 512 pixels, with in-plane pixel sizes of
0.60-1.00 mm and thicknesses of 0.20-0.70 mm. the dataset consists of the
original hepatic ct image with the liver mask and the ""gold-standard"" liver
tumor region manually segmented by a radiologist, as illustrated in fig. 2.
firstly, to have the same physical scale on spatial coordinates, bicubic
interpolation was applied on the original hepatic ct images together with the
corresponding liver mask and ""gold-standard"" tumor segmentation to obtain
isotropic images with a voxel size of 0.60 × 0.60 × 0.60 mm 3 . then, to unify
the image size into the same size, the isotropic image was cropped to obtain the
liver region volume of interest (voi) with an in-plane matrix size of 512 × 512.
an anisotropic diffusion filter was applied to reduce the quantum noise, which
could substantially reduce the noise while major structures such as tumors and
vessels maintained [22]. finally, a z-score normalization was applied to unify
complex histograms of tumors in different cases. the final pre-processed ct
images were used as the input images.in addition, since most liver tumors' shape
is ellipsoidal, the liver tumors can also be enhanced by the hessian-based
method and utilized in the model to improve the performance [23,24]. hence, the
model consisted of these two input channels: segmented liver ct image and its
hessian-enhanced image. also, the patches were extracted from input images from
both channels: a 5 × 5 × 5 sized patch in the same spatial position was
extracted to form a training patch with a size of 2 × 5 × 5 × 5 pixels.seven
cases and 24 cases in the dynamic contrast-enhanced ct scans dataset were used
for training and testing, respectively. 10,000 patches were randomly selected
from the liver mask region in each case, summing up to a total of 70,000
training samples for training. the number of input units in the mtann model with
one hidden layer was 250. the structure optimization process started with 80
hidden units in the hidden layer. the binary cross-entropy (bce) loss function
was used to train the model. the mtann model classified the input patches into
tumor or non-tumor classes, and the output pixels represented the probability of
being a tumor class. during the structure optimization process, the f1 score on
the training patches and the dice coefficient on the training images were also
calculated as the reference to select a suitable compact model that performed
equivalently to the original large model.as observed in the four evaluation
metric curves in fig. 3, as the number of hidden units was reduced from 80 to 9,
the performance of the model fluctuated up and down, and after it was reduced
below 9, the performance of the model dropped dramatically. therefore, we chose
a number of hidden units of 9 as the optimized structure.then, we applied the
unsupervised hierarchical clustering algorithm to the weighted function maps
from the optimized compact model with 9 hidden units. figure 4 shows that the 9
hidden units are clearly divided into 3 different groups. we denote hidden units
3, 4, and 7 as group a, hidden units 2, 6, 1, and 8 as group b, and hidden units
0 and 5 as group c. the hidden units in the same group should have a similar
function, and the function maps from each group should show the function of the
group. as illustrated in fig. 5, the low-intensity areas in the function maps of
hidden units 0 and 5 in group c match the high-intensity areas in the
hessian-enhanced input image, which means they suppress the high-intensity
areas. likewise, group a enhances the liver area, and group b suppresses the
non-tumor area. we also understood that groups a and b worked together to
enhance the tumor area, and group c suppressed the liver's boundary as well as
reduced the false enhancements inside the liver. thus, our xai method was able
to reveal the learned functions of groups of neurons in the neural network,
which we call ""functional explanations"" and define as the explanations of the
model behavior by a combination of functions. our method is a post-hoc method
that offers both instance-based and model-based functional explanations.",2
379,TransLiver: A Hybrid Transformer Model for Multi-phase Liver Lesion Classification,1.0,Introduction,"liver cancer is one of the most deadly cancers and has the second highest
fatality rate [17]. focal liver lesions (flls) are the most common lesions found
in liver cancer, yet flls are challenging to diagnose because they can be either
benign lesions, such as focal nodular hyperplasia (fnh), hepatic abscess (ha),
hepatic hemangioma (hh), and hepatic cyst (hc) or malignant tumors, such as
intrahepatic cholangiocarcinoma (icc), hepatic metastases (hm), and
hepatocellular carcinoma (hcc). accurate early diagnosis of flls is thus
critical to increasing the 5-year survival rate, a task that remains challenging
as of today. dynamic contrast-enhanced ct is a common technique for liver cancer
diagnosis, where four different phases of imaging, namely, non-contrast (nc),
arterial (art), portal venous (pv), and delayed (dl) provide complementary
information about the liver. different types of flls acquired in the four phases
are shown in fig. 1. with the development of deep learning, computer-aided liver
lesion diagnosis has attracted much attention [5,8,16] in recent years. romero
et al. [16] presented an end-to-end framework based on inception-v3 and
inceptionresnet-v2 to discriminate liver lesions between cysts and malignant
tumors. heker et al. [8] combined liver segmentation and classification using
transfer learning and joint learning to increase the performance of cnn. as a
manner to elevate the accuracy of cnns, frid-adar et al. [5] designed a
gan-based network to generate synthetic liver lesion images, improving the
classification performance based on cnn. it is reported in many studies [9,18]
that using multi-phase data, like most professionals do in practice, can help
the network get a more accurate result, which also acts in liver lesion
classification [15,23,24]. yasaka et al. [24] proposed multi-channel cnn to
extract features from multi-phase liver ct by concatenation. roboh et al. [15]
proposed an algorithm based on cnns to handle 3d context in liver cts and
utilized clinical context to assist the classification. xu et al. [23]
constructed a knowledge-guided framework to integrate liver lesion features from
three phases using self-attention and fused them with a cross-feature
interaction module and a cross-lesion correlation module.a single-phase lesion
annotation means the annotation of both lesion position and its class. in
hospitals, collected multi-phase cts are normally grouped by patients rather
than lesions, which makes single-phase lesion annotation insufficient for
feature fusion learning. however, the number of lesions inside a single patient
can vary from one to dozens and they can be of different types in realistic
cases. multi-phase cts are also not co-registered in most cases, therefore, it
is necessary to make sure the lesions extracted from different phases are
somehow aligned for feature fusion, which is called as multi-phase lesion
annotation. moreover, while most works have attached much importance to liver
lesion segmentation [2], its outcome is usually organized at a single-phase
level. additional effort will be needed when consolidating segmentation and
multi-phase classification.self-attention based transformers [19] have shown
strong capability in natural language processing tasks. meanwhile, vision
transformers (vit) [4] have been shown to replace cnn with a transformer encoder
in computer vision tasks and can achieve obvious advantages on large-scale
datasets. to the best of our knowledge, we find no study using vit backbone
network in liver lesion classification. the reason for this is twofold. first,
pure vit has several limitations itself [6], including ignoring local
information within each patch, extracting only single-scale features, and
lacking inductive bias. second, no complete open liver lesion classification
datasets exist. most relevant studies are based on private datasets, which tend
to be small in size and cause overfitting in learning models.in this paper, we
construct a hybrid framework with vit backbone for liver lesion classification,
transliver. we design a pre-processing unit to reduce the annotation cost, where
we obtain lesion area on multi-phase cts from annotations marked on a single
phase. to alleviate the limitations of pure transformers, we propose a
multi-stage pyramid structure and add convolutional layers to the original
transformer encoder. we use additional cross phase tokens at the last stage to
complete a multi-phase fusion, which can focus on cross-phase communication and
improve the fusion effectiveness as compared with conventional modes. while most
multi-phase liver lesion classification studies use datasets with no more than
three phases (without dl phase for its difficulty of collection) or no more than
six lesion classes, we validate the whole framework on an in-house dataset with
four phases of abdominal ct and seven classes of liver lesions. considering the
disproportion of axial lesion slice number and the relatively small scale of the
dataset, we adopt a 2-d network in classification part instead of 3-d in
pre-processing part and achieve a 90.9% accuracy.",2
381,TransLiver: A Hybrid Transformer Model for Multi-phase Liver Lesion Classification,2.1,Pre-processing Unit,"the single-phase annotated lesion has the position and class labels in all
phases but they are not aligned, so we could have difficulty finding out which
lesions in different phases are the same with 2 or more lesions in one patient.
to reduce errors caused by unregistered data and address the situation that one
patient has multiple lesions of different types, we pre-process the multi-phase
liver cts registered and grouped by lesions.the registration network is based on
voxelmorph [1], with a u-net learning registration field and moving data
transformed by the field. we also use auxiliary dice loss function between fixed
image lesion masks and moved image lesion masks to help the registration field
learning. in [1], the network needs to specify an atlas image, otherwise, pairs
of images will be registered to each other. but in our work, we need to register
the original data in a cross-phase form. we choose an atlas phase art as
suggested by clinicians and other phases of cts are registered to the art phase
of every patient.after registration, a lesion matcher finds the same lesions in
different phases. we generate a minimum circumscribed cuboid with padding as the
lesion window for each lesion to keep the surrounding information. the windows
are then converted to 0-1 masks to calculate dice coefficient. lesions with the
maximal window dice coefficient that is no less than a set threshold are
considered the same. only lesions completely found in all phases will be used in
the following classification network.",2
385,TransLiver: A Hybrid Transformer Model for Multi-phase Liver Lesion Classification,3.1,Liver Lesion Classification,"dataset. the employed single-phase annotated dataset is collected from sir run
run shaw hospital (srrsh), affiliated with the zhejiang university school of
medicine, and has received the ethics approval of irb. the collection process
can be found in supplementary materials. the size of each ct slice is decreased
to 224×224 using cubic interpolation. after the pre-processing unit with window
dice threshold of 0.3, we screen 761 lesions from 444 patients with four phases
of cts, seven types of lesions (13.2% of hcc, 5.3% of hm, 11.3% of icc, 22.6% of
hh, 31.1% of hc, 8.7% of fnh, and 7.8% of ha), and totally 4820 slices. to
handle the imbalance of dataset, we randomly select 586 lesions as the training
and validation set with no more than 700 axial slices in each lesion type, and
the rest 175 lesions constitute the test set. lesions from the same patient are
either assigned to the training and validation set or the test set, but not
both.implementations. the training and validation set is randomly divided with a
4:1 ratio. the data is augmented by flip, rotation, crop, shift, and scale. we
initialize the backbone network using pre-trained weights of cmt-s [6]. our
models are implemented by pytorch1.12.1 and timm0.6.13 [22]. then, they are
trained on four nvidia tesla a100 gpus for 200 epochs using cross-entropy loss
function with label smoothing and sgd optimizer with learning rate warmup and
cosine annealing. the batch size is 32 and the learning rate is 1e-3. we
measured performance by precision (pre.), sensitivity (sen.), specificity
(spe.), f1-score (f1), area under the curve (auc), and accuracy (acc.).results.
we first compare the class-wise accuracy of our model against other advanced
methods applying different architectures in multi-phase liver lesion
classification with more than four lesion types [3,11,15,23,24]. transliver gets
the highest overall accuracy of 90.9% classifying the most lesion types of seven
(hcc 90.9%, hm 62.5%, icc 73.7%, hh 91.7%, hc 100.0%, fnh 100.0%, and ha 93.3%).
in the results of our method, hm has a relatively low performance of 62.5%,
mainly due to its low proportion in our dataset. the details can be found in
supplementary materials.because the sources of data are different among the
methods compared above and to the best of our knowledge, no relevant study based
on transformers was found, we further train some sota normal classification
models on our dataset. considering the fairness, all the models below are
initialized with pre-trained weights and adopt 2-d structures using the same
slice-level classification strategy. for completeness, we concatenate the
multi-phase features to execute the fusion. as illustrated in table 1, our
proposed transliver model gets better performance than other models in all
metrics. behind our model, cmt-s achieves the best performance, indicating the
effect of convolutional structures in transformer.",2
407,Explainable Image Classification with Improved Trustworthiness for Tissue Characterisation,3.0,Experiments and Analysis,"dataset. the developed explainability framework has been validated on an in vivo
and ex vivo pcle dataset of meningioma, glioblastoma and metastases of an
invasive ductal carcinoma (idc). all studies on human subjects were performed
according to the requirements of the local ethic committee and in agreement with
the declaration of helsinki (no. cle-001 nr: 2014480). the cellvizio c by mauna
kea technologies, paris, france has been used in combination with the mini laser
probe cystoflex c uhd-r. the distinguishing characteristic of the meningioma is
the psammoma body with concentric circles that show various degrees of
calcification. regarding glioblastomas, the pcle images allow for the
visualization of the characteristic hypercellularity, evidence of irregular
nuclei with mitotic activities or multinuclear appearance with irregular cell
shape. when examining metastases of an idc, the tumor presents as egg-shaped
cells with uniform evenly spaced nuclei. our dataset includes 38 meningioma
videos, 24 glioblastoma and 6 idc. each pcle video represents one tumour type
and corresponds to a different patient. the data has been curated to remove
noisy images and similar frames. this resulted in a training dataset of 2500
frames per class (7500 frames in total) and a testing dataset of the same size.
the dataset is split into a training and testing subset, with the division done
on the patient level.implementation. to implement the dl models we use the
open-source framework pytorch [25] and a nvidia geforce rtx 3090 graphics card
for parallel computation. to show our method generalises we trained two
lightweight models: resnet-18 [17] with a learning rate of 0.01 and mobilenetv2
[29] with a learning rate of 0.001. both were trained using the adam-w [22]
optimiser with a weight decay of 0.01 and dropout probability 0.1. we report the
model's top-1 accuracy for resnet18 as 94.0% and for mobilenet as 86.6%. at test
time, we set t = 100 to create a fair distribution of pa maps. pa methods were
implemented with the help of torchcam [9] and reciprocam was implemented using
the authors' source code.evaluation metrics. evaluating a pa method is not a
trivial task because a pa map may not need to be inline with what a human deems
""reasonable"" [1]. segmentation scores like intersection over union (iou) may be
used with caution to compare thresholded pa maps to ground truth maps with
annotated salient regions. by doing so, we can measure how informed the model is
about a particular class. to quantify how misinformed a model is, we can
estimate at its average drop [6]:where, x = x f s ( ŷ (x). the above equation
measures the effect on the output score of the classification model if we only
include the pixels which the pa method scored highly. a minimum average drop is
desired.as average drop was found to not be sufficient on its own, the unified
method adcc [27] was introduced which is the harmonic mean of average drop,
coherency and complexity, defined as:coherency is the pearson correlation
coefficient which ensures that the remaining pixels after dropping are still
important, defined as:where, cov(., .) is the covariance and σ is the standard
deviation. a higher coherency is better. complexity is the l1 norm of the output
pa map.complexity is used to measure how cluttered a pa map is. for a good pa
map, complexity should be a minimum. as it has been shown in the literature, the
metrics in eqs. ( 3), ( 5) and ( 6), can not be used individually to evaluate a
pa method [27]. adcc combined with computation time gives us a reliable overall
metric of how a pa method is performing. performance evaluation. the proposed
method has been compared to combinations of resnet18 and mobilenetv2 with sota
pa methods. at test time, dropout is not enabled for these standard methods, it
is only enabled for our method. in table 1, we show that our method outperforms
all the compared cnn-pa method combinations on adcc. the dropout version of
scorecam is too computationally expensive and therefore is not included in our
comparison. we believe that the better performance of our method is because of
the random dropping of features taking place during dropout at test time which
helps to suppress noise in the estimated enhanced pa map. the combination of
recipro-cam with our proposed method improves performance (increases adcc) at
the expense of increasing the computational complexity. we believe that this
could be reduced using a batched implementation of recipro-cam. we attribute
slow down in smoothgradcam++ when dropout is applied during test time to the
perturbations it adds on top of the pa method. our validation study shows that
grad-cam, grad-cam++ and recipro-cam are often leading in terms of speed as
expected from the literature. in fig. 1, we can see our proposed method reduces
noise in the pa map around the salient region. the distinguishing characteristic
of the meningioma is the psammoma body which is highlighted by all the pa
methods. risk estimations from eq. ( 2) are also displayed and provide an added
visualisation for a surgeon to trust the model. as it can be seen, areas of low
cv match the areas of high pa values which verifies the trustworthiness of our
method. we believe that the proposed explainability method could be used to
support the surgeon intraoperatively in diagnosis and decision making during
tumour resection. the enhanced pa map extracted with our method highlights the
areas which were the most important to the model's prediction. when these areas
correlate with clinically relevant areas, it shows that the model has learned to
robustly classify the different tissue classes. hence, it can be trusted by the
surgeon for diagnosis.",2
447,Localized Region Contrast for Enhancing Self-supervised Learning in Medical Image Segmentation,3.2,Fine-Tuning Datasets,"during the fine-tuning stage, we perform extensive experiments on three datasets
with respect to different regions of the human body. abd-110 is an abdomen
dataset from [25] that contains 110 ct images from patients with various
abdominal tumors and these ct images were taken during the treatment planning
stage. we report the average dsc on 11 abdominal organs (large bowel, duodenum,
spinal cord, liver, spleen, small bowel, pancreas, left kidney, right kidney,
stomach and gallbladder).thorax-85 is a thorax dataset from [5] that contains 85
thoracic ct images. we report the average dsc on 6 thoracic organs (esophagus,
trachea, spinal cord, left lung, right lung, and heart).han is from [24] and
contains 120 ct images covering the head and neck region. we report the average
dsc on 9 organs (brainstem, mandible, optical chiasm, left optical nerve, right
optical nerve, left parotid, right parotid, left submandibular gland, and right
submandibular gland).",2
463,Faithful Synthesis of Low-Dose Contrast-Enhanced Brain MRI Scans Using Noise-Preserving Conditional GANs,1.0,Introduction,"magnetic resonance imaging (mri) of the brain is an essential imaging modality
to accurately diagnose various neurological diseases ranging from inflammatory
t. pinetz and a. effland-are funded the german research foundation under
germany's excellence strategy -exc-2047/1 -390685813 and -exc2151 -390873048 and
r. haase is funded by a research grant (bonfor; o-194.0002.1). t. pinetz and e.
kobler-contributed equally to this work. lesions to brain tumors and metastases.
for accurate depictions of said pathologies, gadolinium-based contrast agents
(gbca) are injected intravenously to highlight brain-blood barrier dysfunctions.
however, these contrast agents are expensive and may cause nephrogenic systemic
fibrosis in patients with severely reduced kidney function [31]. moreover, [17]
reported that gadolinium accumulates inside patients with unclear health
consequences, especially after repeated application. the american college of
radiology recommends administering the lowest gbca dose to obtain the needed
clinical information [1].driven by this recommendation, several research groups
have recently published dose-reduction techniques focusing on maintaining image
quality. complementary to the development of higher relaxivity contrast agents
[28], virtual contrast [3,8] -replacing a large fraction of the gbca dose by
deep learninghas been proposed. these approaches typically acquire a
contrast-enhanced (ce) scan with a lower gbca dose along with non-ce scans,
e.g., t1w, t2w, flair, or adc. these input images are then processed by a deep
neural network (dnn) to replicate the corresponding standard-dose scan. while
promising, virtual contrast techniques have not been integrated into clinical
practice yet due to falsepositive signals or missed small lesions [3,23]. as
with all deep learning-based approaches, the availability of large datasets is
essential, which is problematic in the considered case since the additional ce
low-dose scan is not acquired in clinical routine exams. hence, there are no
public datasets to easily benchmark and compare different algorithms or evaluate
their performance. in general, the enhancement behavior of pathological tissues
at various gbca dosages has barely been researched due to a lack of data [12].in
recent years, generative models have been used to overcome data scarcity in the
computer vision and medical imaging community. frequently, generative
adversarial networks (gans) [9] are applied as state-of-the-art in image
generation [30] or semantic translation/interpolation [5,18,21]. in a nutshell,
the gan framework trains two competing dnns -the generator and the
discriminator. the generator learns a non-linear transformation of a predefined
noise distribution to fit the distribution of a target dataset, while the
discriminator provides feedback by simultaneously approximating a distance or
divergence between the generated and the target distribution. the choice of this
distance leads to the well-known different gan algorithms, e.g., wasserstein
gans [4,10], least squares gans [24], or non-saturating gans [9]. however, lucic
et al. [22] showed that this choice has only a minor impact on the
performance.learning conditional distributions between images can be
accomplished by additionally feeding a condition (additional scans, dose level,
etc.) into both the generator and discriminator. in particular, for
image-to-image translation tasks, these conditional gans have been successfully
applied using paired [14,25,27] and unpaired training data [35]. within these
methods, an additional content (cycle) loss typically penalizes pixel-wise
deviations (e.g., 1 ) from a corresponding reference to enforce structural
similarity, whereas a local adversarial loss (discriminator with local receptive
field) controls textural similarity. in addition, embeddings have been used to
inject metadata [7,18]. to study the gbca accumulation behavior, we collected
453 ce scans with non-standard gbca doses in the set of {10%, 20%, 33%} along
with the corresponding standard-dose (0.1 mmol/kg) scan after applying the
remaining contrast agent. using this dataset, we aim at the semantic
interpolation of the gbca signal at various fractional dose levels. to this end,
we use gans to learn the contrast enhancement behavior from the dataset
collective and thereby enable the synthesis of contrast signals at various dose
levels for individual cases. further, to minimize the smoothing effect [19] of
typical content losses (e.g., 1 or perceptual [16]), we develop a
noise-preserving content loss function based on the wasserstein distance between
paired image patches calculated using a sinkhornstyle algorithm. this novel loss
enables a faithful generation of noise, which is important for the
identification of enhancing pathologies and their usability as additional
training data.with this in mind, the contributions of this work are as
follows:-synthesis of gbca behavior at various doses using conditional gans,
-loss enabling interpolation of dose levels present in training data,
-noise-preserving content loss function to generate realistic synthetic images.",2
501,SFusion: Self-attention Based N-to-One Multimodal Fusion Block,1.0,Introduction,"people perceive the world with signals from different modalities, which often
carry complementary information about varying aspects of an object or event of
interest. therefore, collecting and utilizing multimodal information is crucial
for artificial intelligence to understand the world around us. data collected
from various sensors (e.g., microphones, cameras, motion controllers) are used
to identify human activity [4]. moreover, multimodal medical images obtained
from different scanning protocols (e.g., computed tomography, magnetic resonance
imaging) are employed for disease diagnosis [12]. satisfactory performances have
been achieved with these multimodal data.in practical application, however,
modality missing is a common scenario. wirelessly connected sensors may
occasionally disconnect and temporarily be unable to send any data [3]. medical
images may be missing due to artifacts and diverse patient conditions [11]. in
these unexpected situations, any combinatorial subset of available modalities
can be given as input. to handle this, one intuitive solution is to train a
dedicated model on all possible subsets of available modalities [6,14,23].
however, these methods are ineffective and timeconsuming. another way is to
predict missing modalities and perform with the completed modalities [20]. but,
these approaches also require additional prediction networks for each missing
situation, and the quality of the recovered data directly affects the
performance, especially when there are only a few available modalities.
recently, fusing the available modalities into a shared representation received
wide attention. however, it is particularly challenging due to the varying
number of input modalities, which results in the n-to-one fusion
problem.currently, existing fusion strategies to tackle this challenge can be
broadly grouped into three categories: the arithmetic strategy, the selection
strategy and the convolution strategy. as shown in fig. 1(a), in the arithmetic
strategy, feature representations of available modalities are merged by an
arithmetic function, such as averaging, computing the first and second moments
or other designed formulas [10,13,17]. for the selection strategy, as shown in
fig. 1(b), each value of fused representation is selected from the values at the
corresponding position of the inputs. the selection rule can be defined as max,
min or probabilitybased [2,8,19]. although the above two fusion strategies are
easily scalable to various data missing situations, their fusion operation is
hard-coded. all available modalities contribute equally and their latent
correlations are neglected. unlike hard-coding the fusion operation, in the
convolution strategy, the convolutional fusion network automatically learns how
to fuse these feature representations, which is beneficial to exploiting the
correlation between multiple modalities. however, as shown in fig. 1(c), this
fusion strategy needs a constant number of data to meet the requirements of the
input channels in the convolutional network. therefore, it has to simulate
missing data by crudely zero-padding or replacing it with similar modalities,
which inevitably introduces a bias in computation and causes performance
degradation [5,18,25].transformer has achieved success in the field of computer
vision, demonstrating that self-attention mechanism has the ability to capture
the latent correlation of image tokens. however, no work has explored the
effectiveness of self-attention mechanism on the n-to-one fusion, where n is
variable during training, rather than fixed. furthermore, the calculation of
self-attention does not require a fixed number of tokens as input, which
represents a potential for handling missing data. therefore, we propose a
self-attention based fusion block (sfusion) to tackle the problems of the above
fusion strategies. as shown in fig. 1(d), sfusion can handle any number of input
data instead of fixing its number. in addition, sfusion is a learning-based
fusion strategy that consists of two components: the correlation extraction (ce)
module and the modal attention (ma) module. in the ce module, feature
representations extracted from available modalities are projected as tokens and
fed into the self-attention layers to learn multimodal correlations. based on
these correlations, a modal softmax function is proposed to generate weight maps
in the ma module. finally, it builds a shared feature representation by fusing
the varying inputs with the weight maps.the contributions of this work are:-we
propose sfusion, which is a data-dependent fusion strategy without impersonating
missing modalities. it can learn the latent correlations between different
modalities and builds a shared representation adaptively. -the sfusion is not
limited to specific deep learning architectures. it takes inputs from any kind
of upstream processing model and serves as the input of the downstream decision
model, which enables applying the sfusion to various backbone networks for
different tasks. -we provide qualitative and quantitative performance
evaluations on activity recognition with the shl [22] dataset and brain tumor
segmentation with the brats2020 [1] dataset. the results show the superiority of
sfusion over competing fusion strategies.",2
507,SFusion: Self-attention Based N-to-One Multimodal Fusion Block,0,GFF.,"in the experiments on brain tumor segmentation, we compare sfusion with a gated
feature fusion block (gff) [5], which belongs to the convolution strategy (shown
in fig. 1(c)). as shown in fig. 4 (b), a feature disentanglement architecture is
employed. multimodal medical images are decomposed into the modality-invariant
content and the modality-specific appearance code by encoders e c and e a ,
respectively. the content codes (e.g., c 2 and c 3 , shown in fig. 4 (b)) of
missing modalities are simulated with zero values. then, all content codes are
fused into a shared representation c s by gff. given c s , the tumor
segmentation results are generated by the decoder d s . for a fair comparison,
we adopt the same encoders (e c i and e a i ) and decoders (d s and d r i ) as
used in [5]. we obtain the performance of our fusion strategy by replacing gff
with sfusion and removing the zero-padding operation. the training max_epoch is
set to 200. following [5] setting, the batch size is set to 1. adam [16] is
utilized with a learning rate of 1 × 10 -4 and progressively multiplies it by (1
-epoch / max_epoch) 0.9 . losses of l kl , l rec and l seg are employed as [5].
during training, to simulate real missing modalities scenarios, each training
patient's data is fixed to one of 15 possible missing cases. for a comprehensive
evaluation, we test the performance of all 15 cases for each test patient.our
implementations are on an nvidia rtx 3090(24g) with pytorch 1.8.1.",2
566,Unpaired Cross-Modal Interaction Learning for COVID-19 Segmentation on Limited CT Images,2.2,Knowledge Condensation and Interaction,"knowledge condensation. it is difficult to directly learn cross-modal
dependencies using the features obtained by the encoder because ct and x-ray
data were collected from different patients. this means that the data may not
have a direct correspondence between two modalities, making it challenging to
capture their relationship. as shown in fig. 1(a), we design a knowledge
condensation (kc) module by introducing a momentum-updated prototype learning
strategy to condensate valuable knowledge in each modality from the learned
features. for the x-ray modality, given its prototypes p cxr = {p cxr 1 , p cxr
2 , ..., p cxr k } initialized randomly and the feature sequence f cxr , kc
module first reduces the spatial resolution of f cxr and groups the reduced f
cxr into k prototypes by calculating the distance between each feature point and
prototypes, shown as followswhere c cxr i suggests the feature points closing to
the i-th prototype. σ(•) represents a linear projection to reduce the feature
sequence length to relieve the computational burden. then we introduce a
momentum learning function to update the prototypes with c cxr i , which means
that the updates at each iteration not only depend on the current c cxr i but
also consider the direction and magnitude of the previous updates, defined
aswhere λ is the momentum factor, which controls the influence of the previous
update on the current update. similarly, the prototypes p ct for ct modality can
be calculated and updated with the feature set f ct . the prototypes effectively
integrate the informative features of each modality and can be considered
modality-specific knowledge to improve the subsequent cross-modal interaction
learning. the momentum term allows prototypes to move more smoothly and
consistently towards the optimal position, even in the presence of noise or
other factors that might cause the prototypes to fluctuate. this can result in a
more stable learning process and more accurate prototypes, thus contributing to
condensate the knowledge of each modality better.knowledge-guided interaction.
the knowledge-guided interaction (ki) module is proposed for unpaired
cross-modality learning, which accepts the learned prototypes from one modality
and features from another modality as inputs. as shown in fig. 1(b), the ki
module contains two multi-head attention (mha) blocks. take ct features f ct and
x-ray prototypes p cxr as input example, the first block considers p cxr as the
query and reduced f ct as the key and value of the attention. it embeds the
x-ray prototypes through the calculated affinity map between f ct and p cxr ,
resulting in the adapted prototype p cxr . the first block can be seen as a
warm-up to make the prototype adapt better to the features from another
modality. the second block treats f ct as the query and the concatenation of
reduced f ct and p cxr as the key and value, improving the f ct through the
adapted prototypes. similarly, for the f cxr and p ct as inputs, the ki module
is also used to boost the x-ray representations. inspired by the knowledge
prototypes, ki modules boost the interaction between the two modalities and
allow for the learning of strong representations for covid-19 segmentation and
x-ray classification tasks.",3
568,Unpaired Cross-Modal Interaction Learning for COVID-19 Segmentation on Limited CT Images,3.1,Materials,"we used the public covid-19 segmentation benchmark [15] to verify the proposed
uci. it is collected from two public resources [5,8] on chest ct images
available on the cancer imaging archive (tcia) [4]. all ct images were acquired
without intravenous contrast enhancement from patients with positive reverse
transcription polymerase chain reaction (rt-pcr) for sars-cov-2. in total, we
used 199 ct images including 149 training images and 50 test images. we also
used two chest x-ray-based classification datasets including chestx-ray14 [18]
and chestxr [1] to assist the uci training. the chestx-ray14 dataset comprises
112,120 x-ray images showing positive cases from 30,805 patients, encompassing
14 disease image labels pertaining to thoracic and lung ailments. an image may
contain multiple or no labels. the chestxr dataset consists of 21,390 samples,
with each sample classified as healthy, pneumonia, or covid-19.",3
610,Pre-trained Diffusion Models for Plug-and-Play Medical Image Enhancement,1.0,Introduction,"computed tomography (ct) and magnetic resonance (mr) are two widely used imaging
techniques in clinical practice. ct imaging uses x-rays to produce detailed,
cross-sectional images of the body, which is particularly useful for imaging
bones and detecting certain types of cancers with fast imaging speed. however,
ct imaging has relatively high radiation doses that can pose a risk of radiation
exposure to patients. low-dose ct techniques have been developed to address this
concern by using lower doses of radiation, but the image quality is degraded
with increased noise, which may compromise diagnostic accuracy [9].mr imaging,
on the other hand, uses a strong magnetic field and radio waves to create
detailed images of the body's internal structures, which can produce
high-contrast images for soft tissues and does not involve ionizing radiation.
this makes mr imaging safer for patients, particularly for those who require
frequent or repeated scans. however, mr imaging typically has a lower resolution
than ct [18], which limits its ability to visualize small structures or
abnormalities.motivated by the aforementioned, there is a pressing need to
improve the quality of low-dose ct images and low-resolution mr images to ensure
that they provide the necessary diagnostic information. numerous algorithms have
been developed for ct and mr image enhancement, with deep learning-based methods
emerging as a prominent trend [5,14], such as using the conditional generative
adversarial network for ct image denoising [32] and convolutional neural network
for mr image super-resolution (sr) [4].these algorithms are capable of improving
image quality, but they have two significant limitations. first, paired images
are required for training, e.g., low-dose and full-dose ct images;
low-resolution and high-resolution mr images). however, acquiring such paired
data is challenging in real clinical scenarios. although it is possible to
simulate low-quality images from high-quality images, the models derived from
such data may have limited generalization ability when applied to real data
[9,14]. second, customized models are required for each task. for example, for
mr super-resolution tasks with different degradation levels (i.e., 4x and 8x
downsampling), one may need to train a customized model for each degradation
level and the trained model cannot generalize to other degradation levels.
addressing these limitations is crucial for widespread adoption in clinical
practice.recently, pre-trained diffusion models [8,11,21] have shown great
promise in the context of unsupervised natural image reconstruction [6,7,12,28].
however, their applicability to medical images has not been fully explored due
to the absence of publicly available pre-trained diffusion models tailored for
the medical imaging community. the training of diffusion models requires a
significant amount of computational resources and training images. for example,
openai's improved diffusion models [21] took 1600-16000 a100 hours to be trained
on the imagenet dataset with one million images, which is prohibitively
expensive. several studies have used diffusion models for low-dose ct denoising
[30] and mr image reconstruction [22,31], but they still rely on paired
images.in this paper, we aim at addressing the limitations of existing image
enhancement methods and the scarcity of pre-trained diffusion models for medical
images. specifically, we provide two well-trained diffusion models on full-dose
ct images and high-resolution heart mr images, suitable for a range of
applications including image generation, denoising, and super-resolution.
motivated by the existing plug-and-play image restoration methods [26,34,35] and
denoising diffusion restoration and null-space models (ddnm) [12,28], we further
introduce a paradigm for plug-and-play ct and mr image denoising and
super-resolution as shown in fig. 1. notably, it eliminates the need for paired
data, enabling greater scalability and wider applicability than existing
paired-image dependent methods. moreover, it eliminates the need to train a
customized model for each task. our method does not need additional training on
specific tasks and can directly use the single pre-trained diffusion model on
multiple medical image enhancement tasks. the pre-trained diffusion models and
pytorch code of the present method are publicly available at
https://github.com/bowang-lab/ dpm-medimgenhance.",3
644,Guiding the Guidance: A Comparative Analysis of User Guidance Signals for Interactive Segmentation of Volumetric Images,2.2,Model Backbone and Datasets,"we use the deepedit [11] model with a u-net backbone [15] and simulate a fixed
number of clicks n during training and evaluation. for each volume, n clicks are
iteratively sampled from over-and undersegmented predictions of the model as in
[16] and represented as foreground and background guidance signals. we
implemented our experiments with monai label [23] and will release our code.we
trained and evaluated all of our models on the openly available autopet [1] and
msd spleen [2] datasets. msd spleen [2] contains 41 ct volumes with voxel size
0.79×0.79×5.00mm 3 and average resolution of 512×512×89 voxels with dense
annotations of the spleen. autopet [1] consists of 1014 pet/ct volumes with
annotated tumor lesions of melanoma, lung cancer, or lymphoma. we discard the
513 tumor-free patients, leaving us with 501 volumes. we also only use pet data
for our experiments. the pet volumes have a voxel size of 2.0 × 2.0 × 2.0mm 3
and an average resolution of 400 × 400 × 352 voxels.",3
680,Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,1.0,Introduction,"lung cancer is the main cause of cancer death worldwide [18]. pulmonary nodules
and masses are both features present in computed tomography images that aid in
the diagnosis of lung cancer. the primary difference is that a nodule is smaller
than 30 mm in diameter, while a mass is larger than 30 mm [22]. early detection
of these features is crucial to aid physicians in making a diagnosis of z. li
and j. yang-equal contributions. visualization on results of four large-scale
mass segmentation given by nnu-net baseline [7]. compared with the ground-truth
segmentation, the recall rate for these four samples is 46.29%, 58.34%, 79.51%,
and 68.51%, respectively. this is significantly lower than the mean value of
81.68%. (b): statistics of the number of nodules at different scales in three
datasets. the range of nodule diameter corresponding to micro, small, medium,
and mass is (0, 10], (10,20], (20,30], [30, ∞), respectively. (c) : the
distribution of recall rate with respect to the nodule size. existing methods
have low recall rates for the segmentation of large scale nodules and
masses.benign or malignant tumors [27] and determining follow-up treatment.
lesion segmentation can be utilized to evaluate two important factors: the
volume of the lesion and its growth rate [5,6,8,12]. furthermore, obtaining
accurate information regarding the nodule can assist in determining the
appropriate resection method and surgical margin required to preserve as much
lung function as possible. [14,17].segmenting nodules is a tedious task that
requires significant human labor. computer aided diagnosis (cad) systems can
significantly reduce such heavy workloads. the accuracy of the existing nodule
detection model reaches 96.1% [9] accuracy. however, the accuracy of the 3d
nodule segmentation model is prone to significantly decline in the application,
regardless of whether its structure is based on cnn or transformer [2]. as shown
in fig. 1(a-c), the recall rate of the large-scale nodule and mass is usually
lower than the average level. the main reason is that the lesion scale in the
two public datasets are relatively small, which matches the fact few patients
have very large nodule or mass. this makes the pulmonary nodule and mass
segmentation task resemble a long-tail problem rather than a mere large scale
span problem. this leads to unsatisfactory results when segmenting large lesions
that require more accurate delineation [26].several studies have proposed
solutions to tackle the large scale span challenges at both the input and
feature level. for instance, some approaches adopt multi-scale inputs [4], where
the input images are resized to different resolu-tion ratios. some other methods
leverage multi-scale feature maps to capture information from different scales,
such as cross-scale feature fusion [19] or using multi-scale convolutional
filters [3]. furthermore, the attention mechanisms [23] has also been utilized
to emphasize the features that are more relevant for segmentation. though these
methods have achieved impressive performance, they still struggle to accurately
segment the extremely imbalanced multi-scale lesions.recently, some click-based
lesion segmentation methods [19][20][21] introduce the click at the input or
feature level and modify the network accordingly, resulting in higher accuracy
results. yet, the click input does not provide the scale information of lesions
for the network.in this paper, we propose a scale-aware test-time click
adaptation (sattca) method, which simply utilizes easily obtainable lesion click
(i.e., the center detected nodule) to adjust the parameters of the network
normalization layers [24] during testing. note that we do not need to exploit
any data from the training set. specifically, we expand the click into an
ellipsoid mask, which supervises the test-time adaptation. this helps to improve
the segmentation performance of large-scale nodules and masses. additionally, we
also propose a multi-scale input encoder to further address the problem of
imbalanced lesion scales. experimental results on two public datasets and one
in-house dataset demonstrate that the proposed method outperforms existing
methods with different backbones.",3
706,Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos,3.3,Location-Based Contrastive Loss,"note that the breast lesion locations of neighboring ultrasound video frames are
close, while the breast lesion location distance is large for different
ultrasound videos, which are often obtained from different patients. motivated
by this, we further devise a location-based contrastive loss to make the breast
lesion locations at the same video to be close, while pushing the lesion
locations of frames from different videos away. by doing so, we can enhance the
breast lesion location prediction in the localization branch. hence, we devise a
location-based contrastive loss based on a triplet loss [15], and the definition
is given by:where α is a margin that is enforced between positive and negative
pairs. h t and h t-1 are predicted heatmaps of neighboring frames from the same
video. n t denotes the heatmap of the breast lesion from a frame from another
ultrasound video. hence, the total loss l total of our network is computed
by:where g h t and g s t denote the ground truth of the breast lesion
segmentation and the breast lesion localization. we empirically set weights",3
735,Asymmetric Contour Uncertainty Estimation for Medical Image Segmentation,3.1,Data,"camus. the camus dataset [20] contains cardiac ultrasounds from 500 patients,
for which two-chamber and four-chamber sequences were acquired.manual
annotations for the endocardium and epicardium borders of the left ventricle
(lv) and the left atrium were obtained from a cardiologist for the end-diastolic
(ed) and end-systolic (es) frames. the dataset is split into 400 training
patients, 50 validation patients, and 50 testing patients. contour points were
extracted by finding the basal points of the endocardium and epicardium and then
the apex as the farthest points along the edge. each contour contains 21
points.private cardiac us. this is a proprietary multi-site multi-vendor dataset
containing 2d echocardiograms of apical two and four chambers from 890 patients.
data comes from patients diagnosed with coronary artery disease, covid, or
healthy volunteers. the dataset is split into a training/validation set (80/20)
and an independent test set from different sites, comprised of 994
echocardiograms from 684 patients and 368 echocardiograms from 206 patients,
respectively. the endocardium contour was labeled by experts who labeled a
minimum of 7 points based on anatomical landmarks and add as many other points
as necessary to define the contour. we resampled 21 points equally along the
contour.jsrt. the japanese society of radiological technology (jsrt) dataset
consists of 247 chest x-rays [26]. we used the 120 points for the lungs and
heart annotation made available by [10]. the set of points contains specific
anatomical points for each structure (4 for the right lung, 5 for the left lung,
and 4 for the heart) and equally spaced points between each anatomical point. we
reconstructed the segmentation map with 3 classes (background, lungs, heart)
with these points and used the same train-val-test split of 70%-10%-20% as [10].",3
762,HENet: Hierarchical Enhancement Network for Pulmonary Vessel Segmentation in Non-contrast CT Images,1.0,Introduction,"segmentation of the pulmonary vessels is the foundation for the clinical
diagnosis of pulmonary vascular diseases such as pulmonary embolism (pe),
pulmonary hypertension (ph) and lung cancer [9]. accurate vascular quantitative
analysis is crucial for physicians to study and apply in treatment planning, as
well as making surgical plans. although contrast-enhanced ct images have better
contrast for pulmonary vessels compared to non-contrast ct images, the
acquisition of contrast-enhanced ct images needs to inject a certain amount of
contrast agent to the patients. some patients have concerns about the possible
risk of contrast media [2]. at the same time, non-contrast ct is the most widely
used imaging modality for visualizing, diagnosing, and treating various lung
diseases.in the literature, several conventional methods [5,16] have been
proposed for the segmentation of pulmonary vessels in contrast-enhanced ct
images. most of these methods employed manual features to segment peripheral
intrapulmonary vessels. in recent years, deep learning-based methods have
emerged as promising approaches to solving challenging medical image analysis
problems and have demonstrated exciting performance in segmenting various
biological structures [10,11,15,17]. however, for vessel segmentation, the
widely used models, such as u-net and its variants, limit their segmentation
accuracy on low-contrast small vessels due to the loss of detailed information
caused by the multiple downsampling operations. accordingly, zhou et al. [17]
proposed a nested structure unet++ to redesign the skip connections for
aggregating multi-scale features and improve the segmentation quality of
varying-size objects. also, some recent methods combine convolutional neural
networks (cnns) with transformer or non-local block to address this issue
[3,6,13,18]. wang et al. [13] replaced the original skip connections with
transformer blocks to better merge the multi-scale contextual information. for
this task, cui et al. [1] also proposed an orthogonal fused u-net++ for
pulmonary peripheral vessel segmentation. however, all these methods ignored the
significant variability in hu values of pulmonary vessels at different
regions.to summarize, there exist several challenges for pulmonary vessel
segmentation in non-contrast ct images: (1) the contrast between pulmonary
vessels and background voxels is extremely low (fig. 1(c)); (2) pulmonary
vessels have a complex structure and significant variability in vessel
appearance, with different scales in different areas. the central extrapulmonary
vessels near the heart have a large irregular ball-like shape, while the shape
of the intrapulmonary vessels is delicate and tubular-like (fig. 1(a) and (b)).
vessels become thinner as they get closer to the peripheral lung; (3) hu values
of vessels in different regions vary significantly, ranging from -850 hu to 100
hu. normally, central extrapulmonary vessels have higher hu values than
peripheral intrapulmonary vessels. thus, we set different ranges of hu values to
better visualize the vessels in fig. 1(d) and(e).to address the above
challenges, we propose a h ierarchical e nhancement n etwork (henet) for
pulmonary vessel segmentation in non-contrast ct images by enhancing the
representation of vessels at both image-and feature-level. for the input ct
images, we propose an auto contrast enhancement (ace) module to automatically
adjust the range of hu values in different areas of ct images. it mimics the
radiologist in setting the window level (wl) and window width (ww) to better
enhance vessels from surrounding voxels, as shown in fig. 1(d) and (e). also, we
propose a cross-scale non-local block (csnb) to replace the skip connections in
vanilla u-net [11] structure for the aggregation of multi-scale feature maps. it
helps to form local-to-global information connections to enhance vessel
information at the feature-level, and address the complex scale variations of
pulmonary vessels.",3
776,Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels,3.1,Datasets,"qubiq is a recent challenge held at miccai 2020 and 2021, specifically designed
to evaluate the inter-rater variability in medical imaging. following [23,41],
we use qubiq 2020, which contains 7 segmentation tasks in 4 different ct and mr
datasets: prostate (55 cases, 2 tasks, 6 raters), brain growth (39 cases, 1
task, 7 raters), brain tumor (32 cases, 3 tasks, 3 raters), and kidney (24
cases, 1 task, 3 raters). for each dataset, we calculate the average dice score
between each rater and the majority votes in table 1. in some datasets, such as
brain tumor t2, the inter-rater disagreement can be quite substantial. in line
with [23], we resize all images to 256 × 256. lits contains 201 high-quality ct
scans of liver tumors. out of these, 131 cases are designated for training and
70 for testing. as the ground-truth labels for the test set are not publicly
accessible, we only use the training set. following [36], all images are resized
to 512×512 and the hu values of ct images are windowed to the range of [-60,
140]. kits includes 210 annotated ct scans of kidney tumors from different
patients. in accordance with [36], all images are resized to 512 × 512 and the
hu values of ct images are windowed to the range of [-200, 300].",3
791,Semi-supervised Domain Adaptive Medical Image Segmentation Through Consistency Regularized Disentangled Contrastive Learning,3.0,Experiments and Results,"datasets: we evaluate our work on two different da tasks to evaluate its
generalizability: (1) polyp segmentation from colonoscopy images in kvasir-seg
[11] and cvc-endoscene still [20], and (2) brain tumor segmentation in mri
images from brats2018 [16]. kvasir and cvc contain 1000 and 912 images
respectively and were split into 4 : 1 training-testing sets following [10].
brats consists of brain mris from 285 patients with t1, t2, t1ce, and flair
scans. the data was split into 4 : 1 train-test ratio, following [14].
source→target: we perform experiments on cv c → kvasir and kvasir → cv c for
polyp segmentation, and t 2 → {t 1, t 1ce, f lair} for tumor segmentation. the
ssda accesses 10 -50% and 1 -5 labels from the target domain for the two tasks,
respectively. for uda, only s is used for l sup , whereas t 1 ∪ t 2 is used for
l reg . implementation details: implementation is done in a pytorch environment
using a tesla v100 gpu with 32gb ram. we use u-net [17] backbone for the
encoder-decoder structure, and the projection heads g s and g c are shallow fc
layers. the model is trained for 300 epochs for pre-training and 500 epochs for
fine-tuning using an adam optimizer with a batch size of 4 and a learning rate
of 1e -4. λ1, λ2, λ3, and t h are set to 0.75, 0.75, 0.5, 0.6, respectively by
validation, τ, α are set to 0.07, 0.999 following [9]. augmentations include
random rotation and translation. metrics: segmentation performance is evaluated
using dice similarity score (dsc) and hausdorff distance (hd).",4
803,BerDiff: Conditional Bernoulli Diffusion Model for Medical Image Segmentation,3.1,Experimental Setup,"dataset and preprocessing. the data used in this experiment are obtained from
lidc-idri [2,7] and brats 2021 [4] datasets. lidc-idri contains 1,018 lung ct
scans with plausible segmentation masks annotated by four radiologists. we adopt
a standard preprocessing pipeline for lung ct scans and the trainvalidation-test
partition as in previous work [5,15,23]. brats 2021 consists of four different
sequence (t1, t2, flair, t1ce) mri images for each patient. all 3d scans are
sliced into axial slices and discarded the bottom 80 and top 26 slices. note
that we treat the original four types of brain tumors as one type following
previous work [25], converting the multi-target segmentation problem into
binary. our training set includes 55,174 2d images scanned from 1,126 patients,
and the test set comprises 3,991 2d images scanned from 125 patients. finally,
the sizes of images from lidc-idri and brast 2021 are resized to a resolution of
128 × 128 and 224 × 224, respectively. implementation details. we implement all
the methods with the pytorch library and train the models on nvidia v100 gpus.
all the networks are trained using the adamw [19] optimizer with a mini-batch
size of 32. the initial learning rate is set to 1 × 10 -4 for brats 2021 and 5 ×
10 -5 for lidc-idri. the bernoulli noise estimation u-net network in fig. 1 of
our berdiff is the same as previous diffusion-based models [20]. we employ a
linear noise schedule for t = 1000 timesteps for all the diffusion models. and
we use the sub-sequence sampling strategy of ddim to accelerate the segmentation
process. during minibatch training of lidc-idri, our berdiff learns diverse
expertise by randomly sampling one from four annotated segmentation masks for
each image. four metrics are used for performance evaluation, including
generalized energy distance (ged), hungarian-matched intersection over union
(hm-iou), soft-dice and dice coefficient. we compute ged using varying numbers
of segmentation samples (1, 4, 8, and 16), hm-iou and soft-dice using 16
samples.",4
825,Multimodal CT and MR Segmentation of Head and Neck Organs-at-Risk,0,Related Work.,"a literature review by zhang et al. [24] divides deep learning (dl)-based
multimodal segmentation methods into three fusion strategy groups: early, late
and hybrid (also named layer ) fusion. the first two groups of methods are most
commonly applied; early fusion comprises simple concatenation of modalities
along the channel dimension before feeding them into the deep neural network.
additionally, concatenating feature maps (fms) from separate modality encoders
can also be considered as early fusion [7]. late fusion, on the other hand,
employs separate branches for each input modality and then fuses the output
features by either plain concatenation or by weighing the contributions of
separate branches at the decision level. for example, zhang et al. [23] proposed
an attention mechanism to fuse fms from two separate u-nets that accepted
contrast-enhanced arterial and venous phase ct images. the third group, hybrid
fusion, aims to combine the strengths of early and late fusion [24] by employing
two or more separate encoders (i.e. one for each modality) and a single decoder,
where features from different resolution levels of the encoder are fused and fed
into the decoder that produces the final full-resolution segmentation. such
hybrid or multi-level fusion along with the adaptive fusion method represents
the current trend in computer vision [24], with the self-supervised model
adaptation method as a prime example [18]. one important aspect is also the
missing modality scenario, meaning that the multimodal model should produce
satisfactory results even if only one input modality is available. nevertheless,
the optimal fusion strategy remains an open question in need of further
exploration. similar conclusions were reached in a review of multimodal
segmentation methods in the medical imaging community by zhou et al. [25]. most
methods implement either early or late fusion, however, the layer fusion
strategy was identified as a better choice, since dense connections among layers
can exploit more complex and complementary information to enhance training. the
highlight is hyperdensenet, a dual-path 3d network proposed by dolz et al. [4]
that employs dense connections between two convolutional paths, and achieves
improvements compared to other fusion strategies and single modality variants.
however, other studies have shown that the best fusion strategy depends on the
specific nature of the problem, e.g. yan et al. [22] demonstrated that the late
fusion outperforms the other two approaches for the longitudinal detection of
diabetic retinopathy. relevant to the field of multimodal segmentation are also
developments on unpaired multimodal segmentation, where cross-modality learning
is employed to take advantage of different image modalities covering the same
anatomy, but without the constraint to collect images from the same patients
[5,10,19]. although the methodologies comprising cyclegans and/or multiple
segmentation networks [10,19] seem promising, they can be excessively complex
for the task of han oar segmentation where both ct and mr image modalities from
the same patient are often available. consequently, our primary focus is the
paired multimodal segmentation problem, including the missing modality
scenario.motivation. when segmenting oars in the han region for the purpose of
rt planning, a multimodal segmentation model that can leverage the information
from ct and mr images of the same patient might be beneficial compared to
separate single-modal models. firstly, as intuition suggests, such a model would
rely on the ct image for bone structures and on the mr image for soft tissues,
and therefore improve the overall segmentation quality by exploiting the
complementary information from both modalities. secondly, a multimodal model
would facilitate cross-modality learning by extracting knowledge from one and
applying that knowledge to the other modality, potentially improving the
segmentation accuracy. several studies indicated that such an approach is
feasible, for example, for improving video classification by training a model on
an auxiliary audio reconstruction task [12], or for audio-based detection by
using the multimodal knowledge distillation concept, where teacher networks
trained on rgb, depth and thermal images improve a student network trained only
on audio data [20]. finally, from the dl infrastructure maintenance perspective,
it is easier to maintain a single model that can handle both modalities than two
separate models for each modality. however, clinical practice differs
considerably from theory, meaning that a number of considerations must be taken
into account. firstly, although mr image acquisition is recommended, it is not
always feasible due to time constraints, scanner occupancy and financial
aspects. consequently, automatic oar multimodal segmentation is required to
handle the missing modality scenario, and provide a similar segmentation quality
as a single-modality system. secondly, because ct and mr images are not acquired
simultaneously and with the same acquisition parameters (e.g. resolution), there
is an inherent misalignment between both modalities. this can be mitigated with
image registration, but not completely, mainly due to different patient
positioning that especially affects the deformation of soft tissues, and various
modality-specific artifacts (e.g. motion, implants, partial volume effect,
etc.).",4
829,Multimodal CT and MR Segmentation of Head and Neck Organs-at-Risk,3.0,Experiments and Results,"image datasets. the proposed methodology was evaluated on two publicly available
datasets: our recently released han-seg dataset [14] and the pddca dataset [15].
the han-seg dataset comprises ct and t1-weighted mr images of 56 patients, which
were deformably registered with the simpleelastix registration tool, and
corresponding curated manual delineations of 30 oars (for details, please refer
to [14]). although only a subset of images is publicly available1 due to the
ongoing han-seg challenge2 , both the publicly available training as well as the
privately withheld test images were used in our 4-fold cross-validation
experiments. on the other hand, to evaluate the generalization ability of our
method, we also conducted experiments on the ct-only pddca dataset (for details,
please refer to [15]), from which we collected 15 images from the offand on-site
test sets of the corresponding challenge for our evaluation. as this dataset is
widely used for evaluating the performance of automatic han oar segmentation
methods, it serves as a valuable benchmark for comparison with other
state-of-the-art methods. note that none of the images from the ct-only pddca
dataset were used for training, and as our model expects two inputs, we
substituted the missing mr modality with an empty matrix (i.e.
zeros).implementation details. all models were trained for all oars using the 3d
fullres configuration of nnu-net, with the only modification that we reduced
rotation around the axial axis and disabled image flipping along the sagittal
plane, which eliminated segmentation errors that were previously observed for
the paired (left and right) oars. the same modification was also used with the
maml model. to ensure a fair model comparison, we set the number of filters in
the encoder of the single modality baseline model to match the number of filters
of the entry-level concatenation encoder. we also halved the number of filters
in networks that have separate encoders so that the overall number of parameters
in the proposed model and the baselines remains approximately the same
(excluding the parameters in the localization part of mfm block). note that the
maml model, which is composed of two u-nets, had a considerably higher number of
parameters. to address the challenge of a relatively small dataset, we adopted a
4-fold cross-validation strategy without using any external training images. all
models were trained until convergence, i.e. when the validation loss plateaued,
and we selected the model with the best validation loss for inference.results.
the quality of the obtained oar segmentation masks was evaluated by computing
the dice similarity coefficient (dsc) and the 95 th -percentile hausdorff
distance (hd 95 ) against reference manual delineations, and the results for all
oars are presented in figs. 2 and3, respectively. since not all images contain
all 30 oars (due to a different field-of-view), we first calculated the mean
metric for each oar and then the overall mean across all oars to ensure that the
contributions were equally weighted. we also performed analysis of statistical
significance by applying paired sample t-tests with the bonferroni correction,
presented with bars on top of the box plots (non-significant: ns (p > 0.05),
significant: * (0.01 < p < 0.05), * * (0.001 < p < 0.01), * * * (0.0001 < p <
0.001) and * * * * (p < 0.0001)).",4
844,Breast Ultrasound Tumor Classification Using a Hybrid Multitask CNN-Transformer Network,3.3,Implementation Details,"the proposed approach was implemented with keras and tensorflow libraries.all
experiments were performed on a machine with nvidia quadro rtx 8000 gpus and two
intel xeon silver 4210r cpus (2.40ghz) with 512 gb of ram. all bus images in the
dataset were zero-padded and reshaped to form square images. to avoid data
leakage and bias, we selected the train, test, and validation sets based on the
cases, i.e., the images from one case (patient) were assigned to only one of the
training, validation, and test sets. furthermore, we employed horizontal flip,
height shift (20%), width shift (20%), and rotation (20 • c) for data
augmentation. the proposed approach utilizes the building blocks of resnet50 and
swin-transformer-v2, pretrained on imagenet dataset. namely, mt-estan uses
pretrained resnet50 as a base model for the five encoder blocks (the
implementation details of mt-estan can be found in [3]). the encoder with aaa
blocks uses the swintransformer v2 base 256 pretrained model as a backbone. for
the composite loss function, we adopted a weight coefficient w 1 = 3, and in the
focal loss α = 0.5 and γ = 2. for model training we utilized adam optimizer with
a learning rate of 10 -5 and mini batch size of 4 images.",4
857,SimPLe: Similarity-Aware Propagation Learning for Weakly-Supervised Breast Cancer Segmentation in DCE-MRI,1.0,Introduction,"breast cancer is the most common cause of cancer-related deaths among women all
around the world [8]. early diagnosis and treatment is beneficial to improve the
survival rate and prognosis of breast cancer patients. mammography,
ultrasonography, and magnetic resonance imaging (mri) are routine imaging
modalities for breast examinations [15]. recent clinical studies have proven
that dynamic contrast-enhanced (dce)-mri has the capability to reflect tumor
morphology, texture, and kinetic heterogeneity [14], and is with the highest
sensitivity for breast cancer screening and diagnosis among current clinical
imaging modalities [17]. the basis for dce-mri is a dynamic t1-weighted contrast
enhanced sequence (fig. 1). t1-weighted acquisition depicts enhancing
abnormalities after contrast material administration, that is, the cancer
screening is performed by using the post-contrast images. radiologists will
analyze features such as texture, morphology, and then make the treatment plan
or prognosis assessment. computer-aided feature quantification and diagnosis
algorithms have recently been exploited to facilitate radiologists analyze
breast dce-mri [12,22], in which automatic cancer segmentation is the very first
and important step.to better support the radiologists with breast cancer
diagnosis, various segmentation algorithms have been developed [20]. early
studies focused on image processing based approaches by conducting graph-cut
segmentation [29] or analyzing low-level hand-crafted features [1,11,19]. these
methods may encounter the issue of high computational complexity when analyzing
volumetric data, and most of them require manual interactions. recently,
deep-learning-based methods have been applied to analyze breast mri. zhang et
al. [28] proposed a mask-guided hierarchical learning framework for breast tumor
segmentation via convolutional neural networks (cnns), in which breast masks
were also required to train one of cnns. this framework achieved a mean dice
value of 72% on 48 testing t1-weighted scans. li et al. [16] developed a
multi-stream fusion mechanism to analyze t1/t2-weighted scans, and obtained a
dice result of 77% on 313 subjects. gao et al. [7] proposed a 2d cnn
architecture with designed attention modules, and got a dice result of 81% on 87
testing samples. zhou et al. [30] employed a 3d affinity learning based
multi-branch ensemble network for the segmentation refinement and generated 78%
dice on 90 testing subjects. wang et al. [24] integrated a combined 2d and 3d
cnn and a contextual pyramid into u-net to obtain a dice result of 76% on 90
subjects. wang et al. [25] proposed a tumor-sensitive synthesis module to reduce
false segmentation and obtained 78% dice value. to reduce the huge annotation
burden for the segmentation task, zeng et al. [27] presented a semi-supervised
strategy to segment the manually cropped dce-mri scans, and attained a dice
value of 78%.although [27] has been proposed to alleviate the annotation effort,
to acquire the voxel-level segmentation masks is still time-consuming and
laborious, see fig. 1(c). weakly-supervised learning strategies such as extreme
points [5,21], bounding box [6] and scribbles [4] can be promising solutions.
roth et al. [21] utilized extreme points to generate scribbles to supervise the
training of the segmentation network. based on [21], dorent et al. [5]
introduced a regularized loss [4] derived from a conditional random field (crf)
formulation to encourage the prediction consistency over homogeneous regions. du
et al. [6] employed bounding boxes to train the segmentation network for organs.
however, the geometric prior used in [6] can not be an appropriate strategy for
the segmentation of lesions with various shapes. to our knowledge, currently
only one weakly-supervised work [18] has been proposed for breast mass
segmentation in dce-mri. this method employed three partial annotation methods
including single-slice, orthogonal-slice (i.e., 3 slices) and interval-slice (∼6
slices) to alleviate the annotation cost, and then constrained segmentation by
estimated volume using the partial annotation. the method obtained a dice value
of 83% using the interval-slice annotation, on a testing dataset containing only
28 patients.in this study, we propose a simple yet effective weakly-supervised
strategy, by using extreme points as annotations (see fig. 1(d)) to segment
breast cancer. specifically, we attempt to optimize the segmentation network via
the conventional trainfine-tuneretrain process. the initial training is
supervised by a contrastive loss to pull close positive voxels in feature space.
the fine-tune is conducted by using a similarity-aware propagation learning
(simple) strategy to update the pseudo-masks for the subsequent retrain. we
evaluate our method on a collected dce-mri dataset containing 206 subjects.
experimental results show our method achieves competitive performance compared
with fully supervision, demonstrating the efficacy of the proposed simple
strategy.",4
870,Uncertainty-Informed Mutual Learning for Joint Medical Image Classification and Segmentation,3.0,Experiments,"dataset and implementation. we evaluate the our uml network on two datasets
refuge [14] and ispy-1 [13]. refuge contains two tasks, classification of
glaucoma and segmentation of optic disc/cup in fundus images. the overall 1200
images were equally divided for training, validation, and testing. all images
are uniformly adjusted to 256 × 256 px. the tasks of ispy-1 are the pcr
prediction and the breast tumor segmentation. a total of 157 patients who suffer
the breast cancer are considered -43 achieve pcr and 114 non-pcr.for each case,
we cut out the slices in the 3d image and totally got 1,570 2d images, which are
randomly divided into the train, validation, and test datasets with 1,230, 170,
and 170 slices, respectively. we implement the proposed method via pytorch and
train it on nvidia geforce rtx 2080ti. the adam optimizer is adopted to update
the overall parameters with an initial learning rate 0.0001 for 100 epochs. the
scale of the regularizer is set as 1 × 10 -5 . we choose vgg-16 and res2net as
the encoders for classification and segmentation, separately.compared methods
and metrics. we compared our method with singletask methods and multi-task
methods. (1) single-task methods: (a) ec [17], (b) tbrats [31] and (c) transunet
[2]. evidential deep learning for classification (ec) first proposed to
parameterize classification probabilities as dirichlet distributions to explain
evidence. tbrats then extended ec to medical image segmentation. meriting both
transformers and u-net, transunet is a strong model for medical image
segmentation. (2) multi-task methods: (d) bcs [25] and (e) dsi [28]. the
baseline of the joint classification and segmentation framework (bcs) is a
simple but useful way to share model parameters, which utilize two different
encoders and decoders for learning respectively. the deep synergistic
interaction network (dsi) has demonstrated superior performance in joint task.
we adopt overall accuracy (acc) and f1 score (f1) as the evaluation criteria for
the classification task. dice score (di) and average symmetric surface distance
(assd) are chosen for the segmentation task. comparison under noisy data. to
further valid the reliability of our model, we introduce gaussian noise with
various levels of standard deviations (σ) to the input medical images. the
comparison results are shown in table 2. as can be observed that, the accuracy
of classification and segmentation significantly decreases after adding noise to
the raw data. however, benefiting from the uncertainty-informed guiding, our uml
consistently deliver impressive results. in fig. 3, we show the output of our
model under the noise. it is obvious that both the image-level uncertainty and
the pixel-wise uncertainty respond reasonably well to noise. these experimental
results can verify the reliability and interpre of the uncertainty guided
interaction between the classification and segmentation in the proposed uml. the
results of more qualitative comparisons can be found in the supplementary
material.ablation study. as illustrated in table 3, both of the proposed un and
ui play important roles in trusted mutual learning. the baseline method is
bcs.md represents the mutual feature decoder. it is clear that the performance
of classification and segmentation is significantly improved when we introduce
supervision of mutual features. as we thought, the introduction of un and ui
takes the reliability of the model to a higher level.",4
883,DBTrans: A Dual-Branch Vision Transformer for Multi-Modal Brain Tumor Segmentation,1.0,Introduction,"glioma is one of the most common malignant brain tumors with varying degrees of
invasiveness [1]. brain tumor semantic segmentation of gliomas based on 3d
spatially aligned magnetic resonance imaging (samm-bts) is crucial for accurate
diagnosis and treatment planning. unfortunately, radiologists suffer from
spending several hours manually performing the samm-bts task in clinical
practice, resulting in low diagnostic efficiency. in addition, manual
delineation requires doctors to have high professionalism. therefore, it is
necessary to design an efficient and accurate glioma lesion segmentation
algorithm to effectively alleviate this problem and relieve doctors' workload
and improve radiotherapy quality.with the rise of deep learning, researchers
have begun to study deep learning-based image analysis methods [2,37].
specifically, many convolutional neural network-based (cnn-based) models have
achieved promising results [3][4][5][6][7][8]. compared with natural images,
medical image segmentation often requires higher accuracy to make subsequent
treatment plans for patients. u-net reaches an outstanding performance on
medical image segmentation by combining the features from shallow and deep
layers using skipconnection [9][10][11]. based on u-net, brugger. et al. [12]
proposed a partially reversible u-net to reduce memory consumption while
maintaining acceptable segmentation results. pei et al. [13] explored the
efficiency of residual learning and designed a 3d resunet for multi-modal brain
tumor segmentation. however, due to the lack of global understanding of images
for convolution operation, cnn-based methods struggle to model the dependencies
between distant features and make full use of the contextual information [14].
but for semantic segmentation tasks whose results need to be predicted at
pixel-level or voxel-level, both local spatial details and global dependencies
are extremely important.in recent years, models based on the self-attention
mechanism, such as transformer, have received widespread attention due to their
excellent performance in natural language processing (nlp) [15]. compared with
convolution operation, the self-attention mechanism is not restricted by local
receptive fields and can capture long-range dependencies. many works
[16][17][18][19] have applied transformers to computer vision tasks and achieved
favorable results. for classification tasks, vision transformer (vit) [19] was a
groundbreaking innovation that first introduced pure transformer layers directly
across domains. and for semantic segmentation tasks, many methods, such as setr
[20] and segformer [21], use vit as the direct backbone network and combine it
with a taskspecific segmentation head for prediction results, reaching excellent
performance on some 2d natural image datasets. for 3d medical image
segmentation, vision transformer has also been preferred by researchers. a lot
of robust variants based on transformer have been designed to endow u-net with
the ability to capture contextual information in long-distance dependencies,
further improving the semantic segmentation results of medical images
[22][23][24][25][26][27]. wang et al. [25] proposed a novel framework named
transbts that embeds the transformer in the bottleneck part of a 3d u-net
structure. peiris et al. [26] introduced a 3d swin-transformer [28] to
segmentation tasks and first incorporated the attention mechanism into
skip-connection.while transformer-based models have shown effectiveness in
capturing long-range dependencies, designing a transformer architecture that
performs well on the samm-bts task remains challenging. first, modeling
relationships between 3d voxel sequences is much more difficult than 2d pixel
sequences. when applying 2d models, 3d images need to be sliced along one
dimension. however, the data in each slice is related to three views, discarding
any of them may lead to the loss of local information, which may cause the
degradation of performance [29]. second, most existing mri segmentation methods
still have difficulty capturing global interaction information while effectively
encoding local information. moreover, current methods just stack modalities and
pass them through a network, which treats each modality equally along the
channel dimension and may ignore the contribution of different modalities. to
address the above limitations, we propose a novel encoder-decoder model, namely
dbtrans, for multi-modal medical image segmentation. in the encoder, two types
of window-based attention mechanisms, i.e., shifted window-based multi-head self
attention (shifted-w-msa) and shuffle window-based multi-head cross attention
(shuffle-w-mca), are introduced and applied in parallel to dual-branch encoder
layers, while in the decoder, in addition to shifted-w-msa mechanism, shifted
window-based multi-head cross attention (shifted-w-mca) is designed for the
dual-branch decoder layers. these mechanisms in the dual-branch architecture
greatly enhance the ability of both local and global feature extraction.
notably, dbtrans is designed for 3d medical images, avoiding the information
loss caused by data slicing.the contributions of our proposed method can be
described as follows: 1) based on transformer, we construct dual-branch encoder
and decoder layers that assemble two attention mechanisms, being able to model
close-window and distant-window dependencies without any extra computational
cost. 2) in addition to the traditional skipconnection structure, in the
dual-branch decoder, we also establish an extra path to facilitate the decoding
process. we design a shifted-w-mca-based global branch to build a bridge between
the decoder and encoder, maintaining affluent information of the segmentation
target during the decoding process. 3) for the multi-modal data adopt in the
task of samm-bts, we improve the channel attention mechanism in se-net by
applying se-weights to features from both branches in the encoder and decoder
layers. by this means, we implicitly consider the importance of multiple mri
modalities and two window-based attention branches, thereby strengthening the
fusion effect of the multi-modal information from a global perspective.",4
925,Edge-Aware Multi-task Network for Integrating Quantification Segmentation and Uncertainty Prediction of Liver Tumor on Multi-modality Non-contrast MRI,1.0,Introduction,"simultaneous multi-index quantification (i.e., max diameter (md), center point
coordinates (x o , y o ), and area), segmentation, and uncertainty prediction of
liver tumor have essential significance for the prognosis and treatment of
patients [6,16]. in clinical settings, segmentation and quantitation are
manually performed by the clinicians through visually analyzing the
contrast-enhanced mri images (cemri) [9,10,18]. however, as shown in fig. 1(b),
contrast-enhanced fig. 1. our method integrates segmentation and quantification
of liver tumor using multi-modality ncmri, which has the advantages of avoiding
contrast agent injection, mutual promotion of multi-task, and reliability and
stability. mri (cemri) has the drawbacks of being toxic, expensive, and
time-consuming due to the need for contrast agents (ca) to be injected [2,4].
moreover, manually annotating medical images is a laborious and tedious process
that requires human expertise, making it manpower-intensive, subjective, and
prone to variation [14]. therefore, it is desirable to provide a reliable and
stable tool for simultaneous segmentation, quantification, and uncertainty
analysis, without requiring the use of contrast agents, as shown in fig.
1(a).recently, an increasing number of works have been attempted on liver tumor
segmentation or quantification [25,26,28,30]. as shown in fig. 1(c), the work
[26] attempted to use the t2fs for liver tumor segmentation, while it ignored
the complementary information between multi-modality ncmri of t2fs and dwi. in
particular, there is evidence that diffusion-weighted imaging (dwi) helps to
improve the detection sensitivity of focal lesions as these lesions typically
have higher cell density and microstructure heterogeneity [20]. the study in
[25,30] attempted to quantify the multi-index of liver tumor, however, the
approach is limited to using multi-phase cemri that requires the injection of
ca. in addition, all these works are limited to a single task and ignore the
constraints and mutual promotion between multi-tasks. available evidence
suggests that uncertainty information regarding segmentation results is
important as it guides clinical decisions and helps understand the reliability
of the provided segmentation. however, current research on liver tumors tends to
overlook this vital task.to the best of our knowledge, although many works focus
on the simultaneous quantization, segmentation, and uncertainty in medical
images (i.e., heart [3,5,11,27], kidney [17], polyp [13]). no attempt has been
made to automatically liver tumor multi-task via integrating multi-modality
ncmri due to the following challenges: (1) the lack of an effective
multi-modality mri fusion mechanism. because the imaging characteristics between
t2fs and dwi have significant differences (i.e., t2fs is good at anatomy
structure information while dwi is good at location information of lesions
[29]). (2) the lack of strategy for capturing the accurate boundary information
of liver tumors. due to the lack of contrast agent injection, the boundary of
the lesion may appear blurred or even invisible in a single ncmri, making it
challenging to accurately capture tumor boundaries [29]. (3) the lack of an
associated multi-task framework. because segmentation and uncertainty involve
pixel-level classification, whereas quantification tasks involve image-level
regression [11]. this makes it challenging to integrate and optimize the
complementary information between multi-tasks.in this study, we propose an
edge-aware multi-task network (eamtnet) that integrates the multi-index
quantification (i.e., center point, max-diameter (md), and area), segmentation,
and uncertainty. our basic assumption is that the model should capture the
long-range dependency of features between multimodality and enhance the boundary
information for quantification, segmentation, and uncertainty of liver tumors.
the two parallel cnn encoders first extract local feature maps of multi-modality
ncmri. meanwhile, to enhance the weight of tumor boundary information, the sobel
filters are employed to extract edge maps that are fed into edge-aware feature
aggregation (eafa) as prior knowledge. then, the eafa module is designed to
select and fuse the information of multi-modality, making our eamtnet edge-aware
by capturing the long-range dependency of features maps and edge maps. lastly,
the proposed method estimates segmentation, uncertainty prediction, and
multi-index quantification simultaneously by combining multi-task and cross-task
joint loss.the contributions of this work mainly include: (1) for the first
time, multiindex quantification, segmentation, and uncertainty of the liver
tumor on multimodality ncmri are achieved simultaneously, providing a
time-saving, reliable, and stable clinical tool. (2) the edge information
extracted by the sobel filter enhances the weight of the tumor boundary by
connecting the local feature as prior knowledge. (3) the novel eafa module makes
our eamtnet edge-aware by capturing the long-range dependency of features maps
and edge maps for feature fusion. the source code will be available on the
author's website.",4
946,Certification of Deep Learning Models for Medical Image Segmentation,1.0,Introduction,"for the past decade, deep neural networks have dominated the computer vision
community and provided near human performance on many different tasks, including
classification [18], segmentation [24], and image generation [16]. given these
impressive results, convolutional neural networks are now used on a daily basis
in fields like healthcare, self-driving cars, and robotics, to cite a few. in
medical imaging, convolutional neural networks are particularly used to segment
organs or regions of interest on different modalities such as x-rays, ct scans,
mris, or ultrasound [36]. indeed, segmentation techniques and variations of 2d
and 3d u-nets are currently the state-of-the-art to identify and isolate tumors,
blood vessels, organs, or other structures within an image and provide crucial
help to physicians for medical diagnosis, screening, and prognosis
[32].nowadays, segmentation models are gaining widespread adoption in modern
clinical practice and are being used with increasing frequency, making the
results of these models critical for many patients. however, it is now commonly
known that neural networks can be vulnerable to adversarial attacks [17,34],
i.e., small input perturbations invisible to humans crafted specifically such
that the network performs errors. over the past few years, a large body of work
has devised empirical defenses against adversarial attacks for classification
tasks [3,17,25], as well as segmentation tasks [37], including applications on
medical imaging [27]. although state-of-the-art empirical defenses provide
significant robustness, these defenses do not guarantee theoretical robustness
and stronger attacks can be crafted to break them [5]. recently, certified
defenses, for classification [2,11,26] and segmentation [15,23], have been
proposed to guarantee the accuracy and reliability of neural networks. however,
certified defenses for segmentation in the context of medical imaging are still
lacking, even if models are getting market approvals (e.g., fda, ce) and are
already adopted in clinical practice.in this paper, we provide the first method
for certified robustness in the context of segmentation for medical imaging. we
leverage the randomized smoothing strategy [11,15], and the recent work on
diffusion models [7] to achieve stateof-the-art certified robustness for
segmentation models. randomized smoothing consists in convolving the neural
network with a gaussian distribution (i.e., by adding noise to the input) in
order to obtain a smooth segmentation model. from the smoothness properties of
the segmentation model, we can derive a robustness guarantee and compute a
certified dice score. we go even further by using diffusion models to first
denoise the perturbed input and boost the certified robustness. by extension, we
show that current diffusion models, trained on 'classical images' generalize
well to medical datasets for denoising tasks. extensive experiments on five
public medical datasets of chest x-rays [21,31], skin lesions [10], and
colonoscopies [6], and different popular segmentation models, prove the
potential of our method. we hope that this study will provide the first step
towards robustness guarantees for medical image segmentation.",4
955,Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data,1.0,Introduction,"prostate segmentation from magnetic resonance imaging (mri) is a crucial step
for diagnosis and treatment planning of prostate cancer. recently, deep
learningbased approaches have greatly improved the accuracy and efficiency of
automatic prostate mri segmentation [7,8]. yet, their success usually requires a
large amount of labeled medical data, which is expensive and expertise-demanding
in practice. in this regard, semi-supervised learning (ssl) has emerged as an
attractive option as it can leverage both limited labeled data and abundant
unlabeled data [3,[9][10][11]15,16,[21][22][23][24][25][26]28]. nevertheless,
the effectiveness of ssl is heavily dependent on the quantity and quality of the
unlabeled data.regarding quantity , the abundance of unlabeled data serves as a
way to regularize the model and alleviate overfitting to the limited labeled
data. unfortunately, such ""abundance"" may be unobtainable in practice, i.e., the
local unlabeled pool is also limited due to restricted image collection
capabilities or scarce patient samples. as a specific case shown in table 1,
there are only limited prostate scans available per center. taking c1 as a case
study, if the amount of local unlabeled data is limited, existing ssl methods
may still suffer from inferior performance when generalizing to unseen test data
(fig. 1). to efficiently enrich the unlabeled pool, seeking support from other
centers is a viable solution, as illustrated in fig. 1. yet, due to differences
in imaging protocols and variations in patient demographics, this solution
usually introduces data heterogeneity, lead-ing to a quality problem. such
heterogeneity may impede the performance of ssl which typically assumes that the
distributions of labeled data and unlabeled data are independent and identically
distributed (i.i.d.) [16]. thus, proper mechanisms are called for this practical
but challenging ssl scenario.here, we define this new ssl scenario as multi-site
semi-supervised learning (ms-ssl), allowing to enrich the unlabeled pool with
multi-site heterogeneous images. being an under-explored scenario, few efforts
have been made. to our best knowledge, the most relevant work is ahdc [2].
however, it only deals with additional unlabeled data from a specific source
rather than multiple arbitrary sources. thus, it intuitively utilizes
image-level mapping to minimize dual-distribution discrepancy. yet, their
adversarial min-max optimization often leads to instability and it is difficult
to align multiple external sources with the local source using a single image
mapping network.in this work, we propose a more generalized framework called
categorylevel regularized unlabeled-to-labeled (cu2l) learning, as depicted in
fig. 2, to achieve robust ms-ssl for prostate mri segmentation. specifically,
cu2l is built upon the teacher-student architecture with customized learning
strategies for local and external unlabeled data: (i) recognizing the importance
of supervised learning in data distribution fitting (which leads to the failure
of cps [3] in ms-ssl as elaborated in sec. 3), the local unlabeled data is
involved into pseudolabel supervised-like learning to reinforce fitting of the
local data distribution; (ii) considering that intra-class variance hinders
effective ms-ssl, we introduce a non-parametric unlabeled-to-labeled learning
scheme, which takes advantage of the scarce expert labels to explicitly
constrain the prototype-propagated predictions, to help the model exploit
discriminative and domain-insensitive features from heterogeneous multi-site
data to support the local center. yet, observing that such scheme is challenging
when significant shifts and various distributions are present, we further
propose category-level regularization, which advocates prototype alignment, to
regularize the distribution of intra-class features from arbitrary external data
to be closer to the local distribution; (iii) based on the fact that
perturbations (e.g., gaussian noises [15]) can be regarded as a simulation of
heterogeneity, perturbed stability learning is incorporated to enhance the
robustness of the model. our method is evaluated on prostate mri data from six
different clinical centers and shows promising performance on tackling ms-ssl
compared to other semi-supervised methods.",4
958,Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data,2.3,Category-Level Regularized Unlabeled-to-Labeled Learning,"unlabeled-to-labeled learning. inherently, the challenge of ms-ssl stems from
intra-class variation, which results from different imaging protocols, disease
progress and patient demographics. inspired by prototypical networks [13,19,25]
that compare class prototypes with pixel features to perform segmentation, here,
we introduce a non-parametric unlabeled-to-labeled (u2l) learning scheme that
utilizes expert labels to explicitly constrain the prototype-propagated
predictions. such design is based on two considerations: (i) a good
prototypepropagated prediction requires both compact feature and discriminative
prototypes, thus enhancing this prediction can encourage the model to learn in a
variation-insensitive manner and focus on the most informative clues; (ii) using
expert labels as final guidance can prevent error propagation from pseudo
labels. specifically, we denote the feature map of the external unlabeled image
x u e before the penultimate convolution in the teacher model as f u,t e . note
that f u,t e has been upsampled to the same size of x u e via bilinear
interpolation but with l channels. with the argmax pseudo label ŷ u,t e and the
predicted probability map p u,t e , the object prototype from the external
unlabeled data can be computed via confidence-weighted masked average pooling:
c.likewise, the background prototype c u(bg) e can also be obtained. considering
the possible unbalanced sampling of prostate-containing slices, ema strategy
across training steps (with a decay rate of 0.9) is applied for prototype
update. then, as shown in fig. 2 , where we use cosine similarity for sim(•, •)
and empirically set the temperature t to 0.05 [19]. note that a similar
procedure can also be applied to the local unlabeled data x u local , and thus
we can obtain another prototype-propagated unlabeledto-labeled prediction p u2l
local for x l local . as such, given the accurate expert label y l local , the
unlabeled-to-labeled supervision can be computed as:category-level
regularization. being a challenging scheme itself, the above u2l learning can
only handle minor intra-class variation. thus, proper mechanisms are needed to
alleviate the negative impact of significant shift and multiple distributions.
specifically, we introduce category-level regularization, which advocates class
prototype alignment between local and external data, to regularize the
distribution of intra-class features from arbitrary external data to be closer
to the local one, thus reducing the difficulty of u2l learning. in u2l, we have
obtained prototypes from local unlabeled data {c where mean squared error is
adopted as the distance function d(•, •). the weight of background prototype
alignment is smaller due to less relevant contexts.",4
960,Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data,3.0,Experiments and Results,"materials. we utilize prostate t2-weighted mr images from six different clinical
centers (c1-6) [1,4,5] to perform a retrospective evaluation. rizes the
characteristics of the six data sources, following [7,8], where [7,8] also
reveal the severity of inter-center heterogeneity here through extensive
experiments. the heterogeneity comes from the differences in scanners, field
strengths, coil types, disease and in-plane/through-plane resolution. compared
to c1 and c2, scans from c3 to c6 are taken from patients with prostate cancer,
either for detection or staging purposes, which can cause inherent semantic
differences in the prostate region to further aggravate heterogeneity. following
[7,8], we crop each scan to preserve the slices with the prostate region only
and then resize and normalize it to 384 × 384 px in the axial plane with zero
mean and unit variance. we take c1 or c2 as the local target center and randomly
divide their 30 scans into 18, 3, and 9 samples as training, validation, and
test sets, respectively.implementation and evaluation metrics. the framework is
implemented on pytorch using an nvidia geforce rtx 3090 gpu. considering the
large variance in slice thickness among different centers, we adopt the 2d
architecture. specifically, 2d u-net [12] is adopted as our backbone. consists
of the cross-entropy loss and the k-regional dice loss [6]. the maximum
consistency weight w max is set to 0.1 [20,26]. t max is set to 20,000. k is
empirically set to 2. the network is trained using the sgd optimizer and the
learning rate is initialized as 0.01 and decayed by multiplication with (1.0t/t
max ) 0.9 . data augmentation is applied, including random flip and rotation. we
adopt the dice similarity coefficient (dsc) and jaccard as the evaluation
metrics and the results are the average over three runs with different
seeds.comparison study. table 2 presents the quantitative results with either c1
or c2 as the local target center, wherein only 6 or 8 local scans are annotated.
besides the supervised-only baselines, we include recent top-performing ssl
methods [2,3,11,14,15,17,20,25,26] for comparison. all methods are implemented
with the same backbone and training protocols to ensure fairness. as observed,
compared to the supervised-only baselines, our cu2l with {6, 8} local labeled
scans achieves {19.15%, 17.42%} and {9.1%, 6.44%} dsc improvements in {c1, c2},
showing its effectiveness in leveraging multi-site unlabeled data. despite the
violation of the assumption of i.i.d. data, existing ssl methods can still
benefit from the external unlabeled data to some extent compared to the results
using local data only as shown in fig. 1, revealing that the quantity of
unlabeled data has a significant impact. however, due to the lack of proper
mechanisms for learning from heterogeneous data, limited improvement can be
achieved by them, especially for cps [3] and fixmatch [14] in c2. particularly,
cps relies on cross-modal pseudo labeling which exploits all the unlabeled data
in a supervised-like fashion. we attribute its degradation to the fact that
supervised learning is crucial for distribution fitting, which supports our
motivation of performing pseudo-label learning on local unlabeled data only. as
a result, its models struggle to determine which distribution to prioritize.
meanwhile, the most relevant ahdc [2] is mediocre in ms-ssl, mainly due to the
instability of adversarial training and the difficulty of aligning multiple
distributions to the local distribution via a single image-mapping network. in
contrast, with specialized mechanisms for simultaneously learning informative
representations from multi-site data and handling heterogeneity, our cu2l
obtains the best performance over the recent ssl methods. figure 3(a) further
shows that the predictions of our method fit more accurately with the ground
truth.ablation study. to evaluate the effectiveness of each component, we
conduct an ablation study under the setting with 6 local labeled scans, as shown
in fig. 2(b). firstly, when we remove l u p l (cu2l-1), the performance drops by
{5.69% (c1), 3.05%(c2)} in dsc, showing that reinforcing confirmation on local
distribution is critical. cu2l-2 represents the removal of both l u2l and l cr ,
and it can be observed that such an unlabeled-to-labeled learning approach
combined with class-level regularization is crucial for exploring multi-site
data. if we remove l cr which accompanies with l u2l (cu2l-3), the performance
degrades, which justifies the necessity of this regularization to reduce the
difficulty of unlabeled-to-labeled learning process. cu2l-4 denotes the removal
of l u sta . as observed, such a typical stability loss [15] can further improve
the performance by introducing hand-crafted noises to enhance the robustness to
real-world heterogeneity.",4
990,Deep Probability Contour Framework for Tumour Segmentation and Dose Painting in PET Images,1.0,Introduction,"fluorodeoxyglucose positron emission tomography (pet) is widely recognized as an
essential tool in oncology [10], playing an important role in the stag-ing,
monitoring, and follow-up radiotherapy (rt) planning [2,19]. delineation of
region of interest (roi) is a crucial step in rt planning. it enables the
extraction of semi-quantitative metrics such as standardized uptake values
(suvs), which normalize pixel intensities based on patient weight and
radiotracer dose [20]. manual delineation is a time-consuming and laborious task
that is prone to poor reproducibility in medical imaging, and this is
particularly true for pet, due to its low signal-to-noise ratio and limited
spatial resolution [10]. in addition, manual delineation depends heavily on the
expert's prior knowledge, which often leads to large inter-observer and
intra-observer variations [8]. therefore, there is an urgent need for developing
accurate automatic segmentation algorithms in pet images which will reduce
expert workload, speed up rt planning while reducing intra-observer
variability.in the last decade, cnns have demonstrated remarkable achievements
in medical image segmentation tasks. this is primarily due to their ability to
learn informative hierarchical features directly from data. however, as
illustrated in [9,23], it is rather difficult for cnns to recognize the object
boundary precisely due to the information loss in the successive downsampling
layers. despite the headway made in using cnns, their applications have been
restricted to the generation of pixel-wise segmentation maps instead of smooth
contour. although cnns may yield satisfactory segmentation results, low values
of the loss function may not always indicate a meaningful segmentation. for
instance, a noisy result can create incorrect background contours and blurry
object boundaries near the edge pixels [6]. to address this, a kernel
smoothing-based probability contour (kspc) approach was proposed in our previous
work [22]. instead of a pixel-wise analysis, we assume that the true suvs come
from a smooth underlying spatial process that can be modelled by kernel
estimates. the kspc provides a surface over images that naturally produces
contour-based results rather than pixel-wise results, thus mimicking experts'
hand segmentation. however, the performance of kspc depends heavily on the
tuning parameters of bandwidth and threshold in the model, and it lacks
information from other patients.beyond tumour delineation, another important use
of functional images, such as pet images is their use for designing imrt dose
painting (dp). in particular, dose painting uses functional images to paint
optimised dose prescriptions based on the spatially varying radiation
sensitivities of tumours, thus enhancing the efficacy of tumour control [14,18].
one of the popular dp strategies is dose painting by contours (dpbc), which
assigns a homogeneous boost dose to the subregions defined by suv thresholds.
however, there is an urgent need to develop image segmentation approaches that
reproducibly and accurately identify the high recurrent-risk contours [18]. our
previously proposed kspc provides a clear framework to calculate the probability
contours of the suv values and can readily be used to define an objective
strategy for segmenting tumours into subregions based on metabolic activities,
which in turn can be used to design the imrt dp strategy.to address both tumour
delineation and corresponding dose painting challenges, we propose to combine
the expressiveness of deep cnns with the versa-tility of kspc in a unified
framework, which we call kspc-net. in the proposed kspc-net, a cnn is employed
to learn directly from the data to produce the pixel-wise bandwidth feature map
and initial segmentation map, which are used to define the tuning parameters in
the kspc module. our framework is completely automatic and differentiable. more
specifically, we use the classic unet [17] as the cnn backbone and evaluate our
kspc-net on the publicly available miccai hecktor (head and neck tumor
segmentation) challenge 2021 dataset. our proposed kspc-net yields superior
results in terms of both dice similarity scores and hausdorff distance compared
to state-of-art models. moreover, it can produce contour-based segmentation
results which provide a more accurate delineation of object edges and provide
probability contours as a byproduct, which can readily be used for dp planning.",4
994,Deep Probability Contour Framework for Tumour Segmentation and Dose Painting in PET Images,3.1,Dataset,"the dataset is from the hecktor challenge in miccai 2021 (head and neck tumor
segmentation challenge). the hecktor training dataset consists of 224 patients
diagnosed with oropharyngeal cancer [1]. for each patient, fdg-pet input images
and corresponding labels in binary description (0 s and 1 s) for the primary
gross tumour volume are provided and co-registered to a size of 144 × 144 × 144
using bounding box information encompassing the tumour. five-fold
cross-validation is used to generalize the performance of models.",4
1007,Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping,1.0,Introduction,"diffuse glioma is a common malignant tumor with highly variable prognosis across
individuals. to improve survival outcomes, many pre-operative survival
prediction methods have been proposed with success. based on the prediction
results, personalized treatment can be achieved. for instance, isensee et al.
[1] proposed a random forest model [2], which adopts the radiomics features [3]
of the brain tumor images, to predict the overall survival (os) time of diffuse
glioma patients. nie et al. [4] developed a multi-channel 3d convolutional
neural network (cnn) [5] to learn features from multimodal mr brain images and
classify os time as long or short using a support vector machine (svm) model
[6]. in [7], an end-to-end cnn-based method was presented that uses multimodal
mr brain images and clinical features such as karnofsky performance score [8] to
predict os time. in [9], an imaging phenotype and genotype based survival
prediction method (pgsp) was proposed, which integrates tumor genotype
information to enhance prediction accuracy.despite the promising results of
existing pre-operative survival prediction methods, they often overlook clinical
knowledge that could aid in improving the prediction accuracy. notably, tumor
types have been found to be strongly correlated with the prognosis of diffuse
glioma [10]. unfortunately, tumor type information is unavailable before
craniotomy. to address this limitation, we propose a new pre-operative survival
prediction method that integrates a tumor subtyping network into the survival
prediction backbone. the subtyping network is responsible for learning
tumor-type-related features from pre-operative multimodal mr brain images.
concerning the inherent issue of imbalanced tumor types in the training data
collected in clinic, a novel ordinal manifold mixup based feature augmentation
is presented and applied in the training stage of the tumor subtyping network.
unlike the original manifold mixup [11], which ignores the feature distribution
of different classes, in the proposed ordinal manifold mixup, feature
distribution of different tumor types is encouraged to be in the order of risk
grade, and the augmented features are produced between neighboring risk grades.
in this way, inconsistency between the augmented features and the corresponding
labels can be effectively reduced.our method is evaluated using pre-operative
multimodal mr brain images of 1726 diffuse glioma patients collected from
cooperation hospitals and a public dataset brats2019 [12] containing multimodal
mr brain images of 210 patients. our method achieves the highest prediction
accuracy of all state-of-the-art methods under evaluation. in addition, ablation
study further confirms the effectiveness of the proposed tumor subtyping network
and the ordinal manifold mixup.",4
1008,Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping,2.0,Methods,"diffuse glioma can be classified into three histological types: the
oligodendroglioma, the astrocytoma, and the glioblastoma [10]. the median
survival times (in months) are 119 (oligodendroglioma), 36 (astrocytoma), and 8
(glioblastoma) [13]. so the tumor types have strong correlation with the
prognosis of diffused glioma. based on this observation, we propose a new
pre-operative survival prediction method (see fig. 1). our network is composed
of two parts: the survival prediction backbone and the tumor subtyping network.
the survival prediction backbone is a deep cox proportional hazard model [14]
which takes the multimodal mr brain images of diffuse glioma patients as inputs
and predicts the corresponding risks. the tumor subtyping network is a
classification network, which classifies the patient tumor types and feeds the
learned tumor-type-related features to the backbone to enhance the survival
prediction performance.the tumor subtyping network is trained independently
before being integrated into the backbone. to solve the inherent issue of
imbalanced tumor type in the training data collected in clinic, a novel ordinal
manifold mixup based feature augmentation is applied in the training of the
tumor subtyping network. it is worth noting that the ground truth of tumor
types, which is determined after craniotomy, is available in the training data,
while for the testing data, tumor types are not required, because
tumor-type-related features can be learned from the pre-operative multimodal mr
brain images.",4
1009,Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping,2.1,The Survival Prediction Backbone,"the architecture of the survival prediction backbone, depicted in fig. 1 top,
consists of an encoder e cox with four resblocks [15], a global average pooling
layer (gap), and three fully connected (fc) layers. assume that d = {x 1 , ...,
x n } is the dataset containing pre-operative multimodal mr brain images of
diffuse glioma patients, and n is the number of patients. the backbone is
responsible for deriving features from x i to predict the risk of the patient.
moreover, after the gap of the backbone, the learned featurefrom the tumor
subtyping network (discussed later), and m is the vector dimension which is set
to 128. as f type i has strong correlation with prognosis, the performance of
the backbone can be improved. in addition, information of patient age and tumor
position is also used. to encode the tumor position, the brain is divided into 3
× 3 × 3 blocks, and the tumor position is represented by 27 binary values (0 or
1) with each value for one block. if a block contains tumors, then the
corresponding binary value is 1, otherwise is 0. the backbone is based on the
deep cox proportional hazard model, and the loss function is defined as:where h
θ (x i ) represents the risk of the i-th patient predicted by the backbone, θ
stands for the parameters of the backbone, x i is the input multimodal mr brain
images of the i-th patient, r(t i ) is the risk group at time t i , which
contains all patients who are still alive before time t i , t i is the observed
time (time of death happened) of x i , and δ i = 0/1 for censored/non-censored
patient.",4
1010,Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping,2.2,The Tumor Subtyping Network,"the tumor subtyping network has almost the same structure as the backbone.it is
responsible for learning tumor-type-related features from each input
preoperative multimodal mr brain image x i and classifying the tumor into
oligodendroglioma, astrocytoma, or glioblastoma. the cross entropy is adopted as
the loss function of the tumor subtyping network, which is defined as:where y k
i and p k i are the ground truth (0 or 1) and the prediction (probability) of
the k-th tumor type (k = 1, 2, 3) of the i-th patient, respectively. the learned
tumor-type-related feature f type i ∈ r m is fed to the survival prediction
backbone and concatenated with f cox i learned in the backbone to predict the
risk. in the in-house dataset, the proportions of the three tumor types are
20.9% (oligodendroglioma), 28.7% (astrocytoma), and 50.4% (glioblastoma), which
is consistent with the statistical report in [13]. to solve the imbalance issue
of tumor types in the training of the tumor subtyping network, a novel ordinal
manifold mixup based feature augmentation is presented.",4
1011,Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping,2.3,The Ordinal Manifold Mixup,"in the original manifold mixup [11], features and the corresponding labels are
augmented using linear interpolation on two randomly selected features (e.g., f
i and f j ). specifically, the augmented feature fi∼j and label ȳi∼j is defined
as:where y k i and y k j stand for the labels of the k-th tumor type of the i-th
and j-th patients, respectively, and λ ∈ [0, 1] is a weighting factor. for
binary classification, the original manifold mixup can effectively enhance the
network performance, however, for the classification of more than two classes,
e.g., tumor types, there exists a big issue.as shown in fig. 2 left, assume that
f i and f j are features of oligodendroglioma (green) and astrocytoma (yellow)
learned in the tumor subtyping network, respectively. the augmented feature fi∼j
(red) produced from the linear interpolation between f i and f j has the
corresponding label ȳi∼j with high probabilities for the tumor types of
oligodendroglioma and astrocytoma. however, since these is no constraint imposed
on the feature distribution of different tumor types, fi∼j could fall into the
distribution of glioblastoma (blue) as shown in fig. 2 left. in this case, fi∼j
and ȳi∼j are inconsistent, which could influence the training and degrade the
performance of the tumor subtyping network. as aforementioned, the survival time
of patients with different tumor types varies largely (oligodendroglioma >
astrocytoma > glioblastoma), so the tumor types can be regarded as risk grade,
which are ordered rather than categorical. based on this assertion and inspired
by [16], we impose an ordinal constraint on the tumor-type-related features to
make the feature distribution of different tumor types in the order of risk
grade. in this way, the manifold mixup strategy can be applied between each two
neighboring tumor types to produce augmented features with consistent labels
that reflect reasonable risk (see fig. 2",4
1012,Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping,0,right).,"normally, the feature distribution of each tumor type is assumed to be
independent normal distribution, so their joint distribution is given by:where f
k , k = 1, 2, 3 represents the feature set of oligodendroglioma, astrocytoma,
and glioblastoma, respectively, μ k and σ 2 k are mean and variance of f k . to
impose the ordinal constraint, we define the desired feature distribution of
each tumor type as n (μ 1 , σ21 ) for k = 1, and n (μ k-1 + δ k , σ2 k ) for k =
2 and 3. in this way, the feature distribution of each tumor type depends on its
predecessor, and the mean feature of each tumor type μk (except μ1 ) is equal to
the mean feature of its predecessor μk-1 shifted by δ k . note that δ k is set
to be larger than 3 × σk to ensure the desired ordering [16]. in this way, the
conditional distribution under the ordinal constraint is defined as:which can be
represented as:where μ1 and σk , k = 1, 2, 3 can be learned by the tumor
subtyping network. finally, the ordinal loss, which is in the form of kl
divergence, is defined as:in our method, μ k and σ 2 k are calculated bywhere φ
θ and g are the encoder and gap of the tumor subtyping network, respectively, θ
is the parameter set of the encoder,stands for the subset containing the
pre-operative multimodal mr brain images of the patients with the k-th tumor
type, n k is the patient number in d k . so we impose the ordinal loss l kl to
the features after the gap of the tumor subtyping network as shown in fig. 1.
since the ordinal constraint, feature distribution of different tumor types
learned in the subtyping network is encouraged to be in the ordered of risk
grade, and features can be augmented between neighboring tumor type. in this
way, inconsistency between the resulting augmented features and labels can be
effectively reduced.the tumor subtyping network is first trained before being
integrated into the survival prediction backbone. in the training stage of the
tumor subtyping network, each input batch contains pre-operative multimodal mr
brain images of n patients and can be divided into k = 3 subsets according to
their corresponding tumor types, i.e., d k , k = 1, 2, 3. with the ordinal
constrained feature distribution, high consistent features can be augmented
between neighboring tumor types. based on the original and augmented features,
the performance of the tumor subtyping network can be enhanced.once the tumor
subtyping network has been trained, it is then integrated into the survival
prediction backbone, which is trained under the constraint of the cox
proportional hazard loss l cox .",4
1013,Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping,3.0,Results,"in our experiment, both in-house and public datasets are used to evaluate our
method. specifically, the in-house dataset collected in cooperation hospitals
contains pre-operative multimodal mr images, including t1, t1 contrast enhanced
(t1c), t2, and flair, of 1726 patients (age 49.7 ± 13.1) with confirmed diffuse
glioma types. the patient number of each tumor type is 361 (oligodendroglioma),
495 (astrocytoma), and 870 (glioblastoma), respectively. in the 1726 patients,
743 have the corresponding overall survival time (dead, non-censored), and 983
patients have the last visiting time (alive, censored). besides the inhouse
dataset, a public dataset brats2019, including pre-operative multimodal mr
images of 210 non-censored patients (age 61.4 ± 12.2), is adopted as the
external independent testing dataset. all images of the in-house and brats2019
datasets go through the same pre-processing stage, including image normalization
and affine transformation to mni152 [17]. based on the tumor mask of each image,
tumor bounding boxes can be calculated. according to the bounding boxes of all
1936 patients, the size of input 3d image patch is set to 96 × 96 × 64 voxels,
which can cover the entire tumor of every patient.besides our method, four
state-of-the-art methods, including random forest based method (rf) [18], deep
convolutional survival model (deepconvsurv) [19], multi-channel survival
prediction method (mcsp) [20], and imaging phenotype and genotype based survival
prediction method (pgsp) [9], are evaluated. it is worth noting that in the rf
method, 100 decision trees and 390 handcrafted radiomics features are used. the
output of rf, mcsp, and pgsp is the overall survival (os) times in days, while
for deepconvsurv and our method, the output is the risk (deep cox proportional
hazard models). concordance index (c-index) is adopted to quantify the
prediction accuracy:where d = {x 1 , ..., x n } is the dataset containing all
patients, t i and t j are ground truth of survival times of the i-th and j-th
patients, r i and r j are the days predicted by rf, mcsp, and pgsp or risks
predicted by the deep cox proportional hazard models (i.e., deepconvsurv and our
method), 1 x<y = 1 if x < y, else 0, and δ i = 0 or 1 when the i-th patient is
censored or non-censored. as rf, mcsp, and pgsp cannot use the censored data in
the in-house dataset, 80% of the non-censored data (594 patients) are randomly
selected as the training data, and the rest 20% non-censored data (149 patients)
are for testing. while deepconvsurv and our method are deep cox models, both
censored and non-censored patients can be utilized. so besides the 80%
non-censored patients, all censored data (983 patients) are also included in the
training data.table 1 shows the evaluation results of the in-house and the
external independent (brats2019) testing datasets using all methods under
evaluation. our method achieves the highest c-index of all the methods under
evaluation. moreover, comparing with deepconvsurv, our method can improve the
prediction accuracy up to 10% (in-house) and 8% (brats2019).",4
1015,Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping,4.0,Conclusions,"we proposed a new method for pre-operative survival prediction of diffuse glioma
patients, where a tumor subtyping network is integrated into the prediction
backbone. based on the tumor subtyping network, tumor type information, which
are only available after craniotomy, can be derived from the pre-operative
multimodal mr images to boost the survival prediction performance. moreover, a
novel ordinal manifold mixup was presented, where ordinal constraint is imposed
to make feature distribution of different tumor types in the order of risk
grade, and feature augmentation only takes place between neighboring tumor
types. in this way, inconsistency between the augmented features and
corresponding labels can be effectively reduced. both in-house and public
datasets containing 1936 patients were used in the experiment. our method
outperformed the state-of-the-art methods in terms of the concordance-index.",4
1031,H-DenseFormer: An Efficient Hybrid Densely Connected Transformer for Multimodal Tumor Segmentation,3.1,Dataset and Metrics,"to validate the effectiveness of our proposed method, we performed extensive
experiments on hecktor21 [1] and pi-cai221 . hecktor21 is a dualmodality dataset
for head and neck tumor segmentation, containing 224 pet-ct image pairs. each
pet-ct pair is registered and cropped to a fixed size of (144,144,144). pi-cai22
provides multimodal mr images of 220 patients with prostate cancer, including
t2-weighted imaging (t2w), high b-value diffusion-weighted imaging (dwi), and
apparent diffusion coefficient (adc) maps. after standard resampling and center
cropping, all images have a size of (24,384,384). we randomly select 180 samples
for each dataset as the training set and the rest as the independent test set
(44 cases for hecktor21 and 40 cases for pi-cai22). specifically, the training
set is further randomly divided into five folds for cross-validation. for
quantitative analysis, we use the dice similarity coefficient (dsc), the jaccard
index (ji), and the 95% hausdorff distance (hd95) as evaluation metrics for
segmentation performance. a better segmentation will have a smaller hd95 and
larger values for dsc and ji. we also conduct holistic t-tests of the overall
performance for our method and all baseline models with the two-tailed p < 0.05.",4
1039,TransNuSeg: A Lightweight Multi-task Transformer for Nuclei Segmentation,3.0,Experiments,"dataset. we evaluated the applicability of our approach across multiple
modalities by conducting evaluations on microscopy and histology datasets. the
private dataset contains 300 images sized at 512 × 512 tessellated from 50 wsis
scanned at 20×, and meticulously labeled by five pathologists according to the
labeling guidelines of the monuseg [10]. for both datasets, we randomly split
80% of the samples on the patient level as the training set and the remaining
20% as the test set. transnuseg demonstrates superior segmentation performance
compared to its counterparts, which can successfully distinguish severely
clustered nuclei from normal edges.implementations. all experiments are
performed on one nvidia rtx 3090 gpu with 24 gb memory. we use adam optimizer
with an initial learning rate of 1 × 10 -4 . we compare transnuseg with unet
[14], unet++ [21], tran-sunet [4], swinunet [2], and ca 2.5 -net [6]. we
evaluate the results by using dice score (dsc), intersection over union (iou),
pixel-level accuracy (acc), and f1-score(f1) as metrics, and ercnt [8]. to
ensure statistical significance, we run all methods five times with different
fixed seeds and report the results as mean ± standard deviation.results. table 1
shows the quantitative comparisons for the nuclei segmentation. the large margin
between the swinunet and the other cnn-based or hybrid networks also confirms
the superiority of the transformer in fine-grained nuclei segmentation. more
importantly, our method can outperform swinunet and the previous methods on both
datasets. for example, in the histology image dataset, transnuseg improves the
dice score, f1 score, accuracy, and iou by 2.08%, 3.41%, 1.25%, and 2.70%
respectively, over the second-best models. similarly, in the fluorescence
microscopy image dataset, our proposed model improves dsc by 0.96%, while also
leading to 1.65%, 1.03% and 1.91% increment in f1 score, accuracy, and iou to
the second-best performance. for better visualization, representative samples
and their segmentation results using different methods are demonstrated in fig.
4. furthermore, table 2 compares the model complexity in terms of the number of
parameters, floating point operations per second (flops), and the training
computational cost, where our approach can significantly reduce around 28% of
the training time compared to the state-ofthe-art cnn multi-task method ca 2.5
-net, while also boosting performance. ablation. our ablation study yields that
token mlp bottleneck and attention sharing schemes can complementarily reduce
the training cost while increasing efficiency, as shown in table 2 (the last 4
rows). to further show the effectiveness of these schemes, as well as
consistency self distillation, we conduct a comprehensive ablation study on both
datasets. as described in table 3, each component proportionally contributes to
the improvement to reach the overall performance boost. moreover, self
distillation can enhance the intrinsic consistency between two branches, as
visualized in fig. 5.",4
1063,Diffusion Kinetic Model for Breast Cancer Segmentation in Incomplete DCE-MRI,1.0,Introduction,"dynamic contrast-enhanced magnetic resonance imaging (dce-mri) revealing tumor
hemodynamics information is often applied to early diagnosis and treatment of
breast cancer [1]. in particular, automatically and accurately segmenting tumor
regions in dce-mri is vital for computer-aided diagnosis (cad) and various
clinical tasks such as surgical planning. for the sake of promoting segmentation
performance, recent methods utilize the dynamic mr sequence and exploit its
temporal correlations to acquire powerful representations [2][3][4]. more
recently, a handful of approaches take advantage of hemodynamic knowledge and
time intensity curve (tic) to improve segmentation accuracy [5,6]. however, the
aforementioned methods require the complete dce-mri sequences and overlook the
difficulty in assessing complete temporal sequences and the missing time point
problem, especially post-contrast phase, due to the privacy protection and
patient conditions. hence, these breast cancer segmentation models cannot be
deployed directly in clinical practice.recently, denoising diffusion
probabilistic model (ddpm) [7,8] has produced a tremendous impact on image
generation field due to its impressive performance. diffusion model is composed
of a forward diffusion process that add noise to images, along with a reverse
generation process that generates realistic images from the noisy input [8].
based on this, several methods investigate the potential of ddpm for natural
image segmentation [9] and medical image segmentation [10][11][12].
specifically, baranchuk et al. [9] explores the intermediate activations from
the networks that perform the markov step of the reverse diffusion process and
find these activations can capture semantic information for segmentation.
however, the applicability of ddpm to medical image segmentation are still
limited. in addition, existing ddpm-based segmentation networks are generic and
are not optimized for specific applications. in particular, a core question for
dce-mri segmentation is how to optimally exploit hemodynamic priors.based on the
above observations, we innovatively consider the underlying relation between
hemodynamic response function (hrf) and denoising diffusion process (ddp). as
shown in fig. 1, during hrf process, only tumor lesions are enhanced and other
non-tumor regions remain unchanged. by designing a network architecture to
effectively transmute pre-contrast images into post-contrast images, the network
should acquire hemodynamic inherent in hrf that can be used to improve
segmentation performance. inspired by the fact that ddpm generates images from
noisy input provided by the parameterized gaussian process, this work aims to
exploit implicit hemodynamic information by a diffusion process that predict
post-contrast images from noisy pre-contrast images. specifically, given the
pre-contrast and post-contrast images, the latent kinetic code is learned using
a score function of ddpm, which contains sufficient hemodynamic characteristics
to facilitate segmentation performance.once the diffusion module is pretrained,
the latent kinetic code can be easily generated with only pre-contrast images,
which is fed into a segmentation module to annotate cancers. to verify the
effectiveness of the latent kinetic code, the sm adopts a simple u-net-like
structure, with an encoder to simultaneously conduct semantic feature encoding
and kinetic code fusion, along with a decoder to obtain voxel-level
classification. in this manner, our latent kinetic code can be interpreted to
provide tic information and hemodynamic characteristics for accurate cancer
segmentation.we verify the effectiveness of our proposed diffusion kinetic model
(dkm) on dce-mri-based breast cancer segmentation using breast-mri-nact-pilot
dataset [13]. compared to the existing state-of-the-art approaches with complete
sequences, our method yields higher segmentation performance even with
precontrast images. in summary, the main contributions of this work are listed
as follows:• we propose a diffusion kinetic model that implicitly exploits
hemodynamic priors in dce-mri and effectively generates high-quality
segmentation maps only requiring pre-contrast images. • we first consider the
underlying relation between hemodynamic response function and denoising
diffusion process and provide a ddpm-based solution to capture a latent kinetic
code for hemodynamic knowledge. • compared to the existing approaches with
complete sequences, the proposed method yields higher cancer segmentation
performance even with pre-contrast images.",4
1068,Diffusion Kinetic Model for Breast Cancer Segmentation in Incomplete DCE-MRI,3.0,Experiments,"dataset: to demonstrate the effectiveness of our proposed dkm, we evaluate our
method on 4d dce-mri breast cancer segmentation using the breast-mri-nact-pilot
dataset [13], which contains a total of 64 patients with the contrastenhanced
mri protocol: a pre-contrast scan, followed by 2 consecutive postcontrast time
points (as shown in fig. 3). each mr volume consists of 60 slices and the size
of each slice is 256 × 256. regarding preprocessing, we conduct zeromean
unit-variance intensity normalization for the whole volume. we divided the
original dataset into training (70%) and test set (30%) based on the scans.
ground truth segmentations of the data are provided in the dataset for tumor
annotation. no data augmentation techniques are used to ensure fairness.",4
1084,EoFormer: Edge-Oriented Transformer for Brain Tumor Segmentation,3.1,Dataset and Evaluation Metric,"in order to validate the performance of eoformer, we conduct extensive
experiments on both the publicly available brats 2020 dataset and a private
medulloblastoma segmentation dataset (medseg). the brats 2020 dataset [14]
consists of mri image data from 369 patients, with each patient having four
modalities (t1, t1ce, t2 and t2-flair) of skull-striped mri, which are aligned
to a standard brain template. the training/validation/test split follows
315/16/37 according to recent works [10,23].the medseg dataset includes mri
images of t1, t1ce, t2, and t2 flair modalities from 255 patients with
medulloblastoma. the dataset includes manual annotations of the wt and et
regions. these annotated masks are reviewed by two experienced physicians to
ensure the accuracy of the annotated results. the images are registered to the
size of 24 × 256 × 256. the training/validation/test split ratio is 3:1:1.
four-fold cross-validation is performed on this dataset.",4
1089,EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,1.0,Introduction,"medical image analysis has greatly benefited from advances in ai [1] yet some
improvements still remain to be addressed, importantly in areas that allow both
algorithmic performance and fairness [2], and in certain medical applications
that promise to significantly lessen morbidity and mortality. early detection of
skin lesions is such an endeavor as it can aid in identifying infectious
diseases with cutaneous manifestations. lyme disease is an example of that with
a potentially diagnostic skin lesion [3]-which is caused by the bacterium
borrelia burgdorferi and leads to nearly 476,000 cases per annum during
2010-2018 [4]. the earliest and most treatable phase of lyme disease is
manifested via a red concentric lesion at the site of a tick bite, called
erythema migrans (em) [5]. while the em pattern may appear simple to recognize,
its diagnosis can be challenging for those with or without a medical background
alike, as only 20% of united states patients have the stereotypical bull's eye
lesion [6]. when skin lesions are atypical they can be mistaken for other
diseases such as tinea corporis (tc) or herpes zoster (hz), two other diseases
acting as confusers for lyme, considered herein. this has increased interest in
medical applications of deep learning (dl), and using deep convolutional neural
networks (cnns), to assist clinicians in timely and accurate diagnosis of
conditions including lyme disease, tc and hz [7][8][9].one important diagnosis
task is to segment lyme lesion, particularly the em pattern, from benign skins.
such dl-assisted segmentation not only helps clinicians in pre-screening
patients but also improves downstream tasks such as lesion classification.
however, while lyme disease lesion segmentation is intuitively simple, it is
challenging due to the following reasons. first, there lacks of a well-segmented
dataset with manual labels on lyme disease. on one hand, some datasets-such as
ham10000 [10] and isbi challenges [11]-have manual annotated segmentations for
diseases like melanoma, but they do not have lyme disease lesions. on the other
hand, some datasets-such as groh et al. [12]-have lyme disease and skin tone and
classification labels, but not segmentation.second, the segmentation of lyme
lesion is itself challenging due to the nature of em pattern. specifically, a
typical lyme lesion exhibits a bull's eye pattern with one central redness and
one outer circle, which is different from darkness lesion in cancer-related skin
disease like melanoma. furthermore, clinical data collected for training is
usually imbalanced in some properties, e.g., more samples with light skins
compared with dark skins. therefore, existing skin disease segmentation [13] as
well as existing general segmentation works, such as u-net [14], polar training
[15], vit-adapter [16], and mfsnet [17], usually suffer from relatively low
performance and reduced fairness [2,18,19].in this paper, we present the first
lyme disease dataset that contains labeled segmentation and skin tones. our lyme
disease dataset contains two parts: (i) a classification dataset, composed of
more than 3,000 diseased skin images that are either obtained from public
resources or clinicians with patient-informed consent, and (ii) a segmentation
dataset containing 185 samples that are manually annotated for three
regions-i.e., background, skin (light vs. dark), and lesionconducted under
clinician supervision and institutional review boards (irb) approval. our
dataset with manual labels is available at this url [20].secondly, we design a
simple yet novel data preprocessing and alternation method, called edgemixup, to
improve lyme disease segmentation and diagnosis fairness on samples with
different skin-tones. the key insight is to alter a skin image with a linear
combination of the source image and a detected lesion boundary so that the
lesion structure is preserved while minimizing skin tone information. such an
improvement is an iterative process that gradually improves lesion edge
detection and segmentation fairness until convergence. then, the detected,
converged edge in the first step also helps classification of lyme diseases via
mixup with improved fairness. our source code is available at this url [20].we
evaluate edgemixup for skin disease segmentation and classification tasks. our
results show that edgemixup is able to increase segmentation utility and improve
fairness. we also show that the improved segmentation further improves
classification fairness as well as joint fairness-utility metrics compared to
existing debiasing methods, e.g., ad [21] and st-debias [22].",4
1102,EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,4.0,Datasets,"we present two datasets: (i) a dataset collected and annotated by us (called
skin), and (ii) a subset of sd-198 [23] with our annotation (called sd-sub).
first, we collect and annotate a dataset with 3,027 images containing three
types of disease/lesions, i.e., tinea corporis (tc), herpes zoster (hz), and
erythema migrans (em). all skin images are either collected from publicly
available sources or from clinicians with patient informed consent. then, a
medical technician and a clinician in our team manually annotate each image. for
the segmentation task, we annotate skin images into three classes: background,
skin, and lesion; then, for the classification task, we annotate skin images by
classifying them into four classes: no disease (no), tc, hz, and em. we name it
as skin-class for later reference. second, we select five classes from sd-198
[23], a benchmark dataset for skin disease classification, as another dataset
for both segmentation and classification tasks. note that due to the amount of
manual work involved in annotation, we select those classes based on the number
of samples in each class. the selected classes are dermatofibroma (df),
keratoacanthoma (ka), pyogenic granuloma (pg), tinea corporis (tc), and tinea
faciale (tf). we choose 30 samples in each class for segmentation task, and we
split them into 0.7, 0.1, and 0.2 ratio for training, validation, and testing,
respectively.table 1 show the characteristics of these two datasets for both
classification and segmentation tasks broken down by the disease type and skin
tone, as calculated by the individual typology angle (ita) [24]. specifically,
we consider tan2, tan1, and dark as dark skin (ds) and others as light skin
(ls). compared to other skin tone classification schemas such as fitzpartick
scale [25], we divide ita scores into more detailed categories (eight). one
prominent observation is that ls images are more abundant than ds images due to
a disparity in the availability of ds imagery found from either public sources
or from clinicians with patient consent.",4
1111,Factor Space and Spectrum for Medical Hyperspectral Image Segmentation,3.1,Experimental Setup,"we conducted experiments on the public multi-dimensional choledoch (mdc) dataset
[31] with 538 scenes and hyperspectral gastric carcinoma (hgc) dataset [33]
(data provided by the author) with 414 scenes, both with highquality labels for
binary mhsi segmentation tasks. these mhsis are collected by hyperspectral
system with an objective lens of 20x, and wavelengths from 550 nm to 1000 nm for
mdc and 450 nm to 750 nm for hgc, resulting in 60 and 40 spectral bands for each
scene. the size of a single band image in mdc and hgc are both resized to 256 ×
320. following [23,27], we partition the datasets into training, validation, and
test sets using a patient-centric hard split approach with a ratio of 3:1:1.
specifically, each patient's data is allocated entirely to one of the three
sets, ensuring that the same patient's data do not appear in multiple sets.we
use data augmentation techniques such as rotation and flipping, and train with
an adam optimizer using a combination of dice loss and cross-entropy loss for 8
batch size and 100 epochs. the segmentation performance is evaluated using
dice-sørensen coefficient (dsc), intersection of union (iou), and hausdorff
distance (hd) metrics, and throughput (images/s) is reported for comparison.
pytorch framework and four nvidia geforce rtx 3090 are used for implementation.",4
1115,Graph-Theoretic Automatic Lesion Tracking and Detection of Patterns of Lesion Changes in Longitudinal CT Studies,1.0,Introduction,"the periodic acquisition and analysis of volumetric ct and mri scans of oncology
patients is essential for the evaluation of the disease status, the selection of
the treatment, and the response to treatment. currently, scans are acquired
every 2-12 months according to the patient's characteristics, disease stage, and
treatment regime. the scan interpretation consists of identifying lesions
(primary tumors, metastases) in the affected organs and characterizing their
changes over time. lesion changes include changes in the size of existing
lesions, the appearance of new lesions, the disappearance of existing lesions,
and complex lesion changes, e.g., the formation of conglomerate lesions. as
treatments improve and patients live longer, the number of scans in longitudinal
studies increases and their interpretation is more challenging and
time-consuming.radiological follow-up requires the quantitative analysis of
lesions and patterns of lesion changes in subsequent scans. it differs from
diagnostic reading since the goal is to find and quantify the differences
between the scans, rather than to find abnormalities in a single scan. in
current practice, quantification of lesion changes is partial and approximate.
the recist 1.1 guidelines call for finding new lesions (if any), identifying up
to the five largest lesions in each scan in the ct slice where they appear
largest, manually measuring their diameters, and comparing their difference [1].
while volumetric measures of individual lesions and of all lesions (tumor
burden) have long been established as more accurate and reliable than partial
linear measurements, they are not used clinically because they require manual
lesion delineation and lesion matching in unregistered scans, which is usually
time-consuming and subject to variability [2].in a previous paper, we presented
an automatic pipeline for the detection and quantification of lesion changes in
pairs of ct liver scans [3]. this paper describes a graph-based lesion tracking
method for the comprehensive analysis of lesion changes and their patterns at
the lesion level. the tasks are formalized as graph-theoretic problems (fig. 1).
complex lesion changes include merged lesions, which occurs when at least two
lesions grow and merge into one (possible disease progression), split lesions,
which occurs when a lesion shrinks and cleaves into several parts (possible
response to treatment) and conglomeration of lesions, which occurs when clusters
of lesions coalesce. while some of these lesion changes have been observed [4],
they have been poorly studied. comprehensive quantitative analysis of lesion
changes and patterns is of clinical importance, since response to treatment may
vary among lesions, so the analysis of a few lesions may not be
representative.the novelties of this paper are: 1) identification and
formalization of longitudinal lesion matching and patterns of lesion changes in
ct in a graph-theoretic framework; 2) new classification and detection of
changes of individual lesions and lesion patterns based on the properties of the
lesion changes graph and its connected components; 3) a simultaneous lesion
matching method with more than two scans; 4) graph-based methods for the
detection of changes in individual lesions and patterns of lesion changes.
experimental results on lung (83 cts, 19 patients) and liver (77 cects, 18
patients) datasets show that our method yields high classification accuracy.to
the best of our knowledge, ours is the first method to perform longitudinal
lesion matching and lesion changes pattern detection. only a few papers address
lesion matching in pairs of ct/mri scans [5][6][7][8][9][10][11][12][13] -none
performs simultaneous matching of all lesions in more than two scans. also, very
few methods [3,14] handle matching of split/merged lesions. although many
methods exist for object tracking in optical images and videos [15][16][17],
they are unsuited for analyzing lesion changes since they assume many
consecutive 2d images where objects have very similar appearance and undergo
small changes between images. overlap-based methods pair two lesions in
registered scans when their segmentations overlap, with a reported accuracy of
66-98% [3,[5][6][7][8][9][10][11]18]. these methods assume that organs and
lesions undergo minor changes, are very sensitive to registration errors, and
cannot handle complex lesion changes. similarity-based methods pair two lesions
with similar features, e.g., intensity, shape, location [13][14][15][16] with an
84-96% accuracy on the deeplesion dataset [14]. they are susceptible to major
changes in the lesion appearance and do not handle complex lesion changes.
split-andmerge matching methods are used for cell tracking in fluorescence
microscopy [19]. they are limited to 2d images, assume registration between
images, and do not handle conglomerate changes.",5
1117,Graph-Theoretic Automatic Lesion Tracking and Detection of Patterns of Lesion Changes in Longitudinal CT Studies,2.1,Problem Formalization,"let s = s 1 , . . . , s n be a series of n ≥ 2 consecutive patient scans
acquired at timesis a set of vertices v i j corresponding to the lesions
associated with the lesion segmentation masks l i = l i 1 , l i 2 , . . . , l i
n i , where n i ≥ 0 is the number of lesions in scan s i at time t i . by
definition, any two lesionsj,l indicates that the lesions corresponding to
vertices v i j , v k l are the same lesion, i.e., that the lesion appears in
scans s i , s k in the same location. edges of consecutive scans s i , s i+1 are
called consecutive edges; edges of non-consecutive scans, s i , s k , i < k -1,
are called non-consecutive edges. the in-and out-degree of a vertex v i j , d in
(v i j ) and d out (v i j ), are the number of incoming and outcoming edges,
respectively.let cc = {cc m } m m=1 be the set of connected components of the
undirected graph version of g, where m is the number of connected components
andby definition, for each 1 ≤ m ≤ m , the sets v m , e m are mutually disjoint
and their unions are v , e, respectively. in a connected component cc m , there
is an undirected path between any two vertices v i j , v k l consisting of a
sequence of undirected edges in e m . . in this setup, connected components
correspond to matched lesions and their pattern of evolution over time (fig.
1d).we define seven mutually exclusive individual lesion change labels for
lesion v i j in scan s i based on the vertex in-and out-degrees (fig. 2). in the
following definitions we refer to the indices: 1 ≤ k < i < l ≤ n ; 1) lone: a
lesion present in scan s i and absent in all previous scans s k and subsequent
scans s l ; 2) new: a lesion present in scan s i and absent in all previous
scans s k ; 3) disappeared: a lesion present in scan s i and absent in all
subsequent scans s l ; 4) unique: a lesion present in scan s i and present as a
single lesion in a previous scan s k and/or in a subsequent scan s l ; 5)
merged: a lesion present in scan s i and present as two or more lesions in a
previous scan s k ; 6) split: a lesion present in scan s i and present as two or
more lesions in a subsequent scan s l ; 7) complex: a lesion present as two or
more lesions in at least one previous scan s k and at least one subsequent scan
s l . we also define as existing a lesion present in scan s i and present in at
least one previous scan s k and one subsequent scan s l , (d in (v i j ) ≥ 1, d
out (v i j ) ≥ 1). for the first and current scans s 1 and s n , we set d in (v
1 j ) = 1, d out (v n j ) = 1, i.e., the lesion existed before the first scan or
remains after the last scan. thus, lesions in the first (last) scan can only be
unique, disappeared or split (unique, new or merged). finally, when lesion v i j
is merged and d out (v i j ) = 0, i < n , it is also labeled disappeared; when
it is split and d in (v i j ) = 0, i > 1, it is also labeled new. we define five
patterns of lesion changes based on the properties of the connected components
cc m of g and on the labels of lesion changes: 1) single_p: a connected
component cc m = v i j consisting of a single lesion labeled as lone, new,
disappeared; 2) linear_p: a connected component consisting of a single earliest
vertex v the changes in individual lesions and the detection and classification
of patterns of lesion changes consist of constructing a graph whose vertices are
the corresponding lesion in the scans, computing the graph consecutive and
non-consecutive edges that correspond to lesion matchings, computing the
connected components of the resulting graph, and assigning an individual lesion
change label to each vertex and a lesion change pattern label to each connected
component according to the categories above.",5
1119,Graph-Theoretic Automatic Lesion Tracking and Detection of Patterns of Lesion Changes in Longitudinal CT Studies,3.0,Experimental Results,"we evaluated our method with two studies on retrospectively collected patient
datasets that were manually annotated by an expert radiologist.dataset: lung and
liver ct studies were retrospectively obtained from two medical centers
(hadassah univ hosp jerusalem israel) during the routine clinical examination of
patients with metastatic disease. each patient study consists of at least 3
scans.dlung consists of 83 chest ct scans from 19 patients with a mean 4.4 ± 2.0
scans/patient, a mean time interval between consecutive scans of 125.9 ± 81.3
days, and voxel sizes of 0.6-1.0 × 0.6-1.0 × 1.0-3.0 mm 3 . dliver consists of
77 abdominal cect scans from 18 patients with a mean 4.3 ± 2.0 scans/patient, a
mean time interval between consecutive scans of 109.7 ± 93.5 days, and voxel
sizes of 0.6-1.0 × 0.6-1.0 × 0.8-5.0 mm 3 .lesions in both datasets were
annotated by an expert radiologist, yielding a total of 1,178 lung and 800 liver
lesions, with a mean of 14.2 ± 19.1 and 10.4 ± 7.9 lesions/scan (lesions with
<20 voxels were excluded). ground-truth lesion matching graphs and lesion
changes labeling were produced by running the method on the datasets and then
having the radiologist review and correct the resulting node labels and
edges.study 1: lesion changes labeling, lesion matching, evaluation of patterns
of lesion changes. we ran our method on the dlungs and dliver lesion
segmentations. the settings of the parameters were: dilation distance d = 1 mm,
overlap percentage p = 10%, number of iterations r = 5 and 7, and centroid
maximum distance δ = 17 and 23 mm for the lungs and liver lesions,
respectively.we compared the computed and ground-truth lesion changes graphs
with two metrics: 1) lesion changes classification accuracy, which is the % of
correct computed labels from the ground truth labels; 2) lesion matching
precision and recall based on the presence/absence of computed vs. ground truth
edges. the precision and recall definitions were adapted so that wrong or missed
non-consecutive edges are counted as true positive when there is a path between
their vertices in either the ground-truth or the computed graph. table 1
summarizes the results. the distribution of lesion changes labels for dlungs
(1,178 lesions) is unique 785 (67%), new 215 (18%), lone 109 (9%), disappeared
51 (4%), merged 12 (1%), split 6 (1%), complex 0 (0%) with class accuracy ≥ 96%
for all except split (66%). for dliver (800 lesions) it is unique 450 (56%), new
185 (23%), lone 45 (6%), disappeared 77 (10%), merged 27 (3%), split 18 (2%),
complex 1 (0.05%) with class accuracy ≥ 81% for all except disappeared (71%) and
split (67%).for the patterns of lesion changes, we compared the computed and
ground truth patterns of lesion changes. the accuracy is the % of identical
connected components in each category. table 1 summarizes the results. note that
the split_p, merged_p and complex_p patterns jointly account for 3% and 8% of
the cases. these patterns are hard to detect manually but their correct
classification and tracking are crucial for the proper application of the recist
1.1 follow-up protocol [1]. study 2: detection of missed lesions in the ground
truth. the expert radiologist was asked to examine non-consecutive edges and
lesions labeled as lone in the lesion changes graph and determine if lesions
were unseen or undetected (actual or presumed false negative) in the skipped or
contiguous scans (fig. 1d). for each non-consecutive edge connecting lesions v i
j , v k l , he analyzed the corresponding region in the skipped scans s j at t j
∈ ]t i , t k [ for possible missed lesions. for the dlungs dataset, 25 visible
and 5 faintly visible or surmised to be present unmarked lesions were found for
27 nonconsecutive edges. for the dliver dataset, 20 visible and 21 faintly
visible or surmised to be present unmarked lesions were found for 25
non-consecutive edges.after reviewing the 42 and 37 lesions labeled as lone in
dlungs and dliver with > 5mm diameter, the radiologist determined that 1 and 8
of them had been wrongly identified as a cancerous lesion. moreover, he found
that 14 and 16 lesions initially labeled as lone, had been wrongly classified:
for these lesions he found 15 and 21 previously unmarked matching lesions in the
next or previous scans. in total, 45 and 62 missing lesions were added to the
ground truth dlungs and dliver datasets, respectively. these hard-to-find
ground-truth false negatives (3.7%, 7.2% of all lesions) may change the
radiological interpretation and the disease status. see the supplemental
material for examples of these scenarios.",5
1120,Graph-Theoretic Automatic Lesion Tracking and Detection of Patterns of Lesion Changes in Longitudinal CT Studies,4.0,Conclusion,"the use of graph-based methods for lesion tracking and detection of patterns of
lesion changes was shown to achieve high accuracy in classifying changes in
individual lesion and identifying patterns of lesion changes in liver and lung
longitudinal ct studies of patients with metastatic disease. this approach has
proven to be useful in detecting missed, faint, and surmised to be present
lesions, otherwise hardly detectable by examining the scans separately or in
pairs, leveraging the added information provided by evaluating all patient's
scans simultaneously using the labels from the lesion changes graph and
non-consecutive edges.",5
1122,Robust Exclusive Adaptive Sparse Feature Selection for Biomarker Discovery and Early Diagnosis of Neuropsychiatric Systemic Lupus Erythematosus,1.0,Introduction,"neuropsychiatric systemic lupus erythematosus (npsle) refers to a complex
autoimmune disease that damages the brain nervous system of patients. the
clinical symptoms of npsle include cognitive disorder, epilepsy, mental illness,
etc., and patients with npsle have a nine-fold increased mortality compared to
the general population [11]. since the pathogenesis and mature treatment of
npsle have not yet been found, it is extremely important to detect npsle at its
early stage and put better clinical interventions and treatments to prevent its
progression. however, the high overlap of clinical symptoms with other
psychiatric disorders and the absence of early non-invasive biomarkers make
accurate diagnosis difficult and time-consuming [3].although conventional
magnetic resonance imaging (mri) tools are widely used to detect brain injuries
and neuronal lesions, around 50% of patients with npsle present no brain
abnormalities in structural mri [17]. in fact, metabolic changes in many brain
diseases precede pathomorphological changes, which indicates proton magnetic
resonance spectroscopy ( 1 h-mrs) to be a more effective way to reflect the
early appearance of npsle. 1 h-mrs is a non-invasive neuroimaging technology
that can quantitatively analyze the concentration of metabolites and detect
abnormal metabolism of the nervous system to reveal brain lesions. however, the
complex noise caused by overlapping metabolite peaks, incomplete information on
background components, and low signal-tonoise ratio (snr) disturb the analysis
results of this spectroscopic method [15]. meanwhile, the individual differences
in metabolism and the interaction between metabolites under low sample size make
it difficult for traditional learning methods to distinguish npsle. figure 1
shows spectra images of four participants including healthy controls (hc) and
patients with npsle. it can be seen that the visual differences between patients
with npsle and hcs in the spectra of the volumes are subtle. therefore, it is
crucial to develop effective learning algorithms to discover metabolic
biomarkers and accurately diagnose npsle. the machine learning application for
biomarker analysis and early diagnosis of npsle is at a nascent stage [4]. most
studies focus on the analysis of mr images using statistical or machine learning
algorithms, such as mann-whitney u test [8], support vector machine (svm)
[7,24], ensemble model [16,22], etc. generally, machine learning algorithms
based on the minimum mean square error (mmse) criterion heavily rely on the
assumption that noise is of gaussian distribution. however, measurement-induced
non-gaussian noise in 1 h-mrs data undoubtedly limits the performance of
mmse-based machine learning methods.on the other hand, for the discovery task of
potential biomarkers, sparse codingbased methods (e.g., 2,1 norm, 2,0 norm,
etc.) force row elements to zero that remove some valuable features [12,21].
more importantly, different brain regions have different functions and
metabolite concentrations, which implies that the metabolic features for each
brain region have different sparsity levels. therefore, applying the same
sparsity constraint to the metabolic features of all brain regions may not
contribute to the improvement of the diagnostic performance of npsle.in light of
this, we propose a robust exclusive adaptive sparse feature selection (reasfs)
algorithm to jointly address the aforementioned problems in biomarker discovery
and early diagnosis of npsle. specifically, we first extend our feature learning
through generalized correntropic loss to handle data with complex non-gaussian
noise and outliers. we also present the mathematical analysis of the adaptive
weighting mechanism of generalized correntropy. then, we propose a novel
regularization called generalized correntropy-induced exclusive 2,1 to
adaptively accommodate various sparsity levels and preserve informative
features. the experimental results on a benchmark npsle dataset demonstrate the
proposed method outperforms comparing methods in terms of early noninvasive
biomarker discovery and early diagnosis.",5
1123,Robust Exclusive Adaptive Sparse Feature Selection for Biomarker Discovery and Early Diagnosis of Neuropsychiatric Systemic Lupus Erythematosus,2.0,Method,"dataset and preprocessing: the t2-weighted mr images of 39 participants
including 23 patients with npsle and 16 hcs were gathered from our affiliated
hospital. all images were acquired at an average age of 30.6 years on a signa
3.0t scanner with an eight-channel standard head coil. then, the mr images were
transformed into spectroscopy by multi-voxel 1 h-mrs based on a point-resolved
spectral sequence (press) with a two-dimensional multi-voxel technique. the
collected spectroscopy data were preprocessed by a sage software package to
correct the phase and frequency. an lcmodel software was used to fit the
spectra, correct the baseline, relaxation, and partial-volume effects, and
quantify the concentration of metabolites. finally, we used the absolute naa
concentration in single-voxel mrs as the standard to gain the absolute
concentration of metabolites, and the naa concentration of the corresponding
voxel of multi-voxel 1 h-mrs was collected consistently. the spectra would be
accepted if the snr is greater than or equal to 10 and the metabolite
concentration with standard deviations (sd) is less than or equal to 20%. the
absolute metabolic concentrations, the corresponding ratio, and the linear
combination of the spectra were extracted from different brain regions: rpcg,
lpcg, rdt, ldt, rln, lln, ri, rpwm, and lpwm. a total of 117 metabolic features
were extracted, and each brain region contained 13 metabolic features: cr,
phosphocreatine (pcr), cr+pcr, naa, naag, naa+naag, naa+naag/cr+pcr, mi,
mi/cr+pcr, cho+phosphocholine (pch), cho+pch/cr+pcr, glu+gln, and
glu+gln/cr+pcr.",5
1127,Robust Exclusive Adaptive Sparse Feature Selection for Biomarker Discovery and Early Diagnosis of Neuropsychiatric Systemic Lupus Erythematosus,3.0,Experimental Results and Conclusion,"experimental settings: the parameters α and λ 1 are are set to 1, while β and λ
2 are searched form {0.5, 1, 2, 5} and {0.1, 0.5, 1}, respectively. we use adam
optimizer and the learning rate is 0.001. to evaluate the performance of
classification, we employ a support vector machine as the basic classifier,
where the kernel is set as the radial basis function (rbf) and parameter c is
set to 1. we average the 3-fold cross-validation results.results and discussion:
we compare the classification accuracy of the proposed reasfs with several sota
baselines, including two filter methods: maximal information coefficient (mic)
[5], gini [23], and four sparse coding-based methods: multi-task feature
learning via 2,1 norm [6,12], discriminative feature selection via 2,0 norm
[21], feature selection via 1,2 norm [10] and exclusive 2,1 [9]. the proposed
reasfs is expected to have better robustness and flexibility. it can be seen
from fig. 2 that the sparse coding-based methods achieve better performance than
filter methods under most conditions, where ""0%"" represents no noise
contamination. the highest accuracy of our reasfs demonstrates the effectiveness
and flexibility of the proposed gcie 2,1 . generally speaking, the probability
of samples being contaminated by random noise is equal. therefore, we randomly
select features from the training set and replace the selected features with
pulse noise. the number of noisy attributes is denoted by the ratio between the
numbers of selected features and total features, such as 15% and 30%. the
classification performances of the npsle dataset contaminated by attribute noise
are shown in fig. 3(a) and fig. 3(b), where one clearly perceives that our
reasfs achieves the highest accuracy under all conditions. besides, it is
unreasonable to apply the same level of sparse regularization to noise features
and uncontaminated features, and our gcie 2,1 can adaptively increase the sparse
level of noise features to remove redundant information, and vice versa. for
label noise, we randomly select samples from the training set and replace
classification labels of the selected samples with opposite values, i.e., 0 → 1
and 1 → 0. the results are shown in fig. 3(c) and fig. 3(d), where the proposed
reasfs is superior to other baselines. it can be seen from fig. 3 that our
reasfs achieves the highest accuracy in different noisy environments, which
demonstrates the robustness of generalized correntropic loss. for non-invasive
biomarkers, our method shows that some metabolic features contribute greatly to
the early diagnosis of npsle, i.e., naag, mi/cr+pcr, and glu+gln/cr+pcr in rpcg;
cr+pcr, naa+naag, naa+naag/cr+pcr, mi/cr+pcr and glu+gln in lpcg; naa, naag, and
cho+pch in ldt; pcr, cr+pcr, cho+pch, cho+pch/cr+pcr and glu+gln/cr+pcr in rln;
mi/cr+pcr, cho+pch and cho+pch/cr+pcr in lln; naa+naag/cr+pcr and cho+pch in ri;
cho+pch/cr+pcr and glu+gln/cr+pcr in rpwm; and pcr, naag and naa+naag/cr+pcr in
lpwm. moreover, we use isometric feature mapping (isomap) [19] to analyze these
metabolic features and find that this feature subset is essentially a
low-dimensional manifold. meanwhile, by combining the proposed reasfs and
isomap, we can achieve 99% accuracy in the early diagnosis of npsle. in
metabolite analysis, some studies have shown that the decrease in naa
concentration is related to chronic inflammation, damage, and tumors in the
brain [18]. in the normal white matter area, different degrees of npsle disease
is accompanied by different degrees of naa decline, but structural mri is not
abnormal, suggesting that naa may indicate the progress of npsle. we also found
that glu+gln/cr+pcr in ri decreased, which indicates that the excitatory
neurotransmitter glu in the brain of patients with npsle may have lower
activity. to sum up, the proposed method provides a shortcut for revealing the
pathological mechanism of npsle and early detection.",5
1135,CARL: Cross-Aligned Representation Learning for Multi-view Lung Cancer Histology Classification,3.1,Dataset,"our dataset nsclc-tcia for lung cancer histological subtype classification is
sourced from two online resources of the cancer imaging archive (tcia) [5]:
nsclc radiomics [1] and nsclc radiogenomics [2]. exclusion criteria involves
patients diagnosed with large cell carcinoma or not otherwise specified, along
with cases that have contouring inaccuracies or lacked tumor delineation [9,13].
finally, a total of 325 available cases (146 adc cases and 179 scc cases) are
used for our study. we evaluate the performance of nsclc classification in
five-fold cross validation on the nsclc-tcia dataset, and measure accuracy
(acc), sensitivity (sen), specificity (spe), and the area under the receiver
operating characteristic (roc) curve (auc) as evaluation metrics. we also
conduct analysis including standard deviations and 95% ci, and delong
statistical test for further auc comparison.for preprocessing, given that the ct
data from nsclc-tcia has an in-plane resolution of 1 mm × 1 mm and a slice
thickness of 0.7-3.0 mm, we resample the ct images using trilinear interpolation
to a common resolution of 1mm × 1mm × 1mm. then one 128 × 128 pixel slice is
cropped from each view as input based on the center of the tumor. finally
following [7], we clip the intensities of the input patches to the interval
(-1000, 400 hounsfield unit) and normalize them to the range of [0, 1].",5
1138,CARL: Cross-Aligned Representation Learning for Multi-view Lung Cancer Histology Classification,4.0,Conclusion,"in summary, we propose a novel multi-view method called cross-aligned
representation learning (carl) for accurately distinguishing between adc and scc
using multi-view ct images of nsclc patients. it is designed with a cross-view
representation alignment learning network which effectively generates
discriminative view-invariant representations in the common subspace to reduce
the discrepancies among multi-view images. in addition, we leverage a
view-specific representation learning network to acquire viewspecific
representations as a necessary complement. the generated view-invariant and
-specific representations together offer a holistic and disentangled perspective
of the multi-view ct images for histological subtype classification of nsclc.
the experimental result on nsclc-tcia demonstrates that carl reaches 0.817 auc,
76.8% acc, 73.2% sen, and 79.7% spe and surpasses other relative approaches,
confirming the effectiveness of the proposed carl method.",5
1139,Self- and Semi-supervised Learning for Gastroscopic Lesion Detection,1.0,Introduction,"gastroscopic lesion detection (gld) plays a key role in computer-assisted
diagnostic procedures. although deep neural network-based object detectors
achieve tremendous success within the domain of natural images, directly
training generic object detectors on gld datasets performs below expectations
for two reasons: 1) the scale of labeled data in gld datasets is limited in
comparison to natural images due to the annotation costs. though gastroscopic
images are abundant, those containing lesions are rare, which necessitates
extensive image review for lesion annotation. 2) the characteristic of
gastroscopic images exhibits distinct differences from the natural images
[18,19,21] and is often of high similarity in global but high diversity in
local. specifically, each type of lesion may have diverse appearances though
gastroscopic images look quite similar. some appearances of lesions are quite
rare and can only be observed in a few patients. generic self-supervised
backbone pre-training or semi-supervised detector training methods can solve the
first challenge for natural images but its effectiveness is undermined for
gastroscopic images due to the second challenge.self-supervised backbone
pre-training methods enhance object detection performance by learning
high-quality feature representations from massive unlabelled data for the
backbone. the mainstream self-supervised backbone pretraining methods adopt
self-supervised contrast learning [3,4,7,9,10] or masked fig. 1. pipeline of
self-and semi-supervised learning (ssl) for gld. ssl consists of a hybrid
self-supervised learning (hsl) method and a prototype-based pseudo-label
generation (ppg) method. hsl combines patch reconstruction with dense
contrastive learning. ppg generates pseudo-labels for potential lesions based on
the similarity to the prototype feature vectors.image modeling [8,15].
self-supervised contrastive learning methods [3,4,7,9] can learn discriminative
global feature representations, and [10] can further learn discriminative local
feature representations by extending contrastive learning to dense paradigm.
however, these methods usually cannot grasp enough local detailed information.
on the other hand, masked image modeling is expert in extracting local detailed
information but is weak in preserving the discriminability of feature
representation. therefore, both types of methods have their own weakness for gld
tasks.semi-supervised object detection methods [12,14,16,17,20,22,23] first use
detectors trained with labeled data to generate pseudo-labels for unlabeled data
and then enhance object detection performance by regarding these unlabeled data
with pseudo-labels as labeled data to train the detector. current pseudolabel
generation methods rely on the objectiveness score threshold to generate
pseudo-labels, which makes them perform below expectations on gld, because the
characteristic of gastroscopic lesions makes it difficult to set a suitable
threshold to discover potential lesions meanwhile avoiding introducing much
noise.the motivation of this paper is to explore how to enhance gld performance
using massive unlabeled gastroscopic images to overcome the labeled data
shortage problem. the main challenge for this goal is the characteristic of
gastroscopic lesions. intuitively, such a challenge requires local feature
representations to contain enough detailed information, meanwhile preserving
discriminability. enlightened by this, we propose the self-and semi-supervised
learning (ssl) framework tailored to address challenges in daily clinical
practice and use massive unlabeled data to enhance gld performance. ssl
overcomes the challenges of gld by leveraging a large volume of unlabeled
gastroscopic images using self-supervised learning for improved feature
representations and semi-supervised learning to discover and utilize potential
lesions to enhance performance. specifically, it consists of a hybrid
self-supervised learning (hsl) method for self-supervised backbone pre-training
and a prototype-based pseudo-label generation (ppg) method for semi-supervised
detector training. the hsl combines the dense contrastive learning [10] with the
patch reconstruction to inherit the advantages of discriminative feature
learning and grasp the detailed information that is important for gld tasks. the
ppg generates pseudo-labels based on the similarity to the prototype feature
vectors (formulated from the feature vectors in its memory module) to discover
potential lesions from unlabeled data, and avoid introducing much noise at the
same time. moreover, we propose the first large-scale gld datasets (lgldd),
which contains 10,083 gastroscopic images with 12,292 well-annotated lesion
bounding boxes of four categories of lesions (polyp, ulcer, cancer, and
sub-mucosal tumor). we evaluate ssl with multiple detectors on lgldd and ssl
brings significant improvement compared with baseline methods (centernet [6]:
+2.7ap, faster rcnn [13]: +2.0ap). in summary, our contributions include:-a
self-and semi-supervise learning (ssl) framework to leverage massive unlabeled
data to enhance gld performance. -a large-scale gastroscopic lesion detection
datasets (lgldd) -experiments on lgldd demonstrate that ssl can bring
significant enhancement compared with baseline methods.",5
1143,Self- and Semi-supervised Learning for Gastroscopic Lesion Detection,3.0,Datasets,"we contribute the first large-scale gstroscopic lesion detection datasets
(lgldd) in the literature. collection : lgmdd collects about 1m+ gastroscopic
images from 2 hospitals of about 500 patients and their diagnosis reports. after
consulting some senior doctors and surveying gastroscopic diagnosis papers [1],
we select to annotate 4-category lesions: polyp(pol), ulcer(ulc), cancer(can)
and sub-mucosal tumor(smt). we invite 10 senior doctors to annotate them from
the unlabeled endoscopic images. to preserve the annotation quality, doctors can
refer to the diagnosis reports, and each lesion is annotated by a doctor and
checked by another. finally, they annotates 12,292 lesion boxes in 10,083 images
after going through about 120,000 images. the polyp, ulcer, cancer, and
sub-mucosal tumor numbers are 7,779, 2,171, 1,164and 1,178, respectively. the
train/val split of lgmdd is 8,076/2,007. the other data serves as unlabeled
data.evaluation metrics : we use standard object detection metrics to evaluate
the gld performance, which computes the average precision (ap) under multiple
intersection-of-union (iou) thresholds and then evaluate the performance using
the mean of aps (map) and the ap of some specific iou threshold. for map, we
follow the popular object detection datasets coco [11] and calculate the mean of
11 aps of iou from 0.5 to 0.95 with stepsize 0.05 (map @[.5:.05:.95]).we also
report ap under some specific iou threshold (ap 50 for .5, ap 75 for .75) and ap
of different scale lesions (ap s , ap m , ap l ) like coco [11].",5
1152,"Joint Prediction of Response to Therapy, Molecular Traits, and Spatial Organisation in Colorectal Cancer Biopsies",1.0,Introduction,"in the uk, approximately 11,500 patients are diagnosed with rectal cancer each
year [19]. a common form of treatment for such patients is neoadjuvant therapy,
including chemotherapy and radiotherapy, which can be given to patients with
locally advanced rectal cancer to shrink the tumour prior to surgery. recent
evidence suggests that 10-20% of patients will have a complete pathological
response to neoadjuvant therapy and can therefore avoid surgery altogether
[2,5]. however, one third of patients do not benefit from radiotherapy treatment
prior to surgery [8], hence it is important to determine how a patient will
respond to radiotherapy with a personalized approach in order to avoid
overtreatment.histology-based digital biomarkers enable the possibility to
predict a patient's response to therapy. the consensus molecular subtypes (cms)
classification system derived from gene expressions [9] has been developed to
provide biological insight into metastatic colorectal cancer. it has been shown
that these four cms classes can be predicted directly from the standard
haematoxylin and eosin (h&e) stained slide images using deep learning [18].
various studies have investigated the link between cms and patient outcomes,
suggesting that patients with tumour classified as cms4, which features stromal
invasion [9] and shows significantly higher stroma content [15], have worse
survival rates compared to the other cms classes [5]. increased stromal content
has independently been shown to be a predictor for increased risk of recurrence
in early rectal cancer [10], and tumour immune infiltrate evaluated with
immunoscore is a useful prognostic marker [3]. the spatial organisation of the
cancerous tissue has been identified as a biomarker for aggressiveness or
recurrence [12], and qi et al. [15] found that the features they developed
representing spatial organisation reflected characteristics of the four cms
classes. interactions between the epithelial tissue (cellular tissue lining) and
other prevalent tissue types in the tumour microenvironment are also indicators
of prognosis [15], since progression of colorectal cancer is dependent on both
the epithelial and stromal tissues [20]. other work has looked at predicting
chemoradiotherapy response in rectal cancer patients from h&e images using
different approaches, but without providing contextual interpretations
[19,22].as opposed to predicting response to radiotherapy alone, we aim to
analyse this prediction in the context of the overall tissue architecture and
the tumour biology as captured by cms. input to our model is a standard h&e
whole slide image (wsi) which is split into smaller patches to overcome the
memory limitations of existing gpus. to achieve our goal we need to capture the
heterogeneity at the slide level, which is why applying full or semi-supervised
approaches on individual tiles followed by a slide aggregation method is not
suitable. instead, we build on recent graph neural network (gnn) approaches that
allow us to model the entire wsi as a graph. as local cell communities form the
nodes of such a graph it can effectively model the micro-anatomy of the tissue.
at the same time it is possible to make predictions at the node-, graph-, and
slide-level. related work. to predict the grading of colorectal cancer (crc),
both cellbased and patch-based graphs have been used in separate works [16,23],
setting the nodes of the graph as either cell nuclei or square patches, defining
the node features as either handcrafted or learned features, and then applying a
gnn for outcome prediction. another patch-based gnn approach to predicting
genetic mutations in crc from h&e slides found their model trained on colon
cancer generalised well to rectal cancer. for other cancers, the slidegraph
pipeline clusters nuclei for the graph nodes, and provides node-level
predictions to make their model more interpretable [13]. other approaches to
setting the graph nodes include using subgraphs to represent regions [14], and
creating superpatches by combining patches [11]. edges between the nodes are
usually defined by a spatial distance metric, which helps model the spatial
organisation of the tissue. common choices for gnns include a graph isomorphism
network (gin) with jumping connectivity [7,13,14], as we use in this
research.our methodology proposes a novel and disease relevant approach to a
more interpretable model that effectively supports a diagnostic task.
pathologists and oncologists can use this information to inspect the validity of
the prediction result and interrogate key aspects of the spatial biology that is
critical for patient management. ultimately, this type of information that is
not available today will help to characterise interactions between the tumour
and the host tissue and therefore help to support choice of therapy. the
developed framework combines self-supervised training of a vision transformer
(vit) to extract morphological features, a superpixel algorithm for determining
nodes of a graph, and a gnn for predictions. we achieve 0.82 auc predicting
complete response to radiotherapy using deep learning on wsis for crc patients,
whilst providing novel interpretability of the results.",5
1153,"Joint Prediction of Response to Therapy, Molecular Traits, and Spatial Organisation in Colorectal Cancer Biopsies",2.0,Methods,"in this section we present the patch-level feature extraction, provide the
detail of the superpixel segmentation of the wsi, and illustrate the resulting
graph presentation. a gnn with three branches for our output predictions is used
to simultaneously make the three different predictions as shown in fig.
1.pipeline. for computational reasons, all images are split into patches of size
256 × 256 pixels. in order to have a common feature set all the way up to the
last layer of the gnn, individual patches should be represented by morphological
features that are label-agnostic. this last layer of the gnn then splits into
three branches to predict response to radiotherapy, the cms4 subtype
classification for crc, and epithelial tissue regions. this way we can guarantee
the common latent features and derivation across branches, maintaining the
contextual importance of each branch. the dino framework [4] uses a
self-distillation training approach, using data augmentation to locally crop the
patches and train with a local-global student-teacher approach. we use the dino
framework to train a vit in a self-supervised manner on our h&e slides [6],
representing each patch with 384 features. we use only the training set to train
this model, and use the image patches at 20x magnification. we extract
patch-level features from each wsi using selfsupervised dino training with a vit
model [4]. the slic superpixel algorithm segments the entire slide into smaller
regions [1]. we calculate the mean patch features for these superpixel regions,
and use the superpixel features and centers as our graph nodes, applying
delaunay triangulation to generate the edges of the graph. a gnn consisting of
ginconv layers is trained on these fixed graphs, and the final layer splits into
three separate mlp branches to provide predictions of three different outcomes,
complete response (cr) to radiotherapy (rt), cms4 classification, and epithelial
tissue. an example output is visualized in fig. 2.to find the nodes of the wsi
graphs, we apply the slic superpixel algorithm [1] on the wsis at 5x
magnification to segment the tissue to capture cellular neighbourhoods that are
roughly between 80-100 µm 2 /pixels in size. it can be seen that the superpixel
boundaries consistently align with the boundaries of tissue compartments.the
superpixels centers are used as the nodes of the graph, and the node features
are the weighted mean of the corresponding patch features which overlap with the
superpixel region. the edges of the graph are determined by nearest neighbours
from delaunay triangulation, as in slidegraph [13].building on the ideas
introduced by slidegraph [13] we use ginconv layers [21], adding tempering to
avoid overfitting, and replace their logistic regression scaler with a simple
sigmoid function. we add three branches to the final layer of the gnn, in the
form of three separate multilayer perceptrons (mlps). two of these mlps return a
graph-level prediction, for the response to rt and cms4 predictions, and the
final branch returns node-level predictions, predicting whether each node is
epithelial tissue or not. our loss function is defined aswhere bce is the binary
cross entropy loss, ŷrt ∈ r is the slide-level prediction of response to
radiotherapy, ŷcms4 ∈ r is the slide-level prediction of cms4, ŷepi ∈ r ni are
the node-level predictions of epithelial tissue and n i is the number of nodes
in the i th wsi graph.for each prediction branch, we can visualize the
individual node predictions from the wsi graph, overlaid on the wsi itself, to
get an idea of how the node predictions vary across the different tissue
regions. each graph-level prediction is derived from the corresponding branch
node predictions, by applying pooling and dropout.data. we train and validate
our methods on two retrospective rectal cancer datasets, grampian and aristotle.
both cohorts received standard chemoradiotherapy of pelvic irradiation
(45-50.4gy in 25 fractions over 5 weeks) with capecitabine 900mg/m 2 . the
pre-treatment biopsy slides were all sectioned and stained in the same
laboratory, and scanned at 20x magnification (0.5 µm 2 /pixel) on an aperio
scanner. pathological complete response, which we use as a target outcome here,
was derived from histopathological assessment from posttreatment resections.the
cms labels for this data are derived from three different transcriptomic
versions (single cohort, combined cohort correcting batch effects and combined
cohort including 2036 cases run with the same platform), in order to generate
robust classifications. in all cases the cms call was calculated using the
cmsclassifier random forest and single sample predictor [9]. final cms calls are
based on matching calls between the three transcriptomic versions. despite our
efforts to minimise the noise from rna sequencing, we still expect a certain
level of noise in our ground truth data, which we discuss in the results
section.the epithelial labels for each graph node are calculated from epithelial
masks for each wsi. these epithelial segmentation masks were generated at 10x
magnification (1 µm 2 /pixel) with a u-net [17] which was trained and validated
on 666 full tissue sections belonging to 362 patients from the focus cohort
[18]. the ground truth annotations for the training of this model were generated
by vk.for consistency the tumour regions were marked up by an expert
pathologist. we use these masks in our analysis to filter out background and
irrelevant tissue from the images. grampian and aristotle are used in both
training and validation, with a 70/30% training-validation split, keeping any
wsis from a single patient in the same dataset. we predict complete response to
radiotherapy against all other responses, such as partial response and no
response. the datasets are unbalanced, since in grampian only 61/244 slides have
complete response, and in aristotle only 24/121 slides have complete response.
they are even more unbalanced for cms4, since only 28/244 slides in grampian and
17/121 slides in aristotle are labelled with cms4. we address this imbalance in
the supplementary materials. there are 365 slides total in our dataset, from 249
patients.",5
1155,"Joint Prediction of Response to Therapy, Molecular Traits, and Spatial Organisation in Colorectal Cancer Biopsies",0,Response branch,"response results. despite the noise in our reference data used for training, our
model achieves good performance in terms of mean auc scores on all three
prediction branches of our model, predicting complete response to radiotherapy
(rt) with 0.819 auc, cms4 with 0.819 auc and epithelial tissue at the node level
with 0.760 auc across folds. further metrics are provided in table 1. the
prediction performance of the model could be improved by utilising a larger
training dataset and performing more exhaustive parameter searches, however the
current performance of the model is sufficient to demonstrate the impact of this
approach.the predicted response to radiotherapy can now be viewed in the context
of disease biology as captured by cms4. for example, the model demonstrates that
cms4 patients are less likely to respond to radiotherapy. in addition, it is now
possible to view the spatial distribution of cms4 active regions in the tissue
architecture context as shown in fig. 2. additional samples are presented in the
supplementary materials. an example of our proposed prediction maps on two
slides can be seen in fig. 2, with further slides in the supplementary
materials. a pathologist reviewing these maps assesses that the observed
patterns fit the known interplay of response to therapy, cms4 activation, and
the spatial localisation of these signals. in the top slide, we observe high
cms4 activation in stromal rich regions, and interestingly also high cms4
activation in the bottom center, dissociating from the response to rt activation
map. this could be explained by the lymphocyte content, supported by the higher
epithelial map activations in the same location. expert pathologists highlight a
similar pattern in certain regions of the maps for the bottom slide. different
from the slide above, the cms4 and response to rt maps have some overlap with
moderate activations here, encouraging discovery into tumour-host interactions.
ultimately, a pathologist confirmed that these maps support an interpretable and
trustworthy prediction in the context of response to radiotherapy. while we
cannot present a more extensive interpretation of these results due to space
limitations, these examples already indicate that the proposed approach enables
a level of analysis that has not been possible before. we find that predicting
these outcomes individually in a single branch model, particularly with response
to radiotherapy, can result in slightly higher auc scores, but we consciously
make this trade-off in order to provide better interpretability of the model
predictions. the focus of this research is not to achieve the best possible
metrics, but to develop robust methods which can add context and explanation to
clinical black box deep learning model predictions, with the view to ease
clinical translation of such models.to explore the effects of the noisy cms4
ground truth labels, we remove from our dataset any wsis classified as
'unmatched' for the cms call, which for the main results of this paper we
defined as 'not cms4'. removing this data and rerunning our analysis improved
our predictions for cms4 by +0.06 auc, and reduced our response to radiotherapy
and epithelial predictions by -0.02 and -0.01 respectively. the results can be
found in the supplementary materials. these small changes indicate that the
noise in our data does not degrade the performance of our classifier,
reinforcing it as a robust and accurate model.",5
1156,"Joint Prediction of Response to Therapy, Molecular Traits, and Spatial Organisation in Colorectal Cancer Biopsies",4.0,Conclusion,"by setting the prediction of response to therapy in context with disease biology
and spatial organisation of the tissue we are providing a novel approach for
enhancing the interpretablity of complex prediction tasks. these results do not
only enhance the interpretability, they also provide new ways to utilise large
retrospective clinical trial cohorts for which no additional molecular data is
available. extending the amount of training data and improving model training
will improve model performance, which is already impressive.we argue that this
work also advances the state of the art in feature representation and analysis.
our prediction maps derive from the same graph model, and hence they share
underlying graph features. the prediction branches only diverge at the final
stage of translating these graph features into outcome predictions for our three
clinically relevant outcomes. importantly, this level of visualisation is not
only accessible to pathologists, this joint prediction model also enhances the
communication between pathologists and oncologists which is critical for patient
management. by cross-referencing these prediction maps with our prior
understanding of cancer biology, this approach can help to establish trust in
the prediction model and also help to identify potential failure cases.this work
relies on access to well annotated clinical trial samples which will limit our
ability to include more data for training and testing. in future, we plan to use
these methods to help better characterise tumour-stromal interactions of the
tissue. we also plan to use a denser graph with less connectivity to be able to
better predict the heterogeneous epithelial tissue.the aristotle trial was
funded by cancer research uk (cruk/08/032). the funders played no role in the
analyses performed or the results presented. financial support: rw -epsrc center
for doctoral training in health data science (ep/s02428x/1), oxford cruk cancer
centre; vhk -promedica foundation (f-87701-41-01) and swiss national science
foundation (p2skp3_168322/1, p2skp3_168322/2); tsm -s:cort (see above); jr, ks
-oxford nihr national oxford biomedical research centre and the pathlake
consortium (innovateuk). the computational aspects of this research were funded
from the nihr oxford brc with additional support from the wellcome trust core
award grant number 203141/z/16/z. the views expressed are those of the author(s)
and not necessarily those of the nhs, the nihr or the department of health.",5
1158,Automatic Bleeding Risk Rating System of Gastric Varices,1.0,Introduction,"esophagogastric varices are one of the common manifestations in patients with
liver cirrhosis and portal hypertension and occur in about 50 percent of
patients with liver cirrhosis [3,6]. the occurrence of esophagogastric variceal
bleeding is the most serious adverse event in patients with cirrhosis, with a
6-week acute bleeding mortality rate as high as 15%-20% percent [14]. it is
crucial to identify high-risk patients and offer prophylactic treatment at the
appropriate time. regular endoscopy examinations have been proven an effective
clinical approach to promptly detect esophagogastric varices with a high risk of
bleeding [7]. different from the grading of esophageal varices (ev) that is
relatively complete [1], the bleeding risk grading of gastric varices (gv)
involves complex variables including the diameter, shapes, colors, and
locations. several rating systems have been proposed to describe gv based on the
anatomical area. sarin et al. [16] described and divided gv into 2 groups
according to their locations and extensions. hashizume et al. [10] published a
more detailed examination describing the form, location, and color. although the
existing rating systems tried to identify the risk from different perspectives,
they still lack clear quantification standard and heavily rely on the
endoscopists' subjective judgment. this may cause inconsistency or even
misdiagnosis due to the variant experience of endoscopists in different
hospitals. therefore, we aim to build an automatic gv bleeding risk rating
method that can learn a stable and robust standard from multiple experienced
endoscopists.recent works have proven the effectiveness and superiority of deep
learning (dl) technologies in handling esophagogastroduodenoscopy (egd) tasks,
such as the detection of gastric cancer and neoplasia [4]. it is even
demonstrated that ai can detect neoplasia in barrett's esophagus at a higher
accuracy than endoscopists [8]. intuitively we may regard the gv bleeding risk
rating as an image classification task and apply typical classification
architectures (e.g., resnet [12]) or state-of-the-art gastric lesion
classification methods to it. however, they may raise poor performance due to
the large intra-class variation between gv with the same bleeding risk and small
inter-class variation between gv and normal tissue or gv with different bleeding
risks. first, the gv area may look like regular stomach rugae as it is caused by
the blood vessels bulging and crumpling up the stomach (see fig. 1). also, since
the gv images are taken from different distances and angles, the number of
pixels of the gv area may not reflect its actual size. consequently, the model
may fail to focus on the important gv areas for prediction as shown in fig. 3.
to encourage the model to learn more robust representations, we constructively
introduce segmentation into the classification framework. with the segmentation
information, we further propose a region-constraint module (rcm) and a
cross-region attention module (cram) for better feature localization and
utilization. specifically, in rcm, we utilize the segmentation results to
constrain the cam heatmaps of the feature maps extracted by the classification
backbone, avoiding the model making predictions based on incorrect areas. in
cram, the varices features are extracted using the segmentation results and
combined with an attention mechanism to learn the intra-class correlation and
cross-region correlation between the target area and the context.to learn from
experienced endoscopists, gv datasets with bleeding risks annotation is needed.
while most works and public datasets focus on colonoscopy [13,15] and esophagus
[5,9], with a lack of study on gastroscopy images. in the public dataset of
endocv challenge [2], the majority are colonoscopies while only few are
gastroscopy images. in this work, we collect a gv bleeding risks rating dataset
(gvbleed) that contains 1678 gastroscopy images from 411 patients with different
levels of gv bleeding risks. three senior clinical endoscopists are invited to
grade the bleeding risk of the retrospective data in three levels and annotated
the corresponding segmentation masks of gv areas.in sum, the contributions of
this paper are: 1) a novel gv bleeding risk rating framework that constructively
introduces segmentation to enhance the robustness of representation learning; 2)
a region-constraint module for better feature localization and a cross-region
attention module to learn the correlation of target gv with its context; 3) a gv
bleeding risk rating dataset (gvbleed) with high-quality annotation from
multiple experienced endoscopists. baseline methods have been evaluated on the
newly collected gvbleed dataset. experimental results demonstrate the
effectiveness of our proposed framework and modules, where we improve the
accuracy by nearly 5% compared to the baseline model.",5
1164,Automatic Bleeding Risk Rating System of Gastric Varices,3.0,GVBleed Dataset,"data collection and annotation. the gvbleed dataset contains 1678 endoscopic
images with gastric varices from 527 cases. all of these cases are collected
from 411 patients in a grade-iii class-a hospital during the period from 2017 to
2022. in the current version, images from patients with ages elder than 18 are
retained 1 . the images are selected from the raw endoscopic videos and frames.
to maximize the variations, non-consecutive frames with larger angle differences
are selected. to ensure the quality of our dataset, senior endoscopists are
invited to remove duplicates, blurs, active bleeding, chromoendoscopy, and nbi
pictures.criterion of gv bleeding risk level rating. based on the clinical
experience in practice, the gv bleeding risks in our dataset are rated into
three levels, i.e., mild, moderate, and severe. the detailed rating standard is
as follows: 1) mild: low risk of bleeding, and regular follow-up is sufficient
(usually with a diameter less than or equal to 5 mm). 2) moderate: moderate risk
of bleeding, and endoscopic treatment is necessary, with relatively low
endoscopic treatment difficulty (usually with a diameter between 5 mm and 10
mm). 3) severe: high risk of bleeding and endoscopic treatment is necessary,
with high endoscopic treatment difficulty. the varices are thicker (usually with
a diameter greater than 10 mm) or less than 10mm but with positive red signs.
note that the diameter is only one reference for the final risk rating since the
gv is with 1 please refer to the supplementary material for more detailed
information about our dataset. various 3d shapes and locations. the other facts
are more subjectively evaluated based on the experience of endoscopists. to
ensure the accuracy of our annotation, three senior endoscopists with more than
10 years of clinical experience are invited to jointly label each sample in our
dataset. if three endoscopists have inconsistent ratings for a sample, the final
decision is judged by voting. a sample is selected and labeled with a specific
bleeding risk level only when two or more endoscopists reach a consensus on it.
the gvbleed dataset is partitioned into training and testing sets for
evaluation, where the training set contains 1337 images and the testing set has
341 images. the detailed statistics of the three levels of gv bleeding risk in
each set are shown in table 1. the dataset is planned to be released in the
future.",5
1169,Improving Outcome Prediction of Pulmonary Embolism by De-biased Multi-modality Model,1.0,Introduction,"bias in medicine has demonstrated a notable challenge for providing
comprehensive and equitable care. implicit biases can negatively affect patient
care, particularly for marginalized populations with lower socioeconomic status
[30]. evidence has demonstrated that implicit biases in healthcare providers
could contribute to exacerbating these healthcare inequalities and create a more
unfair system for people of lower socioeconomic status [30]. based on the data
with racial bias, the unfairness presents in developing evaluative algorithms.
in an algorithm used to predict healthcare costs, black patients who received
the same health risk scores as white patients were consistently sicker [21].
using biased data for ai models reinforces racial inequities, worsening
disparities among minorities in healthcare decision-making [22].within the
radiology arm of ai research, there have been significant advances in
diagnostics and decision making [19]. along these advancements, bias in
healthcare and ai are exposing poignant gaps in the field's understanding of
model implementation and their utility [25,26]. ai model quality relies on input
data and addressing bias is a crucial research area. systemic bias poses a
greater threat to ai model's applications, as these biases can be baked right
into the model's decision process [22].pulmonary embolism (pe) is an example of
health disparities related to race. black patients exhibit a 50% higher
age-standardized pe fatality rate and a twofold risk for pe hospitalization than
white patients [18,24]. hospitalized black patients with pe were younger than
whites. in terms of pe severity, blacks received fewer surgical interventions
for intermediate pe but more for highseverity pe [24]. racial disparities exist
in pe and demonstrate the inequities that affect black patients. the pulmonary
embolism severity index (pesi) is a well-validated clinical tool based on 11
clinical variables and used for outcome prediction measurement [2]. survival
analysis is often used in pe to assess how survival is affected by different
variables, using a statistical method like kaplan-meier method and cox
proportional-hazards regression model [7,12,14].however, one issue with
traditional survival analysis is bias from single modal data that gets
compounded when curating multimodal datasets, as different combinations of modes
and datasets create with a unified structure. multimodal data sets are useful
for fair ai model development as the bias complementary from different sources
can make de-biased decisions and assessments. in that process, the biases of
each individual data set will get pooled together, creating a multimodal data
set that inherits multiple biases, such as racial bias [1,15,23]. in addition,
it has been found that creating multimodal datasets without any debiasing
techniques does not improve performance significantly and does increase bias and
reduce fairness [5]. overall, a holistic approach to model development would be
beneficial in reducing bias aggregation in multimodal datasets. in recent years,
disentangled representation learning (drl) [4] for bias disentanglement improves
model generalization for fairness [3,6,27].we developed a pe outcome model that
predicted mortality and detected bias in the output. we then implemented methods
to remove racial bias in our dataset and model and output unbiased pe outcomes
as a result. our contributions are as follows: (1) we identified bias diversity
in multimodal information using a survival prediction fusion framework. (2) we
proposed a de-biased survival prediction framework with demographic bias
disentanglement. (3) the multimodal cph learning models improve fairness with
unbiased features.",5
1170,Improving Outcome Prediction of Pulmonary Embolism by De-biased Multi-modality Model,2.0,Bias in Survival Prediction,"this section describes the detail of how we identify the varying degrees of bias
in multimodal information and illustrates bias using the relative difference in
survival outcomes. we will first introduce our pulmonary embolism multimodal
datasets, including survival and race labels. then, we evaluate the baseline
survival learning framework without de-biasing in the various racial
groups.dataset. the pulmonary embolism dataset used in this study from 918
patients (163 deceased, median age 64 years, range 13-99 years, 52% female),
including 3978 ctpa images and 918 clinical reports, which were identified via
retrospective review across three institutions. the clinical reports from
physicians that provided crucial information are anonymized and divided into
four parts: medical history, clinical diagnosis, observations and radiologist's
opinion. for each patient, the race labels, survival time-to-event labels and
pesi variables are collected from clinical data, and the 11 pesi variables are
used to calculate the pesi scores, which include age, sex, comorbid illnesses
(cancer, heart failure, chronic lung disease), pulse, systolic blood pressure,
respiratory rate, temperature, altered mental status, and arterial oxygen
saturation at the time of diagnosis [2].diverse bias of multimodal survival
prediction model. we designed a deep survival prediction (sp) baseline framework
for multimodal data as shown in fig. 1, which compares the impact of different
population distributions. the frameworks without de-basing are evaluated for
risk prediction in the test set by performing survival prediction on ctpa
images, clinical reports, and clinical variables, respectively. first, we use
two large-scale data-trained models as backbones to respectively extract
features from preprocessed images and cleaned clinical reports. a
state-of-the-art pe detecting model, penet [11] is used as the backbone model
for analyzing imaging risk and extracting information from multiple slices of
volumetric ctpa scans to locate the pe. the feature with the highest pe
probability from a patient's multiple ctpas is considered as the most pe-related
visual representation. next, the gatortron [29] model is employed to recognize
clinical concepts and identify medical relations for getting accurate patient
information from pe clinical reports. the extracted features from the backbones
and pesi variables are represented as f m , m ∈ [img, text, var]. the survival
prediction baseline framework, built upon the backbones, consists of three
multi-layer perceptron (mlp) modules named imaging-based, text-based and
variable-based sp modules. to encode survival features z m sur from image, text
and pesi variables, these modules are trained to distinguish critical disease
from non-critical disease with cox partial log-likelihood loss (coxphloss) [13].
the framework also consists of a cox proportional hazard (coxph) model [7] that
is trained to predict patient ranking using a multimodal combination of risk
predictions from the above three sp modules. these coxph models calculate the
corresponding time-to-event evaluation and predict the fusion of patients' risk
as the survival outcome. we evaluate the performance of each module with
concordance probability (c-index), which measures the accuracy of prediction in
terms of ranking the order of survival times [8]. for reference, the c-index of
pesi scores is additionally provided for comparative analysis.in table 1
(baseline), we computed the c-index between the predicted risk of each model and
time-to-event labels. when debiasing is not performed, significant differences
exist among the different modalities, with the image modality exhibiting the
most pronounced deviation, followed by text and pesi variables. the biased
performance of the imaging-based module is likely caused by the richness of
redundant information in images, which includes implicit features such as body
structure and posture that reflect the distribution of different races. this
redundancy leads to model overfitting on race, compromising the fairness of risk
prediction across different races. besides, clinical data in the form of text
reports and pesi variables objectively reflect the patient's physiological
information and the physician's diagnosis, exhibiting smaller race biases in
correlation with survival across different races. moreover, the multimodal
fusion strategy is found to be effective, yielding more relevant survival
outcomes than the clinical gold standard pesi scores.",5
1172,Improving Outcome Prediction of Pulmonary Embolism by De-biased Multi-modality Model,4.0,Experiment,"we validate the proposed de-biased survival prediction frameworks on the
collected multi-modality pe data. the data from 3 institutions are randomly
split the lung region of cpta images is extracted with a slice thickness of 1.25
mm and scaled to n × 512 × 512 pixels [10]. hounsfield units (hu) of all slices
are clipped to the range of [-1000, 900] and applied with zero-centered
normalization. the penet-based imaging backbone consists of a 77-layer 3d
convolutional neural network and linear regression layers. it takes in a sliding
window of 24 slices at a time, resulting in a window-level prediction that
represents the probability of pe for the current slices [11]. the penet is
pre-trained on large-scale ctpa studies and shows excellent pe detection
performance with an auroc of 0.85 on our entire dataset. the 2048 dimensional
features from the last convolution with the highest probability of pe, are
designated as the imaging features.the gatortron [29] uses a transformer-based
architecture to extract features from the clinical text, which was pre-trained
on over 82 billion words of de-identified clinical text. we used the huggingface
library [28] to deploy the 345m-parameter cased model as the clinical report
feature extractor. the outputs from each patient's medical history, clinical
diagnosis, observations, and radiologist impression are separately generated and
concatenated to form the 1024 × 4 features.we build the encoders of the baseline
sp modules and de-biased sp modules with multi-layer perceptron (mlp) neural
networks and relu activation. the mlps with 3 hidden layers are used to encode
image and text features, and another mlps with 2 layers encodes the features of
pesi variables. a fully connected layer with sigmoid activation acts as a risk
classifier c m sur (z m sur ) for survival prediction, where z m sur is the
feature encoded from single modal data. for training the biased and de-biased sp
modules, we collect data from one modality as a batch with synchronized batch
normalization. the sp modules are optimized using the adamw [17] optimizer with
a momentum of 0.9, a weight decay of 0.0005, and a learning rate of 0.001. we
apply early stopping when validation loss doesn't decrease for 600 epochs.
experiments are conducted on an nvidia gv100 gpu.",5
1173,Improving Outcome Prediction of Pulmonary Embolism by De-biased Multi-modality Model,4.1,Results,"table 1 shows the quantitative comparisons of the baseline and de-biased
frameworks with the c-indexes of the multimodal survival predictions. in
general, our framework including de-biased sp modules shows significantly better
predictions in testing set than the pesi-based outcome estimation with c-indexes
of 0.669, 0.654, 0.697, 0.043 for the overall testset, white testset, color
testset and race bias. the de-biased results outperform the baseline in overall
survival c-index and show a lower race bias, especially in imaging-and
fusion-based predictions. the results indicate the effectiveness of the proposed
de-biasing in mitigating race inequity. the results also prove the observations
for the different biases present in different modalities, especially in the ctpa
images containing more abundant race-related information. it also explains the
limited effectiveness of de-biasing the clinical results, which contain less
racial identification. the pre- diction performance based on multiply modalities
is significantly better than the pesi-based outcome estimation. the disentangled
representations, transformed from latent space to a 2d plane via tsne and
color-coded by race [9], are shown in fig. 2. we observe the disentanglement in
the visualization of the id features z id , while the survival features z sur
eliminate the race bias. the lack of apparent race bias observed in both the
original features and those encoded in the baseline can be attributed to the
subordinate role that id features play in the multimodal information. the
kaplan-meier (k-m) survival curve [14], as shown in fig. 3, is used to compare
the survival prediction between high-risk and lowrisk patient groups. the
p-values in the hypothesis test were found to be less than 0.001, which is
considered statistically significant difference. in addition, the predictions of
the de-biased framework show favorable performance, and our multimodal fusion
demonstrates a more pronounced discriminative ability in the k-m survival
analysis compared to the single-modal results.we conducted ablation studies to
examine the effect of the two key components, including swapping feature
augmentation and race-balance resampling. as shown in table 2, the different
training settings show significant differences in survival prediction
performance across modalities. the swapping augmentation provides a strong bias
correction effect for image data with obvious bias. for clinical data, the
resampling generally improves performance in most cases. overall, multimodal
fusion approaches are effective in all training settings, and the coxph model
can actively learn the optimal combination of multimodal features to predict
survival outcomes.",5
1174,Improving Outcome Prediction of Pulmonary Embolism by De-biased Multi-modality Model,5.0,Discussions and Conclusions,"in this work, we developed a de-biased survival prediction framework based on
the race-disentangled representation. the proposed de-biased sp framework, based
on the sota pe detection backbone and large-scale clinical language model, can
predict the pe outcome with a higher survival correlation ahead of the clinical
evaluation index. we detected indications of racial bias in our dataset and
conducted an analysis of the multimodal diversity. experimental results
illustrate that our approach is effective for eliminating racial bias while
resulting in an overall improved model performance. the proposed technique is
clinically relevant as it can address the pervasive presence of racial bias in
healthcare systems and offer a solution for minimizing or eliminating bias
without pausing to evaluate their affection for the models and tools. our study
is significant as it highlights and evaluates the negative impact of racial bias
on deep learning models. the proposed de-biased method has already shown the
capacity to relieve them, which is vital when serving patients with an accurate
analysis. the research in our paper demonstrates and proves that eliminating
racial biases from data improves performance, and yields a more precise and
robust survival prediction tool. in the future, these de-biased sp modules can
be plugged into other models, offering a fairer method to predict survival
outcomes.",5
1176,Enhancing Breast Cancer Risk Prediction by Incorporating Prior Images,1.0,Introduction,"breast cancer impacts women globally [15] and mammographic screening for women
over a certain age has been shown to reduce mortality [7,10,23]. however,
studies suggest that mammography alone has limited sensitivity [22]. to mitigate
this, supplemental screening like mri or a tailored screening interval have been
explored to add to the screening protocol [1,13]. however, these imaging
techniques are expensive and add additional burdens for the patient. recently,
several studies [8,32,33] revealed the potential of artificial intelligence (ai)
to develop a better risk assessment model to identify women who may benefit from
supplemental screening or a personalized screening interval and these may lead
to improved screening outcomes.in clinical practice, breast density and
traditional statistical methods for predicting breast cancer risks such as the
gail [14] and the tyrer-cuzick models [27] have been used to estimate an
individual's risk of developing breast cancer. however, these models do not
perform well enough to be utilized in practical screening settings [3] and
require the collection of data that is not always available. recently, deep
neural network based models that predict a patient's risk score directly from
mammograms have shown promising results [3,8,9,20,33]. these models do not
require additional patient information and have been shown to outperform
traditional statistical models.when prior mammograms are available, radiologists
compare prior exams to the current mammogram to aid in the detection of breast
cancer. several studies have shown that utilizing past mammograms can improve
the classification performance of radiologists in the classification of benign
and malignant masses [11,25,26,29], especially for the detection of subtle
abnormalities [25]. more recently, deep learning models trained on both prior
and current mammograms have shown improved performance in breast cancer
classification tasks [24]. integrating prior mammograms into deep learning
models for breast cancer risk prediction can provide a more comprehensive
evaluation of a patient's breast health.in this paper, we introduce a deep
neural network that makes use of prior mammograms, to assess a patient's risk of
developing breast cancer, dubbed prime+ (prior mammogram enabled risk
prediction). we hypothesize that mammographic parenchymal pattern changes
between current and prior allow the model to better assess a patient's risk. our
method is based on a transformer model that uses attention [30], similar to how
radiologists would compare current and prior mammograms.the method is trained
and evaluated on a large and diverse dataset of over 9,000 patients and shown to
outperform a model based on state-of-the art risk prediction techniques for
mammography [33]. although previous models such as lrp-net and radifusion [5,34]
have utilized prior mammograms, prime+ sets itself apart by employing an
attention mechanism to extract information about the prior scan.",5
1177,Enhancing Breast Cancer Risk Prediction by Incorporating Prior Images,2.1,Risk Prediction,"survival analysis is done to predict whether events will occur sometime in the
future. the data comprises three main elements: features x, time of the event t,
and the occurrence of the event e [18]. for medical applications, x typically
represents patient information like age, family history, genetic makeup, and
diagnostic test results (e.g., a mammogram). if the event has not yet occurred
by the end of the study or observation period, the data is referred to as
right-censored (fig. 1).we typically want to estimate the hazard function h(t),
which measures the rate at which patients experience the event of interest at
time t, given that they have survived up to that point. the hazard function can
be expressed as the limit of the conditional probability of an event t occurring
within a small time interval [t, t + δt), given that the event has not yet
occurred by time t: the cumulative hazard function h(t) is another commonly used
function in survival analysis, which gives the accumulated probability of
experiencing the event of interest up to time t. this function is obtained by
integrating the hazard function over time from 0 to t: h(t) = t 0 h(s)ds.",5
1178,Enhancing Breast Cancer Risk Prediction by Incorporating Prior Images,2.2,Architecture Overview,"we build on the current state-of-the art mirai [33] architecture, which is
trained to predict the cumulative hazard function. we use an imagenet pretrained
resnet-34 [12] as the image feature backbone. the backbone network extracts
features from the mammograms, and the fully connected layer produces the final
feature vector x. we make use of two additional fully connected layers to
calculate base hazard θ b and time-dependent hazard θ u , respectively. the
predicted cumulative hazard is obtained by adding the base hazard and
time-dependent hazard, according to:when dealing with right-censored data, we
use an indicator function δ i (t) to determine whether the information for
sample i at time t should be included in the loss calculation or not. this helps
us exclude unknown periods and only use the available information. it is defined
as follows:here, e i is a binary variable indicating whether the event of
interest occurs for sample i (i.e., e i = 1) or not (i.e., e i = 0), and c i is
the censoring time for sample i, which is the last known time when the sample
was cancer-free.we define the ground-truth h is a binary vector of length t max
, where t max is the maximum observation period. specifically, h(t) is 1 if the
patient is diagnosed with cancer within t years and 0 otherwise. we use binary
cross entropy to calculate the loss at time t for sample i:. the total loss is
defined as:here, n is the number of exams in the training set. the goal of
training the model is to minimize this loss function, which encourages the model
to make accurate predictions of the risk of developing breast cancer over time.",5
1180,Enhancing Breast Cancer Risk Prediction by Incorporating Prior Images,3.1,Dataset,"we compiled an in-house mammography dataset comprising 16,113 exams (64,452
images) from 9,113 patients across institutions from the united states, gathered
between 2010 and 2021. each mammogram includes at least one prior mammogram. the
dataset has 3,625 biopsy-proven cancer exams, 5,394 biopsyproven benign exams,
and 7,094 normal exams. mammograms were captured using hologic (72.3%) and
siemens (27.7%) devices. we partitioned the dataset by patient to create
training, validation, and test sets. the validation set contains 800 exams (198
cancer, 210 benign, 392 normal) from 400 patients, and the test set contains
1,200 exams (302 cancer, 290 benign, 608 normal) from 600 patients. all data was
de-identified according to the u.s hhs safe harbor method. therefore, the data
has no phi (protected health information) and irb (institutional review board)
approval is not required.",5
1183,Enhancing Breast Cancer Risk Prediction by Incorporating Prior Images,3.4,Results,"ablation study. to better understand the merit of the transformer decoder, we
first performed an ablation study on the architecture. our findings, summarized
in table 1, include two sets of results: one for all exams in the test set and
the other by excluding cancer exams within 180 days of cancer diagnosis which
are likely to have visible symptoms of cancer, by following a previous study
[33]. this latter set of results is particularly relevant as risk prediction
aims to predict unseen risks beyond visible cancer patterns. we also compare our
method to two other models, the state-of-the-art baseline and prime models.as
shown in the top rows in table 1, the baseline obtained a c-index of 0.68 (0.65
to 0.71). by using the transformer decoder to jointly model prior images, we
observed improved c-index from 0.70 (0.67 to 0.73) to 0.73 (0.70 to 0.76). the
c-index as well as all auc differences between the baseline and the prime+ are
all statistically significant (p < 0.05) except the 4-year auc where we had a
limited number of test cases.we observe similar performance improvements when
evaluating cases with at least 180 days to cancer diagnosis. interestingly, the
c-index as well as timedependent aucs of all three methods decreased compared to
when evaluating using all cases. the intuition behind this result is that
mammograms taken near the cancer diagnosis (<180 days) likely contain visible
signs of cancer and thus the task of risk prediction is easier. the model must
learn patterns of risk, not lastly, we empirically confirm that a transformer
decoder effectively models spatial relations between prior and current
mammograms by demonstrating consistent performance improvements of prime+ across
both short-term and longterm risk prediction settings. our results suggest that
incorporating changes in patients using prior mammograms and a transformer
decoder improves the performance of breast cancer risk prediction
models.analysis based on density. to better understand why adding prior images
improves performance, we divided our test set into subgroups to examine the
performance of the baseline model and the prime+ model on each of these groups.
mammographic breast density is one of the most important risk factor to predict
breast cancer [19,31]. women with dense breasts have a four-to six-fold higher
risk of breast cancer [2]. the addition of mammographic breast density has
improved the performance traditional breast cancer risk models [4] and can
therefore help us understand why the addition of prior images works.mammographic
breast density was determined using the breast imaging reporting and data system
(bi-rads) composition classification. bi-rads category a, b are defined as fatty
breasts and bi-rads category c, d are classified as dense breasts. to determine
the density category, we employed an internally developed density prediction
model, as most exams lack bi-rads ground truth. this model achieved an accuracy
of 0.81 on the internal density validation set.we categorized the exams into two
groups based on changes in density: ""change"" and ""no change"". density change was
defined according to whether the bi-rads category changed in the current image
as compared to the prior image. as shown in table 2, the baseline model performs
poorly for ""change"", with a c-index of 0.63 (0.49 to 0.77), especially for
long-term risk prediction, with 3-year auc of 0.56 (0.40 to 0.72). this suggests
that the baseline model has limitations in accurately predicting long-term risk
when there is a density change from the prior exam. however, prime+ is able to
predict long-term risk accurately even when a density change has occurred
(3-year auc = 0.74 (0.60 to 0.88)), by learning to refer previous exams
properly. this demonstrates the potential usefulness of incorporating past
mammogram information into breast cancer risk prediction models. thus, we
believe that incorporating prior exams is important to identify changes in
texture which are important for long term risk prediction (table 3). lastly, we
divided the exams based on the level of breast density, with a fatty group
consisting of density a and b, and a dense group consisting of density c and d.
both the baseline and prime+ performs better in fatty group than dense group. we
suspect this is because deep neural networks generally work better on low
density images given that visual cues of cancer in images with lower breast
density are more clearly visible.",5
1186,Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,0,abbreviated as PARE.,"at the intra-level, the contextual information of the nodules provides clues
about their shape, size, and surroundings, and the integration of this
information can facilitate a more reliable diagnosis of whether they are benign
or malignant. motivated by this, we first segment the context structure, i.e.,
nodule and its surroundings, and then aggregate the context information to the
nodule representation via the attention-based dependency modeling, allowing for
a more comprehensive understanding of the nodule itself. at the inter-level, we
hypothesize that the diagnosis process does not have to rely solely on the
current nodule itself, but can also find clues from past learned cases. this is
similar to how radiologists rely on their accumulated experience in clinical
practice. thus, the model is expected to have the ability to store and recall
knowledge, i.e., the knowledge learned can be recorded in time and then recalled
as a reference for comparative analysis. to achieve this, we condense the
learned nodule knowledge in the form of prototypes, and recall them to explore
potential inter-level clues as an additional discriminant criterion for the new
case. to fulfill both ldct and ncct screening needs, we curate a large-scale
lung nodule dataset with pathology-or follow-up-confirmed benign/malignant
labels. for the ldct, we annotate more than 12,852 nodules from 8,271 patients
from the nlst dataset [14]. for the ncct, we annotate over 4,029 nodules from
over 2,565 patients from our collaborating hospital. experimental results on
several datasets demonstrate that our method achieves outstanding performance on
both ldct and ncct screening scenarios.our contributions are summarized as
follows: (1) we propose context parsing to extract and aggregate rich contextual
information for each nodule. (2) we condense the diagnostic knowledge from the
learned nodules into the prototypes and use them as a reference to assist in
diagnosing new nodules. (3) we curate the largest-scale lung nodule dataset with
high-quality benign/malignant labels to fulfill both ldct and ncct screening
needs. (4) our method achieves advanced malignancy prediction performance in
both screening scenarios (0.931 auc), and exhibits strong generalization in
external validation, setting a new state of the art on lungx (0.801 auc).",5
1193,Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,3.1,Datasets and Implementation Details,"data collection and curation: nlst is the first large-scale ldct dataset for
low-dose ct lung cancer screening purpose [14]. there are 8,271 patients
enrolled in this study. an experienced radiologist chose the last ct scan of
each for l = 1, ..., l do 12:cross prototype attention 13:end for 15:",5
1195,Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,0,17:,"j ← seg loss(m, s) + 3 i=1 cls loss(y, p i ) update loss18: end for patient, and
localized and labeled the nodules in the scan as benign or malignant based on
the rough candidate nodule location and whether the patient develops lung cancer
provided by nlst metadata. the nodules with a diameter smaller than 4mm were
excluded. the in-house cohort was retrospectively collected from 2,565 patients
at our collaborating hospital between 2019 and 2022. unlike nlst, this dataset
is noncontrast chest ct, which is used for routine clinical care. segmentation
annotation: we provide the segmentation mask for our in-house data, but not for
the nlst data considering its high cost of pixel-level labeling. the nodule mask
of each in-house data was manually annotated with the assistance of ct labeler
[20] by our radiologists, while other contextual masks such as lung, vessel, and
trachea were generated using the totalsegmentator [21].",5
1196,Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,0,Train-Val-Test:,"the training set contains 9,910 (9,413 benign and 497 malignant) nodules from
6,366 patients at nlst, and 2,592 (843 benign and 1,749 malignant) nodules from
2,113 patients at the in-house cohort. the validation set contains 1,499 (1,426
benign and 73 malignant) nodules from 964 patients at nlst. the nlst test set
has 1,443 (1,370 benign and 73 malignant) nodules from 941 patients. the
in-house test set has 1,437 (1,298 benign and 139 malignant) nodules from 452
patients. we additionally evaluate our method on the lungx [2] challenge
dataset, which is usually used for external validation in previous work
[6,11,24]. lungx contains 83 (42 benign and 41 malignant) nodules, part of which
(13 scans) were contrast-enhanced. segmentation: we also evaluate the
segmentation performance of our method on the public nodule segmentation dataset
lidc-idri [3], which has 2,630 nodules with nodule segmentation mask. evaluation
metrics: the area under the receiver operating characteristic curve (auc) is
used to evaluate the malignancy prediction performance.implementation: all
experiments in this work were implemented based on the nnunet framework [8],
with the input size of 32 × 48 × 48, batch size of 64, and total training
iterations of 10k. in the context patch embedding, each patch token is generated
from a window of 8 × 8 × 8. the hyper-parameters of pare are empirically set
based on the ablation experiments on the validation set. for example, the
transformer layer is set to 4 in both sca and cpa modules, and the number of
prototypes is fixed to 40 by default. more details can be found in the ablation.
due to the lack of manual annotation of nodule masks for the nlst dataset, we
can only optimize the segmentation task using our in-house dataset, which has
manual nodule masks.",5
1204,M&M: Tackling False Positives in Mammography with a Multi-view and Multi-instance Learning Sparse Detector,3.0,Experiments,"implementation details. we use pytorch 1.10. the training settings follow sparse
r-cnn [24]. we apply random horizontal flipping and random rotation. we resize
the images' shorter edges to 2560 with the larger edges no longer than 3328. we
utilize a coco-pretrained pvt-b2-li backbone [30]. we use adamw optimizer with 5
× 10 -5 learning rate and 0.0001 weight decay. the model is trained for 9000
iterations, and the learning rate is scaled by 0.1 at the 6750 and 8250
iterations. each batch contains 16 breasts (32 images). we employ a 1:1 sampling
ratio between unannotated and annotated images.datasets. we utilize three 2d
digital mammography datasets: (1) optimam : a development dataset derived from
the optimam database [7], which is funded by cancer research uk. we split the
data into train/val/test with an 80:10:10 ratio at the patient level; (2)
inhouse-a: an evaluation dataset collected from a u.s. multi-site mammography
operator; (3) inhouse-b : an evaluation dataset collected from a u.s. academic
hospital (see [18], sec. 2.2 for more details on the inhouse datasets). we also
utilize two film mammography datasets: (4) ddsm: a dataset maintained at the
university of south florida [8]. we followed the methods by [3,5,13,16] to split
the test set; (5) cbis-ddsm: a curated subset of ddsm [9]. we only include
breasts that have one cc view and one mlo view. dataset statistics are reported
in table 1.metrics. we report average precision with intersection over union
from 0.25 to 0.75. ap mb denotes average precision on the set of annotated
malignant and benign images. ap denotes average precision when all data is
included. we report free response operating characteristic (froc) curves and
recalls at various fp/image (r@t). following [3,5,16,29], a proposal is
considered true positive if its center lies within the ground truth box. for
classification, we report the area under the receiver operating characteristic
curve (auc).detection results. gmic [23] 0.911 0.896 0.814 0.815 0.796 hct [25]
0.923 0.912 0.816 0.817 0.793 m&m (ours) 0.960 0.942 0.920 0.910 0.898resnet50
[14] 0.724 shared resnet [31] 0.735 phresnet50 [14] 0.739 cross-view transformer
[27] 0.803 * m&m (ours) 0.88323 points (pt) between excluding and including
negative images. large δ means the models are producing too many fps on negative
images. sparse r-cnn [24] generalizes significantly better with a gap of 17pt.
this shows the importance of sparsity for reducing fp. by adding both multi-view
and mil, m&m successfully reduces the δ gap to 3.5pt. with this performance gap
closed, m&m is able to achieve a high recall of 87.7% at just 0.1 fp/image.
figure 1a compares m&m with recent literature evaluated on ddsm. m&m adopts the
same ddsm splits used by [3,12,13,16,33], while [5,21,32] use other splits. m&m
(87% r@0.5) outperforms all recent sota with the same test split, including 2022
sota [33] (83% r@0.5), by at least 4%.classification results. table 3a reports
m&m's breast-level and exam-level classification results on optimam and the two
inhouse datasets. we use gmic [23] and hct [25] as baselines since they are
open-sourced classifiers developed for mammography. all three models were
trained only on optimam. for all models, the breast-level score is the average
of the cc score and mlo score, while the exam-level score is the max of the left
breast score and right breast score. both baseline models suffer large
generalization drops of approximately 3b compares m&m with recent literature
reporting on the public cbis-ddsm dataset. in particular, m&m outperforms the
cross-view transformer [27] and phresnet50 [14] by 0.08 and 0.14 breast auc,
respectively. qualitative evaluation. figure 3 presents a qualitative evaluation
of the multi-view module. with multi-view, m&m produces a tighter box on the cc
view and recovers a missed finding on the mlo view. ablation studies. figure 4
presents ablation results using the optimam validation split. on the left, we
demonstrate how each component of m&m contributes to closing the gap δ between
evaluating with and without negative images. notably, without using any extra
training samples, multi-view reasoning reduces δ to only -5.9pt (row 3). mil
allows the model to train with significantly more negative images, reducing δ to
-3.6pt (row 4). on the right of fig. 4, the froc curves show how each component
of m&m improves recall significantly at low fp/image. in particular, m&m's
recall at 0.1fp/image is 86.3%, +21.2% over vanilla sparse r-cnn.further
studies. in the appendix, we present more qualitative evaluation as well as
further ablation studies on (1) number of learnable proposals, (2) different mil
schemes, (3) backbone choices and (4) positional encoding.",5
1208,Reversing the Abnormal: Pseudo-Healthy Generative Networks for Anomaly Detection,1.0,Introduction,"the early detection of lesions in medical images is critical for the diagnosis
and treatment of various conditions, including neurological disorders. stroke is
a leading cause of death and disability, where early detection and treatment can
significantly improve patient outcomes. however, the quantification of lesion
burden is challenging and can be time-consuming and subjective when performed
manually by medical professionals [14]. while supervised learning methods
[10,11] have proven to be effective in lesion segmentation, they rely heavily on
large fig. 1. overview of phanes (see fig. 2). our method can use both expert
annotatedor unsupervised generated masks to reverse and segment anomalies
annotated datasets for training and tend to generalize poorly beyond the learned
labels [21]. on the other hand, unsupervised methods focus on detecting patterns
that significantly deviate from the norm by training only on normal data.one
widely used category of unsupervised methods is latent restoration methods. they
involve autoencoders (aes) that learn low-dimensional representations of data
and detect anomalies through inaccurate reconstructions of abnormal samples
[17]. however, developing compact and comprehensive representations of the
healthy distribution is challenging [1], as recent studies suggest aes perform
better reconstructions on out-of-distribution (ood) samples than on training
samples [23]. various techniques have been introduced to enhance representation
learning, including discretizing the latent space [15], disentangling
compounding factors [2], and variational autoencoders (vaes) that introduce a
prior into the latent distribution [26,29]. however, methods that can enforce
the reconstruction of healthy generally tend to produce blurry
reconstructions.in contrast, generative adversarial networks (gans) [8,18,24]
are capable of producing high-resolution images. new adversarial aes combine
vaes' latent representations with gans' generative abilities, achieving sota
results in image generation and outlier detection [1,5,6,19]. nevertheless,
latent methods still face difficulties in accurately reconstructing data from
their low-dimensional representations, causing false positive detections on
healthy tissues.several techniques have been proposed that make use of the
inherent spatial information in the data rather than relying on constrained
latent representations [12,25,30]. these methods are often trained on a pretext
task, such as recovering masked input content [30]. de-noising aes [12] are
trained to eliminate synthetic noise patterns, utilizing skip connections to
preserve the spatial information and achieve sota brain tumor segmentation.
however, they heavily rely on a learned noise model and may miss anomalies that
deviate from the noise distribution [1]. more recently, diffusion models [9]
apply a more complex de-noising process to detect anomalies [25]. however, the
choice and granularity of the applied noise is crucial for breaking the
structure of anomalies [25]. adapting the noise distribution to the diversity
and heterogeneity of pathology is inherently difficult, and even if achieved,
the noising process disrupts the structure of both healthy and anomalous regions
throughout the entire image.in related computer vision areas, such as industrial
inspection [3], the topperforming methods do not focus on reversing anomalies,
but rather on detecting them by using large nominal banks [7,20], or pre-trained
features from large natural imaging datasets like imagenet [4,22]. salehi et al.
[22] have employed multi-scale knowledge distillation to detect anomalies in
industrial and medical imaging. however, the application of these networks in
medical anomaly segmentation, particularly in brain mri, is limited by various
challenges specific to the medical imaging domain. they include the variability
and complexity of normal data, subtlety of anomalies, limited size of datasets,
and domain shifts.this work aims to combine the advantages of constrained latent
restoration for understanding healthy data distribution with generative
in-painting networks. unlike previous methods, our approach does not rely on a
learned noise model, but instead creates masks of probable anomalies using
latent restoration. these guide generative in-painting networks to reverse
anomalies, i.e., preserve healthy tissues and produce pseudo-healthy in-painting
in anomalous regions. we believe that our proposed method will open new avenues
for interpretable, fast, and accurate anomaly segmentation and support various
clinical-oriented downstream tasks, such as investigating progression of
disease, patient stratification and treatment planning. in summary our main
contributions are:• we investigate and measure the ability of sota methods to
reverse synthetic anomalies on real brain t1w mri data. • we propose a novel
unsupervised segmentation framework, that we call phanes, that is able to
preserve healthy regions and utilize them to generate pseudo-healthy
reconstructions on anomalous regions. • we demonstrate a significant advancement
in the challenging task of unsupervised ischemic stroke lesion segmentation.",5
1215,SHISRCNet: Super-Resolution and Classification Network for Low-Resolution Breast Cancer Histopathology Image,1.0,Introduction,"breast cancer is one of the high-mortality cancers among women in the 21st
century. every year, 1.2 million women around the world suffer from breast
cancer and about 0.5 million die of it [3]. accurate identification of cancer
types will make a correct assessment of the patient's risk and improve the
chances of survival. however, the traditional analysis method is time-consuming,
as it mainly depends on the experience and skills of the doctors. therefore, it
is essential to develop computer-aided diagnosis (cadx) for assisting doctors to
realize the rapid detection and classification.due to being collected by various
devices, the resolution of histopathological images extracted may not always be
high. low-resolution (lr) images lack of lots of details, which will have an
important impact on doctors' diagnosis. considering the improvement of
histopathological images' acquisition equipment will cost lots of money while
significantly increasing patients' expense of detection. the super-resolution
(sr) algorithms that improve the resolution of lr images at a small cost can be
a practical solution to assist doctors in diagnosis. at present, most single
super-resolution methods only have fixed receptive fields [7,10,11,18]. these
models cannot capture multi-scale features and do not solve the problems caused
by lr in various magnification factors well. mrc-net [6] adopted lstm [9] and
multi-scale refined context to improve the effect of reconstructing
histopathological images. it considered the problem of multi-scale, but only
fused two scales features. this limits its performance in the scenarios with
various magnification factors. therefore, designing an appropriate feature
extraction block for sr of the histopathological images is still a challenging
task.in recent years, a series of deep learning methods have been proposed to
solve the breast cancer histopathological image classification issue by the
highresolution (hr) histopathological images. [12,21,22] improved the specific
model structure to classify breast histopathology images, which showed a
significant improvement in recognition accuracy compared with the previous works
[1,20]. ssca [24] considered the problem of multi-scale feature extraction which
utilized feature pyramid network (fpn) [15] and attention mechanism to extract
discriminative features from complex backgrounds. however, it only concatenates
multi-scale features and does not consider the problem of feature fusion. so it
is still worth to explore the potential of extraction and fusion of multi-scale
features for breast images classification.to tackle the problem of lr breast
cancer histopathological images reconstruction and diagnosis, we propose the
single histopathological image super-resolution classification network
(shisrcnet) integrating super-resolution (sr) and classification (cf) modules.
the main contributions of this paper can be described as follows:(1) in the sr
module, we design a new block called multi-features extraction block (mfeblock)
as the backbone. mfeblock adopts multi-scale receptive fields to obtain
multi-scale features. in order to better fuse multi-scale features, a new fusion
method named multi-scale selective fusion (msf) is used for multi-scale
features. these make mfeblock reconstruct lr images into sr images well.(2) the
cf module completes the task of image classification by utilizing the sr images.
like sr module, it also needs to extract multi-scale features. the difference is
that the cf module can use the method of downsampling to capture multi-scale
features. so we combine the multi-scale receptive fields (sknet) [13] with the
feature pyramid network (fpn) to achieve the feature extraction of this module.
in fpn, we design a cross-scale selective fusion block (csfblock) to fuse
features of different scales.(3) through the joint training of these two
designed modules, the superresolution and classification of low-resolution
histopathological images are integrated into our model. for improving the
performance of cf module and reducing the error caused by the reconstructed sr
images, we introduce hr images to cf module in the training stage. the
experimental results demonstrate that the effects of our method are close to
those of sota methods that take hr breast cancer histopathological images as
inputs.",5
1228,Text-Guided Foundation Model Adaptation for Pathological Image Classification,4.0,Experimental Settings,"dataset. we adopt the patchgastric [25] dataset, which includes
histopathological image patches extracted from h&e stained whole slide images
(wsi) of stomach adenocarcinoma endoscopic biopsy specimens. there are 262,777
patches of size 300 × 300 extracted from 991 wsis at x20 magnification. the
dataset contains 9 subtypes of gastric adenocarcinoma. we choose 3 major
subtypes including ""well differentiated tubular adenocarcinoma"", ""moderately
differentiated tubular adenocarcinoma"", and ""poorly differentiated
adenocarcinoma"" to form a 3-class grading-like classification task with 179,285
patches from 693 wsis. we randomly split the wsis into train (20%) and
validation (80%) subsets for measuring the model performance. to extend our
evaluation into the real-world setting with insufficient data, we additionally
choose 1, 2, 4, 8, or 16 wsis with the largest numbers of patches from each
class as the training set.the evaluation metric is patient-wise accuracy, where
the prediction of a wsi is obtained by a soft vote over the patches, and
accuracy is averaged class-wise.implementation. we use clip vit-b/16 [5] as the
visual backbone, with input image size 224 × 224, patch size 16 × 16, and
embedding dimension d v = 512. we adopt biolinkbert-large [11] as the biomedical
language model, with embedding dimension d l = 1, 024. to show the extensibility
of our approach, we additionally test on vision encoders including imagenet-21k
vit-b/16 [24,26] and intern vit-b/16 [6], and biomedical language model
biobert-large [10].our implementation is based on clip 1 , huggingface2 and
mmclassification3 .training details. prompt length p is set to 1. we resize the
images to 224×224 to fit the model and follow the original data pipeline in
patchgastric [25]. a class-balanced sampling strategy is adopted by choosing one
image from each class in turn. training is done with 1,000 iterations of
stochastic gradient descent (sgd), and the mini-batch size is 128, requiring
11.6 gb of gpu memory and 11 min on two nvidia geforce rtx 2080 ti gpus. all our
experiment results are averaged on 3 random seeds unless otherwise specified.",5
1231,Distributionally Robust Image Classifiers for Stroke Diagnosis in Accelerated MRI,1.0,Introduction,"magnetic resonance imaging (mri) has been extensively applied to clinical
diagnosis [16]. compared with computed tomography (ct), a brain mri is more
sensitive for multiple stroke types [3], therefore considered as the gold
standard for stroke diagnosis. nevertheless, the long acquisition time for a
brain mri (20 to 30 min) imposes challenges, especially in cases of acute stroke
where rapid diagnosis is essential and patient movement during this distressing
period of time commonly limits evaluation. as a result, mri acceleration
techniques have been developed to achieve more rapid diagnosis, increasing
resource availability while reducing costs [14,18]. a k-space sub-sampling (ks)
approach serves as a simple mri acceleration solution [20], compared with other
hardware-based acceleration methods. however, the signal loss by ks leads to
blurry reconstructed mr images that are less than ideal for a reliable clinical
diagnosis.artificial intelligence (ai) plays an increasingly important role in
mri-based diagnosis, for both mr image reconstruction and clinical decision
making. deep neural networks (dnn) were trained to reconstruct the mr images
from the sub-sampled k-space [10,13], which provides a better reconstruction
than the inverse fast fourier transform (ifft). nevertheless, detailed
information in the brain may still be lost in the reconstructed mr images due to
the signal sparsity in the k-space. on the other hand, traditional convolutional
neural network (cnn) [15] and the latest vision transformer (vit)-based [6]
predictive models have shown impressive prediction accuracy on stroke diagnosis
tasks, such as slice classification and lesion segmentation [7,11]. however,
these dnns trained on clean images through empirical risk minimization (erm) are
vulnerable to perturbations in the input images [2]. whatever the reconstruction
method used, even the slightest perturbation in accelerated mr images can lead
to a wrong stroke prediction from the ai models. therefore, building robust dnn
models to handle the perturbed mr image input is important for mri
acceleration.in this paper, we introduce a distributionally robust learning
(drl)-based approach [4] into the deep mr image classifier training, in order to
improve the model robustness to the image perturbation resulting from the signal
sparsity in accelerated mri. compared with erm, drl is an optimization method
minimizing the worst-case loss over an ambiguity set, therefore, can tolerate
outliers in the data [5]. we implemented drl to different linear layers in deep
cnn/vit classifiers, and applied a randomized training approach to improve the
training efficiency. our results show that on a real-world dataset, drl can
significantly improve the stroke classification performance of erm and other
baseline defensive training methods, when the signal sparsity and noise in
accelerated mri are generated by the cartesian undersampling (cu) method [20]
and white gaussian noise (wgn). we further show that in highly perturbed mr
images where the erm model and even clinicians cannot give a reliable diagnosis,
our drl model can still correctly recognize stroke, which establishes that our
method can assist accelerated mri diagnosis.",5
1234,Distributionally Robust Image Classifiers for Stroke Diagnosis in Accelerated MRI,3.1,Experimental Materials and Settings,"our dataset included mri brain scans from 226 patients performed at an urban
tertiary referral academic medical center that is a comprehensive stroke center.
clinical scans of adult patients aged 18-89 years with recent (acute or
subacute) strokes were identified between 1/1/2013 and 1/1/2021 for inclusion in
this study via a search of the philips performance bridge. scans meeting this
criteria were downloaded and simultaneously anonymized to preserve patient
anonymity and prevent disclosure of protected health information as part of this
irb exempt study. no patient demographic information was retained for the scans,
as it was considered to represent an unnecessary risk for accidental release of
protected health information. the diffusion weighted images with a gradient of
b=1000 were utilized for the analysis (see the supplement1 for information about
the mri scanner and parameter settings). each mr image contains multiple slices,
and every slice was annotated as normal or stroke by a board-certified
neuroradiologist with a subspecialty certification. annotation of the strokes
was performed on the diffusion weighted images using itk-snap (ver. 3.80) [19],
and all included mri examinations were reviewed by the neuroradiologist during
the annotation process to ensure that the images were of diagnostic quality
without significant motion degradation or other artifacts. to avoid the
dependency among the slices from the same subject, we applied a 2-d acquisition
during the mr imaging, and implemented a slice-level mr image preprocessing.
while the whole dataset includes 4,883 (74.7%) normal slices and 1,650 (25.3%)
stroke slices, we further randomly split them into training/validation/test sets
using the ratio 80%/10%/10%. for the training set, we implemented data
augmentation strategies by rotating or flipping each slice. finally, the
training/validation/test set contains 31,356/653/654 slices, correspondingly. we
implemented drl to both cnn and vit models. for the cnn model, we used a
resnet-18 [9] architecture, while for the vit model, we first pre-trained a
4-layer vit using a self-supervised pre-training method called masked
autoencoder (mae) [8], using the t1/t2-weighted brain mr images in the ixi
dataset [1]. mae pre-training first randomly masks 75% of the image patches in
an mri slice input, and then uses a vit encoder-decoder architecture to
reconstruct the masked mri patches, in order to learn the dependency among
different locations in the brain. after 400 pre-training epochs, an overall
satisfying reconstruction result can be observed in fig. 2.to evaluate the
binary classification performance of different models, we use the area under the
receiver operating characteristic (auroc) curve as our main metric. as our
dataset is unbalanced, we also considered the area under precision-recall curve
(auprc). we ran the experiments 3 times using different random seeds. the
training of our dnns were implemented on 3 nvidia rtx a6000 (48gb vram) gpus,
and each drl training epoch can be completed within 0.03 gpu hours. we used a
learning rate of 1 × 10 -5 and batchsize of 128 for drl training, while no
weight decay was applied. to solve the lmi problem in (5), we used sdpt3 v4.0
[17] as the solver. we set the cu perturbation with the acceleration factor of
4, 6, 8, 12 with the central fraction of 8%, 6%, 4% and 2% in k-space
respectively, and the remaining parts were chosen randomly in the peripheral
region accordingly.",5
1271,MUVF-YOLOX: A Multi-modal Ultrasound Video Fusion Network for Renal Tumor Diagnosis,1.0,Introduction,"renal cancer is the most lethal malignant tumor of the urinary system, and the
incidence is steadily rising [13]. conventional b-mode ultrasound (us) is a good
screening tool but can be limited in its ability to characterize complicated
renal lesions. contrast-enhanced ultrasound (ceus) can provide information on
microcirculatory perfusion. compared with ct and mri, ceus is radiation-free,
cost-effective, and safe in patients with renal dysfunction. due to these
benefits, ceus is becoming increasingly popular in diagnosing renal lesions.
however, recognizing important diagnostic features from ceus videos to diagnose
lesions as benign or malignant is non-trivial and requires lots of experience.to
improve diagnostic efficiency and accuracy, many computational methods were
proposed to analyze renal us images and could assist radiologists in making
clinical decisions [6]. however, most of these methods only focused on
conventional b-mode images. in recent years, there has been increasing interest
in multi-modal medical image fusion [1]. directly concatenation and addition
were the most common methods, such as [3,4,12]. these simple operations might
not highlight essential information from different modalities. weight-based
fusion methods generally used an importance prediction module to learn the
weight of each modality and then performed sum, replacement, or exchange based
on the weights [7,16,17,19]. although effective, these methods did not allow
direct interaction between multi-modal information. to address this,
attention-based methods were proposed. they utilized cross-attention to
establish the feature correlation of different modalities and self-attention to
focus on global feature modeling [9,18]. nevertheless, we prove in our
experiments that these attentionbased methods may have the potential risks of
entangling features of different modalities.in practice, experienced
radiologists usually utilize dynamic information on tumors' blood supply in ceus
videos to make diagnoses [8]. previous researches have proved that temporal
information is effective in improving the performance of deep learning models.
lin et al. [11] proposed a network for breast lesion detection in us videos by
aggregating temporal features, which outperformed other image-based methods.
chen et al. [2] showed that ceus videos can provide more detailed blood supply
information of tumors allowing a more accurate breast lesion diagnosis than
static us images.in this work, we propose a novel multi-modal us video fusion
network (muvf-yolox) based on ceus videos for renal tumor diagnosis. our main
contributions are fourfold. (1) to the best of our knowledge, this is the first
deep learning-based multi-modal framework that integrates both b-mode and
ceusmode information for renal tumor diagnosis using us videos. (2) we propose
an attention-based multi-modal fusion (amf) module consisting of cross-attention
and self-attention blocks to capture modality-invariant and modality-specific
features in parallel. (3) we design an object-level temporal aggregation (ota)
module to make video-based diagnostic decisions based on the information from
multi-frames. (4) we build the first multi-modal us video datatset containing
b-mode and ceus-mode videos for renal tumor diagnosis. experimental results show
that the proposed framework outperforms single-modal, single-frame, and other
state-of-the-art methods in renal tumor diagnosis.",5
1279,Patients and Slides are Equal: A Multi-level Multi-instance Learning Framework for Pathological Image Analysis,1.0,Introduction,"pathological image analysis is a vital area of research within medical image
analysis, focused on utilizing computer technology to aid doctors in diagnosing
and treating diseases by analyzing pathological tissue slide images [5].
advancements in pathological image analysis have been made in early cancer
diagnosis, tumor localization, and grading, and treatment planning [3,10].
multi-instance learning [2] is the primary analysis method used, which involves
analyzing tasks based on slide labels and patches. despite this, the clinical
pathological analysis presents certain challenges and complexities, with the
ultimate diagnosis relying on patients rather than slides.specifically, in
clinical problems of pathological image analysis, doctors usually summarize
patient-level labels based on slide labels as the diagnostic results [1,6]. for
example, for the pathological discrimination diagnosis task of intestinal
tuberculosis(itb) and crohn's desease(cd), the categories of postoperative
slides are divided into three types (normal, cd, itb), and doctors will
summarize the binary results of patients (itb or cd) based on slide-level labels
[6]. similar situations exist in other tasks, such as the classification of
breast cancer metastases in lymph nodes, where slide categories may have
different classifications, and the corresponding diagnosis of the same patient
is whether the cancer has spread to the regional lymph nodes (n-stage) [1].
therefore, as shown in fig. 1, actual pathological image analysis involves the
relationships of patches, slides, and patients, which is called a multi-level
multi-instance learning (ml-mil) problem. among them, for patients and slides,
patients are bags while slides are instances, and for slides and patches, slides
are bags while patches are instances.there are generally two methods to solve
the ml-mil problem. the first method is to directly average the prediction
values of slides or take the maximum prediction value [9]. this method is
relatively simple, but the information exchange between slides is not fully
utilized, which may lead to errors in the summary result. the second method is
to treat slide-patient as a new mil problem according to the traditional mil
thinking, where slides are regarded as instances and patient labels as bags.
although this method seems reasonable, the number of patients is usually
relatively small, and deep learning models usually require a large amount of
data for training. therefore, the insufficient number of samples at the
slide-patient level may make it difficult for the model to learn enough
information.to address the multi-level multi-instance learning (ml-mil) problem
in medical field, we propose a novel framework called patients and slides are
equal (p&sre). inspired by the iterative labeling process in medical diagnosis,
this framework treats patients and slides as instances at the same level and
uses transformers and attention mechanisms to build connections between them.
this simple yet effective method allows for interaction between patient-level
and slidelevel information to correct their respective features and improve
classification performance. our framework consists of two steps: first, at the
patch-slide level, a common mil framework is used to train a mil neural network
and obtain slide-level feature vectors; then, at the slide-patient level, we use
self-attention mechanisms to combine the slides of the same patient into
patient-level feature vectors, and treat these patient-level feature vectors
together with all slide-level feature vectors of the same patient as instances
at the same level, which are inputted into transformers for feature interaction
and prediction of patient-and slide-level labels. our method can effectively
solve the problem of difficult training due to the scarcity of samples at the
highest level in ml-mil, and can be integrated into two state-of-the-art methods
to further improve performance. we conducted rigorous experiments on two
datasets and demonstrated the effectiveness of our method. our contributions
include:1) proposing a novel general framework to address the unique
""patch-slidepatient"" ml-mil problem in the medical field. before this, no other
framework had directly tackled this specific problem, making our proposal a
ground-breaking step in the application of ml-mil in healthcare; 2) proposing a
simple yet highly effective method that leverages self-attention mechanisms and
transformer models to enhance the interaction between slide and patient
information. this innovative approach not only improves the classification
performance at the patient level but also at the slide level, showcasing its
effectiveness and versatility; 3) conducting extensive experiments on two
separate datasets. our method was seamlessly integrated with two prior
state-of-the-art methods, demonstrating its compatibility and adaptability. the
experiments resulted in improved performance, indicating that our method
enhances the efficacy of these existing approaches.",5
1280,Patients and Slides are Equal: A Multi-level Multi-instance Learning Framework for Pathological Image Analysis,2.1,Overview,"our proposed method p&sre is illustrated in fig. 2. specifically, the framework
consists of two parts. the first part is the slide-patch level mil based on a
state-of-the-art mil method. the second part is the patient-slide level mil,
which generates patient-level features using attention mechanism and interacts
the features with transformer. to enhance readability, we first provide the
following symbolization for ml-mil: for a patient x i , it has a patient-level
classification label y i . for patient x i , there may exist n i slides s i ={s
j |j=1 to n i }, where the classification label for each slide s j is denoted as
z j . for each slide s j , it may be divided into m j patches p j ={p k |k=1 to
m j }. here, i,j, and k are indices for patient, slide, and patch levels,
respectively. our proposed framework has strong scalability as it can be based
on any attention-based mil method. therefore, we directly use the
state-of-the-art (sota) mil methods, abmil [8] and dsmil [9] for the slide-patch
stage. these two methods differ in their attention computing approach for each
patch.for abmil, the attention of each patch is computed by an mlp.
specifically, for m j patches p k , an encoder is applied to obtain the patch
feature matrix f i , where,f i ∈ r mj ×1024 . then, f i is passed through an fc
layer followed by a tanh activation and another fc layer followed by a sigmoid
activation to obtain two feature matrices, f i and f i , both ∈ r mj ×128 .
these matrices are elementwise multiplied and then passed through an fc layer to
obtain the weight of each patch, ω k .for dsmil, the attention of each patch is
based on the cosine distance between instances and key instances. first, an fc
layer is applied to the patch feature matrix f i to obtain the importance score
θ k for each patch. the patch with the highest score is selected as the key
instance. then, the feature matrix f i is mapped to a matrix q i ∈ r mj ×128 and
the cosine similarity between all instances and the key instance is computed as
the weight of each patch, ω k .although abmil and dsmil compute attention
differently, both methods compute the attention-weighted sum of patch instances
features as the bag representation of the slide. therefore, the slide feature
output by both methods can be generalized as:finally, we obtain the feature
vector set h i ={h j |j=1 to n i } for all slides {s j } of patientx i through
patch-slide mil.",5
1281,Patients and Slides are Equal: A Multi-level Multi-instance Learning Framework for Pathological Image Analysis,2.3,Patient-Slide Level MIL,"after performing patch-slide level mil, we move on to patient-slide level mil.
in general mil algorithms, the patient is regarded as the bag and the slide as
the instance. however, considering the diagnostic process in clinical practice,
we propose to treat both patients and slides as instances at the same level.
specifically, our p&sre framework for patient-slide level consists of two parts:
patient-level feature generation based on self-attention and patient-slide
feature interaction based on transformer [11].patient-level feature generation
based on self-attention. doctors usually select certain key slides for careful
observation and information aggregation during diagnosis, similar to the
self-attention mechanism. therefore, we directly use a fully connected (fc)
layer to integrate the feature-level features into patient-level features v i
through attention mechanism, serving as patient instances. specifically, given
the feature vector collection {h j } from multiple slides in the previous step,
we input it to the fc layer and apply the sigmoid activation function to output
the weight α j for each h j . then, we perform a weighted average of the vectors
based on this weight to obtain the patient feature v i :patient-slide feature
interaction based on transformer. this process is where our framework shines.
after doctors summarize the patient-level results, they typically review the
slides to double-check the diagnosis results. this patient-slide feature
interaction (psfi) naturally lends itself to the construction of a transformer,
and information exchange and integration between slides and patient level are
bidirectional. thus, self-attention is more ideal for this purpose than other
kinds of attention (such as cross-attention or doctors' attention). by using the
self-attention-based transformer structure, each input token is treated equally
(i.e., viewed as the same instance level), and tokens can interact extensively
with each other, enabling mutual correction between patients and slides and even
between slides. specifically, we merge the slide feature set {h j } and the
patient feature v i into the input tokensand then input them into a multi-layer
transformer through self-attention and feed-forward neural network layers to
obtain the interaction information between slides and output tokens t out i
:where d is the dimension of the token, and t k and t l come from t in i . β k,l
is multi-head attention matrix, and w q , w k , and w v are weight matrices of
query, key, and value, respectively. w r and w o are transformation matrices. b
1 and b 2 are bias vectors. this update procedure is repeated for l layers,
where the t k are fed to the successive transformer layer. finally, we obtain
the output tokensthen, all output tokens are input into a shared fc layer, and
the patient's predicted logits y i and the predicted classification logits {z j
|j = 1 to n i } for each slide are output.training progress and loss function.
during training, we sampled one patient at a time and pre-extracted their
batch-level features for all slides, in order to save gpu memory. due to the
issue of class imbalance in both slide level and patient level, we use the lade
[7] loss function.",5
1282,Patients and Slides are Equal: A Multi-level Multi-instance Learning Framework for Pathological Image Analysis,3.1,Dataset and Evaluation,"cd-itb dataset. cd-itb is a private dataset consisting of 853 slides from 163
patients, with binary patient-level labels of cd or itb in a ratio of 103:60 and
tri-class slide-level labels of cd, itb, and normal slides in a ratio of
436:121:296, respectively. on average, there were 5 slides per patient. the
slides were scanned at a magnification of 40× (0.25 µm/px), and annotations were
curated by experienced pathologists. we adopted a patient-level stratification
approach for 5-fold cross-validation, with 20% of the training set randomly
assigned as the validation set for each fold. the dataset comprises an average
of 2.3k instances per bag, with the largest bag containing over 16k
instances.camelyon17 dataset. camelyon17 [1] is a publicly dataset, and its
training set comprises 500 slides from 100 breast cancer patients with lymph
node metastases. the slides are classified into four distinct categories, namely
negative, itc, micro, and macro, in proportions of 318:36:59:87, respectively.
there were 5 slides per patient on average. the patients are divided into two
groups based on their pn stage, namely lymph node positive and lymph node
negative, in proportions of 24:76, respectively. the data folding method is the
same as the cd-itb dataset. the average number of instances per bag is
approximately 6.1k, and the largest bag contains over 23k instances.metrics. we
report class-wise weighted accuracy (acc), precision(pre), recall, and f1-score
(f1). to avoid randomness, we run all experiments five times and report the
averaged metrics.",5
1283,Patients and Slides are Equal: A Multi-level Multi-instance Learning Framework for Pathological Image Analysis,3.2,Implementation Details,"we utilized resnet50, which was pre-trained on imagenet1k, to extract features
from patches. each patch was of size 512 × 512 pixels. for both abmil and dsmil
networks, we kept the original parameters for the number of channels at each
layer. following the reference [4], we employed a transformer with 8 heads and 8
layers in the patient-slide feature interactions. all networks are implemented
using pytorch and trained on a nvidia rtx titan gpu with 24 gb memory. we
employed two adam optimizers with a maximum learning rate of 1e-4 and a cosine
annealing update strategy that gradually decreased the learning rate to 1e-12
over 300 epochs.",5
1284,Patients and Slides are Equal: A Multi-level Multi-instance Learning Framework for Pathological Image Analysis,3.3,Comparisons and Results,"we compared our strategy with two state-of-the-art mil methods to evaluate its
performance. to investigate the impact of self-attention and transformers on
slide-level and case-level results, we conducted ablation experiments: ""abmil +
p&sre (with/without psfi)"" and ""dsmil + p&sre (with/without psfi)"",
respectively. for slide-level classification, we used mean pooling and max
pooling to pool feature vectors of patches into a representative vector for the
slide, which was then fed into a fully connected layer for classification. at
the patient level, we used two approaches for prediction: maxs, where the
feature of the instance that achieves the maximum positive probability from the
slide-level mil model is selected to patient-level model, and maxmins, where the
mean value of features of the maximum and minimum positive probability from the
slide-level mil model is selected to patient-level model.the results of 5-fold
cv at the slide and patient levels are reported in table 1 and table 2,
respectively. our p&sre framework improves both abmil and dsmil methods at both
levels. abmil with p&sre improves the f1 score from 0.565 to 0.579 for the
cd-itb dataset and from 0.529 to 0.571 for the camelyon17 dataset at the
slide-level, and improves the f1 score from 0.522 to 0.599 for the cd-itb
dataset and from 0.842 to 0.861 for the camelyon17 dataset at the patient-level.
therefore, the ablation experiments demonstrate the effectiveness of p&sre in
enhancing the classification performance at both the slide and patient levels.",5
1285,Patients and Slides are Equal: A Multi-level Multi-instance Learning Framework for Pathological Image Analysis,4.0,Limitations,"our study has some limitations that should be addressed. for instance, we did
not explore the possibility of treating patches as an equivalent level to slides
and patients. the primary reason is that the vast number of patches required for
analysis is significantly larger than that of slides and patients, which
presents a computational challenge for training. as a result, we have not yet
explored this avenue. in the future, we plan to leverage clustering and active
learning methods to reduce the number of patches and enable the interaction of
all three levels with the transformer, which would further enhance the accuracy
and efficiency of our proposed method.",5
1286,Patients and Slides are Equal: A Multi-level Multi-instance Learning Framework for Pathological Image Analysis,5.0,Conclusion,"this study proposes a highly scalable and versatile framework to address m-mil
problems. we first classify the process from patch to slide to the patient in
medical pathology diagnosis as a multi-level mil problem. based on existing
state-of-the-art mil methods, we then extend the framework to p&sre, which
conducts feature extraction and interaction at the slide-patient level. by
introducing a transformer, the framework enables iterative interaction and
correction of information between patients and slides, resulting in better
performance at both the patient level and slide level compared to existing
state-of-the-art algorithms on two validation datasets.",5
1294,What Do AEs Learn? Challenging Common Assumptions in Unsupervised Anomaly Detection,1.0,Introduction,"identifying unusual patterns in data is of great interest in many applications
such as medical diagnosis, industrial defect inspection, or financial fraud
detection. finding anomalies in medical images is especially hard due to large
inter-patient variance of normality, the irregular appearance-, and often rare
occurrence of diseases. therefore, it is difficult and expensive to collect
large amounts of annotated samples that cover the full abnormality spectrum,
with supervised [7,15] fig. 1. morphaeus outperforms aes by generating id
reconstructions even for far ood cases (fig. 1a), enabling accurate pathology
localization (figure 1b). and self-supervised [10,16] methods only capturing
limited facets of the abnormal distribution [31]. however, since it is more
feasible to obtain large data sets with normal samples, it is common to detect
outliers by detecting patterns that deviate from the expected normative
distribution.reconstruction-based aes have emerged as a very popular framework
for unsupervised anomaly detection and are widely adopted in medical imaging
[2]. they provide straight-forward residual error maps, which are essential for
safetycritical domains such as medical image analysis. however, recent work
suggests that aes might reconstruct out-of-distribution (ood) samples even
better than in-distribution (id) samples [28], with the learned likelihood being
dominated by common low-level features [32]. while this can be useful for some
tasks such as reconstruction [34], or restoration [21], it often fails for
pathology detection as anomalies can be missed due to small residual errors. in
fig. 1 we show that aes that have only been trained on healthy chest x-rays are
also able to reconstruct ood samples like pathologies or hands. similarly,
perera et al. [24] showed that aes trained on the digit 8 can also reconstruct
digits 1,5,6 and 9.much effort has been made in the medical imaging community to
improve the limitations of traditional anomaly detection methods, particularly
in the context of brain mri. apart from the reduced dimensionality of the
bottleneck, several other techniques have been introduced to regularize aes
[11,20,29,38]. recently, self-supervised denoising aes [16] achieved sota
results on brain pathology segmentation. they explicitly feed noise-corrupted
inputs x = x+ to the network with the aim at reconstructing the original input
x. however, this is especially beneficial when the anomaly distribution is known
a priori. variational aes (vaes) [5,12,17,40] estimate the distribution over the
latent space that is regularized to be similar to a prior distribution, usually
a standard isotropic gaussian. generative adversarial networks have also been
applied to anomaly detection [24,33]. pidhorskyi et al. [26] trained aes with an
adversarial loss to detect ood samples. more recently, introspective variational
aes [8] use the vae encoder to differentiate between real and reconstructed
samples, achieving sota image generations and outlier detection performance.
recently, zhou et al. [39] investigated the limitations of aes for ood.
similarly, we believe that aes should have two properties: i) minimality: the
networks should be constrained to only reconstruct id samples and ii)
sufficiency: the decoder should have sufficient capacity to reconstruct id
samples with high accuracy. in contrast to [39], where the authors aim at
reconstructing only low-dimensional features needed for the classification task,
we are interested in reconstructing pseudo-healthy images to enable pixel-wise
localization of anomalies.in this work, we first investigate whether sota aes
can learn meaningful representations for anomaly detection. specifically, we
investigate whether aes can learn the healthy anatomy, i.e., absence of
pathology, and generate pseudohealthy reconstructions of abnormal samples on
challenging medical anomaly detection tasks. our findings are that sota aes
either do not efficiently constrain the latent space and allow the
reconstruction of anomalous patterns, or that the decoder cannot accurately
restore images from their latent representation. the imperfect reconstructions
yield high residual errors on normal regions (false positives) that can easily
overshadow residuals of interest, i.e., pathology [23]. we then propose
morphaeus, novel deformable aes to learn minimal and sufficient features for
anomaly detection and drastically reduce false positives. figure 1a shows that
morphaeus learns the training distribution of healthy chest x-rays and yields
pseudo-healthy id reconstructions even for far ood samples. this allows to
localize pathologies, as seen in fig. 1b.our manuscript advances the
understanding of anomaly detection by providing insights into what aes learn. in
summary, our contributions are:• we broaden the understanding of aes and
highlight their limitations.• we test whether sota aes can learn the training
distribution of the healthy population, accurately reconstruct inputs from their
latent representation and reliably detect anomalies. • as a solution, we propose
morphaeus, novel deformable aes that provide pseudo-healthy reconstructions of
abnormal samples and drastically reduce false positives, achieving sota
unsupervised pathology detection.",5
1302,Improved Prognostic Prediction of Pancreatic Cancer Using Multi-phase CT by Integrating Neural Distance and Texture-Aware Transformer,1.0,Introduction,"pancreatic ductal adenocarcinoma (pdac) is one of the deadliest forms of human
cancer, with a 5-year survival rate of only 9% [16]. neoadjuvant chemotherapy
can increase the likelihood of achieving a margin-negative resection and avoid
unnecessary surgery in patients with aggressive tumor types [23]. providing
accurate and objective preoperative biomarkers is crucial for triaging patients
who are most likely to benefit from neoadjuvant chemotherapy. however, current
clinical markers such as larger tumor size and high carbohydrate antigen (ca)
19-9 level may not be sufficient to accurately tailor neoadjuvant treatment for
patients [19]. therefore, multi-phase contrast-enhanced ct has a great potential
to enable personalized prognostic prediction for pdac, leveraging its ability to
provide a wealth of texture information that can aid in the development of
accurate and effective prognostic models [2,10].previous studies have utilized
image texture analysis with hand-crafted features to predict the survival of
patients with pdacs [1], but the representational fig. 1. two examples of
spatial information between vessel (orange region) and tumor (green region). the
minimum distance, which refers to the closest distance between the superior
mesenteric artery (sma) and the pdac tumor region, is almost identical in these
two cases. we define the surface-to-surface distance based on point-to-surface
distance (weighted-average of red lines from ♦ to ) instead of point-to-point
distance (blue lines) to better capture the relationship between the tumor and
the perivascular tissue.here ♦ and are points sampled from subset vc and pc
defined in eq. power of these features may be limited. in recent years, deep
learning-based methods have shown promising results in prognosis models
[3,6,12]. however, pdacs differ significantly from the tumors in these studies.
a clinical investigation based on contrast-enhanced ct has revealed a dynamic
correlation between the internal stromal fractions of pdacs and their
surrounding vasculature [14]. therefore, focusing solely on the texture
information of the tumor itself may not be effective for the prognostic
prediction of pdac. it is necessary to incorporate tumor-vascular involvement
into the feature extraction process of the prognostic model. although some
studies have investigated tumor-vascular relationships [21,22], these methods
may not be sufficiently capable of capturing the complex dynamics between the
tumor and its environment.we propose a novel approach for measuring the relative
position relationship between the tumor and the vessel by explicitly using the
distance between them. typically, chamfer distance [7], hausdorff distance [8],
or other surfaceawareness metrics are used. however, as shown in fig. 1, these
point-to-point distances cannot differentiate the degree of tumor-vascular
invasion [18]. to address this limitation, we propose a learnable neural
distance that considers all relevant points on different surfaces and uses an
attention mechanism to compute a combined distance that is more suitable for
determining the degree of invasion. furthermore, to capture the tumor
enhancement patterns across multi-phase ct images, we are the first to combine
convolutional neural networks (cnn) and transformer [4] modules for extracting
the dynamic texture patterns of pdac and its surroundings. this approach takes
advantage of the visual transformer's adeptness in capturing long-distance
information compared to the cnn-onlybased framework in the original approach. by
incorporating texture information between pdac, pancreas, and peripancreatic
vessels, as well as the local tumor information captured by cnn, we aim to
improve the accuracy of our prognostic prediction model.in this study, we make
the following contributions: (1) we propose a novel approach for aiding survival
prediction in pdac by introducing a learnable neural distance that explicitly
evaluates the degree of vascular invasion between the tumor and its surrounding
vessels. (2) we introduce a texture-aware transformer block to enhance the
feature extraction approach, combining local and global information for
comprehensive texture information. we validate that the cross-attention is
utilized to capture cross-modality information and integrate it with in-modality
information, resulting in a more accurate and robust prognostic prediction model
for pdac. (3) through extensive evaluation and statistical analysis, we
demonstrate the effectiveness of our proposed method. the signature built from
our model remains statistically significant in multivariable analysis after
adjusting for established clinical predictors. our proposed model has the
potential to be used in combination with clinical factors for risk
stratification and treatment decisions for patients with pdac.",5
1305,Improved Prognostic Prediction of Pancreatic Cancer Using Multi-phase CT by Integrating Neural Distance and Texture-Aware Transformer,2.2,Neural Distance: Positional and Structural Information,"between pdac and vesselsthe vascular involvement in patients with pdac affects
the resectability and treatment planning [5]. in this study, we investigate four
important vessels: portal vein and splenic vein (pvsv), superior mesenteric
artery (sma), superior mesenteric vein (smv), and truncus coeliacus (tc). we
used a semi-supervised nnunet model to segment pdac and the surrounding vessels,
following recent work [11,21]. we define a general distance between the surface
boundaries of pdac (p) and the aforementioned four types of vessels (v) as d(v,
p), which can be derived as follows:where v ∈ v and p ∈ p are points on the
surfaces of blood vessels and pdac, respectively. the point-to-surface distance
d ps (v, p) is the distance from a point v on v to p, defined as d ps (v, p) =
min p∈p v -p 2 2 , and vice versa. to numerically calculate the integrals in the
previous equation, we uniformly sample from the surfaces v and p to obtain the
sets v and p consisting of n v points and n p points, respectively. the distance
is then calculated between the two sets using the following equation:however,
the above distance treats all points equally and may not be flexible enough to
adapt to individualized prognostic predictions. therefore, we improve the above
equation in two ways. firstly, we focus on the sub-sets vc and pc of v and p,
respectively, which only contain the k closest points to the opposite surfaces p
and v, respectively. the sub-sets are defined as:secondly, we regard the entire
sets vc and pc as sequences and calculate the distance using a 2-way
cross-attention block (similar to eq. 1) to build a neural distance based on the
3d spatial coordinates of each point:neural distance allows for the flexible
assignment of weights to different points and is able to find positional
information that is more suitable for pdac prognosis prediction. in addition to
neural distance, we use the 3d-cnn model introduced in [22] to extract the
structural relationship between pdac and the vessels. specifically, we
concatenate each pdac-vessel pair x v s ∈ r 2×h×w ×d , where v ∈{pvsv, smv, sma,
tc} and obtain the structure feature f s ∈ r cs .finally, we concatenate the
features extracted from the two components and apply a fully-connected layer to
predict the survival outcome, denoted as o os , which is a value between 0 and
1. to optimize the proposed model, we use the negative log partial likelihood as
the survival loss [9].",5
1306,Improved Prognostic Prediction of Pancreatic Cancer Using Multi-phase CT by Integrating Neural Distance and Texture-Aware Transformer,3.0,Experiments,"dataset. in this study, we used data from shengjing hospital to train our method
with 892 patients, and data from three other centers, including guangdong
provincial people's hospital, tianjin medical university and sun yatsen
university cancer center for independent testing with 178 patients. the
contrast-enhanced ct protocol included non-contrast, pancreatic, and portal
venous phases. pdac masks for 340 patients were manually labeled by a
radiologist from shengjing hospital with 18 years of experience in pancreatic
cancer, while the rest were predicted using self-learning models [11,24] and
checked by the same annotator. other vessel masks were generated using the same
semisupervised segmentation models. c-index was used as our primary evaluation
metric for survival prediction. we also reported the survival auc, which
estimates the cumulative area under the roc curve for the first 36
months.implementation details: we used nested 5-fold cross-validation and
augmented the training data by rotating volumetric tumors in the axial direction
and randomly selecting cropped regions with random shifts. we also set the
output feature dimensions to c t = 64 for the texture-aware transformer, c s =
64 for the structure extraction and k = 32 for the neural distance. the batch
size was 16 and the maximum iteration was set to 1000 epochs, and we selected
the model with the best performance on the validation set during training for
testing. we implemented our experiments using pytorch 1.11 and trained the
models on a single nvidia 32g-v100 gpu.ablation study. we first evaluated the
performance of our proposed textureaware transformer (tat) by comparing it with
the resnet18 cnn backbone and vit transformer backbone, as shown in table 1. our
model leverages the strengths of both local and global information in the
pancreas and achieved the best result. next, we compared different methods for
multi-phase stages, including lstm, early fusion (fusion), and cross-attention
(cross) in our method. cross-attention is more effective and lightweight than
lstm. moreover, we separated texture features into in-phase features and
cross-phase features, which is more reasonable than early fusion.secondly, we
evaluated each component in our proposed method, as shown in fig. 2, and
presented the results in table 1. combining the texture-aware transformer and
regular structure information improved the results from 0.630 to 0.648, as tumor
invasion strongly affects the survival of pdac patients. we also employed a
simple 4-variable regression model that used only the chamfer distance of the
tumor and the four vessels for prognostic prediction. the resulting c-index of
0.611 confirmed the correlation of the distance with the survival, which is
consistent with clinical findings [18]. explicitly adding the distance measure
further improved the results. our proposed neural distance metric outperformed
traditional surface distance metrics like chamfer distance, indicating its
suitability for distinguishing the severity of pdac.",5
1307,Improved Prognostic Prediction of Pancreatic Cancer Using Multi-phase CT by Integrating Neural Distance and Texture-Aware Transformer,0,Comparisons.,"to further evaluate the performance of our proposed model, we compared it with
recent deep prediction methods [17,21] and report the results in table 2. we
modified baseline deep learning models [12,17] and used their network
architectures to take a single pancreatic phase or all three phases as inputs.
deepct-pdac [21] is the most recent method that considers both tumor-related and
tumor-vascular relationships using 3d cnns. our proposed method, which uses the
transformer and structure-aware blocks to capture tumor enhancement patterns and
tumor-vascular involvement, demonstrated its effectiveness with better
performance in both nested 5-fold cross-validation and the multi-center
independent test set.in table 3, we used univariate and multivariate cox
proportional-hazards models to evaluate our signature and other
clinicopathologic factors in the independent test set. the proposed risk
stratification was a significant prognostic factor, along with other factors
like pathological tnm stages. after selecting significant variables (p < 0.05)
in univariate analysis, our proposed staging remained strong in multivariable
analysis after adjusting for important prognostic markers like pt and resection
margins. notably, our proposed marker remained the strongest among all
pre-operative markers, such as tumor size and ca 19-9.neoadjuvant therapy
selection. to demonstrate the added value of our signature as a tool to select
patients for neoadjuvant treatment before surgery, we plotted kaplan-meier
survival curves in fig. 3. we further stratify patients by our signature after
grouping them by tumor size and ca19-9, two clinically used preoperative
criteria for selection, and also age. our signature could significantly stratify
patients in all cases and those in the high-risk group had worse outcomes and
might be considered as potential neoadjuvant treatment candidates (e.g. 33
high-risk patients with larger tumor size and high ca19-9).",5
1308,Improved Prognostic Prediction of Pancreatic Cancer Using Multi-phase CT by Integrating Neural Distance and Texture-Aware Transformer,4.0,Conclusion,"in our paper, we propose a multi-branch transformer-based framework for
predicting cancer survival. our framework includes a texture-aware transformer
that captures both local and global information about the pdac and pancreas. we
also introduce a neural distance to calculate a more reasonable distance between
pdac and vessels, which is highly correlated with pdac survival. we have
extensively evaluated and statistically analyzed our proposed method,
demonstrating its effectiveness. furthermore, our model can be combined with
established high-risk features to aid in the patient selections who might
benefit from neoadjuvant therapy before surgery.",5
1320,Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans,1.0,Introduction,"gastric cancer (gc) is the third leading cause of cancer-related deaths
worldwide [19]. the five-year survival rate for gc is approximately 33% [16],
which is mainly attributed to patients being diagnosed with advanced-stage
disease harboring unresectable tumors. this is often due to the latent and
nonspecific signs and symptoms of early-stage gc. however, patients with
early-stage disease have a substantially higher five-year survival rate of
around 72% [16]. therefore, early detection of resectable/curable gastric
cancers, preferably before the onset of symptoms, presents a promising strategy
to reduce associated mortality. unfortunately, current guidelines do not
recommend any screening tests for gc [22]. while several screening tools have
been developed, such as barium-meal gastric photofluorography [5], upper
endoscopy [4,7,9], and serum pepsinogen levels [15], they are challenging to
apply to the general population due to their invasiveness, moderate
sensitivity/specificity, high cost, or side effects. therefore, there is an
urgent need for novel screening methods that are noninvasive, highly accurate,
low-cost, and ready to distribute.non-contrast ct is a commonly used imaging
protocol for various clinical purposes. it is a non-invasive, relatively
low-cost, and safe procedure that exposes patients to less radiation dose and
does not require the use of contrast injection that may cause serious side
effects (compared to multi-phase contrastenhanced ct). with recent advances in
ai, opportunistic screening of diseases using non-contrast ct during routine
clinical care performed for other clinical indications, such as lung and
colorectal cancer screening, presents an attractive approach to early detect
treatable and preventable diseases [17]. however, whether early detection of
gastric cancer using non-contrast ct scans is possible remains unknown. this is
because early-stage gastric tumors may only invade the mucosal and muscularis
layers, which are difficult to identify without the help of stomach preparation
and contrast injection. additionally, the poor contrast between the tumor and
normal stomach wall/tissues on non-contrast ct scans and various shape
alterations of gastric cancer, further exacerbates this challenge.in this paper,
we propose a novel approach for detecting gastric cancer on non-contrast ct
scans. unlike the conventional ""segmentation for classification"" methods that
directly employ segmentation networks, we developed a clusterinduced mask
transformer that performs segmentation and global classification simultaneously.
given the high variability in shape and texture of gastric cancer, we encode
these features into learnable clusters and utilize cluster analysis during
inference. by incorporating self-attention layers for global context modeling,
our model can leverage both local and global cues for accurate detection. in our
experiments, the proposed approach outperforms nnunet [8] by 0.032 in auc, 5.0%
in sensitivity, and 4.1% in specificity. these results demonstrate the potential
of our approach for opportunistic screening of gastric cancer in asymptomatic
patients using non-contrast ct scans.",5
1321,Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans,2.0,Related Work,"automated cancer detection. researchers have explored automated tumor detection
techniques on endoscopic [13,14], pathological images [20], and the prediction
of cancer prognosis [12]. recent developments in deep learning have
significantly improved the segmentation of gastric tumors [11], which is
critical for their detection. however, our framework is specifically designed
for noncontrast ct scans, which is beneficial for asymptomatic patients. while
previous studies have successfully detected pancreatic [25] and esophageal [26]
cancers on non-contrast ct, identifying gastric cancer presents a unique
challenge due to its subtle texture changes, various shape alterations, and
complex background, e.g., irregular gastric wall; liquid and contents in the
stomach.",5
1324,Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans,0,Knowledge Transfer from Contrast-Enhanced to Non-contrast CT.,"to address difficulties with tumor annotation on non-contrast cts, the
radiologists start by annotating a voxel-wise tumor mask on the
contrast-enhanced ct, referring to clinical and endoscopy reports as needed.
deeds [6] registration is then performed to align the contrast-enhanced ct with
the non-contrast ct and the resulting deformation field is applied to the
annotated mask. any misaligned ones are revised manually. in this manner (fig.
1d), a relatively coarse yet highly reliable tumor mask can be obtained for the
non-contrast ct image. cluster-induced classification with mask transformers.
segmentation for classification is widely used in tumor detection [25,26,32]. we
first train a unet [8,18] to segment the stomach and tumor regions using the
masks from the previous step. this unet considers local information and can only
extract stomach rois well during testing. however, local textures are inadequate
for accurate gastric tumor detection on non-contrast cts, so we need a network
of both local sensitivity to textures and global awareness of the organ-tumor
morphology. mask transformer [3,24] is a well-suited approach to boost the cnn
backbone with stand-alone transformer blocks. recent studies [27,28] suggest
interpreting object queries as cluster centers, which naturally exhibit
intra-cluster similarity and inter-class discrepancy. inspired by this, we
further develop a deep classification model on top of learnable cluster
representations.specifically, given image x ∈ r h×w ×d , annotation y ∈ r k×hw d
, and patient class p ∈ l, our model consists of three components: 1) a cnn
backbone to extract its pixel-wise features f ∈ r c×hw d (fig. 1a), 2) a
transformer module (fig. 1b), and 3) a multi-task cluster inference module (fig.
1c). the transformer module gradually updates a set of randomly initialized
object queries c ∈ r n ×c , i.e., to meaningful mask embedding vectors through
cross-attention between object queries and multi-scale pixel features,where c
and p stand for query and pixel features, q c , k p , v p represent linearly
projected query, key, and value. we adopt cluster-wise argmax from kmax-deeplab
[28] to substitute spatial-wise softmax in the original settings.we further
interpret the object queries as cluster centers from a cluster analysis
perspective. all the pixels in the convolutional feature map are assigned to
different clusters based on these centers. the assignment of clusters (a.k.a.
mask prediction) m ∈ r n ×hw d is computed as the cluster-wise softmax function
over the matrix product between the cluster centers c and pixel-wise feature
matrix f, i.e.,the final segmentation logits z ∈ r k×hw d are obtained by
aggregating the pixels within each cluster according to cluster-wise
classification, which treats pixels within a cluster as a whole. the aggregation
of pixels is achieved by z = c k m, where the cluster-wise classification c k is
represented by an mlp that projects the cluster centers c to k channels (the
number of segmentation classes).the learned cluster centers possess high-level
semantics with both intercluster discrepancy and intra-cluster similarity for
effective classification. rather than directly classifying the final feature
map, we first generate the clusterpath feature vector by taking the channel-wise
average of cluster centers c =additionally, to enhance the consistency between
the segmentation and classification outputs, we apply global max pooling to
cluster assignments r to obtain the pixel-path feature vector r ∈ r n . this
establishes a direct connection between classification features and segmentation
predictions. finally, we concatenate these two feature vectors to obtain the
final feature and project it onto the classification prediction p ∈ r 2 via a
two-layer mlp.the overall training objective is formulated as,where the
segmentation loss l seg (•, •) is a combination of dice and cross entropy
losses, and the classification loss l cls (•, •) is cross entropy loss.",5
1325,Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans,4.1,Experimental Setup,"dataset and ground truth. our study analyzed a dataset of ct scans collected
from guangdong province people's hospital between years 2018 and 2020, with
2,139 patients consisting of 787 gastric cancer and 1,352 normal cases. we used
the latest patients in the second half of 2020 as a hold-out test set, resulting
in a training set of 687 gastric cancer and 1,204 normal cases, and a test set
of 100 gastric cancer and 148 normal cases. we randomly selected 20% of the
training data as an internal validation set. to further evaluate specificity in
a larger population, we collected an external test set of 903 normal cases from
shengjing hospital. cancer cases were confirmed through endoscopy (and
pathology) reports, while normal cases were confirmed by radiology reports and a
two-year follow-up. all patients underwent multi-phase cts with a median spacing
of 0.75 × 0.75 × 5.0 mm and an average size of (512, 512, 108) voxel. tumors
were annotated on the venous phase by an experienced radiologist specializing in
gastric imaging using ctlabeler [23], while the stomach was automatically
annotated using a self-learning model [31].implementation details. we resampled
each ct volume to the median spacing while normalizing it to have zero mean and
unit variance. during training, we cropped the 3d bounding box of the stomach
and added a small margin of (32,32,4). we used nnunet [8] as the backbone, with
four transformer decoders, each taking pixel features with output strides of 32,
16, 8, and 4. we set the number of object queries n to 8, with each having a
dimension of 128, and included an eight-head self-attention layer in each block.
the patch size used during training and inference is (192, 224, 40) voxel. we
followed [8] to augment data. we trained the model with radam using a learning
rate of 10 -4 and a (backbone) learning rate multiplier of 0.1 for 1000 epochs,
with a frozen backbone of the pretrained nnunet [8] for the first 50 epochs. to
enhance performance, we added deep supervision by aligning the cross-attention
map with the final segmentation map, as per kmax-deeplab [27]. the hidden layer
dimension in the two-layer mlp is 128. we also trained a standard unet [8,18] to
localize the stomach region in the entire image in the testing phase.",5
1326,Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans,0,Evaluation Metrics and Reader Study.,"for the binary classification, model performance is evaluated using area under
roc curve (auc), sensitivity (sens.), and specificity (spec.). and successful
localization of the tumors is considered when the overlap between the
segmentation mask generated by the model and the ground truth is greater than
0.01, measured by the dice score. a reader study was conducted with two
experienced radiologists, one from guangdong province people's hospital with 20
years of experience and the other from the first affiliated hospital of zhejiang
university with 9 years of experience in gastric imaging. the readers were given
248 non-contrast ct scans from the test set and asked to provide a binary
decision for each scan, indicating whether the scan showed gastric cancer. no
patient information or records were provided to the readers. readers were
informed that the dataset might contain more tumor cases than the standard
prevalence observed in screening, but the proportion of case types was not
disclosed. readers used itk-snap [30] to interpret the ct scans without any time
constraints. 1 presents a comparative analysis of our proposed method with three
baselines. the first two approaches belong to ""segmentation for classification""
(s4c) [26,32], using nnunet [8] and transunet [2]. a case is classified as
positive if the segmented tumor volume exceeds a threshold that maximizes the
sum of sensitivity and specificity on the validation set. the third baseline
(denoted as ""nnunet-joint"") integrates a cnn classification head into unet [8]
and trained end-to-end. we obtain the 95% confidence interval of auc,
sensitivity, and specificity values from 1000 bootstrap replicas of the test
dataset for statistical analysis. for statistical significance, we conduct a
delong test between two aucs (ours vs. compared method) and a permutation test
between two sensitivities or specificities (ours vs. compared method and
radiologists).",5
1329,Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans,0,Subgroup Analysis.,"in table 2, we report the performance of patient-level detection and tumor-level
localization stratified by tumor (t) stage. we compare our model's performance
with that of both radiologists. the results show that our model performs better
in detecting early stage tumors (t1, t2) and provides more precise tumor
localization. specifically, our model detects 60.0% (6/10) t1 cancers, and 77.8%
(7/9) t2 cancers, surpassing the best performing expert (50% t1, 55.6% t2).
meanwhile, our model maintains a reliable detection rate and credible
localization accuracy for t3 and t4 tumors (2 of 34 t3 tumors missed).comparison
with established screening tools. our method surpasses or performs on par with
established screening tools [4,7,10] in terms of sensitivity for gastric cancer
detection at a similar specificity level with a relatively large testing patient
size (n = 1151 by integrating the internal and external test sets), as shown in
table 3. this finding sheds light on the opportunity to employ automated ai
systems to screen gastric cancer using non-contrast ct scans.",5
1364,Text-Guided Cross-Position Attention for Segmentation: Case of Medical Image,3.1,Setup,"medical datasets. we evaluated cp am t g using three datasets: monuseg [8]
dataset, qata-cov19 [6] dataset, and sacroiliac joint (sij) dataset. the first
two datasets are the same benchmark datasets used in [10]. monuseg [8] contains
30 digital microscopic tissue images of several patients and qata-cov19 are
covid-19 chest x-ray images. the ratio of training, validation, and test sets
was the same as in [10]. sij is the dataset privately prepared for this study
which consists of 804 mri slices of nineteen healthy subjects and sixty patients
diagnosed with axial spondyloarthritis. among all mri slices, we selected the
gadoliniumenhanced fat-suppressed t1-weighted oblique coronal images, excluding
the first and last several slices in which the pelvic bones did not appear, and
added the text annotations for the slices.training and metrics. for a better
training, data augmentation was used. we randomly rotated images by -20 • ∼ +20
• and conducted a horizontal flip with 0.5 probability for only the monuseg and
qata-cov19 datasets. the batch size and learning rate were set to 2 and 0.001,
respectively. the loss function (l t ) for training is the sum of the binary
cross-entropy loss (l bce ) and the dice loss (l dice ):the mdice and miou
metrics, widely used to measure the performance of segmentation models, were
used to evaluate the performance of object segmentation. for experiments,
pytorch (v1.7.0) were used on a computer with nvidia-v100 32 gb gpu.",5
1370,Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,1.0,Introduction,"the spreading digitalisation of pathology labs has enabled the development of
deep learning (dl) tools that can assist pathologists in their daily tasks.
however, supervised dl methods require detailed annotations in whole-slide
images (wsis) which is time-consuming, expensive and prone to inter-observer
disagreements [6]. multiple instance learning (mil) alleviates the need for
detailed annotations and has seen increased adoption in recent years. mil
approaches have proven to work well in academic research on histopathology data
[1,17,29] as well as in commercial applications [26]. most mil methods for
digital pathology employ an attention mechanism as it increases the reliability
of the algorithms, which is essential for successful clinical adoption
[14].domain shift in dl occurs when the data distributions of testing and
training differs [20,34]. this remains a significant obstacle to the deployment
of dl applications in clinical practice [7]. to address this problem previous
work either use domain adaptation when data from the target domain is available
[32], or domain generalisation when the target data is unavailable [34]. domain
adaptation has been explored in the mil setting too [22,23,27]. however, it may
not be feasible to perform an explicit domain adaptation, and an already adapted
model could still experience problems with domain shifts. hence, it is important
to provide indications of the expected performance on a target dataset without
requiring annotations [5,25]. another related topic is out-of-distribution (ood)
detection [33] which aims to detect individual samples that are ood, in contrast
to our objective of estimating a difference of expected performances between
some datasets. for supervised algorithms, techniques of uncertainty estimation
have been used to measure the effect of domain shift [4,15,18] and to improve
the robustness of predictions [19,21,30]. however, the reliability of
uncertainty estimates can also be negatively affected by domain shifts [11,31].
alternatively, a drop in performance can be estimated by comparing the model's
softmax outputs [8] or some hidden features [24,28] acquired on in-domain and
domain shift datasets. although such methods have been demonstrated for
supervised algorithms, as far as we know no previous work has explored domain
shift in the specific context of mil algorithms. hence, it is not clear how well
they will work in such a scenario.in this work, we evaluate an attention-based
mil model on unseen data from a new hospital and propose a way to quantify the
domain shift severity. the model is trained to perform binary classification of
wsis from lymph nodes of breast cancer patients. we split the data from the new
hospital into several subsets to investigate clinically realistic scenarios
triggering different levels of domain shift. we show that our proposed
unsupervised metric for quantifying domain shift correlates best with the
changes in performance, in comparison to multiple baselines. the approach of
validating a mil algorithm in a new site without collecting new labels can
greatly reduce the cost and time of quality assurance efforts and ensure that
the models perform as expected in a variety of settings. the novel contributions
of our work can be summarised as:1. proposing an unsupervised metric named
fréchet domain distance (fdd) for quantifying the effects of domain shift in
attention-based mil; 2. showing how fdd can help to identify subsets of patient
cases for which mil performance is worse than reported on the in-domain test
data; 3. comparing the effectiveness of using uncertainty estimation versus
learnt representations for domain shift detection in mil.",5
1375,Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,3.0,Datasets,"grand challenge camelyon data [16] potentially large shift as some patients have
already started neoadjuvant treatment as well as the tissue may be affected from
the procedure of sentinel lymph node removal. 2a. 207 wsis with ductal carcinoma
(83 wsis with metastases): a small shift as it is the most common type of
carcinoma and relatively easy to diagnose. 2b. 68 wsis with lobular carcinoma
(28 wsis with metastases): potentially large shift as it is a rare type of
carcinoma and relatively difficult to diagnose.the datasets of lobular and
ductal carcinomas each contain 50 % of wsis from sentinel and axillary lymph
node procedures. the sentinel/axillary division is motivated by the differing dl
prediction performance on such subsets, as observed by jarkman et al. [13].
moreover, discussions with pathologists led to the conclusion that it is
clinically relevant to evaluate the performance difference between ductal and
lobular carcinoma. our method is intended to avoid requiring dedicated wsi
labelling efforts. we deem that the information needed to do this type of subset
divisions would be available without labelling since the patient cases in a
clinical setting would already contain such information. all datasets are
publicly available to be used in legal and ethical medical diagnostics research.",5
1383,Liver Tumor Screening and Diagnosis in CT with Pixel-Lesion-Patient Network,1.0,Introduction,"liver cancer is the third leading cause of cancer death world-wide in 2020 [14].
early detection and accurate diagnosis of liver tumors may improve overall
patient outcomes, in which imaging plays a key role [11]. computed tomography
(ct) is one of the most important imaging modalities for liver tumors. dynamic
contrast-enhanced (dce) ct is widely used for diagnostics, but it requires
iodine contrast injection which can cause reaction and potential risks in
patients. recently, non-contrast (nc) ct scans are gaining attention as they are
cheaper and safer to acquire, thus can be potential tools for opportunistic
tumor screening [18,20]. meanwhile, finding and diagnosing tumors in nc cts is
also extremely challenging because of the poor contrast between tumors and
normal tissues compared to those in dce cts. prior works on pancreas [18] and
esophagus [20] have shown that latest deep learning techniques can detect subtle
texture and shape changes in nc ct that even human eyes may miss. thus, we aim
to investigate the performance of liver tumor segmentation and classification in
nc cts. such an approach will be helpful to discover asymptomatic incidental
tumors [12] from routine nc ct scans indicated for general diagnostic purposes
at no additional cost and radiation exposure. after an incidental tumor is
found, the patient may undergo further imaging examination such as a multi-phase
dce ct for differential diagnosis [11], which can provide useful discriminative
information such as the vascularity of lesions and the pattern of contrast agent
enhancement [19]. liver is largest solid organ in body and is the site of many
tumor types [11]. therefore, accurate tumor type classification is important for
the decision of treatment plans and prognosis.many researchers have developed
algorithms to automatically segment [1,9,13,15,23] or classify [19,21,25] liver
tumors in ct to help radiologists improve their accuracy and efficiency. for
example, public datasets such as the liver tumor segmentation benchmark (lits)
[1] fostered a series of works aiming to segment liver tumors with improved
convolutional neural network (cnn) backbones [9,13] and lesion edge information
[15]. lits only has single-phase cts (venous phase). several studies
investigated methods to exploit multi-phase ct by methods such as hetero-phase
fusion [5] and modality-aware mutual learning [23]. there are few work
discussing liver tumor analysis in nc ct [5]. besides lesion segmentation,
cnn-based lesion classification algorithms have been studied to distinguish
common lesion types [19,21,25].in this paper, we build a comprehensive framework
to address both tumor screening and diagnosis. (1) tumor screening involves
finding tumor patients in a large pool of healthy subjects and patients. most
existing works in tumor segmentation and detection did not explicitly consider
it since their training and testing images are all tumor patients. such models
may generate false positives in real-world screening scenario when facing
diverse tumor-free images. we collect a large-scale dataset with both tumor and
non-tumor subjects, where the non-tumor subjects includes not only healthy ones,
but also patients with various diffuse liver diseases such as steatosis and
hepatitis to improve the robustness of the algorithm. (2) most works studied
liver tumor segmentation alone without differentiating tumor types, while a few
works classify liver tumors on cropped tumor patches [19,21,25]. meanwhile, we
learn tumor segmentation and classification with one network using an instance
segmentation framework [3]. we train two networks for nc and multi-phase dce
cts, respectively. (3) for evaluation, previous segmentation works typically use
pixel-level metrics such as dice coefficient. such metrics cannot reflect the
lesion-level accuracy (how many lesion instances are correctly detected and
classified) and may bias to large lesions when a patient has multiple tumors.
patient-level metrics (e.g. classifying whether a subject has malignant tumors)
are also useful for treatment recommendation in clinical practice [18,20].
therefore, we assess our algorithm thoroughly with pixel, lesion, and
patient-level metrics.algorithms for liver tumor segmentation have focused on
improving the feature extraction backbone of a fully-convolutional cnn
[9,13,15,23]. the pixelwise segmentation architectures may not be optimal for
lesion and patient-level evaluation metrics since they cannot consider a lesion
or an image holistically. recently, a series of mask transformer algorithms
[3,4,17] have emerged in the computer vision community and achieved the
state-of-the-art performance in instance segmentation tasks. in brief, they use
object queries to interact with image feature maps and with each other to
produce mask and class predictions for each instance. inspired by them, we
propose a novel end-to-end framework named pixel-lesion-patient network (plan)
for lesion segmentation and classification, as well as patient classification.
it contains three branches with bottomup cooperation: the segmentation map from
the pixel branch helps to initialize the lesion branch, which is an improved
mask transformer aiming to segment and classify each lesion; the patient branch
aggregates information from the whole image and predicts image-level labels of
each lesion type, with regularization terms to encourage consistency with the
lesion branch.we collected a large-scale multi-phase dataset containing 810
non-tumor subjects and 939 tumor patients. 4010 tumor instances of eight types
are extensively annotated based on pathological reports. on the non-contrast
tumor screening and diagnosis task, plan achieves 95.0%, 96.4%, and 0.965 in
patient-level sensitivity, specificity, and average auc for malignant and benign
patients, in contrast to 94.4%, 93.7%, and 0.889 for the widely-used nnu-net
[8]. on multi-phase dce ct, our lesion-level detection precision, recall, and
classification accuracy are 92.2%, 89.0%, 85.9%, outperforming nnu-net [8] and
mask2former [3]. we further conduct a reader study on a holdout set of 250
cases. our algorithm is on par with a senior radiologist (16 yrs experience),
showing the clinical significance of our results. our codes will be made public
upon institutional approval.",5
1385,Liver Tumor Screening and Diagnosis in CT with Pixel-Lesion-Patient Network,2.2,Pixel-Lesion-Patient Network (PLAN),"our goal is to segment the mask and classify the type of each tumor in a liver
ct. we also hope to make patient-level diagnoses for each ct scan. plan is
inspired by mask2former [3] with three key improvements: (1) a pixel branch is
added to provide anchor queries to the lesion branch. (2) the lesion branch is
composed of the transformer decoder in mask2former, and we improve its
segmentation loss to enhance recall of small lesions. (3) a patient branch is
attached to make dedicated image-level predictions with a proposed
lesion-patient consistency loss. our framework is shown in fig. 1.pixel branch
and anchor queries. the pixel branch is a convolutional layer after the pixel
decoder and learns to predict pixel-wise segmentation maps similar to
traditional segmentators. we do cc analysis to the predicted mask to extract
lesion instances, and then average the pixel embeddings inside each predicted
lesion to obtain a feature vector. the feature vectors are regarded as anchor
queries and work the same way as the randomly initialized queries in the lesion
branch. compared to the random queries in the original mask2former, the anchor
queries contain prior information of the lesions to be segmented, helping the
lesion branch to match with the lesion targets more easily [10].lesion branch
and foreground-enhanced sampling loss. similar to mask2former, the lesion branch
predicts a binary mask and a class label for each query, see fig. 1. mask2former
calculates its segmentation loss on k sampled pixels instead of on the whole
image, which is shown to both improve accuracy and reduce gpu memory usage [3].
however, in lesion segmentation, some tumors are very small compared to the
whole 3d image. the importance sampling strategy [3] can hardly select any
foreground pixels in such cases, so the loss only contains background pixels,
degrading the segmentation recall of small lesions. we propose a simple approach
to remedy this issue by sampling an extra n foreground pixels for each
lesion.patient branch. a patient-level diagnosis is useful for triage. for
example, diagnosing the subject as normal, benign, or malignant will result in
completely different treatments [24]. intuitively, we can also infer
patient-level labels from segmentation results by checking if there is any
lesion in the predicted mask. however, certain tumors are often related to signs
outside the tumor, e.g. hepatocellular carcinoma and cirrhosis,
cholangiocarcinoma and bile duct dilatation, etc. we equip plan with a dedicated
patient branch to aggregate such global information to make better patient-level
prediction. since one patient can have multiple liver tumors of different types,
in our problem, we give each image several hierarchical binary labels. the first
label classifies normal and tumor subjects (whether the image contains any
tumor); the second and third labels indicate the existence of respectively
benign and malignant tumors; the rest c labels suggest the existence of c
fine-grained types of tumors. we employ the dual-path transformer block [17] to
fuse multi-scale features from the pixel encoder and decoder to generate a
feature map, followed by global average pooling and a linear classification
layer to predict the c + 3 labels.a lesion-patient consistency loss is further
proposed to encourage coherence of the lesion and patient-level predictions.
inspired by multi-instance learning [6], we compute a pseudo patient-level
prediction c ∈ r c from the lesion-level predictions by max-pooling the class
probability of each class across all lesion queries (discarding the no-object
class). we also have the probability vector from the patient branch p ∈ r c
corresponding to the c fine-grained classes. then, we compute the l2 loss
between them:the overall loss of plan is listed in eq. 1, where l pixel is the
combined crossentropy (ce) and dice loss for the pixel branch as in nnu-net [8];
l lesion-class is the ce loss [3] for lesion classification in the lesion
branch; l lesion-mask is the combined ce and dice loss [3] for binary lesion
segmentation in the lesion branch with the foreground-enhanced sampling
strategy; l patient is the binary ce loss for the multi-label classification
task in the patient branch.",5
1386,Liver Tumor Screening and Diagnosis in CT with Pixel-Lesion-Patient Network,3.0,Experiments,"data. our dataset contains 810 normal subjects and 939 patients with liver
tumors. each normal subject has a non-contrast (nc) ct, while each patient has a
dynamic contrast-enhanced (dce) ct scan with nc, arterial, and venous phases. we
use deeds [7] to register nc and arterial phases to the venous phase, and then
invite a senior radiologist with 10 years of experience to annotate on the
multi-phase cts using ct labeler [16]. the 3d mask and the type of all liver
tumors are annotated based on pathological reports and magnetic resonance scans
if necessary. eight tumor types are considered in our study: hepatocellular
carcinoma (hcc), intrahepatic cholangiocarcinoma (icc), metastasis (meta),
hepatoblastoma (hepato), hemangioma (heman), focal nodular hyperplasia (fnh),
cyst, and others (all other tumor types). if a lesion's type cannot be
determined according to image signs [11] and pathology, it will be marked as
""unknown"" and ignored in training and evaluation. in total, 4010 tumor instances
are annotated, whose volumes range from 11 to 3.7×10 6 mm 3 . detailed
statistics and examples of the lesions are shown in the supplementary material.
we train two separate networks for nc and dce cts. in the former setting, both
normal and patient data are used and randomly split into 1149 training, 100
validation, and 500 testing. in the latter one, only patient data are used with
641 training, 100 validation, and 200 testing. another hold-out set of 150
patients and 100 normal cts are used for reader study to compare our accuracy
with two radiologists. implementation details. each ct is resampled to
0.7×0.7×5mm in spacing. we first train an nnu-net on public datasets to segment
liver and surrounding organs (gallbladder, hepatic vein, spleen, stomach, and
pancreas), and then crop the liver region to train plan. to help plan
differentiate liver tumors and other organs, we train the network to segment
both tumors and organs using the predicted organ labels. plan is built on top of
the nnu-net framework [8]. its pixel encoder is a u-net encoder, whereas its
pixel decoder is a light-weight feature pyramid network [3]. the lesion branch
incorporates three transformer decoder blocks with masked attention [3] which
use feature maps of strides 16, 8, 4 from the pixel decoder. the number of
random queries is q = 20; the embedding dimension is m = 64; the number of
sampled pixels is k = 12544 [3], foreground pixels n = 3; the loss weight is 0.1
for the no-object class while 1 for other classes in the lesion branch [3]. the
weights in eq. 1 arewe use the radam optimizer with an initial learning rate of
0.0001. each training batch contains two patches of size 256 × 256 × 24. for dce
ct, the three phases form a 3-channel image as the network input. extensive data
augmentation is applied including random cropping, scaling, flipping, elastic
deformation, and brightness adjustment [8].during training, we first pretrain
the backbone and the pixel branch for 500 epochs, and then train the whole
network for another 500 epochs.patient-level results. this paper has three major
goals: tumor screening in nc ct (classifying a subject as normal or tumor),
preliminary diagnosis in nc ct (predicting the existence of malignant and benign
tumors), and fine-grained diagnosis in dce ct (predicting the existence of 8
tumor types). among the 8 tumor types, hcc, icc, meta, and hepato are malignant;
heman, fnh, and cyst are benign. ""others"" can be either malignant or benign,
thus are excluded in the preliminary diagnosis task. the nc test set contains
198 tumor cases, 202 completely normal cases, and 100 ""hard"" non-tumor cases
which may have larger image noise, artifact, ascites, diffuse liver diseases
such as hepatitis and steatosis. these cases are used to test the robustness of
the model in real-world screening scenario with diverse tumor-free images. we
compare plan with a widely-used strong baseline, nnu-net [8]. the recent mask
transformer, mask2former [3], is also adapted to 3d for comparison. for the
baselines, patient-level labels are inferred from their predicted masks by
counting lesion pixels. as displayed in table 1, plan achieves the best accuracy
on all tasks, especially in nc preliminary diagnosis tasks, which demonstrates
the effectiveness of its dedicated patient branch that can explicitly aggregate
features from the whole image.lesion and pixel-level results. in lesion-level
evaluation, we treat a prediction as a true positive if its overlap with a
ground-truth lesion is >0.2 in dice. lesions smaller than 3 mm in radius are
ignored. as shown in table 2, the pixellevel accuracy of nnu-net and plan are
comparable, but plan's lesion-level accuracy is consistently higher than
nnu-net. in this work, we focus more on patient and lesion-level metrics.
although nc images have low contrast, they can still be used to segment and
classify lesions with ∼ 80% precision, recall, and classification accuracy. it
implies the potential of nc ct, which has been understudied in previous works.
mask2former has higher precision but lower recall in nc ct, especially for small
lesions, while plan achieves the best recall using the foreground-enhanced
sampling loss. both plan and mask2former achieve better classification accuracy,
which illustrates the mask transformer architecture is good at lesion-level
classification.",5
1387,Liver Tumor Screening and Diagnosis in CT with Pixel-Lesion-Patient Network,0,Comparison with Radiologists.,"in the reader study, we invited a senior radiologist with 16 years of experience
in liver imaging, and a junior radiologist with 2 years of experience. they
first read the nc ct of all subjects and provided a diagnosis of normal, benign,
or malignant. then, they read the dce scans and provided a diagnosis of the 8
tumor types. we consider patients with only one tumor type in this study. their
reading process is without time constraint. in table 3 and fig. 2, all methods
get good specificity probably because the normal subjects are completely
healthy. our model achieves comparable accuracy with the senior radiologist but
outperforms the junior one by a large margin in sensitivity and classification
accuracy. an ablation study for our method is shown in table 4. it can be seen
that our proposed anchor queries produced by the pixel branch, fes loss, and
lesionpatient consistency loss are useful for the final performance. the
efficacy of the lesion and patient branches has been analyzed above based on the
lesion and patient-level results. due to space limit, we will show the accuracy
for each tumor type and more qualitative examples in the supplementary
material.comparison with literature. in the pixel level, we obtain dice scores
of 77.2% and 84.2% using nc and dce cts, respectively. the current state of the
art (sota) of lits [1] achieved 82.2% in dice using cts in venous phase; [23]
achieved 81.3% in dice using dce ct of two phases. in the lesion level, our
precision and recall are 80.1% and 81.9% for nc ct, 92.2% and 89.0% for dce ct,
at 20% overlap. [25] achieved 83% and 93% for dce ct. sota of lits achieved
49.7% and 46.3% at 50% overlap. [21] classified lesions into 5 classes,
achieving 84% accuracy for dce and 49% for nc ct. we classify lesions into 8
classes with 85.9% accuracy for dce and 78.5% for nc ct. in the patient level,
[5] achieved auc=0.75 in nc ct tumor screening, while our auc is 0.985. in
summary, our results are superior or comparable to existing works.",5
1390,Self-supervised Learning for Endoscopic Video Analysis,2.0,Background and Related Work,"there exist a wide variety of endoscopic applications. here, we focus on
colonoscopy and laparoscopy, which combined covers over 70% of all endoscopic
procedures. specifically, our study addresses two important common tasks,
described below.cholecystectomy phase recognition. cholecystectomy is the
surgical removal of the gallbladder using small incisions and specialized
instruments. it is a common procedure performed to treat gallstones,
inflammation, or other conditions affecting the gallbladder. phase recognition
in surgical videos is an important task that aims to improve surgical workflow
and efficiency. apart from measuring quality and monitoring adverse event, this
task also serves in facilitating education, statistical analysis, and evaluating
surgical performance. furthermore, the ability to recognize phases allows
real-time monitoring and decision-making assistance during surgery, thus
improving patient safety and outcomes. ai solutions have shown remarkable
performance in recognizing surgical phases of cholecystectomy procedures
[17,18,32]; however, they typically require large labelled training datasets. as
an alternative, ssl methods have been developed [12,28,30], however, these are
early-days methods that based on heuristic, often require external information
and leads to sub-optimal performance. a recent work [27] presented an extensive
analysis of modern ssl techniques for surgical computer vision, yet on
relatively small laparoscopic datasets.optical polyp characterization.
colorectal cancer (crc) remains a critical health concern and significant
financial burden worldwide. optical colonoscopy is the standard of care
screening procedure for preventing crc through the identification and removal of
polyps [3]. according to colonoscopy guidelines, all identified polyps must be
removed and histologically evaluated regardless of their malignant nature.
optical biopsy enables practitioners to remove pre-cancerous adenoma polyps or
leave distal hyperplastic polyps in situ without the need for pathology
examination, by visually predicting histology. however, this technique is highly
dependent on operator expertise [14]. this limitation has motivated the
development of ai systems for automatic optical biopsy, allowing non-experts to
also effectively perform optical biopsy during polyp management. in recent
years, various ai systems have been developed to this end [1,19]. however,
training such automatic optical biopsy systems relies on a large body of
annotated data, while ssl has not been investigated in this context, to the best
of our knowledge.3 self-supervised learning for endoscopy ssl approaches have
produced impressive results recently [5][6][7][8], relying on two key factors:
(i) effective algorithms for unsupervised learning and (ii) training on
large-scale datasets. here, we first describe masked siamese networks [2], our
chosen ssl framework. additionally, we present our large-scale data collection
(see fig. 2). through extensive experiments in sect. 4, we show that training
msns on these substantial datasets unlocks their potential, yielding effective
representations that transfer well to public laparoscopy and colonoscopy
datasets.",5
1392,Self-supervised Learning for Endoscopic Video Analysis,3.2,Private Datasets,"laparoscopy. we compiled a dataset of laparoscopic procedures videos exclusively
performed on patients aged 18 years or older. the dataset consists of 7,877
videos recorded at eight different medical centers in israel. the dataset
predominantly consists of the following procedures: cholecystectomy (35%),
appendectomy (20%), herniorrhaphy (12%), colectomy (6%), and bariatric surgery
(5%). the remaining 21% of the dataset encompasses various standard laparoscopic
operations. the recorded procedures have an average duration of 47 min, with a
median duration of 40 min. each video recording was sampled at a rate of 1 frame
per second (fps), resulting in an extensive dataset containing 23.3 million
images. further details are given in the supplementary materials.colonoscopy. we
have curated a dataset comprising 13,979 colonoscopy videos of patients aged 18
years or older. these videos were recorded during standard colonoscopy
procedures performed at six different medical centers between the years 2019 and
2022. the average duration of the recorded procedures is 15 min, with a median
duration of 13 min. to identify and extract polyps from the videos, we employed
a pretrained polyp detection model [21,25,26]. using this model, we obtained
bounding boxes around the detected polyps. to ensure high-quality data, we
filtered out detections with confidence scores below 0.5. for each frame, we
cropped the bounding boxes to generate individual images of the polyps. this
process resulted in a comprehensive collection of 2.2 million polyp images.",5
1398,Merging-Diverging Hybrid Transformer Networks for Survival Prediction in Head and Neck Cancer,1.0,Introduction,"head and neck (h&n) cancer refers to malignant tumors in h&n regions, which is
among the most common cancers worldwide [1]. survival prediction, a regression
task that models the survival outcomes of patients, is crucial for h&n cancer
patients: it provides early prognostic information to guide treatment planning
and potentially improves the overall survival outcomes of patients [2].
multi-modality imaging of positron emission tomography -computed tomography
(pet-ct) has been shown to benefit survival prediction as it offers both
anatomical (ct) and metabolic (pet) information about tumors [3,4]. therefore,
survival prediction from pet-ct images in h&n cancer has attracted wide
attention and serves as a key research area. for instance, head and neck tumor
segmentation and outcome prediction challenges (hecktor) have been held for the
last three years to facilitate the development of new algorithms for survival
prediction from pet-ct images in h&n cancer [5][6][7].traditional survival
prediction methods are usually based on radiomics [8], where handcrafted
radiomics features are extracted from pre-segmented tumor regions and then are
modeled by statistical survival models, such as the cox proportional hazard
(coxph) model [9]. in addition, deep survival models based on deep learning have
been proposed to perform end-to-end survival prediction from medical images,
where pre-segmented tumor masks are often unrequired [10]. deep survival models
usually adopt convolutional neural networks (cnns) to extract image features,
and recently visual transformers (vit) have been adopted for its capabilities to
capture long-range dependency within images [11,12]. these deep survival models
have shown the potential to outperform traditional survival prediction methods
[13]. for survival prediction in h&n cancer, deep survival models have achieved
top performance in the hecktor 2021/2022 and are regarded as state-of-the-art
[14][15][16]. nevertheless, we identified that existing deep survival models
still have two main limitations.firstly, existing deep survival models are
underdeveloped in utilizing complementary multi-modality information, such as
the metabolic and anatomical information in pet and ct images. for survival
prediction in h&n cancer, existing methods usually use single imaging modality
[17,18] or rely on early fusion (i.e., concatenating multi-modality images as
multi-channel inputs) to combine multi-modality information [11,[14][15][16]19].
in addition, late fusion has been used for survival prediction in other diseases
such as gliomas and tuberculosis [20,21], where multi-modality features were
extracted by multiple independent encoders with resultant features fused.
however, early fusion has difficulties in extracting intra-modality information
due to entangled (concatenated) images for feature extraction, while late fusion
has difficulties in extracting inter-modality information due to fully
independent feature extraction. recently, tang et al. [22] attempted to address
this limitation by proposing a multi-scale non-local attention fusion (mnaf)
block for survival prediction of glioma patients, in which multi-modality
features were fused via non-local attention mechanism [23] at multiple scales.
however, the performance of this method heavily relies on using tumor
segmentation masks as inputs, which limits its generalizability.secondly,
although deep survival models have advantages in performing end-to-end survival
prediction without requiring tumor masks, this also incurs difficulties in
extracting region-specific information, such as the prognostic information in
primary tumor (pt) and metastatic lymph node (mln) regions. to address this
limitation, recent deep survival models adopted multi-task learning for joint
tumor segmentation and survival prediction, to implicitly guide the model to
extract features related to tumor regions [11,16,[24][25][26]. however, most of
them only considered pt segmentation and ignored the prognostic information in
mln regions [11,[24][25][26]. meng et al. [16] performed survival prediction
with joint pt-mln segmentation and achieved one of the top performances in
hecktor 2022. however, this method extracted entangled features related to both
pt and mln regions, which incurs difficulties in discovering the prognostic
information in pt-/mln-only regions.in this study, we design an x-shape
merging-diverging hybrid transformer network (named xsurv, fig. 1) for survival
prediction in h&n cancer. our xsurv has a merging encoder to fuse complementary
anatomical and metabolic information in pet and ct images and has a diverging
decoder to extract region-specific prognostic information in pt and mln regions.
our technical contributions in xsurv are three folds: (i) we propose a
merging-diverging learning framework for survival prediction. this framework is
specialized in leveraging multi-modality images and extracting regionspecific
information, which potentially could be applied to many survival prediction
tasks with multi-modality imaging. (ii) we propose a hybrid parallel
cross-attention (hpca) block for multi-modality feature learning, where both
local intra-modality and global inter-modality features are learned via parallel
convolutional layers and crossattention transformers. (iii) we propose a
region-specific attention gate (rag) block for region-specific feature
extraction, which screens out the features related to lesion regions. extensive
experiments on the public dataset of hecktor 2022 [7] demonstrate that our xsurv
outperforms state-of-the-art survival prediction methods, including the
top-performing methods in hecktor 2022.",6
1404,Merging-Diverging Hybrid Transformer Networks for Survival Prediction in Head and Neck Cancer,3.1,Dataset and Preprocessing,"we adopted the training dataset of hecktor 2022 (refer to
https://hecktor.grand-cha llenge.org/), including 488 h&n cancer patients
acquired from seven medical centers [7], while the testing dataset was excluded
as its ground-truth labels are not released. each patient underwent pretreatment
pet/ct and has clinical indicators. we present the distributions of all clinical
indicators in the supplementary materials. recurrence-free survival (rfs),
including time-to-event in days and censored-or-not status, was provided as
ground truth for survival prediction, while pt and mln annotations were provided
for segmentation. the patients from two centers (chum and chuv) were used for
testing and other patients for training, which split the data into 386/102
patients in training/testing sets. we trained and validated models using 5-fold
cross-validation within the training set and evaluated them in the testing
set.we resampled pet-ct images into isotropic voxels where 1 voxel corresponds
to 1 mm 3 . each image was cropped to 160 × 160 × 160 voxels with the tumor
located in the center. pet images were standardized using z-score normalization,
while ct images were clipped to [-1024, 1024] and then mapped to [-1, 1]. in
addition, we performed univariate and multivariate cox analyses on the clinical
indicators to screen out the prognostic indicators with significant relevance to
rfs (p < 0.05).",6
1409,ALL-IN: A Local GLobal Graph-Based DIstillatioN Model for Representation Learning of Gigapixel Histopathology Images With Application In Cancer Risk Assessment,1.0,Introduction,"the examination of tissue and cells using microscope (referred to as histology)
has been a key component of cancer diagnosis and prognostication since more than
a hundred years ago. histological features allow visual readout of cancer
biology as they represent the overall impact of genetic changes on cells
[20].the great rise of deep learning in the past decade and our ability to
digitize histopathology slides using high-throughput slide scanners have fueled
interests in the applications of deep learning in histopathology image analysis.
the majority of the efforts, so far, focus on the deployment of these models for
diagnosis and classification [27]. as such, there is a paucity of efforts that
embark on utilizing machine learning models for patient prognostication and
survival analysis (for example, predicting risk of cancer recurrence or expected
patient survival). while prognostication and survival analysis offer invaluable
insights for patient management, biological studies and drug development
efforts, they require careful tracking of patients for a lengthy period of time;
rendering this as a task that requires a significant amount of effort and
funding.in the machine learning domain, patient prognostication can be treated
as a weakly supervised problem, which a model would predict the outcome (e.g.,
time to cancer recurrence) based on the histopathology images. their majority
have utilized multiple instance learning (mil) [8] that is a two-step learning
method. first, representation maps for a set of patches (i.e., small fields of
view), called a bag of instances, are extracted. then, a second pooling model is
applied to the feature maps for the final prediction. different mil variations
have shown superior performances in grading or subtype classification in
comparison to outcome prediction [10]. this is perhaps due to the fact that
mil-based technique do not incorporate patch locations and interactions as well
as tissue heterogeneity which can potentially have a vital role in defining
clinical outcomes [4,26].to address this issue, graph neural networks (gnn) have
recently received more attention in histology. they can model patch relations
[17] by utilizing message passing mechanism via edges connecting the nodes
(i.e., small patches in our case). however, most gnn-based models suffer from
over smoothing [22] which limits nodes' receptive fields [3]. while local
contexts mainly capture cell-cell interactions, global patterns such as immune
cell infiltration patterns and tumor invasion in normal tissue structures (e.g.,
depth of invasion through myometrium in endometrial cancer [1]) could capture
critical information about outcome [10]. hence, locally focused methods are
unable to benefit from the coarse properties of slides due to their high
dimensions which may lead to poor performance.this paper aims to investigate the
potential of extracting fine and coarse features from histopathology slides and
integrating them for risk stratification in cancer patients. therefore, the
contributions of this work can be summarized as: 1) a novel graph-based model
for predicting survival that extracts both local and global properties by
identifying morphological super-nodes; 2) introducing a fine-coarse feature
distillation module with 3 various strategies to aggregate interactions at
different scales; 3) outperforming sota approaches in both risk prediction and
patient stratification scenarios on two datasets; 4) publishing two large and
rare prostate cancer datasets containing more than 220 graphs for active
surveillance and 240 graphs for brachytherapy cases. the code and graph
embeddings are publicly available at https://github.com/pazadimo/all-in 2
related works",6
1413,ALL-IN: A Local GLobal Graph-Based DIstillatioN Model for Representation Learning of Gigapixel Histopathology Images With Application In Cancer Risk Assessment,3.1,Problem Formulation,"for p n , which is the n-th patient, a set of patches {patch j } m j=1 is
extracted from the related whole slide images. in addition, a latent vector z j
∈ r 1×d is extracted from patch j using our encoder network (described in sect.
3.2) that results in feature matrix z n ∈ r m ×d for p n . finally, a specific
graph (g n ) for the n-th patient (p n ) can be constructed by assuming patches
as nodes. also, edges are connected based on the patches' k-nearest neighbour in
the spatial domain resulting in an adjacency matrix a n . therefore, for each
patient such as p n , we have a graph defined by adjacency matrix a n with size
m × m and features matrix z n (g n = graph(z n , a n )). we estimate k
super-nodes as matrix s n ∈ r k×d representing groups of local nodes with
similar properties as coarse features for p n 's slides. the final model ( θ )
with parameters θ utilizes g n and s n to predict the risk associated with this
patient:",6
1414,ALL-IN: A Local GLobal Graph-Based DIstillatioN Model for Representation Learning of Gigapixel Histopathology Images With Application In Cancer Risk Assessment,3.2,Self-supervised Encoder,"due to computational limits and large number of patches available for each
patient, we utilize a self-supervised approach to train an encoder to reduce the
inputs' feature space size. therefore, we use dino [9], a knowledge distillation
model (kdm), with vision transformer (vit) [13] as the backbone. it utilizes
global and local augmentations of the input patch j and passes them to the
student (s θ1,v it ) and teacher (t θ2,v it ) models to find their respective
representations without any labels. then, by using distillation loss, it makes
the representations' distribution similar to each other. finally, the fixed
weights of the teacher model are utilized in order to encode the input patches.",6
1416,ALL-IN: A Local GLobal Graph-Based DIstillatioN Model for Representation Learning of Gigapixel Histopathology Images With Application In Cancer Risk Assessment,3.4,Super-Nodes Extractor,"in order to find the coarse histo-morphological patterns disguised in the local
graph, we propose extracting k super-nodes, which each represents a weighted
cluster of further processed local features. intuitively, the number of
super-nodes k should not be very large or small, as the former encourages them
to only represent local clusters and the latter leads to larger clusters and
loses subtle details. we exploit the mincut [5] idea to extract super-nodes in a
differentiable process after an auxiliary ginconv to focus more on large-scale
interactions and to finally learn the most global correlated super-nodes.
inspired by the relaxation form of the known k-way mincut problem, we create a
continuous cluster matrix c n ∈ r m ×k using mlp layers and can finally estimate
the super-nodes features (s n ∈ r m ×d ) as:where w 1 , w 2 are mlps' weights.
hence, the extracted nodes are directly dependent on the final survival-specific
loss. in addition, two additional unsupervised weighted regularization terms are
optimized to improve the process:mincut regularizer. this term is motivated by
the original mincut problem and intends to solve it for the the patients' graph.
it is defined as:where d n is the diagonal degree matrix for a n . also, t r(.)
represents the trace of matrix and a n,norm is the normalized adjacency matrix.
r mincu t 's minimum value happens when t r(therefore, minimizing r mincu t
causes assigning strongly similar nodes to a same super-node and prevent their
association with others.orthogonality regularizer. r mincu t is non-convex and
potent to local minima such as assigning all vertexes to a super-node or having
multiple super-nodes with only a single vertex. r orthogonal penalizes such
solutions and helps the model to distribute the graph's features between
super-nodes. it can be formulated as:where ||.|| f is the frobenius norm, and i
is the identity matrix. this term pushes the model's parameters to find coarse
features that are orthogonal to each other resulting in having the most useful
global features. overall, utilizing these two terms encourages the model to
extract supernodes by leaning more towards the strongly associated vertexes and
keeping them against weakly connected ones [5], while the main survival loss
still controls the global extraction process.",6
1417,ALL-IN: A Local GLobal Graph-Based DIstillatioN Model for Representation Learning of Gigapixel Histopathology Images With Application In Cancer Risk Assessment,3.5,Fine-Coarse Distillation,"we propose our fine-coarse morphological feature distillation module to leverage
all-scale interactions in the final prediction by finding a local and a global
patientlevel representations ( ĥl,n , ĥg,n ). assume that x n ∈ r m ×d and s n ∈
r k×d are the feature matrices taken from local gnn (sect. 3.3) and super-nodes
for p n , respectively. we explore 3 different attention-based feature
distillation strategies for this task, including:-dual attention (da): two gated
self-attention modules for local and global properties with separate weights (w
φ,l , w φ,g , w k,l , w k,g , w q,l , w q,g ) are utilized to find patches
scores α l ∈ r 1×m and α g ∈ r 1×k and the final features ( ĥl,n , ĥg,n ) as:)
where x n,i and s n,i are rows of x n and s n , respectively, and the final
representation ( ĥ) is generated as ĥ = cat( ĥl , ĥg ).-mixed guided attention
(mga): in the first strategy, the information flows from local and global
features to the final representations in parallel without mixing any knowledge.
the purpose of this policy is the heavy fusion of fine and coarse knowledge by
exploiting shared weights (w φ,shared , w k,shared , w q,shared , w v,shared )
in both routes and benefiting from the guidance of local representation on
learning the global one by modifying eq. ( 7) to:-mixed co-attention (mca):
while the first strategy allows the extreme separation of two paths, the second
one has the highest level of mixing information. here, we take a balanced policy
between the independence and knowledge mixture of the two routes by only sharing
the weights without using any guidance.",6
1418,ALL-IN: A Local GLobal Graph-Based DIstillatioN Model for Representation Learning of Gigapixel Histopathology Images With Application In Cancer Risk Assessment,4.1,Dataset,"we utilize two prostate cancer (pca) datasets to evaluate the performance of our
proposed model. the first set (pca-as) includes 179 pca patients who were
managed with active surveillance (as). radical therapy is considered
overtreatment in these patients, so they are instead monitored with regular
serum prostate-specific antigen (psa) measurements, physical examinations,
sequential biopsies, and magnetic resonance imaging [23]. however, as may be
over-or under-utilized in low-and intermediate-risk pca due to the uncertainty
of current methods to distinguish indolent from aggressive cancers [11].
although majority of patients in our cohort are classified as low-risk based on
nccn guidelines [21], a significant subset of them experienced disease upgrade
that triggered definitive therapy (range: 6.2 to 224 months after diagnosis).the
second dataset (pca-bt) includes 105 pca patients with low to high risk disease
who went through brachytherapy. this treatment involves placing a radioactive
material inside the body to safely deliver larger dose of radiation at one time
[25]. the recorded endpoint for this set is biochemical recurrence with time to
recurrence ranging from 11.7 to 56.1 months. we also utilized the prostate
cancer grade assessment (panda) challenge dataset [7] that includes more than
10,000 pca needle biopsy slides (no outcome data) as an external dataset for
training the encoder of our model.",6
1419,ALL-IN: A Local GLobal Graph-Based DIstillatioN Model for Representation Learning of Gigapixel Histopathology Images With Application In Cancer Risk Assessment,4.2,Experiments,"we evaluate the models' performance in two scenarios utilizing several objective
metrics. implementation details are available in supplementary material.hazard
(risk) prediction. we utilize concordance-index (c-index) that measures the
relative ordering of patients with observed events and un-censored cases
relative to censored instances [2]. using c-index, we compare the quality of
hazard ranking against multiple methods including two mil (deepset [31], amil
[14]) and graph-based (dgc [17] and patch-gcn [10]) models that were utilized
recently for histopathology risk assessment. c-index values are available in
table 1. the proposed model with all strategies outperforms baselines across all
sets and is able to achieve 0.639 and 0.600 on pca-as and pca-bt, while the
baselines, at best, obtain 0.555, and 0.572, respectively. statistical tests
(paired t-test) on c-indices also show that our model is statistically better
than all baselines in pca-as and also superior to all models, except dgc, in
pca-bt. superior performance of our mca policy implies that balanced
exploitation of fine and coarse features with shared weights may provide more
robust contextual information compared to using mixed guided information or
utilizing them independently.patient stratification. the capacity of stratifying
patients into risk groups (e.g., low and high risk) is another criterion that we
employ to assess the utility of models in clinical practice. we evaluate model
performances via kaplan-meier curve [15] (cut-off set as the ratio of patients
with recurrence within 3 years of therapy initiation for pca-bt and the ratio of
upgraded cases for pca-as), logrank test [6] (with 0.05 as significance level),
and median outcome associated with risk groups (table 1 and fig. 2). our model
stratified pca-as patients into high-and low-risk groups with median time to
progression of 36.5 and 131.7 months, respectively. moreover, pca-bt cases
assigned to high-and low-risk groups have median recurrence time of 21.86 and
35.7 months. while none of the baselines are capable of assigning patients into
risk groups with statistical significance, our distillation policies achieve
significant separation in both pca-as and pca-bt datasets; suggesting that
global histo-morphological properties improve patient stratification
performance. furthermore, our findings have significant clinical implications as
they identify, for the first time, highrisk prostate cancer patients who are
otherwise known to be low-risk based on clinico-pathological parameters. this
group should be managed differently from the rest of the low-risk prostate
cancer patients in the clinic. therefore, providing evidence of the predictive
(as opposed to prognostic) clinical information that our model provides. while a
prognostic biomarker provides information about a patient's outcome (without
specific recommendation on the next course of action), a predictive biomarker
gives insights about the effect of a therapeutic intervention and potential
actions that can be taken.ablation study. we perform ablation study (table 2) on
various components of our framework including local nodes, self-supervised
vit-based encoder, and most importantly, super-nodes in addition to fine-coarse
distillation module. although our local-only model is still showing superior
results compared to baselines, this analysis demonstrates that all modules are
essential for learning the most effective representations. we also assess the
impact of our vit on the baselines (full-results in appendix), showing that it
can, on average, improve their performance by an increase of ∼ 0.03 in c-index
for pca-as. however, the best baseline with vit still has poorer performance
compared to our model in both datasets, while the number of parameters (reported
for vit embeddings' size in table 1) in our full-model is about half of this
baseline. achieving higher c-indices in our all model versions indicates the
important role of coarse features and global context in patient risk estimation
in addition to local patterns.",6
1427,DARC: Distribution-Aware Re-Coloring Model for Generalizable Nucleus Segmentation,3.1,Datasets,"the proposed method is evaluated on four datasets, including two h&e stained
image datasets consep [3] and cpm17 [28] and two ihc stained datasets deepliif
[29] and bc-deepliif [29,32]. consep [3] contains 28 training and 14 validation
images, whose sizes are 1000×1000 pixels. the images are extracted from 16
colorectal adenocarcinoma wsis, each of which belongs to an individual patient,
and scanned with an omnyx vl120 scanner within the department of pathology at
university hospitals coventry and warwickshire, uk. cpm17 [28] contains 32
training and 32 validation images, whose sizes are 500 × 500 pixels. the images
are selected from a set of glioblastoma multiforme, lower grade glioma, head and
neck squamous cell carcinoma, and non-small cell lung cancer whole slide tissue
images. deepliif [29] contains 575 training and 91 validation images, whose
sizes are 512 × 512 pixels. the images are extracted from the slides of lung and
bladder tissues. bc-deepliif [29,32] contains 385 training and 66 validation
ki67 stained images of breast carcinoma, whose sizes are 512 × 512 pixels.",6
1432,An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,1.0,Introduction,"accurate spatial characterization of tumor immune microenvironment is critical
for precise therapeutic stratification of cancer patients (e.g. via
immunotherapy). currently, this characterization is done manually by individual
pathologists on standard hematoxylin-and-eosin (h&e) or singleplex
immunohistochemistry (ihc) stained images. however, this results in high
interobserver variability among pathologists, primarily due to the large (> 50%)
disagreement among pathologists for immune cell phenotyping [10]. this is also a
big cause of concern for publicly available h&e/ihc cell segmentation datasets
with immune cell annotations from single pathologists. multiplex staining
resolves this issue by allowing different tumor and immune cell markers to be
stained on the same tissue section, avoiding any phenotyping guesswork from
pathologists. multiplex staining can be performed using expensive multiplex
immunofluorescence (mif) or via cheaper multiplex immunohistochemistry (mihc)
assays. mif staining (requiring expensive scanners and highly skilled lab
technicians) allows multiple markers to be stained/expressed on the same tissue
section (no co-registration needed) while also providing the utility to turn
on/off individual markers as needed. in contrast, current brightfield mihc
staining protocols relying on dab (3,3'-diaminobenzidine) alcohol-insoluble
chromogen, even though easily implementable with current clinical staining
protocols, suffer from occlusion of signal from sequential staining of
additional markers. to this effect, we introduce a new brightfield mihc staining
protocol using alcoholsoluble aminoethyl carbazole (aec) chromogen which allows
repeated stripping, restaining, and scanning of the same tissue section with
multiple markers. this requires only affine registration to align the digitized
restained images to obtain non-occluded signal intensity profiles for all the
markers, similar to mif staining/scanning.in this paper, we introduce a new
dataset that can be readily used out-ofthe-box with any artificial intelligence
(ai)/deep learning algorithms for spatial characterization of tumor immune
microenvironment and several other use cases.to date, only two denovo stained
datasets have been released publicly: bci h&e and singleplex ihc her2 dataset
[7] and deepliif singleplex ihc ki67 and mif dataset [2], both without any
immune or tumor markers. in contrast, we release the first denovo mif/mihc
stained dataset with tumor and immune markers for more accurate characterization
of tumor immune microenvironment. we also demonstrate several interesting use
cases: (1) ihc quantification of cd3/cd8 tumor-infiltrating lymphocytes (tils)
via style transfer, (2) virtual translation of cheap mihc stains to more
expensive mif stains, and (3) virtual tumor/immune cellular phenotyping on
standard hematoxylin images.",6
1433,An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,2.0,Dataset,"the complete staining protocols for this dataset are given in the accompanying
supplementary material. images were acquired at 20× magnification at moffitt
cancer center. the demographics and other relevant information for all eight
head-and-neck squamous cell carcinoma patients is given in table 1.",6
1434,An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,2.1,Region-of-Interest Selection and Image Registration,"after scanning the full images at low resolution, nine regions of interest
(rois) from each slide were chosen by an experienced pathologist on both mif and
mihc images: three in the tumor core (tc), three at the tumor margin (tm), and
three outside in the adjacent stroma (s) area. the size of the rois was
standardized at 1356×1012 pixels with a resolution of 0.5 µm/pixel for a total
surface area of 0.343 mm 2 . hematoxylin-stained rois were first used to align
all the mihc marker images in the open source fiji software using affine
registration. after that, hematoxylin-and dapi-stained rois were used as
references to align mihc and mif rois again using fiji and subdivided into
512×512 patches, resulting in total of 268 co-registered mihc and mif patches
(∼33 co-registered mif/mihc images per patient).",6
1438,An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,0,(b) Style Transfer:,"this sub-network creates the stylized ihc image using an attention module, given
(1) the input hematoxylin and the mif marker images and (2) the style and its
corresponding marker images. for synthetically generating stylized ihc images,
we follow the approach outlined in adaattn [8]. we use a pre-trained vgg-19
network [12] as an encoder to extract multi-level feature maps and a decoder
with a symmetric structure of vgg-19. we then use both shallow and deep level
features by using adaattn modules on multiple layers of vgg. this sub-network is
used to create a stylized image using the structure of the given hematoxylin
image while transferring the overall color distribution of the style image to
the final stylized image. the generated marker image from the first sub-network
is used for a more accurate colorization of the positive cells against the blue
hematoxylin counterstain/background; not defining loss functions based on the
markers generated by the first sub-network leads to discrepancy in the final
brown dab channel synthesis.for the stylized ihc images with ground truth
cd3/cd8 marker images, we also segmented corresponding dapi images using our
interactive deep learning impartial [9] tool
https://github.com/nadeemlab/impartial and then classified the segmented masks
using the corresponding cd3/cd8 channel intensities, as shown in fig. 4. we
extracted 268 tiles of size 512×512 from this final segmented and co-registered
dataset. for the purpose of training and testing all the models, we extract four
images of size 256 × 256 from each tile due to the size of the external ihc
images, resulting in a total of 1072 images. we randomly extracted tiles from
the lyon19 challenge dataset [14] to use as style ihc images. using these
images, we created a dataset of synthetically generated ihc images from the
hematoxylin and its marker image as shown in fig. 3.we evaluated the
effectiveness of our synthetically generated dataset (stylized ihc images and
corresponding segmented/classified masks) using our generated dataset with the
nuclick training dataset (containing manually segmented cd3/cd8 cells) [6]. we
randomly selected 840 and 230 patches of size 256 × 256 from the created dataset
for training and validation, respectively. nuclick training and validation sets
[6] comprise 671 and 200 patches, respectively, of size 256 × 256 extracted from
lyon19 dataset [14]. lyon19 ihc cd3/cd8 images are taken from breast, colon, and
prostate cancer patients. we split their training set into training and
validation sets, containing 553 and 118 images, respectively, and use their
validation set for testing our trained models. we trained three models including
unet [11], fpn [5], unet++ [15] with the backbone of resnet50 for 200 epochs and
early stopping on validation score with patience of 30 epochs, using binary
cross entropy loss and adam optimizer with learning rate of 0.0001. as shown in
table 2, models trained with our synthetic training set outperform those trained
solely with nuclick data in all metrics.we also tested the trained models on
1,500 randomly selected images from the training set of the lymphocyte
assessment hackathon (lysto) [1], containing image patches of size 299 × 299
obtained at a magnification of 40× from breast, prostate, and colon cancer whole
slide images stained with cd3 and cd8 markers. only the total number of
lymphocytes in each image patch are reported in this dataset. to evaluate the
performance of trained models on this dataset, we counted the total number of
marked lymphocytes in a predicted mask and calculated the difference between the
reported number of lymphocytes in each image with the total number of
lymphocytes in the predicted mask by the model. in table 2, the average
difference value (diffcount) of lymphocyte number for the whole dataset is
reported for each model. as seen, the trained models on our dataset outperform
the models trained solely on nuclick data.",6
1441,An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,4.0,Conclusions and Future Work,"we have released the first ai-ready restained and co-registered mif and mihc
dataset for head-and-neck squamous cell carcinoma patients. this dataset can be
used for virtual phenotyping given standard clinical hematoxylin images, virtual
clinical ihc dab generation with ground truth segmentations (to train
highquality segmentation models across multiple cancer types) created from
cleaner mif images, as well as for generating standardized clean mif images from
neighboring h&e and ihc sections for registration and 3d reconstruction of
tissue specimens. in the future, we will release similar datasets for additional
cancer types as well as release for this dataset corresponding whole-cell
segmentations via impartial https://github.com/nadeemlab/impartial.",6
1449,Detection of Basal Cell Carcinoma in Whole Slide Images,0,"Acc(W, d, D","3 experiments the dataset, comprised of 194 skin slides acquired from the
southern sun pathology laboratory, includes 148 bcc cases and 46 other types
(common nevus, scc), all manually annotated by a dermatopathologist. bcc slides
served as positive samples and the rest as negatives. these slides were scanned
at ×20 magnification with a 0.44 µm pixel size using a leica aperio at2 scanner.
the patient data were separated between training and testing to prevent overlap.
details are shown in table 1. the experimental setup involved training models on
two nvidia rtx a6000 gpus using pytorch. these models, initialized from a
zero-mean gaussian with standard deviation σ = 0.001, were trained for 200
epochs with a batch size of 256. training used the adam optimizer with a dynamic
learning rate reduction strategy, starting with a learning rate of 5e-5
following a cosine schedule.",6
1461,IIB-MIL: Integrated Instance-Level and Bag-Level Multiple Instances Learning with Label Disambiguation for Pathological Image Analysis,3.1,Dataset,"we evaluate our model with three datasets. (1) luad-gm dataset: the objective is
to predict the epidermal growth factor receptor (egfr) gene mutations in
patients with lung adenocarcinoma (luad) using 723 whole slide image (wsi)
slices, where 47% of cases have egfr mutations. (2) tcga-nsclc and tcga-rcc
datasets: cancer type classification is performed using the cancer genome atlas
(tcga) dataset. the tcga-nsclc dataset comprised two subtypes, lung squamous
cell carcinoma (lusc) and lung adenocarcinoma (luad), while the tcga-rcc dataset
included three subtypes: renal chromophobe cell carcinoma (kich), renal clear
cell carcinoma (kirc), and renal papillary cell carcinoma (kirp).",6
1468,Multi-scale Prototypical Transformer for Whole Slide Image Classification,2.1,MIL Problem Formulation,"mil is a typical weakly supervised learning method, where the training data
consists of a set of bags, and each bag contains multiple instances. the goal of
mil is to learn a classifier that can predict the label of a bag based on the
instances in it. in binary classification, a bag can be marked as negative if
all in-stances in the bag are negative, otherwise, the bag is labeled as
positive with at least one positive instance. in the mil setting, a wsi is
considered as a bag and the numerous cropped patches in wsi are regarded as
instances in the bag. a wsi dataset t can be defined as:where x i denotes a
patient, y i the label of x i , i j i is the j-th instance of x i , n is the
number of patients and n is the number of instances.",6
1484,Thyroid Nodule Diagnosis in Dynamic Contrast-Enhanced Ultrasound via Microvessel Infiltration Awareness,3.0,Experiments,"dataset. our dataset contained 282 consecutive patients who underwent thyroid
nodule examination at nanjing drum tower hospital. all patients performed
dynamic ceus examination by an experienced sonographer using an iu22 scanner
(philips healthcare, bothell, wa) equipped with a linear transducer l9-3 probe.
these 282 cases included 147 malignant nodules and 135 benign nodules. on the
one hand, the percutaneous biopsy based pathological examination was implemented
to determine the ground-truth of malignant and benign. on the other hand, a
sonographer with more than 10 years of experience manually annotated the nodule
lesion mask to obtain the pixel-level groundtruth of thyroid nodules
segmentation. all data were approved by the institutional review board of
nanjing drum tower hospital, and all patients signed the informed consent before
enrollment into the study.implementation details. our network was implemented
using pytorch framework with the single 12 gb gpu of nvidia rtx 3060. during
training, we first pre-trained the talr backbone via dice loss for 30 epochs and
used adam optimizer with learning rate of 0.0001. then, we loaded the
pre-trained weights to train the whole model for 100 epochs and used adam
optimizer with learning rate of 0.0001. here, we set batch-size to 4 during the
entire training process the ceus consisted the full wash-in and wash-out phases,
and the resolution of each frame was (600 × 800). in addition, we carried out
data augmentation, including random rotation and cropping, and we resize the
resolution of input frames to (224 × 224). we adopted 5-fold cross-validation to
achieve quantitative evaluation. three indexes including dice, recall, and iou,
were used to evaluate the lesion recognition task, while five indexes, namely
average accuracy (acc), sensitivity (se), specificity (sp), f1-score (f1), and
auc, were used to evaluate the diagnosis task.experimental results. as in table
1, we compared our method with sota method including v-net, unet3d, transunet.
for the task of identifying lesions, the index of recall is important, because
information in irrelevant regions can be discarded, but it will be disastrous to
lose any lesion information. v-net achieved the highest recall scores compared
to others; thus, it was chosen as the backbone of tlar. table 1 revealed that
the modules (tpa, saf, and ipo) used in the network greatly improved the
segmentation performance compared to baseline, increasing dice and recall scores
by 7.60% and 7.23%, respectively. for the lesion area recognition task, our
method achieved the highest dice of 85.54% and recall of 90.40%, and the
visualized results were shown in fig. 3. to evaluate the effectiveness of the
baseline of lightweight c3d, we compared the results with sota video
classification methods including c3d, r3d, r2plus1d and convlstm. for fair
comparison, all methods used the manually annotated lesion mask to assist the
diagnosis. experimental results in table 2 revealed that our baseline network
could be useful for the diagnosis. with the effective baseline, the introduced
modules including tlar, saf and ipo further improved the diagnosis accuracy,
increasing the accuracy by 9.5%. the awareness of microvascular infiltration
using saf and ipo unit was helpful for ceus-based diagnosis, as it could improve
the diagnosis accuracy by 7.69% (as in table 2). as in appendix fig. a1,
although sota method fails to focus on lesion areas, our method can pinpoint
discriminating lesion areas. influence of α values. the value of α in saf is
associated with simulating microvessel infiltration. figure 3 (c) showed that
the diagnosis accuracy increased along with the increment of α and then tended
to become stable when α was close to 9. therefore, for balancing the efficiency
and performance, the number of ipo was set as n = 3 and α was set as α = {1, 5,
9} to generate a group of confidence maps that can simulate the process of
microvessel infiltration. (more details about the setting of n is in appendix
fig. a4 of the supplementary material.)",6
1495,Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,1.0,Introduction,"the tumor microenvironment (tme) is comprised of cancer, immune (e.g. b
lymphocytes, and t lymphocytes), stromal, and other cells together with
noncellular tissue components [3,5,24,30]. it is well acknowledged that tumors
evolve in close interaction with their microenvironment. quantitatively
characterizing tme has the potential to predict tumor aggressiveness and
treatment response [3,23,24,30]. different types of lymphocytes such as cd4+
(helper t cells), cd8+ (cytotoxic t cells), cd20+ (b cells), within the tme
naturally interact with tumor and stromal cells. studies [5,9] have shown that
quantifying spatial interplay of these different cell families within the tme
can provide more prognostic/predictive value compared to only measuring the
density of a single biomarker such as tumor-infiltrating lymphocytes (tils)
[3,24]. immunotherapy (io) is the standard treatment for patients with advanced
non-small cell lung cancer (nsclc) [19] but only 27-45% of patients respond to
this treatment [21]. therefore, better algorithms and improved biomarkers are
essential for identifying which cancer patients are most likely to respond to io
in advance of treatment. quantitative features that relate to the complex
spatial interplay between different types of b-and t-cells in the tme might
unlock attributes that are associated with io response. in this study, we
introduce a novel approach called triangular analysis of geographical interplay
of lymphocytes (triangil), representing a unique and interpretable way to
characterize the distribution, and higher-order interaction of various cell
families (e.g., cancerous cells, stromal cells, lymphocyte subtypes) across
digital histopathology slides. we demonstrate the efficacy of triaangil for
characterizing tme in the context of predicting 1) response to io with immune
checkpoint inhibitors (ici), 2) overall survival (os), in patients with nsclc,
and 3) providing novel insights into the spatial interplay between different
immune cell subtype. triangil source code is publicly available at
http://github.com/sarayar/triangil.",6
1496,Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,2.0,Previous Related Work and Novel Contributions,"many studies have only looked at the density of a single biomarker (e.g. tils),
to show that a high density of tils is associated with improved patient survival
and treatment response in nsclc [3,24]. other works have attempted to
characterize the spatial arrangement of cells in tme using computational
graphbased approaches. these approaches include methods that connect cells
regardless of their type (1) using global graphs (gg) such as voronoi that
connect all nuclei [2,14], or (2) using cell cluster graphs (ccg) [16] to create
multiple nuclear subgraphs based on cell-to-cell proximity to predict tumor
aggressiveness and patient outcome [16]. others have explored (3) the spatial
interplay between two different cell types [5].one example approach is spatial
architecture of til (spatil) [9] which attempted to characterize the interplay
between immune and cancer cells and has proven to be helpful in predicting the
recurrence in early-stage nsclc. all of these approaches point to overwhelming
evidence that spatial architecture of cells in tme is critical in predicting
cancer outcome. however, these approaches have not been able to exploit
higher-order interactions and dependencies between multiple cell types (> 2),
relationships that might provide additional actionable insights. the
contributions of this work include:(1) triangil is a computational framework
that characterizes the architecture and relationships of different cell types
simultaneously. instead of measuring only simple two-by-two relations between
cells, it seeks to identify triadic spatial relations (hyperedges [18,20] have
shown great capabilities in solving complex problems in the biomedical field,
these tend to be black-box in nature. a key consideration in cancer immunology
is the need for actionable insights into the spatial relationships between
different types of immune cells. not only does triangil provide predictions that
are on par or superior compared to dl approaches, but also provides a way to
glean insights into the spatial interplay of different immune cell types. these
complex interactions enhance our understanding of the tme and will help pave the
way for new therapeutic strategies that leverage these insights.",6
1499,Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,4.1,Dataset,"the cohort employed in this study was composed of pre-treatment tumor biopsy
specimens from patients with nsclc from five centers (two centers for training
(s t ) and three centers for independent validation (s v )). the entire analysis
was carried out using 122 patients in experiment 1 (73 in s t , and 49 in s v )
and 135 patients in experiment 2 (81 in s t , and 54 in s v ). specimens were
analyzed with a multiplexed quantitative immunofluorescence (qif) panel using
the method described in [22]. from each whole slide image, 7 representative
tiles were obtained and used to train the software inform to define background,
tumor and stromal compartments. then, individual cells were segmented based on
nuclear dapi staining and the segmentation performance was controlled by direct
visualization of samples by a trained observer. next, the software was trained
to identify cell subtypes based on marker expression (cd8, cd4, cd20, ck for
tumor epithelial cells and absence of these markers for stromal cells).",6
1501,Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,0,TIL density (DenTIL):,"for every patient, multiple density measures including the number of different
cells types and their ratios are calculated [3,24] (supplemental table 2).",6
1502,Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,0,"GG: A Delaunay triangulation, a Voronoi diagram, and a Minimum Spanning","tree were constructed [2,14] on all nuclei regardless of their type.
architectural features (e.g., perimeter, triangle area, edge length) were then
calculated on these global graphs for each patient.",6
1503,Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,0,CCG:,"for every patient, subgraphs are built on nuclei regardless of their type and
only based on their euclidean distance. local graph metrics (e.g. clustering
coefficient) [16] are then calculated from these subgraphs. spatil: for each
patient, first, subgraphs are built on individual cell types based on a distance
parameter. the convex hulls are then constructed on these subgraphs. after
selecting every two cell types, features are extracted from their convex hulls
(e.g. the number of clusters of each cell type, area intersected between
clusters [9]; complete list of combinations in supplemental table 3).",6
1505,Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,4.3,Experiment 1: Immunotherapy Response Prediction in Lung Cancer,"design: triangil was also trained to differentiate between patients who
responded to io and those who did not. for our study, the responders to io were
identified as those patients with complete response, partial response, and
stable disease, and non-responders were patients with progressive disease. a
linear discriminant analysis (lda) classifier was trained on s t to predict
which patients would respond to io. for creating the model, the minimum
redundancy maximum relevance (mrmr) method [1] was used to select the top
features. the same procedure using mrmr and lda was performed for the
comparative hand-crafted approaches. the ability to identify responders post-io
was assessed by the area under the receiver operating characteristic curve (auc)
in s v .",6
1506,Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,0,Results:,"the two top predictive triangil features were found to be the number of edges
between stroma and cd4+ cells, and the number of edges between stroma and tumor
cells with more interactions between stromal cells and both cd4+ and tumor cells
being associated with response to io. this finding is concordant with other
studies [13,17,22,27] that stromal tils were significantly associated with
improved os. therefore, triangil approach is not only predictive of treatment
response but more critically it enables biological interpretations that a dl
model might not be able to provide. in s v , this lda classifier was able to
distinguish responders from non-responders to io with au c t ri =0. design: s t
was used to construct a least absolute shrinkage and selection operator (lasso)
[28] regularized cox proportional hazards model [6] using the triangil features,
to obtain risk score for each patient. lasso features are listed in supplemental
table 4. the median risk score in s t was used as a threshold in both s t and s
v to dichotomize patients into low-risk/high-risk categories. kaplan-meier (km)
survival curves [26] were plotted and the model performance was summarized by
hazard ratio (hr), with corresponding (95% confidence intervals (ci)) using the
log-rank test, and harrell's concordance index (c-index) on s v . the c-index
evaluates the correlation between risk predictions and survival times, aiming to
maximize the discrimination between high-risk and low-risk patients [11]. os is
the time between the initiation of io to the death of the patient. the patients
were censored if the date of death was unknown.result: figure 2 presents some
triangil features in a field of view for a patient with long-term survival and
another with short-term survival. more triangular relationships, shorter
triangle edges, and smaller triangles with smaller perimeters are found in the
long-term survival case when analyzing the triadic interactions within
tumor-stroma-cd4, thereby suggesting higher relative presence and closer
interaction of these cell families. figure 3 illustrates the km plots for the
six approaches. we also calculated the concordance index (c-index) for the two
prognostic approaches in s v . the c-index for triangil and gnn methods were
0.64, and 0.63 respectively. therefore, overall triangil worked marginally
better than gnn, with much higher biological interpretability.",6
1507,Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,5.0,Concluding Remarks,"we presented a new approach, triangular analysis of geographical interplay of
lymphocytes (triangil), to quantitatively chartacterize the spatial arrangement
and relative geographical interplay of multiple cell families across
pathological images. compared to previous spatial graph-based methods, triangil
quantifies the spatial interplay between multiple cell families, providing a
more comprehensive portrait of the tumor microenvironment. triangil was
predictive of response after io (n = 122) and also demonstrated a strong
correlation with os in nsclc patients treated with io (n = 135). triangil
outperformed other graph-and dl-based approaches, with the added benefit of
provoding interpretability with regard to the spatial interplay between cell
families. for instance, triangil yielded the insight that more interactions
between stromal cells and both cd4+ and tumor cells appears to be associated
with better response to io. although five cell families were studies in this
work, triangil is flexible and could include other cell types (e.g.,
macrophages). future work will entail larger validation studies and also
evaluation on other use cases.",6
1522,DiffDP: Radiotherapy Dose Prediction via a Diffusion Model,1.0,Introduction,"radiotherapy, one of the mainstream treatments for cancer patients, has gained
notable advancements in past decades. for promising curative effect, a
high-quality radiotherapy plan is demanded to distribute sufficient dose of
radiation to the planning target volume (ptv) while minimizing the radiation
hazard to organs at risk (oars). to achieve this, radiotherapy plans need to be
manually adjusted by the dosimetrists in a trial-and-error manner, which is
extremely labor-intensive and time-consuming [1,2]. additionally, the quality of
treatment plans might be variable among radiologists due to their different
expertise and experience [3]. consequently, it is essential to develop a robust
methodology to automatically predict the dose distribution for cancer patients,
relieving the burden on dosimetrists and accelerating the radiotherapy
procedure.recently, the blossom of deep learning (dl) has promoted the automatic
medical image processing tasks [4][5][6], especially for dose prediction
[7][8][9][10][11][12][13][14]. for example, nguyen et al. [7] modified the
traditional 2d unet [15] to predict the dose of prostate cancer patients. wang
et al. [10] utilized a progressive refinement unet (prunet) to refine the
predictions from low resolution to high resolution. besides the above unetbased
frameworks, song et al. [11] employed the deeplabv3+ [16] to excavate contextual
information from different scales, thus obtaining accuracy improvements in the
dose prediction of rectum cancer. mahmood et al. [12] utilized a generative
adversarial network (gan)-based method to predict the dose maps of oropharyngeal
cancer. furthermore, zhan et al. [13] designed a multi-organ constraint loss to
enforce the deep model to better consider the dose requirements of different
organs. following the idea of multi-task learning, tan et al. [8] utilized
isodose line and gradient information to promote the performance of dose
prediction of rectum cancer. to ease the burden on the delineation of ptv and
oars, li et al. [17] constructed an additional segmentation task to provide the
dose prediction task with essential anatomical knowledge.although the above
methods have achieved good performance in predicting dose distribution, they
suffer from the over-smoothing problem. these dl-based dose prediction methods
always apply the l 1 or l 2 loss to guide the model optimization which
calculates a posterior mean of the joint distribution between the predictions
and the ground truth [17,18], leading to the over-smoothed predicted images
without important high-frequency details [19]. we display predicted dose maps
from multiple deep models in fig. 1. as shown, compared with the ground truth,
i.e., (5) in fig. 1, the predictions from (1) to (3) are blurred with fewer
high-frequency details, such as ray shapes. these high-frequency features formed
by ray penetration reveal the ray directions and dose attenuation with the aim
of killing the cancer cells while protecting the oars as much as possible, which
are critical for radiotherapy. consequently, exploring an automatic method to
generate high-quality predictions with rich high-frequency information is
important to improve the performance of dose prediction. currently, diffusion
model [20] has verified its remarkable potential in modeling complex image
distributions in some vision tasks [21][22][23]. unlike other dl models, the
diffusion model is trained without any extra assumption about target data
distribution, thus evading the average effect and alleviating the over-smoothing
problem [24]. figure 1 (4) provides an example in which the diffusion-based
model predicts a dose map with shaper and clearer boundaries of ray-penetrated
areas. therefore, introducing a diffusion model to the dose prediction task is a
worthwhile endeavor.in this paper, we investigate the feasibility of applying a
diffusion model to the dose prediction task and propose a diffusion-based model,
called diffdp, to automatically predict the clinically acceptable dose
distribution for rectum cancer patients. specifically, the diffdp consists of a
forward process and a reverse process. in the forward process, the model employs
a markov chain to gradually transform dose distribution maps with complex
distribution into gaussian distribution by progressively adding pre-defined
noise. then, in the reverse process, given a pure gaussian noise, the model
gradually removes the noise in multiple steps and finally outputs the predicted
dose map. in this procedure, a noise predictor is trained to predict the noise
added in the corresponding step of the forward process. to further ensure the
accuracy of the predicted dose distribution for both the ptv and oars, we design
a dl-based structure encoder to extract the anatomical information from the ct
image and the segmentation masks of the ptv and oars. such anatomical
information can indicate the structure and relative position of organs. by
incorporating the anatomical information, the noise predictor can be aware of
the dose constraints among ptv and oars, thus distributing more appropriate dose
to them and generating more accurate dose distribution maps.overall, the
contributions of this paper can be concluded as follows: (1) we propose a novel
diffusion-based model for dose prediction in cancer radiotherapy to address the
over-smoothing issue commonly encountered in existing dl-based dose prediction
methods. to the best of our knowledge, we are the first to introduce the
diffusion model for this task. (2) we introduce a structure encoder to extract
the anatomical information available in the ct images and organ segmentation
masks, and exploit the anatomical information to guide the noise predictor in
the diffusion model towards generating more precise predictions. (3) the
proposed diffdp is extensively evaluated on a clinical dataset consisting of 130
rectum cancer patients, and the results demonstrate that our approach
outperforms other state-of-the-art methods.",6
1523,DiffDP: Radiotherapy Dose Prediction via a Diffusion Model,2.0,Methodology,"an overview of the proposed diffdp model is illustrated in fig. 2, containing
two markov chain processes: a forward process and a reverse process. an image
set of cancer patient is defined as {x, y}, where x ∈ r h ×w ×(2+o) represents
the structure images, ""2"" signifies the ct image and the segmentation mask of
the ptv, and o denotes the total number of segmentation mask of oars. meanwhile,
y ∈ r h ×w ×1 is the corresponding dose distribution map for x. concretely, the
forward process produces a sequence of noisy images {y 0 , y 1 , . . . , y t },
y 0 = y by gradually adding a small amount of noise to y in t steps with the
noise increased at each step and a noise predictor f is constructed to predict
the noise added to y t-1 by treating y t , anatomic information from x and
embedding of step t as input. to obtain the anatomic information, a structure
encoder g is designed to extract the crucial feature representations from the
structure images. then, in the reverse process, the model progressively deduces
the dose distribution map by iteratively denoising from y t using the
well-trained noise predictor.",6
1530,DiffDP: Radiotherapy Dose Prediction via a Diffusion Model,3.0,Experiments and Results,"dataset and evaluations. we measure the performance of our model on an in-house
rectum cancer dataset which contains 130 patients who underwent volumetric
modulated arc therapy (vmat) treatment at west china hospital. concretely, for
every patient, the ct images, ptv segmentation, oars segmentations, and the
clinically planned dose distribution are included. additionally, there are four
oars of rectum cancer containing the bladder, femoral head r, femoral head l,
and small intestine. we randomly select 98 patients for model training, 10
patients for validation, and the remaining 22 patients for test. the thickness
of the cts is 3 mm and all the images are resized to the resolution of 256 × 256
before the training procedure.we measure the performance of our proposed model
with multiple metrics. considering dm represents the minimal absorbed dose
covering m% percentage volume of ptv, we involve d 98 , d 2 , maximum dose (d
max ), and mean dose (d mean ) as metrics. besides, the heterogeneity index (hi)
is used to quantify dose heterogeneity [26]. to quantify performance more
directly, we calculate the difference ( ) of these metrics between the ground
truth and the predicted results. more intuitively, we involve the dose volume
histogram (dvh) [27] as another essential metric of dose prediction performance.
when the dvh curves of the predictions are closer to the ground truth, we can
infer higher prediction accuracy.comparison with state-of-the-art methods. to
verify the superior accuracy of our proposed model, we select multiple
state-of-the-art (sota) models in dose prediction, containing unet (2017) [7],
gan (2018) [12], deeplabv3+ (2020) [11], c3d (2021) [9], and prunet (2022) [10],
for comparison. the quantitative comparison results are listed in table . 1
where our method outperforms the existing sotas in terms of all metrics.
specifically, compared with deeplabv3+ with the second-best accuracy in hi
(0.0448) and d 98 (0.0416), the results generated by the proposed are 0.0035 and
0.0014 lower, respectively. as for d 2 and d max , our method gains overwhelming
performance with 0.0008 and 0.0005, respectively. moreover, the paired t-test is
conducted to investigate the significance of the results. the p-values between
the proposed and other sotas are almost all less than 0.05, indicating that the
enhancement of performance is statistically meaningful.besides the quantitative
results, we also present the dvh curves derived by compared methods in fig. 3.
the results are compared on ptv as well as two oars: bladder and small
intestine. compared with other methods, the disparity between the dvh curves of
our method and the ground truth is the smallest, demonstrating the superior
performance of the proposed. furthermore, we display the visualization
comparison in fig. 4. as we can see, the proposed model achieves the best visual
quality with clearer and sharper high-frequency details (as indicated by red
arrows). furthermore, the error map of the proposed is the darkest, suggesting
the least disparity compared with the ground truth.ablation study. to study the
contributions of key components of the proposed method, we conduct the ablation
experiments by 1) removing the structure encoder from the proposed method and
concatenating the anatomical images x and noisy image y t together as the
original input for diffusion model (denoted as baseline); 2) the proposed diffdp
model. the quantitative results are given in table 2. we can clearly see the
performance for all metrics is enhanced with the structure encoder,
demonstrating its effectiveness in the proposed model.",6
1531,DiffDP: Radiotherapy Dose Prediction via a Diffusion Model,4.0,Conclusion,"in this paper, we introduce a novel diffusion-based dose prediction (diffdp)
model for predicting the radiotherapy dose distribution of cancer patients. the
proposed method involves a forward and a reverse process to generate accurate
prediction by progressively transferring the gaussian noise into a dose
distribution map. moreover, we propose a structure encoder to extract anatomical
information from patient anatomy images and enable the model to concentrate on
the dose constraints within several essential organs. extensive experiments on
an in-house dataset with 130 rectum cancer patients demonstrate the superiority
of our method.",6
1532,Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning,1.0,Introduction,"in the past few years, the development of histopathological whole slide image
(wsi) analysis methods has dramatically contributed to the intelligent cancer
diagnosis [4,10,15]. however, due to the limitation of hardware resources, it is
difficult to directly process gigapixel wsis in an end-to-end framework. recent
studies usually divide the wsi analysis into multiple stages.generally, multiple
instance learning (mil) is one of the most popular solutions for wsi analysis
[14,17,18]. mil methods regard wsi recognition as a weakly supervised learning
problem and focus on how to effectively and efficiently aggregate
histopathological local features into a global representation. several studies
introduced attention mechanisms [9], recurrent neural networks [2] and graph
neural network [8] to enhance the capacity of mil in structural information
mining. more recently, transformer-based structures [13,19] are proposed to
aggregate long-term relationships of tissue regions, especially for large-scale
wsis. these transformer-based models achieved state-of-the-art performance in
sub-type classification, survival prediction, gene mutant prediction, etc.
however, these methods still rely on at least patient-level annotations. in the
networkbased consultation and communication platforms, there is a vast quantity
of unlabeled wsis not effectively utilized. these wsis are usually without any
annotations or definite diagnosis descriptions but are available for
unsupervised learning. in this case, self-supervised learning (ssl) is gradually
introduced into the mil-based framework and is becoming a new paradigm for wsi
analysis [1,11,16]. typically, chen et al. [5] explored and posed a new
challenge referred to as slide-level self-learning and proposed hipt, which
leveraged the hierarchical structure inherent in wsis and constructed multiple
levels of the self-supervised learning framework to learn high-resolution image
representations. this approach enables mil-based frameworks to take advantage of
abundant unlabeled wsis, further improving the accuracy and robustness of tumor
recognition.however, hipt is a hierarchical learning framework based on a greedy
training strategy. the bias and error generated in each level of the
representation model will accumulate in the final decision model. moreover, the
vit [6] backbone used in hipt is originally designed for nature sense images in
fixed sizes whose positional information is consistent. however,
histopathological wsis are scale-varying and isotropic. the positional embedding
strategy of vit will bring ambiguity into the structural modeling. to relieve
this problem, kat [19] built hierarchical masks based on local anchors to
maintain multi-scale relative distance information in the training. but these
masks are manually defined which is not trainable and lacked orientation
information. the current embedding strategy for wsi structural description is
not complete.in this paper, we propose a novel whole slide image representation
learning framework named position-aware masked autoencoder (pama), which
achieves slide-level representation learning by reconstructing the local
representations of the wsi in the patch feature space. pama can be trained
end-to-end from the local features to the wsi-level representation. moreover, we
designed a position-aware cross-attention mechanism to guarantee the correlation
of localto-global information in the wsis while saving computational resources.
the proposed approach was evaluated on a public tcga-lung dataset and an
in-house endometrial dataset and compared with 6 state-of-the-art methods. the
results have demonstrated the effectiveness of the proposed method.the
contribution of this paper can be summarized into three aspects. (1) we propose
a novel whole slide image representation learning framework named position-aware
masked autoencoder (pama). pama can make full use of abundant unlabeled wsis to
learn discriminative wsi representations. (2) we propose a position-aware
cross-attention (paca) module with a kernel reorientation (kro) strategy, which
makes the framework able to maintain the spatial integrity and semantic
enrichment of slide representation during the selfsupervised training. (3) the
experiments on two datasets show our pama can achieve competitive performance
compared with sota mil methods and ssl methods.",6
1593,MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,1.0,Introduction,"colorectal cancer is the third most common malignant tumor, and nearly half of
all patients with colorectal cancer develop liver metastasis during the course
of the disease [6,16]. liver metastases after surgery of colorectal cancer is
the major cause of disease-related death. colorectal cancer liver metastases
(crlm) have therefore become one of the major focuses in the medical field.
patients with colorectal cancer typically undergo contrast-enhanced computed
tomography (cect) scans multiple times during follow-up visits after surgery for
early detection of crlm, generating a 5d dataset. in addition to the axial,
sagittal, and coronal planes in 3d ct scans, the data comprises
contrast-enhanced multiple phases as its 4th dimension, along with different
timestamps as its 5th dimension. radiologists heavily rely on this data to
detect the crlm in the very early stage [15].extensive existing works have
demonstrated the power of deep learning on various spatial-temporal data, and
can potentially be applied towards the problem of crlm. for example, originally
designed for natural data, several mainstream models such as e3d-lstm [12],
convlstm [11] and predrnn [13] use convolutional neural networks (cnn) to
capture spatial features and long short-term memory (lstm) to process temporal
features. some other models, such as simvp [4], replace lstms with cnns but
still have the capability of processing spatiotemporal information. these models
can be adapted for classification tasks with the use of proper classification
head.however, all these methods have only demonstrated their effectiveness
towards 3d/4d data (i.e., time-series 2d/3d images), and it is not clear how to
best extend them to work with the 5d cect data. part of the reason is due to the
lack of public availability of such data. when extending these models towards 5d
cect data, some decisions need to be made, for example: 1) what is the most
effective way to incorporate the phase information? simply concatenating
different phases together may not be the optimal choice, because the positional
information of the same ct slice in different phases would be lost.2) shall we
use uni-directional lstm or bi-direction lstm? e3d-lstm [12] shows
uni-directional lstm works well on natural videos while several other works show
bi-directional lstm is needed in certain medical image segmentation tasks
[2,7].in this paper, we investigate how state-of-art deep learning models can be
applied to the crlm prediction task using our 5d cect dataset. we evaluate the
effectiveness of bi-directional lstm and explore the possible method of
incorporating different phases in the cect dataset. specifically, we show that
the best prediction accuracy can be achieved by enhancing e3d-lstm [12] with a
bi-directional lstm and a multi-plane structure. when patients undergo cect
scans to detect crlm, typically three phases are captured: the unenhanced plain
scan phase (p), the portal venous phase (v), and the arterial phase (a). the p
phase provides the basic shape of the liver tissue, while the v and a phases
provide additional information on the liver's normal and abnormal blood vessel
patterns, respectively [10]. professional radiologists often combine the a and v
phases to determine the existence of metastases since blood in the liver is
supplied by both portal venous and arterial routes.",6
1594,MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,2.1,Dataset,"our dataset follows specific inclusion criteria:-no tumor appears on the ct
scans. that means patients have not been diagnosed as crlm when they took the
scans.-patients were previously diagnosed with colorectal cancer tnm stage i to
stage iii, and recovered from colorectal radical surgery. -patients have two or
more times of cect scans.-we already determined whether or not the patients had
liver metastases within 2 years after the surgery, and manually labeled the
dataset based on this. -no potential focal infection in the liver before the
colorectal radical surgery.-no metastases in other organs before the liver
metastases.-no other malignant tumors.our retrospective dataset includes two
cohorts from two hospitals. the first cohort consists of 201 patients and the
second cohort includes 68 patients. each scan contains three phases and 100 to
200 ct slices with a resolution of 512×512. patients may have different numbers
of ct scans, ranging from 2 to 6, depending on the number of follow-up visits.
ct images are collected with the following acquisition parameters: window width
150, window level 50, radiation dose 120 kv, slice thickness 1 mm, and slice gap
0.8 mm. all images underwent manual quality control to exclude any scans with
noticeable artifacts or blurriness and to verify the completeness of all slices.
additional statistics on our dataset are presented in table 1 and examples of
representative images are shown in fig. 1. the dataset is available upon
request.",6
1597,MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,3.1,Data Augmentation and Selection,"we selected 170 patients who underwent three or more cect scans from our
original dataset, and cropped the images to only include the liver area, as
shown in fig. 1. among these cases, we identified 49 positive cases and 121
negative cases. to handle the imbalanced training dataset, we selected and
duplicated 60% of positive cases and 20% of negative cases by applying standard
scale jittering (ssj) [5]. for data augmentation, we randomly rotated the images
from -30 • to 30 • and employed mixup [17]. we applied the same augmentation
technique consistently to all phases and timestamps of each patient's data. we
also used spline interpolated zoom (siz) [18] to uniformly select 64 slices. for
each slice, the dimension was 256 × 256 after cropping. we used the a and v
phases of cect for our crlm prediction task since the p phase is only relevant
when tumors are significantly present, which is not the case in our dataset. the
dimension of our final input is (3 × 2 × 64 × 64 × 64), representing (t × p × d
× h × w ), where t is the number of timestamps, p is the number of different
phases, d is the slice depth, h is the height, and w is the width.",6
1599,MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,4.0,Results and Discussion,"error analysis. in fig. 1, patients b and c are diagnosed with positive crlm
later. mpbd-lstm correctly yields a positive prediction for patient b with a
confidence of 0.82, but incorrectly yields a negative prediction for patient c
with a confidence of 0.77. with similar confidence in the two cases, the error
is likely due to the relatively smaller liver size of patient c. beyond this
case, we find that small liver size is also present in most of the false
negative cases. a possible explanation would be that smaller liver may provide
less information for accurate prediction of crlm. how to effectively address
inter-patient variability in the dataset, perhaps by better fusing the 5d
features, requires further research from the community in the future.",6
1633,An Anti-biased TBSRTC-Category Aware Nuclei Segmentation Framework with a Multi-label Thyroid Cytology Benchmark,3.0,Experiments,"image dataset. we construct a clinical thyroid cytopathology dataset with images
of both image-wise and pixel-wise labels as a benchmark (appear in github upon
acceptance) some representative images are presented in fig. 2, together with
the profile of the dataset. the dataset comprises 4,965 h&e stained image
patches and labels of tbsrtc, where a subset of 1,473 images was densely
annotated for nuclei boundaries by three experienced cytopathologists and
reached a total number of 31,064 elaborately annotated nuclei. patient-level
images were partitioned first for training and test images, and patch-level
curation was performed. we divided the dataset with image-wise labels into 80%
training samples and the remaining 20% testing samples. our collection of
thyroid cytopathology images was granted with an ethics approval document. table
1. quantitative comparisons in both fully-supervised and semi-supervised
manners. the best performance is highlighted in bold, where we can observe that
both tcsegnet and its semi-supervised extension outperform state-of-the-art.",6
1658,Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,1.0,Introduction,"cancers are a group of heterogeneous diseases reflecting deep interactions
between pathological and genomics variants in tumor tissue environments [24].
different cancer genotypes are translated into pathological phenotypes that
could be assessed by pathologists [24]. high-resolution pathological images have
proven their unique benefits for improving prognostic biomarkers prediction via
exploring the tissue microenvironmental features [1,10,12,13,18,25]. meanwhile,
genomics data (e.g., mrna-sequence) display a high relevance to regulate cancer
progression [3,29]. for instance, genome-wide molecular portraits are crucial
for cancer prognostic stratification and targeted therapy [16]. despite their
importance, seldom efforts jointly exploit the multimodal value between cancer
image morphology and molecular biomarkers. in a broader context, assessing
cancer prognosis is essentially a multimodal task in association with
pathological and genomics findings. therefore, synergizing multimodal data could
deepen a crossscale understanding towards improved patient prognostication.the
major goal of multimodal data learning is to extract complementary contextual
information across modalities [4]. supervised studies [5][6][7] have allowed
multimodal data fusion among image and non-image biomarkers. for instance, the
kronecker product is able to capture the interactions between wsis and genomic
features for survival outcome prediction [5,7]. alternatively, the coattention
transformer [6] could capture the genotype-phenotype interactions for prognostic
prediction. yet these supervised approaches are limited by feature
generalizability and have a high dependency on data labeling. to alleviate label
requirement, unsupervised learning evaluates the intrinsic similarity among
multimodal representations for data fusion. for example, integrating image,
genomics, and clinical information can be achieved via a predefined unsupervised
similarity evaluation [4]. to broaden the data utility, the study [28] leverages
the pathology and genomic knowledge from the teacher model to guide the
pathology-only student model for glioma grading. from these analyses, it is
increasingly recognized that the lack of flexibility on model finetuning limits
the data utility of multimodal learning. meanwhile, the size of multimodal
medical datasets is not as large as natural vision-language datasets, which
necessitates the need for data-efficient analytics to address the training
difficulty.to tackle above challenges, we propose a pathology-and-genomics
multimodal framework (i.e., pathomics) for survival prediction (fig. 1). we
summarized our contributions as follows. (1) unsupervised multimodal data
fusion. our unsupervised pretraining exploits the intrinsic interaction between
morphological and molecular biomarkers (fig. 1a). to overcome the gap of
modality heterogeneity between images and genomics, we project the multimodal
embeddings into the same latent space by evaluating the similarity among them.
particularly, the pretrained model offers a unique means by using
similarity-guided modality fusion for extracting cross-modal patterns. (2)
flexible modality finetuning. a key contribution of our multimodal framework is
that it combines benefits from both unsupervised pretraining and supervised
finetuning data fusion (fig. 1b). as a result, the task-specific finetuning
broadens the dataset usage (fig 1b andc), which is not limited by data modality
(e.g., both singleand multi-modal data). (3) data efficiency with limited data
size. our approach could achieve comparable performance even with fewer
finetuned data (e.g., only use 50% of the finetuned data) when compared with
using the entire finetuning dataset.",6
1659,Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,2.0,Methodology,"overview. figure 1 illustrates our multimodal transformer framework. our method
includes an unsupervised multimodal data fusion pretraining and a supervised
flexible-modal finetuning. from fig. 1a, in the pretraining, our unsupervised
data fusion aims to capture the interaction pattern of image and genomics
features. overall, we formulate the objective of multimodal feature learning by
converting image patches and tabular genomics data into groupwise embeddings,
and then extracting multimodal patient-wise embeddings. more specifically, we
construct group-wise representations for both image and genomics modalities. for
image feature representation, we randomly divide image patches into groups;
meanwhile, for each type of genomics data, we construct groups of genes
depending on their clinical relevance [22]. next, as seen in fig. 1b andc, our
approach enables three types of finetuning modal modes (i.e., multimodal,
image-only, and genomics-only) towards prognostic prediction, expanding the
downstream data utility from the pretrained model. group-wise image and genomics
embedding. we define the group-wise genomics representation by referring to n =
8 major functional groups obtained from [22]. each group contains a list of
well-defined molecular features related to cancer biology, including
transcription factors, tumor suppression, cytokines and growth factors, cell
differentiation markers, homeodomain proteins, translocated cancer genes, and
protein kinases. the group-wise genomics representation is defined as g n ∈ r
1×dg , where n ∈ n , d g is the attribute dimension in each group which could be
various. to better extract high-dimensional group-wise genomics representation,
we use a self-normalizing network (snn) together with scaled exponential linear
units (selu) and alpha dropout for feature extraction to generate the group-wise
embedding g n ∈ r 1×256 for each group.for group-wise wsis representation, we
first cropped all tissue-region image tiles from the entire wsi and extracted
cnn-based (e.g., resnet50) d idimensional features for each image tile k as h k
∈ r 1×di , where d i = 1, 024, k ∈ k and k is the number of image patches. we
construct the group-wise wsis representation by randomly splitting image tile
features into n groups (i.e., the same number as genomics categories).
therefore, group-wise image representation could be defined as i n ∈ r kn×1024 ,
where n ∈ n and k n represents tile k in group n. then we apply an
attention-based refiner (abr) [17], which is able to weight the feature
embeddings in the group, together with a dimension deduction (e.g.,
fully-connected layers) to achieve the group-wise embedding. the abr and the
group-wise embedding i n ∈ r 1×256 are defined as:where w,v1 and v2 are the
learnable parameters.patient-wise multimodal feature embedding. to aggregate
patient-wise multimodal feature embedding from the group-wise representations,
as shown in fig. 1a, we propose a pathology-and-genomics multimodal model
containing two model streams, including a pathological image and a genomics data
stream.in each stream, we use the same architecture with different weights,
which is updated separately in each modality stream. in the pathological image
stream, the patient-wise image representation is aggregated by n group
representations as, where p ∈ p and p is the number of patients. similarly, the
patient-wise genomics representation is aggregated as g p ∈ r n ×256 . after
generating patient-wise representation, we utilize two transformer layers [27]
to extract feature embeddings for each modality as follows:where msa denotes
multi-head self-attention [27] (see appendix 1), l denotes the layer index of
the transformer, and h p could either be i p or g p . then, we construct global
attention poolings [17] as eq. 1 to adaptively compute a weighted sum of each
modality feature embeddings to finally construct patientwise embedding as i p
embedding ∈ r 1×256 and g p embedding ∈ r 1×256 in each modality.multimodal
fusion in pretraining and finetuning. due to the domain gap between image and
molecular feature heterogeneity, a proper design of multimodal fusion is crucial
to advance integrative analysis. in the pretraining stage, we develop an
unsupervised data fusion strategy by decreasing the mean square error (mse) loss
to map images and genomics embeddings into the same space. ideally, the image
and genomics embeddings belonging to the same patient should have a higher
relevance between each other. mse measures the average squared difference
between multimodal embeddings. in this way, the pretrained model is trained to
map the paired image and genomics embeddings to be closer in the latent space,
leading to strengthen the interaction between different modalities.in the single
modality finetuning, even if we use image-only data, the model is able to
produce genomic-related image feature embedding due to the multimodal knowledge
aggregation already obtained from the model pretraining. as a result, our
cross-modal information aggregation relaxes the modality requirement in the
finetuning stage. as shown in fig. 1b, for multimodal finetuning, we deploy a
concatenation layer to obtain the fused multimodal feature representation and
implement a risk classifier (fc layer) to achieve the final survival
stratification (see appendix 2). as for single-modality finetuning mode in fig.
1c, we simply feed i p embedding or g p embedding into risk classifier for the
final prognosis prediction. during the finetuning, we update the model
parameters using a log-likelihood loss for the discrete-time survival model
training [6](see appendix 2).",6
1660,Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,3.0,Experiments and Results,"datasets. all image and genomics data are publicly available. we collected wsis
from the cancer genome atlas colon adenocarcinoma (tcga-coad) dataset
(cc-by-3.0) [8,21] and rectum adenocarcinoma (tcga-read) dataset (cc-by-3.0)
[8,20], which contain 440 and 153 patients. we cropped each wsi into 512 × 512
non-overlapped patches. we also collected the corresponding tabular genomics
data (e.g., mrna sequence, copy number alteration, and methylation) with overall
survival (os) times and censorship statuses from cbioportal [2,14]. we removed
the samples without the corresponding genomics data or ground truth of survival
outcomes. finally, we included 426 patients of tcga-coad and 145 patients of
tcga-read.experimental settings and implementations. we implement two types of
settings that involve internal and external datasets for model pretraining and
finetuning. as shown in fig 2a, we pretrain and finetune the model on the same
dataset (i.e., internal setting). we split tcga-coad into training (80%) and
holdout testing set (20%). then, we implement four-fold cross-validation on the
training set for pretraining, finetuning, and hyperparameter-tuning. the test
set is only used for evaluating the best finetuned models from each
cross-validation split. for the external setting, we implement pretraining and
finetuning on the different datasets, as shown in fig 2b ; we use tcga-coad for
pretraining; then, we only use tcga-read for finetuning and final evaluation. we
implement a five-fold cross-validation for pretraining, and the best pretrained
models are used for finetuning. we split tcga-read into finetuning (60%),
validation (20%), and evaluation set (20%). for all experiments, we calculate
the average performance on the evaluation set across the best models.the number
of epochs for pretraining and finetuning are 25, the batch size is 1, the
optimizer is adam [19], and the learning rate is 1e-4 for pretraining and 5e-5
for finetuning. we used one 32gb tesla v100 sxm2 gpu and pytorch. the
concordance index (c-index) is used to measure the survival prediction
performance. we followed the previous studies [5][6][7] to partition the overall
survival (os) months into four non-overlapping intervals by using the quartiles
of event times of uncensored patients for discretized-survival c-index
calculation (see appendix 2). for each experiment, we reported the average
c-index among three-times repeated experiments. conceptionally, our method
shares a similar idea to multiple instance learning (mil) [9,23]. therefore, we
include two types of baseline models, including the mil-based models (deepset
[30], ab-mil [17], and transmil [26]) and mil multimodal-based models (mcat [6],
porpoise [7]). we follow the same data split and processing, as well as the
identical training hyperparameters and supervised fusion as above. notably,
there is no need for supervised finetuning for the baselines when using
tcga-coad (table 1), because the supervised pretraining is already applied to
the training set.",6
1661,Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,0,Results.,"in table 1, our approach shows improved survival prediction performance on both
tcga-coad and tcga-read datasets. compared with supervised baselines, our
unsupervised data fusion is able to extract the phenotype-genotype interaction
features, leading to achieving a flexible finetuning for different data
settings. with the multimodal pretraining and finetuning, our method outperforms
state-of-the-art models by about 2% on tcga-coad and 4% tcga-read. we recognize
that the combination of image and mrna sequencing data leads to reflecting
distinguishing survival outcomes. remarkably, our model achieved positive
results even using a single-modal finetuning when compared with baselines (more
results in appendix 3.1). in the meantime, on the tcga-read, our single-modality
finetuned model achieves a better performance than multimodal finetuned baseline
models (e.g., with model pretraining via image and methylation data, we have
only used the image data for finetuning and achieved a c-index of 74.85%, which
is about 4% higher than the best baseline models). we show that with a
single-modal finetuning strategy, the model could generate meaningful embedding
to combine image-and genomicrelated patterns. in addition, our model reflects
its efficiency on the limited finetuning data (e.g., 75 patients are used for
finetuning on tcga-read, which are only 22% of tcga-coad finetuning data). in
table 1, our method could yield better performance compared with baselines on
the small dataset across the combination of images and multiple types of
genomics data. approach broadens the scope of dataset inclusion, particularly
for model finetuning and evaluation, while enhancing model efficiency on
analyzing multimodal clinical data in real-world settings. in addition, the use
of synthetic data and developing a foundation model training will be helpful to
improve the robustness of multimodal data fusion [11,15].",6
1662,Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,0,Supplementary Information,"the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43987-2 60. ablation analysis. we verify the
model efficiency by using fewer amounts of finetuning data in finetuning. for
tcga-coad dataset, we include 50%, 25%, and 10% of the finetuning data. for the
tcga-read dataset, as the number of uncensored patients is limited, we use 75%,
50%, and 25% of the finetuning data to allow at least one uncensored patient to
be included for finetuning. as shown in fig. 3a, by using 50% of tcga-coad
finetuning data, our approach achieves the c-index of 64.80%, which is higher
than the average performance of baselines in several modalities. similarly, in
fig. 3b, our model retains a good performance by using 50% or 75% of tcga-read
finetuning data compared with the average of c-index across baselines (e.g.,
72.32% versus 64.23%). for evaluating the effect of cross-modality information
extraction in the pretraining, we kept supervised model training (i.e., the
finetuning stage) while removing the unsupervised pretraining. the performance
is lower 2%-10% than ours on multi-and single-modality data. for evaluating the
genomics data usage, we designed two settings: (1) combining all types of
genomics data and categorizing them by groups; (2) removing category information
while keeping using different types of genomics data separately. our approach
outperforms the above ablation studies by 3%-7% on tcga-read and performs
similarly on tcga-coad. in addition, we replaced our unsupervised loss with
cosine similarity loss; our approach outperforms the setting of using cosine
similarity loss by 3%-6%.",6
1663,Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,4.0,Conclusion,"developing data-efficient multimodal learning is crucial to advance the survival
assessment of cancer patients in a variety of clinical data scenarios. we
demonstrated that the proposed pathomics framework is useful for improving the
survival prediction of colon and rectum cancer patients. importantly, our
approach opens up perspectives for exploring the key insights of intrinsic
genotypephenotype interactions in complex cancer data across modalities. our
finetuning",6
1664,Robust Cervical Abnormal Cell Detection via Distillation from Local-Scale Consistency Refinement,1.0,Introduction,"cervical cancer is the second most common cancer among adult women. if diagnosed
early, it can be effectively treated and cured [19]. nevertheless, delayed
diagnosis of cervical cancer until an advanced stage will have a negative impact
on patient prognosis and consume medical resources. currently, early screening
of cervical cancer is recommended worldwide as an effective method to prevent
and treat cervical cancer. thin-prep cytologic test (tct) is the most common and
effective screening method for detecting cervical abnormal and premalignant
cervical lesions [5]. conventionally it is performed by visually examining the
stained cells collected through smearing on a glass slide, and generating a
diagnosis report using the descriptive diagnosis method of the bethesda system
(tbs) [15]. although tct has been widely used in clinical applications and has
significantly reduced the mortality rates caused by cervical cancer, it is still
unavailable for population-wide screening [18]. this is partly due to its
laborintensive, time-consuming, and high cost [1]. therefore, there is a high
demand for automated cervical abnormality screening to facilitate efficient and
accurate identification of cervical abnormalities.with the development of deep
learning [10], several attempts have been made to identify cervical abnormal
cells using convolutional neural networks (cnns). for example, cao et al. [2]
developed an attention feature pyramid network (attfpn) for automatic abnormal
cervical cell detection in cervical cytopathological images to assist
pathologists in making more accurate diagnoses. chen et al. [3] proposed a new
framework that decomposes tasks and compares cells for cervical lesion cell
detection. liang et al. [11] proposed to explore contextual relationships to
boost the performance of cervical abnormal cell detection. lin et al. [22]
presented an automatic cervical cell detection approach based on the
dense-cascade r-cnn. it is worth mentioning that all of the aforementioned
detection methods inevitably produce false positive results, which should be
further refined by pathologists for manual checking or classification models
established for automatic screening. to solve this problem, zhou et al. [23]
proposed a three-stage method including cell-level detection, image-level
classification, and case-level diagnosis obtained by an svm classifier. zhu et
al. [24] developed an artificial intelligence assistive diagnostic solution,
which integrated yolov3 [16] for detection, xception, and patch-based models to
boost classification.although the above-mentioned attempts can improve the
screening performance significantly, there are several issues that need to be
addressed: 1) object detection methods often require accurate annotated data to
guarantee performance with robustness and generalization. however, due to legal
limitations, the scarcity of positive samples, and especially the subjectivity
differences between cytopathologists for manual annotations [20], it is likely
to generate noisy samples that affect the performance of the detection model. 2)
conventional object detection methods intend to directly extract the feature
from the object area to locate and classify the object simultaneously. however,
in clinical practice pathologists usually examine the target cells by comparing
them to the surrounding cells to determine whether they are abnormal. therefore,
the visual feature correlations between the target cells and their surroundings
can provide valuable information to aid the screening process, which also needs
to be utilized when designing the cervical abnormal cell detection network.to
address these issues, we propose a novel method for cervical abnormal cell
detection using distillation from local-scale consistency refinement. inspired
by knowledge distillation, we construct a pre-trained patch correction network
(pcn), which is designed to exploit the supervised information from the pcn to
reduce the impact of noisy labels and utilize the contextual relationships
between cells. in our approach, we begin by utilizing retinanet [12] to locate
suspicious cells and crop the top-k suspicious cells into patches. then we feed
them into the pcn to obtain classification scores and propose a ranking loss to
refine the classifier of the detection network by correcting the score of the
detection model. in addition, we propose an roi-correlation consistency (rcc)
loss between roi features and local-scale features from the pcn, which
encourages the detector to explore the feature correlations of the suspicious
cells. our proposed method achieves improved performance during inference
without changing the detector structure.",6
1684,MixUp-MIL: Novel Data Augmentation for Multiple Instance Learning and a Study on Thyroid Cancer Diagnosis,2.2,Experimental Setting,"as experimental architecture, use the dual-stream mil approach proposed by li et
al [10]. since this model combines both, embedding-based and an instance-based
encoding, the effect of both paths can be individually investigated without
changing any other architectural details. since the method represents a
state-of-the-art approach, it further serves as well-performing baseline. in
instance-based mil, the information per patch is first condensed to a single
scalar value, representing the classification per patch. finally, all of these
patch-based values are aggregated. in embedding-based mil, the information per
patch is translated into a feature vector. all feature vectors from a wsi are
then aggregated followed by a classification. in the investigated model [10] an
instance-and an embedding-based pathway are employed in parallel and are merged
in the end by weighted addition. the embedding-based pathway contains an
attention mechanism, to higher weight patches that are similar to the so-called
critical instance. the model makes use of an individual feature extraction
stage. due to the limited number of wsis, we did not train the feature
extraction stage [7], but utilize a pre-trained network instead. specifically,
we applied a resnet18 pre-trained on the image-net challenge data, due to the
high performance in previous work on similar data [5]. resnet18 was assessed as
particularly appropriate due to the rather low dimensional output (512
dimensions). we actively decided not to use a self-supervised contrastive
learning approach [10] as feature extraction stage since invariant features
could interfere with the effect of data augmentation. we investigated various
settings consisting of instancebased only (inst), embedding-based only (emb) and
the dual-stream approach with weightings 3/1, 2/2 (balanced) and 1/3 for the
instance and the embedding-based pathways.as comparison, several other
augmentation methods on feature level are investigated including random
sampling, selective random sampling and random noise. random sampling
corresponds to the random selection of patches (feature vectors) from each wsi.
thereby the amount of investigated data per wsi is reduced with the benefit of
increasing the variability of the data. in the experiments, we adjust the sample
ratio q between the patch-based features for training and testing. a q of 50 %
indicates that 512 descriptors are used for training while for testing always a
fixed number of 1024 is used. selective random sampling corresponds to the
random sampling strategy, with the difference that the ratio of features is not
fixed but drawn from a uniform random distribution (u (q, 100 %)). here, a q of
50 % indicates that for each wsi, between 512 and 1024 feature vectors are
selected. in the case of the random noise setting, to each feature vector x i ,
a random noise vector r is added (x i = x i + r). the elements of r are randomly
sampled (individually for each x i ) from a normal distribution n (0, σ ).to
incorporate for the fact that the feature dimensions show different magnitudes,
σ is computed as the product of the meta parameter σ and the standard deviation
of the respective feature dimension.in this work, we aimed at distinguishing
different nodular lesions of the thyroid, focusing especially on benign
follicular nodules (fn) and papillary carcinomas (pc). this differentiation is
crucial, due to the different treatment options, in particular with respect to
the extent of surgical resection of the thyroid gland [19]. the data set
utilized in the experiments consists of 80 wsis overall. one half (40) of the
data set consists of frozen and the other half (40) of paraffin sections [5]),
representing the different modalities. all images were acquired during clinical
routine at the kardinal schwarzenberg hospital. procedures were approved by the
ethics committee of the county of salzburg (no. 1088/2021). the mean and median
age of patients at the date of dissection was 47 and 50 years, respectively. the
data set comprised 13 male and 27 female patients, corresponding to a slight
gender imbalance. they were labeled by an expert pathologist with over 20 years
experience. a total of 42 (21 per modality) slides were labeled as papillary
carcinoma while 38 (19 per modality) were labeled as benign follicular nodule.
for the frozen sections, fresh tissue was frozen at -15 • celsius, slides were
cut (thickness 5 µm) and stained immediately with hematoxylin and eosin. for the
paraffin sections, tissue was fixed in 4 % phosphate-buffered formalin for 24 h.
subsequently formalin fixed paraffin embedded tissue was cut (thickness 2 µm)
and stained with hematoxylin and eosin. the images were digitized with an
olympus vs120-ld100 slide loader system. overviews at a 2x magnification were
generated to manually define scan areas, focus points were automatically defined
and adapted if needed. scans were performed with a 20x objective (corresponding
to a resolution of 344.57 nm/pixel). the image files were stored in the oympus
vsi format based on lossless compression. q q q q q q q q 0 25 % 50 % 75 % 100 %
0.4 the data set was randomly separated into training (80 %) and test data (20
%). the whole pipeline, including the separation, was repeated 32 times to
achieve representative scores. due to the almost balanced setting, the overall
classification accuracy (mean and standard deviation) is finally reported. adam
was used as optimizer. the models were trained for 200 epochs with an initial
learning rate of 0.0002. random shuffling of the vector tupels (shuffling within
the wsis) was applied for all experiments.the patches were randomly extracted
from the wsi, based on uniform sampling. for each patch, we checked that at
least 75 % of the area was covered with tissue (green color channel) in order to
exclude empty areas [5]. to obtain a representation independent of the wsi size,
we extracted 1024 patches with a size of 256 × 256 pixel per wsi, resulting in
1024 patch-descriptors per wsi [5]. for feature extraction, a resnet18 network,
pretrained on the image-net challenge was deployed [10]. data and source code
are publicly accessible via https://gitlab.com/mgadermayr/mixupmil. we use the
reference implementation of the dual-stream mil approach [10]. to obtain further
insight into the feature distribution, we randomly selected patch descriptor
pairs and computed the euclidean distances. in detail, we selected 10,000 pairs
(a) from different classes, (b) from different wsis (similar and dissimilar
classes), (c,d) from the same class and different wsis, and (e) from the same
wsi.",6
1687,Transfer Learning-Assisted Survival Analysis of Breast Cancer Relying on the Spatial Interaction Between Tumor-Infiltrating Lymphocytes and Tumors,1.0,Introduction,"breast cancer (bc) is the most common cancer diagnosed among females and the
second leading cause of cancer death among women after lung cancer [1]. the bc
differs greatly in clinical behavior, ranging from carcinoma in site to
aggressive metastatic disease [2,3]. thus, effective and accurate prognosis of
bc as well as stratifying cancer patients into different subgroups for
personalized cancer management has attracted more attention than ever
before.among different types of imaging biomarkers, histopathological images are
generally considered the golden standard for bc prognosis since they can confer
important cell-level information that can reflect the aggressiveness of bc [4].
recently, with the availability of digitalized whole-slide pathological images
(wsis), many computational models have been employed for the prognosis
prediction of various subtypes of bc. for instance, lu et al [5] presented a
novel approach for predicting the prognosis of er-positive bc patients by
quantifying nuclear shape and orientation from histopathological images. liu et
al [6] developed a gradient boosting algorithm to predict the disease
progression for various subtypes of bc. however, due to the high-cost of
collecting survival information from the patients, it is still a challenge to
build effective machine learning models for specific bc subtypes with limited
annotation data.to deal with the above challenges, several researchers began to
design domain adaption algorithms, which utilize the labeled data from a related
cancer subtype to help predict the patients' survival in the target domain.
specifically, alirezazadeh et al [7] presented a new representation
learning-based unsupervised domain adaption method to predict the clinical
outcome of cancer patients on the target domain. zhang et al [8] proposed a
collaborative unsupervised domain adaptation algorithm, which conducts
transferability-aware adaptation and conquers label noise in a collaborative
way. other studies include xu et al [9] developed graph neural networks for
unsupervised domain adaptation in histopathological image analysis, based on a
backbone for embedding input images into a feature space, and a graph neural
layer for propagating the supervision signals of images with labels.although
much progress has been achieved, most of the existing studies applied the
feature alignment strategy to reduce the distribution difference between source
and target domains. however, such transfer learning methods neglected to take
the interaction among different types of tissues into consideration. for
example, it is widely recognized that tumor-infiltrating lymphocytes (tils) and
its correlation with tumors reveal a similar role in the prognosis of different
brca subtypes. for instance, kurozumi et al [10] revealed that high tils
expression was correlated with negative estrogen receptor (er) expression and
high histological grade (p < 0.001). lu et al [11] utilized the tils spatial
pattern for survival analysis in different breast cancer subtypes including
er-negative, er-positive, and triple-negative. it can be expected that better
prognosis performance can be achieved if we leveraged the tils-tumor interaction
information to resolve the survival analysis task on the target domain.based on
the above considerations, in this paper, we proposed a tils-tumor interactions
guided unsupervised domain adaptation (t2uda) algorithm to predict the patients'
survival on the target bc subtype. specifically, t2uda first applied the graph
attention network (gats) to learn node embeddings and the spatial interactions
between tumor and tils patches in wsi. in order to preserve the node-level and
interaction-level similarities across different domains, we not only aligned the
embedding for different types of nodes but also designed a novel tumor-tils
interaction alignment (ttia) module to ensure that the distribution of the
interaction weights are similar in both domains. we evaluated the performance of
our method on the breast invasive carcinoma (brca) cohort derived from the
cancer genome atlas (tcga), and the experimental results indicated that t2uda
outperforms other domain adaption methods for predicting patients' clinical
outcomes.",6
1691,Transfer Learning-Assisted Survival Analysis of Breast Cancer Relying on the Spatial Interaction Between Tumor-Infiltrating Lymphocytes and Tumors,0,Prognosis Prediction by the Cox Proportional Hazard Model.,"the cox proportional hazard model was applied to predict the patients' clinical
outcome [16], and its negative log partial likelihood function can be formulated
as:where x i represents the output of the last layer for the prognosis task and
r (t i ) is the risk set at time t i , which represents the set of patients that
are still under risk before time t. in addition, δ i is an indicator variable.
sample i refers to censored patient ifoverall objective. to achieve
domain-adaptive prognosis prediction, the final loss function included the cox
loss, fa loss, and ttia loss as the following formula:where α and β represent
the weights assigned to the importance of fa component and ttia component
respectively.",6
1692,Transfer Learning-Assisted Survival Analysis of Breast Cancer Relying on the Spatial Interaction Between Tumor-Infiltrating Lymphocytes and Tumors,3.0,Experiments and Results,"datasets. we conducted our experiments on the breast invasive carcinoma (brca)
dataset from the cancer genome atlas (tcga). specifically, the brca dataset
includes 661 patients with hematoxylin and eosin (he)-stained pathological
imaging and corresponding survival information. among the collected brca
patients in tcga, the number of er positive(er+) and er negative(er-) patients
are 515 and 146, respectively. we hope to investigate if the proposed t2uda
could be used to help improve the prognosis performance of (er+) or (er-) with
the aid of the survival information on its counterpart.",6
1694,Transfer Learning-Assisted Survival Analysis of Breast Cancer Relying on the Spatial Interaction Between Tumor-Infiltrating Lymphocytes and Tumors,3.2,Result and Discussion,"in this study, we compared the performance of our proposed model with several
existing domain adaptation methods, including 1) ddc [17]: utilize the maximum
mean discrepancy (mmd) to calculate the domain difference loss between source
and target data and optimize both classification loss and disparity loss. 2)
dann [18]: an adversarial learning method that used gradient backpropagation to
extract domain-independent features. 3) mdd [19]: an adversarial training method
that combined metric learning and domain adaptation. 4) deepjdot [20]: an
unsupervised domain adaptation method based on optimal transport that
simultaneously learns features and optimizes classifiers by measuring joint
feature/label differences. 5) source only: it was trained on the source domain
and applied directly to the target domain. 6) t2uda-v1: it was a variant of
t2uda which didn't use ttia. the experimental results were presented in table
1.the results presented in table 1 revealed several key observations. first, our
proposed method outperformed feature alignment-based methods such as ddc and
deepjdot in terms of both ci and auc values. the reason lies in that these
methods only transferred the knowledge at the feature level and neglected the
inter-relationship between tils and tumors. second, our method outperformed
adversarial-based methods such as dann and mdd, as the high heterogeneity
between the target and source domains results in negative transfer through
adversarial training. instead of directly aligning regions, our proposed method
focused on similar tils-tumor interactions and aligning patches of the same
tissue.we also evaluated the contributions of the key components of our
framework and found that t2uda performed better than source only and t2uda-v1,
which shows the advantage of minimizing differences in tils-tumor interaction
weights.in addition, we also evaluated the patient stratification performance of
different methods. as shown in fig. 3, our proposed t2uda outperformed feature
alignment-based methods (such as ddc and deepjdot), adversarial-based methods
(such as dann and mdd), and t2uda-v1 in stratification performance, proving that
considering the interaction between tils and tumors as migration knowledge leads
to better prognostic results.we also examined the consistency of important edges
in each group of stratified patients based on the tils-tumor interaction weights
calculated by the gat-based framework in the source and target domains. as seen
in fig. 4(a), for both the source and target domains, the proportion of edges
that connect tils and tumor regions in the low-risk group was higher than that
in the highrisk group, showing that the interaction between tils and tumors
played a critical role in prognostic prediction in different bc subtypes.
furthermore, as shown in fig. 4(b), the weights of the edges connecting tumor
and tils regions were higher for patients in the low survival risk group in both
source and target domains. this was consistent with our knowledge that brisk
interaction between tils and tumor regions indicates a better clinical outcome
and demonstrates the transferability of this knowledge.",6
1695,Transfer Learning-Assisted Survival Analysis of Breast Cancer Relying on the Spatial Interaction Between Tumor-Infiltrating Lymphocytes and Tumors,4.0,Conclusion,"in this paper, we presented an unsupervised domain adaptation algorithm that
leverages tils-tumor interactions to predict patients' survival in a target bc
subtype(t2uda). our results demonstrated that the relationship between tils and
tumors is transferable and can be effectively used to improve the accuracy of
survival prediction models. to the best of our knowledge, t2uda was the first
method to successfully achieve interrelationship transfer between tils and
tumors across different cancer subtypes for prognosis tasks.",6
1696,Gene-Induced Multimodal Pre-training for Image-Omic Classification,1.0,Introduction,"pathological image-omic analysis is the cornerstone of modern medicine and
demonstrates promise in a variety of different tasks such as cancer diagnosis
and prognosis [12]. with the recent advance of digital pathology and sequencing
technologies, modern cancer screening has jointly incorporated genomics and
histology analysis of whole slide images (wsis).though deep learning techniques
have revolutionized medical imaging, designing a task-specific algorithm for
image-omic multi-modality analysis is challenging. (1) the gigapixel wsis, which
generally yield 15,000 foreground patches during pre-processing, make
attention-based backbones [6] hard to extract precise image (wsi)-level
representations. (2) learning features from genomics data which have tens of
thousands of genes make models such as transformer [16] impractical to use due
to its quadratic computation complexity. (3) image-omic feature fusion [2,3] may
fail to model high-order relevance and the inherent structural characteristics
of each modality, making the fusion less effective.specifically, to our
knowledge, most multi-modality techniques have been designed for modalities such
as chest x-ray and reports [1,17,23], ct and x-ray [18], ct and mri [21], h&e
cross-staining [22] via global feature, local feature or multi-granularity
alignment. but, none of these works considers the challenges in wsis and genes
processing. besides, vision-language models in the computer vision community
stand out for their remarkable versatility [13,14]. nevertheless, constrained by
computing resources, the most commonly used multimodal representation learning
strategy, contrastive learning, which relies on a large number of negative
samples to avoid model collapse [8], is not affordable for gigapixel wsis
analysis. a big domain gap also hampers their usage in leveraging the structural
characteristic of tumor micro-environment and genomic assay. recently, the
literature corpus has proposed some methods for accomplishing specific
image-omic tasks via kronecker product fusion [2] or co-attention mapping
between wsis and genomics data [3]. but, the kronecker product overly concerns
feature interactions between modalities while ignoring high-order relevance,
w.r.t. decision boundaries across multiple samples, which is critical to
classification tasks. as for the co-attention module, it is unidirectional and
cannot localize significant regions from genetic data with a large amount of
information.in this paper, we propose a task-specific framework dubbed
gene-induced multimodal pre-training (gimp) for image-omic classification.
concretely, we first propose a transformer-based gene encoder, group multi-head
self attention (groupmsa), to capture global structured features in gene
expression cohorts. next, we design a pre-training paradigm for wsis, masked
patch modeling (mpm), masking random patch embeddings from a fixed-length
contiguous subsequence of a wsi. we assume that one patch-level feature
embedding can be reconstructed by its adjacent patches, and this process
enhances the learning ability for pathological characteristics of different
tissues. our mpm only needs to recover the masked patch embeddings in a
fixed-length subsequence rather than processing all patches from wsis.
furthermore, to model the high-order relevance of the two modalities, we combine
cls tokens of paired image and genomic data to form unified representations and
propose a triplet learning module to differentiate patient-level positive and
negative samples in a mini-batch. it is worth mentioning that although our
unified representation fuses features from the whole gene expression cohort and
partial wsis in a mini-batch, we can still learn high-order relevance and
discriminative patient-level information between these two modalities in
pre-training thanks to the triplet learning module. in addition, note that our
proposed method is different from self-supervised pre-training. specifically, we
focus not only on superior representation learning capability, but also
category-related feature distributions, w.r.t. intra-and inter-class variation.
with the training process going on, complete information from wsis can be
integrated and the fused multimodal representations with high discrimination
will make it easier for the classifier to find the classification hyperplane.
experimental results demonstrate that our gimp achieves significant improvement
in accuracy than other image-omic competitors, and our multimodal framework
shows competitive performance even without pre-training.",6
1700,Gene-Induced Multimodal Pre-training for Image-Omic Classification,2.3,Gene-Induced Multimodal Fusion,"in this section, we first describe the formulation of masked patch modeling.
then we introduce the overall pipeline of our pre-training framework and
illustrate how to apply it to downstream classification tasks.masked patch
modeling. in wsis, the foreground patches are spatially contiguous, which means
the adjacent patches have similar feature embeddings. thus, we propose a masked
patch modeling (mpm) pre-training strategy that masks random patch embeddings
from a fixed-length contiguous subsequencej=i in h p and reconstruct the
invisible information. the fixed subsequence length l is empirically set to
6,000 and the sequences shorter than l are duplicated to build mini batches.
besides, the masking ratio is set to 50% and the set of masked subscripts is
denoted as m ∈ r 0.5l . next, a two-layer nystrom-based patch aggregator
followed by a lightweight reconstruction decoder are adopted to process the
masked sequence h mpm and the reconstructed sequence is denoted as. note that we
reconstruct the missing feature embeddings rather than the raw pixels of the
masked areas, which is different from traditional mim methods like simmim [19]
and mae [5]. in this way, the model could consider latent pathological
characteristics of different tissues, which makes the pretext task more
challenging. the reconstruction l 1 loss is computed by:where 1[•] is the
indicator function.gene-induced triplet learning. the transformer-based
backbones in the classification task require the cls token to be able to extract
accurate global information, which is even more important yet difficult in wsis
due to the long sequence challenge. in addition, in order to construct the
mini-batch, the subsequences we intercept in the mpm pre-training phase may not
be sufficiently representative of the image-level characteristics. to overcome
these issues, we further propose a gene-induced triplet learning module, which
uses pathological images and genomic data as input and extracts high-order and
discriminative features via cls tokens. firstly, we pre-train the groupmsa
module by patientlevel annotations in advance and froze it in the following
iterations. next, a learnable cls token cls img for wsis is added to the input
masked sequence h mpm . after extracting the input patch embeddings and gene
sequence separately, we concatenate cls img and cls ge as cls pat ∈ r 2d to
represent patient-level characteristics.suppose we obtain a triplet list {x, x +
, x -} during current iteration, where x, x + , x -are concatenated tokens of
anchor cls pat , positive cls pat , and negative cls pat , respectively. to
enhance the global modeling capability, i.e., extracting more precise
patient-level features, we expect that the distance between the anchor and the
positive sample gets closer, while the negative sample is farther away. the loss
function for optimizing triplet learning is computed by:δ indicates a threshold,
e.g., δ = 0.8. finally, the loss function for gimp pretraining is:multimodal
fine-tuning. applying the pre-trained backbone to image-omic classification task
is straightforward, since gimp pre-training allows it to learn representative
patient-level features. we use a simple multi-layer perceptron (mlp) head to map
cls pat to the final class predictions p , which can be written as p =
softmax(mlp(cls pat )).3 experiments",6
1701,Gene-Induced Multimodal Pre-training for Image-Omic Classification,3.1,Experimental Setup,"datasets. we verify the effectiveness of our method on the caner genome atlas
(tcga) non-small cell lung cancer (nsclc) dataset, which contains two cancer
subtypes, i.e., lung squamous cell carcinoma (lusc) and lung adenocarcinoma
(luad). after pre-processing [11], the patch number extracted from wsis at 20×
magnification varies from 485 to 148,569. we collect corresponding rna-seq fpkm
data for each patient and the length of the input genomic sequence is 60,480.
among 946 image-omic pairs, 470 of them belong to luad and 476 cases are lusc.
we randomly split the data into 567 for training, 189 for validation and 190 for
testing.implementation details. the pre-training process of all algorithms is
conducted on the training set, without any extra data augmentation. note that
our genetic encoder, groupmsa, is fully supervised pre-trained on unimodal
genetic data to accelerate convergence and it is frozen during gimp training
process. the maximum pre-training epoch for all methods is set to 100 and we
finetune the models at the last epoch. during fine-tuning, we evaluate the model
on the validation set after every epoch, and save the parameters when it
performs the best. adamw [10] is used as our optimizer and the learning rate is
10 -4 with cosine decline strategy. the maximum number of fine-tune epoch is 70.
at last, we measure the performance on the test set. training configurations are
consistent throughout the fine-tuning process to ensure fair comparisons. all
experiments are conducted on a single nvidia geforce rtx 3090.",6
1702,Gene-Induced Multimodal Pre-training for Image-Omic Classification,3.2,Comparison Between GiMP and Other Methods,"we conduct comparisons between gimp and three competitors under different
settings. firstly, we compare our proposed patch aggregator with the current
state-of-the-art deep mil models on unimodal tcga-nsclc dataset, i.e., only
pathological wsis are included as input. as shown in table 1, our proposed patch
aggregator outperforms all the compared attention based multiple instance
learning baselines in classification accuracy. in particular, 1.6% higher than
the abmil [6] 0.7737 dsmil [9] 0.7566 clam-sb [11] 0.8519 clam-mb [11] 0.8889
transmil [15] 0.8836 pathology w/o pre-train gimp (w/o groupmsa) 0.8995 porpoise
[4] 0.9524 pathomic fusion [2] 0.9684 mcat [3] 0.9632 w/o pre-train gimp (ours)
0.9737 mgca [17] 0.9105 biovil [1] 0.9316 refers [23] 0 second best compared
method transmil [15]. we then explore the superiority of gimp by comparing to
state-of-the-art medical multi-modal approaches. we particularly compare our
method to biovil [1], mgca [17] and refers [23], three popular multimodal
pre-training algorithms in medical text-image classification task. we can
observe in the table that, our gimp raises acc from 91.05% to 99.47% on
tcga-nsclc dataset. even without pre-training stage, gimp shows competitive
performance compared to porpoise [4], pathomic fusion [2], and mcat [3], three
influential image-omic classification architectures. we further explore why gimp
works by insightful interpretation of the proposed method with t-sne
visualisation. figure 2 shows the feature mixtureness of pre-trained cls pat
extracting global information on training set. compari- son between fig. 2 (a)
and (b) indicates that the addition of the genomic data is indispensable in
increasing the inter-class distance and reducing the intra-class distance, which
confirms our motivation that gene-induced multimodal fusion could model
high-order relevance and yield more discriminative representations. moreover,
compared to the mentioned self-supervised methods biovil [1] and mgca [17] in
fig. 2 (c) and (d), cls pat with gimp pre-trained are well separated between
luad and lusc, i.e., gimp pays more attention to the categoryrelated feature
distribution and could extract more discriminative patient-level features during
triplet learning.",6
1703,Gene-Induced Multimodal Pre-training for Image-Omic Classification,3.3,Ablation Study,"table 2 summarizes the results of ablation study. we first evaluate the
effectiveness of the proposed groupmsa. in the first two rows, groupmsa achieves
0.53% improvement compared to snn [7], a popular genetic encoders used in
porpoise [4] and pathomic fusion [2]. we then analyze the effect of adding
genetic modality during pre-training. the evaluation protocol is first
pre-training, and then fine-tuning on downstream multimodal classification task.
""aggregator + mpm"" means gimp only uses wsis as input and reconstructs the
missing patch embeddings during the pre-training phase. since the fixed
subsequence length l = 6000 is used in our setting, it is sometimes smaller than
the original patch number, e.g., the maximum size 148,569, the pre-trained model
without genetic guidance may be not aware of sufficiently accurate patientlevel
characteristics, i.e., ineffectively focused on normal tissues. ""aggregator +
triplet"" indicates using unimodal image features to build triplets. we can
likewise find that the lack of precise global representation leads to worse
performance. finally, we evaluate the necessity of the mpm module. ""aggregator +
groupmsa + triplet"" means gimp only combines the cls tokens of each modality and
calculates triplet loss during pre-training. we can observe a performance drop
without mpm module, e.g., from 99.47% to 95.26%, which demonstrates that local
pathological information is equally critical as high-order relevance.",6
1710,Histopathology Image Classification Using Deep Manifold Contrastive Learning,3.1,Datasets,"we tested our proposed method on two different tasks: (1) intrahepatic
cholangiocarcinomas(ihccs) subtype classification and (2) liver cancer type
classification. the dataset for the former task was collected from 168 patients
with 332 wsis from seoul national university hospital. ihccs can be further
categorized into small duct type (sdt) and large duct type (ldt). using gene
mutation information as prior knowledge, we collected wsis with wild kras and
mutated idh genes for use as training samples in sdt, and wsis with mutated kras
and wild idh genes for use in ldt. the rest of the wsis were used as testing
samples. the liver cancer dataset for the latter task was composed of 323 wsis,
in which the wsis can be further classified into hepatocellular carcinomas
(hccs) (collected from pathology ai platform [1]) and ihccs. we collected 121
wsis for the training set, and the remaining wsis were used as the testing set.",6
1716,Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,1.0,Introduction,"prostate cancer (pca) diagnosis and grading rely on histopathology analysis of
biopsy slides [1]. however, prostate biopsies are known to have sampling error
as pca is heterogenous and commonly multifocal, meaning cancer legions can be
missed during the biopsy procedure [2]. if significant pca is detected on
biopsies and the patient has organ-confined cancer with no contraindications,
radical prostatectomy (rp) is the standard of care [3,4]. following rp, the
prostate is processed and slices are mounted onto slides for analysis. radical
prostatectomy histopathology samples are essential for validating the
biopsydetermined grade group [5,6]. analysis of whole-mount slides, meaning
slides that include slices of the entire prostate, provide more precise tumor
boundary detection, identification of various tumor foci, and increased tissue
for identifying morphological patterns not visible on biopsy due to a larger
field of view.field effect refers to the spread of genetic and epigenetic
alterations from a primary tumor site to surrounding normal tissues, leading to
the formation of secondary tumors. understanding field effect is essential for
cancer research as it provides insights into the mechanisms underlying tumor
development and progression. tumor-associated stroma, which consists of various
cell types, such as fibroblasts, smooth muscle cells, and nerve cells, is an
integral component of the tumor microenvironment that plays a critical role in
tumor development and progression. reactive stroma, a distinct phenotype of
stromal cells, arises in response to signaling pathways from cancerous cells and
is characterized by altered stromal cells and increased extracellular matrix
components [7,8]. reactive stroma is often associated with tumor-associated
stroma and is thought to be a result of field effects in prostate cancer.
altered stroma can create a pro-tumorigenic environment by producing a multitude
of chemokines, growth factors, and releasing reactive oxygen species [9,10],
which can lead to tumor development and aggressiveness [11]. therefore,
investigating the histological characterization of tumor-associated stroma is
crucial in gaining insights into the field effect and tumor progression of
prostate cancer.manual review for tumor-associated stroma is time-consuming and
lacks quantitative metrics [12,13]. several automated methods have been applied
to analyze the tumor-stroma relationship; however, most of them focus on
identifying a tumor-stroma ratio rather than finding reactive stroma tissue or
require pathologist input. machine learning algorithms have been used to
quantify the percentage of tumor to stroma in bladder cancer patients, but
required dichotomizing patients based on a threshold [14]. software has been
used to segment tumor and stroma tissue in breast cancer patient samples, but
the method required constant supervision by a pathologist [15]. similarly, akoya
biosciences inform software was used to quantify reactive stroma in pca, but
this method required substantial pathologist input to train the software [16].
fully automated deep-learning methods have been developed to identify
tumor-associated stroma in breast cancer biopsies, achieving an auc of 0.962 in
predicting invasive ductal cancer [13]. however, identifying tumor-associated
stroma in prostate biopsies and whole-mount histopathology slides remains
challenging.analyzing tumor-associated stroma in prostate cancer requires
combining whole-mount and biopsy histopathology slides. biopsy slides provide
information on the presence of pca, while whole-mount slides provide information
on the extent and distribution of pca, including more information on
tumor-associated stroma. combining the information from both modalities can
provide a more accurate understanding of the tumor microenvironment. in this
work, we explore the field effect in prostate cancer by analyzing
tumor-associated stroma in multimodal histopathological images. our main
contributions can be summarized as follows:-to the best of our knowledge, we
present the first deep-learning approach to characterize prostate
tumor-associated stroma by integrating histological image analysis from both
whole-mount and biopsy slides. our research offers a promising computational
framework for in-depth exploration of the field effect and cancer progression in
prostate cancer. -we proposed a novel approach for stroma classification with
spatial graphs modeling, which enable more accurate and efficient analysis of
tumor microenvironment in prostate cancer pathology. given the spatial nature of
cancer field effect and tumor microenvironment, our graph-based method offers
valuable insights into stroma region analysis. -we developed a comprehensive
pipeline for constructing tumor-associated stroma datasets across multiple data
sources, and employed adversarial training and neighborhood consistency
regularization techniques to learn robust multimodal-invariant image
representations.",6
1721,Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,3.1,Dataset,"in our study, we utilized three datasets for tumor-associated stroma
analysis.(1) dataset a comprises 513 tiles extracted from the whole mount slides
of 40 patients, sourced from the archives of the pathology department at
cedars-sinai medical center (irb# pro00029960). it combines two sets of tiles:
224 images from 20 patients featuring stroma, normal glands, low-grade and
highgrade cancer [22], along with 289 images from 20 patients with dense
high-grade cancer (gleason grades 4 and 5) and cribriform/non-cribriform glands
[23]. each tile measures 1200×1200 pixels and is extracted from whole slide
images captured at 20x magnification (0.5 microns per pixel). the tiles were
annotated at the pixel-level by expert pathologists to generate stroma tissue
segmentation masks and were cross-evaluated and normalized to account for stain
variability.(2) dataset b included 97 whole mount slides with an average size of
over 174,000×142,000 pixels at 40x magnification. the prostate tissue within
these slides had an average tumor area proportion of 9%, with an average tumor
area of 77 square mm. an expert pathologist annotated the tumor region
boundaries at the region-level, providing exhaustive annotations for all tumor
foci. (3) dataset c comprised 6134 negative biopsy slides obtained from 262
patients' biopsy procedures, where all samples were diagnosed as negative. these
slides are presumed to contain predominantly normal stroma tissues without
phenotypic alterations in response to cancer. dataset a was utilized for
training the stroma segmentation model. extensive data augmentation techniques,
such as image scaling and staining perturbation, were employed during the
training process. the model achieved an average test dice score of 95.57 ± 0.29
through 5-fold cross-validation. this model was then applied to generate stroma
masks for all slides in datasets b and c. to precisely isolate stroma tissues
and avoid data bleeding from epithelial tissues, we only extracted patches where
over 99.5% of the regions were identified as stroma at 40x magnification to
construct the stroma classification dataset.for positive tumor-associated stroma
patches, we sampled patches near tumor glands within annotated tumor region
boundaries, as we presumed that tumor regions represent zones in which the
greatest amount of damage has progressed. for negative stroma patches, we
calculated the tumor distance for each patch by measuring the euclidean distance
from the patch center to the nearest edge of the labeled tumor regions. negative
stroma patches were then sampled from whole mount slides with a gleason group
smaller than 3 and a tumor distance larger than 5 mm. this approach aims to
minimize the risk of mislabeling tumor-associated stroma as normal tissue.
setting a 5mm threshold accounts for the typically minimal inflammatory
responses induced by prostate cancers, particularly in lower-grade cases. to
incorporate multi-modal information, we randomly sampled negative stroma patches
from all biopsy slides in dataset c. overall, we selected over 1.1 million
stroma patches of size 256×256 pixels at 40x magnification for experiments.
during model training and testing, we performed stain normalization and standard
image augmentation methods.",6
1723,Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,4.0,Results and Discussions,"in conclusion, our study introduced a deep learning approach to accurately
characterize the tumor-associated stroma in multi-modal prostate histopathology
slides. our experimental results demonstrate the feasibility of using deep
learning algorithms to identify and quantify subtle stromal alterations,
offering a promising tool for discovering new diagnostic and prognostic
biomarkers of prostate cancer. through exploring field effect in prostate
cancer, our work provides a computational system for further analysis of tumor
development and progression. future research can focus on validating our
approach on larger and more diverse datasets and expanding the method to a
patient-level prediction system, ultimately improving prostate cancer diagnosis
and treatment.",6
1724,Multi-scope Analysis Driven Hierarchical Graph Transformer for Whole Slide Image Based Cancer Survival Prediction,1.0,Introduction,"the ability to predict the future risk of patients with cancer can significantly
assist clinical management decisions, such as treatment and monitoring [21].
generally, pathologists need to manually assess the pathological images obtained
by whole-slide scanning systems for clinical decision-making, e.g., cancer
diagnosis and prognosis [20]. however, due to the complex morphology and
structure of human tissues and the continuum of histologic features phenotyped
across the diagnostic spectrum, it is a tedious and time-consuming task to
manually assess the whole slide image (wsi) [12]. moreover, unlike cancer
diagnosis and subtyping tasks, survival prediction is a future state prediction
task with higher difficulty. therefore, automated wsi analysis method for
survival prediction task is highly demanded yet challenging in clinical
practice.over the years, deep learning has greatly promoted the development of
computational pathology, including wsi analysis [9,17,24]. due to the huge size,
wsis are generally cropped to numerous patches with a fixed size and encoded to
patch features by a cnn encoder (e.g., imagenet pretrained resnet50 [11]) for
further analysis. the attention-dominated learning frameworks (e.g., abmil [13],
clam [18], dsmil [16], transmil [19], scl-wc [25], hipt [4], nagcn [9]) mainly
aim to find the key instances (e.g., patches and tissues) for wsi representation
and decision-making, which prefers the needle-ina-haystack tasks, e.g., cancer
diagnosis, cancer subtyping, etc. to handle cancer survival prediction, some
researchers integrated some attribute priors into the network design [5,26]. for
example, patch-gcn [5] treated the wsi as point cloud data, and the patch-level
adjacent relationship of wsi is learned by a graph convolutional network (gcn).
however, the fixed-size patches cropped from wsi mainly contain single-level
biological entities (e.g., cells), resulting in limited structural information.
deepattnmisl [26] extracted the phenotype patterns of the patient via a
clustering algorithm, which provides meaningful medical prior to guide the
aggregation of patch features. however, this cluster analysis strategy only
focuses on a single sample, which cannot describe the whole picture of the
pathological components specific to the cancer type. additionally, existing
learning frameworks often ignore the capture of contextual interactions of
pathological components (e.g., tumor, stroma, lymphocyte, etc.), which is
considered as important evidence for cancer survival prediction tasks [1,6].
therefore, wsi-based cancer survival prediction still remains a challenging
task.in summary, to better capture the prognosis-related information in wsi, two
technical key points should be fully investigated: (1) an analysis strategy to
mine more comprehensive and in-depth prior of wsis, and (2) a promising learning
network to explore the contextual interactions of pathological components. to
this end, this paper presents a novel multi-scope analysis driven learning
framework, called hierarchical graph transformer (hgt), to pertinently resolve
the above technical key points for more reliable and interpretable w. hou and y.
he-contributed equally to this work. wsi-based survival prediction. first, to
mine more comprehensive and in-depth attribute priors of wsi, we propose a
multi-scope analysis strategy consisting of in-slide superpixels and cross-slide
clustering, which can not only extract the spatial prior but also identify the
semantic prior of wsis. second, to explore the contextual interactions of
pathological components, we design a novel learning network, i.e., hgt, which
consists of a hierarchical graph convolution layer and a transformer-based
prediction head. specifically, based on the extracted spatial topology, the
hierarchical graph convolution layer in hgt progressively aggregate the
patch-level features to the tissue-level features, so as to learn the
topological features of variant microenvironments ranging from fine-grained
(e.g., cell) to coarse-grained (e.g., necrosis, epithelium, etc.). then, under
the guidance of the identified semantic prior, the tissue-level features are
further sorted and assigned to form the feature embedding of pathological
components. furthermore, the contextual interactions of pathological components
are captured with the transformer-based prediction head, leading to reliable
survival prediction and richer interpretability. extensive experiments on three
cancer cohorts (i.e., crc, tcga-lihc and tcga-kirc) demonstrates the
effectiveness and interpretability of our framework. our codes are available at
https:// github.com/baeksweety/superpixel transformer.",6
1728,Multi-scope Analysis Driven Hierarchical Graph Transformer for Whole Slide Image Based Cancer Survival Prediction,0,Loss Function and Training Strategy.,"for the network training, cox loss [26] is adopted for the survival prediction
task, which is defined as:where δ i denote the censorship of i-th patient, o(i)
and o(j) denote the survival output of i-th and j-th patient in a batch,
respectively. gpus. our graph convolutional model is implemented by pytorch
geometric [7].",6
1729,Multi-scope Analysis Driven Hierarchical Graph Transformer for Whole Slide Image Based Cancer Survival Prediction,3.0,Experiments,"the initial number of superpixels of slic algorithm is set to {600, 700, 600},
and the number of clusters of k-means algorithm is set to {16, 16, 16} for crc,
tcga-lihc and tcga-kica cohorts. the non linearity of gcn is relu. the number of
transformer heads is 8, and the attention scores of all heads are averaged to
produce the heatmap of contextual interactions. hgt is trained with a mini-batch
size of 16, and a learning rate of 1e -5 with adam optimizer for 30
epochs.evaluation metric. the concordance index (ci) [23] is used to measure the
fraction of all pairs of patients whose survival risks are correctly ordered. ci
ranges from 0 to 1, where a larger ci indicates better performance. moreover, to
evaluate the ability of patients stratification, the kaplan-meier (km) analysis
is used [23]. in this study, we conduct a 5-fold evaluation procedure with 5
runs to evaluate the survival prediction performance for each method. the result
of mean ± std is reported.",6
1761,Thinking Like Sonographers: A Deep CNN Model for Diagnosing Gout from Musculoskeletal Ultrasound,1.0,Introduction,"gout is the most common inflammatory arthritis and musculoskeletal ultrasound
(mskus) scanning is recommended to diagnose gout due to the non-ionizing
radiation, fast imaging speed, and non-invasive characteristics of mskus [7].
however, misdiagnosis of gout can occur frequently when a patient's clinical
characteristics are atypical. traditional mskus diagnosis relies on the
experience of the radiologist which is time-consuming and labor-intensive.
although convolutional neural networks (cnns) based ultrasound classification
models have been successfully used for diseases such as thyroid nodules and
breast cancer, conspicuously absent from these successful applications is the
use of cnns for gout diagnosis from mskus images. there are significant
challenges in cnn based gout diagnosis. firstly, the gout-characteristics
contain various types including double contour sign, synovial hypertrophy,
synovial effusion, synovial dislocation and bone erosion, and these
gout-characteristics are small and difficult to localize in mskus. secondly, the
surrounding fascial tissues such as the muscle, sarcolemma and articular capsule
have similar visual traits with gout-characteristics, and we found the existing
cnn models can't accurately pay attention to the gout-characteristics that
radiologist doctors pay attention to during the diagnosis process (as shown in
fig. 1). due to these issues, sota cnn models often fail to learn the gouty
mskus features which are key factors for sonographers' decision.in medical image
analysis, recent works have attempted to inject the recorded gaze information of
clinicians into deep cnn models for helping the models to predict correctly
based on lesion area. mall et al. [9,10] modeled the visual search behavior of
radiologists for breast cancer using cnn and injected human visual attention
into cnn to detect missing cancer in mammography. wang et al. [15] demonstrated
that the eye movement of radiologists can be a new supervision form to train the
cnn model. cai et al. [3,4] developed the sononet [1] model, which integrates
eye-gaze data of sonographers and used generative adversarial networks to
address the lack of eye-gaze data. patra et al. [11] proposed the use of a
teacher-student knowledge transfer framework for us image analysis, which
combines doctor's eye-gaze data with us images as input to a large teacher
model, whose outputs and intermediate feature maps are used to condition a
student model. although these methods have led to promising results, they can be
difficult to implement due to the need to collect doctors' eye movement data for
each image, along with certain restrictions on the network structure.different
from the existing studies, we propose a novel framework to adjust the general
cnns to ""think like sonographers"" from three different levels. (1) where to
adjust: modeling sonographers' gaze map to emphasize the region that needs
adjust; (2) what to adjust: classify the instances to systemically detect
predictions made based on unreasonable/biased reasoning and adjust; (3) how to
adjust: developing a training mechanism to strike the balance between gout
prediction accuracy and attention reasonability.",6
1776,Scribble-Based 3D Multiple Abdominal Organ Segmentation via Triple-Branch Multi-Dilated Network with Pixel- and Class-Wise Consistency,3.1,Dataset and Implementation Details,"we used the publicly available abdomen ct dataset word [17] for experiments,
which consists of 150 abdominal ct volumes from patients with rectal cancer,
prostate cancer or cervical cancer before radiotherapy. each ct volume contains
159-330 slices of 512×512 pixels, with an in-plane resolution of 0.976 × 0.976
mm and slice spacing of 2.5-3.0 mm. we aimed to segment seven organs: the liver,
spleen, left kidney, right kidney, stomach, gallbladder and pancreas. following
the default settings in [17], the dataset was split into 100 for training, 20
for validation and 30 for testing, respectively, where the scribble annotations
for foreground organs and background in the axial view of the training volumes
had been provided and were used in model training. for pre-processing, we cut
off the hounsfield unit (hu) values with a fixed window/level of 400/50 to focus
on the abdominal organs, and normalized it to [0, 1]. we used the
commonlyadopted dice similarity coefficient (dsc), 95% hausdorff distance (hd 95
) and the average surface distance (asd) for quantitative evaluation.our
framework was implemented in pytorch [19] on an nvidia 2080ti with 11 gb memory.
we employed the 3d unet [3] as the backbone network for all experiments, and
extended it with three decoders by embedding two auxiliary decoders with
different dilation rates, as detailed in sect. 2.1. to introduce perturbations,
different initializations were applied to each decoder, and random perturbations
(ratio = (0, 0.5)) were introduced in the bottleneck before the auxiliary
decoders. the stochastic gradient descent (sgd) optimizer with momentum of 0.9
and weight decay of 10 -4 was used to minimize the overall loss function
formulated in eq. 5, where α=10.0 and β=1.0 based on the best performance on the
validation set. the poly learning rate strategy [16] was used to decay learning
rate online. the batch size, patch size and maximum iterations t max were set to
1, [80, 96, 96] and 6 × 10 4 respectively. the final segmentation results were
obtained by using a sliding window strategy. for a fair comparison, we used the
primary decoder's outputs as the final results during the inference stage and
did not use any post-processing methods. note that all experiments were
conducted in the same experimental setting. the existing methods are implemented
with the help of open source codebase from [14].table 1. quantitative comparison
between our method and existing weakly supervised methods on word testing set. *
denotes p-value < 0.05 (paired t-test) when comparing with the second place
method [15]. the best values are highlighted in bold.",7
1781,Mammo-Net: Integrating Gaze Supervision and Interactive Information in Multi-view Mammogram Classification,1.0,Introduction,"breast cancer is the most prevalent form of cancer among women and can have
serious physical and mental health consequences if left unchecked [5]. early
detection through mammography is critical for early treatment and prevention
[19]. mammograms provide images of breast tissue, which are taken from two
views: the cranio-caudal (cc) view, and the medio-lateral oblique (mlo) view
[4]. by identifying breast cancer early, patients can receive targeted treatment
before the disease progresses.deep neural networks have been widely adopted for
breast cancer diagnosis to alleviate the workload of radiologists. however,
these models often require a large number of manual annotations and lack
interpretability, which can prevent their broader applications in breast cancer
diagnosis. radiologists typically focus on areas with breast lesions during
mammogram reading [11,22], which provides valuable guidance. we propose using
real-time eye tracking information from radiologists to optimize our model. by
using gaze data to guide model training, we can improve model interpretability
and performance [24].radiologists' eye movements can be automatically and
unobtrusively recorded during the process of reading mammograms, providing a
valuable source of data without the need for manual labeling. previous studies
have incorporated radiologists' eye-gaze as a form of weak supervision, which
directs the network's attention to the regions with possible lesions [15,23].
leveraging gaze from radiologists to aid in model training not only increases
efficiency and minimizes the risk of errors linked to manual annotation, but
also can be seamlessly implemented without affecting radiologists' normal
clinical interpretation of mammograms.mammography primarily detects two types of
breast lesions: masses and microcalcifications [16]. the determination of the
benign or malignant nature of masses is largely dependent on the smoothness of
their edges [13]. the gaze data can guide the model's attention towards the
malignant masses. microcalcifications are small calcium deposits which exhibit
irregular boundaries on mammograms [9]. this feature makes them challenging to
identify, often leading to missed or false detection by models. radiologists
need to magnify mammograms to differentiate between benign scattered
calcifications and clustered calcifications, the latter of which are more likely
to be malignant and necessitate further diagnosis. leveraging gaze data can
guide the model to locate malignant calcifications.in this work, we propose a
novel diagnostic model, namely mammo-net, which integrates radiologists' gaze
data and interactive information between cc-view and mlo-view to enhance
diagnostic performance. to the best of our knowledge, this is the first work to
integrate gaze data into multi-view mammography classification. we utilize class
activation map (cam) [18] to calculate the attention maps for the model.
additionally, we apply pyramid loss to maintain consistency between
radiologists' gaze heat maps and the model's attention maps at multiple scales
of the pyramid [1]. our model is designed for singlebreast cases. mammo-net
extracts multi-view features and utilizes transformerbased attention to
mutualize information [21]. furthermore, there are differences between
multi-view mammograms of the same patient, arising from variations in breast
shape and density. capturing these multi-view shared features can be a challenge
for models. to address this issue, we develop a novel method called
bidirectional fusion learning (bfl) to extract shared features from multi-view
mammograms.our contributions can be summarized as follows:• we emphasize the
significance of low-cost gaze to provide weakly-supervised positioning and
visual interpretability for the model. additionally, we develop a pyramid loss
that adapts to the supervised process. • we propose a novel breast cancer
diagnosis model, namely mammo-net. this model employs transformer-based
attention to mutualize information and uses bfl to integrate task-related
information to make accurate predictions. • we demonstrate the effectiveness of
our approach through experiments using mammography datasets, which show the
superiority of mammo-net.2 proposed method",7
1785,Mammo-Net: Integrating Gaze Supervision and Interactive Information in Multi-view Mammogram Classification,2.3,Interactive Information,"transformer-based mutualization model. we use transformer-based attention to
mutualize information from the two views at the level of the spatial feature
map. for each attention head, we compute embeddings for the source and target
pixels. our model does not utilize positional encoding, as it encodes the
relative position of each pixel and is not suitable for capturing information
between different views of mammograms [21]. the target view feature maps are
transformed into q, the source view feature maps are transformed into k, and the
original source feature maps are transformed into v . we can then obtain a
weighted sum of the features from the source view for each target pixel using
[21]:subsequently, the output is transformed into attention-based feature maps x
and mutualized with the feature maps y from the other view. the mutualized
feature maps are normalized and used for subsequent calculations:bidirectional
fusion learning. to enable the fusion network to retain more of the shared
features between the two views and filter out noise, we propose to use bfl to
learn a fusion representation that maximizes the cross-view mutual information.
the optimization target is to generate a fusion representation i from multi-view
representations p v , where v ∈ {cc, mlo}. we employ the noise-contrastive
estimation framework [6] to maximize the mutual information, which is a
contrastive learning framework:where s(i, p v ) evaluates the correlation
between multi-view fused representations and single-view representations
[17]:where n (i) is a reconstruction of p v generated by a fully connected
network n from i and the euclidean norm || • || 2 is applied to obtain
unit-length vectors.in contrastive learning, we consider the same patient
mammograms as positive samples and those from different patient mammograms in
the same batch p i v = p v \{p i v } as negative samples [17]. minimizing the
similarity between the same patient mammograms enables the model to learn shared
features. maximizing the dissimilarity between different patient mammograms
enhances the model's robustness.in short, we require the fusion representation i
to reversely reconstruct multiview representations p v so that more
view-invariant information can be passed to i. by aligning the prediction n (i)
to p v , we enable the model to decide how much information it should receive
from each view.the overall loss function for this module is the sum of the
losses defined for each view:",7
1793,CTFlow: Mitigating Effects of Computed Tomography Acquisition and Reconstruction with Normalizing Flows,2.1,Datasets,"this study used two unique datasets: (1) the ucla low-dose chest ct dataset, a
collection of 186 exams acquired using siemens ct scanners at an equivalent dose
of 2 mgy following an institutional review board-approved protocol. the raw
projection data of scans were exported, and poisson noise was introduced, as
described in zabic et al. [15], at levels equivalent to 10% of the original
dose. projection data were then reconstructed into an image size of 512 × 512
using three reconstruction kernels (smooth, medium, sharp) at 1.0 mm slice
thickness. the dataset was split into 80 scans for training, 20 for validation,
and 86 for testing. (2) aapm-mayo clinic low-dose ct ""grand challenge"" dataset,
a publicly available grand challenge dataset consisting of 5,936 abdominal ct
images from 10 patient cases reconstructed at 1.0 mm slice thickness. each case
consists of a paired 100% ""normal dose"" scan and a simulated 25% ""low dose""
scan. images from eight patient cases were used for training, and two cases were
reserved for validation. all images were randomly cropped into patches of 128 ×
128 pixels, generated from regions containing the body. this dataset was only
used for evaluating image quality against other harmonization techniques.",7
1820,DisAsymNet: Disentanglement of Asymmetrical Abnormality on Bilateral Mammograms Using Self-adversarial Learning,3.1,Datasets,"this study reports experiments on four mammography datasets. the inbreast
dataset [7] consists of 115 exams with bi-rads labels and pixel-wise
anno-tations, comprising a total of 87 normal (bi-rads = 1) and 342 abnormal
(bi-rads = 1) images. the ddsm dataset [3] consists of 2,620 cases, encompassing
6,406 normal and 4,042 (benign and malignant) images with outlines generated by
an experienced mammographer. the vindr-mammo dataset [8] includes 5,000 cases
with bi-rads assessments and bounding box annotations, consisting of 13,404
normal (bi-rads = 1) and 6,580 abnormal (bi-rads = 1) images. the in-house
dataset comprises 43,258 mammography exams from 10,670 women between 2004-2020,
collected from a hospital with irb approvals. in this study, we randomly select
20% women of the full dataset, comprising 6,000 normal (bi-rads = 1) and 28,732
abnormal (bi-rads = 1) images. due to a lack of annotations, the in-house
dataset is only utilized for classification tasks. each dataset is randomly
split into training, validation, and testing sets at the patient level in an
8:1:1 ratio, respectively (except for that inbreast which is split with a ratio
of 6:2:2, to keep enough normal samples for the test).table 1. comparison of
asymmetric and abnormal classification tasks on four mammogram datasets. we
report the auc results with 95% ci. note that, when ablating the ""asyc "", we
only drop the ""asyt"" and keep the encoders and classifiers.",7
1825,Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,1.0,Introduction,"over 430,000 new cases of renal cancer were reported in 2020 in the world [1]
and this number is expected to rise [22]. when the tumor size is large (greater
than 7 cm) often the whole kidney is removed, however, when the tumor size is
small (less than 4 cm), partial nephrectomy is the preferred treatment [20] as
it could preserve kidney's function. thus, early detection of kidney tumors can
help to improve patient's prognosis. however, early-stage renal cancers are
usually asymptomatic, therefore they are often incidentally found during other
examinations [19], which includes non-contrast ct (ncct) scans.segmentation of
kidney tumors on ncct images adds challenges compared to contrast-enhanced ct
(cect) images, due to low contrast and lack of multiphase images. on cect
images, the kidney tumors have different intensity values compared to the normal
tissues. there are several works that demonstrated successful segmentation of
kidney tumors with high precision [13,21]. however, on ncct images, as shown in
fig. 1b, some tumors called isodensity tumors, have similar intensity values to
the surrounding normal tissues. to detect such tumors, one must compare the
kidney shape with tumors to the kidney shape without the tumors so that one can
recognize regions with protuberance.3d u-net [3] is the go-to network for
segmenting kidney tumors on cect images. however, convolutional neural networks
(cnns) are biased towards texture features [5]. therefore, without any
intervention, they may fail to capture the protuberance caused by isodensity
tumors on ncct images.in this work, we present a novel framework that is capable
of capturing the protuberances in the kidneys. our goal is to segment kidney
tumors including isodensity types on ncct images. to achieve this goal, we
create a synthetic dataset, which has separate annotations for normal kidneys
and protruded regions, and train a segmentation network to separate the
protruded regions from the normal kidney regions. in order to segment whole
tumors, our framework consists of three networks. the first is a base network,
which extracts kidneys and an initial tumor region masks. the second
protuberance detection network receives the kidney region mask as its input and
predicts a protruded region mask. the last fusion network receives the initial
tumor mask and the protruded region mask to predict a final tumor mask. this
proposed framework enables a better segmentation of isodensity tumors and boosts
the performance of segmentation of kidney tumors on ncct images. the
contribution of this work is summarized as follows:1. present a pioneering work
for segmentation of kidney tumors on ncct images. 2. propose a novel framework
that explicitly captures protuberances in a kidney to enable a better
segmentation of tumors including isodensity types on ncct images. this framework
can be extended to other organs (e.g. adrenal gland, liver, pancreas). 3. verify
that the proposed framework achieves a higher dice score compared to the
standard 3d u-net using a publicly available dataset.",7
1826,Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,2.0,Related Work,"the release of two public ct image datasets with kidney and tumor masks from the
2019/2021 kidney and kidney tumor segmentation challenge [8] (kits19, kits21)
attracted researchers to develop various methods for segmentation.looking at the
top 3 teams from each challenge [6,11,13,17,21], all teams utilized 3d u-net [3]
or v-net [16], which bears a similar architecture. the winner of kits19 [13]
added residual blocks [7] to 3d u-net and predicted kidney and tumor regions
directly. however, the paper notes that modifying the architecture resulted in
only slight improvement. the other 5 teams took a similar approach to nnu-net's
coarse-to-fine cascaded network [12], where it predicts from a low-resolution
image in the first stage and then predicts kidneys and tumors from a
high-resolution image in the second stage. thus, although other attempts were
made, using 3d u-net is the go-to method for predicting kidneys and tumors. in
our work, we also make use of 3d u-net, but using this network alone fails to
learn some isodensity tumors. to overcome this issue, we developed a framework
that specifically incorporates protuberances in kidneys, allowing for an
effective segmentation of tumors on ncct images.in terms of focusing on
protruded regions in kidneys, our work is close to [14,15]. [14] developed a
computer-aided diagnosis system to detect exophytic kidney tumors on ncct images
using belief propagation and manifold diffusion to search for protuberances. an
exophytic tumor is located on the outer surface of the kidney that creates a
protrusion. while this method demonstrated high sensitivity (95%), its false
positives per patient remained high (15 false positives per patient). in our
work, we will not only segment protruded tumors but also other tumors as well.
the first base network is responsible for predicting kidney and tumor region
masks. our architecture is based on 3d u-net, which has an encoder-decoder style
architecture, with few modifications. to reduce the required size of gpu memory,
we only use the encoder that has only 16 channels at the first resolution, but
instead we make the architecture deeper by having 1 strided convolution and 4
max-pooling layers. in the decoder, we replace the up-convolution layers with a
bilinear up-sampling layer and a convolution layer. in addition, by only having
a single convolution layer instead of two in the original architecture at each
resolution, we keep the decoder relatively small. throughout this paper, we
refer this architecture as our 3d u-net.the second protuberance detection
network is the same as the base network except it starts from 8 channels instead
of 16. we train this network using synthetic datasets. the details of the
dataset and training procedures are described in sect. 3.2.the last fusion
network combines the outputs from the base network and the protuberance
detection network and makes the final tumor prediction. in detail, we perform a
summation of the initial tumor mask and the protruded region mask, and then
concatenate the result with the input image. this is the input of the last
fusion network, which also has the same architecture as the base network with an
exception of having two input channels. this fusion network do not just combine
the outputs but also is responsible for removing false positives from the base
network and the protuberance detection network.our combined three network is
fully differentiable, however, to train efficiently, we train the model in 3
steps.",7
1836,Skin Lesion Correspondence Localization in Total Body Photography,1.0,Introduction,"evolution, the change of pigmented skin lesions, is a risk factor for melanoma
[1]. therefore, longitudinal tracking of skin lesions over the whole body is
beneficial for early detection of melanoma [5]. however, establishing skin
lesion correspondences across multiple scans from different patient visits has
not been well investigated in the context of full-body imaging.several
techniques have been proposed to match skin lesions across pairs of 2d images
[9][10][11][12][13][14]16,17,25]. early work used geometric constraints imposed
by initial matches of skin lesions (manual selection or automatic detection) to
align images and further match other skin lesions [10,16,17,25]. mirzaalian and
colleagues published a series of works for establishing lesion correspondence in
image space [11][12][13][14]. li et al. [9] used a cnn to output a 2d vector
field for pixel-wise correspondences between the two input images. though
effective at matching skin lesions across pairs of images, the extension of
these methods to the context of total body photography (tbp) for longitudinal
tracking remains a challenge.several works have been proposed for tackling the
skin lesion tracking problem over the full body [7,8,21,22]. however, they are
either only applicable in well-controlled environments or do not extend to the
tracking of lesions across scans at different visits. recently, the concept of
finding lesion correspondence using a 3d representation of the human body has
been explored in [26] and [2] by using a template mesh. however, accurately
deforming a template mesh to fit varying body shapes is challenging when the
scanned shape deviates from the template, leading to large errors in downstream
tasks such as establishing shape correspondence. additionally, [26] does not
take advantage of texture, while [2] uses texture in a common uv map that may
lead to failures when geodesically close locations on the surface are mapped to
distant sites in the texture map (e.g. when the two locations are on opposite
sides of a texture seam).we propose a novel framework for finding skin lesion
correspondence iteratively using geometric and texture information (fig. 1). we
demonstrate the effectiveness of the proposed method in localizing lesion
correspondence across scans in a manner that is robust to changes in body pose
and camera viewing directions. our code is available at
https://github.com/weilunhuang-jhu/ lesioncorrespondencetbp3d.",7
1846,Anatomy-Informed Data Augmentation for Enhanced Prostate Cancer Detection,1.0,Introduction,"data augmentation (da) is a key factor in the success of deep neural networks
(dnn) as it artificially enlarges the training set to increase their
generalization ability as well as robustness [22]. it plays a crucial role in
medical image analysis [8] where annotated datasets are only available with
limited size. dnns have already successfully supported radiologists in the
interpretation of magnetic resonance images (mri) for prostate cancer (pca)
diagnosis [3]. however, the da scheme received less attention, despite its
potential to leverage the data characteristic and address overfitting as the
root of generalization problems.state-of-the-art approaches still rely on
simplistic spatial transformations, like translation, rotation, cropping, and
scaling by globally augmenting the mri sequences [12,20]. they exclude random
elastic deformations, which can change the lesion outline but might alter the
underlying label and thus produce counterproductive examples for training [22].
however, soft tissue deformations, which are currently missing from the da
schemes, are known to significantly affect the image morphology and therefore
play a critical role in accurate diagnosis [6].both lesion and prostate shape
geometrical appearance influence the clinical assessment of prostate
imaging-reporting and data system (pi-rads) [24]. the prostate constantly
undergoes soft tissue deformation dependent on muscle contractions, respiration,
and more importantly variable filling of the adjacent organs, namely the bladder
and the rectum. among these sources, the rectum has the largest influence on the
prostate and lesion shape variability due to its large motion [4] and the fact
that the majority of the lesions are located in the adjacent peripheral prostate
zone [1]. however, only one snapshot of all these functional states is captured
within each mri examination, and almost never will be exactly the same on any
repeat or subsequent examination. ignoring these deformations in the da scheme
can potentially limit model performance.model-driven transformations attempting
to simulate organ functions -like respiration, urinary excretion,
cardiovascular-and digestion mechanics -offer a high degree of diversity while
also providing realistic transformations. currently, the finite element method
(fem) is the standard for modeling biomechanics [13]. however, their computation
is overly complex [10] and therefore does not scale to on-the-fly da [7]. recent
motion models rely on dnns using either a fem model [15] or complex training
with population-based models [18]. motion models have not been integrated into
any deep learning framework as an online data augmentation yet, thereby leaving
the high potential of inducing applicationspecific knowledge into the training
procedure unexploited.in this work we propose an anatomy-informed spatial
augmentation, which leverages information from adjacent organs to mimic typical
deformations of the prostate. due to its lightweight computational requirements,
it can be easily integrated into common da frameworks. this technique allows us
to simulate different physiological states during the training and enrich our
dataset with a wider range of organ and lesion shapes. inducing this kind of
soft tissue deformation ultimately led to improved model performance in
patient-and lesion-level pca detection on an independent test set.",7
1848,Anatomy-Informed Data Augmentation for Enhanced Prostate Cancer Detection,2.2,Experimental Setting,"we evaluate our anatomy-informed da qualitatively as well as
quantitatively.first, we visually inspect whether our assumptions in sect. 2.1
regarding pelvic biomechanics resulted in realistic transformations. we apply
either our proposed transformation to the rectum or the bladder, random
deformable or no transformation in randomly selected exams and conduct a strict
turing test with clinicians having different levels of radiology expertise (a
freshly graduated clinician (c.e.) and resident radiologists (c.m., k.s.z.), 1.5
-3 years of experience in prostate mri) to determine if they can notice the
artificial deformation.finally, we quantify the effect of our proposed
transformation on the clinical task of patient-level pca diagnosis and
lesion-level pca detection. we derive the diagnosis through semantic
segmentation of the malignant lesions following previous studies
[5,11,12,20,21]. semantic segmentation provides interpretable predictions that
are sensitive to spatial transformations, making it appropriate for testing
spatial das. to compare the performance of the trained models to radiologists,
we calculate their performance using the clinical pi-rads scores and
histopathological ground truths. to consider clinically informative results, we
use the partial area under the receiver operating characteristic (pauroc) for
patient-level evaluation with the sensitivity threshold of 78.75%, which is 90%
of the sensitivity of radiologists for pi-rads ≥ 4. additionally, we calculate
the f 1 -score at the sensitivity of pi-rads ≥ 4. afterward, we evaluate model
performances on object-level using the free-response receiver operating
characteristic (froc) and the number of detections at the radiologists' lesion
level performance for pi-rads ≥ 4, at 0.32 average number of false positives per
scan. objects were derived by applying a threshold of 0.5 to the softmax outputs
followed by connected component analysis to identify connected regions in the
segmentation maps. predictions with an intersection over union of 0.1 with a
ground truth object were considered true positives. to systematically compare
the effect of our proposed anatomy-informed da with the commonly used settings,
we create three main da schemes:1. basic da setting of nnu-net [8], which is an
extensive augmentation pipeline containing simple spatial transformations,
namely translation, rotation and scaling. this setting is our reference da
scheme. 2. random deformable transformations as implemented in the nnu-net [8]
da pipeline extending the basic da scheme (1) to test its presence in the
medical domain. our hypothesis is that it will produce counterproductive
examples, resulting in inferior performance compared to our proposed da.3.
proposed anatomy-informed transformation in addition to the simple da scheme
(1). we define two variants of it: (a) deforming only the rectum, as rectal
distension has the highest influence among the organs on the shapes of the
prostate lesions [4]. (b) deforming the bladder in addition to the rectum, as
bladder deformations also have an influence on lesions, although smaller.",7
1849,Anatomy-Informed Data Augmentation for Enhanced Prostate Cancer Detection,2.3,Prostate MRI Data,"774 consecutive bi-parametric prostate mri examinations are included in this
study, which were acquired in-house during the clinical routine. the ethics
committee of the medical faculty heidelberg approved the study (s-164/2019) and
waived informed consent to enable analysis of a consecutive cohort. all
experiments were performed in accordance with the declaration of helsinki [2]
and relevant data privacy regulations. for every exam, pi-rads v2 [24]
interpretation was performed by a board-certified radiologist. every patient
underwent extended systematic and targeted mri trans-rectal ultrasound-fusion
transperineal biopsy. malignancy of the segmented lesions was determined from a
systematic-enhanced lesion ground-truth histopathological assessment, which has
demonstrated reliable ground-truth assessment with sensitivity comparable to
radical prostatectomy [17]. the samples were evaluated according to the
international society of urological pathology (isup) standards under the
supervision of a dedicated uropathologist. clinically significant prostate
cancer (cspca) was defined as isup grade 2 or higher. based on the biopsy
results, every cspca lesion was segmented on the t2-weighted sequences
retrospectively by multiple in-house investigators under the supervision of a
board-certified radiologist. in addition to the lesions, the rectum and the
bladder segmentations were automatically predicted by a model built upon nnu-net
[8] trained iteratively on an in-house cohort initially containing a small
portion of our cohort. multiple radiologists confirmed the quality of the
predicted segmentations.",7
1851,Anatomy-Informed Data Augmentation for Enhanced Prostate Cancer Detection,3.0,Results,"the anatomy-informed transformation produced highly realistic soft tissue
deformations. figure 2 shows an example of the transformation simulating rectum
distensions with prostate lesions at different distances from the rectum. 92% of
the rectum and 93% of the bladder deformation from the randomly picked exams
became so realistic that our freshly graduated clinician did not detect them,
but our residents noticed 87.5% of the rectum and 25% of the bladder
deformations based on small transformation artifacts and their expert intuition.
irregularities resulted from the random elastic deformations can be easily
detected, in contrast to our method being challenging to detect its artificial
nature. in table 1 we summarize the patient-level pauroc and f 1 -scores; and
lesion-level froc results on the independent test set showing the advantage of
using anatomy-informed da. to further highlight the practical advantage of the
proposed augmentation, we compare the performance of the trained models to the
radiologists' diagnostic performance for pi-rads ≥ 4, which locate the most
informative performance point clinically on the roc diagram, see fig. 3.
extending the basic da scheme with the proposed anatomy-informed deformation not
only increased the sensitivity closely matching the radiologists' patient-level
diagnostic performance but also improved the detection of pca on a lesion level.
interestingly, while the use of random deformable transformation also improved
lesion-level performance, it did not approach the diagnostic performance of the
radiologists, unlike the anatomy-informed da.at the selected patient-and
object-level working points, the model with the proposed rectum-and
bladder-informed da scheme reached the best results with significant
improvements (p < 0.05) compared to the model with the basic da setting by
increasing the f 1 -score with 5.11% and identifying 4 more lesions (5.3%) from
the 76 lesions in our test set.the time overhead introduced by anatomy-informed
augmentation caused no increase in the training time, the gpu remained the main
bottleneck.",7
1857,Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,2.1,Data Curation,"ex-vivo: data is collected from fresh breast tissue samples from the patients
referred to bcs at kingston health sciences center over two years. the study is
approved by the institutional research ethics board and patients consent to be
included. peri-operatively, a pathologist guides and annotates the ex-vivo
pointburns, referred to as spectra, from normal or cancerous breast tissue
immediately after excision. in addition to spectral data, clinicopathological
details such as the status of hormone receptors is also provided
post-surgically. in total 51 cancer and 149 normal spectra are collected and
stratified into five folds (4 for cross validation and 1 prospectively) with
each patient restricted to one fold only.",7
1862,Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,0,Ex-vivo Evaluation:,"the performance of the proposed network is compared with 3 baseline models
including gtn, graph convolution network [14], and non-graph convolution
network. four-fold cross validation is used for comparison of the different
approaches, to increase the generalizability (3 folds for train/validation, test
on remaining unseen fold, report average test performance). separate ablation
studies are performed for the baseline models to fine tune their structural
parameters. all experiments are implemented using pytorch with adam optimizer,
learning rate of 10 -4 , batch size of 32, and early stopping based on
validation loss. to demonstrate the robustness of the model and ensure it is not
overfitting, we also report the performance of the ensemble model from the
4-fold cross validation study on the 5th unseen prospective test fold.clinical
relevance: hormone receptor status plays an important role in determining breast
cancer prognosis and tailoring treatment plans for patients [6]. here, we
explore the correlation of the attention maps generated by egt with the status
of her2 and pr hormones associated with each spectrum. these hormones are
involved in different types of signaling that the cell depends on [5].",7
1867,Radiomics-Informed Deep Learning for Classification of Atrial Fibrillation Sub-Types from Left-Atrium CT Volumes,1.0,Introduction,"atrial fibrillation (af) is a cardiac disease characterized by rapid, irregular
heartbeats [4]. the disease can lead to stroke and heart failure, and has a
mortal-ity rate of almost 20% [5,10,13]. af is classified as either persistent
atrial fibrillation (peaf), where abnormal heart rhythms occur continuously for
more than seven days, or paroxysmal atrial fibrillation (paaf), where the heart
rhythm returns to normal within seven days. although af can be treated through a
procedure called catheter ablation, peaf cases have high recurrence rates and
often require re-intervention [8]. accurate knowledge of the disease type is
therefore highly valuable for treatment planning and has high prognostic value
[22].clinical studies have discovered a strong relationship between af and
epicardial adipose tissue (eat), a fat depot layer on the surface of the
myocardium that can cause inflammation and disrupt cardiac function [3,15].
recent works have shown that automatic classification of af sub-types can be
done using ct volumes of the left atrium and surrounding eat, which can be used
to screen for patients with high risk of peaf. huber et al. [7] showed that eat
volume, approximated from left-atrium ct images, can be used as a predictor for
af recurrence. yang et al. [22] trained a random forest model to classify af
subtype based on radiomic features and volume measurements, achieving 85.3% auc.
although these methods demonstrate the usefulness of radiomic features for af
sub-type classification, such features are generic and not specific to the task,
which can limit model performance [12]. radiomic features also rely on summary
statistics such as entropy or homogeneity to obtain global descriptors, and
these have limited effectiveness when capturing local feature variations
[16].deep learning has achieved outstanding results on medical imaging analysis
tasks, largely due to its ability to learn task-specific features and complex
relations between them [17]. naïvely using deep neural networks (dnns) to
predict af sub-types from ct volumes yields poor results however due to
over-fitting on high-dimensional volume inputs (see results for dnn in table 1).
existing works have attempted to combine deep and radiomic features through
methods such as direct concatenation [2,19], attention modules [14], or
contrastive learning between feature types [24]. although these methods propose
different ways of using both approaches, they do not explicitly address the
limitations of either approach or explore ways to combine their complementary
advantages.in this work, we propose a novel approach to atrial fibrillation
sub-type classification from ct volumes by integrating radiomic and deep
learning methods. we note that textural radiomic features identified by feature
selection methods can serve as an information prior to supplement low-level
features from dnns, since they are designed to capture low-level context and
have predictive power [23]. to this end, we locally calculate radiomic features
based on patches surrounding each voxel, and perform feature fusion with
low-level dnn features. this provides the dnn with pre-defined features known to
be relevant to the task to reduce over-fitting, and also allows spatial
relations between radiomic features to be learned. furthermore, we encourage the
dnn to learn features complementary to radiomic features to obtain more
comprehensive signals and design a novel feature de-correlation loss. the
overall framework, which we term radiomics-informed deep learning (ridl), is
illustrated in fig. 1. unlike existing works, our method is designed to directly
addresses the limitations of both deep learning and radiomic approaches and
achieves state-of-the-art performance on af sub-type classification. to
summarize our key contributions: -we propose a novel radiomics-informed deep
learning (ridl) method for af sub-type classification from ct volumes, which
achieves state-of-the-art results and can be used to screen for patients with
high risk of peaf. -our method uses a novel approach of fusing locally computed
radiomic features with low-level dnn features to improve capturing of local
context. -furthermore, we enforce feature de-correlation using a novel
feature-bank design to ensure complementary deep and radiomic features are
extracted.",7
1872,Radiomics-Informed Deep Learning for Classification of Atrial Fibrillation Sub-Types from Left-Atrium CT Volumes,3.1,Implementation Details,"dataset. we use a dataset of 172 patients containing 94 paaf and 78 peaf cases
collected from the sun yat-sen memorial hospital in china. ct volumes are
centered on the left atrium and normalized to between -1 and 1. roi masks for
eat are obtained through hounsfield value thresholding between -250 and 0.
volumes are resized to the same aspect ratio to ensure consistent dimensions
across samples. we use an input size of 96 × 128 × 128 voxels and apply zero
padding for smaller volumes. we use five-fold cross-validation and report
average test performance across folds. cross-validation is implemented by
splitting the dataset into five equal subsets and using three subsets for
training, one subset for validation, and one subset for testing. a rolling
scheme is used such that different validation and test subsets are used for each
of the five folds. data acquisition procedures and statistics are given in the
supplementary materials.setup. we use the pyradiomic package [18] to extract
radiomic features from the input volumes and masks. using the cross-validation
splits, we perform feature selection and classification using lasso regularized
logistic regression. lasso regularization consistently selects four radiomic
features as the ones with the most significant predictive power: maximum 3d
diameter, maximum 2d diameter, maximum voxel value, and normalized inverse
difference of glcm (glcm idn). the texture feature glcm idn is calculated
locally for p ∈ {1, 2, 5, 10} to obtain local radiomic features r l i ∈ r
4×96×128×128 . for our dnn network, we use a modified 3d u-net [1] (abbreviated
as m3dunet) with skip connections between the encoder and decoder removed to
enhance bottle-neck feature compression. bottle-neck features are averaged
across spatial dimensions for classification, whilst decoder outputs are used
for self-reconstruction regularization. the model is trained using the adam
optimizer with learning rate 10 -4 for 100 epochs and 0.1 decay at 30 epochs. we
use batch size b = 1, feature bank size n k = 25, and warm-up period of one
epoch. we use w corr = 2 for de-correlation loss weighting, which was chosen
based on the validation splits. mean and standard deviation of ten runs are
reported. additional experiments and details are included in the supplementary
materials.",7
1876,Radiomics-Informed Deep Learning for Classification of Atrial Fibrillation Sub-Types from Left-Atrium CT Volumes,4.0,Conclusion,"in this work, we propose a new approach to atrial fibrillation sub-type
classification from ct volumes by integrating radiomic and deep learning
approaches through a radiomics-informed deep learning method, ridl. our method
is based on two key ideas: feature fusion of locally computed radiomic features
with lowlevel dnn features to improve local context, and encouraging
complementary deep and radiomic features through feature de-correlation. unlike
existing hybrid approaches, our method specifically addresses the advantages and
limitations of both techniques to improve feature extraction. we achieve
state-of-the-art results on af sub-type classification and outperform existing
radiomic, deep learning, and hybrid methods.future improvements to ridl can be
made by introducing more sophisticated local radiomic features selection
methods, given the large set features to choose from. experiments on larger
datasets or alternative tasks can also be done to provide more empirical
support, since current results show only slight improvements over baseline.
these issues may be addressed in future works. overall, our method is a novel
way of combining radiomic and deep learning approaches, and can be used to
improve accuracy of peaf screening from ct volumes for better preventive care of
high-risk patients.",7
1879,Synthesis of Contrast-Enhanced Breast MRI Using T1- and Multi-b-Value DWI-Based Hierarchical Fusion Network with Attention Mechanism,1.0,Introduction,"breast cancer is the most common cancer and the leading cause of cancer death in
women [18]. early detection of breast cancer allows patients to receive timely
treatment, which may have less burden and a higher probability of survival [6].
among current clinical imaging modalities, magnetic resonance imaging (mri) has
the highest sensitivity for breast cancer detection [12]. especially,
contrastenhanced mri (ce-mri) can identify tumors well and has become an
indispensable technique for detecting and defining cancer [13]. however, the use
of gadolinium-based contrast agents (gbca) requires iv-cannulation, which is a
burden to patients, time consuming and cumbersome in a screening situation.
moreover, contrast administration can lead to allergic reactions and finaly
ce-mri may be associated with nephrogenic systemic fibrosis and lead to
bioaccumulation in the brain, posing a potential risk to human health
[4,9,[14][15][16]. in 2017, the european medicines agency concluded its review
of gbca, confirming recommendations to restrict the use of certain linear gbca
used in mri body scans and to suspend the authorization of other contrast
agents, albeit macrocyclic agents can still be freely used [10].with the
development of computer technology, artificial intelligence-based methods have
shown potential in image generation and have received extensive attention. some
studies have shown that some generative models can effectively perform mutual
synthesis between mr, ct, and pet [19]. among them, synthesis of ce-mri is very
important as mentioned above, but few studies have been done by researchers in
this area due to its challenging nature. li et al. analyzed and studied the
feasibility of using t1-weighted mri and t2-weighted mri to synthesize ce-mri
based on deep learning model [11]. their results showed that the model they
developed could potentially synthesize ce-mri and outperform other cohort
models. however, mri source data of too few sequences (only t1 and t2) may not
provide enough valuable informative to effectively synthesize ce-mri. in another
study, chung et al. investigated the feasibility of using deep learning (a
simple u-net structure) to simulate contrast-enhanced breast mri of invasive
breast cancer, using source data including t1-weighted non-fatsuppressed mri,
t1-weighted fat-suppressed mri, t2-weighted fat-suppressed mri, dwi, and
apparent diffusion coefficient [5]. however, obtaining a complete mri sequence
makes the examination costly and time-consuming. on the other hand, the
information provided by multi-sequences may be redundant and may not contain the
relevant information of ce-mri. therefore, it is necessary to focus on the most
promising sequences to synthesize ce-mri.diffusion-weighted imaging (dwi) is
emerging as a key imaging technique to complement breast ce-mri [3]. dwi can
provide information on cell density and tissue microstructure based on the
diffusion of tissue water. studies have shown that dwi could be used to detect
lesions, distinguish malignant from benign breast lesions, predict patient
prognosis, etc [1,3,7,8,17]. in particular, dwi can capture the dynamic
diffusion state of water molecules to estimate the vascular distribution in
tissues, which is closely related to the contrast-enhanced regions in ce-mri.
dwi may be a valuable alternative in breast cancer detection in patients with
contraindications to gbca [3]. inspired by this, we develop a multi-sequence
fusion network based on t1-weighted mri and multi-b-value dwi to synthesize
ce-mri. our contributions are as follows:i from the perspective of method, we
innovatively proposed a multi-sequence fusion model, designed for combining
t1-weighted imaging and multi-b-value dwi to synthesize ce-mri for the first
time. ii we invented hierarchical fusion module, weighted difference module and
multi-sequence attention module to enhance the fusion at different scale, to
control the contribution of different sequence and maximising the usage of the
information within and across sequences. iii from the perspective of clinical
application, our proposed model can be used to synthesize ce-mri, which is
expected to reduce the use of gbca.",7
1880,Synthesis of Contrast-Enhanced Breast MRI Using T1- and Multi-b-Value DWI-Based Hierarchical Fusion Network with Attention Mechanism,2.1,Patient Collection and Pre-processing,"this study was approved by institutional review board of our cancer institute
with a waiver of informed consent. we retrospectively collected 765 patients
with breast cancer presenting at our cancer institute from january 2015 to
november 2020, all patients had biopsy-proven breast cancers (all cancers
included in this study were invasive breast cancers, and ductal carcinoma in
situ had been excluded). the mris were acquired with philips ingenia all mris
were resampled to 1 mm isotropic voxels and uniformly sized, resulting in
volumes of 352 × 352 pixel images with 176 slices per mri, and subsequent
registration was performed based on advanced normalization tools (ants) [2].",7
1887,Synthesis of Contrast-Enhanced Breast MRI Using T1- and Multi-b-Value DWI-Based Hierarchical Fusion Network with Attention Mechanism,4.0,Conclusion,"we have developed a multi-sequence fusion network based on multi-b-value dwi to
synthesize ce-mri, using source data including dwis and t1-weighted
fatsuppressed mri. compared to existing methods, we avoid the challenges of
using full-sequence mri and aim to be selective on valuable source data dwi.
hierarchical fusion generation module, weighted difference module, and
multisequence attention module have all been shown to improve the performance of
synthesizing target images by addressing the problems of synthesis at different
scales, leveraging differentiable information within and across sequences. given
that current research on synthetic ce-mri is relatively sparse and challenging,
our study provides a novel approach that may be instructive for future research
based on dwis. our further work will be to conduct reader studies to verify the
clinical value of our research in downstream applications, such as helping
radiologists on detecting tumors. in addition, synthesizing dynamic
contrastenhanced mri at multiple time points will also be our future research
direction. our proposed model can potentially be used to synthesize ce-mri,
which is expected to reduce or avoid the use of gbca, thereby optimizing
logistics and minimizing potential risks to patients.",7
1889,Clinical Evaluation of AI-Assisted Virtual Contrast Enhanced MRI in Primary Gross Tumor Volume Delineation for Radiotherapy of Nasopharyngeal Carcinoma,1.0,Introduction,"nasopharyngeal carcinoma (npc), also known as lymphoepithelioma, is a highly
aggressive malignancy that originates in nasopharynx [1]. npc is characterized
by a distinct geographical distribution in southeast asia, north africa, and
arctic [2]. in china, npc accounts for up to 50% of all head and neck cancers,
while in southeast asia, npc accounts for more than 70% of all head and neck
cancers [3]. radiotherapy (rt) is currently the main treatment remedy, which
needs precise tumor delineation to ensure a satisfactory rt outcome. however,
accurately delineating the npc tumor is challenging due to the highly
infiltrative nature of npc and its complex location, which is surrounded by
critical organs such as brainstem, spinal cord, temporal lobes, etc. to improve
the visibility of npc tumor for precise gross-tumor-volume (gtv) delineation,
contrastenhanced mri (ce-mri) is administrated through injection of
gadolinium-based contrast agents (gbcas) during mri scanning. despite the
superior tumor-to-normal tissue contrast of ce-mri, the use of gbcas during mri
scanning can result in a fatal systemic disease known as nephrogenic systemic
fibrosis (nsf) in patients with renal insufficiency [4]. nsf can cause severe
physical impairment, such as joint contractures of fingers, elbows, and knees,
and can progress to involve critical organs such as the heart, diaphragm,
pleura, pericardium, kidney, liver, and lung [5]. it was reported that the
incidence rate of nsf is around 4% after gbca administration in patients with
severe renal insufficiency, and the mortality rate can reach 31% [6]. currently,
there is no effective treatment for nsf, making it crucial to find a ce-mri
alternative for patients at risk of nsf.in recent years, artificial intelligence
(ai), especially deep learning, plays a gamechanging role in medical imaging
[7,8], which showed great potential to eliminate the use of the toxic gbcas
through synthesizing virtual contrast-enhanced mri (vce-mri) from
gadolinium-free sequences, such as t1-weighted (t1w) and t2-weighted (t2w) mri
[9][10][11][12]. in 2018, gong et al. [11] utilized pre-contrast and 10%
low-dose t1w mri to synthesize the vce-mri for brain disease diagnosis using a
u-shape model, they found that gadolinium dose is able to be reduced by 10-fold
by deep learning while the contrast information could be preserved. followed by
their work, kleesiek et al. [10] proposed a bayesian model to explore the
feasibility of synthesizing vce-mri from contrast-free sequences, their study
demonstrated that deep learning is highly feasible to totally eliminate the use
of gbcas. in the area of rt, li et al. [9] developed a multi-input model to
synthesize vce-mri for npc rt. in addition to the advantage of eliminating the
use of gbca, vce-mri synthesis can also speed up the clinical workflow by
eliminating the need for acquiring ce-mri scan, which saves time for both
clinical staff and patients. however, current studies mostly focus on algorithms
development while lack comprehensive clinical evaluations to demonstrate the
efficacy of the synthetic vce-mri in clinical settings.the clinical evaluation
of ai-based techniques is of paramount importance in healthcare. rigorous
clinical evaluations can establish the safety and efficacy of ai-based
techniques, identify potential biases and limitations, and facilitate the
integration of clinical expertise to ensure accurate and meaningful results
[13]. furthermore, the clinical evaluation of ai-based techniques can help
identify areas for improvement and optimization, leading to development of more
effective algorithms.to bridge this bench-to-bedside research gap, in this
study, we conducted a series of clinical evaluations to assess the effectiveness
of synthetic vce-mri in npc delineation, with a particular focus on assessment
in vce-mri image quality and primary gtv delineation. this study has two main
novelties: (i) to the best of our knowledge, this is the first clinical
evaluation study of the vce-mri technique in rt; and (ii) multiinstitutional mri
data were included in this study to obtain more reliable results. the success of
this study would fill the current knowledge gap and provide the medical
community with a clinical reference prior to clinical application of the novel
vce-mri technique in npc rt.",7
1890,Clinical Evaluation of AI-Assisted Virtual Contrast Enhanced MRI in Primary Gross Tumor Volume Delineation for Radiotherapy of Nasopharyngeal Carcinoma,2.1,Data Description,"patient data was retrospectively collected from three oncology centers in hong
kong. this dataset included 303 biopsy-proven (stage i-ivb) npc patients who
received radiation treatment during 2012-2016. the three hospitals were labelled
as institution-1 (110 patients), institution-2 (58 patients), and institution-3
(135 patients), respectively. for each patient, t1w mri, t2w mri,
gadolinium-based ce-mri, and planning ct were retrieved. mri images were
automatically registered as mri images for each patient were scanned in the same
position. the use of this dataset was approved by the institutional review board
of the university of hong kong/hospital authority hong kong west cluster (hku/ha
hkw irb) with reference number uw21-412, and the research ethics committee
(kowloon central/kowloon east) with reference number kc/ke-18-0085/er-1. due to
the retrospective nature of this study, patient consent was waived. for model
development, 288 patients were used for model development and 15 patients were
used to synthesize vce-mri for clinical evaluation. the details of patient
characteristics and the number split for training and testing of each dataset
were illustrated in table 1. prior to model training, mri images were resampled
to 256*224 by bilinear interpolation [14] due to the inconsistent matrix sizes
of the three datasets.",7
1891,Clinical Evaluation of AI-Assisted Virtual Contrast Enhanced MRI in Primary Gross Tumor Volume Delineation for Radiotherapy of Nasopharyngeal Carcinoma,2.2,VCE-MRI Synthesis Network,"the multimodality-guided synergistic neural network (mmgsn-net) was applied to
learn the mapping from t1w mri and t2w mri to ce-mri. the mmgsn-net was a 2d
network. the effectiveness of this network in vce-mri synthesis for npc patients
has been demonstrated by li et al. in [9]. t1w mri and t2w mri were used as
input and corresponding ce-mri was used as learning target. in this work, we
obtained 12806 image pairs for model training. different from the original
study, which used single institutional data for model development and utilized
min-max value of the whole dataset for data normalization, in this work, we used
mean and standard deviation of each individual patient to normalize mri
intensities due to the heterogeneity of the mri intensities across institutions
[15].",7
1892,Clinical Evaluation of AI-Assisted Virtual Contrast Enhanced MRI in Primary Gross Tumor Volume Delineation for Radiotherapy of Nasopharyngeal Carcinoma,2.3,Clinical Evaluations,"the evaluation methods used in this study included image quality assessment of
vce-mri and primary gtv delineation. two board-certified radiation oncologists
(with 8 years' and 6 years' clinical experience, respectively) were invited to
perform the vce-mri quality assessment and gtv delineation according to their
clinical experience. considering the clinical burden of oncologists, 15 patients
were included for clinical evaluations. all clinical evaluations were performed
on an eclipse workstation (v5.0.10411.00, varian medical systems, usa) with the
same monitor, and the window/level can be adjusted freely by the oncologists.
the results were obtained under the consensus of the two oncologists. (i)
distinguishability between ce-mri and vce-mri. to evaluate the reality of
vce-mri, oncologists were invited to differentiate the synthetic patients (i.e.,
image volumes that generated from synthetic vce-mri) from real patients (i.e.,
image volumes that generated from real ce-mri). different from the previous
studies that utilized limited number (20-50 slices, axial view) of 2d image
slices for reality evaluation [9,10], we used 3d volumes in this study to help
oncologists visualize the inter-slice adjacent information. the judgement
results were recorded, and the accuracy of each institution and the overall
accuracy were calculated.",7
1893,Clinical Evaluation of AI-Assisted Virtual Contrast Enhanced MRI in Primary Gross Tumor Volume Delineation for Radiotherapy of Nasopharyngeal Carcinoma,0,Image,"(ii) clarity of tumor-to-normal tissue interface. the clarity of tumor-normal
tissue interface is critical for tumor delineation, which directly affects the
delineation outcomes. oncologists were asked to use a 5-point likert scale
ranging from 1 (poor) to 5 (excellent) to evaluate the clarity of
tumor-to-normal tissue interface. paired two-tailed t-test (with a significance
level of p = 0.05) was applied to analyses if the scores obtained from real
patients and synthetic patients are significantly different. (iii) veracity of
contrast enhancement in tumor invasion risk areas. in addition to the critical
tumor-normal tissue interface, the areas surrounding the npc tumor will also be
considered during delineation. to better evaluate the veracity of contrast
enhancement in vce-mri, we selected 25 tumor invasion risk areas according to
[16], including 13 high-risk areas and 12 medium-risk areas, and asked
oncologists to determine whether these areas were at risk of being invaded
according to the contrast-enhanced tumor regions. the 13 high-risk areas
include: retropharyngeal space, parapharyngeal space, levator veli palatine
muscle, prestyloid compartment, tensor veli palatine muscle, poststyloid
compartment, nasal cavity, pterygoid process, basis of sphenoid bone, petrous
apex, prevertebral muscle, clivus, and foramen lacerum. the 12 medium-risk areas
include foramen ovale, great wing of sphenoid bone, medial pterygoid muscle,
oropharynx, cavernous sinus, sphenoidal sinus, pterygopalatine fossa, lateral
pterygoid muscle, hypoglossal canal, foramen rotundum, ethmoid sinus, and
jugular foramen. the areas considered at risk of tumor invasion were
recorded.the jaccard index (ji) [17] was utilized to quantitatively evaluate the
results of recorded risk areas from ce-mri and vce-mri. the ji could be
calculated by:where r ce and r vce represents the set of risk areas that
recorded from ce-mri and corresponding vce-mri, respectively. ji measures
similarity of two datasets, which ranges from 0% to 100%. higher ji indicates
more similar of the two sets.(iv) efficacy in primary tumor staging. a critical
rt-related application of ce-mri is tumor staging, which plays a critical role
in treatment planning and prognosis prediction [18]. to assess the efficacy of
vce-mri in npc tumor staging, oncologists were asked to determine the stage of
the primary tumor shown in ce-mri and vce-mri. the staging results from ce-mri
were taken as the ground truth and the staging accuracy of vce-mri was
calculated.primary gtv delineation. gtv delineation is the foremost prerequisite
for a successful rt treatment of npc tumor, which demands excellent precision
[19]. an accurate tumor delineation improves local control and reduce toxicity
to surrounding normal tissues, thus potentially improving patient survival [20].
to evaluate the feasibility of eliminating the use of gbca by replacing ce-mri
with vce-mri in tumor delineation, oncologists were asked to contour the primary
gtv under assistance of vce-mri. for comparison, ce-mri was also imported to
eclipse for tumor delineation but assigned as a different patient, which were
shown to oncologists in a random and blind manner.to mimic the real clinical
setting, contrast-free t1w, t2w mri and corresponding ct of each patient were
imported into the eclipse system since sometimes t1w and t2w mri will also be
referenced during tumor delineation. due to both real patients and synthetic
patients were involved in delineation, to erase the delineation memory of the
same patient, we separated the patients to two datasets, each with the same
number of patients, both two datasets with mixed real patients and synthetic
patients without overlaps (i.e., the ce-mri and vce-mri from the same patient
are not in the same dataset).when finished the first dataset delineation, there
was a one-month interval before the delineation of the second dataset. after the
delineation of all patients, the dice similarity coefficient (dsc) [21] and
hausdorff distance (hd) [22] of the gtvs delineated from real patients and
corresponding synthetic patients were calculated to evaluate the accuracy of
delineated contours.dice similarity coefficient (dsc). dsc is a broadly used
metric to compare the agreement between two segmentations [23]. it measures the
spatial overlap between two segmentations, which ranges from 0 (no spatial
overlap) to 1 (complete overlap). the dsc can be expressed as:where c ce and c
vce represent the contours delineated from real patients and synthetic patients,
respectively.",7
1894,Clinical Evaluation of AI-Assisted Virtual Contrast Enhanced MRI in Primary Gross Tumor Volume Delineation for Radiotherapy of Nasopharyngeal Carcinoma,0,Hausdorff Distance (HD).,"even though dsc is a well-accepted segmentation comparison metric, it is easily
influenced by the size of contours. small contours typically receive lower dsc
than larger contours [24].therefore, hd was applied as a supplementary to make a
more thorough comparison. hd is a metric to measure the maximum distance between
two contours. given two contours c ce and c vce , the hd could be calculated
as:where d(x, c vce ) and d(y, c ce ) represent the distance from point x in
contour c ce to contour c vce and the distance from point y in contour c vce to
contour c ce . (i) distinguishability between ce-mri and vce-mri. the overall
judgement accuracy for the mri volumes was 53.33%, which is close to a random
guess accuracy (i.e., 50%). for institution-1, 2 real patients were judged as
synthetic and 1 synthetic patient was considered as real. for institution-2, 2
real patients were determined as synthetic and 4 synthetic patients were
determined as real. for institution-3, 2 real patients were judged as synthetic
and 3 synthetic patients were considered as real. in total, 6 real patients were
judged as synthetic and 8 synthetic patients were judged as real. (ii) clarity
of tumor-to-normal tissue interface. the overall clarity scores of
tumorto-normal tissue interface for real and synthetic patients were 3.67 with a
median of 4 and 3.47 with a median of 4, respectively. no significant difference
was observed between these two scores (p = 0.38). the average scores for real
and synthetic patients were 3.6 and 3, 3.6 and 3.8, 3.8 and 3.6 for
institution-1, institution-2, and institution-3, respectively. 5 real patients
got a higher score than synthetic patients and 3 synthetic patients obtained a
higher score than real patients. the scores of the other 7 patient pairs were
the same. (iii) veracity of contrast enhancement in tumor invasion risk areas.
the overall ji score between the recorded tumor invasion risk areas from ce-mri
and vce-mri was 74.06%. the average ji obtained from institution-1,
institution-2, and institution-3 dataset were similar with a result of 71.54%,
74.78% and 75.85%, respectively. in total, 126 risk areas were recorded from the
ce-mri for all of the evaluation patients, while 10 (7.94%) false positive high
risk invasion areas and 9 (7.14%) false negative high risk invasion areas were
recorded from vce-mri. (iv) efficacy in primary tumor staging. a t-staging
accuracy of 86.67% was obtained using vce-mri. 13 patient pairs obtained the
same staging results. for the institution-2 data, all synthetic patients
observed the same stages as real patients.",7
1895,Clinical Evaluation of AI-Assisted Virtual Contrast Enhanced MRI in Primary Gross Tumor Volume Delineation for Radiotherapy of Nasopharyngeal Carcinoma,3.1,Image Quality of VCE-MRI,"for the two t-stage disagreement patients, one synthetic patient was staged as
phase iv while the corresponding real patient was staged as phase iii, the other
synthetic patient was staged as i while corresponding real patient was staged as
phase iii.",7
1896,Clinical Evaluation of AI-Assisted Virtual Contrast Enhanced MRI in Primary Gross Tumor Volume Delineation for Radiotherapy of Nasopharyngeal Carcinoma,3.2,Primary GTV delineation,"the average dsc and hd between the c ce and c vce was 0.762 (0.673-0.859) with a
median of 0.774, and 1.932 mm (0.763 mm-2.974 mm) with a median of 1.913 mm,
respectively. for institution-1, institution-2, and institution-3, the average
dsc were 0.741, 0.794 and 0.751 respectively, while the average hd were 2.303
mm, 1.456 mm, and 2.037 mm respectively. figure 2 illustrated the delineated
primary gtv contours from an average patient with the dsc of 0.765 and hd of
1.938 mm. the green contour shows the primary gtv that delineated form the
synthetic patient, while the red contour was delineated from corresponding real
gbca-based patient.",7
1897,Clinical Evaluation of AI-Assisted Virtual Contrast Enhanced MRI in Primary Gross Tumor Volume Delineation for Radiotherapy of Nasopharyngeal Carcinoma,4.0,Conclusion,"in this study, we conducted a series of clinical evaluations to validate the
clinical efficacy of vce-mri in rt of npc patients. results showed the vce-mri
has great potential to provide an alternative to gbca-based ce-mri for npc
delineation.",7
1899,Automated CT Lung Cancer Screening Workflow Using 3D Camera,1.0,Introduction,"lung cancer is the leading cause of cancer death in the united states, and early
detection is key to improving survival rates. ct lung cancer screening is a
lowdose ct (ldct) scan of the chest that can detect lung cancer at an early
stage, when it is most treatable. however, the current workflow for performing
ct lung scans still requires an experienced technician to manually perform
pre-scanning steps, which greatly decreases the throughput of this high volume
procedure. while recent advances in human body modeling [4,5,12,13,15] have
allowed for automation of patient positioning, scout scans are still required as
they are used by automatic exposure control system in the ct scanners to compute
the dose to be delivered in order to maintain constant image quality [3].since
ldct scans are obtained in a single breath-hold and do not require any contrast
medium to be injected, the scout scan consumes a significant portion of the
scanning workflow time. it is further increased by the fact that tube rotation
has to be adjusted between the scout and actual ct scan. furthermore, any
patient movement during the time between the two scans may cause misalignment
and incorrect dose profile, which could ultimately result in a repeat of the
entire process. finally, while minimal, the radiation dose administered to the
patient is further increased by a scout scan.we introduce a novel method for
estimating patient scanning parameters from non-ionizing 3d camera images to
eliminate the need for scout scans during pre-scanning. for ldct lung cancer
screening, our framework automatically estimates the patient's lung position
(which serves as a reference point to start the scan), the patient's isocenter
(which is used to determine the table height for scanning), and an estimate of
patient's water equivalent diameter (wed) profiles along the craniocaudal
direction which is a well established method for defining size specific dose
estimate (ssde) in ct imaging [8,9,11,18]. additionally, we introduce a novel
approach for updating the estimated wed in real-time, which allows for
refinement of the scan parameters during acquisition, thus increasing accuracy.
we present a method for automatically aborting the scan if the predicted wed
deviates from real-time acquired data beyond the clinical limit. we trained our
models on a large collection of ct scans acquired from over 60, 000 patients
from over 15 sites across north america, europe and asia. the contributions of
this work can be summarized as follows:-a novel workflow for automated ct lung
cancer screening without the need for scout scan -a clinically relevant method
meeting iec 62985:2019 requirements on wed estimation. -a generative model of
patient wed trained on over 60, 000 patients.-a novel method for real-time
refinement of wed, which can be used for dose modulation",7
1900,Automated CT Lung Cancer Screening Workflow Using 3D Camera,2.0,Method,"water equivalent diameter (wed) is a robust patient-size descriptor [17] used
for ct dose planning. it represents the diameter of a cylinder of water having
the same averaged absorbed dose as the material contained in an axial plane at a
given craniocaudal position z [2]. the wed of a patient is thus a function
taking as input a craniocaudal coordinate and outputting the wed of the patient
at that given position. as wed is defined in an axial plane, the diameter needs
to be known on both the anterior-posterior (ap) and lateral (left-right) axes
noted respectively w ed ap (z) and w ed l (z). as our focus here is on lung
cancer screening, we define 'wed profile' to be the 1d curve obtained by
uniformly sampling the wed function along the craniocaudal axis within the lung
region.our method jointly predicts the ap and lateral wed profiles. while wed
can be derived from ct images, paired ct scans and camera images are rarely
available, making direct regression through supervised learning challenging. we
propose a semi-supervised approach to estimate wed from depth images. first, we
train a wed generative model on a large collection of ct scans. we then train an
encoder network to map the patient depth image to the wed manifold. finally, we
propose a novel method to refine the prediction using real-time scan data.",7
1901,Automated CT Lung Cancer Screening Workflow Using 3D Camera,2.1,WED Latent Space Training,"we use an autodecoder [10] to learn the wed latent space. our model is a fully
connected network with 8 layers of 128 neurons each. we used layer normalization
and relu activation after each layer except the last one. our network takes as
input a latent vector together with a craniocaudal coordinate z and outputs w ed
ap (z) and w ed l (z), the values of the ap and lateral wed at the given
coordinate. in this approach, our latent vector represents the encoding of a
patient in the latent space. this way, a single autodecoder can learn
patient-specific continuous wed functions. since our network only takes the
craniocaudal coordinate and the latent vector as input, it can be trained on
partial scans of different sizes. the training consists of a joint optimization
of the autodecoder and the latent vector: the autodecoder is learning a
realistic representation of the wed function while the latent vector is updated
to fit the data.during training, we initialize our latent space to a unit
gaussian distribution as we want it to be compact and continuous. we then
randomly sample points along the craniocaudal axis and minimize the l1 loss
between the prediction and the ground truth wed. we also apply l2-regularization
on the latent vector as part of the optimization process.",7
1903,Automated CT Lung Cancer Screening Workflow Using 3D Camera,2.3,Real-Time WED Refinement,"while the depth image provides critical information on the patient anatomy, it
may not always be sufficient to accurately predict the wed profiles. for
example, some patients may have implants or other medical devices that cannot be
guessed solely from the depth image. additionally, since the encoder is trained
on a smaller data collection, it may not be able to perfectly project the depth
image to the wed manifold. to meet the strict safety criteria defined by the
iec, we propose to dynamically update the predicted wed profiles at inference
time using real-time scan data. first, we use our encoder network to initialize
the latent vector to a point in the manifold that is close to the current
patient. then, we use our autodecoder to generate initial wed profiles. as the
table moves and the patient gets scanned, ct data is being acquired and ground
truth wed can be computed for portion of the body that has been scanned, along
with the corresponding craniocaudal coordinate. we can then use this data to
optimize the latent vector by freezing the autodecoder and minimizing the l1
loss between the predicted and ground truth wed profiles through gradient
descent. we can then feed the updated latent vector to our autodecoder to
estimate the wed for the remaining portions of the body that have not yet been
scanned and repeat the process.in addition to improving the accuracy of the wed
profiles prediction, this approach can also help detect deviation from real
data. after the latent vector has been optimized to fit the previously scanned
data, a large deviation between the optimized prediction and the ground truth
profiles may indicate that our approach is not able to find a point in the
manifold that is close to the data. in this case, we may abort the scan, which
further reduces safety risks. overall flowchart of the proposed approach is
shown in fig. 1.",7
1904,Automated CT Lung Cancer Screening Workflow Using 3D Camera,3.1,Data,"our ct scan dataset consists of 62, 420 patients from 16 different sites across
north america, asia and europe. our 3d camera dataset consists of 2, 742 pairs
of depth image and ct scan from 2, 742 patients from 6 different sites across
north america and europe acquired using a ceiling-mounted kinect 2 camera. our
evaluation set consists of 110 pairs of depth image and ct scan from 110
patients from a separate site in europe.",7
1905,Automated CT Lung Cancer Screening Workflow Using 3D Camera,3.2,Patient Preparation,"patient positioning is the first step in lung cancer screening workflow. we
first need to estimate the table position and the starting point of the scan. we
propose to estimate the table position by regressing the patient isocenter and
the starting point of the scan by estimating the location of the patient's lung
top.starting position. we define the starting position of the scan as the
location of the patient's lung top. we trained a denseunet [7] taking the camera
depth image as input and outputting a gaussian heatmap centered at the patient's
lung top location. we used 4 dense blocks of 4 convolutional layers for the
encoder and 4 dense blocks of 4 convolutional layers for the decoder. each
convolutional layer (except the last one) is followed by a batch normalization
layer and a relu activation. we trained our model on 2, 742 patients using
adaloss [14] and the adam [6] optimizer with a learning rate of 0.001 and a
batch size of 32 for 400 epochs. our model achieves a mean error of 12.74 mm and
a 95 th percentile error of 28.32 mm. to ensure the lung is fully visible in the
ct image, we added a 2 cm offset on our prediction towards the outside of the
lung. we then defined the accuracy as whether the lung is fully visible in the
ct image when using the offset prediction. we report an accuracy of 100% on our
evaluation set of 110 patients. third and fourth columns show the performance of
our model with real-time refinement every 5 cm and 2 cm respectively. ground
truth is depicted in green and our prediction is depicted in red. while the
original prediction was off towards the center of the lung, the real-time
refinement was able to correct the error.isocenter. the patient isocenter is
defined as the centerline of the patient's body. we trained a densenet [1]
taking the camera depth image as input and outputting the patient isocenter. our
model is made of 4 dense blocks of 3 convolutional layers. each convolutional
layer (except the last one) is followed by a batch normalization layer and a
relu activation. we trained our model on 2, 742 patients using adadelta [16]
with a batch size of 64 for 300 epochs. on our evaluation set, our model
outperforms the technician's estimates with a mean error of 5.42 mm and a 95 th
percentile error of 8.56 mm compared to 6.75 mm and 27.17 mm respectively.
results can be seen in fig. 2.",7
1906,Automated CT Lung Cancer Screening Workflow Using 3D Camera,3.3,Water Equivalent Diameter,"we trained our autodecoder model on our unpaired ct scan dataset of 62, 420
patients with a latent vector of size 32. the encoder was trained on our paired
ct scan and depth image dataset of 2, 742 patients. we first compared our method
against a simple direct regression model. we trained a denseunet [7] taking the
camera depth image as input and outputting the water equivalent diameter
profile. we trained this baseline model on 2, 742 patients using the adadelta
[6] optimizer with a learning rate of 0.001 and a batch size of 32. we table 1.
wed profile errors on our testing set (in mm). 'w' corresponds to the portion
size of the body that gets scanned before updating the prediction (in cm). top
of the table corresponds to lateral wed profile, bottom corresponds to ap wed
profile. updating the prediction every 20 mm produces the best results.",7
1907,Automated CT Lung Cancer Screening Workflow Using 3D Camera,0,Method (lateral),"mean then measured the performance of our model before and after different
degrees of real-time refinement, using the same optimizer and learning rate. we
report the comparative results in table 1.we observed that our method largely
outperforms the direct regression baseline with a mean lateral error 40% lower
and a 90 th percentile lateral error over 30% lower. bringing in real-time
refinement greatly improves the results with a mean lateral error over 40% and a
90 th percentile lateral error over 20% lower than before refinement. ap
profiles show similar results with a mean ap error improvement of nearly 40% and
a 90 th percentile ap error improvement close to 30%. when using our proposed
method with a 20 mm window refinement, our proposed approach outperforms the
direct regression baseline by over 60% for lateral profile and nearly 80% for
ap.figures 3 highlights the benefits of using real-time refinement. overall, our
approach shows best results with an update frequency of 20 mm, with a mean
lateral error of 15.93 mm and a mean ap error of 10.40 mm. figure 4 presents a
qualitative evaluation on patients with different body morphology.finally, we
evaluated the clinical relevancy of our approach by computing the relative error
as described in the international electrotechnical commission (iec) standard iec
62985:2019 on methods for calculating size specific dose estimates (ssde) for
computed tomography [2]. the δ rel metric is defined as:where:-ŵ ed(z) is the
predicted water equivalent diameter -w ed(z) is the ground truth water
equivalent diameter z is the position along the craniocaudal axis of the
patient. iec standard states the median value of the set of δ rel (z) along the
craniocaudal axis (noted δ rel ) should be below 0.1. our method achieved a mean
lateral δ rel error of 0.0426 and a mean ap δ rel error of 0.0428, falling well
within the acceptance criteria.",7
1908,Automated CT Lung Cancer Screening Workflow Using 3D Camera,4.0,Conclusion,"we presented a novel 3d camera based approach for automating ct lung cancer
screening workflow without the need for a scout scan. our approach effectively
estimates start of scan, isocenter and water equivalent diameter from depth
images and meets the iec acceptance criteria of relative wed error. while this
approach can be used for other thorax scan protocols, it may not be applicable
to trauma (e.g. with large lung resections) and inpatient settings, as the
deviation in predicted and actual wed would likely be much higher. in future, we
plan to establish the feasibility as well as the utility of this approach for
other scan protocols and body regions.1",7
1909,Clustering Disease Trajectories in Contrastive Feature Space for Biomarker Proposal in Age-Related Macular Degeneration,1.0,Introduction,"age-related macular degeneration (amd) is the leading cause of blindness in the
elderly, affecting nearly 200 million people worldwide [24]. patients with early
stages of the disease exhibit few symptoms until suddenly converting to the late
stage, at which point their central vision rapidly deteriorates [12]. clinicians
currently diagnose amd, and stratify patients, using biomarkers derived from
optical coherence tomography (oct), which provides high-resolution images of
fig. 1. our method finds common patterns of disease progression in datasets of
longitudinal images. we partition time series into sub-trajectories before
introducing a clinically motivated distance function to cluster the
sub-trajectories in feature space. the clusters are then assessed by
ophthalmologists on their interpretability and ability to capture the
progression of amd.the retina. however, the widely adopted amd grading system
[7,13], which coarsely groups patients into broad categories for early and
intermediate amd, only has limited prognostic value for late amd. clinicians
suspect that this is due to the grading system's reliance on static biomarkers
that are unable to capture temporal dynamics which contain critical information
for assessing progression risk.in their search for new biomarkers, clinicians
have annotated known biomarkers in longitudinal datasets that monitor patients
over time and mapped them against disease progression [2,16,19]. this approach
is resource-intensive and requires biomarkers to be known a priori. others have
proposed deep-learningbased methods to discover new biomarkers at scale by
clustering oct images or detecting anomalous features [17,18,23]. however, these
approaches neglect temporal relationships between images and the obtained
biomarkers are by definition static and cannot capture the dynamic nature of the
disease.our contribution: in this work, we present a method to automatically
propose biomarkers that capture temporal dynamics of disease progression in
longitudinal datasets (see fig. 1). at the core of our method is the novel
strategy to represent patient time series as trajectories in a latent feature
space. individual progression trajectories are partitioned into atomic
sub-sequences that encode transitions between disease states. then, we identify
population-level patterns of amd progression by clustering these
sub-trajectories using a newly introduced distance metric that encodes three
distinct temporal criteria. in experiments involving 160,558 retinal scans, four
ophthalmologists verified that our method identified several candidates for
temporal biomarkers of amd. moreover, our clusters demonstrated greater
prognostic value for late-stage amd when compared to the widely adopted amd
grading system.",7
1911,Clustering Disease Trajectories in Contrastive Feature Space for Biomarker Proposal in Age-Related Macular Degeneration,3.1,OCT Image Datasets,"we use two retinal oct datasets curated in the scope of the pinnacle study [20].
we first design and test our method on a development dataset, which was
collected from the southampton eye unit. afterwards, we test our method on a
second independent unseen dataset, which was obtained from moorfields eye
hospital. all images were acquired using topcon 3d oct devices (topcon
corporation, tokyo, japan). after strict quality control, the development
dataset consists of 46,496 scans of 6,236 eyes from 3,456 patients. eyes were
scanned 7.7 times over 1.9 years on average at irregular time intervals. the
unseen dataset is larger, containing 114,062 scans of 7,253 eyes from 3,819
patients. eyes were scanned 16.6 times over 3.5 years on average. a subset of
1,031 longitudes was labelled using the established amd grading protocols
derived from known imaging biomarkers. early amd was characterised by small
drusen between 63-125µm in diameter. we also recorded cnv, crora (≥ 250µm and
<1000µm), crora (≥ 1000µm) [13] and healthy cases with no visible biomarkers.
visual acuity scores, which measured the patient's functional quality of vision
using a logmar chart, are available at 83,964 time points.",7
1913,Clustering Disease Trajectories in Contrastive Feature Space for Biomarker Proposal in Age-Related Macular Degeneration,3.3,Extracting Sub-trajectories via Partitioning,"naively clustering whole time series of patients ignores two characteristics of
longitudinal data. firstly, individual time series are not directly comparable
as patients enter and leave the study at different stages of their overall
progression.secondly, longer time series can record multiple successive
transitions in disease stage. inspired by traclus [11], the state of the art in
trajectory clustering, we adapt their partition-and-group framework by assuming
that trajectories can be partitioned into a common set of sub-trajectories that
capture singular transitions between progressive states of the disease.for each
eye, we first form piecewise-linear trajectories by linking points in feature
space that were derived from consecutively acquired oct images. we then extract
sub-trajectories by finding all sequences of images spanning 1.0 ± 0.5 years of
elapsed time within each trajectory. next, to avoid oversampling trajectories
with a shorter time interval between images, we randomly sample at most one
sub-trajectory in every 0.5-year time interval.",7
1914,Clustering Disease Trajectories in Contrastive Feature Space for Biomarker Proposal in Age-Related Macular Degeneration,3.4,Sub-trajectory Distance Functions and Clustering,"in order to find common patterns of disease progression among sub-trajectories
we cluster them. to this end we introduce a new distance function between
subtrajectories that incorporates three distinct temporal criteria (see fig. 2).
the first, formulated in eq. 2, matches two sub-trajectories, u and v , of
patients who progress between the same start and end states:since all
sub-trajectories cover a similar temporal duration, d transition also
differentiates between fast and slow progressors and stable periods of no
progression. however, by ignoring intermediary images, this metric does not
respect the disease pathway along which patients progress. to incorporate this,
we include a second metric that measures path dissimilarity, calculated using
dynamic time warping (dtw) [4,14,15]. dtw finds the optimal temporal alignment
between two time series before computing their distance. this re-alignment
allows us to match sub-trajectories that traverse the same disease states in the
same order, irrespective of the rate of change between states. we combine d
transition with dtw using a λ ∈ r, 0 ≤ λ ≤ 1 coefficient so the overall distance
between u and v isthe third and final temporal criteria is to match time series
that progress in the same relative direction, regardless of absolute disease
states. we weight the contribution of this with φ ∈ r, 0 ≤ φ ≤ 1 in eq.
4:spectral clustering: as the non-linearity of d subtraj prohibits the use of
k-means for clustering, we instead use spectral clustering [22] to group similar
sub-trajectories. hereby, we construct an affinity matrix a encoding the
negative of the distance d subtraj between all pairs of sub-trajectories. using
a, we group sub-trajectories into k clusters.",7
1915,Clustering Disease Trajectories in Contrastive Feature Space for Biomarker Proposal in Age-Related Macular Degeneration,3.5,Qualitative and Quantitative Evaluation of Clusters,"initially, we tune the hyperparameters, λ, φ and k, on the development dataset
by heuristically selecting values that result in higher uniformity between
subtrajectories within each cluster. two teams of two ophthalmologists then
review 20 sub-trajectories from distinct patients in each cluster, interpreting
and summarising any consistently observed temporal dynamics. next, using the
same hyperparameters we apply the method directly to the unseen dataset. the
ophthalmologists then review these clusters and confirm whether they capture the
same temporal biomarkers observed in the development dataset.in addition to the
qualitative evaluation, we also validate the utility of our clusters as
biomarkers that stratify risk of disease progression. we test this by predicting
the time until conversion to late amd and its subtypes, cnv and crora.
additionally, we predict current visual acuity. for prediction, each
sub-trajectory is characterised by a vector of size k that encodes proportional
similarity to each cluster. this vector is then used by a lasso linear
regression model. similarly, we fit an equivalent linear regression model to the
static biomarkers from the established grading system detailed in sect. 3.1. we
also include a demographic baseline using age and sex. we also add a temporally
agnostic baseline that clusters only single time points. finally, to demonstrate
the performance gap between our interpretable approach and black-box supervised
learning algorithms, we include a fully supervised deep learning baseline by
fitting an svr directly to the feature space. each experiment uses 10-fold cross
validation on random 80/20 partitions, while ensuring a patient-wise split.
finally, we repeat the entire method, starting from sub-trajectory extraction,
followed by clustering and then regression experiments, using 7 random seeds and
report the means and standard deviations.",7
1916,Clustering Disease Trajectories in Contrastive Feature Space for Biomarker Proposal in Age-Related Macular Degeneration,4.0,Experiments and Results,"sub-trajectory clusters are candidate temporal biomarkers: by first applying our
method to the development dataset we found that using λ = 0.75, φ = 0.75 and k =
30 resulted in the most uniform and homogeneous clusters while still limiting
the total number of clusters to a reasonable amount. achieving the same cluster
quality with smaller values of φ required many more clusters in order to encode
all combinations of possible start and end disease states. the expert
ophthalmologists remarked that many of the identified clusters capture dynamics
that have already been linked to the progression of amd, even though they are
not currently included in any clinical grading system. using the same
hyperparameters our method generalised to the unseen dataset which yielded
clusters with equivalent dynamics and quality (see fig. 3). ophthalmologists
identified clusters capturing the same variants of temporal progression in both
datasets. they named these as 'rapid growth of drusen pigment epithelial
detachments (ped)', 'regression of drusen ped', 'development of subretinal fluid
', 'development of intraretinal fluid ', 'development of hypertransmission' and
'stable state' (no signs of progression at each disease state).sub-trajectory
clusters predict conversion to late amd: next, we validated that our clusters
are predictive of progression to late amd. our subtrajectory clusters were
comparable to, and in some cases outperformed, the current widely adopted
grading system in predicting risk of conversion (see table 1). in all tasks the
standard biomarkers are only marginally more indicative of risk than the
patient's age and sex. this experiment confirms that our clusters are related to
disease progression.",7
1917,Clustering Disease Trajectories in Contrastive Feature Space for Biomarker Proposal in Age-Related Macular Degeneration,5.0,Discussion and Conclusion,"motivated to improve inadequate grading systems for amd that do not incorporate
temporal dynamics we developed a method to automatically propose biomarkers that
are time-dependent, interpretable, and predictive of conversion to late-stage
amd. we applied our method to two large longitudinal datasets, cataloguing 3,218
total years of disease progression. the found time-dependent clusters were
subsequently interpreted by four ophthalmologists. they found them to capture
distinct patterns of disease progression that have been previously linked to
amd, but are not currently included in clinical grading systems. furthermore, we
experimentally demonstrated that the found clusters predict conversion to
late-stage amd on par with the established grading system.in the future,
biomarkers identified by our method can be further refined by clinicians. we
will also use the full volumetric image to model progression dynamics outside
the macular. as late stage patients were overrepresented in our datasets, we
also intend to apply our method to datasets with greater numbers of patients
progressing from earlier disease stages. ultimately, we envision that proposals
from our method may inform the next generation of grading systems for amd that
incorporate the temporal dimension intrinsic to this dynamic disease.",7
1932,Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,1.0,Introduction,"esophageal cancer is a significant contributor to cancer-related deaths globally
[3,15]. one effective treatment option is radiotherapy (rt), which utilizes
high-energy radiation to target cancerous cells [4]. to ensure optimal treatment
outcomes, both the cancerous region and the adjacent organ-at-risk (oar) must be
accurately delineated, to focus the high-energy radiation solely on the
cancerous area while protecting the oars from any harm. gross tumor volume (gtv)
represents the area of the tumor that can be identified with a high degree of
certainty and is of paramount importance in clinical practice.in the clinical
setting, patients may undergo a second round of rt treatment to achieve complete
tumor control when initial treatment fails to completely eradicate cancer [16].
however, the precise delineation of the gtv is laborintensive, and is restricted
to specialized hospitals with highly skilled rt experts. the automatic
identification of the esophagus presents inherent challenges due to its
elongated soft structure and ambiguous boundaries between it and adjacent organs
[12]. moreover, the automatic delineation of the gtv in the esophagus poses a
significant difficulty, primarily attributable to the low contrast between the
esophageal gtv and the neighboring tissue, as well as the limited
datasets.recently, advances in deep learning [21] have promoted research in
automatic esophageal gtv segmentation from computed tomography (ct) [18,19].
since the task is challenging, jin et al. [9,10] improve the segmentation
accuracy by incorporating additional information from paired positron emission
tomography (pet). nevertheless, such approaches require several imaging
modalities, which can be both costly and time-consuming, while disregarding any
knowledge from previous treatment or anatomical understanding. moreover, the
correlation between the first and second courses of rt is rarely investigated,
where detailed prior tumor information naturally exists in the previous rt
planning.in this paper, we present a comprehensive study on accurate gtv
delineation for the second course rt. we proposed a novel prior anatomy and rt
information enhanced second-course esophageal gtv segmentation network (artseg).
a region-preserving attention module (ram) is designed to effectively capture
the long-range prior knowledge in the esophageal structure, while preserving
regional tumor patterns. to the best of our knowledge, we are the first to
reveal the domain gap between the first and second courses for gtv segmentation,
and explicitly leverage prior information from the first course to improve gtv
segmentation performance in the second course.the medical images are labeled
sparsely, which are isolated by different tasks [20]. meanwhile, an ideal method
for automatic esophageal gtv segmentation in the second course of rt should
consider three key aspects: 1) changes in tumor volume after the first course of
rt, 2) the proliferation of cancerous cells from a tumor to neighboring healthy
cells, and 3) the anatomical-dependent our training approach leverages
multi-center datasets containing relevant annotations, that challenges the
network to retrieve information from e1 using the features from e2. the decoder
d utilizes the prior knowledge obtained from i1 and g1 to generate the mask
prediction. our training strategy leverages three datasets that introduce prior
knowledge to the network of the following three key aspects: 1) tumor volume
variation, 2) cancer cell proliferation, and 3) reliance of gtv on esophageal
anatomy.nature of gtv on esophageal locations. to achieve this, we efficiently
exploit knowledge from multi-center datasets that are not tailored for
second-course gtv segmentation. our training strategy does not specific to any
tasks but challenges the network to retrieve information from another encoder
with augmented inputs, which enables the network to learn from the above three
aspects. extensive quantitative and qualitative experiments validate our
designs.",7
1933,Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,2.0,Network Architecture,"in the first course of rt, a ct image denoted as i 1 is utilized to manually
delineate the esophageal gtv, g 1 . during the second course of rt, a ct image i
2 of the same patient is acquired. however, i 2 is not aligned with i 1 due to
soft tissue movement and changes in tumor volume that occurred during the first
course of treatment. both images i 1/2 have the spatial shape of h × w × d.our
objective is to predict the esophageal gtv g 2 of the second course. it would be
advantageous to leverage insights from the first course, as it comprises
comprehensive information pertaining to the tumor in its preceding phase.
therefore, the input to encoder e 1 consists of the concatenation of i 1 and g 1
to encode the prior information (features f d 1 ) from the first course, while
encoder e 2 embeds both low-and high-level features f d 2 of the local pattern
of i 2 (fig. 1),where the spatial shape of, with 2 d+4 channels.
region-preserving attention module. to effectively learn the prior knowledge in
the elongated esophagus, we design a region-preserving attention module (ram),
as shown in fig. 1. the multi-head attention (mha) [17] is employed to gather
long-range informative values in f d 1 with f d 2 as queries and f d 1 as keys.
the features f d 1/2 are reshaped to hw d 2 3d × c before passed to the mha,
where c is the channel dimension. the attentive features f d a can be formulated
as:since mha perturbs the positional information, we preserve the tumor local
patterns by concatenating original features to the attentive features at the
channel dimension, followed by a 1 × 1 × 1 bottleneck convolution ξ 1×1×1 to
squeeze the channel features (named as ram), as shown in the following
equations,where the lower-level features from both encoders are fused by
concatenation.the decoder d generates a probabilistic prediction) with skip
connections (fig. 1). we utilize the 3d dice [14] loss function,",7
1938,Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,4.1,Experimental Setup,"datasets. the paired first-second course dataset, s p , is collected from sun
yat-sen university cancer center (ethics approval number: b2023-107-01),
comprising paired ct scans of 69 distinct patients from south china. we
collected the gtv dataset s v from medmind technology co., ltd., which has ct
scans from 179 patients. for both s p and s v , physicians annotated the
esophageal cancer gtv in each ct. the gtv volume statistics (cm 3 , mean ± std.)
in s v is 40.60 ± 29.75, and is 83.70 ± 55.97/71.66 ± 49.36 for the first/second
course rt in s p respectively. additionally, we collect s e from segthor [12],
consisting of ct scans and esophagus annotations from 40 patients who did not
implementation details. the ct volumes from the first and second course in s p
are aligned based on the center of the lung mask [8]. the ct volumes are applied
with a windowing of [-100, 300] hu, and resampled to 128 3 , with a voxel size
of 1.2 × 1.2 × 3 mm 3 . the augmentations p 1/2 involve a combination of random
3d resized cropping, flipping, rotation in the transverse plane, and gaussian
noise. we employ the adam [11] optimizer with (β 1 , β 2 , lr) = (0.9, 0.999,
0.001) for training for 500 epoches. the network is implemented using pytorch
[2] and monai [1], and detailed configurations are in the supplementary
material. experiments are performed on an nvidia rtx 3090 gpu with 24gb
memory.performance metrics. dice score (dsc), averaged surface distance (asd)
and hausdorff distance (hsd) are used as metrics for evaluation. the wilcoxon
signed-rank test is used to compare the performance of different methods.",7
1939,Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,4.2,Domain Gap Between the First and Second Course,"as previously mentioned, the volume of the tumors changes after the first course
of rt. to demonstrate the presence of a domain gap between the first and second
courses, we train sota methods with datasets s train p and s v , by feeding the
data sequentially into the network. we then evaluate the models on s test p .
the results presented in table 1 indicate a performance gap between gtv
segmentation in the first and second courses, with the latter being more
challenging. notably, the paired first-second course dataset s test p pertains
to the same group of patients, thereby ensuring that any performance drop can be
attributed solely to differences in courses of rt, rather than variations across
different patients.figure 2 illustrates the reduction in the gtv area after the
initial course of rt, where the transverse plane is taken from the same location
relative to the vertebrae (yellow lines). the blue arrows indicate that the
networks failed to track these changes and produced false predictions in the
second course of rt. this suggests that deep learning-based approaches may not
rely solely on the identification of malignant tissue patterns, as doctors do,
but rather predict highrisk areas statistically. therefore, for accurate
second-course gtv segmentation, we need to explicitly propagate prior
information from the first course using dual encoders in artseg, and incorporate
learning about tumor changes.",7
1948,CLIP-Lung: Textual Knowledge-Guided Lung Nodule Malignancy Prediction,3.1,Dataset and Implementation Details,"dataset. lidc-idri [1] is a dataset for pulmonary nodule classification or
detection based on low-dose ct, which involves 1,010 patients. according to the
annotations, we extracted 2, 026 nodules, and all of them were labeled with
scores from 1 to 5, indicating the malignancy progression. we cropped all the
nodules with a square shape of a doubled equivalent diameter at the annotated
center, then resized them to the volume of 32 × 32 × 32. following [9,11], we
modified the first layer of the image encoder to be with 32 channels. according
to existing works [11,18], we regard a nodule with an average score between 2.5
and 3.5 as unsure nodules, benign and malignant categories are those with scores
lower than 2.5 and larger than 3.5, respectively. in this paper, we construct
three sub-datasets: lidc-a contains three classes of nodules both in training
and test sets; according to [11], we construct the lidc-b, which contains three
classes of nodules only in the training set, and the test set contains benign
and malignant nodules; lidc-c includes benign and malignant nodules both in
training and test sets.experimental settings. in this paper, we apply the clip
pre-trained vit-b/16 as the text encoder for clip-lung, and the image encoder we
used is resnet-18 [6] due to the relatively smaller scale of training data. the
image encoder is initialized randomly. note that for the text branch, we froze
the parameters of the text encoder and updated the learnable tokens l and l
during training. the learning rate is 0.001 following the cosine decay, while
the optimizer is stochastic gradient descent with momentum 0.9 and weight decay
0.00005. the temperature τ is initialized as 0.07 and updated during training.
all of our experiments are implemented with pytorch [15] and trained with nvidia
a100 gpus. the experimental results are reported with average values through
five-fold cross-validation. we report the recall and f1-score values for
different classes and use ""±"" to indicate standard deviation.",7
1964,Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,1.0,Introduction,"computer-aided diagnosis (cad) systems have achieved success in many clinical
tasks [5,6,12,17]. most cad studies were developed on regular and selected
datasets in the laboratory environment, which avoided the problems (data noise,
missing data, etc.) in the clinical scenarios [3,6,9,13,18]. in a real clinical
scenario, the clinicians generally synthesize all aspects of information, and
conduct consultations with multidisciplinary team (mdt), to accurately diagnose
and plan the treatment [9,10,13]. real-world studies have received increasing
attention [11,16], and it is challenging for the cad in the real-world scenarios
as: 1) consistent with the clinical workflow, cad needs to consider
multidisciplinary information to obtain multidimensional diagnosis; 2) due to
information collection, storage and manual evaluation, there are missing and
noisy medical data. this phenomenon is especially common in rare tumors like
pancreatic neuroendocrine neoplasms (pnens).in order to overcome above
challenges, some studies [3,9,13,18] used multilabel method because of the
following advantages: 1) the input of the model is only a single modality such
as images, which is easy to apply clinically; 2) the model learns multi-label
and multi-disciplinary knowledge, which is consistent with clinical logic; 3)
multi-label simultaneous prediction, which meets the need of clinical
multi-dimensional description of patients. for the above advantages, multi-label
technology is suitable for real-world cad. the previous multi-label cad studies
were designed based on simple parameter sharing methods [9,15,20] or graph
neural network (gnn) method [2]. the former implicitly interacts with
multi-label information, making it difficult to fully utilize the correlation
among labels; and the latter requires the use of word embeddings pre-trained on
public databases, which is not friendly to many medical domain proper nouns. the
generalizability of previous multi-label cad studies is poor due to these
disadvantages. in addition, none of the current multi-label cad studies have
considered the problem of missing labels and noisy labels.considering these
real-world challenges, we propose a multi-label model named self-feedback
transformer (sft), and validate our method on a realworld pnens dataset. the
main contributions of this work are listed: 1) a transformer multi-label model
based on self-feedback mechanism was proposed, which provided a novel method for
multi-label tasks in real-world medical application; 2) the structure is
flexibility and interactivity to meet the needs of realworld clinical
application by using four inference modes, such as expert-machine combination
mode, etc.; 3) sft has good noise resistance, and can maintain good performance
under noisy label input in expert-assisted mode.",7
1969,Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,3.1,Dataset and Evaluation,"real-world pnens dataset. we validated our method on a real-world pnens dataset
from two centers. all patients with arterial phase computed tomography (ct)
images were included. the dataset contained 264 and 28 patients in center 1 and
center 2, and a senior radiologist annotated the bounding boxes for all 408 and
28 lesions. we extracted 37 labels from clinical reports, including survival,
immunohistochemical (ihc), ct findings, etc. among them, 1)recist drug response
(rs), 2)tumor shrink (ts), 3)durable clinical benefit (dcb), 4)progression-free
survival (pfs), 5)overall survival (os), 6)grade (gd), 7)somatostatin receptor
subtype 2(sstr2), 8)vascular endothelial growth factor receptor 2 (vefgr2),
9)o6-methylguanine methyltransferase (mgmt), 10)metastatic foci (mtf), and
11)surgical recurrence (rt) are main tasks, and the remaining are auxiliary
tasks. 143 and 28 lesions were segmented by radiologists, and the radiomics
features of them were extracted, of which 162 features were selected and
binarized as auxiliary tasks because of its statistically significant
correlation with the main labels. the label distribution and the overlap ratio
(jaccard index) of lesions between pairs of labels are shown in fig. 3. it is
obvious that the real-world dataset has a large number of labels with randomly
missing data, thus, we used an adjusted 5-fold cross-validation. taking a
patient as a sample, we chose the dataset from center 1 as the internal dataset,
of which the samples with most of the main labels were used as dataset 1 (219
lesions) and was split into 5 folds, and the remaining samples are randomly
divided into the training set dataset 2 (138 lesions) and the validation set
dataset 3 (51 lesions), the training set and the validation set of the
corresponding folds were added during cross-validation, respectively. all
samples in center 2 left as external test set. details of each dataset are in
the supplementary material. dataset evaluation metrics. we evaluate the
performance of our method on the 10 main tasks for internal dataset, and due to
missing labels and too few sstr2 labels, only the performance of predicting rt,
pfs, os, gd, mtf are evaluated for external dataset. we employ accuracy (acc),
sensitivity (sen), specificity (spc), f1-score (f1) and area under the receiver
operating characteristic (auc) for each task, and compute the mean value of them
(e.g. mauc).",7
1990,Physics-Informed Conditional Autoencoder Approach for Robust Metabolic CEST MRI at 7T,2,Methods,"data measurements. cest imaging was performed in seven subjects, including two
glioblastoma patients, after written informed consent was obtained to
investigate the dependence of cest effects on b 1 in brain tissue. the local
ethics committee approved the study. all volunteers were measured at three b 1
field strengths 0.72 μt, 1.0 μt, and 1.5 μt. a method as described by mennecke
et al. [9] was used to acquire cest data on a 7t whole-body mri system
(magne-tom terra, siemens healthcare gmbh, erlangen, germany). saturated images
were obtained for 54 non-equidistant frequency offsets ranging from -100 ppm to
+100 ppm. the acquisition time per b 1 level was 6:42 min. the acquisition of
the b 1 map required an additional 1:06 min.conditional autoencoder. we
developed a conditional autoencoder (cae) to solve the b 1 inhomogeneity
problem, which is essential for the generation of metabolic cest contrast maps
at 7t. the left part of fig. 2 describes the cae. the encoding network of cae
took the raw cest-spectrum and the corresponding effective b 1 value as input
and generate a latent space that was concatenated once with the same b 1 input
value and passed to the decoder that reconstruct the uncorrected b 1
cest-spectrum, and another time the latent space was concatenated with the
desired/specific effective b 1 value to reconstruct the cest-spectrum at a
specific b 1 saturation amplitude. both decoders shared the weights (cf. fig.
2). for the development of the cae networks, we used the well-known fully
concatenated (fc) layers with leaky relu activations except for the last layer
of decoder, which had a linear activation. the encoder and decoder both
consisted of 4 layers, where the layers of the encoder successively contain 128,
128, 64, 32 neurons, while the layers of the decoder successively contain 32,
64, 128, 128 neurons. the input, latent space and output layers had 55, 17 and
54 neurons respectively.physics-informed autoencoder. the lorentzian model and
its b 1dispersion can be derived from the underlying spin physics described by
the bloch-mcconnell equation system [8]. the physics-informed autoencoder (piae)
utilized fully connected nn as encoder and lorentzian distribution generator as
a decoder to perform the pixel-wise 5-pool lorentzian curve fit to the
cest-spectrum (water, amide, amine, noe, mt) [14]. the 5-pool model was
described aswhere l denotes the lorentz function. the direct saturation pool
(water) was defined as(the remaining other four pools were defined as) 2 , i ∈
amide, amine, rn oe, ssm t .(the right part of fig. 2 describes the piae. the
encoder of piae mapped the cest-spectrum to the amplitudes a i , the full width
half maximum (fwhm) τ i , and the water peak position δ ds of the 5-pool
lorentzian model. its encoder consisted of four fc layers, each with 128 neurons
with leaky relu activations. it had three so-called fc latent space layers with
linear activation for position and exponential activations for fwhm and
amplitudes of 5-pool lorentzian model. the positions of amide, rnoe, ssmt, and
amine were fixed at 3.5 ppm, -3.5 ppm, -3 ppm, and 2 ppm, respectively, and
shifted with respect to the predicted position of the water peak. the decoder of
piae consisted of a lorentzian distribution generator (cf. fig. 2). it generated
samples of the 5-pool distributions exactly at the offsets δω (i.e. between -100
ppm and 100 ppm) where the input cest-spectrum was sampled, and combined them
according to eq. 1 to generate the input cest spectrum with or without b 0
correction.bound loss. the peak positions δ i and widths τ i of the pools had to
be within certain bounds so that certain neurons in the latent space layer of
piae would not be exchanged and provide the same pool parameters for all
samples. we developed a simple cost function along the lines of the hinge loss
[12], called the bound loss. mathematically, it is defined as followsthe bound
loss increases linearly as the output of the latent space neurons of piae
exceeds or recede from the boundaries. the lower and upper limits for positions
and widths are given in table 1 of the supplementary material.training and
evaluation. four healthy volunteers formed the training and validation sets. the
test set consisted of the two tumor patients and one healthy subject. to ensure
that the outcomes were exclusively based on the cest-spectrum and not influenced
by spatial position, the training was carried out voxel-by-voxel. consequently,
there were approximately one million cestspectra for the training process. cae
was first trained with mse loss. in this step, the cae encoder was fed with the
cest-spectrum of a specific b 1 saturation amplitude, and it generated two
cest-spectra, one for the input b 1 saturation level and the other for the b 1
level injected into the latent space (cf. fig. 2). later, it was trained with a
combination of mse loss and perception loss (mse loss between the latent space
of the cest-spectra at two different b 1 levels). to incorporate perception
loss, we used two forward passes with two different b 1 cest-spectra and used
perception loss to generate a latent space that is independent of b 1 saturation
amplitude. the following equation describes the loss of the second step.piae, on
the other hand, was trained with a combination of mse loss and bound loss. the
piae loss was described as followsfor evaluation we input the uncorrected
cest-spectrum acquired at 1μt and generated corrected cest-spectra at b 1 0.5,
0.72, 1.0, 1.3, 1.5 μt. piae encoder yielded the amplitudes of 5-pool for b 1
corrected cest-spectrum. its decoder reconstructed the b 1 b 0 fitted
cest-spectrum. the b 0 correction simply refers to the shift of the position of
the water peak to 0 ppm.cest quantification. the multi-b 1 cest-spectra allow
quantification of cest effects (amide, rnoe, amine) [14,15] down to the exchange
rate and concentration. the amplitudes of the cest contrasts were expressed
according to the definition in [15] as followswhere f i , k i , and r 2i express
the concentrations, exchange rates, and relaxation rates of the pools. z ref
defines the sum of all 5 distributions at the resonance frequency of the
specific pool in b 1 b 0 corrected cest-spectrum and w 1 is the frequency of the
oscillating field.the amplitudes of cest contrasts in the lorentzian function
have the b 1 dispersion function given by the labeling efficiency α (eq. 7). the
exchange rate occurs here separately from the concentration, which allows their
quantification via the b 1 dispersion. concentration and exchange rate were
fitted as a product and denoted as z 1 (quantified maps), and k(k+r 2 ) was also
fitted with the single term z 2 using trust-region reflective least squares
[10].",8
1991,Physics-Informed Conditional Autoencoder Approach for Robust Metabolic CEST MRI at 7T,3,Results,"the comparison of picae with the conventional method [9,14] is shown in columns
1 and 2 of fig. 3. the top image in column 3 shows the t 1 -weighted reference
image enhanced with the exogenous contrast agent gadolinium (gd-t 1 w), and the
bottom image shows the b 1 -map. the tumor shows a typical so called gadolinium
ring enhancement indicated by the arrow (a 15 ), which is also visible in the
non-invasive and gadolinium-free cest contrast maps (columns 1 and 2). the
picae-cest maps showed better visualization of this tumor feature compared to
the conventional method. the proposed method yielded at least 25% increase in
the structural similarity index (ssim) with the gd-t 1 w image for the ring
enhancement region. the contrast maps also appear less noisy and more
homogeneous over the whole brain compared to the lorentzian fit on the
interpolated-corrected b 1 cest-spectra [14]. to further evaluate the
performance of piae and cae, we b1-corrected the data using cae and fitted it
with the least squares method (cae-lorentzian fit). the comparison of the cest
maps produced by the conventional lorentzian fit, the cae-lorentzian fit, and
picae is shown in table 1 using ssim and gradient cross correlation (gcc) [11]
for the tumor ring region. both the cae-lorentzian fit and picae were better
than the conventional method. cae-lorentzian fit even outperformed picae for
rnoe metabolic map and has similar performance for amide, but it has much lower
performance for amine. the ability of picae to produce b 1 -robust cest maps at
arbitrary levels is shown in fig. 4, where different b 1 levels reveal different
features of the heterogenous tumor. quantification of chemical exchange rates
and concentration, i.e., z 1 = f•k, is shown in column 4. z 1 (quantified maps)
further improve the visualization of the ring enhancement area. column 5 shows
the z 2 maps, which are combination of the exchange rate k and the relaxation
rate r 2 . quantified maps of amide, rnoe and amine for another tumor patient is
shown in supplementary fig. 1. the accuracy of the cae to generate particular b
1 cestspectra is depicted using absolute error for acquisition at different b 1
levels (see supplementary fig. 2). the performance was lower for b 1 0.72 μt,
and 1.5 μt compared to 1 μt.",8
2003,Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,1,Introduction,"machine learning (ml), specifically deep learning (dl), algorithms have shown
exceptional performance on numerous medical image analysis tasks [2].
never-theless, comprehensive reviews highlight major issues of generalizability,
robustness, and reproducibility in medical imaging ai/ml [9,15]. for a
generalizability assessment, reporting only aggregate performance measures is
not sufficient. due to model complexity and limited training data, ml
performance often varies across data subgroups or domains, such as different
patient subpopulations or varied data acquisition scenarios. aggregate
performance measures (e.g., sensitivity, specificity, roc auc) can be dominated
by the larger subgroups, masking the poor ml model performance on smaller but
clinically important subgroups [11]. thus, achieving (through training) and
demonstrating (as part of testing) satisfactory ml model performance across
relevant subgroups is crucial before the real-world clinical deployment of a
medical ml system [13].however, a challenging situation arises when relevant
subgroups are unrecognized. one solution to this issue is to apply a clustering
algorithm to the data, with the goal of identifying the unannotated subgroups.
the main objective of unsupervised clustering is to group data points into
distinct classes of similar traits. however, due to the complexity and high
dimensionality of the medical imaging data and the resulting difficulty in
establishing a concrete notion of similarity, extracting low-dimensional
characteristics becomes the key to establishing the best criteria for grouping.
unsupervised generative clustering aims to simultaneously address both domain
identification and dimensionality reduction. deep unsupervised clustering
algorithms could map the medical imaging data back to their causal factors or
underlying domains, such as image acquisition equipment, patient subpopulations,
or other meaningful data subgroups. however, there is a practical need to be
able to guide the deep clustering model towards the identification of grouping
structures in a given dataset that have not been already annotated. to that end,
we propose a mechanism that is intended to constrain the model towards
identifying clusters in the data that are not associated with given variables of
choice (already known class labels or subgroup structures). the resulting
algorithmic cluster assignments could then be used to improve ml algorithm
training, or for generalizability and robustness evaluation.",8
2013,A Texture Neural Network to Predict the Abnormal Brachial Plexus from Routine Magnetic Resonance Imaging,1,Introduction,"brachial plexopathy is a form of peripheral neuropathy [1]. it occurs when there
is damage to the brachial plexus (bp) which is a complex nerve network under the
skin of the shoulder. there is a wide range of disease that may cause a brachial
plexopathy.radiation fibrosis, primary and metastatic lung cancer, and
metastatic breast cancer account for almost three-fourths of causes [2].
brachial plexus syndrome occurs not infrequently in patients with malignant
disease. it is due to compression or direct invasion of the nerves by tumor
which will bring many serious symptoms [3]. our research focuses on the brachial
plexopathy caused by metastatic breast cancers.magnetic resonance imaging (mri)
and ultrasound of the brachial plexus have become two reliable diagnostic tools
for brachial plexopathy [4]. automatic identification of the bp in mri and
ultrasound images has become a hot topic. currently, most of relevant research
in this field are focusing on ultrasound modality [5][6][7][8]. compared with
ultrasound, mri has become the primary imaging technique in the evaluation of
brachial plexus pathology [9]. however, to our knowledge, radiomics related bp
studies utilizing mri have not been reported previously.many radiomics studies
have experimentally demonstrated that image texture has great potential for
differentiation of different tissue types and pathologies [10]. in the past
several decades, many state-of-the-art methods have been proposed to extract
texture patterns [11,12]. however, how to most effectively combine texture
features with deep learning, called deep texture, is still an open area of
research. one prior approach, termed glcm-cnn, was proposed to carry out a polyp
differentiation task [13]. however, how to arrange these glcms to form the 3d
volume to optimize the performance is a major challenge.with the goal of
classifying normal from abnormal bp, we explored the approach of deep texture
learning. this paper constructed a bp dataset with the most commonly used bp
mris in our clinical practice. considering the shortcoming of traditional
patterns, triple point pattern (tpp) is proposed for the quantitative
representation of the heterogeneity of abnormal bp's. in contrast to glcm-cnn,
tppnet is designed to train models by feeding tpp matrices as the input with a
huge number of channels. finally, we analyze the model's performance in the
experimental section. the major contributions of this study include 1) directed
triangle construction idea for tpp, 2) huge number of tpp matrices as the
heterogeneity representations of bp, 3) tppnet with 15 layers and huge number of
channels, 4) the bp dataset containing mr images and their corresponding roi
masks.",8
2014,A Texture Neural Network to Predict the Abnormal Brachial Plexus from Routine Magnetic Resonance Imaging,2.1,Dataset Preparation and Preprocessing,"following irb approval for this study, we search for patients with metastatic
breast cancer who had a breast cancer mri performed between 2010 and 2020 and
had morphologically positive bp on the mri report from our electronic medical
records (emr) in * hospital. totally, we collect approximate 807 series which
include 274 t2, 254 t1 and 279 post-gadolinium. since some scans are seriously
degraded due to motion artifacts. therefore, each case underwent several
essential image adjustments such as multi-series splitting, two-series merging,
slice swapping, artifact checking and boundary corrections. to yield the roi,
firstly, we randomly sampled -40% of the sequences including both normal and
abnormal ones that were manually segmented with itk-snap by two skilled trainees
[14,15]. then, the manual segmentations were utilized to train a 3d nnunet model
which was utilized to train the model which was used to predict rois for the
rest series [16]. the predicted segmentations were manually divided into three
groups, i.e. good, fair and poor. good cases were added to the training set.
this process was repeated until no improvements in the predictions for the
remaining sequences was seen. the final dataset for radiomic analysis was
constructed by merging the datasets for each sequence type. only patients that
had all three sequences segmented (t2, t1 and post-gadolinium) were included in
the dataset. table 1 shows a breakdown of the final dataset.",8
2065,Prompt-Based Grouping Transformer for Nucleus Detection and Classification,3.1,Datasets and Implementation Details,"consep 1 [10] is a colorectal nuclear dataset with three types, consisting of 41
h&e stained image tiles from 16 colorectal adenocarcinoma whole-slide images
(wsis). the wsis are at 20× magnification and the size of the slides is 500 ×
500. we split them following the official partition [1,10].is a breast cancer
dataset with three types and consists of 120 image tiles from 113 patients. the
wsis are at 20× magnification and the size of the slides ranges from 465 × 465
to 504 × 504. we follow the work [1] to apply the slic [2] algorithm to generate
superpixels as instances and split them into 80/10/30 slides for
training/validation/testing.lizard 3 [9] has 291 histology images of colon
tissue from six datasets, containing nearly half a million labeled nuclei in h&e
stained colon tissue. the wsis are at 20× magnification with an average size of
1,016 × 917 pixels. our implementation and the setting of hyper-parameters are
based on mmdetection [5]. the number of grouping prompts g is 64. random crop,
flipping, and scaling are used for data augmentation. our method is trained with
pytorch on a 48 gb gpu (nvidia a100) for 12-24 h (depending on the dataset
size). more details are listed in the supplementary material.",8
2081,Simulation of Arbitrary Level Contrast Dose in MRI Using an Iterative Global Transformer Model,1,Introduction,"gadolinium-based contrast agents (gbcas) are widely used in mri scans owing to
their capability of improving the border delineation and internal morphology of
different pathologies and have extensive clinical applications [1]. however,
gbcas have several disadvantages like contraindications in patients with reduced
renal function [2], patient inconvenience, high operation costs and
environmental side effects [3]. therefore, there is an increased emphasis on the
paradigm of ""as low as reasonably achievable"" (alara) [4]. to tackle these
concerns of gbcas, several dose reduction [5,6] and elimination approaches [7]
have been proposed. however, these deep learning(dl)-based dose reduction
approaches require high quality low-dose contrast-enhanced (ce) images paired
with pre-contrast and full-dose ce images. acquiring such a dataset requires
modification of the standard imaging protocol and involves additional training
of the mr technicians. therefore, it is important to simulate the process of t1w
low-dose image acquisition, using images from the standard protocol. moreover,
it is crucial for these dose reduction approaches to establish the minimum dose
level required for different pathologies as these are dependent on the scanning
protocol and the gbca compound injected. therefore the simulation tool should
also have the ability to synthesize images with multiple contrast enhancement
levels, that correspond to multiple arbitrary dose levels.currently mri dose
simulation is done using physics-based models [8]. however, these physics-based
methods are dependent on the protocol parameters and the type of gbca and their
relaxation parameters. deep learning (dl) models have been widely used in
medical imaging application due to their high capacity, generazibility, and
transferability [9,10]. the performance of these dl models heavily depend on the
availability of high quality data. there is a dearth of datadriven approaches to
mri dose-simulation given the lack of diverse ground truth data of the different
dose levels. to this effect, we introduce a vision transformer based dl model1
that can synthesize brain 2 mri images that correspond to arbitrary dose levels,
by training on a highly imbalanced dataset with only t1w pre-contrast, t1w 10%
low-dose, and t1w ce standard dose images. the model backbone consists of a
novel global transformer (gformer) with subsampling attention that can learn
long-range dependencies of contrast uptake features. the proposed method also
consists of a rotational shift operation that can further capture the shape
irregularity of the contrast uptake regions. we performed extensive quantitative
evaluation in comparison to other state-of-the art methods. additionally, we
show the clinical utility of the simulated t1w low-dose images using downstream
tasks. to the best of our knowledge, this is the first dl based mri dose
simulation approach.",8
2085,Simulation of Arbitrary Level Contrast Dose in MRI Using an Iterative Global Transformer Model,3,Experiments and Results,"dataset: with irb approval and informed consent, we retrospectively used 126
clinical cases (113 training, 13 testing) from a internal private dataset3 using
gadoterate meglumine contrast agent (site a). for downstream task assessment we
used 159 patient studies from another site (site b) using gadobenate
dimeglumine. the detailed cohort description is given in table 1. the clinical
indications for both sites included suspected tumor, post-op tumor follow-up and
routine brain. for each patient, 3d t1w mprage scans were acquired for the
pre-contrast, low-dose, and post-contrast images. these paired images were then
mean normalized and affine co-registered (pre-contrast as the fixed image) using
simpleelastix [17]. the images were also skull-stripped, to account for
differences in fat suppression, using the hd-bet brain extraction tool [18] for
generating the ""soft labels"". evaluation settings: we quantitatively evaluated
the proposed model using psnr, ssim, rmse, and lpips perceptual metrics [19],
between the synthesized and true low-dose images. we replaced the gformer
backbone with other state-of-the-art methods to compare the efficacy of the
different methods. particularly, the following backbone networks were studied:
simple linear scaling (""scaling"") approach, rednet [20], mapnn [13], restormer
[21], and swinir [22]. unet [23] and swin-unet [24] models were not assessed due
to their tendency to synthesize blurry artifacts in the iterative modelling.
throughput metric (number of images generated per second) was also calculated to
assess the inference efficiency.evaluation results: figure 4(a) shows that the
proposed model is able to generate images that correspond to different dose
levels. as shown in the zoomed inset, the hyperintensity of the contrast uptake
in these images gradually reduces at each iteration. figure 4(b) shows that the
pathological structure in the synthesized low-dose image is similar to that of
the ground truth. figure 4(c) also shows that the model is robust to
hyperintensities that are not related to contrast uptake. figure 3 and table 2
show that proposed model can synthesize enhancement patterns that look close to
the true low-dose and that it performs better than the other competing methods
with a reasonable inference throughput. quantitative assessment of contrast
uptake: the above pixel-based metrics do not specifically focus on the contrast
uptake region. in order to assess the contrast uptake patterns of the
intermediate images, we used the following metrics as described in [25]:
contrast to noise ratio(cnr), contrast to background ratio(cbr), and contrast
enhancement percentage(cep). the roi for the contrast uptake was computed as the
binary mask of the corresponding ""soft labels"" in eq. 2. as shown in fig. 5, the
value of the contrast specific metrics increases in a non-linear fashion as the
iteration step increases. downstream tasks: in order to demonstrate the clinical
utility of the synthesized low-dose images, we performed two downstream tasks:
1) low-dose to full-dose synthesis using the dl-based algorithm to predict
full-dose image from pre-contrast and low-dose images described in [5], we
synthesized t1ce volumes using true low-dose (t1ce-real-ldose) and gformer (rot)
synthesized low-dose (t1ce-synth-ldose). we computed the psnr and ssim metrics
of t1ce vs t1ce-synth/t1ce vs t1ce-synth-sim which are 29.82 ± 3.90 db/28.10 ±
3.20 db and 0.908 ± 0.031/0.892 ± 0.026 respectively. this shows that the
synthesized low-dose images perform similar4 to that of the low-dose image in
the dose reduction task. for this analysis we used the data from site b. 2)
tumor segmentation using the t1ce volumes synthesized in the above step, we
perform tumor segmentation using the winning solution of brats 2018 challenge
[26]. let m true , m ldose and m ldose-sim be the whole tumor (wt) masks
generated using t1ce, t1ce-real-ldose and t1ce-synth-ldose (+ t1, t2 and flair
images) respectively. the mean dice scores dice(m true , m ldose ) and dice(m
true , m ldose-sim ) on the test set were 0.889±0.099 and 0.876±0.092
respectively. figure 6 shows visual examples of tumor segmentation performance.
this shows that the clinical utility provided by the synthesized low-dose is
similar5 to that of the actual low-dose image.",8
2095,TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,0,Table 3 .,"works on unregistered tractography from neonate brains (much smaller than adult
brains). in the challenging btp (tumor patients) dataset, tractcloud reg-free
obtains significantly lower tda values than sota methods and comparable
performance to tractcloud regist . as shown in table2, our registration-free
framework is much faster than other compared methods.",8
2105,Relaxation-Diffusion Spectrum Imaging for Probing Tissue Microarchitecture,3.3,In Vivo Data: Neurite Morphology,"figure 4(a) shows the relaxation times of the restricted compartment in white
matter lesions, indicating that relaxation times are longer in gliomas than
normal white matter tissue. the higher t 2 in grade 4 glioma is associated with
changes in metabolite compositions, resulting in remarkable changes in neurite
morphology in lesioned tissues (fig. 4(c-d)), consistent with previous
observations [12,23]. the rate of longitudinal relaxation time has been shown to
be positively correlated with myelin content. our results indicate that mte dmri
is more sensitive to neurite morphology than ste dmri (fig. 4(b)).figures 4(c-d)
show that the estimated mean nr in the gray matter is approximately in the range
of 10 µm, which is in good agreement with the sizes of somas in human brains,
i.e., 11 ± 7 µm [26]. rdsi improves the detection of small metastases,
delineation of tumor extent, and characterization of the intratumoral
microenvironment when compared to conventional microstructure models (fig.
4(c)). our studies suggest that rdsi provides useful information on
microvascularity and necrosis helpful for facilitating early stratification of
patients with gliomas (fig. 4(d)).",8
2120,Brain Anatomy-Guided MRI Analysis for Assessing Clinical Progression of Cognitive Impairment with Structural MRI,2,Materials and Proposed Method,"data and preprocessing. the pretext model is trained via a tissue segmentation
task on auxiliary mris (without category label) from adni. a total of 9,544
t1-weighted mris from 2,370 adni subjects with multiple scans are used in this
work. to provide accurate brain anatomy, we perform image preprocessing and
brain tissue segmentation for these mris to generate ground-truth segmentation
of three tissues, i.e., white matter (wm), gray matter (gm) and cerebrospinal
fluid (csf), using an in-house toolbox ibeat [16] with manual verification.the
downstream model is trained on 1) a late-life depression (lld) study with 309
subjects from two sites [17,18], and 2) a type 2 diabetes mellitus (dm) study
with 82 subjects from the first affiliated hospital of guangzhou university of
chinese medicine. subjects in lld are categorized into three groups: 1) 89
non-depressed cognitively normal (cn), 2) 179 depressed but cognitively normal
(cnd), 3) 41 depressed subjects (called ci) who developed cognitive impairment
or even dementia in the follow-up years. category labels in the lld study are
determined based on subjects' 5-year follow-up diagnostic information, while
mris are acquired at baseline time. the dm contains 1) 45 health control (hc)
subjects and 2) 37 diabetes mellitus patients with mild ci (mci). detailed image
acquisition protocols are given in table si of supplementary materials. all mris
are preprocessed via the following pipeline: 1) bias field correction, 2) skull
stripping, 3) affine registration to the mni space, 4) resampling to 1 × 1 × 1
mm 3 , 5) deformable registration to aal3 [19] with syn [20], and 6) warping 166
regions-of-interest (rois) of aal3 back to mri volumes.proposed method. while it
is often challenging to annotate mris in practice, there are a large number of
mris (without task-specific category labels) in existing large-scale datasets.
even without category labels, previous studies propose to extract anatomical
features (e.g., roi volumes of gm segmentation maps) to characterize brain
anatomy [21,22]. such brain anatomy prior learned via tissue segmentation can be
employed to boost learning performance intuitively. accordingly, we propose a
brain anatomy-guided representation (bar) learning framework for progression
prediction of cognitive impairment, incorporated with brain anatomy prior
provided by brain tissue segmentation. as shown in fig. 1, the bar consists of a
pretext model for brain tissue segmentation and a downstream model for disease
progression prediction, both equipped with brain anatomy-guided encoders (shared
weights) for mri feature learning.(1) pretext model for segmentation. to learn
brain anatomical features from mris in a data-driven manner, we propose to
employ a segmentation task for pretext model training. as shown in the top of
fig. 1, the pretext model consists of 1) a brain anatomy-guided encoder for mri
feature extraction and 2) a decoder for segmentation. the brain anatomy-guided
encoder takes large-scale auxiliary 3d mris without category labels as input,
and outputs 512 feature maps. it contains 8 convolution blocks, with each block
containing a convolution layer (kernel size: 3 × 3 × 3), followed by instance
normalization and parametric rectified linear unit (prelu) activation. the first
4 blocks downsample the input with a stride of 2 × 2 × 2. the channel numbers of
the eight blocks are [64, 128, 256, 512, 512, 512, 512, 512], respectively. a
skip connection is applied to sum the input and output of every two of the last
4 blocks for residual learning.the decoder takes the 512 feature maps as input
and outputs segmentation maps of three tissues (i.e., wm, gm, and csf), thus
guiding the encoder to learn brain anatomical features. the decoder contains
four deconvolution blocks with 256, 128, 64 and 4 channels, respectively. each
deconvolution block shares the same architecture as the convolution block in the
encoder. the output of the decoder is then fed into a softmax layer to get four
probability maps that indicate the probability of a voxel belonging to a
specific tissue (i.e., background, wm, gm, and csf). besides, the reconstruction
task can be used to train the pretext model instead of segmentation when lacking
ground-truth segmentation maps. for problems without ground-truth segmentation
maps, we can resort to an mri reconstruction task to train the pretext model in
an unsupervised manner.(2) downstream model for prediction. as shown in the
bottom panel of fig. 1, the downstream model takes target mris as input and
outputs probabilities of category labels. it consists of 1) a brain
anatomy-guided encoder and 2) a predictor for prognosis. this encoder shares the
same architecture and parameters as that of the pre-trained pretext model. with
the encoder frozen, predictor parameters will be updated when the downstream
model is trained on target mris. the predictor has two convolution blocks
(kernel size: 3 × 3 × 3, stride: 2 × 2 × 2, channel: 256) with a skip
connection, followed by a flatten layer, an fc layer, and a softmax layer. the
architecture of the predictor can be flexibly adjusted according to the
requirements of different downstream tasks.(3) implementation. the proposed bar
is trained via two steps. 1) the pretext model is first trained on 9,544 mris
from adni, with ground-truth segmentation as supervision. the adam optimizer
[23] with a learning rate of 10 -4 and dice loss are used for training (batch
size: 4, epoch: 30). 2) we then share sii of supplementary materials. such
partition is repeated five times independently to avoid any bias introduced by
random partition, and the mean and standard deviation results are recorded. the
training data is duplicated and augmented using random affine transform. five
evaluation metrics are used, including area under roc curve (auc), accuracy
(acc), sensitivity (sen), specificity (spe), and f1-score (f1s). besides, we
perform tissue segmentation by directly applying the trained pretext model to
target mris from lld and dm studies and visually compare the results of our bar
with those of fsl [24].competing methods. we compare our bar with two classic
machine learning methods and five sota deep learning approaches, including (1)
support vector machine (svm) [25] with a radial basis function kernel
(regularization: 1.0), ( 2) xgboost (xgb) [26] (estimators: 300, tree depth: 4,
learning rate: 0.2), (3) resnetx with x convolution layers, (4) med3dx [27] with
x convolution layers, (5) seresnet [28] that is an improved model by adding
squeeze and excitation blocks to resnet, (6) efficientnet [29], and (7)
mobilenet [30] that is an efficient lightweight cnn model. for svm and xgb, we
extract roibased wm and gm volumes of each mri as input. all competing deep
learning methods (with default architectures) take whole 3d mris as input and
share the same training strategy as that used in the downstream model of the
bar. an early-stop training strategy (epoch: 90) is used in all deep learning
models. results of depression and ci identification. in this task, we aim to
recognize cognitively normal subjects with depression with a higher risk of
progressing to ci than healthy subjects. the results of fourteen methods on the
lld study are reported in table 1, where '*' denotes that the results of bar and
a competing method are statistically significantly different (p < 0.05 via
paired t-test).from the left of table 1, we have the following observations on
cnd vs. cn classification. first, our bar model generally outperforms thirteen
competing methods in most cases. for instance, the bar yields the results of auc
= 70.5% and sen = 77.3%, which are 4.1% and 10.0% higher than those of the
secondbest methods (i.e., seresnet and resnet50), respectively. this implies
that the brain anatomical mri features learned by our pretext model on
large-scale datasets would be more discriminative, compared with those used in
the competing methods. second, among 10 deep models, our bar produces the lowest
standard deviation in most cases (especially on sen and spe), suggesting its
robustness to bias introduced by random data partition in the downstream task.
this could be due to the strong generalizability of the feature encoder guided
by brain anatomy prior (derived from the auxiliary tissue segmentation task). in
addition, our bar significantly outperforms four machine learning methods and
two lightweight deep models (i.e., efficientnet and mobilenet) with p <
0.05.from the right of table 1, we can see that the overall results of fourteen
methods in ci vs. cnd classification are usually worse than cnd vs. cn
classification. this suggests that the task of ci vs. cnd classification is more
challenging, which could be due to the more imbalanced training data in this
task (as shown in table sii of supplementary materials). on the other hand, the
proposed bar still performs best in terms of auc=64.5% and spe=67.0%, which are
2.8% and 2.0% higher than those of the second-best competing methods (i.e.,
xgb-wm and resnet34), respectively. these results further demonstrate the
superiority of the bar in mri-based depression recognition.results of mci
detection. the results of different methods in mci detection (i.e., mci vs. hc
classification) on the dm study are reported in table 2. there are a total of 42
subjects (i.e., 17 mci and 25 hc) used for training in this task, which are
fewer but more balanced than the two tasks in the lld study (see table sii). it
can be observed from tables 1 and2 that the proposed bar yields relatively lower
standard deviations in terms of auc and acc in mci vs. hc classification,
compared with the two tasks on the lld study. these results imply that data
imbalance may be an important issue affecting the performance of deep learning
models when the number of training samples is limited.segmentation results. the
pre-trained pretext model can also be used for brain tissue segmentation in
downstream studies. thus, we visualize brain segmentation maps generated by fsl
and our bar for target mris in both lld and dm studies in fig. 2. note that
t1-weighted mris in the lld study are collected from 2 sites and have more
inconsistent image quality when compared to those from dm. from fig. 2, we have
several interesting observations.first, the segmentation results generated by
the proposed bar are generally better than those of fsl in most cases,
especially for those cortical surface areas on the two studies. for instance,
the wm region in segmentation maps generated by our bar is much cleaner than
that of fsl, indicating that our model is not sensitive to noise in mri. even
for the lld study with significant inter-site data heterogeneity, the boundary
of wm and gm produced by bar is more continuous and smoother, which is in line
with the brain anatomy prior. second, for mris with severe motion artifacts in
the lld study (ids: 1240, 1334, and 1653), our method can produce high-quality
segmentation maps, and the results are even comparable to those of mris without
motion artifacts. this demonstrates that our model is robust to motion
artifacts. the underlying reason could be that the pretext model is trained on
large-scale mris, and thus, has good generalization ability when applied to mris
with different image quality. in addition, both bar and fsl often achieve better
results in the dm study, since dm has relatively higher image quality than lld.
still, the proposed bar can achieve better segmentation results in many
fine-grained brain regions, such as the putamen region (see hc001 and mci003)
and the vermis region (see hc004). these results demonstrate that our method has
good adaptability when applied to classification and segmentation tasks in
mri-based studies.ablation study. to validate the effectiveness of the learned
brain anatomical mri features, we further compare the bar with its two variants
(called bar-b and bar-r) that use anatomy prior derived from different pretext
tasks in cnd vs. cn classification on lld. specifically, the bar-b is trained
from scratch as a baseline on target data without any pre-trained encoder. the
bar-r trains the pretext model through an mri reconstruction task in an
unsupervised learning manner. as shown in fig. 3(a), the bar consistently
performs better than its variants in terms of all five metrics. this implies
that the learned mri features guided by the segmentation task help promote
prediction performance. also, bar and bar-r outperform bar-b in most cases,
implying that brain anatomy prior derived from tissue segmentation or mri
reconstruction can help improve discriminative ability of mri features and boost
prediction performance.influence of training data size. we also study the
influence of training data size on bar in cnd vs. cn classification on lld. with
fixed test data, we randomly select a part of mris (i.e., [20%, 40%, • • • ,
100%]) from target training data to fine-tune the downstream prediction model.
it can be observed from fig. 3(b) that the overall performance in terms of auc
and acc of our bar increases with the increase of training data, and it produces
the best results when using all training data for model fine-tuning. this
suggests that using more data for downstream model fine-tuning helps promote
learning performance.",8
2123,atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,1,Introduction,"diffusion-weighted mri enables visualization of brain white matter structures.
it can be used to generate tractography data consisting of millions of synthetic
fibers or streamlines for a single subject stored in a tractogram that
approximate groups of biological axons [1]. many applications require
streamlines to be segmented into individual tracts corresponding to known
anatomy. tract segmentations are used for a variety of tasks, including surgery
planning or tract-specific analysis of psychiatric and neurodegenerative
diseases [2,11,12,17].automated methods built on supervised machine learning
algorithms have attained the current state-of-the-art in segmenting tracts
[3,18,21]. those are trained using various features, either directly from
diffusion data in voxel space or from tractography data. models may output
binary masks containing the target white matter tract, or perform a
classification on streamline level. however, such algorithms are commonly
trained on healthy subjects and have shown issues in processing cases with
anatomical abnormalities, e.g. brain tumors [20]. consequently, they are
unsuitable for tasks such as preoperative planning of neurosurgical patients, as
they may produce incomplete or false segmentations, which could have harmful
consequences during surgery [19]. additionally, supervised techniques are
restricted to fixed sets of predetermined tracts and are trained on substantial
volumes of hard-to-generate pre-annotated reference data.manual methods are
still frequently used for all cases not yet covered by automatic methods, such
as certain populations like children, animal species, new acquisition schemes or
special tracts of interests. experts determine regions of interest (roi) in
areas where a particular tract is supposed to traverse or through which it must
not pass, and segmentations can be accomplished either (1) by virtually
excluding and maintaining streamlines from tractography according to the defined
roi or (2) by using these regions for tract-specific roi-based tractography.
both approaches require a similar effort, although the latter is more commonly
used. the correct definition of rois can be time-consuming and challenging,
especially for inexperienced users. despite these limitations, roi-based
techniques are currently without vivid alternatives for segmenting tracts that
automated methods cannot handle.methods to simplify tract segmentation have been
proposed before. clustering approaches were developed to reduce complexity of
large amounts of streamlines in the input data [4,6]. tractome is a tool that
allows interactive segmentation of such clusters by representing them as single
streamlines that can interactively be included or excluded from the target tract
[14]. although the approach has shown promise, it has not yet supplanted
conventional roibased techniques.we propose a novel semi-automated tract
segmentation method for efficient and intuitive identification of arbitrary
white matter tracts. the method employs entropy-based active learning of a
random forest classifier trained on features of the dissimilarity representation
[13]. active learning has been utilized for several cases in the medical domain,
while it has never been applied in the context of tract segmentation [7,9,16].
it reduces manual efforts by iteratively identifying the most informative or
ambiguous samples, here, streamlines, during classifier training, to be
annotated by a human expert. the method is implemented as the tool attractive in
mitk diffusion1 , enabling researchers to quickly and intuitively segment tracts
in pathological datasets or other situations not covered by automatic
techniques, simply by annotating a few but informative streamlines.",8
2126,atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,3.1,Data,"the proposed technique was tested on a healthy-subject dataset and on a dataset
containing tumor cases. the first comprises 21 subjects of the human connectome
project (hcp) that were used for testing the automated methods tractseg and
classifyber [3,18]. visual examinations revealed false-negatives in the
reference tracts, meaning that some streamlines that belong to the target tract
were not included in the reference. these false-negatives did not affect the
generation of accurate segmentation masks, since most false-negatives are
occupied by true-positive streamlines, but negatively influenced our
experiments. to reduce false-negatives, the reference segmentation mask as well
as start-and end-region segmentations were used to reassign streamlines from the
tractogram using two criteria: streamlines must be inside the binary reference
segmentation (1) and start and end in the assigned regions (2). as the initial
size of ten million streamlines is computationally challenging and unsuitable
for most tools, all tractograms were randomly down-sampled to one million
streamlines. we focused on the left optic radiation (or), the left
cortico-spinal tract (cst), and the left arcuate-fasciculus (af), representing a
variety of established tracts.to test the proposed method on pathological data,
we used an in-house dataset containing ten presurgical scans of patients with
brain tumors. tractography was performed using probabilistic streamline
tractography in mitk diffusion. to reduce computational costs, we retained one
million streamlines that passed through a manually inserted roi located in an
area traversed by the or [15]. subjects have tumor appearance with varying sizes
((17.87±12.73 cm 3 )) in temporoloccipital, temporal, and occipital regions,
that cause deformations around the or and lead to deviations of the tract from
the normative model.",8
2139,Single-subject Multi-contrast MRI Super-resolution via Implicit Neural Representations,1,Introduction,"in clinical practice, magnetic resonance imaging (mri) provides important
information for diagnosing and monitoring patient conditions [4,16]. to capture
the complex pathophysiological aspects during disease progression,
multiparametric mri (such as t1w, t2w, dir, flair) is routinely acquired. image
acquisition inherently poses a trade-off between scan time, resolution, and
signalto-noise ratio (snr) [19]. to maximize the source of information within a
reasonable time budget, clinical protocol often combines anisotropic 2d scans of
different contrasts in complementary viewing directions. although acquired 2d
scans offer an excellent in-plane resolution, they lack important details in the
orthogonal out-of-plane. for a reliable pathological assessment, radiologists
often resort to a second scan of a different contrast in the orthogonal viewing
direction. furthermore, poor out-of-plane resolution significantly affects the
accuracy of volumetric downstream image analysis, such as radiomics and lesion
volume estimation, which usually require isotropic 3d scans. as multi-parametric
isotropic 3d scans are not always feasible to acquire due to time-constraints
[19], motion [9], and patient's condition [10], super-resolution offers a
convenient alternative to obtain the same from anisotropic 2d scans. recently,
it has been shown that acquiring three complementary 2d views of the same
contrast may yield higher snr at reduced scan time [19,29]. however, it remains
under-explored if orthogonal anisotropic 2d views of different contrasts can
benefit from each other based on the underlying anatomical consistency.
additionally, whether such strategies can further decrease scan times while
preserving similar resolution and snr remains unanswered. moreover, unlike
conventional super-resolution models trained on a cohort, a personalized model
is of clinical relevance to avoid the danger of potential misdiagnosis caused by
cohort-learned biases. in this work, we mitigate these gaps by proposing a novel
multi-contrast super-resolution framework that only requires the
patient-specific low-resolution mr scans of different sequences (and views) as
supervision. as shown in various settings, our approach is not limited to
specific contrasts or views but provides a generic framework for
super-resolution. the contributions in this paper are three-fold: 1. to the best
of our knowledge, our work is the first to enable subject-specific
multi-contrast super-resolution from low-resolution scans without needing any
high-resolution training data. we demonstrate that implicit neural
representations (inr) are good candidates to learn from complementary views of
multi-parametric sequences and can efficiently fuse low-resolution images into
anatomically faithful super-resolution. 2. we introduce mutual information (mi)
[26] as an evaluation metric and find that our method preserves the mi between
high-resolution ground truths in its predictions. further observation of its
convergence to the ground truth value during training motivates us to use mi as
an early stopping criterion. 3. we extensively evaluate our method on multiple
brain mri datasets and show that it achieves high visual quality for different
contrasts and views and preserves pathological details, highlighting its
potential clinical usage.related work. single-image super-resolution (sisr) aims
at restoring a highresolution (hr) image from a low-resolution (lr) input from a
single sequence and targets applications such as low-field mr upsampling or
optimization of mri acquisition [3]. recent methods [3,8] incorporate priors
learned from a training set [3], which is later combined with generative models
[2]. on the other hand, multi-image super-resolution (misr) relies on the
information from complementary views of the same sequence [29] and is especially
relevant to capturing temporal redundancy in motion-corrupted low-resolution mri
[9,27]. multi-contrast super-resolution (mcsr) targets using inter-contrast
priors [20]. in conventional settings [15], an isotropic hr image of another
contrast is used to guide the reconstruction of an anisotropic lr image. zeng et
al. [30] use a two-stage architecture for both sisr and mcsr. utilizing a
feature extraction network, lyu et al. [14] learn multi-contrast information in
a joint feature space. later, multi-stage integration networks [6], separatable
attention [7] and transformers [13] have been used to enhance joint feature
space learning. however, all current mcsr approaches are limited by their need
for a large training dataset. consequently, this constrains their usage to
specific resolutions and further harbors the danger of hallucination of features
(e.g., lesions, artifacts) present in the training set and does not generalize
well to unseen data.originating from shape reconstruction [18] and multi-view
scene representations [17], implicit neural representations (inr) have achieved
state-of-the-art results by modeling a continuous function on a space from
discrete measurements. key reasons behind inr's success can be attributed to
overcoming the low-frequency bias of multi-layer perceptrons (mlp) [21,24,25].
although mri is a discrete measurement, the underlying anatomy is a continuous
space. we find inr to be a good fit to model a continuous intensity function on
the anatomical space. once learned, it can be sampled at an arbitrary resolution
to obtain the super-resolved mri. following this spirit, inrs have recently been
successfully employed in medical imaging applications ranging from k-space
reconstruction [11] to sisr [29]. unlike [22,29], which learn anatomical priors
in single contrasts, and [1,28], which leverage inr with latent embeddings
learned over a cohort, we focus on employing inr in subject-specific,
multi-contrast settings.",8
2140,Single-subject Multi-contrast MRI Super-resolution via Implicit Neural Representations,2,Methods,"in this section, we first formally introduce the problem of joint
super-resolution of multi-contrast mri from only one image per contrast per
patient. next, we describe strategies for embedding information from two
contrasts in a shared space. subsequently, we detail our model architecture and
training configuration.problem statement. we denote the collection of all 3d
coordinates of interest in this anatomical space as ω = {(x, y, z)} with
anatomical function q : ω → a. the image intensities are a function of the
underlying anatomical properties a. two contrasts c 1 and c 2 can be scanned in
a low-resolution subspace ω 1 , ω 2 ⊂ ω. let us consider g 1 , g 2 : a → r that
map from anatomical properties to contrast intensities c 1 and c 2 ,
respectively. we obtain sparse observations, where f i is composition of g i and
q. however, one can easily obtain the global anatomical space ω by knowing ω 1
and ω 2 , e.g., by rigid registration between the two images. in this paper, we
aim to estimate f 1 , f 2 : ω → r given i 1 and i 2 .joint multi-contrast
modelling. since both component-functions f 1 and f 2 operate on a subset of the
same input space, we argue that it is beneficial to model them jointly as a
single function f : ω → r 2 and optimize it based on their estimation error
incurred in their respective subsets. this will enable information transfer from
one contrast to another, thus improving the estimation and preventing
over-fitting in single contrasts, bringing consistency to the prediction.to this
end, we propose to leverage inr to model a continuous multi-contrast function f
from discretely sampled sparse observations i 1 and i 2 .mcsr setup. without
loss of generalization, let us consider two lr input contrasts scanned in two
orthogonal planes p 1 and p 2 , where p 1 , p 2 ∈ {axial, sagittal, coronal}. we
assume they are aligned by rigid registration requiring no coordinate
transformation. their corresponding in-plane resolutions are (s 1 ×s 1 ) and (s
2 × s 2 ) and slice thickness is t 1 and t 2 , respectively. note that s 1 < t 1
and s 2 < t 2 imply high in-plane and low out-of-plane resolution. in the end,
we aim to sample an isotropic (s × s × s) grid for both contrasts where s ≤ s 1
, s 2 .implicit neural representations for mcsr. we intend to project the
information available in one contrast into another by embedding both in the
shared weight space of a neural network. however, a high degree of weight
sharing could hinder contrast-specific feature learning. based on this
reasoning, we aim to hit the sweet spot where maximum information exchange can
be encouraged without impeding contrast-specific expressiveness. we propose a
split-head architecture, as shown in fig. 1, where the initial layers jointly
learn the common anatomical features, and subsequently, two heads specialize in
contrast-specific information. the model takes fourier [25] features v =
[cos(2πbx), sin(2πbx)] t as input and predicts [ î1 , î2 ] = f (v), where x =
(x, y, z) and b is sampled from a gaussian distribution n (μ, σ 2 ). we use
mean-squared error loss, l mse , for training.where α and β are coefficients for
the reconstruction loss of two contrasts. note that for points {(x, y, z)} ∈ ω 2
\ ω 1 , there is no explicit supervision coming from low resolution c 1 . for
these points, one can interpret learning c 1 from the loss in c 2 , and vice
versa, to be a weakly supervised task.",8
2142,Single-subject Multi-contrast MRI Super-resolution via Implicit Neural Representations,3,Experiments and Results,"datasets. to enable fair evaluation between our predictions and the reference hr
ground truths, the in-plane snr between the lr input scan and corresponding
ground truth has to match. to synthetically create 2d lr images, it is necessary
to downsample out-of-plane in the image domain anisotropically [32] while
preserving in-plane resolution. consequently, to mimic realistic 2d clinical
protocol, which often has higher in-plane details than that of 3d scans, we use
spline interpolation to model partial volume and downsampling. we demonstrate
our network's modeling capabilities for different contrasts (t1w, t2w, flair,
dir), views (axial, coronal, sagittal), and pathologies (ms, brain tumor). we
conduct experiments on two public datasets, brats [16], and msseg [4], and an
in-house clinical ms dataset (cms). in each dataset, we select 25 patients that
fulfill the isotropic acquisition criteria for both ground truth hr scans. note
that we only use the ground truth hr for evaluation, not anywhere in training.
we optimize separate inrs for each subject with supervision from only its two lr
scans. if required, we employ skull-stripping [12] and rigid registration to the
mni152 (msseg, cms) or sri24 (brats) templates. for details, we refer to table 2
in the supplementary.metrics. we evaluate our results by employing common sr
[5,14,29] quality metrics, namely psnr and ssim. to showcase perceptual image
quality, we additionally compute the learned perceptual image patch similarity
(lpips) [31] and measure the absolute error mi in mutual information of two
upsampled images to their ground truth counterparts as follows:baselines and
ablation. to the best of our knowledge, there are no prior data-driven methods
that can perform mcsr on a single-subject basis. hence, we provide
single-subject baselines that operate solely on single contrast and demonstrate
the benefit of information transfer from other contrasts with our proposed
models. quantitative analysis. table 1 demonstrates that our proposed framework
poses a trustworthy candidate for the task of mcsr. as observed in [32], lrtv
struggles for anisotropic up-sampling while smore's overall performance is
better than cubic-spline, but slightly worse to single-contrast inr. however,
the benefit of single-contrast inr may be limited if not complemented by
additional views as in [29]. for mcsr from single-subject scans, we achieve
encouraging results across all metrics for all datasets, contrasts, and views.
since t1w and t2w both encode anatomical structures, the consistent improvement
in brats for both sequences serves as a proof-of-concept for our approach. as
flair is the go-to-sequence for ms lesions, and t1w does not encode such
information, the results are in line with the expectation that there could be a
relatively higher transfer of anatomical information to pathologically more
relevant flair than vice-versa. lastly, given their similar physical acquisition
and lesion sensitivity, we note that dir/flair benefit to the same degree in the
cms dataset.qualitative analysis. figure 2 shows the typical behavior of our
models on cms dataset, where one can qualitatively observe that the split-head
inr pre-serves the lesions and anatomical structures shown in the yellow boxes,
which other models fail to capture. while our reconstruction is not identical to
the gt hr, the coronal view confirms anatomically faithful reconstructions
despite not receiving any in-plane supervision from any contrast during
training. we refer to fig. 4 in the supplementary for similar observations on
brats and msseg.",8
2146,Overall Survival Time Prediction of Glioblastoma on Preoperative MRI Using Lesion Network Mapping,1,Introduction,"glioblastomas (gbms, known as grade iv gliomas) are the most common primary
malignant brain tumors with high spatial heterogeneity and varying degrees of
aggressiveness [22]. patients with gbm generally have a very poor survival rate;
the median overall survival time is about 14 months [17]; and the overall
survival time is affected by many factors, including patient characteristics
(e.g., age and physical status), tissue histopathology (e.g., cellular density
and nuclear atypia), and molecular pathology (e.g., mutations and gene
expression levels) [1,14,15]. although these factors, particularly molecular
information, have usually proved to be strong predictors of survival in gbm,
there remain substantial challenges and unmet clinical needs to exploit easily
accessible, noninvasive neuroimaging data acquired preoperatively to predict
overall survival time of gbm patients, which can benefit treatment planning.to
do so, magnetic resonance imaging (mri) and its derived radiomics have been
widely used to study gbm preoperative prognosis over the last few decades. for
example, anand et al. [2] first applied a forest of trees to assign an
importance value to each of the 1022 radiomic features extracted from t1 mri,
and then the 32 most important features were fed to the random forest regressor
for predicting overall survival time of a gbm patient. based on patches from
multi-modal mri images, nie et al. [19] trained a 3d convolutional neural
network (cnn) to learn the high-level semantic features, which were eventually
input to a support vector machine (svm) for classifying long-and short-term gbm
survivors. in addition, an integrated model by fusing radiomics features,
mri-based cnn features, and clinical features, was presented for gbm survival
group classification, resulting in better performance than using any single type
of features [12].although both mri and its derived radiomics features have been
demonstrated to have predictive power for survival analysis in the
aforementioned literature, they do not account for brain's functional
alternations caused by tumors, which are clinically significant as
biologically-interpretable biomarkers of recovery and therapy. these
alternations can be reflected by changes in resting-state functional mri
(fmri)-derived functional connectivities/connections (fcs) between the blood
oxygenation level-dependence (bold) time series of paired brain regions.
therefore, the use of fcs to predict overall survival time for gbm has recently
attracted increasing attention [7,16,24], and more importantly, survival-related
fc patterns or brain regions were found to guide therapeutic solutions aimed at
inhibiting tumor-brain communication.nevertheless, current fc-based survival
prediction still suffers from two main deficiencies when applied to gbm
prognosis. first, due to mass effect and physical infiltration of gbm in the
brain, fcs estimated directly from gbm patients' resting-state fmri might be
inaccurate, especially when the tumors are near or in the regions of interest.
second, resting-state fmri data are not routinely collected for gbm clinical
practices, which restricts the size of annotated datasets such that it is
infeasible to train a reliable prediction model based on deep learning for
survival prediction. in order to circumvent these issues, in this paper we
introduce a novel neuroimaging feature family, namely functional lesion network
(fln) maps that are generated by our augmented lesion network mapping (a-lnm),
for overall survival time prediction of gbm patients. our a-lnm is motivated by
lesion network mapping (lnm) [8] which can localize neurological deficits to
functional brain networks and identify regions relate to a clinical syndrome. by
embedding the lesion into a normative functional connectome and computing
functional connectivity between the lesion and the rest of the brain using fmri
of all healthy subjects in the normative cohort, lnm has been successfully
employed to the identification of the brain network underlying particular
symptoms or behavioral deficits in stoke [4,13].the details of our workflow are
described as follows.1) we first manually segment the whole tumor (regarded as
lesion in this paper) on structural mri for all gbm patients, and the resulting
lesion masks are mapped onto a reference brain template, e.g., the mni152 2mm 3
template.2) the proposed a-lnm is next used to generate fln maps for each gbm
patient by using resting-state fmri from a large cohort of healthy subjects.
specifically, for each patient, we correlate the mean bold time series of all
voxels within the lesion with the bold time series of every voxel in the whole
brain for all n subjects in the normative cohort, producing n functional
disconnection (fdc) maps of voxel-wise correlation values (transformed to
zscores). these resulting n fdc maps are partitioned into m disjoint subsets of
equal size, and m fln maps are separately obtained by averaging the fdc maps in
each of the m subsets. similar to data augmentation schemes, we can artificially
boost data volume (i.e., fln maps) up to m times through producing m fln maps
for each patient in the a-lnm, which helps to mitigate the risk of over-fitting
and improve the performance of overall survival time prediction when learning a
deep neural network from a small sized dataset.for this reason, we propose the
name ""augmented lnm (a-lnm)"", compared to the traditional lnm where only one fln
map is generated per patient by averaging all the n fdc maps. 3) finally, these
augmented fln maps are fed to a 3d resnet-based backbone network followed by the
average pooling operation and fully-connected layers for gbm survival
prediction.to our knowledge, this paper is the first to demonstrate a successful
extension of lnm for survival prediction in gbm. to evaluate the predictive
power of the fln maps generated by our a-lnm, we conduct extensive experiments
on 235 gbm patients in the training dataset of brats 2020 [18] to classify the
patients into three overall survival time groups viz. long, mid, and short.
experimental results show that our a-lnm based survival prediction framework
outperforms previous state-of-the-art methods. in addition, an explainable
analysis driven by the gradient-weighted class activation mapping (grad-cam)
[10] for survivalrelated brain regions is fulfilled.",8
2147,Overall Survival Time Prediction of Glioblastoma on Preoperative MRI Using Lesion Network Mapping,2,Materials and Methods,"2.1 materials gsp1000 processed connectome. it publicly released preprocessed
restingstate fmri data of 1000 healthy right-handed subjects with an average age
21.5 ± 2.9 years and approximately equal numbers of males and females from the
brain genomics superstruct project (gsp) [5], where the concrete image
acquisition parameters and preprocessing procedures can be found as well.
specifically, a slightly modified version of yeo's computational brain imaging
group (cbig) fmri preprocessing pipeline (https://github.com/bchcohenlab/cbig)
was employed to obtain either one or two preprocessed resting-state fmri runs of
each subject that had 120 time points per run and were spatially normalized into
the mni152 template with 2mm 3 voxel size. we downloaded and used the first-run
preprocessed resting-state fmri of each subject for the following analysis.brats
2020. it provided an open-access pre-operative imaging training dataset to
segment brain tumors of glioblastoma (gbm, belonging to high grade glioma) and
low grade glioma (lgg) patients, as well as to predict overall survival time of
gbm patients [18]. this training dataset contained 133 lgg and 236 gbm patients,
and each patient had four mri modalities, including t1, post-contrast
t1-weighted, t2-weighted, and t2 fluid attenuated inversion recovery. manual
expert segmentation delineated three tumor sub-regions, i.e., the gd-enhancing
tumor, the peritumoral edema, and the necrotic and non-enhancing tumor core.the
union of all the three tumor sub-regions was considered as the whole tumor,
which is regarded as the lesion in this paper.",8
2148,Overall Survival Time Prediction of Glioblastoma on Preoperative MRI Using Lesion Network Mapping,2.2,Methods,"in this paper, we propose to investigate the feasibility of the novel
neuroimaging features, i.e., fln maps, for overall survival time prediction of
gbm patients in the training dataset of the brats 2020, in which one patient
alive was excluded, and the remaining 235 patients consisted of 89 short-term
survivors (less than 10 months), 59 mid-term survivors (between 10 and 15
months), and 87 long-term survivors (more than 15 months). to this end, our
framework for the three-class survival classification is shown in fig. 1, and
the details are described as follows.lesion mapping procedures. as stated above,
the whole tumor is referred to as a lesion for each gbm patient. from the manual
expert segmentation labels of lesions in the 235 gbm patients of the brats 2020,
we co-register the lesion masks to the mni152 2mm 3 template by employing a
symmetric normalization algorithm in antspy [3].",8
2149,Overall Survival Time Prediction of Glioblastoma on Preoperative MRI Using Lesion Network Mapping,0,Augmented Lesion Network Mapping (A-LNM).,"after lesion mapping, we introduce a modified lnm (called augmented lnm (a-lnm)
in this paper) to generate fln maps for each gbm patient by using resting-state
fmri of all 1000 gsp healthy subjects, as described below. i) for each patient,
the lesion is viewed as a seed region to calculate fdc in the healthy subjects
with restingstate fmri. specifically, to compute fdc, the mean bold time series
of voxels within each lesion is correlated with the bold time series of every
voxel in the whole brain for all the 1000 healthy subjects, yielding 1000 fdc
maps of voxelwise correlation values (transformed to z-scores), where an fdc map
is actually a three-dimensional voxel-wise matrix of size 91 × 109 × 91 (spatial
resolution: 2mm 3 voxel size). ii) different from the commonly used lnm where
the resulting 1000 fdc maps are thresholded or averaged to obtain a single fln
map for each patient, the a-lnm generates many fln maps for each patient in a
manner that partitions all the 1000 fdc maps into disjoint subsets of equal size
and averages each subset to produce one fln map. one can clearly see that
similar to data augmentation schemes, we artificially boost the number of
training samples (i.e., fln maps) by our a-lnm, which helps to mitigate the risk
of over-fitting and improve the performance of overall survival time prediction
when learning a deep neural network from such a small sized training set used in
this paper. note that in sect. 3 of this paper, according to experimental
results, we divided the 1000 fdc maps into 100 subsets, and randomly chose 10
out of the resulting 100 fln maps for each patient as input to the downstream
prediction model.deep neural network for overall survival time prediction. by
taking the obtained fln maps as input, we apply a 3d resnet-based backbone
network transferred from the encoder of medicalnet [6] to extract cnn features
from each fln map. the features are then combined using the average pooling
operation and fed to a fully-connected layer with kernel size (1, 1, 1) to
classify each gbm patient into one of the three overall survival time groups
(i.e., short-term survival, mid-term survival, and long-term survival).",8
2150,Overall Survival Time Prediction of Glioblastoma on Preoperative MRI Using Lesion Network Mapping,3.1,Experimental Settings,"implementation details. our proposed method was implemented in pytorch 1.13.1 on
nvidia a100 tensor core gpus. the loss function was the standard cross-entropy
loss. the adam optimizer with the weight decay of 10 -5 was adopted. three 3d
resnet-based backbones with different numbers of layers (10, 50, and 101) were
performed, where the initial learning rates were set as 10 -4 , 10 -4 , and 10
-5 , respectively, and would decrease by a factor of 5 if the classification
performance is not improved within 5 epochs. the number of epochs for training
was 50, and the batch size was fixed as 64.performance evaluation. we evaluated
the classification performance of our proposed method using 235 gbm patients in
the brats 2020 training dataset, because only these 235 patients had both
overall survival time and manual expert segmentation labels of lesions. in all
experiments, we conducted five-fold crossvalidation ten times in order to reduce
the effect of sampling bias. moreover, the a-lnm was performed ten times
randomly to avoid particular data distribution and obtain more reliable results.
the classification results were reported in terms of accuracy, macro precision
(macro-p), macro recall (macro-r), and macro f1 score (macro-f1), respectively.",8
2152,Overall Survival Time Prediction of Glioblastoma on Preoperative MRI Using Lesion Network Mapping,3.3,Brain Regions in Relation to GBM Survival,"to identify the most discriminative brain regions associated with overall
survival time in gbm, we estimated the relative contribution of each voxel to
the classification performance in our proposed method by using the grad-cam
[10]. to obtain steady results, as shown in fig. 2(a), the voxels with top 5%
weights in the class activation maps (cams) of all candidate models were
overlapped by class, and the position covered by more than half of the models is
displayed. the cams of three classes of survivors overlapped in fig. 2(b) where
both coincident and non-coincident areas exist.the association of an increased
degree of invasion within the frontal lobe with decreased survival time can be
observed, which is in concordance with a previous study [20]. patients whose
frontal lobe is affected by tumors showed more executive dysfunction, apathy,
and disinhibition [11]. on the dominant left hemisphere, the cams of long-term
survivors and mid-term survivors overlapped at the superior temporal gyrus and
wernicke's area which are involved in the sensation of sound and language
comprehension respectively, and have been associated with decreased survival in
patients with high-grade glioma [26]. in addition, the cam of mid-term survivors
covered more areas of the middle and inferior temporal gyri which were
considered as one of the higher level ventral streams of visual processing
linked to facial recognition [25].",8
2153,Overall Survival Time Prediction of Glioblastoma on Preoperative MRI Using Lesion Network Mapping,4,Conclusion,"in this paper, we introduce a novel neuroimaging feature family, called a-lnm
derived fln maps, for overall survival time prediction of gbm patients. a-lnm
was presented to generate plenty of fln maps for each gbm patient by
partitioning the fdc maps obtained from resting-state fmri of 1000 gsp healthy
subjects into disjoint subsets of equal size and averaging each subset. we
applied a 3d resnet-based backbone network to extract features from the
generated fln maps and classify gbm patients into three overall survival time
groups. experimental results on the brats 2020 training dataset validated the
effectiveness of the a-lnm derived fln maps for gbm survival prediction.
moreover, a visualization analysis implemented by the grad-cam revealed the
brain regions associated with gbm survival. in future work, we will try to fuse
the fln maps and mri-based radiomics features to study their combined predictive
power for gbm survival analysis.",8
2154,A Multi-task Network for Anatomy Identification in Endoscopic Pituitary Surgery,1.0,Introduction,"a difficulty faced by surgeons performing endoscopic pituitary surgery is
identifying the areas of the bone which are safe to open. this is of particular
importance during the sellar phase as there are several critical anatomical
structures within close proximity of each other [9]. the sella, behind which the
pituitary tumour is located, is safe to open. however, the smaller structures
surrounding the sella, behind which the optic nerves and internal carotid
arteries are located, carry greater risk. failure to appreciate these critical
parasellar neurovascular structures can lead to their injury, and adverse
outcomes for the patient [9,11]. the human identification of these structures
relies on visual clues, inferred from the impressions made on the bone, rather
than direct visualisations of the structures [11]. this is especially
challenging as the pituitary tumour often compresses; distorts; or encases the
surrounding structures [11]. neurosurgeons utilise identification instruments,
such as a stealth pointer or micro-doppler, to aid in this task [9]. however,
once an identification instrument is removed, identification is lost upon
re-entry with a different instrument, and so the identification can only be used
in referenced to the more visible anatomical landmarks. automatic identification
from endoscopic vision may therefore aid surgeons in this effort while
minimising disruption to the surgical workflow [11]. this is a challenging
computer vision task due to the narrow camera angles enforced by minimally
invasive surgery, which lead to: (i) structure occlusions by instruments and
biological factors (e.g., blood); and (ii) image blurring caused by rapid camera
movements. additionally, in this specific task there are: (iii) numerous small
structures; (iv) visually similar structures; and (v) unclear structure
boundaries. hence, the task can be split into two sub-tasks to account for these
difficulties in identification: (1) the semantic segmentation of the two larger,
visually distinct, and frequently occurring structures (sella and clival
recess); and (2) the centroid detection of the eight smaller structures (fig.
1).to solve both tasks simultaneously, painet (pituitary anatomy identification
network) is proposed. this paper's contribution is therefore:1. the automated
identification of the ten critical anatomical structures in the sellar phase of
endoscopic pituitary surgery. to the best of the authors' knowledge, this is the
first work addressing the problem at this granularity. 2. the creation of
painet, a multi-task neural network capable of simultaneously semantic
segmentation and centroid detection of numerous anatomical structures within
minimally invasive surgery. painet uniquely utilises two loss functions for
improved performance over single-task neural networks due to the increased
information gain from the complementary task.",9
2158,A Multi-task Network for Anatomy Identification in Endoscopic Pituitary Surgery,5.0,Dataset Description,"images: images come from 64-videos of endoscopic pituitary surgery where the
sellar phase is present [9], recorded between 30 aug 2018 and 20 feb 2021 from
the national hospital of neurology and neurosurgery, london, united kingdom. all
patients have provided informed consent, and the study was registered with the
local governance committee. a high-definition endoscope (hopkins telescope, karl
storz endoscopy) was used to record the surgeries at 24 frames per second (fps),
with at least 720p resolution, and stored as mp4 files. 10-images corresponding
to 10-s of the sellar phase immediately preceding sellotomy were extracted from
each video at 1 fps, and stored as 720p png files. video upload and annotation
was performed using touch surgery tm enterprise. annotations: expert
neurosurgeons identified 10-anatomical-structures as critical based on the
literature (fig. 1a) [9,11]. 640-images were manually segmented to obtain
ground-truth segmentations. a two-stage process was used: (1) two neurosurgeons
segmented each image, with any differences settled through discussion; (2) two
consultant neurosurgeons independently peer-reviewed the segmentations. only
visible structures were annotated (fig. 1b); if the structures were occluded,
the segmentation boundaries were drawn around these occlusions (fig. 1c); and if
an image is too blurry to see the structures no segmentation boundaries were
drawn -this excluded 5 images (fig. 1d). the center of mass of each segmentation
mask was defined as the centroid.the sella is present in all 635-images (fig.
3). other than the clival recess, the remaining 8-structures are found in less
than 65% of images, with planum sphenoidal found in less than 25% of images.
moreover, the area covered by these 8-structures are small, with several
covering less than 10% of the total area covered by all structures in a given
image. furthermore, most smaller structures boundaries are ambiguous as they are
hard to define even by expert neurosurgeons. this emphasizes the challenge of
identifying smaller structure in computer vision, and supports the need for
detection and multi-task solutions.",9
2164,Intraoperative CT Augmentation for Needle-Based Liver Interventions,1.0,Introduction,"needle-based liver tumor ablation techniques (e.g., radiofrequency, microwave,
laser, cryoablation) have a great potential for local curative tumor control
[1], with comparable results to surgery in the early stages for both primary and
secondary cancers. furthermore, as it is minimally invasive, it has a low rate
of major complications and procedure-specific mortality, and is tissue-sparing,
thus, its indications are growing exponentially and extending the limits to more
advanced tumors [3]. ct-guidance is a widely used imaging modality for placing
the needles, monitoring the treatment, and following up patients. however, it is
limited by the exposure to ionizing radiation and the need for intravenous
injection of contrast agents to visualize the intrahepatic vessels and the
target tumor(s).in standard clinical settings, the insertion of each needle
requires multiple check points during its progression, fine-tune maneuvers, and
eventual repositioning. this leads to multiple ct acquisitions to control the
progression of the needle with respect to the vessels, the target, and other
sensible structures [26]. however, intrahepatic vessels (and some tumors) are
only visible after contrast-enhancement, which has a short lifespan and
dose-related deleterious kidney effects. it makes it impossible to perform each
of the control ct acquisitions under contrast injection. a workaround to
shortcut these limitations is to perform an image fusion between previous
contrasted and intraoperative noncontrasted images. however, such a solution is
only available in a limited number of clinical settings, and the registration is
only rigid, usually deriving into bad results. in this work, we propose a method
for visualizing intrahepatic structures after organ motion and needle-induced
deformations, in non-injected images, by exploiting image features that are
generally not perceivable by the human eye in common clinical workflows.to
address this challenge, two main strategies could be considered: image fusion
and image processing techniques. image fusion typically relies on the estimation
of rigid or non-rigid transformations between 2 images, to bring into the
intraoperative image structures of interest only visible in the preoperative
data. this process is often described as an optimization problem [9,10] which
can be computationally expensive when dealing with non-linear deformations,
making their use in a clinical workflow limited. recent deep learning approaches
[11,12,14] have proved to be a successful alternative to solve image fusion
problems, even when a large non-linear mapping is required. when ground-truth
displacement fields are not known, state-of-the-art methods use unsupervised
techniques, usually an encoder-decoder architecture [7,13], to learn the unknown
displacement field between the 2 images. however, such unsupervised methods fail
at solving our problem due to lack of similar image features between the
contrasted (cct) and non-contrasted (ncct) image in the vascular tree region
(see sect. 3.3).on the other hand, deep learning techniques have proven to be
very efficient at solving image processing challenges [15]. for instance, image
segmentation [16], image style transfer [17], or contrast-enhancement to cite a
few. yet, segmenting vessels from non-contrasted images remains a challenge for
the medical imaging community [16]. style transfer aims to transfer the style of
one image to another while preserving its content [17][18][19]. however,
applying such methods to generate a contrasted intraoperative ct is not a
sufficiently accurate solution for the problem that we address.
contrast-enhancement methods could be an alternative. in the method proposed by
seo et al. [20], a deep neural network synthesizes contrast-enhanced ct from non
contrast-enhanced ct. nevertheless, results obtained by this method are not
sufficiently robust and accurate to provide an augmented intraoperative ct on
which needle-based procedures can be guided.in this paper we propose an
alternative approach, where a neural network learns local image features in a
ncct image by leveraging the known preoperative vessel tree geometry and
topology extracted from a matching (undeformed) cct. then, the augmented ct is
generated by fusing the deformed vascular tree with the non-contrasted
intraoperative ct. section 2 presents the method and its integration in the
medical workflow. section 3 presents and discusses the results, and finally we
conclude in sect. 4 and highlight some perspectives.",9
2169,Intraoperative CT Augmentation for Needle-Based Liver Interventions,2.4,Augmented CT,"once the network has been trained on the patient-specific preoperative data, the
next step is to augment and visualize the intraoperative ncct. this is done in 3
steps:-the dilatation operations introduced in sect. 2.1 are not reversible
(i.e. the segmented vessel tree cannot be recovered from the vm by applying the
same number of erosion operations). also, neighboring branches in the vessel
tree could end up being fused, thus changing the topology of the vessel map.
therefore, to retrieve the correct segmented (yet deformed) vascular tree, we
compute a displacement field between the pre-and intraoperative vms. this is
done with the elastix library [21,22]. the resulting displacement field is
applied on the preoperative segmentation to retrieve the intraoperative vessel
tree segmentation. this is illustrated in fig. 4. -the augmented image is
obtained by fusing the predicted intraoperative segmentation with the
intraoperative ncct image. the augmented vessels are displayed in green to
ensure the clinician is aware this is not a true cct image (see fig. 5). -it is
also possible to add anatomical labels to the intraoperative augmented ct to
further assist the clinician. to achieve this objective, we compute a graph data
structure from the preoperative segmentation. we first extract the vessel
centerlines as described in [4]. to define the associated graph structure, we
start by selecting all branches with either no parent or no children. the branch
with the highest radius is then selected as the root edge. an oriented graph is
created using a breadth first search algorithm starting from the root edge.
nodes and edges correspond respectively to vessel tree bifurcations and
branches. we use the graph structure to associate each anatomical label
(manually defined) with a strahler [6] graph ordering. the same process is
applied to the predicted intraoperative segmentation. this makes it possible to
correctly map the preoperative anatomical labels (e.g. vessel name) and display
them on the augmented image.fig. 4. this figure illustrates the different stages
of the pipeline adopted to generate the vm and show how the vessel tree topology
is retrieved from the predicted intraoperative vm by computing a displacement
field between the preoperative vm and the predicted vm. this field is applied to
the preoperative segmentation to get the intraoperative one.",9
2170,Intraoperative CT Augmentation for Needle-Based Liver Interventions,3.1,Dataset and Implementation Details,"to validate our approach, 4 couples of mpcect abdominal porcine images were
acquired from 4 different subjects. for a given subject, each couple corresponds
to a preoperative and an intraoperative mpcect. we recall that an mpcect
contains a set of registered ncct and cct images. these images are then cropped
and down-sampled to 256 × 256 × 256, and the voxels intensities are scaled
between 0 and 255. finally, we extract the vm from each mpcect sample and apply
3 dilation operations, which demonstrated the best performance in terms of
prediction accuracy and robustness on our data. we note that public data sets
such as deeplesion [24], 3dircadb-01 [25] and others do not fit our problem
since they do not include the ncct images. aiming at a patientspecific
prediction, we only train on a ""subject"" at a time. for a given subject, we
generate 100 displacement fields using the data augmentation strategy explained
above with 50 voxels for the control points spacing in the three spatial
directions and a standard deviation of 5 voxels for the normal distributions.
the resulting deformation is applied to the preoperative mpcect and its
corresponding vm. thus, we end up with a set of 100 triplets (ncct, cct and vm).
two out of the 100 triplets are used for each training batch, where one is
considered as the pre-operative mpcect and the other as the intraoperative one.
this makes it possible to generate up to 4950 training and validation samples.
the intraoperative mpcect of the same subject is used to test the network. our
method is implemented in tensorflow 2.4, on a geforce rtx 3090. we use an adam
optimizer (β 1 = 0.001, β 2 = 0.999) with a learning rate of 10 -4 . the
training process converges in about 1,000 epochs with a batch size of 1 and 200
steps per epoch.",9
2174,Intraoperative CT Augmentation for Needle-Based Liver Interventions,4.0,Conclusion,"in this paper, we proposed a method for augmenting intra-operative ncct images
as a means to improve needle ct-guided techniques while reducing the need for
contrast agent injection during tumor ablation procedures, or other needle-based
procedures. our method uses a u-net architecture to learn local vessel tree
image features in the ncct by leveraging the known vessel tree geometry and
topology extracted from a matching cct image. the augmented ct is generated by
fusing the predicted vessel tree with the ncct. our method is validated on
several porcine images, achieving an average dice score of 0.81 on the predicted
vessel tree location. in addition, it demonstrates robustness even in the
presence of large deformations between the preoperative and intraoperative
images. our future steps will essentially involve applying this method to
patient data and perform a small user study to evaluate the usefulness and
limitations of our approach.aknowledgments. this work was partially supported by
french state funds managed by the anr under reference anr-10-iahu-02 (ihu
strasbourg). the authors would like to thank paul baksic and robin enjalbert for
proofreading the manuscript.",9
2175,Optical Coherence Elastography Needle for Biomechanical Characterization of Deep Tissue,1.0,Introduction,"healthy and cancerous soft tissue display different elastic properties, e.g. for
breast [19], colorectal [7] and prostate cancer [4]. different imaging
modalities can be used to detect the biomechanical response to an external load
for the characterization of cancerous tissue, e.g., ultrasound, magnetic
resonance and optical coherence elastography (oce). the latter is based on
optical coherence tomography (oct), which provides excellent visualization of
microstructures and superior spatial and temporal resolution in comparison to
ultrasound or magnetic resonance elastography [8]. one common approach for
quantitative oce is to determine the elastic properties from the deformation of
the sample and the magnitude of a quasi-static, compressive load [10]. however,
due to the attenuation and scattering of the near-infrared light, imaging depth
is generally limited to approximately 1 mm in soft tissue. therefore, oce is
well suited for sampling surface tissue and commonly involves bench-top imaging
systems [26], e.g. in ophthalmology [21,22] or as an alternative to
histopathological slice examination [1,16]. handheld oce systems for
intraoperative assessment [2,23] have also been proposed. while conventional oce
probes have been demonstrated at the surface, regions of interest often lie deep
within the soft tissue, e.g., cancerous tissue in percutaneous biopsy.taking
prostate cancer as an example, biomechanical characterization could guide needle
placement for improved cancer detection rates while reducing complications
associated with increased core counts, e.g. pain and erectile dysfunction
[14,18]. however, the measurement of both the applied load and the local sample
compression is challenging. friction forces superimpose with tip forces as the
needle passes through tissue, e.g., the perineum. furthermore, the prostate is
known to display large bulk displacement caused by patient movement and needle
insertions [20,24] in addition to actual sample compression (fig. 1, left). tip
force sensing for estimating elastic properties has been proposed [5] but bulk
tissue displacement of deep tissue was not considered. in principle, compression
and tip force could be estimated by oct. yet, conventional oce probes typically
feature flat tip geometry [13,17].to perform oce in deep tissue structures, we
propose a novel bevel tip oce needle design for the biomechanical
characterization during needle insertions. we consider a dual-fiber setup with
temporal multiplexing for the combined load and compression sensing at the
needle tip. we design an experimental setup that can simulate friction forces
and bulk displacement occurring during needle biopsy (fig. 1). we consider
tissue-mimicking phantoms for surface and deep tissue indentation experiments
and compare our results with force-position curves externally measured at the
needle shaft. finally, we consider how the obtained elasticity estimates can be
used for the classification of both materials.",9
2187,Pelvic Fracture Segmentation Using a Multi-scale Distance-Weighted Neural Network,2.4,Data and Annotation,"although large-scale datasets on pelvic segmentation have been studied in some
research [13], to the best of our knowledge, currently there is no
well-annotated fractured pelvic dataset publicly available. therefore, we
curated a dataset of 100 preoperative ct scans covering all common types of
pelvic fractures. these data is collected from 100 patients (aged 18-74 years,
41 females) who were to undergo pelvic reduction surgery at beijing jishuitan
hospital between 2018 and 2022, under irb approval (202009-04). the ct scans
were acquired on a toshiba aquilion scanner. the average voxel spacing is 0.82 ×
0.82 × 0.94 mm 3 . the average image shape is 480 × 397 × 310.to generate
ground-truth labels for bone fragments, a pre-trained segmentation network was
used to create initial segmentations for the ilium and sacrum [13]. then, these
labels were further modified and annotated by two annotators and checked by a
senior expert.",9
2192,Towards Multi-modal Anatomical Landmark Detection for Ultrasound-Guided Brain Tumor Resection with Contrastive Learning,1.0,Introduction,"gliomas are the most common central nervous system (cns) tumors in adults,
accounting for 80% of primary malignant brain tumors [1]. early surgical
treatment to remove the maximum amount of cancerous tissues while preserving the
eloquent brain regions can improve the patient's survival rate and functional
outcomes of the procedure [2]. although the latest multi-modal medical imaging
(e.g., pet, diffusion/functional mri) allows more precise pre-surigcal planning,
during surgery, brain tissues can deform under multiple factors, such as
gravity, intracranial pressure change, and drug administration. the phenomenon
is referred to as brain shift, and often invalidates the pre-surgical plan by
displacing surgical targets and other vital anatomies. with high flexibility,
portability, and cost-effectiveness, intra-operative ultrasound (us) is a
popular choice to track and monitor brain shift. in conjunction with effective
mri-us registration algorithms, the tool can help update the pre-surgical plan
during surgery to ensure the accuracy and safety of the intervention.as the true
underlying deformation from brain shift is impossible to obtain and the
differences of image features between mri and us are large, quantitative
validation of automatic mri-us registration algorithms often rely on homologous
anatomical landmarks that are manually labeled between corresponding mri and
intra-operative us scans [3]. however, manual landmark identification requires
strong expertise in anatomy and is costly in labor and time. moreover, inter-and
intra-rater variability still exists. these factors make quality assessment of
brain shift correction for us-guided brain tumor resection challenging. in
addition, due to the time constraints, similar evaluation of inter-modal
registration quality during surgery is nearly impossible, but still highly
desirable. to address these needs, deep learning (dl) holds the promise to
perform efficient and automatic inter-modal anatomical landmark
detection.previously, many groups have proposed algorithms to label landmarks in
anatomical scans [4][5][6][7][8][9]. however, almost all earlier techniques were
designed for mono-modal applications, and inter-modal landmark detection, such
as for usguided brain tumor resection, has rarely been attempted. in addition,
unlike other applications, where the full anatomy is visible in the scan and all
landmarks have consistent spatial arrangements across subjects, intra-operative
us of brain tumor resection only contains local regions of the pathology with
noncanonical orientations. this results in anatomical landmarks with different
spatial distributions across cases. to address these unique challenges, we
proposed a new contrastive learning (cl) framework to detect matching landmarks
in intra-operative us with those from mri as references. specifically, the
technique leverages two convolutional neural networks (cnns) to learn features
between mri and us that distinguish the inter-modal image patches which are
centered at the matching landmarks from those that are not. our approach has two
major novel contributions to the field. first, we proposed a multi-modal
landmark detection algorithm for us-guided brain tumor resection for the first
time. second, cl is employed for the first time in inter-modal anatomical
landmark detection. we developed and validated the proposed technique with the
public resect database [10] and compared its landmark detection accuracy against
the popular scale-invariant feature transformation (sift) algorithm in 3d [11].",9
2201,Towards Multi-modal Anatomical Landmark Detection for Ultrasound-Guided Brain Tumor Resection with Contrastive Learning,5.0,Results,"table 1 lists the mean and standard deviation of landmark identification errors
(in mm) between the predicted position and the ground truth in intra-operative
us for each patient of the resect dataset. in the table, we also provide the
severity of brain shift for each patient. here, tissue deformation measured as
mean target registration errors (mtres) with the ground truth anatomical
landmarks is classified as small (mtre below 3 mm), median (3-6 mm), or large
(above 6 mm). the results show that our cl-based landmark selection technique
can locate the corresponding us landmarks with a mean landmark identification
error of 5.88±4.79 mm across all cases while the sift algorithm has an error
18.78±4.77 mm. with a two-sided paired-samples t-test, our method outperformed
the sift approach with statistical significance (p <1e-4). when reviewing the
mean landmark identification error using our proposed technique, we also found
that the magnitude is associated with the level of brain shift. however, no such
trend is observed when using sift features for landmark identification. when
inspecting landmark identification errors across all subjects between the cl and
sift techniques, we also noticed that our cl framework has significantly lower
standard deviations (p <1e-4), implying that our technique has a better
performance consistency.",9
2218,Surgical Video Captioning with Mutual-Modal Concept Alignment,3.1,Dataset and Implementation Details,"neurosurgery video captioning dataset. to evaluate the effectiveness of surgical
video captioning, we collect a large-scale dataset with 41 surgical videos of
endonasal skull base neurosurgery. these surgical videos are recorded at the
prince of wales hospital, chinese university of hong kong, where surgeons remove
pituitary tumors through the endonasal corridor to the skull base. after
necessary data cleaning, we divide these surgical videos with resolution of 1,
920× 1, 080 into 11, 004 thirty-second video clips with clear surgical purposes.
these video clips are annotated under tool-tissue interaction (tti) principle
[18], and include a total of 16 instruments, 8 targets, and 10 surgical actions.
the annotation preprocessing follows [26] using nltk [16] toolkit. the
proportion of surgical concepts is illustrated in fig. 3. we split these video
clips at patientlevel, where the video clips of 31 patients are used for
training and the rest of 10 patients are utilized for test.endovis image
captioning dataset. we further compare our method with state-of-the-arts on the
public endovis-2018 image captioning dataset [1,23]. this dataset reveals
robotic nephrectomy procedures acquired by the da vinci x or xi system, and is
annotated with surgical actions between 9 possible tools and surgical targets
[23]. we follow the official split in [24] with 11 sequences for training and 3
sequences for test. in this way, these two datasets can comprehensively evaluate
the captioning tasks under both surgical videos and images.implementation
details. we implement our sca-net and state-of-the-art captioning methods
[5,9,21,24,26] in pytorch [20]. we optimize the sca-net and compared captioning
methods using adam with the batch size of 12 for both captioning datasets. all
models are trained for 20 and 50 epochs in neurosurgery and endovis datasets,
respectively. we adopt the step-wise learning rate decay strategy to facilitate
training convergence, where the learning rate is initialized as 1 × 10 -2 and
halved after every 5 epochs. the loss coefficients λ 1 of l scl and λ 2 of of l
mca are empirically set to 0.1 and 0.01, respectively. all experiments are
performed on a single nvidia a100 gpu.evaluation metrics. to evaluate the
captioning performance, we adopt standard metrics, including bleu@4 [19], meteor
[3], spice [2], rouge [12] and cider [22]. specifically, bleu@4 [19] evaluates
the 4-gram precision of the predicted caption, and cider [22] is based on the
n-gram similarity with tf-idf weights. meteor [3] considers both precision and
recall. rouge [12] and spice [2] measure the matching between predictions and
ground truth. the higher scores of these metrics indicate better performance in
surgical captioning.",9
2232,Regularized Kelvinlet Functions to Model Linear Elasticity for Image-to-Physical Registration of the Breast,1.0,Introduction,"image-to-physical registration is a necessary process for computer-assisted
surgery to align preoperative imaging to the intraoperative physical space of
the patient to in-form surgical decision making. most intraoperatively utilized
image-to-physical regis-trations are rigid transformations calculated using
fiducial landmarks [1]. however, with better computational resources and more
advanced surgical field monitoring sensors, nonrigid registration techniques
have been proposed [2,3]. this has made image-guided surgery more tractable for
soft tissue organ systems like the liver, prostate, and breast [4][5][6]. this
work focuses specifically on nonrigid breast registration, although these
methods could be adapted for other soft tissue organs. current guidance
technologies for breast conserving surgery localize a single tumor-implanted
seed without providing spatial information about the tumor boundary. as a
result, resections can have several centimeters of tissue beyond the cancer
margin. despite seed information and large resections, reoperation rates are
still high (~17%) emphasizing the need for additional guidance technologies such
as computer-assisted surgery systems with nonrigid registration
[7].intraoperative data available for registration is often sparse and subject
to data collection noise. image-to-physical registration methods that accurately
model an elastic soft-tissue environment while also complying with
intraoperative data constraints is an active field of research. determining
correspondences between imaging space and geometric data is required for
image-to-physical registration, but it is often an inexact and ill-posed
problem. establishing point cloud correspondences using machine learning has
been demonstrated on liver and prostate datasets [8,9]. deep learning image
registration methods like voxelmorph have also been used for this purpose [10].
however, these methods require extensive training data and may struggle with
generalizability. other non-learning image-to-physical registration strategies
include [11] which utilized a corotational linear-elastic finite element method
(fem) combined with an iterative closest point algorithm. similarly, the
registration method introduced in [12] iteratively updated the image-to-physical
correspondence between surface point clouds while solving for an optimal
deformation state.in addition to a correspondence algorithm, a technique for
modeling a deformation field is required. both [11] and [12] leverage fem, which
uses a 3d mesh to solve for unique deformation solutions. however, large
deformations can cause mesh distortions with the need for remeshing. mesh-free
methods have been introduced to circumvent this limitation. the element-free
galerkin method is a mesh-free method that requires only nodal point data and
uses a moving least-squares approximation to solve for a solution [13]. other
mesh-free methods are reviewed in [14]. although these methods do not require a
3d mesh, solving for a solution can be costly and boundary condition designation
is often unintuitive. having identified these same shortcomings, [15] proposed
regularized kelvinlet functions for volumetric digital sculpting in computer
animation applications. this sculpting approach provided de-formations
consistent with linear elasticity without large computational overhead.in this
work, we propose an image-to-physical registration method that uses regularized
kelvinlet functions as a novel deformation basis for nonrigid registration.
regularized kelvinlet functions are analytical solutions to the equations for
linear elasticity that we superpose to compute a nonrigid deformation field
nearly instantaneously [15].we utilize ""grab"" and ""twist"" regularized kelvinlet
functions with a linearized iterative reconstruction approach (adapted from
[12]) that is well-suited for sparse data registration problems. sensitivity to
regularized kelvinlet function hyperparameters is explored on a supine mr breast
imaging dataset. finally, our approach is validated on an exemplar breast cancer
case with a segmented tumor by comparing performance to previously proposed
registration methods.",9
2236,Regularized Kelvinlet Functions to Model Linear Elasticity for Image-to-Physical Registration of the Breast,3.0,Experiments and Results,"in this section, two experiments are conducted. the first explores sensitivity
to regularized kelvinlet function hyperparameters k grab , k twist , ε grab ,
and ε twist and establishes optimal hyperparameters in a training dataset of 11
breast deformations. the second validates the registration method in a breast
cancer patient and compares registration accuracy and computation time to
previously proposed methods.",9
2238,Regularized Kelvinlet Functions to Model Linear Elasticity for Image-to-Physical Registration of the Breast,3.2,Registration Methods Comparison,"this dataset consists of supine breast mr images simulating surgical
deformations from one breast cancer patient. a 71-year-old patient with invasive
mammary carcinoma in the left breast was enrolled in a study approved by the
institutional review board at vanderbilt university. skin fiducial placement,
image acquisition, arm placement, and preprocessing steps followed the same
protocol detailed in sect. 3.1. the tumor was segmented in both images by a
subject matter expert, and a 3d tumor model was created to evaluate tumor
overlap metrics after registration.regularized kelvinlet function registration
was compared to 3 other registration methods: rigid registration, an fem-based
image-to-physical registration method, and an image-to-image registration
method. a point-based rigid registration using the skin fiducials provided a
baseline comparator for accuracy without deformable correction. the fem-based
image-to-physical registration method, detailed in [12] and implemented in
breast in [16], utilizes the same optimization scheme as this method but with an
fem-generated basis. k = 40 control points were used for the fem-based
registration. the image-to-image registration method was a symmetric
diffeomorphic method with explicit b-spline regularization publicly available in
the advanced normalization toolkit (ants) repository [19,20]. image-to-image
registration would not be possible for intraoperative registration in most
surgical settings. however, it was included to demonstrate accuracy when
volumetric imaging data is available, as opposed to sparse geometric point data
as in the surgical application case. the rigid and image-to-physical
registrations were performed on a single thread of a 3.6 ghz amd ryzen 7 3700x
cpu. image-to-image registration was multithreaded on 2.3 ghz intel xeon
(e5-4610 v2) cpus.registration results for the 4 methods are shown in table 1.
the regularized kelvinlet method accuracy was comparable (if not slightly
improved) to the fem-based method for this example case. runtime for the
regularized kelvinlet method was improved compared to the fem-based method. as
expected, registration without deformable correction was poor, and
image-to-image registration had the best accuracy. registered tumor geometry
results are shown in fig. 4.",9
2241,Learning Expected Appearances for Intraoperative Registration During Neurosurgery,1.0,Introduction,"we address the important problem of intraoperative patient-to-image registration
in a new way by relying on preoperative data to synthesize plausible
transformations and appearances that are expected to be found intraoperatively.
in particular, we tackle intraoperative 3d/2d registration during neurosurgery,
where preoperative mri scans need to be registered with intraoperative surgical
views of the brain surface to guide neurosurgeons towards achieving a maximal
safe tumor resection [22]. indeed, the extent of tumor removal is highly
correlated with patients' chances of survival and complete resection must be
balanced against the risk of causing new neurological deficits [5] making
accurate intraoperative registration a critical component of
neuronavigation.most existing techniques perform patient-to-image registration
using intraoperative mri [11], cbct [19] or ultrasound [9,17,20]. for 3d-3d
registration, 3d shape recovery of brain surfaces can be achieved using
near-infrared cameras [15], phase-shift 3d shape measurement [10], pattern
projections [17] or stereovision [8]. the 3d shape can subsequently be
registered with the preoperative mri using conventional point-to-point methods
such as iterative closest point (icp) or coherent point drift (cpd). most of
these methods rely on cortical vessels that bring salient information for such
tasks. for instance, in [6], cortical vessels are first segmented using a deep
neural network (dnn) and then used to constrain a 3d/2d non-rigid registration.
the method uses physics-based modeling to resolve depth ambiguities. a manual
rigid alignment is however required to initialize the optimization.
alternatively, cortical vessels have been used in [13] where sparse 3d points,
manually traced along the vessels, are matched with vessels extracted from the
preoperative scans. a model-based inverse minimization problem is solved by
estimating the model's parameters from a set of pre-computed transformations.
the idea of pre-computing data for registration was introduced by [26], who used
an atlas of pre-computed 3d shapes of the brain surface for registration. in
[7], a dnn is trained on a set of pre-generated preoperative to intraoperative
transformations. the registration uses cortical vessels, segmented using another
neural network, to find the best transformation from the pre-generated set.the
main limitation of existing intraoperative registration methods is that they
rely heavily on processing intraoperative images to extract image features (eg.,
3d surfaces, vessels centerlines, contours, or other landmarks) to drive
registration, making them subject to noise and low-resolution images that can
occur in the operating room [2,25]. outside of neurosurgery, the concept of
pregenerating data for optimizing dnns for intraoperative registration has been
investigated for ct to x-ray registration in radiotherapy where x-ray images can
be efficiently simulated from cts as digital radiographic reconstructions
[12,27]. in more general applications, case-centered training of dnns is gaining
in popularity and demonstrates remarkable results [16].",9
2242,Learning Expected Appearances for Intraoperative Registration During Neurosurgery,0,Contribution:,"we propose a novel approach for patient-to-image registration that registers the
intraoperative 2d view through the surgical microscope to preoperative mri 3d
images by learning expected appearances. as shown in fig. 1, we formulate the
problem as a camera pose estimation problem that finds the optimal 3d pose
minimizing the dissimilarity between the intraoperative 2d image and its
pre-generated expected appearance. a set of expected appearances are synthesized
from the preoperative scan and for a set of poses covering the range of
plausible 6 degrees-of-freedom (dof) transformations. this set is used to train
a patient-specific pose regressor network to obtain a model that is
texture-invariant and is cross-modality to bridge the mri and rgb camera
modalities. similar to other methods, our approach follows a monocular
singleshot registration, eliminating cumbersome and tedious calibration of
stereo cameras, the laser range finder, or optical trackers. in contrast to
previous methods, our approach does not involve processing intraoperative images
which have several advantages: it is less prone to intraoperative image
acquisition noise; it does not require pose initialization; and is
computationally fast thus supporting real-time use. we present results on both
synthetic and clinical data and show that our approach outperformed
state-of-the-art methods.",9
2244,Learning Expected Appearances for Intraoperative Registration During Neurosurgery,2.1,Problem Formulation,"as illustrated in fig. 1, given a 3d surface mesh of the cortical vessels m,
derived from a 3d preoperative scan, and a 2d monocular single-shot image of the
brain surface i, acquired intraoperatively by a surgical camera, we seek to
estimate the 6-dof transformation that aligns the mesh m to the image i.
assuming a set of 3d points u = {u j ∈ r 3 } ⊂ m and a set of 2d points in the
image v = {v i ∈ r 2 } ⊂ i, solving for this registration problem can be
formalized as finding the 6-dof camera pose that minimizes the reprojection
error:where r ∈ so(3) and t ∈ r 3 represent a 3d rotation and 3d translation,
respectively, and a is the camera intrinsic matrix composed of the focal length
and the principal points (center of the image) while {c i } i is a
correspondence map and is built so that if a 2d point v i corresponds to a 3d
point u j where c i = j for each point of the two sets. note that the set of 3d
points u is expressed in homogenous coordinates in the minimization of the
reprojection error.in practice, finding the correspondences set {c i } i between
u and v is nontrivial, in particular when dealing with heterogeneous
preoperative and intraoperative modality pairs (mri, rgb cameras, ultrasound,
etc.) which is often the case in surgical guidance. existing methods often rely
on feature descriptors [14], anatomical landmarks [13], or organ's contours and
segmentation [6,18] involving tedious processing of the intraoperative image
that is sensitive to the computational image noise. we alleviate these issues by
directly minimizing the dissimilarity between the image i and its expected
appearance synthesized from m.by defining a synthesize function s θ that
synthesizes a new image i given a projection of a 3d surface mesh for different
camera poses, i.e. i = s θ (a[r|t], m), the optimization problem above can be
rewritten as:argminthis new formulation is correspondence-free, meaning that it
alleviates the requirement of the explicit matching between u and v. this is one
of the major strengths of our approach. it avoids the processing of i at
run-time, which is the main source of registration error. in addition, our
method is patient-specific, centered around m, since each model is trained
specifically for a given patient. these two aspects allow us to transfer the
computational cost from the intraoperative to the preoperative stage thereby
optimizing intraoperative performance.the following describes how we build the
function s θ and how to solve eq. 1.",9
2245,Learning Expected Appearances for Intraoperative Registration During Neurosurgery,2.2,Expected Appearances Synthesis,"we define a synthesis network s θ : (a[r|t], m, t) → i, that will generate a new
image resembling a view of the brain surface from the 2d projection of the input
mesh m following [r|t], and a texture t. several methods can be used to optimize
θ. however, they require a large set of annotated data [3,24] or perform only on
modalities with similar sensors [12,27]. generating rgb images from mri scans is
a challenging task because it requires bridging a significant difference in
image modalities. we choose to use a neural image analogy method that combines
the texture of a source image with a high-level content representation of a
target image without the need for a large dataset [1]. this approach transfers
the texture from t to i constrained by the projection of m using a[r|t] by
minimizing the following loss function:where g l ij (t ) is the gram matrix of
texture t at the l-th convolutional layer (pre-trained vgg-19 model), and w l,c
t class are the normalization factors for each gram matrix, normalized by the
number of pixels in a label class c of t class . this allows for the
quantification of the differences between the texture image t and the generated
image i as it is being generated. importantly, computing the inner-most sum over
each label class c allows for texture comparison within each class, for
instance: the background, the parenchyma, and the cortical vessels.in practice,
we assume constant camera parameters a and first sample a set of binary images
by randomly varying the location and orientation of a virtual camera [r|t]
w.r.t. to the 3d mesh m before populating the binary images with the textures
using s θ (see fig. 2). we restrict this sampling to the upper hemisphere of the
3d mesh to remain consistent with the plausible camera positions w.r.t.
patient's head during neurosurgery.we use the l-bfgs optimizer and 5
convolutional layers of vgg-19 to generate each image following [1] to find the
resulting parameters θ. the training to synthesize for a single image typically
takes around 50 iterations to converge.",9
2246,Learning Expected Appearances for Intraoperative Registration During Neurosurgery,2.3,Pose Regression Network,"in order to solve eq. 1, we assume a known focal length that can be obtained
through pre-calibration. to obtain a compact representation of the rotation and
since poses are restricted to the upper hemisphere of the 3d mesh (no gimbal
lock), the euler-rodrigues representation is used. therefore, there are six
parameters to be estimated: rotations r x , r y , r z and translations t x , t y
, t z . we estimate our 6-dof pose with a regression network p ω : i → p and
optimize its weights ω to map each synthetic image i to its corresponding camera
pose p = [r x , r y , r z , t x , t y , t z ] t .the network architecture of p ω
consists of 3 blocks each composed of two convolutional layers and one relu
activation. to decrease the spatial dimension, an average pooling layer with a
stride of 2 follows each block except the last one. at the end of the last
hierarchy, we add three fully-connected layers with 128, 64, and 32 neurons and
relu activation followed by one fully-connected with 6 neurons with a linear
activation. we use the set of generated expected appearances t p = {(i i ; p i
)} i ; and optimize the following loss function over the parameters ω of the
network p ω :where t and r vec are the translation and rotation vector,
respectively. we experimentally noticed that optimizing these entities
separately leads to better results. the model is trained for each case (patient)
for 200 epochs using mini-batches of size 8 with adam optimizer and a learning
rate of 0.001 and decays exponentially to 0.0001 over the course of the
optimization. finally, at run-time, given an image i we directly predict the
corresponding 3d pose p so that: p ← p(i; ω), where ω is the resulting
parameters from the training.",9
2247,Learning Expected Appearances for Intraoperative Registration During Neurosurgery,3.0,Results,"dataset. we tested our method retrospectively on 6 clinical datasets from 6
patients (cases) (see fig. 5). these consisted of preoperative t1 contrast mri
scans and intraoperative images of the brain surface after dura opening.
cortical vessels around the tumors were segmented and triangulated to generate
3d meshes using 3d slicer. we generated 100 poses for each 3d mesh (i.e.: each
case) and used a total of 15 unique textures from human brain surfaces
(different from our 6 clinical datasets) for synthesis using s θ . in order to
account for potential intraoperative brain deformations [4] we augment the
textured projection with elastic deformation [21] resulting in approximately
1500 images per case. the surgical images of the brain (left image of the
stereoscopic camera) were acquired with a carl zeiss surgical microscope. the
ground-truth poses were obtained by manually aligning the 3d meshes on their
corresponding images. we evaluated the pose regressor network on both synthetic
and real data. the model training and validation were performed on the
synthesized images while the model testing was performed on the real images.
because a conventional train/validation/test split would lead to texture
contamination, we created our validation dataset so that at least one texture is
excluded from the training set. on the other hand, the test set consisted of the
real images of the brain surface acquired using the surgical camera and are
never used in the training. accuracy-threshold curves on the validation
set.metrics. we chose the average distance metric (add) as proposed in [23] for
evaluation. given a set of mesh's 3d vertices, the add computes the mean of the
pairwise distance between the 3d model points transformed using the ground truth
and estimated transformation. we also adjusted the default 5 cm-5 deg
translation and rotation error to our neurosurgical application and set the new
threshold to 3 mm-3 deg.accuracy-threshold curves. we calculated the number of
'correct' poses estimated by our model. we varied the distance threshold on the
validation sets (excluding 2 textures) in order to reveal how the model performs
w.r.t. that threshold. we plotted accuracy-threshold curves showing the
percentage of pose accuracy variation with a threshold in a range of 0 mm to 20
mm. we can see in fig. 3 that a 80.23% pose accuracy was reached within the 3
mm-3 deg threshold for all cases. this accuracy increases to 95.45% with a 5
mm-5 deg threshold.",9
2249,Learning Expected Appearances for Intraoperative Registration During Neurosurgery,4.0,Discussion and Conclusion,"clinical feasibility. we have shown that our method is clinically viable. our
experiments using clinical data showed that our method provides accurate
registration without manual intervention, that it is computationally efficient,
and it is invariant to the visual appearance of the cortex. our method does not
require intraoperative 3d imaging such as intraoperative mri or ultrasound,
which require expensive equipment and are disruptive during surgery. training
patient-specific models from preoperative imaging transfers computational tasks
to the preoperative stage so that patient-to-image registration can be performed
in near real-time from live images acquired from a surgical
microscope.limitations. the method presented in this paper is limited to 6-dof
pose estimation and does not account for deformation of the brain due to changes
in head position, fluid loss, or tumor resection and assumes a known focal
length. in the future, we will expand our method to model non-rigid deformations
of the 3d mesh and to accommodate expected changes in zoom and focal depth
during surgery. we will also explore how texture variability can be controlled
and adapted to the observed image to improve model accuracy.",9
2250,Learning Expected Appearances for Intraoperative Registration During Neurosurgery,0,Conclusion.,"we introduced expected appearances, a novel learning-based method for
intraoperative patient-to-image registration that uses synthesized expected
images of the operative field to register preoperative scans with intraoperative
views through the surgical microscope. we demonstrated state-ofthe-art,
real-time performance on challenging neurosurgical images using our method. our
method could be used to improve accuracy in neuronavigation and in image-guided
surgery in general.",9
2251,Dose Guidance for Radiotherapy-Oriented Deep Learning Segmentation,1.0,Introduction,"radiotherapy (rt) has proven effective and efficient in treating cancer
patients. however, its application depends on treatment planning involving
target lesion and radiosensitive organs-at-risk (oar) segmentation. this is
performed to guide radiation to the target and to spare oar from inappropriate
irradiation. hence, this manual segmentation step is very time-consuming and
must be performed accurately and, more importantly, must be patient-safe.
studies have shown that the manual segmentation task accounts for over 40% of
the treatment planning duration [7] and, in addition, it is also error-prone due
to expert-dependent variations [2,24]. hence, deep learning-based (dl)
segmentation is essential for reducing time-to-treatment, yielding more
consistent results, and ensuring resource-efficient clinical workflows.nowadays,
training of dl segmentation models is predominantly based on loss functions
defined by geometry-based (e.g., softdice loss [15]), distributionbased
objectives (e.g., cross-entropy), or a combination thereof [13]. the general
strategy has been to design loss functions that match their evaluation
counterpart. nonetheless, recent studies have reported general pitfalls of these
metrics [4,19] as well as a low correlation with end-clinical objectives
[11,18,22,23]. furthermore, from a robustness point of view, models trained with
these loss functions have been shown to be more prone to generalization issues.
specifically, the dice loss, allegedly the most popular segmentation loss
function, has been shown to have a tendency to yield overconfident trained
models and lack robustness in out-of-distribution scenarios [5,14]. these
studies have also reported results favoring distribution-matching losses, such
as the cross-entropy being a strictly proper scoring rule [6], providing
better-calibrated predictions and uncertainty estimates. in the field of rt
planning for brain tumor patients, the recent study of [17] shows that current
dl-based segmentation algorithms for target structures carry a significant
chance of producing false positive outliers, which can have a considerable
negative effect on applied radiation dose, and ultimately, they may impact
treatment effectiveness. in rt planning, the final objective is to produce the
best possible radiation plan that jointly targets the lesion and spares healthy
tissues and oars. therefore, we postulate that training dl-based segmentation
models for rt planning should consider this clinical objective.in this paper, we
propose an end-to-end training loss function for dl-based segmentation models
that considers dosimetric effects as a clinically-driven learning objective. our
contributions are: (i) a dosimetry-aware training loss function for dl
segmentation models, which (ii) yields improved model robustness, and (iii)
leads to improved and safer dosimetry maps. we present results on a clinical
dataset comprising fifty post-operative glioblastoma (gbm) patients. in
addition, we report results comparing the proposed loss function, called
dose-segmentation loss (doselo), with models trained with a combination of
binary cross-entropy (bce) and softdice loss functions.",9
2252,Dose Guidance for Radiotherapy-Oriented Deep Learning Segmentation,2.0,Methodology,"figure 1 describes the general idea of the proposed doselo. a segmentation model
(u-net [20]) is trained to output target segmentation predictions for the gross
tumor volume (gtv) based on patient mri sequences. predicted segmentations and
their corresponding ground-truth (gt) are fed into a dose predictor model, which
outputs corresponding dose predictions (denoted as d p and d p in fig. 1). a
pixel-wise mean squared error between both dose predictions is then a
segmentation model (u-net [20]) is trained to output target segmentation
predictions ( st ) for the gross tumor volume (gtv) based on patient mri
sequences imr. predicted ( st ) and ground-truth segmentations (st ) are fed
into the dose predictor model along with the ct-image (ict ), and oar
segmentation (sor). the dose predictor outputs corresponding dose predictions dp
and dp . a pixel-wise mean squared error between both dose predictions is
calculated, and combined with the binary crossentropy (bce) loss to form the
final loss, l total = lbce + λldsl. calculated and combined with the bce loss to
form the final loss. in the next sections we describe the adopted dose
prediction model [9,12], and the proposed doselo.",9
2253,Dose Guidance for Radiotherapy-Oriented Deep Learning Segmentation,2.1,Deep Learning-Based Dose Prediction,"recent dl methods based on cascaded u-nets have demonstrated the feasibility of
generating accurate dose distribution predictions from segmentation masks,
approximating analytical dose maps generated by rt treatment planning systems
[12]. originally proposed for head and neck cancer [12], this approach has been
recently extended for brain tumor patients [9] with levels of prediction error
below 2.5 gy, which is less than 5% of the prescribed dose. this good level of
performance, along with its ability to yield near-instant dose predictions,
enables us to create a training pipeline that guides learned features to be
dose-aware.following [12], the dose predictor model consists of a cascaded u-net
(i.e., the input to the second u-net is the output of the first concatenated
with the input to the first u-net) trained on segmentation masks, ct images, and
reference dose maps. the model's input is a normalized ct volume and
segmentation masks for target volume and oars. as output, it predicts a
continuous-valued dose map of the same dimension as the input. the model is
trained via deep supervision as a linear combination of l2-losses from the
outputs of each u-net in the cascade. we refer the reader to [9,12] for further
implementation details. we remark that the dose predictor model was also trained
with data augmentation, so imperfect segmentation masks and corresponding dose
plans are included. this allows us in this study to use the dose predictor to
model the interplay between segmentation variability and dosimetric
changes.formally, the dose prediction model m d receives as inputs:
segmentations masks for the gtv s t ∈ z w ×h and the oars s or ∈ z w ×h , the ct
image (used for tissue attenuation calculation purposes in rt) i ct ∈ r w ×h ,
and outputs m d (s t , s or , i ct ) → d p ∈ r w ×h , a predicted dose map where
each pixel value in d corresponds to the local predicted dose in gy. due to the
limited data availability, we present results using 2d-based models but remark
that their extension to 3d is straightforward. working in 2d is also feasible
from an rt point of view because the dose predictor is based on co-planar
volumetric modulated arc therapy (vmat) planning, commonly used in this clinical
scenario.",9
2256,Dose Guidance for Radiotherapy-Oriented Deep Learning Segmentation,0,Dose Prediction:,"the dose prediction model was trained on an in-house dataset comprising a total
of 50 subjects diagnosed with post-operative gbm. this includes ct imaging data,
segmentation masks of 13 oars, and the gtv. gtvs were defined according to the
estro-acrop guidelines [16]. the oars were contoured by one radiotherapist
according to [21] and verified by mutual consensus of three experienced
radiation oncology experts. each subject had a reference dose map, calculated
using a standardized clinical protocol with eclipse (varian medical systems
inc., palo alto, usa). this reference was generated on basis of a double arc
co-planar vmat plan to deliver 30 times 2 gy while maximally sparing oars. we
divided the dataset into training (35 cases), validation (5 cases), and testing
(10 cases). we refer the reader to [9] for further details.segmentation models:
to develop and test the proposed approach, we employed a separate in-house
dataset (i.e., different cases than those used to train the dose predictor
model) of 50 cases from post-operative gmb patients receiving standard rt
treatment. we divided the dataset into training (35 cases), validation (5
cases), and testing (10 cases). all cases comprise a planning ct registered to
the standard mri images (t1-post-contrast (gd), t1-weighted, t2-weighted,
flair), and gt segmentations containing oars as well as the gtv. we note that
for this first study, we decided to keep the dose prediction model fixed during
the training of the segmentation model for a simpler presentation of the concept
and modular pipeline. hence, only the parameters of the segmentation model are
updated.",9
2260,Dose Guidance for Radiotherapy-Oriented Deep Learning Segmentation,3.3,Results,"figure 2 shows results on the test set, sorted by their dosimetric impact. we
found an overall reduction of the relative mean absolute error (rmae) with
respect to the reference dose maps, from 0.449 ± 0.545, obtained via the
bce+softdice combo-loss, to 0.258 ± 0.201 for the proposed doselo (i.e., an
effective 42.5% reduction with λ = 1). this significant dose error reduction
shows the ability of the proposed approach to yield segmentation results in
better agreement with dose maps obtained using gt segmentations than those
obtained using the state-of-the-art bce+softdice combo-loss.table 1 shows
results for the first and most significant four cases from a rt point of view
(due to space limitations, all other cases are shown in supplementary material).
we observe the ability of the proposed approach to significantly reduce
outliers, generating a negative dosimetry impact on the dose fig. 2. relative
mean absolute dose errors/differences (rmae) between the reference dose map and
dose maps obtained using the predicted segmentations. lower is better. across
all tested cases and folds we observe a large rmae reduction for dose maps using
the proposed doselo (average rmae reduction of 42.5%).maps. we analyzed case
number 3, 4, and 5 from fig. 2 for which the standard bce+softdice was slightly
better than the proposed doselo. for case no. 3 the tumor presents a non-convex
shape alongside the skull's parietal lobe, which was not adequately modeled by
the training dataset used to train the segmentation models. indeed, we remark
that both models failed to yield acceptable segmentation quality in this area.
in case no. 4, both models failed to segment the diffuse tumor area alongside
the skull; however, as shown in fig. 2-case no. 4, the standard bce+softdice
model would yield a centrally located radiation dose, with strong negative
clinical impact to the patient. case no. 5 (shown in supplementary material) is
an interesting case called butterfly gbm, which is a rare type of gbm (around 2%
of all gbm cases [3]), characterized by bihemispheric involvement and invasion
of the corpus callosum. in this case, the training data also lacked
characterization for such cases. despite this limitation, we observed favorable
dose distributions with the proposed method.although we are aware that classical
segmentation metrics poorly correlate with dosimetric effects [18], we report
that the proposed method is more robust than the baseline bce+softdice loss
function, which yields outliers with hausdorff distances: 64.06 ± 29.84 mm vs
28.68 ± 22.25 mm (-55.2% reduction) for the proposed approach. as pointed out by
[17], segmentation outliers can have a detrimental effect on rt planning. we
also remark that the range of hd values is in range with values reported by
models trained using much more training data (see [1]), alluding to the
possibility that the problem of robustness might not be directly solvable with
more data. dice coefficients did not deviate significantly between the baseline
and the doselo models (dsc: 0.713 ± 0.203 (baseline) vs. 0.697 ± 0.216
(doselo)).table 1. comparison of dose maps and their absolute differences to the
reference dose maps (bce+softdice (bce+sd), and the proposed doselo). it can be
seen that doselo yields improved dose maps, which are in better agreement with
the reference dose maps (dose map color scale: 0 (blue) -70gy (red)).",9
2261,Dose Guidance for Radiotherapy-Oriented Deep Learning Segmentation,4.0,Discussion and Conclusion,"the ultimate goal of dl-based segmentation for rt planning is to provide
reliable and patient-safe segmentations for dosimetric planning and optimally
targeting tumor lesions and sparing of healthy tissues. however, current loss
functions used to train models for rt purposes rely solely on geometric
considerations that have been shown to correlate poorly with dosimetric
objectives [11,18,22,23]. in this paper, we propose a novel dosimetry-aware
training loss function, called doselo, to effectively guide the training of
segmentation models toward dosimetric-compliant segmentation results for rt
purposes. the proposed doselo uses a fast-dose map prediction model, enabling
model guidance on how dosimetry is affected by segmentation variations. we merge
this information into a simple yet effective loss function that can be combined
with existing ones. these first results on a dataset of post-operative gbm
patients show the ability of the proposed doselo to deliver improved
dosimetric-compliant segmentation results. future work includes extending our
database of gbm cases and to other anatomies, as well as verifying potential
improvements when cotraining the segmentation and dose predictor models, and
jointly segmenting gtvs and oars. with this study, we hope to promote more
research toward clinically-relevant dl training loss functions.",9
2262,FLIm-Based in Vivo Classification of Residual Cancer in the Surgical Cavity During Transoral Robotic Surgery,1.0,Introduction,"residual tumor in the cavity after head and neck cancer (hnc) surgery is a
significant concern as it increases the risk of cancer recurrence and can
negatively impact the patient's prognosis [1]. hnc comprises the third highest
positive surgical margins (psm) rate across all oncology fields [2]. achieving
clear margins can be challenging in some cases, particularly in tumors with
involved deep margins [3,4].during transoral robotic surgery (tors), surgeons
may assess the surgical margin via visual inspection, palpation of the excised
specimen and intraoperative frozen sections analysis (ifsa) [5]. in the surgical
cavity, surgeons visually inspect for residual tumors and use specimen driven or
defect-driven frozen section analysis to identify any residual tumor [6,7]. the
latter involves slicing a small portion of the tissue at the edge of the cavity
and performing a frozen section analysis. these approaches are error-prone and
can result in psms and a higher risk of cancer recurrence [7]. in an effort to
improve these results, recent studies reported the use of exogenous fluorescent
markers [8] and wide-field optical coherence tomography [9] to inspect psms in
the excised specimen. while promising, each modality presents certain
limitations (e.g., time-consuming analysis, administration of a contrast agent,
controlled lighting environment), which has limited their clinical adoption
[10,11].label-free mesoscopic fluorescence lifetime imaging (flim) has been
demonstrated as an intraoperative imaging guidance technique with high
classification performance (auc = 0.94) in identifying in vivo tumor margins at
the epithelial surface prior to tumor excision [12]. flim can generate optical
contrast using autofluorescence derived from tissue fluorophores such as
collagen, nadh, and fad. due to the sensitivity of these fluorophores to their
microenvironment, the presence of tumor changes their emission properties (i.e.,
intensity and lifetime characteristics) relative to healthy tissue, thereby
enabling the optical detection of cancer [13].however, ability of label-free
flim to identify residual tumors in vivo in the surgical cavity (deep margins)
has not been reported. one significant challenge in developing a flim-based
classifier to detect tumor in the surgical cavity is the presence of highly
imbalanced labels.surgeons aim to perform an en bloc resection, removing the
entire tumor and a margin of healthy tissue around it to ensure complete
excision. therefore, in most cases, only healthy tissue in left in the cavity.
to address the technical challenge of highly imbalanced label distribution and
the need for intraoperative real-time cavity imaging, we developed an
intraoperative flim guidance model to identify residual tumors by classifying
residual cancer as anomalies. our proposed approach identified all patients with
psm. in contrast, the ifsa reporting a sensitivity of 0.5 [6,7].",9
2264,FLIm-Based in Vivo Classification of Residual Cancer in the Surgical Cavity During Transoral Robotic Surgery,2.1,FLIm Hardware and Data Acquisition,"this study used a multispectral fluorescence lifetime imaging (flim) device to
acquire data [14]. the flim device features a 355 nm uv laser for fluorescence
excitation, which is pulsed at a 480 hz repetition rate. a 365 µm multimode
optical fiber (0.22 na) delivers excitation light to tissue and relays the
corresponding fluorescence signal to a set of dichroic mirrors and bandpass
filters to spectrally resolve the autofluorescence. three variable gain uv
enhanced si apd modules with integrated trans-impedance amplifiers receive the
autofluorescence, which is spectrally resolved as follows: (1) 390/40 nm
attributed to collagen autofluorescence, (2) 470/28 nm to nadh, and (3) 542/50
nm to fad. the resulting autofluorescence waveform measurements for each channel
are averaged four times, thus with a 480 hz excitation rate, resulting in 120
averaged measurements per second [15].the flim device includes a 440 nm
continuous wave laser that serves as an aiming beam; this aiming beam enables
real-time visualization of the locations where fluorescence (point measurements)
is collected by generating visible blue illumination at the location where data
is acquired. segmentation of the 'aiming beam' allows for flim data points to be
localized as pixel coordinates within a surgical white light image (see fig. 1).
localization of these coordinates is essential to link the regions where data is
obtained to histopathology, which is used as the ground truth to link flim
optical data to pathology status [16]. flim data was acquired using the da vinci
sp robotic surgical platform. as part of the approved protocol for this study,
the surgeon performed in vivo flim scan on the tumor epithelial surface and the
surrounding uninvolved benign tissue. upon completing the scan, the surgeon
proceeded with en bloc excision of the tissue suspected of cancer. an ex vivo
flim scan was then performed on the surgically excised specimen. finally, the
patient's surgical cavity was scanned to check for residual tumor.",9
2265,FLIm-Based in Vivo Classification of Residual Cancer in the Surgical Cavity During Transoral Robotic Surgery,2.2,Patient Cohort and FLIm Data Labeling,"the research was performed under the approval of the uc davis institutional
review board (irb) and with the patient's informed consent. all patients were
anesthetized, intubated, and prepared for surgery as part of the standard of
care. n = 22 patients are represented in this study, comprising hnc in the
palatine tonsil (n = 15) and the base of the tongue (n = 7). for each patient,
the operating surgeon conducted an en bloc surgical tumor resection procedure
(achieved by tors-electrocautery instruments), and the resulting excised
specimen was sent to a surgical pathology room for grossing. the tissue specimen
was serially sectioned to generate tissue slices, which were then
formalin-fixed, paraffin-embedded, sectioned, and stained to create hematoxylin
& eosin (h&e) slides for pathologist interpretation (see fig. 1).after the
surgical excision of the tumor, an in vivo flim scan of approximately 90 s was
conducted within the patient's surgical cavity, where the tumor was excised. to
validate optical measurements to pathology labels (e.g., benign tissue vs.
residual tumor), pathology labels from the excision margins were digitally
annotated by a pathologist on each h&e section. the aggregate of h&e sections
was correspondingly labeled on the ex vivo specimen at the cut lines where the
tissue specimen was serially sectioned.thereafter, the labels were spatially
registered in vivo within the surgical cavity. this process enables the direct
validation of flim measurements to the pathology status of the electrocauterized
surgical margins (see table 1).",9
2268,FLIm-Based in Vivo Classification of Residual Cancer in the Surgical Cavity During Transoral Robotic Surgery,2.5,Classifier Training and Evaluation,"the novelty detection model used for detecting residual cancer is evaluated at
the pointmeasurement level to assess the diagnostic capability of the method
over an entire tissue surface. the evaluation followed a leave-one-patient-out
cross-validation approach. the study further compared gods with two other
novelty detection models: robust covariance and, one-class support vector
machine (oc-svm) [22]. novelty detection model solely used healthy labels from
the in vivo cavity scan for training. the testing data contained both healthy
and residual cancer labels. we used grid search to optimize the hyper-parameters
and features used in each model and are tabulated in the supplementary section
table s1. the sensitivity, specificity, and accuracy were used as evaluation
metrics to assess the performance of classification models in the context of the
study.results of a binary classification model using svm are also shown in the
supplementary section table s2.",9
2270,FLIm-Based in Vivo Classification of Residual Cancer in the Surgical Cavity During Transoral Robotic Surgery,3.0,Results,"table 2 tabulates the classification performance comparison of novelty detection
models for classifying residual cancer vs. healthy on in vivo flim scans in the
cavity. three novelty detection models were evaluated, and all three models
could identify the presence of residual tumors in the cavity for the three
patients. however, the extent of the tumor classification over the entire tissue
surface varied among the models. the gods reported the best classification
performance with an average sensitivity of 0.75 ± 0.02 (see fig. 2). the lower
standard deviation indicates that the model generalizes well. the oc-svm and
robust covariance reported a high standard deviation, indicating that the
performance of the classification model is inconsistent across different
patients. the model's ability to correctly identify negative instances is
essential to its reliability. the gods model reported the highest mean
specificity of 0.78 ± 0.14 and the lowest standard deviation. the robust
covariance model reported the lowest specificity, classifying larger portions of
healthy tissue in the cavity as a residual tumor; indicating that the model did
not generalize well to the healthy labels. we also observed that changing the
hyper-parameter, such as the anomaly factor, biased the model toward a single
class indicating overfitting (see supplementary section fig. s1).the gods uses
two separating hyperplanes to minimize the distance between the two classifiers
by learning a low-dimensional subspace containing flim data properties of
healthy labels. residual tumor labels are detected by calculating the distance
between the projected data points and the learned subspace. points that are far
from the subspace are classified as residual tumors. we observed that the gods
with the flim decay curves in the cdt space achieve the best classification
performance compared to other novelty detection models with a mean accuracy of
0.76 ± 0.02. this is mainly due to the robustness of the model, the ability to
handle high-dimensional data, and the contrast in the flim decay curves. the
contrast in the flim decay curves was further improved in the cdt space by
transforming the flim decay curves to a normalized scale and improving linear
separability.",9
2271,FLIm-Based in Vivo Classification of Residual Cancer in the Surgical Cavity During Transoral Robotic Surgery,4.0,Discussion,"curent study demonstrates that label-free flim parameters-based classification
model, using a novelty detection aproach, enables identification of residual
tumors in the surgical cavity. the proposed model can resolve residual tumor at
the point-measurement level over a tissue surface. the model reported low
point-level false negatives and positives. moreover, the current approach
correctly identified all patients with psms (see fig. 2). this enhances surgical
precision for tors procedures otherwise limited to visual inspection of the
cavity, palpation of the excised specimen, and ifsa. the flimbased
classification model could help guide the surgical team in real-time, providing
information on the location and extent of cancerous tissue.in context to the
standard of care, the proposed residual tumor detection model exhibits high
patient-level sensitivity (sensitivity = 1) in detecting patients with psms. in
contrast, defect-driven ifsa reports a patient-level sensitivity of 0.5 [6,7].
our approach exhibits a low patient-level specificity compared to ifsa. surgeons
aim to achieve negative margins, meaning the absence of cancer cells at the
edges of the tissue removed during surgery. the finding of positive margins from
final histology would result in additional surgical resection, potentially
impacting the quality of life. combining the proposed approach and ifsa could
lead to an image-guided frozen section analysis to help surgeons achieve
negative margins in a more precise manner. therefore, completely resecting
cancerous tissue and improving patient outcomes.the false positive predictions
from the classification model presented two trends: false positives in an
isolated region and false positives spreading across a larger region. isolated
false positives are often caused by the noise of the flim system and are
accounted for by the interpolation approach used for the classifier augmentation
(refer to supplementary section fig. s2). on the other hand, false positives
spreading across a larger region are much more complex to interpret. one insight
is that the electrocautery effects on the tissues in the cavity may have
influenced them [24]. according to jackson's burn wound model, the thermal
effects caused by electrocautery vary with the different burnt zones. we
observed a correlation between a larger spread of false positive predictions
associated with a zone of coagulation to a zone of hyperemia.the novelty
detection model generalizes to the healthy labels and considers data falling off
the healthy distribution as residual cancer. the flim properties associated with
the healthy labels in the cavity are heterogeneous due to the electrocautery
effects. electrocautery effects are mainly thermal and can be observed by the
levels of charring in the tissue. refining the training labels based on the
levels of charring could lead to a more homogeneous representation of the
training set and result in an improved classification model with better
generalization.",9
2272,FLIm-Based in Vivo Classification of Residual Cancer in the Surgical Cavity During Transoral Robotic Surgery,5.0,Conclusion,"this study demonstrates a novel flim-based classification method to identify
residual cancer in the surgical cavity of the oropharynx. the preliminary
results underscore the significance of the proposed method in detecting psms.
the model will be validated on a larger patient cohort in future work and
address the limitations of the point-level false positive and negative
predictions. this work may enhance surgical precision for tors procedures as an
adjunctive technique in combination with ifsa.",9
2273,FocalErrorNet: Uncertainty-Aware Focal Modulation Network for Inter-modal Registration Error Estimation in Ultrasound-Guided Neurosurgery,1.0,Introduction,"resection of early-stage brain tumors can greatly reduce the mortality rate of
patients. during the surgery, brain tissue deformation (called brain shift) can
occur due to various causes, such as gravity, drug administration, and pressure
change after craniotomy. while modern magnetic resonance imaging (mri)
techniques can provide rich anatomical and physiological information with
various contrasts (e.g., fmri) for more elaborate pre-surgical planning,
intra-operative mri that can track brain shift requires a complex setup and is
costly. in contrast, intra-operative ultrasound (ius) has gained popularity for
real-time imaging during surgery to monitor tissue deformation and surgical
tools because of its lower cost, portability, and flexibility [1]. accurate and
robust mri-ius registration techniques [2] can greatly enhance the value of ius
for updating pre-surgical plans and guiding the interpretation of ius, which has
an unintuitive contrast and non-standard orientations. this can greatly enhance
the safety and outcomes of the surgical procedure by allowing maximum brain
tumor removal while avoiding eloquent regions [3]. however, as the true
underlying tissue deformation is unknown due to the 3d nature of the surgical
data and the time constraint, real-time manual inspection of mri-ius
registration results is challenging and error-prone, especially for
precision-sensitive neurosurgery. therefore, algorithms that can detect and
quantify unreliable inter-modal medical image registration results are highly
beneficial.recently, automatic quality assessment for medical image registration
has attracted increasing attention [4] from the domains of big medical data
analysis and surgical interventions. with high efficiency, machine, and deep
learning techniques have been proposed to allow automatic grading and dense
estimation of medical image registration errors. early endeavors on this topic
primarily relied on hand-crafted features, including information theory-based
metrics [5][6][7][8][9][10]. more recently, deep learning (dl) techniques that
learn task-specific features have also been adopted in automatic evaluation of
medical image registration, with a primary focus on intra-contrast/modal
applications, including ct [9,10] and mri [11]. unfortunately, so far, error
grading and estimation in inter-contrast/modal registration have rarely been
explored, despite the particular demand in surgical applications. in this
direction, bierbrier et al. [12] made the first attempt using simulated ius from
mri to train 3d convolutional neural networks (cnns) to perform dense error
regression for mri-ius registration in brain tumor resection. although their
algorithm performed well in simulated cases, the results on real clinical scans
still required improvements. in this paper, we propose a novel 3d cnn to perform
patch-wise error estimation for mri-ius registration in neurosurgery, by using
focal modulation [13], a recent alternative dl technique to self-attention [14]
for encoding contextual information, and uncertainty estimation. we call our
method focalerrornet, which has three main novelties. first, we adapted the
focal modulation network [13] from 2d to 3d and employed the technique in
registration error assessment for the first time. second, we incorporated
uncertainty estimation using monte carlo (mc) dropouts [15] to offer assurance
for error regression. lastly, we developed and thoroughly evaluated our
technique against a recent baseline model [12] using real clinical data and
showed excellent results.",9
2274,FocalErrorNet: Uncertainty-Aware Focal Modulation Network for Inter-modal Registration Error Estimation in Ultrasound-Guided Neurosurgery,2.1,Dataset and Preprocessing,"for methodological development and assessment, we used the resect
(retro-spective evaluation of cerebral tumors) dataset [16], which has
pre-operative mri, and ius scans at different surgical stages from 23 subjects
who underwent low-grade glioma resection surgeries. as it is still challenging
to model ius scans with tissue resection, we took 22 cases with t2flair mri that
better depicts tumor boundaries and ius acquired before resection. an example of
an mri-ius pair from a patient is shown in fig. 1. we hypothesized that directly
leveraging clinical ius could help learn more realistic image features with
potentially better outcomes in clinical applications than with simulated
contrasts [9,12]. however, since the true brain shift model is impossible to
obtain, we followed the strategy of creating silver ground truths for image
alignment [9,12], upon which simulated misalignment is augmented in the ius to
build and test our dl model. to create the silver registration ground truths, we
used the homologous landmarks between mri and ius in the resect dataset to
perform landmark-based 3d b-spline nonlinear registration to register ius to the
corresponding mri for all 22 cases. to tackle the limited field of view (fov) in
ius, we cropped the t2flair mri to the same fov of the ius, which was resampled
to a 0.5 × 0.5 × 0.5 mm 3 resolution. to perform spatial misalignment
augmentation, we continued to leverage 3d b-spline transformation, similar to
earlier reports on the same topic [10,12,17]. in short, b-spline transformation
can be modeled by a grid of regularly spaced control points and the associated
parameters to allow various levels of nonlinear deformation. while the spacing
of the control points determines the levels of details in local deformation
fields, the displacement parameters control the magnitude of the deformation. to
ensure that simulated registration errors are of different varieties and sizes,
we randomly selected the number of control points and the associated
displacements (in each 3d axis) with a maximum of 20 points and 30 mm,
respectively. note that the control point grid is isotropic, and the density is
arbitrarily determined per deformation in our case. each coregistered ius scan
was deformed ten times. after misalignment augmentation on the previously
co-registered ius, matching pairs of 3d image patches of size 33 × 33 × 33
voxels were taken from both the ius volume and the corresponding mri. as ius has
limited fov and may contain no anatomical features, to ensure that the patches
we extracted contain useful information (e.g. to avoid the dark background) in
ius, we focused on acquiring patches centered around the anatomical landmark
locations available through the resect database. since b-spline transformation
offers a displacement vector at each voxel of the ius volume, we directly
considered the norm of the vector as the simulated registration error at the
associated voxel. in our design, we determined the registration error of the
image patch pair as the mean of all voxel-wise errors within the ius patch.
finally, the image patch pairs, along with corresponding registration errors
were then fed to the proposed dl algorithm for training and validation.",9
2276,FocalErrorNet: Uncertainty-Aware Focal Modulation Network for Inter-modal Registration Error Estimation in Ultrasound-Guided Neurosurgery,2.3,Uncertainty Quantification,"for registration error regression in surgical applications, knowledge regarding
the reliability of the automated results is instrumental for the safety and
wellbeing of the patients. uncertainty estimation has gained popularity in
probing the trustworthiness and credence of dl algorithms. although the concept
has been widely applied in image segmentation and classification, it has not
been employed for registration error estimation, especially in the case of
multi-modal situations, such as mri-ius alignment. therefore, we incorporated
uncertainty estimation in our proposed focalerrornet. for each mri-ius patch
pair, 200 regression samples were collected by random sampling from mc dropouts
[15] at test time. while the final patch registration error was obtained as the
mean of all the samples, the sample standard deviation was used as the
uncertainty metric.",9
2277,FocalErrorNet: Uncertainty-Aware Focal Modulation Network for Inter-modal Registration Error Estimation in Ultrasound-Guided Neurosurgery,2.4,Experimental Setup and Implementation,"from the transformation augmentation, we acquired 3380 samples of mri-ius pairs.
for our experiments, we arbitrarily split the subjects into training,
validation, and test sets with the proportion of 60%, 20%, and 20%,
respectively. to prevent information leakage, we ensured that each patient was
included in only one of the split sets. for model training, we adopted the adam
optimization with a learning rate of 5 × 10 -5 and a batch size of 64. for the
loss function, we used mean squared error (mse) to minimize the difference
between the predicted mri-ius registration error and the ground truths.
furthermore, in addition to the transformation augmentation, we also included
additional data augmentation, including random noise addition and random image
flipping on training sets to mitigate overfitting and increase the model's
generalizability. to assess our proposed focalerrornet, we compared it against a
3d cnn [9,12] (see fig. 3) that was employed for medical image registration
error regression. the two dl models were trained with the same dataset and
procedure, and their prediction accuracies, measured as the absolute error
between the predicted and ground truths mis-registration on the test set were
compared with two-sided pairedsamples t-tests to confirm the superiority of the
proposed method, in addition to correlations between their estimated and ground
truth errors. to validate the proposed uncertainty estimation method, we
calculated the correlation between the uncertainty measure and absolute error of
focalerrornet, and the correlation between the uncertainty and mutual
information between mri and ius, which is often used to measure the information
overlap in multi-modal registration. finally, to test the robustness of the
focalerrornet, we acquired additional mri-ius patch pairs from the test
subjects, by introducing random linear shifts (the max displacement from
landmark locations is 10 voxels) from the selected locations in the original
set, and evaluated the dl model performance.",9
2281,FocalErrorNet: Uncertainty-Aware Focal Modulation Network for Inter-modal Registration Error Estimation in Ultrasound-Guided Neurosurgery,4.0,Discussion,"in image-guided interventions, there is an urgent need for automatic assessment
of image registration quality. multi-modal registration quality evaluation poses
major challenges due to three main factors. first, dissimilar contrasts between
images require more elaborate strategies to derive relevant features for error
assessment. second, unlike segmentation or classification, the ground truths of
registration errors are difficult to obtain. finally, compared with
classification, regression tasks tend to be more error-prone for deep learning
algorithms. to tackle these challenges, we employed 3d focal modulation with
depth-wise convolution to encode contextual information for the image pair.
compared with the vit and its variants, focal modulation allows a more
lightweight setup, which could be desirable for 3d data. although we admit that
residual errors still remain after landmark-based b-spline nonlinear alignment,
this approach has been adopted in different prior studies, considering the
residual landmark registration error is fairly low (mtre of 0.0008 ± 0.0010mm).
although simulated ultrasound has been used to provide a perfect alignment with
mris, the fidelity of the simulated results is still suboptimal, and this may
explain the underperformance of the previous technique in real clinical data
[12]. to ensure the performance of our focalerrornet, we opted to regress the
mean registration error of image patches than simplistic error grades or
voxel-wise error maps. we believe that this design choice offers a more stable
performance, which is supported by our validation. we adopted uncertainty
estimation in inter-modal registration error assessment for the first time.
while other techniques exist to provide model uncertainty [18], mc dropout is
more flexible for various dl models. furthermore, the use of standard deviation
as an uncertainty measurement maintains the same unit as the regressed errors,
thus making the interpretation more intuitive. from quantitative and qualitative
evaluations using correlation coefficients and scatter plots to assess the
association of uncertainty measures with the prediction errors and image
entropy, we confirmed the validity of the proposed uncertainty estimation
approach. for our focalerrornet, we achieved a prediction error of 0.59 ± 0.57
mm, which is on par with the image resolution (0.5 mm). additionally, the
standard deviation of our results is lower than the baseline model [12]. these
signify a robust performance of the focalerrornet. one limitation of our work
lies in the limited patient data, as public ius datasets are scarce, while the
settings and properties of us scanners can vary, potentially affecting the dl
model designs. therefore, we created random deformations for patch-wise error
estimation, and will further explore data-efficient approaches for registration
error assessment.",9
2283,Detecting the Sensing Area of a Laparoscopic Probe in Minimally Invasive Cancer Surgery,1.0,Introduction,"cancer remains a significant public health challenge worldwide, with a new
diagnosis occurring every two minutes in the uk (cancer research uk 1 ). surgery
is one of the main curative treatment options for cancer. however, despite
substantial advances in pre-operative imaging such as ct, mri, or pet/spect to
aid diagnosis, surgeons still rely on the sense of touch and naked eye to detect
cancerous tissues and disease metastases intra-operatively due to the lack of
reliable intraoperative visualization tools. in practice, imprecise
intraoperative cancer tissue detection and visualization results in missed
cancer or the unnecessary removal of healthy tissues, which leads to increased
costs and potential harm to the patient. there is a pressing need for more
reliable and accurate intraoperative visualization tools for minimally invasive
surgery (mis) to improve surgical outcomes and enhance patient care. a recent
miniaturized cancer detection probe (i.e., 'sensei r ' developed by lightpoint
medical ltd.) leverages the cancer-targeting ability of nuclear agents typically
used in nuclear imaging to more accurately identify cancer intraoperatively from
the emitted gamma signal (see fig. 1b) [6]. however, the use of this probe
presents a visualization challenge as the probe is non-imaging and is air-gapped
from the tissue, making it challenging for the surgeon to locate the
probe-sensing area on the tissue surface.it is crucial to accurately determine
the sensing area, with positive signal potentially indicating cancer or affected
lymph nodes. geometrically, the sensing area is defined as the intersection
point between the gamma probe axis and the tissue surface in 3d space, but
projected onto the 2d laparoscopic image. however, it is not trivial to
determine this using traditional methods due to poor textural definition of
tissues and lack of per-pixel ground truth depth data. similarly, it is also
challenging to acquire the probe pose during the surgery.problem redefinition.
in this study, in order to provide sensing area visualization ground truth, we
modified a non-functional 'sensei' probe by adding a miniaturized laser module
to clearly optically indicate the sensing area on the laparoscopic images -i.e.
the 'probe axis-surface intersection'. our system consists of four main
components: a customized stereo laparoscope system for capturing stereo images,
a rotation stage for automatic phantom movement, a shutter for illumination
control, and a daq-controlled switchable laser module (see fig. 1a). with this
setup, we aim to transform the sensing area localization problem from a
geometrical issue to a high-level content inference problem in 2d. it is
noteworthy that this remains a challenging task, as ultimately we need to infer
the probe axis-surface intersection without the aid of the laser module to
realistically simulate the use of the 'sensei' probe.",9
2318,Unified Brain MR-Ultrasound Synthesis Using Multi-modal Hierarchical Representations,4.0,Experiments,"in this section, we report experiments conducted on the challenging problem of
mr and ius image synthesis.data. we evaluated our method on a dataset of 66
consecutive adult patients with brain gliomas who were surgically treated at the
brigham and women's hospital, boston usa, where both pre-operative 3d t2-space
and pre-dural opening intraoperative us (ius) reconstructed from a tracked
handheld 2d probe were acquired. the data will be released on tcia in 2023. 3d
t2-space scans were affinely registered with the pre-dura ius using niftyreg
[20] following the pipeline described in [10]. three neurological experts
manually checked registration outputs. the dataset was randomly split into a
training set (n = 56) and a testing set (n = 10). images were resampled to an
isotropic 0.5 mm resolution, padded for an in-plane matrix of (192,192) using
spade [21], pix2pix [14], mvae [30], resvit [4] and mhvae (ours) without and
with gan loss. as highlighted by the arrows, our approach better preserves
anatomy compared to gan-based approach and produces more realistic approach than
the transformer-based approach (resvit).for the encoder and decoder from
mobilenetv2 [23] are used with squeeze and excitation [13] and swish activation.
the image decoders (μ i ) m i=1 correspond to 5 resnet blocks. following
state-of-the-art bidirectional inference architectures [19,27], the
representations extracted in the contracting path (from x i to (z l ) l ) and
the expansive path (from z l to x i and (z l ) l<l ) are partially shared.
models are trained for 1000 epochs with a batch size of 16. to improve
convergence, λ gan is set to 0 for the first 800 epochs. network architecture is
presented in appendix, and the code is available at https://github.com/reubendo/
mhvae.evaluation. since paired data was available for evaluation, standard
supervised evaluation metrics are employed: psnr (peak signal-to-noise ratio),
ssim (structural similarity), and lpips [31] (learned perceptual image patch
similarity). quantitative results are presented in table 1, and qualitative
results are shown in fig. 2. wilcoxon signed rank tests (p < 0.01) were
performed.ablation study. to quantify the importance of each component of our
approach, we conducted an ablation study. first, our model (mhvae) was compared
with mvae, the non-hierarchical multi-modal vae described in [30]. it can be
observed in table 1 that mhvae (ours) significantly outperformed mvae. this
highlights the benefits of introducing a hierarchy in the latent representation.
as shown in fig. 2, mvae generated blurry images, while our approach produced
sharp and detailed synthetic images. second, the impact of the gan loss was
evaluated by comparing our model with (λ gan = 0) and without (λ gan = 1) the
adversarial loss. both models performed similarly in terms of evaluation
metrics. however, as highlighted in fig. 2, adding the gan loss led to more
realistic textures with characteristic ius speckles on synthetic ius. finally,
the image similarity between the target and reconstructed images (i.e., target
image used as input) was excellent, as highlighted in table 1. this shows that
the learned latent representations preserved the content information from input
modalities. state-of-the-art comparison. to evaluate the performance of our
model (mhvae) against existing image synthesis frameworks, we compared it to two
state-of-the-art gan-based conditional image synthesis methods: pix2pix [14] and
spade [21]. these models have especially been used as synthesis backbones in
previous mr/ius synthesis studies [6,15]. results in table 1 show that our
approach statistically outperformed these gan methods with and without
adversarial learning. as shown in fig. 2, these conditional gans produced
realistic images but did not preserve the brain anatomy. given that these models
are not unified, pix2pix and spade must be trained for each synthesis direction
(t 2 → ius and ius → t 2 ). in contrast, mhvae is a unified approach where one
model is trained for both synthesis directions, improving inference practicality
without a drop in performance. finally, we compared our approach with resvit
[4], a transformer-based method that is the current state-of-the-art for unified
multi-modal medical image synthesis. our approach outperformed or reached
similar performance depending on the metric. in particular, as shown in fig. 2
and in table 1 for the perceptual lpips metric, our gan model synthesizes images
that are visually more similar to the target images. finally, our approach
demonstrates significantly lighter computational demands when compared to the
current sota unified image synthesis framework (resvit), both in terms of time
complexity (8g macs vs. 487g macs) and model size (10m vs. 293m parameters).
compared to mvaes, our hierarchical multi-modal approach only incurs a marginal
increase in time complexity (19%) and model size (4%).overall, this set of
experiments demonstrates that variational auto-encoders with hierarchical latent
representations, which offer a principled formulation for fusing multi-modal
images in a shared latent representation, are effective for image synthesis.",10
2323,StructuRegNet: Structure-Guided Multimodal 2D-3D Registration,1.0,Introduction,"2d-3d registration refers to the highly challenging process of aligning an input
2d image to its corresponding slice inside a given 3d volume [4]. it has
received growing attention in medical imaging due to the various contexts where
it applies, like image fusion between 2d real-time acquisitions and either
pre-operative 3d images for guided interventions or reference planning volumes
for patient positioning in radiation therapy (rt). another important task is the
volumetric reconstruction of a sequence of misaligned slices ex vivo, enabling
multimodal comparison toward improved diagnosis. in this respect, overlaying 3d
radiology and 2d histology could significantly enhance radiologists'
understanding of the links between tissue characteristics and radiologic signals
[9]. indeed, mri or ct scans are the baseline source of information for cancer
treatment but fail to provide an accurate assessment of disease proliferation,
leading to high variability in tumor detection [5,13,17]. on the other hand,
high-resolution digitized histopathology, called whole slide imaging (wsi),
provides cell-level information on the tumor environment from the surgically
resected specimens. however, the registration process is substantially difficult
due to the visual characteristics, resolution scale, and dimensional differences
between the two modalities. in addition, histological preparation involves
tissue fixation and slicing, leading to severe collapse and out-of-plane
deformations. (semi-)automated methods have been developed to avoid
time-consuming and biased manual mapping, including protocols with 3d mold or
landmarks [10,22], volume reconstruction to perform 3d registration
[2,18,19,23], or optimization algorithms for direct multimodal comparison
[3,15]. more recently, deep learning (dl) has been introduced but is limited to
2d/2d and requires prior plane selection [20]. on the other hand, successful dl
methods have been proposed to address the 2d/3d mapping problem for other
medical modalities [6,8,16,21]. however, given the extreme deformation that the
tissue undergoes during the histological process, additional guidance is needed.
one promising solution is to rely on rigid structures that are supposedly more
robust during the preparation. structural information to guide image
registration has been studied with the help of segmentations into the training
loop [11], or by learning new image representations for refined mapping [12].in
this paper, we propose to leverage the structural features of tissue and more
particularly the rigid areas to guide the registration process with two distinct
contributions: (1) a cascaded rigid alignment driven by stiff regions and
coupled with recursive plane selection, and (2) an improved 2d/3d deformable
motion model with distance field regularization to handle out-of-plane
deformation. to our knowledge, no previous study proposed 2d/3d registration
combined with structure awareness. we also use the cyclegan for image
translation and direct monomodal signal comparison [25]. like [14,24], we
combine registration with modality translation and integrate the two
aforementioned components. we demonstrate superior quantitative results for head
and neck (h&n) 3d ct and 2d wsis than traditional approaches failing due to the
histological constraints. in addition, we show that structuregnet performs
better than the state-of-the-art model from [14] on 3d ct/2d mr for the pelvis
in rt.",10
2327,StructuRegNet: Structure-Guided Multimodal 2D-3D Registration,3.0,Experiments,"dataset and preprocessing. our clinical dataset consists of 108 patients for
whom were acquired both a pre-operative h&n ct scan and 4 to 11 wsis after
laryngectomy (with a total amount of 849 wsis). the theoretical spacing between
each slice is 5 mm, and the typical pixel size before downsampling is 100k ×
100k. two expert radiation oncologists on ct delineated both the thyroid and
cricoid cartilages for structure awareness and the gross tumor volume (gtv) for
clinical validation, while two expert pathologists did the same on wsis. they
then meet and agreed to place 6 landmarks for each slice at important locations
(not used for training). we ended up with images of size 256 × 256 (×64 for 3d
ct) of 1 mm isotropic grid space. we split the dataset patient-wise into three
groups for training (64), validation (20), and testing (24). to demonstrate the
performance of our model on another application, we also retrieved the datasets
from [14] for pelvis 3d ct/2d mr. it is made of 451 pairs between ct and
truefisp sequences, and 217 other pairs between ct and t2 sequences. we guided
the registration thanks to the rigid left/right femoral heads and computed
similarity metrics on the 7 additional organs at risk (anal canal, bladder,
rectum, penile bulb, seminal vesicle, and prostate). all masks were provided by
the authors and were originally segmented by internal experts.hyperparameters.
we drew our code from cyclegan and voxelmorph implementations with modifications
explained above, and we thank the authors of msv-regsynnet for making their code
and data available to us [1,14,25]. a detailed description of architectures and
hyperparameters can be found in the supplementary material. we implemented our
model with pytorch1.13 framework and trained for 600 (800 for mr/ct) epochs with
a batch size of 8 (4 for mr/ct) patients parallelized over 4 nvidia gtx 1080
tis.evaluation. we benchmarked our method against three baselines: first, to
assess the benefit of modality translation over the multimodal loss, we re-used
the original 3d voxelmorph model with mind as a multimodal metric for
optimization. we also modified this approach by masking the loss function to
account for the 2d-3d setting. next, we implemented the modality
translation-based msv-regsyn-net and modified it for our application to measure
the importance of joint structure-aware initialization and regularization.
finally, to differentiate the latter contributions, we tested two ablation
studies: without the cascaded rigid mapping or without the distance field
control. according to the mr/ct application in rt, we compared our model against
the state-of-the-art results of msv-regsynnet which were computed on the same
dataset.",10
2332,X2Vision: 3D CT Reconstruction from Biplanar X-Rays with Deep Structure Prior,1.0,Introduction,"tomographic imaging estimates body density using hundreds of x-ray projections,
but it's slow and harmful to patients. acquisition time may be too high for
certain applications, and each projection adds dose to the patient. a quick,
low-cost 3d estimation of internal structures using only bi-planar x-rays can
revolutionize radiology, benefiting dental imaging, orthopedics, neurology, and
more. this can improve image-guided therapies and preoperative planning,
especially for radiotherapy, which requires precise patient positioning with
minimal radiation exposure.however, this task is an ill-posed inverse problem:
x-ray measurements are the result of attenuation integration across the body,
which makes them very fig. 1. current methods vs our method. feed-forward
methods do not manage to predict a detailed and matching tomographic volume from
a few projections. iterative methods based on neural radiance fields lack prior
for good reconstruction. by learning an embedding for the possible volumes, we
can recover an accurate volume from very few projections with an optimization
based on a bayesian formulation.ambiguous. traditional reconstruction methods
require hundreds of projections to get sufficient constraints on the internal
structures. with very few projections, it is very difficult to disentangle the
structures for even coarse 3d estimation. in other words, many 3d volumes may
have generated such projections a priori.classical analytical and iterative
methods [8] fail when very few projections are available. several works have
attempted to largely decrease the number of projections needed for an accurate
volumetric reconstruction. some deep learning methods [7,12,24,25,30] predict
directly a 3d volume in a forward way from very few projections. the volume is
however not guaranteed to be consistent with the projections and it is not clear
which solution is retrieved. other recent methods have adapted nerfs [20] to
tomographic reconstruction [23,31]. these non-learning methods show good results
when the number of input projections remains higher than a dozen but fail when
very few projections are provided, as our experiments in sect. 3.3 show.as
illustrated in fig. 1, to be able to reconstruct a volume accurately given as
low as two projections only, we first learn a prior on the volume. to do this,
we leverage the potential of generative models to learn a low-dimensional
manifold of the target body part. given projections, we find by a bayesian
formulation the intermediate latent vectors conditioning the generative model
that minimize the error between synthesized projections of our reconstruction
and these input projections. our work builds on hong et al. [10]'s 3d
style-based generative model, which we extend via a more complex network and
training framework.compared to other 3d gans, it is proven to provide the best
disentanglement of the feature space related to semantic features [2].by
contrast with feed-forward methods, our approach does not require paired
projections-reconstructions, which are very tedious to acquire, and it can be
used with different numbers of projections and different projection geometries
without retraining. compared to nerf-based methods, our method exploits prior
knowledge from many patients to require only two projections. we evaluate our
method on reconstructing cancer patients' head-and-neck cts, which involves
intricate and complicated structures. we perform several experiments to compare
our method with a feed-forward-based method [30] and a recent nerf-based method
[23], which are the previous state-of-the-art methods for the very few or few
projections cases, respectively.we show that our method allows to retrieve
results with the finest reconstructions and better matching structures, for a
variety of number of projections. to summarize, our contributions are two-fold:
(i) a new paradigm for 3d reconstruction with biplanar x-rays: instead of
learning to invert the measurements, we leverage a 3d style-based generative
model to learn deep image priors of anatomic structures and optimize over the
latent space to match the input projections; (ii) a novel unsupervised method,
fast and robust to sampling ratio, source energy, angles and geometry of
projections, all of which making it general for downstream applications and
imaging systems.",10
2336,X2Vision: 3D CT Reconstruction from Biplanar X-Rays with Deep Structure Prior,2.3,Reconstruction from Biplanar Projections,"since our generative model provides a volume v as a function of vectors w and n,
we can reparameterize our optimization from eq. ( 1) into:note that by contrast
with [18] for example, we optimize on the noise vectors n as well: as we
discovered in our early experiments, the n are also useful to embed
high-resolution details. we take our regularization term r(w, n) as:term l w (w)
=k log n (w k |μ, σ) ensures that w lies on the same distribution as during
training. n (•|μ, σ) represents the density of the standard normal distribution
of mean μ and standard deviation σ.term l c (w) =i,j log m(θ i,j |0, κ)
encourages the w i vectors to be collinear so to keep the generation of
coarse-to-fine structures coherent. m(•; μ, κ) is the density of the von mises
distribution of mean μ and scale κ, which we take fixed, and θ i,j = arccos(
wi•wj wi wj ) is the angle between vectors w i and w j .term l n (n) =j log n (n
j |0, i ) ensures that the n j lie on the same distribution as during training,
i.e., a multivariate standard normal distribution. the λ * are fixed
weights.projection operator. in practice, we take operator a as a 3d cone beam
projection that simulates x-ray attenuation across the patient, adapted from
[21,27]. we model a realistic x-ray attenuation as a ray tracing projection
using material and spectrum awareness:with μ(m, e) the linear attenuation
coefficient of material m at energy state e that is known [11], t m the material
thickness, i 0 the intensity of the source x-ray.for materials, we consider the
bones and tissues that we separate by threshold on electron density. a inverts
the attenuation intensities i atten to generate an x-ray along few directions
successively. we make a differentiable using [21] to allow end-to-end
optimization for reconstruction.3 experiments and results",10
2337,X2Vision: 3D CT Reconstruction from Biplanar X-Rays with Deep Structure Prior,3.1,Dataset and Preprocessing,"manifold learning. we trained our model with a large dataset of 3500 cts of
patients with head-and-neck cancer, more exactly 2297 patients from the publicly
available the cancer imaging archive (tcia) [1,6,16,17,28,32] and 1203 from
private internal data, after obtention of ethical approbations. we split this
data into 3000 cases for training, 250 for validation, and 250 for testing. we
focused ct scans on the head and neck region above shoulders, with a resolution
of 80 × 96 × 112, and centered on the mouth after automatic segmentation using a
pre-trained u-net [22]. the cts were preprocessed by min-max normalization after
clipping between -1024 and 2000 hounsfield units (hu).3d reconstruction. to
evaluate our approach, we used an external private cohort of 80 patients who had
undergone radiotherapy for head-and-neck cancer, with their consent. planning ct
scans were obtained for dose preparation, and cbct scans were obtained at each
treatment fraction for positioning with full gantry acquisition. as can be seen
in fig. 3 and the supplementary material, all these cases are challenging as
there are large changes between the original ct scan and the cbct scans. we
identified these cases automatically by comparing the cbcts with the planning
cts. to compare our reconstruction in the calibrated hu space, we registered the
planning cts on the cbcts by deformable registration with mrf minimization [4].
we hence obtained 3d volumes as virtual cts we considered as ground truths for
our reconstructions after normalization. from these volumes, we generated
projections using the projection module described in sect. 2.3.",10
2339,X2Vision: 3D CT Reconstruction from Biplanar X-Rays with Deep Structure Prior,3.3,Results and Discussion,"manifold learning. we tested our model's ability to learn the low-dimensional
manifold. we used fid [9] to measure the distance between the distribution of
generated volumes and real volumes, and ms-ssim [29] to evaluate volumes'
diversity and quality. we obtained a 3d fid of 46 and a ms-ssim of 0.92. for
reference, compared to 3dstylegan [10], our model achieved half their fid score
on another brain mri dataset, with comparable ms-ssim. this may be due to a more
complex architecture, discriminator augmentation, or simpler anatomy.baselines.
we compared our method against the main feed-forward method x2ct-gan [30] and
the neural radiance fields with prior image embedding method nerp [23] meant for
modest sparsely-sampled reconstruction. recent methods like [24] and [12] were
excluded because they provide only minor improvements compared to x2ct-gan [30]
and have similar constraints to feed-forward methods. additionally, no public
implementation is available. [26] uses a flow-based generative model, but the
results are of lower quality compared to gans and similar to x2ct-gan [30].3d
reconstruction. to evaluate our method's performance with biplanar projections,
we focused on positioning imaging for radiotherapy. figure 3 compares our
reconstruction with those of the baselines from biplanar projections. our method
achieves better fitting of the patient structure, including bones, tissues, and
air separations, almost matching the real ct volume. x2ct-gan [30] produced
realistic structures, but failed to match the actual structures as it does not
enforce consistency with the projections. in some clinical procedures, an
earlier ct volume of the patient may be available and can be used as an
additional input for nerp [23]. without a previous ct volume, nerp lacks the
necessary prior to accurately solve the ill-posed problem. even when initialised
with a previous ct volume, nerp often fails to converge to the correct volume
and introduces many artifacts when few projections are used. in contrast, our
method is more versatile and produces better results. we used quantitative
metrics (psnr and ssim) to evaluate reconstruction error and human perception,
respectively. table 1 shows these metrics for our method and baselines with 1 to
8 cone beam projections. deviation from projections, as in x2ct-gan, leads to
inaccurate reconstruction. however, relying solely on projection consistency is
inadequate for this ill-posed problem. nerp matches projections but cannot
reconstruct the volume correctly. our approach balances between instant and
iterative methods by providing a reconstruction in 25 s with 100 optimization
steps, while ensuring maximal consistency. in contrast, nerp requires 7 min, and
x2ct-gan produces structures instantly but unmatching. clinical cbct acquisition
and reconstruction by fdk [3] take about 1-2 min and 10 s respectively. our
approach significantly reduces clin-ical time and radiation dose by using
instant biplanar projections, making it promising for fast 3d visualization
towards complex positioning.",10
2340,X2Vision: 3D CT Reconstruction from Biplanar X-Rays with Deep Structure Prior,4.0,"Conclusion, Limitations, and Future Work","we proposed a new unsupervised method for 3d reconstruction from biplanar x-rays
using a deep generative model to learn the structure manifold and retrieve the
maximum a posteriori volume with the projections, leading to stateof-the-art
reconstruction. our approach is fast, robust, and applicable to various human
body parts, making it suitable for many clinical applications, including
positioning and visualization with reduced radiation.future hardware
improvements may increase resolution, and our approach could benefit from other
generative models like latent diffusion models. this approach may provide coarse
reconstructions for patients with rare abnormalities, as most learning methods,
but a larger dataset or developing a prior including tissue abnormalities could
improve robustness.",10
2342,Structure-Preserving Synthesis: MaskGAN for Unpaired MR-CT Translation,1.0,Introduction,"magnetic resonance imaging (mri) and computed tomography (ct) are two commonly
used cross-sectional medical imaging techniques. mri and ct produce different
tissue contrast and are often used in tandem to provide complementary
information. while mri is useful for visualizing soft tissues (e.g. muscle, [20]
fails to preserve the smooth anatomy of the mri. (c) attentiongan [12] inflates
the head area in the synthetic ct, which is inconsistent with the original mri.
quantitative evaluations in mae (lower is better) are shown in yellow.fat), ct
is superior for visualizing bony structures. some medical procedures, such as
radiotherapy for brain tumors, craniosynostosis, and spinal surgery, typically
require both mri and ct for planning. unfortunately, ct imaging exposes patients
to ionizing radiation, which can damage dna and increase cancer risk [9],
especially in children and adolescents. given these issues, there are clear
advantages for synthesizing anatomically accurate ct data from mri.most
synthesis methods adopt supervised learning paradigms and train generative
models to synthesize ct [1][2][3]6,17]. despite the superior performance,
supervised methods require a large amount of paired data, which is prohibitively
expensive to acquire. several unsupervised mri-to-ct synthesis methods [4,6,14],
leverage cyclegan with cycle consistency supervision to eliminate the need for
paired data. unfortunately, the performance of unsupervised ct synthesis methods
[4,14,15] is inferior to supervised counterparts. due to the lack of direct
constraints on the synthetic outputs, cyclegan [20] struggles to preserve the
anatomical structure when synthesizing ct images, as shown in fig. 1(b). the
structural distortion in synthetic results exacerbates when data from the two
modalities are heavily misaligned, which usually occurs in pediatric scanning
due to the rapid growth in children.recent unsupervised methods impose
structural constraints on the synthesized ct through pixel-wise or shape-wise
consistency. pixel-wise consistency methods [8,14,15] capture and align
pixel-wise correlations between mri and synthesized ct. however, enforcing
pixel-wise consistency may introduce undesirable artifacts in the synthetic
results. this problem is particularly relevant in brain scanning, where both the
pixel-wise correlation and noise statistics in mr and ct images are different,
as a direct consequence of the signal acquisition technique. the alternative
shape-wise consistency methods [3,4,19] aim to preserve the shapes of major body
parts in the synthetic image. notably, shape-cyclegan [4] segments synthesized
ct and enforces consistency with the ground-truth mri segmentation. however,
these methods rely on segmentation annotations, which are time-consuming,
labor-intensive, and require expert radiological annotators. a recent natural
image synthesis approach, called attention-gan [12], learns attention masks to
identify discriminative structures. atten-tiongan implicitly learns prominent
structures in the image without using the ground-truth shape. unfortunately, the
lack of explicit mask supervision can lead to imprecise attention masks and, in
turn, produce inaccurate mappings of the anatomy, as shown in fig. 1(c). in this
paper, we propose maskgan, a novel unsupervised mri-to-ct synthesis method, that
preserves the anatomy under the explicit supervision of coarse masks without
using costly manual annotations. unlike segmentationbased methods [4,18],
maskgan bypasses the need for precise annotations, replacing them with standard
(unsupervised) image processing techniques, which can produce coarse anatomical
masks. such masks, although imperfect, provide sufficient cues for maskgan to
capture anatomical outlines and produce structurally consistent images. table 1
highlights our differences compared with previous shape-aware methods [4,12].
our major contributions are summarized as follows. 1) we introduce maskgan, a
novel unsupervised mri-to-ct synthesis method. maskgan is the first framework
that maintains shape consistency without relying on human-annotated
segmentation. 2) we present two new structural supervisions to enforce
consistent extraction of anatomical structures across mri and ct domains. 3)
extensive experiments show that our method outperforms state-of-the-art methods
by using automatically extracted coarse masks to effectively enhance structural
consistency.",10
2347,Structure-Preserving Synthesis: MaskGAN for Unpaired MR-CT Translation,3.1,Experimental Settings,"data collection. we collected 270 volumetric t1-weighted mri and 267 thinslice
ct head scans with bony reconstruction performed in pediatric patients under
routine scanning protocols1 . we targeted the age group from 6-24 months since
pediatric patients are more susceptible to ionizing radiation and experience a
greater cancer risk (up to 24% increase) from radiation exposure [7].
furthermore, surgery for craniosynostosis, a birth defect in which the skull
bones fuse too early, typically occurs during this age [5,16]. the scans were
acquired by ingenia 3.0t mri scanners and philips brilliance 64 ct scanners. we
then resampled the volumetric scans to the same resolution of 1.0 × 1.0 × 1.0 mm
3 . the dataset comprises brain mr and ct volumes from 262 subjects. 13 mri-ct
volumes from the same patients that were captured less than three months apart
are registered using rigid registration algorithms. the dataset is divided into
249, 1 and 12 subjects for training, validating and testing set. following [13],
we conducted experiments on sagittal slices. each mr and ct volume consists of
180 to 200 slices, which are resized and padded to the size of 224 × 224. the
intensity range of ct is clipped into [-1000, 2000]. all models are trained
using the adam optimizer for 100 epochs, with a learning rate of 0.0002 which
linearly decays to zero over the last 50 epochs. we use a batch size of 16 and
train on two nvidia rtx 3090 gpus.evaluation metrics. to provide a quantitative
evaluation of methods, we compute the same standard performance metrics as in
previous works [6,14] including mean absolute error (mae), peak signal-to-noise
ratio (psnr), and structural similarity (ssim) between ground-truth and
synthesized ct. the scope of the paper centers on theoretical development;
clinical evaluations such as dose calculation and treatment planning will be
conducted in future work.",10
2358,FreeSeed: Frequency-Band-Aware and Self-guided Network for Sparse-View CT Reconstruction,3.1,Experimental Settings,"we conduct experiments on the dataset of ""the 2016 nih-aapm mayo clinic low dose
ct grand challenge"" [8], which contains 5,936 ct slices in 1 mm image thickness
from 10 anonymous patients, where a total of 5,410 slices from 9 patients,
resized to 256 × 256 resolution, are randomly selected for training and the 526
slices from the remaining one patient for testing without patient overlap.
fan-beam ct projection under 120 kvp and 500 ma is simulated using torchradon
toolbox [11]. specifying the distance from the x-ray source to the rotation
center as 59.5 cm and the number of detectors as 672, we generate sinograms from
full-dose images with multiple sparse views n v ∈ {18, 36, 72, 144} uniformly
sampled from full 720 views covering [0, 2π].the models are implemented in
pytorch [10] and are trained for 30 epochs with a mini-batch size of 2, using
adam optimizer [5] with (β 1 , β 2 ) = (0.5, 0.999) and a learning rate that
starts from 10 -4 and is halved every 10 epochs. experiments are conducted on a
single nvidia v100 gpu using the same setting. all sparse-view ct reconstruction
methods are evaluated quantitatively in terms of root mean squared error (rmse),
peak signal-to-noise ratio (psnr), and structural similarity (ssim) [15].",10
2368,Co-learning Semantic-Aware Unsupervised Segmentation for Pathological Image Registration,0,3D Pseudo Brain MRI.,"to evaluate the performance of atlas-based registration, it is essential to have
the correct mapping of pathological regions to healthy brain regions. to create
such a mapping, we created a pseudo dataset by utilizing images from the oasis-1
and brats2020. from the resulting t1 sequences, a pseudo dataset of 300 images
was randomly selected for further analysis. appendix b provides a detailed
process for creating the pseudo dataset.real data with landmarks. brats-reg 2022
[2] provides extensive annotations of landmarks points within both the
pre-operative and the follow-up scans that have been generated by clinical
experts. a total of 140 images are provided, of which 112 are for training, and
28 for testing. comparison to pathology registration. we compared our method
(gir-net) with competitive algorithms: 1) three cutting-edge deep learning-based
unsupervised deformable registration approaches: voxelmorph [3], voxelmorph-df
[8] and symnet [14]. 2) two unsupervised deformable registration methods for
pathological images: dramms [18] and dirac [16]. dramms is an optimization-based
method that reduces the impact of non-corresponding regions. dirac jointly
estimates regions with absent correspondence and bidirectional deformation
fields and ranked first in the bratsreg2022 challenge.atlas-based registration.
after creating the pseudo dataset, we warped brain mr images without tumors to
the atlas and used the resulting deformation field as the gold standard for
evaluation. we then evaluated the mean deformation error (mde) [10], which is
calculated as the average euclidean distance between the coordinates of the
deformation field and the gold standard within specific regions of interest.
these regions include: 1) the tumor region. 2) the normal region near the tumor
(within 30 voxels). 3) the normal region far from the tumor (over 30 voxels but
within brain tissue). our results, presented in fig. 2, show that our method
with histogram matching (hm) outperforms other methods in all three regions,
particularly in the normal regions (near and far). by utilizing hm, our network
achieves an mde of less than 1 mm compared to the gold standard deformations.
these results demonstrate the effectiveness of our method in differentiating the
impact of pathology in atlas-based registration tasks. specifically, dirac is
unable to eliminate the influence of domain differences and resulting in the
largest registration error among the evaluated methods.longitudinal
registration. to perform the longitudinal registration task, we registered each
pre-operative scan to the corresponding follow-up scan of the same patient and
measured the mean target registration error (tre) of the paired landmarks using
the resulting deformation field. for this purpose, we leveraged segnet, trained
on brats2020, to segment the tumor of brat-sreg2022 and separated the landmarks
into two regions: near tumor and far from tumor. figure 3 shows the mean tre for
the various registration approaches. in our proposed framework, we replaced
regnet with cir-dm [15] (denoted as gir(cirdm)) without the need for supervised
training or pretraining, and achieved comparable performance with the
state-of-the-art method dirac. moreover, our gir approach outperforms other deep
learning-based methods and achieved accurate segmentation of pathological
images.to quantitatively evaluate the segmentation capability of our proposed
framework, we compared its performance with other unsupervised segmentation
techniques methods, including unsupervised clustering toolbox aucseg [25], joint
non-correspondence segmentation and registration method ncrnet [1], and dirac.
we used the mean dice similarity coefficient (dsc) to evaluate the similarity
between predicted masks and the ground truth. as shown in table 1, aucseg fails
to detect the lesion in t1 scans. our proposed framework achieved the highest
dsc result of 0.83, following post-processing.ablation study. we compared the
performance of the inpnet trained with histogram matching (hm) and the segnet
trained with ground truth masks (supervised). the results, shown in table 1 and
fig. 2, demonstrate that domain differences between s and t have a significant
effect on segmentation accuracy (without hm), leading to lower registration
quality overall. additionally, fig. 4 shows an example of a pseudo image. we
reconstructed the spatial correspondence by first using segnet to localize the
lesion and then using inpnet to inpaint it with the normal appearance.",10
2371,Fast Reconstruction for Deep Learning PET Head Motion Correction,1.0,Introduction,"positron emission tomography (pet) has been widely used in human brain imaging,
thanks to the availability of a vast array of specific radiotracers. these
compounds allow for studying various neurotransmitters and receptor dynamics for
different brain targets [11]. brain pet images are commonly used to diagnose and
monitor neurodegenerative diseases, such as alzheimer's disease, parkinson's
disease, epilepsy, and certain types of brain tumors [3]. head motion in pet
imaging reduces brain image resolution, lowers tracer distribution estimation,
and introduces attenuation correction (ac) mismatch artifacts [12].
consequently, the capability to monitor and correct head motion is of utmost
importance in brain pet studies.the first step of pet head motion correction is
motion tracking. when head motion information is acquired, either frame-based
motion correction or eventby-event (ebe) motion correction methods can be
applied in the reconstruction workflow to derive motion-free pet images. ebe
motion correction provides better results for real-time motion tracking compared
to frame-based methods, as the latter does not allow for correction of motion
that occurs within each dynamic frame [1]. currently, there are two main
categories of head motion tracking methods, hardware-based motion tracking (hmt)
and data-driven methods. for hmt, head motion is obtained from external devices.
generally, hmt systems offer accurate tracking results with high time
resolution. marker-based hmt such as polaris vicra (ndi, canada) use
light-reflecting markers on the patient's head and track the markers for motion
correction [6]. however, vicra is not routinely used in the clinic, as setup and
calibration of the tracking device can be complicated and attaching markers to
each patient increases the logistical burden of the scan. in response, some
researchers began to use markerless motion tracking systems for brain pet
[4,13]. these methods typically rely on the use of cameras and computer vision
algorithms to detect and analyze the movement of a person's head in real-time,
but these methods still require additional hardware setup. in data-driven motion
tracking methods, head motion is estimated from pet reconstructions or raw data.
with the development of commercial pet systems and technological advancements
such as time of flight (tof), data-driven head pet motion tracking has shown
promising results in reducing motion artifacts and improving image quality. for
instance, [12] developed a novel data-driven head motion detection method based
on the centroid of distribution (cod) of pet 3d point cloud image (pci). image
registration methods that seek to align two or more images offer a data-driven
solution for correcting head motion. intensity-based registration methods have
been used to track head motion using good-quality pet reconstruction frames to
achieve stable performance [14]. however, because of the dynamic change in pet
images, current registration-based methods need to split the data into several
discrete time frames, e.g., 5 min. therefore, they will introduce a cumulative
error when dealing with inter-frame motion. finally, inspired by the development
of deep learning-based registration methods, a deep learning head motion
correction (dl-hmc) network using vicra as ground truth was proposed [15]. this
study achieved accurate motion tracking on single subject testing data, but
showed less accurate motion predictions for multi-subject motion studies.
meanwhile, the input images were low-resolution pcis without tof and had large
voxel spacing, which can negatively affect motion tracking accuracy.in this
study, we proposed a new method to perform deep learning-based brain pet motion
prediction across multiple subjects by utilizing high-resolution one-second fast
reconstruction images (fris) with tof. a novel encoder and data augmentation
strategy was also applied to improve model performance. ablation studies were
conducted to assess the individual contributions of key method components.
multi-subject studies were conducted on a dataset of 20 subject and its results
were quantitatively and qualitatively evaluated by molar reconstruction studies
and corresponding brain region of interest (roi) standard uptake values (suv)
evaluation.",10
2381,Revealing Anatomical Structures in PET to Generate CT for Attenuation Correction,3.1,Materials,"the data used in our experiments are collected from the cancer image archive
(tcia) [4] (https://www.cancerimagingarchive.net/collections/), where a series
of public datasets with different types of lesions, patients, and scanners are
open-access. among them, 401, 108, 46, and 20 samples are extracted from the
head and neck scamorous cell carcinoma (hnscc), non-small cell lung cancer
(nsclc), the cancer genome atlas (tcga) -head-neck squamous cell carcinoma
(tcga-hnsc), and tcga -lung adenocarcinoma (tcga-luad), respectively. we use
these samples in hnscc for training and in other three datasets for
evaluation.each sample contains co-registered (acquired with pet-ct scans) ct,
pet, and nac-pet whole-body scans. in our experiments, we re-sampled all of them
to a voxel spacing of 2×2×2 and re-scaled the intensities of nac-pet/ac-pet
images to a range of [0, 1], of ct images by multiplying 0.001. the input and
output of our aseg framework are cropped patches with the size of 192 × 192 ×
128 voxels. to achieve full-fov output, the consecutive outputs of each sample
are composed into a single volume where the overlapped regions are averaged.",10
2417,DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration,3.0,Method,"we train our model to approximate the three-dimensional lc 2 similarity, as it
showed good performance on a number of tasks, including ultrasound [2,22]. the
lc 2 similarity quantifies whether a target patch can be approximated by a
linear combination of the intensities and the gradient magnitude of the source
patch. in order to reduce the sensitivity on the scale, our target is actually
the average lc 2 over different radiuses of 3, 5, and 7. in order to be
consistent with the original implementation of lc 2 we use the same weighting
function w based on local patch variance. note that the network will be trained
only once, on a fixed dataset that is fully independent of the datasets that
will be used in the evaluation (see sect. 4).dataset. our neural network is
trained using patches from the ""gold atlas -male pelvis -gentle radiotherapy""
[14] dataset, which is comprised of 18 patients each with a ct, mr t1, and mr t2
volumes. we resample each volume to a spacing of 2 mm and normalize the voxel
intensities to have zero mean and standard variation of one. since our approach
is unsupervised, we don't make use of the provided registration but leave the
volumes in their standard dicom orientation. as lc 2 requires the usage of
gradient magnitude in one of the modalities, we randomly pick it from either ct
or mr. we would like to report that, initially, we also made use of a
proprietary dataset including us volumes. however, as our investigation
progressed, we observed that the incorporation of us data did not significantly
contribute to the generalization capabilities of our model. consequently, for
the purpose of ensuring reproducibility, all evaluations presented in this paper
exclusively pertain to the model trained solely on the public mr-ct
dataset.patch sampling from unregistered datasets. for each pair of volumes (m,
f ) we repeat the following procedure 5000 times: (1) select a patch from m with
probability proportional to its weight w; (2) compute the similarity with all
the patches of f ; (3) uniformly sample t ∈ [0, 1]; (4) pick the patch of f with
similarity score closest to t. running this procedure on our training data
results in a total of 510000 pairs of patches.",10
2421,DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration,4.2,Deformable Registration of Abdominal MR-CT,"our second application is the abdomen mr-ct task of the learn2reg challenge 2021
[8]. the dataset comprises 8 sets of mr and ct volumes, both depicting the
abdominal region of a single patient and exhibiting notable deformations. we
estimate dense deformation fields using the methodology outlined in [6] (without
inverse consistency) which first estimates a discrete displacement using
explicit search and then iteratively enforces global smoothness. segmentation
maps of anatomical structures are used to measure the quality of the
registration. in particular, we compute the 25th, 50th, and 75th quantile of the
dice similarity coefficient (dsc) and the 95th quantile of the hausdorff
distance (hd95) between the registered label maps. we compare mind-scc and
disa-lc 2 used with different strides and followed by a downsampling operation
that brings the spacing of the descriptors volumes to 8 mm. the hyperparameters
of the registration algorithm have been manually optimized for each approach.
table 2 shows that our method obtains significantly better results than mind-scc
on the dsc metrics while being not significantly better on hd95.",10
2422,DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration,4.3,Deformable Registration of Abdominal US-CT and US-MR,"as the most challenging experiment, we finally use our method to achieve
deformable registration of abdominal 3d freehand us to a ct or mr volume.we are
using a heterogeneous dataset of 27 cases, comprising liver cancer patients and
healthy volunteers, different ultrasound machines, as well as optical vs.
electro-magnetic external tracking, and sub-costal vs. inter-costal scanning of
the liver. all 3d ultrasound data sets are accurately calibrated, with overall
system errors in the range of commercial ultrasound fusion options. between 4
and 9 landmark pairs (vessel bifurcations, liver gland borders, gall bladder,
kidney) were manually annotated by an expert. in order to measure the capture
range, we start the registration from 50 random rigid poses around the ground
truth and calculate the fiducial registration error (fre) after optimization.
for local optimization, lc 2 is used in conjunction with bobyqa [15] as in the
original paper [22], while mind-scc and disa-lc 2 are instead used with bfgs.
due to an excessive computation time, we don't do global optimization with lc 2
while with other methods we use bfgs with 500 random initializations within a
range of ±40 • and ±150 mm. we use six parameters to define the rigid pose and
two parameters to describe the deformation caused by the ultrasound probe
pressure.from the results shown in table 3 and fig. 2, it can be noticed that
the proposed method obtains a significantly larger capture range than mind-scc
and lc 2 while being more than 300 times faster per evaluation than lc 2 (the
times reported in the table include not just the optimization but also
descriptor extraction). the differentiability of our objective function allows
our method to converge in fewer iterations than derivative-free methods like
bobyqa. furthermore, the evaluation speed of our objective function allows us to
exhaustively search the solution space, escaping local minima and converging to
the correct solution with pose and deformation parameters at once, in less than
two seconds.note that this registration problem is much more challenging than
the prior two due to difficult ultrasonic visibility in the abdomen, strong
deformations, and ambiguous matches of liver vasculature. therefore, to the best
of our knowledge, these results present a significant leap towards reliable and
fully automatic fusion, doing away with cumbersome manual landmark placements.",10
2425,Solving Low-Dose CT Reconstruction via GAN with Local Coherence,1.0,Introduction,"computed tomography (ct) is one of the most widely used technologies in medical
imaging, which can assist doctors for diagnosing the lesions in human internal
organs. due to harmful radiation exposure of standard-dose ct, the low dose ct
is more preferable in clinical application [4,6,34]. however, when the dose is
low together with the issues like sparse-view or limited angles, it becomes
quite challenging to reconstruct high-quality ct images. the high-quality ct
images are important to improve the performance of diagnosis in clinic [27]. in
mathematics, we model the ct imaging as the following procedure y = t (x r ) +
δ, (1) where x r ∈ r d denotes the unknown ground-truth picture, y ∈ r m denotes
the received measurement, and δ is the noise. the function t represents the
forward operator that is analogous to the radon transform, which is widely used
in medical imaging [23,28]. the problem of ct reconstruction is to recover x r
from the received y.solving the inverse problem of ( 1) is often very
challenging if there is no any additional information. if the forward operator t
is well-posed and δ is neglectable, we know that an approximate x r can be
easily obtained by directly computing t -1 (y). however, t is often ill-posed,
which means the inverse function t -1 does not exist and the inverse problem of
(1) may have multiple solutions. moreover, when the ct imaging is low-dose, the
filter backward projection (fbp) [11] can produce serious detrimental artifact.
therefore, most of existing approaches usually incorporate some prior knowledge
during the reconstruction [14,17,26]. for example, a commonly used method is
based on regularization:where • p denotes the p-norm and r(x) denotes the
penalty item from some prior knowledge.in the past years, a number of methods
have been proposed for designing the regularization r. the traditional
model-based algorithms, e.g., the ones using total variation [3,26], usually
apply the sparse gradient assumptions and run an iterative algorithm to learn
the regularizers [12,18,24,29]. another popular line for learning the
regularizers comes from deep learning [13,17]; the advantage of the deep
learning methods is that they can achieve an end-to-end recovery of the true
image x r from the measurement y [1,21]. recent researches reveal that
convolutional neural networks (cnns) are quite effective for image denoising,
e.g., the cnn based algorithms [10,34] can directly learn the reconstructed
mapping from initial measurement reconstructions (e.g., fbp) to the ground-truth
images. the dual-domain network that combines the sinograms with reconstructed
low-dose ct images were also proposed to enhance the generalizability [15,30].a
major drawback of the aforementioned reconstruction methods is that they deal
with the input ct 2d slices independently (note that the goal of ct
reconstruction is to build the 3d model of the organ). namely, the neighborhood
correlations among the 2d slices are often ignored, which may affect the
reconstruction performance in practice. in the field of computer vision,
""optical flow"" is a common technique for tracking the motion of object between
consecutive frames, which has been applied to many different tasks like video
generation [35], prediction of next frames [22] and super resolution synthesis
[5,31]. to estimate the optical flow field, existing approaches include the
traditional brightness gradient methods [2] and the deep networks [7]. the idea
of optical flow has also been used for tracking the organs movement in medical
imaging [16,20,33]. however, to the best of our knowledge, there is no work
considering gans with using optical flow to capture neighbor slices coherence
for low dose 3d ct reconstruction.in this paper, we propose a novel optical flow
based generative adversarial network for 3d ct reconstruction. our intuition is
as follows. when a patient is located in a ct equipment, a set of consecutive
cross-sectional images are generated. if the vertical axial sampling space of
transverse planes is small, the corresponding ct slices should be highly
similar. so we apply optical flow, though there exist several technical issues
waiting to solve for the design and implementation, to capture the local
coherence of adjacent ct images for reducing the artifacts in low-dose ct
reconstruction. our contributions are summarized below:1. we introduce the
""local coherence"" by characterizing the correlation of consecutive ct images,
which plays a key role for suppressing the artifacts. 2. together with the local
coherence, our proposed generative adversarial networks (gans) can yield
significant improvement for texture quality and stability of the reconstructed
images. 3. to illustrate the efficiency of our proposed approach, we conduct
rigorous experiments on several real clinical datasets; the experimental results
reveal the advantages of our approach over several state-of-the-art ct
reconstruction methods.",10
2429,Solving Low-Dose CT Reconstruction via GAN with Local Coherence,4.0,Experiment,"datasets. first, our proposed approaches are evaluated on the ""mayo-clinic
low-dose ct grand challenge"" (mayo-clinic) dataset of lung ct images [19].the
dataset contains 2250 two dimensional slices from 9 patients for training, and
the remaining 128 slices from 1 patient are reserved for testing. the lowdose
measurements are simulated by parallel-beam x-ray with 200 (or 150) uniform
views, i.e., n v = 200 (or n v = 150), and 400 (or 300) detectors, i.e., n d =
400 (or n d = 300). in order to further verify the denoising ability of our
approaches, we add the gaussian noise with standard deviation σ = 2.0 to the
sinograms after x-ray projection in 50% of the experiments. to evaluate the
generalization of our model, we also consider another dataset rider with
nonsmall cell lung cancer under two ct scans [36] for testing. we randomly
select 4 patients with 1827 slices from the dataset. the simulation process is
identical to that of mayo-clinic. the proposed networks were implemented in the
pytorch framework and trained on nvidia 3090 gpu with 100 epochs.baselines and
evaluation metrics. we consider several existing popular algorithms for
comparison. ( 1) fbp [11]: the classical filter backward projection on low-dose
sinograms. ( 2) fbpconvnet [10]: a direct inversion network followed by the cnn
after initial fbp reconstruction. ( 3) lpd [1]: a deep learning method based on
proximal primal-dual optimization. ( 4) uar [21]: an end-toend reconstruction
method based on learning unrolled reconstruction operators and adversarial
regularizers. our proposed method is denoted by gan-lc.we set λ pix = 1.0, λ adv
= 0.01 and λ per = 1.0 for the optimization objective in eq. ( 7) during our
training process. following most of the previous articles on 3d ct
reconstruction, we evaluate the experimental performance by two metrics: the
peak signal-to-noise ratio (psnr) and the structural similarity index (ssim)
[32]. psnr measures the pixel differences of two images, which is negatively
correlated with mean square error. ssim measures the structure similarity
between two images, which is related to the variances of the input images. for
both two measures, the higher the better.results. a similar increasing trend
with our approach across different settings but has worse reconstruction
quality. to evaluate the stability and generalization of our model and the
baselines trained on mayo-clinic dataset, we also test them on the rider
dataset. the results are shown in table 2. due to the bias in the datasets
collected from different facilities, the performances of all the models are
declined to some extents. but our proposed approach still outperforms the other
models for most testing cases.to illustrate the reconstruction performances more
clearly, we also show the reconstruction results for testing images in fig. 3.
we can see that our network can reconstruct the ct image with higher quality.
due to the space limit, the experimental results of different views n v and more
visualized results are placed in our supplementary material.",10
2432,Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,1.0,Introduction,"radiotherapy (rt) is one of the cornerstones of cancer patients. it utilizes
ionizing radiation to eradicate all cells of a tumor. the total radiation dose
is typically divided over 3-30 daily fractions to optimize its effect. as the
surrounding normal tissue is also sensitive to radiation, highly accurate
delivery is vital. image guided rt (igrt) is a technique to capture the anatomy
of the day using in room imaging in order to align the treatment beam with the
tumor location [1]. cone beam ct (cbct) is the most widely used imaging modality
for igrt.a major challenge especially for cbct imaging of the thorax and
upperabdomen is the respiratory motion that introduces blurring of the anatomy,
reducing the localization accuracy and the sharpness of the image.a technique
used to alleviate motion artifacts is respiratory correlated cbct (4dcbct) [16].
from the projections, it is possible to extract a respiratory signal [12], which
indicates the position of the organs within the patient during breathing. with
this, subsets of the projections can be defined to create reconstructions that
resolve the motion. however, only 20 to 60 respiratory periods are imaged. this
limits the number of projections available and results in view-aliasing [16].
additionally, the projections are affected by stochastic measurement noise
caused by the finite imaging dose used, which further degrades the quality of
the reconstruction even when all projections are used.several traditional
methods based on iterative reconstruction algorithms and motion compensation
techniques are used to reduce view-aliasing in 4dcbcts [7,10,11,14,15]. although
effective, these methods suffer from motion modeling uncertainty and prolonged
reconstruction times.deep learning has been proposed as a way to address
view-aliasing with accelerated reconstruction [6]. however, the method cannot
reduce measurement noise because it is still present in the images used as
targets during training.a different method, called noise2inverse, uses an
unsupervised approach to reduce measurement noise in the traditional ct setting
[4]. there are two ways to apply it to 4dcbct and both fail to reduce stochastic
noise effectively. the first is to apply noise2inverse to each
respiratory-correlated reconstruction. in this case, the method will struggle
because of the very low number of projections that are available. the second is
to apply noise2inverse directly to all the projections. in this case, the motion
artifacts that blur the image will appear again, as noise2inverse requires
averaging the sub-reconstructions to obtain a clean reconstruction.we propose
noise2aliasing to address these limitations. the method can be used to provably
train models to reduce both view-aliasing artifacts and stochastic noise from
4dcbcts in an unsupervised way. training deep learning models for medical
applications often needs new data. this was not the case for noise2aliasing, and
historical clinical data sufficed for training.we validated our method on
publicly available data [15] against a supervised approach [6] and applied it to
an internal clinical dataset of 30 lung cancer patients. we explore different
dataset sizes to understand their effects on the reconstructed images.",10
2436,Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,4.0,Experiments,"first, we used the spare varian dataset to study whether noise2aliasing can
match the performance of the supervised baseline and if it can outperform it
when adding noise to the projections. then, we use the internal dataset to
explore the requirements for the method to be applied to an existing clinical
dataset. these required around 64 gpu days on nvidia a100 gpus.training of the
model is done on 2d slices. the projections obtained during a scan are
sub-sampled according to the pseudo-average subset selection method described in
[6] and then used to obtain 3d reconstructions. in noise2aliasing these are used
for both input and target during training. given two volumes (x, y), the
training pairs (x i (k) , y i (k) ) are the same i-th slice along the k-th
dimension of each volume chosen to be the axial plane.the datasets used in this
study are two:1. the spare varian dataset was used to provide performance
results on publicly available patient data. to more closely resemble normal
respiratory motion per projection image, the 8 min scan has been used from each
patient (five such scans are available in the dataset). training is performed
over 4 patients while 1 patient is used as a test set. the hyperparameters are
optimized over the training dataset.2. an internal dataset (irb approved) of 30
lung cancer patients' 4dcbcts from 2020 to 2022, originally used for igrt, with
25 patients for training and 5 patients for testing. the scans are 4 min 205 •
scans with 120kev source and 512 × 512 sized detector, using elekta linacs. the
data were anonymized prior to analysis.projection noise was added using the
poisson distribution to the spare varian dataset to evaluate the ability of the
unsupervised method to reduce it. given a projected value of p and a photon
count π (chosen to be 2500), the rate of the poisson distribution is defined as
πe -p and given a sample q from this distribution, then the new projected value
is p =log q π .the architecture used in this work is the mixed scale dense cnn
(msd) [8], the most successful architecture from noise2inverse [4]. the msd
makes use of dilated convolutions to process features at all scales of the
image. we use the msd with depth 200 and width 1, adam optimizer, mse loss, a
batch size of 16, and a learning rate of 0.0001.the baselines we compare against
are two. the first is the traditional fdk obtained using rtk [13]. the second is
the supervised approach proposed by [6], where we replace the model with the
msd, for a fair comparison. in the supervised approach, the model is trained by
using as input reconstructions obtained from subsets defined with pseudo-average
subset selection while the targets use all of the projections available.the
metrics used in this work are the root mean squared error (rmse), peak
signal-to-noise ratio (psnr), and structural similarity index measure (ssim)
[17] all the metrics are defined between the output of the neural network and a
3d (cb)ct scan. for the spare varian dataset, we use the rois defined provided
[15] and used the 3d reconstruction using all the projections available as a
ground truth. for the internal dataset, we deformed the planning ct to each of
the phases reconstructed using the fdk algorithm and evaluate the metric over
only the 4dcbct volume boundaries.",10
2437,Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,5.0,Results and Discussion,"spare varian. inference speed with the nvidia a100 gpu averages 600ms per volume
made of 220 slices. from the qualitative evaluation of the methods in fig. 1,
noise2aliasing matches the visual quality of the supervised approach on the
low-noise dataset on both soft tissue and bones. the metrics in table 1 show
mean and standard deviation across all phases for a single patient. in the
lownoise setting, both supervised and noise2aliasing outperform fdk with very
similar results, often within a single standard deviation. noise2aliasing
successfully matches the performance of the supervised baseline. noisy spare
varian. from fig. 1 and table 1, the supervised approach reproduces the noise
that was seen during training, while noise2aliasing manages to remove it
consistently, outperforming the supervised approach, especially in the soft
tissue area around the lungs, where the noise affects attenuation coefficients
the most.noise2aliasing is capable of reducing the artifacts present in
reconstructions caused by stochastic noise in the projections used,
outperforming the supervised baseline.internal dataset. noise2alisting trained
on 25 patients and tested on 5 achieved mean psnr of 35.24 and ssim of 0.91,
while the clinical method achieved mean psnr of 29.97 and 0.74 ssim with p-value
of 0.048 for the psnr and 0.0015 for the ssim, so noise2aliasing was
significantly better according to both metrics. additionally, from fig. 3 we can
see how the breathing extent is matched with sharp reconstruction of the
diaphragm. overall, using more patients results in better noise reduction and
sharper reconstructions (see fig. 2), fig. 2. reconstruction using
noise2aliasing with different-sized datasets. with fewer patients, the model is
more conservative and tends to keep more noise, but also smudges the interface
between tissues and bones. with more patients, more of the view-aliasing is
addressed, and the reconstruction is sharper, however, a few small anatomical
structures tend to be suppressed by the model.especially between fat tissue and
skin and around the bones. however, the model also tends to remove small
anatomical structures as high-frequency objects that cannot be distinguished
from the noise.when applied to a clinical dataset, noise2aliasing benefits from
more patients being included in the dataset, however, qualitatively good
performance is already achieved with 5 patients. no additional data collection
was required and the method can be applied without major changes to the current
clinical practice.",10
2438,Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,6.0,Conclusion,"we have presented noise2aliasing, a method to provably remove both viewaliasing
and stochastic projection noise from 4dcbcts using an unsupervised deep learning
method. we have empirically demonstrated its performance on a publicly available
dataset and on an internal clinical dataset. noise2aliasing outperforms a
supervised approach when stochastic noise is present in the projections and
matches its performance on a popular benchmark. noise2aliasing can be trained on
existing historical datasets and does not require changing current clinical
practices. the method removes noise more reliably when the dataset size is
increased, however further analysis is required to establish a good quantitative
measurement of this phenomenon. as future work, we plan to study noise2aliasing
in the presence of changes in the breathing frequency and amplitude between
patients and during a scan.",10
2447,DULDA: Dual-Domain Unsupervised Learned Descent Algorithm for PET Image Reconstruction,3.1,Experimental Evaluations,"forty 128 × 128 × 40 3d zubal brain phantoms [24] were used in the simulation
study as ground truth, and one clinical patient brain images with different dose
level were used for the robust analysis. two tumors with different size were
added in each zubal brain phantom. the ground truth images were firstly
forward-projected to generate the noise-free sinogram with count of 10 6 for
each transverse slice and then poisson noise were introduced. 20 percent of
uniform random events were simulated. in total, 1600 (40 × 40) 2d sinograms were
generated. among them, 1320 (33 samples) were used in training, 200 (5 samples)
for testing, and 80 (2 samples) for validation. a total of 5 realizations were
simulated and each was trained/tested independently for bias and variance
calculation [15]. we used peak signal to noise ratio (psnr), structural
similarity index (ssim) and root mean square error (rmse) for overall
quantitative analysis. the contrast recovery coefficient (crc) [25] was used for
the comparison of reconstruction results in the tumor region of interest (roi)
area.",10
2449,DULDA: Dual-Domain Unsupervised Learned Descent Algorithm for PET Image Reconstruction,4.0,Discussion,"to test the robustness of proposed dulda, we forward-project one patient brain
image data with different dose level and reconstructed it with the trained dulda
model. the results compared with mlem are shown in fig. 3. the patient is
scanned with a ge discovery mi 5-ring pet/ct system. the real image has very
different cortex structure and some deflection compared with the training data.
it can be observed that dulda achieves excellent reconstruction results in both
details and edges across different dose level and different slices.table 2 shows
the ablation study on phase numbers and loss function for dulda. it can be
observed that the dual domain loss helps improve the performance and when the
phase number is 4, dulda achieves the best performance.",10
2451,Low-Dose CT Image Super-Resolution Network with Dual-Guidance Feature Distillation and Dual-Path Content Communication,1.0,Introduction,"following the ""as low as reasonably achievable"" (alara) principle [22], lowdose
computer tomography (ldct) has been widely used in various medical applications,
for example, clinical diagnosis [18] and cancer screening [28]. to balance the
high image quality and low radiation damage compared to normaldose ct (ndct),
numerous algorithms have been proposed for ldct superresolution [3,4].in the
past decades, image post-processing techniques attracted much attention from
researchers because they did not rely on the vendor-specific parameters [2] like
iterative reconstruction algorithms [1,23] and could be easily applied to
current ct workflows [29]. image post-processing super-resolution (sr) methods
could be divided into 3 categories: interpolated-based methods [16,25],
modelbased methods [13,14,24,26] and learning-based methods [7][8][9]17].
interpolatedbased methods could recover clear results in those flattened regions
but failed to reconstruct detailed textures because they equally recover
information with different frequencies. and model-based methods often involved
time-consuming optimization processes and degraded quickly when image statistics
were biased from the image prior [6].with the development of deep learning (dl),
various learning-based methods have been proposed, such as edsr [20], rcan [31],
and swinir [19]. those methods optimized their trainable parameters by
pre-degraded low-resolution (lr) and high-resolution (hr) pairs to build a
robust model with generalization and finally reconstruct sr images. however,
they were designed for known degradation (for example bicubic degradation) and
failed to deal with more complex and unknown degradation processes (such as ldct
degradation). facing more complex degradation processes, blind sr methods have
attracted attention. huang et al. [11] introduced a deep alternating network
(dan) which estimated the degradation kernels and corrected those kernels
iteratively and reconstructed results following the inverse process of the
estimated degradation. more recently, aiming at improving the quality of medical
images further, huang et al. [12] first composited degradation model proposed
for radiographs and proposed attention denoising super-resolution generative
adversarial network (aid-srgan) which could denoise and super-resolve
radiographs simultaneously. to accurately reconstruct hr ct images from lr ct
images, hou et al. [10] proposed a dual-channel joint learning framework which
could process the denoising reconstruction and sr reconstruction in parallel.the
aforementioned methods still have drawbacks: (1) they treated the regions of
interest (roi) and regions of uninterest equally, resulting in the extra cost in
computing source and inefficient use for hierarchical features. (2) most of them
extracted the features with a fixed resolution, failing to effectively leverage
multi-scale features which are essential to image restoration task [27,32].(3)
they connected the sr task and the ldct denoising task stiffly, leading to
smooth texture, residual artifacts and unclear edges.to deal with those issues,
as shown in fig. 1(a), we propose an ldct image sr network with dual-guidance
feature distillation and dual-path content com-fig. 1. architecture of our
proposed method. sam is sampling attention module. cam is channel attention
module. avg ct is the average image among adjacent ct slices of each patient.
munication. our contributions are as follows: (1) we design a dual-guidance
fusion module (dgfm) which could fuse the 3d ct information and roi guidance by
mutual attention to make full use of ct features and reconstruct clearer
textures and sharper edges. (2) we propose a sampling attention block (sab)
which consists of sampling attention module (sam), channel attention module
(cam) and elaborate multi-depth residual connection aiming at the essential
multi-scale features by up-sampling and down-sampling to leverage the features
in ct images. (3) we design a multi-supervised mechanism based on shared task
heads, which introducing the denoising head into sr task to concentrate on the
connection between the sr task and the denoising task. such design could
suppress more artifacts while decreasing the number of parameters.",10
2452,Low-Dose CT Image Super-Resolution Network with Dual-Guidance Feature Distillation and Dual-Path Content Communication,2.1,Overall Architecture,"the pipeline of our proposed method is shown in fig. 1(a). we first calculate
the average ct image of adjacent ct slices of each patient to provide the 3d
spatial structure information of ct volume. meanwhile, the roi mask is obtained
by a pre-trained segmentation network to guide the network to concentrate on the
focus area or tissue area. then those guidance images and the input ldct image
are fed to the dual-guidance feature distillation backbone to extract the deep
features. finally, the proposed dual-path architecture consisting of
parametershared sr heads and denoising heads leverages the deep visual features
obtained by our backbone to build the connection between the sr task and the
denoising task, resulting in noise-free and detail-clear reconstructed
results.dual-guidance feature distillation backbone. to decrease the redundant
computation and make full use of the above-mentioned extra information, we
design a dual-guidance feature distillation backbone consisting of a
dual-guidance fusion module (dgfm) and sampling attention block(sab).firstly, we
use a 3 × 3 convolutional layer to extract the shallow features of the three
input images. then, those features are fed into 10 dgfm-sab blocks to obtain the
deep visual features.especially, the dgfm-sab block is composed of dgfm
concatenated with sab. considering the indicative function of roi, we calculate
the correlation matrix between ldct and its mask and then acquire the response
matrix between the correlation matrix and the average ct image by multi-heads
attention mechanism:where, f sab i are the output of i-th sab. f mask and f av g
represent the shallow features of the input roi mask and the average ct image
respectively. meanwhile, p rj(•) is the projection function, sof tmax[•] means
the softmax function and f i are the output features of the i-th dgfm. the dgfm
helps the backbone to focus on the roi and tiny structural information by
continuously introducing additional guidance information.furthermore, to take
advantage of the multi-scale information which is essential for obtaining the
response matrix containing the connections between different levels of features,
as shown in fig. 1(b), we design the sampling attention block (sab) which
introduces the resampling features into middle connection to fuse the
multi-scale information. in the sab, the input features are up-sampled and
down-sampled simultaneously and then down-sampled and up-sampled to recover the
spatial resolution, which can effectively extract multi-scale features. in
addition, as shown in fig. 1(c), we introduce the channel attention module (cam)
to focus on those channels with high response values, leading to detailed
features with high differentiation to different regions. shared heads mechanism.
singly using the sr head that consists of pixel shuffle layer and convolution
layer fails to suppress the residual artifacts because of its poor noise removal
ability. to deal with this problem, we develop a dualpath architecture by
introducing the shared denoising head into sr task where the parameters of sr
heads and denoising heads in different paths are shared respectively. two paths
are designed to process the deep features extracted from our backbone: (1) the
sr path transfers the deep features to those with highfrequency information and
reconstructs the sr result, and (2) the denoising path migrates the deep
features to those without noise and recovers the clean result secondly.
especially, the parameters of those two paths are shared and optimized by
multiple supervised strategy simultaneously. this process could be formulated
as:where,",10
2459,Non-iterative Coarse-to-Fine Transformer Networks for Joint Affine and Deformable Image Registration,1.0,Introduction,"image registration is a fundamental requirement for medical image analysis and
has been an active research focus for decades [1]. it aims to find a spatial
transformation between a pair of fixed and moving images, through which the
moving image can be warped to spatially align with the fixed image. similar to
natural image registration [2], medical image registration usually requires
affine registration to eliminate rigid misalignments and then performs
additional deformable registration to address non-rigid deformations.
traditional methods usually formulate medical image registration as a
time-consuming iterative optimization problem [3,4]. recently, deep registration
methods based on deep learning have been widely adopted to perform end-to-end
registration [5,6]. deep registration methods learn a mapping from image pairs
to spatial transformations based on training data in an unsupervised manner,
which have shown advantages in registration accuracy and computational
efficiency [7][8][9][10][11][12][13][14][15][16][17][18].many deep registration
methods perform coarse-to-fine registration to improve registration accuracy,
where the registration is decoupled into multiple coarse-to-fine registration
steps that are iteratively performed by using multiple cascaded networks
[10][11][12][13] or repeatedly running a single network for multiple iterations
[14,15]. mok et al. [13] proposed a laplacian pyramid image registration network
(lapirn), where multiple networks at different pyramid levels were cascaded. shu
et al. [14] proposed to use a single network (ulae-net) to perform
coarse-to-fine registration with multiple iterations. these methods perform
iterative coarse-to-fine registration and extract image features repeatedly in
each iteration, which inevitably increases computational loads and prolongs the
registration runtime. recently, non-iterative coarse-to-fine (nice) registration
methods have been proposed to perform coarse-to-fine registration with a single
network in a single iteration [16][17][18]. for example, we previously proposed
a nice registration network (nice-net) [18,19], where multiple coarse-to-fine
registration steps are performed with a single network in a single iteration.
these nice registration methods show advantages in both registration accuracy
and runtime on the benchmark task of intra-patient brain mri registration.
nevertheless, we identified that existing nice registration methods still have
two main limitations.firstly, existing nice registration methods merely focus on
deformable coarseto-fine registration, while affine registration, a common
prerequisite, is still reliant on traditional registration methods [16,18] or
extra affine registration networks [17]. using traditional registration methods
incurs time-consuming iterative optimization, while cascading extra networks
consumes additional computational resources (e.g., extra gpu memory and
runtime). secondly, existing nice registration methods are based on convolution
neural networks (cnn) and thus are limited by the intrinsic locality (i.e.,
limited receptive field) of convolution operations. transformers have been
widely adopted in many medical applications for their capabilities to capture
long-range dependency [20]. recently, transformers have also been shown to
improve registration with conventional voxelmorph [7]-like architecture
[21][22][23]. however, the benefits of using transformers for nice registration
have not been explored.in this study, we propose a non-iterative coarse-to-fine
transformer network (nice-trans) for joint affine and deformable registration.
our technical contributions are two folds: (i) we extend the existing nice
registration framework to affine registration, where multiple steps of both
affine and deformable coarse-to-fine registration are performed with a single
network in a single iteration. (ii) we explore the benefits of transformers for
nice registration, where swin transformer [24] is embedded into the nice-trans
to model long-range relevance between fixed and moving images. this is the first
deep registration method that integrates previously separated affine and
deformable coarse-to-fine registration into a single network, and this is also
the first deep registration method that exploits transformers for nice
registration. extensive experiments with seven public datasets show that our
nice-trans outperforms state-of-the-art registration methods on both
registration accuracy and runtime.",10
2464,Non-iterative Coarse-to-Fine Transformer Networks for Joint Affine and Deformable Image Registration,3.1,Dataset and Preprocessing,"we evaluated the proposed nice-trans on the task of inter-patient brain mri
registration, which is a common benchmark task in medical image registration
studies [7][8][9][12][13][14][15][16][17][18]. we followed the dataset settings
in [18]: 2,656 brain mri images acquired from four public datasets (adni [27],
abide [28], adhd [29], and ixi [30]) were used for training; two public brain
mri datasets with anatomical segmentation (mindboggle [31] and buckner [32])
were used for validation and testing. the mindboggle dataset contains 100 mri
images and were randomly split into 50/50 images for validation/testing. the
buckner dataset contains 40 mri images and were used for testing only. in
addition to the original settings of [18], we adopted an additional public brain
mri dataset (lpba [33]) for testing, which contains 40 mri images.we performed
brain extraction and intensity normalization for each mri image with freesurfer
[32]. each image was placed at the same position via center of mass (com)
initialization [34], and then was cropped into 144 × 192 × 160 voxels.",10
2468,Non-iterative Coarse-to-Fine Transformer Networks for Joint Affine and Deformable Image Registration,4.0,Results and Discussion,"table 1 presents the registration performance of our nice-trans and all
comparison methods. the registration accuracy of all methods degraded by 1-3% in
dsc when affine registration was not performed, which demonstrates the
importance of affine registration. however, using flirt or affinenet for affine
registration incurred extra computational loads and increased the registration
runtime. our nice-trans performed joint affine and deformable registration,
which enabled it to realize affine registration with negligible additional
runtime. moreover, we suggest that integrating affine and deformable
registration into a single network also brings convenience for network training.
training two separate affine and deformable registration networks will prolong
the whole training time, while joint training will consume more gpu memory. as
for registration accuracy, the transmorph and swin-vm achieved higher dscs than
the conventional vm and difvm, but still cannot outperform the existing
cnn-based coarse-to-fine registration methods (lapirn, ulae-net, and nice-net).
our nice-trans leverages swin transformer to perform coarse-to-fine
registration, which enabled it to achieve the highest dscs among all methods.
this means that our nice-trans also has advantages on registration accuracy. we
present a qualitative comparison in the supplementary materials, which shows
that the registration result produced by our nice-trans is more consistent with
the fixed image. in addition, there usually exists a trade-off between dsc and
njd as imposing constraints on the spatial transformations limits their
flexibility, which results in degraded registration accuracy [13,18]. for
example, compared with vm, the difvm with diffeomorphic constraints achieved
better njds and worse dscs. nevertheless, our nice-trans achieved both the best
dscs and njds. we suggest that, if we set λ as 0 to maximize the registration
accuracy with the cost of transformation invertibility, our nice-trans can
achieve higher dscs and outperform the comparison methods by a larger margin
(refer to the regularization analysis in the supplementary materials).table 2
shows the results of our ablation study. swin transformer improved the
registration performance when embedded into the decoder, but had limited
benefits in the encoder. this suggests that swin transformer can benefit
registration in modeling inter-image spatial relevance while having limited
benefits in learning intra-image representations. this finding is intuitive as
image registration aims to find spatial relevance between images, instead of
finding the internal relevance within an image. under this aim, embedding
transformers in the decoder helps to capture long-range relevance between images
and improves registration performance. we noticed that previous studies gained
improvements by embedding swin transformer in the encoder [21] or leveraging a
full transformer network [22]. this is attributed to the fact that they used a
vm-like architecture that entangles image representation learning and spatial
relevance modeling throughout the whole network. our nice-trans decouples these
two parts and provides further insight on using transformers for registration:
leveraging transformers to learn intra-image relevance might not be beneficial
but merely incurs extra computational loads. it should be acknowledged that
there are a few limitations in our study. first, the experiment (table 1)
demonstrated that our nice-trans can well address the inherent misalignments
among inter-patient brain mri images, but the sensitivity of affine registration
to different degrees of misalignments is still awaiting further exploration.
second, in this study, we evaluated the nice-trans on the benchmark task of
inter-patient brain mri registration, while we believe that our nice-trans also
could apply to other image registration applications (e.g., brain tumor
registration [37]).",10
2472,Trackerless Volume Reconstruction from Intraoperative Ultrasound Images,1.0,Introduction,"liver cancer is the most prevalent indication for liver surgery, and although
there have been notable advancements in oncologic therapies, surgery remains as
the only curative approach overall [20].liver laparoscopic resection has
demonstrated fewer complications compared to open surgery [21], however, its
adoption has been hindered by several reasons, such as the risk of unintentional
vessel damage, as well as oncologic concerns such as tumor detection and margin
assessment. hence, the identification of intrahepatic landmarks, such as
vessels, and target lesions is crucial for successful and safe surgery, and
intraoperative ultrasound (ious) is the preferred technique to accomplish this
task. despite the increasing use of ious in surgery, its integration into
laparoscopic workflows (i.e., laparoscopic intraoperative ultrasound) remains
challenging due to combined problems.performing ious during laparoscopic liver
surgery poses significant challenges, as laparoscopy has poor ergonomics and
narrow fields of view, and on the other hand, ious demands skills to manipulate
the probe and analyze images. at the end, and despite its real-time
capabilities, ious images are intermittent and asynchronous to the surgery,
requiring multiple iterations and repetitive steps (probe-in -→ instruments-out
-→ probe-out -→ instruments-in). therefore, any method enabling a continuous and
synchronous us assessment throughout the surgery, with minimal iterations
required would significantly improve the surgical workflow, as well as its
efficiency and safety.to overcome these limitations, the use of intravascular
ultrasound (ivus) images has been proposed, enabling continuous and synchronous
inside-out imaging during liver surgery [19]. with an intravascular approach, an
overall view and full-thickness view of the liver can quickly and easily be
obtained through mostly rotational movements of the catheter, while this is
constrained to the lumen of the inferior vena cava, and with no interaction with
the tissue (contactless, a.k.a. standoff technique) as illustrated in fig. 1.
however, to benefit from such a technology in a computer-guided solution, the
different us images would need to be tracked and possibly integrated into a
volume for further processing. external us probes are often equipped with an
electromagnetic tracking system to track its position and orientation in
realtime. this information is then used to register the 3d ultrasound image with
the patient's anatomy. the use of such an electromagnetic tracking system in
laparoscopic surgery is more limited due to size reduction. the tracking system
may add additional complexity and cost to the surgical setup, and the tracking
accuracy may be affected by metallic devices in the surgical field [22].several
approaches have been proposed to address this limitation by proposing a
trackerless ultrasound volume reconstruction. physics-based methods have
exploited speckle correlation models between different adjacent frames [6][7][8]
to estimate their relative position. with the recent advances in deep learning,
recent works have proposed to learn a higher order nonlinear mapping between
adjacent frames and their relative spatial transformation. prevost et al. [9]
first demonstrated the effectiveness of a convolution neural network to learn
the relative motion between a pair of us images. xie et al. [10] proposed a
pyramid warping layer that exploits the optical flow features in addition to the
ultrasound features in order to reconstruct the volume. to enable a smooth 3d
reconstruction, a case-wise correlation loss based on 3d cnn and pearson
correlation coefficient was proposed in [10,12]. qi et al. [13] leverages past
and future frames to estimate the relative transformation between each pair of
the sequence; they used the consistency loss proposed in [14]. despite the
success of these approaches, they still suffer significant cumulative drift
errors and mainly focus on linear probe motions. recent work [15,16] proposed to
exploit the acceleration and orientation of an inertial measurement unit (imu)
to improve the reconstruction performance and reduce the drift error. motivated
by the weakness of the state-of-the-art methods when it comes to large
non-linear probe motions, and the difficulty of integrating imu sensors in the
case of minimally invasive procedures, we introduce a new method for pose
estimation and volume reconstruction in the context of minimally invasive
trackerless ultrasound imaging. we use a siamese architecture based on a
sequence to vector(seq2vec) neural network that leverages image and optical flow
features to learn relative transformation between a pair of images.our method
improves upon previous solutions in terms of robustness and accuracy,
particularly in the presence of rotational motion. such motion is predominant in
the context highlighted above and is the source of additional nonlinearity in
the pose estimation problem. to the best of our knowledge, this is the first
work that provides a clinically sound and efficient 3d us volume reconstruction
during minimally invasive procedures. the paper is organized as follows: sect. 2
details the method and its novelty, sect. 3 presents our current results on ex
vivo porcine data, and finally, we conclude in sect. 4 and discuss future work.",10
2478,Trackerless Volume Reconstruction from Intraoperative Ultrasound Images,3.1,Dataset and Implementation Details,"to validate our method, six tracked sequences were acquired from an ex vivo
swine liver. a manually manipulated ivus catheter was used (8 fr lateral firing
acunav tm 4-10 mhz) connected to an ultrasound system (acuson s3000 helx touch,
siemens healthineers, germany), both commercially available. an electromagnetic
tracking system (trakstar tm , ndi, canada) was used along with a 6 dof sensor
(model 130) embedded close to the tip of the catheter, and the plus toolkit [17]
along with 3d slicer [18] were used to record the sequences. the frame size was
initially 480 × 640. frames were cropped to remove the patient and probe
characteristics, then down-sampled to a size of 128 × 128 with an image spacing
of 0.22 mm per pixel. first and end stages of the sequences were removed from
the six acquired sequences, as they were considered to be largely stationary,
and aiming to avoid training bias. clips were created by sliding a window of 7
frames (corresponding to a value of k = 2) with a stride of 1 over each
continuous sequence, yielding a data set that contains a total of 13734 clips.
the tracking was provided for each frame as a 4×4 transformation matrix.we have
converted each to a vector of six degrees of freedom that corresponds to three
translations in mm and three euler angles in degrees. for each clip, relative
frame to frame transformations were computed for the frames number 0, 3 and 6.
the distribution of the relative transformation between the frames in our clips
is illustrated in the fig. 5. it is clear that our data mostly contains
rotations, in particular over the axis x. heatmaps were calculated for two
points (m = 2) and with a quality level of 0.1, a minimum distance of 7 and a
block size of 7 for the optical flow algorithm (see [4] for more details). the
number of heatmaps m and the frame jump k were experimentally chosen among 0, 2,
4, 6.the data was split into train, validation and test sets by a ratio of
7:1.5:1.5. our method is implemented in pytorch1 1.8.2, trained and evaluated on
a geforce rtx 3090. we use an adam optimizer with a learning rate of 10 -4 . the
training process converges in 40 epochs with a batch size of 16. the model with
the best performance on the validation data was selected and used for the
testing.",10
2481,CoLa-Diff: Conditional Latent Diffusion Model for Multi-modal MRI Synthesis,1.0,Introduction,"magnetic resonance imaging (mri) is critical to the diagnosis, treatment, and
follow-up of brain tumour patients [26]. multiple mri modalities offer
complementary information for characterizing brain tumours and enhancing patient
l. jiang and y. mao-contribute equally in this work.management [4,27]. however,
acquiring multi-modality mri is time-consuming, expensive and sometimes
infeasible in specific modalities, e.g., due to the hazard of contrast agent
[15]. trans-modal mri synthesis can establish the mapping from the known domain
of available mri modalities to the target domain of missing modalities,
promising to generate missing mri modalities effectively. the synthetic methods
leveraging multi-modal mri, i.e., many-to-one translation, have outperformed
single-modality models generating a missing modality from another available
modality, i.e., one-to-one translation [23,33]. traditional multi-modal methods
[21,22], e.g., sparse encoding-based, patch-based and atlasbased methods, rely
on the alignment accuracy of source and target domains and are poorly scalable.
recent generative adversarial networks (gans) and variants, e.g., mm-gan [23],
diamondgan [13] and provogan [30], have been successful based on multi-modal
mri, further improved by introducing multi-modal coding [31], enhanced
architecture [7], and novel learning strategies [29].despite the success,
gan-based models are challenged by the limited capability of adversarial
learning in modelling complex multi-modal data distributions [25] recent studies
have demonstrated that gans' performance can be limited to processing and
generating data with less variability [1]. in addition, gans' hyperparameters
and regularization terms typically require fine-tuning, which otherwise often
results in gradient vanish and mode collapse [2].diffusion model (dm) has
achieved state-of-the-art performance in synthesizing natural images, promising
to improve mri synthesis models. it shows superiority in model training [16],
producing complex and diverse images [9,17], while reducing risk of modality
collapse [12].for instance, lyu et al. [14] used diffusion and score-marching
models to quantify model uncertainty from monte-carlo sampling and average the
output using different sampling methods for ct-to-mri generation; özbey et al.
[19] leveraged adversarial training to increase the step size of the inverse
diffusion process and further designed a cycle-consistent architecture for
unpaired mri translation.however, current dm-based methods focus on one-to-one
mri translation, promising to be improved by many-to-one methods, which requires
dedicated design to balance the multiple conditions introduced by multi-modal
mri. moreover, as most dms operate in original image domain, all markov states
are kept in memory [9], resulting in excessive burden. although latent diffusion
model (ldm) [20] is proposed to reduce memory consumption, it is less feasible
for many-to-one mri translation with multi-condition introduced. further,
diffusion denoising processes tend to change the original distribution structure
of the target image due to noise randomness [14], rending dms often ignore the
consistency of anatomical structures embedded in medical images, leading to
clinically less relevant results. lastly, dms are known for their slow speed of
diffusion sampling [9,11,17], challenging its wide clinical application.we
propose a dm-based multi-modal mri synthesis model, cola-diff, which facilitates
many-to-one mri translation in latent space, and preserve anatomical structure
with accelerated sampling. our main contributions include: -present a denoising
diffusion probabilistic model based on multi-modal mri.as far as we know, this
is the first dm-based many-to-one mri synthesis model. -design a bespoke
architecture, e.g., similar cooperative filtering, to better facilitate
diffusion operations in the latent space, reducing the risks of excessive
information compression and high-dimensional noise. -introduce structural
guidance of brain regions in each step of the diffusion process, preserving
anatomical structure and enhancing synthesis quality. -propose an auto-weight
adaptation to balance multi-conditions and maximise the chance of leveraging
relevant multi-modal information.",10
2486,CoLa-Diff: Conditional Latent Diffusion Model for Multi-modal MRI Synthesis,3.1,Comparisons with State-of-the-Art Methods,"datasets and baselines. we evaluated cola-diff on two multi-contrast brain mri
datasets: brats 2018 and ixi datasets. the brats 2018 contains mri scans from
285 glioma patients. each includes four modalities: t1, t2, t1ce, and flair. we
split them into (190:40:55) for training/validation/testing. for each subject,
we automatically selected axial cross-sections based on the perceptible
effective area of the slices, and then cropped the selected slices to a size of
224 × 224. the ixi1 dataset consists of 200 multi-contrast mris from healthy
brains, plit them into (140:25:35) for training/validation/testing. for
preprocessing, we registered t2-and pd-weighted images to t1-weighted images
using fsl-flirt [10], and other preprocessing are identical to the brats 2018.
we compared cola-diff with four state-of-the-art multi-modal mri synthesis
methods: mm-gan [23], hi-net [33], provogan [30] and ldm [20]. implementation
details. our code is publicly available at https://github. com/seemeincrown/cola
diff multimodal mri synthesis. the hyperparameters of cola-diff are defined as
follows: diffusion steps to 1000; noise schedule to linear; attention
resolutions to 32, 16, 8; batch size to 8, learning rate to 9.6e -5.the noise
variances were in the range of β 1 = 10 -4 and β t = 0.02. an exponential moving
average (ema) over model parameters with a rate of 0.9999 was employed. the
model is trained on 2 nvidia rtx a5000, 24 gb with adam optimizer on pytorch. an
acceleration method [11] based on knowledge distillation was applied for fast
sampling. quantitative results. we performed synthesis experiments for all
modalities, with each modality selected as the target modality while remaining
modalities and the generated region masks as conditions. seven cases were tested
in two datasets (table 1). the results show that cola-diff outperforms other
models by up to 6.01 db on psnr and 5.74% on ssim. even when compared to the
best of other models in each task, cola-diff is a maximum of 0.81 db higher in
psnr and 0.82% higher in ssim. qualitative results. the first three and last
three rows in fig. 2 illustrate the synthesis results of t1ce from brats and pd
from the ixi, respectively. from the generated images, we observe that cola-diff
is most comparable to the ground truth, with fewer errors shown in the heat
maps. the synthesis uncertainty for each region is derived by performing 100
generations of the same slice and calculating the pixel-wise variance. from the
uncertainty maps, cola-diff is more confident in synthesizing the gray and white
matter over other comparison models. particularly, cola-diff performs better in
generating complex brain sulcus and tumour boundaries. further, cola-diff could
better maintain the anatomical structure over comparison models.",10
2488,CoLa-Diff: Conditional Latent Diffusion Model for Multi-modal MRI Synthesis,4.0,Conclusion,"this paper presents cola-diff, a dm-based multi-modal mri synthesis model with a
bespoke design of network backbone, similar cooperative filtering, structural
guidance and auto-weight adaptation. our experiments support that cola-diff
achieves state-of-the-art performance in multi-modal mri synthesis tasks.
therefore, cola-diff could serve as a useful tool for generating mri to reduce
the burden of mri scanning and benefit patients and healthcare providers.",10
