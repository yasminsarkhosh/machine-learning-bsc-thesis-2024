<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Model-Agnostic Framework for Universal Anomaly Detection of Multi-organ and Multi-modal Images</title>
				<funder ref="#_y5FJpJD">
					<orgName type="full">National Key Research and Development Program of China</orgName>
				</funder>
				<funder ref="#_2GkMvbz">
					<orgName type="full">National Key R&amp;D Program of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yinghao</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Institute for Data Science in Health and Medicine</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<settlement>Xiamen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Donghuan</forename><surname>Lu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Tencent Healthcare Co</orgName>
								<address>
									<addrLine>Jarvis Lab</addrLine>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Munan</forename><surname>Ning</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Shenzhen Graduate School</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Liansheng</forename><surname>Wang</surname></persName>
							<email>lswang@xmu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">National Institute for Data Science in Health and Medicine</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<settlement>Xiamen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dong</forename><surname>Wei</surname></persName>
							<email>donwei@tencent.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Tencent Healthcare Co</orgName>
								<address>
									<addrLine>Jarvis Lab</addrLine>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yefeng</forename><surname>Zheng</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Tencent Healthcare Co</orgName>
								<address>
									<addrLine>Jarvis Lab</addrLine>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Model-Agnostic Framework for Universal Anomaly Detection of Multi-organ and Multi-modal Images</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="232" to="241"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">DF2AC1ECD6508BA155C0C754D27D4139</idno>
					<idno type="DOI">10.1007/978-3-031-43898-1_23</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T10:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Anomaly Detection</term>
					<term>Medical Images</term>
					<term>Multi-Organ</term>
					<term>Multi-modality</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The recent success of deep learning relies heavily on the large amount of labeled data. However, acquiring manually annotated symptomatic medical images is notoriously time-consuming and laborious, especially for rare or new diseases. In contrast, normal images from symptom-free healthy subjects without the need of manual annotation are much easier to acquire. In this regard, deep learning based anomaly detection approaches using only normal images are actively studied, achieving significantly better performance than conventional methods. Nevertheless, the previous works committed to develop a specific network for each organ and modality separately, ignoring the intrinsic similarity among images within medical field. In this paper, we propose a model-agnostic framework to detect the abnormalities of various organs and modalities with a single network. By imposing organ and modality classification constraints along with center constraint on the disentangled latent representation, the proposed framework not only improves the generalization ability of the network towards the simultaneous detection of anomalous images with various organs and modalities, but also boosts the performance on each single organ and modality. Extensive experiments with four different baseline models on three public datasets demonstrate the superiority of the proposed framework as well as the effectiveness of each component.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Although deep learning has achieved great success in various computer vision tasks <ref type="bibr" target="#b10">[11]</ref>, the requirement of large amounts of labeled data limits its application in Y. Zhang and D. Lu-Contributed equally. Fig. <ref type="figure">1</ref>. Overview of the proposed framework incorporated into the DPA method <ref type="bibr" target="#b14">[15]</ref>. Two classification constraints (organ and modality), and a center constraint are applied on the disentangled latent representation in addition to the original loss(es) of the baseline model.</p><p>the field of medical image analysis. Annotated abnormal images are difficult to acquire, especially for rare or new diseases, such as the COVID-19, whereas normal images are much easier to obtain. Therefore, many efforts <ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref> have been made on deep learning based medical anomaly detection, which aims to learn the distribution of normal patterns from healthy subjects and detect the anomalous ones as outliers.</p><p>Due to the absence of anomalous subjects, most previous studies adopted the encoder-decoder structure or generative adversarial network (GAN) as backbone to obtain image reconstruction error as the metric for recognition of outliers <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b19">20]</ref>. Other approaches <ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref> learned the normal distribution <ref type="foot" target="#foot_0">1</ref> along with the decision boundaries to differentiate anomalous subjects from normal ones. Despite the improvement they achieved over traditional one-class classification methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b11">12]</ref>, all these works committed to train a specific network to detect the anomalies of each organ, neglecting the intrinsic similarity among different organs. We hypothesize that there are underlying patterns in the normal images of various organs and modalities despite their seemly different appearance, and a model, which can fully exploit their latent information, not only has better generalization ability to recognize the anomalies within different organs/modalities, but also can achieve superior performance for detecting the anomalies of each of them.</p><p>To this end, we propose a novel model-agnostic framework, denoted as Multi-Anomaly Detection with Disentangled Representation (MADDR), for the simultaneous detection of anomalous images within different organs and modalities. As displayed in Fig. <ref type="figure">1</ref>, to fully explore the underlying patterns of normal images as well as bridge the appearance gap among different organs and modalities, the latent representation z is disentangled into three parts, i.e., two categorical parts (z o and z m , corresponding to organ and modality, respectively) and a continuous variable (z c ). The first two parts represent the categorical information for the distinction of specific organs and modalities, respectively, while the last part denotes the feature representation for characterizing each individual subject. Specifically, we propose to impose an organ classification constraint (L o ) as well as a modality classification constraint (L m ) on the categorical parts to leverage the categorical information, and a center constraint on the continuous variable part to compact the feature representation of the normal distribution so that the outliers can be easier to identify. It is worth mentioning that the categorical label of medical images are easy to obtain because such information should be recorded during image acquisition, and with the disentanglement strategy, the potential contradiction between the classification constraint (which aims to separate images of different organs and different modalities) and the center constraint (which tends to compact the feature representation) can be avoided. Our contributions can be summarized as follows:</p><p>-To the best of our knowledge, this is the first study to detect the anomalies of multiple organs and modalities with a single network. We show that introducing images from different organs and modalities with the proposed framework not only extends the generalization ability of the network towards the recognition of the anomalies within various data, but also improves its performance on each single kind. -We propose to disentangle the latent representation into three parts so that the categorical information can be fully exploited through the classification constraints, and the feature representation of normal images is tightly clustered with the center constraint for better identification of anomalous pattern. -Extensive experiments demonstrate the superiority of the proposed framework regarding the medical anomaly detection task, as well as its universal applicability to various baseline models. Moreover, the effectiveness of each component is evaluated and discussed with thorough ablation study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>Unlike previous approaches which trained a separate model to capture the anomalies for each individual organ and modality, in this study we aim to exploit the normal images of multiple organs and modalities to train a generic network towards better anomaly detection performance for each of them. The proposed framework is model-agnostic and can be readily applied to most standard anomaly detection methods. For demonstration, we adopt four state-of-the-art anomaly detection methods, i.e., deep perceptual autoencoder (DPA) <ref type="bibr" target="#b14">[15]</ref>, memory-augmented autoencoder (MemAE) <ref type="bibr" target="#b3">[4]</ref>, generative adversarial networks based anomaly detection (GANomaly) <ref type="bibr" target="#b1">[2]</ref>, and fast unsupervised anomaly detection with generative adversarial networks (f-AnoGAN) <ref type="bibr" target="#b12">[13]</ref> as baseline methods.</p><p>In this section, we present the proposed universal framework for the anomaly detection task of medical images in details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Framework Overview</head><p>We first formulate the anomaly detection task for images with various organs and modalities. For a dataset targeting a specific organ k (k ∈ [0, K]) and modality l (l ∈ [0, L]), there is a training dataset D k,l with only normal images and a test set D t k,l with both normal and abnormal images. In this study, we use N k to represent total number of images targeting organ k and N l to represent total number of images belong to modality l. The goal of our study is to train a generic model with these multi-organ and multi-modality normal images to capture the intrinsic normal distribution of training sets, such that the anomalies in test sets can be recognized as outliers.</p><p>To better elaborate the process of applying our MADDR framework on standard anomaly detection methods, we first briefly introduce the workflow of a baseline method, DPA <ref type="bibr" target="#b14">[15]</ref>. As shown in Fig. <ref type="figure">1</ref>, the network of DPA consists of an autoencoder and a pre-trained feature extractor. Through autoencoder, the images are encoded into latent representations and then reconstructed into the original image space. The relative perceptual loss is adopted as the objective function for the optimization of autoencoder and the measurement of anomaly, which is defined as</p><formula xml:id="formula_0">L rec (x, x) = f (x)-f (x) 1 f (x) 1</formula><p>, where</p><p>x and x denote the input image and the reconstructed one, respectively. f (x) = f (x)-μ σ is the normalized feature with mean μ and standard deviation σ pre-calculated on a large dataset, where f (•) represents the mapping function of the pre-trained feature extractor. By comparing the features of original images and the reconstructed ones through the relative perceptual loss, the subjects with loss larger than the threshold are recognized as abnormal ones. To fully exploit the underlying patterns in the normal images of various organs and modalities, we incorporate additional constraints on the encoded latent representations.</p><p>Specifically, our MADDR approach encourages the model to convert the input image x into a latent representation z, which consists of disentangled category and individuality information. To be more precise, the encoded latent representation z is decomposed into three parts, i.e., the organ category part z o , the modality category part z m and the continuous variable part z c . Here, z o and z m represent the categorical information (which is later converted into the probabilities of x belonging to each organ and modality through two separate fully-connected layers), and z c denotes the feature representation for characterizing each individual image (which should be trained to follow the distribution of normal images). Leveraging the recorded categorical information of the images, we impose two classification constraints on z o and z m , respectively, along with a center constraint on z c to compact the cluster of feature representation in addition to the original loss(es) of the baseline methods. In this study, we evaluate the proposed model-agnostic framework on four cutting-edge methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b14">15]</ref> with their networks as baseline models, and their original losses along with the proposed constraints as the training objective functions. If we use L b , L o , L m and L c to represent the loss of the baseline method, the organ classification constraint, the modality classification constraint and the center constraint, respectively, the overall loss function of the proposed framework can be formulated as:</p><formula xml:id="formula_1">L = L b (X) + λ 1 L o (X, Y ) + λ 2 L m (X, Y ) + λ 3 L c (X),<label>(1)</label></formula><p>where X and Y denote the set of images and labels, respectively, while λ 1 , λ 2 and λ 3 are the weights to balance different losses, and set to 1 in this study (results of exploratory experiment are displayed in the supplementary material).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Organ and Modality Classification Constraints</head><p>Benefiting from the acquisition procedure of medical images, the target organ and modality of each scan should be recorded and can be readily used in our study. Based on the assumption that a related task could provide auxiliary guidance for network training towards superior performance regrading the original task, we introduce two additional classification constraints (organ and modality classification) to fully exploit such categorical information. Through an additional organ classifier (a fully-connected layer), the organ classification constraint is applied on the transformed category representation z o by distinguishing different organs. A similar constraint is also applied on the modality representation z m . Considering the potential data imbalance issue among images of different organs and modalities, we adopt the focal loss <ref type="bibr" target="#b6">[7]</ref> as the classification constraints by adding a modulating factor to the cross entropy loss. Using z i o to represent the organ category representation of image i, the organ classification loss can be formulated as:</p><formula xml:id="formula_2">L o = - K k=1 N k i=1 α k o 1 -P k (z i o ) γ log P k (z i o ) , (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>where α k o denotes the weight to balance the impact of different organs and P k (z i o ) represents the probability of image i belonging to class k. The focusing parameter γ can reduce the contribution of easy samples to the loss function and extend the range of loss values for comparison.</p><p>Similarity, with the modality category representation z i m , the modality classification loss can be written as:</p><formula xml:id="formula_4">L m = - L l=1 N l i=1 α l m 1 -P l (z i m ) γ log P l (z i m ) . (<label>3</label></formula><formula xml:id="formula_5">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Center Constraint</head><p>Intuitively, the desired quality of a representation is to have similar feature embeddings for images of the same class. Because in the anomaly detection task, all the images used for training belong to the same normal group, we impose a center constraint so that the features from the normal images are tightly clustered to the center and the encoded features of abnormal images lying far from the normal cluster are easy to identify. However, directly compacting the latent representation into a cluster is potentially contradictory to the organ and modality classification tasks which aim to separate different organs and modalities. To avoid the contradiction, we propose to impose the center constraint only on the continuous variable part z c with Euclidean distance as the measurement of the compactness. Similar to <ref type="bibr" target="#b9">[10]</ref>, if we use z i c to represent the continuous variable representation of image i, the measurement of compactness can be defined as:</p><formula xml:id="formula_6">L c = B b=1 1 L b L b i=1 (z i c -m i ) T (z i c -m i ),<label>(4)</label></formula><p>where L b represents the number of images in batch b, B denotes the number of batches and</p><formula xml:id="formula_7">m i = 1 L b -1 j =i z j c</formula><p>is the mean of the rest images in the same batch as image i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Optimization and Inference</head><p>To demonstrate the effectiveness of the proposed framework, we inherit the network architectures and most procedures of the baseline methods for a fair comparison. During the optimization stage, there are only two differences: 1) we introduce three additional losses as stated above; 2) considering the limited number of images, mixup <ref type="bibr" target="#b16">[17]</ref> is applied for data augmentation to better bridge the gap among different organs and modalities. For other factors, such as optimization algorithm and related hyperparameters, we follow the original settings of the baseline methods. During inference, the same metrics of the baseline methods are adopted to measure the anomaly scores. For the details about the baseline methods, please refer to their original studies <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b14">15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Setting</head><p>We evaluate our method on three benchmark datasets targeting various organs and modalities as stated below.</p><p>-The LiTS-CT dataset <ref type="bibr" target="#b2">[3]</ref> consists of 3D volumetric data with rare healthy subjects.</p><p>To ensure a sufficient amount of training data, we remove the 2D abnormal slices from some patients and use the rest normal slices for training. Therefore, 186 axial CT slices from 77 3D volumes are used as normal data for training, 192 normal and 105 abnormal slices from 27 3D volumes for validation, and 164 normal and 249 abnormal slices from the rest 27 3D volumes for testing. -The Lung-X-rays <ref type="bibr" target="#b5">[6]</ref> is a small dataset consists of 2D frontal chest X-ray images primarily from a hospital clinical routine. Following the same data split protocol provided by <ref type="bibr" target="#b18">[19]</ref>, the images are divided into three groups for training (228 normal images), validation (33 normal and 34 abnormal images) and testing (65 normal and 67 abnormal images). -The Lung-CT <ref type="bibr" target="#b17">[18]</ref> contains 2D slices regarding COVID-19. An official data split is provided, which contains 234 normal images for training, 58 normal and 60 abnormal images for validation, and 105 normal and 98 abnormal images for testing.</p><p>To evaluate the performance of proposed framework, we adopt three widely used metrics, including the area under the curve (AUC) of the receiver operating characteristic, F1-score and accuracy (ACC). Grid search is performed to find the optimal threshold based on the F1-score of the validation set. For all three metrics, a higher score implies better performance. The experiments are repeated three times with different random seeds to verify the robustness of the framework and provide more reliable results.</p><p>The framework is implemented with PyTorch 1.4 toolbox <ref type="bibr" target="#b7">[8]</ref> using an NVIDIA Titan X GPU. As detailed in the supplementary material, we keep most parameters the same as the baseline methods, except for the original dimension of DPA's latent feature, which is increased from 16 to 128 to ensure sufficient model capacity for valid latent representation disentanglement of each image. For the hyper-parameters of focal loss, both α k o and α l m are set as 0.25, and γ is set to 2. For preprocessing, we first </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Comparison Study</head><p>The quantitative results of the proposed framework and the state-of-the-art methods are displayed in Table <ref type="table" target="#tab_0">1</ref>. In four experiments with different baseline methods, we can observe significant improvement in all three metrics on various organs and modalities, demonstrating that the proposed framework can effectively boost the anomaly detection performance regardless the baseline approaches. In addition, the baseline methods need to train separate networks for different organs and modalities, while with the proposed framework, a generic network can be applied to recognize the abnormalities of all datasets. In the supplementary material, we further present the t-SNE visualization of the continuous variable part of the latent representation to show that introducing the proposed MADDR framework can deliver obviously more tightly compacted normal distributions, leading to better identification of the abnormal outliers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Ablation Study</head><p>To further demonstrate the effectiveness of each component, we perform an ablation study with the following variants: 1) the baseline DPA method (the dimensions of z is 16) which trains networks for each organ and modality separately; 2) the baseline DPA method with the dimensions of z increased to 128; 3) using the images of LiTS, Lung-CT and Lung-X-rays datasets together to train the same DPA network; The results of all these variants are displayed in Table <ref type="table" target="#tab_1">2</ref>. As shown in variant 3, directly introducing more datasets for network training does not necessarily improve the performance on each dataset, due to the interference of organ and modality information. However, with the additional classification constraints, the organ and modality information can be effectively separated from the latent representation of normal distribution, such that better anomaly detection performance can be achieved for each dataset, as shown in variants 4, 7 and 10. Furthermore, variants 5, 8 and 11 show that the center constraint can tightly compact the feature representation of normal distribution so that abnormal outliers can be easier to identify. Last but not the least, the proposed feature split strategy can help disentangle the characteristic of normal distribution from classification information, as demonstrated by variants 6, 9 and 12. For the impact of hyper-parameters, including the weights for different loss terms and the dimensions of the disentangled category representations, please refer to the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>Unlike previous studies which committed to train exclusive networks to recognize the anomalies of specific organs and modalities separately, in this work we hypothesized that normal images of various organs and modalities could be combined and utilized to train a generic network and superior performance could be achieved for the recognition of each type of anomaly with proper methodology. With the proposed model-agnostic framework, the organ/modality classification constraint and the center constraint were imposed on the disentangled latent representation to fully utilize the available information as well as improve the compactness of representation to facilitate the identification of outliers. Four state-of-the-art methods were adopted as baseline models for thorough evaluation, and the results on various organs and modalities demonstrated the validity of our hypothesis as well as the effectiveness of the proposed framework.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>4) imposing the organ classification constraint on the encoded latent representation z without feature disentanglement (f.d.); 5) imposing the organ classification constraint and the center constraint on the encoded latent representation z without feature disentanglement; 6) MADDR + DPA trained on LiTS and Lung-CT; 7) imposing the modality classification constraint on the encoded latent representation z without feature disentanglement; 8) imposing the modality classification constraint and the center constraint on the encoded latent representation z without feature disentanglement; 9) MADDR + DPA trained on Lung-CT and Lung-X-rays; 10) imposing both the organ and modality classification constraints on the encoded latent representation z without feature disentanglement; 11) imposing both the organ classification constraint, modality classification constraint and the center constraint on the encoded latent representation z without feature disentanglement (f.d.); 12) MADDR + DPA trained on all three datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="2,59,46,54,14,333,22,171,58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Anomaly detection performance on LiTS, Lung-X-rays and Lung-CT datasets. Mean and standard deviations of three metrics are presented. GANomaly 67.16 ± 4.93 66.37 ± 5.99 69.91 ± 2.39 resize each image to 256 × 256 pixels, and then crop to 224 × 224 pixels. The data augmentation methods are applied on the fly during training, including random cropping, random rotation, mixup and random flipping. The code can be found in https://github. com/lianjizhe/MADDR_code.</figDesc><table><row><cell>Test-Datasets</cell><cell>Methods</cell><cell>ACC (%)</cell><cell>F1-score (%) AUC (%)</cell></row><row><cell>LiTS [3]</cell><cell>DPA [15]</cell><cell cols="2">56.23 ± 2.67 55.92 ± 3.71 61.34 ± 4.64</cell></row><row><cell></cell><cell>MADDR + DPA</cell><cell cols="2">64.87 ± 5.11 64.85 ± 5.36 72.38 ± 1.99</cell></row><row><cell></cell><cell>MemAE [4]</cell><cell cols="2">51.40 ± 3.04 50.58 ± 4.14 55.96 ± 3.16</cell></row><row><cell></cell><cell>MADDR + MemAE</cell><cell cols="2">51.63 ± 3.91 51.23 ± 3.58 57.73 ± 6.96</cell></row><row><cell></cell><cell>f-AnoGAN [13]</cell><cell cols="2">59.26 ± 3.21 58.77 ± 1.89 55.90 ± 5.09</cell></row><row><cell></cell><cell cols="3">MADDR + f-AnoGAN 59.93 ± 4.29 59.64 ± 2.66 61.77 ± 6.40</cell></row><row><cell></cell><cell>GANomaly [2]</cell><cell cols="2">64.76 ± 6.06 63.93 ± 7.12 72.54 ± 2.09</cell></row><row><cell></cell><cell cols="3">MADDR + GANomaly 75.64 ± 2.44 76.03 ± 2.43 79.20 ± 9.87</cell></row><row><cell cols="2">Lung-X-rays [6] DPA [15]</cell><cell cols="2">64.14 ± 0.43 63.98 ± 0.35 72.23 ± 2.25</cell></row><row><cell></cell><cell>MADDR + DPA</cell><cell cols="2">66.92 ± 1.16 66.30 ± 1.80 75.01 ± 0.95</cell></row><row><cell></cell><cell>MemAE [4]</cell><cell cols="2">67.17 ± 1.16 66.40 ± 1.20 73.62 ± 3.09</cell></row><row><cell></cell><cell>MADDR + MemAE</cell><cell cols="2">67.93 ± 3.82 67.39 ± 4.19 76.27 ± 3.53</cell></row><row><cell></cell><cell>f-AnoGAN [13]</cell><cell cols="2">54.29 ± 3.81 52.78 ± 3.12 57.45 ± 5.48</cell></row><row><cell></cell><cell cols="3">MADDR + f-AnoGAN 57.77 ± 7.95 56.22 ± 6.34 66.19 ± 7.59</cell></row><row><cell></cell><cell>GANomaly [2]</cell><cell cols="2">57.71 ± 2.28 56.30 ± 3.20 64.97 ± 2.32</cell></row><row><cell></cell><cell cols="3">MADDR + GANomaly 64.68 ± 6.89 63.13 ± 9.04 68.45 ± 4.67</cell></row><row><cell>Lung-CT [18]</cell><cell>DPA [15]</cell><cell cols="2">57.31 ± 1.42 57.09 ± 1.27 56.17 ± 0.87</cell></row><row><cell></cell><cell>MADDR + DPA</cell><cell cols="2">59.28 ± 3.20 58.90 ± 3.12 58.86 ± 0.86</cell></row><row><cell></cell><cell>MemAE [4]</cell><cell cols="2">51.40 ± 4.19 50.10 ± 5.86 52.67 ± 5.04</cell></row><row><cell></cell><cell>MADDR + MemAE</cell><cell cols="2">55.67 ± 2.47 54.64 ± 3.36 57.13 ± 2.45</cell></row><row><cell></cell><cell>f-AnoGAN [13]</cell><cell cols="2">56.65 ± 5.49 55.85 ± 6.54 58.13 ± 1.59</cell></row><row><cell></cell><cell cols="3">MADDR + f-AnoGAN 58.17 ± 2.80 57.60 ± 3.14 61.30 ± 3.03</cell></row><row><cell></cell><cell>GANomaly [2]</cell><cell cols="2">62.73 ± 1.99 61.61 ± 2.19 67.74 ± 1.15</cell></row><row><cell></cell><cell>MADDR +</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Quantitative ablation study of MADDR + DPA<ref type="bibr" target="#b14">[15]</ref> with AUC (%) as evaluation metric.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Mix Datasets</cell><cell></cell><cell>Loss</cell><cell cols="2">Test Datasets</cell></row><row><cell cols="2">Index Method</cell><cell cols="5">LiTS Lung-CT Lung-X-rays LO LM LC f.d. z</cell><cell>LiTS [3]</cell><cell>Lung-CT [18] Lung-X-rays [6]</cell></row><row><cell>1</cell><cell>Single dataset</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell cols="2">× × × ×</cell><cell>16 58.11 ± 5.82 53.63 ± 1.19 71.93 ± 1.01</cell></row><row><cell>2</cell><cell>Single dataset</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell cols="3">× × × × 128 61.34 ± 4.64 56.17 ± 0.87 72.23 ± 2.25</cell></row><row><cell>3</cell><cell>Three datasets</cell><cell></cell><cell></cell><cell></cell><cell cols="3">× × × × 128 59.75 ± 3.61 55.83 ± 1.37 69.14 ± 1.66</cell></row><row><cell>4</cell><cell>Multi-organ</cell><cell></cell><cell></cell><cell>×</cell><cell cols="3">× × × 128 61.80 ± 4.27 56.19 ± 2.24 -</cell></row><row><cell>5</cell><cell>Multi-organ</cell><cell></cell><cell></cell><cell>×</cell><cell>×</cell><cell cols="2">× 128 61.81 ± 1.86 56.56 ± 2.37 -</cell></row><row><cell>6</cell><cell>Multi-organ</cell><cell></cell><cell></cell><cell>×</cell><cell>×</cell><cell cols="2">128 62.49 ± 0.96 56.96 ± 1.61 -</cell></row><row><cell>7</cell><cell>Multi-modality</cell><cell>×</cell><cell></cell><cell></cell><cell>×</cell><cell cols="2">× × 128 -</cell><cell>56.65 ± 2.15 73.03 ± 1.33</cell></row><row><cell>8</cell><cell>Multi-modality</cell><cell>×</cell><cell></cell><cell></cell><cell>×</cell><cell cols="2">× 128 -</cell><cell>56.18 ± 0.79 73.45 ± 1.20</cell></row><row><cell>9</cell><cell>Multi-modality</cell><cell>×</cell><cell></cell><cell></cell><cell>×</cell><cell cols="2">128 -</cell><cell>57.74 ± 1.75 74.28 ± 0.66</cell></row><row><cell>10</cell><cell>Multi-organ and -modality</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">× × 128 69.11 ± 7.23 57.38 ± 1.89 73.04 ± 0.40</cell></row><row><cell>11</cell><cell>Multi-organ and -modality</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">× 128 70.44 ± 1.30 57.74 ± 1.09 73.15 ± 0.84</cell></row><row><cell>12</cell><cell>MADDR</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">128 72.38 ± 1.99 58.86 ± 0.86 75.01 ± 0.95</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Note in this work we use the term 'normal distribution' to refer to the distribution of images of normal, healthy subjects, instead of the Gaussian distribution.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work was supported by the <rs type="funder">National Key Research and Development Program of China</rs> (<rs type="grantNumber">2019YFE0113900</rs>) and the <rs type="funder">National Key R&amp;D Program of China</rs> under Grant <rs type="grantNumber">2020AAA0109500/2020AAA0109501</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_y5FJpJD">
					<idno type="grant-number">2019YFE0113900</idno>
				</org>
				<org type="funding" xml:id="_2GkMvbz">
					<idno type="grant-number">2020AAA0109500/2020AAA0109501</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1_23.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Latent space autoregression for novelty detection</title>
		<author>
			<persName><forename type="first">D</forename><surname>Abati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Porrello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Calderara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="481" to="490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">GANomaly: semi-supervised anomaly detection via adversarial training</title>
		<author>
			<persName><forename type="first">S</forename><surname>Akcay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Atapour-Abarghouei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Breckon</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-20893-6_39</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-20893-6_39" />
	</analytic>
	<monogr>
		<title level="m">ACCV 2018</title>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Mori</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Schindler</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11363</biblScope>
			<biblScope unit="page" from="622" to="637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Bilic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.04056</idno>
		<title level="m">The liver tumor segmentation benchmark (LiTS)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Memorizing normality to detect anomaly: memory-augmented deep autoencoder for unsupervised anomaly detection</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1705" to="1714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">MADGAN: unsupervised medical anomaly detection GAN using multiple adjacent brain MRI slice reconstruction</title>
		<author>
			<persName><forename type="first">C</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinform</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Two public chest X-ray datasets for computer-aided screening of pulmonary diseases</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jaeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Candemir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Antani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">X J</forename><surname>Wáng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Thoma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quant. Imaging Med. Surg</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">475</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">PyTorch: an imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">OCGAN: one-class novelty detection using GANs with constrained latent representations</title>
		<author>
			<persName><forename type="first">P</forename><surname>Perera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2898" to="2906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning deep features for one-class classification</title>
		<author>
			<persName><forename type="first">P</forename><surname>Perera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5450" to="5463" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A survey on deep learning: algorithms, techniques, and applications</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pouyanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="36" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep one-class classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ruff</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4393" to="4402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">f-AnoGAN: fast unsupervised anomaly detection with generative adversarial networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Schlegl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Seeböck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Waldstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Langs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Schmidt-Erfurth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="30" to="44" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Computer-aided diagnostic system for early detection of acute renal transplant rejection using diffusion-weighted MRI</title>
		<author>
			<persName><forename type="first">M</forename><surname>Shehata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="539" to="552" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Tuluptceva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bakker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Fedulova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">V</forename><surname>Dylov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.13265</idno>
		<title level="m">Anomaly detection with deep perceptual autoencoders</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multi-site diagnostic classification of schizophrenia using discriminant deep learning with functional connectivity MRI</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">L</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EBioMedicine</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="74" to="85" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.13865</idno>
		<title level="m">COVID-CT-dataset: a CT scan dataset about COVID-19</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Generalized radiograph representation learning via cross-supervision between images and free-text radiology reports</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="40" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Memorizing structure-texture correspondence for image anomaly detection</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="page" from="2335" to="2349" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
