<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HENet: Hierarchical Enhancement Network for Pulmonary Vessel Segmentation in Non-contrast CT Images</title>
				<funder ref="#_rgDUNZD">
					<orgName type="full">Science and Technology Commission of Shanghai Municipality</orgName>
					<orgName type="abbreviated">STCSM</orgName>
				</funder>
				<funder ref="#_sHpqkHG">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_RhtCdhT">
					<orgName type="full">National Key Research and Development Program of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wenqi</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Shanghai United Imaging Intelligence Co., Ltd</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiao</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Information Science and Technology</orgName>
								<orgName type="institution">Northwest University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dongdong</forename><surname>Gu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Shanghai United Imaging Intelligence Co., Ltd</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sheng</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Shanghai United Imaging Intelligence Co., Ltd</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiayu</forename><surname>Huo</surname></persName>
							<affiliation key="aff5">
								<orgName type="department">School of Biomedical Engineering and Imaging Sciences (BMEIS)</orgName>
								<orgName type="institution">King&apos;s College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rui</forename><surname>Zhang</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Pulmonary and Critical Care Medicine</orgName>
								<orgName type="institution">West China Hospital</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">Sichuan University</orgName>
								<address>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhihao</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Feng</forename><surname>Shi</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Shanghai United Imaging Intelligence Co., Ltd</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhong</forename><surname>Xue</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Shanghai United Imaging Intelligence Co., Ltd</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yiqiang</forename><surname>Zhan</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Shanghai United Imaging Intelligence Co., Ltd</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xi</forename><surname>Ouyang</surname></persName>
							<email>xi.ouyang@uii-ai.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Shanghai United Imaging Intelligence Co., Ltd</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dinggang</forename><surname>Shen</surname></persName>
							<email>dgshen@shanghaitech.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Shanghai United Imaging Intelligence Co., Ltd</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">HENet: Hierarchical Enhancement Network for Pulmonary Vessel Segmentation in Non-contrast CT Images</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="551" to="560"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">61D7F0FFBE133A1E1DD90F0482194491</idno>
					<idno type="DOI">10.1007/978-3-031-43898-1_53</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T10:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Pulmonary vessel segmentation</term>
					<term>Non-contrast CT</term>
					<term>Hierarchical enhancement</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Pulmonary vessel segmentation in computerized tomography (CT) images is essential for pulmonary vascular disease and surgical navigation. However, the existing methods were generally designed for contrast-enhanced images, their performance is limited by the low contrast and the non-uniformity of Hounsfield Unit (HU) in non-contrast CT images, meanwhile, the varying size of the vessel structures are not well considered in current pulmonary vessel segmentation methods. To address this issue, we propose a hierarchical enhancement network (HENet) for better image-and feature-level vascular representation learning in the pulmonary vessel segmentation task. Specifically, we first design an Auto Contrast Enhancement (ACE) module to adjust the vessel contrast dynamically. Then, we propose a Cross-Scale Nonlocal Block (CSNB) to effectively fuse multi-scale features by utilizing both local and global semantic information. Experimental results show that our approach achieves better pulmonary vessel segmentation outcomes compared to other state-of-the-art methods, demonstrating the efficacy of the proposed ACE and CSNB module. Our code is available at https://github.com/CODESofWenqi/HENet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Segmentation of the pulmonary vessels is the foundation for the clinical diagnosis of pulmonary vascular diseases such as pulmonary embolism (PE), pulmonary hypertension (PH) and lung cancer <ref type="bibr" target="#b8">[9]</ref>. Accurate vascular quantitative analysis is crucial for physicians to study and apply in treatment planning, as well as making surgical plans. Although contrast-enhanced CT images have better contrast for pulmonary vessels compared to non-contrast CT images, the acquisition of contrast-enhanced CT images needs to inject a certain amount of contrast agent to the patients. Some patients have concerns about the possible risk of contrast media <ref type="bibr" target="#b1">[2]</ref>. At the same time, non-contrast CT is the most widely used imaging modality for visualizing, diagnosing, and treating various lung diseases.</p><p>In the literature, several conventional methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16]</ref> have been proposed for the segmentation of pulmonary vessels in contrast-enhanced CT images. Most of these methods employed manual features to segment peripheral intrapulmonary vessels. In recent years, deep learning-based methods have emerged as promising approaches to solving challenging medical image analysis problems and have demonstrated exciting performance in segmenting various biological structures <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17]</ref>. However, for vessel segmentation, the widely used models, such as U-Net and its variants, limit their segmentation accuracy on low-contrast small vessels due to the loss of detailed information caused by the multiple downsampling operations. Accordingly, Zhou et al. <ref type="bibr" target="#b16">[17]</ref> proposed a nested structure UNet++ to redesign the skip connections for aggregating multi-scale features and improve the segmentation quality of varying-size objects. Also, some recent methods combine convolutional neural networks (CNNs) with transformer or non-local block to address this issue <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b17">18]</ref>. Wang et al. <ref type="bibr" target="#b12">[13]</ref> replaced the original skip connections with transformer blocks to better merge the multi-scale contextual information. For this task, Cui et al. <ref type="bibr" target="#b0">[1]</ref> also proposed an orthogonal fused U-Net++ for pulmonary peripheral vessel segmentation. However, all these methods ignored the significant variability in HU values of pulmonary vessels at different regions.</p><p>To summarize, there exist several challenges for pulmonary vessel segmentation in non-contrast CT images: <ref type="bibr" target="#b0">(1)</ref> The contrast between pulmonary vessels and background voxels is extremely low (Fig. <ref type="figure" target="#fig_0">1(c</ref>)); (2) Pulmonary vessels have a complex structure and significant variability in vessel appearance, with different scales in different areas. The central extrapulmonary vessels near the heart have a large irregular ball-like shape, while the shape of the intrapulmonary vessels is delicate and tubular-like (Fig. <ref type="figure" target="#fig_0">1</ref>(a) and (b)). Vessels become thinner as they get closer to the peripheral lung; (3) HU values of vessels in different regions vary significantly, ranging from -850 HU to 100 HU. Normally, central extrapulmonary vessels have higher HU values than peripheral intrapulmonary vessels. Thus, we set different ranges of HU values to better visualize the vessels in Fig. <ref type="figure" target="#fig_0">1(d)</ref> and<ref type="figure">(e)</ref>.</p><p>To address the above challenges, we propose a H ierarchical E nhancement N etwork (HENet) for pulmonary vessel segmentation in non-contrast CT images by enhancing the representation of vessels at both image-and feature-level. For the input CT images, we propose an Auto Contrast Enhancement (ACE) module to automatically adjust the range of HU values in different areas of CT images. It mimics the radiologist in setting the window level (WL) and window width (WW) to better enhance vessels from surrounding voxels, as shown in Fig. <ref type="figure" target="#fig_0">1(d)</ref> and (e). Also, we propose a Cross-Scale Non-local Block (CSNB) to replace the skip connections in vanilla U-Net <ref type="bibr" target="#b10">[11]</ref> structure for the aggregation of multi-scale feature maps. It helps to form local-to-global information connections to enhance vessel information at the feature-level, and address the complex scale variations of pulmonary vessels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>The overview of the proposed method is illustrated in Fig. <ref type="figure" target="#fig_1">2</ref>. Our proposed Hierarchical Enhancement Network (HENet) consists of two main modules: (1) Auto Contrast Enhancement (ACE) module, and (2) Cross-Scale Non-local Block (CSNB) as the skip connection bridge between encoders and decoders. In this section, we present the design of these proposed modules. First, the ACE module is developed to enhance the contrast of vessels in the original CT images for the following vessel segmentation network. After that, we introduce the CSNB module to make the network pay more attention to multi-scale vessel information in the latent feature space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Auto Contrast Enhancement</head><p>In non-contrast CT images, the contrast between pulmonary vessels and the surrounding voxels is pretty low. Also, the HU values of vessels in different regions vary significantly as ranging from -850 HU to 100 HU. Normally, radiologists have to manually set the suitable window level (WL) and window width (WW) for different regions in images to enhance vessels according to the HU value range of surrounding voxels, just as different settings to better visualize the extrapulmonary and intrapulmonary vessels (Fig. <ref type="figure" target="#fig_0">1(d)</ref> and<ref type="figure">(e)</ref>). Instead of a fixed WL/WW as employed in existing methods, we address it by adding an ACE module to automatically enhance the contrast of vessels.</p><p>The ACE module leverages convolution operations to generate dynamic WL and WW for the input CT images according to the HU values covered by the kernel. Here we set the kernel size as 15 × 15 × 15. First, we perform min-max normalization to linearly transform the HU values of the original image X to the range (-1, 1). Then, it passes through a convolution layer to be downsampled into half-size of the original shape, which is utilized to derive the following shift map and scale map. Here, the learned shift map and scale map act as the window level and window width settings of the "width/level" scaling in CT images. We let values in the shift map be the WL, so the tanh activation function is used to limit them within (-1, 1). The values in the scale map denote the half of WW, and we perform the sigmoid activation function to get the range (0, 1). It matches the requirement of the positive integer for WW. After that, the shift map and scale map will be upsampled by the nearest neighbor interpolation into the original size of the input X. This operation can generate identical shift and scale values with each 2 × 2 × 2 window, avoiding sharp contrast changes in the neighboring voxels. The upsampled shift map and scale map are denoted as M shif t and M scale , respectively, and then the contrast enhancement image X ACE can be generated through:</p><formula xml:id="formula_0">X ACE = clip( X -M shif t M scale ).<label>(1)</label></formula><p>It can be observed that the intensity values of input X are re-centered and re-scaled by M shif t and M scale (Fig. <ref type="figure" target="#fig_2">3(c)</ref>). The clip operation (clip(•)) truncates the final output into the range [-1, 1], which sets the intensity value above 1 to 1, and below -1 to -1. In our experiments, we find that a large kernel size for learning of M shif t and M scale could deliver better performance, which can capture more information on HU values from the CT images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Cross-Scale Non-local Block</head><p>There are studies <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b17">18]</ref> showing that non-local operations could capture longrange dependency to improve network performance. To segment pulmonary vessels with significant variability in scale and shape, we design a Cross-Scale Nonlocal Block (CSNB) to fuse the local features extracted by CNN backbone from different scales, and to accentuate the cross-scale dependency to address the complex scale variations of pulmonary vessels.</p><p>Inspired by <ref type="bibr" target="#b17">[18]</ref>, our CSNB incorporates 6 modified Asymmetric Non-local Blocks (ANBs), which integrate pyramid sampling modules into the non-local blocks to largely reduce the computation and memory consumption. As illustrated in Fig. <ref type="figure" target="#fig_1">2</ref>, the CSNB works as the information bridge between encoders and decoders while also ensuring the feasibility of experiments involving large 3D data. Specifically, the I 1 ∼ I 4 are the inputs of CSNB, and O 1 ∼ O 4 are the outputs. Within the CSNB, there are three levels of modified ANBs, we denote them as ANB-H (ANB-Head) and ANB-P (ANB-Post). For the two ANBs in each level, the ANB-H has two input feature maps, and the lower-level feature maps (denoted as F l ) contain more fine-grained information than the higherlevel feature maps (denoted as F h ). We use F h to generate embedding Q, while embeddings K and V are derived from F l . By doing this, CSNB can enhance the dependencies of cross-scale features. The specific computation of ANB proceeds as follows: First, three 1×1×1 convolutions (denoted as Conv(•)) are applied to transform F h and F l into different embeddings Q, K, and V ; then, spatial pyramid pooling operations (denoted as P (•)) are implemented on K and V . The calculation can be expressed as:</p><formula xml:id="formula_1">Q = Conv(F h ), K p = P (Conv(F l )), V p = P (Conv(F l )).<label>(2)</label></formula><p>Next, these three embeddings are reshaped to</p><formula xml:id="formula_2">Q ∈ R N × Ĉ , K p ∈ R S× Ĉ , V p ∈ R S× Ĉ ,</formula><p>where N represents the total count of the spatial locations, i.e., N = D × H × W and S is equivalent to the concatenated output size after the spatial pyramid pooling, i.e., setting S = 648. The similarity matrix between Q and K p is obtained through matrix multiplication and normalized by softmax function to get a unified similarity matrix. The attention output is acquired by:</p><formula xml:id="formula_3">O = Sof tmax(Q × (K p ) T ) × V p ,<label>(3)</label></formula><p>where the output O ∈ R N × Ĉ . The final output of ANB is given as:</p><formula xml:id="formula_4">O f inal = Conv(cat(O T , F h )),<label>(4)</label></formula><p>where the final convolution is used as a weighting parameter to adjust the importance of this non-local operation and recover the channel dimension to C h . ANB-P has the same structure as ANB-H, but the inputs F h and F l here are the same, which is the output of ANB-H at the same level. The ANB-P is developed to further enhance the intra-scale connection of the fused features in different regions, which is equivalent to the self-attention mechanism. Note that, O 1 is directly skipped from I 1 . For the first level of CSNB, the input F l of ANB-H is the I 1 , while for the other levels, the input F l is the output of ANB-P of the above level. That is, each level of CSNB can fuse the feature maps from its corresponding level with the fused feature maps from all of the above lower levels. Thereby, the response of multi-scale vessels can be enhanced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head><p>Dataset and Evaluation Metrics. We use a total of 160 non-contrast CT images with the inplane size of 512 × 512, where the slice number varies from 217 to 622. The axial slices have the same spacing ranging from 0.58 to 0.86 mm, and the slice thickness varies from 0.7 to 1.0 mm. The annotations of pulmonary vessels are semi-automatically segmented in 3D by two radiologists using the 3D Slicer software. This study is approved by the ethical committee of West China Hospital of Sichuan University, China. These cases are randomly split into a training set (120 scans) and a testing set (40 scans). The quantitative results are reported by Dice Similarity Coefficient (Dice), mean Intersection over Union (mIoU), False Positive Rate (FPR), Average Surface Distance (ASD), and Hausdorff Distance (HD). For the significance test, we use the paired t-test. Implementation Details. Our experiments are implemented using Pytorch framework and trained using a single NVIDIA-A100 GPU. We pre-process the data by truncating the HU value to the range of [-900, 900] and then linearly scaling it to [-1, 1]. In the training stage, we randomly crop sub-volumes with the size of 192 × 192 × 64 near the lung field, and then the cropped sub-volumes are augmented by random horizontal and vertical flipping with a probability of 0.5. In the testing phase, we perform the sliding window average prediction with strides of (64, 64, 32) to cover the entire CT images. For a fair comparison, we use the same hyper-parameter settings and Dice similarity coefficient loss across all experiments. In particular, we use the same data augmentation, no post-processing scheme, Adam optimizer with an initial learning rate of 10 -4 , and train for 800 epochs with a batch size of 4. In our experiments, we use a two-step optimization strategy: 1) first, train the ACE module with the basic U-Net; 2) Integrate the trained ACE module and a new CSNB module into the U-Net, and fix the parameters of ACE module when training this network.</p><p>Ablation Study. We conduct ablation studies to validate the efficacy of the proposed modules in our HENet by combining them with the baseline U-Net <ref type="bibr" target="#b10">[11]</ref>. The quantitative results are summarized in Table <ref type="table" target="#tab_0">1</ref>. Compared to the baseline, both ACE module and CSNB lead to better performance. With the two components, our HENet has significant improvements over baseline on all the metrics. For regional measures Dice and mIoU, it improves by 3.02% and 2.32% respectively. For surface-aware measures ASD and HD, it improves by 35% and 53%, respectively. Results demonstrate effectiveness of the proposed ACE module and CSNB.</p><p>To validate the efficacy of ACE module, we show the qualitative result in Fig. <ref type="figure" target="#fig_2">3</ref>. As shown in Fig. <ref type="figure" target="#fig_2">3</ref>(c), the ACE module effectively enhances the contrast of pulmonary vessels at the image-level. We also visualize the summation of feature maps from the final decoder in Fig. <ref type="figure" target="#fig_2">3(d</ref>) and (e). As can be seen, the baseline U-Net can focus on local features for certain intrapulmonary vessels, but it fails to activate complete vascular regions of multiple scales. Comparison with State-of-the-Art Methods. Since we adopt U-Net as our baseline, we compare our method with several state-of-the-art encoder-decoder CNNs and the transformer-based method VT-UNet <ref type="bibr" target="#b7">[8]</ref> within a considerable computational complexity. We also compare our method with state-of-the-art deep learning-based vessel segmentation methods, including clDice <ref type="bibr" target="#b11">[12]</ref>, CS 2 -Net <ref type="bibr" target="#b6">[7]</ref>, and OF-Net <ref type="bibr" target="#b0">[1]</ref>. The quantitative and qualitative results are presented in Table <ref type="table" target="#tab_1">2</ref> and Fig. <ref type="figure" target="#fig_3">4</ref>, respectively. As shown in Table <ref type="table" target="#tab_1">2</ref>, our method outperforms the competing methods and achieves the best Dice, mIoU, ASD, and HD. CS 2 -Net performs best on FPR,  but our method has better Dice and mIoU than CS 2 -Net (increasing 0.56% and 0.43%, respectively), indicating the under-segmentation of CS 2 -Net and more vessels being correctly segmented by our method. In the first row of qualitative results (Fig. <ref type="figure" target="#fig_3">4</ref>), the competing methods can produce satisfactory results for the overall structure but generate many false positives. Furthermore, due to low contrast between small intrapulmonary vessels and the surrounding voxels, results of competing methods exist many discontinuities (the second row), while our method obtains more connective segmentation for these small vessels. Also, for the segmentation of large extrapulmonary vessels (the last row), our method can produce more accurate results. Note that, although clDice can also yield connective results for small vessels, their FPR is 0.16% higher than ours. This implies that clDice tends to over-segment vessels, and it cannot obtain precise segmentation for the large extrapulmonary vessels. Results proved the superiority of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we have proposed a hierarchical enhancement network to enhance the representation of vessels at both image-and feature-level for pulmonary vessel segmentation first time in non-contrast CT images. In the proposed HENet, an Auto Contrast Enhancement module is designed to enhance vessels in different regions of the input CT. And the Cross-Scale Non-local Block is further designed as the information bridge between encoders and decoders, to enhance the ability to capture and integrate vascular features of multiple scales. Experimental results show that our method outperforms the competing methods and demonstrates effectiveness of the proposed ACE module and CSNB. At the same time, it can be observed that the learning of M shif t and M scale is only supervised by the final segmentation loss. One of our future research directions is to develop explicit constraints for the ACE module to better re-center and re-scale the CT images.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The challenges of accurate pulmonary vessel segmentation. (a-b) The central extrapulmonary vessels (pointed by blue arrows) are large compared to tubular-like intrapulmonary vessels (pointed by green arrows), which become thinner as they get closer to the peripheral lung. (c) Hard to distinguish vessels in non-contrast CT images. (d-e) HU values of vessels in different regions vary significantly. (Color figure online)</figDesc><graphic coords="3,70,98,53,72,310,84,55,00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Overview of the proposed HENet. It consists of two components: (1) Auto Contrast Enhancement module (bottom). (2) The Asymmetric Non-local Blocks in Cross-Scale Non-local Block (top-right corner).</figDesc><graphic coords="4,47,31,54,53,329,68,267,16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The effectiveness of the proposed components, with the images in red circles generated from our method. (a-c) The contrast of vessels is significantly enhanced in the CT images processed by ACE module. (d-e) Compared to baseline, CSNB can enhance the ability to capture vascular features with widely variable size, shape, and location. (Color figure online)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Qualitative segmentation results. The blue arrows are used to highlight the regions for visual presentation. (Color figure online)</figDesc><graphic coords="8,71,31,196,52,282,04,156,16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Quantitative results of ablation study. * denotes significant improvement compared to the baseline U-Net (p&lt;0.05). Net+ACE 84.71 ± 2.69 * 86.40 ± 2.09 * 0.88 ± 0.46 * 10.80 ± 11.29 * 0.33 ± 0.15 U-Net+CSNB 85.22 ± 2.79 * 86.80 ± 2.18 * 0.85 ± 0.43 * 14.00 ± 16.64 0.30 ± 0.14</figDesc><table><row><cell>Method</cell><cell>Dice [%]↑</cell><cell>mIoU [%]↑</cell><cell cols="2">ASD [mm]↓ HD [mm]↓</cell><cell>FPR [%]↓</cell></row><row><cell>U-Net</cell><cell>82.88 ± 3.44</cell><cell>85.02 ± 2.56</cell><cell>1.11 ± 0.56</cell><cell cols="2">20.16 ± 21.78 0.35 ± 0.15</cell></row><row><cell>U-Ours</cell><cell cols="5">85.90 ± 2.92  *  87.34 ± 2.29  *  0.72 ± 0.41  *  9.43 ± 10.91  *  0.27 ± 0.14  *</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Quantitative comparison with the state-of-the-art methods. ± 3.33 81.14 ± 2.27 1.96 ± 2.05 30.02 ± 31.92 0.49 ± 0.17 OF-Net [1] 82.70 ± 3.53 84.89 ± 2.65 1.45 ± 0.88 28.62 ± 31.99 0.31 ± 0.14 U-Net [11] 82.88 ± 3.44 85.02 ± 2.56 1.11 ± 0.56 20.16 ± 21.78 0.35 ± 0.15 clDice [12] 85.08 ± 2.65 86.67 ± 2.04 0.82 ± 0.41 10.38 ± 10.59 0.43 ± 0.17 ResUNet++ [4] 85.30 ± 3.21 86.88 ± 2.49 1.31 ± 1.27 23.67 ± 31.40 0.30 ± 0.15 CS 2 -Net [7] 85.34 ± 3.04 86.91 ± 2.37 0.89 ± 0.46 13.50 ± 17.13 0.</figDesc><table><row><cell>Method</cell><cell>Dice [%]↑</cell><cell>mIoU [%]↑</cell><cell>ASD [mm]↓ HD [mm]↓</cell><cell>FPR [%]↓</cell></row><row><cell>VT-UNet [8]</cell><cell>77.52</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>25 ± 0.13 Ours 85.90 ± 2.92 87.34</head><label></label><figDesc></figDesc><table /><note><p>± 2.29 0.72 ± 0.41 9.43 ± 10.91 0.27 ± 0.14</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>. This work was supported in part by <rs type="funder">National Key Research and Development Program of China</rs> (<rs type="grantNumber">2021ZD0111100</rs>), <rs type="funder">National Natural Science Foundation of China</rs> (<rs type="grantNumber">62131015</rs>), and <rs type="funder">Science and Technology Commission of Shanghai Municipality (STCSM)</rs> (<rs type="grantNumber">21010502600</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_RhtCdhT">
					<idno type="grant-number">2021ZD0111100</idno>
				</org>
				<org type="funding" xml:id="_sHpqkHG">
					<idno type="grant-number">62131015</idno>
				</org>
				<org type="funding" xml:id="_rgDUNZD">
					<idno type="grant-number">21010502600</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pulmonary vessel segmentation based on orthogonal fused U-Net++ of chest CT images</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32226-7_33</idno>
		<idno>978-3-030-32226-7 33</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11769</biblScope>
			<biblScope unit="page" from="293" to="300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Toxicity of MRI and CT contrast agents</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Hasebroock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Serkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Opin. Drug Metab. Toxicol</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="403" to="416" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">ScaleFormer: revisiting the transformer-based backbones from a scale-wise perspective for medical image segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.14552</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">ResUNet++: an advanced architecture for medical image segmentation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Multimedia (ISM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="225" to="2255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fuzzy pulmonary vessel segmentation in contrast enhanced CT data</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Kaftan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Kiraly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bakai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Processing</title>
		<imprint>
			<biblScope unit="volume">6914</biblScope>
			<biblScope unit="page" from="585" to="596" />
			<date type="published" when="2008">2008. 2008</date>
			<publisher>SPIE</publisher>
		</imprint>
	</monogr>
	<note>Medical Imaging</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">V-net: fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Ahmadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="565" to="571" />
		</imprint>
	</monogr>
	<note>fourth international conference on 3D vision (3DV</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">CS 2 -net: deep learning segmentation of curvilinear structures in medical imaging</title>
		<author>
			<persName><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page">101874</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A robust volumetric transformer for accurate 3D tumor segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Peiris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Egan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Harandi</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16443-9_16</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16443-916" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13435</biblScope>
			<biblScope unit="page" from="162" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Automated identification of pulmonary arteries and veins depicted in non-contrast chest CT scans</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page">102367</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning bronchiole-sensitive airway segmentation CNNs by feature recalibration and attention distillation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-M</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59710-8_22</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59710-822" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12261</biblScope>
			<biblScope unit="page" from="221" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">U-net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-428" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">clDice-a novel topology-preserving loss function for tubular structure segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="16560" to="16569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">UCTransNet: rethinking the skip connections in U-Net from a channel-wise perspective with transformer</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">R</forename><surname>Zaiane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="2441" to="2449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Progressive deep segmentation of coronary artery via hierarchical topology learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16443-9_38</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16443-938" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13435</biblScope>
			<biblScope unit="page" from="391" to="400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Automatic multiscale enhancement and segmentation of pulmonary vessels in CT pulmonary angiography images for cad applications</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Phys</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4567" to="4577" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">UNet++: a nested U-net architecture for medical image segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Rahman Siddiquee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-00889-5_1</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-00889-51" />
	</analytic>
	<monogr>
		<title level="m">DLMIA/ML-CDS -2018</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Stoyanov</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11045</biblScope>
			<biblScope unit="page" from="3" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Asymmetric non-local neural networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="593" to="602" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
