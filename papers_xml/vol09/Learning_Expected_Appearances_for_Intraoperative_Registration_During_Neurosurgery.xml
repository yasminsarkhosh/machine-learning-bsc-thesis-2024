<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Expected Appearances for Intraoperative Registration During Neurosurgery</title>
				<funder ref="#_9DuQcEN #_zZ4fQey #_SMWTAfT #_EPw9DE6">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Nazim</forename><surname>Haouchine</surname></persName>
							<email>nhaouchine@bwh.harvard.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Harvard Medical School</orgName>
								<orgName type="department" key="dep2">Brigham and Women&apos;s Hospital</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Reuben</forename><surname>Dorent</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Harvard Medical School</orgName>
								<orgName type="department" key="dep2">Brigham and Women&apos;s Hospital</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Parikshit</forename><surname>Juvekar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Harvard Medical School</orgName>
								<orgName type="department" key="dep2">Brigham and Women&apos;s Hospital</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Erickson</forename><surname>Torio</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Harvard Medical School</orgName>
								<orgName type="department" key="dep2">Brigham and Women&apos;s Hospital</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">William</forename><forename type="middle">M</forename><surname>Wells</surname><genName>III</genName></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Harvard Medical School</orgName>
								<orgName type="department" key="dep2">Brigham and Women&apos;s Hospital</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tina</forename><surname>Kapur</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Harvard Medical School</orgName>
								<orgName type="department" key="dep2">Brigham and Women&apos;s Hospital</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alexandra</forename><forename type="middle">J</forename><surname>Golby</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Harvard Medical School</orgName>
								<orgName type="department" key="dep2">Brigham and Women&apos;s Hospital</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sarah</forename><surname>Frisken</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Harvard Medical School</orgName>
								<orgName type="department" key="dep2">Brigham and Women&apos;s Hospital</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Expected Appearances for Intraoperative Registration During Neurosurgery</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E13C051DE6459BD56A46048CBE213940</idno>
					<idno type="DOI">10.1007/978-3-031-43996-4_22</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T12:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Intraoperative Registration</term>
					<term>Image-guided Neurosurgery</term>
					<term>Augmented Reality</term>
					<term>Neural Image Analogy</term>
					<term>3D Pose Estimation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a novel method for intraoperative patient-toimage registration by learning Expected Appearances. Our method uses preoperative imaging to synthesize patient-specific expected views through a surgical microscope for a predicted range of transformations. Our method estimates the camera pose by minimizing the dissimilarity between the intraoperative 2D view through the optical microscope and the synthesized expected texture. In contrast to conventional methods, our approach transfers the processing tasks to the preoperative stage, reducing thereby the impact of low-resolution, distorted, and noisy intraoperative images, that often degrade the registration accuracy. We applied our method in the context of neuronavigation during brain surgery. We evaluated our approach on synthetic data and on retrospective data from 6 clinical cases. Our method outperformed state-of-the-art methods and achieved accuracies that met current clinical standards.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>We address the important problem of intraoperative patient-to-image registration in a new way by relying on preoperative data to synthesize plausible transformations and appearances that are expected to be found intraoperatively. In particular, we tackle intraoperative 3D/2D registration during neurosurgery, where preoperative MRI scans need to be registered with intraoperative surgical views of the brain surface to guide neurosurgeons towards achieving a maximal safe tumor resection <ref type="bibr" target="#b22">[22]</ref>. Indeed, the extent of tumor removal is highly correlated with patients' chances of survival and complete resection must be balanced against the risk of causing new neurological deficits <ref type="bibr" target="#b4">[5]</ref> making accurate intraoperative registration a critical component of neuronavigation.</p><p>Most existing techniques perform patient-to-image registration using intraoperative MRI <ref type="bibr" target="#b10">[11]</ref>, CBCT <ref type="bibr" target="#b19">[19]</ref> or ultrasound <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b20">20]</ref>. For 3D-3D registration, 3D shape recovery of brain surfaces can be achieved using near-infrared cameras <ref type="bibr" target="#b14">[15]</ref>, phase-shift 3D shape measurement <ref type="bibr" target="#b9">[10]</ref>, pattern projections <ref type="bibr" target="#b17">[17]</ref> or stereovision <ref type="bibr" target="#b7">[8]</ref>. The 3D shape can subsequently be registered with the preoperative MRI using conventional point-to-point methods such as iterative closest point (ICP) or coherent point drift (CPD). Most of these methods rely on cortical vessels that bring salient information for such tasks. For instance, in <ref type="bibr" target="#b5">[6]</ref>, cortical vessels are first segmented using a deep neural network (DNN) and then used to constrain a 3D/2D non-rigid registration. The method uses physics-based modeling to resolve depth ambiguities. A manual rigid alignment is however required to initialize the optimization. Alternatively, cortical vessels have been used in <ref type="bibr" target="#b12">[13]</ref> where sparse 3D points, manually traced along the vessels, are matched with vessels extracted from the preoperative scans. A model-based inverse minimization problem is solved by estimating the model's parameters from a set of pre-computed transformations. The idea of pre-computing data for registration was introduced by <ref type="bibr" target="#b26">[26]</ref>, who used an atlas of pre-computed 3D shapes of the brain surface for registration. In <ref type="bibr" target="#b6">[7]</ref>, a DNN is trained on a set of pre-generated preoperative to intraoperative transformations. The registration uses cortical vessels, segmented using another neural network, to find the best transformation from the pre-generated set.</p><p>The main limitation of existing intraoperative registration methods is that they rely heavily on processing intraoperative images to extract image features (eg., 3D surfaces, vessels centerlines, contours, or other landmarks) to drive registration, making them subject to noise and low-resolution images that can occur in the operating room <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b25">25]</ref>. Outside of neurosurgery, the concept of pregenerating data for optimizing DNNs for intraoperative registration has been investigated for CT to x-ray registration in radiotherapy where x-ray images can be efficiently simulated from CTs as digital radiographic reconstructions <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b27">27]</ref>. In more general applications, case-centered training of DNNs is gaining in popularity and demonstrates remarkable results <ref type="bibr" target="#b16">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contribution:</head><p>We propose a novel approach for patient-to-image registration that registers the intraoperative 2D view through the surgical microscope to preoperative MRI 3D images by learning Expected Appearances. As shown in Fig. <ref type="figure">1</ref>, we formulate the problem as a camera pose estimation problem that finds the optimal 3D pose minimizing the dissimilarity between the intraoperative 2D image and its pre-generated Expected Appearance. A set of Expected Appearances are synthesized from the preoperative scan and for a set of poses covering the range of plausible 6 Degrees-of-Freedom (DoF) transformations. This set is used to train a patient-specific pose regressor network to obtain a model that is texture-invariant and is cross-modality to bridge the MRI and RGB camera modalities. Similar to other methods, our approach follows a monocular singleshot registration, eliminating cumbersome and tedious calibration of stereo cameras, the laser range finder, or optical trackers. In contrast to previous methods, our approach does not involve processing intraoperative images which have several advantages: it is less prone to intraoperative image acquisition noise; it does not require pose initialization; and is computationally fast thus supporting real-time use. We present results on both synthetic and clinical data and show that our approach outperformed state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Expected Appearances MRI Scan</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Intraoperative Registration Image Synthesis Pose Regression</head><p>Pose Sampling Fig. <ref type="figure">1</ref>. Our approach estimates the 6-DoF camera pose that aligns a preoperative 3D mesh derived from MRI scans onto an intraoperative RGB image acquired from a surgical camera. We optimize a regressor network PΩ over a set of Expected Appearances that are generated by first sampling multiple poses and appearances from the 3D mesh using neural image analogy through SΘ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Problem Formulation</head><p>As illustrated in Fig. <ref type="figure">1</ref>, given a 3D surface mesh of the cortical vessels M, derived from a 3D preoperative scan, and a 2D monocular single-shot image of the brain surface I, acquired intraoperatively by a surgical camera, we seek to estimate the 6-DoF transformation that aligns the mesh M to the image I. Assuming a set of 3D points u = {u j ∈ R 3 } ⊂ M and a set of 2D points in the image v = {v i ∈ R 2 } ⊂ I, solving for this registration problem can be formalized as finding the 6-DoF camera pose that minimizes the reprojection error:</p><formula xml:id="formula_0">nc i A[R|t] u w c i 1 -v i 2 2 ,</formula><p>where R ∈ SO(3) and t ∈ R 3 represent a 3D rotation and 3D translation, respectively, and A is the camera intrinsic matrix composed of the focal length and the principal points (center of the image) while {c i } i is a correspondence map and is built so that if a 2D point v i corresponds to a 3D point u j where c i = j for each point of the two sets. Note that the set of 3D points u is expressed in homogenous coordinates in the minimization of the reprojection error.</p><p>In practice, finding the correspondences set {c i } i between u and v is nontrivial, in particular when dealing with heterogeneous preoperative and intraoperative modality pairs (MRI, RGB Cameras, ultrasound, etc.) which is often the case in surgical guidance. Existing methods often rely on feature descriptors <ref type="bibr" target="#b13">[14]</ref>, anatomical landmarks <ref type="bibr" target="#b12">[13]</ref>, or organ's contours and segmentation <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b18">18]</ref> involving tedious processing of the intraoperative image that is sensitive to the computational image noise. We alleviate these issues by directly minimizing the dissimilarity between the image I and its Expected Appearance synthesized from M.</p><p>By defining a synthesize function S Θ that synthesizes a new image I given a projection of a 3D surface mesh for different camera poses, i.e. I = S Θ (A[R|t], M), the optimization problem above can be rewritten as:</p><p>argmin</p><formula xml:id="formula_1">A[R|t] min Θ I -S Θ A[R|t], M<label>(1)</label></formula><p>This new formulation is correspondence-free, meaning that it alleviates the requirement of the explicit matching between u and v. This is one of the major strengths of our approach. It avoids the processing of I at run-time, which is the main source of registration error. In addition, our method is patient-specific, centered around M, since each model is trained specifically for a given patient. These two aspects allow us to transfer the computational cost from the intraoperative to the preoperative stage thereby optimizing intraoperative performance.</p><p>The following describes how we build the function S Θ and how to solve Eq. 1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Expected Appearances Synthesis</head><p>We define a synthesis network S Θ : (A[R|t], M, T) → I, that will generate a new image resembling a view of the brain surface from the 2D projection of the input mesh M following [R|t], and a texture T. Several methods can be used to optimize Θ. However, they require a large set of annotated data <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b24">24]</ref> or perform only on modalities with similar sensors <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b27">27]</ref>. Generating RGB images from MRI scans is a challenging task because it requires bridging a significant difference in image modalities. We choose to use a neural image analogy method that combines the texture of a source image with a high-level content representation of a target image without the need for a large dataset <ref type="bibr" target="#b0">[1]</ref>. This approach transfers the texture from T to I constrained by the projection of M using A[R|t] by minimizing the following loss function:</p><formula xml:id="formula_2">L Θ = l ij w l,c T class G l ij (T) -w l,c A[R|t],M G l ij ( I) for c ∈ {0, 1, 2}<label>(2)</label></formula><p>where G l ij (T ) is the Gram matrix of texture T at the l-th convolutional layer (pre-trained VGG-19 model), and w l,c T class are the normalization factors for each Gram matrix, normalized by the number of pixels in a label class c of T class . This allows for the quantification of the differences between the texture image T and the generated image I as it is being generated. Importantly, computing the inner-most sum over each label class c allows for texture comparison within each class, for instance: the background, the parenchyma, and the cortical vessels.</p><p>In practice, we assume constant camera parameters A and first sample a set of binary images by randomly varying the location and orientation of a virtual camera [R|t] w.r.t. to the 3D mesh M before populating the binary images with the textures using S Θ (see Fig. <ref type="figure" target="#fig_0">2</ref>). We restrict this sampling to the upper hemisphere of the 3D mesh to remain consistent with the plausible camera positions w.r.t. patient's head during neurosurgery.</p><p>We use the L-BFGS optimizer and 5 convolutional layers of VGG-19 to generate each image following <ref type="bibr" target="#b0">[1]</ref> to find the resulting parameters Θ. The training to synthesize for a single image typically takes around 50 iterations to converge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Pose Regression Network</head><p>In order to solve Eq. 1, we assume a known focal length that can be obtained through pre-calibration. To obtain a compact representation of the rotation and since poses are restricted to the upper hemisphere of the 3D mesh (No Gimbal lock), the Euler-Rodrigues representation is used. Therefore, there are six parameters to be estimated: rotations r x , r y , r z and translations t x , t y , t z . We estimate our 6-DoF pose with a regression network P Ω : I → p and optimize its weights Ω to map each synthetic image I to its corresponding camera pose p = [r x , r y , r z , t x , t y , t z ] T .</p><p>The network architecture of P Ω consists of 3 blocks each composed of two convolutional layers and one ReLU activation. To decrease the spatial dimension, an average pooling layer with a stride of 2 follows each block except the last one. At the end of the last hierarchy, we add three fully-connected layers with 128, 64, and 32 neurons and ReLU activation followed by one fully-connected with 6 neurons with a linear activation. We use the set of generated Expected Appearances T P = {(I i ; p i )} i ; and optimize the following loss function over the parameters Ω of the network P Ω :</p><formula xml:id="formula_3">L Ω = |T P | i=1 t i -t i 2 + R vec i -R vec i 2<label>(3)</label></formula><p>where t and R vec are the translation and rotation vector, respectively. We experimentally noticed that optimizing these entities separately leads to better results. The model is trained for each case (patient) for 200 epochs using mini-batches of size 8 with Adam optimizer and a learning rate of 0.001 and decays exponentially to 0.0001 over the course of the optimization. Finally, at run-time, given an image I we directly predict the corresponding 3D pose p so that: p ← P(I; Ω), where Ω is the resulting parameters from the training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>Dataset. We tested our method retrospectively on 6 clinical datasets from 6 patients (cases) (see Fig. <ref type="figure" target="#fig_4">5</ref>). These consisted of preoperative T1 contrast MRI scans and intraoperative images of the brain surface after dura opening. Cortical vessels around the tumors were segmented and triangulated to generate 3D meshes using 3D Slicer. We generated 100 poses for each 3D mesh (i.e.: each case) and used a total of 15 unique textures from human brain surfaces (different from our 6 clinical datasets) for synthesis using S Θ . In order to account for potential intraoperative brain deformations <ref type="bibr" target="#b3">[4]</ref> we augment the textured projection with elastic deformation <ref type="bibr" target="#b21">[21]</ref> resulting in approximately 1500 images per case. The surgical images of the brain (left image of the stereoscopic camera) were acquired with a Carl Zeiss surgical microscope. The ground-truth poses were obtained by manually aligning the 3D meshes on their corresponding images. We evaluated the pose regressor network on both synthetic and real data. The model training and validation were performed on the synthesized images while the model testing was performed on the real images. Because a conventional train/validation/test split would lead to texture contamination, we created our validation dataset so that at least one texture is excluded from the training set. On the other hand, the test set consisted of the real images of the brain surface acquired using the surgical camera and are never used in the training. Accuracy-threshold curves on the validation set.</p><p>Metrics. We chose the average distance metric (ADD) as proposed in <ref type="bibr" target="#b23">[23]</ref> for evaluation. Given a set of mesh's 3D vertices, the ADD computes the mean of the pairwise distance between the 3D model points transformed using the ground truth and estimated transformation. We also adjusted the default 5 cm-5 deg translation and rotation error to our neurosurgical application and set the new threshold to 3 mm-3 deg.</p><p>Accuracy-Threshold Curves. We calculated the number of 'correct' poses estimated by our model. We varied the distance threshold on the validation sets (excluding 2 textures) in order to reveal how the model performs w.r.t. that threshold. We plotted accuracy-threshold curves showing the percentage of pose accuracy variation with a threshold in a range of 0 mm to 20 mm. We can see in Fig. <ref type="figure" target="#fig_1">3</ref> that a 80.23% pose accuracy was reached within the 3 mm-3 deg threshold for all cases. This accuracy increases to 95.45% with a 5 mm-5 deg threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Validation and Evaluation of Texture Invariance.</head><p>We chose to follow a Leave-one-Texture-out cross-validation strategy to validate our model. This strategy seemed the most adequate to prevent over-fitting on the textures. We measured the ADD errors of our model for each case and report the results in  We observed a variance in the ADD error that depends on which texture is left out. This supports the need for varying textures to improve the pose estimation. However, the errors remain low, with a 2.01 ± 0.58 mm average ADD error, over all cases. The average ADD error per case (over all left-out textures) is reported in Table <ref type="table" target="#tab_0">1</ref>. We measured the impact of the number of textures on the pose accuracy by progressively adding new textures to the training set, starting from 3 to 12 textures, while leaving 3 textures out for validation. We kept the size of the training set constant to not introduce size biases.  shows that increasing the number and variation of textures improved model performances. Test and Comparison on Clinical Images. We compared our method (Ours) with segmentation-based methods (ProbSEG) and (BinSEG) <ref type="bibr" target="#b6">[7]</ref>. These methods use learning-based models to extract binary images and probability maps of cortical vessels to drive the registration. We report in Table <ref type="table" target="#tab_0">1</ref> the distances between the ground truth and estimated poses. Our method outperformed ProbSEG and BinSEG with an average ADM error of 3.26 ± 1.04 mm compared to 4.13 ± 0.70 mm and 8.67 ± 2.84 mm, respectively. Our errors remain below clinically measured neuronavigation errors reported in <ref type="bibr" target="#b3">[4]</ref>, in which a 5.26 ± 0.75 mm average initial registration error was measured in 15 craniotomy cases using intraoperative ultrasound. Our method outperformed ProbSEG in 5 cases out of 6 and BinSEG in all cases and remained within the clinically measured errors without the need to segment cortical vessels or select landmarks from the intraoperative image. Our method also showed fast intraoperative computation times. It required an average of only 45 ms to predict the pose (tested on research code on a laptop with NVidia GeForce GTX 1070 8 GB without any specific optimization), suggesting a potential use for real-time temporal tracking.</p><p>Figure <ref type="figure" target="#fig_4">5</ref> shows our results as Augmented Reality views with bounding boxes and overlaid meshes. Our method produced visually consistent alignments for all 6 clinical cases without the need for initial registration. Because our current method does not account for brain-shift deformation, our method produced some misalignment errors. However, in all cases, our predictions are similar to the ground truth. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion and Conclusion</head><p>Clinical Feasibility. We have shown that our method is clinically viable. Our experiments using clinical data showed that our method provides accurate registration without manual intervention, that it is computationally efficient, and it is invariant to the visual appearance of the cortex. Our method does not require intraoperative 3D imaging such as intraoperative MRI or ultrasound, which require expensive equipment and are disruptive during surgery. Training patient-specific models from preoperative imaging transfers computational tasks to the preoperative stage so that patient-to-image registration can be performed in near real-time from live images acquired from a surgical microscope.</p><p>Limitations. The method presented in this paper is limited to 6-DoF pose estimation and does not account for deformation of the brain due to changes in head position, fluid loss, or tumor resection and assumes a known focal length. In the future, we will expand our method to model non-rigid deformations of the 3D mesh and to accommodate expected changes in zoom and focal depth during surgery. We will also explore how texture variability can be controlled and adapted to the observed image to improve model accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion.</head><p>We introduced Expected Appearances, a novel learning-based method for intraoperative patient-to-image registration that uses synthesized expected images of the operative field to register preoperative scans with intraoperative views through the surgical microscope. We demonstrated state-ofthe-art, real-time performance on challenging neurosurgical images using our method. Our method could be used to improve accuracy in neuronavigation and in image-guided surgery in general.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. An example of a set of Expected Appearances showing the cortical brain surface with the parenchyma and vessels. The network SΘ uses the binary image (top-left corner) computed from projecting M using [R|t] to semantically transfer 15 different textures {T} and synthesize the Expected Appearances { I}.</figDesc><graphic coords="4,82,29,250,58,259,51,65,11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3.Accuracy-threshold curves on the validation set.</figDesc><graphic coords="6,264,18,342,98,112,24,107,80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Evaluation of texture invariance: (left) Leave-one-Texture-out cross validation and (right) impact of the number of textures on model accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 -</head><label>4</label><figDesc>Fig.4-(left) for each left-out texture. We observed a variance in the ADD error that depends on which texture is left out. This supports the need for varying textures to improve the pose estimation. However, the errors remain low, with a 2.01 ± 0.58 mm average ADD error, over all cases. The average ADD error per case (over all left-out textures) is reported in Table1. We measured the impact of the number of textures on the pose accuracy by progressively adding new textures to the training set, starting from 3 to 12 textures, while leaving 3 textures out for validation. We kept the size of the training set constant to not introduce size biases. shows that increasing the number and variation of textures improved model performances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Qualitative results on 6 patient datasets retrospectively showed in the first row.The second row shows Augmented Reality views with our predicted poses. The third row highlights 3D mesh-to-image projections using the ground-truth poses (green), and our predicted poses (blue). Our predictions are close to the ground truth for all cases. Note: microscope-magnified images with visible brain surface diameter ≈35 mm. (Color figure online)</figDesc><graphic coords="8,42,30,219,80,339,40,169,00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Validation on synthetic data and comparisons using real data.</figDesc><table><row><cell>Data</cell><cell>Experiment</cell><cell>Metric</cell><cell cols="6">Case 1 Case 2 Case 3 Case 4 Case 5 Case 6</cell></row><row><cell>Synth.</cell><cell cols="8">LOTO CV Acc./Thresh. 3mm-3deg (%) Avg. ADD (mm) 1.80 89.33 96.66 98.07 81.81 93.33 80.23 1.56 1.27 2.83 2.58 2.07</cell></row><row><cell></cell><cell>Ours</cell><cell>ADD (mm)</cell><cell cols="3">3.64 2.98 1.62</cell><cell>4.83</cell><cell cols="2">3.02 3.32</cell></row><row><cell>Real</cell><cell>ProbSEG</cell><cell>ADD (mm)</cell><cell>4.49</cell><cell>3.82</cell><cell>3.12</cell><cell>4.69</cell><cell>4.99</cell><cell>3.67</cell></row><row><cell></cell><cell>BinSEG</cell><cell>ADD (mm)</cell><cell>9.2</cell><cell cols="5">4.87 12.12 8.09 11.43 6.29</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. The authors were partially supported by the following National Institutes of Health grants: <rs type="grantNumber">R01EB027134</rs>, <rs type="grantNumber">R03EB032050</rs>, <rs type="grantNumber">R01EB032387</rs>, and <rs type="grantNumber">R01EB034223</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_9DuQcEN">
					<idno type="grant-number">R01EB027134</idno>
				</org>
				<org type="funding" xml:id="_zZ4fQey">
					<idno type="grant-number">R03EB032050</idno>
				</org>
				<org type="funding" xml:id="_SMWTAfT">
					<idno type="grant-number">R01EB032387</idno>
				</org>
				<org type="funding" xml:id="_EPw9DE6">
					<idno type="grant-number">R01EB034223</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Texture networks: feed-forward synthesis of textures and stylized images</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ulyanov</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning Research</title>
		<meeting>Machine Learning Research</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="1349" to="1357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Surgical data science -from concepts toward clinical translation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Maier-Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="102" to="306" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Can segmentation models be trained with fully synthetically generated data?</title>
		<author>
			<persName><forename type="first">V</forename><surname>Fernandez</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16980-9_8</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16980-98" />
	</analytic>
	<monogr>
		<title level="m">SASHIMI 2022</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Zhao</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Svoboda</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Wolterink</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Escobar</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13570</biblScope>
			<biblScope unit="page" from="79" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A comparison of thin-plate spline deformation and finite element modeling to compensate for brain shift during tumor resection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Frisken</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11548-019-02057-2</idno>
		<ptr target="https://doi.org/10.1007/s11548-019-02057-2" />
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Assist. Radiol. Surg</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="75" to="85" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">State of the art&apos; of the craniotomy in the early twentyfirst century and future development</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>González-Darder</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-22212-3_34</idno>
		<idno>978-3-030-22212-3 34</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">Trepanation, Trephining and Craniotomy</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>González-Darder</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="421" to="427" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deformation aware augmented reality for craniotomy using 3D/2D non-rigid registration of cortical vessels</title>
		<author>
			<persName><forename type="first">N</forename><surname>Haouchine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Juvekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename><surname>Wells</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Cotin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Golby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Frisken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59719-1_71</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59719-171" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12264</biblScope>
			<biblScope unit="page" from="735" to="744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pose estimation and non-rigid registration for augmented reality during neurosurgery</title>
		<author>
			<persName><forename type="first">N</forename><surname>Haouchine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Juvekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nercessian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename><surname>Wells</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Golby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Frisken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1310" to="1317" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cortical surface shift estimation using stereovision and optical flow motion tracking via projection image registration</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hartov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Paulsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1169" to="1183" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mutual-informationbased image to patient re-registration using intraoperative ultrasound in imageguided neurosurgery</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hartov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Paulsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Phys</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4612" to="4624" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Marker-less tracking of brain surface deformations by non-rigid registration integrating surface and vessel/sulci features</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11548-016-1358-7</idno>
		<ptr target="https://doi.org/10.1007/s11548-016-1358-7" />
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Assist. Radiol. Surg</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1687" to="1701" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Brain shift compensation and neurosurgical image fusion using intraoperative MRI: current status and future challenges</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kuhnt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H A</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Nimsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Crit. Rev. Trade Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="175" to="185" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">CNN-based real-time 2D-3D deformable registration from a single X-ray projection</title>
		<author>
			<persName><forename type="first">F</forename><surname>Lecomte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Dillenseger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cotin</surname></persName>
		</author>
		<idno>CoRR abs/2003.08934</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An integrated multiphysics finite element modeling framework for deep brain stimulation: preliminary study on impact of brain shift on neuronal pathways</title>
		<author>
			<persName><forename type="first">M</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Konrad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Miga</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32254-0_76</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32254-076" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11768</biblScope>
			<biblScope unit="page" from="682" to="690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Non-rigid registration of 3d ultrasound for neurosurgery using automatic feature detection and matching</title>
		<author>
			<persName><forename type="first">I</forename><surname>Machado</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11548-018-1786-7</idno>
		<ptr target="https://doi.org/10.1007/s11548-018-1786-7" />
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Assist. Radiol. Surg</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1525" to="1538" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Non-rigid deformation pipeline for compensation of superficial brain shift</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M M</forename><surname>Marreiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rossitti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ö</forename><surname>Smedby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2013</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Mori</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Sakuma</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Sato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Barillot</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">8150</biblScope>
			<biblScope unit="page" from="141" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-40763-5_18</idno>
		<ptr target="https://doi.org/10.1007/978-3-642-40763-518" />
		<imprint>
			<date type="published" when="2013">2013</date>
			<pubPlace>Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">NeRF: representing scenes as neural radiance fields for view synthesis</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="99" to="106" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Estimation of intraoperative brain shift by combination of stereovision and doppler ultrasound: phantom and animal model study</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mohammadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ahmadian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Sheykh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Amiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Alirezaie</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11548-015-1216-z</idno>
		<ptr target="https://doi.org/10.1007/s11548-015-1216-z" />
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Assist. Radiol. Surg</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1753" to="1764" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep cortical vessel segmentation driven by data augmentation with neural image analogy</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nercessian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Haouchine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Juvekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Frisken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Golby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="721" to="724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Volumetric measurements of brain shift using intraoperative cone-beam computed tomography: preliminary study</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Oper. Neurosurg</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="13" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deformable registration of preoperative MR, pre-resection ultrasound, and post-resection ultrasound images of neurosurgery</title>
		<author>
			<persName><forename type="first">H</forename><surname>Rivaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Collins</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11548-014-1099-4</idno>
		<ptr target="https://doi.org/10.1007/s11548-014-1099-4" />
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Assist. Radiol. Surg</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1017" to="1028" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-428" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An extent of resection threshold for newly diagnosed glioblastomas: clinical article</title>
		<author>
			<persName><forename type="first">N</forename><surname>Sanai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Polley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Parsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Berger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Neurosurg. JNS</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="8" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Scene coordinate regression forests for camera relocalization in RGB-D images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2930" to="2937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">GANs for medical image synthesis: an empirical study</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Skandarani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Jodoin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lalande</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Imaging</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">69</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Surgical vision</title>
		<author>
			<persName><forename type="first">D</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10439-011-0441-z</idno>
		<ptr target="https://doi.org/10.1007/s10439-011-0441-z" />
	</analytic>
	<monogr>
		<title level="j">Ann. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="332" to="345" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Near realtime computer assisted surgery for brain shift correction using biomechanical models</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pheiffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Miga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transl. Eng. Health Med</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">LiftReg: limited angle 2D/3D deformable registration</title>
		<author>
			<persName><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Z</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>San José Estépar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Niethammer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16446-0_20</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16446-020" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13436</biblScope>
			<biblScope unit="page" from="207" to="216" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
