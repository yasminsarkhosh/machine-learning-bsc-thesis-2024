<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Regressing Simulation to Real: Unsupervised Domain Adaptation for Automated Quality Assessment in Transoesophageal Echocardiography</title>
				<funder ref="#_T5qe6xr">
					<orgName type="full">EPSRC</orgName>
				</funder>
				<funder ref="#_xzKRdAC #_AT56kRT">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_FcGFfnp">
					<orgName type="full">Horizon</orgName>
				</funder>
				<funder ref="#_RtNznCA #_rX7VEMv">
					<orgName type="full">Wellcome/EPSRC Centre for Interventional and Surgical Sciences</orgName>
					<orgName type="abbreviated">WEISS</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jialang</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Medical Physics and Biomedical Engineering</orgName>
								<orgName type="institution">UCL Wellcome/EPSRC Centre for Interventional and Surgical Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yueming</forename><surname>Jin</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Department of Biomedical Engineering and Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<settlement>Singapore</settlement>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bruce</forename><surname>Martin</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">St Bartholomew&apos;s Hospital</orgName>
								<orgName type="institution" key="instit2">NHS Foundation Trust</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Andrew</forename><surname>Smith</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">St Bartholomew&apos;s Hospital</orgName>
								<orgName type="institution" key="instit2">NHS Foundation Trust</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Susan</forename><surname>Wright</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">St George&apos;s University Hospitals</orgName>
								<orgName type="institution" key="instit2">NHS Foundation Trust</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Danail</forename><surname>Stoyanov</surname></persName>
							<idno type="ORCID">0000-0002-0980-3227</idno>
							<affiliation key="aff0">
								<orgName type="department">Department of Medical Physics and Biomedical Engineering</orgName>
								<orgName type="institution">UCL Wellcome/EPSRC Centre for Interventional and Surgical Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Evangelos</forename><forename type="middle">B</forename><surname>Mazomenos</surname></persName>
							<email>e.mazomenos@ucl.ac.uk</email>
							<idno type="ORCID">0000-0003-0357-5996</idno>
							<affiliation key="aff0">
								<orgName type="department">Department of Medical Physics and Biomedical Engineering</orgName>
								<orgName type="institution">UCL Wellcome/EPSRC Centre for Interventional and Surgical Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Regressing Simulation to Real: Unsupervised Domain Adaptation for Automated Quality Assessment in Transoesophageal Echocardiography</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0C5D47CAA8F7A905843A3D30D43368C4</idno>
					<idno type="DOI">10.1007/978-3-031-43996-415.</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T12:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Automated quality assessment</term>
					<term>Unsupervised domain adaptation regression</term>
					<term>Transoesophageal echocardiography</term>
					<term>Uncertainty</term>
					<term>Consistency learning</term>
					<term>Style transfer</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automated quality assessment (AQA) in transoesophageal echocardiography (TEE) contributes to accurate diagnosis and echocardiographers' training, providing direct feedback for the development of dexterous skills. However, prior works only perform AQA on simulated TEE data due to the scarcity of real data, which lacks applicability in the real world. Considering the cost and limitations of collecting TEE data from real cases, exploiting the readily available simulated data for AQA in real-world TEE is desired. In this paper, we construct the first simulation-to-real TEE dataset, and propose a novel Simulation-to-Real network (SR-AQA) with unsupervised domain adaptation for this problem. It is based on uncertainty-aware feature stylization (UFS), incorporating style consistency learning (SCL) and task-specific learning (TL), to achieve high generalizability. Concretely, UFS estimates the uncertainty of feature statistics in the real domain and diversifies simulated images with style variants extracted from the real images, alleviating the domain gap. We enforce SCL and TL across different real-stylized variants to learn domain-invariant and task-specific representations. Experimental results demonstrate that our SR-AQA outperforms state-of-theart methods with 3.02% and 4.37% performance gain in two AQA regression tasks, by using only 10% unlabelled real data. Our code and dataset are available at https://doi.org/10.5522/04/23699736.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Transoesophageal echocardiography (TEE) is a valuable diagnostic and monitoring imaging modality with widespread use in cardiovascular surgery for anaesthesia management and outcome assessment, as well as in emergency and intensive care medicine. The quality of TEE views is important for diagnosis and professional organisations publish guidelines for performing TEE exams <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b21">22]</ref>. These guidelines standardise TEE view acquisition and set benchmarks for the education of new echocardiographers. Computational methods for automated quality assessment (AQA) will have great impact, guaranteeing quality of examinations and facilitating training of new TEE operators. Deep models for AQA have shown promise in transthoracic echocardiography (TTE) and other ultrasound (US) applications <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref>. Investigation of such methods in the real TEE domain remains underexplored and has been restricted to simulated datasets from Virtual Reality (VR) systems <ref type="bibr" target="#b14">[15]</ref>. Although VR technology is useful for developing and retaining US scanning skills <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">20]</ref>, AQA methods developed on simulation settings cannot meet real-world usability without addressing the significant domain gap. As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, there are significant content differences between simulated and real TOE images. Simulated images are free of speckle noise and contain only the heart muscle, ignoring tissue in the periphery of the heart. In this work, we take the first step in exploring AQA in the real TEE domain. We propose to leverage readily accessible simulated data, and transfer knowledge learned in the simulated domain, to boost performance in real TEE space.</p><p>To alleviate the domain mismatch, a feasible solution is unsupervised domain adaptation (UDA). UDA aims to increase the performance on the target domain by using labelled source data with unlabelled target data to reduce the domain shift. For example, Mixstyle <ref type="bibr" target="#b23">[24]</ref>, performs style regularization by mixing instance-level feature statistics of training samples. The most relevant UDA work for our AQA regression task is representation subspace distance (RSD) <ref type="bibr" target="#b3">[4]</ref>, which aligns features from simulated and real domains via representation sub-spaces. Despite its effectiveness in several tasks, performing UDA on the simulation-to-real AQA task of TEE has two key challenges that need to be addressed. From Fig. <ref type="figure" target="#fig_0">1</ref>, it is evident that: 1) there are many unknown intradomain shifts in real TEE images due to different scanning views and complex heart anatomy, which requires uncertainty estimation; 2) the inter-domain gap (heart appearance, style, and resolution) between simulated and real data is considerable, necessitating robust, domain-invariant features.</p><p>In this paper, we propose a novel UDA regression network named SR-AQA that performs style alignment between TEE simulated and real domains while retaining domain-invariant and task-specific information to achieve promising AQA performance. To estimate the uncertainty of intra-domain style offsets in real data, we employ uncertainty-aware feature stylization (UFS) utilizing multivariate Gaussians to regenerate feature statistics (i.e. mean and standard deviation) of real data. To reduce the inter-domain gap, UFS augments simulated features to resemble diverse real styles and obtain real-stylized variants. We then design a style consistency learning (SCL) strategy to learn domaininvariant representations by minimizing the negative cosine similarity between simulated features and real-stylized variants in an extra feature space. Enforcing task-specific learning (TL) in real-stylized variants allows SR-AQA to keep task-specific information useful for AQA. Our work represents the original effort to address the TEE domain shift in AQA tasks. For method evaluation, we present the first simulation-to-real TEE dataset with two AQA tasks (see Fig. <ref type="figure" target="#fig_0">1</ref>), and benchmark the proposed SR-AQA model against four state-of-the-art UDA methods. Our proposed SR-AQA outperforms other UDA methods, achieving 2.13%-5.08% and 4.37%-16.28% mean squared error (MSE) reduction for two AQA tasks, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Dataset Collection</head><p>We collected a dataset of 16,192 simulated and 4,427 real TEE images from 9 standard views. From Fig. <ref type="figure" target="#fig_0">1</ref>, it is clear that significant style differences (e.g. brightness, contrast, acoustic shadowing, and refraction artifact) exist between simulated and real data, posing a considerable challenge to UDA. Simulated images were collected with the HeartWorks TEE simulation platform from 38 participants of varied experience asked to image the 9 views. Fully anonymized real TEE data were collected from 10 cardiovascular procedures in 2 hospitals, with ethics for research use and collection approved by the respective Research Ethics Committees. Each image is annotated by 3 expert anaesthetists with two independent scores w.r.t. two AQA tasks for TEE. The criteria percentage (CP) score ranging from "0-100", measuring the number of essential criteria, from the checklists (provided in supplementary material) of the ASE/SCA/BSE imaging guidelines <ref type="bibr" target="#b4">[5]</ref>, met during image acquisition and a general impression (GI) score ranging from "0-4", representing overall US image quality.</p><p>As the number of criteria thus the maximum score varies for different views, we normalise CP as a percentage to provide a consistent measure across all views. Scores from the 3 raters were averaged to obtain the final score for each view. The Pearson product-moment correlation coefficients between CP and GI are 0.81 for simulated data and 0.70 for real data, indicating that these two metrics are correlated but focus on different clinical quality aspects. Inter-rater variability is assessed using the two-way mixed-effects interclass correlation coefficient (ICC) with the definition of absolute agreement. Both CP and GI, show very good agreement between the 3 annotators with ICCs of 0.959, 0.939 and 0.813, 0.758 for simulated and real data respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Simulation-to-Real AQA Network (SR-AQA)</head><p>Overview of SR-AQA. Illustrated in Fig. <ref type="figure" target="#fig_1">2</ref>, the proposed SR-AQA is composed of ResNet-50 <ref type="bibr" target="#b5">[6]</ref> encoders, regressors and projectors, with shared weights. Given the simulated x s and real TEE image x r as input, SR-AQA first estimates the uncertainty in the real styles of x s , from the batch of real images and transfers real styles to simulated features by normalizing their feature statistics (i.e. mean and standard deviation) via UFS. Then, we perform style consistency learning with L SCL and task-specific learning with L T L for the final real-stylized features f s→r f inal and the final simulated features f s f inal to learn domain-invariant and task-specific information. Ultimately, the total loss function of SR-AQA is</p><formula xml:id="formula_0">L total = L MSE + λ 1 L SCL + λ 2 L T L , where L MSE = 1 N N i=1 (R s i -y s i )</formula><p>2 is the MSE loss calculated from the simulated data result R s and its label y s , while λ 1 and λ 2 are parameters empirically set to "10" and "1" to get a uniform order of magnitude at the early training stage. The input is fed into one encoder and regressor to predict the score during inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Uncertainty-Aware Feature Stylization (UFS).</head><p>The UFS pipeline is shown in the right part of Fig. <ref type="figure" target="#fig_1">2</ref>. Different domains generally have inconsistent feature statistics <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b20">21]</ref>. Since style is related to the features' means and standard deviations <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b23">24]</ref>, simulated features can be augmented to resemble real styles by adjusting their mean and standard deviation with the help of unlabelled real data. Let f s l and f r l be the simulated features and real features extracted from the l th layer of the encoder, respectively. We thus can transfer the style of f r l to f s l to obtain real-stylized features f s→r l as:</p><formula xml:id="formula_1">f s→r l = σ(f r l ) f s l -μ (f s l ) σ (f s l ) + μ(f r l ),<label>(1)</label></formula><p>where: μ(f ) and σ(f ) denote channel-wise mean and standard deviation of feature f , respectively. However, due to the complexity of real-world TEE, there are significant intra-domain differences, leading to uncertainties in the feature statistics of real data. To explore the potential space of unknown intra-domain shifts, instead of using fixed feature statistics, we generate multivariate Gaussian distributions to represent the uncertainty of the mean and standard deviation in the real data. Considering this, the new feature statistics of real features f r l , i.e. mean β(f r l ) and standard deviation α(f r l ), are sampled from</p><formula xml:id="formula_2">N μ(f r l ), Σ 2 μ (f r l ) and N σ(f r l ), Σ 2 σ (f r l )</formula><p>, respectively and computed as:</p><formula xml:id="formula_3">β(f r l ) = μ (f r l ) + ( Σ μ (f r l )) • I ρ&gt;0.5 , α(f r l ) = σ (f r l ) + ( Σ σ (f r l )) • I ρ&gt;0.5 ,<label>(2)</label></formula><p>where:</p><formula xml:id="formula_4">∼ N(0, 1) 1 , variances Σ 2 μ (f r l ) = 1 B B b=1 (μ(f r l ) -E b [μ(f r l )]) 2 and Σ 2 σ (f r l ) = 1 B B b=1 (σ(f r l ) -E b [σ(f r l )])</formula><p>2 are estimated from the mean and standard deviation of the batch B of real images, I ρ&gt;0.5 is an indicator function and ρ ∼ U(0, 1). Finally, our UFS for l th layer is defined as:</p><formula xml:id="formula_5">f s→r l = α(f r l ) f s l -μ (f s l ) σ (f s l ) + β(f r l ).<label>(3)</label></formula><p>To this end, the proposed UFS approach can close the reality gap by generating real-stylized features with sufficient variations, so that the network interprets real data as just another variation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Style Consistency Learning (SCL).</head><p>Through the proposed UFS, we obtain the final real-stylized features f s→r f inal that contain a diverse range of real styles. The f s→r f inal can be seen as style perturbations of the final simulated features f s f inal . We thus incorporate a SCL step, that maximizes the similarity between f s f inal and f s→r f inal to enforce their consistency in the feature level, allowing the encoder to learn robust representations. Specifically, the SCL adds a projector independently of the regressor to transform the f s f inal (f s→r f inal ) in an extra feature embedding, and then matches it to the other one. To prevent the Siamese encoder and Siamese projector (i.e. the top two encoders and projectors in Fig. <ref type="figure" target="#fig_1">2</ref>) from collapsing to a constant solution, similar to <ref type="bibr" target="#b2">[3]</ref>, we adopt the stop-gradient (stopgrad) operation for the projected features z s and z s→r . The SCL process is summarized as:</p><formula xml:id="formula_6">L SCL = 1 N N i=1 1 2 D f s f inal , stopgrad (z s→r ) + 1 2 D f s→r final , stopgrad (z s ) , (<label>4</label></formula><formula xml:id="formula_7">) where: D f 1 , f 2 = -f 1 f 1 2 • f 2 f 2 2</formula><p>is the negative cosine similarity between the input features f 1 and f 2 , and • 2 is L2-normalization.</p><p>The SCL guides the network to learn domain-invariant features, via various style perturbations, so that it can generalize well to the different visual appearances of the real domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task-Specific Learning (TL).</head><p>While alleviating the style differences between the simulated and real domain, UFS filters out some task-specific information (e.g. semantic content) encoded in the simulated features, as content and style are not orthogonal <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b18">19]</ref>, resulting in performance deterioration. Therefore, we embed TL to retain useful representations for AQA. Specifically, f s→r final should retain task-specific information to allow the regressor to predict results R s→r that correspond to the quality scores (CP, GI) in the simulated data. In TL, simulated labels y s are used as the supervising signal:</p><formula xml:id="formula_8">L T L = 1 N N i=1 (R s→r i -y s i ) 2 . (<label>5</label></formula><formula xml:id="formula_9">)</formula><p>The TL performs AQA tasks for style variants to complement the loss of information due to feature stylization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Settings</head><p>Experiments are implemented in PyTorch on a Tesla V100 GPU. The maximum training iteration is 40,000 with a batch size of 32. We adopted the SGD optimizer with a weight decay of 5e-4, a momentum of 0.9 and a learning rate of 1e-4. Input images are resized to 224 × 224, the CP and GI scores are normalized to [0, 1]. The MSE is adopted as the evaluation metric for both the CP and GI regression tasks. Following the standard approach for UDA <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b22">23]</ref>, we use all 16,192 labelled simulated data and all 4,427 unlabelled real data for domain adaptation, and test on all 4,427 real data. To further explore the data efficiency of UDA methods on our simulation-to-real AQA tasks, we also conduct experiments with fractions (10%, 30%, and 50%) of unlabeled real data for domain adaptation, randomly selected from the 10 TEE real cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Comparison with State-of-the-Arts</head><p>We compare the proposed SR-AQA with MixStyle <ref type="bibr" target="#b23">[24]</ref>, MDD <ref type="bibr" target="#b22">[23]</ref>, RSD <ref type="bibr" target="#b3">[4]</ref>, and SDAT <ref type="bibr" target="#b17">[18]</ref>. All methods are implemented based on their released code and original literature, and fine-tuned to fit our tasks to provide a basis for a fair comparison.</p><p>As shown in Table <ref type="table" target="#tab_0">1</ref>, all UDA methods show better AQA overall performance (lower MSE) compared to the model trained only with simulated data ("Simulated Only"), demonstrating the effectiveness of UDA methods in TEE simulation-to-real AQA. The proposed SR-AQA achieves superior performance with the lowest MSE in all CP experiments and all but one GI experiment (50%), in which is very close (0.7648 to 0.76) to the best-performing SDAT. We calculate the MSE reduction percentage between our proposed SR-AQA and the second-best method, to obtain the degree of performance improvement. Specifically, on the CP task among the five real data ratio settings, the MSE of our method dropped by 2.13%-5.08% against the suboptimal method SDAT. It is evident that even with a small amount (10%) of unlabelled real data used for UDA, our SR-AQA still achieves a significant MSE reduction, of at least 3.02% and 4.37% compared to other UDA methods, on the CP and GI tasks respectively, showcasing high data efficiency. We also conduct paired t-tests on MSE results, from multiple runs with different random seeds. The obtained p-values (p &lt; 0.05 in all but one case, see supplementary material Table <ref type="table" target="#tab_2">S3</ref>), validate that the improvements yielded by the proposed SR-AQA are statistically significant. In Table <ref type="table" target="#tab_1">2</ref>, we report the performance over different (low, medium, and high) score ranges with SR-AQA, obtaining promising results among all ranges 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Ablation Study</head><p>We first explore the impact of the amount of UFS on generalization performance. As shown in the left part of Table <ref type="table" target="#tab_2">3</ref>, the performance continues to improve as UFS is applied to more shallow layers, but decreases when UFS is added to deeper layers. This is because semantic information is more important than style in the deeper layers. Using a moderate number of UFS to enrich simulated features with real-world styles, without corrupting semantic information, improves model generalization. Secondly, we study the effect of uncertainty-aware, SCL, and TL, as shown in the right part of Table <ref type="table" target="#tab_2">3</ref>, removing each component leads to performance degradation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>This paper presents the first annotated TEE dataset for simulation-to-real AQA with 16,192 simulated images and 4,427 real images. Based on this, we propose a novel UDA network named SR-AQA for boosting the generalization of AQA performance. The network transfers diverse real styles to the simulated domain based on uncertainty-award feature stylization. Style consistency learning enables the encoder to learn style-independent representations while taskspecific learning allows our model to naturally adapt to real styles by preserving task-specific information. Experimental results on two AQA tasks for CP and GI scores show that the proposed method outperforms state-of-the-art methods with at least 5.08% and 16.28% MSE reduction, respectively, resulting in superior TEE AQA performance. We believe that our work provides an opportunity to leverage large amounts of simulated data to improve the generalisation performance of AQA for real TEE. Future work will focus on reducing negative transfer to extend UDA methods towards simulated-to-real TEE quality assessment.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overview of our proposed dataset. The 9 TEE views in our dataset: 1: Mid-Esophageal 4-Chamber, 2: Mid-Esophageal 2-Chamber, 3: Mid-Esophageal Aortic Valve Short-Axis, 4: Transgastric Mid-Short-Axis, 5: Mid-Esophageal Right Ventricle inflow-outflow, 6: Mid-Esophageal Aortic Valve Long-Axis, 7: Transgastric 2-Chamber, 8: Deep Transgastric Long-Axis, 9: Mid-Esophageal Mitral Commissural</figDesc><graphic coords="2,87,96,263,00,276,64,162,16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Overall architecture of our proposed SR-AQA. The right part shows uncertainty-aware feature stylization (UFS) for one simulated image x s with real images at l th layer. No gradient calculation is performed on the dotted line. The red line indicates the path of model inference.</figDesc><graphic coords="4,56,46,53,84,340,00,228,85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>MSE results for CP and GI scores, in different unlabelled real data ratios. Lower MSE means better model performance. The 'Reduction' row lists the MSE reduction percentage of our proposed SR-AQA compared to the second-best method. Top two results are in bold and underlined.</figDesc><table><row><cell>Methods</cell><cell cols="2">CP Task (MSE)</cell><cell></cell><cell></cell><cell cols="2">GI Task (MSE)</cell><cell></cell></row><row><cell></cell><cell cols="3">Unlabelled Real Data Ratio</cell><cell></cell><cell cols="4">Unlabelled Real Data Ratio</cell></row><row><cell></cell><cell>100%</cell><cell>50%</cell><cell>30%</cell><cell>10%</cell><cell>100%</cell><cell>50%</cell><cell>30%</cell><cell>10%</cell></row><row><cell cols="2">SR-AQA (Ours) 411</cell><cell>411</cell><cell>413</cell><cell>417</cell><cell cols="4">0.6992 0.7648 0.7440 0.7696</cell></row><row><cell>Reduction</cell><cell cols="8">-5.08% -2.38% -2.13% -3.02% -16.28% +0.63% -7.55% -4.37%</cell></row><row><cell>SDAT [18]</cell><cell>433</cell><cell>421</cell><cell>422</cell><cell>430</cell><cell>0.9792</cell><cell cols="3">0.7600 0.8048 1.1024</cell></row><row><cell>RSD [4]</cell><cell>540</cell><cell>507</cell><cell>501</cell><cell>513</cell><cell>0.8768</cell><cell cols="3">0.8768 0.9392 0.8048</cell></row><row><cell>Mixstyle [24]</cell><cell>466</cell><cell>474</cell><cell>465</cell><cell>496</cell><cell>0.8352</cell><cell cols="3">1.0048 0.8736 1.0400</cell></row><row><cell>MDD [23]</cell><cell>766</cell><cell>787</cell><cell>755</cell><cell>742</cell><cell>1.1696</cell><cell cols="3">1.1360 1.1328 1.1936</cell></row><row><cell cols="2">Simulated Only 913</cell><cell></cell><cell></cell><cell></cell><cell>1.2848</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>MSE results on AQA tasks for different score ranges. The full real dataset (without labels) is used for unsupervised domain adaptation.</figDesc><table><row><cell>Methods</cell><cell cols="2">CP Task (MSE)</cell><cell></cell><cell cols="2">GI Task (MSE)</cell><cell></cell></row><row><cell></cell><cell>Low</cell><cell cols="2">Medium High</cell><cell>Low</cell><cell cols="2">Medium High</cell></row><row><cell></cell><cell>(0,30]</cell><cell cols="3">(30, 60] (60, 100] (0,1.2]</cell><cell cols="2">(1.2, 2.4] (2.4, 4]</cell></row><row><cell cols="2">SR-AQA (Ours) 1668</cell><cell>403</cell><cell>286</cell><cell cols="3">2.3200 0.4080 0.4832</cell></row><row><cell>Reduction</cell><cell cols="6">-2.68% -2.66% -1.38% -13.38% -19.56% -1.30%</cell></row><row><cell>SDAT [18]</cell><cell>1886</cell><cell>441</cell><cell>290</cell><cell>3.2512</cell><cell>0.6400</cell><cell>0.4896</cell></row><row><cell>RSD [4]</cell><cell>1881</cell><cell>489</cell><cell>427</cell><cell>2.6784</cell><cell>0.5168</cell><cell>0.6912</cell></row><row><cell>Mixstyle [24]</cell><cell>1752</cell><cell>414</cell><cell>359</cell><cell>2.7152</cell><cell>0.5072</cell><cell>0.5632</cell></row><row><cell>MDD [23]</cell><cell>1714</cell><cell>550</cell><cell>771</cell><cell>3.1376</cell><cell>0.8208</cell><cell>0.8480</cell></row><row><cell cols="2">Simulated Only 2147</cell><cell>850</cell><cell>831</cell><cell>3.4592</cell><cell>0.8896</cell><cell>1.0336</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Ablation results on two AQA tasks via different UFS settings (left part), and for the proposed uncertainty-aware, LSCL, and LTL (right part). Headings 1, 1-2, 1-3, 1-4, 1-5 refer to replacing the 1 st , 1-2 nd , 1-3 rd , 1-4 th , 1-5 th batch normalization layer(s) of ResNet-50 with the UFS. The full real dataset (without labels) is used for unsupervised domain adaptation.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Feature Stylization LSCL LTL CP Task (MSE) GI Task (MSE)</cell></row><row><cell>UFS</cell><cell>1</cell><cell>1-2</cell><cell>1-3</cell><cell>1-4</cell><cell>1-5</cell><cell>w/o Uncertainty</cell><cell>426 419</cell><cell>0.8080 0.7408</cell></row><row><cell cols="6">CP Task (MSE) 421 GI Task (MSE) 0.7808 0.7488 0.6992 0.7312 0.7840 412 411 422 430</cell><cell>w/ Uncertainty</cell><cell>422 411</cell><cell>0.7840 0.6992</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Re-parameterization trick is applied here to make the sampling operation differentiable.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work is supported by the <rs type="funder">Wellcome/EPSRC Centre for Interventional and Surgical Sciences (WEISS)</rs> [<rs type="grantNumber">203145Z/16/Z</rs> and <rs type="grantNumber">NS/A000050/1</rs>]; EPSRC-funded <rs type="projectName">UCL Centre for Doctoral Training in Intelligent, Integrated Imaging in Healthcare</rs> (i4health) [EP/S021930/1]; <rs type="funder">Horizon</rs> <rs type="grantNumber">2020 FET [863146</rs>]; a <rs type="affiliation">UCL Graduate Research Scholarship</rs>; and <rs type="person">Singapore MoE Tier 1 Start</rs> up grant (<rs type="grantNumber">WBS: A-8001267-00-00</rs>). Danail Stoyanov is supported by a <rs type="grantName">RAE Chair in Emerging Technologies</rs> [<rs type="grantNumber">CiET1819/2/36</rs>] and an <rs type="funder">EPSRC</rs> <rs type="grantName">Early Career Research Fellowship</rs> [<rs type="grantNumber">EP/P012841/1</rs>].</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_RtNznCA">
					<idno type="grant-number">203145Z/16/Z</idno>
				</org>
				<org type="funded-project" xml:id="_rX7VEMv">
					<idno type="grant-number">NS/A000050/1</idno>
					<orgName type="project" subtype="full">UCL Centre for Doctoral Training in Intelligent, Integrated Imaging in Healthcare</orgName>
				</org>
				<org type="funding" xml:id="_FcGFfnp">
					<idno type="grant-number">2020 FET [863146</idno>
				</org>
				<org type="funding" xml:id="_xzKRdAC">
					<idno type="grant-number">WBS: A-8001267-00-00</idno>
					<orgName type="grant-name">RAE Chair in Emerging Technologies</orgName>
				</org>
				<org type="funding" xml:id="_T5qe6xr">
					<idno type="grant-number">CiET1819/2/36</idno>
					<orgName type="grant-name">Early Career Research Fellowship</orgName>
				</org>
				<org type="funding" xml:id="_AT56kRT">
					<idno type="grant-number">EP/P012841/1</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Automatic quality assessment of echocardiograms using convolutional neural networks: feasibility on the apical four-chamber view</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Abdi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1221" to="1230" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">MaxStyle: adversarial style composition for robust medical image segmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sinclair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16443-9_15</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16443-915" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13435</biblScope>
			<biblScope unit="page" from="151" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Exploring simple Siamese representation learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2021</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="15745" to="15753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Representation subspace distance for domain adaptation regression</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML 2021</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1749" to="1759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Guidelines for performing a comprehensive transesophageal echocardiographic examination: recommendations from the American society of echocardiography and the society of cardiovascular anesthesiologists</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Hahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Soc. Echocardiogr</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="921" to="964" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2016</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Impact of simulator-based training on acquisition of transthoracic echocardiography skills in medical students</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hempel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Card. Anaesth</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">293</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Arbitrary style transfer in real-time with adaptive instance normalization</title>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="1501" to="1510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Automated assessment of transthoracic echocardiogram image quality using deep neural networks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Labs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vrettos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Loo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zolgharni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intell. Med</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The use of ultrasound simulators to strengthen scanning skills in medical students: a randomized controlled trial</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Steinmetz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dyachenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oleskevich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Ultrasound Med</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1249" to="1257" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">WildNet: learning domain generalized semantic segmentation from the wild</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Seong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2022</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="9926" to="9936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Uncertainty modeling for out-of-distribution generalization</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<idno>ICLR 2022</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On modelling label uncertainty in deep neural networks: automatic estimation of intra-observer variability in 2D echocardiography quality assessment</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1868" to="1883" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-task learning for quality assessment of fetal head ultrasound images</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page">101548</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Automated performance assessment in transoesophageal echocardiography with convolutional neural networks</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Mazomenos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-00937-3_30</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-00937-330" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2018</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Schnabel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Davatzikos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Alberola-López</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Fichtinger</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11073</biblScope>
			<biblScope unit="page" from="256" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Motion-based technical skills assessment in transoesophageal echocardiography</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Mazomenos</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-43775-0_9</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-43775-09" />
	</analytic>
	<monogr>
		<title level="m">MIAR 2016</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Cattin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S.-L</forename><surname>Lee</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">9805</biblScope>
			<biblScope unit="page" from="96" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imaging skills for transthoracic echocardiography in cardiology fellows: the value of motion metrics</title>
		<author>
			<persName><forename type="first">M</forename><surname>Montealegre-Gallegos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Card. Anaesth</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">245</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A closer look at smoothness in domain adversarial training</title>
		<author>
			<persName><forename type="first">H</forename><surname>Rangwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Aithal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">B</forename><surname>Radhakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML 2022</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="18378" to="18399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Task-relevant feature replenishment for cross-centre polyp segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Q H</forename><surname>Meng</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16440-8_57</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16440-857" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13434</biblScope>
			<biblScope unit="page" from="599" to="608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Innovative transesophageal echocardiography training and competency assessment for Chinese anesthesiologists: role of transesophageal echocardiography simulation training</title>
		<author>
			<persName><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">G</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Curr. Opin. Anaesthesiol</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="686" to="691" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Transferable calibration with lower bias and variance in domain adaptation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="19212" to="19223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A minimum dataset for a standard transoesphageal echocardiogram: a guideline protocol from the British society of echocardiography</title>
		<author>
			<persName><forename type="first">R</forename><surname>Wheeler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Echo Res. Pract</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">29</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bridging theory and algorithm for domain adaptation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML 2019</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7404" to="7413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Domain generalization with mixstyle</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
