<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Scaling up 3D Kernels with Bayesian Frequency Re-parameterization for Medical Image Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ho</forename><forename type="middle">Hin</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Vanderbilt University</orgName>
								<address>
									<postCode>37212</postCode>
									<settlement>Nashville</settlement>
									<region>TN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Quan</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Vanderbilt University</orgName>
								<address>
									<postCode>37212</postCode>
									<settlement>Nashville</settlement>
									<region>TN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shunxing</forename><surname>Bao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Vanderbilt University</orgName>
								<address>
									<postCode>37212</postCode>
									<settlement>Nashville</settlement>
									<region>TN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qi</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Vanderbilt University</orgName>
								<address>
									<postCode>37212</postCode>
									<settlement>Nashville</settlement>
									<region>TN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xin</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Vanderbilt University</orgName>
								<address>
									<postCode>37212</postCode>
									<settlement>Nashville</settlement>
									<region>TN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Leon</forename><forename type="middle">Y</forename><surname>Cai</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Biomedical Engineering</orgName>
								<orgName type="institution">Vanderbilt University</orgName>
								<address>
									<postCode>37212</postCode>
									<settlement>Nashville</settlement>
									<region>TN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Thomas</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Biomedical Engineering</orgName>
								<orgName type="institution">Vanderbilt University</orgName>
								<address>
									<postCode>37212</postCode>
									<settlement>Nashville</settlement>
									<region>TN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuankai</forename><surname>Huo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Vanderbilt University</orgName>
								<address>
									<postCode>37212</postCode>
									<settlement>Nashville</settlement>
									<region>TN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Vanderbilt University</orgName>
								<address>
									<postCode>37212</postCode>
									<settlement>Nashville</settlement>
									<region>TN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xenofon</forename><surname>Koutsoukos</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Vanderbilt University</orgName>
								<address>
									<postCode>37212</postCode>
									<settlement>Nashville</settlement>
									<region>TN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bennett</forename><forename type="middle">A</forename><surname>Landman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Vanderbilt University</orgName>
								<address>
									<postCode>37212</postCode>
									<settlement>Nashville</settlement>
									<region>TN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Vanderbilt University</orgName>
								<address>
									<postCode>37212</postCode>
									<settlement>Nashville</settlement>
									<region>TN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Biomedical Engineering</orgName>
								<orgName type="institution">Vanderbilt University</orgName>
								<address>
									<postCode>37212</postCode>
									<settlement>Nashville</settlement>
									<region>TN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Scaling up 3D Kernels with Bayesian Frequency Re-parameterization for Medical Image Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8EFCE21F3B15A4C44FEA7A0036CF0725</idno>
					<idno type="DOI">10.1007/978-3-031-43901-860.</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T10:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Bayesian Frequency Re-parameterization</term>
					<term>Large Kernel Convolution</term>
					<term>Medical Image Segmentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With the inspiration of vision transformers, the concept of depth-wise convolution revisits to provide a large Effective Receptive Field (ERF) using Large Kernel (LK) sizes for medical image segmentation. However, the segmentation performance might be saturated and even degraded as the kernel sizes scaled up (e.g., 21 × 21 × 21) in a Convolutional Neural Network (CNN). We hypothesize that convolution with LK sizes is limited to maintain an optimal convergence for locality learning. While Structural Re-parameterization (SR) enhances the local convergence with small kernels in parallel, optimal small kernel branches may hinder the computational efficiency for training. In this work, we propose RepUX-Net, a pure CNN architecture with a simple large kernel block design, which competes favorably with current network state-of-the-art (SOTA) (e.g., 3D UX-Net, SwinUNETR) using 6 challenging public datasets. We derive an equivalency between kernel reparameterization and the branch-wise variation in kernel convergence. Inspired by the spatial frequency in the human visual system, we extend to vary the kernel convergence into element-wise setting and model the spatial frequency as a Bayesian prior to re-parameterize convolutional weights during training. Specifically, a reciprocal function is leveraged to estimate a frequency-weighted value, which rescales the corresponding kernel element for stochastic gradient descent. From the experimental results, RepUX-Net consistently outperforms 3D SOTA benchmarks with internal validation (FLARE: 0.929 to 0.944), external validation (MSD: 0.901 to 0.932, KiTS: 0.815 to 0.847, LiTS: 0.933 to 0.949, TCIA: 0.736 to 0.779) and transfer learning (AMOS: 0.880 to 0.911) scenarios in Dice Score. Both codes and pre-trained models are available at: https:// github.com/MASILab/RepUX-Net.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the introduction of Vision Transformers (ViTs), CNNs have been greatly challenged as seen with the leading performance in multiple volumetric data benchmarks, especially for medical image segmentation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23]</ref>. The key contribution of ViTs is largely credited to the large Effective Receptive Field (ERF) with a multi-head self-attention mechanism <ref type="bibr" target="#b5">[6]</ref>. Note the attention mechanism is computationally unscalable with respect to the input resolutions <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>. Therefore, the concept of depth-wise convolution is revisited to provide a scalable and efficient feature computation with large ERF using large kernel sizes (e.g., 7 × 7 × 7) <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b17">18]</ref>. However, either from prior works or our experiments, the model performance becomes saturated or even degraded when the kernel size is scaled up in encoder blocks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b15">16]</ref>. We hypothesize that scaling up the kernel size in convolution may limit the optimal learning convergences across local to global scales. Recently, the feasibility of leveraging large kernel convolutions (e.g., 31 × 31 <ref type="bibr" target="#b3">[4]</ref>, 51 × 51 <ref type="bibr" target="#b15">[16]</ref>) has been shown with natural image domain with Structural Re-parameterization (SR), which adapts Constant-Scale Linear Addition (CSLA) block (Fig. <ref type="figure">2b</ref>) and re-parameterizes the large kernel weights during inference <ref type="bibr" target="#b3">[4]</ref>. As convolutions with small kernel sizes converge more easily, the convergence of small kernel regions enhances in the re-parameterized weight, as shown in Fig. <ref type="figure">1a</ref>. With such observation, we further ask: Can we adapt variable convergence across elements of the convolution kernel during training, instead of regional locality only?</p><p>In this work, we first derive and extend the theoretical equivalency of the weight optimization in the CSLA block. We observe that the kernel weight of each branch can be optimized with variable convergence using branch-specific learning rates. Furthermore, the ERF with SR is visualized to be more widely distributed from the center element to the global surroundings <ref type="bibr" target="#b3">[4]</ref>, demonstrating a similar behavior to the spatial frequency in the human visual system <ref type="bibr" target="#b12">[13]</ref>. Inspired by the reciprocal characteristics of spatial frequency, we model the spatial frequency as a Bayesian prior to adapt variable convergence of each kernel element with stochastic gradient descent (Fig. <ref type="figure">1b</ref>). Specifically, we compute a scaling factor with respect to the distance from the kernel center and multiply the corresponding element for re-parameterization during training. Furthermore, we simplify the encoder block design into a plain convolution block only to minimize the computation burden in training and achieve State-Of-The-Art (SOTA) performance. We propose RepUX-Net, a pure 3D CNN with the large kernel size (e.g., 21 × 21 × 21) in encoder blocks, to compete favorably with current SOTA segmentation networks. We evaluate RepUX-Net on supervised multi-organ segmentation with 6 different public volumetric datasets. RepUX-Net demonstrates significant improvement consistently across all datasets compared to all SOTA networks. We summarize our contributions as below:</p><p>-We propose RepUX-Net with better adaptation in large kernel convolution than 3D UX-Net, achieving SOTA performance in 3D segmentation. To our best knowledge, this is the first network that effectively leverages large kernel convolution with plain design in the encoder for 3D segmentation. -We propose a novel theory-inspired re-parameterization strategy to scale the element-wise learning convergence in large kernels with Bayesian prior knowledge. To our best knowledge, this is the first re-parameterization strategy to adapt 3D large kernels in the medical domain. -We leverage six challenging public datasets to evaluate RepUX-Net in 1)</p><p>direct training and 2) transfer learning scenarios with 3D multi-organ seg-mentation. RepUX-Net achieves significant improvement consistently in both scenarios across all SOTA networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Weights Re-parameterization: SR is a methodology of equivalently converting model structures via transforming the parameters in kernel weights. For example, RepVGG demonstrates to construct one extra ResNet-style shortcut as a 1 × 1 convolution, parallel to 3 × 3 convolution during training <ref type="bibr" target="#b4">[5]</ref>. Such parallel branch design is claimed to enhance the learning efficiency during training, in which the 1 × 1 branch is then merged into the parallel 3 × 3 kernel via a series of linear transformation in the inference stage. OREPA further adds more parallel branches with linear scaling modules to enhance training efficiency <ref type="bibr" target="#b9">[10]</ref>. Inspired by the parallel branches design, RepLKNet is proposed to scale up the 2D kernel size (e.g., 31 × 31) with a 3 × 3 convolution as the parallel branch <ref type="bibr" target="#b3">[4]</ref>. SLaK further extends the kernel size to 51 × 51 by decomposing the large kernel into two rectangular parallel kernels with sparse groups and training the model with dynamic sparsity <ref type="bibr" target="#b15">[16]</ref>. However, the proposed models' FLOPs remain at a high-level with the parallel branch design and demonstrates to have a trade-off between model performance and training efficiency. To tackle the tradeoff, RepOptimizer provides an alternative to re-parameterize the back-propagate gradient, instead of the structural parameters of kernel weights, to enhance the training efficiency with plain convolution block design <ref type="bibr" target="#b2">[3]</ref>. Significant efforts have been demonstrated to enlarge the 2D kernel size in the natural image domain, while limited studies have been proposed for 3D kernels in medical domain. As 3D kernels have a larger number of parameters than 2D, it is challenging to directly leverage the parallel branch design and maintain an optimal convergence of learning large kernel convolution without trading off the computation efficiency significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>Instead of changing the gradient dynamics during training <ref type="bibr" target="#b2">[3]</ref>, we introduce RepUX-Net, a pure 3D CNN architecture that performs element-wise scaling in large kernel weights to enhance the learning convergence and effectively adapts large receptive field for volumetric segmentation. To design such behavior, we adapt a two-step pipeline: 1) we define the theoretical equivalency of variable learning convergence in convolution branches; 2) we simulate the behavior of spatial frequency to re-weight the learning importance of each element in kernels for stochastic gradient descent. Note the theoretical derivation depends on the optimization with first-order gradient-driven optimizer (e.g., SGD, AdamW) <ref type="bibr" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Variable Learning Convergence in Multi-Branch Design</head><p>From Fig. <ref type="figure">2b</ref> &amp; 2c, previous re-parameterization strategies only demonstrate the benefits of the parallel branch design by either adding up the encoded outputs from both small and large kernels with SR (RepLKNet <ref type="bibr" target="#b3">[4]</ref>) or performing Gradient Re-parameterization (GR) by multiplying with constant values (RepOptimizer <ref type="bibr" target="#b2">[3]</ref>) in a Single Operator (SO) to enhance the locality learning in large kernels. Inspired by the concepts of SR and GR, we extend the theoretical equivalency proof in RepOptimizer to adapt variable learning convergence in branches.</p><p>Here, we only showcase the conclusion with two convolutions and two constant scalars as the scaling factors for simplicity. The complete proof of equivalency is demonstrated in Supplementary 1.1. Let {α L , α S } and {W L , W S } be the two constant scalars and two convolution kernels (Large &amp; Small) respectively. Let X and Y be the input and output features, the CSLA block is formulated as</p><formula xml:id="formula_0">Y CSLA = α L (X W L ) + α S (X W S )</formula><p>, where denotes as convolution. For SO blocks, we train the plain structure parameterized by W and Y SO = X W . Let i be the number of training iterations, we ensure that</p><formula xml:id="formula_1">Y (i) CSLA = Y (i)</formula><p>SO , ∀i ≥ 0 and derive the stochastic gradient descent of parallel branches as follows:</p><formula xml:id="formula_2">α L W L(i+1) + α S W S(i+1) = α L W L(i) -λ L α L ∂L ∂W Li + α S W S(i) -λ S α S ∂L ∂W Si , (<label>1</label></formula><formula xml:id="formula_3">)</formula><p>where L is the objective function; λ L and λ S are the Learning Rate (LR) of each branch respectively. We observe that the optimization of each branch can be different, which is feasible to control by adjusting the branch-specific LR.</p><p>The locality convergence in large kernels enhance with the quick convergence in small kernels. Additionally from our experiments, a significant improvement is demonstrated with different branch-wise LR using SGD (Table <ref type="table" target="#tab_1">2</ref>). Building upon this insight, we further hypothesize that the convergence of each large kernel element can be optimized differently by linear scaling with prior knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Bayesian Frequency Re-parameterization (BFR)</head><p>With the visualization of ERF in RepLKNet <ref type="bibr" target="#b3">[4]</ref>, the diffused distribution (from local to global) in ERF demonstrates similar behavior with the spatial frequency in the human visual system <ref type="bibr" target="#b12">[13]</ref>. High spatial frequency (small ERF) allows to refine and sharpen details with high acuity, while global details are demonstrated with low spatial frequency. Inspired by the reciprocal characteristics in spatial frequency, we first generate a Bayesian prior distribution to model the spatial frequency by computing a reciprocal distance function between each element and the central point of the kernel weight as follows:</p><formula xml:id="formula_4">d(x, y, z, c) = (x -c) 2 + (y -c) 2 + (z -c) 2 δ(x k , y k , z k , c, α) = α d(x k , y k , z k , c) + α (2)</formula><p>where k and c are the element and central index of the kernel weight, α is the hyperparameter to control the shape of the generated frequency distribution. Instead of adjusting the LR in parallel branches, we propose to re-parameterize the convolution weights by multiplying the scaling factor δ to each kernel element and apply a static LR λ for stochastic gradient descent in single operator setting as follows:</p><formula xml:id="formula_5">W i+1 = δW i -λ ∂L ∂δW i (3)</formula><p>With the multiplication with δ, each element in the kernel weight is rescaled with respect to the frequency level and allow to converge differently with a static LR in stochastic gradient descent. Such design demonstrates to influence the weighted convergence diffused from local to global in theory, thus tackling the limitation of enhancing the local convergence only in branch-wise setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Model Architecture</head><p>The backbone of RepUX-Net is based on 3D UX-Net <ref type="bibr" target="#b13">[14]</ref>, which comprises multiple volumetric convolution blocks that directly utilize 3D patches and leverage skip connections to transfer hierarchical multi-resolution features for end-to-end optimization. Inspired by <ref type="bibr" target="#b14">[15]</ref>, we choose a kernel size of 21 × 21 × 21 for Depth-Wise Convolution (DWC-21) as the optimal choice without significant trade-off between model performance and computational efficiency in 3D. We further simplify the block design as a plain convolution block design to minimize the computational burden from additional modules. The encoder blocks in layers l and l + 1 are defined as follows:</p><formula xml:id="formula_6">ẑl = GeLU(DWC-21(BN(z l-1 ))), ẑl+1 = GeLU(DWC-21(BN(z l )))<label>(4)</label></formula><p>where ẑl and ẑl+1 are the outputs from the DWC layer in each depth level; BN denotes as the batch normalization layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head><p>Datasets. We perform experiments on six public datasets for volumetric segmentation, which comprise with 1) Medical Segmentation Decathlon (MSD) spleen dataset <ref type="bibr" target="#b0">[1]</ref>, 2) MICCAI 2017 LiTS Challenge dataset (LiTS) <ref type="bibr" target="#b1">[2]</ref>, 3) MIC-CAI 2019 KiTS Challenge dataset (KiTS) <ref type="bibr" target="#b8">[9]</ref>, 4) NIH TCIA Pancreas-CT dataset (TCIA) <ref type="bibr" target="#b19">[20]</ref>, 5) MICCAI 2021 FLARE Challenge dataset (FLARE) <ref type="bibr" target="#b18">[19]</ref>, and 6) MICCAI 2022 AMOS challenge dataset (AMOS) <ref type="bibr" target="#b11">[12]</ref>. More details of each dataset (including data split for training and inference) are described in Supplementary Material (SM) Table <ref type="table" target="#tab_0">1</ref>.</p><p>Implementation. We evaluate RepUX-Net with three different scenarios: 1) internal validation with direct supervised learning, 2) external validation with the unseen datasets, and 3) transfer learning with pretrained weights. All preprocessing and training details including baselines, are followed with <ref type="bibr" target="#b13">[14]</ref> for benchmarking. For external validations, we leverage the AMOS-pretrained weights to evaluate 4 unseen datasets. In summary, we evaluate the segmentation performance of RepUX-Net by comparing current SOTA networks in a fully-supervised setting. Furthermore, we perform ablation studies to investigate the effect on Bayesian frequency distribution with different scales generated by α and the variability of branch-wise learning rates with first-order gradient optimizers (e.g., SGD, AdamW) for volumetric segmentation. Dice similarity coefficient is leveraged as an evaluation metric to measure the overlapping regions between the model predictions and the manual ground-truth labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>Different Scenarios Evaluations. Table <ref type="table" target="#tab_0">1</ref> shows the result comparison of current SOTA networks on medical image segmentation in a volumetric setting.</p><p>With our designed convolutional blocks as the encoder backbone, RepUX-Net demonstrates the best performance across all segmentation task with significant improvement in Dice score (FLARE: 0.934 to 0.944, AMOS: 0.891 to 0.902). Furthermore, RepUX-Net demonstrates the best generalizability consistently with a significant boost in performance across 4 different external datasets (MSD: 0.926 to 0.932, KiTS: 0.836 to 0.847, LiTS: 0.939 to 0.949, TCIA: 0.750 to 0.779). For transfer learning scenario, the performance of RepUX-Net significantly outper-   <ref type="table" target="#tab_1">2</ref>. RepUX-Net demonstrates its capabilities across the generalizability of unseen datasets and transfer learning ability. The qualitative representations (in Fig. <ref type="figure" target="#fig_1">3</ref>) further provides additional confidence of the quality improvement in segmentation predictions with RepUX-Net (Table <ref type="table" target="#tab_2">3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Studies with Block Designs &amp; Optimizers.</head><p>With the plain convolution design, a mean dice score of 0.906 is demonstrated with AdamW optimizer and perform slightly better than that with SGD. With the additional design of a parallel small kernel branch, the segmentation performance significantly improved (SGD: 0.898 to 0.917, AdamW: 0.906 to 0.929) with the optimized parallel branch LR using SR. The performance is further enhanced (SGD: 0.917 to 0.930, AdamW: 0.929 to 0.937) without being saturated with the increase of the training steps. By adapting BFR, the segmentation performance outperforms the parallel branch design significantly with a Dice score of 0.944.</p><p>Effectiveness on Different Frequency Distribution. From Fig. <ref type="figure">1</ref> in SM, RepUX-Net demonstrates the best performance when α = 1, while comparable performance is demonstrated in both α = 0.5 and α = 8. A possible family of Bayesian distributions (different shapes) may need to further optimize the learning convergence of kernels across each channel.</p><p>Limitations. The shape of the generated Bayesian distribution is fixed across all kernel weights with an unlearnable distance function. Each channel in kernels is expected to extract variable features with different distributions. Exploring different families of distributions to rescale the element-wise convergence in kernels will be our potential future direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We introduce RepUX-Net, the first 3D CNN adapting extreme large kernel convolution in encoder network for medical image segmentation. We propose to model the spatial frequency in the human visual system as a reciprocal function, which generates a Bayesian prior to rescale the learning convergence of each element in kernel weights. By introducing the frequency-guided importance during training, RepUX-Net outperforms current SOTA networks on six challenging public datasets via both direct training and transfer learning scenarios.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .Fig. 2 .</head><label>12</label><figDesc>Fig. 1. With the fast convergence in small kernels, SR merges the branches weights and enhances the locality convergence with respect to the kernel size (deep blue region), while the global convergence is yet to be optimal (light blue region). By adapting BFR, the learning convergence can rescale in an element-wise setting and distribute the learning importance from local to global. (Color figure online)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Qualitative Representations of organ segmentation in LiTS and TCIA datasets</figDesc><graphic coords="8,55,98,283,67,340,18,117,64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison of SOTA approaches on the five different testing datasets. (*: p &lt; 0.01, with Paired Wilcoxon signed-rank test to all baseline networks)</figDesc><table><row><cell></cell><cell></cell><cell>Internal Testing</cell><cell></cell><cell></cell><cell cols="2">External Testing</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>FLARE</cell><cell></cell><cell></cell><cell>MSD</cell><cell>KiTS</cell><cell>LiTS</cell><cell>TCIA</cell></row><row><cell>Methods</cell><cell cols="4">#Params FLOPs Spleen Kidney Liver Pancreas Mean</cell><cell cols="3">Spleen Kidney Liver</cell><cell>Pancreas</cell></row><row><cell>nn-UNet [11]</cell><cell>31.2M</cell><cell>743.3G 0.971 0.966</cell><cell>0.976 0.792</cell><cell>0.926</cell><cell>0.917</cell><cell>0.829</cell><cell>0.935</cell><cell>0.739</cell></row><row><cell>TransBTS [22]</cell><cell>31.6M</cell><cell>110.4G 0.964 0.959</cell><cell>0.974 0.711</cell><cell>0.902</cell><cell>0.881</cell><cell>0.797</cell><cell>0.926</cell><cell>0.699</cell></row><row><cell>UNETR [8]</cell><cell>92.8M</cell><cell>82.6G 0.927 0.947</cell><cell>0.960 0.710</cell><cell>0.886</cell><cell>0.857</cell><cell>0.801</cell><cell>0.920</cell><cell>0.679</cell></row><row><cell>nnFormer [23]</cell><cell>149.3M</cell><cell>240.2G 0.973 0.960</cell><cell>0.975 0.717</cell><cell>0.906</cell><cell>0.880</cell><cell>0.774</cell><cell>0.927</cell><cell>0.690</cell></row><row><cell>SwinUNETR [7]</cell><cell>62.2M</cell><cell>328.4G 0.979 0.965</cell><cell>0.980 0.788</cell><cell>0.929</cell><cell>0.901</cell><cell>0.815</cell><cell>0.933</cell><cell>0.736</cell></row><row><cell>3D UX-Net (k = 7) [14]</cell><cell>53.0M</cell><cell>639.4G 0.981 0.969</cell><cell>0.982 0.801</cell><cell>0.934</cell><cell>0.926</cell><cell>0.836</cell><cell>0.939</cell><cell>0.750</cell></row><row><cell>3D UX-Net (k = 21) [14]</cell><cell>65.9M</cell><cell>757.6G 0.980 0.968</cell><cell>0.979 0.795</cell><cell>0.930</cell><cell>0.908</cell><cell>0.808</cell><cell>0.929</cell><cell>0.720</cell></row><row><cell>RepOptimizer [3]</cell><cell>65.8M</cell><cell>757.4G 0.981 0.969</cell><cell>0.981 0.822</cell><cell>0.937</cell><cell>0.913</cell><cell>0.833</cell><cell>0.934</cell><cell>0.746</cell></row><row><cell cols="2">3D RepUX-Net (Ours) 65.8M</cell><cell cols="2">757.4G 0.984 0.970 0.983 0.837</cell><cell cols="5">0.944* 0.932* 0.847* 0.949* 0.779*</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation studies with quantitative Comparison on Block Designs with/out frequency modeling using different optimizer Optimizer Main Branch Para. Branch BFR Train Steps Main LR Para. LR Mean Dice</figDesc><table><row><cell>SGD</cell><cell>21 × 21 × 21 ×</cell><cell>×</cell><cell>40000</cell><cell>0.0003</cell><cell>×</cell><cell>0.898</cell></row><row><cell cols="2">AdamW 21 × 21 × 21 ×</cell><cell>×</cell><cell>40000</cell><cell>0.0001</cell><cell>×</cell><cell>0.906</cell></row><row><cell>SGD</cell><cell>21 × 21 × 21 3 × 3 × 3</cell><cell>×</cell><cell>40000</cell><cell>0.0003</cell><cell>0.0006</cell><cell>0.917</cell></row><row><cell cols="2">AdamW 21 × 21 × 21 3 × 3 × 3</cell><cell>×</cell><cell>40000</cell><cell>0.0001</cell><cell>0.0001</cell><cell>0.929</cell></row><row><cell cols="2">AdamW 21 × 21 × 21 ×</cell><cell></cell><cell>40000</cell><cell>0.0001</cell><cell>×</cell><cell>0.938</cell></row><row><cell>SGD</cell><cell>21 × 21 × 21 3 × 3 × 3</cell><cell>×</cell><cell>60000</cell><cell>0.0003</cell><cell>0.0006</cell><cell>0.930</cell></row><row><cell cols="2">AdamW 21 × 21 × 21 3 × 3 × 3</cell><cell>×</cell><cell>60000</cell><cell>0.0001</cell><cell>0.0001</cell><cell>0.938</cell></row><row><cell cols="2">AdamW 21 × 21 × 21 ×</cell><cell></cell><cell>60000</cell><cell>0.0001</cell><cell>×</cell><cell>0.944</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Evaluations on the AMOS testing split in different scenarios. (*: p &lt; 0.01, with Paired Wilcoxon signed-rank test to all baseline networks)</figDesc><table><row><cell cols="2">Train From Scratch Scenario</cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell>Spleen R. Kid L. Kid Gall. Eso.</cell><cell>Liver Stom. Aorta IVC</cell><cell>Panc. RAG LAG Duo. Blad. Pros. Avg</cell></row><row><cell>nn-UNet</cell><cell cols="3">0.951 0.919 0.930 0.845 0.797 0.975 0.863 0.941 0.898 0.813 0.730 0.677 0.772 0.797 0.815 0.850</cell></row><row><cell>TransBTS</cell><cell cols="3">0.930 0.921 0.909 0.798 0.722 0.966 0.801 0.900 0.820 0.702 0.641 0.550 0.684 0.730 0.679 0.783</cell></row><row><cell>UNETR</cell><cell cols="3">0.925 0.923 0.903 0.777 0.701 0.964 0.759 0.887 0.821 0.687 0.688 0.543 0.629 0.710 0.707 0.740</cell></row><row><cell>nnFormer</cell><cell cols="3">0.932 0.928 0.914 0.831 0.743 0.968 0.820 0.905 0.838 0.725 0.678 0.578 0.677 0.737 0.596 0.785</cell></row><row><cell>SwinUNETR</cell><cell cols="3">0.956 0.957 0.949 0.891 0.820 0.978 0.880 0.939 0.894 0.818 0.800 0.730 0.803 0.849 0.819 0.871</cell></row><row><cell>3D UX-Net (k=7)</cell><cell cols="3">0.966 0.959 0.951 0.903 0.833 0.980 0.910 0.950 0.913 0.830 0.805 0.756 0.846 0.897 0.863 0.890</cell></row><row><cell cols="4">3D UX-Net (k=21) 0.963 0.959 0.953 0.921 0.848 0.981 0.903 0.953 0.910 0.828 0.815 0.754 0.824 0.900 0.878 0.891</cell></row><row><cell>RepOptimizer</cell><cell cols="3">0.968 0.964 0.953 0.903 0.857 0.981 0.915 0.950 0.915 0.826 0.802 0.756 0.813 0.906 0.867 0.892</cell></row><row><cell cols="4">RepUX-Net (Ours) 0.972 0.963 0.964 0.911 0.861 0.982 0.921 0.956 0.924 0.837 0.818 0.777 0.831 0.916 0.879 0.902*</cell></row><row><cell cols="2">Transfer Learning Scenario</cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell>Spleen R. Kid L. Kid Gall. Eso.</cell><cell>Liver Stom. Aorta IVC</cell><cell>Panc. RAG LAG Duo. Blad. Pros. Avg</cell></row><row><cell>nn-UNet</cell><cell cols="3">0.965 0.959 0.951 0.889 0.820 0.980 0.890 0.948 0.901 0.821 0.785 0.739 0.806 0.869 0.839 0.878</cell></row><row><cell>TransBTS</cell><cell cols="3">0.885 0.931 0.916 0.817 0.744 0.969 0.837 0.914 0.855 0.724 0.630 0.566 0.704 0.741 0.650 0.792</cell></row><row><cell>UNETR</cell><cell cols="3">0.926 0.936 0.918 0.785 0.702 0.969 0.788 0.893 0.828 0.732 0.717 0.554 0.658 0.683 0.722 0.762</cell></row><row><cell>nnFormer</cell><cell cols="3">0.935 0.904 0.887 0.836 0.712 0.964 0.798 0.901 0.821 0.734 0.665 0.587 0.641 0.744 0.714 0.790</cell></row><row><cell>SwinUNETR</cell><cell cols="3">0.959 0.960 0.949 0.894 0.827 0.979 0.899 0.944 0.899 0.828 0.791 0.745 0.817 0.875 0.841 0.880</cell></row><row><cell>3D UX-Net</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The medical segmentation decathlon</title>
		<author>
			<persName><forename type="first">M</forename><surname>Antonelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Commun</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">4128</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The liver tumor segmentation benchmark (LITS)</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page">102680</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.15242</idno>
		<title level="m">Re-parameterizing your optimizers rather than architectures</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Scaling up your kernels to 31x31: revisiting large kernel design in CNNs</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="11963" to="11975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">RepVGG: making VGGstyle convnets great again</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="13733" to="13742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Swin UNETR: swin transformers for semantic segmentation of brain tumors in MRI images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hatamizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-08999-2_22</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-08999-222" />
	</analytic>
	<monogr>
		<title level="m">BrainLes 2021</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Crimi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bakas</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">12962</biblScope>
			<biblScope unit="page" from="272" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">UNETR: transformers for 3D medical image segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hatamizadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="574" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">An international challenge to use artificial intelligence to define the state-of-the-art in kidney and kidney tumor segmentation in CT imaging</title>
		<author>
			<persName><forename type="first">N</forename><surname>Heller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Online convolutional re-parameterization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="568" to="577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Isensee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Jaeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Kohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Maier-Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="203" to="211" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Amos: a large-scale abdominal multi-organ benchmark for versatile medical image segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.08023</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Theory of spatial position and spatial frequency relations in the receptive fields of simple cells in the visual cortex</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Kulikowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Marčelja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biol. Cybern</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="187" to="198" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">D UX-Net: a large kernel volumetric convnet modernizing hierarchical transformer for medical image segmentation</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Landman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.15076</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Large-kernel attention for 3D medical image segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Nan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Del Ser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.11225</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">More convnets in the 2020s: scaling up kernels beyond 51x51 using sparsity</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.03620</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Swin transformer: hierarchical vision transformer using shifted windows</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A convnet for the 2020s</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="11976" to="11986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Abdomenct-1k: is abdominal organ segmentation a solved problem?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2021.3100536</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2021.3100536" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">DeepOrgan: multi-level deep convolutional networks for automated pancreas segmentation</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Roth</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24553-9_68</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24553-968" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9349</biblScope>
			<biblScope unit="page" from="556" to="564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Self-supervised pre-training of swin transformers for 3D medical image analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="20730" to="20740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">TransBTS: multimodal brain tumor segmentation using transformer</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87193-2_11</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87193-211" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12901</biblScope>
			<biblScope unit="page" from="109" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">nnFormer: interleaved transformer for volumetric segmentation</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.03201</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
