<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ACTION++: Improving Semi-supervised Medical Image Segmentation with Adaptive Anatomical Contrast</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Chenyu</forename><surname>You</surname></persName>
							<email>chenyu.you@yale.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Yale University</orgName>
								<address>
									<settlement>New Haven</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Weicheng</forename><surname>Dai</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Radiology and Biomedical Imaging</orgName>
								<orgName type="institution">Yale University</orgName>
								<address>
									<settlement>New Haven</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yifei</forename><surname>Min</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Statistics and Data Science</orgName>
								<orgName type="institution">Yale University</orgName>
								<address>
									<settlement>New Haven</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lawrence</forename><surname>Staib</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Yale University</orgName>
								<address>
									<settlement>New Haven</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Radiology and Biomedical Imaging</orgName>
								<orgName type="institution">Yale University</orgName>
								<address>
									<settlement>New Haven</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Biomedical Engineering</orgName>
								<orgName type="institution">Yale University</orgName>
								<address>
									<settlement>New Haven</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jas</forename><surname>Sekhon</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Statistics and Data Science</orgName>
								<orgName type="institution">Yale University</orgName>
								<address>
									<settlement>New Haven</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Department of Political Science</orgName>
								<orgName type="institution">Yale University</orgName>
								<address>
									<settlement>New Haven</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">James</forename><forename type="middle">S</forename><surname>Duncan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Yale University</orgName>
								<address>
									<settlement>New Haven</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Radiology and Biomedical Imaging</orgName>
								<orgName type="institution">Yale University</orgName>
								<address>
									<settlement>New Haven</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Biomedical Engineering</orgName>
								<orgName type="institution">Yale University</orgName>
								<address>
									<settlement>New Haven</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Department of Statistics and Data Science</orgName>
								<orgName type="institution">Yale University</orgName>
								<address>
									<settlement>New Haven</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ACTION++: Improving Semi-supervised Medical Image Segmentation with Adaptive Anatomical Contrast</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="194" to="205"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">762083457CBA095FBD36994E683B8BE1</idno>
					<idno type="DOI">10.1007/978-3-031-43901-8_19</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T10:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Semi-Supervised Learning</term>
					<term>Contrastive Learning</term>
					<term>Imbalanced Learning</term>
					<term>Long-tailed Medical Image Segmentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Medical data often exhibits long-tail distributions with heavy class imbalance, which naturally leads to difficulty in classifying the minority classes (i.e., boundary regions or rare objects). Recent work has significantly improved semi-supervised medical image segmentation in long-tailed scenarios by equipping them with unsupervised contrastive criteria. However, it remains unclear how well they will perform in the labeled portion of data where class distribution is also highly imbalanced.</p><p>In this work, we present ACTION++, an improved contrastive learning framework with adaptive anatomical contrast for semi-supervised medical segmentation. Specifically, we propose an adaptive supervised contrastive loss, where we first compute the optimal locations of class centers uniformly distributed on the embedding space (i.e., off-line), and then perform online contrastive matching training by encouraging different class features to adaptively match these distinct and uniformly distributed class centers. Moreover, we argue that blindly adopting a constant temperature τ in the contrastive loss on long-tailed medical data is not optimal, and propose to use a dynamic τ via a simple cosine schedule to yield better separation between majority and minority classes. Empirically, we evaluate ACTION++ on ACDC and LA benchmarks and show that it achieves state-of-the-art across two semi-supervised settings. Theoretically, we analyze the performance of adaptive anatomical contrast and confirm its superiority in label efficiency.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the recent development of semi-supervised learning (SSL) <ref type="bibr" target="#b2">[3]</ref>, rapid progress has been made in medical image segmentation, which typically learns rich anatomical representations from few labeled data and the vast amount of unlabeled data. Existing SSL approaches can be generally categorized into adversarial training <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b35">36]</ref>, deep co-training <ref type="bibr" target="#b22">[23,</ref><ref type="bibr">40]</ref>, mean teacher schemes <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr">39]</ref>, multi-task learning <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b21">22]</ref>, and contrastive learning <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b36">37]</ref>. Contrastive learning (CL) has become a remarkable approach to enhance semi-supervised medical image segmentation performance without significantly increasing the amount of parameters and annotation costs <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b34">35]</ref>. In realworld clinical scenarios, since the classes in medical images follow the Zipfian distribution <ref type="bibr">[41]</ref>, the medical datasets usually show a long-tailed, even heavytailed class distribution, i.e., some minority (tail) classes involving significantly fewer pixel-level training instances than other majority (head) classes, as illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>. Such imbalanced scenarios are usually very challenging for CL methods to address, leading to noticeable performance drop <ref type="bibr" target="#b17">[18]</ref>.</p><p>To address long-tail medical segmentation, our motivations come from the following two perspectives in CL training schemes <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b34">35]</ref>: ❶ Training objective -the main focus of existing approaches is on designing proper unsupervised contrastive loss in learning high-quality representations for long-tail medical segmentation. While extensively explored in the unlabeled portion of long-tail medical data, supervised CL has rarely been studied from empirical and theoretical perspectives, which will be one of the focuses in this work; ❷ Temperature scheduler -the temperature parameter τ , which controls the strength of attraction and repulsion forces in the contrastive loss <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, has been shown to play a crucial role in learning useful representations. It is affirmed that a large τ emphasizes anatomically meaningful group-wise patterns by group-level discrimination, whereas a small τ ensures a higher degree of pixel-level (instance) discrimination <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b27">28]</ref>. On the other hand, as shown in <ref type="bibr" target="#b24">[25]</ref>, group-wise discrimination often results in reduced model's instance discrimination capabilities, where the model will be biased to "easy" features instead of "hard" features. It is thus unfavorable for long-tailed medical segmentation to blindly treat τ as a constant hyperparameter, and a dynamic temperature parameter for CL is worth investigating. In this paper, we introduce ACTION++, which further optimizes anatomically group-level and pixel-level representations for better head and tail class separations, on both labeled and unlabeled medical data. Specifically, we devise two strategies to improve overall segmentation quality by focusing on the two aforementioned perspectives: <ref type="bibr" target="#b0">(1)</ref> we propose supervised adaptive anatomical contrastive learning (SAACL) for long-tail medical segmentation. To prevent the feature space from being biased toward the dominant head class, we first pre-compute the optimal locations of class centers uniformly distributed on the embedding space (i.e., off-line), and then perform online contrastive matching training by encouraging different class features to adaptively match these distinct and uniformly distributed class centers; (2) we find that blindly adopting the constant temperature τ in the contrastive loss can negatively impact the segmentation performance. Inspired by an average distance maximization perspective, we leverage a dynamic τ via a simple cosine schedule, resulting in significant improvements in the learned representations. Both of these enable the model to learn a balanced feature space that has similar separability for both the majority (head) and minority (tail) classes, leading to better generalization in long-tail medical data. We evaluated our ACTION++ on the public ACDC and LA datasets <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b30">31]</ref>. Extensive experimental results show that our ACTION++ outperforms prior methods by a significant margin and sets the new state-of-the-art across two semi-supervised settings. We also theoretically show the superiority of our method in label efficiency (Appendix A). Code is released at here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overview</head><p>Problem Statement. Given a medical image dataset (X, Y ), our goal is to train a segmentation model F that can provide accurate predictions that assign each pixel to their corresponding K-class segmentation labels.</p><p>Setup. Figure <ref type="figure" target="#fig_1">2</ref> illustrates an overview of ACTION++. By default, we build this work upon ACTION pipeline <ref type="bibr" target="#b34">[35]</ref>, the state-of-the-art CL framework for semi-supervised medical image segmentation. The backbone model adopts the student-teacher framework that shares the same architecture, and the parameters of the teacher are the exponential moving average of the student's parameters. Hereinafter, we adopt their model as our backbone and briefly summarize its major components: ( <ref type="formula" target="#formula_1">1</ref> Global and Local Pre-training. <ref type="bibr" target="#b34">[35]</ref> first creates two types of anatomical views as follows: (1) augmented viewsx<ref type="foot" target="#foot_0">1</ref> and x 2 are augmented from the unlabeled input scan with two separate data augmentation operators; (2) mined views -n samples (i.e., x 3 ) are randomly sampled from the unlabeled portion with additional augmentation. The pairs x 1 , x 2 are then processed by studentteacher networks [F s , F t ] that share the same architecture and weight, and similarly, x 3 is encoded by F t . Their global latent features after the encoder E (i.e., h 1 , h 2 , h 3 ) and local output features after decoder D (i.e., f 1 , f 2 , f 3 ) are encoded by the two-layer nonlinear projectors, generating global and local embeddings v g and v l . v from F s are separately encoded by the non-linear predictor, producing w in both global and local manners 1 . Third, the relational similarities between augmented and mined views are processed by SoftMax function as follows:</p><formula xml:id="formula_0">u s = log exp sim w 1 ,v 3 /τs N n=1 exp sim w 1 ,v 3 n /τs , u t = log exp sim w 2 ,v 3 /τt N n=1 exp sim w 2 ,v 3 n /τt ,</formula><p>where τ s and τ t are two temperature parameters. Finally, we minimize the unsupervised instance discrimination loss (i.e., Kullback-Leibler divergence KL) as:</p><formula xml:id="formula_1">L inst = KL(u s ||u t ). (<label>1</label></formula><formula xml:id="formula_2">)</formula><p>We formally summarize the pretraining objective as the equal combination of the global and local L inst , and supervised segmentation loss L sup (i.e., equal combination of Dice loss and cross-entropy loss).</p><p>Anatomical Contrast Fine-Tuning. The underlying motivation for the finetuning stage is that it reduces the vulnerability of the pre-trained model to long-tailed unlabeled data. To mitigate the problem, <ref type="bibr" target="#b34">[35]</ref> proposed to fine-tune the model by anatomical contrast. First, the additional representation head ϕ is used to provide dense representations with the same size as the input scans.</p><p>Then, <ref type="bibr" target="#b34">[35]</ref> explore pulling queries r q ∈R to be similar to the positive keys r + k ∈R, and push apart the negative keys r - k ∈R. The AnCo loss is defined as follows:</p><formula xml:id="formula_3">L anco = c∈C rq∼R c q -log exp(r q • r c,+ k /τ an ) exp(r q • r c,+ k /τ an ) + r - k ∼R c k exp(r q • r - k /τ an ) ,<label>(2)</label></formula><p>where C denotes a set of all available classes in the current mini-batch, and τ an is a temperature hyperparameter. For class c, we select a query representation set R c q , a negative key representation set R c k whose labels are not in class c, and the positive key r c,+ k which is the c-class mean representation. Given P is a set including all pixel coordinates with the same size as R, these queries and keys can be defined as:</p><formula xml:id="formula_4">R c q = [i,j]∈A 1(y [i,j] = c) r [i,j] , R c k = [i,j]∈A 1(y [i,j] = c) r [i,j] , r c,+ k = 1 |R c q | rq∈R c q r q .</formula><p>We formally summarize the fine-tuning objective as the equal combination of unsupervised L anco , unsupervised cross-entropy loss L unsup , and supervised segmentation loss L sup . For more details, we refer the reader to <ref type="bibr" target="#b34">[35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Supervised Adaptive Anatomical Contrastive Learning</head><p>The general efficacy of anatomical contrast on long-tail unlabeled data has previously been demonstrated by the authors of <ref type="bibr" target="#b34">[35]</ref>. However, taking a closer look, we observe that the well-trained F shows a downward trend in performance, which often fails to classify tail classes on labeled data, especially when the data shows long-tailed class distributions. This indicates that such well-trained F is required to improve the segmentation capabilities in long-tailed labeled data. To this end, inspired by <ref type="bibr" target="#b16">[17]</ref> tailored for the image classification tasks, we introduce supervised adaptive anatomical contrastive learning (SAACL), a training framework for generating well-separated and uniformly distributed latent feature representations for both the head and tail classes. It consists of three main steps, which we describe in the following.</p><p>Anatomical Center Pre-computation. We first pre-compute the anatomical class centers in latent representation space. The optimal class centers are chosen as K positions from the unit sphere S d-1 = {v ∈ R d : v 2 = 1} in the ddimensional space. To encourage good separability and uniformity, we compute the class centers {ψ c } K c=1 by minimizing the following uniformity loss L unif :</p><formula xml:id="formula_5">L unif ({ψ c } K c=1 ) = K c=1 log K c =1 exp(ψ c • ψ c /τ ) . (<label>3</label></formula><formula xml:id="formula_6">)</formula><p>In our implementation, we use gradient descent to search for the optimal class centers constrained to the unit sphere S d-1 , which are denoted by {ψ c } K c=1 . Furthermore, the latent dimension d is a hyper-parameter, which we set such that d K to ensure the solution found by gradient descent indeed maximizes the minimum distance between any two class centers <ref type="bibr" target="#b5">[6]</ref>. It is also known that any analytical minimizers of Eq. 3 form a perfectly regular K-vertex inscribed simplex of the sphere S d-1 <ref type="bibr" target="#b5">[6]</ref>. We emphasize that this first step of pre-computation of class centers is completely off-line as it does not require any training data.</p><p>Adaptive Allocation. As the second step, we explore adaptively allocating these centers among classes. This is a combinatorial optimization problem and an exhaustive search of all choices would be computationally prohibited. Therefore, we draw intuition from the empirical mean in the K-means algorithm and adopt an adaptive allocation scheme to iteratively search for the optimal allocation during training. Specifically, consider a batch B = {B 1 , • • • , B K } where B c denotes a set of samples in a batch with class label c,</p><formula xml:id="formula_7">for c = 1, • • • , K. Define φ c (B) = i∈Bc φ i /</formula><p>i∈Bc φ i 2 be the empirical mean of class c in current batch, where φ i is the feature embedding of sample i. We compute assignment π by minimizing the distance between pre-computed class centers and the empirical means:</p><formula xml:id="formula_8">π = arg min π K c=1 ψ π(c) -φ c 2 . (<label>4</label></formula><formula xml:id="formula_9">)</formula><p>In implementation, the empirical mean is updated using moving average. That is, for iteration t, we first compute the empirical mean φ c (B) for batch B as described above, and then update by</p><formula xml:id="formula_10">φ c ← (1 -η)φ c + ηφ c (B).</formula><p>Adaptive Anatomical Contrast. Finally, the allocated class centers are wellseparated and should maintain the semantic relation between classes. To utilize these optimal class centers, we want to induce the feature representation of samples from each class to cluster around the corresponding pre-computed class center. To this end, we adopt a supervised contrastive loss for the label portion of the data. Specifically, given a batch of pixel-feature-label tuples</p><formula xml:id="formula_11">{(ω i , φ i , y i )} n i=1</formula><p>where ω i is the i-th pixel in the batch, φ i is the feature of the pixel and y i is its label, we define supervised adaptive anatomical contrastive loss for pixel i as:</p><formula xml:id="formula_12">Laaco = -1 n n i=1 ⎛ ⎜ ⎝ φ + i log exp(φ i • φ + i /τsa) φ j exp(φ i • φ j /τsa) + λa log exp(φ i • ν i /τsa) φ j exp(φ i • φ j /τsa) ⎞ ⎟ ⎠ , (<label>5</label></formula><formula xml:id="formula_13">)</formula><p>where</p><formula xml:id="formula_14">ν i = ψ π (yi)</formula><p>is the pre-computed center of class y i . The first term in Eq. 5 is supervised contrastive loss, where the summation over φ + i refers to the uniformly sampled positive examples from pixels in batch with label equal to y i . The summation over φ j refers to all features in the batch excluding φ i . The second term is contrastive loss with the positive example being the pre-computed optimal class center.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Anatomical-Aware Temperature Scheduler (ATS)</head><p>Training with a varying τ induces a more isotropic representation space, wherein the model learns both group-wise and instance-specific features <ref type="bibr" target="#b11">[12]</ref>. To this end, we are inspired to use an anatomical-aware temperature scheduler in both the supervised and the unsupervised contrastive losses, where the temperature parameter τ evolves within the range [τ -, τ + ] for τ + &gt; τ -. Specifically, for iteration t = 1, • • • , T with T being the total number of iterations, we set τ t as:</p><formula xml:id="formula_15">τ t = τ -+ 0.5(1 + cos(2πt/T ))(τ + -τ -).<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>Experimental Setup. We evaluate ACTION++ on two benchmark datasets: the LA dataset <ref type="bibr" target="#b30">[31]</ref> and the ACDC dataset <ref type="bibr" target="#b0">[1]</ref>. The LA dataset consists of 100 gadolinium-enhanced MRI scans, with the fixed split <ref type="bibr" target="#b28">[29]</ref> using 80 and 20 scans for training and validation. The ACDC dataset consists of 200 cardiac cine MRI scans from 100 patients including three segmentation classes, i.e., left ventricle (LV), myocardium (Myo), and right ventricle (RV), with the fixed split<ref type="foot" target="#foot_1">2</ref> using 70, 10, and 20 patients' scans for training, validation, and testing. For all our experiments, we follow the identical setting in <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr">39]</ref>, and perform evaluations under two label settings (i.e., 5% and 10% ) for both datasets.</p><p>Implementation Details. We use an SGD optimizer for all experiments with a learning rate of 1e-2, a momentum of 0.9, and a weight decay of 0.0001. Following <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr">39</ref>] on both datasets, all inputs were normalized as zero mean and unit variance. The data augmentations are rotation and flip operations. Our work is built on ACTION <ref type="bibr" target="#b34">[35]</ref>, thus we follow the identical model setting except for temperature parameters because they are of direct interest to us. For the sake of completeness, we refer the reader to <ref type="bibr" target="#b34">[35]</ref> for more details. We set λ a , d as 0.2, 128, and regarding all τ , we use τ + = 1.0 and τ -= 0.1 if not stated otherwise.</p><p>On ACDC, we use the U-Net model <ref type="bibr" target="#b25">[26]</ref> as the backbone with a 2D patch size of 256 × 256 and batch size of 8. For pre-training, the networks are trained for 10K iterations; for fine-tuning, 20K iterations. On LA, we use the V-Net <ref type="bibr" target="#b20">[21]</ref>  Main Results. We compare our ACTION++ with current state-of-the-art SSL methods, including UAMT [39], SASSNet <ref type="bibr" target="#b15">[16]</ref>, DTC <ref type="bibr" target="#b18">[19]</ref>, URPC <ref type="bibr" target="#b19">[20]</ref>, MC-Net <ref type="bibr" target="#b29">[30]</ref>, SS-Net <ref type="bibr" target="#b28">[29]</ref>, and ACTION <ref type="bibr" target="#b34">[35]</ref>, and the supervised counterparts (UNet <ref type="bibr" target="#b25">[26]</ref>/VNet <ref type="bibr" target="#b20">[21]</ref>) trained with Full/Limited supervisions -using their released code. To evaluate 3D segmentation ability, we use Dice coefficient (DSC) and Average Surface Distance (ASD). Table <ref type="table" target="#tab_2">2</ref> and Table <ref type="table" target="#tab_0">1</ref> display the results on the public ACDC and LA datasets for the two labeled settings, respectively. We next discuss our main findings as follows. ( <ref type="formula" target="#formula_1">1</ref>) LA: As shown in Table <ref type="table" target="#tab_0">1</ref>, our method generally presents better performance than the prior SSL methods under all settings. Figure <ref type="figure">4</ref> (Appendix) also shows that our model consistently outperforms all other competitors, especially in the boundary region; (2) ACDC: As Table <ref type="table" target="#tab_2">2</ref> shows, ACTION++ achieves the best segmentation performance in terms of Dice and ASD, consistently outperforming the previous SSL methods across two labeled settings. In Fig. <ref type="figure">3</ref> (Appendix), we can observe that ACTION++ can yield the segmentation boundaries accurately, even for very challenging regions (i.e., RV and Myo). This suggests that ACTION++ is inherently better at long-tailed learning, in addition to being a better segmentation model in general. Ablation Study. We first perform ablation studies on LA with 10% label ratio to evaluate the importance of different components. Table <ref type="table" target="#tab_3">3</ref> shows the effectiveness of supervised adaptive anatomical contrastive learning (SAACL). Table <ref type="table" target="#tab_4">4</ref> (Appendix) indicates that using anatomical-aware temperature scheduler (ATS) and SAACL yield better performance in both pre-training and fine-tuning stages.</p><p>We then theoretically show the superiority of our method in Appendix A. Finally, we conduct experiments to study the effects of cosine boundaries, cosine period, different methods of varying τ , and λ a in Table <ref type="table">5</ref>, Table <ref type="table">6</ref> (Appendix), respectively. Empirically, we find that using our settings (i.e., τ -= 0.1, τ + = 1.0, T /#iterations=1.0, cosine scheduler, λ a = 0.2) attains optimal performance (Table <ref type="table" target="#tab_4">4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we proposed ACTION++, an improved contrastive learning framework with adaptive anatomical contrast for semi-supervised medical segmentation. Our work is inspired by two intriguing observations that, besides the unlabeled data, the class imbalance issue exists in the labeled portion of medical data and the effectiveness of temperature schedules for contrastive learning on longtailed medical data. Extensive experiments and ablations demonstrated that our model consistently achieved superior performance compared to the prior semisupervised medical image segmentation methods under different label ratios. Our theoretical analysis also revealed the robustness of our method in label efficiency. In future, we will validate CT/MRI datasets with more foreground labels and try t-SNE.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Examples of two benchmarks (i.e., ACDC and LA) with imbalanced class distribution. From left to right: input image, ground-truth segmentation map, class distribution chart, training data feature distribution for multiple classes.</figDesc><graphic coords="2,56,97,174,14,338,98,61,51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Overview of ACTION++: (1) global and local pre-training with proposed anatomical-aware temperature scheduler, (2) our proposed adaptive anatomical contrast fine-tuning, which first pre-computes the optimal locations of class centers uniformly distributed on the embedding space (i.e., off-line), and then performs online contrastive matching training by encouraging different class features to adaptively match these distinct and uniformly distributed class centers with respect to anatomical features.</figDesc><graphic coords="3,41,79,54,29,340,33,155,41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>) global contrastive distillation pre-training; (2) local contrastive distillation pre-training; and (3) anatomical contrast fine-tuning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Quantitative comparison (DSC[%]/ASD[voxel]) for LA under two unlabeled settings (5% or 10%). All experiments are conducted as<ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b34">35,</ref> 39]  in the identical setting for fair comparisons. The best results are indicated in bold.</figDesc><table><row><cell>VNet-F</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>as the backbone. For training, we randomly crop 112 × 112 × 80 patches and the batch size is 2. For pre-training, the networks are trained for 5K iterations. For fine-tuning, the networks are for 15K iterations. For testing, we adopt a sliding window strategy with a fixed stride(18 × 18 × 4). All experiments are conducted in the same environments with fixed random seeds (Hardware: Single NVIDIA GeForce RTX 3090 GPU; Software: PyTorch 1.10.2+cu113, and Python 3.8.11).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Quantitative comparison (DSC[%]/ASD[voxel]) for ACDC under two unlabeled settings (5% or 10%). All experiments are conducted as<ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b34">35,</ref> 39]   in the identical setting for fair comparisons. The best results are indicated in bold.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>3 Labeled (5%)</cell><cell></cell><cell></cell><cell cols="2">7 Labeled (10%)</cell><cell></cell></row><row><cell>Method</cell><cell>Average</cell><cell>RV</cell><cell>Myo</cell><cell>LV</cell><cell>Average</cell><cell>RV</cell><cell>Myo</cell><cell>LV</cell></row><row><cell>UNet-F [26]</cell><cell cols="3">91.5/0.996 90.5/0.606 88.8/0.941</cell><cell>94.4/1.44</cell><cell cols="3">91.5/0.996 90.5/0.606 88.8/0.941</cell><cell>94.4/1.44</cell></row><row><cell>UNet-L</cell><cell>51.7/13.1</cell><cell>36.9/30.1</cell><cell>54.9/4.27</cell><cell>63.4/5.11</cell><cell>79.5/2.73</cell><cell>65.9/0.892</cell><cell>82.9/2.70</cell><cell>89.6/4.60</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Ablation studies of Supervised Adaptive Anatomical Contrast (SAACL).</figDesc><table><row><cell>Method</cell><cell cols="2">DSC[%]↑ ASD[voxel]↓</cell></row><row><cell>KCL [9]</cell><cell>88.4</cell><cell>2.19</cell></row><row><cell>CB-KCL [10]</cell><cell>86.9</cell><cell>2.47</cell></row><row><cell>SAACL (Ours)</cell><cell>89.9</cell><cell>1.74</cell></row><row><cell>SAACL (random assign)</cell><cell>88.0</cell><cell>2.79</cell></row><row><cell cols="2">SAACL (adaptive allocation) 89.9</cell><cell>1.74</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Effect of cosine boundaries in with the largest difference between τ -and τ + .</figDesc><table><row><cell>τ -τ +</cell></row><row><cell>0.2 0.3 0.4 0.5 1.0</cell></row><row><cell>0.07 84.1 85.0 86.9 87.9 89.7</cell></row><row><cell>0.1 84.5 85.9 87.1 88.3 89.9</cell></row><row><cell>0.2 84.2 84.4 85.8 87.1 87.6</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>For simplicity, we omit details of local instance discrimination in the following.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://github.com/HiLab-git/SSL4MIS/tree/master/data/ACDC.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8 19.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep learning techniques for automatic MRI cardiac multistructures segmentation and diagnosis: is the problem solved?</title>
		<author>
			<persName><forename type="first">O</forename><surname>Bernard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="2514" to="2525" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Contrastive learning of global and local features for medical image segmentation with limited annotations</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chaitanya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Erdil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Karani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Konukoglu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semi-supervised learning</title>
		<author>
			<persName><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<editor>
			<persName><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="542" to="542" />
			<date type="published" when="2006">2006. 2009</date>
		</imprint>
	</monogr>
	<note>book reviews</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno>PMLR</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
		<respStmt>
			<orgName>ICML</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Improved baselines with momentum contrastive learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Dissecting supervised contrastive learning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Graf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Niethammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kwitt</surname></persName>
		</author>
		<editor>ICML. PMLR</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Interpretable minority synthesis for imbalanced classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">F</forename><surname>Tzeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Towards the generalization of contrastive selfsupervised learning</title>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.00743</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Exploring balanced feature spaces for representation learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Decoupling representation and classifier for long-tailed recognition</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.09217</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Curriculum semi-supervised segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kervadec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dolz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">É</forename><surname>Granger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ben Ayed</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32245-8_63</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32245-863" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11765</biblScope>
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Temperature schedules for self-supervised contrastive methods on long-tail data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kukleva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Böhle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Sar: self-adaptive refinement on pseudo labels for multiclass-imbalanced semi-supervised learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">N</forename><surname>Chuah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="4091" to="4100" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Smoothed adaptive weighting for imbalanced semi-supervised learning: Improve reliability against unknown distribution data</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gunawan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C S</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">N</forename><surname>Chuah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="11828" to="11843" />
		</imprint>
		<respStmt>
			<orgName>ICML</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Joint semi-supervised and active learning for segmentation of gigapixel pathology images with cost-effective labeling</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">N</forename><surname>Dugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">N</forename><surname>Chuah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="591" to="600" />
		</imprint>
		<respStmt>
			<orgName>ICCV</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Shape-aware semi-supervised 3D semantic segmentation for medical images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59710-8_54</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59710-8" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12261</biblScope>
			<biblScope unit="page">54</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Targeted supervised contrastive learning for long-tailed recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Analyzing overfitting under class imbalance in neural networks for image segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kamnitsas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Medi. Imaging</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1065" to="1077" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semi-supervised medical image segmentation through dual-task consistency</title>
		<author>
			<persName><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Efficient semi-supervised gross target volume of nasopharyngeal carcinoma segmentation via uncertainty rectified pyramid consistency</title>
		<author>
			<persName><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87196-3_30</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87196-330" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12902</biblScope>
			<biblScope unit="page" from="318" to="329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">V-net: fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Ahmadi</surname></persName>
		</author>
		<editor>3DV</editor>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="565" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Generalizable semi-supervised learning strategies for multiple learning tasks using 1-d biomedical signals</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Siefkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">N</forename><surname>Chuah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS Workshop on Learning from Time Series for Health</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Deep co-training for semisupervised image recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Q</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.07118</idno>
		<title level="m">Information-guided pixel augmentation for pixel-wise contrastive learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Can contrastive learning avoid shortcut solutions?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Batmanghelich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-428" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Mean teachers are better role models: weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>NeurIPS</publisher>
			<biblScope unit="page" from="1195" to="1204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Understanding the behaviour of contrastive loss</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Exploring smoothness and class-separation for semi-supervised medical image segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16443-9_4</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16443-94" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13435</biblScope>
			<biblScope unit="page" from="34" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Semi-supervised left atrium segmentation with mutual consistency training</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87196-3_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87196-328" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12902</biblScope>
			<biblScope unit="page" from="297" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A global benchmark of algorithms for segmenting the left atrium from late gadolinium-enhanced cardiac magnetic resonance imaging</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page">101832</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">SegAN: adversarial network with multi-scale l 1 loss for medical image segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroinformatics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="383" to="392" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Mine your own anatomy: Revisiting medical image segmentation with extremely limited labels</title>
		<author>
			<persName><forename type="first">C</forename><surname>You</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.13476</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Rethinking semi-supervised medical image segmentation: a variancereduction perspective</title>
		<author>
			<persName><forename type="first">C</forename><surname>You</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.01735</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Bootstrapping semi-supervised medical image segmentation with anatomical-aware contrastive distillation</title>
		<author>
			<persName><forename type="first">C</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Staib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Duncan</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-34048-2_49</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-34048-2" />
	</analytic>
	<monogr>
		<title level="m">IPMI 2023</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Frangi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Wassermann</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">13939</biblScope>
			<biblScope unit="page">49</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Class-aware adversarial transformers for medical image segmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Momentum contrastive voxel-wise representation learning for semi-supervised volumetric medical image segmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Staib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Duncan</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16440-8_61</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16440-8" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13434</biblScope>
			<biblScope unit="page">61</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">SimCVD: simple contrastive voxel-wise representation distillation for semi-supervised medical image segmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Staib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Duncan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="2228" to="2237" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
