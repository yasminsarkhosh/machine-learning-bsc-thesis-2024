<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Morphology-Inspired Unsupervised Gland Segmentation via Selective Semantic Grouping</title>
				<funder ref="#_JxaY96J">
					<orgName type="full">Foshan HKUST Projects</orgName>
				</funder>
				<funder ref="#_KJKsu5e">
					<orgName type="full">Hong Kong Innovation and Technology Fund</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Qixiang</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic and Computer Engineering</orgName>
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yi</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic and Computer Engineering</orgName>
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xue</forename><surname>Cheng</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Southeast University</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaomeng</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic and Computer Engineering</orgName>
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Morphology-Inspired Unsupervised Gland Segmentation via Selective Semantic Grouping</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2F7426A50188101914648A56DDCAA54E</idno>
					<idno type="DOI">10.1007/978-3-031-43901-827.</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T10:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Whole Slide Image</term>
					<term>Unsupervised Gland Segmentation</term>
					<term>Morphology-inspired Learning</term>
					<term>Semantic Grouping</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Designing deep learning algorithms for gland segmentation is crucial for automatic cancer diagnosis and prognosis. However, the expensive annotation cost hinders the development and application of this technology. In this paper, we make a first attempt to explore a deep learning method for unsupervised gland segmentation, where no manual annotations are required. Existing unsupervised semantic segmentation methods encounter a huge challenge on gland images. They either over-segment a gland into many fractions or under-segment the gland regions by confusing many of them with the background. To overcome this challenge, our key insight is to introduce an empirical cue about gland morphology as extra knowledge to guide the segmentation process. To this end, we propose a novel Morphology-inspired method via Selective Semantic Grouping. We first leverage the empirical cue to selectively mine out proposals for gland sub-regions with variant appearances. Then, a Morphology-aware Semantic Grouping module is employed to summarize the overall information about glands by explicitly grouping the semantics of their sub-region proposals. In this way, the final segmentation network could learn comprehensive knowledge about glands and produce well-delineated and complete predictions. We conduct experiments on the GlaS dataset and the CRAG dataset. Our method exceeds the second-best counterpart by over 10.56% at mIOU.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Accurate gland segmentation from whole slide images (WSIs) plays a crucial role in the diagnosis and prognosis of cancer, as the morphological features of glands can provide valuable information regarding tumor aggressiveness <ref type="bibr" target="#b10">[11]</ref>. With the Prior USS methods in medical image research <ref type="bibr" target="#b1">[2]</ref> and natural image research <ref type="bibr" target="#b5">[6]</ref> vs. Our MSSG. Green and orange regions denote the glands and the background respectively. (Color figure online) emergence of deep learning (DL), there has been a growing interest in developing DL-based methods for semantic-level <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b35">36]</ref> and instance-level <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b34">35]</ref> gland segmentation. However, such methods typically rely on large-scale annotated image datasets, which usually require significant effort and expertise from pathologists and can be prohibitively expensive <ref type="bibr" target="#b27">[28]</ref>.</p><p>To reduce the annotation cost, developing annotation-efficient methods for semantic-level gland segmentation has attracted much attention <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b36">37]</ref>. Recently, some researchers have explored weakly supervised semantic segmentation methods which use weak annotations (e.g., Bound Box <ref type="bibr" target="#b36">[37]</ref> and Patch Tag <ref type="bibr" target="#b17">[18]</ref>) instead of pixel-level annotations to train a gland segmentation network. However, these weak annotations are still laborious and require expert knowledge <ref type="bibr" target="#b36">[37]</ref>. To address this issue, previous works have exploited conventional clustering <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref> and metric learning <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b28">29]</ref> to design annotation-free methods for gland segmentation. However, the performance of these methods can vary widely, especially in cases of malignancy. This paper focuses on unsupervised gland segmentation, where no annotations are required during training and inference.</p><p>One potential solution is to adopt unsupervised semantic segmentation (USS) methods which have been successfully applied to medical image research and natural image research. On the one hand, existing USS methods have shown promising results in various medical modalities, e.g., magnetic resonance images <ref type="bibr" target="#b18">[19]</ref>,</p><p>x-ray images <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b14">15]</ref> and dermoscopic images <ref type="bibr" target="#b1">[2]</ref>. However, directly utilizing these methods to segment glands could lead to over-segment results where a gland is segmented into many fractions rather than being considered as one target (see Fig. <ref type="figure" target="#fig_0">1(b)</ref>). This is because these methods are usually designed to be extremely sensitive to color <ref type="bibr" target="#b1">[2]</ref>, while gland images present a unique challenge due to their highly dense and complex tissues with intricate color distribution <ref type="bibr" target="#b17">[18]</ref>. On the other hand, prior USS methods for natural images can be broadly categorized into coarse-to-fine-grained <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b30">31]</ref> and end-to-end (E2E) cluster-ing <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b16">17]</ref>. The former ones typically rely on pre-generated coarse masks (e.g., super-pixel proposals <ref type="bibr" target="#b15">[16]</ref>, salience masks <ref type="bibr" target="#b30">[31]</ref>, and self-attention maps <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b20">21]</ref>) as prior, which is not always feasible on gland images. The E2E clustering methods, however, produce under-segment results on gland images by confusing many gland regions with the background; see Fig. <ref type="bibr">1(b)</ref>. This is due to the fact that E2E clustering relies on the inherent connections between pixels of the same class <ref type="bibr" target="#b32">[33]</ref>, and essentially, grouping similar pixels and separate dissimilar ones. Nevertheless, the glands are composed of different parts (gland border and interior epithelial tissues, see Fig. <ref type="figure" target="#fig_0">1(a)</ref>) with significant variations in appearance. Gland borders typically consist of dark-colored cells, whereas the interior epithelial tissues contain cells with various color distributions that may closely resemble those non-glandular tissues in the background. As such, the E2E clustering methods tend to blindly cluster pixels with similar properties and confuse many gland regions with the background, leading to under-segment results.</p><p>To tackle the above challenges, our solution is to incorporate an empirical cue about gland morphology as additional knowledge to guide gland segmentation. The cue can be described as: Each gland is comprised of a border region with high gray levels that surrounds the interior epithelial tissues. To this end, we propose a novel Morphology-inspired method via Selective Semantic Grouping, abbreviated as MSSG. To begin, we leverage the empirical cue to selectively mine out proposals for the two gland sub-regions with variant appearances. Then, considering that our segmentation target is the gland, we employ a Morphology-aware Semantic Grouping module to summarize the semantic information about glands by explicitly grouping the semantics of the sub-region proposals. In this way, we not only prioritize and dedicate extra attention to the target gland regions, thus avoiding under-segmentation; but also exploit the valuable morphology information hidden in the empirical cue, and force the segmentation network to recognize entire glands despite the excessive variance among the sub-regions, thus preventing over-segmentation. Ultimately, our method produces well-delineated and complete predictions; see Fig. <ref type="figure" target="#fig_0">1(b)</ref>.</p><p>Our contributions are as follows: (1) We identify the major challenge encountered by prior unsupervised semantic segmentation (USS) methods when dealing with gland images, and propose a novel MSSG for unsupervised gland segmentation. <ref type="bibr" target="#b1">(2)</ref> We propose to leverage an empirical cue to select gland sub-regions and explicitly group their semantics into a complete gland region, thus avoiding over-segmentation and under-segmentation in the segmentation results. <ref type="bibr" target="#b2">(3)</ref> We validate the efficacy of our MSSG on two public glandular datasets (i.e., the GlaS dataset <ref type="bibr" target="#b26">[27]</ref> and the CRAG dataset <ref type="bibr" target="#b12">[13]</ref>), and the experiment results demonstrate the effectiveness of our MSSG in unsupervised gland segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>The overall pipeline of MSSG is illustrated in Fig. <ref type="figure" target="#fig_1">2</ref>. The proposed method begins with a Selective Proposal Mining (SPM) module which generates a proposal map that highlights the gland sub-regions. The proposal map is then used to train a segmentation network. Meantime, a Morphology-aware Semantic Grouping (MSG) module is used to summarize the overall information about glands from their sub-region proposals. More details follow in the subsequent sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Selective Proposal Mining</head><p>Instead of generating pseudo-labels for the gland region directly from all the pixels of the gland images as previous works typically do, which could lead to over-segmentation and under-segmentation results, we propose using the empirical cue as extra hints to guide the proposal generation process.</p><p>Specifically, let the i th input image be denoted as X i ∈ R C×H×W , where H, W , and C refer to the height, width, and number of channels respectively. We first obtain a normalized feature map F i for X i from a shallow encoder f with 3 convolutional layers, which can be expressed as F i = f (X i ) 2 . We train the encoder in a self-supervised manner, and the loss function L consists of a typical self-supervised loss L SS , which is the cross-entropy loss between the feature map F i and the one-hot cluster label C i = arg max (F i ), and a spatial continuity loss L SC , which regularizes the vertical and horizontal variance among pixels within a certain area S to assure the continuity and completeness of the gland border regions (see Fig. <ref type="figure" target="#fig_0">1</ref> in the Supplementary Material). The expressions for L SS and L SC are given below:</p><formula xml:id="formula_0">LSS(Fi[:, h, w], Ci[:, h, w]) = - D d Ci[d, h, w] • ln Fi[d, h, w]</formula><p>(1)</p><formula xml:id="formula_1">LSC (Fi) = S,H-s,W -s s,h,w (Fi[:, h + s, w] -Fi[:, h, w]) 2 + (Fi[:, h, w + s] -Fi[:, h, w]) 2 .</formula><p>(2)</p><p>Then we employ K-means <ref type="bibr" target="#b23">[24]</ref> to cluster the feature map F i into 5 candidate regions, denoted as Y i = y i,1 ∈ R D×n0 , y i,2 ∈ R D×n2 , ..., y i,5 ∈ R D×n5 , where n 1 + n 2 + ... + n 5 equals the total number of pixels in the input image (H × W ). Sub-region Proposal Selection via the Empirical Cue. The aforementioned empirical cue is used to select proposals for the gland border and interior epithelial tissues from the candidate regions Y i . Particularly, we select the region with the highest average gray level as the proposal for the gland border. Then, we fill the areas surrounded by the gland border proposal and consider them as the proposal for the interior epithelial tissues, while the rest areas of the gland image are regarded as the background (i.e., non-glandular region). Finally, we obtain the proposal map P i ∈ R 3×H×W , which contains the two proposals for two gland sub-regions and one background proposal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Morphology-Aware Semantic Grouping</head><p>A direct merge of the two sub-region proposals to train a fully-supervised segmentation network may not be optimal for our case. Firstly, the two gland subregions exhibit significant variation in appearance, which can impede the segmentation network's ability to recognize them as integral parts of the same object. Secondly, the SPM module may produce proposals with inadequate highlighting of many gland regions, particularly the interior epithelial tissues, as shown in Fig. <ref type="figure" target="#fig_1">2</ref> where regions marked with × are omitted. Consequently, applying pixel-level cross-entropy loss between the gland image and the merged proposal map could introduce undesired noise into the segmentation network, thus leading to under-segment predictions with confusion between the glands and the background. As such, we propose two types of Morphology-aware Semantic Grouping (MSG) modules (i.e., MSG for Variation and MSG for Omission) to respectively reduce the confusion caused by the two challenges mentioned above and improve the overall accuracy and comprehensiveness of the segmentation results. The details of the two MSG modules are described as follows.</p><p>Here, we first slice the gland image and its proposal map into patches as inputs. Let the input patch and its corresponding sliced proposal map be denoted as X ∈ R C× Ĥ× Ŵ and P ∈ R 3× Ĥ× Ŵ . We can obtain the feature embedding map F which is derived as F = f feat ( X) and the prediction map X as X = f cls ( F ), where f feat and f cls refers to the feature extractor and pixel-wise classifier of the segmentation network respectively.</p><p>MSG for Variation is designed to mitigate the adverse impact of appearance variation between the gland sub-regions. It regulates the pixel-level feature embeddings of the two sub-regions by explicitly reducing the distance between them in the embedding space. Specifically, according to the proposal map P , we divide the pixel embeddings in F ∈ R D× Ĥ× Ŵ into gland border set G = g 0 , g 1 , ..., g kg , interior epithelial tissue set I = {i 0 , i 1 , ..., i ki } and non-glandular (i.e., background) set N = {n 0 , n 1 , ..., n kn }, where k g + k i + k n = Ĥ × Ŵ . Then, we use the average of the pixel embeddings in gland border set G as the alignment anchor and pull all pixels of I towards the anchor:</p><formula xml:id="formula_2">LMSGV = 1 I i∈I i - 1 G g∈G g 2 .</formula><p>(</p><formula xml:id="formula_3">)<label>3</label></formula><p>MSG for Omission is designed to overcome the problem of partial omission in the proposals. It identifies and relabels the overlooked gland regions in the proposal map and groups them back into the gland semantic category. To achieve this, for each pixel n in the non-glandular (i.e., background) set N , two similarities are computed with the gland sub-regions G and I respectively:</p><formula xml:id="formula_4">S G n = 1 G g∈G g g 2 • n n 2 , S I n = 1 I i∈I i i 2 • n n 2 . (<label>4</label></formula><formula xml:id="formula_5">)</formula><p>S G n (or S I n ) represents the similarity between the background pixel n and gland borders (or interior epithelial tissues). If either of them is higher than a preset threshold β (set to 0.7), we consider n as an overlooked pixel of gland borders (or interior epithelial tissues), and relabel n to G (or I). In this way, we could obtain a refined proposal map RP . Finally, we impose a pixel-level cross-entropy loss on the prediction and refined proposal RP to train the segmentation network:</p><formula xml:id="formula_6">LMSGO = - Ĥ, Ŵ ĥ, ŵ RP [:, ĥ, ŵ] • ln X[:, ĥ, ŵ],<label>(5)</label></formula><p>The total objective function L for training the segmentation network can be summarized as follows: <ref type="bibr" target="#b5">(6)</ref> where λ v (set to 1) is the coefficient.</p><formula xml:id="formula_7">L = LMSGO + λvLMSGV ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>We evaluate our MSSG on The Gland Segmentation Challenge (GlaS) dataset <ref type="bibr" target="#b26">[27]</ref> and The Colorectal Adenocarcinoma Gland (CRAG) dataset <ref type="bibr" target="#b12">[13]</ref>. The GlaS dataset contains 165 H&amp;E-stained histopathology patches extracted from 16 WSIs. The CRAG dataset owns 213 H&amp;E-stained histopathology patches extracted from 38 WSIs. The CRAG dataset has more irregular malignant glands, which makes it more difficult than GlaS, and we would like to emphasize that the results on CRAG are from the model trained on GlaS without retraining.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation Details</head><p>The experiments are conducted on four RTX 3090 GPUs. For the SPM, a 3layer encoder is trained for each training sample. Each convolutional layer uses a 3 × 3 convolution with a stride of 1 and a padding size of 1. The encoder is trained for 50 iterations using an SGD optimizer with a polynomial decay policy and an initial learning rate of 1e-2. For the MSG, MMSegmentation <ref type="bibr" target="#b6">[7]</ref> is used to construct a PSPNet <ref type="bibr" target="#b37">[38]</ref> with a ResNet-50 backbone as the segmentation network. The network is trained for 20 epochs with an SGD optimizer, a learning rate of 5e-3, and a batch size of 16. For a fair comparison, the results of all  unsupervised methods in Table <ref type="table" target="#tab_0">1</ref> are obtained using the same backbone trained with the corresponding pseudo-labels. The code is available at https://github. com/xmed-lab/MSSG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Comparison with State-of-the-art Methods</head><p>We compare our MSSG with multiple approaches with different supervision settings in Table <ref type="table" target="#tab_0">1</ref>. On the GlaS dataset, the end-to-end clustering methods (denoted by " * ") end up with limited improvement over a randomly initialized network. Our MSSG, on the contrary, achieves significant advances. Moreover, MSSG surpasses all other unsupervised counterparts, with a huge margin of 10.56% at mIOU, compared with the second-best unsupervised counterpart. On CRAG dataset, even in the absence of any hints, MSSG still outperforms all unsupervised methods and even some of the fully-supervised methods. Additionally, we visualize the segmentation results of MSSG and its counterpart (i.e., SGSCN <ref type="bibr" target="#b1">[2]</ref>) in Fig. <ref type="figure" target="#fig_2">3</ref>. On both datasets, MSSG obtains more accurate and complete results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Ablation Study</head><p>Table <ref type="table" target="#tab_1">2</ref> presents the ablation test results of the two MSG modules. It can be observed that the segmentation performance without the MSG modules is not satisfactory due to the significant sub-region variation and gland omission. With the gradual inclusion of the MSG for Variation and Omission, the mIOU is improved by 6.42% and 2.57%, respectively. Moreover, with both MSG modules incorporated, the performance significantly improves to 62.72% (+14.30%). we also visualize the results with and without MSG modules in Fig. <ref type="figure" target="#fig_3">4</ref>. It is apparent that the model without MSG ignores most of the interior epithelial tissues.</p><p>With the incorporation of MSG for Variation, the latent distance between gland borders and interior epithelial tissues is becoming closer, while both of these two sub-regions are further away from the background. As a result, the model can highlight most of the gland borders and interior epithelial tissues. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>This paper explores a DL method for unsupervised gland segmentation, which aims to address the issues of over/under segmentation commonly observed in previous USS methods. The proposed method, termed MSSG, takes advantage of an empirical cue to select gland sub-region proposals with varying appearances. Then, a Morphology-aware Semantic Grouping is deployed to integrate the gland information by explicitly grouping the semantics of the selected proposals. By doing so, the final network is able to obtain comprehensive knowledge about glands and produce well-delineated and complete predictions. Experimental results prove the superiority of our method qualitatively and quantitatively.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. (a): Example of a gland and its gland border and interior epithelial tissues. (b) Prior USS methods in medical image research [2] and natural image research [6] vs. Our MSSG. Green and orange regions denote the glands and the background respectively. (Color figure online)</figDesc><graphic coords="2,41,79,54,47,340,15,121,24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Overview of our Morphology-inspired Unsupervised Gland Segmentation via Selective Semantic Grouping. (a) Selective Proposal Mining pipeline. We leverage an empirical cue to select proposals for gland sub-regions from the prediction of a shallow encoder f (•) which emphasizes low-level appearance features rather than highlevel semantic features. (b) Morphology-aware Semantic Grouping (MSG) pipeline.We deploy a MSG for Variation module to group the two gland sub-regions in the embedding space with LMSGV , and a MSG for Omission module to dynamically refine the proposal map generated by the proposal mining frame (see Gland boundary in P and RP ).</figDesc><graphic coords="4,41,79,54,17,340,21,115,54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Visualization of predictions on GlaS (left) and CRAG dataset(right). Black denotes glandular tissues and white denotes non-glandular tissues (More in the Supplementary Material).</figDesc><graphic coords="7,55,98,54,11,340,15,74,59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Ablation Study on MSG modules. Without MSG, the performance is not good enough, due to significant sub-region variation and gland omission. With MSG modules, the performance of the network is progressively improved (More in the Supplementary Material).</figDesc><graphic coords="8,41,79,54,17,340,15,108,55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Finally, with both MSG modules, the model presents the most accurate and similar result to the ground truth. More ablation tests on the SPM (Tab. 1 &amp; 2) and hyper-parameters (Tab. 3) are in the Supplementary Material.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison results on GlaS and CRAG dataset. Bold and underline denote best and second-best results of the unsupervised methods.</figDesc><table><row><cell>Dataset</cell><cell>Method</cell><cell>Backbone</cell><cell cols="2">Supervision F1</cell><cell>DICE</cell><cell>mIOU</cell></row><row><cell></cell><cell>Unet [26]</cell><cell>U-Net</cell><cell>Fully</cell><cell cols="3">77.78% 79.04% 65.34%</cell></row><row><cell></cell><cell>ResUNet [34]</cell><cell>U-Net</cell><cell>Fully</cell><cell cols="3">78.83% 79.48% 65.95%</cell></row><row><cell></cell><cell>MedT [30]</cell><cell cols="2">Transformer Fully</cell><cell cols="3">81.02% 82.08% 69.61%</cell></row><row><cell></cell><cell cols="2">Randomly Initial PSPNet</cell><cell>None</cell><cell cols="3">49.72% 48.63% 32.13%</cell></row><row><cell></cell><cell cols="2">DeepCluster  *  [3] PSPNet</cell><cell>None</cell><cell cols="3">57.03% 57.32% 40.17%</cell></row><row><cell>GlaS</cell><cell>PiCIE  *  [6]</cell><cell>PSPNet</cell><cell>None</cell><cell cols="3">64.98% 65.61% 48.77%</cell></row><row><cell>Dataset</cell><cell>DINO [4]</cell><cell>PSPNet</cell><cell>None</cell><cell cols="3">56.93% 57.38% 40.23%</cell></row><row><cell></cell><cell>DSM [21]</cell><cell>PSPNet</cell><cell>None</cell><cell cols="3">68.18% 66.92% 49.92%</cell></row><row><cell></cell><cell>SGSCN  *  [2]</cell><cell>PSPNet</cell><cell>None</cell><cell cols="3">67.62% 68.72% 52.16%</cell></row><row><cell></cell><cell>MSSG</cell><cell>PSPNet</cell><cell>None</cell><cell cols="3">78.26% 77.09% 62.72%</cell></row><row><cell></cell><cell>Unet [26]</cell><cell>U-Net</cell><cell>Fully</cell><cell cols="3">82.70% 84.40% 70.21%</cell></row><row><cell></cell><cell>VF-CNN [20]</cell><cell>RotEqNet</cell><cell>Fully</cell><cell cols="3">71.10% 72.10% 57.24%</cell></row><row><cell>CRAG Dataset</cell><cell>MILDNet [13] PiCIE  *  [6]</cell><cell cols="2">MILD-Net Fully PSPNet None</cell><cell cols="3">86.90% 88.30% 76.95% 67.04% 64.33% 52.06%</cell></row><row><cell></cell><cell>DSM [21]</cell><cell>PSPNet</cell><cell>None</cell><cell cols="3">67.22% 66.07% 52.28%</cell></row><row><cell></cell><cell>SGSCN  *  [2]</cell><cell>PSPNet</cell><cell>None</cell><cell cols="3">69.29% 67.88% 55.31%</cell></row><row><cell></cell><cell>MSSG</cell><cell>PSPNet</cell><cell>None</cell><cell cols="3">77.43% 77.26% 65.89%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Performance gains with MSG modules. The segmentation performance is progressively improved as the involvement of MSG for Variation &amp; MSG for Omission.</figDesc><table><row><cell>MSG for Variation</cell><cell>MSG for Omission</cell><cell>mIOU</cell><cell>Improvement(Δ)</cell></row><row><cell>× √ × √</cell><cell>× × √ √</cell><cell>48.42% 56.12% 50.18% 62.72%</cell><cell>-+7.70% +1.64% +14.30%</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. This work was supported in part by the <rs type="funder">Hong Kong Innovation and Technology Fund</rs> under Project <rs type="grantNumber">ITS/030/21</rs> and in part by a grant from <rs type="funder">Foshan HKUST Projects</rs> under <rs type="grantNumber">FSUST21-HKUST11E</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_KJKsu5e">
					<idno type="grant-number">ITS/030/21</idno>
				</org>
				<org type="funding" xml:id="_JxaY96J">
					<idno type="grant-number">FSUST21-HKUST11E</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unsupervised medical image segmentation based on the local center of mass</title>
		<author>
			<persName><forename type="first">I</forename><surname>Aganj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Harisinghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Weissleder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Fischl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13012</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A spatial guided self-supervised clustering network for medical image segmentation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87193-2_36</idno>
		<idno>978-3-030-87193-2 36</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12901</biblScope>
			<biblScope unit="page" from="379" to="388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ECCV</title>
		<meeting>the ECCV</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ICCV</title>
		<meeting>the ICCV</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9630" to="9640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">DCAN: deep contour-aware networks for accurate gland segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE CVPR</title>
		<meeting>the IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2487" to="2496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">PiCIE: Unsupervised semantic segmentation using invariance and equivariance in clustering</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Mall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CVPR</title>
		<meeting>the CVPR</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="16794" to="16804" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">MMSegmentation: Openmmlab semantic segmentation toolbox and benchmark</title>
		<author>
			<persName><forename type="first">M</forename><surname>Contributors</surname></persName>
		</author>
		<ptr target="https://github.com/open-mmlab/mmsegmentation" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Color and texture based segmentation of molecular pathology images using HSOMs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Datar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Padfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cline</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 5th IEEE International Symposium on Biomedical Imaging: From Nano to Macro</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="292" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multi-scale fully convolutional network for gland segmentation using three-class classification</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Cen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">380</biblScope>
			<biblScope unit="page" from="150" to="161" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">PCG-cut: graph driven segmentation of the prostate central gland</title>
		<author>
			<persName><forename type="first">J</forename><surname>Egger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">76645</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Colorectal carcinoma: pathologic aspects</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fleming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Tatishchev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Gastrointest. Oncol</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">153</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Automatic multi-tissue segmentation in pancreatic pathological images with selected multi-scale attention network</title>
		<author>
			<persName><forename type="first">E</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CBM</title>
		<imprint>
			<biblScope unit="volume">151</biblScope>
			<biblScope unit="page">106228</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">MILD-Net: minimal information loss dilated network for gland instance segmentation in colon histology images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Graham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="199" to="211" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised semantic segmentation by distilling feature correspondences</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ICLR</title>
		<meeting>the ICLR</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A Chan-vese model based on the Markov chain for unsupervised medical image segmentation</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tsinghua Sci. Technol</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="833" to="844" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">SegSort: segmentation by discriminative sorting of segments</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ICCV</title>
		<meeting>the ICCV</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7333" to="7343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Invariant information clustering for unsupervised image classification and segmentation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ICCV</title>
		<meeting>the ICCV</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9865" to="9874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Online easy example mining for weaklysupervised gland segmentation from histology images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16440-8_55</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16440-855" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13434</biblScope>
			<biblScope unit="page" from="578" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Contrastive registration for unsupervised medical image segmentation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Aviles-Rivero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Schönlieb</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.08894</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rotation equivariant vector field networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Marcos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Volpi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tuia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ICCV</title>
		<meeting>the ICCV</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5058" to="5067" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep spectral methods: a surprisingly strong baseline for unsupervised semantic segmentation and localization</title>
		<author>
			<persName><forename type="first">L</forename><surname>Melas-Kyriazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CVPR</title>
		<meeting>the CVPR</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="8364" to="8375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Automated gland segmentation and classification for Gleason grading of prostate tissue images</title>
		<author>
			<persName><forename type="first">K</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Allen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="1497" to="1500" />
		</imprint>
		<respStmt>
			<orgName>ICPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Gland segmentation from histology images using informative morphological scale space</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Mukherjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4121" to="4125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Scikit-learn: machine learning in Python</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Improving nuclei/gland instance segmentation in histopathology images by full resolution neural network and spatial constrained loss</title>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Riedlinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32239-7_42</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32239-7" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11764</biblScope>
			<biblScope unit="page">42</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-428" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Gland segmentation in colon histology images: the GlaS challenge contest</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sirinukunwattana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="489" to="502" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep neural network models for computational histopathology: a survey</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Srinidhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ciga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page">101813</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Graph run-length matrices for histopathological image segmentation</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Tosun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gunduz-Demir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="721" to="732" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Medical transformer: gated axial-attention for medical image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M J</forename><surname>Valanarasu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Oza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Hacihaliloglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87193-2_4</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87193-24" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12901</biblScope>
			<biblScope unit="page" from="36" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unsupervised semantic segmentation by contrasting object mask proposals</title>
		<author>
			<persName><forename type="first">W</forename><surname>Van Gansbeke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vandenhende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ICCV</title>
		<meeting>the ICCV</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10052" to="10062" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Ta-net: topology-aware network for gland segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vakanski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE WACV</title>
		<meeting>the IEEE WACV</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1556" to="1564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via nonparametric instance discrimination</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CVPR</title>
		<meeting>the CVPR</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Weighted Res-Unet for high-quality retina vessel segmentation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Information Technology in Medicine and Education</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="327" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Gland instance segmentation using deep multichannel neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2901" to="2912" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">ConCL: concept contrastive learning for dense prediction pre-training in pathology images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-19803-8_31</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-19803-831" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2022, ECCV 2022</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Brostow</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Cissé</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13681</biblScope>
			<biblScope unit="page" from="523" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">BoxNet: deep learning based biomedical image segmentation using boxes only annotation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CVPR</title>
		<meeting>the CVPR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
