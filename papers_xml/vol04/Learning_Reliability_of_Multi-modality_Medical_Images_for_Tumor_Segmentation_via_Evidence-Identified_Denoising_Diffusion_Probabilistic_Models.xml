<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Reliability of Multi modality Medical Images for Tumor Segmentation via Evidence Identified Denoising Diffusion Probabilistic Models</title>
				<funder ref="#_jzYF9XY">
					<orgName type="full">China Scholarship Council</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jianfeng</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">Western University</orgName>
								<address>
									<settlement>London</settlement>
									<region>ON</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Shuo</forename><surname>Li</surname></persName>
							<email>slishuo@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">Case Western Reserve University</orgName>
								<address>
									<settlement>Cleveland</settlement>
									<region>OH</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Reliability of Multi modality Medical Images for Tumor Segmentation via Evidence Identified Denoising Diffusion Probabilistic Models</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="682" to="691"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">6FEDF1D104F03DC2214CC6E3330E400C</idno>
					<idno type="DOI">10.1007/978-3-031-43901-8_65</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T10:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Denoising diffusion probabilistic models</term>
					<term>Evidence theory</term>
					<term>Multi-modality</term>
					<term>Tumor segmentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Denoising diffusion probabilistic models (DDPM) for medical image segmentation are still a challenging task due to the lack of the ability to parse the reliability of multi-modality medical images. In this paper, we propose a novel evidence-identified DDPM (EI-DDPM) with contextual discounting for tumor segmentation by integrating multimodality medical images. Advanced compared to previous work, the EI-DDPM deploys the DDPM-based framework for segmentation tasks under the condition of multi-modality medical images and parses the reliability of multi-modality medical images through contextual discounted evidence theory. We apply EI-DDPM on a BraTS 2021 dataset with 1251 subjects and a liver MRI dataset with 238 subjects. The extensive experiment proved the superiority of EI-DDPM, which outperforms the state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Integrating multi-modality medical images for tumor segmentation is crucial for comprehensive diagnosis and surgical planning. In the clinic, the consistent information and complementary information in multi-modality medical images provide the basis for tumor diagnosis. For instance, the consistent anatomical structure information offers the location feature for tumor tracking <ref type="bibr" target="#b21">[22]</ref>, while the complementary information such as differences in lesion area among multimodality medical images provides the texture feature for tumor characterization. Multi-modality machine learning aims to process and relate information from multiple modalities <ref type="bibr" target="#b3">[4]</ref>. But it is still tricky to integrate multi-modality medical images due to the complexity of medical images.</p><p>Existing methods for multi-modality medical image integration can be categorized into three groups: <ref type="bibr" target="#b0">(1)</ref> input-based integration methods that concatenate multi-modality images at the beginning of the framework to fuse them directly [ <ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b20">21]</ref>, (2) feature-based fusion methods that incorporate a fusion module to merge feature maps <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b22">23]</ref>, and (3) decision-based fusion methods that use weighted averaging to balance the weights of different modalities <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b14">15]</ref>. Essentially, these methods differ in their approach to modifying the number of channels, adding additional convolutional layers with a softmax layer for attention, or incorporating fixed modality-specific weights. However, there is no mechanism to evaluate the reliability of information from multi-modality medical images. Since the anatomical information of different modality medical images varies, the reliability provided by different modalities may also differ. Therefore, it remains challenging to consider the reliability of different modality medical images when combining multi-modality medical image information.</p><p>Dempster-Shafer theory (DST) <ref type="bibr" target="#b17">[18]</ref>, also known as evidence theory, is a powerful tool for modeling information, combining evidence, and making decisions by integrating uncertain information from various sources or knowledge <ref type="bibr" target="#b9">[10]</ref>.Some studies have attempted to apply DST to medical image processing <ref type="bibr" target="#b7">[8]</ref>. However, using evidence theory alone does not enable us to weigh the different anatomical information from multi-modality medical images. To explore the reliability of different sources when using evidence theory, the work by Mercier et al. <ref type="bibr" target="#b12">[13]</ref> proposed a contextual discounting mechanism to assign weights to different sources. Furthermore, for medical image segmentation, denoising diffusion probabilistic models (DDPM <ref type="bibr" target="#b6">[7]</ref>) have shown remarkable performance <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b19">20]</ref>. Inspired by these studies, if DDPM can parse the reliability of multi-modality medical images to weigh the different anatomy information from them, it will provide a significant approach for tumor segmentation.</p><p>In this paper, we propose an evidence-identified DDPM (EI-DDPM) with contextual discounting for tumor segmentation via integrating multi-modality medical images. Our basic assumption is that we can learn the segmentation feature on single modality medical images using DDPM and parse the reliability of different modalities medical images by evidence theory with a contextual discounting mechanism. Specifically, the EI-DDPM first utilizes parallel conditional DDPM to learn the segmentation feature from a single modality image. Next, the evidence-identified layer (EIL) preliminarily integrates multi-modality images by comprehensively using the multi-modality uncertain information. Lastly, the contextual discounting operator (CDO) performs the final integration of multimodality images by parsing the reliability of information from multi-modality medical images. The contributions of this work are:</p><p>-Our EI-DDPM achieves tumor segmentation by using DDPM under the guidance of evidence theory. It provides a solution to integrate multi-modality medical images when deploying the DDPM algorithm. -The proposed EIL and CDO apply contextual discounting guided DST to parse the reliability of information from different modalities of medical images. This allows for the integration of multi-modality medical images with learned weights corresponding to their reliability. -We conducted extensive experiments using the BraTS 2021 <ref type="bibr" target="#b11">[12]</ref> dataset for brain tumor segmentation and a liver MRI dataset for liver tumor segmentation. Experimental results demonstrate the superiority of EI-DDPM over other State-of-The-Art (SoTA) methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>The EI-DDPM achieves tumor segmentation by parsing the reliability of multimodality medical images. Specifically, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>, the EI-DDPM is fed with multi-modality medical images into the parallel DDPM path and performs the conditional sampling process to learn the segmentation feature from the single modality image (Sect. 2.1). Next, the EIL preliminary integrates multimodality images by embedding the segmentation features from multi-modality images into the combination rule of DST (Sect. 2.2). Lastly, the CDO integrates multi-modality medical images for tumor segmentation by contextual discounting mechanism (Sect. 2.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Parallel DDPM Path for Segmentation Feature Learning</head><p>Background of DDPM: As an unconditional generative method, DDPM <ref type="bibr" target="#b6">[7]</ref> has the form of p θ (x 0 ) := p θ (x 0:T )dx 1:T , where x 1 , ..., x T represent latents with the same dimensionality as the data x 0 ∼ q(x 0 ). It contains the forward process of diffusion and the reverse process of denoising. The forward process of diffusion that the approximate posterior q(x 1:T |x 0 ), it is a Markov Chain by gradually adding Gaussian noise for converting the noise distribution to the data distribution according to the variance schedule β 1 , ..., β T :</p><formula xml:id="formula_0">q(x t |x t-1 ) := N (x t ; 1 -β t x t-1 , β t I), q(x t |x 0 ) = N (x t ; √ ᾱt x 0 , (1-ᾱt )I) (1)</formula><p>The reverse process of denoising that the joint distribution p θ (x 0:T ), it can be defined as a Markov chain with learnt Gaussian transitions starting from p(x T ) = N (x T ; 0, I):</p><formula xml:id="formula_1">p θ (x t-1 |x t ) := N (x t-1 ; μ θ (x t , t), σ 2 t I), p θ (x 0:T ) := p(x T ) T t=1 p θ (x t-1 |x t ) (2)</formula><p>where α t := 1β t , ᾱt := t s=1 α s , and Multi-modality Medical Images Conditioned DDPM: In Eq. 2 from DDPM <ref type="bibr" target="#b6">[7]</ref>, the unconditional prediction x t-1 at each step is obtained by subtracting the predicted noise from the previous x t , which can be defined as:</p><formula xml:id="formula_2">σ 2 t = 1-ᾱt-1 1-ᾱt β t .</formula><formula xml:id="formula_3">x t-1 = 1 √ α t (x t - 1 -α t √ 1 -ᾱt θ (x t , t)) + σ t z, z ∼ N (0, I)<label>(3)</label></formula><p>As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, to perform the conditional sampling in EI-DDPM, the prediction x t-1 at each step t is on the basis of the concatenation "⊕" of previous x t and the conditional image x C,t . Thus, the x t-1 here can be defined as:</p><formula xml:id="formula_4">x t-1 = 1 √ α t (x t - 1 -α t √ 1 -ᾱt θ (x t ⊕ x C,t , t)) + σ t z, z ∼ N (0, I)<label>(4)</label></formula><p>where the conditional image x C ∈ {T 1, T 2, F lair, T 1ce} corresponding to the four parallel conditional DDPM path. And in each step t, the x C,t was also performed the operation of adding Gaussian noise to convert the distribution:</p><formula xml:id="formula_5">x C,t = x C,0 + N (0, (1 -ᾱt )I)<label>(5)</label></formula><p>where x C,0 presents the multi-modality medical images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">EIL Integrates Multi-modality Images</head><p>As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, the EIL is followed at the end of the parallel DDPM path.</p><p>The hypothesis is to regard multi-modality images as independent and different sources knowledge. And then embedding multi-phase features into the combination rule of DST for evidence identification, which comprehensively parses the multi-phase uncertainty information for confident decision-making. The basic concepts of evidence identification come from DST <ref type="bibr" target="#b17">[18]</ref>. It is assumed that Θ = {θ 1 , θ 2 , ..., θ n } is a finite domain called discriminant frame. Mass function is defined as M. So, the evidence about Θ can be represented by M as:</p><formula xml:id="formula_6">M : 2 Θ → [0, 1] where M(∅) = 0 and A⊆Θ M(A) = 1<label>(6)</label></formula><p>where the M(A) denotes the whole belief and evidence allocated to A. The associated belief (Bel) and plausibility (Pls) functions are defined as:</p><formula xml:id="formula_7">Bel(A) = B⊆A M(B) and P ls(A) = B∩A =∅ M(B)<label>(7)</label></formula><p>And using the contour function pls to restrict the plausibility function P ls of singletons (i.e. pls(θ) = P ls({θ}) for all θ ∈ Θ) <ref type="bibr" target="#b17">[18]</ref>. According to the DST, the mass function M of a subset always has lower and upper bound as Bel(A) ≤ M(A) ≤ P ls(A). From the evidence combination rule of DST, for the mass functions of two independent items M 1 and M 2 (i.e. two different modality images), it can be calculated by a new mass function M 1 ⊕ M 2 (A), which is the orthogonal sum of M 1 and M 2 as:</p><formula xml:id="formula_8">M 1 ⊕ M 2 (A) = 1 1 -γ B∩C=A M 1 (B)M 2 (C)<label>(8)</label></formula><p>where γ = B∩C=∅ M 1 (B)M 2 (C) is conflict degree between M 1 and M 2 . And the combined contour function P ls 12 corresponding to M 1 ⊕ M 2 is:</p><formula xml:id="formula_9">P ls 12 = P ls 1 P ls 2 1 -γ (9)</formula><p>The specific evidence identification layer mainly contains three sub-layers (i.e. activation layer, mass function layer, and belief function layer). For activation layer, the activation of i unit can be defined as:</p><formula xml:id="formula_10">y i = a i exp(-λ i ||x -w i || 2 )</formula><p>. The w i is the weight of i unit. λ i &gt; 0 and a i are parameters. The mass function layer calculates the mass of each K classes using M i ({θ k }) = u ik y i , where K k=1 u ik = 1. u ik means the degree of i unit to class θ k . Lastly, the third layer yields the final belief function about the class of each pixel using the combination rule of DST (Eq. 8). </p><formula xml:id="formula_11">η M = ηM + (1 -η)M ? (<label>10</label></formula><formula xml:id="formula_12">)</formula><p>where M ? is a vacuous mass function defined by M(Θ) = 1, η M is a mixture of M and M ? . The coefficient η plays the role of weighting mass function η M.</p><p>The corresponding contour function of η M is:</p><formula xml:id="formula_13">η pls({θ k }) = 1 -η k + η k pls({θ k }), k = 1, ..., K<label>(11)</label></formula><p>Advantage: EIL and CDO parse the reliability of different modality medical images in different contexts. For example, if we feed two modality medical images like T 1 and T 2 into the EI-DDPM, with the discount rate 1η T 1 and 1η T 2 , we will have two contextual discounted contour functions ηT<ref type="foot" target="#foot_0">1</ref> pls T 1 and ηT 2 pls T 2 .</p><p>The combined contour function in Eq. 9 is proportional to the ηT 1 pls T 1 ηT 2 pls T 2 . In this situation, the η T 1 and η T 2 can be trained to weight the two modality medical images by parsing the reliability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiment and Results</head><p>Dataset. We used two MRI datasets that BraTS 2021 <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b11">12]</ref> and a liver MRI dataset. BraTS 2021 1 contains 1251 subject with 4 aligned MRI modalities: T1, T2, Flair, and contrast-enhanced T1 (T1ce). The segmentation labels consist of Implementation Details. For the number of DDPM paths, BraTS 2021 dataset is equal to 4 corresponding to the input 4 MRI modalities and the liver MRI dataset is equal to 3 corresponding to the input 3 MRI modalities. In the parallel DDPM path, the noise schedule followed the improved-DDPM <ref type="bibr" target="#b13">[14]</ref>, and the U-Net <ref type="bibr" target="#b16">[17]</ref> was utilized as the denoising model with 300 sampling steps.</p><p>In EIL, the initial values of a i equal 0.5 and λ i is equal to 0.01. For CDO, the initial of parameter η k is equal to 0.5. With the Adam optimization algorithm, the denoising process was optimized using L 1 , and the EIL and CDO were optimized using Dice loss. The learning rate of EI-DDPM was set to 0.0001. The Quantitative and Visual Evaluation. The performance of EI-DDPM is evaluated by comparing with three methods: a classical CNN-based method (U-Net <ref type="bibr" target="#b16">[17]</ref>), a Transformer-based method (TransU-Net <ref type="bibr" target="#b4">[5]</ref>), and a DDPM-based method for multi-modality medical image segmentation (SegDDPM <ref type="bibr" target="#b19">[20]</ref>). The Dice score is used for evaluation criteria. Figure <ref type="figure" target="#fig_2">3</ref> shows the visualized segmentation results of EI-DDPM and compared methods. It shows some ambiguous area lost segmentation in three compared methods but can be segmented by our EI-DDPM. Discussion of Learnt Reliability Coefficients η. Table <ref type="table" target="#tab_2">3</ref> and Table <ref type="table" target="#tab_3">4</ref> show the learned reliability coefficients η on BraTS 2021 dataset and liver MRI dataset. The higher η value, the higher reliability of the corresponding region segmentation. As shown in Table <ref type="table" target="#tab_2">3</ref>, the Flair modality provides the highest reliability for ED segmentation. And both the T2 modality and T1ce modality provide relatively high reliability for ET and NCR segmentation. As shown in Table <ref type="table" target="#tab_3">4</ref>, the T1ce modality provides the highest reliability for hemangioma and HCC segmentation. These reliability values are the same as clinical experience <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we proposed a novel DDPM-based framework for tumor segmentation under the condition of multi-modality medical images. The EIL and CDO enable our EI-DDPM to capture the reliability of different modality medical images with respect to different tumor regions. It provides a way of deploying contextual discounted DST to parse the reliability of multi-modality medical images. Extensive experiments prove the superiority of EI-DDPM for tumor segmentation on multi-modality medical images, which has great potential to aid in clinical diagnosis. The weakness of EI-DDPM is that it takes around 13 s to predict one segmentation image. In future work, we will focus on improving sampling steps in parallel DDPM paths to speed up EI-DDPM.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The workflow of EI-DDPM for brain tumor segmentation of BraTS 2021 dataset. It mainly contains three parts: parallel DDPM path for multi-modality medical images feature extraction, EIL for preliminary multi-modality medical images integration, and CDO for parsing the reliability of multi-modality medical images.</figDesc><graphic coords="2,84,48,54,35,283,51,141,34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The algorithm of multi-modality medical images conditioned DDPM.</figDesc><graphic coords="4,98,46,165,86,255,22,171,01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Visualized segmentation results on BraTS 2021 dataset and liver MRI dataset. The first row presents tumor segmentation on BraTS 2021, where the label of green, yellow, and red represent the ED, ET, and NCR, respectively. The second row is HCC segmentation on liver MRI. The major segmentation differences between different methods are marked with blue circles. (Color figure online)</figDesc><graphic coords="6,55,98,54,47,340,15,110,23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The quantitative evaluation of the comparison between EI-DDPM and other methods. The criteria of Dice evaluated the performance. Avg presents the average value in the whole dataset.</figDesc><table><row><cell></cell><cell cols="3">Dice for BraTS 2021</cell><cell></cell><cell>Dice for Liver MRI</cell></row><row><cell></cell><cell>WT</cell><cell>ET</cell><cell>TC</cell><cell>Avg</cell><cell>Hemangioma HCC Avg</cell></row><row><cell>U-Net [17]</cell><cell cols="5">91.30 83.23 86.53 86.49 91.89</cell><cell>84.21 87.12</cell></row><row><cell cols="6">TransU-Net [5] 91.56 84.72 87.26 87.43 92.44</cell><cell>85.35 88.42</cell></row><row><cell cols="6">SegDDPM [20] 92.04 85.74 88.37 88.65 93.10</cell><cell>86.27 89.07</cell></row><row><cell>EI-DDPM</cell><cell cols="5">93.52 87.31 90.23 90.17 94.57</cell><cell>88.03 90.47</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation studies for EIL and CDO. The criteria of Dice evaluated the performance. Avg presents the average value in the whole dataset.</figDesc><table><row><cell></cell><cell cols="3">Dice for BraTS 2021</cell><cell></cell><cell>Dice for Liver MRI</cell></row><row><cell></cell><cell>WT</cell><cell>ET</cell><cell>TC</cell><cell>Avg</cell><cell>Hemangioma HCC Avg</cell></row><row><cell>No CDO</cell><cell cols="5">92.10 86.42 89.27 89.57 94.01</cell><cell>87.26 89.89</cell></row><row><cell cols="6">No EIL&amp;CDO 92.33 85.83 88.50 89.03 93.24</cell><cell>86.33 89.25</cell></row><row><cell>EI-DDPM</cell><cell cols="5">93.52 87.31 90.23 90.17 94.57</cell><cell>88.03 90.47</cell></row><row><cell cols="6">GD-enhancing tumor (ET), necrotic tumor core (NCR), and peritumoral ede-</cell></row><row><cell cols="6">matous (ED), which are combined into 3 nested subregions: Enhancing Tumor</cell></row><row><cell cols="6">(ET) region, Tumor Core (TC) region (i.e. ET+NCR), and Whole Tumor (WT)</cell></row><row><cell cols="6">region (i.e. ET+ED+NCR). The liver MRI dataset contains 238 subjects with</cell></row><row><cell cols="6">110 hemangioma subjects and 128 hepatocellular carcinoma (HCC) subjects.</cell></row><row><cell cols="6">Each subject has corresponding 3 aligned MRI modalities: T1, T2, and T1ce</cell></row><row><cell cols="6">(protocols were gadobutrol 0.1 mmol/kg on a 3T MRI scanner). The segmen-</cell></row><row><cell cols="6">tation labels of hemangioma and HCC were performed by two radiologists with</cell></row><row><cell cols="6">more than 5-year-experience. The resolution of the BraTS 2021 image and liver</cell></row><row><cell cols="6">MRI image are 240×240 and 256×256, respectively. Both BraTS 2021 and liver</cell></row><row><cell cols="6">MRI datasets were randomly divided into 3 groups following the ratio of train-</cell></row><row><cell cols="3">ing/validation/test as 7:1:2.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>The learnt reliability coefficients η of BraTS 2021</figDesc><table><row><cell>η</cell><cell>ED</cell><cell>ET</cell><cell>NCR</cell></row><row><cell cols="4">T1 0.2725 0.3751 0.3318</cell></row><row><cell cols="4">T2 0.8567 0.9458 0.6912</cell></row><row><cell cols="4">Flair 0.9581 0.7324 0.5710</cell></row><row><cell cols="4">T1ce 0.7105 0.9867 0.9879</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>The learnt reliability coefficients η of Liver MRI</figDesc><table><row><cell>η</cell><cell cols="2">Hemangioma HCC</cell></row><row><cell cols="2">T1 0.8346</cell><cell>0.5941</cell></row><row><cell cols="2">T2 0.8107</cell><cell>0.5463</cell></row><row><cell cols="2">T1ce 0.9886</cell><cell>0.9893</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Table 1 reports the quantitative results of EI-DDPM and compared methods. Our EI-DDPM achieves highest Dice value on both BraTS dataset and liver MRI dataset. All these results proved EI-DDPM outperforms the three other methods. No EIL&amp;CDO). Table 2 shows the quantitative results of ablation studies. Experimental results proved both EIL and CDO contribute to tumor segmentation on BraTS 2021 dataset and liver MRI dataset.</figDesc><table><row><cell>Ablation Study. To prove the contribution from EIL and CDO, we performed</cell></row><row><cell>2 types of ablation studies: (1) removing CDO (i.e. No CDO) and (2) removing</cell></row><row><cell>EIL and CDO (i.e.</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>http://braintumorsegmentation.org/.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work is partly supported by the <rs type="funder">China Scholarship Council</rs> (No. <rs type="grantNumber">202008370191</rs>)</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_jzYF9XY">
					<idno type="grant-number">202008370191</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The rsna-asnr-miccai brats 2021 benchmark on brain tumor segmentation and radiogenomic classification</title>
		<author>
			<persName><forename type="first">U</forename><surname>Baid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.02314</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Advancing the cancer genome atlas glioma MRI collections with expert segmentation labels and radiomic features</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bakas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Data</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Hepatocellular carcinoma: a review</title>
		<author>
			<persName><forename type="first">J</forename><surname>Balogh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of hepatocellular carcinoma</title>
		<imprint>
			<biblScope unit="page" from="41" to="53" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multimodal machine learning: a survey and taxonomy</title>
		<author>
			<persName><forename type="first">T</forename><surname>Baltrušaitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="423" to="443" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Transunet: transformers make strong encoders for medical image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.04306</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A new evidential k-nearest neighbor rule based on contextual discounting with partially supervised learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Denoeux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kanjanatarakul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sriboonchitta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Approximate Reasoning</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="page" from="287" to="302" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6840" to="6851" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Lymphoma segmentation from 3d PET-CT images using a deep evidential network</title>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Decazes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Denoeux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Approximate Reasoning</title>
		<imprint>
			<biblScope unit="volume">149</biblScope>
			<biblScope unit="page" from="39" to="60" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Diffusion adversarial representation learning for selfsupervised vessel segmentation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Ye</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.14566</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Joint tumor segmentation in PET-CT images using co-clustering and fusion based on belief functions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Denoeux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="755" to="766" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A multi-phase semi-automatic approach for multisequence brain tumor image segmentation</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Y</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mandava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="page" from="288" to="300" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The multimodal brain tumor image segmentation benchmark (brats)</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Menze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1993" to="2024" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Refined modeling of sensor reliability in the belief function framework using contextual discounting</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mercier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Quost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Denoeux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Fusion</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="246" to="258" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improved denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Q</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8162" to="8171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">M3net: a multi-scale multi-view framework for multi-phase pancreas segmentation based on cross-phase non-local attention</title>
		<author>
			<persName><forename type="first">T</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page">102232</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Co-heterogeneous and adaptive segmentation from multi-source and multi-phase ct imaging data: a study on pathological liver and lesion segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Raju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="448" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-4_28" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A mathematical theory of evidence</title>
		<author>
			<persName><forename type="first">G</forename><surname>Shafer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1976">1976</date>
			<publisher>Princeton University Press</publisher>
			<biblScope unit="volume">42</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Tensor-based sparse representations of multi-phase medical images for classification of focal liver lesions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn. Lett</title>
		<imprint>
			<biblScope unit="volume">130</biblScope>
			<biblScope unit="page" from="207" to="215" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Diffusion models for implicit image segmentation ensembles</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wolleb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sandkühler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bieder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valmaggia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Cattin</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Imaging with Deep Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1336" to="1348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Robust pancreatic ductal adenocarcinoma segmentation with multi-institutional multi-phase partially-annotated CT scans</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59719-1_48</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59719-1_48" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12264</biblScope>
			<biblScope unit="page" from="491" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">United adversarial learning for liver tumor segmentation and detection of multi-modality non-contrast MRI</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page">102154</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hyper-pairing network for multi-phase pancreatic ductal adenocarcinoma segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32245-8_18</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32245-8_18" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11765</biblScope>
			<biblScope unit="page" from="155" to="163" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
