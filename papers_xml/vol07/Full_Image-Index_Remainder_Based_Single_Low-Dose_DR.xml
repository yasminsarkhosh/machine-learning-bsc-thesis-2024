<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Full Image-Index Remainder Based Single Low-Dose DR</title>
				<funder ref="#_yukn77C #_MJRf74j #_QZQHrC6">
					<orgName type="full">Guangdong Basic and Applied Basic Research Foundation</orgName>
				</funder>
				<funder ref="#_KJHdmft #_Bysqd8F">
					<orgName type="full">Shenzhen Science and Technology Program</orgName>
				</funder>
				<funder ref="#_t7PKycg">
					<orgName type="full">National Key Research and Development Program of China</orgName>
				</funder>
				<funder ref="#_3AjpKU5 #_jES529A">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yifei</forename><surname>Long</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">Shenzhen Campus of Sun Yat-sen University</orgName>
								<address>
									<postCode>518107</postCode>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiayi</forename><surname>Pan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">Shenzhen Campus of Sun Yat-sen University</orgName>
								<address>
									<postCode>518107</postCode>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yan</forename><surname>Xi</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Shanghai First-Imaging Information Technology Co., LTD</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Jianjia</forename><surname>Zhang</surname></persName>
							<email>zhangjj225@mail.sysu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">Shenzhen Campus of Sun Yat-sen University</orgName>
								<address>
									<postCode>518107</postCode>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Weiwen</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">Shenzhen Campus of Sun Yat-sen University</orgName>
								<address>
									<postCode>518107</postCode>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Full Image-Index Remainder Based Single Low-Dose DR</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="466" to="475"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">77F9A7AB82E7323A81C7E2B24E1CC266</idno>
					<idno type="DOI">10.1007/978-3-031-43990-2_44</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T11:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Digital radiography</term>
					<term>Computed tomography</term>
					<term>Self-supervised</term>
					<term>Remainder</term>
					<term>Image denoising</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Low-dose digital radiography (DR) and computed tomography (CT) play a crucial role in minimizing health risks during clinical examinations and diagnoses. However, reducing the radiation dose often leads to lower signal-to-noise ratio measurements, resulting in degraded image quality. Existing supervised and self-supervised reconstruction techniques have been developed with noisy and clean image pairs or noisy and noisy image pairs, implying they cannot be adapted to single DR and CT image denoising. In this study, we introduce the Full Image-Index Remainder (FIRE) method. Our method begins by dividing the entire highdimensional image space into multiple low-dimensional sub-image spaces using a full image-index remainder technique. By leveraging the data redundancy present within these sub-image spaces, we identify similar groups of noisy sub-images for training a self-supervised denoising network. Additionally, we establish a sub-space sampling theory specifically designed for selfsupervised denoising networks. Finally, we propose a novel regularization optimization function that effectively reduces the disparity between selfsupervised and supervised denoising networks,thereby enhancing denoising training. Through comprehensive quantitative and qualitative experiments conducted on both clinical low-dose CT and DR datasets, we demonstrate the remarkable effectiveness and advantages of our FIRE method compared to other state-of-the-art approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Digital Radiography (DR) and Computed Tomography (CT) techniques are extensively utilized in the diagnosis of various clinical conditions <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b23">24]</ref>. However, one major concern associated with these imaging methods is the exposure of patients to X-ray radiation <ref type="bibr" target="#b11">[12]</ref>. Reducing the X-ray radiation dose unavoidably leads to a decline in the number of photons detected, resulting in measurements with a low signal-to-noise ratio and consequent regression in image quality. Consequently, accurately diagnosing clinical conditions based on degraded DR/CT images poses significant challenges. Hence, the development of sophisticated and efficient image-denoising techniques that can effectively address both DR and CT modalities becomes imperative and urgent in clinical applications.</p><p>Traditional image denoising methods have relied on exploring spatial pixel features and properties in the transform domain <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b7">8]</ref>. These methods include approaches like non-local mean (NLM) <ref type="bibr" target="#b22">[23]</ref> and BM3D <ref type="bibr" target="#b4">[5]</ref>. However, the need for complex parameter adjustment and their relatively slow speed have limited the practical applications of these traditional denoising methods.</p><p>With the advancement of neural networks, deep learning-based denoising techniques have shown superior performance compared to traditional methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b8">9]</ref>. Supervised denoising methods, such as U-Net <ref type="bibr" target="#b5">[6]</ref>, DnCNN <ref type="bibr" target="#b24">[25]</ref>, and RED-CNN <ref type="bibr" target="#b3">[4]</ref>, have demonstrated promising results. However, when it comes to digital radiography (DR) and computed tomography (CT), these supervised methods face challenges since they rely on paired noisy and clean image data, which are not readily available in DR and CT imaging <ref type="bibr" target="#b6">[7]</ref>.</p><p>Toovercomethelimitationsofsupervisedtechniques,self-supervisedorunsupervised denoising methods have been proposed and developed, leveraging the similaritybetweennoisyimages <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b25">26]</ref>.Severalunsupervisedlearning-basedmethodshave been introduced for image denoising, including Noise2Noise <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b15">16]</ref>, Noise2Sim <ref type="bibr" target="#b19">[20]</ref>,Noise2Void <ref type="bibr" target="#b13">[14]</ref>,andNeighbor2Neighbor <ref type="bibr" target="#b10">[11]</ref>amongothers <ref type="bibr" target="#b16">[17]</ref>.However,these methods have defects in the DR and CT denoising tasks. They either require noisy and noisy image pairs in training, or sampling strategies resulting in information missing and the same noise level of the sub-image pairs. Therefore, we designed a new self-supervised image denoising technique, i.e., the full image-index remainder (FIRE), adapting to low-dose DR and CT to address the challenge. Specifically, our FIRE method first divided the whole high-dimensional image space into a series of low-dimensional sub-image spaces with the image-index remainder technique. Based on the remainder of the full image index, a specific image sampler is designed to sample the sub-images. With this strategy, a set of sampled noisy sub-images from a single noisy image is obtained for self-supervised training of the denoising neural network without clean images. We further proved that our proposed sampling strategy is effective in supplementary materials. In addition, we proposed a new loss function to train the unsupervised image-denoising network with dedicated parameter tuning. For further optimizing the feasible domain, a regularization strategy is introduced to reduce the gap between the self-supervised and the supervised denoising network. We evaluated the FIRE on several large-scale clinical DR and CT datasets without clean data. The experimental results show that our FIRE method achieves the best results in noise suppression and visual perception. We also compared our FIRE and other methods using public clinical data with ground truth, and the quantitative and qualitative results demonstrate the out-performance of our method.</p><p>The remaining sections are organized as follows. Sections 2 and 3, introduce our proposed FIRE method in terms of the network architectures, sampler design, and loss functions. In Sect. 4, we will test our model on large-scale low-dose DR and CT image datasets and compare it with other well-performing methods. Finally, a summary will be given.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Related Theories</head><p>Given a clean image x and its corresponding noisy image y, the supervised image denoising methods try to train the denoising network f parameterized by θ minimizing the loss function below:</p><formula xml:id="formula_0">E {x,y} = {f θ (y), x}.</formula><p>(</p><p>Implementing the supervised denoising network on noisy and clean image pairs, the usual loss function can be formulated as:</p><formula xml:id="formula_2">θ = arg min θ E x,y f θ (y) -x 2 2 . (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>However, in actual situations, it is difficult to obtain the corresponding clean and noisy image pairs, especially in medical imaging scans. To address this issue, Noise2Noise proposed a self-denoising method that does not require real clean images, but it depends on pairs of independent noisy images of the same scene. The method proves that the results obtained by minimizing the following equation are the same as the results obtained by the supervised case above:</p><formula xml:id="formula_4">θ = arg min θ E x,y,z f θ (y) -z 2 2 (3)</formula><p>where y and z are two independent noisy images conditioned on x. In previous studies <ref type="bibr" target="#b15">[16]</ref>, it has been proved that the results of training using only noisy and noisy image pairs can be approximately equal to supervised cases. Neigh-bor2Neighbor <ref type="bibr" target="#b10">[11]</ref> focuses on sampling a single noisy image with independent noisy and noisy image pairs from the same scene for network training. However, there are still challenges in applying this method to DR and CT image denoising tasks. Specifically, DR and CT images have a larger range of pixel values and more details in the image, while the current method uses only the part image information in each iteration. In the following section, we propose our FIRE method for DR and CT image denoising tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Framework</head><p>In this section, we propose a self-supervised framework for training a single noisy image-denoising network. First, we design a new subspace sampling technique for generating subspace image groups to train the network. Next, for recovering finer image details and features, a specialized loss function consisting of reconstruction terms and regularization terms is proposed and used for training the network. The overall of our FIRE framework is shown in Fig. <ref type="figure" target="#fig_1">1</ref>  Subspace Sampling: The remainder with respect to a positive integer (i.e., N ) of image pixel coordination is excellent to be used to develop a sampler. With it, a raw noisy image can be divided into a series of sub-image spaces, and such design can fully use the advantages of the image pixels index resulting in satisfying the condition of independence. Let y ∈ R (W * H) be one noisy image sample, where W and H represent the width and height of the raw image y. Considering the size of the medical image is usually a multiple of 256, the N is chosen as 4 in this study. In fact, the N can be adjusted depending on the image size. In this case, the sampler G consists of four sub-sampler G = (g 1 , g 2 , g 3 , g 4 ).</p><p>The details of the remainder sampler G can be developed as follows: a) Making the sampled pictures contain all the pixels within the original noisy image, it is necessary to ensure that the length and width of the image y can be divisible by N, which is chosen as 4; b) Encoding pixel index of the image y from the left-upper corner, all image pixels can be accessed by the abscissa i and ordinate j. The image indexes can range from (0, 0) to (W -1, H -1). c) Calculating the remainder of (2×i+j)%4 and defined it as k. k is an integer in [0, 3]. All pixel coordinations satisfying k = 0 are retained, and the remaining pixel values are set to 0 to generate the first mask. The second mask is obtained with pixel coordinations satisfying k = 1. The third and fourth masks are obtained by satisfying k = 2 and k = 3. d) Selecting the reserved pixel value from the four pixels in each 2×2 area as the pixel value for the corresponding position of the subsampled image, and put it into (g 1 , g 2 , g 3 and g 4 ). By doing this, four sampled pictures are obtained, i.e., (g 1 (y), g 2 (y), g 3 (y) and g 4 (y)). The width and length of all sampled subspace images are W/2 and H/2 respectively.</p><p>To clearly demonstrate the generation diagram of the subspace image group, Fig. <ref type="figure" target="#fig_1">1(b</ref>) summarizes the idea of our proposed remainder subspace subsampler. Since the corresponding pixels of (g 1 (y), g 2 (y), g 3 (y), g 4 (y)) generated by the sampler are adjacent but different in the original image. Each sampled subspace image is independent of the others, and the completeness and difference of image content in the four sampled sub-images can be guaranteed. Here we can divide the four sub-images into two groups of g 1 (y) and (g 2 (y), g 3 (y), g 4 (y)). Please refer to the supplementary materials for detailed explanations of this grouping.</p><p>Regularization Optimization: For self-supervised training, the following minimization optimization problem is formulated by taking advantage of the constraint</p><formula xml:id="formula_5">min θ E y|x 3f θ (g 1 (y)) -(g 2 (y) + g 3 (y) + g 4 (y)) 2 2 + λE y|x 3f θ (g 1 (y)) -(g 2 (y) + g 3 (y) + g 4 (y)) + (3g 1 (f θ (y)) -(g 2 (f θ (y)) + g 3 (f θ (y)) + g 4 (f θ (y))) 2 2 . (<label>4</label></formula><formula xml:id="formula_6">)</formula><p>According to E x,y = E x E y|x , it can be converted into the following regularization optimization problem</p><formula xml:id="formula_7">min θ E x,y 3f θ (g 1 (y)) -(g 2 (y) + g 3 (y) + g 4 (y)) 2 2 + λE x,y 3f θ (g 1 (y)) -(g 2 (y) + g 3 (y) + g 4 (y)) + (3g 1 (f θ (y)) -(g 2 (f θ (y)) + g 3 (f θ (y)) + g 4 (f θ (y))) 2 2 . (5)</formula><p>Finally, the loss function incorporating the regularization term is proposed to train the denoising network with</p><formula xml:id="formula_8">L = L rec + λ × L reg = 3f θ (g 1 (y)) -(g 2 (y) + g 3 (y) + g 4 (y)) 2 2 + λ 3f θ (g 1 (y)) -(g 2 (y) + g 3 (y) + g 4 (y)) + (3g 1 (f θ (y)) -(g 2 (f θ (y)) + g 3 (f θ (y)) + g 4 (f θ (y))) 2 2 (6)</formula><p>where f θ is the denoising network and λ is a hyperparameter to balance the regularization term. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>In this section, we first introduce the details and configuration of clinical experiments. To evaluate the effectiveness of this method, several advanced imagedenoising methods for CT and DR were involved in the comparison. The experiments were conducted on several large-scale CT and DR data sets. Experimental Configuration: The network architecture of our proposed FIRE framework was a modified U-Net <ref type="bibr" target="#b20">[21]</ref>. All experiments were conducted on a PC server equipped with Python3.9.7, PyTorch1.8, and NVIDIA TITAN RTX graphics processors.</p><p>In terms of the comparisons, we include BM3D <ref type="bibr" target="#b4">[5]</ref>, DIP <ref type="bibr" target="#b16">[17]</ref>, Noise2Sim <ref type="bibr" target="#b19">[20]</ref>, Noise2Void <ref type="bibr" target="#b13">[14]</ref>, Blind2Unblind <ref type="bibr" target="#b21">[22]</ref>, Neighbor2Neighbor(Nei2Nei) <ref type="bibr" target="#b10">[11]</ref> and two supervised denoising algorithms DnCNN <ref type="bibr" target="#b24">[25]</ref> and DD-net <ref type="bibr" target="#b26">[27]</ref>. According to the applicability of denoising methods, we chose different comparison methods in different experiments. More details can be found in Figs. <ref type="figure" target="#fig_2">2 to</ref>   Experimental Results: In three different denoising tasks, our FIRE method showed the best results. For the clinical brain CT image denoising, the excellent denoising results of FIRE can be seen in Fig. <ref type="figure" target="#fig_2">2</ref>. Due to a certain gap in ground truth between adjacent frames images used in network training, Noise2Sim cannot achieve good enough results. DIP produces a lot of artifacts after removing the noise and misses a lot of details and features. Besides, BM3D and DIP consume long time, and the parameters need to be manually adjusted on different denoising conditions. For the real DR image dataset, Fig. <ref type="figure" target="#fig_3">3</ref> demonstrates the DR image denoising results of different body parts for different patients. It can be seen that the denoising performance of Neighbor2Neighbor is limited. It may be because the Neighbor2Neighbor only uses information of half pixels for training, and the entire structure and pixel information within the DR image is partially missed. On the public CT image data set, we can intuitively compare the advantages of our method over other methods from the PSNR and SSIM measures. Figure <ref type="figure" target="#fig_4">4</ref> shows the typical slice denoising results. Figure <ref type="figure" target="#fig_6">5</ref>(a)-(j) presents the noise power spectrum(NPS) of all methods, where blue indicates it is closer to the reference. As seen, our FIRE obtains the most blue and the least red results. Figure <ref type="figure" target="#fig_6">5</ref>(k) reflects the comparison of pixel values on the profile line of a slice of the test set. Among them, our FIRE is closest to the reference. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Influence of Regularization Term:</head><p>We proposed a regularization term in Sect. 3, and the hyperparameter λ is used to adjust the regularization term. Figure <ref type="figure" target="#fig_7">6</ref> shows the visual results of FIRE with different λ values. When λ = 0, the regularization term is removed. When λ = 1, the denoising effect of the network is the best. As λ increases, the residual degree of noise increases. This shows that the regularization term can adjust the smoothness and noisiness of denoising. Therefore, choosing an appropriate λ value helps to obtain better denoising results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We proposed a full image-index remainder method (FIRE) using only a single noisy image. The proposed FIRE first divided the whole high-dimensional image space into a series of low-dimensional sub-image spaces with the full image-index remainder technique. Our FIRE retains the complete information within the original noisy image. In addition, we proposed a new regularization optimization function to regularize sub-space image training by reducing the gap between the self-supervised and supervised denoising networks. Quantitative and qualitative experiment results indicate that the proposed FIRE is effective in both DR and CT image denoising.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) and (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. (a) An overview of our proposed FIRE framework. The regularization loss L is calculated as following two stages. First, the subsampled image is calculated as the image reconstruction term Lrec between the network output and the target. Second, we added the regularization term Lreg to correct the difference of implying ground truth between the subsampled noisy image. (b) Details of the remainder sampler based on pixel coordinates. The different color boxes, including blue and pink, respectively represent the transformation from one part of the image to this part after sampling.</figDesc><graphic coords="4,57,48,54,02,337,72,197,68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Visualization comparison of different methods in the clinical brain CT results. The visualization of the results after denoising by different methods is shown above, and the noise gap level between the output image and the original noisy image is shown below.</figDesc><graphic coords="6,57,48,54,23,337,84,112,48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Visual comparison of different methods in real DR denoising results. The boxes of different colors represent images of different parts. The green image is represented as the spine and the blue as the hand. (Color figure online)</figDesc><graphic coords="6,56,97,332,42,338,44,116,44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>4 .</head><label>4</label><figDesc>The clinical DR and brain CT image data are collected from the hospital. The public CT image data came from the 2016 NIH-AAPM Mayo Clinical low-dose CT competition [19] (Moen, Chen, Holmes III, Duan, Yu, Yu, Leng, Fletcher &amp; McCollough 2021). In the three experiments of brain CT, clinical DR, and public clinical CT, the image size was 512 × 512 pixels. The initial learning rate of the ADAM optimizer was set as 0.0003, 0.0003, and 0.0004 respectively. The batch size was set as 1, 16, and 1 respectively. The epoch was set as 40, 200, and 10 respectively to ensure the model was convergent. The regularization term parameter λ was set to λ = 2, 10, and 1 respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Visual comparison of different methods on the public CT dataset. (a) is the reference obtained with the high dose, (b) is the noisy image, (c)-(j) are results of BM3D, DnCNN, DDnet, Noise2Sim, Noise2Void, Blind2Unblind, Neighbor2Neighbor, and our proposed FIRE. PSNR and SSIM values are marked at the top of the picture.</figDesc><graphic coords="7,42,81,219,74,338,08,135,04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. (a)-(j) correspond to the noise power spectrum of Fig. 4. (k) is the profile map of different methods.</figDesc><graphic coords="8,57,96,160,67,336,76,88,36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. The ablation study of the influence of regularization terms. The following results are the results of different λ values. (a) λ = 0, (b) λ = 1, (c) λ = 8, and (d) λ = 16.</figDesc><graphic coords="8,56,97,440,54,338,20,66,28" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work was supported in part by <rs type="funder">National Natural Science Foundation of China</rs> (grant numbers <rs type="grantNumber">62101611</rs> and <rs type="grantNumber">62201628</rs>), <rs type="funder">National Key Research and Development Program of China</rs> (<rs type="grantNumber">2022YFA1204200</rs>), <rs type="funder">Guangdong Basic and Applied Basic Research Foundation</rs> (grant number <rs type="grantNumber">2022A1515011375</rs>, <rs type="grantNumber">2023A1515012278</rs>, <rs type="grantNumber">2023A1515011780</rs>) and <rs type="funder">Shenzhen Science and Technology Program</rs> (grant number <rs type="grantNumber">JCYJ20220530145411027</rs>, <rs type="grantNumber">JCYJ20220818102414031</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_3AjpKU5">
					<idno type="grant-number">62101611</idno>
				</org>
				<org type="funding" xml:id="_jES529A">
					<idno type="grant-number">62201628</idno>
				</org>
				<org type="funding" xml:id="_t7PKycg">
					<idno type="grant-number">2022YFA1204200</idno>
				</org>
				<org type="funding" xml:id="_yukn77C">
					<idno type="grant-number">2022A1515011375</idno>
				</org>
				<org type="funding" xml:id="_MJRf74j">
					<idno type="grant-number">2023A1515012278</idno>
				</org>
				<org type="funding" xml:id="_QZQHrC6">
					<idno type="grant-number">2023A1515011780</idno>
				</org>
				<org type="funding" xml:id="_KJHdmft">
					<idno type="grant-number">JCYJ20220530145411027</idno>
				</org>
				<org type="funding" xml:id="_Bysqd8F">
					<idno type="grant-number">JCYJ20220818102414031</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 44.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Real image denoising with feature attention</title>
		<author>
			<persName><forename type="first">S</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="3155" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A non-local algorithm for image denoising</title>
		<author>
			<persName><forename type="first">A</forename><surname>Buades</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Coll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;05)</title>
		<imprint>
			<date type="published" when="2005">2005. 2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="60" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<title level="m">Spatial-adaptive network for single image denoising</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Low-dose CT with a residual encoder-decoder convolutional neural network</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2524" to="2535" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Image denoising by sparse 3-D transform-domain collaborative filtering</title>
		<author>
			<persName><forename type="first">K</forename><surname>Dabov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2080" to="2095" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">U-Net: deep learning for cell counting, detection, and morphometry</title>
		<author>
			<persName><forename type="first">T</forename><surname>Falk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="67" to="70" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Image processing in digital radiography</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Artz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">digital Radiography Using Storage Phosphor Technology</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="25" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Weighted nuclear norm minimization with application to image denoising</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2862" to="2869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Toward convolutional blind denoising of real photographs</title>
		<author>
			<persName><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="1712" to="1722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hybrid-collaborative Noise2Noise denoiser for low-dose CT images</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Mohebbian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Wahid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Babyn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Radiat. Plasma Med. Sci</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="235" to="244" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Neighbor2Neighbor: a self-supervised framework for deep image denoising</title>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="4023" to="4038" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The use of deep learning towards dose optimization in low-dose computed tomography: a scoping review</title>
		<author>
			<persName><forename type="first">E</forename><surname>Immonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiography</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="208" to="214" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Kashyap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tambwekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Manohara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Subramanyam</surname></persName>
		</author>
		<title level="m">Speech denoising without clean training data: a Noise2Noise approach</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Noise2Void -learning denoising from single noisy images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">O</forename><surname>Buchholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jug</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="2124" to="2132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Digital radiography image quality: image processing and display</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Krupinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Coll. Radiol</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="389" to="400" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Noise2Noise: learning image restoration without clean data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep image prior</title>
		<author>
			<persName><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ulyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="9446" to="9454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">NIRN: self-supervised noisy image reconstruction network for real-world image denoising</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Intell</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Low dose CT image and projection data, LDCT and projection data, version 5, data set, the cancer imaging archive</title>
		<author>
			<persName><forename type="first">C</forename><surname>Mccollough</surname></persName>
		</author>
		<idno type="DOI">10.7937/9NPB-2637</idno>
		<ptr target="https://doi.org/10.7937/9NPB-2637" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Suppression of correlated noise with similarity-based unsupervised deep learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Niu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Blind2Unblind: self-supervised image denoising with visible blind spots</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2017" to="2026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Non-local means variants for denoising of diffusion-weighted and diffusion tensor MRI</title>
		<author>
			<persName><forename type="first">N</forename><surname>Wiest-Daesslé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Prima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Coupé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Morrissey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Barillot</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-540-75759-7_42</idno>
		<ptr target="https://doi.org/10.1007/978-3-540-75759-742" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer-Assisted Intervention -MICCAI 2007</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Ayache</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Ourselin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Maeder</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin; Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="344" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Digital radiography image quality: image acquisition</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Coll. Radiol</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="371" to="388" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A deep normalization and convolutional neural network for image smoke detection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="18429" to="18438" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Self-supervised physics-based denoising for computed tomography</title>
		<author>
			<persName><forename type="first">E</forename><surname>Zainulina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chernyavskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">V</forename><surname>Dylov</surname></persName>
		</author>
		<idno>ArXiv abs/2211.00745</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A sparse-view CT reconstruction method based on combination of DenseNet and deconvolution</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1407" to="1417" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
