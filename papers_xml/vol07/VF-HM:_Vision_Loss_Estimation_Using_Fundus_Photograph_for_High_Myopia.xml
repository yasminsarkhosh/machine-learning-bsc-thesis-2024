<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">VF-HM: Vision Loss Estimation Using Fundus Photograph for High Myopia</title>
				<funder ref="#_qCQmH6w">
					<orgName type="full">The Hong Kong Polytechnic University</orgName>
				</funder>
				<funder ref="#_qkMvZVG">
					<orgName type="full">Research Institute for Artificial Intelligence of Things, The Hong Kong Polytechnic University, HK RGC Research Impact</orgName>
				</funder>
				<funder>
					<orgName type="full">Centre for Myopia Research, School of Optometry</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zipei</forename><surname>Yan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
								<address>
									<settlement>Kowloon, Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Dong</forename><surname>Liang</surname></persName>
							<email>dong1.liang@connect.polyu.hk</email>
							<affiliation key="aff1">
								<orgName type="department">School of Optometry</orgName>
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
								<address>
									<settlement>Kowloon, Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Linchuan</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
								<address>
									<settlement>Kowloon, Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiahang</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhengji</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
								<address>
									<settlement>Kowloon, Hong Kong</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Optometry</orgName>
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
								<address>
									<settlement>Kowloon, Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shuai</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Biomedical Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiannong</forename><surname>Cao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
								<address>
									<settlement>Kowloon, Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chea-Su</forename><surname>Kee</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Optometry</orgName>
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
								<address>
									<settlement>Kowloon, Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">VF-HM: Vision Loss Estimation Using Fundus Photograph for High Myopia</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="649" to="659"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">B401B29652699B7E66368DD90215AE85</idno>
					<idno type="DOI">10.1007/978-3-031-43990-2_61</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T11:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Vision loss estimation</term>
					<term>Visual field</term>
					<term>Fundus photograph</term>
					<term>Ordinal classification</term>
					<term>Auxiliary learning Z. Yan and D. Liang-Equal contribution</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>High myopia (HM) is a leading cause of irreversible vision loss due to its association with various ocular complications including myopic maculopathy (MM). Visual field (VF) sensitivity systematically quantifies visual function, thereby revealing vision loss, and is integral to the evaluation of HM-related complications. However, measuring VF is subjective and time-consuming as it highly relies on patient compliance. Conversely, fundus photographs provide an objective measurement of retinal morphology, which reflects visual function. Therefore, utilizing machine learning models to estimate VF from fundus photographs becomes a feasible alternative. Yet, estimating VF with regression models using fundus photographs fails to predict local vision loss, producing stationary nonsense predictions. To tackle this challenge, we propose a novel method for VF estimation that incorporates VF properties and is additionally regularized by an auxiliary task. Specifically, we first formulate VF estimation as an ordinal classification problem, where each VF point is interpreted as an ordinal variable rather than a continuous one, given that any VF point is a discrete integer with a relative ordering. Besides, we introduce an auxiliary task for MM severity classification to assist the generalization of VF estimation, as MM is strongly associated with vision loss in HM. Our method outperforms conventional regression by 16.61% in MAE metric on a real-world dataset. Moreover, our method is the first work for VF estimation using fundus photographs in HM, allowing for more convenient and accurate detection of vision loss in HM, which could be useful for not only clinics but also large-scale vision screenings.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>High myopia (HM) has become a global concern for public health, with its markedly growing prevalence <ref type="bibr" target="#b9">[10]</ref> and its increased risk of irreversible vision loss and even blindness <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27]</ref>. In brief, excessive axial elongation in HM eyes will produce mechanical stretching on the posterior segment of eyeballs, leading to various structural changes and HM-related complications, e.g., myopic maculopathy (MM), and consequently, functional changes, resulting in vision loss.</p><p>Accurate quantification of vision loss is integral to the early detection and timely treatment for MM and other HM-related complications <ref type="bibr" target="#b15">[16]</ref>. Currently, the diagnosis of vision loss is made on the basis of visual field (VF) sensitivity by standard automated perimetry, which is a systematic metric and gold standard to quantify visual function <ref type="bibr" target="#b18">[19]</ref>. However, measuring VF is prohibitively time-consuming and subjective as it highly requires patients' concentration and compliance during the test <ref type="bibr" target="#b11">[12]</ref>.</p><p>Conversely, imaging techniques, such as fundus photography (a.k.a., fundus), provide a relatively objective and robust measurement of the retinal morphology, which likely corresponds to the VF with an underlying "structure-function relationship" <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b31">32]</ref>. Actually, fundus is most commonly used for the diagnosis and evaluation of HM and its complications, in particular in rural and developing regions, with its lower cost and convenience of acquisition <ref type="bibr" target="#b16">[17]</ref>.</p><p>Therefore, utilizing machine learning models to estimate VF from fundus becomes a promising and feasible alternative for HM subjects in clinical practice. To the best of our knowledge, there is no existing approach to estimate VF from fundus. Some studies have been proposed to estimate the global indices (e.g., mean deviation) of VF from fundus <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b10">11]</ref>, and others estimate VF using retinal thickness <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30]</ref>. It is worth mentioning that, all these studies were conducted for the glaucoma population <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30]</ref>, in which most cases of visual abnormality or defect were likely glaucomatous. However, MM and other HM-related complications may lead to non-glaucomatous vision loss.</p><p>Actually, estimating VF with conventional regression <ref type="bibr" target="#b17">[18]</ref> using fundus fails to predict local vision loss in our HM population, producing stationary nonsense predictions. As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, these predictions from regression exhibit a relatively similar and consistent pattern in most HM subjects, failing to capture/learn the The reason for such failure lies in regression's inability to learn high-entropy feature representations <ref type="bibr" target="#b30">[31]</ref>, which is further confirmed by measuring the entropy of feature representations, as marked in blue in Fig. <ref type="figure" target="#fig_1">2a</ref>.</p><p>To tackle this challenge, we propose a novel method for estimating VF for HM using fundus, namely VF-HM. In general, VF-HM incorporates VF properties and is additionally regularized by an auxiliary task, thereby learning relatively high-entropy feature representations (see the orange line in Fig. <ref type="figure" target="#fig_1">2a</ref>). In detail, we formulate VF estimation as an ordinal classification problem, where each VF point is interpreted as an ordinal variable rather than a continuous one, given that any VF point is a discrete integer with a relative ordering. Besides, we introduce an auxiliary task for MM severity classification to assist the generalization of VF estimation, because MM is strongly associated with vision loss in HM <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b31">32]</ref> and its symptom can be observed from the fundus directly. As a result, VF-HM significantly outperforms conventional regression and accurately predicts vision loss (see Fig. <ref type="figure" target="#fig_0">1</ref>).</p><p>Our contributions are summarized as follows:</p><p>-We propose a novel method, VF-HM, for estimating VF from fundus for HM. VF-HM more accurately detects the local vision loss and significantly outperforms conventional regression by 16.61% in the MAE metric on a real dataset. -VF-HM is the first work for VF estimation using fundus for HM, allowing for more convenient and cost-efficient detection of vision loss in HM, which could be useful for not only clinics but also large-scale vision screenings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem Formulation</head><p>Let D = {(x i , m i )} denote the training set, where x i ∈ X denotes the fundus, m i ∈ M denotes its corresponded VF. And A = {(x i , y i )} denotes the auxiliary set, where y i ∈ Y denotes the MM severity category of a given x i . The objective is to learn a model f : X -→ M by utilizing both D and A. The novelty of this formulation is additionally utilizing the auxiliary set to improve the model's generalization. And challenges mainly come from the following two aspects. First, how to design the model f , as mentioned earlier, conventional regression fails to predict local vision loss. Second, how to properly utilize the auxiliary set to assist the generalization of f , as the auxiliary information is not always helpful during the learning progress, i.e., sometimes may interfere <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b21">22]</ref>.</p><p>3 Proposed Method: VF-HM</p><p>In this section, we first present an overview of the proposed method. Then, we introduce the details of different components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>We present an overview of the proposed method in Fig. <ref type="figure" target="#fig_1">2b</ref>. Specifically, the primary task (denoted by T pri ) is the VF estimation and the auxiliary task is MM classification (denoted by T aux ). Then, our method aims to solve T pri with the assistance of T aux . We propose to parameterize the solution for T pri and T aux by two neural networks: f (•; θ, φ) and g(•; θ, ψ), where they share the same backbone θ and have their own task-specific parameters φ and ψ. Thereafter, the overall objective function is formulated as follows:</p><formula xml:id="formula_0">L = L pri (θ, φ) + λL aux (θ, ψ)<label>(1)</label></formula><p>where L pri and L aux denote the loss function for T pri and T aux , respectively. λ ∈ (0, 1] is a hyper-parameter to control the importance of L aux .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Primary Task: VF Estimation</head><p>The overall interest is only the primary task T pri , which is parameterized by f (•; θ, φ) : X -→ M. Specifically, we formulate T pri as an ordinal classification (aka, rank learning) problem, where each VF point m j i represents an ordinal variable/rank rather than a continuous one. Such a formulation incorporates the distinct properties of VF, which include: 1) Discretization:</p><formula xml:id="formula_1">∀m j i ∈ [0, 40] ∩ Z, that is, any VF value is a positive discrete integer. 2) Ordinalization: m 0 i ≺ m 1 i ≺ ... ≺ m j</formula><p>i , there is a relative order among VF values. To achieve this goal, we extend the ordinal variable/rank into binary labels <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">13]</ref>, i.e., m j i = [r j,1 i , ..., r j,K-1 i ] T where r j,k i ∈ {0, 1} indicates whether m j i exceeds k-th rank or not. To ensure rank-monotonic and guarantee prediction consistency, we utilize the ordinal bias <ref type="bibr" target="#b1">[2]</ref>. In detail, the task-specific parameter φ contains independent bias for each ordinal variable. Thereafter, T pri can be solved by the binary cross-entropy loss, which is defined as follows:</p><formula xml:id="formula_2">L pri (θ, φ) = E (xi,m i)∈X ×M [L BCE (f (x i ; θ, φ), m i )]<label>(2)</label></formula><p>where L BCE (•) denotes the binary cross-entropy loss In addition, we propose to reuse the features from different blocks, as they contain distinct spatial information. Specifically, we propose Multi-scale Feature Fusion (MFF) for aggregating features from different blocks. As highlighted in orange in Fig. <ref type="figure" target="#fig_1">2b</ref>, MFF aggregates features from all blocks at the last in an addition operation. The detailed implementation is reported in Sect. 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Auxiliary Task: MM Classification</head><p>The auxiliary task T aux is introduced only to assist the generalization of T pri . Specifically, T aux is to predict MM severity category y i from fundus x i , which is parameterized by g(•; θ, ψ) : X -→ Y. MM is highly correlated to vision loss <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b31">32]</ref>, and its symptom can be observed from the fundus directly. According to its increasing severity, MM can be classified into five categories <ref type="bibr" target="#b25">[26]</ref>, i.e., C 0 ≺ C 1 ... ≺ C 4 . Therefore, we also interpret the MM category as the ordinal variable/rank. Similar to the label extension in T pri , we extend the MM category into binary labels y i = [r 1 , r 2 , r 3 , r 4 ] T . The loss function L aux for solving T aux is also the binary cross-entropy, which is defined as follows:</p><formula xml:id="formula_3">L aux (θ, ψ) = E (xi,y i )∈X ×Y [L BCE (g(x i ; θ, ψ), y i )]<label>(3)</label></formula><p>However, the T aux is not always helpful for T pri because of the negative transfer <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b21">22]</ref>. The negative transfer refers to a problem that sometimes T aux becomes harmful for T pri . Specifically, let ∇ θ L denote the gradient of Eq. ( <ref type="formula" target="#formula_0">1</ref>) in terms of the shared parameters θ, and it can be decomposed as follows:</p><formula xml:id="formula_4">∇ θ L = ∇ θ L pri + λ∇ θ L aux<label>(4)</label></formula><p>T aux becomes harmful for T pri , when the cosine similarity between ∇ θ L pri and ∇ θ L aux becomes negative <ref type="bibr" target="#b5">[6]</ref>, i.e., cos(∇ θ L aux , ∇ θ L pri ) &lt; 0. Negative transfer is observed in our setting when optimizing Eq. ( <ref type="formula" target="#formula_0">1</ref>) directly, as illustrated in Fig. <ref type="figure" target="#fig_2">3a</ref>. Following <ref type="bibr" target="#b5">[6]</ref>, we mitigate negative transfer by refining ∇ θ L aux . Specifically, we adapt the weighted cosine similarity to refine ∇ θ L aux , which is defined as follows:</p><formula xml:id="formula_5">∇ θ L aux = max (0, cos(∇ θ L aux , ∇ θ L pri )) • ∇ θ L aux (5)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we conduct experiments on a clinic-collected real-world dataset to evaluate the performance of our proposed method<ref type="foot" target="#foot_0">1</ref> . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The Studied Data</head><p>The studied data comes from a HM population, including 75 patients, each with diagnosis information for both eyes. For each eye, there are one fundus, VF, and MM severity category. Specifically, the fundus is captured in colorful mode, the VF is measured in the 24-2 mode with 52 effective points, and MM category is labeled by registered ophthalmologists. Besides, 34 patients (i.e., 68 eyes) have SD-OCT scans in the macular region. For these SD-OCT scans, we extract the retinal thickness with the pre-trained model <ref type="bibr" target="#b19">[20]</ref> in order to compare our method to conventional regression using retinal thickness. According to whether the eye has SD-OCT scans or not, we divide the whole data into a training set and a test set. Specifically, the training and test data contain 68 eyes (from 34 patients) and 82 eyes (from 41 patients), respectively. It is worth mentioning that the training data and test data do not have the same patient. Besides, in the following K-fold cross-validation experiments, we split the training data based on the patient's ID to ensure that there is no information leakage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Setup</head><p>Data Pre-processing. We choose the left eye pattern as our base. For fundus, VF and retinal thickness are not in the left eye pattern, we convert them using the horizontal flip.</p><p>Data Augmentation. Following <ref type="bibr" target="#b0">[1]</ref>, we consolidate a set of data augmentations for both fundus and retinal thickness, respectively. The details are reported in the supplementary material. Different from applying all <ref type="bibr" target="#b0">[1]</ref> augmentations during training, we utilize the TrivialAugment <ref type="bibr" target="#b14">[15]</ref> instead, which randomly selects one from the given data augmentations, generating more diverse augmented data.</p><p>Evaluation Methods. For quantitative evaluation, we utilize three metrics <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b32">33]</ref>: RMSE, MAE and SMAPE. For qualitative evaluation, we visualize two representative predictions on the test set, and more visualized results are presented in the supplementary material. Baseline Methods. We mainly compare our method to conventional regression that estimates VF from fundus. Besides, for a more comprehensive comparison, we also compare our approach to conventional regression using different retinal thicknesses. In detail, we consider three variants: (a) the combination of GCIPL, RNFL and RCL <ref type="bibr" target="#b32">[33]</ref>, (b) the combination of GCIPL and RNFL <ref type="bibr" target="#b17">[18]</ref>, (c) only RNFL <ref type="bibr" target="#b3">[4]</ref>. Due to the limited data, we compare our method to conventional regression using the above thickness by K-fold cross-validation on training data.</p><p>Implementation Details. We utilize the ResNet-18 <ref type="bibr" target="#b7">[8]</ref> as the backbone. For the regression baseline, we use only one linear layer at last. For our method, we use the combination of Conv2D, BatchNorm2D and ReLU as the classification head for T pri . For the MFF, we utilize the above classification head to aggregate features from different blocks. Note that the features from earlier blocks have relatively large features, thus we use AdaptiveAvgPooling2D to perform downsampling first. For T aux , we use only one linear layer as the classifier. For a fair comparison, we train all methods with the same training configurations. Specifically, we train the models with 80 training epochs and the SGD optimizer, where the batch size is set to 32, the learning rate is set to 0.01, momentum is set to 0.9 and L2 weight decay is set to 1e -4 . Besides, we utilize a cosine learning rate decay <ref type="bibr" target="#b8">[9]</ref> to adjust the learning rate per epoch. Finally, we fix all input resolutions to 384 × 384 for both training and evaluation. All experiments are run independently with four seeds: 0, 1, 2, and 3. As for hyper-parameters, we search them on training data with K-fold cross-validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experimental Results</head><p>Main Results. Table <ref type="table" target="#tab_0">1</ref> reports the performance of our method and baselines. In general, our method achieves the best performance compared to these baselines. Specifically, compared to conventional regression using fundus, our method outperforms it by 13.79% and 16.61% according to the RMSE and MAE metric on test data. Besides, our method achieves better performance than baselines using different retinal thicknesses. Visualization of Predictions. As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, we visualize predictions from methods using fundus on two representative cases. Specifically, conventional regression fails to predict local vision loss, as its predictions share a similar and consistent pattern for both cases. In contrast, predictions from our method are more precise, revealing the local vision loss. More visualized results are presented in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>To get a better understanding of the effectiveness of the main components in our proposed method, we conduct a series of ablation studies.</p><p>Effectiveness of Main Components. We first examine the effectiveness of the main components by ablating them. The results are reported in Table <ref type="table" target="#tab_1">2</ref>. In general, we can observe that all components can improve performance except AUX. Specifically, AUX denotes solely introducing the auxiliary task, which brings a degradation, because of the existence of negative transfer. Meanwhile, with the help of Eq. ( <ref type="formula">5</ref>), the negative transfer can be mitigated. Besides, we observe these main components allow the model to learn high-entropy feature representations, thereby improving the model's performance <ref type="bibr" target="#b30">[31]</ref>. More details are reported in the supplementary material.</p><p>Impact of Hyper-parameter λ. We study the impact of the hyper-parameter λ with K-fold cross validation on training data. We choose λ ∈ {1.0, 0.1, 0.01, 0.001, 0.0001}. According to the results shown in Fig. <ref type="figure" target="#fig_2">3b</ref>, we observe that λ = 0.1 achieves the best performance.</p><p>Different Methods for Mitigating the Negative Transfer. We consider three alternatives to refine the auxiliary gradient for mitigating the negative transfer: (1) weighted cosine (WC) similarity <ref type="bibr" target="#b5">[6]</ref> (2) unweighted cosine (UC) similarity <ref type="bibr" target="#b5">[6]</ref> (3) projection (P) <ref type="bibr" target="#b21">[22]</ref>. For a fair comparison, we set λ = 0.1, then conduct experiments on training data with K-fold cross-validation. As shown in Fig. <ref type="figure" target="#fig_2">3c</ref>, and we observe that (1) WC achieves the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we propose VF-HM for estimating VF from fundus for HM, which is the first work for VF estimation in HM; and it provides a more convenient and cost-effective way to detect HM-related vision loss. The major limitations include: first, our sample size is limited; second, we utilize both eyes from one patient as two independent inputs, which ignores their similarity; third, we only include the MM severity as the auxiliary information. Future work could be conducted as follows. First, collecting more data from different clinical sites. Second, modeling the relationship between both eyes from the same patient <ref type="bibr" target="#b32">[33]</ref>.</p><p>Third, exploring more auxiliary information. Besides, studying how to adapt our method to different domains is a crucial problem <ref type="bibr" target="#b23">[24]</ref>, as we seek to improve the generalizability. In addition, exploring VF prediction with the missing modalities <ref type="bibr" target="#b22">[23]</ref>: either fundus or thickness is another interesting direction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Estimated VF from different methods using fundus. GT denotes the ground truth, Reg denotes the regression baseline, and Ours denotes our method.</figDesc><graphic coords="2,41,79,53,84,340,33,69,85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. (a) The entropy of feature space on training data during training progress from conventional regression (denoted by Reg) and our VF-HM. (b) An overview of our proposed method: VF-HM.</figDesc><graphic coords="3,64,74,54,35,327,73,78,55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Visualization of (a) Negative transfer when optimizing Eq. (1) directly, (b) Impact of hyper-parameter λ, and (c) Different methods for mitigating the negative transfer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Main results. 'K-fold' denotes performance from K-fold cross-validation on training data. 'Test' denotes performance on test data (pre-trained on training data). (↓) denotes the lower value indicates better performance. RT-(•) denotes different retinal thicknesses. And the better results are bold-faced.</figDesc><table><row><cell>Method</cell><cell>Modality</cell><cell>K-fold(K = 5) RMSE (↓) MAE (↓)</cell><cell cols="2">Test SMAPE (↓) RMSE (↓) MAE (↓)</cell><cell>SMAPE (↓)</cell></row><row><cell>Regression</cell><cell>RT-(a)</cell><cell cols="2">4.94 ± 0.23 3.12 ± 0.05 13.47 ± 0.16 -</cell><cell>-</cell><cell>-</cell></row><row><cell>Regression</cell><cell>RT-(b)</cell><cell cols="2">4.80 ± 0.17 3.04 ± 0.12 13.21 ± 0.36 -</cell><cell>-</cell><cell>-</cell></row><row><cell>Regression</cell><cell>RT-(c)</cell><cell cols="2">4.86 ± 0.22 3.13 ± 0.18 13.42 ± 0.57 -</cell><cell>-</cell><cell>-</cell></row><row><cell>Regression</cell><cell cols="5">Fundus 4.62 ± 0.07 2.95 ± 0.07 12.94 ± 0.32 4.28 ± 0.03 2.89 ± 0.06 12.13 ± 0.30</cell></row><row><cell cols="6">Ours(λ=0.1) Fundus 4.44 ± 0.27 4.44 ± 0.27 2.78 ± 0.10 2.78 ± 0.10 12.50 ± 0.26 12.50 ± 0.26 3.69 ± 0.03 3.69 ± 0.03 2.41 ± 0.04 2.41 ± 0.04 11.38 ± 0.14 11.38 ± 0.14</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation study on main components. OC denotes the ordinal classification baseline. MFF denotes multi-scale feature fusion. AUX denotes the auxiliary task. MNT denotes mitigating negative transfer from Eq. (5).</figDesc><table><row><cell>OC MFF AUX MNT RMSE (↓) MAE (↓)</cell></row><row><cell>3.69 ± 0.03 3.69 ± 0.03 2.41 ± 0.04 2.41 ± 0.04</cell></row><row><cell>3.74 ± 0.02 2.46 ± 0.03</cell></row><row><cell>3.73 ± 0.04 2.45 ± 0.02</cell></row><row><cell>3.77 ± 0.02 2.49 ± 0.03</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Our code is available at https://github.com/yanzipei/VF-HM.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work was supported by the <rs type="funder">Research Institute for Artificial Intelligence of Things, The Hong Kong Polytechnic University, HK RGC Research Impact</rs> Fund No. <rs type="grantNumber">R5060-19</rs>; and the <rs type="funder">Centre for Myopia Research, School of Optometry</rs>; the <rs type="institution">Research Centre for SHARP Vision (RCSV)</rs>, <rs type="funder">The Hong Kong Polytechnic University</rs>; and <rs type="institution">Centre for Eye and Vision Research (CEVR)</rs>, <rs type="projectName">InnoHK CEVR</rs> Project <rs type="grantNumber">1.5</rs>, <rs type="affiliation">17W Hong Kong Science Park, HKSAR</rs>. We thank <rs type="person">Drs Rita Sum</rs> and <rs type="person">Vincent Ng</rs> for their guidance on data analysis of clinical population; and <rs type="person">Prof. Ruihua Wei</rs> for external validation of the model.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_qkMvZVG">
					<idno type="grant-number">R5060-19</idno>
				</org>
				<org type="funded-project" xml:id="_qCQmH6w">
					<idno type="grant-number">1.5</idno>
					<orgName type="project" subtype="full">InnoHK CEVR</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2_61.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Impact of data augmentation on retinal oct image segmentation for diabetic macular edema analysis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bar-David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bar-David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soudry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fischer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="148" to="158" />
		</imprint>
		<respStmt>
			<orgName>MICCAI</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Rank consistent ordinal regression for neural networks with application to age estimation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mirjalili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Raschka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn. Lett</title>
		<imprint>
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="page" from="325" to="331" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep learning approaches predict glaucomatous visual field damage from OCT optic nerve head EN face images and retinal nerve fiber layer thickness maps</title>
		<author>
			<persName><forename type="first">M</forename><surname>Christopher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ophthalmology</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="346" to="356" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Retinervenet: using recursive deep learning to estimate pointwise 24-2 visual field data based on retinal structure</title>
		<author>
			<persName><forename type="first">S</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Mariottoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Jammal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Medeiros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Auxiliary task update decomposition: The good, the bad and the neutral</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Dery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Farajtabar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.02224</idno>
		<title level="m">Adapting auxiliary losses using gradient similarity</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Long-term pattern of progression of myopic maculopathy: a natural history study</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hayashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ophthalmology</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1595" to="1611" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Bag of tricks for image classification with convolutional neural networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="558" to="567" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Global prevalence of myopia and high myopia and temporal trends from 2000 through 2050</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Holden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ophthalmology</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1036" to="1042" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Estimating visual field loss from monoscopic optic disc photography using deep learning model</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Variability of quantitative automated perimetry in normal observers</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Keltner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Labermeier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ophthalmology</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="878" to="881" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Ordinal regression by extended binary classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="865" to="872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Classification of visual field abnormalities in highly myopic eyes without pathologic change</title>
		<author>
			<persName><forename type="first">F</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ophthalmology</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="803" to="812" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Trivialaugment: tuning-free yet state-of-the-art data augmentation</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="754" to="762" />
		</imprint>
		<respStmt>
			<orgName>ICCV</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">K</forename><surname>Ohno-Matsui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IMI pathologic myopia. Investigative Ophthalmol. Visual Sci</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="5" to="5" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fundus photography in the 21st century-a review of recent technological advances and their implications for worldwide healthcare</title>
		<author>
			<persName><forename type="first">N</forename><surname>Panwar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Telemedicine and e-Health</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="198" to="208" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A deep learning approach to predict visual field using optical coherence tomography</title>
		<author>
			<persName><forename type="first">K</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The value of visual field testing in the era of advanced imaging: clinical and psychophysical perspectives</title>
		<author>
			<persName><forename type="first">J</forename><surname>Phu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Khuu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yapp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Assaad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Hennessy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kalloniatis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Clin. Exp. Optom</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="313" to="332" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Relaynet: retinal layer and fluid segmentation of macular optical coherence tomography using fully convolutional networks</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomed. Opt. Express</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3627" to="3642" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Myopic maculopathy: a review</title>
		<author>
			<persName><forename type="first">R</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ophthalmologica</title>
		<imprint>
			<biblScope unit="volume">228</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="197" to="213" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning through auxiliary tasks</title>
		<author>
			<persName><surname>Vivien</surname></persName>
		</author>
		<ptr target="https://vivien000.github.io/blog/journal/learning-though-auxiliary_tasks.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Prototype knowledge distillation for medical segmentation with missing modality</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Feature alignment and uniformity for test time adaptation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Epidemiology and disease burden of pathologic myopia and myopic choroidal neovascularization: an evidence-based systematic review</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Am. J. Ophthalmol</title>
		<imprint>
			<biblScope unit="volume">157</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="9" to="25" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Distribution and severity of myopic maculopathy among highly myopic eyes</title>
		<author>
			<persName><forename type="first">O</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Investigative Ophthalmol. Visual Sci</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4880" to="4885" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Structural abnormalities in the papillary and peripapillary areas and corresponding visual field defects in eyes with pathologic myopia</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Investigative Ophthal. Visual Sci</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="13" to="13" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Predicting the glaucomatous central 10-degree visual field from optical coherence tomography using deep learning and tensor regression</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Am. J. Ophthalmol</title>
		<imprint>
			<biblScope unit="volume">218</biblScope>
			<biblScope unit="page" from="304" to="313" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pami: a computational module for joint estimation and progression prediction of glaucoma</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Asaoka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kiwaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Murata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fujino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yamanishi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KDD</title>
		<imprint>
			<biblScope unit="page" from="3826" to="3834" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Improving visual field trend analysis with oct and deeply regularized latent-space linear regression</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ophthalmol. Glaucoma</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="78" to="88" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Improving deep regression with ordinal entropy</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Morphological characteristics and visual acuity of highly myopic eyes with different severities of myopic maculopathy</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Retina</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="461" to="467" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Glaucoma progression prediction using retinal thickness via latent space linear regression</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KDD</title>
		<imprint>
			<biblScope unit="page" from="2278" to="2286" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
