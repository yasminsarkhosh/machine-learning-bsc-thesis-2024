<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Reliable Multimodality Eye Disease Screening via Mixture of Student’s t Distributions</title>
				<funder ref="#_JPHg7bB">
					<orgName type="full">National Research Foundation, Singapore</orgName>
				</funder>
				<funder ref="#_AfeAnJY">
					<orgName type="full">Science and Technology Department of Sichuan Province</orgName>
				</funder>
				<funder ref="#_debZW6H">
					<orgName type="full">China Scholarship Council</orgName>
				</funder>
				<funder ref="#_yM8Veks">
					<orgName type="full">A*STAR Central Research Fund</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ke</forename><surname>Zou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">National Key Laboratory of Fundamental Science on Synthetic Vision</orgName>
								<orgName type="institution">Sichuan University</orgName>
								<address>
									<settlement>Sichuan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="institution">Sichuan University</orgName>
								<address>
									<settlement>Sichuan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tian</forename><surname>Lin</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Joint Shantou International Eye Center</orgName>
								<orgName type="institution" key="instit1">Shantou University</orgName>
								<orgName type="institution" key="instit2">Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Guangdong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Medical College</orgName>
								<orgName type="institution">Shantou University</orgName>
								<address>
									<settlement>Guangdong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xuedong</forename><surname>Yuan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">National Key Laboratory of Fundamental Science on Synthetic Vision</orgName>
								<orgName type="institution">Sichuan University</orgName>
								<address>
									<settlement>Sichuan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="institution">Sichuan University</orgName>
								<address>
									<settlement>Sichuan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Haoyu</forename><surname>Chen</surname></persName>
							<email>drchenhaoyu@gmail.com</email>
							<affiliation key="aff2">
								<orgName type="department">Joint Shantou International Eye Center</orgName>
								<orgName type="institution" key="instit1">Shantou University</orgName>
								<orgName type="institution" key="instit2">Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Guangdong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Medical College</orgName>
								<orgName type="institution">Shantou University</orgName>
								<address>
									<settlement>Guangdong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaojing</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">National Key Laboratory of Fundamental Science on Synthetic Vision</orgName>
								<orgName type="institution">Sichuan University</orgName>
								<address>
									<settlement>Sichuan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">College of Mathematics</orgName>
								<orgName type="institution">Sichuan University</orgName>
								<address>
									<settlement>Sichuan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
							<affiliation key="aff5">
								<orgName type="institution">Institute of High Performance Computing, A*STAR</orgName>
								<address>
									<settlement>Singapore</settlement>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Huazhu</forename><surname>Fu</surname></persName>
							<affiliation key="aff5">
								<orgName type="institution">Institute of High Performance Computing, A*STAR</orgName>
								<address>
									<settlement>Singapore</settlement>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Reliable Multimodality Eye Disease Screening via Mixture of Student’s t Distributions</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="596" to="606"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">6B5BF4FFCD74570085D251C5F856C686</idno>
					<idno type="DOI">10.1007/978-3-031-43990-2_56</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T11:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Multimodality</term>
					<term>uncertainty estimation</term>
					<term>eye disease</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multimodality eye disease screening is crucial in ophthalmology as it integrates information from diverse sources to complement their respective performances. However, the existing methods are weak in assessing the reliability of each unimodality, and directly fusing an unreliable modality may cause screening errors. To address this issue, we introduce a novel multimodality evidential fusion pipeline for eye disease screening, EyeMoSt, which provides a measure of confidence for unimodality and elegantly integrates the multimodality information from a multi-distribution fusion perspective. Specifically, our model estimates both local uncertainty for unimodality and global uncertainty for the fusion modality to produce reliable classification results. More importantly, the proposed mixture of Student's t distributions adaptively integrates different modalities to endow the model with heavy-tailed properties, increasing robustness and reliability. Our experimental findings on both public and in-house datasets show that our model is more reliable than current methods. Additionally, EyeMost has the potential ability to serve as a data quality discriminator, enabling reliable decision-making for multimodality eye disease screening.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Retinal fundus images and Optical Coherence Tomography (OCT) are common 2D and 3D imaging techniques used for eye disease screening. Multimodality learning usually provides more complementary information than unimodality learning <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b31">31]</ref>. This motivates researchers to integrate multiple modalities to improve the performance of eye disease screening. Current multimodality learning methods can be roughly classified into early, intermediate, and late fusion, depending on the fusion stage <ref type="bibr" target="#b1">[2]</ref>. For multimodality ophthalmic image learning, recent works have mainly focused on the early fusion <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b23">23]</ref> and intermediate fusion stages <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b27">27]</ref>. Early fusion-based approaches integrate multiple modalities directly at the data level, usually by concatenating the raw or preprocessed multimodality data. Hua et al. <ref type="bibr" target="#b9">[10]</ref> combined preprocessed fundus images and wide-field swept-source optical coherence tomography angiography at the early stage and then extracted representational features for diabetic retinopathy recognition. Intermediate fusion strategies allow multiple modalities to be fused at different intermediate layers of the neural networks. He et al. <ref type="bibr" target="#b8">[9]</ref> extracted different modality features with convolutional block attention module <ref type="bibr" target="#b28">[28]</ref> and modality-specific attention mechanisms, then concatenated them to realize the multimodality fusion for retinal image classification. However, few studies have explored multimodality eye disease screening at the late fusion stage. Furthermore, the above methods do not adequately assess the reliability of each unimodality, and may directly fuse an unreliable modality with others. This could lead to screening errors and be challenging for real-world clinical safety deployment. To achieve this goal, we propose a reliable framework for the multimodality eye disease screening, which provides a confidence (uncertainty) measure for each unimodality and adaptively fuses multimodality predictions in principle.</p><p>Uncertainty estimation is an effective way to provide a measure of reliability for ambiguous network predictions. The current uncertainty estimation methods mainly include Bayesian neural networks, deep ensemble methods, and deterministic-based methods. Bayesian neural networks <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b22">22]</ref> learn the distribution of network weights by treating them as random variables. However, these methods are affected by the challenge of convergence and have a large number of computations. The dropout method has alleviated this issue to a certain extent <ref type="bibr" target="#b11">[12]</ref>. Another uncertainty estimation way is to learn an ensemble of deep networks <ref type="bibr" target="#b13">[14]</ref>. Recently, to alleviate computational complexity and overconfidence <ref type="bibr" target="#b25">[25]</ref>, deterministic-based methods <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b32">32]</ref> have been proposed to directly output uncertainty in a single forward pass through the network. For multimodal uncertainty estimation, the Trusted Multi-view Classification (TMC) <ref type="bibr" target="#b7">[8]</ref> is a representative method that proposes a new paradigm of multiview learning by dynamically integrating different views at the evidence level. However, TMC has a limited ability to detect Out-Of-Distribution (OOD) samples <ref type="bibr" target="#b10">[11]</ref>. This attributes to TMC is particularly weak in modeling epistemic uncertainty for each single view <ref type="bibr" target="#b11">[12]</ref>. Additionally, the fusion rule in TMC fails to account for conflicting views, making it unsuitable for safety-critical deployment <ref type="bibr" target="#b30">[30]</ref>. To address these limitations, we propose EyeMoSt, a novel evidential fusion method that models both aleatoric and epistemic uncertainty in unimodality, while efficiently integrating different modalities from a multi-distribution fusion perspective.</p><p>In this work, we propose a novel multimodality eye disease screening method, called EyeMoSt, that conducts Fundus and OCT modality fusion in a reliable manner. Our EyeMoSt places Normal-inverse Gamma (NIG) prior distributions over the pre-trained neural networks to directly learn both aleatoric and epistemic uncertainty for unimodality. Moreover, Our EyeMoSt introduces the Mixture of Student's t (MoSt) distributions, which provide robust classification results with global uncertainty. More importantly, MoSt endows the model with robustness under heavy-tailed property awareness. We conduct sufficient experiments on two datasets for different eye diseases (e.g., glaucoma grading, age-related macular degeneration, and polypoid choroidal vasculopathy) to verify the reliability and robustness of the proposed method. In summary, the key contributions are as follows: 1) We propose a novel multimodality eye disease screening method, EyeMoSt, which conducts reliable fusion of Fundus and OCT modalities.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>In this section, we introduce the overall framework of our EyeMoSt, which efficiently estimates the aleatoric and epistemic uncertainty for unimodality and adaptively integrates Fundus and OCT modalities in principle. As shown in Fig. <ref type="figure" target="#fig_0">1</ref> (a), we first employ the 2D/3D neural network encoders to capture different modality features. Then, we place multi-evidential heads after the trained networks to model the parameters of higher-order NIG distributions for unimodality. To merge these predicted distributions, We derive the posterior predictive of the NIG distributions as Student's t (St) distributions. Particularly, the Mixture of Student's t (MoSt) distributions is introduced to integrate the distributions of different modalities in principle. Finally, we elaborate on the training pipeline for the model evidence acquisition.</p><p>Given a multimodality eye dataset</p><formula xml:id="formula_0">D = x i m M m=1</formula><p>and the corresponding label y i , the intuitive goal is to learn a function that can classify different categories. Fundus and OCT are common imaging modalities for eye disease screening. Therefore, here M = 2, x i 1 and x i 2 represent Fundus and OCT input modality data, respectively. We first train 2D encoder Θ of Res2Net <ref type="bibr" target="#b6">[7]</ref> and 3D encoder Φ of MedicalNet <ref type="bibr" target="#b4">[5]</ref> to identify the feature-level informativeness, which can be defined as Θ x i 1 and Φ x i 2 , respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Uncertainty Estimation for Unimodality</head><p>We extend the deep evidential regression model <ref type="bibr" target="#b0">[1]</ref> to multimodality evidential classification for eye disease screening. To this end, to model the uncertainty for Fundus or OCT modality, we assume that the observe label y i is drawn from a Gaussian N y i |μ, σ 2 , whose mean and variance are governed by an evidential prior named the NIG distribution:</p><formula xml:id="formula_1">NIG μ, σ 2 |p m = N μ|γ m , σ 2 δ m Γ -1 σ 2 |α m , β m , (<label>1</label></formula><formula xml:id="formula_2">)</formula><p>where Γ -1 is an inverse-gamma distribution, γ m ∈ R, δ m &gt; 0, α m &gt; 1, β m &gt; 0 are the learning parameters. Specifically, the multi-evidential heads will be placed after the encoders Θ and Φ (as shown in Fig. <ref type="figure" target="#fig_2">1 (a)</ref>), which outputs the prior NIG parameters p m = (γ m , δ m , α m , β m ). As a result, the aleatoric (AL) and epistemic (EP) uncertainty can be estimated by the E σ 2 and the Var [μ], respectively, as:</p><formula xml:id="formula_3">AL = E σ 2 = α m β m -1 , EP = Var [μ] = β m δ m (α m -1) . (<label>2</label></formula><formula xml:id="formula_4">)</formula><p>Then, given the evidence distribution parameter p m , the marginal likelihood is calculated by marginalizing the likelihood parameter:</p><formula xml:id="formula_5">p y i |x i m , p m = μ σ 2 p y i |x i m , μ, σ 2 NIG μ, σ 2 |p m dμdσ 2 . (<label>3</label></formula><formula xml:id="formula_6">)</formula><p>Interacted by the prior and the Gaussian likelihood of each unimodality <ref type="bibr" target="#b0">[1]</ref>, its analytical solution does exist and yields an St prediction distribution as:</p><formula xml:id="formula_7">p y i |x i m , p m = Γ α m + 1 2 Γ (α m ) δ m 2πβ m (1 + δ m ) 1 + δ m y i -γ m 2 2β m (1 + δ m ) -(αm+ 1 2 ) = St y i ; γ m , o m , 2α m ,<label>(4)</label></formula><p>with </p><formula xml:id="formula_8">o m = β m (1 + δ m ) δ m α m .</formula><formula xml:id="formula_9">i ; u m , Σ m , v m = St y i ; γ m , o m , 2α m , with u m ∈ R, Σ m &gt; 0, v m &gt; 2.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Mixture of Student's t Distributions (MoSt)</head><p>Then, we focus on fusing multiple St Distributions from different modalities. How to rationally integrate multiple Sts into a unified St is the key issue. To this end, the joint modality of distribution can be denoted as:</p><formula xml:id="formula_10">St (y i ; u F , Σ F , v F ) = St y i ; u i 1 u i 2 , Σ 11 Σ 12 Σ 21 Σ 22 , v i 1 v i 2 . (<label>5</label></formula><formula xml:id="formula_11">)</formula><p>In order to preserve the closed St distribution form and the heavy-tailed properties of the fusion modality, the updated parameters are given by <ref type="bibr" target="#b24">[24]</ref>. In simple terms, we first adjust the degrees of freedom of the two distributions to be consistent. As shown in Fig. <ref type="figure" target="#fig_0">1</ref> (b), the smaller values of degrees of freedom (DOF) v has heavier tails. Therefore, we construct the decision value τ m = v m to approximate the parameters of the fused distribution. We assume that multiple St distributions are still an approximate St distribution after fusion. Assuming that the degrees of freedom of τ 1 are smaller than τ 2 , then, the fused St distribution St (y i ; u F , Σ F , v F ) will be updated as:</p><formula xml:id="formula_12">v F =v 1 , u F =u 1 , Σ F = 1 2 Σ 1 + v 2 (v 1 -2) v 1 (v 2 -2) Σ 2 . (<label>6</label></formula><formula xml:id="formula_13">)</formula><p>More intuitively, the above formula determines the modality with a stronger heavy-tailed attribute. That is, according to the perceived heavy-tailed attribute of each modality, the most robust modality is selected as the fusion modality. Finally, the prediction and uncertainty of the fusion modality is given by:</p><formula xml:id="formula_14">ŷi = E p(x i F ,pF ) y i = u F , ÛF = E σ 2 F = Σ F v F v F -2 . (<label>7</label></formula><formula xml:id="formula_15">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Learning the Evidential Distributions</head><p>Under the evidential learning framework, we expect more evidence to be collected for each modality, thus, the proposed model is expected to maximize the likelihood function of the model evidence. Equivalently, the model is expected to minimize the negative log-likelihood function, which can be expressed as:</p><formula xml:id="formula_16">L NLL m = log Γ (α m ) π δm Γ α m + 1 2 -α m log (2β m (1 + δ m )) + α m + 1 2 log (y i -γ m ) 2 δ m + 2β m (1 + δ m ) . (<label>8</label></formula><formula xml:id="formula_17">)</formula><p>Then, to fit the classification tasks, we introduce the cross entropy term L CE m :</p><formula xml:id="formula_18">L NIG m = L NLL m + λL CE m , (<label>9</label></formula><formula xml:id="formula_19">)</formula><p>where λ is the balance factor set to 0.5. For further information on the selection of hyperparameter λ, please refer to Supplementary S2. Similarly, for the fusion modality, we first maximize the likelihood function of the model evidence as follows:</p><formula xml:id="formula_20">L NLL F = logΣ F + log Γ vF 2 Γ vF +1 2 + log √ v F π + (v F + 1) 2 log 1 + (y i -u F ) 2 v F Σ F , (<label>10</label></formula><p>) Complete derivations of Eq. 8 are available in Supplementary S1.2. Then, to achieve better classification performance, the cross entropy term L CE m is also introduced into Eq. 8 as below:</p><formula xml:id="formula_21">L St F = L NLL F + λL CE F ,<label>(11)</label></formula><p>Totally, the evidential learning process for multimodality screening can be denoted as:</p><formula xml:id="formula_22">L all = M m=1 L NIG m + L St F . (<label>12</label></formula><formula xml:id="formula_23">)</formula><p>In this paper, we mainly consider the fusion of two modalities, M = 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>Datasets: In this paper, we verify the effectiveness of EyeMoSt on the two datasets. For the glaucoma recognition, We validate the proposed method on the GAMMA <ref type="bibr" target="#b29">[29]</ref> dataset. It contains 100 paired cases with a three-level glaucoma grading. They are divided into the training set and test set with 80 and 20 respectively. We conduct the five-fold cross-validation on it to prevent performance improvement caused by accidental factors. Then, we test our method on the in-house collected dataset, which includes Age-related macular degeneration (AMD) and polypoid choroidal vasculopathy (PCV) diseases. They are divided into training, validation, and test sets with 465, 69, and 70 cases respectively. More details of the dataset can be found in Supplementary S2. Both of these datasets are including the paired cases of Fundus (2D) and OCT (3D).  Training Details: Our proposed method is implemented in PyTorch and trained on NVIDIA GeForce RTX 3090. Adam optimization <ref type="bibr" target="#b12">[13]</ref> is employed to optimize the overall parameters with an initial learning rate of 0.0001. The maximum of epoch is 100. The data augmentation techniques for GAMMA dataset are similar to <ref type="bibr" target="#b2">[3]</ref>, including random grayscaling, random color jitter, and random horizontal flipping. All inputs are uniformly adjusted to 256 × 256 and 128 × 256 × 128 for Fundus and OCT modalities. The batch size is 16.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Compared Methods and Metrics:</head><p>We compare the following six methods: For different fusion stage strategies, a) B-EF Baseline of the early fusion <ref type="bibr" target="#b9">[10]</ref> strategy, b) B-IF Baseline of the intermediate typical fusion method, c) M 2 LC <ref type="bibr" target="#b28">[28]</ref> of the intermediate fusion method and the later fusion method d) TMC <ref type="bibr" target="#b7">[8]</ref> are used as comparisons. B-EF is first integrated at the data level, and then passed through the same MedicalNet <ref type="bibr" target="#b4">[5]</ref>. B-IF first extracts features by the encoders (same with us), and then concatenates their output features as the final prediction. For the uncertainty quantification methods, e) MCDO (Monte Carlo Dropout) employs the test time dropout as an approximation of a Bayesian neural network <ref type="bibr" target="#b5">[6]</ref>. f) DE (Deep ensemble) quantifies the uncertainties by ensembling multiple models <ref type="bibr" target="#b13">[14]</ref>. We adopt the accuracy (ACC) and Kappa metrics for intuitive comparison with different methods. Particularly, expected calibration error (ECE) <ref type="bibr" target="#b20">[20]</ref> is used to compare the calibration of the uncertainty algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison and Analysis:</head><p>We reported our algorithm with different methods on the GAMMA and in-house datasets in Table <ref type="table" target="#tab_2">1</ref>. First, we compare these methods under the clean multimodality eye data. Our method obtained competitive results in terms of ACC and Kappa. Then, to verify the robustness of our model, we added Gaussian noise to Fundus or OCT modality (σ = 0.1/0.3) on  the two datasets. Compared with other methods, our EyeMoSt maintains classification accuracy in noisy OCT modality, while comparable in noisy Fundus modality. More generally, we added different Gaussian noises to Fundus or OCT modality, as shown in Fig. <ref type="figure" target="#fig_1">2</ref>. The same conclusion can be drawn from Fig. <ref type="figure" target="#fig_1">2</ref>. This is attributed to the perceived long tail in the data when fused. The visual comparisons of different noises to the Fundus/OCT modality on the in-house dataset can be found in Supplementary S2. To further quantify the reliability of uncertainty estimation, we compared different algorithms using the ECE indicator.</p><p>As shown in Table <ref type="table" target="#tab_2">1</ref> and Fig. <ref type="figure" target="#fig_1">2</ref>, our proposed algorithm performs better in both clean and single pollution modalities. The inference times of the uncertaintybased methods on two modalities on the in-house dataset are 5.01 s (MCDO), 8.28 s (DE), 3.98 s (TMC), and 3.22 s (Ours). It can be concluded that the running time of EyeMost is lower than other methods. In brief, we conclude that our proposed model is more robust and reliable than the above methods.</p><p>Understanding Uncertainty for Unimodality/Multimodality Eye Data: To make progress towards the multimodality ophthalmic clinical application of uncertainty estimation, we conducted unimodality and multimodality uncertainty analysis for eye data. First, we add more Gaussian noise with varying variances to the unimodality (Fundus or OCT) in the GAMMA and in-house datasets to simulate OOD data. The original samples without noise are denoted as in-distribution (ID) data. Figure <ref type="figure" target="#fig_4">3</ref> (a) shows a strong relationship between uncertainty and OOD data. Uncertainty in unimodality images increases positively with noise. Here, uncertainty acts as a tool to measure the reliable unimodality eye data. Second, we analyze the uncertainty density of unimodality and fusion modality before and after adding Gaussian noise. As shown in Fig. <ref type="figure" target="#fig_4">3</ref> (b), take adding noise with σ = 0.1 to the Fundus modality on the GAMMA dataset as an example. Before the noise is added, the uncertainty distributions of unimodality and fusion modality are relatively concentrated. After adding noise, the uncertainty distribution of the fusion modality is closer to that of the modality without noise. Hence, EyeMoSt can serve as a tool for measuring the reliable modality in ophthalmic multimodality data fusion. To this end, our algorithm can be used as an out-of-distribution detector and data quality discriminator to inform reliable and robust decisions for multimodality eye disease screening.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we propose the EyeMoSt for reliable and robust screening of eye diseases using evidential multimodality fusion. Our EyeMoSt produces the uncertainty for unimodality and then adaptively fuses different modalities in a distribution perspective. The different NIG evidence priors are employed to model the distribution of encoder observations, which supports the backbones to directly learn aleatoric and epistemic uncertainty. We then derive an analytical solution to the Student's t distributions of the NIG evidence priors on the Gaussian likelihood function. Furthermore, we propose the MoSt distributions in principle adaptively integrates different modalities, which endows the model with heavy-tailed properties and is more robust and reliable for eye disease screening. Extensive experiments show that the robustness and reliability of our method in classification and uncertainty estimation on GAMMA and in-house datasets are competitive with previous methods. Overall, our approach has the potential to multimodality eye data discriminator for trustworthy medical AI decision-making. In future work, our focus will be on incorporating uncertainty into the training process and exploring the application of reliable multimodality screening for eye diseases in a clinical setting.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Reliable multimodality Eye Disease Screening pipeline. (a) Overall framework of EyeMoSt. (b) Student's t Distributions with different degrees of freedom. (c) The overall learning process of EyeMoSt.</figDesc><graphic coords="3,41,79,53,90,340,33,162,82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>2 )</head><label>2</label><figDesc>Our EyeMoSt introduces the MoSt distributions, which provide robust classification results with local and global uncertainty. 3) We conduct extensive experiments on two datasets for different eye diseases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1</head><label>1</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Accuracy and ECE performance of different algorithms in contaminated single modality with different levels of noise on GAMMA and in-house datasets.</figDesc><graphic coords="8,65,34,63,14,328,24,118,96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Uncertainty density of unimodality and multimodality eye data.</figDesc><graphic coords="8,58,98,246,41,334,72,65,08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The complete derivations of Eq. 4 are available in Supplementary S1.1. Thus, the two modalities distributions are transformed into the student's t Distributions St y</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Comparisons with different algorithms on the GAMMA and in-house dataset. F and O denote the Fundus and OCT modalities, respectively. The top-2 results are highlighted in Red and Blue. Higher ACC and Kappa, and Lower ECE mean better.</figDesc><table><row><cell>Methods</cell><cell cols="2">GAMMA dataset</cell><cell cols="2">In-house dataset</cell></row><row><cell></cell><cell>Original</cell><cell>Gaussian noise</cell><cell>Original</cell><cell>Gaussian noise</cell></row><row><cell></cell><cell></cell><cell>σ = 0.1 (F) σ = 0.3 (O)</cell><cell></cell><cell>σ = 0.1 (F) σ = 0.3 (O)</cell></row><row><cell></cell><cell cols="4">ACC Kappa ACC Kappa ACC Kappa ACC ECE ACC ECE ACC ECE</cell></row><row><cell>B-IF</cell><cell cols="4">0.700 0.515 0.623 0.400 0.530 0.000 0.800 0.250 0.593 0.450 0.443 0.850</cell></row><row><cell cols="5">B-EF [10] 0.660 0.456 0.660 0.452 0.500 0.000 0.800 0.200 0.777 0.223 0.443 0.557</cell></row><row><cell cols="5">M 2 LC [28] 0.710 0.527 0.660 0.510 0.500 0.000 0.814 0.186 0.786 0.214 0.443 0.557</cell></row><row><cell cols="5">MCDO [6] 0.758 0.636 0.601 0.341 0.530 0.000 0.786 0.214 0.771 0.304 0.429 0.571</cell></row><row><cell>DE [14]</cell><cell cols="4">0.710 0.539 0.666 0.441 0.530 0.000 0.800 0.200 0.800 0.200 0.609 0.391</cell></row><row><cell>TMC [8]</cell><cell cols="4">0.810 0.658 0.430 0.124 0.550 0.045 0.829 0.171 0.814 0.186 0.443 0.557</cell></row><row><cell>Our</cell><cell cols="4">0.850 0.754 0.663 0.458 0.830 0.716 0.829 0.171 0.800 0.200 0.829 0.171</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Our code has been released in https://github.com/Cocofeat/EyeMoSt.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>The ethical approval of this dataset was obtained from the Ethical Committee.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>. This work was supported by the <rs type="funder">National Research Foundation, Singapore</rs> under its <rs type="programName">AI Singapore Programme (AISG Award</rs> No: <rs type="grantNumber">AISG2-TC-2021-003</rs>), A*<rs type="programName">STAR AME Programmatic Funding Scheme Under Project A20H4b0141</rs>, <rs type="funder">A*STAR Central Research Fund</rs>, the <rs type="funder">Science and Technology Department of Sichuan Province</rs> (Grant No. <rs type="grantNumber">2022YFS0071 &amp; 2023YFG0273</rs>), and the <rs type="funder">China Scholarship Council</rs> (No. <rs type="grantNumber">202206240082</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_JPHg7bB">
					<idno type="grant-number">AISG2-TC-2021-003</idno>
					<orgName type="program" subtype="full">AI Singapore Programme (AISG Award</orgName>
				</org>
				<org type="funding" xml:id="_yM8Veks">
					<orgName type="program" subtype="full">STAR AME Programmatic Funding Scheme Under Project A20H4b0141</orgName>
				</org>
				<org type="funding" xml:id="_AfeAnJY">
					<idno type="grant-number">2022YFS0071 &amp; 2023YFG0273</idno>
				</org>
				<org type="funding" xml:id="_debZW6H">
					<idno type="grant-number">202206240082</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 56.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep evidential regression</title>
		<author>
			<persName><forename type="first">A</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Schwarting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Soleimany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="14927" to="14937" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multimodal machine learning: a survey and taxonomy</title>
		<author>
			<persName><forename type="first">T</forename><surname>Baltrušaitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="423" to="443" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">COROLLA: an efficient multi-modality fusion framework with supervised contrastive learning for glaucoma grading</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Biomedical Imaging (ISBI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Uni4Eye: unified 2D and 3D self-supervised pre-training via masked image modeling transformer for ophthalmic image classification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16452-1_9</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16452-19" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI 2022. MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13438</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.00625</idno>
		<title level="m">Med3D: transfer learning for 3D medical image analysis</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dropout as a Bayesian approximation: representing model uncertainty in deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1050" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Res2net: a new multi-scale backbone architecture</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="652" to="662" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Trusted multi-view classification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multi-modal retinal image classification with modality-specific attention network</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1591" to="1602" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional network with twofold feature augmentation for diabetic retinopathy recognition from multi-modal images</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Biomed. Health Inform</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2686" to="2697" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Uncertainty estimation for multi-view data: the power of seeing the whole picture</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dipnall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gabbe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">What uncertainties do we need in Bayesian deep learning for computer vision?</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adam: a method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Simple and scalable predictive uncertainty estimation using deep ensembles</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Self-supervised feature learning via exploiting multi-modal data for retinal disease diagnosis</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4023" to="4033" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multimodal information fusion for glaucoma and diabetic retinopathy classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>El Habib Daho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Conze</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16525-2_6</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16525-26" />
	</analytic>
	<monogr>
		<title level="m">Ophthalmic Medical Image Analysis</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Antony</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Macgillivray</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13576</biblScope>
		</imprint>
	</monogr>
	<note>OMIA 2022</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">K</forename><surname>Zou</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Simple and principled uncertainty estimation with deterministic deep learning via distance awareness</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Padhy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bedrax-Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Neural Information Processing Systems</title>
		<meeting>the 34th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A practical Bayesian framework for backpropagation networks</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Mackay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="448" to="472" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Predictive uncertainty estimation via prior networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Malinin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Calibration of deep probabilistic models with decoupled Bayesian neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Maronas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Paredes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">407</biblScope>
			<biblScope unit="page" from="194" to="205" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Bayesian learning for neural networks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Neal</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-1-4612-0745-0</idno>
		<ptr target="https://doi.org/10.1007/978-1-4612-0745-0" />
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Springer</publisher>
			<biblScope unit="volume">118</biblScope>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Black box variational inference</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gerrish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence and Statistics</title>
		<imprint>
			<biblScope unit="page" from="814" to="822" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Element: multi-modal retinal vessel segmentation based on a coupled region growing and machine learning approach</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">O</forename><surname>Rodrigues</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Conci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liatsis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Biomed. Health Inform</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3507" to="3519" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A student&apos;s t filter for heavy tailed process and measurement noise</title>
		<author>
			<persName><forename type="first">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Özkan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gustafsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="5770" to="5774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Evidential deep learning to quantify classification uncertainty</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sensoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kandemir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Neural Information Processing Systems</title>
		<meeting>the 32nd International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3183" to="3193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Uncertainty estimation using a single deep deterministic neural network</title>
		<author>
			<persName><forename type="first">J</forename><surname>Van Amersfoort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9690" to="9700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning two-stream CNN for multi-modal age-related macular degeneration categorization</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Biomed. Health Inform</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="4111" to="4122" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">CBAM: convolutional block attention module</title>
		<author>
			<persName><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01234-2_1</idno>
		<idno>978- 3-030-01234-2 1</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2018</title>
		<editor>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11211</biblScope>
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.06511</idno>
		<title level="m">Gamma challenge: glaucoma grading from multimodality images</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Review of a mathematical theory of evidence</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Zadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Mag</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">81</biblScope>
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A review: deep learning for medical image segmentation using multi-modality fusion</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Canu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Array</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">100004</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Zou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.00349</idno>
		<title level="m">EvidenceCap: towards trustworthy medical image segmentation via evidential identity cap</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
