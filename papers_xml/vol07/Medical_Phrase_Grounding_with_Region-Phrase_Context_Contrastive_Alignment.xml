<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Medical Phrase Grounding with Region-Phrase Context Contrastive Alignment</title>
				<funder ref="#_v2PhAwz">
					<orgName type="full">Agency for Science, Technology and Research (A*STAR)</orgName>
				</funder>
				<funder>
					<orgName type="full">A*STAR Central Research Fund &quot;A Secure and Privacy Preserving AI Platform for Digital Health</orgName>
				</funder>
				<funder ref="#_nHTyKXF">
					<orgName type="full">Tianjin Natural Science Foundation</orgName>
				</funder>
				<funder ref="#_yhEAA4a">
					<orgName type="full">National Research Foundation, Singapore</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhihao</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Intelligence and Computing</orgName>
								<orgName type="institution">Tianjin University</orgName>
								<address>
									<settlement>Tianjin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yang</forename><surname>Zhou</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Institute of High Performance Computing (IHPC)</orgName>
								<orgName type="department" key="dep2">Agency for Science, Technology and Research (A*STAR)</orgName>
								<address>
									<addrLine>1 Fusionopolis Way, #16-16</addrLine>
									<postCode>138632</postCode>
									<settlement>Connexis, Singapore</settlement>
									<country>Republic of Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Anh</forename><surname>Tran</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Singapore General Hospital</orgName>
								<address>
									<settlement>Singapore</settlement>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Junting</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Intelligence and Computing</orgName>
								<orgName type="institution">Tianjin University</orgName>
								<address>
									<settlement>Tianjin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Liang</forename><surname>Wan</surname></persName>
							<email>lwan@tju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">College of Intelligence and Computing</orgName>
								<orgName type="institution">Tianjin University</orgName>
								<address>
									<settlement>Tianjin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gideon</forename><forename type="middle">Su Kai</forename><surname>Ooi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Lionel Tim-Ee</forename><surname>Cheng</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Singapore General Hospital</orgName>
								<address>
									<settlement>Singapore</settlement>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Choon</forename><forename type="middle">Hua</forename><surname>Thng</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">National Cancer Center Singapore</orgName>
								<address>
									<settlement>Singapore</settlement>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xinxing</forename><surname>Xu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Institute of High Performance Computing (IHPC)</orgName>
								<orgName type="department" key="dep2">Agency for Science, Technology and Research (A*STAR)</orgName>
								<address>
									<addrLine>1 Fusionopolis Way, #16-16</addrLine>
									<postCode>138632</postCode>
									<settlement>Connexis, Singapore</settlement>
									<country>Republic of Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yong</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Institute of High Performance Computing (IHPC)</orgName>
								<orgName type="department" key="dep2">Agency for Science, Technology and Research (A*STAR)</orgName>
								<address>
									<addrLine>1 Fusionopolis Way, #16-16</addrLine>
									<postCode>138632</postCode>
									<settlement>Connexis, Singapore</settlement>
									<country>Republic of Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Huazhu</forename><surname>Fu</surname></persName>
							<email>hzfu@ieee.org</email>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Institute of High Performance Computing (IHPC)</orgName>
								<orgName type="department" key="dep2">Agency for Science, Technology and Research (A*STAR)</orgName>
								<address>
									<addrLine>1 Fusionopolis Way, #16-16</addrLine>
									<postCode>138632</postCode>
									<settlement>Connexis, Singapore</settlement>
									<country>Republic of Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Medical Phrase Grounding with Region-Phrase Context Contrastive Alignment</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="371" to="381"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">4133778E3672ED76411ED48FFD03AA66</idno>
					<idno type="DOI">10.1007/978-3-031-43990-2_35</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T11:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Medical phrase grounding</term>
					<term>vision-language model</term>
					<term>contrastive learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Medical phrase grounding (MPG) aims to locate the most relevant region in a medical image, given a phrase query describing certain medical findings, which is an important task for medical image analysis and radiological diagnosis. However, existing visual grounding methods rely on general visual features for identifying objects in natural images and are not capable of capturing the subtle and specialized features of medical findings, leading to a sub-optimal performance in MPG. In this paper, we propose MedRPG, an end-to-end approach for MPG. MedRPG is built on a lightweight vision-language transformer encoder and directly predicts the box coordinates of mentioned medical findings, which can be trained with limited medical data, making it a valuable tool in medical image analysis. To enable MedRPG to locate nuanced medical findings with better region-phrase correspondences, we further propose Tri-attention Context contrastive alignment (TaCo). TaCo seeks context alignment to pull both the features and attention outputs of relevant region-phrase pairs close together while pushing those of irrelevant regions far away. This ensures that the final box prediction depends more on its finding-specific regions and phrases. Experimental results on three MPG datasets demonstrate that our MedRPG outperforms stateof-the-art visual grounding approaches by a large margin. Additionally, the proposed TaCo strategy is effective in enhancing finding localization ability and reducing spurious region-phrase correlations.</p><p>Z. Chen and Y. Zhou-Contributed equally to this work.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Medical phrase grounding (MPG) is the task of associating text descriptions with corresponding regions of interest (ROIs) in medical images. It enables machines to understand and interpret medical findings mentioned in medical reports in the context of medical images, which is crucial in medical image analysis and radiological diagnosis. Figure <ref type="figure" target="#fig_0">1</ref> illustrates how an MPG system facilitates the radiological diagnosis process. Radiologists first review the medical images (e.g., X-rays, CT scans, and MRI scans) to find out possible abnormalities and then write a report that summarizes their findings. Then, given the image and report, the MPG system can help doctors to locate and link ROIs to the corresponding phrases in the reports, which reduces the time of the diagnostic process and improves the quality of risk stratification and treatment planning. In this paper, we study the MPG problem and focus on a typical setting to learn the grounding between Chest X-ray images and medical reports. As far as we know, there are only a few related works on the medical phrase grounding problem. (This is probably because medical annotations of grounding data require specialized expertise and are time-consuming and expensive to be collected.) Benedikt et al. <ref type="bibr" target="#b0">[1]</ref> made use of text semantics to improve biomedical vision-language processing. They first evaluated the grounding performance of self-supervised biomedical vision-language models by proposing an MPG benchmark. However, their focus is on vision-language pre-training rather than addressing the MPG problem. Qin et al. <ref type="bibr" target="#b19">[19]</ref> proposed to transfer the knowledge of general vision-language models for detection tasks in medical domains. The key idea is to guide the vision-language model through hand-crafted prompting of visual attributes such as color, shape, and location that may be shared between natural and medical domains. This approach fails to consider unique characteristics in radiological images and reports and is inapplicable to MPG for radiological images.</p><p>Although visual grounding has been well studied for natural images <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b4">4,</ref><ref type="bibr" target="#b22">[22]</ref><ref type="bibr" target="#b23">[23]</ref><ref type="bibr" target="#b24">[24]</ref>, it is non-trivial to apply these approaches to radiological images. Specifically, MPG requires learning specialized visual-textual features so that the model can identify medical findings with subtle differences in texture and shape and interpret the relative positions mentioned in the medical reports. In contrast, general grounding methods often rely on visual features that are useful for object detection or classification but not specific to medical images, leading to inaccurate region-phrase correlations and thus sub-optimal results. In addition, many grounding models for general domains are too heavy to be trained with limited annotated data, which is common. Such heavy model structures are generally difficult to be trained with limited annotated data.</p><p>In this work, we propose MedRPG, an end-to-end approach for MPG. MedRPG has a lightweight model architecture and explicitly captures the finding-specific correlations between ROIs and report phrases. Specifically, we propose to stack a few vision-language transformer layers to encode both the medical images and report phrases and directly predict the box coordinates of desired medical findings. Compared to general grounding methods with heavy model architectures, this design is more robust against overfitting for MPG with limited training data. To locate nuanced medical findings with better regionphrase correspondences, we further propose Tri-attention Context contrastive alignment (TaCo). TaCo seeks context alignment to learn finding-specific representations that jointly align the region, phrase, and box prediction under the same context of a vision-language transformer encoder. It pulls both the features and attention outputs close together for semantically relevant region-phrase pairs while pushing those of irrelevant pairs far away. This encourages the alignment between regions and phrases at both feature and attention levels, leading to enhanced finding-identification ability and reduced spurious region-phrase correlations. Experimental results on three medical datasets demonstrate that our MedRPG is more effective in localizing medical findings, achieves better regionphrase correspondences, and significantly outperforms general visual grounding approaches on the MPG task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Proposed Method</head><p>The MPG problem can be defined as follows: Given a radiological image I associated with medical phrases T written by specialist radiologists, MPG aims to locate the described findings and then output a 4-dim bounding box (bbox) coordinates b = (x, y, w, h), where (x, y) is the box center coordinates and w, h are the box width and height, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Model Architecture</head><p>Figure <ref type="figure" target="#fig_1">2</ref> illustrates the framework of our method. Given image I and phrase T, we first leverage the Vision Encoder and Language Encoder to generate the image and text embeddings. Next, we concatenate the multi-modal feature embeddings and append a learnable token (named [REG] token), and then feed them into a lightweight Vision-Language Transformer to encode the intra and inter-modality context in a common semantic space. Finally, the output state of the [REG] token is employed to predict the 4-dim bbox via a grounding head. Additionally, to ensure a consistent representation of medical findings across modalities, we introduce TaCo, which aligns the context of region and phrase embeddings at both the feature and attention levels.</p><p>Vision Encoder: Following the common practice <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b4">4,</ref><ref type="bibr" target="#b13">13]</ref>, the visual encoder starts with a CNN backbone, followed by the visual transformer. We choose the ResNet-50 <ref type="bibr" target="#b9">[9]</ref> as the CNN backbone. The visual transformer includes 6 stacked transformer encoder layers <ref type="bibr" target="#b5">[5]</ref>. Given a radiological image I ∈ R 3×W ×H , it is fed into the CNN backbone to obtain the high-level deep features. Next, we apply a 1 × 1 ConV layer to project the deep features into a C v -dimensional subspace. Finally, we exploit the visual transformer to mine the long-range visual relations and further output the visual features</p><formula xml:id="formula_0">F v = [f n v ] Nv n=1</formula><p>, where N v is the number of visual tokens and f n v ∈ R Cv is the n-th token of F v . Language Encoder: We leverage pre-trained language models such as BERT <ref type="bibr" target="#b14">[14]</ref> as the language encoder, which includes 12 transformer encoder layers. Given a medical phrase T, we first utilize the BERT tokenizer to convert it into a sequence of tokens. Next, we follow the common practice to append a [CLS] token at the beginning as the global representation of the input medical phrases and append a [SEP] token at the end, and then pad the sequence to a fixed length. Finally, we use BERT to encode the tokens into the text embeddings</p><formula xml:id="formula_1">F l = [f cls , f n l ] N l n=2</formula><p>, where N l is the number of text tokens, C l is the feature dimensions, and f n l ∈ R C l is the n-th token of F l . Vision-Language Transformer: After the individual vision and language encoding, we obtain F v and F l . To capture the correspondence between the image and phrase embeddings, we first project them into the common space (channel= C vl ) and then fed them into a Vision-Language Transformer (VLT), together with an extra learnable [REG] token, which is further used to predict the bbox:</p><formula xml:id="formula_2">H = VLT ([ϕ v (F v ), ϕ l (F l ), r]) ,<label>(1)</label></formula><p>where ϕ v (•) and ϕ l (•) denote the project functions for vision and language tokens, respectively. r∈R C vl is the [REG] token and VLT(•) denotes the VLT encoder with learnable position embeddings. H ∈ R C vl ×N vl (where</p><formula xml:id="formula_3">N vl = N v + N l + 1)</formula><p>is the output of VLT that consist of three parts: vision embeddings</p><formula xml:id="formula_4">H v =[h n v ] Nv n=1 , language embeddings H l =[h cls , h n l ] N l n=2</formula><p>, and [REG] embedding h reg . To perform the final grounding results, we further feed h reg into a 3-layer MLP to predict the final 4-dim box coordinates b = MLP(h reg ). Given the grounding-truth box b 0 , we leverage smooth L1 loss <ref type="bibr" target="#b7">[7]</ref> and GIoU <ref type="bibr" target="#b20">[20]</ref> loss which are popular in grounding and detection tasks to optimize our model:</p><formula xml:id="formula_5">L box = Φ l1 (b, b 0 ) + λ • Φ giou (b, b 0 ),<label>(2)</label></formula><p>where L box is the box loss. Φ l1 and Φ giou are the smooth L1 and GIoU loss functions, respectively. λ is the trade-off parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Tri-Attention Context Contrastive Alignment</head><p>Medical findings often share subtle differences in texture and brightness level due to the low contrast of the medical images, which makes it challenging for the MPG methods to capture accurate region-phrase correlations. To identify nuanced medical findings with better region-phrase correspondences, we propose the Tri-attention Context Contrastive Alignment (TaCo) strategy to learn finding-specific representations with accurate region-phrase correlations by explicitly aligning relevant regions and phrases at both feature and attention levels.</p><p>Feature-Level Alignment. The feature-level alignment aims to make visual and textual embeddings with the same semantics meaning to be similar. To this end, given the bbox b 0 related to a given phrase query, we first obtain the positive ROI embeddings</p><formula xml:id="formula_6">h box 0 ∈ R C vl = Pool(H v , b 0</formula><p>) by aggregating visual embeddings H v within the bbox b 0 . Next, we randomly select K bbox {b k } K k=1 that have low IoUs with b 0 (i.e., regions that are irrelevant to the given phrase query) and obtain K negative region embeddings</p><formula xml:id="formula_7">{h box k ∈ R C vl } K k=1 .</formula><p>Let h cls be the features of the input phrases. We want to make the positive ROI embedding h box 0 close to the corresponding phrase embedding h cls whereas negative region embeddings {h box k } K k=1 far away. This is achieved by exploiting the InfoNCE <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b17">17]</ref> loss as:</p><formula xml:id="formula_8">L fea = -log exp(h cls • h box 0 /τ ) K k=0 exp(h cls • h box k /τ ) , (<label>3</label></formula><formula xml:id="formula_9">)</formula><p>where L fea denotes the feature-level alignment loss, τ is a temperature hyperparameter and '•' represents the inner (dot) product.</p><p>Attention-Level Alignment. In addition to the feature-level alignment, we also consider attention-level alignment, which encourages the attention outputs of VLT for relevant region-phrase pairs to be similar. To realize this, we extract the attention weight A ∈ R N vl ×N vl from the last multi-head attention layer of VLT. We denote a reg , a cls and {a box k } K k=0 as the attention weights for the embeddings of the [REG] token, the [CLS] token, and the K + 1 bboxes, respectively, where a box k = Pool(A, b k ). Given the k-th bbox embedding, we calculate the joint attention weights of bbox, [CLS], and [REG] embeddings and then further product H to get the triple-attention context pooling c k as follows:</p><formula xml:id="formula_10">c k = N vl j=0 t (j) k • H[:, j] , where t k = Norm(a cls • a reg • a box k ),<label>(4)</label></formula><p>where t k represents the joint attention weights, t (j) k denotes the j-th element of t k , H[:, j] denotes the j-th column of H, and Norm(•) is the L2 normalization operation to constrain the sum of the squared weights to be equal to 1. Such triple-attention context pooling c k characterizes the contextual dependencies among regions, phrases, and box predictions in the VLT. Intuitively, the box prediction of certain medical findings should be made based on its relevant regions and phrases rather than irrelevant ones. Therefore, the attention outputs c 0 for relevant region-phrase pairs should be similar to their individual embeddings h box 0 and h cls , leading to attention-level alignment.</p><p>Table <ref type="table">1</ref>. Grounding results on MS-CXR <ref type="bibr" target="#b0">[1]</ref>, ChestX-ray8 <ref type="bibr" target="#b21">[21]</ref>, and the in-house datasets with respect to Acc and mIoU. ). This leads to the TaCo loss as follows:</p><formula xml:id="formula_11">L taco = -log exp((h cls + c 0 ) • (h box 0 + c 0 )/τ ) K k=0 exp((h cls + c k ) • (h box k + c k )/τ ) . (<label>5</label></formula><formula xml:id="formula_12">)</formula><p>Finally, we combine the TaCo loss ( <ref type="formula" target="#formula_11">5</ref>) and box loss (2) to get the overall loss functions of MedRPG:</p><formula xml:id="formula_13">L MedRP G = L box + μ • L taco . (<label>6</label></formula><formula xml:id="formula_14">)</formula><p>where L MedRP G denotes total loss for MedRPG and μ is the trade-off parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>Dataset. Our experiments are conducted on two public datasets, i.e., MS-CXR <ref type="bibr" target="#b0">[1]</ref>, ChestX-ray8 <ref type="bibr" target="#b21">[21]</ref>, and one in-house datase <ref type="foot" target="#foot_0">1</ref> . MS-CXR is sourced from MIMIC-CXR <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b12">12]</ref> and consists of 1,153 samples of Image-Phrase-BBox triples.</p><p>We pre-process MS-CXR to make sure a given phrase query corresponds to only one bounding box, which results in 890 samples from 867 patients. ChestX-ray8 is a large-scale dataset for diagnosing 8 common chest diseases, of which 984 images with pathology are provided with hand-labeled bounding boxes. Due to the lack of finding-specific phrases from medical reports, we use category labels as the phrase queries to build the Image-Report-BBox triples for our task. Our in-house dataset comprises 1,824 Image-Phrase-BBox samples from 635 patients, including 23 categories of chest abnormalities with more complex phrases. For a fair comparison, all datasets are split into train-validation-test sets by 7:1:2 based on the patients.</p><p>Evaluation Metrics. To evaluate the quality of the MPG task, we follow the standard protocol of nature image grounding <ref type="bibr" target="#b4">[4]</ref> to report Acc(%), where a predicted region will be regarded as a positive sample if its intersection over union (IoU) with the ground-truth bounding box is greater than 0.5. Besides, we also report mIoU (%) metric for a more comprehensive comparison. Baselines. We compare our MedRPG with SOTA methods for general visual grounding, such as RefTR <ref type="bibr" target="#b15">[15]</ref>, TransVG <ref type="bibr" target="#b4">[4]</ref>, VGTR <ref type="bibr" target="#b6">[6]</ref>, and SeqTR <ref type="bibr" target="#b24">[24]</ref>. We choose their official implementations for a fair comparison. Since the medical datasets are too small to train a data-hungry transformer-based model from scratch, we initialize our MedRPG (encoders) from the general grounding models pre-trained on natural images. The compared methods share the same settings. We also compare two self-supervised biomedical vision-language processing methods BioViL <ref type="bibr" target="#b0">[1]</ref> and GLoRIA <ref type="bibr" target="#b10">[10]</ref> which pre-trained on MIMIC-CXR <ref type="bibr" target="#b11">[11]</ref>. Implementation Details. The experiments are conducted on the PyTorch <ref type="bibr" target="#b18">[18]</ref> platform with an NVIDIA RTX 3090 GPU. The input image size is 640×640.</p><p>The channel numbers C v , C l , and C vl are 256, 768, and 256. The sample number K is set to 5. The trade-off parameter λ in Eq. 2 and μ in Eq. 6 are set to 1 and 0.05, respectively. The base learning rates for the vision encoder, language encoder, and vision-language transformer are set to 1×10 -5 , 1×10 -5 , and 5×10 -5 , respectively. We train our MedRPG model by the AdamW <ref type="bibr" target="#b16">[16]</ref> optimizer for 90 epochs with a learning rate dropped by a factor of 10 after 60 epochs. For all the baselines and MedRPG, we select the best checkpoint for testing based on validation performance and report the average performance metrics computed by repeating each experiment with three different random seeds. Experimental Results. Table <ref type="table">1</ref> provides the grounding results on the MS-CXR, ChestX-ray8, and in-house datasets. As can be seen, our MedRPG consistently achieves the best performance in all cases. In particular, we note that lightweight models like TransVG and our MedRPG generally perform better, which indicates lightweight models are more applicable for MPG. Despite this, our method still outperforms TransVG by a margin of 6.1% in Acc on MS-CXR.</p><p>This can be attributed to the proposed TaCo strategy in learning finding-specific representations and improving region-phrase alignment. On ChestX-ray8, all methods get degraded results due to the lack of position cues in the phrase queries. Nevertheless, our method still outperforms the second-best method by 4.3% in Acc and 1.6% in mIoU. On the in-house dataset, our method is still the best even when there exist much more types of findings to be grounded. Note that the self-supervised methods BioViL and GloRIA achieve very poor results compared to other methods on all three datasets.</p><p>Ablation Study. We conduct ablative experiments on the MS-CXR dataset to verify the effectiveness of each component in MedRPG. Table <ref type="table" target="#tab_1">2</ref> shows the quantitative results of each combination. To verify how the vision and language modalities contribute to the MPG performance, we perform MedRPG with either image or test inputs. As expected, MedRPG can only achieve poor results under the unimodal setting. Next, we consider the inputs with both images and phrases and observe a significant improvement in performance compared to MedRPG trained from a single modality. Then, we equip MedRPG with feature-level alignment and gain the improvement of 0.6% in Acc and 0.5% in mIoU, which suggests it is helpful but still not good enough to learn the accurate region-phrase correspondences. Finally, with the proposed TaCo, MedRPG further gains a significant improvement by 3.8% in Acc. This shows that TaCo is effective in improving the MPG performance with better region-phrase correlations. In addition, we study the impact of hyper-parameters (trade-off parameter μ and number of negative samples K). Table <ref type="table" target="#tab_2">3</ref> shows two metrics of our MedRPG method with varying hyper-parameters onni the MS-CXR dataset. As can be seen, our method is not very sensitive to hyper-parameter choices. Additional analysis of the model's confidence intervals and its ability to generalize can be found in the supplementary material.</p><p>Qualitative Results. In Fig. <ref type="figure" target="#fig_2">3</ref>, we show the box predictions and attention maps obtained by MedRPG with and without TaCo to demonstrate the effectiveness of TaCo in identifying abnormal medical findings and capturing region-phrase correlations. For instance, in Case 1, pneumothorax is present in an uncommon location (i.e., lower left lung) and the phrase does not provide an accurate location cue. Without TaCo, MedRPG overfits the upper lung regions where pneumothorax appears more frequently. In contrast, with TaCo, the model can better learn pneumothorax representations and identify the corresponding ROI even without accurate location information. In other cases, although the method without TaCo can also roughly find the location of the medical findings, MedRPG with TaCo can obtain more focused attention maps on the medical findings. It suggests that TaCo is effective in reducing spurious region-phrase correlations, leading to more accurate and interpretable bbox predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>This study introduces MedRPG, a lightweight and efficient method for medical phrase grounding. A novel tri-attention context contrastive alignment (TaCo) is proposed to learn finding-specific representations and improve region-phrase alignment in feature and attention levels. Experimental results show that MedRPG outperforms existing visual grounding methods and achieves more consistent correlations between phrases and mentioned regions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Illustration on how MPG helps radiological diagnosis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. An overview of the proposed MedRPG method.</figDesc><graphic coords="4,42,81,53,90,338,80,128,80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Visualized grounding results for MedRPG w/ and w/o TaCo. We show the ground-truth box (red box), prediction box (cyan or yellow box), and the [REG] token's attention to visual tokens (a heatmap with high values in red). (Color figure online)</figDesc><graphic coords="8,56,79,305,48,310,72,149,56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Context Contrastive Alignment. With the above results, we can propose our TaCo strategy by integrating both feature and attention-level alignments. Specifically, we modify the InfoNCE loss (3) to simultaneously perform feature and attention-level alignments by adding the triple-attention context pooling c k to the respective region and phrase features (i.e., h cls , h box k</figDesc><table><row><cell>Method</cell><cell>Vision</cell><cell>Language</cell><cell>MS-CXR</cell><cell>ChestX-ray8 In-house</cell></row><row><cell></cell><cell cols="2">Encoder Encoder</cell><cell cols="2">Acc↑ mIoU↑ Acc↑ mIoU↑ Acc↑ mIoU↑</cell></row><row><cell>BioViL  † [1]</cell><cell cols="2">ResNet-50 CXR-BERT</cell><cell cols="2">7.78 19.99 6.56 12.78 3.65 13.55</cell></row><row><cell>GLoRIA  † [10]</cell><cell></cell><cell cols="3">BioClinicalBERT 28.74 31.17 8.58 16.39 5.74 14.91</cell></row><row><cell>RefTR [15]</cell><cell></cell><cell>BERT</cell><cell cols="2">53.69 50.11 29.27 29.59 46.03 40.99</cell></row><row><cell>VGTR [6]</cell><cell></cell><cell>Bi-LSTM</cell><cell cols="2">60.27 53.58 32.65 34.02 47.37 41.92</cell></row><row><cell>SeqTR [24]</cell><cell></cell><cell>Bi-GRU</cell><cell cols="2">63.20 56.63 32.88 33.09 44.42 39.45</cell></row><row><cell>TransVG [4]</cell><cell></cell><cell>BERT</cell><cell cols="2">65.87 58.91 34.51 33.98 48.30 43.35</cell></row><row><cell>Ours</cell><cell cols="2">ResNet-50 BERT</cell><cell cols="2">69.86 59.37 36.02 34.59 49.87 43.86</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation study for network components on MS-CXR dataset.</figDesc><table><row><cell cols="6">Metrics Only images Only phrases Baseline w/ Feature-level w/ TaCo</cell></row><row><cell>Acc ↑</cell><cell>41.91</cell><cell>40.12</cell><cell>66.86</cell><cell>67.26</cell><cell>69.86</cell></row><row><cell cols="2">mIoU ↑ 38.44</cell><cell>39.34</cell><cell>58.93</cell><cell>59.25</cell><cell>59.37</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Ablation study for hyperparameters µ and K on MS-CXR dataset.</figDesc><table><row><cell cols="6">Metrics µ = 0.1 µ = 0.05 µ = 0.025 µ = 0.01 K=3 K=5 K=7</cell></row><row><cell>Acc ↑</cell><cell>66.86</cell><cell>69.86</cell><cell>68.86</cell><cell>68.66</cell><cell>67.66 69.86 66.67</cell></row><row><cell cols="2">mIoU ↑ 58.92</cell><cell>59.37</cell><cell>59.12</cell><cell>59.14</cell><cell>59.19 59.37 58.59</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The ethical approval of this dataset was obtained from the Ethical Committee.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work is supported by the grant from <rs type="funder">Tianjin Natural Science Foundation</rs> (Grant No. <rs type="grantNumber">21JCYBJC00510</rs>), the <rs type="funder">Agency for Science, Technology and Research (A*STAR)</rs> through its <rs type="programName">AME Programmatic Funding Scheme Under Project A20H4b0141</rs>, the <rs type="funder">A*STAR Central Research Fund "A Secure and Privacy Preserving AI Platform for Digital Health</rs>". This research/project is supported by the <rs type="funder">National Research Foundation, Singapore</rs> under its <rs type="programName">AI Singapore Programme (AISG Award</rs> No: <rs type="grantNumber">AISG2-TC-2021-003</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_nHTyKXF">
					<idno type="grant-number">21JCYBJC00510</idno>
				</org>
				<org type="funding" xml:id="_v2PhAwz">
					<orgName type="program" subtype="full">AME Programmatic Funding Scheme Under Project A20H4b0141</orgName>
				</org>
				<org type="funding" xml:id="_yhEAA4a">
					<idno type="grant-number">AISG2-TC-2021-003</idno>
					<orgName type="program" subtype="full">AI Singapore Programme (AISG Award</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2_35.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Making the most of text semantics to improve biomedical vision-language processing</title>
		<author>
			<persName><forename type="first">B</forename><surname>Boecking</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision. ECCV 2022</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Brostow</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Cissé</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">13696</biblScope>
			<biblScope unit="page" from="1" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-20059-5_1</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-20059-5_1" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Endto-end object detection with transformers</title>
		<author>
			<persName><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58452-8_13</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-58452-8_13" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2020</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12346</biblScope>
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-modal dynamic graph transformer for visual grounding</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">TranSVG: end-to-end visual grounding with transformers</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An image is worth 16×16 words: transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Visual grounding with transformers</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICME</title>
		<meeting>ICME</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Noise-contrastive estimation: a new estimation principle for unnormalized statistical models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Artificial Intelligence and Statistics</title>
		<meeting>International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Gloria: a multimodal global-local representation learning framework for label-efficient medical image recognition</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Lungren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">MIMIC-CXR, a de-identified publicly available database of chest radiographs with free-text reports</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">317</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">MIMIC-CXR database (version 2.0.0)</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Berkowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Horng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PhysioNet</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mdetrmodulated detection for end-to-end multi-modal understanding</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D M W C</forename><surname>Kenton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Referring transformer: a one-step approach to multi-task visual grounding</title>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V D</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">PyTorch: an imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Medical image understanding with pretrained vision language models: a comprehensive study</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Generalized intersection over union: a metric and a loss for bounding box regression</title>
		<author>
			<persName><forename type="first">H</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">ChestX-ray8: hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bagheri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A fast and accurate one-stage approach to visual grounding</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">MattNet: modular attention network for referring expression comprehension</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">SeqTR: a simple yet universal network for visual grounding</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-19833-5_35</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-19833-5_35" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision. ECCV 2022</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Brostow</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Cissé</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13695</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
