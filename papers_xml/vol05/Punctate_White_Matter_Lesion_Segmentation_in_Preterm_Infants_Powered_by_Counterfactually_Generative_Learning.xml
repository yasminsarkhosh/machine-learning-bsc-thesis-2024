<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Punctate White Matter Lesion Segmentation in Preterm Infants Powered by Counterfactually Generative Learning</title>
				<funder ref="#_Qy2Ra2h #_cGXCHeq #_Kua32DU #_TDTqsrw">
					<orgName type="full">NSFC</orgName>
				</funder>
				<funder ref="#_HMMbE6B">
					<orgName type="full">Natural Science Basic Research Program of Shaanxi</orgName>
				</funder>
				<funder ref="#_EYvaRJG">
					<orgName type="full">STI 2030-Major Projects</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zehua</forename><surname>Ren</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Life Science and Technology</orgName>
								<orgName type="laboratory">The Key Laboratory of Biomedical Information Engineering of Ministry of Education</orgName>
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yongheng</forename><surname>Sun</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Mathematics and Statistics</orgName>
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Miaomiao</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Radiology</orgName>
								<orgName type="institution">The First Affiliated Hospital of Xi&apos;an Jiaotong University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuying</forename><surname>Feng</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Radiology</orgName>
								<orgName type="institution">The First Affiliated Hospital of Xi&apos;an Jiaotong University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xianjun</forename><surname>Li</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Radiology</orgName>
								<orgName type="institution">The First Affiliated Hospital of Xi&apos;an Jiaotong University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chao</forename><surname>Jin</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Radiology</orgName>
								<orgName type="institution">The First Affiliated Hospital of Xi&apos;an Jiaotong University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jian</forename><surname>Yang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Radiology</orgName>
								<orgName type="institution">The First Affiliated Hospital of Xi&apos;an Jiaotong University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chunfeng</forename><surname>Lian</surname></persName>
							<email>chunfeng.lian@mail.xjtu.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">School of Mathematics and Statistics</orgName>
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fan</forename><surname>Wang</surname></persName>
							<email>fan.wang@mail.xjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Life Science and Technology</orgName>
								<orgName type="laboratory">The Key Laboratory of Biomedical Information Engineering of Ministry of Education</orgName>
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Radiology</orgName>
								<orgName type="institution">The First Affiliated Hospital of Xi&apos;an Jiaotong University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Punctate White Matter Lesion Segmentation in Preterm Infants Powered by Counterfactually Generative Learning</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="220" to="229"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">2858151817505E5D2559B5CFC436A9FE</idno>
					<idno type="DOI">10.1007/978-3-031-43904-9_22</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-25T18:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Accurate segmentation of punctate white matter lesions (PWMLs) are fundamental for the timely diagnosis and treatment of related developmental disorders. Automated PWMLs segmentation from infant brain MR images is challenging, considering that the lesions are typically small and low-contrast, and the number of lesions may dramatically change across subjects. Existing learning-based methods directly apply general network architectures to this challenging task, which may fail to capture detailed positional information of PWMLs, potentially leading to severe under-segmentations. In this paper, we propose to leverage the idea of counterfactual reasoning coupled with the auxiliary task of brain tissue segmentation to learn fine-grained positional and morphological representations of PWMLs for accurate localization and segmentation. A simple and easy-to-implement deep-learning framework (i.e., DeepPWML) is accordingly designed. It combines the lesion counterfactual map with the tissue probability map to train a lightweight PWML segmentation network, demonstrating state-of-the-art performance on a real-clinical dataset of infant T1w MR images. The code is available at https://github.com/ladderlab-xjtu/DeepPWML.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Punctate white matter lesion (PWML) is a typical type of cerebral white matter injury in preterm infants, potentially leading to psychomotor developmental delay, motor delay, and cerebral palsy without timely treatment <ref type="bibr" target="#b1">[2]</ref>. The early detection and quantitative analysis of PWMLs are critical for diagnosis and treatment, especially considering that some PWML subtypes are only detectable by magnetic resonance imaging (MRI) shortly after birth (e.g., around the third week) and will become invisible thereafter <ref type="bibr" target="#b5">[6]</ref>. The PWMLs are small targets that typically locate anterior or adjacent to the ventricles <ref type="bibr" target="#b7">[8]</ref>. Manually annotating them in MR images is very time-consuming and relies on expertise. Thus there is an urgent need, from neuroradiologists, to develop reliable and fully automatic methods for 3D PWML segmentation.</p><p>Automated localization and delineation of PWML are practically challenging. This is mainly because that PWMLs are isolated small objects, with typically only dozens of voxels for a lesion and varying numbers of lesions across different subjects. Also, due to underlying immature myelination of infant brains <ref type="bibr" target="#b0">[1]</ref>, the tissue-to-tissue and lesion-to-tissue contrasts are both very low, especially in T1w MR images commonly used in clinical practice. In addition to conventional methods based on thresholding <ref type="bibr" target="#b2">[3]</ref> or stochastic likelihood estimation <ref type="bibr" target="#b3">[4]</ref>, recent works attempted to apply advanced deep neural networks in the specific task of PWML segmentation <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12]</ref>. For example, Liu et al. <ref type="bibr" target="#b9">[10]</ref> extended Mask R-CNN <ref type="bibr" target="#b6">[7]</ref> to detect and segment PWMLs in 2D image slices. Li et al. <ref type="bibr" target="#b8">[9]</ref> implemented a 3D ResU-Net to segment diffuse white matter abnormality from T2w images. Overall, these existing learning-based methods usually use general network architectures. They may fail to completely capture fine-grained positional information to localize small and low-contrast PWMLs, potentially resulting in high under-segmentations.</p><p>Counterfactual reasoning, explained by our task, studies how a real clinical brain image appearance (factual) changes in a hypothetical scenario (whether lesion exist or not). This idea has been applied as structural causal models (SCMs) in a deep learning way in recent years. At the theoretical level, Monteiro et al. <ref type="bibr" target="#b10">[11]</ref> have presented a theoretically grounded framework to evaluate counterfactual inference models. Due to the advantage of being verifiable, this idea appeared in many medical scenarios. Pawlowski et al. <ref type="bibr" target="#b13">[14]</ref> proposed a general framework for building SCMs and validated on a MNIST-like dataset and a brain MRI dataset. Reinhold et al. <ref type="bibr" target="#b14">[15]</ref> developed a SCM that generates images to show what an MR image would look like if demographic or disease covariates are changed. In this paper, we propose a fully automatic deep-learning framework (DeepPWML) that leverages counterfactual reasoning coupled with location information from the brain tissue segmentation to capture fine-grained positional information for PWML localization and segmentation. Specifically, based on patch-level weak-supervision, we design a counterfactual reasoning strategy to learn voxel-wise residual maps to manipulate the classification labels of input patches (i.e., containing PWMLs or not). In turn, such fine-grained residual maps could initially capture the spatial locations and morphological patterns of potential lesions. In this article, we define this residual map as counterfactual map which may be different from the meaning of counterfactual map in other articles, hereby declare. And to refine the information learned by the counterfactual part, we further include brain tissue segmentation as an auxiliary task. Given the fact that PWMLs have specific spatial correlations with different brain tissues, the segmentation probability maps (and inherent location information) could provide a certain level anatomical contextual information to assist lesion identification. Finally, by using the counterfactual maps and segmentation proba- bility maps as the auxiliary input, we learn a lightweight sub-network for PMWL segmentation.</p><p>Overall, our DeepPWML is practically easy to implement, as the counterfactual part learns simple but effective linear manipulations, the tissue segmentation part can adopt any off-the-shelf networks, and the PWML segmentation part only needs a lightweight design. On a real-clinical dataset, our method led to a state-of-the-art performance in the infant PWML segmentation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, our DeepPMWL consists of four parts, i.e., the tissue segmentation module (T-SEG), the classification module (CLS), the counterfactual map generator (CMG), and the PWML segmentation module (P-SEG). Specifically, in the training stage, T-SEG is learned on control data, while other modules are learned on PWML data. Given an image patch as the input, CLS is trained to distinguish positive (containing PWML) or negative (no PWML) cases, based on which CMG is further trained to produce a counterfactual map to linearly manipulate the input to change the CLS result.</p><p>The high-resolution counterfactual maps (CF maps) and segmentation probability maps (SP maps) are further combined with the input patch to train a lightweight P-SEG for PWML segmentation. In the test stage, an input patch is first determined by the CLS module whether it is positive. Positive inputs will pass through the T-SEG, CMG, and P-SEG modules to get the PWML segmentation results. It is worth noting that the test patches are generated by sliding windows, and the overlapping results are averaged to get the final segmentation results for the image, which reduces the impact of incorrect classification of the CLS module. In our experiments, T-SEG used the voxel-wise Cross-Entropy Loss. CLS used the Categorical Cross-Entropy Loss, CMG combined the sparsity loss (L1 and L2 norms) with the classification loss. Finally, P-SEG used the Dice Loss. In the following subsections, we will introduce every module in our design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Tissue Segmentation Module</head><p>The task is to mark every pixel of the brain as cerebrospinal fluid (CSF), gray matter (GM), or white matter (WM). The choice of this module can be flexible, and there are many off-the-shelf architecture designs available. We adopt a simple Dense-Unet architecture <ref type="bibr" target="#b15">[16]</ref> for the T-SEG module. It is trained on control premature infants' images. This module will output the SP map in which segmentation result can be obtained. Therefore, this SP map naturally contains some level anatomy information. Moreover, when an input with PWML goes through a network that has only been trained on control data, the segmentation mistake is partly due to the existence of PWML. Therebefore, this module can output a SP map carrying both potential location and anatomy guidance for PWML localization and segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Classification Module and Counterfactual Map Generator</head><p>The CLS and the CMG are trained sequentially. The CLS is trained to determine whether the current patches have lesions. The CMG is a counterfactual reasoning step for the CLS. Based on the characteristic of PWML, CMG learns a simple linear sparse transform shown as the CF map. This map aims to offset the bright PWML pixels of the image patches, which are classified as positive, or seed PWML on the patches judged as negative. In other words, CMG is learning a residual activation map for conversion between control and PWML. We adopt the T-SEG module's encoder with two fully connected layers as the CLS module. Furthermore, the architecture of CMG is a simple U-net adding a "switch" state in its skip-connection parts according to the method of Oh et al. <ref type="bibr" target="#b12">[13]</ref>. Based on the nature of PWMLs, the last layer of CMG is Relu activation to ensure that the generated CF map is a positive activation.</p><p>The state of the "switch" is determined by the classifier's result on the current patch. If the judgement is positive, correspondingly, the "switch" status is 0. In this condition, the activated areas in the CF map should be where PWMLs exist. Then the pseudo patches in Fig. <ref type="figure" target="#fig_0">1</ref>, obtained by subtracting the CF map from the input patches, should be judged as negative by the fixed CLS. Another state of the "switch" is used to generate PWMLs. When the CLS judges are negative, the "switch" status is 1. in this situation, the input patches combining the CF map should be classified as positive. This training strategy is to make CMG learn PWML features better. When it comes to the test phase, the switch status will be fixed to 0. Because in the test phase, the CF map only needs to capture PWML.</p><p>The CMG module is summarised as follows: Firstly, PWML patches C P and control patches C N are fed to the encoder to obtain encoded representations F P and F N :</p><formula xml:id="formula_0">F P = Encoder(C P ),<label>(1)</label></formula><formula xml:id="formula_1">F N = Encoder(C N ).<label>(2)</label></formula><p>Secondly, "switch" filled with zeros/ones with the same size as PWML/normal representations F P /F N are added to these representations and then pass through the decoder to obtain the CF maps M P /M N :</p><formula xml:id="formula_2">M P = Decoder(F P + Zeros), (<label>3</label></formula><formula xml:id="formula_3">) M N = Decoder(F N + Ones).<label>(4)</label></formula><p>Finally, the original patches C P /C N are added/subtracted to the CF maps M P /M N to yield the transformed patches C P / C P , which are classified by the CLS module as the opposite classes:</p><formula xml:id="formula_4">C P = C P + M P ,<label>(5)</label></formula><formula xml:id="formula_5">C N = C N -M N . (<label>6</label></formula><formula xml:id="formula_6">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">PWML Segmentation Module</head><p>The SP map includes the potential PWML existence, but also a lot of tissue segmentation uncertainty. The CF map directly shows the PWML location, but due to the accuracy of the CLS module, the CF map itself will also carry some false positives fault. If we synthesize the CF map, the SP map and the original input patches for appearance information, the best segmentation result can be achieved by allowing the network to verify and filter out each information in a learnable way. The P-SEG module is implemented as a lightweight variant of the Dense-Unet. Different simplified versions have been tested, with the results summarized in Sect. 3.2. After getting the PWML segmentation result, we use the tissue segmentation result to filter out PWMLs mis-segmented at the background and CSF. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset and Experimental Setting</head><p>Dataset: Experiments were performed on a dataset with two groups (control and PWML), where control included 52 subjects without PWML observed, and PWML included 47 subjects with PWMLs. All infants in this study were born with gestational age (GA) between 28 to 40 weeks and scanned at postmenstrual age (PMA) between 37 to 42 weeks. Two neuroscientists manually labeled PWML areas and corrected tissue labels generated by iBeat <ref type="bibr" target="#b4">[5]</ref>. Written Consent was obtained from all parents under the institutional review board, and T1-weighted MR images were collected using a 3T MRI scanner, resampling the resolution of all images into 0.9375 × 0.9375 × 1 mm 3 . All images are cropped to 130 × 130 × 170.</p><p>Experimental Setting: Our method was implemented using Tensorflow. All modules were trained and tested on an NVIDIA GeForce RTX 3060 GPU. We adopted Adam as the optimizer, with the learning rate varying from 0.001 to 0.00001 according to modules. The inputs were fixed-size patches (32 × 32 × 32) cut from the T1w images. The train/validation/test ratio was 0.7/0.15/0.15 and divided on subject-level. We didn't use any data augmentation during training. We used Dice, True Positive Rate (TPR), and Positive Predictive Value (PPV) to quantitatively evaluate the segmentation performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head><p>First, the T-SEG module is trained using a fully supervised way. Its tissue segmentation accuracy on the test set is about 93% in terms of Dice. Second, the CLS and other modules are trained with PWML group data. We defined the input training patches' class labels by whether they contain PWMLs or not. In other words, if any patch has at least one lesion voxel, it is positive. The accuracy of the test set can reach around 90%. Third, we train the CMG module based on the well-trained and fixed CLS module. Finally, based on T-SEG and CMG, we train P-SEG. We combine the SP map, CF map, and T1w image in a channel-wise way as the input of the module without any additional processing of these features.</p><p>Comparison Results: We compared our method with the state-of-the-art method <ref type="bibr" target="#b9">[10]</ref>. As is shown in Table <ref type="table" target="#tab_0">1</ref>, our method outperforms the state-of-the-art method and the baseline model in all three indexes. The visualization results are shown in Fig. <ref type="figure" target="#fig_1">2</ref>, from which it can be seen that our method can segment smallsize PWMLs more accurately and segment PWMLs with different severities more completely.</p><p>Ablation Studies: We further evaluated the effectiveness of our design by comparing the results of the pipelines with and without SP maps and CF maps. The ablation results are shown in the last six rows of Table <ref type="table" target="#tab_0">1</ref>. The baseline model using the same dense-Unet is trained to segment the PWML from T1w images. Other settings are consistent with our final module. Then we will add our designed modules step by step to verify the effectiveness of two kinds of auxiliary information.</p><p>By comparing "baseline", "SP map", and "CF map", we can find that the two kinds of information individually are not good for segmenting PWMLs. The reason is as follows. The SP map mainly focuses on tissue segmentation task. The CF map has some false activation due to the offset of the highlighted areas for PWML. Fusing these two kinds of information has reduced their respective defects ("SP map + CF map"). The icing on the cake is that when the appearance features of T1w are used again, the accuracy will be significantly improved ("SP map + T1" and "CF map + T1"). This means "SP map" and "CF map" each can be an auxiliary information but not sufficient resource for this task. Finally, after combining the three together, all indicators have been significantly improved ("SP map + CF map + T1").</p><p>Visual Analysis: Figure <ref type="figure" target="#fig_2">3</ref> shows the T1w images, tissue segmentation maps, CF maps, labels, and segmentation results. By selecting the most likely category from the SP map as the label, the tissue segmentation map can be obtained. As shown in the tissue segmentation maps, PWML voxels tend to be classified as gray matter surrounded by white matter which obviously does not conform to the general anatomy knowledge. The reason of this phenomenon may be that the intensity of gray matter and PWML are higher than white matter in T1w image at this age. It also can be seen from the CF maps that these maps have a preliminary localization of PWML. The last row shows the situation without PWML. It can be seen that the tissue segmentation is reasonable. The CF map has a small amount of activation and the intensity is significantly lower than the first three rows. In conclusion, these two maps complementarily provide the anatomical and morphological information for the segmentation of the PWML.</p><p>Comparison of Different Backbones of the P-SEG Module: We test from simple several layers to the whole dense-Unet to determine the required complexity in Table <ref type="table" target="#tab_1">2</ref>. We compared six designs with different network sizes in Table <ref type="table" target="#tab_1">2</ref>. The first three methods are several convolution layers with the same resolution. The latter three reduce the number of down-samplings in the original dense-Unet. By comparing the Dice index, it is obvious that the simple convolution operation cannot integrate the three kinds of input information well. The results show that the encoder-decoder can better fuse information. Perhaps because of the small size of PWML, it does not require too much down-sampling to get a similar result as the optimal result. The result also indicates that a certain degree of network size is needed to learn the PWMl characteristics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this study, we designed a simple and easy-to-implement deep learning framework (i.e. DeepPWML) to segment PWMLs. Leveraging the idea of generative counterfactual inference combined with an auxiliary task of brain tissue segmentation, we learn fine-grained positional and morphological representations of PWMLs to achieve accurate localization and segmentation. Our lightweight PWML segmentation network combines lesion counterfactual maps with tissue segmentation probability maps, achieving state-of-the-art performance on a real clinical dataset of infant T1w MR images. Moreover, our method provides a new perspective for the small-size segmentation task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overview of the training and test steps of our DeepPWML framework that consists of four components, i.e., T-SEG, CLS, CMG, and P-SEG modules.</figDesc><graphic coords="3,59,13,76,82,305,08,276,76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Visual comparisons of the representative PWML segmentation results.</figDesc><graphic coords="6,104,01,54,17,289,00,213,52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Example visualization of T1w images, tissue segmentation maps, CF maps, labels, and segmentation results. Tissue segmentation maps are the final segmentation output of the T-SEG module. CF maps are the output of the CMG module.</figDesc><graphic coords="8,58,98,53,75,334,72,197,80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison of our method (and its variants) with the state-of-the-art method and the baseline model. All metrics are presented as "mean (std)".</figDesc><table><row><cell>Methods</cell><cell>Dice</cell><cell>TPR</cell><cell>PPV</cell></row><row><cell>Baseline [16]</cell><cell>0.649(0.239)</cell><cell>0.655(0.244)</cell><cell>0.704(0.281)</cell></row><row><cell>RS R-CNN [10]</cell><cell>0.667(0.172)</cell><cell>0.754(0.250)</cell><cell>0.704(0.187)</cell></row><row><cell>Ours SP map</cell><cell>0.649(0.142)</cell><cell>0.726(0.210)</cell><cell>0.677(0.213)</cell></row><row><cell>CF map</cell><cell>0.507(0.169)</cell><cell>0.543(0.286)</cell><cell>0.647(0.180)</cell></row><row><cell>SP map + T1</cell><cell>0.680(0.178)</cell><cell>0.794(0.211)</cell><cell>0.699(0.237)</cell></row><row><cell>CF map + T1</cell><cell>0.672(0.198)</cell><cell>0.741(0.249)</cell><cell>0.719(0.178)</cell></row><row><cell>SP map + CF map</cell><cell>0.670(0.184)</cell><cell>0.781(0.181)</cell><cell>0.684(0.251)</cell></row><row><cell cols="4">SP map + CF map + T1 0.721(0.177) 0.797(0.185) 0.734(0.211)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison of different backbones of the P-SEG module.</figDesc><table><row><cell>Methods</cell><cell>Dice</cell><cell>Network size (KB)</cell></row><row><cell>Four convolutional layers</cell><cell>0.5843</cell><cell>107</cell></row><row><cell>One Dense-Block</cell><cell>0.6220</cell><cell>420</cell></row><row><cell>Two Dense-Blocks</cell><cell>0.6644</cell><cell>836</cell></row><row><cell cols="3">Dense-Unet with one down-sampling 0.7099 2127</cell></row><row><cell cols="3">Dense-Unet with two down-sampling 0.7180 5345</cell></row><row><cell>Dense-Unet</cell><cell cols="2">0.7214 9736</cell></row></table></figure>
		</body>
		<back>

			<div type="funding">
<div><p>Funding. This work was supported in part by <rs type="funder">STI 2030-Major Projects</rs> (No. <rs type="grantNumber">2022ZD0209000</rs>), <rs type="funder">NSFC</rs> Grants (Nos. <rs type="grantNumber">62101430</rs>, <rs type="grantNumber">62101431</rs>, <rs type="grantNumber">82271517</rs>, <rs type="grantNumber">81771810</rs>), and <rs type="funder">Natural Science Basic Research Program of Shaanxi</rs> (No. <rs type="grantNumber">2022JM-464</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_EYvaRJG">
					<idno type="grant-number">2022ZD0209000</idno>
				</org>
				<org type="funding" xml:id="_Qy2Ra2h">
					<idno type="grant-number">62101430</idno>
				</org>
				<org type="funding" xml:id="_cGXCHeq">
					<idno type="grant-number">62101431</idno>
				</org>
				<org type="funding" xml:id="_Kua32DU">
					<idno type="grant-number">82271517</idno>
				</org>
				<org type="funding" xml:id="_TDTqsrw">
					<idno type="grant-number">81771810</idno>
				</org>
				<org type="funding" xml:id="_HMMbE6B">
					<idno type="grant-number">2022JM-464</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">White matter injury in the preterm infant: pathology and mechanisms</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Back</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Neuropathol</title>
		<imprint>
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="331" to="349" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Clinical implications of MR imaging findings in the white matter in very preterm infants: a 2-year follow-up study</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">T</forename><surname>De Bruïne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiology</title>
		<imprint>
			<biblScope unit="volume">261</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="899" to="906" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">White matter injury detection in neonatal MRI</title>
		<author>
			<persName><forename type="first">I</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer-Aided Diagnosis</title>
		<imprint>
			<biblScope unit="volume">8670</biblScope>
			<biblScope unit="page" from="664" to="669" />
			<date type="published" when="2013">2013. 2013</date>
			<publisher>SPIE</publisher>
		</imprint>
	</monogr>
	<note>Medical Imaging</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Stochastic process for white matter injury detection in preterm neonates</title>
		<author>
			<persName><forename type="first">I</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage Clin</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="622" to="630" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">iBEAT: a toolbox for infant brain magnetic resonance image processing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroinformatics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="211" to="225" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Limitations of ultrasonography for diagnosing white matter damage in preterm infants</title>
		<author>
			<persName><forename type="first">T</forename><surname>Debillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>N'guyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Muet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Quere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Moussaly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Roze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Arch. Dis. Child. Fetal Neonatal. Ed</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="275" to="F279" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Different patterns of punctate white matter lesions in serially scanned preterm infants</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Kersbergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">108904</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Automatic segmentation of diffuse white matter abnormality on T2-weighted brain MR images using deep learning in very preterm infants</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S P</forename><surname>Illapani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiol. Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2021">200166. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Refined segmentation R-CNN: a two-stage convolutional neural network for punctate white matter lesion segmentation in preterm infants</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32248-9_22</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32248-9_22" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11766</biblScope>
			<biblScope unit="page" from="193" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Monteiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">D S</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pawlowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.01274</idno>
		<title level="m">Measuring axiomatic soundness of counterfactual image models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A fast segmentation-free fully automated approach to white matter injury detection in preterm infants</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Basu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Biol. Eng. Comput</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="71" to="87" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learn-explain-reinforce: counterfactual reasoning and its guidance to reinforce an Alzheimer&apos;s disease diagnosis model</title>
		<author>
			<persName><forename type="first">K</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">I</forename><surname>Suk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="4843" to="4857" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep structural causal models for tractable counterfactual inference</title>
		<author>
			<persName><forename type="first">N</forename><surname>Pawlowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Coelho De Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="857" to="869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A structural causal model for MR images of multiple sclerosis</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Reinhold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Carass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Prince</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87240-3_75</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87240-3_75" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12905</biblScope>
			<biblScope unit="page" from="782" to="792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">3D-MASNet: 3D mixed-scale asymmetric convolutional segmentation network for 6-month-old infant brain MR images</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Hum. Brain Mapp</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1779" to="1792" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
