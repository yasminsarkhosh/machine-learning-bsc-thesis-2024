<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Automatic Bleeding Risk Rating System of Gastric Varices</title>
				<funder ref="#_yGzGbTj">
					<orgName type="full">Shenzhen Science and Technology Program</orgName>
				</funder>
				<funder ref="#_kCtfJ93">
					<orgName type="full">Shenzhen Sustainable Development Project</orgName>
				</funder>
				<funder ref="#_w4UwTpk">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_PY6TfJZ">
					<orgName type="full">Chinese Key-Area Research and Development Program of Guangdong Province</orgName>
				</funder>
				<funder ref="#_9fQQ6UF">
					<orgName type="full">Guangdong Basic and Applied Basic Research Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yicheng</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shenzhen Research Institute of Big Data</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Luyue</forename><surname>Shi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shenzhen Research Institute of Big Data</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><surname>Qi</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Gastroenterology</orgName>
								<orgName type="laboratory">Hebei Key Laboratory of Gastroenterology</orgName>
								<orgName type="institution" key="instit1">The Second Hospital of Hebei Medical University</orgName>
								<orgName type="institution" key="instit2">Hebei Institute of Gastroenterology</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Hebei Clinical Research Center for Digestive Diseases</orgName>
								<address>
									<settlement>Shijiazhuang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lei</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Gastroenterology</orgName>
								<orgName type="laboratory">Hebei Key Laboratory of Gastroenterology</orgName>
								<orgName type="institution" key="instit1">The Second Hospital of Hebei Medical University</orgName>
								<orgName type="institution" key="instit2">Hebei Institute of Gastroenterology</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Hebei Clinical Research Center for Digestive Diseases</orgName>
								<address>
									<settlement>Shijiazhuang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Guanbin</forename><surname>Li</surname></persName>
							<affiliation key="aff5">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaoguang</forename><surname>Han</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">FNii</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Shenzhen, Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">School of Science and Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Shenzhen, Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiang</forename><surname>Wan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shenzhen Research Institute of Big Data</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Siqi</forename><surname>Liu</surname></persName>
							<email>siqiliu@sribd.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Shenzhen Research Institute of Big Data</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Automatic Bleeding Risk Rating System of Gastric Varices</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="3" to="12"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">EAC4B6E38385C0AE0F14BEDD1A4258D3</idno>
					<idno type="DOI">10.1007/978-3-031-43904-9_1</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-25T18:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Gastric Varices</term>
					<term>Bleeding Risk Rating</term>
					<term>Cross-region Attention</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>An automated bleeding risk rating system of gastric varices (GV) aims to predict the bleeding risk and severity of GV, in order to assist endoscopists in diagnosis and decrease the mortality rate of patients with liver cirrhosis and portal hypertension. However, since the lack of commonly accepted quantification standards, the risk rating highly relies on the endoscopists' experience and may vary a lot in different application scenarios. In this work, we aim to build an automatic GV bleeding risk rating method that can learn from experienced endoscopists and provide stable and accurate predictions. Due to the complexity of GV structures with large intra-class variation and small inter-class variations, we found that existing models perform poorly on this task and tend to lose focus on the important varices regions. To solve this issue, we constructively introduce the segmentation of GV into the classification framework and propose the region-constraint module and cross-region attention module for better feature localization and to learn the correlation of context information. We also collect a GV bleeding risks rating dataset (GVbleed) with 1678 gastroscopy images from 411 patients that are jointly annotated in three levels of risks by senior clinical endoscopists. The experiments on our collected dataset show that our method can improve the rating accuracy by nearly 5% compared to the baseline. Codes and dataset will be available at https://github.com/ LuyueShi/gastric-varices.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Esophagogastric varices are one of the common manifestations in patients with liver cirrhosis and portal hypertension and occur in about 50 percent of patients with liver cirrhosis <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6]</ref>. The occurrence of esophagogastric variceal bleeding is the most serious adverse event in patients with cirrhosis, with a 6-week acute bleeding mortality rate as high as 15%-20% percent <ref type="bibr" target="#b13">[14]</ref>. It is crucial to identify high-risk patients and offer prophylactic treatment at the appropriate time. Regular endoscopy examinations have been proven an effective clinical approach to promptly detect esophagogastric varices with a high risk of bleeding <ref type="bibr" target="#b6">[7]</ref>. Different from the grading of esophageal varices (EV) that is relatively complete <ref type="bibr" target="#b0">[1]</ref>, the bleeding risk grading of gastric varices (GV) involves complex variables including the diameter, shapes, colors, and locations. Several rating systems have been proposed to describe GV based on the anatomical area. Sarin et al. <ref type="bibr" target="#b15">[16]</ref> described and divided GV into 2 groups according to their locations and extensions. Hashizume et al. <ref type="bibr" target="#b9">[10]</ref> published a more detailed examination describing the form, location, and color. Although the existing rating systems tried to identify the risk from different perspectives, they still lack clear quantification standard and heavily rely on the endoscopists' subjective judgment. This may cause inconsistency or even misdiagnosis due to the variant experience of endoscopists in different hospitals. Therefore, we aim to build an automatic GV bleeding risk rating method that can learn a stable and robust standard from multiple experienced endoscopists.</p><p>Recent works have proven the effectiveness and superiority of deep learning (DL) technologies in handling esophagogastroduodenoscopy (EGD) tasks, such as the detection of gastric cancer and neoplasia <ref type="bibr" target="#b3">[4]</ref>. It is even demonstrated that AI can detect neoplasia in Barrett's esophagus at a higher accuracy than endoscopists <ref type="bibr" target="#b7">[8]</ref>. Intuitively we may regard the GV bleeding risk rating as an image classification task and apply typical classification architectures (e.g., ResNet <ref type="bibr" target="#b11">[12]</ref>) or state-of-the-art gastric lesion classification methods to it. However, they may raise poor performance due to the large intra-class variation between GV with the same bleeding risk and small inter-class variation between GV and normal tissue or GV with different bleeding risks. First, the GV area may look like regular stomach rugae as it is caused by the blood vessels bulging and crumpling up the stomach (see Fig. <ref type="figure" target="#fig_0">1</ref>). Also, since the GV images are taken from different distances and angles, the number of pixels of the GV area may not reflect its actual size. Consequently, the model may fail to focus on the important GV areas for prediction as shown in Fig. <ref type="figure" target="#fig_2">3</ref>. To encourage the model to learn more robust representations, we constructively introduce segmentation into the classification framework. With the segmentation information, we further propose a region-constraint module (RCM) and a cross-region attention module (CRAM) for better feature localization and utilization. Specifically, in RCM, we utilize the segmentation results to constrain the CAM heatmaps of the feature maps extracted by the classification backbone, avoiding the model making predictions based on incorrect areas. In CRAM, the varices features are extracted using the segmentation results and combined with an attention mechanism to learn the intra-class correlation and cross-region correlation between the target area and the context.</p><p>To learn from experienced endoscopists, GV datasets with bleeding risks annotation is needed. While most works and public datasets focus on colonoscopy <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15]</ref> and esophagus <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9]</ref>, with a lack of study on gastroscopy images. In the public dataset of EndoCV challenge <ref type="bibr" target="#b1">[2]</ref>, the majority are colonoscopies while only few are gastroscopy images. In this work, we collect a GV bleeding risks rating dataset (GVbleed) that contains 1678 gastroscopy images from 411 patients with different levels of GV bleeding risks. Three senior clinical endoscopists are invited to grade the bleeding risk of the retrospective data in three levels and annotated the corresponding segmentation masks of GV areas.</p><p>In sum, the contributions of this paper are: 1) a novel GV bleeding risk rating framework that constructively introduces segmentation to enhance the robustness of representation learning; 2) a region-constraint module for better feature localization and a cross-region attention module to learn the correlation of target GV with its context; 3) a GV bleeding risk rating dataset (GVbleed) with high-quality annotation from multiple experienced endoscopists. Baseline methods have been evaluated on the newly collected GVbleed dataset. Experimental results demonstrate the effectiveness of our proposed framework and modules, where we improve the accuracy by nearly 5% compared to the baseline model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>The architecture of the proposed framework is depicted in Fig. <ref type="figure" target="#fig_1">2</ref>, which consists of a segmentation module (SM), a region constraint module (RCM), and a crossregion attention module (CRAM). Given a gastroscopy image, the SM is first applied to generate the varices mask of the image. Then, the image together with the mask are fed into the CRAM to extract the cross-region attentive feature map, and a class activation map (CAM) is calculated to represent the concentrated regions through RCM. Finally, a simple classifier is used to predict the bleeding risk using the extracted feature map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Segmentation Module</head><p>Due to the large intra-class variation between GV with the same bleeding risk and small inter-class variation between GV and normal tissue or GV with different bleeding risks, existing classification models exhibit poor perform and tend to lose focus on the GV areas. To solve this issue, we first embed a segmentation network into the classification framework. The predict the varices mask is then used to assist the GV feature to obtain the final bleeding risk rate. Specifically, we use SwinUNet <ref type="bibr" target="#b10">[11]</ref> as the segmentation network, considering its great performance, and calculate the DiceLoss between the segmentaion result M p and ground truth mask of vaices region M gt for optimizing the network:</p><formula xml:id="formula_0">l se = 1 - 2ΣM p * M gt ΣM 2 p + ΣM 2 gt + , (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where is a smooth constant equals to 10 -5 .</p><p>A straightforward strategy to utilize the segmentation mask is directly using it as an input of the classification model, such as concatenating the image with the mask as the input. Although such strategy can improve the classification performance, it may still lose focus in some hard cases where the GV area can hardly be distinguished. To further regularize the attention and fully utilize the context information around the GV area, on top of the segmentation framework we proposed the cross-region attention module and the region-constraint module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Cross-Region Attention Module</head><p>Inspired by the self-attention mechanism <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>, we propose a cross-region attention module (CRAM) to learn the correlation of context information. The CRAM consists of an image encoder f im , a varices local encoder f vl and a varices global encoder f ve . Given the image I and the predicted varices mask M p , a feature extraction step is first performed to generate the image feature V m , the local varices feature V vl and global varices feature V vg :</p><formula xml:id="formula_2">V m = f im (I), V vl = f vl (I * M p ), V vg = f vg (concat[I, M p ]),<label>(2)</label></formula><p>Then, through similarity measuring, we can compute the attention with</p><formula xml:id="formula_3">A = (V vl ) T V vg , W ij = exp(A ij ) Σ p (exp(A pj )) ,<label>(3)</label></formula><p>which composes of two correlations: self-attention over varices regions and crossregion attention between varices and background regions. Finally, the output feature is calculated as:</p><formula xml:id="formula_4">V = γV m W + V m , (<label>4</label></formula><formula xml:id="formula_5">)</formula><p>where γ is a learnable parameter. Then the cross-region attentive feature V is fed into a classifier to predict the bleeding risk.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Region Constraint Module</head><p>To improve the focus ability of the model, we propose the region constraint module (RCM) to add a constraint on the class activation map (CAM) of the classification model. Specifically, we use the feature map after the last convolutional layer to calculate the CAM <ref type="bibr" target="#b18">[19]</ref>, which computes the weighted sum of feature maps from the convolutional layer using the weights of the FC layer.</p><p>After getting the CAM, we regularize CAM by calculating the dice loss between the CAM and ground truth mask of varices region l co .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Network Training</head><p>In our framework, we use the cross entropy loss as the classification loss: where p is the prediction of the classifier and y is the ground-truth label. And the total loss of our framework can be summarized as:</p><formula xml:id="formula_6">l cl = - C c=1 log exp(p c ) Σ C i=1 exp(p i ) y c (5)</formula><formula xml:id="formula_7">L total = 1 N (ω s l se + ω co l co + ω cl l cl ),<label>(6)</label></formula><p>where N is the total number of samples, ω s , ω co and ω cl are weights of the three losses, respectively.</p><p>The training process of the proposed network consists of three steps: 1) The segmentation network is trained first; 2) The ground-truth segmentation masks and images are used as the inputs of the CRAM, the classification network, including CRAM and RCM, are jointly trained; 3) The whole framework is jointly fine-tuned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">GVBleed Dataset</head><p>Data Collection and Annotation. The GVBleed dataset contains 1678 endoscopic images with gastric varices from 527 cases. All of these cases are collected from 411 patients in a Grade-III Class-A hospital during the period from 2017 to 2022. In the current version, images from patients with ages elder than 18 are retained 1 . The images are selected from the raw endoscopic videos and frames. To maximize the variations, non-consecutive frames with larger angle differences are selected. To ensure the quality of our dataset, senior endoscopists are invited to remove duplicates, blurs, active bleeding, chromoendoscopy, and NBI pictures.</p><p>Criterion of GV Bleeding Risk Level Rating. Based on the clinical experience in practice, the GV bleeding risks in our dataset are rated into three levels, i.e., mild, moderate, and severe. The detailed rating standard is as follows: 1) Mild: low risk of bleeding, and regular follow-up is sufficient (usually with a diameter less than or equal to 5 mm). 2) Moderate: moderate risk of bleeding, and endoscopic treatment is necessary, with relatively low endoscopic treatment difficulty (usually with a diameter between 5 mm and 10 mm). 3) Severe: high risk of bleeding and endoscopic treatment is necessary, with high endoscopic treatment difficulty. The varices are thicker (usually with a diameter greater than 10 mm) or less than 10mm but with positive red signs. Note that the diameter is only one reference for the final risk rating since the GV is with 1 Please refer to the supplementary material for more detailed information about our dataset. various 3D shapes and locations. The other facts are more subjectively evaluated based on the experience of endoscopists. To ensure the accuracy of our annotation, three senior endoscopists with more than 10 years of clinical experience are invited to jointly label each sample in our dataset. If three endoscopists have inconsistent ratings for a sample, the final decision is judged by voting. A sample is selected and labeled with a specific bleeding risk level only when two or more endoscopists reach a consensus on it. The GVBleed dataset is partitioned into training and testing sets for evaluation, where the training set contains 1337 images and the testing set has 341 images. The detailed statistics of the three levels of GV bleeding risk in each set are shown in Table <ref type="table" target="#tab_0">1</ref>. The dataset is planned to be released in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation Details</head><p>In experiments, the weights ω s , ω co , and ω cl of the segmentation loss, region constraint loss, and classification loss are set to 0.2, 1, and 1, respectively. The details of the three-step training are as follows: 1) Segmentation module: We trained the segmentation network for 600 epochs, using Adam as the optimizer, and the learning rate is initialized as 1e-3 and drops to 1e-4 after 300 epochs. 2) Cross-region attention module and region constraint module: We used the ground-truth varices masks and images as the inputs of the CRAM, and jointly trained the CRAM and RCM for 100 epochs. Adam is used as the optimizer, the learning rate is set to 1e-3; 3) Jointly fine-tuning: The whole framework is jointly fine-tuned for 100 epochs with Adam as optimizer and the learning rate set to 1e-3. In addition, common data augmentation techniques such as rotation and flipping were adopted here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results Analysis</head><p>Table <ref type="table" target="#tab_1">2</ref> reports the quantitative results of different models and Fig. <ref type="figure" target="#fig_2">3</ref> shows the CAM visualizations. We tested several baseline models, including both simple CNN models and state-of-the-art transformer-based models. However, the transformer-based models achieves much worse performances since they always require more training data, which is not available in our task. Thus, we selected the simple CNN models as baselines since they achieve better performances. As shown in the figure, the baseline model cannot focus on the varices regions due to the complexity of GV structures with large intra-class variations and small inter-class variations. By introducing the segmentation of GV into the framework, concatenating the image with its segmentation mask as the inputs of the classifier can improve the classification accuracy by 1.2%. And the focus ability of the classifier is stronger than the baseline model. With the help of CRAM, the performance of the model can be further improved. Although the model can extract more important context information at the varices regions, the performance improvement is not very large since the focus ability is not the best and the model may still make predictions based on the incorrect regions for some hard images. By adding the RCM to the CRAM, the focus ability of the model can be further improved, and thus the model has a significant improvement in performance by 5% compared to the baseline model, this proves the effectiveness of our proposed modules. Note that, the baseline model tends to predict the images as severe, thus the f1-score of severe is high but the f1-scores of mild and moderate are significantly lower than other models. More quantitative and visualization results are shown in supplementary material. In addition, given the input image with resolution 512 × 512, the parameters and computational cost of our framework are 40.2M, and 52.4G MACs, and 29 ms inference time for a single image on GPU RTX2080. For comparison, a single ResNet152 model has 60.19M parameters with 60.62 G MACs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we propose a novel bleeding risk rating framework for gastric varices. Due to the large intra-class variation between GV with the same bleeding risk and small inter-class variation between GV and normal tissue or GV with different bleeding risks, existing classification models cannot correctly focus on the varices regions and always raise poor performance. To solve this issue, we constructively introduce segmentation to enhance the robustness of representation learning. Besides, we further design a region-constraint module for better feature localization and a cross-region attention module to learn the correlation of target GV with its context. In addition, we collected the GVBleed dataset with high-quality annotation of three-level of GV bleeding risks. The experiments on our dataset demonstrated the effectiveness and superiority of our framework.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. Visualization of our GVbleed dataset. The three rows from up to down show the typical samples of gastric varices with mild, moderate, and severe bleeding risk, respectively. The green line represents the contour of the target gastric varices regions, from which we can sense the complex patterns involving the diameter, shapes, and colors.</figDesc><graphic coords="3,58,98,54,20,334,72,167,50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Pipeline of the proposed automatic bleeding risk rating framework. The framework consists of a segmentation module, a cross-region attention module, and a region constraint module.</figDesc><graphic coords="4,44,79,54,11,334,45,166,60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Visualization of CAM. Green lines represent the contours of varices regions. The blue areas represent the primary areas of concern. With the proposed SegMask and RCM module, the network tends to focus on the important gastric varices areas as endoscopists do.</figDesc><graphic coords="8,95,31,335,00,233,14,192,46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Distribution of the three-level GV bleeding risks in the GVBleed dataset.</figDesc><table><row><cell cols="4">Mild Moderate Severe Total</cell></row><row><cell cols="2">Train 398 484</cell><cell>455</cell><cell>1337</cell></row><row><cell>Test 94</cell><cell>145</cell><cell>102</cell><cell>341</cell></row><row><cell cols="2">Total 462 629</cell><cell>557</cell><cell>1678</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Quantitative results of the proposed method and modules.</figDesc><table><row><cell>Models</cell><cell cols="2">Accuracy(%) F1-Score(%)</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Mild Moderate Severe</cell></row><row><cell>ResNet18</cell><cell>65.40</cell><cell>60.17 60.38</cell><cell>73.25</cell></row><row><cell>ResNet18 + SegMask (Concat)</cell><cell>66.86</cell><cell>69.30 66.46</cell><cell>65.98</cell></row><row><cell>ResNet18 + SegMask + Attention</cell><cell>67.16</cell><cell>69.21 65.50</cell><cell>66.62</cell></row><row><cell cols="2">ResNet18 + SegMask + Attention + Constraint 70.97</cell><cell>73.33 68.32</cell><cell>72.88</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work is supported by <rs type="funder">Chinese Key-Area Research and Development Program of Guangdong Province</rs> (<rs type="grantNumber">2020B0101350001</rs>), and the <rs type="funder">Shenzhen Science and Technology Program</rs> (<rs type="grantNumber">JCYJ20220818103001002</rs>), and the <rs type="institution">Guangdong Provincial Key Laboratory of Big Data Computing</rs>, <rs type="affiliation">The Chinese University of Hong Kong, Shenzhen</rs>. It was also supported by the <rs type="funder">Shenzhen Sustainable Development Project</rs> (<rs type="grantNumber">KCXFZ20201221173008022</rs>), the <rs type="funder">National Natural Science Foundation of China</rs> (<rs type="grantNumber">NO. 61976250</rs>), the <rs type="funder">Guangdong Basic and Applied Basic Research Foundation</rs> (NO. <rs type="grantNumber">2020B1515020048</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_PY6TfJZ">
					<idno type="grant-number">2020B0101350001</idno>
				</org>
				<org type="funding" xml:id="_yGzGbTj">
					<idno type="grant-number">JCYJ20220818103001002</idno>
				</org>
				<org type="funding" xml:id="_kCtfJ93">
					<idno type="grant-number">KCXFZ20201221173008022</idno>
				</org>
				<org type="funding" xml:id="_w4UwTpk">
					<idno type="grant-number">NO. 61976250</idno>
				</org>
				<org type="funding" xml:id="_9fQQ6UF">
					<idno type="grant-number">2020B1515020048</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9_1.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Oesophageal and gastric varices: historical aspects, classification and grading: everything in one place</title>
		<author>
			<persName><forename type="first">Abby</forename><surname>Philips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sahney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Gastroenterol. Rep</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="186" to="195" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ghatwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<title level="m">EndoCV 2021 3rd International Workshop and Chal-lenge on Computer Vision in En-doscopy</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Prediction of the first variceal hemorrhage in patients with cirrhosis of the liver and esophageal varices: a prospective multicenter study</title>
		<author>
			<persName><forename type="first">E</forename><surname>Brocchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">New England J. Med</title>
		<imprint>
			<biblScope unit="volume">319</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page" from="983" to="989" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Potentials of AI in medical image analysis in gastroenterology and hepatology</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Sung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Gastroenterol. Hepatol</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="38" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improving the classification performance of esophageal disease on small dataset by semi-supervised efficient contrastive learning</title>
		<author>
			<persName><forename type="first">W</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Med. Syst</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Portal hypertensive bleeding in cirrhosis: risk stratification, diagnosis, and management: 2016 practice guidance by the American association for the study of liver diseases</title>
		<author>
			<persName><forename type="first">G</forename><surname>Garcia-Tsao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Abraldes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Berzigotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bosch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Hepatology</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="310" to="335" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Endoscopic diagnosis and management of esophagogastric variceal hemorrhage: European society of gastrointestinal endoscopy (ESGE) guideline</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">M</forename><surname>Gralnek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Endoscopy</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1094" to="1120" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep-learning system detects neoplasia in patients with Barrett&apos;s esophagus with higher accuracy than endoscopists in a multistep training and validation study with benchmarking</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>De Groof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Gastroenterology</title>
		<imprint>
			<biblScope unit="volume">158</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="915" to="929" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Real-time automated diagnosis of precancerous lesions and early esophageal squamous cell carcinoma using a deep learning model (with videos)</title>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Gastrointest. Endosc</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="51" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Endoscopic classification of gastric varices</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hashizume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kitano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yamaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Koyanagi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sugimachi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Gastrointest. Endosc</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="276" to="280" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Swin UNETR: swin transformers for semantic segmentation of brain tumors in MRI images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hatamizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-08999-2_22</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-08999-2_22" />
	</analytic>
	<monogr>
		<title level="m">BrainLes 2021</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Crimi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bakas</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">12962</biblScope>
			<biblScope unit="page" from="272" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Ensemble of instance segmentation models for polyp segmentation in colonoscopy images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="26440" to="26447" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Covered tips versus endoscopic band ligation plus propranolol for the prevention of variceal rebleeding in cirrhotic patients with portal vein thrombosis: a randomised controlled trial</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Gut</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2156" to="2168" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">LDPolypVideo benchmark: a largescale colonoscopy video dataset of diverse polyps</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87240-3_37</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87240-3_37" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12905</biblScope>
			<biblScope unit="page" from="387" to="396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Gastric varices: profile, classification, and management</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sarin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Am. J. Gastroenterol. (Springer Nature)</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cross-modal self-attention network for referring image segmentation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rochan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="10502" to="10511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
