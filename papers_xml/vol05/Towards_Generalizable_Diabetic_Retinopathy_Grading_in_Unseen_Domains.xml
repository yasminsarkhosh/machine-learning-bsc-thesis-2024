<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards Generalizable Diabetic Retinopathy Grading in Unseen Domains</title>
				<funder ref="#_Yspy2GS">
					<orgName type="full">Shenzhen Science and Technology Innovation Committee Fund</orgName>
				</funder>
				<funder ref="#_8yM8FZk">
					<orgName type="full">Hong Kong Innovation and Technology Fund</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Haoxuan</forename><surname>Che</surname></persName>
							<email>hche@cse.ust.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
								<address>
									<settlement>Kowloon, Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuhan</forename><surname>Cheng</surname></persName>
							<email>ychengbj@cse.ust.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
								<address>
									<settlement>Kowloon, Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haibo</forename><surname>Jin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
								<address>
									<settlement>Kowloon, Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hao</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
								<address>
									<settlement>Kowloon, Hong Kong</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Chemical and Biological Engineering</orgName>
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
								<address>
									<settlement>Kowloon, Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Towards Generalizable Diabetic Retinopathy Grading in Unseen Domains</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="430" to="440"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">8AF27BF70050BD4CAB9513314838660B</idno>
					<idno type="DOI">10.1007/978-3-031-43904-9_42</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-25T18:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Diabetic Retinopathy (DR) is a common complication of diabetes and a leading cause of blindness worldwide. Early and accurate grading of its severity is crucial for disease management. Although deep learning has shown great potential for automated DR grading, its realworld deployment is still challenging due to distribution shifts among source and target domains, known as the domain generalization problem. Existing works have mainly attributed the performance degradation to limited domain shifts caused by simple visual discrepancies, which cannot handle complex real-world scenarios. Instead, we present preliminary evidence suggesting the existence of three-fold generalization issues: visual and degradation style shifts, diagnostic pattern diversity, and data imbalance. To tackle these issues, we propose a novel unified framework named Generalizable Diabetic Retinopathy Grading Network (GDRNet). GDRNet consists of three vital components: fundus visual-artifact augmentation (FundusAug), dynamic hybrid-supervised loss (DahLoss), and domain-class-aware re-balancing (DCR). FundusAug generates realistic augmented images via visual transformation and image degradation, while DahLoss jointly leverages pixel-level consistency and image-level semantics to capture the diverse diagnostic patterns and build generalizable feature representations. Moreover, DCR mitigates the data imbalance from a domain-class view and avoids undesired over-emphasis on rare domain-class pairs. Finally, we design a publicly available benchmark for fair evaluations. Extensive comparison experiments against advanced methods and exhaustive ablation studies demonstrate the effectiveness and generalization ability of GDRNet. The source code is released at https://github.com/chehx/DGDR.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Diabetic Retinopathy (DR) is a leading cause of blindness, affecting millions of people worldwide, and early severity grading is vital for disease management <ref type="bibr" target="#b0">[1]</ref>. Although deep learning (DL) has shown promising results in automatic DR grading <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref>, its real-world deployment is still challenging. For instance, Google's DR grading system performed ideally in controlled lab settings <ref type="bibr" target="#b3">[4]</ref>, but failed to generalize well to complex scenarios which suffer from data shifts <ref type="bibr" target="#b4">[5]</ref>. It is a common problem known as domain generalization (DG) <ref type="bibr" target="#b5">[6]</ref>, where the model performance significantly drops when applied to unseen domains different from the training data. Such an issue hinders the wide adoption and success of DLbased diagnostic tools in clinical practice <ref type="bibr" target="#b6">[7]</ref>.</p><p>Recently, several studies have explored the DG problem and reported significant performance drops in the retinal vessel segmentation <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>. Similarly, in DR grading, previous works showed a significant decrease in performance when presented with unseen domains and attempted to solve this problem through the perspective of feature disentanglement <ref type="bibr" target="#b9">[10]</ref> and domain-invariant feature learning <ref type="bibr" target="#b11">[11]</ref>. Although these methods have improved performance towards unseen domains, they may not be effective in more complex real-world scenarios because they attribute the generalization issue only to limited domain shifts, such as simple visual discrepancies. However, the generalization issues across domains cannot be solely attributed to visual discrepancies <ref type="bibr" target="#b12">[12]</ref>.</p><p>In contrast to previous works, we argue that three factors contribute to poor generalization in DGDR: visual and degradation style shifts, diagnostic pattern diversity, and data imbalance. Specifically, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>, we first conduct a preliminary analysis of three public datasets/domains. First, style shifts arise due to various factors, not only limited to visual style discrepancies, but also factors such as variations in lighting conditions <ref type="bibr" target="#b14">[13]</ref>, image resolution <ref type="bibr" target="#b15">[14]</ref>, or the presence of artifacts or noise <ref type="bibr" target="#b16">[15]</ref>. These factors have been neglected in previous works yet are essential for building a generalizable model. Second, domains may contain diverse diagnostic patterns, such as variations in lesion types, distribution, combination and severity in certain categories <ref type="bibr" target="#b17">[16,</ref><ref type="bibr" target="#b18">17]</ref>. This diversity makes learning a generalizable model towards unseen domains challenging because they may contain partially-overlapped or even unknown diagnostic patterns. Finally, data imbalance across categories and domains causes samples from specific datasets and minority classes to be underrepresented. Moreover, this imbalance can exacerbate the issue of omitting rare diagnostic patterns, leading to shortcuts in learning and poor generalization <ref type="bibr" target="#b19">[18]</ref>.</p><p>In this paper, we propose a novel framework, Generalizable Diabetic Retinopathy Grading Network (GDRNet) to address the DGDR problem. Our framework consists of three critical components: fundus visual-artifact augmentation (FundusAug), dynamic hybrid-supervised loss (DahLoss), and domainclass-aware re-balancing (DCR). By simulating visual transformations and image degradations, FundusAug enables the model to learn robust features that are less sensitive to style shifts caused by factors such as lighting conditions or artifacts and noise. DahLoss employs a hybrid-supervised learning paradigm to handle diagnostic pattern diversity and dynamically balances the influence of supervised and unsupervised learning. Jointly functioning with FundusAug, it enables the model to preserve pixel-level diagnostic information and learn generalizable features with sufficient intra-class variations. Furthermore, DCR assigns soft-balanced weights to each domain-class pair to prevent underrepresentation caused by data imbalance while avoiding undesired over-emphasis introduced by hard weighting. Finally, to evaluate generalization ability, we design a publicly available benchmark named Generalizable Diabetic Retinopathy Grading Benchmark (GDRBench), comprising eight popular datasets and two evaluation settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>An overview of GDRNet is shown in Fig. <ref type="figure" target="#fig_1">2</ref>. It addresses the mentioned generalization issues, including style shifts, diagnostic pattern diversity, and domain-class data imbalance, by the proposed FundusAug, DahLoss, and DCR, respectively. Overall, GDRNet provides a unified solution to improve the generalization ability in unseen domains. This section will introduce each component in detail.</p><p>Fundus Visual-Artifact Augmentation. The external machine and internal retinal illumination conditions can cause differences in visual attributes <ref type="bibr" target="#b14">[13]</ref>, such as contrast and brightness, while image degradations, like spot artifacts and blurring, are also very common in fundus imaging <ref type="bibr" target="#b16">[15,</ref><ref type="bibr" target="#b20">19]</ref>, as depicted in Fig. <ref type="figure" target="#fig_0">1</ref>. To bridge the style shifts caused by visual discrepancies and image degradations, we developed FundusAug, which is parameter-free and plug-anduse, designed to generate diverse and realistic augmented views. It employs five basic image transformations, including brightness, contrast, saturation, hue, and sharpness adjustments, to fill visual discrepancy gaps. Moreover, it uses extra four degradation-based image transformations, including halo simulation, hole generation, spot addition, and image blur, to address image degradation gaps. Specifically, given training data x with label y, the augmented view x of original data can be derived through the following equation:</p><formula xml:id="formula_0">x := FundusAug(x) = π n pn,mn (π n-1 pn-1,mn-1 (...π 1 p1,m1 (x))),<label>(1)</label></formula><p>where π n pn,mn denotes transformation n with probability p n and random intensity m n . To reduce the parameter space of FundusAug while still ensuring image diversity, we implemented FundusAug by applying each operation with a parameter-free procedure that uniformly selects a process with a probability of 0.5. FundusAug can generate realistic augmented views while preserving their diagnostic semantics, as well as providing a robust foundation for subsequent generalizable feature learning by increasing image diversity. A detailed description and visualization of operations can be found in the appendix.</p><p>Dynamic Hybrid-Supervised Loss. While the supervised loss (SupLoss), e.g., the cross-entropy loss (CE), effectively guides the model to learn effective feature representations <ref type="bibr" target="#b21">[20]</ref>, it has two disadvantages in DGDR. First, SupLoss leads to dense features within categories while sufficient intra-class variations are crucial for effectively generalizing to unseen domains <ref type="bibr" target="#b22">[21]</ref>. Second, the potential variety of diagnostic patterns in unseen domains requires the models to learn pixel-level lesion semantics as much as possible, while SupLoss lacks such functionality <ref type="bibr" target="#b23">[22]</ref>. To tackle these issues, we proposed DahLoss to encourage models to learn features with sufficient intra-class variations and preserve diagnostic patterns, by introducing a hybrid-supervised paradigm to jointly leverage imagelevel severity supervision and pixel-level semantics consistency. A basic form of DahLoss is as</p><formula xml:id="formula_1">L dhl = (1 -α)L sup + αL scon ,<label>(2)</label></formula><p>where α decreasing within range [0, 1] is a hyper-parameter to dynamically control the task focus, and L sup and L scon could be any supervised and selfsupervised contrastive loss functions. In this paper, we adopt CE and instance discrimination loss <ref type="bibr" target="#b24">[23]</ref> as L sup and L scon . Specifically, within a multiviewed batch, let i ∈ I ≡ {1...N } be the index of arbitrary augmented samples by Fun-dusAug and j(i) ∈ J ≡ {1 ...N } be the index of weakly-augmented samples originating from the same source sample, we denote L i dhs for sample i as</p><formula xml:id="formula_2">L i dhl = -(1 -α) log p i t -α log exp(f i • f j(i) /τ ) a∈A(i) exp(f i • f a )/τ ) , (<label>3</label></formula><formula xml:id="formula_3">)</formula><p>where p • t denotes the predicted probability of the true class under one-hot encoding label, f • denotes the l 2 normalized feature, the • symbol denotes the inner product, τ is the temperature parameter and A(i) ≡ (J ∪ I)/i.</p><p>As illustrated in Fig. <ref type="figure" target="#fig_1">2</ref> (b), the use of L sup alone tends to force samples within the same class to cluster tightly together, resulting in a highly concentrated feature representation that may not be beneficial for generalization to unseen domains. However, by incorporating L scon , DahLoss can achieve a balance between the intra-class variation and inter-class distance in learned feature representations. Specifically, while L sup encourages samples within the same class to cluster together, L scon simultaneously pulls the augmented views of these samples closer to each other and pushes them far away from those of other samples. It results in a feature representation with both sufficient intraclass variation and clear inter-class separation. Moreover, DahLoss also leverages pixel-level consistency to preserve crucial diagnostic patterns. It is achieved by enforcing the model to maintain feature representations with semantics similarity between strong-weak augmented views originating from the same sample, which ensures that critical details and structures are learned. By jointly considering both image-level supervision and pixel-level consistency, DahLoss encourages the model to learn features with sufficient intra-class variations and preserve crucial diagnostic information, improving generalization performance in unseen domains. Finally, gradually decreasing the value of α during training would guide the model to focus more on the grading task, leading to a balance between representation learning and grading performance. In this paper, we simply set α decay from 1 to 0 linearly across training epochs to verify the effectiveness of DahLoss.</p><p>Domain-Class-Aware Re-balancing. The domain-class data imbalance can result in certain categories and diagnostic patterns in specific datasets being underrepresented, leading to biased and inaccurate model predictions <ref type="bibr" target="#b12">[12]</ref>. To complicate the situation, adopting a hard balancing style, such as weighting domain-class pairs based on the reciprocal of occurrence frequency, could potentially cause performance degradation <ref type="bibr" target="#b25">[24]</ref>. Such decay is owing to the fact that underrepresented domain-class pairs often have a small ratio, resulting in excessively high weights to dominate gradients. Inspired by <ref type="bibr" target="#b26">[25]</ref>, we designed the DCR method to address this issue. It assigns weights to each sample based on the occurrence probability q d c of its category c and domain d. Specifically, DCR calculates the weight w d c for category c in domain d as</p><formula xml:id="formula_4">w d c = d ∈D N j=1 (q d j ) β (q d c ) β , (<label>4</label></formula><formula xml:id="formula_5">)</formula><p>where D denotes the set of domains, N is the amount of classes, and β with a range of [0, 1] is a hyperparameter that adjusts the balancing intensity. When β approaches to 0, DCR assigns weight more equally, and when β closes to 1, it acts more like the naive hard balancing method. By introducing β, DCR enables more nuanced weighting of samples based on their domain and class, reducing the risk of over-emphasizing underrepresented samples in the loss function. By considering the occurrence probabilities of all categories across all domains, DCR mitigates the data imbalance problem of underrepresented class-domain pairs. These two make DCR an effective solution for handling domain-class data imbalance and improving the generalizability of DR grading models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>Experimental Settings, Implementation Details and Evaluation Metrics.</p><p>To comprehensively analyze and evaluate our framework, we designed the GDRBench involving two generalization ability evaluation settings and eight popular public datasets. First, GDRBench preserves the classic leave-onedomain-out protocol (DG test), which requires leaving one domain for evaluation and training models on the rest. It involves six datasets, including DeepDR <ref type="bibr" target="#b17">[16]</ref>, Messidor <ref type="bibr" target="#b18">[17]</ref>, IDRID <ref type="bibr" target="#b32">[31]</ref>, APTOS <ref type="bibr" target="#b33">[32]</ref>, FGADR <ref type="bibr" target="#b34">[33]</ref>, and RLDR <ref type="bibr" target="#b35">[34]</ref>. Further, we designed an extreme single-domain generalization setting (ESDG test), which follows the train-on-single-domain protocol with datasets mentioned above but adds two extra large-scale datasets, DDR <ref type="bibr" target="#b36">[35]</ref> and EyePACS <ref type="bibr" target="#b37">[36]</ref> for evaluation. It simulates real-world generalization issues, in which models are trained only on thousands of samples but are required to generalize well on one hundred thousand images from multiple hospitals and areas. We used ResNet50 pre-trained on ImageNet as the backbone and a fully connected layer as the linear classifier. For evaluation, we report three critical metrics, namely accuracy (ACC), the area under the ROC curve (AUC), and macro F1-score (F1). We used bold and underline to indicate the first-and second-highest scores. A detailed illustration of the datasets and implementation settings can be found in the appendix.</p><p>Comparison with Other Methods. We conducted a comprehensive experiment to evaluate our framework, comparing it with a vanilla baseline (ERM) and other state-of-the-art methods from various categories, including ophthalmic disease diagnosis (OSD) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, domain generalization techniques (DGT) <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b27">[26]</ref><ref type="bibr" target="#b28">[27]</ref><ref type="bibr" target="#b29">[28]</ref><ref type="bibr" target="#b30">[29]</ref>, and feature representation learning (FRL) <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b31">30]</ref>. These chosen methods can be adopted in DGDR with minimal or no modifications, and their brief descriptions are in the appendix. We employed a standard DG augmentation pipeline <ref type="bibr" target="#b12">[12]</ref> for all methods except DRGen with a default augmentation strategy <ref type="bibr" target="#b11">[11]</ref>. Table <ref type="table" target="#tab_0">1</ref> shows the quantitative results of the DG test, where the row of "target" indicates the test domain. GDRNet performs better than other methods and significantly improves at least two of AUC, ACC, and F1 on all sub-tests except the test on IDRiD, whose small scale leads to unobvious diagnostic pattern diversity. Typically, ODS methods do not consider domain shifts, and they thus fail to generalize as well as other methods. As expected, DGT and FRL methods consistently improve the performance compared with ERM due to specific designs towards limited domain shifts. However, GDRNet still outperforms these methods markedly due to it handles three-fold generalization issues via increasing training diversity via realistic augmentation, learning generalizable features with sufficient intra-class variations, preserving diagnostic patterns, and rebalancing the minority of samples. Overall, the result shows the effectiveness of GDRNet.</p><p>Ablation Study of Proposed Components. To evaluate the effectiveness of our proposed components, we conducted an extensive ablation study under the DG test and presented the AUC score achieved by different models in Table <ref type="table" target="#tab_1">2</ref>.</p><p>We first examined the individual effects of FundusAug with only visual transformation (VT), FundusAug with only image degradation (ID), DCR and DahLoss  <ref type="table" target="#tab_2">3</ref>, where the row of "source" indicates the training dataset. As expected, the average performance of all methods decreases significantly, indicating the difficulty of ESDG. Although some methods outperform others in the DG test, such as MixStyle, due to their specific design to leverage the discrepancy of domains, they fail to generalize in the strict ESDG test. Despite more strict requirements, GDRNet still outperforms other methods in average performance and at least one metric in all sub-tests, owing to its effective designs towards three-fold generalization issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we tackled the three-fold generalization issues that hinder the generalizability of DR grading, including style shifts, diagnostic pattern diversity, and data imbalance. To overcome these challenges, we proposed a novel and unified framework called GDRNet, incorporating three effective components: FundusAug, DahLoss, and DCR. FundusAug enables the generation of realistic augmented views, while DahLoss leverages supervised and unsupervised learning to preserve diagnostic patterns and increase intra-class variations of features. Finally, DCR softly handles the data imbalance across categories and domains to avoid potential performance decay. Together, these three components work synergistically to improve the generalization performance of the model. GDR-Net achieved superior performance on both DG and ESDG tests of the proposed publicly available GDRBench, demonstrating its effectiveness and robustness in addressing the three-fold generalization issues in DR grading. Overall, our work provides valuable insights and practical solutions for improving the generalization capability of deep learning in medical image analysis, and has the potential to benefit real-world clinical applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The RGB statistics, category histograms and proliferative DR (PDR) samples from different datasets/domains. It can be observed that the existence of visual differences, image degradations and diverse diagnostic patterns from PDR samples and RGB statistics. Besides, the divergence among label histograms shows the data imbalance problem across domains and categories.</figDesc><graphic coords="2,59,46,53,93,333,91,96,76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The pipeline of GDRNet and a high-level visual understanding of DahLoss. FundusAug generates diverse, realistic augmented views, then leverages DahLoss to preserve pixel-level diagnostic patterns and learn generalizable features with sufficient intra-class variations. Moreover, DCR is applied to prevent minority classes from being underrepresented.</figDesc><graphic coords="3,44,79,54,02,334,48,179,68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison with state-of-the-art approaches under the DG test.</figDesc><table><row><cell>Target</cell><cell>APTOS</cell><cell>DeepDR</cell><cell>FGADR</cell><cell>IDRID</cell><cell>Messidor</cell><cell>RLDR</cell><cell>Average</cell></row><row><cell>Metrics</cell><cell>AUC ACC F1</cell><cell>AUC ACC F1</cell><cell>AUC ACC F1</cell><cell>AUC ACC F1</cell><cell>AUC ACC F1</cell><cell>AUC ACC F1</cell><cell>AUC ACC F1</cell></row><row><cell>ERM</cell><cell cols="7">75.0 44.4 38.9 77.0 39.5 34.3 66.2 32.0 27.1 82.3 50.0 44.1 79.1 60.7 43.4 75.9 36.5 35.7 75.9 43.8 37.3</cell></row><row><cell cols="8">DRGen [11] 79.9 58.1 40.2 83.0 38.7 34.1 69.4 41.7 24.7 84.7 44.6 37.4 79.0 60.1 40.5 79.5 43.1 37.0 79.3 47.7 37.3</cell></row><row><cell>Mixup [26]</cell><cell cols="7">75.3 62.6 43.2 75.3 29.0 25.2 66.7 42.3 32.3 78.8 39.0 27.6 76.7 54.7 32.6 76.9 43.6 37.7 75.0 45.2 33.1</cell></row><row><cell cols="8">MixStyle [27] 79.0 65.8 39.9 76.9 32.9 27.9 71.2 35.8 22.7 83.0 51.4 39.2 75.2 62.2 36.5 75.5 41.1 31.4 76.8 48.2 32.9</cell></row><row><cell cols="8">GREEN [3] 75.1 53.8 38.9 76.4 28.1 24.9 69.5 41.3 31.5 79.9 41.3 32.2 75.8 52.0 36.8 74.8 34.0 34.4 75.3 41.8 33.1</cell></row><row><cell cols="8">CABNet [2] 75.8 55.5 39.4 75.2 42.7 31.8 73.2 43.7 34.8 79.2 44.8 37.3 74.2 56.1 34.1 75.8 37.0 35.6 75.6 46.6 35.5</cell></row><row><cell cols="8">DDAIG [28] 78.0 67.1 41.0 75.6 37.6 32.2 73.6 42.0 33.8 82.1 37.4 27.0 76.6 58.4 35.3 75.6 36.1 27.7 76.9 46.4 32.8</cell></row><row><cell>ATS [29]</cell><cell cols="7">77.1 56.9 38.3 79.4 36.1 31.6 74.7 46.7 33.4 83.0 41.5 34.9 77.2 64.7 35.8 76.5 37.4 34.9 78.0 47.2 34.8</cell></row><row><cell>Fishr [30]</cell><cell cols="7">79.2 66.6 43.4 81.1 48.1 34.4 73.3 44.4 34.4 82.7 40.3 27.6 76.4 65.1 41.1 77.4 36.8 34.7 78.4 50.2 35.9</cell></row><row><cell>MDLT [12]</cell><cell cols="7">77.3 57.2 41.5 80.0 39.5 36.2 74.1 45.7 29.0 81.5 44.2 35.4 75.4 58.9 36.9 75.7 37.6 35.0 77.3 47.2 35.7</cell></row><row><cell>GDRNet</cell><cell cols="7">79.9 66.8 46.0 84.7 53.1 45.3 80.8 45.3 39.4 84.0 40.3 35.9 83.2 63.4 50.9 82.9 45.8 43.5 82.6 52.5 43.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation studies on proposed components under the DG test.</figDesc><table><row><cell cols="9">Method VT ID DCR Ldhl APTOS DeepDR FGADR IDRiD Messidor RLDR Average</cell></row><row><cell>ERM</cell><cell>-</cell><cell>--</cell><cell>-</cell><cell>75.04</cell><cell>77.02</cell><cell>66.19</cell><cell>82.31 79.10</cell><cell>75.86 75.92</cell></row><row><cell>Model A</cell><cell></cell><cell>--</cell><cell>-</cell><cell>77.16</cell><cell>80.22</cell><cell>74.87</cell><cell>82.83 80.48</cell><cell>80.23 79.30</cell></row><row><cell cols="2">Model B -</cell><cell>-</cell><cell>-</cell><cell>75.38</cell><cell>79.13</cell><cell>68.35</cell><cell>82.72 77.94</cell><cell>78.93 77.08</cell></row><row><cell cols="2">Model C -</cell><cell>-</cell><cell>-</cell><cell>75.58</cell><cell>78.76</cell><cell>67.34</cell><cell>82.59 78.86</cell><cell>78.39 76.92</cell></row><row><cell cols="2">Model D -</cell><cell>--</cell><cell></cell><cell>77.67</cell><cell>81.22</cell><cell>75.34</cell><cell>81.98 79.09</cell><cell>80.57 79.31</cell></row><row><cell>Model E</cell><cell></cell><cell>-</cell><cell>-</cell><cell>77.28</cell><cell>83.41</cell><cell>77.44</cell><cell>83.63 80.22</cell><cell>80.82 80.46</cell></row><row><cell>Model F</cell><cell></cell><cell></cell><cell>-</cell><cell>77.37</cell><cell>83.90</cell><cell>77.91</cell><cell>83.44 82.43</cell><cell>83.33 81.39</cell></row><row><cell>Model G</cell><cell></cell><cell>-</cell><cell></cell><cell>80.40</cell><cell>85.07</cell><cell>78.65</cell><cell>83.68 84.91</cell><cell>81.97 82.45</cell></row><row><cell>GDRNet</cell><cell></cell><cell></cell><cell></cell><cell>77.67</cell><cell>84.69</cell><cell>80.78</cell><cell>84.01 83.16</cell><cell>82.91 82.57</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Comparison with state-of-the-art approaches under the ESDG test.(Models A-D, respectively). Our results demonstrate that all proposed components effectively improve model generalization performance and address the three-fold generalization issues discussed in this paper. We then combined these components to analyze their joint effects (Models E-G). Notably, DahLoss and FundusAug contribute significantly to the improvement, demonstrating their capability to increase the training diversity, handle diagnostic pattern diversity, and dynamically balance the influence of supervised and unsupervised learning. Finally, we achieved the best performance by combining all the components in GDRNet, showing that our components play complementary roles. It is worth mentioning that we can observe different performance trends across the various datasets in the ablation study, which indicates the complexity of generalization issues in DGDR. It also suggests that different domains may have varying grades of severity in the three-fold generalization issues identified in this paper.Generalization from a Single Source Domain. Further, to comprehensively investigate the generalization performance, we introduced the ESDG test. It is a more demanding evaluation setting than the DG test because it requires models trained on a single source dataset to generalize to new domains with significantly larger scales and different data distributions. In contrast, the DG test involves training on multiple domains to fill domain gaps naturally. The quantitative results are presented in Table</figDesc><table><row><cell>Source</cell><cell>APTOS</cell><cell>DeepDR</cell><cell>FGADR</cell><cell>IDRID</cell><cell>Messidor</cell><cell>RLDR</cell><cell>Average</cell></row><row><cell>Metrics</cell><cell>AUC ACC F1</cell><cell>AUC ACC F1</cell><cell cols="2">AUC ACC F1 AUC ACC F1</cell><cell>AUC ACC F1</cell><cell>AUC ACC F1</cell><cell>AUC ACC F1</cell></row><row><cell>ERM</cell><cell cols="3">66.4 53.2 31.6 70.7 47.3 31.2 55.3 5.6</cell><cell cols="4">7.1 69.6 56.5 33.9 70.6 51.3 33.7 70.1 27.3 26.4 67.1 40.2 27.3</cell></row><row><cell cols="4">DRGen [11] 69.4 60.7 35.7 78.5 39.4 31.6 59.8 6.8</cell><cell cols="4">8.4 70.8 67.7 30.6 77.0 64.5 37.4 78.9 19.0 21.2 72.4 43.0 27.5</cell></row><row><cell>Mixup [26]</cell><cell cols="3">65.5 49.4 30.2 70.7 49.7 33.3 58.8 5.8</cell><cell cols="4">7.4 70.2 64.0 32.6 71.5 63.0 32.6 72.9 27.7 27.0 68.3 43.3 27.2</cell></row><row><cell cols="4">MixStyle [27] 62.0 48.8 25.0 53.3 32.0 14.6 51.0 7.0</cell><cell cols="3">7.9 53.0 53.5 19.4 51.4 57.6 16.8 53.5 18.3 6.4</cell><cell>54.0 36.2 15.0</cell></row><row><cell cols="4">GREEN [3] 67.5 52.6 33.3 71.2 44.6 31.1 58.1 5.7</cell><cell cols="4">6.9 68.5 60.7 33.0 71.3 54.5 33.1 71.0 31.9 27.8 67.9 41.7 27.5</cell></row><row><cell cols="4">CABNet [2] 67.3 52.2 30.8 70.0 55.4 32.0 57.1 6.1</cell><cell cols="4">7.5 67.4 62.7 31.7 72.3 63.8 35.3 75.2 23.0 25.4 68.2 43.8 27.2</cell></row><row><cell cols="4">DDAIG [28] 67.4 48.7 31.6 73.2 38.5 29.7 59.9 5.0</cell><cell cols="4">5.5 70.2 60.2 33.4 73.5 69.1 35.6 74.4 25.4 23.5 69.8 41.2 26.7</cell></row><row><cell>ATS [29]</cell><cell cols="3">68.8 51.7 32.4 72.7 52.4 33.5 60.3 5.3</cell><cell cols="4">5.7 69.1 66.6 30.6 73.4 64.8 32.4 75.0 24.2 23.9 69.9 44.2 26.4</cell></row><row><cell>Fishr [30]</cell><cell cols="3">64.5 61.7 31.0 72.1 61.0 30.1 56.3 6.0</cell><cell cols="4">7.2 71.8 48.0 30.6 74.3 52.0 33.8 78.6 19.3 21.3 69.6 41.3 25.7</cell></row><row><cell>MDLT [12]</cell><cell cols="3">67.6 53.3 32.4 73.1 50.2 33.7 57.1 7.1</cell><cell cols="4">7.8 71.9 61.7 32.4 73.4 58.9 34.1 76.6 29.0 30.0 70.0 43.4 28.4</cell></row><row><cell>GDRNet</cell><cell cols="7">69.8 52.8 35.2 76.1 40.0 35.0 63.7 7.5 9.2 72.9 70.0 35.1 78.1 65.7 40.5 79.7 44.3 37.9 73.4 46.7 32.2</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. This work was supported by the <rs type="funder">Hong Kong Innovation and Technology Fund</rs> (Project No. <rs type="grantNumber">ITS/028/21FP</rs>), <rs type="funder">Shenzhen Science and Technology Innovation Committee Fund</rs> (Project No. <rs type="grantNumber">SGDX20210823103201011</rs>) and <rs type="programName">HKUST 30 for 30 Research Initiative Scheme</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_8yM8FZk">
					<idno type="grant-number">ITS/028/21FP</idno>
				</org>
				<org type="funding" xml:id="_Yspy2GS">
					<idno type="grant-number">SGDX20210823103201011</idno>
					<orgName type="program" subtype="full">HKUST 30 for 30 Research Initiative Scheme</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9_42.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">IDF diabetes atlas: global estimates of diabetes prevalence for 2017 and projections for 2045</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">H</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karuranga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Da Rocha Fernandes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Diabetes Res. Clin. Pract</title>
		<imprint>
			<biblScope unit="volume">138</biblScope>
			<biblScope unit="page" from="271" to="281" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">CabNet: category attention block for imbalanced diabetic retinopathy grading</title>
		<author>
			<persName><forename type="first">A</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="143" to="153" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">GREEN: a graph REsidual rE-ranking network for grading diabetic retinopathy</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59722-1_56</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59722-1_56" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12265</biblScope>
			<biblScope unit="page" from="585" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A human-centered evaluation of a deep learning system deployed in clinics for the detection of diabetic retinopathy</title>
		<author>
			<persName><forename type="first">E</forename><surname>Beede</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2020 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Google&apos;s medical AI was super accurate in a lab. Real life was a different story</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">D</forename><surname>Heaven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MIT Technol. Rev</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Generalizing to unseen domains: a survey on domain generalization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Applications of deep learning in fundus images: a review</title>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page">101971</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">DoFE: domain-oriented feature embedding for generalizable fundus image segmentation on unseen datasets</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4237" to="4248" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">AADG: automatic augmentation for domain generalization on retinal image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3699" to="3711" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning robust representation for joint grading of ophthalmic diseases via adaptive curriculum and feature disentanglement</title>
		<author>
			<persName><forename type="first">H</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="page" from="523" to="533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16437-8_50</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16437-8_50" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">DRGen: domain generalization in diabetic retinopathy classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Atwany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yaqub</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16434-7_61</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16434-7_61" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="635" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On multi-domain long-tailed recognition, imbalanced domain generalization and beyond</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Katabi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Brostow</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Cissé</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="page" from="57" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-20044-1_4</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-20044-1_4" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Modeling and enhancing low-quality retinal fundus images</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="996" to="1006" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep multi-task learning for diabetic retinopathy grading in fundus images</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2826" to="2834" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Understanding how fundus image quality degradation affects CNN-based diagnosis</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">44th Annual International Conference of the IEEE Engineering in Medicine &amp; Biology Society</title>
		<imprint>
			<biblScope unit="page" from="438" to="442" />
			<date type="published" when="2022">2022</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">DeepDRiD: diabetic retinopathy-grading and image quality estimation challenge</title>
		<author>
			<persName><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Patterns</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">100512</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improved automated detection of diabetic retinopathy on a publicly available dataset through integration of deep learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Abràmoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Erginay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Invest. Ophthalmol. Vis. Sci</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="5200" to="5206" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Shortcut learning in deep neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="665" to="673" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Image quality-aware diagnosis via meta-knowledge co-embedding</title>
		<author>
			<persName><forename type="first">H</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="19819" to="19829" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unleashing the power of contrastive self-supervised visual models via contrast-regularized fine-tuning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hooi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="29848" to="29860" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Encouraging intra-class diversity through a reverse contrastive loss for single-source domain generalization</title>
		<author>
			<persName><forename type="first">T</forename><surname>Duboudin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Dellandréa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="51" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A broad study on the transferability of visual representations with contrastive learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Panda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8845" to="8855" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning to reweight examples for robust deep learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4334" to="4343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Cross-lingual language model pretraining</title>
		<author>
			<persName><forename type="first">A</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">mixup: beyond empirical risk minimization</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Domain generalization with MixStyle</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep domain-adversarial image generation for domain generalisation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="13025" to="13032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Adversarial teacher-student representation learning for domain generalization</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">Y</forename><surname>Shiau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C F</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="19448" to="19460" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fishr: invariant gradient variances for out-ofdistribution generalization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dancette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="18347" to="18377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Indian diabetic retinopathy image dataset (IDRiD): a database for diabetic retinopathy screening research</title>
		<author>
			<persName><forename type="first">P</forename><surname>Porwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pachade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kamble</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kokare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Deshmukh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">25</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><surname>Aptos</surname></persName>
		</author>
		<ptr target="https://www.kaggle.com/c/aptos2019-blindness-detection" />
		<title level="m">Aptos 2019 blindness detection website</title>
		<imprint>
			<date type="published" when="2022-02-20">20 Feb 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A benchmark for studying diabetic retinopathy: segmentation, grading, and transferability</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="818" to="828" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learn to segment retinal lesions and beyond</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="7403" to="7410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Diagnostic assessment of deep learning algorithms for diabetic retinopathy screening</title>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">501</biblScope>
			<biblScope unit="page" from="511" to="522" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<ptr target="https://paperswithcode.com/dataset/kaggle-eyepacs" />
		<title level="m">EYEPACS: Kaggle eyepacs dataset</title>
		<imprint>
			<date type="published" when="2023-02-20">20 Feb. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
