<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adaptive Region Selection for Active Learning in Whole Slide Image Semantic Segmentation</title>
				<funder>
					<orgName type="full">German Research Foundation (DFG)</orgName>
				</funder>
				<funder>
					<orgName type="full">Erlangen National High Performance Computing Center (NHR@FAU) of the Friedrich-Alexander-Universität Erlangen-Nürnberg</orgName>
					<orgName type="abbreviated">FAU</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Jingna</forename><surname>Qiu</surname></persName>
							<email>jingna.qiu@fau.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department Artificial Intelligence in Biomedical Engineering</orgName>
								<orgName type="institution">Friedrich-Alexander-Universität Erlangen-Nürnberg</orgName>
								<address>
									<settlement>Erlangen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Frauke</forename><surname>Wilm</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department Artificial Intelligence in Biomedical Engineering</orgName>
								<orgName type="institution">Friedrich-Alexander-Universität Erlangen-Nürnberg</orgName>
								<address>
									<settlement>Erlangen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">Pattern Recognition Lab</orgName>
								<orgName type="institution">Friedrich-Alexander-Universität Erlangen-Nürnberg</orgName>
								<address>
									<settlement>Erlangen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mathias</forename><surname>Öttl</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">Pattern Recognition Lab</orgName>
								<orgName type="institution">Friedrich-Alexander-Universität Erlangen-Nürnberg</orgName>
								<address>
									<settlement>Erlangen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Maja</forename><surname>Schlereth</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department Artificial Intelligence in Biomedical Engineering</orgName>
								<orgName type="institution">Friedrich-Alexander-Universität Erlangen-Nürnberg</orgName>
								<address>
									<settlement>Erlangen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chang</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">Pattern Recognition Lab</orgName>
								<orgName type="institution">Friedrich-Alexander-Universität Erlangen-Nürnberg</orgName>
								<address>
									<settlement>Erlangen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tobias</forename><surname>Heimann</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Digital Technology and Innovation</orgName>
								<orgName type="institution">Siemens Healthineers</orgName>
								<address>
									<settlement>Erlangen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marc</forename><surname>Aubreville</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Technische Hochschule Ingolstadt</orgName>
								<address>
									<settlement>Ingolstadt</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Katharina</forename><surname>Breininger</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department Artificial Intelligence in Biomedical Engineering</orgName>
								<orgName type="institution">Friedrich-Alexander-Universität Erlangen-Nürnberg</orgName>
								<address>
									<settlement>Erlangen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Adaptive Region Selection for Active Learning in Whole Slide Image Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">96F336A1439C52FFC39EBE547E8EB3CE</idno>
					<idno type="DOI">10.1007/978-3-031-43895-09.</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T10:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Active learning</term>
					<term>Region selection</term>
					<term>Whole slide images</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The process of annotating histological gigapixel-sized whole slide images (WSIs) at the pixel level for the purpose of training a supervised segmentation model is time-consuming. Region-based active learning (AL) involves training the model on a limited number of annotated image regions instead of requesting annotations of the entire images. These annotation regions are iteratively selected, with the goal of optimizing model performance while minimizing the annotated area. The standard method for region selection evaluates the informativeness of all square regions of a specified size and then selects a specific quantity of the most informative regions. We find that the efficiency of this method highly depends on the choice of AL step size (i.e., the combination of region size and the number of selected regions per WSI), and a suboptimal AL step size can result in redundant annotation requests or inflated computation costs. This paper introduces a novel technique for selecting annotation regions adaptively, mitigating the reliance on this AL hyperparameter. Specifically, we dynamically determine each region by first identifying an informative area and then detecting its optimal bounding box, as opposed to selecting regions of a uniform predefined shape and size as in the standard method. We evaluate our method using the task of breast cancer metastases segmentation on the public CAMELYON16 dataset and show that it consistently achieves higher sampling efficiency than the standard method across various AL step sizes. With only 2.6% of tissue area annotated, we achieve full annotation performance and thereby substantially reduce the costs of annotating a WSI dataset. The source code is available at https://github.com/ DeepMicroscopy/AdaptiveRegionSelection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Semantic segmentation on histological whole slide images (WSIs) allows precise detection of tumor boundaries, thereby facilitating the assessment of metastases <ref type="bibr" target="#b2">[3]</ref> and other related analytical procedures <ref type="bibr" target="#b16">[17]</ref>. However, pixel-level annotations of gigapixel-sized WSIs (e.g. 100, 000 × 100, 000 pixels) for training a segmentation model are difficult to acquire. For instance, in the CAMELYON16 breast cancer metastases dataset <ref type="bibr" target="#b9">[10]</ref>, 49.5% of WSIs contain metastases that are smaller than 1% of the tissue, requiring a high level of expertise and long inspection time to ensure exhaustive tumor localization; whereas other WSIs have large tumor lesions and require a substantial amount of annotation time for boundary delineation <ref type="bibr" target="#b17">[18]</ref>. Identifying potentially informative image regions (i.e., providing useful information for model training) allows requesting the minimum amount of annotations for model optimization, and a decrease in annotated area reduces both localization and delineation workloads. The challenge is to effectively select annotation regions in order to achieve full annotation performance with the least annotated area, resulting in high sampling efficiency.</p><p>We use region-based active learning (AL) <ref type="bibr" target="#b12">[13]</ref> to progressively identify annotation regions, based on iteratively updated segmentation models. Each region selection process consists of two steps. First, the prediction of the most recently trained segmentation model is converted to a priority map that reflects informativeness of each pixel. Existing studies on WSIs made extensive use of informativeness measures that quantify model uncertainty (e.g., least confidence <ref type="bibr" target="#b7">[8]</ref>, maximum entropy <ref type="bibr" target="#b4">[5]</ref> and highest disagreement between a set of models <ref type="bibr" target="#b18">[19]</ref>). The enhancement of priority maps, such as highlighting easy-to-label pixels <ref type="bibr" target="#b12">[13]</ref>, edge pixels <ref type="bibr" target="#b5">[6]</ref> or pixels with a low estimated segmentation quality <ref type="bibr" target="#b1">[2]</ref>, is also a popular area of research. Second, on the priority map, regions are selected according to a region selection method. Prior works have rarely looked into region selection methods; the majority followed the standard approach <ref type="bibr" target="#b12">[13]</ref> where a sliding window divides the priority map into fixed-sized square regions, the selection priority of each region is calculated as the cumulative informativeness of its constituent pixels, and a number of regions with the highest priorities are then selected. In some other works, only non-overlapping or sparsely overlapped regions were considered to be candidates <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b18">19]</ref>. Following that, some works used additional criteria to filter the selected regions, such as finding a representative subset <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b18">19]</ref>. All of these works selected square regions of a manually predefined size, disregarding the actual shape and size of informative areas.</p><p>This work focuses on region selection methods, a topic that has been largely neglected in literature until now, but which we show to have a great impact on AL sampling efficiency (i.e., the annotated area required to reach the full annotation performance). We discover that the sampling efficiency of the aforementioned standard method decreases as the AL step size (i.e., the annotated area at each AL cycle, determined by the multiplication of the region size and the number of selected regions per WSI) increases. To avoid extensive AL step size tuning, we propose an adaptive region selection method with reduced reliance on this AL hyperparameter. Specifically, our method dynamically determines an annotation region by first identifying an informative area with connected component detection and then detecting its bounding box. We test our method using a breast cancer metastases segmentation task on the public CAMELYON16 dataset and demonstrate that determining the selected regions individually provides greater flexibility and efficiency than selecting regions with a uniform predefined shape and size, given the variability in histological tissue structures. Results show that our method consistently outperforms the standard method by providing a higher sampling efficiency, while also being more robust to AL step size choices. Additionally, our method is especially beneficial for settings where a large AL step size is desirable due to annotator availability or computational restrictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Region-Based Active Learning for WSI Annotation</head><p>We are given an unlabeled pool U = {X 1 . . . X n }, where X i ∈ R Wi×Hi denotes the i th WSI with width W i and height H i . Initially, X i has no annotation; regions are iteratively selected from it and annotated across AL cycles. We denote the j th annotated rectangular region in</p><formula xml:id="formula_0">X i as R ij = (c ij x , c ij y , w ij , h ij )</formula><p>, where (c ij x , c ij y ) are the center coordinates of the region and w ij , h ij are the width and height of that region, respectively. In the standard region selection method, where fixedsize square regions are selected, w ij = h ij = l, ∀i, j, where l is predefined.</p><p>Figure <ref type="figure" target="#fig_0">1</ref> illustrates the workflow of region-based AL for WSI annotation. The goal is to iteratively select and annotate potentially informative regions from WSIs in U to enrich the labeled set L in order to effectively update the model g. To begin, k regions (each containing at least 10% of tissue) per WSI are randomly selected and annotated to generate the initial labeled set L. The model g is then trained on L and predicts on U to select k new regions from each WSI for the new round of annotation. The newly annotated regions are added to L for retraining g in the next AL cycle. The train-select-annotate process is repeated until a certain performance of g or annotation budget is reached.</p><p>The selection of k new regions from X i is performed in two steps based on the model prediction P i = g(X i ). First, P i is converted to a priority map M i using a per-pixel informativeness measure. Second, k regions are selected based on M i using a region selection method. The informativeness measure is not the focus of this study, we therefore adopt the most commonly used one that quantifies model uncertainty (details in Sect. 3.2). Next we describe the four region selection methods evaluated in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Region Selection Methods</head><p>Random. This is the baseline method where k regions of size l × l are randomly selected. Each region contains at least 10% of tissue and does not overlap with other regions. Standard <ref type="bibr" target="#b12">[13]</ref> M i is divided into overlapping regions of a fixed size l × l using a sliding window with a stride of 1 pixel. The selection priority of each region is calculated as the summed priority of the constituent pixels, and k regions with the highest priorities are then selected. Non-maximum suppression is used to avoid selecting overlapping regions. Standard (non-square) We implement a generalized version of the standard method that allows non-square region selections by including multiple region candidates centered at each pixel with various aspect ratios. To save computation and prevent extreme shapes, such as those with a width or height of only a few pixels, we specify a set of candidates as depicted in Fig. <ref type="figure" target="#fig_1">2</ref>. Specifically, we define a variable region width w as spanning from 1  2 l to l with a stride of 256 pixels and determine the corresponding region height as h = l 2 w . Adaptive (proposed) Our method allows for selecting regions with variable aspect ratios and sizes to accommodate histological tissue variability. The k regions are selected sequentially; when selecting the j th region R ij in X i , we first set the priorities of all pixels in previously selected regions (if any) to zero. We then find the highest priority pixel (c ij x , c ij y ) on M i ; a median filter with a kernel size of 3 is applied beforehand to remove outliers. Afterwards, we create a mask on M i with an intensity threshold of τ th percentile of intensities in M i , detect the connected component containing (c ij x , c ij y ), and select its bounding box. As depicted in Fig. <ref type="figure" target="#fig_2">3</ref>, τ is determined by performing a bisection search over [98, 100] th percentiles, such that the bounding box size is in range</p><formula xml:id="formula_1">[ 1 2 l × 1 2 l, 3 2 l × 3 2 l]</formula><p>. This size range is chosen to be comparable to the other three methods, which select regions of size l 2 . Note that Standard (non-square) can be understood as an ablation study of the proposed method Adaptive to examine the effect of variable region shape by maintaining constant region size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">WSI Semantic Segmentation Framework</head><p>This section describes the breast cancer metastases segmentation task we use for evaluating the AL region selection methods. The task is performed with patch-wise classification, where the WSI is partitioned into patches, each patch is classified as to whether it contains metastases, and the results are assembled.</p><p>Training. The patch classification model h(x, w) : R d×d -→ [0, 1] takes as input a patch x and outputs the probability p(y = 1|x, w) of containing metastases, where w denotes model parameters. Patches are extracted from the annotated regions at 40× magnification (0.25 µm px ) with d = 256 pixels. Following <ref type="bibr" target="#b10">[11]</ref>, a patch is labeled as positive if the center 128 × 128 pixels area contains at least  one metastasis pixel and negative otherwise. In each training epoch, 20 patches per WSI are extracted at random positions within the annotated area; for WSIs containing annotated metastases, positive and negative patches are extracted with equal probability. A patch with less than 1% tissue content is discarded. Data augmentation includes random flip, random rotation, and stain augmentation <ref type="bibr" target="#b11">[12]</ref>. Inference. X i is divided into a grid of uniformly spaced patches (40× magnification, d = 256 pixels) with a stride s. The patches are predicted using the trained patch classification model and the results are stitched to a probability map P i ∈ [0, 1] W i ×H i , where each pixel represents a patch prediction. The patch extraction stride s determines the size of P i (W i = Wi s , H i = Hi s ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset</head><p>We used the publicly available CAMELYON16 Challenge dataset <ref type="bibr" target="#b9">[10]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation Details</head><p>Training Schedules. We use MobileNet v2 <ref type="bibr" target="#b14">[15]</ref> initialized with ImageNet <ref type="bibr" target="#b13">[14]</ref> weights as the backbone of the patch classification model. It is extended with two fully-connected layers with sizes of 512 and 2, followed by a softmax activation layer. The model is trained for up to 500 epochs using cross-entropy loss and the Adam optimizer <ref type="bibr" target="#b6">[7]</ref>, and is stopped early if the validation loss stagnates for 100 consecutive epochs. Model selection is guided by the lowest validation loss. The learning rate is scheduled by the one cycle policy <ref type="bibr" target="#b15">[16]</ref> with a maximum of 0.0005. The batch size is 32. We used Fastai v1 <ref type="bibr" target="#b3">[4]</ref> for model training and testing. The running time of one AL cycle (select-train-test) on a single NVIDIA Geforce RTX3080 GPU (10GB) is around 7 h.</p><p>Active Learning Setups. Since the CAMELYON16 dataset is fully annotated, we perform AL by assuming all WSIs are unannotated and revealing the annotation of a region only after it is selected during the AL procedure. We divide the WSIs in U randomly into five stratified subsets of equal size and use them sequentially. In particular, regions are selected from WSIs in the first subset at the first AL cycle, from WSIs in the second subset at the second AL cycle, and so on. This is done because WSI inference is computationally expensive due to the large patch amount, reducing the number of predicted WSIs to one fifth helps to speed up AL cycles. We use an informativeness measure that prioritizes pixels with a predicted probability close to 0.5 (i.e., M i = 1-2|P i -0.5|), following <ref type="bibr" target="#b8">[9]</ref>. We annotate validation WSIs in the same way as the training WSIs via AL.</p><p>Evaluations. We use the CAMELYON16 challenge metric Free Response Operating Characteristic (FROC) score <ref type="bibr" target="#b0">[1]</ref> to validate the segmentation framework.</p><p>To evaluate the WSI segmentation performance directly, we use mean intersection over union (mIoU). For comparison, we follow <ref type="bibr" target="#b2">[3]</ref> to use a threshold of 0.5 to generate the binary segmentation map and report mIoU (Tumor), which is the average mIoU over the 48 test WSIs with metastases. We evaluate the model trained at each AL cycle to track performance change across the AL procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head><p>Full Annotation Performance. To validate our segmentation framework, we first train on the fully-annotated data (average performance of five repetitions reported). With a patch extraction stride s = 256 pixels, our framework yields an FROC score of 0.760 that is equivalent to the Challenge top 2, and an mIoU (Tumor) of 0.749, which is higher than the most comparable method in <ref type="bibr" target="#b2">[3]</ref> that achieved 0.741 with s = 128 pixels. With our framework, reducing s to 128 pixels improves both metastases identification and segmentation (FROC score: 0.779, mIoU (Tumor): 0.758). However, halving s results in a 4-fold increase in inference time. This makes an AL experiment, which involves multiple rounds of WSI inference, extremely costly. Therefore, we use s = 256 pixels for all following AL experiments to compromise between performance and computation costs. Because WSIs without metastases do not require pixel-level annotation, we exclude the 159 training and validation WSIs without metastases from all following AL experiments. This reduction leads to a slight decrease of full annotation performance (mIoU (Tumor) from 0.749 to 0.722).  All experiments (except for Random) use uncertainty sampling. When using region selection method Standard, the sampling efficiency advantage of uncertainty sampling over random sampling decreases as AL step size increases. A small AL step size minimizes the annotated tissue area for a certain high level of model performance, such as an mIoU (Tumor) of 0.7, yet requires a large number of AL cycles to achieve full annotation performance (Fig. <ref type="figure" target="#fig_4">4 (a-d</ref>)), Table <ref type="table">1</ref>. Annotated tissue area (%) required to achieve full annotation performance. The symbol "/" indicates that the full annotation performance is not achieved in the corresponding experimental setting in Fig. <ref type="figure" target="#fig_3">4</ref>.  resulting in high computation costs. A large AL step size allows for full annotation performance to be achieved in a small number of AL cycles, but at the expense of rapidly expanding the annotated tissue area (Fig. <ref type="figure" target="#fig_3">4</ref>(e), (f), (h) and (i)). Enabling selected regions to have variable aspect ratios does not substantially improve the sampling efficiency, with Standard (non-square) outperforming Standard only when the AL step size is excessively large (Fig. <ref type="figure" target="#fig_3">4</ref>(i)). However, allowing regions to be of variable size consistently improves sampling efficiency. Table <ref type="table">1</ref> shows that Adaptive achieves full annotation performance with fewer AL cycles than Standard for small AL step sizes and less annotated tissue area for large AL step sizes. As a result, when region selection method Adaptive is used, uncertainty sampling consistently outperforms random sampling. Furthermore, Fig. <ref type="figure" target="#fig_4">4(e-i</ref>)) shows that Adaptive effectively prevents the rapid expansion of annotated tissue area as AL step size increases, demonstrating greater robustness to AL step size choices than Standard. This is advantageous because extensive AL step size tuning to balance the annotation and computation costs can be avoided. This behavior can also be desirable in cases where frequent interaction with annotators is not possible or to reduce computation costs, because the proposed method is more tolerant to a large AL step size. We note in Fig. <ref type="figure" target="#fig_3">4</ref>(h) that the full annotation performance is not achieved with Adaptive within 15 AL cycles; in Fig. <ref type="figure" target="#fig_0">S1</ref> in the supplementary materials we show that allowing for oversampling of previously selected regions can be a solution to this problem. Additionally, we visualize examples of selected regions in Fig. <ref type="figure" target="#fig_5">5</ref> and show that Adaptive avoids two region selection issues of Standard : small, isolated informative areas are missed, and irrelevant pixels are selected due to the region shape and size restrictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison of Region Selection Methods</head><note type="other">.</note><formula xml:id="formula_2">k</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion and Conclusion</head><p>We presented a new AL region selection method to select annotation regions on WSIs. In contrast to the standard method that selects regions with predetermined shape and size, our method takes into account the intrinsic variability of histological tissue and dynamically determines the shape and size for each selected region. Experiments showed that it outperforms the standard method in terms of both sampling efficiency and the robustness to AL hyperparameters. Although the uncertainty map was used to demonstrate the efficacy of our approach, it can be seamlessly applied to any priority maps. A limitation of this study is that the annotation cost is estimated only based on the annotated area, while annotation effort may vary when annotating regions of equal size. Future work will involve the development of a WSI dataset with comprehensive documentation of annotation time to evaluate the proposed method and an investigation of potential combination with self-supervised learning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Region-based AL workflow for selecting annotation regions. The exemplary selected regions are of size 8192 × 8192 pixels. (Image resolution: 0.25 µm px )</figDesc><graphic coords="3,44,79,54,53,334,57,71,17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Standard (nonsquare): Region candidates for l = 8192 pixels.</figDesc><graphic coords="5,154,89,60,53,221,95,91,18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Adaptive: (a) Priority map Mi and the highest priority pixel (arrow). (b-c) Bisection search of τ : (b) τ = 99 th , (c) τ = 98.5 th .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig.4. mIoU (Tumor) as a function of annotated tissue area (%) for four region selection methods across various AL step sizes. Results show average and min/max (shaded) performance over three repetitions with distinct initial labeled sets. The final annotated tissue area of Random can be less than Standard as it stops sampling a WSI if no region contains more than 10% of tissue. Curves of Adaptive are interpolated as the annotated area differs between repetitions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4</head><label>4</label><figDesc>compares the sampling efficiency of the four region selection methods across various AL step sizes (i.e., the combinations of region size l ∈ {4096, 8192, 12288} pixels and the number of selected regions per WSI k ∈ {1, 3, 5}). Experiments with large AL step sizes perform 10 AL cycles (Fig.4(e), (f), (h) and (i)); others perform 15 AL cycles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Visualization of five regions selected with three region selection methods, applied to an exemplary priority map produced in a second AL cycle (regions were randomly selected in the first AL cycle, k = 5, l = 4096 pixels). Region sizes increase from top to bottom: l ∈ {4096, 8192, 12288} pixels. Fully-annotated tumor metastases overlaid with WSI in red. (Color figure online)</figDesc><graphic coords="8,57,48,194,51,337,30,242,44" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Test 114 is excluded due to non-exhaustive annotation, as stated by data provider.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. We thank <rs type="person">Yixing Huang</rs> and <rs type="person">Zhaoya Pan</rs> (<rs type="affiliation">FAU</rs>) for their feedback on the manuscript. We gratefully acknowledge support by d.hip campus -Bavarian aim (J.Q. and K.B.) as well as the scientific support and HPC resources provided by the <rs type="funder">Erlangen National High Performance Computing Center (NHR@FAU) of the Friedrich-Alexander-Universität Erlangen-Nürnberg (FAU)</rs>. The hardware is funded by the <rs type="funder">German Research Foundation (DFG)</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Diagnostic assessment of deep learning algorithms for detection of lymph node metastases in women with breast cancer</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Bejnordi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAMA</title>
		<imprint>
			<biblScope unit="volume">318</biblScope>
			<biblScope unit="issue">22</biblScope>
			<biblScope unit="page" from="2199" to="2210" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Metabox+: a new region based active learning method for semantic segmentation using priority maps</title>
		<author>
			<persName><forename type="first">P</forename><surname>Colling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Roese-Koerner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gottschalk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rottmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.01884</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A fast and refined cancer regions segmentation framework in wholeslide breast pathological images</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fastai: a layered API for deep learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gugger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">108</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Reducing the annotation cost of whole slide histology images using active learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 3rd International Conference on Image Processing and Machine Vision (IPMV)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="47" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Region-based active learning for efficient labeling in semantic segmentation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kasarla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Nagendar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Hegde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Balasubramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1109" to="1117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: a method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Joint semi-supervised and active learning for segmentation of gigapixel pathology images with cost-effective labeling</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">N</forename><surname>Dugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">N</forename><surname>Chuah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="591" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A sequential algorithm for training text classifiers</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Gale</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-1-4471-2099-5_1</idno>
		<ptr target="https://doi.org/10.1007/978-1-4471-2099-51" />
	</analytic>
	<monogr>
		<title level="m">SIGIR 1994</title>
		<editor>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Croft</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Van Rijsbergen</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="3" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">1399 H&amp;E-stained sentinel lymph node sections of breast cancer patients: the CAMELYON dataset</title>
		<author>
			<persName><forename type="first">G</forename><surname>Litjens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ehteshami Bejnordi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Geessink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Balkenhol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bult</surname></persName>
		</author>
		<ptr target="http://gigadb.org/dataset/100439" />
	</analytic>
	<monogr>
		<title level="j">GigaScience</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">65</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.02442</idno>
		<title level="m">Detecting cancer metastases on gigapixel pathology images</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A method for normalizing histology slides for quantitative analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Macenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Niethammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Marron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Biomedical Imaging: From Nano to Macro</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="1107" to="1110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Cereals-cost-effective region-based active learning for semantic segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mackowiak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ghori</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.09726</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mobilenetv 2: inverted residuals and linear bottlenecks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">A disciplined approach to neural network hyper-parameters: part 1-learning rate, batch size, momentum, and weight decay</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">N</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.09820</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pan-tumor canine cutaneous cancer histology (CATCH)</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wilm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">dataset. Sci. Data</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Clinical-realistic annotation for histopathology images with probabilistic semi-supervision: a worst-case study</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16434-7_8</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16434-78" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13432</biblScope>
			<biblScope unit="page" from="77" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Suggestive annotation: a deep active learning framework for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Z</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-66179-7_46</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-66179-7" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2017</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Descoteaux</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Maier-Hein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Franz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Jannin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Collins</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Duchesne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">10435</biblScope>
			<biblScope unit="page">46</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
