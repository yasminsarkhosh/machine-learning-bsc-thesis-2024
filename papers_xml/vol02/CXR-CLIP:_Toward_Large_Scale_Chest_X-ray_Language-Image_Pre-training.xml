<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CXR-CLIP: Toward Large Scale Chest X-ray Language-Image Pre-training</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kihyun</forename><surname>You</surname></persName>
							<idno type="ORCID">0000-0002-8490-9398</idno>
							<affiliation key="aff0">
								<address>
									<settlement>Kakaobrain, Seongnam</settlement>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jawook</forename><surname>Gu</surname></persName>
							<email>jawook.gu@kakaobrain.com</email>
							<idno type="ORCID">0000-0002-9291-7929</idno>
							<affiliation key="aff0">
								<address>
									<settlement>Kakaobrain, Seongnam</settlement>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiyeon</forename><surname>Ham</surname></persName>
							<email>jiyeon.ham@kakaobrain.com</email>
							<affiliation key="aff0">
								<address>
									<settlement>Kakaobrain, Seongnam</settlement>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Beomhee</forename><surname>Park</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Kakaobrain, Seongnam</settlement>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiho</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Kakaobrain, Seongnam</settlement>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Eun</forename><forename type="middle">K</forename><surname>Hong</surname></persName>
							<idno type="ORCID">0000-0002-5440-0451</idno>
							<affiliation key="aff0">
								<address>
									<settlement>Kakaobrain, Seongnam</settlement>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Woonhyuk</forename><surname>Baek</surname></persName>
							<email>wbaek@kakaobrain.com</email>
							<affiliation key="aff0">
								<address>
									<settlement>Kakaobrain, Seongnam</settlement>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Byungseok</forename><surname>Roh</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Kakaobrain, Seongnam</settlement>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CXR-CLIP: Toward Large Scale Chest X-ray Language-Image Pre-training</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="101" to="111"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">5D333586F26E11DC7FEF3D576724621B</idno>
					<idno type="DOI">10.1007/978-3-031-43895-0_10</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T10:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Chest X-ray</term>
					<term>Vision-Language Pre-training</term>
					<term>Contrastive Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A large-scale image-text pair dataset has greatly contributed to the development of vision-language pre-training (VLP) models, which enable zero-shot or few-shot classification without costly annotation. However, in the medical domain, the scarcity of data remains a significant challenge for developing a powerful VLP model. In this paper, we tackle the lack of image-text data in chest X-ray by expanding image-label pair as image-text pair via general prompt and utilizing multiple images and multiple sections in a radiologic report. We also design two contrastive losses, named ICL and TCL, for learning study-level characteristics of medical images and reports, respectively. Our model outperforms the state-of-the-art models trained under the same conditions. Also, enlarged dataset improve the discriminative power of our pre-trained model for classification, while sacrificing marginal retrieval performance. Code is available at https://github.com/kakaobrain/cxr-clip.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Chest X-ray (CXR) plays a vital role in screening and diagnosis of thoracic diseases <ref type="bibr" target="#b19">[19]</ref>. The effectiveness of deep-learning based computer-aided diagnosis has been demonstrated in disease detection <ref type="bibr" target="#b21">[21]</ref>. However, one of the major challenges in training deep learning models for medical purposes is the need for extensive, high-quality clinical annotation, which is time-consuming and costly.</p><p>Recently, CLIP <ref type="bibr" target="#b22">[22]</ref> and ALIGN <ref type="bibr" target="#b10">[10]</ref> have shown the ability to perform vision tasks without any supervision. However, vision-language pre-training (VLP) in the CXR domain still lacks sufficient image-text datasets because many public datasets consist of image-label pairs with different class compositions. Med-CLIP <ref type="bibr" target="#b26">[26]</ref> attempted to a rule-based labler to use both image-text data and image-label data. However, it relies on the performance of the rule-based labeler and is not scalable to other diseases that the labeler cannot address.</p><p>In this paper, we propose a training method, CXR-CLIP, that integrates image-text data with image-label data using class-specific prompts made by radiologists. Our method does not depend on a rule-based labeler and can be applied to any image-label data. Also, inspired by DeCLIP <ref type="bibr" target="#b13">[13]</ref>, we used Multi-View Supervision (MVS) utilizing multiple images and texts in a CXR study to make more image-text pairs for efficient learning. In addition, we introduce two contrastive loss functions, named image contrastive loss (ICL) and text contrastive loss (TCL), to learn study-level characteristics of the CXR images and reports respectively.</p><p>The main contributions of this paper are summarized as follows. 1) We tackle the lack of data for VLP in CXR by generating image-text pairs from image-label datasets using prompt templates designed by radiologists and utilizing multiple images and texts in a study. 2) Two additional contrastive losses are introduced to learn discriminate features of image and text, improving image-text retrieval performances. 3) Performance of our model is validated on diverse datasets with zero-shot and few-shot settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Data Efficient VLP. Recent studies <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b17">17]</ref> have proposed data-efficient VLP via joint learning with self-supervision. DeCLIP <ref type="bibr" target="#b13">[13]</ref> suggested MVS that utilizes image and text augmentation to leverage positive pairs along with other selfsupervisions. In CXR domain, GloRIA <ref type="bibr" target="#b6">[7]</ref> aligned words in reports and subregions in an image for label efficiency, and BioVIL <ref type="bibr" target="#b1">[2]</ref> combined self-supervision for label efficiency. We modify MVS as two distinct images and texts from a study and present self-supervised loss functions, ICL and TCL for efficient learning.</p><p>Self-supervision Within CXR Study. A CXR study could include several images in different views and two report sections: 'findings' and 'impression'. The impression section includes the differential diagnosis inferred from the findings section. BioVIL <ref type="bibr" target="#b1">[2]</ref> enhanced the text encoder by matching two sections during language pre-training. MedAug <ref type="bibr" target="#b24">[24]</ref> shows that self-supervised learning by matching images in a study is better than differently augmented images. We utilize both of multiple images and texts from a single study in VLP in an end-to-end fashion.</p><p>Leveraging Image-Label Data in VLP. MedCLIP <ref type="bibr" target="#b26">[26]</ref> integrated unpaired images, texts, and labels using rule-based labeler <ref type="bibr" target="#b8">[8]</ref>, which is less capable of retrieving the exact report for a given image due to the effect of decoupling image-text pairs. UniCL <ref type="bibr" target="#b28">[28]</ref> suggested using prompts to leverage image-label dataset <ref type="bibr" target="#b3">[4]</ref>, considering the samples from the same label to be a positive pair. To our knowledge, this is the first work to utilize prompting for training in CXR domain. For the image-label data, two different prompts are generated from class labels as (t 1 , t 2 ). Using sampled pairs, the encoders are trained with three kinds of contrastive losses (MVS, ICL, and TCL).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>CXR-CLIP samples image-text pairs from not only image-text data but also image-label data, and learns study-level characteristics with two images and two texts per study. The overview of the proposed method is illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Sampling</head><p>We define a CXR study as s = {X, T }, where X is a set of images, and T is a set of "findings" and "impression" sections. The study of image-label dataset has a set of image labels Y instead of T . For the image-label dataset, we make promptbased texts T = Concat({p ∼ P (y)} y∈Y ), where p is a sampled prompt sentence, P (y) is a set of prompts given the class name and value y, and Concat(•) means concatenating texts. The set of prompts is used to generate sentences such as actual clinical reports, taking into account class labels and their values (positive, negative, etc.), unlike the previous prompt <ref type="bibr" target="#b6">[7]</ref> for evaluation which randomly combines a level of severity, location, and sub-type of disease. Our prompts are available in Appendix.</p><p>We sample two images (x 1 , x 2 ) in X if there are multiple images. Otherwise, we use augmented image A i (x 1 ) as x 2 , where A i is image augmentation. To leverage various information from different views in CXR (AP, PA, or lateral), we sample images from two distinct views as possible. Similarly, we sample two texts (t 1 , t 2 ) in T if there are both "findings" and "impression". Otherwise, we use augmented text A t (t 1 ) as t 2 , where A t is text augmentation. For the imagelabel data, we sample two prompt sentences as t 1 and t 2 from the constructed T = Concat({p ∼ P (y)} y∈Y ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model Architecture</head><p>We construct image encoder E i and text encoder E t to obtain global representations of image and text, and a projection layer f i and f t to match the size of final embedding vectors.</p><p>Image Encoder. We have tested two different image encoders; ResNet-50 <ref type="bibr" target="#b5">[6]</ref> and Swin-Tiny <ref type="bibr" target="#b14">[14]</ref> as follow <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b26">26]</ref>. We extract global visual features from the global average pooled output of the image encoder. A linear layer is adopted to project the embeddings into the same size as text embeddings. The normalized visual embedding v is obtained by v = f i (E i (x)) / ||f i (E i (x))||. We denote a batch of the visual embeddings as V = {v} n i=1 , where n is a batch size. Text Encoder. We use BioClinicalBERT <ref type="bibr" target="#b0">[1]</ref> model, which is the same architecture as BERT <ref type="bibr" target="#b4">[5]</ref> but pre-trained with medical texts <ref type="bibr" target="#b11">[11]</ref> as follow <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b26">26]</ref>. We use [EOS] token's final output as the global textual representation. Also, a linear projection layer is adopted the same as the image encoder. The normalized text embedding u is denoted as u = f t (E t (t)) / ||f t (E t (t))||. We denote a batch of the text embedding as U = {u} n i=1 and (v i , u i ) are paired.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Loss Function</head><p>In this section, we first describe CLIP loss <ref type="bibr" target="#b22">[22]</ref> and then describe our losses (MVS, ICL, TCL) in terms of CLIP loss. The goal of CLIP loss is to pull image embedding and corresponding text embedding closer and to push unpaired image and text farther in the embedding space. The InfoNCE loss is generally adopted as a type of contrastive loss, and CLIP uses the average of two InfoNCE losses; image-to-text and text-to-image. The formula for CLIP loss is given by</p><formula xml:id="formula_0">LCLIP (U, V ) = - 1 2n ( u i ∈U log exp(v T i ui/τ ) v j ∈V exp(u T i vj/τ ) + v i ∈V log exp(u T i vi/τ ) u j ∈U exp(v T i uj/τ ) ),</formula><p>(1) where τ is a learnable temperature to scale logits.</p><p>In DeCLIP <ref type="bibr" target="#b13">[13]</ref>, MVS uses four L CLIP loss with all possible pairs augmented views; (x, t), (x, A t (t)), (A i (x), t) and (A i (x), A t (t)). We modify DeCLIP's MVS to fit the CXR domain by the composition of the second example. DeCLIP only utilizes an augmented view of the original sample, but we sample a pair of the second image and text as described in Sect. 3.1. We denote the first and the second sets of image embeddings as U 1 , U 2 , and text embeddings as</p><formula xml:id="formula_1">V 1 , V 2 . L MV S = 1 4 (L CLIP (U 1 , V 1 )+L CLIP (U 2 , V 1 )+L CLIP (U 1 , V 2 )+L CLIP (U 2 , V 2 ))</formula><p>(2) The goal of ICL and TCL is to learn modality-specific characteristics in terms of image and text respectively. We design ICL and TCL as same as CLIP loss, but the input embeddings are different. ICL only uses image embeddings;</p><formula xml:id="formula_2">L ICL = L CLIP (V 1 , V 2 ) and TCL only uses text embeddings; L T CL = L CLIP (U 1 , U 2 ).</formula><p>ICL pulls image embeddings from the same study and pushes image embeddings from the different studies, so that, the image encoder can learn study-level diversity. Similarly, TCL pulls embeddings of "findings" and "impression" in the same study or diverse expressions of prompts from the same label and pushes the other studies' text embeddings, so that the text encoder can match diverse clinical expressions on the same diagnosis. Thereby, the final training objective consists of three contrastive losses balanced each component by λ I and λ T , formulated by</p><formula xml:id="formula_3">L = L MV S + λ I L ICL + λ T L T CL .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We used three pre-trained datasets and tested with various external datasets to test the generalizability of models. The statistics of the datasets used are summarized in Table <ref type="table" target="#tab_0">1</ref>.</p><p>MIMIC-CXR <ref type="bibr" target="#b12">[12]</ref> consists of CXR studies, each with one or more images and free-form reports. We extracted "findings" and "impression" from the reports. We used the training split for pre-training and the test split for image-to-text retrieval.</p><p>CheXpert <ref type="bibr" target="#b8">[8]</ref> is an image-label data with 14 classes, obtained from the impression section by its rule-based labeler, and each class is labeled as positive, Note that only the reports of CheXpert5x200 are publicly available, but the reports of CheXpert are not. Following the previous works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b26">26]</ref>, we excluded CheXpert5x200 from the training set and used it for test. ChestX-ray14 <ref type="bibr" target="#b25">[25]</ref> consists of frontal images with binary labels for 14 diseases. Prompts are generated by sampling 3 negative classes per study. We used 20% of the original training set for validation, and the remaining 80% for pretraining.</p><p>RSNA pneumonia <ref type="bibr" target="#b23">[23]</ref> is binary-labeled data as pneumonia or normal. We split train/valid/test set 70%, 15%, 15% of the dataset following <ref type="bibr" target="#b6">[7]</ref> for the external classification task.</p><p>SIIM Pneumothorax<ref type="foot" target="#foot_0">1</ref> is also binary labeled as pneumothorax or normal. We split the train/valid/test set same ratio as RSNA pneumonia following <ref type="bibr" target="#b6">[7]</ref> and used it for the classification task.</p><p>VinDR-CXR <ref type="bibr" target="#b18">[18]</ref> contains 22 local labels and 6 global labels of disease, which were obtained by experienced radiologists. We split the validation set from the original training set. Of 28 classes, "other diseases" and "other lesions" classes were excluded. Then, only 18 classes having 10 or more samples within the test set were evaluated for the binary classification of each class as follow <ref type="bibr" target="#b9">[9]</ref>.</p><p>Open-I [3] is an image-text dataset. From each study, one of the report sections and one frontal-view image were sampled and used for image-to-text retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>We used augmentations A i and A t to fit medical images and reports. For A i , we resize and crop with scale [0.8, 1.1], randomly adapt CLAHE <ref type="bibr" target="#b20">[20]</ref>, and random color jittering; brightness, hue ratios from [0.9, 1.1] and contrast, saturation [0.8, 1.2]. For A t , to preserve clinical meaning, sentence swap and back-translation<ref type="foot" target="#foot_1">2</ref> from Italian to English is used. The image size and final-embedding size are set to 224 and 512 respectively as in previous work <ref type="bibr" target="#b26">[26]</ref>. We set λ I and λ T to 1.0, 0.5 for balancing total loss. Two encoders were trained for 15 epochs in a mixed-precision manner, early stopped by validation loss, and optimized by AdamW <ref type="bibr" target="#b16">[16]</ref> with an initial learning rate 5e-5 and a weight decay 1e-4. We used cosine-annealing learning-rate scheduler <ref type="bibr" target="#b15">[15]</ref> with warm-up for 1 epoch. A training batch consists of 128 studies with 256 image-text pairs. We implemented all experiments on PyTorch with 4 NVIDIA V100 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with State-of-the-Arts</head><p>Zero-Shot and Few-Shot Classification. Table <ref type="table" target="#tab_1">2</ref> shows performance on classification tasks of our models and state-of-the-art models. To evaluate zero- Table <ref type="table">3</ref>. Comparison with state-of-the-arts for image-to-text retrieval. The notations of datasets and models are same to Table <ref type="table" target="#tab_1">2</ref>.</p><p>Model Name Pre-Train Dataset CheXpert5x200 MIMIC-CXR Open-I Total RSUM R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 shot classification fairly, we used evaluation prompts suggested from previous works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b9">9]</ref>. The evaluation prompts are available in Appendix. We evaluate binary classification computed by Area Under ROC (AUC) and multi-class classification computed by accuracy (ACC). Our ResNet model trained with MIMIC-CXR outperforms GloRIA <ref type="bibr" target="#b6">[7]</ref> except for CheXpert5x200, as GloRIA trained with image-text pair in CheXpert. Our SwinTiny model trained with MIMIC-CXR and CheXpert outperforms MedCLIP <ref type="bibr" target="#b26">[26]</ref>, which is the same architecture trained with the same datasets, in most of the metrics. Adding more pre-training datasets by prompting image-label datasets tends to improve performance for classifications, while the SwinTiny CXR-CLIP pre-trained with three datasets, performs the best for most of the metrics. More comparison with self-supervised models is available in Appendix.</p><p>Image-to-Text Retrieval. We evaluated image-to-text retrieval computed by R@K, the recall of the exact report in the top K retrieved reports for a given image. (Table <ref type="table">3</ref>) While GloRIA <ref type="bibr" target="#b6">[7]</ref> uses image-text pairs in CheXpert(C*) which is not available in public, CXR-CLIP uses image-text in MIMIC-CXR. So we adapt an external image-text dataset Open-I <ref type="bibr" target="#b2">[3]</ref> for a fair comparison. GloRIA Table <ref type="table">4</ref>. Ablations and comparison with CLIP <ref type="bibr" target="#b22">[22]</ref> and DeCLIP <ref type="bibr" target="#b13">[13]</ref>. Our augmentations effectively preserves clinical meaning than EDA. Our full methodology (CXR-CLIP) outperforms DeCLIP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>CheXpert 5x200 MIMIC-CXR Total RSUM ACC R@1 R@5 R@10 R@1 R@5 R@10 Vanila CLIP 58.9 4. <ref type="bibr" target="#b3">4</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablations</head><p>For the ablation study, models with ResNet-50 <ref type="bibr" target="#b5">[6]</ref> backbone were trained on MIMIC-CXR and CheXpert datasets and tested on zero-shot classification and image-to-text retrieval tasks with MIMIC-CXR and CheXpert5x200 datasets. We conducted two ablations shown in Table <ref type="table">4</ref>. First, we analyzed the effect of each component of CXR-CLIP by adding the components to vanilla CLIP <ref type="bibr" target="#b22">[22]</ref> one by one. To validate our data sampling closer, we divided the sampling method into three parts 1) study-level sampling 2) data augmentations 3) Multi-view and Multi-text sampling (MVS). Our study-level sampling strategy improves performance compared to vanilla CLIP, which uses a naive sampling method bringing an image and corresponding report. Additionally, the modified data augmentation to fit the CXR domain contributes to performance increment of classification, the similar performance on retrieval. MVS slightly improves performances in both classification and image-text retrieval. Adding more supervision (ICL and TCL) improves performance by utilizing better multi-views and multi-text inputs. However, TCL drops the performance of recalls in CheXpert5x200, TCL could be hard to optimize variation of the radiologic report and prompt not diverse as images.</p><p>In the second ablation study, CXR-CLIP was compared to DeCLIP <ref type="bibr" target="#b13">[13]</ref> to confirm that our MVS using two image-text pairs per study is better than the MVS of DeCLIP which uses naively augmented images and texts. We show that our text augmentation outperforms DeCLIP's text augmentation named EDA <ref type="bibr" target="#b27">[27]</ref> in terms of image-to-text recall, which implies our text augmentation preserves clinical meaning. The superiority of our MVS over DeCLIP's MVS confirms that using multiple images and texts from one study is better than using images and texts from augmented examples. Also, our full methodology (CXR-CLIP) outperforms DeCLIP, suggesting that our method efficiently learns in the CXR domain more than DeCLIP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We presented a framework enlarging training image-text pair by using imagelabel datasets as image-text pair with prompts and utilizing multiple images and report sections in a study. Adding image-label datasets achieved performance gain in classification tasks including zero-shot and few-shot settings, on the other hand, lost the performance of retrieval tasks. We also proposed loss functions ICL and TCL to enhance the discriminating power within each modality, which effectively increases image-text retrieval performance. Our additional loss functions are designed to efficiently learn CXR domain knowledge along with image-text contrastive learning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overview of the proposed method with a training batch sampling n studies, where each study has a pair of images (x 1 , x 2 ) and a pair of text (t 1 , t 2 ). If a study has one image or one text, data augmentation is conducted to make second examples.For the image-label data, two different prompts are generated from class labels as (t 1 , t 2 ). Using sampled pairs, the encoders are trained with three kinds of contrastive losses (MVS, ICL, and TCL).</figDesc><graphic coords="3,56,46,53,81,339,49,297,91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The number of studies for each dataset and split in this paper , or none (not mentioned). We used the training split for pretraining with class-specific prompts. CheXpert5x200 is a subset of CheXpert for 5-way classification, which has 200 exclusively positive images for each class.</figDesc><table><row><cell cols="2">Data Split Pre-training</cell><cell></cell><cell></cell><cell cols="2">Evaluation</cell></row><row><cell></cell><cell cols="5">MIMIC-CXR CheXpert ChestX-ray14 VinDR RSNA SIIM Open-I</cell></row><row><cell>Train</cell><cell>222,628</cell><cell>216,478</cell><cell>89,696</cell><cell cols="2">12,000 18,678 8,422</cell></row><row><cell>Valid</cell><cell>1,808</cell><cell>233</cell><cell>22,423</cell><cell>3,000</cell><cell>4,003 1,808</cell></row><row><cell>Test</cell><cell>3,264</cell><cell>1,000</cell><cell></cell><cell>3,000</cell><cell>4,003 1,807 3,788</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison with state-of-the-art for zero-shot(ZS) or few-shot(10%) classification tasks. M, C, and C14 mean MIMIC-CXR, CheXpert, and ChestX-ray14, respectively. C * means CheXpert with reports, which are not publicly available. ResNet50 (R50) and SwinTiny (SwinT ) mean the image encoder used for each model.</figDesc><table><row><cell>Model Name</cell><cell cols="3">Pre-train Dataset VinDR-CXR</cell><cell cols="2">RSNA</cell><cell>SIIM</cell><cell>C5x200</cell></row><row><cell></cell><cell></cell><cell>ZS</cell><cell cols="2">10% 100% ZS</cell><cell>10% 100% ZS</cell><cell>10% 100% ZS-ACC</cell></row><row><cell>GloRIAR50</cell><cell>C*</cell><cell cols="4">78.0 73.0 73.1 80.6 88.2 88.5 84.0 91.5 91.9 62.4  *</cell></row><row><cell>CXR-CLIPR50</cell><cell>M</cell><cell cols="4">78.8 82.1 82.2 83.3 88.5 89.2 85.2 88.3 90.5 56.2</cell></row><row><cell cols="2">CXR-CLIPSwinT M</cell><cell cols="4">78.3 84.9 85.4 81.3 88.0 88.4 85.5 86.9 88.3 54.3</cell></row><row><cell>MedCLIPSwinT</cell><cell>M,C</cell><cell cols="4">82.4 84.9 85.1 81.9 88.9 89.0 89.0 90.4 90.8 59.2</cell></row><row><cell>CXR-CLIPR50</cell><cell>M,C</cell><cell cols="4">83.0 81.4 82.1 81.7 88.5 88.9 86.4 88.4 90.7 61.7</cell></row><row><cell cols="2">CXR-CLIPSwinT M,C</cell><cell cols="4">82.7 86.1 86.7 84.5 88.1 88.8 87.9 89.6 91.2 60.1</cell></row><row><cell>CXR-CLIPR50</cell><cell>M,C,C14</cell><cell cols="4">78.1 80.2 81.0 81.8 88.7 89.3 85.2 91.5 92.8 60.3</cell></row><row><cell cols="2">CXR-CLIPSwinT M,C,C14</cell><cell cols="4">78.9 88.0 89.0 80.1 89.2 89.8 91.4 92.9 94.0 62.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>has the best performance on CheXpert but our model trained with MIMIC-CXR, which has similar amounts of studies to CheXpert, outperforms on Open-I. MedCLIP almost lost the ability to retrieve image-text due to decoupling pairs of image and text during pre-training. In CXR-CLIP, adding more image-label datasets such as CheXpert and ChestX-ray14 degrades the image-text retrieval performance, possibly because the contribution of the text in original reports was diluted.</figDesc><table><row><cell></cell><cell>14.4 22.6 17.3 41.2 52.6 152.5</cell></row><row><cell>+ Study Level Sampling</cell><cell>58.7 4.6 15.1 23.2 17.8 42.5 54.2 157.4</cell></row><row><cell>+ Augmentations</cell><cell>60.6 5.7 17.0 24.9 16.1 40.2 51.5 155.4</cell></row><row><cell>+ MVS</cell><cell>61.2 5.4 17.1 24.7 16.3 40.6 53.3 157.4</cell></row><row><cell>+ ICL</cell><cell>61.6 6.8 20.3 28.6 17.5 41.6 53.2 168.0</cell></row><row><cell>+ TCL (CXR-CLIP)</cell><cell>61.7 6.2 18.2 29.1 19.6 44.8 56.6 174.5</cell></row><row><cell>MVS of DeCLIP (EDA)</cell><cell>59.5 3.2 15.5 22.9 15.8 39.1 51.5 148.0</cell></row><row><cell cols="2">MVS of DeCLIP (Our aug) 59.4 6.0 17.0 24.4 15.1 38.8 51.8 153.1</cell></row><row><cell>DeCLIP (Our aug)</cell><cell>59.4 5.7 16.1 24.6 18.1 44.0 55.3 163.8</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://siim.org/page/pneumothorax_challenge.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://huggingface.co/Helsinki-NLP.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0_10.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Publicly available clinical BERT embeddings</title>
		<author>
			<persName><forename type="first">E</forename><surname>Alsentzer</surname></persName>
		</author>
		<idno>CoRR abs/1904.03323</idno>
		<ptr target="http://arxiv.org/abs/1904.03323" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Making the most of text semantics to improve biomedical vision-language processing</title>
		<author>
			<persName><forename type="first">B</forename><surname>Boecking</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-20059-5_1</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-20059-5_1" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2022</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Brostow</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Cissé</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13696</biblScope>
			<biblScope unit="page" from="1" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Preparing a collection of radiology examinations for distribution and retrieval</title>
		<author>
			<persName><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Med. Inform. Assoc</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="304" to="310" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Imagenet: a large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>CoRR abs/1810.04805</idno>
		<ptr target="http://arxiv.org/abs/1810.04805" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>CoRR abs/1512.03385</idno>
		<ptr target="http://arxiv.org/abs/1512.03385" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Gloria: a multimodal global-local representation learning framework for label-efficient medical image recognition</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Lungren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3942" to="3951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">K</forename><surname>You</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Chexpert: a large chest radiograph dataset with uncertainty labels and expert comparison</title>
		<author>
			<persName><forename type="first">J</forename><surname>Irvin</surname></persName>
		</author>
		<idno>CoRR abs/1901.07031</idno>
		<ptr target="http://arxiv.org/abs/1901.07031" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Significantly improving zero-shot X-ray pathology classification via fine-tuning pre-trained image-text encoders</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kyung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Choi</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2212.07050" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Scaling up visual and vision-language representation learning with noisy text supervision</title>
		<author>
			<persName><forename type="first">C</forename><surname>Jia</surname></persName>
		</author>
		<idno>CoRR abs/2102.05918</idno>
		<ptr target="https://arxiv.org/abs/2102.05918" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mark</surname></persName>
		</author>
		<title level="m">MIMIC-III clinical database</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E W</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Berkowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Horng</surname></persName>
		</author>
		<title level="m">The MIMIC-CXR database</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Supervision exists everywhere: a data efficient contrastive languageimage pre-training paradigm</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<idno>CoRR abs/2110.05208</idno>
		<ptr target="https://arxiv.org/abs/2110.05208" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Swin transformer: hierarchical vision transformer using shifted windows</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<idno>CoRR abs/2103.14030</idno>
		<ptr target="https://arxiv.org/abs/2103.14030" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">SGDR: stochastic gradient descent with restarts</title>
		<author>
			<persName><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno>CoRR abs/1608.03983</idno>
		<ptr target="http://arxiv.org/abs/1608.03983" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Fixing weight decay regularization in adam</title>
		<author>
			<persName><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno>CoRR abs/1711.05101</idno>
		<ptr target="http://arxiv.org/abs/1711.05101" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">SLIP: self-supervision meets languageimage pre-training</title>
		<author>
			<persName><forename type="first">N</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<idno>CoRR abs/2112.12750</idno>
		<ptr target="https://arxiv.org/abs/2112.12750" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">VinDr-CXR: an open dataset of chest X-rays with radiologist&apos;s annotations</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41597-022-01498-w</idno>
		<ptr target="https://doi.org/10.1038/s41597-022-01498-w" />
	</analytic>
	<monogr>
		<title level="j">Sci. Data</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">429</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m">Communicating radiation risks in paediatric imaging: information to support health care discussions about benefit and risk</title>
		<imprint>
			<publisher>World Health Organization</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Contrast limited adaptive histogram equalization image processing to improve the detection of simulated spiculations in dense mammograms</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Pisano</surname></persName>
		</author>
		<idno type="DOI">10.1007/BF03178082</idno>
		<ptr target="https://doi.org/10.1007/BF03178082" />
	</analytic>
	<monogr>
		<title level="j">J. Digit. Imaging</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">193</biblScope>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Computer-aided detection in chest radiography based on artificial intelligence: a survey</title>
		<author>
			<persName><forename type="first">C</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<idno type="DOI">10.1186/s12938-018-0544-y</idno>
		<ptr target="https://doi.org/10.1186/s12938-018-0544-y" />
	</analytic>
	<monogr>
		<title level="j">Biomed. Eng. Online</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">113</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<idno>CoRR abs/2103.00020</idno>
		<ptr target="https://arxiv.org/abs/2103.00020" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Augmenting the national institutes of health chest radiograph dataset with expert annotations of possible pneumonia</title>
		<author>
			<persName><forename type="first">G</forename><surname>Shih</surname></persName>
		</author>
		<idno type="DOI">10.1148/ryai.2019180041</idno>
		<ptr target="https://doi.org/10.1148/ryai.2019180041" />
	</analytic>
	<monogr>
		<title level="j">Radiol. Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">180041</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Medaug: contrastive learning leveraging patient metadata improves representations for chest x-ray interpretation</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N T</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Balachandar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="https://proceedings.mlr.press/v149/vu21a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Machine Learning for Healthcare Conference. Proceedings of Machine Learning Research</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Jung</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Yeung</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sendak</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sjoding</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Ranganath</surname></persName>
		</editor>
		<meeting>the 6th Machine Learning for Healthcare Conference. Machine Learning Research</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">149</biblScope>
			<biblScope unit="page" from="755" to="769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Chestx-ray8: hospital-scale chest X-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bagheri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Medclip: contrastive learning from unpaired medical images and text</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2210.10163" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">EDA: easy data augmentation techniques for boosting performance on text classification tasks</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zou</surname></persName>
		</author>
		<idno>CoRR abs/1901.11196</idno>
		<ptr target="http://arxiv.org/abs/1901.11196" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Unified contrastive learning in image-text-label space</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2204.03610" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
