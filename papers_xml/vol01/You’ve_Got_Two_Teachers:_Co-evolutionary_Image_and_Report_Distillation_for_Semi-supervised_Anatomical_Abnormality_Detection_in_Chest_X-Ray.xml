<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">You’ve Got Two Teachers: Co-evolutionary Image and Report Distillation for Semi-supervised Anatomical Abnormality Detection in Chest X-Ray</title>
				<funder ref="#_vk6z4Cs">
					<orgName type="full">National Key R&amp;D Program of China</orgName>
				</funder>
				<funder ref="#_f2U8hDh">
					<orgName type="full">National Key Research and Development Program of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jinghan</forename><surname>Sun</surname></persName>
							<email>jhsun@stu.xmu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">National Institute for Data Science in Health and Medicine</orgName>
								<orgName type="institution" key="instit2">Xiamen University</orgName>
								<address>
									<settlement>Xiamen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Tencent Healthcare (Shenzhen) Co., LTD</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<region>Tencent Jarvis Lab</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dong</forename><surname>Wei</surname></persName>
							<email>donwei@tencent.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Tencent Healthcare (Shenzhen) Co., LTD</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<region>Tencent Jarvis Lab</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhe</forename><surname>Xu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Tencent Healthcare (Shenzhen) Co., LTD</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<region>Tencent Jarvis Lab</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Donghuan</forename><surname>Lu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Tencent Healthcare (Shenzhen) Co., LTD</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<region>Tencent Jarvis Lab</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hong</forename><surname>Liu</surname></persName>
							<email>liuhong@stu.xmu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">National Institute for Data Science in Health and Medicine</orgName>
								<orgName type="institution" key="instit2">Xiamen University</orgName>
								<address>
									<settlement>Xiamen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Tencent Healthcare (Shenzhen) Co., LTD</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<region>Tencent Jarvis Lab</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Liansheng</forename><surname>Wang</surname></persName>
							<email>lswang@xmu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">National Institute for Data Science in Health and Medicine</orgName>
								<orgName type="institution" key="instit2">Xiamen University</orgName>
								<address>
									<settlement>Xiamen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yefeng</forename><surname>Zheng</surname></persName>
							<email>yefengzheng@tencent.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Tencent Healthcare (Shenzhen) Co., LTD</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<region>Tencent Jarvis Lab</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">You’ve Got Two Teachers: Co-evolutionary Image and Report Distillation for Semi-supervised Anatomical Abnormality Detection in Chest X-Ray</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="363" to="373"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">F8B2CF1319885D7E8CE8428401211213</idno>
					<idno type="DOI">10.1007/978-3-031-43907-0_35</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T12:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Anatomical abnormality detection</term>
					<term>Semi-supervised learning</term>
					<term>Co-evolution</term>
					<term>Visual and textual grounding</term>
					<term>Chest X-ray</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Chest X-ray (CXR) anatomical abnormality detection aims at localizing and characterising cardiopulmonary radiological findings in the radiographs, which can expedite clinical workflow and reduce observational oversights. Most existing methods attempted this task in either fully supervised settings which demanded costly mass perabnormality annotations, or weakly supervised settings which still lagged badly behind fully supervised methods in performance. In this work, we propose a co-evolutionary image and report distillation (CEIRD) framework, which approaches semi-supervised abnormality detection in CXR by grounding the visual detection results with text-classified abnormalities from paired radiology reports, and vice versa. Concretely, based on the classical teacher-student pseudo label distillation (TSD) paradigm, we additionally introduce an auxiliary report classification model, whose prediction is used for report-guided pseudo detection label refinement (RPDLR) in the primary vision detection task. Inversely, we also use the prediction of the vision detection model for abnormality-guided pseudo classification label refinement (APCLR) in the auxiliary report classification task, and propose a co-evolution strategy where the vision and report models mutually promote each other with RPDLR and APCLR performed alternatively. To this end, we effectively incorporate the weak supervision by reports into the semi-supervised TSD pipeline. Besides J. Sun and D. Wei-Contributed equally; J. Sun contributed to this work during an internship at Tencent.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Chest X-ray (CXR) is the most commonly performed diagnostic radiograph in medicine, which helps spot abnormalities or diseases of the airways, blood vessels, bones, heart, and lungs. Given the complexity and workload of clinical CXR reading, there is a growing interest in developing automated methods for anatomical abnormality detection in CXR <ref type="bibr" target="#b18">[19]</ref>-especially using deep neural networks (DNNs) <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">20]</ref>, which are expected to expedite clinical workflow and reduce observational oversights. Here, the detection task involves both localization (e.g., with bounding boxes) and characterization (e.g., cardiomegaly) of the abnormalities. However, training accurate DNN-based detection models usually requires large-scale datasets with high-quality per-abnormality annotations, which is costly in time, effort, and expense.</p><p>To completely relieve the burden of annotation, a few works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25]</ref> resorted to the radiology reports as a form of weak supervision for localization of pneumonia and pneumothorax in CXR. The text report describes important findings in each CXR and is available for most archive radiographs, thus is a valuable source of image-level supervision signal unique to medical image data. However, studies have shown that there are still apparent gaps in performance between imagelevel weakly supervised and bounding-box-level fully supervised detection <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b10">11]</ref>. Alternatively, seeking for a trade-off between annotation effort and model performance, semi-supervised learning aims to achieve reasonable performance with an acceptable quantity of manual annotations. Semi-supervised object detection methods have achieved noteworthy advances in the natural image domain <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23]</ref>. Most of these methods were built on the teacher-student distillation (TSD) paradigm <ref type="bibr" target="#b9">[10]</ref>, where a teacher model is firstly trained on the labeled data, and then a student model is trained on both the labeled data with real annotations and the unlabeled data with pseudo labels generated (predicted) by the teacher. However, compared with objects in natural images, the abnormalities in CXR can be subtle and less well-defined with ambiguous boundaries, thus likely to introduce great noise to the pseudo labels and eventually lead to suboptimal performance of semi-supervised learning with TSD.</p><p>In this paper, we present a co-evolutionary image and report distillation (CEIRD) framework for semi-supervised anatomical abnormality detection in CXR, incorporating the weak supervision by radiology reports. Above all, on the basis of TSD <ref type="bibr" target="#b9">[10]</ref>, CEIRD introduces an auxiliary, also semi-supervised, multi-label report classification natural language processing task, whose prediction is used for noise reduction in the pseudo labels of the primary vision detection task, i.e., report-guided pseudo detection label refinement (RPDLR). Then, noting that the performance of the auxiliary language task is crucial to RPDLR, we inversely use the abnormalities detected in the vision task to filter the pseudo labels in the language task, for abnormality-guided pseudo classification label refinement (APCLR). In addition, we implement an iterative coevolution strategy where RPDLR and APCLR are performed alternatively in a loop, where either model is trained while fixing the other and using the other's prediction for pseudo label refinement. To the best of our knowledge, this is the first work that approaches semi-supervised abnormality detection in CXR by grounding report-classified abnormalities with the visual detection results in the paired radiograph <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b23">24]</ref>, and vice versa. Besides the cross-modal pseudo label refinement, we additionally propose self-adaptive non-maximum suppression (SA-NMS) for intra-(image-)modal refinement, too, where the predictions by both the teacher and student vision models go through NMS together to produce new pseudo detection labels for training. In this way, the pseudo labels generated by the teacher are dynamically rectified by high-confidence predictions of the student who is getting better as training goes. Experimental results on the MIMIC-CXR <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref> public benchmark show that our CEIRD outperforms various up-to-date weakly and semi-supervised methods, and that its building elements are effective.</p><p>To summarize, our contributions include: (1) the complementary RPDLR and APCLR for noise reduction in both vision and language pseudo labels for improved semi-supervised training via mutual grounding, (2) the co-evolution strategy for joint optimization of the primary and auxiliary tasks, and (3) the SA-NMS for dynamic intra-image-modal pseudo label refinement, all contributing to the superior performance of the proposed CEIRD framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>Problem Setting. In semi-supervised anatomical abnormality localization, a data set comprising both unlabeled samples</p><formula xml:id="formula_0">D u = {(x u i , r u i )} Nu i=1 and labeled ones D l = {(x l i , r l i , A i )} N l</formula><p>i=1 is provided for training, where x and r are a CXR and accompanying report, respectively, A i = {(y l , B l )} is the annotation for a labeled sample including both bounding boxes {B l } and corresponding categories {y l }, and N l N u for practical use scenario. It is worth noting that {y l } can also be considered as classification labels for the report. The objective is to obtain a detection model that can accurately localize and classify the abnormalities in any testing CXR (without report in practice), by making good use of both the labeled and unlabeled CXRs plus the accompanying reports in the training set. detection in CXR is given, together with a pretrained language model F R s for multi-label abnormality classification of reports. On the one hand, we generate for an unlabeled image x u i pseudo detection labels with F I t and filter the pseudo labels by self-adaptive non-maximum suppression (NMS). Meanwhile, we feed the corresponding report r u i into F R s and use the prediction for report-guided pseudo detection label refinement (RPDLR). To this end, we obtain refined pseudo labels to supervise the student vision model F I s toward better anatomical abnormality localization. On the other hand, we also pass the detection predictions by F I s to a teacher language model F R t for abnormality-guided pseudo classification label refinement (APCLR), to better supervise the student language model F R s on unlabeled data for report-based abnormality classification. In turn, the better language model F R s helps train better vision models via RPDLR, thus both types of models co-evolve during training. Note that the real labels are used to train both student models along with the pseudo ones. After training, we only need the student vision model F I s for abnormality localization in testing CXRs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Overview.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preliminary Pseudo Label Distillation for Semi-supervised Learning.</head><p>Both of our baseline semi-supervised vision and language models follow the teacher-student knowledge distillation (TSD) procedure <ref type="bibr" target="#b9">[10]</ref>. For report classification, we first train a teacher model F R t on labeled reports, and then train a student model F R s to predict real labels on labeled reports and pseudo labels produced by F R t on unlabeled ones with the loss function:</p><formula xml:id="formula_1">L R = L R sup + L R unsup = D l L R cls ŷl , y l + Du L R cls (ŷ, y pr ) ,<label>(1)</label></formula><p>where L R cls is the cross-entropy loss, {ŷ} = F R s (r) is the prediction by the student model, {y pr } = F R t (r u ) is the set of pseudo labels generated by the teacher model. In each batch, labeled and unlabeled instances are sampled according to a controlled ratio. The resulting report classification model F R s will be utilized later to help with the primary task of abnormality detection in CXR. Similarly, a student vision model F I s for abnormality detection in CXR is trained in semisupervised setting by distilling from a teacher vision model F I t trained on labeled CXRs, with the loss function:</p><formula xml:id="formula_2">L I = L I sup + L I unsup = D l L I cls ŷl , y l + L I reg Bl , B l + Du L I cls (ŷ, y pv ) + L I reg B, B pv , (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>where {(ŷ, B)} = F I s (x) are the predictions by the student model, {(y pv , B pv )} = F I t (x u ) are the pseudo class and bounding box labels generated by the teacher model, L I cls is the focal loss <ref type="bibr" target="#b15">[16]</ref> for abnormality classification, and L I reg is the smooth L1 loss for bounding box regression.</p><p>Self-adaptive Non-maximum Suppression. During the TSD, the teacher vision model F I t is kept fixed. While its knowledge suffices for guiding the student vision model F I s in the early stage of TSD, it may somehow impede the learning of F I s when F I s gradually improves by also learning from the large amount of unlabeled data. Therefore, to gradually improve quality and robustness of the pseudo detection labels as F I s learns, we propose to perform self-adaptive non-maximum suppression (SA-NMS) to combine the pseudo labels {(y pv , B pv )} output by F I t and the predictions {(ŷ, B)} by F I s in each mini batch. Specifically, we perform NMS on the combined set of the pseudo labels and predictions: {(y cv , B cv )} = NMS {(y pv , B pv )} {(ŷ, B)} , and replace {(y pv , B pv )} in Eq.</p><p>(2) with {(y cv , B cv )} for supervision by unlabeled CXRs. In this way, highly confident predictions by the maturing student can rectify imprecise ones by the teacher, leading to better supervision signals stemming from unlabeled data.</p><p>Report-Guided Pseudo Label Refinement. In routine clinics, almost every radiograph in archive is accompanied by a report describing findings, abnormalities (if any), and diagnosis. Compared with the captions of natural images, the report texts constitute a unique (to medical image analysis) and rich source of extra information in addition to the image modality. To this end, we propose report-guided pseudo detection label refinement (RPDLR) to make use of this cross-modal information for semi-supervised anatomical abnormality detection in CXR. Specifically, we use the student language model F R s (trained with Eq. ( <ref type="formula" target="#formula_1">1</ref>)) to refine the pseudo detection labels. Given a pair of unlabeled image x u and report r u , we obtain the set of abnormalities {(y cv , B cv )} detected in x u after SA-NMS, and the set of abnormalities {ŷ} classified in r u by F R s . Then, we only keep the pseudo detection labels whose categories are in the report-classified abnormalities:</p><formula xml:id="formula_4">{(y v , B v )} = y cv j , B cv j y cv j ∈ {ŷ} . (<label>3</label></formula><formula xml:id="formula_5">)</formula><p>Eventually, we train the student vision model F I s using {(y v , B v )} in Eq. ( <ref type="formula" target="#formula_2">2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Co-evolutionary Semi-supervised Learning with Cycle Pseudo Label</head><p>Refinement. As the auxiliary student language model F R s plays an important role in RPDLR, it is reasonable to optimize its performance which in turn would benefit the primary task of abnormality detection. Therefore, we further propose an inverse, abnormality-guided pseudo classification labels refinement (APCLR) to help with semi-supervised training of the report classification model. Similarly in concept to the RPDLP, given a pair of unlabeled image x u and report r u , we obtain the set of abnormalities {(ŷ, B)} detected in x u by the student vision model F I s , and the set of classification pseudo labels {y pr } generated for r u by the teacher language model F R t . We retain only the pseudo labels {y pr j |y pr j ∈ {ŷ}}, by excluding the report-classified abnormalities not detected in the paired CXR.</p><p>Ideally, one should use an optimal report classification model for refinement of the abnormality detection pseudo labels, and vice versa. However, the two models are mutually dependent on each other in a circle. To solve this dilemma, we implement an alternative co-evolution strategy to refine the abnormality detection and report classification pseudo labels iteratively, in generations. As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, the k th generation student vision model F I s,k is distilled from the teacher F I t,k-1 , whose pseudo labels are refined by the prediction of the frozen student language model F R s,k via RPDLR. Subsequently, F I s,k is frozen and used to 1) help train the (k + 1) th student language model F R s,k+1 via APCLR, and 2) serve as the teacher vision model in next generation: F I s,k → F I t,k .<ref type="foot" target="#foot_0">1</ref> Note that in each generation the students are reborn with random initialization <ref type="bibr" target="#b7">[8]</ref>. Thus the co-evolution continues to optimize the vision and report models cyclically with cross-modal mutual promotion. After training, we only need the K th generation student vision model F I s,K for abnormality detection in upcoming test CXRs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>Dataset and Evaluation Metrics. We conduct experiments on the chest radiography dataset MIMIC-CXR <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>, with the detection annotations provided by MS-CXR <ref type="bibr" target="#b2">[3]</ref>. MIMIC-CXR is a large publicly available dataset of CXR and free-text radiology reports. MS-CXR provides bounding box annotations for part of the CXRs in MIMIC-CXR (1,026 samples). MIMIC-CXR includes 14 categories of anatomical abnormalities for multi-label classification of the reports, while there are only eight categories in the bounding box annotations of MS-CXR. For consistency, we exclude samples in MIMIC-CXR that have abnormality labels outside the eight categories of MS-CXR, leaving 112,425 samples. Thus, in our semi-supervised setting, 1,026 samples are labeled and the rest are not. <ref type="foot" target="#foot_1">2</ref> We split the labeled samples for training, validation, and testing according to the ratio of 7:1:2, and use the remaining samples as our unlabeled training data. We focus on the frontal views in this work. The mean average precision (mAP) <ref type="bibr" target="#b6">[7]</ref> with the intersection over union (IoU) threshold of 0.25, 0.5, and 0.75 is employed to evaluate the performance of abnormality detection in CXR.</p><p>Implementation. The PyTorch <ref type="bibr" target="#b17">[18]</ref> framework (1.4.0) is used for experiments.</p><p>For report classification, we employ the BERT-base uncased model <ref type="bibr" target="#b5">[6]</ref> with eight linear heads. Stochastic gradient descent with the momentum of 0.9 and learning rate of 10 -4 is used for optimization. The batch size is set to 16 reports. For abnormality detection, we employ RetinaNet <ref type="bibr" target="#b15">[16]</ref> with FPN <ref type="bibr" target="#b14">[15]</ref>+ResNet-101 <ref type="bibr" target="#b8">[9]</ref> as backbone. We resize all images to 512×512 pixels and use a batch size of 16. Data augmentation including random cropping and flipping is performed.</p><p>Our implementation and hyper-parameters follow the official settings <ref type="bibr" target="#b15">[16]</ref>. Unless otherwise stated, we evolve the vision and language models for two generations, and train both models for 2000 iterations in each generation (including initial training of the teacher models). The ratio of labeled to unlabeled samples in each mini batch during the semi-supervised training is empirically set to 1:1 and 2:1 for the language and vision models, respectively. The source code is available at: https://github.com/jinghanSunn/CEIRD.</p><p>Comparison with State-of-the-Art (SOTA) Methods. We compare our proposed co-evolution image and report distillation (CEIRD) framework with several up-to-date detection methods, including weakly supervised: CAM <ref type="bibr" target="#b25">[26]</ref> (locating objects based on class activation maps), AGXNet <ref type="bibr" target="#b24">[25]</ref> (aiding CAMbased localization with report representations), fully supervised on labeled training data only (Sup.), baseline semi-supervised via teacher-student pseudo-label distillation (TSD; see Eq. ( <ref type="formula" target="#formula_2">2</ref>)) <ref type="bibr" target="#b9">[10]</ref>, and three SOTA semi-supervised (STAC <ref type="bibr" target="#b20">[21]</ref>, LabelMatch <ref type="bibr" target="#b3">[4]</ref>, and Soft Teacher <ref type="bibr" target="#b22">[23]</ref>) object detection methods.</p><p>The results are shown in Table <ref type="table" target="#tab_0">1</ref>, from which we have the following observations. First, both fully supervised (by the labeled data only) and semi-supervised methods outperform the weakly-supervised by large margins, proving the efficacy of using limited annotations. Second, all semi-supervised methods outperform the fully supervised (by the labeled data only) by various margins, demonstrating  apparent benefit of making use of the unlabeled data, too. Third, our CEIRD achieves the best performance of all the semi-supervised methods for the mAPs evaluated at three different IoU thresholds, outperforming the second best (Soft Teacher) by up to 1.76%. These results clearly demonstrate the advantage of our method which innovatively integrates the semi-supervision by unlabeled images and the weak supervision by texts. In addition, we evaluate a semi-oracle of our method, where the ground truth report labels provided in MIMIC-CXR are used for RPDLR, instead of the auxiliary model's prediction. As we can see, our method is marginally short of the semi-oracle, e.g., 37.20% versus 37.39% for mAP@0.5, suggesting that our co-evolution strategy can effectively mine the relevant information from the reports. We provide in the supplementary material visualizations of example detection results by Soft Teacher and our method, where ours are visually superior with fewer misses (left), fewer false positives (middle), and better localization (right). Lastly, we also provide performance evaluation of the auxiliary report classification task in the supplement.</p><p>Ablation Study. We conduct ablation studies on the validation data to investigate efficacy of the novel building elements of our CEIRD framework, including: report-guided pseudo detection label refinement (RPDLR), co-evolution strategy (CoE) with abnormality-guided pseudo classification label refinement (APCLR), and self-adaptive non-maximum suppression (SA-NMS). We use the preliminary teacher-to-student pseudo label distillation as baseline (Eq. ( <ref type="formula" target="#formula_2">2</ref>)). As shown in Table <ref type="table" target="#tab_1">2</ref>, RPDLR (column (a)) substantially boosts performance upon the baseline by 1.79-4.06% in mAPs thanks to the refined pseudo detection labels. By adopting CoE+APCLR (column (b)), we achieve further performance improvements up to 1.53% as the auxiliary report classification model gets better together. Last but not the least, the introduction of SA-NMS (column (c)) also brings improvements up to 2.14%. These results validate the novel design of our framework. In addition, we conduct experiments to empirically determine the optimal number of generations for the co-evolution. The results are shown in Fig. <ref type="figure" target="#fig_2">3</ref>, where the 0 th generation means fully supervised models trained on the labeled data only (i.e., the initial teacher models F I t,0 and F R t,0 ). As we can see, the vision and report models improve in the first two and three generations, respectively, and then remain stable in the following ones, confirming that both models promote each other with the co-evolution strategy. Since our ulti-mate objective is abnormality detection in CXR, we select two generations for comparison with other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this work, we proposed a new co-evolutionary image and report distillation (CEIRD) framework for semi-supervised anatomical abnormality detection in chest X-ray. On the basis of a preliminary teacher-student pseudo label distillation, we first presented self-adaptive NMS to mingle highly confident predictions by both the teacher and student for improved pseudo labels. We then proposed report-guided pseudo detection label refinement (RPDLR) that used abnormalities classified from the accompanying radiology reports by an auxiliary language model to eliminate unmatched pseudo labels. Meanwhile, we further proposed an inverse, abnormality-guided pseudo classification label refinement (APCLR) making use of the abnormalities detected in X-ray images for better language model training. In addition, we implemented a co-evolution strategy that looped the RPDLR and APCLR to iteratively optimize the main vision detection model and auxiliary report classification model in an alternative manner. Experimental results showed that our CEIRD framework achieved superior performance to up-to-date semi-/weakly-supervised methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overview of the proposed framework. RPDLR: report-guided pseudo detection label refinement; APCLR: abnormality-guided pseudo classification label refinement.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig.2. Illustration of the co-evolution strategy. "R" and "A" represent report-and abnormality-guided pseudo label refinements (RPDLR and APCLR), respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Performance of (a) vision and (b) report models as a function of generations. AUC: area under the receiver operating characteristic curve.</figDesc><graphic coords="8,167,76,56,69,209,56,55,42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Abnormality detection results on the test data, using mAP (%) with the IoU thresholds of 0.25, 0.5, and 0.75. TSD: teacher-student distillation.</figDesc><table><row><cell cols="7">mAP CAM [26] AGXNet [25] Sup. TSD [10] STAC [21] LabelMatch [4] Soft Teacher [23] Ours Semi-oracle</cell></row><row><cell>@0.25 20.47</cell><cell>29.96</cell><cell>37.91 38.29</cell><cell>39.26</cell><cell>39.92</cell><cell>40.17</cell><cell>41.93 42.61</cell></row><row><cell>@0.5 11.20</cell><cell>15.62</cell><cell>32.84 33.95</cell><cell>35.01</cell><cell>36.40</cell><cell>36.59</cell><cell>37.20 37.39</cell></row><row><cell>@0.75 3.05</cell><cell>7.44</cell><cell>19.21 19.51</cell><cell>23.90</cell><cell>24.06</cell><cell>24.78</cell><cell>25.12 25.66</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation study results on the validation data.</figDesc><table><row><cell></cell><cell cols="2">Baseline (a)</cell><cell>(b)</cell><cell>(c)</cell></row><row><cell>RPDLR</cell><cell>-</cell><cell></cell><cell></cell></row><row><cell cols="2">CoE+APCLR -</cell><cell>-</cell><cell></cell></row><row><cell>SA-NMS</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>mAP@0.25</cell><cell>37.31</cell><cell cols="3">39.10 39.72 41.86</cell></row><row><cell>mAP@0.5</cell><cell>32.76</cell><cell cols="3">35.87 35.90 36.18</cell></row><row><cell>mAP@0.75</cell><cell>17.96</cell><cell cols="3">22.02 23.55 24.49</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The initial teachers F I t,0 and F R t,0 are obtained by training on the labeled data only.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>In this work, we deliberately construct the semi-supervised setting by ignoring the report labels provided in MIMIC-CXR for methodology development. When it comes to a practical new application with no such label available, e.g., semi-supervised lesion detection in color fundus photography, our method can be readily applied.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. This work was supported by the <rs type="funder">National Key R&amp;D Program of China</rs> under Grant <rs type="grantNumber">2020AAA0109500/2020AAA0109501</rs> and the <rs type="funder">National Key Research and Development Program of China</rs> (<rs type="grantNumber">2019YFE0113900</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_vk6z4Cs">
					<idno type="grant-number">2020AAA0109500/2020AAA0109501</idno>
				</org>
				<org type="funding" xml:id="_f2U8hDh">
					<idno type="grant-number">2019YFE0113900</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0 35.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">What&apos;s the point: semantic segmentation with point supervision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bearman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46478-7_34</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46478-734" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2016</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">9911</biblScope>
			<biblScope unit="page" from="549" to="565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Improving pneumonia localization via cross-attention on medical images and reports</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bhalodia</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87196-3_53</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87196-353" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12902</biblScope>
			<biblScope unit="page" from="571" to="581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Making the most of text semantics to improve biomedical vision-language processing</title>
		<author>
			<persName><forename type="first">B</forename><surname>Boecking</surname></persName>
		</author>
		<idno>arxiv:2204.09817</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arxiv Preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Label matching semi-supervised object detection</title>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="14381" to="14390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Align2Ground: weakly supervised phrase grounding guided by image-caption alignment</title>
		<author>
			<persName><forename type="first">S</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sikka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Divakaran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2601" to="2610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>arxiv Preprint: arxiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The PASCAL visual object classes (VOC) challenge</title>
		<author>
			<persName><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="303" to="308" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Born again neural networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Furlanello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1607" to="1616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno>arxiv Preprint: arxiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A benchmark for weakly semi-supervised abnormality localization in chest X-rays</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16437-8_24</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16437-824" />
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">13433</biblScope>
			<biblScope unit="page" from="249" to="260" />
			<date type="published" when="2022">2022</date>
			<publisher>Springer</publisher>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
	<note>MIC-CAI 2022</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">MIMIC-CXR-JPG-chest radiographs with structured labels</title>
		<author>
			<persName><forename type="first">A</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PhysioNet</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">MIMIC-CXR-JPG, a large publicly available database of labeled chest radiographs</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Johnson</surname></persName>
		</author>
		<idno>arxiv Preprint: arxiv:1901.07042</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep learning at chest radiography: automated classification of pulmonary tuberculosis by using convolutional neural networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lakhani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sundaram</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiology</title>
		<imprint>
			<biblScope unit="volume">284</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="574" to="582" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Lung nodule detection in Xray images: a new feature set</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">B</forename><surname>Ogul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Koşucu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Özçam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Kanik</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-11128-5_38</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-11128-538" />
	</analytic>
	<monogr>
		<title level="m">6th European Conference of the International Federation for Medical and Biological Engineering</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Lacković</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Vasic</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="150" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">PyTorch: an imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Computer-aided detection in chest radiography based on artificial intelligence: a survey</title>
		<author>
			<persName><forename type="first">C</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomed. Eng. Online</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">CheXNet: radiologist-level pneumonia detection on chest Xrays with deep learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<idno>arxiv:1711.05225</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arxiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">A simple semi-supervised learning framework for object detection</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<idno>arxiv Preprint: arxiv:2005.04757</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Weakly supervised onestage vision and language disease detection using large scale pneumonia and pneumothorax studies</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Turkbey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59719-1_5</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59719-15" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12264</biblScope>
			<biblScope unit="page" from="45" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">End-to-end semi-supervised object detection with soft teacher</title>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3060" to="3069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A fast and accurate one-stage approach to visual grounding</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4683" to="4693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Anatomy-guided weakly-supervised abnormality localization in chest X-rays</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Deible</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Batmanghelich</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16443-9_63</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16443-963" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13435</biblScope>
			<biblScope unit="page" from="658" to="668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
