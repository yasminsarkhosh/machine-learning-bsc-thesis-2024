<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-IMU with Online Self-consistency for Freehand 3D Ultrasound Reconstruction</title>
				<funder ref="#_7agRf7k">
					<orgName type="full">Shenzhen-Hong Kong Joint Research Program</orgName>
				</funder>
				<funder ref="#_garbdNt #_nMV7WEz">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_s4WCXNj">
					<orgName type="full">Shenzhen Science and Technology Innovations Committee</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mingyuan</forename><surname>Luo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Biomedical Engineering</orgName>
								<orgName type="department" key="dep2">Health Science Center</orgName>
								<orgName type="laboratory">National-Regional Key Technology Engineering Laboratory for Medical Ultrasound</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Medical Ultrasound Image Computing (MUSIC) Lab</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Marshall Laboratory of Biomedical Engineering</orgName>
								<orgName type="institution" key="instit2">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xin</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Biomedical Engineering</orgName>
								<orgName type="department" key="dep2">Health Science Center</orgName>
								<orgName type="laboratory">National-Regional Key Technology Engineering Laboratory for Medical Ultrasound</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Medical Ultrasound Image Computing (MUSIC) Lab</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Marshall Laboratory of Biomedical Engineering</orgName>
								<orgName type="institution" key="instit2">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhongnuo</forename><surname>Yan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Junyu</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Biomedical Engineering</orgName>
								<orgName type="department" key="dep2">Health Science Center</orgName>
								<orgName type="laboratory">National-Regional Key Technology Engineering Laboratory for Medical Ultrasound</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Medical Ultrasound Image Computing (MUSIC) Lab</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Marshall Laboratory of Biomedical Engineering</orgName>
								<orgName type="institution" key="instit2">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuanji</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Biomedical Engineering</orgName>
								<orgName type="department" key="dep2">Health Science Center</orgName>
								<orgName type="laboratory">National-Regional Key Technology Engineering Laboratory for Medical Ultrasound</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Medical Ultrasound Image Computing (MUSIC) Lab</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Marshall Laboratory of Biomedical Engineering</orgName>
								<orgName type="institution" key="instit2">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiongquan</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Biomedical Engineering</orgName>
								<orgName type="department" key="dep2">Health Science Center</orgName>
								<orgName type="laboratory">National-Regional Key Technology Engineering Laboratory for Medical Ultrasound</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Medical Ultrasound Image Computing (MUSIC) Lab</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Marshall Laboratory of Biomedical Engineering</orgName>
								<orgName type="institution" key="instit2">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xindi</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Biomedical Engineering</orgName>
								<orgName type="department" key="dep2">Health Science Center</orgName>
								<orgName type="laboratory">National-Regional Key Technology Engineering Laboratory for Medical Ultrasound</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Medical Ultrasound Image Computing (MUSIC) Lab</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Marshall Laboratory of Biomedical Engineering</orgName>
								<orgName type="institution" key="instit2">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Shenzhen RayShape Medical Technology Inc</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jikuan</forename><surname>Qian</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Shenzhen RayShape Medical Technology Inc</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jun</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Biomedical Engineering</orgName>
								<orgName type="department" key="dep2">Health Science Center</orgName>
								<orgName type="laboratory">National-Regional Key Technology Engineering Laboratory for Medical Ultrasound</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Medical Ultrasound Image Computing (MUSIC) Lab</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Marshall Laboratory of Biomedical Engineering</orgName>
								<orgName type="institution" key="instit2">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Dong</forename><surname>Ni</surname></persName>
							<email>nidong@szu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Biomedical Engineering</orgName>
								<orgName type="department" key="dep2">Health Science Center</orgName>
								<orgName type="laboratory">National-Regional Key Technology Engineering Laboratory for Medical Ultrasound</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Medical Ultrasound Image Computing (MUSIC) Lab</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Marshall Laboratory of Biomedical Engineering</orgName>
								<orgName type="institution" key="instit2">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-IMU with Online Self-consistency for Freehand 3D Ultrasound Reconstruction</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="342" to="351"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">7EC57C47CCC00F1ACDD462839414BBB7</idno>
					<idno type="DOI">10.1007/978-3-031-43907-0_33</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T12:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Multiple IMU</term>
					<term>Online Learning</term>
					<term>Freehand 3D</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Ultrasound (US) imaging is a popular tool in clinical diagnosis, offering safety, repeatability, and real-time capabilities. Freehand 3D US is a technique that provides a deeper understanding of scanned regions without increasing complexity. However, estimating elevation displacement and accumulation error remains challenging, making it difficult to infer the relative position using images alone. The addition of external lightweight sensors has been proposed to enhance reconstruction performance without adding complexity, which has been shown to be beneficial. We propose a novel online self-consistency network (OSCNet) using multiple inertial measurement units (IMUs) to improve reconstruction performance. OSCNet utilizes a modal-level self-supervised strategy to fuse multiple IMU information and reduce differences between reconstruction results obtained from each IMU data. Additionally, a sequencelevel self-consistency strategy is proposed to improve the hierarchical consistency of prediction results among the scanning sequence and its sub-sequences. Experiments on large-scale arm and carotid datasets with multiple scanning tactics demonstrate that our OSCNet outperforms previous methods, achieving state-of-the-art reconstruction performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Ultrasound (US) imaging has been widely used in clinical diagnosis due to its advantages of safety, repeatability, and real-time imaging. Compared with 2D US, 3D US can provide more comprehensive spatial information. Freehand 3D US can enhance the understanding of physicians about the scanned region of interest without increasing the complexity of scanning <ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref>. However, the difficulty in estimating elevation displacement and accumulation error makes it very challenging to infer the relative position only from images. In this regard, it is expected to improve the reconstruction performance with the help of external lightweight sensors, which will not significantly increase the scanning complexity.</p><p>Sensorless freehand 3D US reconstructs the volume by calculating the relative transformation of a series of US images. Previous studies were mainly based on speckle decorrelation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b16">17]</ref>, which estimates out-of-plane motion through the correlation of speckle patterns in two successive frames. With the development of deep learning technology, recent studies were mainly based on convolutional neural network (CNN). Prevost et al. <ref type="bibr" target="#b14">[15]</ref> proposed an end-to-end method based on CNN to estimate the relative motion of US images. Guo et al. <ref type="bibr" target="#b3">[4]</ref> proposed a deep contextual learning network (DCL-Net) based on 3D CNN to estimate the trajectory of US probe, and in a more recent study <ref type="bibr" target="#b2">[3]</ref>, they proposed a deep contextual-contrastive network (DC 2 -Net), which introduced a contrastive learning strategy to improve the reconstruction performance by leveraging the label efficiently. Luo et al. <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref> proposed an online learning framework (OLF) that improves reconstruction performance by online learning and shape priors.</p><p>Due to the low cost, small size, and low power consumption of micro-electromechanical-systems (MEMS), the sensor called inertial measurement unit (IMU) has been widely used in navigation systems. Prevost et al. <ref type="bibr" target="#b13">[14]</ref> incorporated IMU orientation into neural network to improve reconstruction performance. Luo et al. <ref type="bibr" target="#b11">[12]</ref> proposed a deep motion network (MoNet) to mine the valuable information of low signal-to-noise acceleration, and an online self-supervised strategy was designed to further improve reconstruction performance. However, the main disadvantage of IMU is that its measurement noise can not be completely eliminated by calibration. Existing studies have shown that combining multiple IMUs may help reduce noise and improve accuracy <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b15">16]</ref>.</p><p>In this study, we propose a multi-IMU-based online self-consistency network (OSCNet) for freehand 3D US reconstruction. Our contribution is two-fold. First, we equip multiple IMUs (see Fig. <ref type="figure" target="#fig_0">1</ref>) to reduce the influence of noise in individual IMU data. We propose a modal-level self-supervised strategy (MSS) to fuse the information from multiple IMUs. MSS improves reconstruction performance by reducing the differences between reconstruction results obtained from each IMU data. Second, to reduce the estimation instability caused by scanning differences such as frame rates, we propose a sequence-level self-consistency strategy (SCS), which improves the hierarchical consistency of prediction results among the scanning sequence and its sub-sequences based on a consistent context. Experimental results show that the proposed OSCNet can effectively fuse the information of multiple IMUs and achieve state-of-the-art reconstruction performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>Figure <ref type="figure" target="#fig_1">2</ref> illustrates the proposed OSCNet, which consists of two essential components: backbone and online learning. We construct a backbone using the temporal and multi-branch structure from <ref type="bibr" target="#b11">[12]</ref>. The main branch in the backbone consists of ResNet <ref type="bibr" target="#b4">[5]</ref> for feature extraction and LSTM <ref type="bibr" target="#b5">[6]</ref> for processing temporal information. It aids future estimation by leveraging temporal contextual information. Additionally, there is an independent motion branch in the backbone that fuses IMU information from a motion perspective with US images. For more details, please refer to <ref type="bibr" target="#b11">[12]</ref>.</p><p>In the training phase, we input an N -length scanning sequence</p><formula xml:id="formula_0">I = {I i |i = 1, 2, • • • , N} and corresponding multiple IMU data U = {U i |i = 1, 2, • • • , N -1} into the backbone to estimate the 3D relative transformation parameters θ = {θ i |i = 1, 2, • • • , N -1}</formula><p>, where θ i includes 3-axis translations t i = (t x , t y , t z ) i and rotation angles φ i = (φ x , φ y , φ z ) i between image I i and I i+1 . The multiple IMU data consists of M independent IMU data</p><formula xml:id="formula_1">U i = {U j i |j = 1, 2, • • • , M}, where U j i consists of 3-axis angles Φ j i = (Φ x , Φ y , Φ z ) j i and accelerations A j i = (A x , A y , A z ) j i .</formula><p>The pre-processing process for Φ i and A i is consistent with <ref type="bibr" target="#b11">[12]</ref>. Compared to traditional offline inference strategies, online learning can leverage valuable information from unlabeled data to improve the model's performance <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12]</ref>. In the testing phase, we propose two online self-supervised strategies based on both the multiple IMU data (modal-level) and the scanning sequence itself (sequencelevel) to improve the performance of the backbone's estimations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Modal-Level Self-supervised Strategy</head><p>Multiple IMUs mounted in different directions provide diverse measurement constraints for the model's estimation, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>. This makes it possible to reduce the influence of noise in individual IMU data while adaptively optimizing for estimation. We construct an online modal-level self-supervised strategy (MSS) that leverages the consistency between the backbone's estimation and multiple IMU data to improve the reconstruction performance.</p><p>As shown in the top of Fig. <ref type="figure" target="#fig_1">2</ref>, during the training phase, we repeatedly input the US images and M different IMU data into the backbone to obtain M estimated parameters. We use the average of the M estimated parameters θ = 1 M M j=1 θj as the final output of the backbone. We then calculate training loss between θ and ground truth θ using mean absolute error (MAE) and Pearson correlation loss <ref type="bibr" target="#b3">[4]</ref>:</p><formula xml:id="formula_2">L = θ -θ 1 + (1 - Cov( θ, θ) σ( θ)σ(θ) ),<label>(1)</label></formula><p>where Cov, σ and • 1 denote the covariance, the standard deviation, and L1 normalization, respectively. As shown in the bottom of Fig. <ref type="figure" target="#fig_1">2</ref>, during the testing phase, we use each IMU data U j (j = 1, 2, • • • , M) as a weak label to constrain the corresponding estimated parameters θj . We calculate the estimated acceleration Âj at the center point of each image using the estimated θj . To reduce the influence of acceleration noise, we scale the Âj to match the mean-zeroed IMU acceleration.</p><formula xml:id="formula_3">Âj i = (( tj i-1 ) -1 + tj i ) - 1 N -2 i (( tj i-1 ) -1 + tj i ), i = 2, 3, • • • , N -1,<label>(2)</label></formula><p>where ( tj i-1 ) -1 represents the translations in the inversion of θj i-1 . Similar to <ref type="bibr" target="#b11">[12]</ref>, we use Pearson correlation loss to measure the difference between the estimated and IMU acceleration, while the angle is measured using MAE. Therefore, the single-IMU consistency constraint between the estimated parameters and corresponding IMU data can be expressed as:</p><formula xml:id="formula_4">L single-IMU = M j=1 (1 - Cov( Âj , A j ) σ( Âj )σ(A j ) ) + φj -Φ j 1 . (<label>3</label></formula><formula xml:id="formula_5">)</formula><p>In addition, the consistency among multiple IMU data itself also provides the possibility to improve the reconstruction performance. It constrains the backbone to obtain similar estimated parameters for different IMU data inputs from the same scan. Specifically, we construct multi-IMU consistency constraints as:</p><formula xml:id="formula_6">L multi-IMU = M j,k=1,2,••• ,M,j&lt;k (1 - Cov( Âj , Âk ) σ( Âj )σ( Âk ) ) + φj -φk 1 . (<label>4</label></formula><formula xml:id="formula_7">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Sequence-Level Self-consistency Strategy</head><p>Consistent context should lead to consistent parameter estimation, which constrains the model at the sequence level, reducing the estimation instability caused by scanning differences such as frame rates. Inspired by contrastive learning <ref type="bibr" target="#b6">[7]</ref>,</p><p>we construct an online sequence-level self-consistency strategy (SCS). SCS randomly generates sub-sequences with consistent context for each scan. The hierarchical consistency constraint among the generated sub-sequences and the original sequence improves the reconstruction performance of the backbone. Specifically, as shown in Fig. <ref type="figure" target="#fig_1">2</ref>, we randomly interval sample and flip each scanning sequence I and its IMU data U to generate a sub-sequence I sub (U sub ) with consistent context. In the testing phase, we obtain the estimated parameters θsub of I sub (U sub ) using the trained backbone. Then compare θsub with the original estimated parameters θ after the same interval sampling and flipping to construct the self-consistency constraint:</p><formula xml:id="formula_8">L self -consistency = θsub -H τ ( θ) 1 = 1 M M j=1 B(H τ (I), H τ (U j )) -H τ ( 1 M M j=1 B(I, U j )) 1 ,<label>(5)</label></formula><p>where H τ converts the parameters, sequences, or IMU data under interval sampling and flipping operation τ . B denotes the backbone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>Materials and Implementation. The equipment we used to collect data includes a portable US machine, four IMU sensors (WT901C-232, WitMotion) and an electromagnetic (EM) positioning system. The US images were acquired with a linear probe at 10 MHz, and the depth was set at 4 cm. As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, we bound four IMU sensors to the probe in different orientations (three for 3axis directions and one for redundancy) using a 3D-printed bracket, which can compensate for errors and reduce measurement singularities <ref type="bibr" target="#b8">[9]</ref>. The resolutions of the IMU acceleration and angle are 5 × 10 -4 g/LSB and 0.5 • , respectively. We used the EM positioning system to trace the scan route accurately. The direction and angle resolutions of the EM positioning system are 1.4 mm and 0.5 • , respectively. We calibrated the multiple IMU sensors and the EM positioning system using the Levenberg-Marquardt algorithm <ref type="bibr" target="#b7">[8]</ref> to ensure accurate measurements and minimise system errors. As shown in Fig. <ref type="figure" target="#fig_2">3</ref>, the calibrated IMU data exhibits a generally consistent overall trend, although differences still exist. We constructed two datasets, including arm and carotid, from 36 volunteers. The arm dataset contains 288 scans, with scanning tactics including linear, curved, loop, and sector scan. The carotid dataset contains 216 scans, with scanning tactics including linear, loop, and sector scan. The average lengths of the arm and carotid scans are 323.96 mm and 203.25 mm, respectively. The size of scanned images is 248 × 260 pixels, and the image spacing is 0.15 × 0.15 mm 2 . The collection and use of the data are approved by the local IRB.</p><p>The arm and carotid datasets were randomly divided into 200/40/48 and 150/30/36 scans based on volunteer level to construct training/validation/test set. To prevent overfitting and enhance the model's robustness, we performed random augmentations on each scan, including sub-sequence intercepting, interval sampling, and sequence inversion. We randomly augmented each training scan to 20 sequences and each test scan to 10 sequences to simulate complex real-world situations. We used the Adam optimizer to optimize the OSCNet. During the training phase, the epochs and batch size are set to 200 and 1, respectively. To avoid overfitting, we set the initial learning rate to 2 × 10 -4 and used a learning rate decay strategy that halves the learning rate every 30 epochs. During the online learning phase, the iteration epoch and learning rate  are set to 60 and 2 × 10 -6 , respectively. All code was implemented in PyTorch and executed on an RTX 3090 GPU.</p><p>Quantitative and Qualitative Analysis. To demonstrate the effectiveness of our OSCNet, we compared it with three state-of-the-art methods, including CNN <ref type="bibr" target="#b13">[14]</ref>, DC 2 -Net <ref type="bibr" target="#b2">[3]</ref> and MoNet <ref type="bibr" target="#b11">[12]</ref>. All comparison methods were trained to convergence using the experimental settings given in the corresponding papers. We adopt six metrics from <ref type="bibr" target="#b11">[12]</ref> to evaluate reconstruction performance: final drift rate (FDR), average drift rate (ADR), maximum drift (MD), sum of drift (SD), symmetric Hausdorff distance (HD), and mean error of angle (EA). In addition, ablation experiments are conducted to validate the effectiveness of MSS and SCS as proposed in our OSCNet. Table <ref type="table" target="#tab_0">1</ref> shows that our OSCNet significantly outperforms CNN, DC 2 -Net, MoNet, and our Backbone in all metrics for both arm and carotid scans (t-test, p &lt; 0.05), except for MoNet's ADR on the carotid scans (t-test, p = 0.10). Notably, sensor-based methods (MoNet and OSCNet) have exhibited improvements in all metrics compared to sensorless methods (DC 2 -Net and CNN). The multi-IMU-based OSCNet outperforms the single-IMU-based MoNet, verifying the effectiveness of multiple IMU integration. Moreover, the ablation experiments further demonstrate that both multiple IMU integration (MSS) and selfconsistency constraint (SCS) greatly improve the reconstruction performance. In addition, Fig. <ref type="figure" target="#fig_3">4</ref> displays the metric decline curves during the online learning phase of MoNet and OSCNet on the arm and carotid datasets. All metric curves exhibit a decreasing trend followed by stabilization. We note that our OSCNet has achieved further improvements compared to MoNet, with 13.56% /7.32%/30.65% and 7.62%/4.00%/29.16% improvement in FDR/ADR/EA on the arm and carotid datasets, respectively. Figure <ref type="figure" target="#fig_4">5</ref> presents several typical reconstruction results of all methods. It can be observed that our OSCNet outperforms other methods in reconstruction results and closely approximates the ground truth across all scanning tactics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this study, we propose a novel multi-IMU-based online self-consistency network (OSCNet) to conduct freehand 3D US reconstruction. We propose an online modal-level self-supervised strategy (MSS) that integrates multiple IMUs to reduce the influence of single IMU noise and enhance reconstruction performance. We propose an online sequence-level self-consistency strategy (SCS) to improve the reconstruction stability using hierarchical consistency among the generated sub-sequences and the original sequence. The experimental results on the arm and carotid datasets show that our OSCNet achieves state-of-the-art reconstruction performance. Future research will focus on exploring more general reconstruction methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Pipeline of freehand 3D US reconstruction with multiple lightweight inertial measurement unit (IMU) sensors.</figDesc><graphic coords="2,55,98,53,96,340,18,101,74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Overview of our proposed multi-IMU online self-consistency network (OSCNet). IMU data diagrams (U/ Û ) show angle curves (Φ/ Φ, top) and acceleration curves (A/ Â, bottom). Relative transformation parameter diagrams (θ/ θ) show angle curves (φ/ φ, top) and translation curves (t/ t, bottom).</figDesc><graphic coords="3,41,79,54,56,340,21,245,14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Comparison of multiple IMU data. The abscissa of each subfigure indicates the image index.</figDesc><graphic coords="6,55,98,54,32,340,18,143,38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Metric decline curves (with 95% confidence interval). Red: MoNet, Blue: OSC-Net. The abscissa and ordinate of each subfigure represent the number of iterations and the value of metrics, respectively. (Color figure online)</figDesc><graphic coords="7,42,30,318,35,339,79,108,43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Typical reconstruction cases. Top: comparison of different methods, Bottom: comparison of different scanning tactics. At the bottom, all of the estimated image positions of OSCNet are marked with red boxes to visualize the scanning tactics. (Color figure online)</figDesc><graphic coords="8,55,98,208,70,340,51,332,68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The mean (std) results of different models on the arm and carotid scans. DC 2 : DC 2 -Net, Bk: Backbone. The best results are shown in blue.</figDesc><table><row><cell>Models</cell><cell>FDR(%)↓ ADR(%)↓ MD(mm)↓ SD(mm)↓</cell><cell>HD(mm)↓ EA(deg)↓</cell></row><row><cell></cell><cell>Arm scans</cell><cell></cell></row><row><cell>CNN [14]</cell><cell cols="2">23.31(13.0) 32.54(13.7) 67.79(30.0) 2313.08(1852.5) 62.48(31.5) 4.35(1.8)</cell></row><row><cell>DC 2 [3]</cell><cell cols="2">14.02(7.3) 26.15(10.2) 45.50(21.3) 1560.30(1181.4) 42.25(40.9) 4.71(2.3)</cell></row><row><cell cols="3">MoNet [12] 11.58(6.2) 20.35(6.8) 32.21(11.0) 1205.42(742.6) 31.03(11.3) 3.98(1.5)</cell></row><row><cell>Bk</cell><cell cols="2">13.32(8.2) 23.21(9.6) 36.39(13.6) 1339.17(822.4) 34.91(13.7) 4.32(1.7)</cell></row><row><cell>Bk+MSS</cell><cell cols="2">10.78(5.6) 19.53(6.3) 30.52(10.5) 1142.42(636.8) 29.32(10.8) 3.18(1.4)</cell></row><row><cell>Bk+SCS</cell><cell cols="2">10.56(5.9) 19.57(6.6) 29.84(11.1) 1126.28(614.9) 28.64(11.6) 3.65(1.9)</cell></row><row><cell>OSCNet</cell><cell cols="2">10.01(5.7) 18.86(6.5) 28.61(11.0) 1064.06(582.5) 27.38(11.4) 2.76(1.3)</cell></row><row><cell></cell><cell>Carotid scans</cell><cell></cell></row><row><cell>CNN [14]</cell><cell cols="2">25.85(15.0) 33.95(16.8) 49.64(25.5) 1944.72(1485.1) 39.30(18.7) 3.73(2.3)</cell></row><row><cell>DC 2 [3]</cell><cell cols="2">13.54(7.1) 21.68(9.2) 26.47(9.6) 1025.06(622.8) 24.49(10.3) 4.30(3.0)</cell></row><row><cell cols="2">MoNet [12] 11.80(5.7) 20.42(8.8) 23.48(8.7) 894.39(381.0)</cell><cell>20.78(9.2) 3.67(2.1)</cell></row><row><cell>Bk</cell><cell>12.85(6.5) 21.78(10.5) 24.65(9.1) 965.12(466.6)</cell><cell>21.81(9.5) 3.83(2.0)</cell></row><row><cell>Bk+MSS</cell><cell>11.31(5.4) 20.04(8.8) 22.72(8.1) 850.68(321.7)</cell><cell>20.02(8.5) 3.16(1.8)</cell></row><row><cell>Bk+SCS</cell><cell>11.30(5.4) 20.16(8.6) 23.01(8.4) 863.48(320.9)</cell><cell>20.56(8.7) 3.36(1.8)</cell></row><row><cell>OSCNet</cell><cell>10.90(5.3) 19.61(8.5) 21.81(7.2) 804.27(282.8)</cell><cell>19.30(7.6) 2.60(1.6)</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work was supported by the grant from <rs type="funder">National Natural Science Foundation of China</rs> (Nos. <rs type="grantNumber">62171290</rs>, <rs type="grantNumber">62101343</rs>), <rs type="funder">Shenzhen-Hong Kong Joint Research Program</rs> (No. <rs type="grantNumber">SGDX20201103095613036</rs>), and <rs type="funder">Shenzhen Science and Technology Innovations Committee</rs> (No. <rs type="grantNumber">20200812143441001</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_garbdNt">
					<idno type="grant-number">62171290</idno>
				</org>
				<org type="funding" xml:id="_nMV7WEz">
					<idno type="grant-number">62101343</idno>
				</org>
				<org type="funding" xml:id="_7agRf7k">
					<idno type="grant-number">SGDX20201103095613036</idno>
				</org>
				<org type="funding" xml:id="_s4WCXNj">
					<idno type="grant-number">20200812143441001</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Determination of scan-plane motion using speckle decorrelation: theoretical considerations and initial test</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Carson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Imaging Syst. Technol</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="38" to="44" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Improving accuracy with multiple sensors: study of redundant memsimu/gps configurations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Guerrier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Technical Meeting of the Satellite Division of the Institute of Navigation (ION GNSS 2009)</title>
		<meeting>the 22nd International Technical Meeting of the Satellite Division of the Institute of Navigation (ION GNSS 2009)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="3114" to="3121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Ultrasound volume reconstruction from freehand scans without tracking</title>
		<author>
			<persName><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="970" to="979" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Sensorless freehand 3D ultrasound reconstruction via deep contextual learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yan</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59716-0_44</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59716-0_44" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12263</biblScope>
			<biblScope unit="page" from="463" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Contrastive representation learning: a framework and review</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Le-Khac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Smeaton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="193907" to="193934" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A method for the solution of certain non-linear problems in least squares</title>
		<author>
			<persName><forename type="first">K</forename><surname>Levenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Q. Appl. Math</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="164" to="168" />
			<date type="published" when="1944">1944</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Peripheral-free calibration method for redundant IMUs based on array-based consumer-grade MEMS information fusion</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Micromachines</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">1214</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Self context and shape prior for sensorless freehand 3D ultrasound reconstruction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Luo</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87231-1_20</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87231-1_20" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12906</biblScope>
			<biblScope unit="page" from="201" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">RecON: online learning for sensorless freehand 3D ultrasound reconstruction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page">102810</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep motion network for freehand 3D ultrasound reconstruction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ni</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16440-8_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16440-8_28" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13434</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A survey on 3D ultrasound reconstruction techniques</title>
		<author>
			<persName><forename type="first">F</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">V</forename><surname>Siang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence, chap. 4. IntechOpen</title>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Aceves-Fernandez</surname></persName>
		</editor>
		<meeting><address><addrLine>Rijeka</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">3d freehand ultrasound without external tracking using deep learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Prevost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="187" to="202" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep learning for sensorless 3D freehand ultrasound imaging</title>
		<author>
			<persName><forename type="first">R</forename><surname>Prevost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Salehi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sprung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ladikos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wein</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-66185-8_71</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-66185-8_71" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2017</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Descoteaux</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Maier-Hein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Franz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Jannin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Collins</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Duchesne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">10434</biblScope>
			<biblScope unit="page" from="628" to="636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Implementation of a low-cost multi-IMU hardware by using a homogenous multi-sensor fusion</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rasoulzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Shahri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 4th International Conference on Control, Instrumentation, and Automation (ICCIA)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="451" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automated threedimensional us frame positioning computed from elevational speckle decorrelation</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Tuthill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krücker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Carson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiology</title>
		<imprint>
			<biblScope unit="volume">209</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="575" to="582" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
