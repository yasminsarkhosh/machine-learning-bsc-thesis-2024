<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PROnet: Point Refinement Using Shape-Guided Offset Map for Nuclei Instance Segmentation</title>
				<funder ref="#_WfyYH2x">
					<orgName type="full">Korean government (MSIT)</orgName>
				</funder>
				<funder>
					<orgName type="full">National Research Foundation of Korea</orgName>
					<orgName type="abbreviated">NRF</orgName>
				</funder>
				<funder>
					<orgName type="full">IITP</orgName>
				</funder>
				<funder ref="#_YWz5wgc">
					<orgName type="full">Korean Government (MSIT)</orgName>
				</funder>
				<funder ref="#_cBuGvDU">
					<orgName type="full">Ministry of Science and ICT of KOREA</orgName>
				</funder>
				<funder ref="#_PJ3QHwb">
					<orgName type="full">Korean National Police Agency</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Siwoo</forename><surname>Nam</surname></persName>
							<email>siwoonam@dgist.ac.kr</email>
						</author>
						<author>
							<persName><forename type="first">Jaehoon</forename><surname>Jeong</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Miguel</forename><surname>Luna</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Philip</forename><surname>Chikontwe</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sang</forename><forename type="middle">Hyun</forename><surname>Park</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Robotics and Mechatronics Engineering</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Daegu Gyeongbuk Institute of Science and Technology (DGIST)</orgName>
								<address>
									<settlement>Daegu</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">PROnet: Point Refinement Using Shape-Guided Offset Map for Nuclei Instance Segmentation</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="528" to="538"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">85CF4B98AEC71225B1C5262B38B7D6F2</idno>
					<idno type="DOI">10.1007/978-3-031-43907-0_51</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T12:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Weakly Supervised Nuclei Segmentation</term>
					<term>Instance Segmentation</term>
					<term>Point Refinement</term>
					<term>Offset Map</term>
					<term>Geodesic Distance</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, weakly supervised nuclei segmentation methods using only points are gaining attention, as they can ease the tedious labeling process. However, most methods often fail to separate adjacent nuclei and are particularly sensitive to point annotations that deviate from the center of nuclei, resulting in lower accuracy. In this study, we propose a novel weakly supervised method to effectively distinguish adjacent nuclei and maintain robustness regardless of point label deviation. We detect and segment nuclei by combining a binary segmentation module, an offset regression module, and a center detection module to determine foreground pixels, delineate boundaries and identify instances. In training, we first generate pseudo binary masks using geodesic distancebased Voronoi diagrams and k-means clustering. Next, segmentation predictions are used to repeatedly generate pseudo offset maps that indicate the most likely nuclei center. Finally, an Expectation Maximization (EM) based process iteratively refines initial point labels based on the offset map predictions to fine-tune our framework. Experimental results show that our model consistently outperforms state-of-the-art methods on public datasets regardless of the point annotation accuracy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Nuclei segmentation in histopathology images is an important task for cancer diagnosis and immune response prediction <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b18">18]</ref>. While several fully supervised deep learning approaches to segment nuclei exist <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b25">25]</ref>, labeling S. Nam and J. Jeong-Equal contribution.</p><p>thousands of instances are tedious and the ambiguous nature of nuclei boundaries requires high-level expert annotators. To address this, weakly-supervised nuclei segmentation methods <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b28">28]</ref> have emerged as an attractive alternative using cheap and inexact labels e.g., center point annotations. As point labels alone do not provide sufficient foreground information, it is common to use Euclidean distance-based Voronoi diagrams and k-means clustering <ref type="bibr" target="#b7">[7]</ref> to generate pseudo segmentation labels for training. However, since Euclidean distance-based schemes only use distance information while ignoring color, they often fail to capture nuclei shape information; resulting in inadequate boundary delineation between adjacent nuclei. Moreover, prior methods <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b22">22]</ref> typically assume that point labels are located precisely at the center of the nuclei. In real-world scenarios, point annotation locations may shift from nuclei centers as a result of the expert labeling process, leading to a lower performance after model training.</p><p>To overcome these challenges, we propose a novel weakly supervised instance segmentation method that effectively distinguishes adjacent nuclei and is robust to point shifts. The proposed model consists of three modules responsible for binary segmentation, boundary delineation, and instance separation. To train the binary segmentation module, we generate pseudo binary segmentation masks using geodesic distance-based Voronoi labels and cluster labels from point annotations. Geodesic distance provides more precise nuclei shape information than previous Euclidean distance-based schemes. To train the offset map module, we generate pseudo offset maps by computing the offset distance between binary segmentation pixel predictions and the point label. The offset information facilitates precise delineation of the boundaries between adjacent nuclei. To make the model robust to center point shifts, we introduce an Expectation Maximization (EM) <ref type="bibr" target="#b4">[4]</ref> algorithm-based process to refine point labels. Note that previous approaches <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b22">22]</ref> optimize model parameters only using a fixed set of point labels, while we instead alternatively update model parameters and the center point locations. This refinement process ensures that the model maintains high performance even when the point annotation is not exactly located at the center of the nuclei.</p><p>The contributions of this paper are as follows: <ref type="bibr" target="#b0">(1)</ref> We propose an end-toend weakly supervised segmentation model that simultaneously predicts binary mask, offset map, and center map to accurately identify and segment nuclei.</p><p>(2) By utilizing geodesic distance, we produce more detailed Voronoi and cluster labels that precisely delineate the boundary between adjacent nuclei. (3) We introduce an EM algorithm-based refinement process to encourage model robustness on center-shifted point labels. (4) Ablation and evaluation studies on two public datasets demonstrate our model's ability to outperform state-of-the-art techniques not only with ideal labels but also with shifted labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>We propose an end-to-end nuclei segmentation method that only uses point annotations P to predict nuclei instance segmentation masks Åœ. The proposed Fig. <ref type="figure">1</ref>. Overview of the proposed method. It consists of an encoder and three modules for binary segmentation, offset map and center map prediction. To train offset map and center map modules(blue lines), pseudo labels are generated using point label and predicted binary segmentation mask(green lines). During inference, the instance map, obtained by predicted offset map and center map, is multiplied with predicted binary mask to produce instance segmentation prediction(orange lines). (Color figure online) model consists of three modules: 1) binary segmentation module, 2) offset map module, and 3) center map module (Fig. <ref type="figure">1</ref>). For a given input image, we extract feature maps with an ImageNet-pretrained VGG16 backbone encoder. The feature maps are further processed through a series of residual units (RUs) and attention units (AUs) to predict a binary segmentation mask B, an offset map Ã”, and a center map Äˆ. The RUs are employed to maintain feature information so that subsequent modules can reuse the features from early-stage modules. In contrast, the AUs are used to refine the features of initial modules by using the predictions of later modules. In particular, the AUs use the point predictions to refine the features in the offset module, and the offset predictions to refine the features in the binary module.</p><p>In the training stage, we first generate a Voronoi label V and a cluster label K along the green lines in Fig. <ref type="figure">1</ref> to train the segmentation module. Then, we generate the pseudo offset map O by using B and P . Next, following <ref type="bibr" target="#b29">[29]</ref>, we generate the center map C by expanding the point label P with Gaussian kernel within a radius r. Herein, our model is trained wih a segmentation loss L B (V,K, B), an offset map loss L O (O, Ã”), and a center map loss L C (C, Äˆ). Note that P can not sufficiently enable model robustness to imprecise point annotations. Thus, we employ an EM algorithm to search the optimal model parameters Î¸ to obtain more reliable points P .</p><p>In the inference stage, B, Ã”, Äˆ are predicted following the orange lines in Fig. <ref type="figure">1</ref>. Then, we generate an instance map I, which shares the same values among the same instances as follows:</p><formula xml:id="formula_0">I(x, y) = arg min i ||(x Äˆi , y Äˆi ) -((x, y) + Ã”(x, y))|| 2 , (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where (x, y) represents a coordinate and (x Äˆi , y Äˆi ) means the location of i th point obtained from Äˆ. Finally, the instance segmentation output Åœ is obtained by B Ã— I. 2.1 Loss Functions Using Pseudo Labels Segmentation Loss. We generate V and K to train the binary segmentation module. In <ref type="bibr" target="#b21">[21]</ref>, V was generated based on Euclidean distance between points without considering color information. As a result, the Voronoi boundaries are often created across nuclei instances, and the offset map's quality was limited. To mitigate this, we instead generate V using Geodesic distance <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b24">24]</ref> by computing distances d i between all center points p i âˆˆ P and pixels. The boundaries of the diagram in V are defined as 0, while center points and the other regions are defined as 1 and 2, respectively. For k-means clustering, we concatenate the RGB values and the geodesic distance value d i truncated by d * to generate the feature vectors</p><formula xml:id="formula_2">f i = (d i , r i , g i , b i ).</formula><p>We cluster f into three clusters (0 for background, 1 for foreground, and 2 for ignore) to generate K (Fig. <ref type="figure" target="#fig_0">2d</ref>). To train the binary segmentation module using V and K, we employ a Voronoi loss L V and a cluster loss L K based on the cross-entropy:</p><formula xml:id="formula_3">L V = 1 N Î©V x,yâˆˆÎ©V V(x, y)log( B(x, y)) + (1 -V(x, y))log(1 -B(x, y)), L K = 1 N Î©K x,yâˆˆÎ©K K(x, y)log( B(x, y)) + (1 -K(x, y))log(1 -B(x, y)), (<label>2</label></formula><formula xml:id="formula_4">)</formula><p>where Î© V and Î© K are the set of foreground and background pixels in V and K, N Î©V and N Î©K denote the cardinality of Î© V and Î© K . Following <ref type="bibr" target="#b17">[17]</ref>, we define the final segmentation loss as</p><formula xml:id="formula_5">L B = L V + L K .</formula><p>Center Map Loss. To achieve instance-level predictions, we introduce a center map module. The module predicts a keypoint heatmap Äˆ âˆˆ [0, 1] W Ã—H where Äˆ = 1 identifies nuclei centers and Äˆ = 0 for other pixels. W and H are the width and height of the input image. To train the module we employ a focal loss, commonly used in point detection problems. This loss can focus on a set of sparse hard examples while preventing easy negatives from dominating the model <ref type="bibr" target="#b16">[16]</ref>:</p><formula xml:id="formula_6">L C = -1 N P x,y (1 -Äˆ(x, y)) Î± log( Äˆ(x, y)) if C(x, y) = 1 (1 -C(x, y)) Î² ( Äˆ(x, y)) Î± log(1 -Äˆ(x, y)) otherwise,<label>(3)</label></formula><p>where N P denotes the number of point labels. We set the focal loss hyperparameters Î± = 2 and Î² = 4 following <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b29">29]</ref>. By placing the center map module at the end of the model, the model is able to retain center point information along the RUs, so that each module can inherently reflect the information into their predictions. Offset Map Loss. We employ an offset map module that considers the shape of each nucleus to improve boundary detection. Inspired by <ref type="bibr" target="#b1">[2]</ref>, we define an offset vector O(x, y) that indicates the displacement of a point (x, y) to the center of its corresponding nucleus. To train the offset module, we first compute O(x, y) of each nucleus segmented by B. Then, L O is defined as an L1 loss:</p><formula xml:id="formula_7">L O = 1 W Ã— H x,y |O(x, y) -Ã”(x, y)|. (<label>4</label></formula><formula xml:id="formula_8">)</formula><p>It is worth noting that in the early stages of training, the pseudo offset map O generated by B and P is unreliable. Thus, we empirically use L O for backpropagation after 20 epochs. We optimize the entire model using the loss</p><formula xml:id="formula_9">L = Î» B L B + Î» O L O + Î» C L C</formula><p>, where Î» B , Î» O and Î» C denote loss weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Refinement via Expectation Maximization Algorithm</head><p>Training with nuclei (center) shifted point labels can lead to blurry center map predictions (see Fig. <ref type="figure" target="#fig_1">3c</ref>). This in turn limits model optimization and it's ability to distinguish objects, resulting in poor adjacent nuclei segmentation. To address this, we propose an EM based center point refinement process. Instead of the standard fixed-point label based model optimization, we alternatively optimize both model parameters and point labels.</p><p>In the E-step, we update the center of each nucleus according to Ã”. We use Ã” to generate refined point labels P , since Ã” is reliable regardless of the point location i.e., center of the nuclei or shifted.</p><formula xml:id="formula_10">p i = arg min x,y | x,È³âˆˆvi B(x, y) Ã— Ã”(x + x, y + È³)|,<label>(5)</label></formula><p>where v i is i th Voronoi region and p i is the refined center point. We repeat this for all Voronoi regions to obtain P , and replace P with P if the distance between them is &lt; Î´. In the M -step of iteration n, we generate C by adapting the Gaussian mask to P , and then use it to train offset and center map modules. As maximizing a probability distribution is the same as minimizing the loss, the model parameter Î¸ minimizing L is optimized as:</p><formula xml:id="formula_11">Î¸ n := arg min Î¸ (L(Î¸ n ; Î¸ n-1 , X, V, K, O, C )).<label>(6)</label></formula><p>Since reliable Ã” is necessary to refine nuclei centers, refinement starts after 30 epochs. E and M steps are alternately repeated to correct imprecise annotations bringing them closer to the real nuclei center points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>Dataset. To validate the effectiveness of our model, we use two public nuclei segmentation datasets i.e., CPM17 <ref type="bibr" target="#b26">[26]</ref> &amp; MoNuSeg <ref type="bibr" target="#b12">[12]</ref>. CPM17 contains 64 H&amp;E stained images with 7,570 annotated nuclei boundaries sized from 500Ã—500 to 600Ã—600. The set is split into 32/32 images for training and testing. Images were normalized and cropped to 300Ã—300. MoNuSeg is a multi-organ nuclei segmentation dataset consisting of 30 H&amp;E stained images (1000Ã—1000) extracted from seven different organs. We used 16 images (4 images from the breast, liver, kidney, and prostate) as training and 14 images (2 images from each breast, liver, kidney, prostate, bladder, brain, and stomach) as testing. For a fair comparison, images were pre-processed before training/testing i.e., normalized and cropped to 250Ã—250 patches following the setting used in <ref type="bibr" target="#b17">[17]</ref>.</p><p>To make point labels, we use the center point of full mask annotations. For a realistic scenario, we generate shifted point label. The shift is performed in pixels and is randomly selected between the minimum and maximum values.</p><p>Implementation Details. For training, all evaluated models were run for 150 epochs with the Adam optimizer <ref type="bibr" target="#b11">[11]</ref> using a learning rate of 1e-4, weight decay of 3e-2, and batch size of 4. The GeodisTK <ref type="bibr" target="#b27">[27]</ref> library was used to compute geodesic distances. For clustering, we set the maximum distance d * as 90 and 70 on CPM17 and MoNuSeg, respectively. The Gaussian kernel r was set as r = 6 and Î´ was set as 8 for refinement on CPM17. For MoNuSeg, r = 8 and Î´ = 8, respectively. A threshold of 0.2 was applied to eliminate the noise and find important points in Äˆ. Finally, a variety of augmentations were employed i.e., random resizing, cropping, and rotations etc., following <ref type="bibr" target="#b17">[17]</ref>, with loss weights Î» B , Î» O and Î» C empirically set to 1. We used a NVIDIA RTX A5000 GPU and PyTorch version 1.7.1.  <ref type="bibr" target="#b21">[21]</ref> 75.0 55.5 75.3 56.9 74.4 53.7 72.2 49.9 70.1 44.9 69.9 45.0 66.3 39.9 61.0 31.5 Mixed Anno <ref type="bibr" target="#b22">[22]</ref> 75.3 53.2 75.9 55.5 73.3 52.3 73.1 49.9 73.3 51.6 72.0 49.4 66.0 40.5 66.9 41.8 SPN+IEN <ref type="bibr" target="#b17">[17]</ref> 74. Main Results. Table <ref type="table" target="#tab_0">1</ref> shows the performance of our method against stateof-the-art weakly supervised nuclei segmentation methods <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b22">22]</ref> based on Dice and Aggregated Jaccard Index (AJI) metrics. As opposed to the Dice score, AJI is key when evaluating adjacent nuclei separation in instance segmentation tasks. On CPM17, our method outperformed the prior approach by a large margin of +3.4% in Dice and +7.2% in AJI when the point label is located at the nuclei center. More importantly, our approach surpassed prior approaches by a substantial margin when the shift exists. We obtain statistically significant (p-value &lt;0.05) for the AJI of all comparison methods on two datasets in all scenarios. Regarding refinement, we observed that our strategy is more beneficial when points exhibit significant shifts i.e., on both CPM and MoNuSeg. Figure <ref type="figure" target="#fig_1">3</ref> showcases the effectiveness of the refinement process wherein the model generates precise instance and center maps. With the geodesic distance and the refinement process, our proposed method achieved state-of-the-art performance. This demonstrates that our method separates adjacent nuclei accurately, and maintains its robustness, achieving consistent performance even when the point annotations are not located at the center of the nuclei. Additionally, in Fig. <ref type="figure" target="#fig_2">4</ref>, we qualitatively show the results to highlight how our method precisely separates adjacent nuclei.  Ablation Studies. We conducted ablation studies to assess the impact of the offset regression module, geodesic distance, and point refinement process (Table <ref type="table" target="#tab_2">2</ref>). When the binary segmentation module is combined only with the center map module without the offset module, the model could separate nuclei only trained by the ideal label. On the other hand, since there was no refinement process due to the absence of the offset map, inaccurate points extracted from the center map are obtained in the real-world scenario. We also demonstrate that labels with Geodesic distance help improve overall performance. This is because it creates confident labels and more decent divides the boundaries between nuclei. Finally, using the full set of modules along with a complete instance map, the model was able to separate adjacent nuclei with precise boundaries, ultimately reporting higher scores. These findings validate the utility of the center map and offset map modules i.e., they synergistically facilitate precise instance delineation and nuclei boundary prediction. The geodesic distance and refinement process also improved the accuracy by contributing to more accurate pseudo labels. Especially, most variants show a significant drop in performance when the annotations shift was over 4 pixels. Compared to other variants, our proposed model is more robust to the point shift in both datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this work, we proposed a novel and robust framework for weakly supervised nuclei segmentation. We demonstrated the effectiveness of geodesic distancebased Voronoi diagrams and k-means clustering to generate accurate pseudo binary segmentation labels. This allowed us to generate reliable pseudo offset maps, and then we iteratively improve the pseudo offset maps that facilitate the precise separation of adjacent nuclei as well as progressively refine the location of the center point labels. According to our experimental results, we established a new state-of-art on two publicly available datasets across different levels of point annotation imperfections. We believe being able to use low-precision point annotations while retaining good segmentation performance is an essential step for automatic nuclei segmentation models to become a widespread tool in realworld clinical practice.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Visualization of cluster label on CPM17(left) and MoNuSeg(right). (a) Input image; (b) ground truth; (c) the cluster labels generated by Euclidean distance, and (d) those by Geodesic distance. The green, red, and black colors are foreground, background, and ignored, respectively. (Color figure online)</figDesc><graphic coords="4,57,48,178,58,337,36,56,80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. (a) Input image (top) and ground truth (bottom), (b) Instance map (top) and center map (bottom) generated by the optimal nuclei center points, (c) those by shifted points (6-8), and (d) those by refined points.</figDesc><graphic coords="5,56,31,270,86,311,32,163,48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Nuclei instance segmentation results on CPM17 (top 2 rows) and MoNuSeg (bottom 2 rows) images. The images and the ground truth (GT) are shown in the left column. The results using the precise point annotations are shown in i), while those using shifted (6-8) points are shown in ii). (a) PROnet (ours), (b) SPN+IEN<ref type="bibr" target="#b17">[17]</ref>, (c) Mixed Anno<ref type="bibr" target="#b22">[22]</ref> and (d) MIDL<ref type="bibr" target="#b21">[21]</ref>. The yellow circles indicate the major differences.</figDesc><graphic coords="8,79,47,176,93,294,04,236,08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="3,43,29,53,90,337,36,218,80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Performance comparison of nuclei segmentation on two public datasets. Shift indicates the number of pixels point annotations deviate from the nuclei center. Dice AJI Dice AJI Dice AJI Dice AJI Dice AJI Dice AJI Dice AJI Dice AJI MIDL</figDesc><table><row><cell>CPM17</cell><cell></cell><cell></cell><cell></cell><cell>MoNuSeg</cell><cell></cell><cell></cell><cell></cell></row><row><cell>shift0</cell><cell>shift2-4</cell><cell>shift4-6</cell><cell>shift6-8</cell><cell>shift0</cell><cell>shift2-4</cell><cell>shift4-6</cell><cell>shift6-8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Evaluation on the effect of offset and center maps. Dice AJI Dice AJI Dice AJI Dice AJI Dice AJI Dice AJI Dice AJI Dice AJI</figDesc><table><row><cell cols="4">offset geo refine CPM17</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>MoNuSeg</cell></row><row><cell></cell><cell></cell><cell></cell><cell>shift0</cell><cell>shift2-4</cell><cell>shift4-6</cell><cell cols="2">shift6-8</cell><cell>shift0</cell><cell>shift2-4</cell><cell>shift4-6</cell><cell>shift6-8</cell></row><row><cell>x</cell><cell>x</cell><cell>x</cell><cell cols="4">78.0 62.2 77.4 61.2 52.6 38.1 -</cell><cell>-</cell><cell>69.6 45.9 68.3 43.8 67.1 40.5 64.0 38.4</cell></row><row><cell>x</cell><cell>o</cell><cell>x</cell><cell cols="6">78.5 62.7 78.0 61.8 59.7 43.8 59.5 42.8 73.6 54.2 73.6 54.2 68.0 42.6 64.0 38.5</cell></row><row><cell>o</cell><cell>x</cell><cell>x</cell><cell cols="6">77.9 61.8 77.4 60.3 74.0 56.2 67.8 48.7 74.5 55.0 73.4 52.7 71.9 49.3 66.4 39.8</cell></row><row><cell>o</cell><cell>o</cell><cell>x</cell><cell cols="6">78.3 62.5 78.0 61.5 75.2 58.0 74.1 55.2 75.0 55.3 74.2 54.4 72.5 52.0 67.5 42.0</cell></row><row><cell>o</cell><cell>x</cell><cell>o</cell><cell cols="6">78.1 61.9 78.1 61.7 76.6 58.4 75.0 55.8 74.6 55.4 74.7 54.7 72.6 50.2 70.3 47.4</cell></row><row><cell>o</cell><cell>o</cell><cell>o</cell><cell cols="6">78.7 62.7 78.2 61.8 77.4 60.7 77.0 60.2 75.0 55.5 74.8 54.8 73.3 53.2 72.5 50.9</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgment. This work was supported by <rs type="funder">IITP</rs> grant funded by the <rs type="funder">Korean government (MSIT)</rs> (No.<rs type="grantNumber">2021-0-02068</rs>, <rs type="projectName">Artificial Intelligence Innovation Hub</rs>), the <rs type="funder">National Research Foundation of Korea (NRF)</rs> grant funded by the <rs type="funder">Korean Government (MSIT)</rs> (No. <rs type="grantNumber">2019R1C1C1008727</rs>), <rs type="programName">Smart Health Care Program</rs> funded by the <rs type="funder">Korean National Police Agency</rs> (<rs type="grantNumber">220222M01</rs>), <rs type="programName">DGIST R&amp;D program</rs> of the <rs type="funder">Ministry of Science and ICT of KOREA</rs> (<rs type="grantNumber">21-DPIC-08</rs>)</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_WfyYH2x">
					<idno type="grant-number">2021-0-02068</idno>
					<orgName type="project" subtype="full">Artificial Intelligence Innovation Hub</orgName>
				</org>
				<org type="funding" xml:id="_YWz5wgc">
					<idno type="grant-number">2019R1C1C1008727</idno>
					<orgName type="program" subtype="full">Smart Health Care Program</orgName>
				</org>
				<org type="funding" xml:id="_PJ3QHwb">
					<idno type="grant-number">220222M01</idno>
					<orgName type="program" subtype="full">DGIST R&amp;D program</orgName>
				</org>
				<org type="funding" xml:id="_cBuGvDU">
					<idno type="grant-number">21-DPIC-08</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0_51.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A bottom-up approach for tumour differentiation in whole slide images of lung adenocarcinoma</title>
		<author>
			<persName><forename type="first">N</forename><surname>Alsubaie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sirinukunwattana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E A</forename><surname>Raza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Snead</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Rajpoot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Digital Pathology</title>
		<imprint>
			<biblScope unit="volume">10581</biblScope>
			<biblScope unit="page" from="104" to="113" />
			<date type="published" when="2018">2018. 2018</date>
			<publisher>SPIE</publisher>
		</imprint>
	</monogr>
	<note>Medical Imaging</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Panoptic-DeepLab: a simple, strong, and fast baseline for bottomup panoptic segmentation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="12475" to="12485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">GeoS: geodesic image segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2008</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Torr</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">5302</biblScope>
			<biblScope unit="page" from="99" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-540-88682-2_9</idno>
		<ptr target="https://doi.org/10.1007/978-3-540-88682-2_9" />
		<imprint>
			<date type="published" when="2008">2008</date>
			<pubPlace>Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the EM algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Royal Stat. Soc.: Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Towards neuron segmentation from macaque brain images: a weakly supervised approach</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dong</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59722-1_19</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59722-1_19" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12265</biblScope>
			<biblScope unit="page" from="194" to="203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hover-Net: simultaneous segmentation and classification of nuclei in multi-tissue histology images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Graham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page">101563</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Algorithm as 136: a k-means clustering algorithm</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Hartigan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Royal Stat. Soc. Ser. c (Applied Statistics)</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="100" to="108" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">CDNet: centripetal direction network for nuclear instance segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4026" to="4035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-task learning using uncertainty to weigh losses for scene geometry and semantics</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7482" to="7491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Simple does it: weakly supervised instance and semantic segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="876" to="885" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: a method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A multi-organ nucleus segmentation challenge</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1380" to="1391" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A dataset and a technique for generalized nuclear segmentation for computational pathology</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vahadane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sethi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1550" to="1560" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">CornerNet: detecting objects as paired keypoints</title>
		<author>
			<persName><forename type="first">H</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="734" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">ScribbleSup: scribble-supervised convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3159" to="3167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>DollÃ¡r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Weakly supervised nuclei segmentation via instance learning</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 19th International Symposium on Biomedical Imaging (ISBI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022. 2022</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Nuclear shape and orientation features from h&amp;e images predict survival in early-stage estrogen receptor-positive breast cancers</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lab. Investig</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1438" to="1448" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Instance segmentation by jointly optimizing spatial embeddings and clustering bandwidth</title>
		<author>
			<persName><forename type="first">D</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Brabandere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8837" to="8845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Weakly supervised cell instance segmentation by propagating from detection response</title>
		<author>
			<persName><forename type="first">K</forename><surname>Nishimura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F E</forename><surname>Ker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bise</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Weakly supervised deep nuclei segmentation using points annotation in histopathology images</title>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Imaging with Deep Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="390" to="400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Nuclei segmentation using mixed points and masks selected from uncertainty</title>
		<author>
			<persName><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="973" to="976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Weakly-supervised nucleus segmentation based on point annotations: a coarse-to-fine self-stimulated learning strategy</title>
		<author>
			<persName><forename type="first">K</forename><surname>Tian</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59722-1_29</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59722-1_29" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12265</biblScope>
			<biblScope unit="page" from="299" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">New geodosic distance transforms for gray-scale images</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Toivanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn. Lett</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="437" to="450" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Box2Pix: single-shot instance segmentation by assigning pixels to object boxes</title>
		<author>
			<persName><forename type="first">J</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rehder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>FrÃ¶hlich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="292" to="299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Methods for segmentation and classification of digital microscopy tissue images</title>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">D</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Graham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front. Bioeng. Biotech</title>
		<imprint>
			<biblScope unit="page">53</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">DeepiGeoS: a deep interactive geodesic framework for medical image segmentation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2018.2840695</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2018.2840695" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1559" to="1572" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">PseudoEdgeNet: nuclei segmentation only with point annotations</title>
		<author>
			<persName><forename type="first">I</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Paeng</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32239-7_81</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32239-7_81" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11764</biblScope>
			<biblScope unit="page" from="731" to="739" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>KrÃ¤henbÃ¼hl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<title level="m">Objects as points</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
