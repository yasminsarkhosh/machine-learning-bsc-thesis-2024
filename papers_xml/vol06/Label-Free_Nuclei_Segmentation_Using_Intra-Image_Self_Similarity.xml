<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Label-Free Nuclei Segmentation Using Intra-Image Self Similarity</title>
				<funder ref="#_YVbKFXy">
					<orgName type="full">Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_Z38WWYM">
					<orgName type="full">Open Fund Project of Guangdong Academy of Medical Sciences, China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Long</forename><surname>Chen</surname></persName>
							<email>chenlong171@mails.ucas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country>People&apos;s Republic of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Han</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">School of Biomedical Engineering</orgName>
								<orgName type="department" key="dep2">Division of Life Sciences and Medicine</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<postCode>230026</postCode>
									<settlement>Hefei, Anhui</settlement>
									<country>People&apos;s Republic of China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Center for Medical Imaging</orgName>
								<orgName type="laboratory">Analytic Computing and Learning (MIRACLE)</orgName>
								<orgName type="institution">Suzhou Institute for Advanced Research</orgName>
								<address>
									<settlement>Robotics</settlement>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<postCode>215123</postCode>
									<settlement>Suzhou, Jiangsu</settlement>
									<country>People&apos;s Republic of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">S</forename><forename type="middle">Kevin</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country>People&apos;s Republic of China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">School of Biomedical Engineering</orgName>
								<orgName type="department" key="dep2">Division of Life Sciences and Medicine</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<postCode>230026</postCode>
									<settlement>Hefei, Anhui</settlement>
									<country>People&apos;s Republic of China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Center for Medical Imaging</orgName>
								<orgName type="laboratory">Analytic Computing and Learning (MIRACLE)</orgName>
								<orgName type="institution">Suzhou Institute for Advanced Research</orgName>
								<address>
									<settlement>Robotics</settlement>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<postCode>215123</postCode>
									<settlement>Suzhou, Jiangsu</settlement>
									<country>People&apos;s Republic of China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Label-Free Nuclei Segmentation Using Intra-Image Self Similarity</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="673" to="682"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">8095B43285895C446054F0F0CA108104</idno>
					<idno type="DOI">10.1007/978-3-031-43987-2_65</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T11:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Label-free</term>
					<term>Nuclei segmentation</term>
					<term>Pseudo Label</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In computational pathology, nuclei segmentation from histology images is a fundamental task. While deep learning based nuclei segmentation methods yield excellent results, they rely on a large amount of annotated images; however, annotating nuclei from histology images is tedious and time-consuming. To get rid of labeling burden completely, we propose a label-free approach for nuclei segmentation, motivated from one pronounced yet omitted property that characterizes histology images and nuclei: intra-image self similarity (IISS), that is, within an image, nuclei are similar in their shapes and appearances. First, we leverage traditional machine learning and image processing techniques to generate a pseudo segmentation map, whose connected components form candidate nuclei, both positive or negative. In particular, it is common that adjacent nuclei are merged into one candidate due to imperfect staining and imaging conditions, which violate the IISS property. Then, we filter the candidates based on a custom-designed index that roughly measures if a candidate contains multiple nuclei. The remaining candidates are used as pseudo labels, which we use to train a U-Net to discover the hierarchical features distinguish nuclei pixels from background. Finally, we apply the learned U-Net to produce final nuclei segmentation. We validate the proposed method on the public dataset MoNuSeg. Experimental results demonstrate the effectiveness of our design and, to the best of our knowledge, it achieves the state-of-the-art performances of label-free segmentation on the benchmark MoNuSeg dataset with a mean Dice score of 79.2%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Nuclei segmentation is a fundamental step in histology image analysis. In recent advances, with a large amount of labeled data, fully-supervised learning methods can easily achieve reasonable results <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref>. However, accurate pixel-level annotation of nuclei is not always accessible for segmentation labeling is a laborintensive and time-consuming procedure. Methods to relieve the high dependency on the accurate annotations of nuclei are highly needed.</p><p>Unsupervised learning (UL) methods achieved great success in the data dependency problem for nuclei segmentation, which learns from the structural properties in the data without any manual annotations. Based on the character of these methods, we can group them into two categories: the traditional UL methods and the deep learning UL methods. Traditional UL nuclei segmentation methods include watershed <ref type="bibr" target="#b5">[6]</ref>, contour detection <ref type="bibr" target="#b6">[7]</ref>, clustering <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref> and random field <ref type="bibr" target="#b9">[10]</ref>. These methods focus on either pixel value or shape information but fail to take advantage of both of them. Moreover, due to the heavily rely on preset parameters, these traditional methods also show weak robustness.</p><p>Therefore, some researchers <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref> resort to deep UL segmentation models to better utilize both pixel value and shape information and develop a robust approach. The common and effective way is to employ image clustering by maximizing mutual information between image and predicted labels to distinguish foreground and background regions. Many image-clustering-based deep UL methods for natural tasks still achieve strong performances in nuclei segmentation. Kanezaki et al. <ref type="bibr" target="#b10">[11]</ref> constrain a convolutional neural network (CNN) with superpixel level segmentation results. Ji et al. <ref type="bibr" target="#b11">[12]</ref> propose the invariant information clustering. While reasonable results are obtained, these deep clustering-based methods still suffer difficulties: (i) Poor segmentation of the regions between adjacent nuclei. Deep clustering models succeed in transferring images to highdimensional feature space and obtaining image segmentation results by means of clustering pixels' features. However, the regions between adjacent nuclei are similar to the nuclei regions in terms of color values and textures (as shown in Fig. <ref type="figure" target="#fig_0">1</ref>). Deep clustering-based methods experience difficulties in dealing with these regions due to the lack of supervision. (ii) Underutilization of intra-image self similarity (IISS) information. As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, in terms of value, shape and texture, nuclei show a similar appearance within the same image but vary greatly among different images <ref type="foot" target="#foot_0">1</ref> . This phenomenon offers valuable information for networks to use but the current clustering models do not take this into account.</p><p>To address the above issues and motivated by the IISS property, we hereby propose a novel self-similarity-driven segmentation network (SSimNet) for unsupervised nuclei segmentation. As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, instead of designing complex discriminative network architectures, our framework derives knowledge from the IISS property to aid the segmentation. Specifically, we obtain candidate nuclei with some unsupervised image processing. For the obtained candidates, it is common that adjacent nuclei merged into one candidate due to imperfect staining and low image quality, which violate the IISS property. Hence, we filter the candidates based on a custom-designed index that roughly measures if a candidate contains multiple nuclei. The remaining candidates are used as pseudo labels, which we use to train a U-Net (aka SSimNet) to discover the hierarchical features that distinguish nuclei pixels from the background. Finally, we apply the learned SSimNet to produce the final nuclei segmentation.</p><p>To validate the effectiveness of our method, we conduct extensive experiments on the MoNuSeg dataset <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref> based on ten existing unsupervised segmentation methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref>. Our method outperforms all comparison methods with an average Dice score of 0.792 and aggregated Jaccard index of 0.498 on the MoNuSeg dataset which is close to the supervised method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, our SSimNet aims at unsupervised segmentation of nuclei from histology images. Specifically, by using a matrix factorization on hematoxylin and eosin (H&amp;E) stained histology images, we get the hematoxylin channel image for clustering, active contour refining and softening to generate the final soft candidate label. Then according to the designed unsupervised evaluation metric driven from the IISS property, an SSimNet is trained with highlyrated soft pseudo labels and corresponding original patches. Last, while testing on the test image, to adapt the network to learn nucleus similarity within the same image, we fine tune the network with soft pseudo labels of some patches in current test images. In the following, we elaborate on each part in detail. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Candidate Nucleus Generation</head><p>Channel Decomposition. Suppose that we are given a training set I S = {I S i } N i=1 of histopathology images without any manual annotation. For each image, stained tissue colors are results from light attenuation, which depends on the type and amount of dyes that the tissues have absorbed. This property is prescribed by the Beer-Lambert law:</p><formula xml:id="formula_0">V = log(I 0 /I) = W H, (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where I ∈ R 3×n represents the histology image with three color channels and n pixels, I 0 is the illuminating light intensity of sample with I 0 = 255 for 8-bit images in our cases, W ∈ R 3×r is the stain color matrix that encodes the color appearance of each stain with r representing the number of stains, and H ∈ R r×n is the stain density map. In this work, we follow the sparse non-negative matrix factorization in <ref type="bibr" target="#b20">[21]</ref> to get the stain color matrices W = {W i } N i=1 and stain density maps H = {H i } N i=1 for I S . Note that usually histopathology images are stained with H&amp;E and nuclei mainly absorb hematoxylin <ref type="bibr" target="#b21">[22]</ref>; therefore, r = 2. We reconstruct the nuclei stain map with the first channel of W and H:</p><formula xml:id="formula_2">I T = {I T i } N i=1 = {W i [:, 0] • H i [0, :]} N i=1 (2)</formula><p>Clustering and Active Contour. We transform I T into CIELAB color space and invoke the Fuzzy C-Means method (FCM) with 2 clusters to obtain the candidate foreground pixels. To reduce the noise in clustering results, we use active contour method as a smoothing operation to get hard candidate labels:</p><formula xml:id="formula_3">P = {P i } N i=1 = {ActiveContour(F CM(I T i ))} N i=1 (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>Label Smoothing. Since hard label is overconfident at the border of nuclei, which is detrimental to the training of the network, we soften the hard label one by one for each connected component in P i using the following formulation:</p><formula xml:id="formula_5">B[k + 1] = B[k] + P [k] 2 • A (4)</formula><p>where  <ref type="formula">4</ref>), we obtain our soft candidate labels P from P.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Data Purification and SSimNet Learning</head><p>So far, soft candidate labels Pi have been acquired for each image I S i . However, it is common that adjacent nuclei are merged into one candidate due to imperfect staining and imaging conditions, which violate the IISS property. To this, we conduct data purification to build a reliable training set for subsequent learning.</p><p>Data Purification. We sample K patches with overlap from original image I S i . The sampled results are expressed as patch tissue</p><formula xml:id="formula_6">X = {x i } N •K i=1 and patch label Y = {y i } N •K i=1 .</formula><p>We design the Unsupervised Shape Measure Index (USMI) and calculate it using the algorithm in Fig. <ref type="figure" target="#fig_3">3</ref>(left). Based on thresholding the USMI, we obtain pairs (y i , u i ) N •K i=1 . Note that the smaller USMI is, the more the pseudo label conforms to prior knowledge. Sorting these pairs by USMI from the smallest to largest, only maintain the first α%(0 &lt; α &lt; 100) of data pairs as</p><formula xml:id="formula_7">( X = {x u(i) } α•N •K i=1 , Ỹ = {y u(i) } α•N •K i=1</formula><p>). Figure <ref type="figure" target="#fig_3">3</ref>(right) shows a separation of candidates into two groups (yellow and blue) with a typical yellow patch containing merger nuclei and a blue patch containing isolated nuclei.</p><p>To further separate possible adjacent nuclei in a blue patch, we follow <ref type="bibr" target="#b22">[23]</ref> to construct the Voronoi label as in Fig. <ref type="figure" target="#fig_1">2</ref> by setting the center of connected component as 1, constructing Voronoi diagram, setting Voronoi edge as 0, and ignoring other pixels. Then, a Voronoi tri-label set Z can be acquired.</p><p>SSimNet Learning and Finetuning. By denoting our segmentation network as F , our final loss function to supervise the network training can be formulated as:</p><formula xml:id="formula_8">Loss = x∈ X ,ỹ∈ Ỹ,z∈ Z λL BCE (F (x), ỹ) + (1 -λ)L CE (F (x), z),<label>(5)</label></formula><p>where L BCE is the binary cross-entropy loss and L CE is the cross-entropy loss. Also, we can obtain tissue patches and corresponding pseudo labels for each image in the test set termed as</p><formula xml:id="formula_9">SET k = ( X T k , ỸT k , ZT k ).</formula><p>Before evaluation, we first fine tune our network F using SET k for several epochs. As shown in the ablation study, this operation is simple but effective. And this fine tuning process can help the network capture the size and shape information in the current test slice. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets and Settings</head><p>MoNuSeg. Multi-organ nuclei segmentation <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref> (MoNuSeg) is used to evaluated our SSimNet. The MoNuSeg dataset consists of 44 H&amp;E stained histopathology images with 28,846 manually annotated nuclei. With 1000 × 1000 pixel resolution, these images were extracted from whole slide images from the The Cancer Genome Atlas (TCGA) repository, representing 9 different organs from 44 individuals.</p><p>CPM17. The CPM17 dataset <ref type="bibr" target="#b23">[24]</ref> is also derived from TCGA repository. The training and test set each consisted of 32 images tiles selected and extracted from a set of Non-Small Cell Lung Cancer (NSCLC), Head and Neck Squamous Cell Carcinoma (HNSCC), Glioblastoma Multiforme (GBM) and Lower Grade Glioma (LGG) tissue images. Moreover, each type cancer has 8 tiles and the size of patch is 500 × 500 or 600 × 600.</p><p>Settings. We compare our SSimNet with several current unsupervised segmentation methods. We follow the DCGN <ref type="bibr" target="#b14">[15]</ref> to conduct comparison experiments. We crop the image indataset into patches of 256 × 256 pixels for training. All the methods were trained for 150 epochs on MoNuSeg and 200 epochs on CPM17 each time and experimented with an initial learning rate of 5e -5 and a decay of 0.98 per epoch. Our experiment repeated ten times on MoNuSeg dataset and only once on CPM17 dataset for an augmented convenience. Specially for our SSimNet training, we set α = 70% for data purification and λ = 0.9 for loss in training. Moreover, we fine tune the network with only five epochs for each image on test set with optimizer parameter saved in checkpoint. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experimental Results</head><p>To evaluate the effectiveness of SSimNet, we compare it with several deep learning based and conventional unsupervised segmentation methods on the mentioned datasets, including minibatch K-Means (termed as mKMeans), Gaussian Mixture Model <ref type="bibr" target="#b8">[9]</ref> (termed as GMM), Invariant Information Clustering <ref type="bibr" target="#b11">[12]</ref> (termed as IIC), Double DIP <ref type="bibr" target="#b17">[18]</ref>, Deep Clustering via Adaptive GMM model <ref type="bibr" target="#b18">[19]</ref> (termed as DCAGMM), Deep Image Clustering <ref type="bibr" target="#b12">[13]</ref> (termed as DIC), Kim's work <ref type="bibr" target="#b19">[20]</ref>, Kanezaki's work <ref type="bibr" target="#b10">[11]</ref>, Deep Conditional GMM <ref type="bibr" target="#b13">[14]</ref> (termed as DCGMM), and Deep Constrained Gaussian Network <ref type="bibr" target="#b14">[15]</ref> (termed as DCGN).</p><p>For the methods without public codes, we report the results from the original publications for a fair comparison. The results are shown in Table <ref type="table" target="#tab_0">1</ref>.</p><p>As Table <ref type="table" target="#tab_0">1</ref> shows, firstly, our SSimNet outperforms all other unsupervised model and performs even close to fully supervised U-Net under the metrics of Dice coefficient and Aggregated Jaccard Index (AJI). Secondly, while the recall of all comparison methods is higher than precision, our SSimNet's recall (0.772) is lower than precision (0.820) and also lower than the state-of-the-art method's recall (0.834). The reason lies in that our method considers mining as strong prior knowledge from tissue slice itself, which renders a tighter constraint on our model, leading the model to predict a lower confidence in the easilyconfused region. Moreover, Figure <ref type="figure" target="#fig_4">4</ref> shows the visualization of two test slice. It also conforms the effectiveness of our method on eliminating the model confusion in the region between adjacent nuclei and the ability in capturing nuclei shape.</p><p>Besides, we conduct an additional comparison experiment based on CPM17 dataset to demonstrate the generalization of our method. As shown in Table <ref type="table" target="#tab_1">2</ref>, our method again achieves the top performances. Moreover, as the image size of CPM17 is smaller than that of MoNuSeg, the performance gain is not as big as on the MoNuSeg dataset.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Ablation Study</head><p>We perform ablation studies by disabling each component to the SSimNet framework to evaluate their effectiveness. As shown in Table <ref type="table" target="#tab_2">3</ref>, each component in our SSimNet can bring different degrees of improvement, which shows that all of the label softening, data purification and finetuning process are significant parts of our SSimNet and play an indispensable role in achieving superior performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we propose an SSimNet framework for label-free nuclei segmentation. Motivated by the intra-image self similarity (IISS) property, which characterize the histology images and nuclei, we design a series of operations to capture the prior knowledge and generate pseudo labels as supervision signal, which is used to learn the SSimNet for final nuclei segmentation. The IISS property renders us a tighter prior constraint for better model building compared to other unsupervised nuclei segmentation. Comprehensive experimental results demonstrate that SSimNet achieves the best performances on the benchmark MoNuSeg and CPM17 datasets, outperforming other unsupervised segmentation methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The limitations of (deep) clustering model. Green, yellow and red colors refer to the true positive, the false positive and the false negative predictions. (Color figure online)</figDesc><graphic coords="2,67,80,53,72,288,64,163,99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The overview pipeline of the proposed method.</figDesc><graphic coords="4,57,78,53,99,316,36,154,72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>k represents the k th epoch erosion of the connected component, B[k] is the confidence score of pixels eroded in the k th epoch and B[0] = 0.5 as the initial condition, P [k] means number of pixels eroded in the k th epoch, and A is the area of the connected component. As a terminal condition, we set the termination of erosion when B[k] &gt; 0.975. Following Eq. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Left: The algorithm that computes USMI. Right: Illustration of average convex hull area and the average of connected component area based on USMI. Yellow (or cyan) points denote the samples whose USMI is greater (or less) than the threshold. (Color figure online)</figDesc><graphic coords="6,50,79,54,08,322,90,132,61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Comparison of unsupervised nuclei segmentation results on MoNuSeg. Green, yellow and red colors refer to the true positive, the false positive and the false negative predictions. (Color figure online)</figDesc><graphic coords="8,43,29,53,84,337,45,119,86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Performance of the nuclei segmentation on MoNuSeg dataset. The best results are highlighted in bold and the second best underlined. 'ft' means fine tuning. The results are shown as "mean±standard deviation(upper-bound results)".</figDesc><table><row><cell>Methods</cell><cell>Precision%↑</cell><cell>Recall%↑</cell><cell>Dice%↑</cell><cell>AJI%↑</cell></row><row><cell>mKMeans</cell><cell cols="4">65.7±17.5(67.9) 79.2±17.4(77.3) 67.8± 9.4(68.2) 30.5±14.0(33.8)</cell></row><row><cell>GMM [9]</cell><cell cols="4">63.1±15.0(66.4) 82.2±10.9(81.9) 69.5± 8.5(71.7) 29.0±15.1(31.9)</cell></row><row><cell>IIC [12]</cell><cell cols="3">46.7± 9.2(51.6) 72.5±12.1(79.6) 56.0± 8.7(61.8)</cell><cell>5.6± 3.0( 7.2)</cell></row><row><cell>Kim et al. [20]</cell><cell cols="4">57.5±24.9(69.8) 82.4±18.9(77.2) 60.6±17.1(69.4) 22.0±17.6(32.3)</cell></row><row><cell>Double DIP [18]</cell><cell cols="3">22.1± 5.1(22.1) 82.0±10.9(85.1) 34.4± 6.7(35.0)</cell><cell>1.3± 0.6( 1.3)</cell></row><row><cell>Kanezaki et al. [11]</cell><cell cols="4">62.9±19.5(72.5) 82.2±16.2(78.3) 66.9±11.9(72.7) 26.0±16.6(35.1)</cell></row><row><cell>DCGMM [14]</cell><cell cols="4">69.3±13.5(69.8) 78.6±17.1(80.1) 70.7± 6.4(71.9) 31.4±12.4(34.5)</cell></row><row><cell>DIC [13]</cell><cell cols="4">51.1±24.9(59.5) 84.8±17.0(83.2) 57.1±16.5(64.4) 14.7±16.9(19.3)</cell></row><row><cell>DCAGMM [19]</cell><cell cols="4">61.9±13.7(69.1) 76.7±13.1(76.3) 66.4± 7.9(70.6) 30.0±12.6(36.5)</cell></row><row><cell>DCGN [15]</cell><cell cols="4">68.5±11.3(71.6) 83.4±11.5(80.8) 73.7± 4.3(74.3) 35.2±11.3(37.9)</cell></row><row><cell cols="3">Our SSimNet w/o ft 80.8± 2.1(79.7) 76.1± 3.3(79.3)</cell><cell cols="2">76.7± 1.4(78.5) 44.1± 1.5(45.3)</cell></row><row><cell>Our SSimNet</cell><cell cols="2">82.0± 1.7(82.0) 77.2± 2.5(78.6)</cell><cell cols="2">79.2± 0.6(80.0) 49.8± 0.9(51.2)</cell></row><row><cell>U-Net (supervised)</cell><cell cols="2">73.8± 1.3(75.5) 85.3± 0.4(85.8)</cell><cell cols="2">78.7± 1.0(80.0) 51.0± 0.9(52.4)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Performance of the nuclei segmentation on CPM17 dataset. The best results are highlighted in bold and the second best underlined.</figDesc><table><row><cell>Methods</cell><cell cols="4">Precision%↑ Recall%↑ Dice%↑ AJI%↑</cell></row><row><cell>mKMeans</cell><cell>79.4</cell><cell>74.9</cell><cell>74.5</cell><cell>46.1</cell></row><row><cell>GMM [9]</cell><cell>79.0</cell><cell>75.5</cell><cell>72.7</cell><cell>43.0</cell></row><row><cell>Kanezaki et al. [11]</cell><cell>82.0</cell><cell>65.4</cell><cell>72.1</cell><cell>46.7</cell></row><row><cell cols="2">Our SSimNet w/o finetune 85.4</cell><cell>80.7</cell><cell>81.2</cell><cell>49.3</cell></row><row><cell>Our SSimNet</cell><cell>85.8</cell><cell>80.5</cell><cell>81.6</cell><cell>49.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Ablation study on SSimNet using MoNuSeg dataset.</figDesc><table><row><cell cols="4">LabelSoftening DataPurification Finetune Precision%↑ Recall%↑ Dice%↑ AJI%↑</cell></row><row><cell>78.8</cell><cell>78.7</cell><cell>78.3</cell><cell>45.6</cell></row><row><cell>79.6</cell><cell>77.3</cell><cell>77.9</cell><cell>47.6</cell></row><row><cell>80.8</cell><cell>76.1</cell><cell>76.7</cell><cell>44.1</cell></row><row><cell>82.0</cell><cell>77.2</cell><cell>79.2</cell><cell>49.8</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Note that in our experiments, we use an image of size 1000</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>or 500 2 .</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. Supported by <rs type="funder">Natural Science Foundation of China</rs> under Grant <rs type="grantNumber">62271465</rs> and <rs type="funder">Open Fund Project of Guangdong Academy of Medical Sciences, China</rs> (No. <rs type="grantNumber">YKY-KF202206</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_YVbKFXy">
					<idno type="grant-number">62271465</idno>
				</org>
				<org type="funding" xml:id="_Z38WWYM">
					<idno type="grant-number">YKY-KF202206</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Hover-net: simultaneous segmentation and classification of nuclei in multi-tissue histology images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Graham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page">101563</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mild-net: minimal information loss dilated network for gland instance segmentation in colon histology images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Graham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="199" to="211" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A fast and refined cancer regions segmentation framework in wholeslide breast pathological images</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">882</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Cryonuseg: a dataset for nuclei instance segmentation of cryosectioned h&amp;e-stained histological images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mahbod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Biol. Med</title>
		<imprint>
			<biblScope unit="volume">132</biblScope>
			<biblScope unit="page">104349</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">BiO-Net: learning recurrent bi-directional connections for encoder-decoder architecture</title>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cai</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59710-8_8</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59710-88" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12261</biblScope>
			<biblScope unit="page" from="74" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An efficient method based on watershed and rule-based merging for segmentation of 3-d histo-pathological images</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Umesh Adiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">B</forename><surname>Chaudhuri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1449" to="1458" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Detection and segmentation of cell nuclei in virtual microscopy images: a minimum-model approach</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wienert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">503</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Automatic lymph node cluster segmentation using holisticallynested neural networks and structured optimization in CT images</title>
		<author>
			<persName><forename type="first">I</forename><surname>Nogues</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46723-8_45</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46723-8" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2016</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Ourselin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Joskowicz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Sabuncu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Unal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><surname>Wells</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">9901</biblScope>
			<biblScope unit="page">45</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised segmentation of cervical cell images using gaussian mixture model</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ragothaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Basavaraj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dewar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="70" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Segmentation of brain mr images through a hidden markov random field model and the expectation-maximization algorithm</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="45" to="57" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised image segmentation by backpropagation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kanezaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1543" to="1547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Invariant information clustering for unsupervised image classification and segmentation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9865" to="9874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">DIC: deep image clustering for unsupervised image segmentation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="34481" to="34491" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Histopathology stain-color normalization using deep generative models</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">G</forename><surname>Zanjani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Bejnordi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A W M</forename><surname>Van Der Laak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Imaging with Deep Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unsupervised tissue segmentation via deep constrained gaussian network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Nan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3799" to="3811" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A dataset and a technique for generalized nuclear segmentation for computational pathology</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vahadane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sethi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1550" to="1560" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A multi-organ nucleus segmentation challenge</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1380" to="1391" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">double-dip&quot;: unsupervised image decomposition via coupled deep-image-priors</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gandelsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shocher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="11026" to="11035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised deep clustering via adaptive gmm modeling and optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">433</biblScope>
			<biblScope unit="page" from="199" to="211" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unsupervised learning of image segmentation based on differentiable feature clustering</title>
		<author>
			<persName><forename type="first">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kanezaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tanaka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="8055" to="8068" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Structure-preserving color normalization and sparse stain separation for histological images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vahadane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1962" to="1971" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Tissue processing and hematoxylin and eosin staining</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wolfe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Histopathology: Methods and Protocols</title>
		<imprint>
			<biblScope unit="page" from="31" to="43" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Weakly supervised deep nuclei segmentation using partial points annotation in histopathology images</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Hui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3655" to="3666" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Methods for segmentation and classification of digital microscopy tissue images</title>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">D</forename><surname>Vu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front. Bioeng. Biotechnol</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
