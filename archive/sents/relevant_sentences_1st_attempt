Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_29.pdf:
changes in image resolution or observed ﬁeld of view can result in
inaccurate predictions, even with signiﬁcant data pre-processing and aug-
mentation.
we showcase that this spatial condi-
tioning mechanism statistically-signiﬁcantly improves model performance
on whole-body data compared to the same model without conditioning,
while allowing the model to perform inference at varying data geometries.
most recent approaches
have focused on improvements in performance rather than ﬂexibility, thus lim-
iting approaches to speciﬁc input types – little research has been carried out to
generate models unhindered by variations in data geometries.
often, research
assumes certain similarities in data acquisition parameters, from image dimen-
sions to voxel dimensions and ﬁelds-of-view (fov).
this strong assumption can often be
complex to maintain in the real-world and although image pre-processing steps
can mitigate some of this complexity, test error often largely increases as new
data variations arise.
usually training
data, especially when acquired from diﬀering sources, undergoes signiﬁcant pre-
processing such that data showcases the same fov and has the same input
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43907-0_29.
https://doi.org/10.1007/978-3-031-43907-0_29
geometry-invariant abnormality detection
301
dimensions, e.g. by registering data to a population atlas.
given
this, the task of generating an anomaly detection model that works on inputs
with a varying resolution, dimension and fov is a topic of importance and the
main focus of this research.
unsupervised methods have become an increasingly prominent ﬁeld for auto-
matic anomaly detection by eliminating the necessity of acquiring accurately
labelled data [4,7] therefore relaxing the stringent data requirements of medical
imaging.
in [22], the authors explore the advan-
tage of tractably maximizing the likelihood of the normal data to model the
long-range dependencies of the training data.
even though these methods are state-of-the-art, they have stringent data
requirements, such as having a consistent geometry of the input data, e.g., in a
whole-body imaging scenario, it is not possible to crop a region of interest and
feed it to the algorithm, as this cropped region will be wrongly detected as an
anomaly.
furthermore, we show that
the performance of our model with spatial conditioning is at least equivalent to,
and sometimes better, than a model trained on whole-body data in all testing
scenarios, with the added ﬂexibility of a “one model ﬁts all data” approach.
we greatly reduce the pre-processing requirements for generating a model (as
visualised in fig. 1), demonstrating the potential use cases of our model in more
ﬂexible environments with no compromises on performance.
speciﬁcally, a vq-vae plus a transformer are jointly used to learn the proba-
bility density function of 3d pet images as explored in prior research [21,22,24].
302
a. patel et al.
fig.
the vq-vae
is composed of an encoder that maps an image x ∈ rh×w ×d onto a com-
pressed latent representation z ∈ rh×w×d×nz where nz is the latent embedding
vector dimension.
each spatial code
zijl ∈ rnz is then replaced by its nearest codebook element ek ∈ rnz, k ∈ 1, ..., k
where k denotes the codebook vocabulary size, thus obtaining zq.
the vq-vae codebook used
had 256 atomic elements (vocabulary size), each of length 128.
see
appendix a for implementation details.
2.2
transformer
after training a vq-vae model, the next stage is to learn the probability den-
sity function of the discrete latent representations.
using the vq-vae, we can
obtain a discrete representation of the latent space by replacing the codebook
elements in zq with their respective indices in the codebook yielding ziq.
the
transformer is then trained to maximize the log-likelihoods of the latent tokens
sequence in an autoregressive manner.
the performer
used in this work corresponds to a decoder transformer architecture with 14
layers, each with 8 heads, and an embedding dimension of 256.
similarly the
embedding dimension for the ct data and the spatial conditioning data had an
embedding dimension of 256.
see appendix b for implementation details.
we then resample anomalous
tokens p(si) < t where t is the resampling threshold chosen empirically using
the validation set performance.
this generates multiple healed representations of
the original image.
our kde implementation used 60 samples for each
anomalous token in s, followed by ﬁve decodings with dropout, yielding 300
“healed” reconstructions that are then used to calculate the kde.
3
method
3.1
vq-vae spatial conditioning
to date, there has been little research on generating autoencoder models capable
of using images of varying sizes and resolutions (i.e. the input tensor shape to
a autoencoder is assumed to be ﬁxed).
although fully convolutional models can
304
a. patel et al.
ingest images of varying dimensions, we have found that using training data with
varying resolutions resulted in poor auto-encoder reconstructions.
a coordconv layer is a concatenation of channels to the input image refer-
encing a predeﬁned coordinate system.
the advantage of the coordconv implementation is the constant scale of
0–1 across the channels regardless of image resolution.
for example, two whole-
body images with large diﬀerences in voxel-size will have coordconv channels
from 0–1 along each axis, thus conveying the notion of spatial resolution to the
network.
we found when training the vq-vae model on data with varying reso-
lutions and dimensions that reconstructions showcased unwanted and signiﬁcant
artifacts, while by adding the coordconv channels this issue was not present
(see appendix c for examples).
furthermore, when dealing with images of a
ranging fov, we adapted the [0, 1] channel values to convey the image’s fov.
for example, suppose a whole body image (neck to upper leg) represented our
range
in that case, we can
contract this range to represent the area displayed in the image (fig. 2).
for the implementation of the coordconv layer, these channels are
added once to the original input image and at the beginning of the vq-vae
decoder, concatenated to the latent space, using the same value ranges but at a
lower resolution given the reduced spatial dimension of the latent space.
3.2
transformer spatial conditioning
numerous approaches have used transformers in the visual domain [7,8].
given
that transformers work natively on 1d sequences, the spatial information in
images is often lost.
while various works have aimed to convey the spatial infor-
mation of the original image when projected onto a 1d sequence [14,28], we
require our spatial positioning to encode both where in the image ordering a
token belongs, and where the token belongs in the context of the whole body.
as the images have diﬀerent fovs and the image resolution, this results in
geometry-invariant abnormality detection
305
fig.
2. coordconv example showing whole-body image with values from 0 to 1 vs. a
cropped image with values from 0.2 to 0.7 to reﬂect the ﬁeld of view
varying token sequence lengths.
in order to map image coordinates to the token latent represen-
tation, we apply average pooling to each coordconv channel separately, with
kernel size and stride equal to the downsampling used in the vq-vae (8 used
in this research).
this gives us three channels i, j, k in the range of [0, 1], the
same dimension as our latent space, but at lower spatial resolution to the original
input.
the choice of b = 20 bins was
empirically chosen to closely resemble the average latent dimension of images.
during training, whole-body images and random crops are used.
the spatial
conditioning tokens are then generated and fed through an embedding layer of
equal dimension to the ct embedding.
3.
3.3
data
for this work we leveraged whole-body pet/ct data from diﬀerent sources to
explore the eﬃcacy of our approach for varying image geometries.
211 scans from
nsclc radiogenomics [2,3,10,16] combined with 83 scans from a proprietary
dataset constitute our lower resolution dataset with voxel dimensions of 3.6 ×
3.6×3 mm.
our higher resolution dataset uses autopet [10,15] (1014 scans)
with voxel dimensions of 2.036 × 2.036 × 3 mm.
all baseline models work in a single space with constant dimensions, obtained
by registering the autopet images to the space of the nsclc dataset.
as the cropped and rotated dataset cannot be fed into the baseline
models, we pad the images to the common image sizing before inference.
we then test
our model and baselines on 4 hold-out test sets: a low-resolution whole-body set,
a low-resolution cropped set, a high-resolution rotated set and a high-resolution
test set of pet images with varying cancers.
4. columns display (1st) the input image; (2nd) the gold standard segmentation;
(3rd) residual for the vae, (4th) ae spatial, (5th) a kde anomaly map for vq-vae
transformer trained on the whole body, (6th) trained with varied geometries, (7th)
with spatial conditioning.
performance using the dice score, obtained by thresholding the residual/density
score maps.
in addition, we calculate the area under the precision-recall curve
(auprc) as a suitable measure for segmentation performance under class imbal-
ance.
we additionally showcase the performance of the classic vq-vae + trans-
former approach trained on whole-body data only (without the proposed spatial
conditioning), as well as the proposed coordconv model trained with varying
image geometries but without the transformer spatial conditioning to explicitly
showcase the added contribution of both spatial conditionings.
4. we can observe
that the addition of spatial conditioning improves performance even against the
same model without conditioning trained on whole-body data (mann whitney
u test, p < 0.01 on high resolution and p < 0.001 on cropped data for dice
and auprc).
note that the vq-vae + transformer trained on
varying geometries still shows adequate performance, highlighting the resilience
of the transformer network to varying sequence lengths without any form of
spatial conditioning.
however, by adding the transformer spatial conditioning,
we see improvements across all test sets (most signiﬁcantly on cropped data and
the rotated data p < 0.001) for both evaluation metrics.
for the rotated data,
we see little performance degradation in the conditioned model thanks to the
spatial conditioning.
the same model without conditioning showed much lower
performance with higher false positives likely due to the model’s inability to
comprehend the anatomical structures present due to the rotated orientation.
308
a. patel et al.
5
conclusion
detection and segmentation of anomalous regions, particularly for cancer
patients, is essential for staging, treatment and intervention planning.
not only does
the proposed model showcase strong and statistically-signiﬁcant performance
improvements on varying image resolutions and fov, but also on whole-body
data.
through this, we demonstrate that one can improve the adaptability and
ﬂexibility to varying data geometries while also improving performance.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_14.pdf:
automated segmentation of the blood vessels in 3d volumes
is an essential step for the quantitative diagnosis and treatment of many
vascular diseases.
3d vessel segmentation is being actively investigated
in existing works, mostly in deep learning approaches.
however, training
3d deep networks requires large amounts of manual 3d annotations from
experts, which are laborious to obtain.
this is especially the case for 3d
vessel segmentation, as vessels are sparse yet spread out over many slices
and disconnected when visualized in 2d slices.
in this work, we pro-
pose a novel method to segment the 3d peripancreatic arteries solely
from one annotated 2d projection per training image with depth
supervision.
we perform extensive experiments on the segmentation of
peripancreatic arteries on 3d contrast-enhanced ct images and demon-
strate how well we capture the rich depth information from 2d pro-
jections.
we demonstrate that by annotating a single, randomly chosen
projection for each training sample, we obtain comparable performance
to annotating multiple 2d projections, thereby reducing the annotation
eﬀort.
furthermore, by mapping the 2d labels to the 3d space using
depth information and incorporating this into training, we almost close
the performance gap between 3d supervision and 2d supervision.
our
code is available at: https://github.com/alinafdima/3dseg-mip-depth.
keywords: vessel segmentation · 3d segmentation · weakly
supervised segmentation · curvilinear structures · 2d projections
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43907-0_14.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
https://doi.org/10.1007/978-3-031-43907-0_14
142
a. f. dima et al.
1
introduction
automated segmentation of blood vessels in 3d medical images is a crucial step
for the diagnosis and treatment of many diseases, where the segmentation can aid
in visualization, help with surgery planning, be used to compute biomarkers, and
further downstream tasks.
automatic vessel segmentation has been extensively
studied, both using classical computer vision algorithms
[8], or more recently with deep learning [3,5,6,11,19,21], where state-of-
the-art performance has been achieved for various vessel structures.
this is especially the case for 3d vessel segmentation.
manually delineating 3d vessels typically involves visualizing and annotating
a 3d volume through a sequence of 2d cross-sectional slices, which is not a good
medium for visualizing 3d vessels.
in order to segment a vessel, the annotator has
to track the cross-section of that vessel through several adjacent slices, which
is especially tedious for curved or branching vessel trees.
projecting 3d vessels
to a 2d plane allows for the entire vessel tree to be visible within a single 2d
image, providing a more robust representation and potentially alleviating the
burden of manual annotation.
[13] propose to annotate up to
three maximum intensity projections (mip) for the task of centerline segmen-
tation
compared to
centerline segmentation, where the vessel diameter is disregarded, training a 3d
vessel segmentation model from 2d annotations poses additional segmentation-
speciﬁc challenges, as 2d projections only capture the outline of the vessels,
providing no information about their interior.
furthermore, the axes of projec-
tion are crucial for the model’s success, given the sparsity of information in 2d
annotations.
to achieve 3d vessel segmentation with only 2d supervision from projec-
tions, we ﬁrst investigate which viewpoints to annotate in order to maximize
segmentation performance.
we show that it is feasible to segment the full extent
of vessels in 3d images with high accuracy by annotating only a single randomly-
selected 2d projection per training image.
secondly, by mapping the 2d annotations to the 3d space using the depth of
the mips, we obtain a partially segmented 3d volume that can be used as an
additional supervision signal.
we demonstrate the utility of our method on the
challenging task of peripancreatic arterial segmentation on contrast-enhanced
arterial-phase computed tomography (ct) images, which feature large variance
in vessel diameter.
our contribution to 3d vessel segmentation is three-fold:
◦ our work shows that highly accurate automatic segmentation of 3d vessels
can be learned by annotating single mips.
◦ based on extensive experimental results, we determine that the best annota-
tion strategy is to label randomly selected viewpoints, while also substantially
reducing the annotation cost.
3d arterial segmentation via single 2d projections and depth supervision
143
◦ by incorporating additional depth information obtained from 2d annotations
at no extra cost to the annotator, we almost close the gap between 3d super-
vision and 2d supervision.
weak annotations have been used in
deep learning segmentation to reduce the annotation eﬀort through cheaper,
less accurate, or sparser labeling [20].
[1] learn to perform aortic image
segmentation by sparsely annotating only a subset of the input slices.
[12] use this approach to segment cancer on histopathology
images successfully.
annotating 2d projections for 3d data is another approach
to using weak segmentation labels, which has garnered popularity recently in
the medical domain.
[22] use multi-planar mips for multi-organ
segmentation of the abdomen.
kozinski et al.[13] propose to segment vessel cen-
terlines using as few as 2-3 annotated mips.
[4] train a vessel segmen-
tation model from unsupervised 2d labels transferred from a publicly available
dataset, however, there is still a gap to be closed between unsupervised and
supervised model performance.
our work uses weak annotations in the form of
annotations of 2d mips for the task of peripancreatic vessel segmentation, where
we attempt to reduce the annotation cost to a minimum by only annotating a
single projection per training input without sacriﬁcing performance.
loss of depth information occurs whenever 3d data is projected onto a
lower dimensional space.
in natural images, depth loss is inherent through image
acquisition, therefore attempts to recover or model depth have been employed
for 3d natural data.
[9] use neural implicit ﬁelds to
semantically segment images by transferring labels from 3d primitives to 2d
images.
[14] propose to segment 3d point clouds by projecting
them onto 2d and training a 2d segmentation network.
at inference time, the
predicted 2d segmentation labels are remapped back to the original 3d space
using the depth information.
we use depth information to map the 2d annotations to the original
3d space at annotation time and generate partial 3d segmentation volumes,
which we incorporate in training as an additional loss term.
(1)
for simplicity, we only describe mips along the z-axis, but they can be performed
on any image axis.
fig.
we train a 3d network to segment vessels from 2d annota-
tions.
given an input image i, depth-encoded mips pfw, pbw are generated by project-
ing the input image to 2d.
2d binary labels a are generated by annotating one 2d
projection per image.
the 2d annotation is mapped to the 3d space using the depth
information, resulting in a partially labeled 3d volume d. during training, both 2d
annotations and 3d depth maps are used as supervision signals in a combined loss,
which uses both predicted 3d segmentation y and its 2d projection mip(y ).
exploiting the fact that arteries are hyperintense in arterial phase cts, we
propose to annotate mips of the input volume for binary segmentation.
given a binary 2d annotation of a mip a ∈ {0, 1}nx×ny, we map the fore-
ground pixels in a to the original 3d image space.
furthermore, we can partially ﬁll this surface volume, resulting in a 3d depth
map d, which is a partial segmentation of the vessel tree.
we use the 2d anno-
tations as well as the depth map to train a 3d segmentation network in a weakly
supervised manner.
3d arterial segmentation via single 2d projections and depth supervision
145
an overview of our method is presented in fig.
1. in the following, we describe
these components and how they are combined to train a 3d segmentation net-
work in more detail.
the reason
why the maximum intensity is achieved multiple times along a ray is because
our images are clipped, which removes a lot of the intensity ﬂuctuations.
fig.
the input images are
contrast-enhanced.(color ﬁgure online)
depth-enhanced mip.
foreground pixels from the 2d annotations are
mapped to the 3d space by combining a 2d annotation with the forward and
backward depth, resulting in a 3d partial vessel segmentation:
146
a. f. dima et al.
1.
if the ﬂuctuation in intensity between zfw and zbw along the ray rxy is
below a certain threshold in the source image i, the intermediate pixels
are also labeled as foreground in d.
training loss.
we train a 3d segmentation network to predict 3d binary vessel
segmentation given a 3d input volume using 2d annotations.
notably, the 2d loss constrains
the shape of the vessels, while the depth loss promotes the segmentation of the
vessel interior.
4
experimental design
dataset.
we use an in-house dataset of contrast-enhanced abdominal computed
tomography images (cts) in the arterial phase to segment the peripancreatic
arteries
the cohort consists of 141 patients with pancreatic ductal adeno-
carcinoma, of an equal ratio of male to female patients.
details of the exact preprocessing steps can be found in table 2 of the supple-
mentary material.
the 2d annotations we use in our experiments are projections of these
3d annotations.
for more information about the dataset, see [6].
image augmentation and transformation.
as the annotations lie on a 2d
plane, 3d spatial augmentation cannot be used due to the information sparsity
in the ground truth.
a
detailed description of the augmentations and transformations used can be found
in table 1 in the supplementary material.
3d arterial segmentation via single 2d projections and depth supervision
147
training and evaluation.
2 in the supplementary material.
the loss weight α
is tuned at 0.5, as this empirically yields the best performance.
our experiments
are averaged over 5-fold cross-validation with 80 train samples, 20 validation
samples, and a ﬁxed test set of 41 samples.
the network initialization is diﬀer-
ent for each fold but kept consistent across diﬀerent experiments run on the same
fold.
to measure the performance of our models, we use the
dice score, precision, recall, and mean surface distance (msd).
we also compute
the skeleton recall as the percentage of the ground truth skeleton pixels which
are present in the prediction.
experiment
model selection dice ↑
precision ↑
recall ↑
skeleton recall ↑ msd ↓
3d
3d
92.18 ± 0.35 93.86 ± 0.81 90.64 ± 0.64
76.04 ± 4.51
1.15 ± 0.11
ﬁxed 3vp
3d
92.02 ± 0.52
93.05 ± 0.61
91.13 ± 0.79
78.61 ± 1.52
1.13 ± 0.11
ﬁxed 2vp
3d
91.29 ± 0.78
91.46 ± 2.13
91.37 ± 1.45 78.51 ± 2.78
1.13 ± 0.09
ﬁxed 3vp
2d
90.78 ± 1.30
90.66 ± 1.30
91.18 ± 3.08
81.77 ± 2.13
1.16 ± 0.13
ﬁxed 2vp
2d
90.22 ± 1.19
88.16 ± 2.86
92.74 ± 1.63
82.18 ± 2.47
1.14 ± 0.09
ﬁxed 1vp
2d
60.76 ± 24.14 50.47 ± 23.21 92.52 ± 3.09
81.19 ± 2.39
2.96 ± 3.15
random 1vp−d 2d
91.29 ± 0.81
91.42 ± 0.92 91.45 ± 1.00
80.16 ± 2.35
1.13 ± 0.04
random 1vp+d 2d
91.69 ± 0.48 90.77 ± 1.76
92.79 ± 0.95 81.27 ± 2.02
1.15 ± 0.11
5
results
the eﬀectiveness of 2d projections and depth supervision.
we implement [13] as a baseline on our dataset, training on up to 3
ﬁxed orthogonal projections.
we distinguish between models selected according
to the 2d performance on the validation set (2d) which is a fair baseline, and
models selected according to the 3d performance on the validation set (3d),
which is an unfair baseline as it requires 3d annotations on the validation set.
with the exception of the single ﬁxed viewpoint baselines where the models have
the tendency to diverge towards over- or segmentation, we perform binary hole-
ﬁlling on the output of all of our other models, as producing hollow objects is a
common under-segmentation issue.
randomly selecting view-
points for training acts as powerful data augmentation, which is why we are
able to obtain performance comparable to using more ﬁxed viewpoints.
under
ideal 3d-based model selection, three views would come even closer to full 3d
performance; however, with realistic 2d-based model selection, ﬁxed viewpoints
are more prone to diverge.
this occurs because sometimes 2d-based model selec-
tion favors divergent models which only segment hollow objects, which cannot
be ﬁxed in postprocessing.
single ﬁxed viewpoints contain so little information
on their own that models trained on such input fail to learn how to segment
the vessels and generally converge to over-segmenting in the blind spots in the
projections.
we theorize that this is because
the dataset itself contains noisy annotations and fully supervised models better
overﬁt to the type of data annotation, whereas our models converge to follow-
ing the contrast and segmenting more vessels, which are sometimes wrongfully
labeled as background in the ground truth.
msd are not very telling in our
dataset due to the noisy annotations and the nature of vessels, as an under- or
over-segmented vessel branch can quickly translate into a large surface distance.
our depth loss oﬀers
consistent improvement across multiple dataset sizes and reduces the overall per-
formance variance.
the performance boost is noticeable across the board, the
only exception being precision.
the smaller the dataset size is, the greater the
performance boost from the depth.
we
conclude that the depth information complements the segmentation eﬀectively.
3d arterial segmentation via single 2d projections and depth supervision
149
table 2.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_11.pdf:
most existing weakly-supervised segmentation methods rely
on class activation maps (cam) to generate pseudo-labels for training
segmentation models.
although some recent methods have attempted to
extend cam to cover more areas, the fundamental problem still needs
to be solved.
we believe this problem is due to the huge gap between
image-level labels and pixel-level predictions and that additional infor-
mation must be introduced to address this issue.
thus, we propose a
text-prompting-based weakly supervised segmentation method (tpro),
which uses text to introduce additional information.
tpro employs a
vision and label encoder to generate a similarity map for each image,
which serves as our localization map.
pathological knowledge is gathered
from the internet and embedded as knowledge features, which are used to
guide the image features through a knowledge attention module.
our approach outperforms other weakly super-
vised segmentation methods on benchmark datasets luad-histoseg and
bcss-wsss datasets, setting a new state of the art.
code is available
at: https://github.com/zhangst431/tpro.
keywords: histopathology tissue segmentation · weakly-supervised
semantic segmentation · vision-language
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43907-0_11.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
https://doi.org/10.1007/978-3-031-43907-0_11
110
s. zhang et al.
1
introduction
automated segmentation of histopathological images is crucial, as it can quantify
the tumor micro-environment, provide a basis for cancer grading and prognosis,
and improve the diagnostic eﬃciency of clinical doctors [6,13,19].
however, pixel-
level annotation of images is time-consuming and labor-intensive, especially for
histopathology images that require specialized knowledge.
therefore, there is an
urgent need to pursue weakly supervised solutions for pixel-wise segmentation.
nonetheless, weakly supervised histopathological image segmentation presents
a challenge due to the low contrast between diﬀerent tissues, intra-class vari-
ations, and inter-class similarities [4,11].
additionally, the tissue structures in
histopathology images can be randomly arranged and dispersed, which makes it
diﬃcult to identify complete tissues or regions of interest [7].
1. comparison of activation maps extracted from cam and our method, from left
to right: origin image, ground truth, three activation maps of tumor epithelial (red),
necrosis (green), and tumor-associated stroma (orange) respectively.
on the right side,
there are some examples of the related language knowledge descriptions used in our
method.
it shows that cam only highlights a small portion of the target, while our
method, which incorporates external language knowledge, can encompass a wider and
more precise target tissue.
(color ﬁgure online)
recent studies on weakly supervised segmentation primarily follow class acti-
vation mapping (cam) [20], which localizes the attention regions and then
generates the pseudo labels to train the segmentation network.
however, the
cam generated based on the image-level labels can only highlight the most dis-
criminative region, but fail to locate the complete object, leading to defective
pseudo labels, as shown in fig.
accordingly, many attempts have been made
to enhance the quality of cam and thus boost the performance of weakly super-
vised segmentation.
[11] utilized the conﬁdence method to remove any noise that may
exist in the pseudo labels and only included the conﬁdent pixel labels for the
segmentation training.
[18] leveraged the transformer to model the
long-distance dependencies on the whole histopathological images to improve the
cam’s ability to ﬁnd more complete regions.
however, these improved variants still face diﬃculties in capturing the
tpro for weakly supervised histopathology tissue segmentation
111
complete tissues.
the primary limitation is that the symptoms and manifesta-
tions of histopathological subtypes cannot be comprehensively described by an
abstract semantic category.
as a result, the image-level label supervision may
not be suﬃcient to pinpoint the complete target area.
to remedy the limitations of image-level supervision, we advocate for the inte-
gration of language knowledge into weakly supervised learning to provide reliable
guidance for the accurate localization of target structures.
to this end, we pro-
pose a text-prompting-based weakly supervised segmentation method (tpro)
for accurate histopathology tissue segmentation.
the text information originates
from the task’s semantic labels and external descriptions of subtype manifesta-
tions.
for each semantic label, a pre-trained medical language model is utilized
to extract the corresponding text features that are matched to each feature point
in the image spatial space.
a higher similarity represents a higher possibility of
this location belonging to the corresponding semantic category.
additionally,
the text representations of subtype manifestations, including tissue morphol-
ogy, color, and relationships to other tissues, are extracted by the language
model as external knowledge.
the discriminative information can be explored
from the text knowledge to help identify and locate complete tissues accurately
by jointly modeling long-range dependencies between image and text.
we con-
duct experiments on two weakly supervised histological segmentation bench-
marks, luad-histoseg and bcss-wsss, and demonstrate the superior quality
of pseudo labels produced by our tpro model compared to other cam-based
methods.
our contributions are summarized as follows: (1) to the best of our knowl-
edge, this is the ﬁrst work that leverages language knowledge to improve the
quality of pseudo labels for weakly-supervised histopathology image segmenta-
tion.
(2) the proposed text prompting models the correlation between image
representations and text knowledge, eﬀectively improving the quality of pseudo
labels.
tumor epithelial tissue is ....
necrosis tissue is ....
lymphocyte tissue is ....
tumor-associated stroma tissue is.....
knowledge 
input
knowledge 
features
image 
features
reshape
label 
input
tumor epithelial tissue
necrosis tissue
lymphocyte tissue
tumor-associated stroma tissue
gap
gap
gap
stage 1
stage 2
stage 3
stage 4
sim
sim
sim
input image
knowledge input
label input
input image
bert
clip
bert: clinicalbert  
clip: medclip
sim: pixel-label correlation
knowledge attention
search from internet
1
1
0
1
1
1
0
1
1
1
0
1
reshape
reshape
fc: fc+relu+fc
fig.
112
s. zhang et al.
2
method
figure 2 displays the proposed tpro framework, a classiﬁcation network
designed to train a suitable model and extract segmentation pseudo-labels.
the vision encoder is composed of four stages that encode
the input image into image features.
the image features are denoted as ts ∈
rms×cs, where 2 ≤ s ≤ 4 indicates the stage number.
the label encoder encodes the text labels in the dataset into
n label features, denoted as l ∈ rn×cl, where n represents the number of
classes in the dataset and cl represents the dimension of label features.
since
the label features will be used to calculate the similarity with image features, it
is important to choose a language model that has been pre-trained on image-text
pairs.
the knowledge encoder is responsible for embedding
the descriptions of subtype manifestations into knowledge features, denoted as
k ∈ rn×ck.
the knowledge features guide the image features to focus on regions
relevant to the target tissue.
to encode the subtype manifestations description
into more general semantic features, we employ clinicalbert
clinicalbert is a language model that has been ﬁne-tuned on the
mimic-iii
after the input image and text labels are embedded.
we employ the inner product to compute the similarity between image features
and label features, denoted as fs.
specially, we ﬁrst reshape the image features
from a token format into feature maps.
(1)
then, we perform a global average-pooling operation on the produced similarity
map to obtain the class prediction, denoted as ps ∈ r1×n.
tpro for weakly supervised histopathology tissue segmentation
113
ls = − 1
n
n

n=1
y
to leverage the shallow features in the network, we employ
a deep supervision strategy by calculating the similarity between the image fea-
tures from diﬀerent stages and the label features from diﬀerent adaptive layers.
(3)
2.2
knowledge attention module
to enhance the model’s understanding of the color, morphology, and relation-
ships between diﬀerent tissues, we gather text representations of diﬀerent sub-
type manifestations from the internet and encode them into external knowledge
via the knowledge encoder.
the knowledge attention module uses this exter-
nal knowledge to guide the image features toward relevant regions of the target
tissues.
the image features t4 ∈ rm4×c4 and knowledge features
after adaptive layer k ∈ rn×c4 are concatenated in the token dimension to
obtain tfuse ∈ r(m4+n)×c4.
the output tokens
are split, and the part corresponding to the image features is taken out.
noted
that the knowledge attention module is added only after the last stage of the
vision encoder to save computational resources.
2.3
pseudo label generation
in the classiﬁcation process, we calculate the similarity between image features
and label features to obtain a similarity map f, and then directly use the result
of global average pooling on the similarity map as a class prediction.
referring to [1] and combined with our own experiments, we set α to
114
s. zhang et al.
10.
in order to make full use of the shallow information of the network,
we perform weighted fusion on the localization maps from diﬀerent stages by the
following formula:
fall = γ2 ˆf2 +
3
experiments
3.1
dataset
luad-histoseg2
[7] is a weakly-supervised histological semantic segmenta-
tion dataset for lung adenocarcinoma.
bcss-wsss3 is a weakly supervised tissue
semantic segmentation dataset extracted from the fully supervised segmenta-
tion dataset bcss [3], which contains 151 representative h&e-stained breast
cancer pathology slides.
3.2
implementation details
for the classiﬁcation part, we adopt mixtransformer
tpro for weakly supervised histopathology tissue segmentation
115
knowledge encoder, respectively.
the hyperparameters during training and eval-
uation can be found in the supplementary materials.
we conduct all of our
experiments on 2 nvidia geforce rtx 2080 ti gpus.
[18] consists of a classiﬁcation and a segmentation branch, and table 1
displays the pseudo-label scores generated by the classiﬁcation branch.
[18] for
single-label image segmentation, with the segmentation branch simpliﬁed to
binary segmentation to reduce the diﬃculty, while our dataset consists of multi-
label images.
[20] in terms of the quality of the generated pseudo-labels, with
its proposed progressive dropout attention eﬀectively expanding the coverage
of target regions beyond what cam [20] can achieve.
our proposed method
outperformed all previous methods on both luad-histoseg and bcss-wsss
datasets, with improvements of 2.64% and 5.42% over the second-best method,
respectively (table 2).
table 2. comparison of the ﬁnal segmentation results between our method and the
methods in previous years.
[7]
73.90
77.48
73.61
69.53
73.63
74.54
64.45
52.54
58.67
62.55
tpro (ours)
75.80 80.56 78.14 72.69 76.80 77.95 65.10 54.55 64.96 65.64
comparison on segmentation results.
to further evaluate our proposed
method, we trained a segmentation model using the extracted pseudo-labels and
compared its performance with previous methods.
as we have previously analyzed since the datasets
we used are all multi-label images, it was challenging for the segmentation branch
of transws
experimental results also indicate that the iou scores of its segmentation
116
s. zhang et al.
table 3. comparison the eﬀectiveness of
label text(lt), knowledge text(kt), and
deep supervision(ds).
lt ds kt te
nec
lym
tas
miou
68.11
75.24
64.95
66.57
68.72
✓
72.39
72.44
71.37
68.67
71.22
✓
✓
72.41
72.11
74.21
70.07
72.20
✓
✓
✓
74.82 77.55 76.40 70.98 74.94
table 4. comparison of pseudo labels
extracted from the single stage and our
fused version.
te
nec
lym
tas
miou
stage2 67.16
65.28
67.38
55.09
63.73
stage3 72.13
70.83
73.47
69.46
71.47
stage4 72.69
77.57
76.06
69.81
74.03
fusion
74.82 77.55 76.40 70.98 74.94
branch were even lower than the pseudo-labels of the classiﬁcation branch.
by
training the segmentation model of oeem [11] using the pseudo-labels extracted
by cam [20] in table 1, we can observe a signiﬁcant improvement in the ﬁnal
segmentation results.
the ﬁnal segmentation results of mlps [7] showed some
improvement compared to its pseudo-labels, indicating the eﬀectiveness of the
multi-layer pseudo supervision and classiﬁcation gate mechanism strategy pro-
posed by mlps [7].
our segmentation performance surpassed all previous meth-
ods.
additionally,
it is worth noting that we did not use any strategies speciﬁcally designed for the
segmentation stage.
3.4
ablation study
the results of our ablation experiments are presented in table 3.
these ﬁndings demonstrate the signiﬁcant
contribution of each proposed module to the overall improvement of the results.
in order to demonstrate the eﬀectiveness of fusing pseudo-labels from the last
three stages, we have presented in table 4 the iou scores for each stage’s pseudo-
labels as well as the fused pseudo-labels.
it can be observed that after fusing the
pseudo-labels, not only have the iou scores for each class substantially increased,
but the miou score has also increased by 0.91% compared to the fourth stage.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_58.pdf:
the fpn is pre-trained to produce sim-
ilar representations for the same voxel in diﬀerent augmented contexts
and distinctive representations for diﬀerent voxels.
this results in uni-
ﬁed multi-scale representations that capture both global semantics (e.g.,
body part) and local semantics (e.g., diﬀerent small organs or healthy
versus tumor tissue).
we use vox2vec to pre-train a fpn on more than
6500 publicly available computed tomography images.
we evaluate the
pre-trained representations by attaching simple heads on top of them
and training the resulting models for 22 segmentation tasks.
moreover, a non-linear head trained on top of the frozen
vox2vec representations achieves competitive performance with the fpn
trained from scratch while having 50 times fewer trainable parameters.
the code is available at https://github.com/mishgon/vox2vec.
keywords: contrastive self-supervised representation learning ·
medical image segmentation
1
introduction
medical image segmentation often relies on supervised model training [14], but
this approach has limitations.
firstly, it requires costly manual annotations.
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43907-0_58.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
even small changes in the task may result in a signiﬁcant drop in performance,
requiring re-training from scratch [18].
self-supervised learning (ssl) is a promising solution to these limitations.
then, a simple linear or non-linear head on top of the frozen
pre-trained backbone can be trained for various downstream tasks in a supervised
manner (linear or non-linear probing).
pre-training the backbone
in a self-supervised manner enables scaling to larger datasets across multiple
data and task domains.
in contrastive learning, the model
is trained to produce similar vector representations for augmented views of
the same image and dissimilar representations for diﬀerent images.
contrastive
methods can also be used to learn dense, i.e., patch-level or even pixel- or voxel-
level representations: pixels of augmented image views from the same region
of the original image should have similar representations, while diﬀerent pixels
should have dissimilar ones
[23].
several works have implemented contrastive learning of dense representa-
tions in medical imaging [2,7,25,26,29]. representations in [7,25] do not resolve
nearby voxels due to the negative sampling strategy and the architectural rea-
sons.
this makes them unsuitable for full-resolution segmentation, especially in
linear and non-linear probing regimes.
in
[29], separate global and voxel-wise representations are learned in a contrastive
manner to implement eﬃcient dense image retrieval.
the common weakness of all the above works is that they do not evaluate
their ssl models in linear or non-linear probing setups, even though these setups
are de-facto standards for evaluation of ssl methods in natural images [8,13,23].
our simple negative sampling
strategy and the idea of storing voxel-level representations in a feature pyramid
form result in high-dimensional, ﬁne-grained, multi-scale representations suitable
for the segmentation of diﬀerent organs and tumors in full resolution.
second,
we employ vox2vec to pre-train a fpn architecture on a diverse collection of six
unannotated datasets, totaling over 6,500 ct images of the thorax and abdomen.
we make the pre-trained model publicly available to simplify the reproduction
of our results and to encourage practitioners to utilize this model as a starting
vox2vec
607
point for the segmentation algorithms training.
finally, we compare the pre-
trained model with the baselines on 22 segmentation tasks on seven ct datasets
in three setups: linear probing, non-linear probing, and ﬁne-tuning.
several methods produce dense or pixel-wise vector representations [6,23,28]
to pre-train models for downstream tasks like segmentation or object detection.
the methods initially proposed for natural images are often used to pre-
train models on medical images.
in [25], authors propose the 3d adaptation of
jigsaw puzzle, rotation prediction, patch position prediction, and image-level
contrastive learning.
another common way for pre-training on medical images
is to combine diﬀerent approaches such as rotation prediction [26], restorative
autoencoders
[2,26], and image-level contrastive learning
the model [29] maxi-
mizes the consistency of local features in the intersection between two diﬀerently
augmented images.
[29] was mainly proposed for image retrieval
and uses only feature representations in the largest and smallest scales in separate
contrastive losses, while vox2vec produce voxels’ representations via concatena-
tion of feature vectors from a feature pyramid and pre-train them in a uniﬁed
manner using a single contrastive loss.
finally, a number of works propose semi-
supervised contrastive learning methods [20], however, they require additional
task-speciﬁc manual labeling.
left: two overlapping aug-
mented 3d patches are sampled from each volume in a batch.
we also describe the methodology of the evaluation of the pre-trained
representations on downstream segmentation tasks in sect.
we apply color augmentations to them, including random gaussian
vox2vec
609
blur, random gaussian sharpening, adding random gaussian noise, clipping the
intensities to the random hounsﬁeld window, and rescaling them to the (0, 1)
interval.
in our experiments we set (h, w, d) = (128, 128, 32), n = 10 and m = 1000.
however,
our experiments show that this feature map alone is insuﬃcient for modeling
self-supervised voxel-level representations.
meanwhile, to be suitable for many downstream tasks, rep-
resentations should have a dimensionality of about 1000, as in [8].
to address this issue, we utilize a 3d fpn architecture instead of a stan-
dard 3d unet.
each next pyramid level has twice as many channels
and two times lower resolution than the previous one.
we use fpn with six pyramid levels, which results in 1008-dimensional
representations.
i , i = 1, . . . , n. following [8], instead of
penalizing the representations directly, we project them on 128-dimensional unit
sphere via a trainable 3-layer perceptron g(·) followed by l2-normalization: z(1)
i
=
g(h(1)
i )/∥g(h(1)
i )∥, z(2)
610
m. goncharov et al.
3.4
evaluation protocol
we evaluate the quality of self-supervised voxel-level representations on down-
stream segmentation tasks in three setups: 1) linear probing, 2) non-linear prob-
ing, and 3) end-to-end ﬁne-tuning.
linear or non-linear probing means training a voxel-wise linear or non-linear
classiﬁer on top of the frozen representations.
if the representations are modeled
by the unet model, such classiﬁer can be implemented as one or several 1 × 1
convolutional layers with a kernel size 1 on top of the output feature map.
a
linear voxel-wise head (linear fpn head) can be implemented as follows.
4
experiments
4.1
pre-training
we use vox2vec to pre-train both fpn and unet models (further vox2vec-
fpn and vox2vec-unet) in order to ablate the eﬀect of using a feature pyramid
instead of single full-resolution feature map for modeling voxel-wise representa-
tions.
[1,3,5,15,21,27], totaling
more than 6550 cts, covering abdomen and thorax domains.
we do not use
the annotations for these datasets during the pre-training stage.
pre-processing
includes the following steps: 1) cropping to the minimal volume containing all
the voxels with the intensity greater than −500 hu; 2) interpolation to the voxel
spacing of 1 × 1 × 2 mm3 (intensities are clipped and rescaled at the augmen-
tation step, see sect. 3.1).
both models are trained on
a single a100-40gb gpu for an average of 3 days.
further details about the
pre-training setup can be found in supplementary materials.
4.2
evaluation
we evaluate our method on the beyond the cranial vault abdomen (btcv)
[19]
and medical segmentation decathlon (msd)
we test our
method on 6 ct msd datasets, which include 9 diﬀerent organ and tumor
segmentation tasks.
a 5 fold cross-validation is used for btcv experiments, and
vox2vec
611
a 3 fold cross-validation for msd experiments.
the segmentation performance
of each model on btcv and msd datasets is evaluated by the dice score.
for our method, the pre-processing steps are the same for all datasets, as at
the pre-training stage, but in addition, intensities are clipped to (−1350, 1000)
hu window and rescaled to (0, 1).
in these experiments, we keep the crucial pipeline hyperpa-
rameters (e.g., spacing, clipping window, patch size) the same as in the original
works.
we
demonstrate an example of the excellent performance of vox2vec-fpn in both
linear and non-linear probing regimes in supplementary materials.
we reproduce the key results on msd challenge ct datasets, which contain
tumor and organ segmentation tasks.
a t-sne embedding of vox2vec
representations on msd is available in the supplementary materials.
average cross validation dice scores
on btcv multi-organ segmentation dataset.
2.
dice score on btcv cross-
validation averaged for all organs w.r.t.
we pre-train a fpn architecture on
more than 6500 ct images and test it on various segmentation tasks, including
diﬀerent organs and tumors segmentation in three setups: linear probing, non-
linear probing, and ﬁne-tuning.
we plan to investigate
further how the performance of vox2vec scales with the increasing size of the
vox2vec
613
pre-training dataset and the pre-trained architecture size.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_64.pdf:
transfer learning is a critical technique in training deep neu-
ral networks for the challenging medical image segmentation task that
requires enormous resources.
with the abundance of medical image data,
many research institutions release models trained on various datasets
that can form a huge pool of candidate source models to choose from.
to make up for its deﬁciency when applying trans-
fer learning to medical image segmentation, in this paper, we therefore
propose a new transferability estimation (te) method.
we ﬁrst
analyze the drawbacks of using the existing te algorithms for medical
image segmentation and then design a source-free te framework that
considers both class consistency and feature variety for better estima-
tion.
extensive experiments show that our method surpasses all current
algorithms for transferability estimation in medical image segmentation.
keywords: transferability estimation · model selection · medical
image analysis · deep learning
1
introduction
the development of deep neural networks has greatly promoted medical imaging-
based computer-aided diagnosis.
however, the labeling process of medical images is tedious and time-
consuming.
to address this problem, the common paradigm of transfer learning,
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43907-0_64.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
https://doi.org/10.1007/978-3-031-43907-0_64
pick the best pre-trained model
675
which ﬁrst pre-trains a model on upstream image datasets and then ﬁne-tunes it
on various target tasks, has been widely investigated in recent years
[18] enable researchers
to experiment across a large number of downstream datasets and tasks.
these
pre-trained models require less training time and have better performance and
robustness compared with the learning-from-scratch models.
when the knowledge is transferred from a less
relevant source, it may not improve the performance or even negatively aﬀect
the intended outcome [24].
however, most of these works
require source information available while medical images have more privacy and
ethical issues and fewer datasets are publicly available than natural images.
considering the issues mentioned above, this work focused on source-free
pre-trained model selection for segmentation tasks in the medical image.
logme [27] computed evidence based on the linear parameters assumption and
eﬃciently leverages the compatibility between features and labels.
these methods have achieved
promising performance on classiﬁcation and regression tasks without fully con-
sidering the properties of medical image segmentation.
first, unlike classiﬁcation
and regression problems that can use a single n-dimensional feature vector to rep-
resent each image, segmentation problems lack a global semantic representation,
which poses diﬃculties for direct transferability estimation.
third, medical images face severe class imbalance problems,
with excessive diﬀerences between foreground and background.
676
y. yang et al.
fine-tune
performance
model bank 
dnn1
upstream data
downstream data
dnn2
dnn3
pre-train-then-fine-tune process
pre-train
match or not?
our main goal is to
predict the performance of models in the model bank after ﬁne-tuning on downstream
tasks without actually ﬁne-tuning.
besides, for semantic segmentation tasks, the feature pyramid is critical for
the segmentation output of multi-scale objects while existing works neglect it.
in our work, we propose a new method using class consistency and feature
variety(cc-fv) with an eﬃcient framework to estimate the transferability in
medical image segmentation tasks.
extensive experiments
have proved the superiority of our method compared with baseline methods.
= {xj,yj}n
j=1, where
xj is the image and yj is the ground truth of segmentation.
after ﬁne-tuning,
the performance of mi can be measured with the segmentation metric (e.g. dice
score), which is denoted by pi
s→t in this paper.
given a pair of target data xj and xj′, the distribution of the features is
modeled with the n-dimensional gaussian distribution.
and σf k
j
and
σf k
j′ are covariance matrices of fk
j and fk
j′. compared to some commonly used
metrics like kl-divergence or bhattacharyya distance [17], wasserstein distance
is more stable during the computation of high-dimensional matrices because it is
unnecessary to compute the determinant or inverse of a high-dimensional matrix,
which can easily lead to an overﬂow in numerical computation.
we calculate the
wasserstein distance of the distribution with voxels of the same class in a sample
pair comprised of every two samples in the dataset, and obtained the following
deﬁnition of class consistency ccons
ccons =
1
n(n − 1)
c

k=1

ij
w2(fk
i , fk
j )
(2)
given that 3d medical images are computationally intensive, and prone to
causing out-of-memory problems, in the sliding window inference process for
678
y. yang et al.
each case, we do not concatenate the output of each patch into the ﬁnal predic-
tion result, but directly sample from the patched output and concatenate them
into the ﬁnal sampled feature matrix.
in the calculation of class consistency, we
only sample the foreground voxels with a pre-deﬁned sampling number which is
proportional to the voxel number of each class in the image because of the severe
class imbalance problem.
we
believe that the essential reason for this phenomenon is that class consistency is
only concerned with local homogeneity of information while neglecting the inte-
gral feature quality assessment.
ij log
vi − vj
−1
,
s = 0
(3)
here v is sampled feature of each image with point-wise embedding vi and
l is the length of the feature, which is also the number of sampled voxels.
as for semantic segmentation problems, the feature pyra-
mid structure is critical for segmentation results [14,29].
pick the best pre-trained model
679
3
experiment
3.1
experiment on msd dataset
the medical segmentation decathlon (msd)
[2] dataset is composed of ten dif-
ferent datasets with various challenging characteristics, which are widely used in
the medical image analysis ﬁeld.
to evaluate the eﬀectiveness of cc-fv, we con-
duct extensive experiments on 5 of the msd dataset, including task03 liver(liver
and tumor segmentation), task06 lung(lung nodule segmentation), task07 pan-
creas(pancreas and pancreas tumor segmentation), task09 spleen(spleen seg-
mentation), and task10 colon(colon cancer segmentation).
all of the datasets
are 3d ct images.
the public part of the msd dataset is chosen for our experi-
ments, and each dataset is divided into a training set and a test set at a scale of
80% and 20%.
for each dataset, we use the other four datasets to pre-train the
model and ﬁne-tune the model on this dataset to evaluate the performance as
well as the transferability using the correlation between two ranking sequences
of upstream pre-trained models.
[15] are also implemented.
figure 2 visualizes the average dice score and the estimation value on task
03 liver.
[20]
and unetr [8] are applied in the experiment and each model is pre-trained
for 250k iterations and ﬁne-tuned for 100k iterations with batch size 2 on a
single nvidia a100 gpu.
besides, we use the model at the end of training
for inference and calculate the ﬁnal dsc performance on the test set.
[27] and pearson correlation coeﬃcient for the cor-
relation between the te results and ﬁne-tuning performance.
the kendall’s τ
ranges from [-1, 1], and τ=1 means the rank of te results and performance
are perfectly correlated(t i
s→t >
it is clear that the
te results of our method have a more positive correlation with respect to dsc
performance.
most
of the existing methods are inferior to ours because they are not designed for seg-
mentation tasks with a serious class imbalance problem.
2. correlation between the ﬁne-tuning performance and transferability metrics
using task03 as an example.
the vertical axis represents the average dice of the
model, while the horizontal axis represents the transferability metric results.
we have
standardized the various metrics uniformly, aiming to observe a positive relationship
between higher performance and higher transferability estimations.
pre-trained models tend to have a more consistent
distribution within a class than the randomly initialized model and after ﬁne-tuning
they often have a better dice performance than the randomly initialized models.
then we com-
pare the performance of our method at single and multiple scales to prove the
eﬀectiveness of our multi-scale strategy.
kl-divergence and bha-distance are unstable in
high dimension matrics calculation and the performance is also inferior to the
wasserstein distance.
we can easily ﬁnd that with models with a pre-training process
have a more compact intra-class distance and a higher ﬁne-tuning performance.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_71.pdf:
cell segmentation plays a critical role in diagnosing various
cancers.
in this paper, we present
a novel framework for cross-tissue domain adaptative cell segmentation
without access both source domain data and model parameters, namely
multi-source black-box domain adaptation (mbda).
given the target
domain data, our framework can achieve the cell segmentation based on
knowledge distillation, by only using the outputs of models trained on
multiple source domain data.
sec-
ond, we design a pseudo-label cutout and selection strategy for these pre-
dictions to facilitate the knowledge distillation from local cells to global
pathological images.
experimental results on four types of pathologi-
cal tissues demonstrate that our proposed black-box domain adaptation
approach can achieve comparable and even better performance in com-
parison with state-of-the-art white-box approaches.
the code and dataset
are released at: https://github.com/neuronxjtu/mbda-cellseg.
keywords: multi-source domain adaptation · black-box model · cell
segmentation · knowledge distillation
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
https://doi.org/10.1007/978-3-031-43907-0_71
750
x. wang et al.
1
introduction
semantic segmentation plays a vital role in pathological image analysis.
it can
help people conduct cell counting, cell morphology analysis, and tissue analysis,
which reduces human labor [19].
however, data acquisition for medical images
poses unique challenges due to privacy concerns and the high cost of manual
annotation.
moreover, pathological images from diﬀerent tissues or cancer types
often show signiﬁcant domain shifts, which hamper the generalization of mod-
els trained on one dataset to others.
due to the abovementioned challenges,
some researchers have proposed various white-box domain adaptation methods
to address these issues.
recently, [8,16] propose to use generative adversarial networks to align the
distributions of source and target domains and generate source-domain look-
alike outputs for target images.
there are also many studies
on multi-source white-box domain adaptation.
[13] extend the above work to semantic segmentation and proposed
a method named model-invariant feature learning, which takes full advantage of
the diverse characteristics of the source-domain models.
nonetheless, several recent investigations have demonstrated that the domain
adaptation methods for source-free white-box models still present a privacy risk
due to the potential leakage of model parameters [4].
such privacy breaches
may detrimental to the privacy protection policies of hospitals.
we thus present
a more challenging task of relying solely on black-box models from vendors to
avoid parameter leakage.
in clinical applications, various vendors can oﬀer output
interfaces for diﬀerent pathological images.
while black-box models are proﬁcient
in speciﬁc domains, their performances greatly degrade when the target domain
is updated with new pathology slices.
therefore, how to leverage the existing
knowledge of black-box models to eﬀectively train new models for the target
domain without accessing the source domain data remains a critical challenge.
in this paper, we present a novel source-free domain adaptation framework
for cross-tissue cell segmentation without accessing both source domain data
and model parameters, which can seamlessly integrate heterogeneous models
from diﬀerent source domains into any cell segmentation network with high gen-
erality.
to the best of our knowledge, this is the ﬁrst study on the exploration of
multi-source black-box domain adaptation for cross-tissue cell segmentation.
in
this setting, conventional multi-source ensemble methods are not applicable due
to the unavailability of model parameters, and simply aggregating the black-box
outputs would introduce a considerable amount of noise, which can be detri-
mental to the training of the target domain model.
therefore, we develop two
black-box domain adaptative cell segmentation
751
multi-source 
models
(black-box)
logits map
prediction uncertainty boundary ambiguity
score
weighted logits map
confidence threshold
adaptive
unlabeled
labeled
pseudo-cutout 
label
student model
pixel-level weight
target domain
0.97 0.90
0.95
0.95
0.95
0.92
0.93 0.99
0.92 0.92
0.95 0.92
vote
teacher model
ema
fig.
this method eﬀectively addresses two signiﬁcant
challenges encountered in the analysis of cellular images, namely, the uncertainty
in source domain output and the ambiguity in cell boundary semantics.
secondly,
we also take into account the structured information from cells to images, which
may be overlooked during distillation, and design an adaptive knowledge vot-
ing strategy.
2
method
overview: figure 1 shows a binary cell segmentation task with three source
models trained on diﬀerent tissues and a target model, i.e., the student model
in fig.
the η
and η′ indicate that diﬀerent perturbations are added to the target images.
sub-
sequently, we feed the perturbed images into the source domain predictor to
generate the corresponding raw segmentation outputs.
finally, we obtain a weighted logit for knowledge distillation
from pixel level and a high-conﬁdence pseudo-cutout label for further structured
distillation from cell to global pathological image.
accordingly, direct knowledge transfer using the output of the source domain
predictor may lead to feature bias in the student model due to the unavoid-
able covariance [20] between the target and source domains.
for a given target image xt
to leverage the rich
semantic information from the source domain predictor predictions, we utilize
predictive entropy of the softmax outputs to measure the prediction uncertainty
scores.
in the semantic segmentation scenario of c-classes classiﬁcation, we deﬁne
the pixel-level uncertainty score u(i,j)
n
as follow:
u(i,j)
n
= −
c

c=1
on(i,j,c)
s
log on(i,j,c)
s
(2)
where on
s denotes softmax output,i.e.,on
s = softmax(pn
s ) from nth source predic-
tor.
after that, we determine the degree of
black-box domain adaptative cell segmentation
753
impurity in an area of interest by analyzing the statistics of the boundary region,
which represents the level of semantic information ambiguity.
by assigning lower weights to the pixels with high uncertainty and boundary
ambiguity, we can obtain pixel-level weight scores wn for each pn
s , i.e.,
wn = − log

exp (un ⊙ pn)
n
n=1 exp (un ⊙ pn)

(4)
where ⊙ denotes element-wise matrix multiplication.
adaptive pseudo-cutout label: as previously mentioned, the outputs from
the source domain black-box predictors have been adjusted by the pixel-level
weight.
we have revised the method in [7]
to generate high-quality pseudo labels that resemble the cutout augmentation
technique.
[15].
we adopt the classical and eﬀective mean-teacher framework as a baseline
for semi-supervised learning and update the teacher model parameters by expo-
nential moving average.
3
experiments
dataset and setting: we collect four pathology image datasets to validate our
proposed approach.
firstly, we acquire 50 images from a cohort of patients with
triple negative breast cancer (tnbc), which is released by naylor et al
[10] publish a dataset of nucleus segmentation containing 5,060 seg-
mented slides from 10 tcga cancer types.
in this work, we use 98 images from
black-box domain adaptative cell segmentation
755
fig.
2. visualized segmentation on the brca and kirc target domains respectively.
we have also included 463 images
of kidney renal clear cell carcinoma (kirc) in our dataset, which are made
publicly available by irshad et al
[2] publicly release a dataset
containing tissue slide images and associated clinical data on colorectal cancer
(crc), from which we randomly select 200 patches for our study.
in our exper-
iments, we transfer knowledge from three black-box models trained on diﬀerent
source domains to a new target domain model (e.g.,from crc, tnbc, kirc
to brca).
the backbone network for the student model and source domain
black-box predictors employ the widely adopted residual u-net [12], which is
commonly used for medical image segmentation.
for each source domain net-
work, we conduct full-supervision training on the corresponding source domain
data and directly evaluate its performance on target domain data.
the upper
performance metrics (source-only upper) are shown in the table 1.
experimental results: to validate our method, we compare it with the fol-
lowing approaches: (1) cellsegssda
for single-source domain
adaptation approach, cellsegssda and sfda-dpl, we employ two strategies
to ensure the fairness of the experiments: (1) single-source, i.e. performing adap-
tation on each single source, where we select the best results to display in the
table 1; (2) source-combined, i.e. all source domains are combined into a tra-
ditional single source.
2 demonstrate that our proposed
method exhibits superior performance, even when compared to these white-box
methods, surpassing them in various evaluation metrics and visualization results.
in addition, the experimental results also show that simply combining multiple
756
x. wang et al.
table 1.
quantitative comparison with unsupervised and semi-supervised domain
adaptation methods under 3 segmentation metrics.
crc&kirc&brca to tnbc
wl pcl mmi
dice
hd95
assd
×
×
×
0.6708
56.9111
16.3837
✓
×
×
0.6822
54.3386 14.9817
✓
✓
×
0.6890
57.0889
12.9512
✓
✓
✓
0.7075 58.8798
10.7247
source data into a traditional single source will result in performance degrada-
tion in some cases, which also proves the importance of studying multi-source
domain adaptation methods.
ablation study: to evaluate the impact of our proposed methods of weighted
logits(wl), pseudo-cutout label(pcl) and maximize mutual information(mmi)
on the model performance, we conduct an ablation study.
the results of these experiments, presented in the table 2, show that our pro-
posed modules are indeed useful.
4
conclusion
our proposed multi-source black-box domain adaptation method achieves com-
petitive performance by solely relying on the source domain outputs, without
the need for access to the source domain data or models, thus avoiding informa-
tion leakage from the source domain.
additionally, the method does not assume
black-box domain adaptative cell segmentation
757
the same architecture across domains, allowing us to learn lightweight target
models from large source models, improving learning eﬃciency.
by leveraging multi-source domain
knowledge, we aim to improve the reliability of the target model and enable more
eﬃcient annotation for better model performance.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_59.pdf:
statistical shape modeling is the computational process of
discovering signiﬁcant shape parameters from segmented anatomies cap-
tured by medical images (such as mri and ct scans), which can fully
describe subject-speciﬁc anatomy in the context of a population.
the
presence of substantial non-linear variability in human anatomy often
makes the traditional shape modeling process challenging.
we propose mesh2ssm, a new approach that leverages
unsupervised, permutation-invariant representation learning to estimate
how to deform a template point cloud to subject-speciﬁc meshes, form-
ing a correspondence-based shape model.
mesh2ssm can also learn a
population-speciﬁc template, reducing any bias due to template selec-
tion.
keywords: statistical shape modeling · representation learning ·
point distribution models
1
introduction
statistical shape modeling (ssm) is a powerful tool in medical image analysis
and computational anatomy to quantify and study the variability of anatomical
structures within populations.
[19,25], and treatment
planning [27].
ssm has enabled researchers to better understand the underlying
biological processes, leading to the development of more accurate and personal-
ized diagnostic and treatment plans
[3,9,14,17].
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43907-0 59.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
[5].
ssm performance depends on the underlying process used to generate shape
correspondences and the quality of the input data.
non-optimized methods
manually label a reference shape and warp the annotated landmarks using reg-
istration techniques [10,16,18].
[15] and shapeflow [11] operate
on surface meshes and use neural networks to parameterize the deformations ﬁeld
between two shapes in a low dimensional latent space and rely on an encoder-
free setup.
[1] learn the pdm directly from unsegmented ct/mri images, and
hence alleviate the need for pdm optimization given new samples and can bypass
anatomy segmentation by operating directly on unsegmented images.
however,
these methods rely on supervised losses and require volumetric images, seg-
mented images, and established/optimized pdms for training.
mesh2ssm leverages unsupervised, permutation-invariant representation learn-
ing to learn the low dimensional nonlinear shape descriptor directly from mesh
data and uses the learned features to generate a correspondence model of the
population.
mesh2ssm also includes an analysis network that operates on the
learned correspondences to obtain a data-driven template point cloud (i.e., tem-
plate point cloud), which can replace the initial template, and hence reducing
the bias that could arise from template selection.
we performed experiments with three templates: medoid, sphere,
and box without the bump.
moreover, flowssm fails to identify the correct mode
of variation, the horizontal movement of the bump as the primary variation,
which can also be inferred by comparing the compactness curves in fig.
flowssm
fails to capture the horizontal movement as the primary mode of variation.
this goal is achieved by learning a low dimensional represen-
tation of the surface mesh zm ∈ rl using the mesh autoencoder and then zm is
mesh2ssm
619
used to transform the template point cloud via the implicit ﬁeld decoder (im-
net)
by capturing the underlying structure of the
pdm through a low-dimensional representation, sp-vae allows for the estima-
tion of the mean shape of the learned correspondences.
importantly, the sp-vae maintains
the same ordering of correspondences at the input and output, so it does not
use permutation-invariant layers or operations like pooling.
2.3
training
we begin with a burn-in stage, where only the correspondence generation mod-
ule is trained while the analysis module is frozen.
after the burn-in stage, alter-
nate optimization of the correspondence and analysis module begins.
the mean template is deﬁned by taking the average of these gener-
ated samples.
all hyperparameters and network
architecture details are mentioned in the supplementary material.
3
experiments and discussion
dataset: we use the publicly available decath-pancreas dataset of 273 seg-
mentations from patients who underwent pancreatic mass resection
the segmentations were isotropi-
cally resampled, smoothed, centered, and converted to meshes with roughly 2000
vertices.
although the dgcnn mesh autoencoder used in mesh2ssm does not
require the same number of vertices, uniformity across the dataset makes it com-
putationally eﬃcient; hence, we pad the smallest mesh by randomly repeating
the vertices (akin to padding image for convolutions).
the color map and arrows show
the signed distance and direction from the mean shape.
3.1
results
we perform experiments with two templates: sphere and medoid.
we compare
the performance of flowssm
generalization measures the average sur-
face distance between all test shapes and their reconstructions, and speciﬁcity
measures the distance between randomly generated pca samples.
using
the analysis module of mesh2ssm, we visualized the top three modes of varia-
tion identiﬁed by sorting the latent dimensions of sp-vae based on the stan-
dard deviations of the latent embeddings of the training dataset.
like most deep learning models, perfor-
mance of mesh2ssm could be aﬀected by small dataset size, and it can produce
overconﬁdent estimates.
an augmentation scheme and a layer uncertainty cali-
bration are could improve its usability in medical scenarios.
additionally, enforc-
ing disentanglement in the latent space of sp-vae can make the analysis module
interpretable and allow for eﬀective non-linear shape analysis by clinicians.
[13,21] analysis module helps in mitigating bias and
capturing non-linear characteristics of the data.
the method is demonstrated to
have superior performance in identifying shape variations using fewer parameters
on synthetic and clinical datasets.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_57.pdf:
multiple instance learning is an ideal mode of analysis for
histopathology data, where vast whole slide images are typically anno-
tated with a single global label.
in such cases, a whole slide image is
modelled as a collection of tissue patches to be aggregated and classi-
ﬁed.
although powerful compression algo-
rithms, such as deep pre-trained neural networks, are used to reduce
the dimensionality of each patch, the sequences arising from whole slide
images remain excessively long, routinely containing tens of thousands
of patches.
across experiments
in metastasis detection, cancer subtyping, mutation classiﬁcation, and
multitask learning, we demonstrate the competitiveness of this new class
of models with existing state of the art approaches.
keywords: multiple instance learning · whole slide images · state
space models
1
introduction
precision medicine eﬀorts are shifting cancer care standards by providing novel
personalised treatment plans with promising outcomes.
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43907-0 57.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
https://doi.org/10.1007/978-3-031-43907-0_57
state space models in digital pathology
595
treatment regimes is based principally on the assessment of tissue biopsies and
the characterisation of the tumor microenvironment.
this is typically performed
by experienced pathologists, who closely inspect chemically stained histopatho-
logical whole slide images (wsis).
the resulting images are of gigapixel size, rendering their computational analysis
challenging.
in
such schemes, the wsi is typically divided into a grid of patches, with general
purpose features derived from pretrained imagenet
in this paper, we present the
ﬁrst use of state space models for wsi mil. extensive experiments on three
publicly available datasets show the potential of such models for the processing
of gigapixel-sized images, under both weakly and multi-task schemes.
moreover,
comparisons with other commonly used mil schemes highlight their robust per-
formance, while we demonstrate empirically the superiority of state space models
in processing the longest of wsi sequences with respect to commonly used mil
methods.
similar to our multitask experiments, [6] explores combining slide-level and tile-
level annotations with a minimal point-based annotation strategy.
the hippo mode of memorisation is shown empirically to be better suited to
modeling long-range dependencies (lrd) than other neural memory layers, for
which it serves as a drop-in replacement.
note
that as a linear operator, the inverse discrete fourier transform is amenable
to backpropagation in the context of a neural network.
3
method
given that the patch extraction of whole slide images at high magniﬁcations
results in long sequences of patches, we propose to incorporate a state space
layer in a mil aggregation network to better represent each patch sequence.
note also that although eq. 3 is posed as modeling a one-
dimensional signal, in practice multi-dimensional inputs are modelled simply by
stacking ssm layers together, followed by an aﬃne “mixing” layer.
fig.
a pretrained resnet50 is then used
to extract a 1024-dimensional feature vector from each patch {u1, u2, . .
(5)
the architecture of f is composed of an initial linear projection layer, used to
lower the dimensionality of each vector in the input sequence.
a linear “mixing” layer is applied
token-wise, doubling the dimensionality of each token, followed by a gated linear
unit [5] acting as an output gate, which restores the input dimensionality.
for
598
l. fillioux et al.
the ssm layer, we used the oﬃcial implementation of s4d1.
the model is trained
according to,
lmil = − 1
m
m

m=1
log ˆycm,
(6)
where ˆycm denotes the probability corresponding to cm, the slide-level label of
the sequence corresponding to the mth of m whole slide images.
3.3
multitask training
one advantage of processing an entire slide as a sequence is the ease with
which additional supervision may be incorporated, when available.
3.4
implementation details
we extracted patches of size 256 × 256 from the tissue regions of wsis at 20x
magniﬁcation.
[13] was used as a feature extractor, followed by a mean pooling oper-
ation, resulting in a 1024-dimensional representation for each patch.
all model training was performed under
a 10-fold cross-validation, and all reported results are averaged over the vali-
dation sets of the folds, aside from camelyon16, for which the predeﬁned
test set was utilized.
thus, for camelyon16, we report test set performances
averaged over the validation.
our vanilla transformer is composed of two stacked
self-attention blocks, with four attention heads, a model dimension of 256, and
1 https://github.com/hazyresearch/state-spaces.
state space models in digital pathology
599
a hidden dimension of 256.
for the s4
models, the dimension of the state matrix a was tuned to 32 for camelyon16
and tcga-rcc, and 128 for tcga-luad.
our implementation is publicly available2.
4
experiments and discussion
4.1
data
camelyon16
in multitask experiments, we use
this annotation to give each patch a label indicating local tumour presence.
in our experiments, the average patch sequence length arising from
camelyon16 is 6129 (ranging from 127 to 27444).
the average sequence length is 10557
(ranging from 85 to 34560).
the average sequence length is 12234 (ranging from 319 to 62235).
for multiclass classiﬁcation, these were computed in a one-versus-rest
manner.
similarly, in the tcga-luad dataset the proposed model achieves comparable
performance with both clam models, while outperforming transmil and the
other methods.
moreover, our method outperforms clam models on the
tcga-rcc dataset, while reporting very similar performance with respect to
transmil.
overall, looking at the average metrics per model across all three
datasets, our proposed method achieves the highest accuracy and the second
highest auroc, only behind clam-mb.
a pairwise t-test between the pro-
posed method, clam, and transmil shows that there is no statistical signiﬁ-
cance performance diﬀerence (see supplementary material).
the number of parameters is computed with all models con-
ﬁgured to be binary classiﬁers, and the inference time is computed as the average
time over 100 samples for processing a random sequence of 1024-dimensional vec-
tors of length 30000.
for our proposed method, we report both models with the
diﬀerent state dimensions (ours (ssm32)) and (ours (ssm128)).
compared
table 1. comparison of accuracy and auroc on three datasets camelyon16,
tcga-luad, tcga-rcc, and on average.
all metrics in the table are the average
of 10 runs.
∗ indicates results from [19].
dataset
camelyon16
tcga-luad
tcga-rcc
average
metric
acc.
auroc acc.
auroc acc.
auroc acc.
auroc
mean-pooling
0.5969
0.5810
0.6261
0.6735
0.8608
0.9612
0.6946
0.7386
max-pooling
0.7078
0.7205
0.6328
0.6686
0.8803
0.9659
0.7403
0.7850
transformer [21] 0.5419
0.5202
0.5774
0.6214
0.7932
0.9147
0.6375
0.6854
lstm
models a and
b show that stacking multiple ssm layers results in lower accuracy, which was
observed over all three datasets, while models c and d show that modifying the
state dimension of the ssm module can have an impact on the accuracy.
the
optimal state space dimension varies depending on the dataset.
this
indicates that the use of patch-level annotations complements the learning of the
slide-level label.
we map the sequence of out-
put probabilities to their slide coordinates giving a heatmap localising metastasis
(see supplementary material).
model ssm layers state dimension accuracy auroc
a
2
32
0.9236
0.9813
b
3
32
0.9179
0.9834
c
1
16
0.9352
0.9846
d
1
64
0.9352
0.9861
ours
1
32
0.9426
0.9885
table 4. comparison of accuracy and auroc for models trained as multitask classi-
ﬁers on the camelyon16 dataset.
[19] 0.8403
0.8828
ours
0.8488
0.8998
602
l. fillioux et al.
performance on longest sequences.
in order to highlight the inherent
ability of ssm models to eﬀectively model long sequences, we performed an
experiment on only the largest wsis of the tcga-rcc dataset.
table 5 shows the
obtained average accuracy (weighted by the number of long sequences in each
validation set) and auroc on both clam models, transmil, and our proposed
method.
both in terms of auroc and accuracy, our method outperforms the
other methods on long sequences, while the performances are comparable to
table 1, albeit slightly lower, illustrating the challenge of processing large wsis.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_7.pdf:
we developed vesselvae, a recursive
variational neural network that fully exploits the hierarchical organiza-
tion of the vessel and learns a low-dimensional manifold encoding branch
connectivity along with geometry features describing the target surface.
by leveraging the power of deep neural networks, we
generate 3d models of blood vessels that are both accurate and diverse,
which is crucial for medical and surgical training, hemodynamic simula-
tions, and many other purposes.
these meshes are typically generated using
either image segmentation or synthetic methods.
despite signiﬁcant advances in
vessel segmentation [26], reconstructing thin features accurately from medical
images remains challenging [2].
manual editing of vessel geometry is a tedious
and error prone task that requires expert medical knowledge, which explains the
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43907-0_7.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
https://doi.org/10.1007/978-3-031-43907-0_7
68
p. feldman et al.
scarcity of curated datasets.
in recent years, deep neural networks led to the development of powerful gen-
erative models [30], such as generative adversarial networks [8,12] and diﬀusion
models [11], which produced groundbreaking performance in many applications,
ranging from image and video synthesis to molecular design.
our generative framework is based on a
recursive variational neural network (rvnn), that has been applied in various
contexts, including natural language
[23,24], shape semantics modeling
[14,15],
and document layout generation [20].
in contrast to previous data-driven meth-
ods, our recursive network fully exploits the hierarchical organization of the ves-
sel and learns a low-dimensional manifold encoding branch connectivity along
with geometry features describing the target surface.
experiments show that synth and real blood vessel
geometries are highly similar measured with the cosine similarity: radius (.97),
length (.95), and tortuosity (.96).
formally, each tree is deﬁned as a tuple (t, e), where t is the set of
nodes, and e is the set of directed edges connecting a pair of nodes (n, m), with
n, m ∈ t. in order to encode a 3d model into this representation, vessel segments
v are parameterized by a central axis consisting of ordered points in euclidean
space: v = v1, v2, . . .
similarly, the decoder only uses
right/left dec-mlps when the node classiﬁer predicts bifurcations.
where each node ni represents a vessel segment v and contains an attribute vector
xi =
the encoder transforms a tree struc-
ture into a hierarchical encoding on the learned manifold.
this
70
p. feldman et al.
is implemented as a multi-layer perceptron trained to predict a three-category
bifurcation probability based on the encoded vector as input.
complementing
the node classiﬁer, the features dec-mlp is responsible for reconstructing the
attributes of each node, speciﬁcally its coordinates and radius.
in addition to the core architecture, our model is further augmented with
three auxiliary, shallow, fully-connected neural networks: fμ, fσ, and gz.
lectively, these supplementary networks streamline the data transformation pro-
cess through the model.
see the appendix for implementation details.
objective.
our generative model is trained to learn a probability distribution
over the latent space that can be used to generate new blood vessel segments.
the implemented
method iterates through the points in the curve generating a coarse quadrilateral
recursive variational autoencoders for 3d blood vessel synthesis
71
fig.
mesh along the segments and joints.
3
experimental setup
materials.
this subset consisted of 1694
healthy vessel segments reconstructed from 2d mra images of patients.
the centerline points were determined based on the ratio
between the sphere step and the local maximum radius, which was computed
using the advancement ratio speciﬁed by the user.
to improve compu-
tational eﬃciency during recursive tree traversal, we implemented an algorithm
that balances each tree by identifying a new root.
we additionally trimmed trees
to a depth of ten in our experiments.
this decision reﬂects a balance between
the computational demands of depth-ﬁrst tree traversal in each training step
and the complexity of the training meshes.
72
p. feldman et al.
that exhibited greater depth, nodes with more than two children, or with loops.
implementation details.
for the centerline extraction, we set the advance-
ment ratio in the vmtk script to 1.05.
in those cases, we selected the
sample with the lowest radius, which ensures proper alignment with the center-
line principal direction.
the data pre-processing pipeline and network code were implemented in
python and pytorch framework.
in all stages, we set the batch size to 10 and used the adam optimizer
with β1 = 0.9, β2 = 0.999, and a learning rate of 1 × 10−4.
we set α = .3
and γ = .001 for eq. 1 in our experiments.
to enhance computation speed, we
implemented dynamic batching [16], which groups together operations involving
input trees of dissimilar shapes and diﬀerent nodes within a single input graph.
this means that the amount of memory required to store
and manipulate our training data structures is minimal.
during preliminary experiments, we observed that accurately classifying nodes
closer to the tree root is critical.
we deﬁned a set of metrics to evaluate our trained network’s perfor-
mance.
we analyzed
tortuosity per branch, the vessel centerline total length, and the average radius
of the tree.
3. (a) shows the histograms of total length, average radius and tortuosity per
branch for both, real and synthetic samples.
average radius were used in previous work to distinguish healthy vasculature
from cancerous malformations.
4
results
we conducted both quantitative and qualitative analyses to evaluate the model’s
performance.
for the quantitative analyses, we implemented a set of metrics
commonly used for characterizing blood vessels.
we
measured the closeness of histograms with the cosine similarity by projecting
the distribution into a vector of n-dimensional space (n is the number of bins in
the histogram).
we
obtain a radius cosine similarity of .97, a total length cosine similarity of .95,
74
p. feldman et al.
and a tortuosity cosine similarity of .96.
overall, we believe that our
proposed approach holds great promise for advancing 3d blood vessel geometry
synthesis and contributing to the development of new clinical tools for healthcare
professionals.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_6.pdf:
current approaches thus use weakly supervised object detec-
tion to learn the (rough) localization of pathologies from image-level
annotations, which is however limited in performance due to the lack of
bounding box supervision.
we study two training
approaches: supervised training using anatomy-level pathology labels
and multiple instance learning (mil) with image-level pathology labels.
keywords: pathology detection · anatomical regions · chest x-rays
1
introduction
chest radiographs (chest x-rays) represent the most widely utilized type of medi-
cal imaging examination globally and hold immense signiﬁcance in the detection
of prevalent thoracic diseases, including pneumonia and lung cancer, making
them a crucial tool in clinical care
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43907-0_6.
https://doi.org/10.1007/978-3-031-43907-0_6
58
p. müller et al.
however, while image classiﬁcation labels can be automatically extracted
from electronic health records or radiology reports [7,20], this is typically not
possible for bounding boxes, thus limiting the availability of large datasets
for pathology detection.
additionally, manually annotating pathology bound-
ing boxes is a time-consuming task, further exacerbating the issue.
however, as these meth-
ods are not guided by any form of bounding boxes, their performance is limited.
– we study two training approaches: using localized (anatomy-level) pathology
labels for our model loc-adpd and using image-level labels with multiple
instance learning (mil) for our model mil-adpd.
– we train our models on the chest imagenome
due to the scarcity of bounding box
annotations, pathology detection on chest x-rays is often tackled using weakly
supervised object detection with class activation mapping (cam) [25], which
only requires image-level classiﬁcation labels.
after training a classiﬁcation
model with global average pooling (gap), an activation heatmap is com-
puted by classifying each individual patch (extracted before pooling) with the
trained classiﬁer, before thresholding this heatmap for predicting bounding
boxes.
unlike
our method, none of these methods utilize anatomical regions as proxies for
predicting pathology bounding boxes, therefore leading to inferior performance.
along with the chest imagenome dataset
[21] several localized pathology classiﬁcation models have been proposed which
use a faster r-cnn
this
one-to-one assignment of tokens and regions allows us to remove the hungarian
60
p. müller et al.
box
prediction
pneumonia: 0.74
pneumonia: 0.71
pneumonia: 0.72
weighted
box fusion
pneumonia
infiltration
cardiomegaly
0.27
0.15
0.42
pneumonia
infiltration
cardiomegaly
pneumonia
infiltration
cardiomegaly
0.74
0.71
0.61
0.15
0.54
0.16
fig.
we experimented with more complex pathology predictors like an mlp or
a transformer layer but did not observe any beneﬁts.
we also did not observe
improvements when using several decoder layers and observed degrading perfor-
mance when using roi pooling to compute region features.
3.2
inference
during inference, the trained model predicts anatomical region bounding boxes
and per-region pathology probabilities, which are then used to predict pathology
bounding boxes in two steps, as shown in fig.
[19] merges bounding boxes of the same pathology
with iou-overlaps above 0.03 and computes weighted averages (using box scores
as weights) of their box coordinates.
as many anatomical regions are at least
partially overlapping, and we use a small iou-overlap threshold, this allows the
model to either pull the predicted boxes to relevant subparts of an anatomical
region or to predict that pathologies stretch over several regions.
for training the pathology
classiﬁer, we experiment with two diﬀerent levels of supervision (fig. 3).
mil-adpd: region predictions are ﬁrst aggregated
using lse pooling and then trained using image-level supervision.
[17] loss
function independently on each region-pathology pair and average the results
over all regions and pathologies.
the decoder feature dimension is set to 512.
for our mil-adpd model, we experiment with a weaker form of supervision,
where pathology classiﬁcation labels are only available on the per-image level.
we utilize multiple instance learning (mil), where an image is considered a bag
of individual instances (i.e. the anatomical regions), and only a single label (per
pathology) is provided for the whole bag, which is positive if any of its instances
is positive.
to train using mil, we ﬁrst aggregate the predicted pathology prob-
abilities of each region over all detected regions in the image using lse pooling
the resulting per-image
probability for each pathology is then trained using the asl
in this
model, the decoder feature dimension is set to 256.
in both models, the asl loss is weighted by a factor of 0.01 before adding
it to the detr loss.
we train on the chest imagenome dataset [4,21,22]1, con-
sisting of roughly 240 000 frontal chest x-ray images with corresponding scene
graphs automatically constructed from free-text radiology reports.
amongst other information, each scene graph contains bounding boxes for 29
1 https://physionet.org/content/chest-imagenome/1.0.0
(physionet
credentialed
health data license 1.5.0).
we con-
sider the image-level label for a pathology to be positive if any region is positively
labeled with that pathology.
we use the provided jpg-images [11]2 and follow the oﬃcial mimic-cxr
training split but only keep samples containing a scene graph with at least ﬁve
valid region bounding boxes, resulting in a total of 234 307 training samples.
during training, we use random resized cropping with size 224 × 224, apply
contrast and brightness jittering, random aﬃne augmentations, and gaussian
blurring.
we evaluate our method on the subset
of 882 chest x-ray images with pathology bounding boxes, annotated by radiol-
ogists, from the nih chestxray-8 (cxr8) dataset
all images are center-cropped and resized to 224 × 224.
for some evaluation classes, we therefore use a many-to-one map-
ping where the class probability is computed as the mean over several training
classes.
material for a detailed study on class mappings.
4
experiments and results
4.1
experimental setup and baselines
we compare our method against several weakly supervised object detection
methods (chexnet [14], stl
[13]), trained on the cxr8 training set using only
image-level pathology labels.
note that some of these methods focus on (image-
level) classiﬁcation and do not report quantitative localization results.
it was trained on mimic-cxr (sharing the images with our method) with
labels from radgraph [8] and ﬁnetuned on the cxr8 training set with image-
level labels.
our models loc-adpd and
mil-adpd, trained using anatomy (an) bounding boxes, both outperform all weakly
supervised methods trained with image-level pathology (pa) and anatomy-level pathol-
ogy (an-pa) labels by a large margin.
we report the standard object detection metrics average preci-
sion (ap) at diﬀerent iou-thresholds and the mean ap (map) over thresholds
(0.1, 0.2, . . .
compared to the best
weakly supervised method with image-level supervision (chexnet) our methods
improve by large margins (mil-adpd by δ+35.2%, loc-adpd by δ+87.8% in
map).
improvements are especially high when considering larger iou-thresholds
and huge improvements are also achieved in loc-acc at all thresholds.
(color
ﬁgure online)
margins (mil-adpd by δ + 47.9% and loc-adpd by δ + 105.5% map),
while improvements on larger thresholds are smaller here.
while
using image-level annotations (mil-adpd) already gives promising results, the
full potential is only achieved using anatomy-level supervision (loc-adpd).
we found
that the improvements of mil-adpd are mainly due to improved performance
on cardiomegaly and mass detection, while loc-adpd consistently outperforms
all baselines on all classes except nodule, often by a large margin.
ablation study.
combining the training strategies of loc-adpd
and mil-adpd does not lead to an improved performance.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_56.pdf:
endoscopy is the gold standard procedure for early detec-
tion and treatment of numerous diseases.
obtaining 3d reconstructions
from real endoscopic videos would facilitate the development of assis-
tive tools for practitioners, but it is a challenging problem for current
structure from motion (sfm) methods.
feature extraction and match-
ing are key steps in sfm approaches, and these are particularly diﬃcult in
the endoscopy domain due to deformations, poor texture, and numerous
artifacts in the images.
in our experiments, superpoint-
e obtains more and better features than any of the baseline detectors
used as supervision.
keywords: deep learning · structure from motion · local features ·
endoscopy
1
introduction
endoscopy is an important medical procedure with many applications, from
routine screening to detection of early signs of cancer and minimally invasive
treatment.
automatic analysis and understanding of these videos raises many
opportunities for novel assistive and automatization tasks on endoscopy proce-
dures.
obtaining 3d models from the intracorporeal scenes captured in endo-
scopies is an essential step to enable these novel tasks and build applications,
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43907-0 56.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
https://doi.org/10.1007/978-3-031-43907-0_56
584
o. l. barbed et al.
for example, for improved monitoring of existing patients or augmented reality
during training or real explorations.
endoscopic images are a challenging
case for feature detection and matching, due to several well known challenges
for these tasks, such as lack of texture, or the presence of frequent artifacts, like
specular reﬂections.
these problems are accentuated when all the elements in
the scene are deformable, as it is the case in most endoscopy scenarios, and in
particular in the real use case studied in our work, the lower gastrointestinal
tract explored with colonoscopies.
this work introduces superpoint-e, a new model to extract interest points
from endoscopic images.
we select good features with the colmap
sfm pipeline [21], generating training examples with feature points that can
be tracked across several images according to colmap result.
[7] have evaluated the perfor-
mance of modern slam approaches on endoscopic sequences.
[13] improved the performance of such methods in laparoscopic sequences.
however, trans-
ferring that performance to endoscopy settings remains a diﬃcult task due to
1
https://www.cs.ubc.ca/research/image-matching-challenge/2021/leaderboard/.
tracking adaptation to improve superpoint for reconstruction in endoscopy
585
several challenges.
notable mentions are superpoint
exporting
this progress to the matching stage, disk
[24] proposes a formulation of the
problem to optimize in an end-to-end manner.
other recent works have extended
the networks to take advantage of the advances in attention for the matching
task, as in superglue [20] and loftr
[23].
in this work we improve the performance of superpoint
[5] on endoscopy
images.
we chose superpoint because it is a seminal work that has inspired
many follow up works, and it is still among the top performers on current feature
matching challenges [10]. similar to detone et al.
[4], we explore improvements
on feature extraction that provide good properties for downstream tasks.
instead,
we propose to use 3d reconstructions of points tracked along image sequences.
uration parameters are detailed in the supplementary materials.
we additionally compute the 3d reconstruction for the
586
o. l. barbed et al.
same sequences with a modiﬁed colmap pipeline that uses the oﬃcial super-
point and superglue2 implementation with the indoor set of weights.
the
reliable track for this point is the green segment.
(c) movement of the point along the video.
a successful 3d recon-
struction includes the computed positions of the cameras that took the images
and a point cloud with 3d coordinates of the triangulated points.
we use the
camera poses, the points’ coordinates and the camera calibration parameters
to reproject the 3d point cloud points into every image.
if they were “originally” detected and matched in a particular
image, we set them to green.
it achieves this by using as supervision y the average detections over sev-
eral random homographic deformations of the same image.
the feature extrac-
tion network then is run on an image
instead of an image
i and a warped version i′, we use
diﬀerent images ia and ib from the same sequence.
t is the set of all the tracks that appear in both images.
two descriptors from diﬀerent images dai and dbj
are a positive pair if they belong to the same track (i = j), and negative pair
otherwise (i ̸= j).
4
experiments
the following experiments demonstrate the proposed feature detection eﬃcacy
to obtain 3d models on real colonoscopy videos, comparing diﬀerent variations
of our approach and relevant baseline methods.
dataset.
the exact details are in the supplementary material.
all models were trained with a
modiﬁcation of a pytorch implementation of superpoint [9].
training parame-
ters in supplementary material.
conﬁguration of the training (left), and average reconstruc-
tion results, i.e., quality metrics (right).
loss (loss used for training): sp: original superpoint training loss; tr-2 or tr-n: track-based loss.
tr-2 means that the loss is computed for every pair of images in the track.
tr-n means we optimize
simultaneously n views of the track (n=4 in our experiments).
table 1 (last ﬁve columns) summarizes the performance of
our approach variations.
matches between the points in two images are obtained with
bi-directional nearest neighbor algorithm with l2 distance.
points and matches
are given to colmap and the mapper module (conﬁguration in supplementary
material) attempts to generate a 3d reconstruction.
the reconstruction quality
statistics used to illustrate the performance of each detector are:
– ∥3dim∥: fraction of images from the subsequence successfully introduced
in the reconstruction.
the more
points the better, since it means a denser coverage of the scene.
– err: mean reprojection error of the 3d points after being reprojected onto
the images of the subsequence.
– err-10k: mean reprojection error of the best 10000 points of the recon-
struction.
since all reconstructions have outliers that skew the average, this
metric is more representative of the performance of the models.
tracking adaptation to improve superpoint for reconstruction in endoscopy
589
– len(tr): mean track length represents the average number of images where
a point is being consecutively matched, tracked.
this experiment compares the performance of
the considered baselines against the best conﬁguration of our feature extraction
model.
in most metrics we observe a
signiﬁcant improvement using sp-e compared to the others.
each point in each image
has been reconstructed after the corresponding colmap reconstruction process.
subsequence 001 1
002 1
014 1
016 1
017 1
095 1
095 2
avg
(std)
reconstructed images (∥3dim∥)
total+
+ total number of images in the subsequence.
* if 10k points are not available, average is computed over all available reconstructed points.
the mean reprojection error of all the points is the lowest
for sift, possibly due to it being more restrictive in all other aspects (number
of images reconstructed, number of points, track length).
note that even though sp-e
obtains many more points, it is not at the cost of quality.
we analyze additional aspects of our detected features to showcase the higher
quality with respect to other methods in table 3. to measure the spread of the
features over the images we deﬁned a 16 × 16 grid over each image and computed
the percentage of those cells that have at least one reconstructed point.
we also
measure how many extracted points fall on top of specularities (we consider
a pixel as part of a specularity if its intensity is higher than 180).
sp only reconstructed 3 out of the 5 sequences
while sift and sp-e correctly reconstructed the 5 sequences, with an average
rmse of 4.61mm and 4.71 mm respectively.
simulated data lacks some of the
biggest challenges of endoscopy images (e.g. specularities, deformations), but
this experiment suggests that the camera motion estimation quality is similarly
good for all methods when they manage to converge.
5
conclusions
this work presents a novel training strategy for superpoint to improve its per-
formance in sfm from endoscopy images.
our proposed model is able to obtain more suitable features
for 3d reconstruction, and to reconstruct larger sets of images with much denser
point clouds.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_4.pdf:
fully-supervised polyp segmentation has accomplished sig-
niﬁcant triumphs over the years in advancing the early diagnosis of col-
orectal cancer.
however, label-eﬃcient solutions from weak supervision
like scribbles are rarely explored yet primarily meaningful and demand-
ing in medical practice due to the expensiveness and scarcity of densely-
annotated polyp data.
besides, various deployment issues, including data
shifts and corruption, put forward further requests for model generaliza-
tion and robustness.
concretely, for the ﬁrst time
in weakly-supervised medical image segmentation, we promote the dual-
branch co-teaching framework by leveraging the intrinsic complemen-
tarity of features extracted from the spatial and spectral domains and
encouraging cross-space consistency through collaborative optimization.
ultimately, we formulate a
holistic optimization objective to learn from the hybrid supervision of
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43907-0 4.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
extensive experiments and evaluation on
four public datasets demonstrate the superiority of our method regarding
in-distribution accuracy, out-of-distribution generalization, and robust-
ness, highlighting its promising clinical signiﬁcance.
keywords: polyp image segmentation · weakly-supervised learning ·
spatial-spectral dual branches · mutual teaching · ensemble learning
1
introduction
colorectal cancer is a leading cause of cancer-related deaths worldwide [1].
early
detection and eﬃcient diagnosis of polyps, which are precursors to colorectal
cancer, is crucial for eﬀective treatment.
recently, deep learning has emerged
as a powerful tool in medical image analysis, prompting extensive research into
its potential for polyp segmentation.
besides, scribbles provide
a more robust supervision signal, which can be prone to noise and outliers [5].
hence, this work investigates the feasibility of conducting polyp segmentation
using scribble annotation as supervision.
the eﬀectiveness of medical applica-
tions during in-site deployment depends on their ability to generalize to unseen
data and remain robust against data corruption.
dual-branch learning has been widely adopted in annotation-eﬃcient learn-
ing to encourage mutual consistency through co-teaching.
while existing
approaches are typically designed for learning in the spatial domain [21,25,29,
30], a novel spatial-spectral dual-branch structure is introduced to eﬃciently
leverage domain-speciﬁc complementary knowledge with synergistic mutual
teaching.
furthermore, the outputs from the spatial-spectral branches are aggre-
gated to produce mixed pseudo labels as supplementary supervision.
1. overview of our spatial-spectral dual-branch mutual teaching and pixel-level
entropy-guided pseudo label ensemble learning (s2me) for scribble-supervised polyp
segmentation.
spatial-spectral cross-domain consistency is encouraged through mutual
teaching.
overall, the contributions of this work are threefold: first,
we devise a spatial-spectral dual-branch structure to leverage cross-space knowl-
edge and foster collaborative mutual teaching.
to our best knowledge, this is the
ﬁrst attempt to explore the complementary relations of the spatial-spectral dual
branch in boosting weakly-supervised medical image analysis.
lastly, our proposed hybrid loss optimization, comprising scribbles-
supervised loss, mutual training loss with domain-speciﬁc pseudo labels, and
ensemble learning loss with fused-domain pseudo labels, facilitates obtaining
a generalizable and robust model for polyp image segmentation.
an extensive
assessment of our approach through the examination of four publicly accessible
datasets establishes its superiority and clinical signiﬁcance.
[26] has gained increasing popularity in medical image
analysis [23] for its ability to identify subtle frequency patterns that may not be
well detected by the pure spatial-domain network like unet [20].
in addi-
tion, spectrum learning also exhibits advantageous robustness and generaliza-
tion against adversarial attacks, data corruption, and distribution shifts [19].
in
label-eﬃcient learning, some preliminary works have been proposed to encourage
38
a. wang et al.
mutual consistency between outputs from two networks [3], two decoders
this has motivated
us to develop the cross-domain cooperative mutual teaching scheme to leverage
the favorable properties when learning in the spectral space.
besides consistency constraints, utilizing pseudo labels as supplementary
supervision is another principle in label-eﬃcient learning [11,24].
in contrast to prior
weakly-supervised learning methods that have merely emphasized spatial con-
siderations, our approach designs a dual-branch structure consisting of a spatial
branch fspa(x, θspa) and a spectral branch fspe(x, θspe), with x and θ being the
input image and randomly initialized model parameters.
1,
the spatial and spectral branches take the same training image as the input and
extract domain-speciﬁc patterns.
through cross-domain engagement, these two
branches complement each other, with each providing valuable domain-speciﬁc
insights and feedback to the other.
in addition to mutual
teaching, we consider aggregating the pseudo labels from the spatial and spec-
tral branches in ensemble learning, aiming to take advantage of the distinctive
1 for convenience, we omit the input x and model parameters θ.
s2me: spatial-spectral mutual teaching and ensemble learning
39
yet complementary properties of the cross-domain features.
that the pixels of the polyp boundary exhibit greater diﬃcul-
ties in accurate segmentation, presenting with higher entropy values (the white
contours).
unlike previous
image-level ﬁxed-ratio mixing or random mixing as eq.
besides the
scribble annotations for partial pixels, the aforementioned three types of pseudo
labels ˆyspa, ˆyspe, and ˆys2 can oﬀer complementary supervision for every pixel,
with diﬀerent learning regimes.
the hybrid loss considers all possible
supervision signals in the spatial-spectral dual-branch network and exceeds par-
tial combinations of its constituent elements, as evidenced in the ablation study.
3
experiments
3.1
experimental setup
datasets.
we employ the sun-seg [10] dataset with scribble annotations for
training and assessing the in-distribution performance.
implementation details.
we implement our method with pytorch
[18] and
run the experiments on a single nvidia rtx3090 gpu.
the sgd optimizer
is utilized for training 30k iterations with a momentum of 0.9, a weight decay
of 0.0001, and a batch size of 16.
the execution time for each experiment is
approximately 4 h. the initial learning rate is 0.03 and updated with the poly-
scheduling policy [15].
all
the images are randomly cropped at the border with maximally 7 pixels and
resized to 224×224 in width and height.
[6] as the respective segmentation model in
the spatial and spectral branches.
the performance of the scribble-supervised
model with partial cross entropy [13] loss (scrib-pce) and the fully-supervised
2 some exemplary polyp frames are presented in the supplementary materials.
[15] are employed as the compar-
ative baselines and implemented with unet [20] as the segmentation backbone
referring to the wsl4mis3 repository.
quantitative comparison of the in-distribution segmentation performance.
[3]
0.658±0.004
0.539±0.005
0.676±0.005
5.092±0.063
dmpls [15]
0.656±0.006
0.539±0.005
0.659±0.011
5.208±0.061
s2me (ours) 0.674±0.003 0.565±0.001 0.719±0.003 4.583±0.014
fully-ce
0.713±0.021
0.617±0.023
0.746±0.027
4.405±0.119
the performance of weakly-supervised methods is assessed with four metrics,i.e.,
dice similarity coeﬃcient (dsc), intersection over union (iou), precision
(prec), and a distance-based measure of hausdorﬀ distance (hd).
2, our s2me achieves superior in-distribution performance quan-
titatively and qualitatively compared with other baselines on the sun-seg [10]
dataset.
these results suggest the eﬃcacy and reliability of the pro-
posed solution s2me in fulﬁlling polyp segmentation tasks with only scribble
annotations.
notably, the encouraging performance on unseen datasets exhibits
promising clinical implications in deploying our method to real-world scenarios.
4 complete results of all four metrics are present in the supplementary materials.
42
a. wang et al.
fig.
2. qualitative performance comparison of one camouﬂaged polyp image with dsc
values on the left top.
as shown in table 3, the spatial-spectral conﬁguration of our s2me
yields superior performance compared to single-domain counterparts with me,
conﬁrming the signiﬁcance of utilizing cross-domain features.
as demonstrated in table 4,
our method achieves improved performance compared to two image-level fusion
strategies, i.e., random [15] and equal mixing.
[15]
image 0.665 ± 0.008
4.750 ± 0.169
equal (0.5)
image 0.667 ± 0.001
4.602 ± 0.013
entropy (ours) pixel
0.674 ± 0.003 4.583 ± 0.014
table 5. ablation study on the loss com-
ponents on the sun-seg [10] dataset.
4
conclusion
to our best knowledge, we propose the ﬁrst spatial-spectral dual-branch net-
work structure for weakly-supervised medical image segmentation that eﬃciently
leverages cross-domain patterns with collaborative mutual teaching and ensem-
ble learning.
our pixel-level entropy-guided fusion strategy advances the relia-
bility of the aggregated pseudo labels, which provides valuable supplementary
supervision signals.
moreover, we optimize the segmentation model with the
hybrid mode of loss supervision from scribbles and pseudo labels in a holistic
manner and witness improved outcomes.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_55.pdf:
longitudinal lesion or tumor tracking is an essential task in
diﬀerent clinical workﬂows, including treatment monitoring with follow-
up imaging or planning of re-treatments for radiation therapy.
the multi-scale approach allows the
eﬃcient and robust learning of a similarity map between multi-timepoint
image acquisitions to derive correspondence, while the self-supervised
learning formulation enables the generic application to diﬀerent types
of lesions and image modalities.
we train our approach at large scale
with more than 50,000 computed tomography (ct) scans and validate
it on two diﬀerent applications: 1) tracking of generic lesions based on
the deeplesion dataset, including liver tumors, lung nodules, enlarged
lymph-nodes, for which we report highest matching accuracy of 92%,
with localization accuracy that is nearly 10% higher than the state-of-
the-art; and 2) tracking of lung nodules based on the nlst dataset
for which we achieve similarly high performance.
keywords: self-supervised learning · multi-scale · longitudinal lesion
tracking
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43907-0 55.
https://doi.org/10.1007/978-3-031-43907-0_55
574
a. vizitiu et al.
1
introduction
longitudinal lesion or tumor tracking is a fundamental task in treatment moni-
toring workﬂows, and for planning of re-treatments in radiation therapy.
this
information can be leveraged to assess treatment response, e.g., by analyzing the
evolution of size and morphology for a given tumor [1], but also for adaptation
of (re-)treatment radiotherapy plans that take into account new tumors.
in practice, the development of automatic and reliable lesion tracking solu-
tions is hindered by the complexity of the data (over diﬀerent modalities), the
absence of large, annotated datasets, and the diﬃculties associated with lesion
identiﬁcation (i.e., varying sizes, poses, shapes, and sparsely distributed loca-
tions).
in addition, as imaging
oﬀers contextual information about the human body that is naturally consis-
tent, we design the model to beneﬁt from biologically-meaningful points (i.e.,
anatomical landmarks).
the reasoning behind this strategy is that simple data
augmentation methods cannot faithfully model inter-subject variability or pos-
sible organ deformations.
our proposed method brings two elements of novelty from a technical point
of view: (1) the multi-scale approach for the anatomical embedding learning
and (2) a positive sampling approach that incorporates anatomically signiﬁcant
landmarks across diﬀerent subjects.
furthermore, a signiﬁcant
focus and contribution of our research is the experimental study at a very large
scale: we (1) train a pixel-wise self-supervised system using a very large and
diverse dataset of 52,487 ct volumes and (2) evaluate on two publicly available
datasets.
2
background and motivation
the problem of lesion tracking in longitudinal data is typically divided into two
steps: (1) detection of lesions and (2) tracking the same lesion over multiple
multi-scale self-supervised learning for longitudinal lesion tracking
575
input image
(e.g., 3d-ct)
augmentaon
4d mul-scale
embeddings 
mul-scale
self-supervised 
opmizaon
hard and diverse 
negave sampling 
landmarks
oponal supervision:
dur-
ing training, we randomly extract positive samples (optionally, include same anatom-
ical landmarks from diﬀerent volumes), hard-negative samples, and diverse negative
samples of pixels from augmented 3d paired patches.
during inference, the extracted
embeddings are used to generate a cascade of cosine similarity maps that initially locate
the corresponding location in a follow-up image within a larger area and subsequently
improve the matching accuracy through gradual reﬁnement.
time points.
classical methods to solve this problem rely on image registration,
where tracking is performed via image alignment and rule-based correspondence
matching [15,16,21].
[5] uses a
self-supervised anatomical embedding model (sam) to create semantic embed-
dings for each image pixel, avoiding the detection step.
training exclusively on
augmented paired data prevents sam from accurately representing anatomical
changes and deformations that occur over time.
3
method
3.1
problem deﬁnition
let i1 (i.e., template or baseline image) and i2 (i.e., query or follow-up image)
be two 3d-ct scans acquired at time t1 and t2, respectively, additionally, let p1
576
a. vizitiu et al.
and p2 denote the point of interest (i.e., the lesion center) in both images.
the
problem of lesion tracking can be formulated as ﬁnding the optimal transforma-
tion that maps p1 to its corresponding location, p2, in i2.
3.2
training stage
let d = {x1, x2, ..., xn} be a set of n unpaired and unlabeled 3d-ct volumes.
as shown in fig. 1, given an image x ∈ rd×h×w from the training dataset d,
we randomly select two overlapping 3d patches (anchor and query), namely xa
and xq.
to create synthetic paired data that mimics appearance changes across
diﬀerent images, we apply random data augmentation (i.e., random spatial and
intensity-related transformations) to the content of xa and xq.
we implement a
similar augmentation strategy to that described in [5].
given xa and xq, we use
an embedding extraction model to construct a hierarchy of multi-scale semantic
embeddings for each image pixel, labeled fa and fq respectively.
given the nature of contrastive learning, the sampling strategy (extract-
ing negative and positive pixel pairs from augmented 3d paired patches) is
essential to achieving discriminative pixel-wise embeddings.
the ﬁnal loss is then calculated as
the average of all these individual losses.
multi-scale self-supervised learning for longitudinal lesion tracking
577
3.3
inference stage
let xa be a 3d-ct volume template with an input point of interest pa ∈ xa,
and xq a corresponding query 3d-ct volume.
the ﬁrst step is to project the
image xa into a multi-scale feature space, creating a hierarchy of multi-scale
semantic embeddings fa for each pixel in the image (i.e., a 4d feature map).
next, we follow a similar process for the query image xa and acquire the pixel-
level embeddings fq.
to measure the similarity between the embeddings of the input xa at the
point of interest pa and the query embeddings fq, we compute cosine similarity
maps at each scale:
si =
f i
a(pa) ·
4
experiments
4.1
datasets and setup
datasets: we train the universal and ﬁne-grained anatomical point matching
model using an in-house ct dataset (variousct).
the training dataset contains
52,487 unlabeled 3d ct volumes capturing various anatomies, including chest,
head, abdomen, pelvis, and more.
for nlst, we randomly selected
a subset of 1045 test images coming from 420 patients with up to 3 studies.
system training: our learning model is implemented in pytorch and uses
the torchio library [13] for medical data manipulation and augmentation.
the exact same test set was used to com-
pute the performance of each approach listed in the table; however, we retrained only
sam.
[5]
81.67
90.21
2.9 ± 8.0
2.5 ± 3.8
3.6 ± 5.2
6.5 ± 9.6
ours
83.13†
91.87†
2.9 ± 6.0
2.2 ± 3.2
3.1 ± 3.9
5.9 ± 7.1†
† improvement is statistically signiﬁcant compared to sam
in the standard resnet to 3d convolutions and allows the use of pre-trained
imagenet weights.
the model is trained with adamw optimizer [6] for 64 epochs using an early
stopping strategy with a patience of 5 epochs, a batch size of 8 augmented 3d
paired patches of 32 × 96 × 96, and a learning rate of 0.0001.
for data augmentation, we apply random cropping, scaling, rotation, and
gaussian noise injections.
evaluation metrics: we use mean euclidean distance (med) to measure
the distance between predicted lesion center and ground truth, and the center
point matching accuracy (i.e., percentage of accurately matched lesions given the
annotated lesion radius), denoted with cpm@radius.
to ensure that such small nodules are
not missed during evaluation, we relax the minimum distance requirement and
consider a distance of 6 mm as a permissible matching error.
hence, for performance comparison against
self-supervised anatomical embedding tracker, we retrain sam
[5] with images
from variousct dataset.
to conﬁrm the signiﬁcance of the
improvement achieved by our method compared to sam
[5], we conduct a paired
t-test for statistical analysis and show that the improvement is statistically sig-
niﬁcant (p-value < 10−6).
compared to the self-supervised version of dlt, the
diﬀerence in performance is signiﬁcantly greater, the proposed systems outper-
forms dlt-ssl by more than 10%.
when imposing a maximum distance limit
of 10 mm between the ground truth and prediction, our method increases perfor-
mance by 1.46%, showing the importance of the multi-scale approach in lesion
fig.
we denote the
ground-truth points using green markers in both the baseline and follow-up images,
whereas the predicted points are indicated by red markers.
to illustrate the extent of
the lesions, we also display the annotated bounding boxes on the follow-up images.
(color ﬁgure online)
580
a. vizitiu et al.
location reﬁnement.
through large-scale experiments and validation on two longitudinal datasets, we
highlight the superiority of the proposed method in comparison to state-of-the-
art.
our future work aims to enhance the matching accuracy by examining the
implications of correlation magnitude, conducting robustness studies on slight
variations in tracking initialization, and implementing a more advanced fusion
strategy for the multi-scale similarity maps.
in addition, we aim to expand to
more applications, e.g., treatment monitoring for brain cancer using mri.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_45.pdf:
the displacement estimation step of ultrasound elastogra-
phy (use) can be done by optical ﬂow convolutional neural networks
(cnn).
even though displacement estimation in use and computer
vision share some challenges, use displacement estimation has two dis-
tinct characteristics that set it apart from the computer vision coun-
terpart: high-frequency nature of rf data, and the physical rules that
govern the motion pattern.
however, insuﬃcient attention has been
placed on the integration of physical laws of deformation into the dis-
placement estimation.
in use, lateral displacement estimation, which
is highly required for elasticity and poisson’s ratio imaging, is a more
challenging task compared to the axial one since the motion in the lat-
eral direction is limited, and the sampling frequency is much lower than
the axial one.
picture tries to limit the range of the lateral
displacement by the feasible range of poisson’s ratio and the estimated
high-quality axial displacement.
despite the improvement, the regular-
ization was only applied during the training phase.
we exploit the concept of
known operators to incorporate iterative reﬁnement optimization meth-
ods into the network architecture so that the network is forced to remain
within the physically plausible displacement manifold.
the reﬁnement
optimization methods are embedded into the diﬀerent pyramid levels of
the network architecture to improve the estimate.
our results on exper-
imental phantom and in vivo data show that the proposed method sub-
stantially improves the estimated displacements.
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43907-0_45.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
ultrasound (us) data before and after the tissue deformation (which
can be caused by an external or internal force) are collected and compared
to calculate the displacement map, indicating each individual sample’s relative
motion.
the strain is computed by taking the derivative of the displacement
ﬁelds.
convolutional neural networks (cnn) have been successfully employed
for use displacement estimation [11,12,15].
unsupervised and semi-supervised
training methods have been proposed, which enable the networks to use real us
images for training [1,14,17].
this method aims to improve lat-
eral displacement by exploiting the high-quality axial displacement estimation
and the relation between the lateral and axial strains deﬁned by the physics of
motion.
despite the substantial improvement, the regularization is only applied
during the training phase.
maier
et al. investigated known operators in diﬀerent applications such as computed
tomography, magnetic resonance imaging, and vessel segmentation, and showed
a substantial reduction in the maximum error bounds
[7].
in this paper, we aim to embed two lateral displacement reﬁnement algo-
rithms in the cnns to improve the lateral strains.
the second algorithm employs the reﬁnement method
proposed be gou et al.
[2] which exploits incompressibility constraint to reﬁne
known operators for ultrasound elastography
469
the lateral displacement.
1- detect out-of-range eprs by:
m(i, j) =

0 vemin < ve(i, j) < vemax
1
otherwise

(1)
where ve is the epr obtained from the estimated displacements.
(2)
where < ve > is the average of epr values within the feasible range.
it should be noted in con-
trast to [5] in which only out-of-range samples were contributing to the loss,
in this work, all samples contribute to lvd to reduce the estimation bias.
we employ two known operators to impose physically known constraints
on the lateral displacement.
it should be noted that the
algorithm can be employed for compressible tissues as well, and the incompress-
ibility constraint is employed for the reﬁnement of the obtained displacement.
while the
latter involves adjusting trained weights based on the training data and keep-
ing them ﬁxed during testing, the former relies on iterative reﬁnement that is
adaptable to the test data and does not require any learnable weights.
2.3
unsupervised training
we followed a similar unsupervised training approach presented in [5] for both
picture and kpicture methods.
λv lv
(6)
known operators for ultrasound elastography
471
algorithm 1: poisson’s ratio clipper
input : lateral displacement wl, axial displacement wa, vemin,vemax, iteration
output: reﬁned lateral displacement wref
1 wref ← wl
2 for q ← 1 to iteration do
3
e22 ← ∂wl
∂l
// gradient in lateral direction.
epr × e11
// use the displacement of previous line and the clipped epr to find the displacement of the next line
algorithm 2: guo et al. reﬁnement [2] employed as known operator
input : lateral displacement wl, axial displacement wa of size w × h,
iteration, λ1, λ2
output: reﬁned lateral displacement wref
1 wref ← wl
2 for q ← 1 to iteration do
3
for i, j in w, h do
4
δ = wl(i, j − 1) − 2wl(i, j) + wl(i, j + 1) + wa(i
the known operators are added after optical ﬂow estimation,
and reﬁne the estimated lateral displacement in each pyramid level (added from level
3) to provide improved lateral displacement to the next pyramid level.
where ld denotes photometric loss which is obtained by comparing the pre-
compressed and warped compressed rf data, ls is smoothness loss in both
axial and lateral directions.
the young’s mod-
ulus of the experimental phantom was 20 kpa and contains several inclusions
with young’s modulus of higher than 40 kpa.
in vivo data was collected at johns hopkins hospital from patients with liver
cancer during open-surgical rf thermal ablation by a research antares siemens
system using a vf 10-5 linear array with the sampling frequency of 40 mhz and
the center frequency of 6.67 mhz.
this has the advantage of correcting lateral
displacements in diﬀerent pyramid levels.
the hyper-parameters’ values
of unsupervised training and the known operators are given in supplementary
materials.
3
results and discussions
3.1
compared methods
kpicture is compared to the following methods:
known operators for ultrasound elastography
473
fig.
2. lateral strains in the experimental phantom obtained by diﬀerent methods.
the target and background windows for calculation of cnr and sr are marked in the
b-mode images.
the inclusion on the bottom of sample (1) is highlighted in picture
and kpicture strain images by purple and blue arrows.
axial strains
are available in supplementary materials.
[2], which employs the output of
overwind as the initial displacement (overwind+ guo et al.).
we also
employed a similar hyper-parameters and training schedule for experimental
phantom and in vivo data.
3.2
results and discussions
the lateral strains of ultrasound rf data collected from three diﬀerent loca-
tions of the tissue-mimicking breast phantom are depicted in fig. 2, and the
quantitative results are given in table 1.
[2] improves the displacement obtained by
overwind.
the strain images obtained by kpicture have a much higher
quality than those of picture.
furthermore, kpicture has the highest qual-
ity strain images among the compared methods.
for example, the inclusion on
474
a. k. z. tehrani and h. rivaz
the bottom in sample 1 (highlighted by the arrows) is clearly visible in kpic-
ture, a substantial improvement over all other methods that do not even show
the inclusion.
quantitative results of lateral strains for experimental phantoms.
the pair marked by † is not statistically signiﬁcant (p-value > 0.05,
using friedman test).
the histograms of epr values of overwind+gou et al., picture and
kpicture are illustrated for the experimental phantom sample (1).
3 (b), and axial
strains are given in the supplementary materials (the quality of axial strains is
high in all methods).
while picture may produce an adequate strain image,
it still contains noisy regions.
on the other hand, kpicture delivers excep-
tionally reﬁned strain images and surpasses the other compared methods.
the histogram of epr values for experimental phantom sample 1 (a).
their performance on anisotropic materials can be investigated by
experiments on anisotropic tissues such as muscles.
it should be noted that after incorporating the known operators, the inference
time of the network increased from an average of 195 ms to 240 ms (having 10
iterations for algorithm 1 and 100 iterations for algorithm 2).

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_51.pdf:
recently, weakly supervised nuclei segmentation methods
using only points are gaining attention, as they can ease the tedious
labeling process.
we detect and segment nuclei by combining a binary segmentation mod-
ule, an oﬀset regression module, and a center detection module to deter-
mine foreground pixels, delineate boundaries and identify instances.
next, segmentation pre-
dictions are used to repeatedly generate pseudo oﬀset maps that indi-
cate the most likely nuclei center.
experimental results
show that our model consistently outperforms state-of-the-art methods
on public datasets regardless of the point annotation accuracy.
keywords: weakly supervised nuclei segmentation · instance
segmentation · point reﬁnement · oﬀset map · geodesic distance
1
introduction
nuclei segmentation in histopathology images is an important task for cancer
diagnosis and immune response prediction [1,13,18].
while several fully super-
vised deep learning approaches to segment nuclei exist [2,6,8,9,19,25], labeling
s. nam and j. jeong—equal contribution.
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43907-0_51.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
https://doi.org/10.1007/978-3-031-43907-0_51
pronet for nuclei instance segmentation
529
thousands of instances are tedious and the ambiguous nature of nuclei bound-
aries requires high-level expert annotators.
to address this, weakly-supervised
nuclei segmentation methods
as
point labels alone do not provide suﬃcient foreground information, it is com-
mon to use euclidean distance-based voronoi diagrams and k-means clustering
[7] to generate pseudo segmentation labels for training.
in real-world scenarios, point annotation locations may shift from nuclei centers
as a result of the expert labeling process, leading to a lower performance after
model training.
to overcome these challenges, we propose a novel weakly supervised instance
segmentation method that eﬀectively distinguishes adjacent nuclei and is robust
to point shifts.
the proposed model consists of three modules responsible for
binary segmentation, boundary delineation, and instance separation.
to train
the binary segmentation module, we generate pseudo binary segmentation masks
using geodesic distance-based voronoi labels and cluster labels from point anno-
tations.
to train the oﬀset map module, we
generate pseudo oﬀset maps by computing the oﬀset distance between binary
segmentation pixel predictions and the point label.
this reﬁnement process ensures that the model maintains high
performance even when the point annotation is not exactly located at the center
of the nuclei.
the contributions of this paper are as follows: (1) we propose an end-to-
end weakly supervised segmentation model that simultaneously predicts binary
mask, oﬀset map, and center map to accurately identify and segment nuclei.
(3) we
introduce an em algorithm-based reﬁnement process to encourage model robust-
ness on center-shifted point labels.
2
methodology
we propose an end-to-end nuclei segmentation method that only uses point
annotations p to predict nuclei instance segmentation masks ˆs.
it consists of an encoder and three modules
for binary segmentation, oﬀset map and center map prediction.
to train oﬀset map
and center map modules(blue lines), pseudo labels are generated using point label and
predicted binary segmentation mask(green lines).
during inference, the instance map,
obtained by predicted oﬀset map and center map, is multiplied with predicted binary
mask to produce instance segmentation prediction(orange lines).
(color ﬁgure online)
model consists of three modules: 1) binary segmentation module, 2) oﬀset map
module, and 3) center map module (fig. 1).
for a given input image, we extract
feature maps with an imagenet-pretrained vgg16 backbone encoder.
the fea-
ture maps are further processed through a series of residual units (rus) and
attention units (aus) to predict a binary segmentation mask ˆb, an oﬀset map
ˆo, and a center map ˆc.
the rus are employed to maintain feature information
so that subsequent modules can reuse the features from early-stage modules.
in the training stage, we ﬁrst generate a voronoi label v and a cluster label k
along the green lines in fig.
1 to train the segmentation module.
then, we gener-
ate the pseudo oﬀset map o by using ˆb and p. next, following [29], we generate
the center map c by expanding the point label p with gaussian kernel within
a radius r. herein, our model is trained wih a segmentation loss lb(v,k, ˆb), an
oﬀset map loss lo(o, ˆo), and a center map loss lc(c, ˆc).
thus, we
employ an em algorithm to search the optimal model parameters θ to obtain
more reliable points p ′.
pronet for nuclei instance segmentation
531
in the inference stage, ˆb, ˆo, ˆc are predicted following the orange lines in
fig.
finally, the instance segmentation output ˆs is obtained
by ˆb × i.
fig.
(a) input
image; (b) ground truth; (c) the cluster labels generated by euclidean distance, and
(d) those by geodesic distance.
(color ﬁgure online)
2.1
loss functions using pseudo labels
segmentation loss.
we generate v and k to train the binary segmentation
module.
to train the binary segmentation module using
v and k, we employ a voronoi loss lv and a cluster loss lk based on the
cross-entropy:
lv =
1
nωv

x,y∈ωv
v(x, y)log( ˆb(x, y))
following [17], we deﬁne
the ﬁnal segmentation loss as lb =
w and h are the
width and height of the input image.
3. (a) input image (top) and ground truth (bottom), (b) instance map (top) and
center map (bottom) generated by the optimal nuclei center points, (c) those by shifted
points (6–8), and (d) those by reﬁned points.
inspired by [2], we deﬁne an oﬀset
vector o(x, y) that indicates the displacement of a point (x, y) to the center of
its corresponding nucleus.
to train the oﬀset module, we ﬁrst compute o(x, y)
of each nucleus segmented by ˆb.
(4)
pronet for nuclei instance segmentation
533
it is worth noting that in the early stages of training, the pseudo oﬀset map o
generated by ˆb and p is unreliable.
2.2
reﬁnement via expectation maximization algorithm
training with nuclei (center) shifted point labels can lead to blurry center map
predictions (see fig.
this in turn limits model optimization and it’s ability
to distinguish objects, resulting in poor adjacent nuclei segmentation.
to address
this, we propose an em based center point reﬁnement process.
(6)
since reliable ˆo is necessary to reﬁne nuclei centers, reﬁnement starts after 30
epochs.
3
experiments
dataset.
to validate the eﬀectiveness of our model, we use two public nuclei
segmentation datasets i.e., cpm17
[12]. cpm17 contains
64 h&e stained images with 7,570 annotated nuclei boundaries sized from
500×500 to 600×600.
the set is split into 32/32 images for training and testing.
images were normalized and cropped to 300×300.
monuseg is a multi-organ
nuclei segmentation dataset consisting of 30 h&e stained images (1000×1000)
extracted from seven diﬀerent organs.
we used 16 images (4 images from the
breast, liver, kidney, and prostate) as training and 14 images (2 images from each
breast, liver, kidney, prostate, bladder, brain, and stomach) as testing.
for a fair
comparison, images were pre-processed before training/testing i.e., normalized
and cropped to 250×250 patches following the setting used in [17].
534
s. nam et al.
to make point labels, we use the center point of full mask annotations.
the shift is performed in
pixels and is randomly selected between the minimum and maximum values.
implementation details.
for training, all evaluated models were run for 150
epochs with the adam optimizer
the gaussian kernel r was set as
r = 6 and δ was set as 8 for reﬁnement on cpm17.
finally, a variety of augmentations were employed i.e.,
random resizing, cropping, and rotations etc., following [17], with loss weights
λb, λo and λc empirically set to 1.
performance comparison of nuclei segmentation on two public datasets.
table 1 shows the performance of our method against state-
of-the-art
weakly supervised nuclei segmentation methods
as opposed to the dice score,
aji is key when evaluating adjacent nuclei separation in instance segmentation
tasks.
regarding reﬁnement, we observed that our strategy is more bene-
ﬁcial when points exhibit signiﬁcant shifts i.e., on both cpm and monuseg.
figure 3 showcases the eﬀectiveness of the reﬁnement process wherein the model
generates precise instance and center maps.
with the geodesic distance and the
reﬁnement process, our proposed method achieved state-of-the-art performance.
this demonstrates that our method separates adjacent nuclei accurately, and
maintains its robustness, achieving consistent performance even when the point
annotations are not located at the center of the nuclei.
additionally, in fig. 4, we
qualitatively show the results to highlight how our method precisely separates
adjacent nuclei.
pronet for nuclei instance segmentation
535
table 2. evaluation on the eﬀect of oﬀset and center maps.
4.
nuclei instance segmentation results on cpm17 (top 2 rows) and monuseg
(bottom 2 rows) images.
the images and the ground truth (gt) are shown in the left
column.
we conducted ablation studies to assess the impact of
the oﬀset regression module, geodesic distance, and point reﬁnement process
(table 2).
when the binary segmentation module is combined only with the cen-
ter map module without the oﬀset module, the model could separate nuclei only
trained by the ideal label.
on the other hand, since there was no reﬁnement pro-
cess due to the absence of the oﬀset map, inaccurate points extracted from the
center map are obtained in the real-world scenario.
we also demonstrate that
labels with geodesic distance help improve overall performance.
the geodesic distance and reﬁnement
process also improved the accuracy by contributing to more accurate pseudo
labels.
especially, most variants show a signiﬁcant drop in performance when
the annotations shift was over 4 pixels.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_1.pdf:
to address this dilemma, pet enhancement
methods have been developed by improving the quality of low-dose pet
(lpet) images to standard-dose pet (spet) images.
however, previ-
ous pet enhancement methods rely heavily on the paired lpet and
spet data which are rare in clinic.
thus, in this paper, we propose
an unsupervised pet enhancement (upete) framework based on the
latent diﬀusion model, which can be trained only on spet data.
speciﬁ-
cally, our spet-only upete consists of an encoder to compress the input
spet/lpet images into latent representations, a latent diﬀusion model
to learn/estimate the distribution of spet latent representations, and a
decoder to recover the latent representations into spet images.
more-
over, from the theory of actual pet imaging, we improve the latent
diﬀusion model of upete by 1) adopting pet image compression
for reducing the computational cost of diﬀusion model, 2) using pois-
son diﬀusion to replace gaussian diﬀusion for making the perturbed
samples closer to the actual noisy pet, and 3) designing ct-guided
cross-attention for incorporating additional ct images into the inverse
process to aid the recovery of structural details in pet.
with extensive
experimental validation, our upete can achieve superior performance
over state-of-the-art methods, and shows stronger generalizability to the
dose changes of pet imaging.
the code of our implementation is avail-
able at https://github.com/jiang-cw/pet-diﬀusion.
keywords: positron emission tomography (pet) ·
enhancement ·
latent diﬀusion model · poisson diﬀusion · ct-guided cross-attention
1
introduction
positron emission tomography (pet) is a sensitive nuclear imaging technique,
and plays an essential role in early disease diagnosis, such as cancers and
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
however, acquiring high-quality pet images requires
injecting a suﬃcient dose (standard dose) of radionuclides into the human body,
which poses unacceptable radiation hazards for pregnant women and infants
even following the as low as reasonably achievable (alara) principle
[19].
to reduce the radiation hazards, besides upgrading imaging hardware, design-
ing advanced pet enhancement algorithms for improving the quality of low-dose
pet (lpet) images to standard-dose pet (spet) images is a promising alter-
native.
in recent years, many enhancement algorithms have been proposed to
improve pet image quality.
[22], which are quite robust but tend to over-smooth
images and suppress the high-frequency details.
subsequently, with the devel-
opment of deep learning, the end-to-end pet enhancement networks [9,14,21]
were proposed and achieved signiﬁcant performance improvement.
consequently, unsupervised pet enhancement
methods such as deep image prior
fortunately, the recent glowing diﬀusion model [6] provides us with the idea
for proposing a clinically-applicable pet enhancement approach, whose train-
ing only relies on spet data.
generally, the diﬀusion model consists of two
reversible processes, where the forward diﬀusion adds noise to a clean image
until it becomes pure noise, while the reverse process removes noise from pure
noise until the clean image is recovered.
by combining the mechanics of diﬀusion
model with the observation that the main diﬀerences between lpet and spet
are manifested as levels of noises in the image [11], we can view lpet and spet
as results at diﬀerent stages in an integrated diﬀusion process.
however, extending the diﬀusion
model developed for 2d photographic images to pet enhancement still faces two
problems: a) three-dimensionsal (3d) pet images will dramatically increase the
computational cost of diﬀusion model; b) pet is the detail-sensitive images and
may be introduced/lost some details during the procedure of adding/removing
noise, which will aﬀect the downstream diagnosis.
taking all into consideration, we propose the spet-only unsupervised pet
enhancement (upete) framework based on the latent diﬀusion model.
speciﬁ-
cally, upete has an encoder-<diﬀusion model>-decoder structure that ﬁrst uses
the encoder to compress input the lpet/spet images into latent representa-
tions, then uses the latent diﬀusion model to learn/estimate the distribution
of spet latent representations, and ﬁnally uses the decoder to recover spet
images from the estimated spet latent representations.
the keys of our upete
pet-diﬀusion: unsupervised pet enhancement
5
fig.
(a) and (b) provide the framework of upete as
well as depict its implementation during both the training and testing phases, and (c)
illustrates the details of ct-guided cross-attention.
include 1) compressing the 3d pet images into a lower dimensional space for
reducing the computational cost of diﬀusion model, 2) adopting the poisson
noise, which is the dominant noise in pet imaging
[20], to replace the gaussian
noise in the diﬀusion process for avoiding the introduction of details that are not
existing in pet images, and 3) designing ct-guided cross-attention to incorpo-
rate additional ct images into the inverse process for helping the recovery of
structural details in pet.
our work had three main features/contributions: i) proposing a clinically-
applicable unsupervised pet enhancement framework, ii) designing three tar-
geted strategies for improving the diﬀusion model, including pet image com-
pression, poisson diﬀusion, and ct-guided cross-attention, and iii) achieving
better performance than state-of-the-art methods on the collected pet datasets.
1. when given an input pet
image x (i.e., spet for training and lpet for testing), x is ﬁrst compressed
into the latent representation z0 by the encoder e. subsequently, z0 is fed into a
latent diﬀusion model followed by the decoder d to output the expected spet
image ˆx
in addition, a specialized encoder ect is used to compress the ct
image corresponding to the input pet image into the latent representation zct ,
which is fed into each denoising network for ct-guided cross-attention.
in the
following, we introduce the details of image compression, latent diﬀusion model,
and implementation.
6
c. jiang et al.
2.1
image compression
the conventional diﬀusion model is computationally-demanding due to its
numerous inverse denoising steps, which severely restricts its application to
3d pet enhancement.
to overcome this limitation, we adopt two strategies
including 1) compressing the input image and 2) reducing the diﬀusion steps (as
described in sect.
similar to [10,18], we adopt an autoencoder (e and d) to compress the
3d pet images into a lower dimensional but more compact space.
the crucial
aspects of this process is to ensure that the latent representation contains the
necessary and representative information for the input image.
among
them, the perceptual loss, designed on a pre-trained 3d resnet [2], constrains
higher-level information such as texture and semantic content, and the patch-
based adversarial loss ensures globally coherent while remaining locally realistic.
let x ∈ rh,w,z denote the input image and z0 ∈ rh,w,z,c denote the latent
representation.
in this way, we compress the input image by a factor of f = h/h =
w/w = z/z. the results of spet estimation under diﬀerent compression rates
f are provided in the supplement.
2.2
latent diﬀusion model
after compressing the input pet image, its latent representation is fed into the
latent diﬀusion model, which is the key to achieving the spet-only unsupervised
pet enhancement.
but the diﬀusion model is developed from photographic images, which
have signiﬁcant diﬀerence with the detail-sensitive pet images.
to improve
its applicability for pet images, we design several targeted strategies for the
diﬀusion process and inverse process, namely poisson diﬀusion and ct-guided
cross-attention, respectively.
however, in
pet images, the dominant source of noise is poisson noise, rather than gaussian
noise.
considering this, in our upete we choose to adopt poisson diﬀusion to
perturb the input samples, which facilitates the diﬀusion model for achieving
better performance on the pet enhancement task.
(1)
pet-diﬀusion: unsupervised pet enhancement
7
at each diﬀusion step, we apply the perturb function to the previous perturbed
sample zt−1 by imposing a poisson noise with an expectation of λt, which is
linearly interpolated from [0, 1] and incremented with t. in our implementation,
we apply the same poisson noise imposition operation as in [20], i.e., applying
poisson deviates on the projected sinograms, to generate a sequence of perturbed
samples with increasing poisson noise intensity as the step number t increases.
the attenuation correction of pet typically
relies on the corresponding anatomical image (ct or mr), resulting in a pet
scan usually accompanied by a ct or mr scan.
to fully utilize the extra-
modality images (i.e., ct in our work) as well as improve the applicability of
diﬀusion models, we design a ct-guided cross-attention to incorporate the ct
images into the reverse process for assisting the recovery of structural details.
as shown in fig. 1, to achieve a particular spet estimation, the correspond-
ing ct image is ﬁrst compressed into the latent representation zct by encoder
ect .
denoting the output of previous layer
as zp et , the ct-guided cross-attention can be formulated as follows:
output = softmax(qct kt
ct
√
d
+ b) · vp et ,
qct = convq(zct ),
kct = convk(zct ),
vp et = convv (zp et ),
(2)
where d is the number of channels, b is the position bias, and conv(·) denotes
the 1 × 1 × 1 convolution with stride of 1.
2.3
implementation details
typically, the trained diﬀusion model generates target images from random
noise, requiring a large number of steps t to make the ﬁnal perturbed sam-
ple (zt ) close to pure noise.
however, in our task, the target spet image is
generated from a given lpet image during testing, and making zt as close to
pure noise as possible is not necessary since the remaining pet-related informa-
tion can also beneﬁt the image recovery.
therefore, we can considerably reduce
the number of diﬀusion steps t to accelerate the model training, and t is set
to 400 in our implementation.
2. generalizability to dose changes.
3
experiments
3.1
dataset
our dataset consists of 100 spet images for training and 30 paired lpet and
spet images for testing.
among them, 50 chest-abdomen spet images are
collected from (total-body) uexplorer pet/ct scanner
[25], and 20 paired
chest-abdomen images are collected by list mode of the scanner with 256 mbq of
[18f]-fdg injection.
speciﬁcally, the spet images are reconstructed by using
the 1200 s data between 60–80 min after tracer injection, while the corresponding
lpet images are simultaneously reconstructed by 120 s data uniformly sampled
from 1200 s data.
as a basic data preprocessing, all images are resampled to voxel spacing of
2 × 2 × 2 mm3 and resolution of 256 × 256 × 160, while their intensity range is
normalized to [0, 1] by min-max normalization.
for increasing the training sam-
ples and reducing the dependence on gpu memory, we extract the overlapped
patches of size 96 × 96 × 96 from every whole pet image.
all methods use the same experi-
mental settings, and their quantitative results are given in table 1.
from table 1, we can have the following observations.
(1) ldm-p achieves
better performance than ldm.
this proves that the poisson diﬀusion is more
appropriate than the gaussian diﬀusion for pet enhancement.
(2) ldm-ct
with the corresponding ct image for assisting denoising achieves better results
than ldm.
this can be reasonable as the ct image can provide anatomical infor-
mation, thus beneﬁting the recovery of structural details (e.g., organ boundaries)
in spet images.
(3) ldm-p-ct achieves better results than all other variants
pet-diﬀusion: unsupervised pet enhancement
9
table 2.
quantitative comparison of our upete with several state-of-the-art pet
enhancement methods, in terms of psnr and ssim, where ∗ denotes unsupervised
method and † denotes fully-supervised method.
on both psnr and ssim, which shows both of our proposed strategies con-
tribute to the ﬁnal performance.
these three comparisons conjointly verify the
eﬀective design of our proposed upete, where the poisson diﬀusion process and
ct-guided cross-attention both beneﬁt the pet enhancement.
3.3
comparison with state-of-the-art methods
we further compare our upete with several state-of-the-art pet enhancement
methods, which can be divided into two classes: 1) fully-supervised methods,
including la-gan [21], transformer-gan (trans-gan)
[9], and ar-gan [14]; 2) unsupervised methods, including
deep image prior (dip)
compared to the fully-supervised method ar-gan which
achieves sub-optimal performance, our upete does not require paired lpet and
spet, yet still achieves improvement.
additionally, upete also achieves notice-
able performance improvement to noise2void (which is a supervised method).
speciﬁcally, the average improvement in psnr and ssim on spet estimation
are 1.554 db and 0.005, respectively.
first, compared to unsupervised methods such
as dip and noise2void, the spet images estimated by our upete have less noise
but clearer boundaries.
second, our upete performs better on the structural
details compared to the fully-supervised methods, i.e., missing unclear tissue
(trans-gan) or introducing non-existing artifacts in pet image (df-gan).
3. visual comparison of estimated spet images on two typical cases.
3.4
generalization evaluation
we further evaluate the generalizability of our upete to tracer dose changes by
simulating poisson noise on spet to produce diﬀerent doses for lpet, which
is a common way to generate noisy pet data [20].
the main reason is that the unsupervised learning has the ability to extract
patterns and features from the data based on the inherent structure and distri-
bution of the data itself [15].
pet-diﬀusion: unsupervised pet enhancement
11
4

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_52.pdf:
uda allows for the use of
large-scale datasets from various domains for model deployment, but it
can face diﬃculties in performing adaptive feature extraction when deal-
ing with unlabeled data in an unseen target domain.
this helps the framework to converge to a local mini-
mum that is better-suited for the target domain, allowing for improved
performance in domain adaptation tasks.
to evaluate our framework, we
carried out experiments on uda segmentation tasks using breast can-
cer datasets acquired from multiple domains.
our experimental results
demonstrated that our framework achieved state-of-the-art performance,
outperforming other competing uda models, in segmenting breast can-
cer on ultrasound images from an unseen domain, which supports its
clinical potential for improving breast cancer diagnosis.
keywords: unsupervised domain adaptation · test-time tuning ·
breast cancer · segmentation · ultrasound imaging
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43907-0_52.
https://doi.org/10.1007/978-3-031-43907-0_52
540
k. lee et al.
1
introduction
in recent years, deep learning (dl) methods have demonstrated remarkable
performance in detecting and localizing tumors on ultrasound images [2,27].
compared with conventional image processing methods, dl methods provide
an accurate feature extraction capability on ultrasound images, despite their
low resolution and noise disturbance, leading to superior segmentation accu-
racy
yet, acquiring large training datasets and
their corresponding labels, especially from a cohort of patients, can be costly
or even infeasible, which poses a signiﬁcant challenge in developing a dl model
with high performance [7].
second, even when large-scale datasets are available
through collaborative research from multiple sites, dl models trained on such
datasets may yield sub-optimal solutions due to domain gaps caused by diﬀer-
ences in images acquired from diﬀerent sites [20].
third, due to the small number
of datasets from each domain, the images for each individual domain may not
capture representative features, limiting the ability of dl models to generalize
across domains [3].
domain adaptation (da) has been extensively studied to alleviate the afore-
mentioned limitations, the goal of which is to reduce the domain gap caused by
the diversity of datasets from diﬀerent domains
nonetheless,
unlike natural images, generating labels can be a challenging task, making it dif-
ﬁcult to apply general da methods; thus bridging domain gaps by da methods
remains limited
this can lead to
signiﬁcant degradation of the performance of dl models, as errors can compound
and become more pronounced over time [17,25].
to alleviate the problem of pseudo-label-based uda, in this work, we propose
an advanced uda framework based on self-supervised da with a test-time ﬁne-
tuning network.
the distinctive feature
of our test-time self-supervised da is that it enables the dl network (i) to learn
knowledge about the features of target domains by ﬁne-tuning the network itself
during the test-time phase, rather than generating pseudo-labels and then (ii) to
provide precise predictions on images in target domains, by using the ﬁne-tuned
network.
our framework was tested on the task of breast
self-supervised domain adaptive segmentation of breast cancer
541
cancer segmentation in ultrasound images, but it could also be applied to other
lesion segmentation tasks.
to summarize, our contributions are three-fold:
• we design a self-supervised da framework that includes a parameter search
method and provide a mathematical justiﬁcation for it.
with our framework,
we are able to identify the best-performing parameters that result in improved
performance in da tasks.
• we applied our framework to the task of segmenting breast cancer from ultra-
sound imaging data, demonstrating its superior performance over competing
uda methods.
our results indicate that our framework is eﬀective in improving the accuracy of
breast cancer segmentation from ultrasound images, which could have potential
implications for improving the diagnosis and treatment of breast cancer.
1. in
the main task, an encoder (e), a decoder for segmentation (dseg), and a seg-
mentation header (h) are included.
the main task is the segmentation task,
(h◦dseg◦e)(x).
in predicting segmentation labels in the target domain (t ), dft
is also involved in the main task, and the ﬁnal prediction after the ﬁne-tuning is
542
k. lee et al.
provided by

h ◦ (dseg ⊕ dft) ◦ e

(x), where ⊕ is the concatenation operation.
the pretext task aims to generate synthetic images, (dgen ◦ e)(t).
however, since the headers of image reconstruction and generating segmen-
tation mask are diﬀerent (diﬀerent output), a new header incorporating df t
and dseg is devised and leverages the outputs of two decoders.
besides, dgen =
df t is ﬁne-tuned during the ﬁne-tuning step, and the df t learns the knowledge
of the input domain via image reconstruction.
the model m is ﬁrst trained in s in a
supervised manner with (s, ¯s) ∼ s in both main and pretext tasks as below:
θm
s , θp
s = argmin
θm
s ,θp
s

s

lbce

(h ◦ dseg ◦ e)(s), ¯s

+ lgan

(dgen ◦ e)(s), s

,
(1)
where lbce and lgan represent the loss functions for binary cross-entropy and
generative adversarial network
to this end, in the pretext task, for self-supervised learning, the model
is ﬁne-tuned in t to generate synthetic images identical to the input images as
below:
θp
t = argmin
θp
t

t
lgan

(ds
gen ◦ es)(t), t

⇒
θp
t ⊇ es ∪ ds→t
gen
,
(2)
where only dgen is ﬁne-tuned to achieve memory eﬃciency and to decrease the
ﬁne-tuning time, and ds
gen is ﬁne-tuned as ds→t
gen
.
(3)
since ds
seg is fully optimized for s in a supervised manner, it guarantees a
baseline segmentation performance.
furthermore, since dt
ft is ﬁne-tuned in t
self-supervised domain adaptive segmentation of breast cancer
543
fig.
wi ·x. since ds
seg provides the baseline segmentation performance, dt
f t
should provide similar feature maps to achieve the baseline performance.
(5)
3
experiments
3.1
experimental set-ups
to evaluate the segmentation performance of our ttft framework, we used
three diﬀerent ultrasound databases: bus
all three databases contain ultrasound
imaging data and segmentation masks for breast cancer, with the masks labeled
as 0 (background) and 1 (lesion) using a one-hot encoding.
the bus database
consists of 163 images along with corresponding labels.
the busi database con-
tains 780 images, with 133 images belonging to the normal class and having
labels containing only 0 values.
while the database also provides
labels for the detection task, we processed these labels as segmentation masks
using a region growing method [15].
[21] were employed as our baseline models, since u-net
is a widely used basic model for segmentation, and fusionnet contains advanced
residual modules, compared with u-net.
additionally, mib-
net [28], which is a state-of-the-art model for breast cancer segmentation using
ultrasound images, was employed for comparison.
our experimen-
tal set-ups included: (i) individual databases were used to assess the baseline
segmentation performance (appendix); (ii) the domain adaptive segmentation
performance was assessed using the three databases, where two databases were
regarded as the source domain, and the remaining database was regarded as the
target domain; and (iii) the ablation study was carried out to evaluate the pro-
posed network architecture along with the randomized re-initialization method.
3.2
comparison analysis
since all compared dl models show similar d. coef, only uda performance is
comparable as a control in our experiments.
in this experiment, two databases
were used for training, and the remaining database was used for testing.
3 illustrates the bus database was used for testing, and
self-supervised domain adaptive segmentation of breast cancer
545
fig.
3. comparison analysis of our framework and comparison models: performance
comparison table (left) and box-and-whisker plot (right).
5 shows the sample segmentation results.
unlike the experiment using the individual database, u-net, fusionnet, and
mib-net showed signiﬁcantly inferior scores due to domain gaps.
in contrast,
uda methods of cbst and ct-net showed superior scores, compared with
others, and the scores were not strongly reduced, compared with the experiment
with the single database.
note that, our ttft framework achieved the best
performance compared with other dl models.
5. segmentation results by ours and comparison models on each database.
fig.
6. illustration of feature maps: style loss comparison (left) and a t-sne plot of
generated images by diﬀerent decoders (right)
3.3
ablation study
in order to assess the eﬀectiveness of each of the proposed modules, includ-
ing the parameter ﬂuctuation and ﬁne-tuning methods, the ablation study was
carried out.
the higher d. coef value (+3.4%) of pre-train + pf than
that of pre-train + random init and pre-train + oﬀset conﬁrms the eﬀec-
tiveness of the parameter ﬂuctuation in the uda performance.
additionally,
the higher score (+11%) of fine-tuning than pre-train shows an outstanding
uda performance of the ﬁne-tuning pipeline.
using dual-pipeline and parameter ﬂuctuation yielded
the best performance.
however, the utilization of ensemble pipelines of multiple
ﬁne-tuning modules was ineﬃcient, since negligible performance improvements
(+0.002) were observed, despite the heavy memory utilization.
our framework was ﬁne-tuned as ds
seg → dﬂ
seg → ds→t
seg
self-supervised domain adaptive segmentation of breast cancer
547
table 1.
additionally, the generated images
by decoders, including ds
seg, dﬂ
seg, and ds→t
seg
in s and t are plotted with t-
sne, where the short distance represents the similar features [19].
the generated
images became similar to t in order of ds
seg, dﬂ
seg, and ds→t
seg
, which conﬁrmed
the eﬀectiveness of the ﬁne-tuning method in terms of knowledge distillation.
additionally, the parameters were successfully re-positioned from the local min-
imum in s by parameter ﬂuctuation, which was conﬁrmed by the distances from
s to ds
gen and dﬂ
gen.
4
discussion and conclusion
in this work, we proposed a dl-based segmentation framework for multi-domain
breast cancer segmentation on ultrasound images.
due to the low resolution of
ultrasound images, manual segmentation of breast cancer is challenging even
for expert clinicians, resulting in a sparse number of labeled data.
to address
this issue, we introduced a novel self-supervised da network for breast cancer
segmentation in ultrasound images.
since uda is susceptible to error accumulation due to
imprecise pseudo-labels, which can lead to degraded performance, we employed
a self-supervised learning-based pretext task.
speciﬁcally, we utilized an auto-
encoder-based network architecture to generate synthetic images that matched
the input images.
this approach enabled our framework to eﬃciently
ﬁne-tune the network in the target domain and achieve better segmentation
performance.
experimental results, carried out with three ultrasound databases
from diﬀerent domains, demonstrated the superior segmentation performance of
our framework over other competing methods.
[21] as baseline models to evaluate the basic performance of our
ttft framework.
however, the use of more advanced baseline models could lead
to even better segmentation performance, which is a subject for our future work.
moreover, our proposed framework is not limited to breast cancer segmentation
on ultrasound images acquired from diﬀerent domains.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_20.pdf:
automated detection of gallbladder cancer (gbc) from
ultrasound (us) images is an important problem, which has drawn
increased interest from researchers.
in this paper, we focus on gbc detection using
only image-level labels.
however, our analysis reveals that it is diﬃ-
cult to train a standard image classiﬁcation model for gbc detection.
this is due to the low inter-class variance (a malignant region usually
occupies only a small portion of a us image), high intra-class variance
(due to the us sensor capturing a 2d slice of a 3d object leading to large
viewpoint variations), and low training data availability.
we posit that
even when we have only the image level label, still formulating the prob-
lem as object detection (with bounding box output) helps a deep neural
network (dnn) model focus on the relevant region of interest.
our proposed method demon-
strates an improvement of ap and detection sensitivity over the sota
transformer-based and cnn-based wsod methods.
project page is at
https://gbc-iitd.github.io/wsod-gbc.
keywords: weakly supervised object detection · ultrasound ·
gallbladder cancer
1
introduction
gbc is a deadly disease that is diﬃcult to detect at an early stage [12,15].
non-ionizing radiation,
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43907-0 20.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. (eds.): miccai 2023, lncs 14220, pp.
206–215, 2023.
https://doi.org/10.1007/978-3-031-43907-0_20
gall bladder cancer detection from us images
207
low cost, and accessibility make us a popular non-invasive diagnostic modality for
patients with suspected gall bladder (gb) aﬄictions.
in recent
years, automated gbc detection from us images has drawn increased interest
[3,5] due to its potential for improving diagnosis and treatment outcomes.
many
of these works formulate the problem as an object detection, since training a
image classiﬁcation model for gbc detection seems challenging due to the reasons
outlined in the abstract (also see fig. 1).
fig.
however, the appearance of the gb
in all three images is very similar.
all three images
have been scanned from the same patient, but due to the sensor’s scanning plane, the
appearances change drastically.
recently, gbcnet [3], a cnn-based model, achieved sota performance on clas-
sifying malignant gb from us images.
gbcnet uses a two-stage pipeline consisting
of object detection followed by classiﬁcation, and requires bounding box annota-
tions for gb as well as malignant regions for training.
in another recent work, [5]
has exploited additional unlabeled video data for learning good representations
for downstream gbc classiﬁcation and obtained performance similar to [3] using
a resnet50 [13] classiﬁer.
on the other hand, the image-level
malignancy label is usually available at a low cost, as it can be obtained readily
from the diagnostic report of a patient without additional eﬀort from clinicians.
however, since we only have image-level labels avail-
able, we formulate the problem as a weakly supervised object detection (wsod)
problem.
however, in our initial experiments sota wsod methods for
transformers failed miserably.
four images from
each of the disease and non-disease classes are shown on the left and right, respectively.
disease locations are shown by drawing bounding boxes.
in this, one generates region proposals for images, and then con-
siders the images as bags and region proposals as instances to solve the instance
classiﬁcation (object detection) under the mil constraints
our experiments val-
idate the utility of this approach in circumventing the challenges in us images
and detecting gbc accurately from us images using only image-level labels.
contributions: the key contributions of this work are:
– we design a novel detr variant based on mil with self-supervised instance
learning towards the weakly supervised disease detection and localization task
in medical images.
– we formulate the gbc classiﬁcation problem as a weakly supervised object
detection problem to mitigate the eﬀect of low inter-class and large intra-class
variances, and solve the diﬃcult gbc detection problem on us images without
using the costly and diﬃcult to obtain additional annotation (bounding box)
or video data.
– our method provides a strong baseline for weakly supervised gbc detection
and localization in us images, which has not been tackled earlier.
further, to
assess the generality of our method, we apply our method to polyp detection
from colonoscopy images.
gall bladder cancer detection from us images
209
2
datasets
gallbladder cancer detection in ultrasound images: we use the pub-
lic gbc us dataset
[3] consisting of 1255 image samples from 218 patients.
the dataset contains 990 non-malignant (171 patients) and 265 malignant (47
patients) gb images (see fig.
2 for some sample images).
the dataset contains
image labels as well as bounding box annotations showing the malignant regions.
note that, we use only the image labels for training.
we did the cross-validation splits at the patient level, and all
images of any patient appeared either in the train or validation split.
fig.
polyp detection in colonoscopy images: we use the publicly available
kvasir-seg
[17] dataset consisting of 1000 white light colonoscopy images show-
ing polyps (see fig.
since kvasir-seg does not contain any control images,
we add 600 non-polyp images randomly sampled from the polypgen [1] dataset.
for gbc classiﬁcation, if the model generates
bounding boxes for the input image, then we predict the image to be malignant,
since the only object present in the data is the cancer.
mil setup: the decoder of the ﬁne-tuning detr generates r d-dimensional
output embeddings.
the two
matrices are element-wise multiplied and summed over the proposal dimension
to generate the image-level classiﬁcation predictions, φ ∈ rnc:
φj =
r

i=1
cij · dij
(1)
notice, φj ∈ (0, 1) since cij and dij are normalized.
finally, the negative log-
likelihood loss between the predicted labels, and image labels y ∈ rnc is com-
puted as the mil loss:
+ (1 − yi) log (1 − φi)]
(2)
the mil classiﬁer further suﬀers from overﬁtting to the distinctive classiﬁcation
features due to the mismatch of classiﬁcation and detection probabilities [24].
to tackle this, we further use a self-supervised module to improve the instances.
gall bladder cancer detection from us images
211
self-supervised instance learning: inspired by [24], we design a instance
learning module with nr blocks in a self-supervised framework to reﬁne the
instance scores with instance-level supervision.
weakly supervised disease detection performance comparison of our method
and sota baselines in gbc and polyps.
we report average precision at iou 0.25
(ap25).
performance of mil-framework variants on detr.
5:
l = lmil + λlins
(5)
4
experiments and results
experimental setup: we use a machine with intel xeon gold 5218@2.30ghz
processor and 8 nvidia tesla v100 gpus for our experiments.
the model is
trained using sgd with lr 0.001 (for mil head), weight decay 10−6, and
momentum 0.9 for 100 epochs with batch size 32.
our method surpasses all latest sota wsod techniques by 9
points, and establishes itself as a strong wsod baseline for gbc localization in us
images.
generality of the method: we assess the generality of our method by apply-
ing it to polyp detection on colonoscopy images.
the applicability of our method
on two diﬀerent tasks - (1) gbc detection from us and (2) polyp detection from
colonoscopy, indicates the generality of the method across modalities.
gall bladder cancer detection from us images
213
ablation study: we show the detection sensitivity to the self-supervised
instance learning module in table 2 for two variants, (1) vanilla mil head
on detr, and (2) mil with self-supervised instance learning on detr.
table 2
shows the average precision and detection sensitivity for both diseases.
other
ablations related to the hyper-parameter sensitivity is given in supplementary
fig.
classiﬁcation performance: we compare our model with the standard cnn-
based and transformer-based classiﬁers, sota wsod-based classiﬁers, and sota
classiﬁers using additional data or annotations (table 3).
performance comparison of our method and other sota methods in gbc
classiﬁcation.
0.861 ± 0.089
table 4. comparison with sota wsod baselines in classifying polyps from colonoscopy
images.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_22.pdf:
pretraining strategies, like contrastive learning (cl)
methods, can leverage unlabeled or weakly-annotated datasets.
these
methods typically require large batch sizes, which poses a diﬃculty in the
case of large 3d images at full resolution, due to limited gpu memory.
we illustrate our method on cirrhosis prediction using a large volume of
weakly-labeled images, namely radiological low-conﬁdence annotations,
and small strongly-labeled (i.e., high-conﬁdence) datasets.
it is however possible to obtain lower conﬁdence assessments for a large
amount of images, either by a clinical questioning, or directly by a radiological
diagnosis.
to take advantage of large volumes of unlabeled or weakly-labeled
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43907-0 22.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. (eds.): miccai 2023, lncs 14220, pp.
https://doi.org/10.1007/978-3-031-43907-0_22
228
e. sarfati et al.
images, pre-training encoders with self-supervised methods showed promising
results in deep learning for medical imaging [1,4,21,27–29].
in particular, con-
trastive learning (cl) is a self-supervised method that learns a mapping of
the input images to a representation space where similar (positive) samples are
moved closer and diﬀerent (negative) samples are pushed far apart.
in this work, we focus on the scenario where radiological meta-data
(thus, low-conﬁdence labels) are available for a large amount of images, whereas
high-conﬁdence labels, obtained by histological analysis, are scarce.
naive extensions of contrastive learning methods, such as [5,10,11], from 2d
to 3d images may be diﬃcult due to limited gpu memory and therefore small
batch size.
how-
ever, these methods pose two diﬃculties: they reduce the spatial context (lim-
ited by the size of the patch), and they require similar spatial resolution across
images.
furthermore, we also propose to simultaneously leverage weak discrete attributes
during pre-training by using a novel and eﬃcient contrastive learning composite
kernel loss function, denoting our global method weakly-supervised positional
(wsp).
2
method
let xt be an input 2d image, usually called anchor, extracted from a 3d volume,
yt a corresponding discrete weak variable and dt a related continuous variable.
in this paper, yt refers to a weak radiological annotation and dt corresponds
to the normalized depth position of the 2d image within its corresponding 3d
volume: if vmax corresponds to the maximal depth-coordinate of a volume v ,
we compute dt =
pt
vmax with pt ∈ [0, vmax] being the original depth coordinate.
weakly-supervised positional contrastive learning
229
let x−
j and x+
i be two semantically diﬀerent (negative) and similar (positive)
images with respect to xt, respectively.
for instance, in unsupervised cl, methods such as simclr [5,
6] choose as positive samples random augmentations of the anchor x+
i = t(xt),
where t ∼ t is a random transformation chosen among a user-selected family t .
negative images x−
j are all other (transformed) images present in the batch.
once x−
j and x+
i are deﬁned, the goal of cl is to compute a mapping function
fθ : x → sd, where x is the set of images and sd the representation space, so
that similar samples are mapped closer in the representation space than dissimi-
lar samples.
[13], the authors deﬁne
as positives all images with the same discrete label y. however, when working
with continuous labels d, one cannot use the same strategy since all images are
somehow positive and negative at the same time.
[13].
in this work, we propose to leverage both continuous d and discrete y labels,
by combining (here by multiplying) the previously deﬁned kernels, wσ and wδ,
into a composite kernel loss function.
wσ(dt, di)



composite kernel wti
(stj − sti) ≤ 0
∀t, i, j ̸= i
(2)
where the indices t, i, j traverse all n images in the batch since there are no
“hard” positive or negative samples, as in simclr or supcon, but all images
are considered as positive and negative at the same time.
= {i : yi = yt} as the set of indices of images xi in the
batch with the same discrete label yi as the anchor xt, we can rewrite our ﬁnal
loss function as:
lw sp = −
n

t=1

i∈p (t)
wσ(dt, di) log
	
exp(sti)

n
j̸=i exp(stj)

(4)
where wσ(dt, di) is normalized over i ∈ p(t).
a robustness study is available in the supplementary material.
for
the experiments, we ﬁx σ = 0.1.
3
experiments
we compare the proposed method with diﬀerent contrastive and non-contrastive
methods, that either use no meta-data (simclr [5], byol [10]), or leverage
weakly-supervised positional contrastive learning
231
only discrete labels (supcon [13]), or continuous labels (depth-aware [8]).
in all experiments, we work with 2d slices rather
than 3d volumes due to the anisotropy of abdominal ct-scans in the depth
direction and the limited spatial context or resolution obtained with 3d patch-
based or downsampling methods, respectively, which strongly impacts the cir-
rhosis diagnosis that is notably based on the contours irregularity.
3.1
datasets
three datasets of abdominal ct images are used in this study.
all images
have a 512 × 512 size, and we clip the intensity values between -100 and 400.
dradio.
first, dradio contains 2,799 ct-scans of patients in portal venous
phase with a radiological (weak) annotation, i.e. realized by a radiologist, indi-
cating four diﬀerent stages of cirrhosis: no cirrhosis, mild cirrhosis, moderate
cirrhosis and severe cirrhosis (yradio).
in all datasets, we select the slices based on the liver segmentation of the
patients.
to gain in precision, we keep the top 70% most central slices with
respect to liver segmentation maps obtained manually in dradio, and automati-
cally for d1
histo and d2
histo using a u-net architecture pretrained on dradio
[18].
for the latter pretraining dataset, it presents an average slice spacing of 3.23 mm
with a standard deviation of 1.29 mm.
for the x and y axis, the dimension is
0.79 mm per voxel on average, with a standard deviation of 0.10 mm.
3.2
architecture and optimization
backbones.
in comparison, resnet-18 has 11.2m parameters, a represen-
tation space of dimension 512 and a latent space of dimension 128.
more details
and an illustration of tinynet are available in the supplementary material, as
well as a full illustration of the algorithm ﬂow.
data augmentation, sampling and optimization.
[5,10,11]
require strong data augmentations on input images, in order to strengthen the
association between positive samples [22].
in our work, we leverage three types
of augmentations: rotations, crops and ﬂips.
data augmentations are computed
on the gpu, using the kornia library [17].
during inference, we remove the aug-
mentation module to only keep the original input images.
as we work
with 2d slices rather than 3d volumes, we compute the average probability per
patient of having the pathology.
finally, we run our experiments on a tesla v100 with 16gb of ram and
a 6 cpu cores, and we used the pytorch-lightning library to implement our
models.
all models share the same data augmentation module, with a batch size
of b = 64 and a ﬁxed number of epochs nepochs = 200.
for all experiments, we
ﬁx a learning rate (lr) of α = 10−4 and a weight decay of λ = 10−4.
for byol, we
initialize the moving average decay at 0.996.
evaluation protocol.
= we use the pretrained weights from
imagenet with resnet-18 and run a logistic regression on the frozen representations.
0.77 (±0.08)
supcon
✓
✗
0.76 (±0.09)
0.93 (±0.07)
0.72 (±0.06)
depth-aware
✗
✓
0.80 (±0.13)
0.81 (±0.08)
0.77 (±0.08)
ours
✓
✓
0.84 (±0.12)
0.91 (±0.11)
0.79 (±0.11)
resnet-18
supervised
✗
✗
0.77 (±0.10)
0.56 (±0.29)
0.72 (±0.08)
none (random)
✗
✗
0.69 (±0.19)
0.73 (±0.12)
0.68 (±0.09)
imagenet*
✗
✗
0.72 (±0.17)
0.76 (±0.04)
0.66 (±0.10)
simclr
✗
✗
0.79 (±0.09)
0.82 (±0.14)
0.79 (±0.08)
byol
✗
✗
0.78 (±0.09)
0.77 (±0.11)
0.78 (±0.08)
supcon
✓
✗
0.69 (±0.07)
0.69 (±0.13)
0.76 (±0.12)
depth-aware
✗
✓
0.83 (±0.07)
0.82 (±0.11)
0.80 (±0.07)
ours
✓
✓
0.84 (±0.07)
0.85 (±0.10)
0.84 (±0.07)
fig.
blue = healthy
subjects.
4
results and discussion
we present in table 1 the results of all our experiments.
supcon per-
forms well on the training set of dradio (ﬁgure available in the supplementary
material), as well as d2
histo with tinynet, but it poorly generalizes to d1
histo
and d1+2
histo.
the method depth-aware manages to correctly encode the depth
position but not the diagnostic class label.
to assess the clinical performance of the pretraining methods, we also com-
pute the balanced accuracy scores (bacc) of the trained classiﬁers, which is
compared in table 2 to the bacc achieved by radiologists who were asked to
visually assess the presence or absence of cirrhosis for the n=106 cases of d1
histo.
pretraining method bacc models bacc radiologists
supervised
0.78 (±0.04)
none (random)
0.71 (±0.13)
imagenet
0.74 (±0.13)
simclr
0.78 (±0.08)
byol
0.77 (±0.04)
0.82
supcon
0.77 (±0.10)
depth-aware
0.84 (±0.04)
ours
0.85 (±0.09)

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_25.pdf:
improving the feature representation ability is the founda-
tion of many whole slide pathological image (wsis) tasks.
aiming towards
slide-level representations, we propose slide-level prototypical distil-
lation (slpd) to explore intra- and inter-slide semantic structures for
context modeling on wsis.
speciﬁcally, we iteratively perform intra-slide
clustering for the regions (4096 × 4096 patches) within each wsi to yield
the prototypes and encourage the region representations to be closer
to the assigned prototypes.
slpd
achieves state-of-the-art results on multiple slide-level benchmarks and
demonstrates that representation learning of semantic structures of slides
can make a suitable proxy task for wsi analysis.
keywords: computational pathology · whole slide images(wsis) ·
self-supervised learning
1
introduction
in computational histopathology, visual representation extraction is a fundamen-
tal problem [14], serving as a cornerstone of the (downstream) task-speciﬁc learn-
ing on whole slide pathological images (wsis).
our community has witnessed
the progress of the de facto representation learning paradigm from the super-
vised imagenet pre-training to self-supervised learning (ssl)
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43907-0_25.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
[12,22,27] directly apply the ssl
algorithms developed for natural images (e.g., simclr
[11]) to wsi analysis tasks, and the improved performance proves the
eﬀectiveness of ssl.
however, wsi is quite diﬀerent from natural images in that
it exhibits a hierarchical structure with giga-pixel resolution.
since the pretext tasks encourage to mine the patho-
logically relevant patterns, the learned representations are expected to be more
suitable for wsi analysis.
[8], a milestone work,
introduces hierarchical pre-training (dino [6]) for the patch-level (256 × 256)
and region-level (4096 × 4096) in a two-stage manner, achieving superior per-
formance on slide-level tasks.
[13] uses eﬃcientnet-b0 for image
compression in the ﬁrst stage and then derives multi-task learning on the com-
pressed wsis, which assumes the primary site information, e.g., the organ type,
is always available and can be used as pseudo labels.
[35] also pro-
poses a two-stage pre-training framework for wsis using contrastive learning
(simclr [10]), where the diﬀerently subsampled bags1 from the same wsi are
positive pairs in the second stage.
a similar idea can be found in giga-ssl [20]
with delicate patch- and wsi-level augmentations.
the aforementioned meth-
ods share the same two-stage pre-training paradigm, i.e., patch-to-region/slide.
however, they are essentially instance discrimination
where only the self-invariance of region/slide is considered, leaving the intra-
and inter-slide semantic structures unexplored.
in this paper, we propose to encode the intra- and inter-slide semantic struc-
tures by modeling the mutual-region/slide relations, which is called slpd: slide-
level prototypical distillation for wsis.
in order to learn this intra-slide semantic structure, we
encourage the region representations to be closer to the assigned prototypes.
by
representing each slide with its prototypes, we further select semantically simi-
1 by formulating wsi tasks as a multi-instance learning problem, the wsi is treated
as a bag with corresponding patches as instances.
slpd
261
(e) intra-slide disllaon 
(f) inter-slide disllaon 
(d) global (le) vs. slide-level clustering (right)
(a) hierarchical structure of wsi
(b) two-stage pretraining
probability 
distribuon 
prototype 
within slide 
prototypes 
across slides 
(c) slide-level prototypical disllaon 
wsi
region
patch
image
fig.
1. (a) a wsi possesses the hierarchical structure of wsi-region-patch-image, from
coarse to ﬁne.
(b) two-stage pre-training paradigm successively performs the image-to-
patch and patch-to-region aggregations.
slpd explores the
semantic structure by slide-level clustering.
then, we learn the inter-slide
semantic structure by building correspondences between region representations
and cross-slide prototypes.
we conduct experiments on two benchmarks, nsclc
subtyping and brca subtyping.
slpd achieves state-of-the-art results on mul-
tiple slide-level tasks, demonstrating that representation learning of semantic
structures of slides can make a suitable proxy task for wsi analysis.
1(a), a wsi exhibits hierarchical structure at varying resolu-
tions under 20× magniﬁcation: 1) the 4096×4096 regions describing macro-scale
organizations of cells, 2) the 256 × 256 patches capturing local clusters of cells,
3) and the 16 × 16 images characterizing the ﬁne-grained features at the cell-
level.
slpd
262
z. yu et al.
is built upon the two-stage pre-training paradigm proposed by hipt, which
will be described in sect.
we
characterize the semantic structure of slides in sect.
2.2, which is leveraged to
establish the relationship within and across slides, leading to the proposed intra-
and inter-slide distillation in sect.
2.5.
2.2
preliminaries
we revisit hierarchical image pyramid transformer (hipt)
, hipt proposes a two-stage pre-training paradigm
considering the hierarchical structure of wsis.
in stage one, a patch-level vision
transformer, denoted as vit256-16, aggregates non-overlapping 16 × 16 images
within 256 × 256 patches to form patch-level representations.
in stage two, the
pre-trained vit256-16 is freezed and leveraged to tokenize the patches within
4096 × 4096 regions.
hipt leverages dino
taking stage two as an
example, dino distills the knowledge from teacher to student by minimizing the
cross-entropy between the probability distributions of two views at region-level:
lself = ex∼pdh(gt(ˆz), gs(z)),
(1)
where h(a, b) = −a log b, and pd is the data distribution that all regions are
drawn from.
the parameters of the student
are exponentially moving averaged to the parameters of the teacher.
2.3
slide-level clustering
many histopathologic features have been established based on the morpho-
logic phenotypes of the tumor, such as tumor invasion, anaplasia, necrosis and
mitoses, which are then used for cancer diagnosis, prognosis and the estimation
of response-to-treatment in patients [3,9]. to obtain meaningful representations
of slides, we aim to explore and maintain such histopathologic features in the
latent space.
however, the obtained
clustering centers, i.e., the prototypes, are inclined to represent the visual bias
slpd
263
related to staining or scanning procedure rather than medically relevant fea-
tures [33].
each group of proto-
types is expected to encode the semantic structure (e.g., the combination of
histopathologic features) of the wsi.
2.4
intra-slide distillation
the self-distillation utilized by hipt in stage two encourages the correspondence
between two views of a region at the macro-scale because the organizations of
cells share mutual information spatially.
2.3, a
slide can be abstracted by a group of prototypes, which capture the semantic
structure of the wsi.
, we assume that the representation
z and its assigned prototype c also share mutual information and encourage z
to be closer to c with the intra-slide distillation:
lintra = ex∼pdh (gt(c), gs(z)) ,
(2)
we omit super-/sub-scripts of z for brevity.
through eq. 2, we can leverage more
intra-slide correspondences to guide the learning process.
for further understand-
ing, a prototype can be viewed as an augmented representation aggregating the
slide-level information.
thus this distillation objective is encoding such informa-
tion into the corresponding region embedding, which makes the learning process
semantic structure-aware at the slide-level.
previous self-supervised learning methods applied to histopatho-
logic images only capture such correspondences with positive pairs at the patch-
level [22,23], which overlooks the semantic structure of the wsi.
due to the heterogeneity of the slides, comparing them with the local
crops or the averaged global features are both susceptible to being one-sided.
to
264
z. yu et al.
address this, we bridge the slides with their semantic structures and deﬁne the
semantic similarity between two slides wi and wj through an optimal bipartite
matching between two sets of prototypes:
d(wi, wj) = max{ 1
m
m

m=1
cos(cm
i , cσ(m)
[−1, 1],
(3)
where cos(·, ·) measures the cosine similarity between two vectors, and sm enu-
merates the permutations of m elements.
speciﬁcally, for a region embedding z belonging to the slide w and
assigned to the prototype c, we ﬁrst search the top-k nearest neighbors of w in
the dataset based on the semantic similarity, denoted as { ˆwk}k
k=1.
finally, we encourage z to be
closer to ˆck with the inter-slide distillation:
(4)
the inter-slide distillation can encode the sldie-level information complementary
to that of intra-slide distillation into the region embeddings.
we believe the performance
can be further improved by tuning this.
3
experimental results
datasets.
we conduct experiments on two public wsi datasets2.
we extract 62,852 and 60,153 regions at 20× magniﬁcation from
tcga-nsclc and tcga-brca for pre-training vit4096-256 in stage two.
we
leverage the pre-trained vit256-16 in stage one provided by hipt to tokenize
the patches within each region.
we adopt the 10-fold cross validated accuracy (acc.)
and area under the curve (auc) to evaluate the weakly-supervised classiﬁcation
performance.
“mean” leverages the averaged pre-extracted embed-
dings to evaluate knn performance.
bold and underlined numbers highlight the best
and second best performance
#
feature
aggragtor
feature
extraction
pretrain
method
nsclc
brca
acc.
auc
acc.
auc
weakly supervised classiﬁcation
1
patch-level
dino
0.780±0.126
0.864±0.089
0.822±0.047
0.783±0.056
2
mil
[22]
region-level
dino
0.841±0.036
0.917±0.035
0.854±0.032
0.848±0.075
5
region-level
slpd
0.858±0.040
0.938±0.026
0.854±0.039
0.876±0.050
6
region-level
dino
0.843±0.044
0.926±0.032
0.849±0.037
0.854±0.069
7
region-level
dino+lintra
0.850±0.042
0.931±0.041
0.866±0.030
0.881±0.069
8
vitwsi-4096 [8]
region-level
dino+linter
0.850±0.043
0.938±0.028
0.860±0.030
0.874±0.059
9
region-level
slpd
0.864±0.042
0.939±0.022
0.869±0.039
0.886±0.057
k-nearest neighbors (knn) evaluation
10
mean
region-level
dino
0.770±0.031
0.840±0.038
0.837±0.014
0.724±0.055
11
region-level
dino+lintra
0.776±0.039
0.850±0.023
0.841±0.012
0.731±0.064
12
region-level
dino+linter
0.782±0.027
0.854±0.025
0.845±0.014
0.738±0.080
13
region-level
slpd
0.792±0.035
0.863±0.024
0.849±0.014
0.751±0.079
3.1
weakly-supervised classiﬁcation
we conduct experiments on two slide-level classiﬁcation tasks, nsclc subtyp-
ing and brca subtyping, and report the results in table 1.
this illustrates that learning rep-
resentations with broader image contexts is more suitable for wsi analysis.
compared with the strong baseline, i.e., the two-stage pre-training method pro-
posed by hipt (#6), slpd achieves performance increases of 1.3% and 3.2%
auc on nsclc and brca (#9).
nontrivial performance improvements are
also observed under knn evaluation (#10 vs.#13): +2.3% and +3.1% auc
on nsclc and brca.
the superior performance of slpd demonstrates that
learning representations with slide-level semantic structure appropriately can
signiﬁcantly narrow the gap between pre-training and downstream slide-level
3 the feature extraction of the patch-level is impracticable for the vit-based model
due to its quadratic complexity in memory usage.
moreover, intra-slide and inter-slide distillation show consistent perfor-
mance over the baseline, corroborating the eﬀectiveness of these critical compo-
nents of slpd.
3.2
ablation study
diﬀerent clustering methods.
the inferior performance of the
global clustering is due to the visual bias underlying the whole dataset.
the proposed inter-slide distillation is
semantic structure-aware at the slide-level, since we build the correspondence
between the region embedding and the matched prototype (#4 in table 2).
as can be seen, the region-level correspondences
lead to inferior performances, even worse than the baseline (#5 in table 1),
because the learning process is not guided by the slide-level information.
as shown in table 2(#5∼7), the performance of
slpd is relatively robust to the number of prototypes on nsclc, but is some-
what aﬀected by it on brca.
as demonstrated in table 2(#5∼7), the per-
formance of slpd is robust to the number of slide neighbors.
for more results, please refer to the supplementary.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_24.pdf:
the adoption of multi-instance learning (mil) for classi-
fying whole-slide images (wsis) has increased in recent years.
our work aims to unleash the
full potential of pyramidal structured wsi; to do so, we propose a graph-
based multi-scale mil approach, termed das-mil, that exploits mes-
sage passing to let information ﬂows across multiple scales.
by means of
a knowledge distillation schema, the alignment between the latent space
representation at diﬀerent resolutions is encouraged while preserving the
diversity in the informative content.
the source code is
available at https://github.com/aimagelab/mil4wsi.
keywords: whole-slide images · multi-instance learning ·
knowledge distillation
1
introduction
modern microscopes allow the digitalization of conventional glass slides into
gigapixel whole-slide images (wsis)
[18], facilitating their preservation and
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43907-0 24.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. (eds.): miccai 2023, lncs 14220, pp.
furthermore, a knowledge distillation mechanism encourages the
agreement between the predictions delivered by diﬀerent scales.
retrieval, but also introducing multiple challenges.
on the other hand,
feeding modern neural networks with the entire gigapixel image is not a feasible
approach, forcing to crop data into small patches and use them for training.
this
process is usually performed considering a single resolution/scale among those
provided by the wsi image.
mil approaches consider the image slide as a bag composed of many
patches, called instances; afterwards, to provide a classiﬁcation score for the
entire bag, they weigh the instances through attention mechanisms and aggregate
them into a single representation.
[15],
which have been proven to be more eﬀective than single-resolution [4,13,15,19].
however, to the best of our knowledge, none of the existing proposals leverage the
full potential of the wsi pyramidal structure.
a proﬁcient learning app-
roach should instead consider the heterogeneity between global structures and
local cellular regions, thus allowing the information to ﬂow eﬀectively across the
image scales.
through knowledge distillation, we encour-
250
g. bontempo et al.
age agreement across the predictions delivered at diﬀerent resolutions, while indi-
vidual scale features are learned in isolation to preserve the diversity in terms
of information content.
the authors of [7] leverage dino [5] as feature extrac-
tor, highlighting its eﬀectiveness for medical image analysis.
typically, a tailored learning objective encourages the stu-
dent to mimic the behaviour of its teacher.
recently, self-supervised representa-
tion learning approaches have also employed such a schema: as an example, [5,9]
exploit kd to obtain an agreement between networks fed with diﬀerent views
of the same image.
while existing works [19,20,25] take into account inter-
scales interactions by mostly leveraging trivial operations (such as concatenation
of related feature representations), we instead provide a novel technique that
builds upon: i) a gnn module based on message passing, which propagates
patches’ representation according to the natural structure of multi-resolutions
wsi; ii) a regulation term based on (self) knowledge distillation, which pins the
most eﬀective resolution to further guide the training of the other one(s).
we hence devise an initial
stage with multiple self-supervised feature extractors f(·; θ1), . .
to
perform message passing, we adopt graph attention layers (gat)
however, as these learned
metrics are inferred from diﬀerent wsi zooms, a disagreement may emerge:
indeed, we have observed (see table 4) that the higher resolutions generally yield
better classiﬁcation performance.
further than improving
the results of the lowest scale only, we expect its beneﬁts to propagate also to
the shared message-passing module, and so to the higher resolution.
it encourages the two
resolutions to assign criticality scores in a consistent manner: intuitively, if a low-
resolution patch has been considered critical, then the average score attributed
to its children patches should be likewise high.
we encourage such a constraint
by minimizing the euclidean distance between the low-resolution criticality grid
map z1 and its subsampled counterpart computed by the high-resolution branch:
lcrit = ∥z1 − graphpooling(z2)∥2
2.
(2)
in the equation above, graphpooling identiﬁes a pooling layer applied over the
higher scale: to do so, it considers the relation “part of” between scales and then
averages the child nodes, hence allowing the comparison at the instance level.
das-mil: distilling across scales for mil classiﬁcation of wsis
253
4
experiments
wsis pre-processing.
we remove background patches through an approach
similar to the one presented in the clam framework [20]: after an initial seg-
mentation process based on otsu [22] and connected component analysis [2],
non-overlapped patches within the foreground regions are considered.
optimization.
the dino feature extractor has been trained with two rtx5000
gpus: diﬀerently, all subsequent experiments have been performed with a single
rtx2080 gpu using pytorch-geometric
to asses the performance of our
approach, we adhere to the protocol of [19,28] and use the accuracy and auc
metrics.
as can be observed: i) the joint
exploitation of multiple resolutions is generally more eﬃcient; ii) our das-mil
yields robust and compelling results, especially on camelyon16, where it provides
0.945 of accuracy and 0.973 auc (i.e., an improvement of +3.3% accuracy and
+1.9% auc with respect to the sota).
to assess its merits, we con-
ducted several experiments varying the values of the corresponding balancing
coeﬃcients (see table 2).
= 0, i.e., no distillation is performed) negatively aﬀects the performance.
such a statement holds not only for the lower resolution (as one could expect),
but also for the higher one, thus corroborating the claims we made in sect.
0.816
20×
20×
0.891
0.931
5×, 20×
5×, 20×
0.891
0.938
5×, 20×
5×, [5× ∥ 20×]
0.898
0.941
10×, 20×
10×, 20×
0.945
0.973
10×, 20×
10×, [10× ∥ 20×] 0.922
0.953
we have also performed an
assessment on the temperature
τ, which controls the smooth-
ing factor applied to teacher’s
predictions (table 3).
for single-
scale experiments, the model is fed only with patches extracted at a single ref-
erence scale.
we ascribe it to the specimen-
level pixel size relevant for cancer diagnosis task; diﬀerent datasets/tasks may
beneﬁt from diﬀerent scale combinations.
in doing so, we
ﬁx the input resolutions to 5× and 20×. we draw the following conclusions: i)
when our das-mil feature propagation layer is used, the selection of the optimal
feature extractor (i.e., simclr vs dino) has less impact on performance, as the
message-passing can compensate for possible lacks in the initial representation;
ii) das-mil appears a better features propagator w.r.t. h2-mil.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_39.pdf:
medical education is essential for providing the best patient
care in medicine, but creating educational materials using real-world
data poses many challenges.
for example, the diagnosis and treatment
of a disease can be aﬀected by small but signiﬁcant diﬀerences in med-
ical images; however, collecting images to highlight such diﬀerences is
often costly.
therefore, medical image editing, which allows users to
create their intended disease characteristics, can be useful for educa-
tion.
however, existing image-editing methods typically require manu-
ally annotated labels, which are labor-intensive and often challenging to
represent ﬁne-grained anatomical elements precisely.
herein, we present
a novel algorithm for editing anatomical elements using segmentation
labels acquired through self-supervised learning.
our self-supervised seg-
mentation achieves pixel-wise clustering under the constraint of invari-
ance to photometric and geometric transformations, which are assumed
not to change the clinical interpretation of anatomical elements.
the user
then edits the segmentation map to produce a medical image with the
intended detailed ﬁndings.
evaluation by ﬁve expert physicians demon-
strated that the edited images appeared natural as medical images and
that the disease characteristics were accurately reproduced.
keywords: image editing · self-supervised segmentation · education
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43895-0_38.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. (eds.): miccai 2023, lncs 14221, pp.
1. editing of anatomical elements.
(a) users can edit the segmentation map
obtained from an input image to express intended ﬁne-grained disease characteristics.
(c) a synthetic rectal
tumor (c-1) and the contrasting tumor with t2 hyperintensity of extracellular mucin
suspicious for mucinous adenocarcinoma (c-2).
for example, although small but signif-
icant disease characteristics (e.g., depth of cancer invasion) can sometimes alter
diagnosis and treatment, collecting pairs with and without these characteristics
is cumbersome.
another major challenge is longitudinal tracking of pathologi-
cal progression over time (e.g., from the early stage of cancer to the advanced
stage), which is diﬃcult to understand because medical images are often snap-
shots.
privacy is also a concern since images of educational materials are widely
distributed.
therefore, medical image editing that allows users to generate their
intended disease characteristics is useful for precise medical education
[3].
image editing can synthesize low- or high-level image contents
our goal
is to develop high-precision medical image editing according to the ﬁne-grained
characteristics of individual diseases, rather than at the level of disease cate-
gories.
these ﬁne-grained characteristics consist of low- to mid-level image
medical image editing
405
features to distinguish the substructures of organs and diseases, which we call
anatomical elements.
several types of image editing techniques for medical imaging have been
introduced, mainly using generative adversarial networks [5] and, more recently,
diﬀusion models [2].
nevertheless, editing speciﬁc anatomical elements remains
a challenge [1,11].
latent space manipulation generates images by controlling
latent feature axes [4,14], but the editable attributes are often global rather than
ﬁne-grained.
conditional generation can precisely edit image content by using
class or segmentation labels.
however, it requires manually provided labels
image interpolation
[17] requires actual images with targeted
content, which limits its applicability.
here, we propose a novel framework for image editing called u3-net that
allows the generation of anatomical elements with precise conditions.
the core
technique is self-supervised segmentation, which aims to achieve pixel-wise clus-
tering without manually annotated labels [6,7].
1a, u3-net
converts an input image into a segmentation map corresponding to the anatomi-
cal elements.
once the user has completed editing, u3-net synthesizes an image
in which the targeted anatomical element has been modiﬁed.
as a result, our
synthesized medical images can highlight hypothetical pathological changes and
signiﬁcant clinical diﬀerences in a single image.
1b shows
that whether or not rectal cancer invades the muscularis propria (i.e., b-2 vs.
b-3) aﬀects cancer staging (i.e., t1 vs. t2) as well as treatment strategy (i.e.,
endoscopic resection vs. surgery).
these synthetic images can help trainees intu-
itively comprehend clinically signiﬁcant ﬁndings and alleviate privacy concerns.
five expert physicians evaluated the edited images from a clinical perspective
using two datasets: a pelvic mri dataset and chest ct dataset.
contributions: our contributions are as follows:
– we propose a novel image-editing algorithm, u3-net, to synthesize images
for medical education via self-supervised segmentation.
– u3-net can faithfully synthesize intended anatomical elements according to
the editing operation on the segmentation labels.
– evaluation by ﬁve expert physicians showed that the edited images were
natural as medical images with the intended features.
the encoder achieves self-supervised segmentation with a fea-
ture extraction (fe) module and a pixel-wise clustering (cl) module.
we apply two random transformations to
the input image to produce images in diﬀerent views, i1 and i2.
the encoder converts
the transformed images into quantized embedding as well as segmentation maps con-
sisting of cluster indices, s1 and s2.
pixel-wise clustering, which should be consistent
between views, is performed for the self-supervised segmentation.
the decoder gen-
erates reconstructed images, r1 and r2, from the quantized embedding maps.
the
discriminator adversarially enhances the natural appearance by judging whether the
images are real or fake on a pixel-by-pixel basis.
geometric transformations [6], with the assumption that these transformations
should not change the clinical interpretation of the anatomical elements.
given
a pair of diﬀerently transformed images, the fe module produces embedding
maps corresponding to the input images.
the cl module then performs k-
means clustering on the embedding maps to produce two interchangeable out-
puts: segmentation maps and corresponding quantized embedding maps.
the decoder then
estimates the corresponding images from the quantized embedding maps, while
the discriminator forces the decoder to produce more realistic images.
2.1
first training stage for self-supervised segmentation
the training process for u3-net is two-stage.
first, we train the encoder and
decoder (excluding the discriminator) to conduct k-class self-supervised seg-
mentation.
to achieve pixel-wise clustering that is consistent between two trans-
formed views of the input images, we introduce four constraints:
random image transformation: we consider a sequence of image transfor-
mations
, tn] speciﬁed by the type (e.g., image rotation) and magnitude
(e.g., degree of rotation) of each transformation: t = tn ◦
two
random transformation sequences are applied to an input image i ∈ rc×h×w
to produce two transformed images, t1(i) = i1 and t2(i) = i2.
medical image editing
407
fig.
cluster assignment and update: in the cl module, k-means clustering
in the ﬁrst iteration initializes k mean vectors µk ∈ rd.
the cluster indices form the segmentation maps
s =
the mean vectors µk are updated by
using the exponential moving average [9].
cross-view consistency: the segmentation maps from the diﬀerent views,
s1 and s2, should overlap after re-transforming to align the coordinates.
using the re-transformed seg-
mentation maps, we impose a third term, cross-view consistency loss, which
forces the embedding vectors of one view to match the mean vector of the other
(see fig. 3), as deﬁned: lcross = 
i∈h×w ∥µyi2 −ei1∥2+
i∈h×w ∥µyi1 −ei2∥2.
reconstruction loss: without user editing, the decoder reconstructs the
input images from quantized embedding maps h(eq)
we thus
employ reconstruction loss, which minimizes the mean squared error between
the reconstructed and input images.
learning objective: the weighted sum of the loss functions is set to be
minimized: ltotal = wclusterlcluster + wdistldist + wcrosslcross + wreconlrecon.
2.2
second training stage for faithful image synthesis
in the second stage, we train the decoder and discriminator (excluding the
encoder) to produce naturally appearing images from the quantized embedding
maps.
the decoder, initially optimized in the ﬁrst training stage, undergoes fur-
ther training to enhance its image generation capabilities.
lapp = wmselmse + wﬄlﬄ + wlpipsllpips + wintlint, where intermediate
loss lint refers to the l2 distance of the intermediate features of the discriminator
between the reconstructed and input images.
learning objective: we impose generator loss lgen for the decoder to pro-
duce more faithful images by deceiving the discriminator, and discriminator loss
ldis for the discriminator to judge the real or fake of the images as the per-
pixel feedback
we also add cutmix augmentation lcutmix and consistency
regularization lcons to the latter [16].
in this stage, the decoder and discrimi-
nator are trained by alternately minimizing the following competing objectives:
ldec = lapp + wgenlgen and ldis = wdisldis + wcutmixlcutmix + wconslcons.
2.3
inference stage for medical image editing
after training, the encoder can output a segmentation map from a testing image.
1a, when a user edits the segmentation map s → s′ by
changing the cluster indices yi → y′
i, the quantized embedding map is sub-
sequently updated eq → e′
q by reassigning the mean vectors according to
the edited indices µyi → µy′
i. finally, the decoder converts the quantized
embedding map into a synthetic image with the intended disease characteris-
tics h(e′
q) = r ∈ rc×h×w .
medical image editing
409
3
experiments and results
implementation and datasets: all neural networks were implemented in
python 3.8 using the pytorch library 1.10.0
the encoder, decoder, and discriminator were imple-
mented based on u-net
[13] (see supplementary information for details).
the pelvic mri dataset with rectal cancer contained 289 image series for train-
ing and 100 image series for testing.
for each image series, the min-max nor-
malization converted the pixel values to [−1, 1].
the chest ct dataset with lung
cancer contained 500 image series for training and 100 image series for testing.
every image series comprises
two-dimensional (2d) consecutive slices, and we applied our algorithm on a per
2d slice basis.
self-supervised medical image segmentation: we began by optimiz-
ing the hyperparameters to achieve self-supervised segmentation.
because anatomical elements,
including the substructures of organs and diseases, are too detailed for human
annotators to segment, it was diﬃcult to create ground-truth labels.
by comparing diﬀerent settings on the
pelvic mri training dataset (see supplementary information), the number
of segmentation classes of 10, the combination of t1, t2, and t3 with moderate
magnitude, the weakly imposed reconstruction loss, and a certain value of the
margin parameter were considered suitable for self-supervised segmentation.
in
particular, we found that reconstruction loss is essential for obtaining segmen-
tation maps corresponding to anatomical elements, although such a loss term
was not included in previous studies
the resultant segmentation maps are shown
in fig.
the anatomical substructures, including the histological structure
of the colorectal wall and subregions within the lung, corresponded well with
the segmentation maps in both the pelvic mri and chest ct testing datasets.
because our self-supervised segmentation extracts low- to mid-level image con-
tent, a semantic object (e.g., rectum or lung cancer) typically consists of multiple
segmentation classes shared with other objects (see the magniﬁed images in
fig.
these anatomical elements may be too detailed for humans to
annotate, demonstrating the necessity of self-supervised segmentation for high-
precision medical-image editing.
evaluation of the synthesized images: we measured the quality of image
reconstruction using mean square error (mse), structural similarity (ssim), and
peak signal-to-noise ratio (psnr).
4. results of the image segmentation and editing.
the segmentation maps
were well aligned with the anatomical elements in both (a) the pelvic mri and (b)
the chest ct testing datasets.
(c) a synthetic image generated by editing the testing
image with the caption, “axial t2-weighted mr image shows a tumor approximately
4 cm in size on the dorsal wall of the rectum.
(d) a synthetic
image with the caption, “axial ct image showing a pulmonary nodule with a length of
2–3 cm and a cavity on the dorsal side of the right upper lobe of the lung.”
ssim, and psnr were 1.41×10−2 ±1.04×10−2, 7.40×10−1 ±0.57×10−1, and
22.5±2.7 in the pelvic mri testing dataset and 5.03×10−4 ±3.03×10−4, 9.08×
10−1 ±0.34×10−1, and 38.6±1.7 in the chest ct testing dataset.
subsequently,
segmentation maps from the testing images were edited to generate images with
the intended characteristics (see fig.
first, we tested whether the evaluators could identify
real or synthesized images from 20 images, which include ten real images and ten
synthesized images.
the accuracies (i.e., the ratio of images correctly identiﬁed
as real or synthetic) were 0.69 ± 0.11 and 0.65 ± 0.11, for the pelvic mri and
chest ct testing datasets, respectively.
note that when the synthetic images
cannot be distinguished at all, the accuracy should be 0.5.
second, we presented
medical image editing
411
image captions explaining the radiological features, which also represented the
editing intention for the synthetic images.
we asked the evaluators to rate each
presented image from a to c. a: the image is natural as a medical image,
and the caption is consistent with the image.
b: the image is natural as a
medical image, but the caption is not consistent with the image.
c: the image
is not natural as a medical image.
this test was conducted after informing
the evaluators of the assumption that all 20 images could be synthetic, without
indicating which image was real or synthetic.
as a result, the ratio of synthetic
images (vs. that of real images) categorized as a, b, and c were 0.80 ± 0.15
(vs. 0.78 ± 0.20), 0.02 ± 0.04 (vs. 0.08 ± 0.07), and 0.18 ± 0.11 (vs. 0.14 ±
0.13) for the pelvic mri testing dataset, and 0.74 ± 0.28 (vs. 0.76 ± 0.30), 0.08
± 0.09 (vs. 0.12 ± 0.15), and 0.18 ± 0.21 (vs. 0.12 ± 0.14) for the chest ct
testing dataset.
there were no signiﬁcant diﬀerences between real and synthetic
images (t-test: p > 0.05).
consequently, the majority of the edited images were
natural-looking medical images with accurately reproduced disease features.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_62.pdf:
we perform unsupervised disentan-
glement of latent clinical signatures and leverage time-distance scaled
self-attention to jointly learn from clinical signatures expressions and
chest computed tomography (ct) scans.
evaluation on 227 subjects with challenging
spns revealed a signiﬁcant auc improvement over a longitudinal mul-
timodal baseline (0.824 vs 0.752 auc), as well as improvements over
a single cross-section multimodal scenario (0.809 auc) and a longitu-
dinal imaging-only scenario (0.741 auc).
this work demonstrates sig-
niﬁcant advantages with a novel approach for co-learning longitudinal
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43895-0 61.
[1,5,12,21], but approaches
that only consider imaging are fundamentally limited.
previous studies overcome these chal-
lenges by aggregating over visits and binning time series within a bidirectional
encoder representations from transformers (bert) architecture [2,14,20,25],
limiting their scope to data collected on similar time scales, such as icu mea-
surements, [11,29], or leveraging graph guided transformers to handle asyn-
chrony
[17]
or continuous variables as latent functions [16]. operating with the hypothesis
that distinct disease mechanisms manifest independently of one another in a
probabilistic manner, one can learn a transformation that disentangles latent
sources, or clinical signatures, from these longitudinal curves.
clinical signa-
tures learned in this way are expert-interpretable and have been well-validated
to reﬂect known pathophysiology across many diseases [15,18].
in this work, we jointly learn from longitudinal medical imag-
ing, demographics, billing codes, medications, and lab values to classify spns.
we leverage a transformer-based encoder to fuse features from
both longitudinal imaging and clinical signature expressions sampled at inter-
vals ranging from weeks to up to ﬁve years.
ica learns independent latent signatures, s, in an unsupervised manner on a
large non-imaging cohort.
input embeddings are the sum of 1) token embedding derived
from signatures or imaging, 2) a ﬁxed positional embedding indicating the token’s
position in the sequence, and 3) a learnable segment embedding indicating imaging
or non-imaging modality.
we set c = 2000 and estimated s in an unsupervised manner
using fastica [13].
given longitudinal curves for another cohort, for instance
dimage-ehr = {x′
k | k = 1, . . .
we represent our multi-
modal datasets dimage-ehr and dimage-ehr-spn = {(ek, gk) | k
, ek,t } sampled at the same dates
as images gk = {gk,1, . .
embeddings that incorporate positional and segment information
are computed for each item in the sequence (fig. 1, right).
token embeddings
for images are a convolutional embeddings of ﬁve concatenated
likewise, token embeddings for clinical sig-
nature expressions are linear transformations to the same dimension as imag-
ing token embeddings.
following [5,19,32], we intuit that if medical
data is sampled as a cross-sectional manifestation of a continuously progressing
longitudinal multimodal transformer integrating imaging
653
phenotype, we can use a temporal emphasis model (tem) emphasize the impor-
tance of recent observations over older ones.
formally, if subject k has a sequence of t images at rel-
ative acquisition days t1 . .
the trans-
former encoder computes query, key, and value matrices as linear transformations
of input embedding h = { ˆe ∥ ˆg} at attention head p
qp = hpw q
p
kp = hpw k
p
vp = hpw v
p
tem-scaled self-attention is computed via element-wise multiplication of the
query-key product and ˆr:
softmax

relu(qpk⊤
p + m) ◦ ˆr
√
d

vp
(3)
where m is the padding mask [31] and d is the dimension of the query and key
matrices.
our search for contextual embeddings for med-
ications and laboratory values did not yield any robust published models that
were compatible with our ehr’s nomenclature, so these were not included in
tdcode2vec.
we also performed experiments using only image sequences as
input, which we call tdimage.
finally, we implemented single cross-sectional
versions of tdimage, tdcode2vec, and tdsig, csimage, cscode2vec, and
cssig respectively, using the scan date closest to the lung malignancy diag-
nosis for cases or spn date for controls.
all baselines except csimage, which
654
t. z. li et al.
table 1. breakdown of modalities, size, and longitudinality of each dataset.
modalities
counts (cases/controls)
demo img code med lab subjects scans
ehr-pulmonary
–
288,428
–
nlst
–
–
–
533/801
1066/1602
image-ehr
257/665
641/1624
image-ehr-spn
58/169
76/405
demo: demographics, img: chest cts, code:
this work was supported by pytorch 1.13.1, cuda 11.7.
3
experimental setup
datasets.
next, ehr-
pulmonary was the unlabeled dataset used to learn clinical signatures in an
unsupervised manner.
additionally, image-
ehr was a labeled dataset with paired imaging and ehrs.
in the ehr-image cohort, malignant cases were labeled as those with a billing
code for lung malignancy and no cancer of any type prior.
finally, image-ehr-spn was a
subset of image-ehr with the inclusion criteria that subjects had a billing code
for an spn and no cancer of any type prior to the spn.
a description of the billing codes used to
deﬁne spn and lung cancer events are provided in supplementary 1.2.
longitudinal multimodal transformer integrating imaging
655
fig.
while this was the
only pretraining step for image-only models (csimage and tdimage), the mul-
timodal models underwent another stage of pretraining using the image-ehr
cohort with subjects from image-ehr-spn subtracted.
in this stage, we ran-
domly selected one scan and the corresponding clinical signature expressions for
each subject and each training epoch.
models were trained until the running
mean over 100 global steps of the validation loss increased by more than 0.2.
for evaluation, we performed ﬁve-fold cross-validation with image-ehr-spn,
using up to three of the most recent scans in the longitudinal models.
we report
the mean auc and 95% conﬁdence interval from 1000 bootstrapped samples,
sampling with replacement from the pooled predictions across all test folds.
we performed a reclassiﬁcation analysis of low,
medium, and high-risk tiers separated by thresholds of 0.05 and 0.65, which are
the cutoﬀs used to guide clinical management.
4
results
the signiﬁcant improvement with tdsig over cssig demonstrates the advan-
tage of longitudinally in the context of combining images and clinical signatures
(table 2).
there were large performance gaps between tdsig and tdcode2vec,
as well as between cssig and cscode2vec, demonstrating the advantage of
656
t. z. li et al.
table 2. performance on spn classiﬁcation using diﬀerent approaches and modalities.
[95% ci]
img demo code med lab nlst image-ehr
csimage
0.7392
[0.8075, 0.8120]
tdimage
0.7406 [0.7381, 0.7432]
–
–
–
–
–
tdcode2vec 0.7524
however, the
subject’s highest expressed clinical signature at the 3-month mark was a new pattern
of bacterial pneumonia (b), oﬀering to the model a benign explanation of an image
that it would otherwise be less correctly conﬁdent in.
cross-sectional embed-
ded billing codes did not signiﬁcantly improve performance over images alone
(cscode2vec vs csimage, p = 0.56), but adding clinical signatures did (cssig vs
csimage, p < 0.01; tdsig vs tdimage, p < 0.01) and the greatest improvement
in longitudinal data over single cross sections occurred when clinical signatures
were included.
for control subjects, tdsig correctly/incorrectly reclassiﬁed 40/18 from
tdcode2vec, 54/8 from tdimage, 12/18 from cssig, 104/7 from cscode2vec,
and 125/5 from csimage.
for case subjects, tdsig correctly/incorrectly reclas-
siﬁed 13/10 from tdcode2vec, 17/8 from tdimage, 12/2 from cssig, 23/16
from cscode2vec, and 29/16 from csimage (fig. 2).
full reclassiﬁcation matri-
ces are reported in supplementary 6.1.
we demonstrated large performance gains in spn classiﬁ-
cation compared with baselines, although calibration of our models is needed
to assess clinical utility.
in this setting, we found that adding clinical context increased the performance
gap between longitudinal data and single cross-sections.
we release our implemen-
tation at https://github.com/masilab/lmsignatures.
we were able to
overcome our small cohort size (image-ehr-spn) by leveraging unsupervised
learning on datasets without imaging (ehr-pulmonary), pretraining on public
datasets without ehrs (nlst), and pretraining on paired multimodal data with
noisy labels (image-ehr) within a ﬂexible transformer architecture.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_9.pdf:
the process of annotating histological gigapixel-sized whole
slide images (wsis) at the pixel level for the purpose of training a super-
vised segmentation model is time-consuming.
region-based active learn-
ing (al) involves training the model on a limited number of annotated
image regions instead of requesting annotations of the entire images.
these annotation regions are iteratively selected, with the goal of opti-
mizing model performance while minimizing the annotated area.
we evaluate our method
using the task of breast cancer metastases segmentation on the public
camelyon16 dataset and show that it consistently achieves higher
sampling eﬃciency than the standard method across various al step
sizes.
with only 2.6% of tissue area annotated, we achieve full annota-
tion performance and thereby substantially reduce the costs of annotat-
ing a wsi dataset.
keywords: active learning · region selection · whole slide images
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43895-0 9.
90–100, 2023.
https://doi.org/10.1007/978-3-031-43895-0_9
adaptive region selection for al in wsi semantic segmentation
91
1
introduction
semantic segmentation on histological whole slide images (wsis) allows precise
detection of tumor boundaries, thereby facilitating the assessment of metas-
tases
however, pixel-level anno-
tations of gigapixel-sized wsis (e.g. 100, 000 × 100, 000 pixels) for training a
segmentation model are diﬃcult to acquire.
[18]. identifying potentially informative image regions (i.e., providing
useful information for model training) allows requesting the minimum amount
of annotations for model optimization, and a decrease in annotated area reduces
both localization and delineation workloads.
the challenge is to eﬀectively select
annotation regions in order to achieve full annotation performance with the least
annotated area, resulting in high sampling eﬃciency.
we use region-based active learning (al) [13] to progressively identify anno-
tation regions, based on iteratively updated segmentation models.
first, the prediction of the most recently
trained segmentation model is converted to a priority map that reﬂects infor-
mativeness of each pixel.
[5] and highest disagreement between a set of models [19]).
the enhancement of priority maps, such as highlighting easy-to-label pixels [13],
edge pixels [6] or pixels with a low estimated segmentation quality [2], is also
a popular area of research.
all of these works selected square regions of a manually predeﬁned
size, disregarding the actual shape and size of informative areas.
this work focuses on region selection methods, a topic that has been largely
neglected in literature until now, but which we show to have a great impact on al
sampling eﬃciency (i.e., the annotated area required to reach the full annotation
performance).
we discover that the sampling eﬃciency of the aforementioned
standard method decreases as the al step size (i.e., the annotated area at each
al cycle, determined by the multiplication of the region size and the number of
selected regions per wsi) increases.
(image resolution: 0.25 µm
px )
region by ﬁrst identifying an informative area with connected component detec-
tion and then detecting its bounding box.
we test our method using a breast
cancer metastases segmentation task on the public camelyon16 dataset and
demonstrate that determining the selected regions individually provides greater
ﬂexibility and eﬃciency than selecting regions with a uniform predeﬁned shape
and size, given the variability in histological tissue structures.
the train-select-annotate process is
repeated until a certain performance of g or annotation budget is reached.
the informativeness measure is not the focus
adaptive region selection for al in wsi semantic segmentation
93
of this study, we therefore adopt the most commonly used one that quantiﬁes
model uncertainty (details in sect.
standard (non-square) we
implement a generalized version of the standard method that allows non-square
region selections by including multiple region candidates centered at each pixel
with various aspect ratios.
note that standard (non-square)
can be understood as an ablation study of the proposed method adaptive to
examine the eﬀect of variable region shape by maintaining constant region size.
2.3
wsi semantic segmentation framework
this section describes the breast cancer metastases segmentation task we use
for evaluating the al region selection methods.
data augmentation includes random ﬂip, random rotation, and stain augmenta-
tion [12]. inference.
3
experiments
3.1
dataset
we used the publicly available camelyon16 challenge dataset
the collection of the data was
approved by the responsible ethics committee (commissie mensgebonden onder-
zoek regio arnhem-nijmegen).
the test set contains 48 wsis with and 80 wsis without metastases 1.
3.2
implementation details
training schedules.
[15] initialized with imagenet
adaptive region selection for al in wsi semantic segmentation
95
fully-connected layers with sizes of 512 and 2, followed by a softmax activation
layer.
[1] to validate the segmentation framework.
to evaluate the wsi segmentation performance directly, we use mean intersec-
tion over union (miou).
for comparison, we follow [3] to use a threshold of 0.5
to generate the binary segmentation map and report miou (tumor), which is
the average miou over the 48 test wsis with metastases.
we evaluate the model
trained at each al cycle to track performance change across the al procedure.
3.3
results
full annotation performance.
to validate our segmentation framework, we
ﬁrst train on the fully-annotated data (average performance of ﬁve repetitions
reported).
with our framework, reducing s to 128 pixels
improves both metastases identiﬁcation and segmentation (froc score: 0.779,
miou (tumor): 0.758).
this makes an al experiment, which involves multiple rounds of
wsi inference, extremely costly.
therefore, we use s = 256 pixels for all fol-
lowing al experiments to compromise between performance and computation
costs.
because wsis without metastases do not require pixel-level annotation,
we exclude the 159 training and validation wsis without metastases from all
following al experiments.
this reduction leads to a slight decrease of full anno-
tation performance (miou (tumor) from 0.749 to 0.722).
results show average and min/max (shaded)
performance over three repetitions with distinct initial labeled sets.
experiments with large al step sizes
perform 10 al cycles (fig. 4 (e), (f), (h) and (i)); others perform 15 al cycles.
all experiments (except for random) use uncertainty sampling.
when using region selection method standard, the sampling eﬃciency advan-
tage of uncertainty sampling over random sampling decreases as al step size
increases.
a small al step size minimizes the annotated tissue area for a certain
high level of model performance, such as an miou (tumor) of 0.7, yet requires a
large number of al cycles to achieve full annotation performance (fig. 4 (a–d)),
adaptive region selection for al in wsi semantic segmentation
97
table 1.
annotated tissue area (%) required to achieve full annotation performance.
the symbol “/” indicates that the full annotation performance is not achieved in the
corresponding experimental setting in fig.
a large al step size allows for full anno-
tation performance to be achieved in a small number of al cycles, but at the
expense of rapidly expanding the annotated tissue area (fig.
table 1 shows that adaptive achieves full annotation performance with fewer
al cycles than standard for small al step sizes and less annotated tissue area
for large al step sizes.
this is advantageous because extensive
al step size tuning to balance the annotation and computation costs can be
avoided.
4(h) that the full annotation performance is not achieved
with adaptive within 15 al cycles; in fig.
s1 in the supplementary materials
we show that allowing for oversampling of previously selected regions can be a
solution to this problem.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_72.pdf:
deep learning based medical image recognition systems often
require a substantial amount of training data with expert annotations,
which can be expensive and time-consuming to obtain.
recently, syn-
thetic augmentation techniques have been proposed to mitigate the issue
by generating realistic images conditioned on class labels.
to further reduce the depen-
dency on annotated data, we propose a synthetic augmentation method
called histodiﬀusion, which can be pre-trained on large-scale unlabeled
datasets and later applied to a small-scale labeled dataset for augmented
training.
in particular, we train a latent diﬀusion model (ldm) on
diverse unlabeled datasets to learn common features and generate real-
istic images without conditional inputs.
then, we ﬁne-tune the model
with classiﬁer guidance in latent space on an unseen labeled dataset
so that the model can synthesize images of speciﬁc categories.
with histodiﬀusion augmentation, the
classiﬁcation accuracy of a backbone classiﬁer is remarkably improved
by 6.4% using a small set of the original labels.
1
introduction
the recent advancements in medical image recognition systems have greatly ben-
eﬁted from deep learning techniques [15,28].
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43895-0 71.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
https://doi.org/10.1007/978-3-031-43895-0_71
synthetic augmentation with large-scale unconditional pre-training
755
fig.
1. comparison between diﬀerent deep generative models for synthetic augmenta-
tion.
(a) cgan-based method which requires relatively large-scale annotated training
data; (b) diﬀusion model (dm) which cannot take conditional input; (c) our proposed
histodiﬀusion model that can be pretrained on large-scale unannotated data and later
applied to unseen small-scale annotated data for augmentation.
are one of the key components for training deep learning models to achieve sat-
isfactory results [3,17].
however, unlike natural images in computer vision, the
number of medical images with expert annotations is often limited by the high
labeling cost and privacy concerns.
to overcome this challenge, a natural choice
is to employ data augmentation to increase the number of training samples.
although conventional augmentation techniques [23] such as ﬂipping and crop-
ping can be directly applied to medical images, they merely improve the diver-
sity of datasets, thus leading to marginal performance gains
[10] to
synthesize visually appealing medical images that closely resemble those in the
original datasets
while existing works have proven eﬀective in improv-
ing the performance of downstream models to some extent, a suﬃcient amount
of labeled data is still required to adequately train models to generate decent-
quality images.
more recently, diﬀusion models have become popular for natural
image generation due to their impressive results and training stability [4,13,31].
a few studies have also demonstrated the potential of diﬀusion models for med-
ical image synthesis [19,24].
although annotated data is typically hard to acquire for medical images,
unannotated data is often more accessible.
to mitigate the issue existed in cur-
rent cgan-based synthetic augmentation methods [8,36–38], in this work, we
propose to leverage the diﬀusion model with unlabeled pre-training to reduce
the dependency on the amount of labeled data (see comparisons in fig.
we
propose a novel synthetic augmentation method, named histodiﬀusion, which
can be pre-trained on large-scale unannotated datasets and adapted to small-
scale annotated datasets for augmented training.
this large-scale pre-training enables the model to learn
756
j. ye et al.
common yet diverse image characteristics and generate realistic medical images.
synthetic images are then generated with classiﬁer guidance [4] in
the latent space.
following the prior work [36], we select generated images based
on the conﬁdence of target labels and feature similarity to real labeled images.
we evaluate our proposed method on a histopathology image dataset of colorec-
tal cancer (crc).
experiment results show that when presented with limited
annotations, the classiﬁer trained with our augmentation method outperforms
the ones trained with the prior cgan-based methods.
our experimental results
show that once histodiﬀusion is well pre-trained using large datasets, it can be
applied to any future incoming small dataset with minimal ﬁne-tuning and may
substantially improve the ﬂexibility and eﬃcacy of synthetic augmentation.
to enable conditional image synthesis, we also train a latent classiﬁer on
the same labeled dataset to guide the diﬀusion model in ldm.
once the classiﬁer
is trained, we apply the ﬁne-tuned ldm to generate a pool of candidate images
conditioned on the target class labels.
these candidate images are then passed
through the image selection module to ﬁlter out any low-quality results.
finally,
we can train downstream classiﬁcation models on the expanded training data,
which includes the selected images, and then use them to perform inference on
test data.
(2)
synthetic augmentation with large-scale unconditional pre-training
757
fig.
the architecture of our proposed histodiﬀusion, which consists of a pre-training
process (blue solid lines), a ﬁne-tuning process (blue dashed lines), and a selective aug-
mentation process (orange lines).
during pre-training, a latent autoencoder (lae) and
a diﬀusion model (dm) are trained on large-scale unlabeled datasets for unconditional
image synthesis.
histodiﬀusion is then ﬁne-tuned on a small-scale dataset for condi-
tional image synthesis under the guidance of a trained latent classiﬁer.
during selective
augmentation, given a target class label, the synthetic images generated by the ﬁne-
tuned model are selected and added to the training set based on their distances to the
class centroids in the feature space.
the denoising
model ϵθ is typically implemented using a time-conditioned u-net [27] with
residual blocks
our proposed histodiﬀusion is built on latent diﬀu-
sion models (ldm) [26], which requires fewer computational resources without
degradation in performance, compared to prior works [4,15,28].
[16] to encode images as lower-dimensional latent
representations and then learns a diﬀusion model (dm) for image synthesis by
758
j. ye et al.
modeling the latent space of the trained lae.
particularly, the encoder e of
the lae encodes the input image x ∈ rh×w ×3 into a latent representation
z = e(x) ∈ rh×w×c in a lower-dimensional latent space z. here h and w are
the height and width of image x, and h, w, and c are the height, width, and
channel of latent z, respectively.
the latent z is then passed into the decoder
d to reconstruct the image ˆx = d(z).
through this process, the compositional
features from the image space x can be extracted to form the latent space z,
and we then model the distribution of z by learning a dm.
for the dm in
ldm, both the forward and reverse sampling processes are performed in the
latent space z instead of the original image space x.
unconditional large-scale pre-training.
speciﬁcally, we gather unlabeled
images from m diﬀerent sources to construct a large-scale set of datasets
s = {s1, s2, . .
we then train an lae using the data from s with the
following self-reconstruction loss to learn a powerful latent space z that can
describe diverse features:
llae = lrec(ˆx, x) + λkldkl(q(z)||n(0, i)) ,
(4)
where lrec is the loss measuring the diﬀerence between the output reconstructed
image ˆx and the input ground truth image x.
here we implement lrec with a
combination of a pixel-wise l1 loss, a perceptual loss
in the dm
reverse sampling process to synthesize a novel latent ˜z0 ∈ rh×w×c and employ
the trained decoder d to generate a new image ˜x = d(˜z0), which should satisfy
the similar distribution as the data in s.
conditional small-scale fine-tuning.
using the lae and dm pretrained
on s, we can only generate the new image ˜x following the similar distribution in
s. to generalize our histodiﬀusion to the small-scale labeled dataset s′ collected
from a diﬀerent source (i.e., s′ ̸⊂ s), we further ﬁne-tune histodiﬀusion using
the labeled data from s′. let y be the label of image x in s′. to minimize the
training cost, we ﬁx both the trained encoder e and trained dm model ϵθ to
keep latent space z unchanged.
then we only ﬁne-tune the decoder d using
labeled data (x, y) from s′ with the following loss function:
ld = lrec(ˆx, x) + λcelce(ϕ(ˆx), y) ,
(5)
where lrec(ˆx, x) is the self-reconstruction loss between the output reconstructed
image ˆx = d(e(x)) and the input ground truth image x. to enhance the corre-
lation between the decoder output ˆx and label y, we also add an auxiliary image
synthetic augmentation with large-scale unconditional pre-training
759
classiﬁer ϕ trained with (x, y) on the top of d and impose the cross-entropy
classiﬁcation loss lce when ﬁne-tuning d. λce is the balancing parameter.
to enable conditional image gen-
eration with our histodiﬀusion, we further apply the classiﬁer-guided diﬀusion
sampling proposed in [4,29,30,33] using the labeled data (x, y) from small-scale
labeled dataset s′. we ﬁrst utilize the trained encoder e to encode the data x
from s′ as latent z0.
(8)
the ﬁnal image ˜x of class y can be generated by applying the ﬁne-tuned decoder
d′, i.e., ˜x = d′( ˜z0).
selective augmentation.
to further improve the eﬃcacy of synthetic aug-
mentation, we follow [36] to selectively add synthetic images to the original
labeled training data based on centroid feature distance.
the augmentation ratio
is deﬁned as the ratio between the selected synthetic images and the original
training images.
more results are demonstrated later in table 1.
3
experiments
datasets.
we employ three public datasets of histopathology images during
the large-scale pre-training procedure.
[2], containing 312,320 patches extracted from the hematoxylin & eosin
(h&e) stained human breast cancer tissue micro-array (tma) images [18].
the second dataset is pannuke [9],
a pan-cancer histology dataset for nuclei instance segmentation and classiﬁca-
tion.
3. comparison of real images from training subset, synthesized images generated
by stylegan2
as for ﬁne-tuning and evaluation, we employ the
nct-crc-he-100k dataset that contains 100,000 patches from h&e stained
histological images of human colorectal cancer (crc) and normal tissue.
the
patches have been divided into 9 classes: adipose (adi), background (back),
debris (deb), lymphocytes (lym), mucus (muc), smooth muscle (mus), nor-
synthetic augmentation with large-scale unconditional pre-training
761
mal colon mucosa (norm), cancer-associated stroma (str), colorectal adeno-
carcinoma epithelium (tum).
this subset has been carefully selected through an even sampling without
replacement from each tissue type present in the train set.
by ensuring that the ﬁne-tuning process is representative
of the entire dataset through even sampling from each tissue type, we can elim-
inate bias towards any particular tissue type.
the related data use declaration and acknowledgment
can be found in our supplementary materials.
evaluation metrics.
[12]
to assess the image quality of the synthetic samples.
we further compute the
accuracy, f1-score, sensitivity, and speciﬁcity of the downstream classiﬁers to
evaluate the performance gain from diﬀerent augmentation methods.
model implementation.
our implementation of histodiﬀusion basically
follows the ldm-4 [26] architecture, where the input is downsampled by a factor
of 4, resulting in a latent representation with dimensions of 64 × 64 × 3.
we use the same architecture for the auxiliary image classiﬁer ϕ. for down-
stream evaluation, we implement the classiﬁer using the vit-b/16 architecture
[5] in all experiments to ensure fair comparisons.
to
ensure a fair comparison, all images synthesized by stylegan2 and histodiﬀu-
sion model are further selected based on feature centroid distances [36].
more
implementation details of our proposed histodiﬀusion, stylegan2, and baseline
classiﬁer can also be found in our supplementary materials.
as shown in table 1, under the same synthetic augmenta-
tion setting, histodiﬀusion shows better fid scores and outperforms the state-
of-the-art cgan model stylegan2 in all classiﬁcation metrics.
a qualitative
comparison between synthetic images by histodiﬀusion and stylegan2 can be
762
j. ye et al.
table 1.
quantitative comparison results of synthetic image quality and augmented
classiﬁcation.
“random” refers to directly augmenting the training dataset with syn-
thesized images without any image selections while “selective” indicates applying selec-
tive module [36] to ﬁlter out low-quality images.
the number (x%) suggests that the
number of the synthesized images is x% of the original training set.
fid↓
accuracy↑ f1 score↑ sensitivity↑ speciﬁcity↑
baseline (5% real images) /
0.855
0.850
0.855
0.983
stylegan2
3, where histodiﬀusion consistently generates more realistic images
matching the given class conditions than sytlegan2, especially for classes adi
and back.
when augmenting the training dataset with diﬀerent numbers of images syn-
thesized from histodiﬀusion and stylegan2, one can observe that when increas-
ing the ratio of synthesized data to 100%, the fid score of stylegan2 increases
quickly and can become even worse than the one without using image selection
strategy.
in contrast, histodiﬀusion can keep synthesizing high-quality images
until the augmentation ratio reaches 300%.
regarding classiﬁcation performance
improvement of the baseline classiﬁer, the accuracy and f1 score of using his-
todiﬀusion augmentation are increased by up to 6.4% and 6.6%, respectively.
even when not using the image selection module to ﬁlter out the low-quality
results (i.e., +random 50%), our histodiﬀusion can still improve the accuracy
by 1.5%.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_57.pdf:
to tackle this problem, we pro-
pose reveal to revise (r2r), a framework entailing the entire explain-
able artiﬁcial intelligence (xai) life cycle, enabling practitioners to iter-
atively identify, mitigate, and (re-)evaluate spurious model behavior with
a minimal amount of human interaction.
secondly (2), the responsi-
ble artifacts are detected and spatially localized in the input data, which
is then leveraged to (3) revise the model behavior.
concretely, we apply
the methods of rrr, cdep and clarc for model correction, and (4)
(re-)evaluate the model’s performance and remaining sensitivity towards
the artifact.
using two medical benchmark datasets for melanoma detec-
tion and bone age estimation, we apply our r2r framework to vgg,
resnet and eﬃcientnet architectures and thereby reveal and correct
real dataset-intrinsic artifacts, as well as synthetic variants in a con-
trolled setting.
completing the xai life cycle, we demonstrate multiple
r2r iterations to mitigate diﬀerent biases.
keywords: xai life cycle · bias identiﬁcation · model correction
1
introduction
deep neural networks (dnns) have successfully been applied in research
and industry for a multitude of complex tasks.
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43895-0 56.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
https://doi.org/10.1007/978-3-031-43895-0_56
reveal to revise: an xai life cycle for iterative bias correction of dnns
597
fig.
at this point,
the artifact localization can be leveraged for (3) model correction, and (4) to evaluate
the model’s performance on a poisoned test set and measure its remaining attention
on the artifact.
applications for which dnns have even shown to be superior to medical experts,
such as with melanoma detection
however, the reasoning of these highly
complex and non-linear models is generally not transparent [23,24], and as such,
their decisions may be biased towards unintended or undesired features, poten-
tially caused by shortcut learning [2,9,14,27].
as such, local xai methods reveal
(input) features that are most relevant to a model, which, for image data, can
be presented as heatmaps.
while multiple approaches exist
for either revealing or revising model biases, only few combine both steps, to
be applicable as a framework.
such frameworks, however, either rely heavily on
human feedback [25,29], are limited to speciﬁc bias types
[13,25].
598
f. pahde et al.
to that end, we propose reveal to revise (r2r), an iterative xai life cycle
requiring low amounts of human interaction that consists of four phases, illus-
trated in fig.
the
generated annotations are then leveraged to (3) correct and (4) (re-)evaluate the
model, followed by a repetition of the entire life cycle if required.
for revealing
model bias, we propose two orthogonal xai approaches: while spectral rele-
vance analysis (spray)
the arti-
fact masks are further used for evaluation on a poisoned test set and to measure
the remaining attention on the bias.
we demonstrate the applicability and high
automation of r2r on two medical tasks, including melanoma detection and bone
age estimation, using the vgg-16, resnet-18 and eﬃcientnet-b0 dnn architec-
tures.
in our experiments, we correct model behavior w.r.t.
lastly, we showcase the r2r life
cycle through multiple iterations, unveiling and unlearning diﬀerent biases.
2
related work
among other methods, e.g., leveraging auxiliary information [15,18,19,21],
or training on de-biased representations [4,16], shortcut unlearning is often
approached with xai.
the former is based on presenting individual local explanations to a
human, who, if necessary, provides feedback used for model correction
in our r2r framework, we automate the annotation
by following [2] for data labeling through spray outlier clusters, or by collecting
the most representative samples of bias concepts according to crp.
3.1, thereby considerably easing the step from bias identiﬁcation
to correction.
reveal to revise: an xai life cycle for iterative bias correction of dnns
599
existing works for model correction measure the performance on the original
or clean test set, with corrected models often showing an improved generaliza-
tion [13,20].
3
reveal to revise framework
our reveal to revise (r2r) framework comprises the entire xai life cycle, includ-
ing methods for (1) the identiﬁcation of model bias, (2) artifact labeling and local-
ization, (3) the correction of detected misbehavior, and (4) the evaluation of the
improved model.
the
spray clusters then naturally allow us to label data containing the bias.
the artifact localization is given by a modiﬁed backward
pass on the biased model with lrp for an artifact sample x, where we initialize
the relevances rl(x) at layer l as
rl(x) = al(x) ◦ hl
(1)
600
f. pahde et al.
with activations al and element-wise multiplication operator ◦.
3.2
methods for model correction
in the following, we present the methods used for mitigating model biases.
clarc for latent space correction.
the framework consists of two methods, namely
augmentive clarc (a-clarc) and projective clarc (p-clarc).
dependent on input x. parameter γ(x) is chosen
such that the activation in direction of the cav is as high as the average value
over non-artifactual or artifactual samples for p-clarc or a-clarc, respectively.
rrr and cdep for correction through prior knowledge.

1
.
(4)
4
experiments
the experimental section is divided into the two parts of (1) identiﬁcation, miti-
gation and evaluation of spurious model behavior with various correction meth-
ods and (2) showcasing the whole r2r framework in an iterative fashion.
reveal to revise: an xai life cycle for iterative bias correction of dnns
601
fig.
shown are band-aid, ruler,
skin marker, and synthetic artifacts for the isic dataset, as well as “l”-marker and
synthetic artifacts for the bone age dataset.
4.1
experimental setup
we train vgg-16
[26], resnet-18 [11] and eﬃcientnet-b0 [28] models on the
isic 2019 dataset [7,8,30] for skin lesion classiﬁcation and pediatric bone age
dataset
[10] for bone age estimation based on hand radiographs.
see appendix a.1 for additional experiment details.
4.2
revealing and revising spurious model behavior
revealing bias: in the ﬁrst step of the r2r life cycle, we can reveal the use
of several artifacts by the examined models, including the well-known band-aid,
ruler and skin marker [6] and our synthetic clever hans for the isic dataset, as
shown in fig. 2 for vgg-16.
besides the synthetic clever hans for bone age classiﬁcation, we
encountered the use of “l” markings, resulting from physical lead markers placed
by radiologist to specify the anatomical side.
interestingly, the “l” markings are
larger for hands of younger children, as all hands are scaled to similar size [10],
oﬀering the model to learn a shortcut by estimating the bone age based on the
relative size of the “l” markings, instead of valid features.
while we revealed
the “l” marking bias using crp, we did not ﬁnd corresponding spray clusters,
underlining the importance of both approaches for model investigation.
we evaluate the eﬀectiveness of model corrections based on two metrics:
the attributed fraction of relevance to artifacts and prediction performance on
both the original and a poisoned test set (in terms of f1-score and accuracy).
note that artifacts might
overlap clinically informative features in poisoned samples, limiting the compa-
rability of poisoned and original test performance.
as shown in tab. 1 (isic
2019) and appendix a.2 (bone age), we are generally able to improve model
behavior with all methods.
the only exception is the synthetic artifact for vgg-
16, where only rrr mitigates the bias to a certain extent, indicating that
the artifact signal is too strong for the model.
interestingly,
despite successfully decreasing the models’ output sensitivity towards artifacts,
1 cdep is not applied to eﬃcientnets, as existing implementations are incompatible.
reveal to revise: an xai life cycle for iterative bias correction of dnns
603
fig.
overall, rrr yields the most consistent results, constantly reduc-
ing the artifact relevance while increasing the model performance on poisoned
test sets.
604
f. pahde et al.
5
conclusion
we present r2r, an xai life cycle to reveal and revise spurious model behavior
requiring minimal human interaction via high automation.
to reveal model bias,
r2r relies on crp and spray.
moreover, crp
is ideal for large datasets, as the concept space dimension remains constant.
by automatically localizing artifacts, we successfully perform model revision,
thereby reducing attention on the artifact and leading to improved performance
on corrupted data.
when applying r2r iteratively, we did not ﬁnd the emergence
of new biases, which, however, might happen if larger parts of the model are ﬁne-
tuned or retrained to correct strong biases.
future research directions include the
application to non-localizable artifacts, and addressing fairness issues in dnns.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_42.pdf:
interpretability is often an essential requirement in medical
imaging.
advanced deep learning methods are required to address this
need for explainability and high performance.
we propose
an innovative solution called proto-caps that leverages the beneﬁts of
capsule networks, prototype learning and the use of privileged informa-
tion.
evaluating the proposed solution on the lidc-idri dataset shows
that it combines increased interpretability with above state-of-the-art
prediction performance.
keywords: explainable ai · capsule network · prototype learning
1
introduction
deep learning-based systems show remarkable predictive performance in many
computer vision tasks, including medical image analysis, and are often compa-
rable to human performance.
a common misconception is that the additional
explanation comes with a decrease in performance.
our work proves this once again by providing a powerful and explainable
solution for medical image classiﬁcation.
besides using
the additional knowledge to improve performance, it can also help to increase
explainability, as has already been shown using the lidc-idri dataset [3].
prototype networks are another line of research implementing the idea
that the representations of images cluster around a prototypical representation
for each class [17].
the goal is to ﬁnd embedded prototypes (i.e. examples) that
best separate the images by their classes [5].
however, these networks can only tell which prototyp-
ical samples resemble the query image, not why.
it is up to the user to
guess which features of the image regions are relevant to the network and are
exempliﬁed by the prototypes.
our method addresses the limitations of privileged information-based and
prototype-based explanation by combining case-based visual reasoning through
exemplary representation of high-level attributes to achieve explainability and
high-performance.
the proposed method is an image classiﬁer that satisﬁes
explainable-by-design with two elements: first, decisive intermediate results of a
high-performance cnn are trained on human-deﬁned attributes which are being
predicted during application.
second, the model provides prototypical natural
images to validate the attribute prediction.
proto-caps: interpretable medical image classiﬁcation
437
the main contributions of our work are:
– a novel method that, for the ﬁrst time to our knowledge, combines privileged
information and prototype learning to provide increased explanatory power
for medical classiﬁcation tasks.
– a prototype network architecture based on a capsule network that leverages
the beneﬁts of both techniques.
– an explainable solution outperforming state-of-the-art explainable and non-
explainable methods on the lidc-idri dataset.
an attribute head is used to ensure that each
capsule represents a single attribute, a reconstruction head learns the original
segmentation, and the main target prediction head learns the ﬁnal classiﬁcation.
1.
the backbone of our approach is a capsule network consisting of three layers:
features of the input image of size 1×32×32 are extracted by a 2d convolutional
layer containing 256 kernels of size 9×9.
we decided not to use 3d convolutional
layers, as preliminary experiments showed only marginal diﬀerences (within std.
dev. of results), but required signiﬁcantly more computing time.
the ﬁnal dense capsule layer
consists of one capsule for each attribute and extracts high-level features, overall
producing eight 16-dimensional vectors.
[11], where the distribution of radiologist malignancy annotations is opti-
mized with the kullback-leibler divergence lmal to reﬂect the inter-observer
agreement and thus uncertainty.
the reconstruction branch to predict the
segmentation mask of the nodule consists of a simple decoder with three fully
connected layers with the output ﬁlters 512, 1024, and the size of the resulting
image 1 × 32 × 32.
the reconstruction loss lrecon implements the mean square
error between the output and the binary segmentation mask.
it has been shown
that incorporating reconstruction learning is beneﬁcial to performance [11].
438
l. gallée et al.
fig.
for the attribute head, we propose to use fully connected layers, instead of
determining the attribute manifestation by the length of the capsule encoding,
as was done previously [11].
during the training, a combined loss
function encourages a training sample to be close to a prototype of the correct
attribute class and away from prototypes dedicated to others, similar to existing
approaches [6].
proto-caps: interpretable medical image classiﬁcation
439
lclu = 1
a
a

a
min
pj∈pas
∥oa − pj∥2 .
the original image of the
training sample is stored and used for prototype visualization.
during inference,
the predicted attribute value is set to the ground truth attribute value of the
closest prototype, ignoring the learned dense layers in the attribute head at this
stage.
+ 0.125 · (lclu + 0.1 · lsep)
(4)
3
experiments
data.
each lung nodule with a mini-
mum size of 3 mm was segmented and annotated with a malignancy score rang-
ing from 1-highly unlikely to 5-highly suspicious by one to four expert raters.
experiment designs.
440
l. gallée et al.
the algorithm was implemented using the pytorch framework version 1.13 and
cuda version 11.6.
with a maximum of 1000 epochs, but stopping
early if there was no improvement in target accuracy within 100 epochs, the
experiments lasted an average of three hours on a geforce rtx 3090 graphics
card.
besides pure performance, the eﬀect of reduced availability of attribute anno-
tations was investigated.
this was done by using attribute information only for
a randomly selected fraction of the nodules during the training.
to investigate the eﬀect of prototypes on the network performance, an abla-
tion study was performed.
the respective original image for each attribute prototype is being saved dur-
ing the training process and used for visualization during inference.
table 1 shows the results of our experiments compared to other
state-of-the-art approaches, with results taken from original reports.
the experiments indicate that the performance
of the given approach is maintained up to a fraction of 10 %.
using no attribute
annotations at all, i.e. no privileged information, achieves a similar performance,
but results in a loss of explainability, as the high-level features extracted in the
capsules are not understandable to humans.
this result suggests that privileged
proto-caps: interpretable medical image classiﬁcation
441
information here leads to an increase in interpretability for humans by providing
attribute predictions and prototypes without interfering with the model perfor-
mance.
fig.
the average diﬀerence in attribute accuracy compared to the proposed
methods is 1.7 % and 1.5 % better, respectively, and is more robust across exper-
iments.
the best result was obtained when the prototypes were learned but not
used, possibly indicating that the prototypes may have a regularising eﬀect dur-
ing training, but further experiments are needed to conﬁrm this due to the close
results.
to give an indication of the decoder performance, proto-capsw/o use
achieved a dice score of 79.7 %.
mean μ and standard deviation σ calculated from 5-fold exper-
iments.
mean μ and standard deviation σ calculated
from 5-fold experiments.
attribute prediction accuracy in %
malig-
nancy
sub
is
cal
sph mar lob spic tex
100 % attribute labels μ
89.1 99.8 95.4 96.0 88.3 87.9 89.1 93.3
93.0
σ
5.2
0.2
1.3
2.2
3.1
0.8
1.3
1.0
1.5
10 % attribute labels μ
92.6 99.8 95.7 94.9 90.3 88.8 86.9 92.3
92.4
σ
0.9
0.2
0.9
4.1
1.6
1.6
2.4
1.4
0.8
1 % attribute labels μ
91.0 99.8 92.8 95.5 79.9 85.7 85.6 91.2
90.2
σ
4.5
0.2
1.4
2.3 13.1
4.4
6.8
1.7
1.1
0 % attribute labels μ
–
–
–
–
–
–
–
–
92.4
σ
–
–
–
–
–
–
–
–
1.0
5
discussion and conclusion
we propose a new method, named proto-caps, which combines the advantages
of privileged information, and prototype learning for an explainable network,
achieving more than 6 % better accuracy than the state-of-the-art explainable
method.
proto-caps: interpretable medical image classiﬁcation
443
the experiments demonstrate that it outperforms state-of-the-art methods that
provide less explainability.
while we did see a reduction in performance with too few labels, our
results suggest that this is mainly due to inhomogeneous coverage of individ-
ual attribute values.
in this respect, it would be interesting to ﬁnd out how a
speciﬁc selection of the annotated samples, e.g. with extremes, aﬀects the accu-
racies, especially since our results show that the overall performance is robust
even when the attributes are not explicitly trained, i.e. without additional priv-
ileged information.
in conclusion, we believe that the approach of leveraging privileged informa-
tion with comprehensible architectures and prototype learning is promising for
various high-risk application domains and oﬀers many opportunities for further
research.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_1.pdf:
experiments on ﬁne-grained classiﬁ-
cation of pathology images show that openal can signiﬁcantly improve
the query quality of target class samples and achieve higher performance
than current state-of-the-art al methods.
keywords: active learning · openset · pathology image classiﬁcation
1
introduction
deep learning techniques have achieved unprecedented success in the ﬁeld of
medical image classiﬁcation, but this is largely due to large amount of annotated
data [5,18,20].
however, obtaining large amounts of high-quality annotated data
is usually expensive and time-consuming, especially in the ﬁeld of pathology
image processing [5,12–14,18].
therefore, a very important issue is how to obtain
the highest model performance with a limited annotation budget.
1. description of the open-set al scenario for pathology image classiﬁcation.
the unlabeled sample pool contains k target categories (red-boxed images) and l
non-target categories (blue-boxed images).
(color ﬁgure online)
active learning (al) is an eﬀective approach to address this issue from a
data selection perspective, which selects the most informative samples from an
unlabeled sample pool for experts to label and improves the performance of the
trained model with reduced labeling cost [1,2,9,10,16,17,19].
[11]. figure 1 shows an al
scenario for pathology image classiﬁcation in an open world, which is very com-
mon in clinical practice.
in this scenario, the whole slide images (wsis) are cut
into many small patches that compose the unlabeled sample pool, where each
patch may belong to tumor, lymph, normal tissue, fat, stroma, debris, back-
ground, and many other categories.
therefore, for real-world open-set pathology image classiﬁcation scenarios,
an al method that can accurately query the most informative samples from the
target classes is urgently needed.
[11] proposed the ﬁrst al algorithm for open-set anno-
tation in the ﬁeld of natural images.
although promising performance is achieved,
their detection of target class samples is based on the activation layer values
of the detection network which has limited accuracy and high uncertainty with
small initial training samples.
in this paper, we propose a novel al framework under an open-set scenario,
and denote it as openal, which cannot only query as many target class samples
as possible but also query the most informative samples from the target classes.
openal adopts an iterative query paradigm and uses a two-stage sample selec-
tion strategy in each query.
in the ﬁrst stage, we do not rely on a detection
network to select target class samples and instead, we propose a feature-based
target sample selection strategy.
speciﬁcally, we ﬁrst train a feature extractor
using all samples in a self-supervised learning manner, and map all samples to
the feature space.
in the second
stage, we select the most informative samples from the candidate set by utilizing
a model-based informative sample selection strategy.
in this stage, we measure
the uncertainty of all unlabeled samples in the candidate set using the classiﬁer
trained with the target class samples labeled in previous iterations, and select
the samples with the highest model uncertainty as the ﬁnal selected samples in
this round of query.
after the second stage, the queried samples are sent for
annotation, which includes distinguishing target and non-target class samples
and giving a ﬁne-grained label to every target class sample.
we conducted two experiments with diﬀerent matching ratios (ratio of the
number of target class samples to the total number of samples) on a public 9-class
colorectal cancer pathology image dataset.
the experimental results demonstrate
that openal can signiﬁcantly improve the query quality of target class samples
and obtain higher performance with equivalent labeling cost compared with the
current state-of-the-art al methods.
to the best of our knowledge, this is the
ﬁrst open-set al work in the ﬁeld of pathology image analysis.
2
method
we consider the al task for pathology image classiﬁcation in an open-set sce-
nario.
iterative queries are
performed to query a ﬁxed number of samples in each iteration, and the objec-
tive is to select as many target class samples as possible from pu in each query,
while selecting as many informative samples as possible in the target class sam-
ples.
openal
performs a total of n iterative queries, and each query is divided into two stages.
in stage 1, openal uses a feature-based target sample selection (ftss) strategy
to query the target class samples from the unlabeled sample pool to form a
candidate set.
in stage 2, openal
adopts a model-based informative sample selection (miss) strategy.
here, we adopt dino [3,4] as the self-supervised network because
of its outstanding performance.
2.3
model-based informative sample selection
to select the most informative samples from the candidate set, we utilize the
model-based informative sample selection strategy in stage 2.
3
experiments
3.1
dataset, settings, metrics and competitors
to validate the eﬀectiveness of openal, we conducted two experiments with
diﬀerent matching ratios (the ratio of the number of samples in the target class
to the total number of samples) on a 9-class public colorectal cancer pathology
image classiﬁcation dataset (nct-crc-he-100k)
the dataset contains a
total of 100,000 patches of pathology images with ﬁne-grained labeling, with
nine categories including adipose (adi 10%), background (back 11%), debris
(deb 11%), lymphocytes (lym 12%), mucus (muc 9%), smooth muscle (mus
14%), normal colon mucosa (norm 9%), cancer-associated stroma (str 10%),
and colorectal adenocarcinoma epithelium (tum, 14%).
in the
two experiments, we set the matching ratio to 33% (3 target classes, 6 non-target
classes), and 42% (3 target classes, 4 non-target classes), respectively.
metrics.
following [11], we use three metrics, precision, recall and accuracy to
compare the performance of each al method.
we use precision and recall to
measure the performance of diﬀerent methods in target class sample selection.
we measure the ﬁnal performance of each al method using the accuracy
of the ﬁnal classiﬁer on the test set of target class samples.
after each query
round, we train a resnet18 model of 100 epochs, using sgd as the optimizer
with momentum of 0.9, weight decay of 5e-4, initial learning rate of 0.01, and
batchsize of 128.
for
each method, we ran four experiments and recorded the average results for four
randomly selected seeds.
3.2
performance comparison
figure 3 a and b show the precision, recall and model accuracy of all comparing
methods at 33% and 42% matching ratios, respectively.
the
inferior performance of the al methods based on the closed-set assumption
is due to the fact that they are unable to accurately identify more target class
samples, thus wasting a large amount of annotation budget.
although lfosa [11]
utilizes a dedicated network for target class sample detection, the performance of
the detection network is not stable when the number of training samples is small,
thus limiting its performance.
in contrast, our method uses a novel feature-based
target sample selection strategy and achieves the best performance.
3. a. selection and model performance results under a 33% matching ratio.
b.
selection and model performance results under a 42% matching ratio.
this severe sample
imbalance weakens the performance of lfosa compared to random selection
initially.
it can be seen that the distance modeling of both the target class samples
and the non-target class samples is essential in the ftss strategy, and missing
either one results in a decrease in performance.
although the miss strategy does
not signiﬁcantly facilitate the selection of target class samples, it can eﬀectively
help select the most informative samples among the samples in the candidate
set, thus further improving the model performance with a limited labeling bud-
get.
in contrast, when the samples are selected based on uncertainty alone, the
performance decreases signiﬁcantly due to the inability to accurately select the
target class samples.
the above experiments demonstrate the eﬀectiveness of
each component of openal.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_46.pdf:
the spatial-temporal
deformable attention module enables deep feature aggregation in each
stage of both encoder and decoder.
the exper-
iments on the public breast lesion ultrasound video dataset show that
our stnet obtains a state-of-the-art detection performance, while oper-
ating twice as fast inference speed.
https://doi.org/10.1007/978-3-031-43895-0_45
480
c. qin et al.
most existing breast lesion detection methods can be categorized into image-
based [10,11,16,17,19] and video-based [1,9] breast lesion detection approaches.
image-based breast lesion detection approaches perform detection in each frame
independently.
compared to image-based breast lesion detection approaches,
methods based on videos are capable of utilizing temporal information for
improved detection performance.
although the recent cva-net aggregates clip and video
level features, we distinguish two key issues that hamper its performance.
second, cva-
net only performs one-frame prediction based on multiple frame inputs, which
is very time-consuming.
to address the aforementioned issues, we propose a spatial-temporal
deformable attention based network, named stnet, for detecting the breast
lesions in ultrasound videos.
we conduct extensive experiments on a public breast lesion ultra-
sound video dataset, named bluvd-186
the experimental results validate
the eﬃcacy of our proposed stnet that has a superior detection performance.
afterwards, stda performs cross-attention
operation between these feature maps and the queries, where the key elements
are these output feature maps of st-encoder.
3
experiments
3.1
dataset and implementation details
dataset.
we conduct the experiments on the public bluvd-186 dataset [9],
comprising 186 videos including 112 malignant and 74 benign cases.
our approach achieves a superior perfor-
mance on three diﬀerent metrics.
method
type
backbone
ap
ap50 ap75
gfl [7]
image resnet-50 23.4
46.3
22.2
cascade rpn
[14]
image resnet-50 24.8
42.4
27.3
faster r-cnn
[12]
image resnet-50 25.2
49.2
22.3
vfnet
[20]
image resnet-50 28.0
47.1
31.0
retinanet
[8]
image resnet-50 29.5
50.4
32.4
dff [24]
video
resnet-50 25.8
48.5
25.1
fgfa
three commonly-used metrics are employed for perfor-
mance evaluation of breast lesion detection methods on the ultrasound videos,
namely average precision (ap), ap50, and ap75.
implementation details.
we train the model on a single
nvidia a100 gpu and set the batch size as 1.
3.2
state-of-the-art comparison
our proposed approach is compared with eleven state-of-the-art methods, com-
prising image-based and video-based methods.
we report the detection perfor-
mance of these state-of-the-art methods generated by cva-net [9].
speciﬁcally,
cva-net acquires the detection performance of these methods by utilizing their
publicly available codes or re-implementing them if no publicly available codes.
our stnet achieves improved detection performance, com-
pared to cva-net.
as a general trend, video-based methods tend to yield
higher average precision (ap), ap50, and ap75 scores compared to image-based
breast lesion detection methods.
speciﬁcally, our stnet
achieves a signiﬁcant improvement in the overall ap score from 36.1 to 40.0,
the ap50 score from 65.1 to 70.3, and the ap75 score from 38.5 to 43.3.
the
signiﬁcant improvement demonstrates the eﬃcacy of our approach for detecting
breast lesions in ultrasound videos.
further, although cva-net manages to identify the breast lesions
in the ﬁrst and ﬁfth frames, the classiﬁcation results are inaccurate (as high-
lighted by the blue rectangle in fig. 3).
3 accurately detects the breast lesions in all video frames and
achieves accurate classiﬁcation performance for each frame.
486
c. qin et al.
table 2. ablation study with diﬀerent design choices.
our proposed stnet achieves
a superior performance compared to the baseline and some diﬀerent designs.
we present the inference speed comparison
between our proposed stnet and cva-net on an nvidia rtx 3090 gpu
using the same environment.
we use fps (frames per second) as the performance
metric.
speciﬁcally, our proposed stnet achieves an averaged inference speed
of 21.84 fps, while cva-net achieves an averaged speed of 12.17 fps.
furthermore, our proposed stnet
improves the ap by 5.1 and 4.2 compared to “st-encoder + da-decoder” and
“da-encoder + st-decoder”, respectively, indicating that the integration of
stda in both the encoder and decoder is crucial for achieving superior detec-
tion performance.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_2.pdf:
medical image analysis using deep learning is often challenged
by limited labeled data and high annotation costs.
fine-tuning the entire
network in label-limited scenarios can lead to overﬁtting and suboptimal
performance.
however, previous work has overlooked the importance
of selective labeling in downstream tasks, which aims to select the most
valuable downstream samples for annotation to achieve the best perfor-
mance with minimum annotation cost.
to address this, we propose a frame-
work that combines selective labeling with prompt tuning (slpt) to boost
performance in limited labels.
we evaluate our method on
liver tumor segmentation and achieve state-of-the-art performance, out-
performing traditional ﬁne-tuning with only 6% of tunable parameters,
also achieving 94% of full-data performance by labeling only 5% of the data.
keywords: active learning · prompt tuning · segmentation
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43895-0 2.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
https://doi.org/10.1007/978-3-031-43895-0_2
slpt: selective labeling meets prompt tuning
15
1
introduction
deep learning has achieved promising performance in computer-aided diagnosis
[1,12,14,24], but it relies on large-scale labeled data to train, which is challenging
in medical imaging due to label scarcity and high annotation cost [3,25]. specif-
ically, expert annotations are required for medical data, which can be costly and
time-consuming, especially in tasks such as 3d image segmentation.
[5,18] is emerging from natural language processing (nlp), which introduces
additional tunable prompt parameters to the pre-trained model and updates only
prompt parameters using supervision signals obtained from a few downstream
training samples while keeping the entire pre-trained unchanged.
however, previous prompt tuning research [18,28], whether on language or
visual models, has focused solely on the model-centric approach.
vpt [13] explores prompt tuning with
a vision transformer, and spm [17] attempts to handle downstream segmen-
tation tasks through prompt tuning on cnns, which are also model-centric.
in al, given the initial labeled
data, the model actively selects a subset of valuable samples for labeling and
improves performance with minimum annotation eﬀort.
therefore, this paper proposes the ﬁrst framework for selective labeling and
prompt tuning (slpt), combining model-centric and data-centric methods to
improve performance in medical label-limited scenarios.
the results
show that slpt outperforms ﬁne-tuning with just 6% of tunable parameters and
achieves 94% of full-data performance by selecting only 5% of labeled data.
2
methodology
given a task-agnostic pre-trained model and unlabeled data for an initial med-
ical task, we propose slpt to improve model performance.
2.1
prompt-based visual model
the pre-trained model, learned by supervised or unsupervised training, is a pow-
erful tool for improving performance on label-limited downstream tasks.
finally, the attention output and f out
i−1 are element-wise multiplied and
added to obtain the updated feature fi.
speciﬁcally, we set the foreground to 1 and the background to 0 in the ground-
truth mask, and then average all masks and downsample to 1 × d
2 × h
2 × w
2 .
to enhance prompt diversity, we introduce a prompt diversity loss ldiv that
regularizes the cosine similarity between the generated prompts and maximizes
their diversity.
to achieve this, we design k diﬀerent data augmentation, heads,
and losses based on corresponding k prompts.
by varying hyperparameters, we
can achieve diﬀerent data augmentation strengths, increasing the model’s diver-
sity and generalization.
to achieve this, we leverage the pre-trained model
to obtain feature representations for all unlabeled data.
slpt: selective labeling meets prompt tuning
19
in the latter, we evaluate intra-prompts uncertainty by computing the mean
prediction of the prompts and propose to estimate prompt-based gradients as
the model’s performance depends on the update of prompt parameters θp.
sg =

θp
||∇θp(−

ymean ∗ log ymean)||2
(6)
to avoid manual weight adjustment, we employ multiplication instead of
addition.
we sort the unlabeled data by their
corresponding s values in ascending order and select the top b data to annotate.
3
experiments and results
3.1
experimental settings
datasets and pre-trained model.
we conducted experiments on automating
liver tumor segmentation in contrast-enhanced ct scans, a crucial task in liver
cancer diagnosis and surgical planning [1].
although there are publicly available
liver tumor datasets [1,24], they only contain major tumor types and diﬀer in
image characteristics and label distribution from our hospital’s data.
we col-
lected a dataset from our in-house hospital comprising 941 ct scans with eight
categories: hepatocellular carcinoma, cholangioma, metastasis, hepatoblastoma,
hemangioma, focal nodular hyperplasia, cyst, and others.
our objective is to segment all types of lesions accurately.
we utilized a pre-trained model for liver segmentation using supervised learning
on two public datasets [24] with no data overlap with our downstream task.
to evaluate the performance, we employed a 5-fold cross-
validation (752 for selection, 189 for test).
we evaluated lesion segmentation performance using pixel-wise and
lesion-wise metrics.
evaluation of diﬀerent tunings on the lesion segmentation with limited data
(40 class-balanced patients).
in the prompt tuning experiment, we compared
our method with three types of tuning: full parameter update (fine-tuning,
learn-from-scratch), partial parameter update (head-tuning, encoder-tuning,
decoder-tuning), and prompt update (spm [17]).
in the unsupervised diversity
selection experiment, we compared our method with random sampling.
in the
supervised uncertainty selection experiment, we compared our method with ran-
dom sampling, diversity sampling (coreset
we conducted the experiments using the pytorch frame-
work on a single nvidia tesla v100 gpu.
the nnunet [12] framework was
used for 3d lesion segmentation with training 500 epochs at an initial learn-
ing rate of 0.01.
during training, we set
k = 3 and employed diverse data augmentation techniques such as scale, elas-
tic, rotation, and mirror.
to ensure fairness and eliminate model ensemble eﬀects, we
only used the model’s prediction with k = 1 during testing.
we used ﬁxed ran-
dom seeds and 5-fold cross-validation for all segmentation experiments.
3.2
results
evaluation of prompt tuning.
using this sub-dataset, we evaluated various tuning methods for limited
slpt: selective labeling meets prompt tuning
21
table 2. comparison of data selection methods for label-limited lesion segmentation.
the pre-trained model is crucial for downstream tasks with limited data,
as it improves performance by 9.52% compared to learn-from-scratch.
among
the three partial tuning methods, the number of tuning parameters positively
correlates with the model’s performance, but they are challenging to surpass
ﬁne-tuning.
evaluation of selective labeling.
as
shown in table 2, the complete tesla achieved the best performance, outper-
forming the version without sd by 1.84% and the version without sg by 1.98%.
it shows that each component plays a critical role in improving performance.
4
conclusions
we proposed a pipeline called slpt that enhances model performance in label-
limited scenarios.
slpt pipeline is a promising solution for practical medical tasks
with limited data, providing good performance, few tunable parameters, and low
labeling costs.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_20.pdf:
the recent develop-
ments of artiﬁcial neural networks in computational pathology have shown that
these methods hold great potential for improving the accuracy and quality of
cancer diagnosis.
herein, we propose a centroid-aware
feature recalibration network that can conduct cancer grading in an accurate and
robust manner.
the proposed network maps an input pathology image into an
embedding space and adjusts it by using centroids embedding vectors of different
cancer grades via attention mechanism.
equipped with the recalibrated embed-
ding vector, the proposed network classiﬁers the input pathology image into a
pertinent class label, i.e., cancer grade.
we evaluate the proposed network using
colorectal cancer datasets that were collected under different environments.
the
experimental results conﬁrm that the proposed network is able to conduct cancer
grading in pathology images with high accuracy regardless of the environmental
changes in the datasets.
in cancer diagnosis, treatment, and management, pathology-
driven information plays a pivotal role.
cancer grade is, in particular, one of the major
factors that determine the treatment options and life expectancy.
however, the current
pathology workﬂow is sub-optimal and low-throughput since it is, by and large, manu-
ally conducted, and the large volume of workloads can result in dysfunction or errors in
cancer grading, which have an adversarial effect on patient care and safety [2].
there-
fore, there is a high demand to automate and expedite the current pathology workﬂow
and to improve the overall accuracy and robustness of cancer grading.
recently, many computational tools have shown to be effective in analyzing pathol-
ogy images [3].
to further improve the efﬁciency and effec-
tiveness of dcnns in pathology image analysis, advanced methods that are tailored to
pathology images have been proposed.
[9] proposed to re-formulate cancer
classiﬁcation in pathology images as both categorical and ordinal classiﬁcation prob-
lems.
moreover, attention mechanisms have been utilized for an improved pathology
image analysis.
in this study, we propose a centroid-aware feature recalibration network (cafenet)
for accurate and robust cancer grading in pathology images.
the feature extractor is
utilized to obtain the feature representation of pathology images.
this indicates that the centroid embedding vectors can be used to recal-
ibrate the input embedding vectors of pathology images.
in
this manner, the feature representations of the input pathology images are re-calibrated
and stabilized for a reliable cancer classiﬁcation.
the experimental results demonstrate
that cafenet achieves the state-of-the-art cancer grading performance in colorectal can-
cer grading datasets.
1. cafenet employs a
deep convolutional neural network as a feature extractor and an attention mechanism to
produce robust feature representations of pathology images and conducts cancer grading
with high accuracy.
cafenet consists of a feature extractor, a cafe module, a cup
module, and a classiﬁcation layer.
2.1
centroid-aware feature recalibration
let {xi, yi}n
i=1 be a set of pairs of pathology images and ground truth labels where n
is the number of pathology image-ground truth label pairs, xi ∈ rh×w×c is the i th
pathology image, yi ∈ {c1, . . .
speciﬁcally, cup module adds up the embedding vectors of different class labels over
the iterations per epoch, computes the average embedding vectors, and updates the
centroid embedding vectors ec =

ec
j |j = 0, . . .
efﬁcientnet-b0 is composed of
one convolution layer and 16 stages of mobile inverted bottleneck blocks, of which each
with a different number of layers and channels.
264
192
8394
md
2997
370
738
61985
pd
1391
234
205
11895
3
experiments and results
3.1
datasets
two publicly available colorectal cancer datasets [9] were employed to evaluate the
effectiveness of the proposed cafenet.
both
datasets provide colorectal pathology images with ground truth labels for cancer grad-
ing.
the ﬁrst dataset
includes 1600 bn, 2322 wd, 4105 md, and 1830 pd image patches that were col-
lected between 2006 and 2008 using an aperio digital slide scanner (leica biosystems)
at 40x magniﬁcation.
each image patch has a spatial size of 1024 × 1024 pixels.
the second dataset, designated as ctestii, contains 27986 bn, 8394
wd, 61985 md, and 11985 pd image patches of size 1144 × 1144 pixels.
3.2
comparative experiments
we conducted a series of comparative experiments to evaluate the effectiveness of
cafenet for cancer grading, in comparison to several existing methods: 1) three dcnn-
based models: resnet
[9], which
demonstrates the state-of-the-art performance on the two colorectal cancer datasets under
consideration.
3.3
implementation details
we initialized all models using the pre-trained weights on the imagenet dataset, and then
trained them using the adam optimizer with default parameter values (β1= 0.9, β2 =
0.999, ε = 1.0e-8) for 50 epochs.
after data augmentation,
all patches, except for those used in vit [17] and swin
we implemented all models using the pytorch platform and trained on a workstation
equipped with two rtx 3090 gpus.
to increase the variability of the dataset during
the training phase, we applied several data augmentation techniques, including afﬁne
transformation, random horizontal and vertical ﬂip, image blurring, random gaussian
noise, dropout, random color saturation and contrast conversion, and random contrast
transformations.
all these techniques were implemented using the aleju library (https://
github.com/aleju/imgaug).
[18]
87.4
0.847
0.820
0.832
0.941
mmae−ceo[9]
87.7
–
–
0.843
0.940
cafenet (ours)
87.5
0.853
0.816
0.832
0.940
3.4
result and discussions
we evaluated the performance of colorectal cancer grading by the proposed cafenet
and other competing models using ﬁve evaluation metrics, including accuracy (acc),
218
j. lee et al.
precision, recall, f1-score (f1), and quadratic weighted kappa (κw).
table 2 demon-
strates the quantitative experimental results on ctesti.
metric learning was able to improve the classiﬁcation
performance.
effcientnet was the worst model among them, but with the help of triplet
loss (triplet) or supervised contrastive loss (sc), the overall performance increased by
≥2.8% acc, ≥0.023 precision, ≥0.001 recall, ≥0.010 f1, and ≥0.047 κw.
among the
transformer-based models, swin was one of the best performing models, but vit showed
much lower performance in all evaluation metrics.
table 3. result of colorectal cancer grading on ctestii.
in a head-
to-head comparison of the classiﬁcation results between ctesti and ctestii, there was
a consistent performance drop in the proposed cafenet and other competing models.
in regard to such differences, it is
striking that the proposed cafenet achieved the best performance on ctestii.
however,
resnet, swin, and mmae−ceo showed a higher performance drop in all evaluation
metrics.
cafenet had a minimal performance drop except efﬁcientnet.
efﬁcientnet,
however, obtained poorer performance on both ctesti and ctestii.
these results suggest
that cafenet has the better generalizability so as to well adapt to unseen histopathology
image data.
we conducted ablation experiments to investigate the effect of the cafe module on
cancer classiﬁcation.
the exclusion of the cafe
centroid-aware feature recalibration for cancer grading
219
module, i.e., efﬁcientnet, resulted in much worse performance than cafenet.
using
only the recalibrated embedding vectors er, a substantial drop in performance was
observed.
these two results indicate that the recalibrated embedding vectors complement
to the input embedding vectors e.
using addition, instead of concatenation, there was
a consistent performance drop, indicating that concatenation is the superior approach
for combining the two embedding vectors together.
these results conﬁrm that the proposed cafenet is computational
efﬁcient and it does not achieve its superior learning capability and generalizability at
the expense of the model complexity.
4
conclusions
herein, we propose an attention mechanism-based deep neural network, called cafenet,
for cancer classiﬁcation in pathology images.
in the experiments on colorectal cancer datasets against several competing mod-
els, the proposed network demonstrated that it has a better learning capability as well as
a generalizability in classifying pathology images into different cancer grades.
however,
the experiments were only conducted on two public colorectal cancer datasets from a
single institute.
additional experiments need to be conducted to further verify the ﬁnd-
ings of our study.
therefore, future work will focus on validating the effectiveness of
the proposed network for other types of cancers and tissues in pathology images.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_23.pdf:
skin image datasets often suﬀer from imbalanced data distri-
bution, exacerbating the diﬃculty of computer-aided skin disease diag-
nosis.
despite achieving signiﬁcant performance,
these scl-based methods focus more on head classes, yet ignoring the
utilization of information in tail classes.
in this paper, we propose class-
enhancement contrastive learning (ecl), which enriches the informa-
tion of minority classes and treats diﬀerent classes equally.
for infor-
mation enhancement, we design a hybrid-proxy model to generate class-
dependent proxies and propose a cycle update strategy for parameters
optimization.
experimental results on the clas-
siﬁcation of imbalanced skin lesion data have demonstrated the superior-
ity and eﬀectiveness of our method.
keywords: contrastive learning · dermoscopic image · long-tailed
classiﬁcation
1
introduction
skin cancer is one of the most common cancers all over the world.
serious skin
diseases such as melanoma can be life-threatening, making early detection and
treatment essential [3].
as computer-aided diagnosis matures, recent advances
with deep learning techniques such as cnns have signiﬁcantly improved the per-
formance of skin lesion classiﬁcation [7,8].
however, as data-hungry approaches,
deep learning models require large balanced and high-quality datasets to meet the
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43895-0 23.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
https://doi.org/10.1007/978-3-031-43895-0_23
ecl: class-enhancement contrastive learning
245
sample
enhanced proxy
anchor sample
anchor proxy
many class
medium class
few class
aggregation
separation
(a) supervised contrastive learning  
(b) class-enhancement contrastive learning
over-treatment 
equal-treatment 
fig.
accuracy and robustness requirements in applications, which is hard to suﬃce due
to the long-tailed occurrence of diseases in the real-world.
thus, existing public skin
datasets usually suﬀer from imbalanced problems which then results in class bias
of classiﬁer, for example, poor model performance especially on tail lesion types.
to tackle the challenge of learning unbiased classiﬁers with imbalanced data,
many previous works focus on three main ideas, including re-sampling data [1,
18], re-weighting loss [2,15,22] and re-balancing training strategies [10,23].
re-
sampling methods over-sample tail classes or under-sample head classes, re-
weighting methods adjust the weights of losses on class-level or instance-level,
and re-balancing methods decouple the representation learning and classiﬁer
learning into two stages or assign the weights between features from diﬀerent
sampling branches
despite the great results achieved, these methods either
manually interfere with the original data distribution or improve the accuracy
of minority classes at the cost of reducing that of majority classes [12,13].
[11] aggregates semantically similar samples
and separates diﬀerent classes by training in pairs, leading to impressive success
in long-tailed classiﬁcation of both natural and medical images [16].
to address the above issues, we propose a class-enhancement contrastive
learning (ecl) method for skin lesion classiﬁcation, diﬀerences between scl
and ecl are illustrated in fig.
(2)
we present a balanced-hybrid-proxy loss to balance the optimization of each
class and leverage relations among samples and proxies.
(3) a new balanced-
weighted cross-entropy loss is designed for an unbiased classiﬁer, which considers
both “imbalanced data” and “imbalanced diagnosis diﬃculty”.
(4) experimental
results demonstrate that the proposed framework outperforms other state-of-the-
art methods on two imbalanced dermoscopic image datasets and the ablation
study shows the eﬀectiveness of each element.
the two branches take in diﬀerent
augmentations t
i, i ∈ {1, 2} from input images x and the backbone is shared
between branches to learn the features ˜xi, i ∈ {1, 2}.
we use a fully connected
layer as a logistic projection for classiﬁcation g(·) : ˜
x → ˜y and a one-hidden
layer mlp h(·) : ˜
x → z ∈ rd as a sample embedding head where d denotes the
dimension.
l2-normalization is applied to z by using inner product as distance
measurement in cl.
for
better representation, we design a cycle update strategy to optimize the proxies’
parameters in hybrid-proxy model, together with a curriculum learning schedule
for achieving unbiased classiﬁers.
ecl: class-enhancement contrastive learning
247
training set
sampling
classifier
mlp
mlp
logits
backbone
backbone
mini-batch
mini-batch
hybrid-proxy model
cycle update strategy  
aggregation
separation
∙
(∙)
classifier branch
contrastive learning branch
fig.
with such
248
y. zhang et al.
algorithm 1: training process of ecl.
input: training set x, validation set xval, training epochs e, iterations t,
batch size b, learning rate lr, stages in balanced-weighted cross-entropy
loss e2
1 initialize model parameters θ and hybrid-proxy model p parameters φ
2 for e in e do
3
for t in t do
4
getting a batch of samples

x(1,2)
− lr ∗ gradt
θ// update parameters θ of model
11
φ ← φ − t
t lr ∗ gradt
φ // update parameters φ of p
12
if e > e2 then
13
f e = v alidate(model, xval)
a strategy, tail proxies can be optimized in a view of whole data distribution,
thus playing better roles in class information enhancement.
for an anchor sample zi ∈ z in class c, we unify the positive image
set as z+ = {zj|yj = yi = c, j ̸= i}.
− 1

sj∈{z+∪p+}
log exp(si · sj/τ)
e
(2)
ecl: class-enhancement contrastive learning
249
e =

c∈c
1
2bc + n p
c
the average operation in the denominator of balanced-
hybrid-proxy loss can eﬀectively reduce the gradients of the head classes, making
an equal contribution to optimizing each class.
moreover, as
the skin datasets are often small, richer relations can eﬀectively help form a
high-quality distribution in the embedding space and improve the separation of
features.
2.3
balanced-weighted cross-entropy loss
taking both “imbalanced data” and “imbalanced diagnosis diﬃculty” into con-
sideration, we design a curriculum schedule and propose balanced-weighted
cross-entropy loss to train an unbiased classiﬁer.
the training phase are divided
into three stages.
we ﬁrst train a general classiﬁer, then in the second stage we
assign larger weight to tail classes for “imbalanced data”.
in the last stage, we
utilize the results on the validation set as the diagnosis diﬃculty indicator of
skin disease types to update the weights for “imbalanced diagnosis diﬃculty”.
we assume
f e
c is the evaluation result of class c on validation set after epoch e and we use
f1-score in our experiments.
the network is trained for e epochs, e1 and e2 are
hyperparameters for stages.
3
experiment
3.1
dataset and implementation details
dataset and evaluation metrics.
3. the results of confusion matrix illustrate that ecl obtains great performance
on most classes especially for minority classes.
dataset consists of 10015 images in 7 classes while a larger 2019 dataset provides
25331 images in 8 classes.
we adopt ﬁve metrics for evaluation: accuracy (acc), average precision (pre),
average sensitivity (sen), macro f1-score (f1) and macro area under curve
(auc).
implementation details.
the proposed algorithm is implemented in python
with pytorch library and runs on a pc equipped with an nvidia a100 gpu.
[9] as backbone and the embedding dimension d is set to 128.
we use the default data augmentation
strategy on imagenet in [9] as t1 for classiﬁcation branch.
we conduct experiments in 3 independent
runs and report the standard deviations in the supplementary material.
3.2
experimental results
quantitative results.
to evaluate the performance of our ecl, we compare
our method with 10 advanced methods.
to ensure fairness, we re-train all methods by rerun their
released codes on our divided datasets with the same experimental settings.
ecl: class-enhancement contrastive learning
251
table 1. comparison results on isic2018 and isic2019 datasets.
it can be seen that ecl has
a signiﬁcant advantage with the highest level in most metrics on two datasets.
to further verify the eﬀectiveness of the designs in ecl, we
conduct a detailed ablation study shown in table 2 (the results on isic2018 are
shown in supplementary material table s2).
we can see from the results that
adding cl branch can signiﬁcantly improve the network’s data representation
ability with better performance than only adopting a classiﬁer branch.
and our
bwce loss can help in learning a more unbiased classiﬁer with an improvement
of 2.7% in f1 compared to ce in dual branch setting.
the overall performance of the network has declined
compared with training w/ the strategy, indicating that this strategy can bet-
ter enhance proxies learning through the whole data distribution.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_32.pdf:
to
achieve high accuracy, cts in diﬀerent phases are integrated to provide
more information than single-phase images.
we propose a hybrid model called
transliver, which has a transformer backbone and complementary con-
volutional modules.
extensive experiments are conducted, in which we achieve
an overall accuracy of 90.9% on an in-house dataset of four ct phases
and seven liver lesion classes.
the results also show distinct advantages
in comparison to state-of-art approaches in classiﬁcation.
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43895-0 31.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
https://doi.org/10.1007/978-3-031-43895-0_31
330
x. wang et al.
hemangioma (hh), and hepatic cyst (hc) or malignant tumors, such as intra-
hepatic cholangiocarcinoma (icc), hepatic metastases (hm), and hepatocellular
carcinoma (hcc).
dynamic
contrast-enhanced ct is a common technique for liver cancer diagnosis, where
four diﬀerent phases of imaging, namely, non-contrast (nc), arterial (art), por-
tal venous (pv), and delayed (dl) provide complementary information about
the liver.
in each image, the phase sequence from left to
right and top to bottom is nc, art, pv, and dl, respectively.
with the development of deep learning, computer-aided liver lesion diag-
nosis has attracted much attention [5,8,16] in recent years.
[8] combined liver segmentation and classiﬁcation using transfer learning and
joint learning to increase the performance of cnn.
as a manner to elevate the
accuracy of cnns, frid-adar et al.
[5] designed a gan-based network to gener-
ate synthetic liver lesion images, improving the classiﬁcation performance based
on cnn.
it is reported in many studies [9,18] that using multi-phase data,
like most professionals do in practice, can help the network get a more accu-
rate result, which also acts in liver lesion classiﬁcation [15,23,24].
hybrid transformer model for multi-phase liver lesion classiﬁcation
331
moreover, while most works have attached much importance to liver lesion seg-
mentation
additional
eﬀort will be needed when consolidating segmentation and multi-phase classiﬁ-
cation.
self-attention based transformers [19] have shown strong capability in nat-
ural language processing tasks.
[4] have
been shown to replace cnn with a transformer encoder in computer vision tasks
and can achieve obvious advantages on large-scale datasets.
[6], including ignoring local information within each patch, extracting only
single-scale features, and lacking inductive bias.
to alleviate the limitations of pure transformers,
we propose a multi-stage pyramid structure and add convolutional layers to the
original transformer encoder.
we use additional cross phase tokens at the last
stage to complete a multi-phase fusion, which can focus on cross-phase com-
munication and improve the fusion eﬀectiveness as compared with conventional
modes.
as the backbone of the whole framework, transformer encoder
employs a 4-stage pyramid structure extracting multi-scale features, with each
stage connected by a convolutional down-sampler.
extracted features from diﬀerent phases
are averaged and classiﬁed by two successive fully connected networks.
we also use auxiliary
dice loss function between ﬁxed image lesion masks and moved image lesion
masks to help the registration ﬁeld learning.
in [1], the network needs to specify
an atlas image, otherwise, pairs of images will be registered to each other.
hybrid transformer model for multi-phase liver lesion classiﬁcation
333
2.2
convolutional encoder and convolutional down-sampler
in pure vision transformer, input images are converted to tokens by patch embed-
ding and added with positional encoding to keep the positional information.
for an input image x ∈ rb×h×w ×1,
b is the batch size, and h × w is the size of the input.
we add convolutional down-samplers between stages of transformer encoder
so that they can produce hierarchical representation like cnn structure.
we also utilize a convolutional
layer with a kernel size of 2 and stride of 2, which halves the image resolution
and doubles the number of channels.
2.3
single-phase liver transformer block
vision transformers can get excellent performance on large-scale datasets such
as imagenet
(1)
where q, k, v are the same with original vit, dh is the head dimension, and
p is the relative positional encoding.
spatial reduction sr consists of a k × k
depthwise convolution with a stride of k and a batch normalization, where k is
the spatial reduction ratio set in each stage.
the ﬁrst and third convolutions are pointwise for dimension translation,
which has a similar eﬀect to the original linear layers.
the second convolution
with a shortcut connection extracts local information in a higher dimension and
improves the gradient propagation ability across layers
the structure also
has two gelu activation layers between convolutional layers and three batch
normalizations after the gelus and the last convolutional layer for better per-
formance.
inspired by [14], in stage 4, we design a multi-phase
liver transformer block (mpltb) for communication between phases.
then, they are sepa-
rated and averaged for the next layer.
compared to the direct fusion of input images
or output features like average and concatenation, cross phase tokens can also
reduce fusion granularity to suﬃciently explore the relationship among phases.
the fusion is conducted in deep
layers because the semantic concepts are learned in higher layers which beneﬁts
the cross phase connection.
3
experiments
3.1
liver lesion classiﬁcation
dataset.
the employed single-phase annotated dataset is collected from sir run
run shaw hospital (srrsh), aﬃliated with the zhejiang university school of
medicine, and has received the ethics approval of irb.
the collection process
can be found in supplementary materials.
lesions from the same patient are either
assigned to the training and validation set or the test set, but not both.
implementations.
the data is augmented by ﬂip, rotation, crop, shift, and scale.
our
models are implemented by pytorch1.12.1 and timm0.6.13
we
measured performance by precision (pre.), sensitivity (sen.), speciﬁcity (spe.),
f1-score (f1), area under the curve (auc), and accuracy (acc.).
results.
in the results of our method, hm has a relatively low performance
of 62.5%, mainly due to its low proportion in our dataset.
the details can be
found in supplementary materials.
considering the fairness, all the models below are initialized with pre-trained
weights and adopt 2-d structures using the same slice-level classiﬁcation strategy.
as illustrated in table 1, our proposed transliver model gets better performance
than other models in all metrics.
behind our model, cmt-s achieves the best
performance, indicating the eﬀect of convolutional structures in transformer.
table 1. performance of transliver and other sota classiﬁcation methods.
[7] 71.7
72.6
96.1
71.2
92.6
77.1
vit-s [4]
79.6
79.4
97.2
78.6
92.9
82.9
swin-s [12]
77.7
78.1
97.1
77.3
93.8
82.3
cmt-s [6]
80.5
80.5
97.6
80.0
94.1
85.7
transliver
88.7 87.4 98.5 87.3 95.1
90.9
336
x. wang et al.
3.2
ablation study
to verify the improvement of our modules, we conduct three baseline experi-
ments for comparison.
a 3-d version of baseline 2 utilizing 3-d
patch embedding is also studied in baseline 3 to validate the advantage of our
2-d model.
it is worth mentioning that the 2-d structure is prone to redun-
dancy between axial slices and ignores the relation between slices compared with
the 3-d structure but gets observably higher accuracy.
furthermore, vision
transformers are mostly pretrained in 2-d images, causing poor performance
when transferring to 3-d pipeline.
we also evaluate the model performance under diﬀerent phase combinations
by cutting the branch of certain phases.
it shows that information from vari-
ous phases can signiﬁcantly inﬂuence the classiﬁcation performance.
figure 4 contains average results
of phase number and details with all phase combinations can be found in sup-
plementary materials.
we report per-
formance of an overall 90.9% classiﬁcation accuracy on a four-phase seven-class
dataset through quantitative experiments and show obvious improvement com-
pared with sota classiﬁcation methods.
in future work, we will extend classi-
ﬁcation to instance segmentation and provide an end-to-end eﬀective model for
liver lesion diagnosis.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_33.pdf:
colonoscopy analysis, particularly automatic polyp segmen-
tation and detection, is essential for assisting clinical diagnosis and treat-
ment.
however, as medical image annotation is labour- and resource-
intensive, the scarcity of annotated data limits the eﬀectiveness and gen-
eralization of existing methods.
although recent research has focused on
data generation and augmentation to address this issue, the quality of the
generated data remains a challenge, which limits the contribution to the
performance of subsequent tasks.
inspired by the superiority of diﬀusion
models in ﬁtting data distributions and generating high-quality data,
in this paper, we propose an adaptive reﬁnement semantic diﬀusion
model (arsdm) to generate colonoscopy images that beneﬁt the down-
stream tasks.
speciﬁcally, arsdm utilizes the ground-truth segmentation
mask as a prior condition during training and adjusts the diﬀusion loss
for each input according to the polyp/background size ratio.
further-
more, arsdm incorporates a pre-trained segmentation model to reﬁne
the training process by reducing the diﬀerence between the ground-truth
mask and the prediction mask.
extensive experiments on segmentation
and detection tasks demonstrate the generated data by arsdm could
signiﬁcantly boost the performance of baseline methods.
keywords: diﬀusion models · colonoscopy · polyp segmentation ·
polyp detection
y. du and y. jiang—equal contributions.
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43895-0_32.
deep learning methods have shown powerful abilities in
automatic colonoscopy analysis, including polyp segmentation [5,22,26,27,29]
and polyp detection
however, the scarcity of annotated data due to high
manual annotation costs results in poorly trained and low generalizable models.
[9,25]
or data augmentation methods [3,13,28] to enhance learning features, but
these methods yielded limited improvements in downstream tasks.
gt masks
original images
synthesis images
combine
segmentation
detection
downstream tasks
… …
arsdm
e.g.
e.g.
diffusion sampler
fig.
2.
despite recent progress in these methods for medical image analysis, existing
models face two major challenges when applied to colonoscopy image analysis.
firstly, the foreground (polyp) of colonoscopy images contains rich pathological
information yet is often tiny compared with the background (intestine wall) and
can be easily overwhelmed during training.
thus, naive generative models may
generate realistic colonoscopy images but those images seldom contain polyp
regions.
in addition, in order to generate high-quality annotated samples, it is
crucial to maintain the consistency between the polyp morphologies in synthe-
sized images and the original masks, which current generative models struggle
to achieve.
to tackle these issues and inspired by the remarkable success achieved by dif-
fusion models in generating high-quality ct or mri data [8,11,23], we creatively
propose an eﬀective adaptive reﬁnement semantic diﬀusion model (arsdm) to
generate polyp-contained colonoscopy images while preserving the original anno-
tations.
speciﬁcally, we use the original segmentation masks as condi-
tions to train a conditional diﬀusion model, which makes the generated sam-
ples share the same masks with the input images.
in addition,
we ﬁne-tune the diﬀusion model by minimizing the distance between the original
ground truth masks and the prediction masks from synthesis images via a pre-
trained segmentation network.
in summary, our contributions are three-fold: (1) adaptive reﬁnement
sdm: based on the standard semantic diﬀusion model [21], we propose a novel
arsdm with the adaptive loss re-weighting and the prediction-guided sample
reﬁnement mechanisms, which is capable of generating realistic polyp-contained
colonoscopy images while preserving the original annotations.
to the best of our
knowledge, this is the ﬁrst work for adapting diﬀusion models to colonoscopy
image synthesis.
(2) large-scale colonoscopy generation: the proposed
approach can be used to generate large-scale datasets with no/arbitrary anno-
tations, which signiﬁcantly beneﬁts the medical image society, laying the foun-
dation for large-scale pre-training models in automatic colonoscopy analysis.
(3)
qualitative and quantitative evaluation: we conduct extensive experi-
ments to evaluate our method on ﬁve public benchmarks for polyp segmentation
and detection.
the results demonstrate that our approach could help deep learn-
ing methods achieve better performances.
= n

xt;

1 − βtxt−1, βti

,
(1)
where q (x0) is the original data distribution with x0 ∼ q (x0), x1:t are latents
with the same dimension of x0 and βt is a variance schedule.
(3)
342
y. du et al.
diffusion
process
sampler
u-net
re-weighting
module
diffusion loss
ℒ
refinement loss
ℒ
weights map 
pranet
condition
input 
gt mask
sample  
prediction mask  
noised
estimated  
 
fig.
in this paper, we propose an adaptive reﬁnement semantic diﬀusion model,
a variant of ddpm, which has three key parts, i.e., mask conditioning, adaptive
loss re-weighting, and prediction-guided sample reﬁnement.
2.
2.1
mask conditioning
unlike the previous generative methods, our work aims to generate a synthetic
image with an identical segmentation mask to the original annotation.
speciﬁcally, for an input
image x0 ∈

+ σiz
4 end for
5 ˜c0 = p(˜x0)
6 take gradient descent step on ∇θltotal
2.2
adaptive loss re-weighting
the polyp regions in the colonoscopy images diﬀer from the background regions,
which contain more pathological information and should be adequately treated
to learn a better model.
(8)
2.3
prediction-guided sample reﬁnement
the downstream tasks of polyp segmentation and detection require rich semantic
information on polyp regions to train a good model.
through extensive exper-
iments, we found inaccurate sample images with coarse polyp boundary that
is not aligned properly with the original masks may introduce large biases and
noises to the datasets.
the model can be confused by several conﬂicting training
images with the same annotation.
to this end, we design a reﬁnement strategy
344
y. du et al.
table 1. comparisons of diﬀerent settings applied on three polyp segmentation base-
lines.
methods endoscene
clinicdb
kvasir
colondb
etis
overall
mdice miou mdice miou mdice miou mdice miou mdice miou mdice miou
pranet
87.1
79.7
89.9
84.9
89.8
84.0
70.9
64.0
62.8
56.7
74.0
67.5
+ldm
83.7
76.9
88.2
83.5
88.4
83.0
62.6
56.0
56.2
50.3
67.8
61.7
+sdm
89.9
83.2
89.2
83.7
88.4
82.6
74.2
66.5
66.4
60.3
76.4
69.6
+ours
89.7
82.7
93.3
88.5
89.9
84.5
76.1
68.9
75.5
68.1
80.0
73.2
sanet
88.8
81.5
91.6
85.9
90.4
84.7
75.3
67.0
75.0
65.4
79.4
71.4
+ldm
72.7
60.5
88.8
82.8
88.7
82.7
64.3
55.4
58.0
49.2
68.3
59.8
+sdm
90.2
83.0
89.9
84.1
90.9
85.4
77.6
69.3
74.7
66.8
80.4
72.9
+ours
90.2
83.2
91.4
86.1
91.1
85.6
77.7
70.0
78.0
69.5
81.5
74.1
pvt
90.0
83.3
93.7
88.9
91.7
86.4
80.8
72.7
78.7
70.6
83.3
76.0
+ldm
88.2
81.2
92.3
87.1
91.2
85.7
78.7
70.4
78.0
69.6
81.9
74.2
+sdm
88.8
81.7
93.9
89.2
91.2
86.1
81.3
73.5
78.7
71.1
83.4
76.3
+ours
88.2
81.2
92.2
87.5
91.5
86.3
81.7
73.8
80.6
72.9
84.0
76.7
that uses the prediction of a pre-trained segmentation model on the sampled
images to guide the training process and restore the proper polyp boundary
information.
speciﬁcally, at each iteration of training, the output ˜ϵ = ϵθ (xt, t, c0)
will go into the sampler to generate sample image ˜x0.
then, we take the sample
image as the input of the segmentation model to predict the pseudo masks ˜c0.
we propose the following reﬁnement loss based on iou loss and binary cross
entropy (bce) loss between ˜c0 and c0.
the reﬁnement loss is:
lreﬁne = l(c, ˜cg) +
i=5

i=3
l ( ˜ci) ,
˜c0 = { ˜c3, ˜c4, ˜c5, ˜cg} = p (s (˜ϵ)) ,
(9)
where l = liou + lbce is the sum of the iou loss and bce loss, ˜c0 is the
collection of the three side-outputs ( ˜c3, ˜c4, ˜c5) and the global map ˜cg as described
in [5]. p(·) represents the pranet model and s(·) is the ddim [16] sampler.
(10)
3
experiments
3.1
arsdm experimental settings
we conducted our experiments on ﬁve public polyp segmentation datasets:
endoscene
following the standard of pranet, 1,450 image-mask pairs from kvasir
and cvc-clinicdb are taken as the training set.
the training image-mask pairs are padded to have the same height and
width and then resized to the size of 384 × 384.
experiments with prediction-
guided sample reﬁnement are trained with around one-half nvidia a100 days,
while others are trained with approximately one day for convergence.
we use
the ddim sampler with a maximum timestep of 200 for sampling images.
3.2
downstream experimental settings
we conduct the evaluation of our methods and the state-of-the-art counterparts
on polyp segmentation and detection tasks.
[22], and polyp-pvt [2] as baseline segmentation models with default set-
tings, and evaluated them using mean intersection over union (iou) and mean
dice metrics.
evaluated them using average precision (ap) and f1-scores.
3.3
quantitative comparisons
the experimental results presented in table 1 and 2 demonstrate the eﬀective-
ness of our proposed method in training better downstream models to achieve
superior performance.
speciﬁcally, data generated by our approach assists the
346
y. du et al.
table 3. ablation study of diﬀerent com-
ponents on polyp segmentation tasks.
ap
f1
ap
f1
✗
✗
61.8
79.1
65.2
76.7
✓
✗
62.2
80.1
65.8
77.2
✗
✓
64.0
80.4
66.0
77.6
✓
✓
65.7 81.3 66.4 79.0
ori. images
masks
samples
fig.
3. illustration of generated samples with the corresponding masks and original
images for comparison reference.
signiﬁcant improvements for each model in mdice and miou, with increases of
6.0% and 5.7% over pranet, 2.1% and 2.7% over sanet, and 0.7% and 0.7% over
polyp-pvt.
moreover, we
conducted a comprehensive comparison with sota models, noting that these
models were not speciﬁcally designed for colonoscopy images and may generate
data that hinder the training process or lack the ability for eﬀective improvement.
nevertheless, our experimental results conﬁrm the superiority of our proposed
method.
ablation study.
the results demonstrate both components contribute to the accuracy
improvement of baseline models, indicating their essential roles in achieving the
best ﬁnal performance.
3.4
qualitative analyses
to further investigate the generative performance of our approach, we present
visualization results in fig.
3, which displays the generated samples and their
corresponding masks, alongside the original images for reference.
the gener-
ated samples demonstrate diﬀerences from the original images in both the polyp
arsdm
347
regions and the backgrounds while maintaining alignment with the masks.
addi-
tionally, we sought evaluations from medical professionals to assess the authen-
ticity of the generated samples, and non-medical professionals to locate polyps
in the images, which yielded positive feedback on the quality of the generated
samples.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_16.pdf:
the presence of corrupted labels is a common problem in the
medical image datasets due to the diﬃculty of annotation.
meanwhile,
corrupted labels might signiﬁcantly deteriorate the performance of deep
neural networks (dnns), which have been widely applied to medical
image analysis.
extensive experiments on three popular med-
ical image datasets demonstrate the superior performance of our frame-
work over recent state-of-the-art methods.
however, either of them is very
diﬃcult to be obtained for conducting medical image analysis with dnns.
in
particular, obtaining high-quality labels needs professional experience so that
corrupted labels can often be found in medical datasets, which can seriously
degrade the eﬀectiveness of medical image analysis.
based on whether correcting corrupted labels, previous methods can be roughly
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43898-1_16.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
robustness-based methods are designed to utilize vari-
ous techniques, such as dropout, augmentation and loss regularization, to avoid
the adverse impact of corrupted labels, thereby outputting a robust model.
for example, co-correction [9] simultaneously trains two models and cor-
rects labels for medical image analysis, and lcc
to address the aforementioned issues, in this paper, we propose a new
co-assistant framework, namely co-assistant networks for label correction
(cnlc) (shown in fig.
first, we propose a new label correction method (i.e., a co-assistant framework)
to boost the model robustness for medical image analysis by two sequential
modules.
speciﬁcally, the samples with n1 smallest losses
are regarded as clean samples and the samples with n1 largest loss values are
regarded as corrupted samples, where n1 is experimentally set as 5.0% of all
training sample for each class.
in particular, based on the partition mentioned in the above section, the
clean samples within the same class should have the same label and the cor-
rupted samples should have diﬀerent labels from clean samples within the same
class.
we list the optimization
details of our proposed algorithm in the supplemental materials.
3
experiments
3.1
experimental settings
the used datasets are breakhis
breakhis
consists of 7,909 breast cancer histopathological images including 2,480 benigns
and 5,429 malignants.
we randomly select 5,537 images for training and 2,372
ones for testing.
isic has 12,000 digital skin images where 6,000 are normal and
6,000 are with melanoma.
nihcc has 10,280 frontal-view x-ray images,
where 5,110 are normal and 5,170 are with lung diseases.
we randomly select
8,574 images for training and the rest of images for testing.
in particular, the
random selection in our experiments guarantees that three datasets (i.e., the
training set, the testing set, and the whole set) have the same ratio for each
co-assistant networks for label correction
165
table 1.
the classiﬁcation results (average ± std) on three datasets.
we compare our proposed method with six popular methods, including one
fundamental baseline (i.e., cross-entropy (ce)), three robustness-based meth-
ods (i.e., co-teaching (ct) [6], nested co-teaching (nct)
for fair-
ness, in our experiments, we adopt the same neural network for all comparison
methods based on their public codes and default parameter settings.
due
to the space limitation, we present the results at ϵ = 0.0 of all methods in the
supplemental materials.
the classiﬁcation results (average ± std) of the ablation study on isic.
for example, our method on average improves by 2.4% and 15.3%,
respectively, compared to the best comparison method (i.e., ct) and the worst
comparison method (i.e., ce), on all cases.
second, all methods out-
perform the fundamental baseline (i.e., ce) on all cases.
addi-
tionally, cnlc obtains better performance than mlp because it considers the
relationship among samples.
both of the above observations verify the conclusion
mentioned in the last section again.
to verify the eﬀectiveness of the resistance loss in eq.
(1) for medical image analysis, which has been theoretically and experimen-
tally veriﬁed in the application of natural images [12].
co-assistant networks for label correction
167
4
conclusion
in this paper, we proposed a novel co-assistant framework, to solve the prob-
lem of dnns with corrupted labels for medical image analysis.
experiments
on three medical image datasets demonstrate the eﬀectiveness of the proposed
framework.
although our method has achieved promising performance, its accu-
racy might be further boosted by using more powerful feature extractors, like
pre-train models on large-scale public datasets or some self-supervised methods,
e.g., contrastive learning.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_13.pdf:
in this work, we propose a few-shot colorectal tissue image
generation method for addressing the scarcity of histopathological train-
ing data for rare cancer tissues.
our few-shot generation method, named
xm-gan, takes one base and a pair of reference tissue images as input
and generates high-quality yet diverse images.
within our xm-gan, a
novel controllable fusion block densely aggregates local regions of refer-
ence images based on their similarity to those in the base image, resulting
in locally consistent features.
to the best of our knowledge, we are the ﬁrst
to investigate few-shot generation in colorectal tissue images.
we evaluate
our few-shot colorectral tissue image generation by performing extensive
qualitative, quantitative and subject specialist (pathologist) based evalu-
ations.
speciﬁcally, in specialist-based evaluation, pathologists could dif-
ferentiate between our xm-gan generated tissue images and real images
only 55% time.
moreover, we utilize these generated images as data aug-
mentation to address the few-shot tissue image classiﬁcation task, achiev-
ing a gain of 4.4% in terms of mean accuracy over the vanilla few-shot clas-
siﬁer.
keywords: few-shot image generation · cross modulation
1
introduction
histopathological image analysis is an important step towards cancer diagno-
sis.
however, shortage of pathologists worldwide along with the complexity of
histopathological data make this task time consuming and challenging.
there-
fore, developing automatic and accurate histopathological image analysis meth-
ods that leverage recent progress in deep learning has received signiﬁcant atten-
tion in recent years.
in this work, we investigate the problem of diagnosing
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43898-1_13.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
https://doi.org/10.1007/978-3-031-43898-1_13
few-shot image generation for colorectal tissue classiﬁcation
129
colorectal cancer, which is one of the most common reason for cancer deaths
around the world and particularly in europe and america [23].
existing deep learning-based colorectal tissue classiﬁcation methods [18,21,22]
typically require large amounts of annotated histopathological training data for all
tissue types to be categorized.
to this end, it is desirable to develop
a few-shot colorectal tissue classiﬁcation method, which can learn from seen tissue
classes having suﬃcient training data, and be able to transfer this knowledge to
unseen (novel) tissue classes having only a few exemplar training images.
[6] have been utilized to syn-
thesize images, they typically need to be trained using large amount of real
images of the respective classes, which is not feasible in aforementioned few-shot
setting.
therefore, we propose a few-shot (fs) image generation approach for
generating high-quality and diverse colorectal tissue images of novel classes using
limited exemplars.
moreover, we demonstrate the applicability of these generated
images for the challenging problem of fs colorectal tissue classiﬁcation.
contributions: we propose a few-shot colorectal tissue image generation
framework, named xm-gan, which simultaneously focuses on generating high-
quality yet diverse images.
within our tissue image generation framework, we
introduce a novel controllable fusion block (cfb) that enables a dense aggrega-
tion of local regions of the reference tissue images based on their congruence to
those in the base tissue image.
our cfb employs a cross-attention based feature
aggregation between the base (query) and reference (keys, values) tissue image
features.
consequently, colorectal tissue images are generated with reduced artifacts.
to further enhance the diversity and quality of the generated tissue images,
we introduce a mapping network along with a controllable cross-modulated layer
normalization (cln) within our cfb.
our mapping network generates ‘meta-
weights’ that are a function of the global-level features of the reference tissue
image and the control parameters.
this enables
the cross-attended tissue image features to be re-weighted and enriched in a
controllable manner, based on the reference tissue image features and associated
control parameters.
consequently, it results in improved diversity of the tissue
images generated by our transformer-based framework (see fig. 3).
we validate our xm-gan on the fs colorectral tissue image generation task
by performing extensive qualitative, quantitative and subject specialist (pathol-
ogist) based evaluations.
our xm-gan generates realistic and diverse colorectal
tissue images (see fig.
in our subject specialist (pathologist) based evalua-
tion, pathologists could diﬀerentiate between our xm-gan generated colorec-
tral tissue images and real images only 55% time.
furthermore, we evaluate the
eﬀectiveness of our generated tissue images by using them as data augmentation
during training of fs colorectal tissue image classiﬁer, leading to an absolute gain
of 4.4% in terms of mean classiﬁcation accuracy over the vanilla fs classiﬁer.
130
a. kumar et al.
2
related work
the ability of generative models [6,15] to ﬁt to a variety of data distributions
has enabled great strides of advancement in tasks, such as image generation [3,
12,13,19], and so on.
in contrast, few-
shot (fs) image generation approaches [2,4,7,9,16] strive to generate natural
images from disjoint novel categories from the same domain as in the training.
existing fs natural image generation approaches can be broadly divided into
three categories based on transformation [1], optimization
the transformation-based approach learns to perform generalized data
augmentations to generate intra-class images from a single conditional image.
on
the other hand, optimization-based approaches typically utilize meta-learning
techniques to adapt to a diﬀerent image generation task by optimizing on a few
reference images from the novel domain.
diﬀerent from these two paradigms
that are better suited for simple image generation task, fusion-based approaches
ﬁrst aggregate latent features of reference images and then employ a decoder to
generate same class images from these aggregated features.
our approach: while the aforementioned works explore fs generation in nat-
ural images, to the best of our knowledge, we are the ﬁrst to investigate fs gener-
ation in colorectal tissue images.
generating
colorectal tissue images of these diverse categories is a challenging task, espe-
cially in the fs setting.
generating realistic and diverse tissue images require
ensuring both global and local texture consistency (patterns).
[5,20] from all relevant local regions of the reference
tissue images at a global-receptive ﬁeld along with a controllable mechanism for
modulating the tissue image features by utilizing meta-weights computed from
the input reference tissue image features.
as a result, this leads to high-quality
yet diverse colorectal tissue image generation in fs setting.
3
method
problem formulation: in our few-shot colorectal tissue image generation
framework, the goal is to generate diverse set of images from k input exam-
ples x of a unseen (novel) tissue classes.
let ds and du be the set of seen and
unseen classes, respectively, where ds ∩du = ∅. in the training stage, we sample
images from ds and train the model to learn transferable generation ability to
produce new tissue images for unseen classes.
during inference, given k images
from an unseen class in du, the trained model strives to produce diverse yet
plausible images for this unseen class without any further ﬁne-tuning.
few-shot image generation for colorectal tissue classiﬁcation
131
fig.
our xm-gan comprises a cnn encoder, a transformer-based controllable
fusion block (cfb), and a cnn decoder for tissue image generation.
for k-shot set-
ting, a shared encoder fe takes a base tissue image xb
along with k−1 reference
tissue images {xref
i
}k−1
i=1
and outputs visual features hb and {href
i
}k−1
i=1 , respectively.
the
cross-attended features fi are fused and input to a decoder fd that generates an image
ˆx.
overall architecture: figure 1 shows the overall architecture of our proposed
framework, xm-gan.
here, we randomly assign a tissue image from x as a base
image xb, and denote the remaining k−1 tissue images as reference {xref
i
}k−1
i=1 .
given the input images x, we obtain feature representation of the base tis-
sue image and each reference tissue image by passing them through the shared
encoder fe.
the resulting
fused representation f is input to a decoder fd to generate tissue image ˆx.
here, the cross-transformer
is based on multi-headed cross-attention mechanism that densely aggregates rel-
evant input image features, based on pairwise attention scores between each posi-
tion in the base tissue image with every region of the reference tissue image.
2. cross-attending the base
and reference tissue image features
using controllable cross-modulated
layer norm (cln) in our cfb.
as a result of this control-
lable feature modulation, the out-
put features fi enable the gen-
eration of tissue images that are
diverse yet aligned with the seman-
tics of the input tissue images.
next, we introduce a controllable fea-
ture modulation mechanism in our cross-
transformer to further enhance the diversity
and quality of generated images.
controllable feature modulation: the
standard cross-attention mechanism, described
above, computes locally consistent features
that generate images with reduced artifacts.
however, given the deterministic nature of the
cross-attention and the limited set of reference
images, simultaneously generating diverse and
high-quality images in the few-shot setting is
still a challenge.
to this end, we introduce
a controllable feature modulation mechanism
within our cfb that aims at improving the
diversity and quality of generated images.
the
proposed modulation incorporates stochastic-
ity as well as enhanced control in the fea-
ture aggregation and reﬁnement steps.
gref
i
is
global-level feature computed from the reference features href
i
through a linear
transformation and a global average pooling operation.
controllable cross-modulated layer normalization (cln): our cln learns
sample-dependent modulation weights for normalizing features since it is desired
few-shot image generation for colorectal tissue classiﬁcation
133
to generate images that are similar to the few-shot samples.
such a dynamic
modulation of features enables our framework to generate images of high-quality
and diversity.
here, λ(wi)
is computed as the element-wise multiplication between meta-weights wi and
sample-independent learnable weights λ ∈ rd, as λ⊙wi.
consequently, our proposed normalization mechanism
achieves a controllable modulation of the input features based on the reference
image inputs and enables enhanced diversity and quality in the generated images.
the resulting features oi are then passed through a feed-forward network (ffn)
followed by another cln for preforming point-wise feature reﬁnement, as shown
in fig.
finally, the decoder fd generates the ﬁnal image ˆx.
3.2
training and inference
training: the whole framework is trained end-to-end following the hinge ver-
sion gan [17] formulation.
(4)
additionally, to encourage the generated image ˆx to be perceptually similar
to the reference images based on the speciﬁed control parameters α, we use a
parameterized formulation of the standard perceptual loss [11], given by
lp =

i
αilp
i ,
where
lp
i = e[∥φ(ˆx)
(5)
moreover, a classiﬁcation loss lcl enforces that the images generated by the
decoder are classiﬁed into the corresponding class of the input few-shot samples.
inference: during inference, multiple high-quality and diverse images ˆx are
generated by varying the control parameter αi for a set of ﬁxed k-shot samples.
while a base image xb and αi can be randomly selected, our framework enables
a user to have control over the generation based on the choice of αi values.
134
a. kumar et al.
4
experiments
we conduct experiments on human colorectal cancer dataset [14].
to enable few-shot
setting, we split the 8 categories into 5 seen (for training) and 3 unseen categories
(for evaluation) with 40 images per category.
[8] and learned perceptual image
patch similarity (lpips)
the input and generated image size is 128 × 128.
the linear transformation
ψ(·) is implemented as a 1 × 1 convolution with input and output channels set
to d. the weights ηp and ηcl are set to 50 and 1.
we set k = 3 in all the
experiments, unless speciﬁed otherwise.
our xm-gan is trained with a batch-
size of 8 using the adam optimizer and a ﬁxed learning rate of 10−4.
4.1
state-of-the-art comparison
fs tissue image generation: in tab.
1, we compare our xm-gan approach
for fs tissue image generation with state-of-the-art lofgan
our xm-gan achieves consis-
tent gains in performance on both fid and
lpips scores, outperforming lofgan on
[14] dataset.
[7]
85.9
0.44
ours: xm-gan
55.8
0.48
low-data classiﬁcation: here, we
evaluate the applicability of the tissue
images generated by our xm-gan as
a source of data augmentation for the
downstream task of low-data colorec-
tal tissue classiﬁcation for unseen cate-
gories.
images of an unseen
class are split into 10:15:15.
then, we
augment dtr with 30 tissue images generated by our xm-gan using the same
dtr as few-shot samples for each unseen class.
table 2 shows the classiﬁcation
performance comparison.
few-shot image generation for colorectal tissue classiﬁcation
135
table 2. low-data image classi-
ﬁcation.
the proposed xm-gan
achieves superior classiﬁcation per-
formance
compared
to
recently
introduced lofgan.
3. on the left: few-shot input images of colorectal tissues.
in the middle: images
generated by lofgan.
on the right: images generated by our xm-gan. compared
to lofgan, our xm-gan generates images that are high-quality yet diverse.
additional results are provided in the supplementary material.
we conduct an additional experiment using random values of αi
s.t.
2. this is denoted
here as baseline+ppl+cln†. our approach based on the novel cfb achieves the
best performance amongst all baselines.
4.3
human evaluation study
we conducted a study with a group of ten pathologists having an average subject
experience of 8.5 years.
each pathologist is shown a random set of 20 images
136
a. kumar et al.
(10 real and 10 xm-gan generated) and asked to identify whether they are
real or generated.
the study shows that pathologists could diﬀerentiate between
the ai-generated and real images only 55% time, which is comparable with a
random prediction in a binary classiﬁcation problem, indicating the ability of
our proposed generative framework to generate realistic colorectal images.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_11.pdf:
delivering meaningful uncertainty estimates is essential for
a successful deployment of machine learning models in the clinical prac-
tice.
although many methods have been proposed to improve calibration,
no technique can match the simple, but expensive approach of train-
ing an ensemble of deep neural networks.
we show that the resulting averaged
predictions can achieve excellent calibration without sacriﬁcing accuracy
in two challenging datasets for histopathological and endoscopic image
classiﬁcation.
our experiments indicate that multi-head multi-loss clas-
siﬁers are inherently well-calibrated, outperforming other recent cali-
bration techniques and even challenging deep ensembles’ performance.
code to reproduce our experiments can be found at https://github.com/
agaldran/mhml_calibration.
keywords: model calibration · uncertainty quantiﬁcation
1
introduction and related work
when training supervised computer vision models, we typically focus on improv-
ing their predictive performance, yet equally important for safety-critical tasks
is their ability to express meaningful uncertainties about their own predictions
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43898-1_11.
nonetheless, if we ask
the model about a colon biopsy with ambiguous visual content, i.e. a hard-to-
diagnose image, then it could express aleatoric uncertainty, as it may not know
how to solve the problem, but the ambiguity comes from the data.
broadly speaking, one can attempt to promote calibration during training,
by means of a post-processing stage, or by model ensembling.
training-time calibration.
these techniques often rely on correctly tuning a hyper-parameter controlling the
trade-oﬀ between discrimination ability and conﬁdence, and can easily achieve
better calibration at the expense of decreasing predictive performance [22].
examples of medical image analysis works adopting this approach are diﬀerence
between conﬁdence and accuracy regularization [20] for medical image diag-
nosis, or spatially-varying and margin-based label smoothing [14,27], which
extend and improve label smoothing for biomedical image segmentation tasks.
their
greatest shortcoming is the dependence on the i.i.d. assumption implicitly made
when using validation data to learn the mapping: these approaches suﬀer to gen-
eralize to unseen data [28]. other than that, these techniques can be combined
with training-time methods and return compounded performance improvements.
an obvious weakness of deep ensembles is the requirement of
training and then keeping for inference purposes a set of models, which results
in a computational overhead that can be considerable for larger architectures.
examples of applying ensembling in medical image computing include [17,24].
in this work we achieve model calibration by means of multi-head models
trained with diverse loss functions.
detailed derivations of all the results below are
provided in the online supplementary materials.
2.1
multi-head ensemble diversity
consider a k-class classiﬁcation problem, and a neural network uθ taking an
image x and mapping it onto a representation uθ(x) ∈ rn, which is linearly
transformed by f into a logits vector z = f(uθ(x))
we now wish to implement a multi-head ensemble model like the one shown
in fig.
the resulting probability vectors pm = σ(zm) are then averaged to obtain a ﬁnal
prediction pμ = (1/m) 
m pm.
an image x goes through a
neural network uθ
the ﬁnal loss lmh is the
sum of per-head weighted-ce losses lω m-ce(pm, y) and the ce loss lce(pμ, y) of the
average prediction pμ = μ(p1, ..., pm).
as a consequence, diversity in the predictions that make up the
output pμ of the network would be damaged.
the total loss of the complete model is the addition of the per-head
losses and the overall loss acting on the average prediction:
lmh(p, y) = lce(pμ, y) +
m

m=1
lω m-ce(pm, y),
(2)
where p = (p1, ..., pm) is an array collecting all the predictions the network
makes.
this
can be quantiﬁed by the expected calibration error (ece), given by:
ece =
n

s=1
|bs|
n |acc(bs) − conf(bs)|,
(5)
where 
s bs form a uniform partition of the unit interval, and acc(bs), conf(bs)
are accuracy and average conﬁdence (maximum softmax value) for test samples
predicted with conﬁdence in bs.
in practice, the ece alone is not a good measure in terms of practical usabil-
ity, as one can have a perfectly ece-calibrated model with no predictive power
[29].
finally, we show as summary
metric the average rank when aggregating rankings of ece, nll, and accuracy.
3
experimental results
we now describe the data we used for experimentation, carefully analyze per-
formance for each dataset, and end up with a discussion of our ﬁndings.
3.1
datasets and architectures
we conducted experiments on two datasets: 1) the chaoyang dataset1, which
contains colon histopathology images.
it has 6,160 images unevenly distributed in
4 classes (29%, 19%, 37%, 15%), with some amount of label ambiguity, reﬂecting
high aleatoric uncertainty.
2) kvasir2, a dataset for the task of endoscopic image
classiﬁcation.
the annotated part of this dataset contains 10,662 images, and it
represents a challenging classiﬁcation problem due a high amount of classes (23)
and highly imbalanced class frequencies [2].
for the sake of readability we do not
show measures of dispersion, but we add them to the supplementary material
(appendix b), together with further experiments on other datasets.
we implement the proposed approach by optimizing several popular neural
network architectures, namely a common resnet50 and two more recent mod-
els: a convnext [23] and a swin-transformer [23].
code to reproduce our results
and hyperparameter speciﬁcations are shared at https://github.com/agaldran/
mhml_calibration.
3.2
performance analysis
notation: we train three diﬀerent multi-head classiﬁers: 1) a 2-head model
where each head optimizes for standard (unweighted) ce, referred to as 2hsl
(2 heads-single loss); 2) a 2-head model but with each head minimizing a
diﬀerently weighed ce loss as described in sect.
we also show the performance of deep ensembles (d-ens
we also expect
to achieve good calibration without sacriﬁcing predictive performance
(high accuracy).
finally we would ideally
observe improved performance as we increase the diversity (comparing
2hsl to 2hml) and as we add heads (comparing 2hml to 4hml).
note that models minimizing the dca loss do manage to bring
the ece down, although by giving up accuracy.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_39.pdf:
a predominant source of failures in image classiﬁca-
tion is distribution shifts between training data and deployment data.
on the basis of various examples,
we demonstrate how this tool can help researchers gain insight into the
requirements for safe application of classiﬁcation systems in the medi-
cal domain.
a primary reason is the lack of reliability, i.e. failure cases produced
by the system, which predominantly occur when deployment data diﬀers from
the data it was trained on, a phenomenon known as distribution shifts.
in med-
ical applications, these shifts can be caused by image corruption (“corruption
shift”), unseen variants of pathologies (“manifestation shift”), or deployment in
new clinical sites with diﬀerent scanners and protocols (“acquisition shift”)
the robustness of a classiﬁer, i.e. its ability to generalize across these shifts, is
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43898-1 39.
https://doi.org/10.1007/978-3-031-43898-1_39
silent failures in medical image classiﬁcation
401
fig.
note that true/false positives/negatives (t/f p/n) do not
refer to the classiﬁer decision, but to the failure detection outcome, i.e.
the assessment of the csf.
b) sf-visuals allows to identify and analyze silent failures in a dataset
based on an interactive scatter plot in the classiﬁer’s latent space (each dot repre-
sents one image, which is displayed when selecting the dot).
[3] studied failure detection on several biomedical datasets,
but only assessed the performance of csfs in isolation without considering the
classiﬁer’s ability to prevent failures.
speciﬁcally, we introduce corruptions of various intensity
levels to the images in four datasets in the form of brightness, motion blur, elas-
tic transformations and gaussian noise.
we further simulate acquisition shifts
and manifestation shifts by splitting the data into “source domain” (development
data) and “target domain” (deployment data) according to sub-class informa-
tion from the meta-data such as lesion subtypes or clinical sites.
we emulate two acquisition shifts by deﬁning either images
from the memorial sloan kettering cancer center (mskcc) or hospital clinic
barcelona (hcb) as the target domain and the remaining images as the source
silent failures in medical image classiﬁcation
403
domain.
further, a manifestation shift is designed by deﬁning the lesion sub-
types “keratosis-like” (benign) and “actinic keratosis” (malignant) as the tar-
get domain.
since the images
were acquired in 51 deviating acquisition steps, we deﬁne 10 of these batches as
target-domain to emulate an acquisition shift.
average malignancy ratings (four raters
per nodule, scores between 1 and 5) > 2 are considered malignant and all others
as benign.
we emulate two manifestation shifts by deﬁning nodules with high
spiculation (rating > 2), and low texture (rating < 3) as target domains.
we ﬁrst reduce the dimensionality of the classiﬁer’s
latent space to 50 using principal component analysis and use t-sne to obtain
the ﬁnal 3-dimensional embedding.
the associated images are displayed
upon selection of a dot to establish a direct visual link between input space
and embedding.
therefore,
k-means clustering is applied to the 3-dimensional embedding.
nine clusters are
identiﬁed per concept and the resulting plots show the closest-to-center image
per cluster as a visual representation of the concept.
2. we sort all failures by the classiﬁer conﬁdence and
by default show the images associated with the top-two most conﬁdent failures.
for corruption shifts, we further allow investigating the predictions on a ﬁxed
input image over varying intensity levels.
3
experimental setup
evaluating silent failure prevention: we follow jaeger et al.
the area
404
t. j. bungert et al.
under the risk-coverage curve aurc reﬂects this task, since it considers both the
classiﬁer’s accuracy as well as the csf’s ability to detect failures by assigning
low conﬁdence scores.
thus, it can be interpreted as a silent failure rate or the
error rate averaged over steps of ﬁltering cases one by one according to their rank
of conﬁdence score (low to high).
exemplary risk-coverage curves are shown in
appendix fig.
however, the method is not
reliable across all settings, falling short on manifestation shifts and corruptions
on the lung nodule ct dataset.
silent failures in medical image classiﬁcation
405
table 1.
all values denote an average of three runs.
“cor” denotes the average over all corruption types and intensities levels.
similarly,
“acq”/“man” denote averages over all acquisition/manifestation shifts per dataset.
results with further metrics are
reported in appendix table 2
dataset
chest x-ray
dermoscopy
fc-microscopy
lung nodule ct
study
iid
cor
acq
iid
cor
acq
man
iid
cor
acq
iid
cor
man
msr
15.3
when looking beyond the averages displayed in table 1 and ana-
lyzing the results of individual clinical centers, corruptions and manifestation
shifts, one remarkable pattern can be observed: in various cases, the same csf
showed opposing behavior between two variants of the same shift on the same
dataset.
on the chest x-ray dataset, mcd worsens the performance for darkening cor-
ruptions across all csfs and intensity levels, whereas the opposite is observed
for brightening corruptions.
further, on the lung nodule ct dataset, dg-mcd-
res performs best on bright/dark corruptions and the spiculation manifestation
shift, but worst on noise corruption and falls behind on the texture manifestation
shift.
figure 1c provides a concept
cluster plot that visually conﬁrms how some of these lesions (purple dot) share
characteristics of the benign cluster of the source domain (turquoise dot), such
silent failures in medical image classiﬁcation
407
as being smaller, brighter, and rounder compared to malignant source-lesions
(blue dot).
further towards the
cluster boundary, the ambiguity in images seems to increase, as the csf is able
to detect the failures (light blue layer of dots).
in both examples, the
brightening of the image leads to a malignant lesion taking on benign character-
istics (brighter and smoother skin on the dermoscopy data, decreased contrast
between lesion and background on the lung nodule ct data).
this example shows how the tool allows the
comparison of csfs and can help to identify failure modes speciﬁc to each csf.
manifestation shift: on the dermoscopy data (fig.
2g), we see how a mani-
festation shift can cause silent failures.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_62.pdf:
the medical imaging community generates a wealth of data-
sets, many of which are openly accessible and annotated for speciﬁc
diseases and tasks such as multi-organ or lesion segmentation.
we propose multitalent, a method
that leverages multiple ct datasets with diverse and conﬂicting class
deﬁnitions to train a single model for a comprehensive structure seg-
mentation.
our results demonstrate improved segmentation performance
compared to previous related approaches, systematically, also compared
to single-dataset training using state-of-the-art methods, especially for
lesion segmentation and other challenging structures.
we show that mul-
titalent also represents a powerful foundation model that oﬀers a supe-
rior pre-training for various segmentation tasks compared to commonly
used supervised or unsupervised pre-training baselines.
our ﬁndings oﬀer
a new direction for the medical imaging community to eﬀectively uti-
lize the wealth of available data for improved segmentation performance.
keywords: medical image segmentation ·
multitask learning ·
transfer learning · foundation model · partially labeled datasets
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43898-1_62.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
https://doi.org/10.1007/978-3-031-43898-1_62
multitalent: a multi-dataset approach to medical image segmentation
649
1
introduction
the success of deep neural networks heavily relies on the availability of large
and diverse annotated datasets across a range of computer vision tasks.
to learn
a strong data representation for robust and performant medical image segmen-
tation, huge datasets with either many thousands of annotated data structures
or less speciﬁc self-supervised pretraining objectives with unlabeled data are
needed
the annotation of 3d medical images is a diﬃcult and labo-
rious task.
thus, depending on the task, only a bare minimum of images and
target structures is usually annotated.
recent eﬀorts have
resulted in a large dataset of >1000 ct images with >100 annotated classes
each, thus providing more than 100,000 manual annotations which can be used
for pre-training [30].
focusing on such a dataset prevents leveraging the poten-
tially precious additional information of the above mentioned other datasets that
are only partially annotated.
integrating information across diﬀerent datasets
potentially yields a higher variety in image acquisition protocols, more anatom-
ical target structures or details about them as well as information on diﬀerent
kinds of pathologies.
in
contrast to dataset 11, in dataset 7 the aorta is also annotated in the lower abdomen.
[5,27] and penalizing overlapping predictions
by taking advantage of the fact that organs are mutually exclusive [7,28].
some
other methods only predicted one structure of interest for each forward pass by
incorporating the class information at diﬀerent stages of the network [4,22,31].
however,
most approaches are primarily geared towards multi-organ segmentation as they
do not support overlapping target structures, like vessels or cancer classes within
an organ [6,8,12,23].
so far, all previous methods do not convincingly leverage
cross-dataset synergies.
as liu et al. pointed out, one common caveat is that
many methods force the resulting model to average between distinct annota-
tion protocol characteristics [22] by combining labels from diﬀerent datasets for
the same target structure (visualized in fig.
hence, they all fail to reach
segmentation performance on par with cutting-edge single dataset segmentation
methods.
multitalent can be used in two scenarios: first, in a combined multi-dataset
(md) training to generate one foundation segmentation model that is able to
predict all classes that are present in any of the utilized partially annotated
datasets, and second, for pre-training to leverage the learned representation of
this foundation model for a new task.
in experiments with a large collection
of abdominal ct datasets, the proposed model outperformed state-of-the-art
segmentation networks that were trained on each dataset individually as well
as all previous methods that incorporated multiple datasets for training.
additionally, at the example of three challenging datasets, we demonstrate
that ﬁne-tuning multitalent yields higher segmentation performance than train-
ing from scratch or initializing the model parameters using unsupervised pre-
training strategies [29,33].
2
methods
we introduce multitalent, a multi dataset learning and pre-training method,
to train a foundation medical image segmentation model.
it comes with a novel
multitalent: a multi-dataset approach to medical image segmentation
651
dataset and class adaptive loss function.
[1, k], with n (k)
image and label pairs d(k) = {(x, y)(k)
1 , ..., (x, y)(k)
n (k)}.
in these datasets, every
image voxel x(k)
i
, i
to solve the label contradiction problem we decou-
ple the segmentation outputs for each class by applying a sigmoid activation
function instead of the commonly used softmax activation function across the
dataset.
but it has indepen-
dent segmentation head parameters θc for each class.
consequently, the
segmentation of each class can be thought of as a binary segmentation task.
based on the well established
combination of a cross-entropy and dice loss for single dataset medical image
segmentation, we employ the binary binary cross-entropy loss (bce) and a
modiﬁed dice loss for each class over all b, b ∈
[1, b], images in a batch:
i,b,c) −
2 
b,i ˆy(k)
i,b,c y(k)
i,b,c

b,i ˆy(k)
i,b,c + 
b,i y(k)
i,b,c
(1)
while the regular dice loss is calculated for each image within a batch, we cal-
culate the dice loss jointly for all images of the input batch.
this regularizes the
loss if only a few voxels of one class are present in one image and a larger area
is present in another image of the same batch.
thus, an inaccurate prediction of
a few pixels in the ﬁrst image has a limited eﬀect on the loss.
in the following,
652
c. ulrich et al.
we unite the sum over the image voxels i and the batch b to 
z.
to demonstrate the general applicability of this app-
roach, we applied it to three segmentation networks.
we employed a 3d u-net
[24], an extension with additional residual blocks in the encoder (resenc u-net),
that demonstrated highly competitive results in previous medical image segmen-
tation challenges [14,15] and a recently proposed transformer based architecture
(swinunetr [29]).
we implemented our approach in the nnu-net framework
[13].
however, the automatic pipeline conﬁguration from nnu-net was not used
in favor of a manually deﬁned conﬁguration that aims to reﬂect the peculiarities
of each of the datasets, irrespective of the number of training cases they contain.
we manually selected a patch size of [96, 192, 192] and image spacing of 1mm in
plane and 1.5mm for the axial slice thickness, which nnu-net used to automati-
cally create the two cnn network topologies.
we trained multitalent with 13 public
abdominal ct datasets with a total of 1477 3d images, including 47 classes
(multi-dataset (md) collection)
we increased the batch size to 4 and
the number of training epochs to 2000 to account for the high number of train-
ing images.
to compensate for the varying number of training images in each
dataset, we choose a sampling probability per case that is inversely proportional
to √n, where n is the number of training cases in the corresponding source
dataset.
naturally, the target datasets were excluded
multitalent: a multi-dataset approach to medical image segmentation
653
from the respective pre-training.
first, the
segmentation heads were warmed up over 10 epochs with linearly increasing
learning rate, followed by a whole-network warm-up over 50 epochs.
2.3
baselines
as a baseline for the multitalent, we applied the 3d u-net generated by the
nnu-net without manual intervention to each dataset individually.
all
baseline networks were also implemented within the nnu-net framework and
follow the default training procedure.
as supervised baseline,
we used the weights resulting from training the three model architectures on
the totalsegmentator dataset, which consists of 1204 images and 104 classes
we used the same patch
size, image spacing, batch size and number of epochs as for the multitalent
training.
finally, for the swinunetr architecture, we compared the utility
of the weights from our multitalent with the ones provided by tan et al. who
performed self-supervised pre-training on 5050 ct images.
implementation of swinunetr because the rec-
ommended settings for ﬁne tuning were used.
multitalent improves the performance of the purely convolutional architectures
(u-net and resenc u-net) and outperforms the corresponding baseline models
that were trained on each dataset individually.
since a simple average over all
classes would introduce a biased perception due to the highly varying numbers
of images and classes, we additionally report an average over all datasets.
for
example, dataset 7 consists of only 30 training images but has 13 classes, whereas
654
c. ulrich et al.
fig.
dataset 6 has 126 training images but only 1 class.
averaged over all datasets, the multitalent gains
1.26 dice points for the resenc u-net architecture and 1.05 dice points for the u-
net architecture.
compared to the default nnu-net, conﬁgured without manual
intervention for each dataset, the improvements are 1.56 and 0.84 dice points.
both class groups, but espe-
cially the cancer classes, experience notable performance improvements from
multitalent.
the
advantages of multitalent include not only better segmentation results, but also
considerable time savings for training and inference due to the simultaneous pre-
diction of all classes.
although multitalent was trained with a substantially lower amount of manually
multitalent: a multi-dataset approach to medical image segmentation
655
annotated structures (˜3600 vs. ˜105 annotations), it also exceeds the supervised
pre-training baseline.
especially for the small multi-organ dataset, which only
has 30 training images (btcv), and for the kidney tumor (kits19), the multi-
talent pre-training boosts the segmentation results.
in general, the results show
that supervised pre-training can be beneﬁcial for the swinunetr as well, but
pre-training on the large totalsegmentator dataset works better than the md
pre-training.
for the amos dataset, no pre-training scheme has a substantial
impact on the performance.
* indicates usage of multiple datasets.
[32]
single model
84.97
18.47
multitalent resenc u-net*
single model
88.82
16.35
multitalent resenc u-net*
5-fold ensemble 88.91
14.68
resenc u-net (pre-trained multitalent*) 5-fold ensemble 89.07
15.01
4
discussion
multitalent demonstrates the remarkable potential of utilizing multiple pub-
licly available partially labeled datasets to train a foundation medical segmen-
tation network, that is highly beneﬁcial for pre-training and ﬁnetuning various
segmentation tasks.
furthermore, multitalent takes less time for training and inference, sav-
ing resources compared to training many single dataset models.
in the transfer
learning setting, the feature representations learned by multitalent boost seg-
mentation performance and set a new state-of-the-art on the btcv leaderboard.
swinunetr implementation and the provided
self-supervised weights as additional baseline
implement self-supervised [29]
74.71
0.30
86.11
0.20
87.62
0.55 43.64
0.97
swinunetr
from scratch
81.44
87.59
95.97
76.52
supervised (˜105 annot.)
84.92
0.03
89.81
0.16
96.89
0.04 84.01
0.12
allows including any publicly available datasets (e.g. amos and totalsegmen-
tator).
this paves the way towards holistic whole body segmentation model that
is even capable of handling pathologies.
acknowledgements.
references
1. antonelli, m., et al.: the medical segmentation decathlon.
med3d: transfer learning for 3d medical image anal-
ysis.
imaging 26, 1045–1057 (2013)
4. dmitriev, k., kaufman, a.e.: learning multi-class segmentations from single-class
datasets.
multi-organ segmentation over partially labeled datasets with
multi-scale feature abstraction.
ms-kd: multi-organ segmen-
tation with multiple binary-labeled datasets.
learning
from partially overlapping labels: image segmentation under annotation shift.
https://doi.org/10.1007/978-3-030-87722-4_12
multitalent: a multi-dataset approach to medical image segmentation
657
9.
gibson, e., et al.: automatic multi-organ segmentation on abdominal ct with
dense v-networks.
unetr: transformers for 3d medical image segmentation.
heller, n., et al.: the kits19 challenge data: 300 kidney tumor cases with clini-
cal context, ct semantic segmentations, and surgical outcomes.
huang, r., zheng, y., hu, z., zhang, s., li, h.: multi-organ segmentation via co-
training weight-averaged models from few-organ datasets.
nnu-net:
a self-conﬁguring method for deep learning-based biomedical image segmentation.
ji, y., et al.: amos: a large-scale abdominal multi-organ benchmark for versatile
medical image segmentation.
segthor: segmentation of tho-
racic organs at risk in ct images.
landman, b., xu, z., igelsias, j.e., styner, m., langerak, t., klein, a.: mic-
li, h., zhou, j., deng, j., chen, m.: automatic structure segmentation for radio-
therapy planning challenge (2019).
li, s., wang, h., meng, y., zhang, c., song, z.: multi-organ segmentation: a
progressive exploration of learning paradigms under scarce annotation (2023)
22.
liu, j., et al.: clip-driven universal model for organ segmentation and tumor detec-
tion.
liu, p., zheng, g.: context-aware voxel-wise contrastive learning for label eﬃcient
multi-organ segmentation.
medical image computing and computer assisted intervention – miccai
2022.
ronneberger, o., fischer, p., brox, t.: u-net: convolutional networks for biomed-
ical image segmentation.
roth, h.r., et al.: deeporgan: multi-level deep convolutional networks for auto-
mated pancreas segmentation.
roth, h.r., et al.: deeporgan: multi-level deep convolutional networks for auto-
mated pancreas segmentation.
roulet, n., slezak, d.f., ferrante, e.: joint learning of brain lesion and anatomy
segmentation from heterogeneous datasets.
in: proceedings of the 2nd interna-
tional conference on medical imaging with deep learning (2019)
28. shi, g., xiao, l., chen, y., zhou, s.k.: marginal loss and exclusion loss for partially
supervised multi-organ segmentation.
image anal.
tang, y., et al.: self-supervised pre-training of swin transformers for 3d medi-
cal image analysis.
30. wasserthal, j., meyer, m., breit, h.c., cyriac, j., yang, s., segeroth, m.:
totalsegmentator: robust segmentation of 104 anatomical structures in ct images.
arxiv:2208.05868 (2022)
31.
dodnet: learning to segment multi-organ and
tumors from multiple partially labeled datasets.
prior-aware neural network for partially-supervised multi-organ
segmentation.
image
anal.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_61.pdf:
interactive segmentation reduces the annotation time of
medical images and allows annotators to iteratively reﬁne labels with
corrective interactions, such as clicks.
while existing interactive mod-
els transform clicks into user guidance signals, which are combined with
images to form (image, guidance) pairs, the question of how to best
represent the guidance has not been fully explored.
we conduct
our study on the msd spleen and the autopet datasets to explore the
segmentation of both anatomy (spleen) and pathology (tumor lesions).
our results show that choosing the guidance signal is crucial for inter-
active segmentation as we improve the performance by 14% dice with
our adaptive heatmaps on the challenging autopet dataset when com-
pared to non-interactive models.
this brings interactive models one step
closer to deployment in clinical workﬂows.
code: https://github.com/
zrrr1997/guiding-the-guidance/.
keywords: interactive segmentation · comparative study · click
guidance
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43898-1 61.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. (eds.): miccai 2023, lncs 14222, pp.
https://doi.org/10.1007/978-3-031-43898-1_61
638
z. marinov et al.
1
introduction
deep learning models have achieved remarkable success in segmenting anatomy
and lesions from medical images but often rely on large-scale manually annotated
datasets [1–3].
interactive segmen-
tation models address this issue by utilizing weak labels, such as clicks, instead
of voxelwise annotations [5–7].
the clicks are transformed into guidance sig-
nals, e.g., gaussian heatmaps or euclidean/geodesic distance maps, and used
together with the image as a joint input for the interactive model.
annotators
can make additional clicks in missegmented areas to iteratively reﬁne the seg-
mentation mask, which often signiﬁcantly improves the prediction compared to
non-interactive models [4,13].
2. we introduce 5 guidance evaluation metrics (m1)-(m5), which evaluate the
performance, eﬃciency, and ability to improve with new clicks.
our adaptive heatmaps mitigate the weaknesses of the 5 guidances
and achieve the best performance on autopet [1] and msd spleen [2].
however, neither of them explore diﬀerent parameter settings for each
guidance and both work with natural 2d images.
however, they use only initial clicks and
do not add iterative corrective clicks to reﬁne the segmentation.
σi = ⌊ae−bx⌋, where x =
1
|nci|

v∈nci
gdt(v, c)
(6)
here, nci is the 9-neighborhood of ci, a = 13 limits the maximum radius to
13, and b = 0.15 is set empirically1 (details in supplementary).
for each volume, n clicks
are iteratively sampled from over- and undersegmented predictions of the model
as in [16] and represented as foreground and background guidance signals.
we
implemented our experiments with monai label [23] and will release our code.
msd spleen [2] contains 41 ct vol-
umes with voxel size 0.79×0.79×5.00mm3 and average resolution of 512×512×89
voxels with dense annotations of the spleen.
we also only use pet data for our experiments.
the pet volumes have a voxel
size of 2.0 × 2.0 × 2.0mm3 and an average resolution of 400 × 400 × 352 voxels.
2.3
hyperparameters: experiments
we keep these parameters constant for all models: learning rate = 10−5, #clicks
n = 10, dice cross-entropy loss [24], and a ﬁxed 80–20 training-validation split
(dtrain/dval).
we apply the same data augmentation transforms to all models
and simulate clicks as proposed in sakinis et al.
(1) and (2)
and also explore how this parameter inﬂuences the performance of the distance-
based signals in eq.
(3)–(5) aﬀects the performance.
unlike mideepseg [5], we compute the θ threshold for
each image individually, as ﬁxed thresholds may not be suitable for all images.
guiding the guidance: a comparative analysis of user guidance signals
641
table 1. variation of hyperparameters (h1) – (h4) in our experiments.
we randomly decide for each volume
whether to add the n clicks or not, with a probability of p, in order to make the
model more independent of interactions and improve its initial segmentation.
2.4
additional evaluation metrics
we use 5 metrics (m1)–(m5) (table 2) to evaluate the validation performance.
a higher initial
dice indicates less work for the annotator
(m3) eﬃciency
inverted∗ time measurement (1 − t) in seconds needed to compute the
guidance.
∗our maximum measurement tmax is shorter than 1 second
(m4) consistent
improve-
ment
ratio of clicks c+ that improve the dice score to the total number of
validation clicks:
|c+|
n·|dval|, where n = 10 and dval is the validation dataset
(m5) ground-
truth
overlap
overlap of the guidance g with the ground-truth mask m: |m∩g|
|g|
.
1b) show that on msd spleen [2], the highest
dice scores are at σ = 5, with a slight improvement for two samples at σ = 1,
but performance decreases for higher values σ > 5.
on autopet [1], σ = 5 and
two samples with σ = 0 show the best performance, while higher values again
demonstrate a signiﬁcant performance drop.
geodesic
maps exhibit lower dice scores for small σ < 5 and achieve the best performance
for σ = 5 on both datasets.
figure 1a) shows that
the highest ﬁnal dice scores are achieved with θ = 10 for msd spleen [2].
on autopet [1], the scores are relatively similar when varying θ with a slight
improvement at θ
that not truncating values on msd
spleen [2], i.e. θ = 0, leads to a sharp drop in performance.
for our next experiments, we ﬁx the optimal (σ, θ) pair for each of the ﬁve
guidances (see table 3) and train a deepedit
1e) indicate that the best performance is achieved by
simply concatenating the guidance signal with the input volume.
this holds true
for both datasets and the diﬀerence in performance is substantial.
figure 1e) shows that p ∈ {75%, 100%}
results in the best performance on msd spleen [2], with a faster convergence
rate for p = 75%.
however, with p = 50%, the performance is worse than the
non-interactive baseline (p = 0%).
for the rest of our
experiments, we use the optimal hyperparameters for each guidance in table 1.
3.2
additional evaluation metrics: results
the comparison of the guidance signals using our ﬁve metrics (m1)–(m5)
can be seen in fig.
although the concrete values for msd spleen [2] and
644
z. marinov et al.
0.2
0.4
0.6
0.8
initial
dice
final
dice
efficiency
consistent
improvement
overlap
with gt
exp.
geodesic maps
geodesic maps
euclidean maps
disks
heatmaps
adaptive heatmaps (ours)
0.96
0.95
0.95
0.95
0.95
0.95
0.80
0.65
0.52
0.42
0.39
0.37
0.81
0.73
0.73
0.72
0.66
0.63
0.96
0.95
0.89
0.81
0.22
0.21
0.90
0.89
0.86
0.85
0.83
0.80
0.31
0.25
0.20
0.17
0.04
0.01
0.64
0.61
0.46
0.43
0.42
0.20
msd spleen
autopet
0.2
0.4
0.8
initial
dice
overlap
with gt
consistent
improvement
0.6
final
dice
efficiency
0.61
0.60
0.56
0.47
0.47
0.43
0.78
0.68
0.62
0.60
0.60
0.59
0.79
0.78
0.78
0.75
0.73
0.73
fig.
(m3) consistent improvement.
the consistent improvement is ≈ 65%
for both datasets, but it is slightly worse for autopet [1] as it is more chal-
lenging.
heatmaps and disks achieve the most consistent improvement, which
means they are more precise in correcting errors.
these changes may confuse the model and lead to inconsistent improvement.
this results in substantially
higher consistent improvement and overlap with ground truth and the best initial
and ﬁnal dice (table 3).
thus, our comparative study has led to the creation
of a more consistent and ﬂexible signal with a slight performance boost, albeit
with an eﬃciency cost due to the need to compute both gdt and heatmaps.
4
conclusion
our comparative experiments yield insights into tuning existing guiding signals
and designing new ones.
weaknesses in existing signals include overly large
radiuses near edges and inconsistent improvement for geodesic-based signals that
change with each click.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_48.pdf:
breast lesion segmentation in ultrasound (us) videos is essen-
tial for diagnosing and treating axillary lymph node metastasis.
to overcome this issue, we meticulously curated a
us video breast lesion segmentation dataset comprising 572 videos and
34,300 annotated frames, covering a wide range of realistic clinical sce-
narios.
furthermore, we propose a novel frequency and localization fea-
ture aggregation network (fla-net) that learns temporal features from
the frequency domain and predicts additional lesion location positions to
assist with breast lesion segmentation.
our experiments on
our annotated dataset and two public video polyp segmentation datasets
demonstrate that our proposed fla-net achieves state-of-the-art perfor-
mance in breast lesion segmentation in us videos and video polyp segmen-
tation while signiﬁcantly reducing time and space complexity.
keywords: ultrasound video · breast lesion · segmentation
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43898-1_48.
early detection and timely treatment are crucial for improving outcomes and
reducing the risk of recurrence.
in breast cancer diagnosis, accurately segmenting
breast lesions in ultrasound (us) videos is an essential step for computer-aided
diagnosis systems, as well as breast cancer diagnosis and treatment.
bbox: whether provide segmentation mask
annotation.
[12] 2022 188
25,272
✓
×
✓
×
ours
2023 572
34,300 ✓
✓
✓
✓
the work presented in [10] proposed the ﬁrst pixel-wise annotated benchmark
dataset for breast lesion segmentation in us videos, but it has some limitations.
although their eﬀorts were commendable, this dataset is private and contains
only 63 videos with 4,619 annotated frames.
in this work, we
collected a larger-scale us video breast lesion segmentation dataset
with 572 videos and 34,300 annotated frames, of which 222 videos contain aln
metastasis, covering a wide range of realistic clinical scenarios.
although the existing benchmark method dpstt [10] has shown promis-
ing results for breast lesion segmentation in us videos, it only uses the ultra-
sound image to read memory for learning temporal features.
however, ultrasound
images suﬀer from speckle noise, weak boundaries, and low image quality.
thus,
there is still considerable room for improvement in ultrasound video breast lesion
segmentation.
to address this, we propose a novel network called fre-
quency and localization feature aggregation network (fla-net) to
improve breast lesion segmentation in ultrasound videos.
our fla-net learns
frequency-based temporal features and then uses them to predict auxiliary breast
lesion location maps to assist the segmentation of breast lesions in video frames.
additionally, we devise a contrastive loss to enhance the breast lesion location
shifting more attention to breast lesion segmentation in ultrasound videos
499
fig.
1. examples of our ultrasound video dataset for breast lesion segmentation.
the experimental results unequiv-
ocally showcase that our network surpasses state-of-the-art techniques in the
realm of both breast lesion segmentation in us videos and two video polyp
segmentation benchmark datasets (fig. 1).
2
ultrasound video breast lesion segmentation dataset
to support advancements in breast lesion segmentation and aln metastasis
prediction, we collected a dataset containing 572 breast lesion ultrasound videos
with 34,300 annotated frames.
nine experienced pathologists were invited to manually annotate breast lesions
at each video frame.
moreover, apart from the segmentation annotation, our
dataset also includes lesion bounding box labels, which enables benchmarking
breast lesion detection in ultrasound videos.
more dataset statistics are available
in the supplementary.
then frequency-based
feature aggregation module is then used to aggregate these features and the aggre-
gated feature map is then passed into our two-branch decoder to predict the breast
lesion segmentation mask of it, and a lesion localization heatmap.
features ot of the ffa module into two decoder branches (similar to the unet
decoder [14]): one is the localization branch to predict the localization map of
the breast lesions, while another segmentation branch integrates the features of
the localization branch to fuse localization feature for segmenting breast lesions.
note that the current spectral fea-
tures ( ˆft, ˆft−1, and ˆft−2) are complex numbers and incompatible with the neural
shifting more attention to breast lesion segmentation in ultrasound videos
501
layers.
therefore we concatenate the real and imaginary parts of these com-
plex numbers along the channel dimension respectively and thus obtain three
new tensors (xt ∈ r2c×h×w, xt−1 ∈ r2c×h×w, and xt−2 ∈ r2c×h×w) with dou-
ble channels.
then, we element-wise multiply the obtained atten-
tion map from each group with the input features, and the multiplication results
(see y1 and y2) are then transformed into complex numbers by splitting them
into real and imaginary parts along the channel dimension.
finally, we further element-wisely add z1 and z2 and then pass it
into a “bconv” layer to obtain the output feature ot of our ffa module.
math-
ematically, ot is computed by ot = bconv(z1 + z2), where “bconv” contains a
3 × 3 convolution layer, a group normalization, and a relu activation function.
3.2
two-branch decoder
after obtaining the frequency features, we introduce a two-branch decoder con-
sisting of a segmentation branch and a localization branch to incorporate tem-
poral features from nearby frames into the current frame.
let d1
s and d2
s
denote the features at the last two layers of the segmentation decoder branch,
and d1
l and d2
l denote the features at the last two layers of the localization decoder
branch.
then, we element-wisely add d1
l and d1
s, and element-
wisely add d2
l and d2
s, and pass the addition result into a “bconv” convolution
layer to predict the segmentation map st of the input video frame it.
to do so, we compute
a bounding box of the annotated breast lesion segmentation result, and then take
the center coordinates of the bounding box.
quantitative comparisons between our fla-net and the state-of-the-art
methods on our test set in terms of breast lesion segmentation in ultrasound videos.
method
image/video dice ↑ jaccard ↑ f1-score ↑ mae ↓
unet
[14]
image
0.745
0.636
0.777
0.043
unet++
[19]
image
0.749
0.633
0.780
0.039
transunet [4]
image
0.733
0.637
0.784
0.042
setr
[18]
image
0.709
0.588
0.748
0.045
stm
λ3liou(st, gs
t ),
(2)
where gh
t
and gs
t denote the ground truth of the breast lesion segmentation
and the breast lesion localization.
we empirically set weights λ1 = λ2 = λ3 = 1.
4
experiments and results
implementation details.
[6] on the imagenet dataset, while the remaining components
of our network were trained from scratch.
prior to inputting the training video
frames into the network, we resize them to 352×352 dimensions.
our network is
implemented in pytorch and employs the adam optimizer with a learning rate
of 5 × 10−5, trained over 100 epochs, and a batch size of 24.
shifting more attention to breast lesion segmentation in ultrasound videos
503
fig.
3. visual comparisons of breast lesion segmentation results produced by our net-
work and state-of-the-art methods.
for more visual-
ization results, please refer to the supplementary material.
quantitative comparison results of ablation study experiments.
our method
✓
✓
✓
0.789
0.687
0.815
0.033
4.1
comparisons with state-of-the-arts
we conduct a comparative analysis between our network and nine state-of-the-
art methods, comprising four image-based methods and ﬁve video-based meth-
ods.
four image-based methods are unet
to ensure a fair and equitable compar-
ison, we acquire the segmentation results of all nine compared methods by utiliz-
ing either their publicly available implementations or by implementing them our-
selves.
additionally, we retrain these networks on our dataset and ﬁne-tune their
network parameters to attain their optimal segmentation performance, enabling
accurate and meaningful comparisons.
the quantitative results of our network and the
nine compared breast lesion segmentation methods are summarized in table 2.
analysis of the results reveals that, in terms of quantitative metrics, video-based
methods generally outperform image-based methods.
[16] in terms of dice, jaccard,
and f1-score metrics, and has a superior mae performance over pns+ [9] and
dpstt [10].
quantitative comparison results on diﬀerent video polyp segmentation
datasets.
for more quantitative results please refer to the supplementary material.
figure 3 visually presents a comparison of breast
lesion segmentation results obtained from our network and three other methods
across various input video frames.
apparently, our method accurately segments
breast lesions of the input ultrasound video frames, although these target breast
lesions have varied sizes and diverse shapes in the input video frames.
the supe-
rior metric performance of “basic+fla” and “basic+lb” compared to “basic”
clearly indicates that our fla module and the localization encoder branch eﬀec-
tively enhance the breast lesion segmentation performance in ultrasound videos.
then, the superior performance of “basic+fla+lb” over “basic+fla” and
“basic+lb” demonstrate that combining our fla module and the localization
encoder branch can incur a more accurate segmentation result.
moreover, our
method has larger dice, jaccard, f1-score results and a smaller mae result than
“basic+fla+lb”, which shows that our location-based contrastive loss has its
contribution to the success of our video breast lesion segmentation method.
4.3
generalizability of our network
to further evaluate the eﬀectiveness of our fla-net, we extend its application
to the task of video polyp segmentation.
following the experimental protocol
shifting more attention to breast lesion segmentation in ultrasound videos
505
employed in a recent study on video polyp segmentation [8], we retrain our net-
work and present quantitative results on two benchmark datasets, namely cvc-
300-tv
similarly, for the cvc-612-v dataset, our method achieves improve-
ments of 0.012, 0.014, 0.019, and 0 in dice, iou, eφ, and mae scores, respec-
tively.
although our sα results (0.907 on cvc-300-tv and 0.920 on cvc-612-v)
take the 2nd rank, they are very close to the best sα results, which are 0.909
on cvc-300-tv and 0.923 on cvc-612-v. hence, the superior metric results
obtained by our network clearly demonstrate its ability to accurately segment
polyp regions more eﬀectively than state-of-the-art video polyp segmentation
methods.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_58.pdf:
accurate automated segmentation of infected regions in ct
images is crucial for predicting covid-19’s pathological stage and treat-
ment response.
although deep learning has shown promise in medical
image segmentation, the scarcity of pixel-level annotations due to their
expense and time-consuming nature limits its application in covid-19
segmentation.
in this paper, we propose utilizing large-scale unpaired chest
x-rays with classiﬁcation labels as a means of compensating for the lim-
ited availability of densely annotated ct scans, aiming to learn robust
representations for accurate covid-19 segmentation.
the encoder is built to capture optimal feature represen-
tations for both ct and x-ray images.
to facilitate information interaction
between unpaired cross-modal data, we propose the kc that introduces
a momentum-updated prototype learning strategy to condense modality-
speciﬁc knowledge.
the condensed knowledge is fed into the ki module
for interaction learning, enabling the uci to capture critical features and
relationships across modalities and enhance its representation ability for
covid-19 segmentation.
the results on the public covid-19 segmenta-
tion benchmark show that our uci with the inclusion of chest x-rays can
signiﬁcantly improve segmentation performance, outperforming advanced
segmentation approaches including nnunet, cotr, nnformer, and swin
unetr.
keywords: covid-19 segmentation · unpaired data · cross-modal
q. guan and y. xie—contributed equally to this work.
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43898-1 58.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
https://doi.org/10.1007/978-3-031-43898-1_58
604
q. guan et al.
1
introduction
the covid-19 pneumonia pandemic has posed an unprecedented global health
crisis, with lung imaging as a crucial tool for identifying and managing aﬀected
individuals [16].
the latter has been
the preferred method for detecting acute lung manifestations of the virus due to its
exceptional imaging quality and ability to produce a 3d view of the lungs.
eﬀec-
tive segmentation of covid-19 infections using ct can provide valuable insights
into the disease’s development, prediction of the pathological stage, and treatment
response beyond just screening for covid-19 cases.
however, the current method
of visual inspection by radiologists for segmentation is time-consuming, requires
specialized skills, and is unsuitable for large-scale screening.
automated segmen-
tation is crucial, but it is also challenging due to three factors: the infected regions
often vary in shape, size, and location, appear similar to surrounding tissues, and
can disperse within the lung cavity.
the success of deep convolutional neural net-
works (dcnns) in image segmentation has led researchers to apply this approach
to covid-19 segmentation using ct scans
this
limited data scale currently constrains the potential of dcnns for covid-19 seg-
mentation using ct scans.
in comparison to ct scans, 2d chest x-rays are a more accessible and cost-
eﬀective option due to their fast imaging speed, low radiation, and low cost,
especially during the early stages of the pandemic [21].
we advocate using chest x-ray datasets such as chestx-
ray and chestxr may beneﬁt covid-19 segmentation using ct scans because
of three reasons: (1) supplement limited ct data and contribute to training
a more accurate segmentation model; (2) provide large-scale chest x-rays with
labeled features, including pneumonia, thus can help the segmentation model
to recognize patterns and features speciﬁc to covid-19 infections; and (3) help
improve the generalization of the segmentation model by enabling it to learn from
diﬀerent populations and imaging facilities.
inspired by this, in this study, we
propose a new learning paradigm for covid-19 segmentation using ct scans,
involving training the segmentation model using limited ct scans with pixel-
wise annotations and unpaired chest x-ray images with image-level labels.
although “chilopod”-shaped multi-modal learning [6] has been
unpaired cross-modal interaction learning for covid-19 segmentation
605
proposed to share all cnn kernels across modalities, it is still limited when
the diﬀerent modalities have a signiﬁcant dimension gap.
second, the presence
of unpaired data, speciﬁcally ct and x-ray data, in the feature fusion/cross-
attention interaction can potentially cause the model to learn incorrect or irrel-
evant information due to the possible diﬀerences in their image distributions
and objectives, leading to reduced covid-19 segmentation accuracy.
it’s worth
noting that the method using paired multimodal data [2] is not suitable for our
application scenario, and the latest unpaired cross-modal [3] requires pixel-level
annotations for both modalities, while our method can use x-ray images with
image-level labels for training.
this paper proposes a novel unpaired cross-modal interaction (uci) learn-
ing framework for covid-19 segmentation, which aims to learn strong represen-
tations from limited dense annotated ct scans and abundant image-level anno-
tated x-ray images.
the uci framework learns representations from both seg-
mentation and classiﬁcation tasks.
it includes three main components: a multi-
modal encoder for image representations, a knowledge condensation and inter-
action module for unpaired cross-modal data, and task-speciﬁc networks.
this design enables the network to capture optimal feature represen-
tations for both ct and x-ray images while maintaining the ability to learn
shared representations between the two modalities despite dimensional diﬀer-
ences.
to address the challenge of information interaction between unpaired
cross-modal data, we introduce a momentum-updated prototype learning strat-
egy to condense modality-speciﬁc knowledge.
this strategy groups similar repre-
sentations into the same prototype and iteratively updates the prototypes with a
momentum term to capture essential information in each modality.
finally, the task-speciﬁc networks, including
the segmentation decoder and classiﬁcation head, are presented to learn from all
available labels.
the proposed uci framework has signiﬁcantly improved per-
formance on the public covid-19 segmentation benchmark [15], thanks to the
inclusion of chest x-rays.
the main contributions of this paper are three-fold: (1) we are the ﬁrst to
employ abundant x-ray images with image-level annotations to improve covid-
19 segmentation on limited ct scans, where the ct and x-ray data are unpaired
and have potential distributional diﬀerences; (2) we introduce the knowledge con-
densation and interaction module, in which the momentum-updated prototype
learning is oﬀered to concentrate modality-speciﬁc knowledge, and a knowledge-
guided interaction module is proposed to harness the learned knowledge for
boosting the representations of each modality; and (3) our experimental results
demonstrate our uci learning method’s eﬀectiveness and strong generalizability
in covid-19 segmentation and the potential for related disease screening.
2
approach
the proposed uci aims to explore eﬀective representations for covid-19 seg-
mentation by leveraging both limited dense annotated ct scans and abundant
image-level annotated x-rays.
figure 1 illustrates the three primary components
of the uci framework: a multi-modal encoder used to extract features from each
modality, the knowledge condensation and interaction module used to model
unpaired cross-modal dependencies, and task-speciﬁc heads designed for seg-
mentation and classiﬁcation purposes.
2.1
multi-modal encoder
the multi-modal encoder f(·) consists of three stages of blocks, with modality-
speciﬁc patch embedding layers and shared transformer layers in each block,
capturing modality-speciﬁc and shared patterns, which can be more robust and
discriminative across modalities.
notice that due to the dimensional gap between
ct and x-ray, we use the 2d convolution block as patch embedding for x-rays
and the 3d convolution block as patch embedding for cts.
in each stage, the
patch embedding layers down-sample the inputs and generate the sequence of
modality-speciﬁc embedded tokens.
given a ct volume xct, and a chest x-ray image xcxr, we denote the output
feature sequence of the multi-modal encoder as
f ct = f(xct; 3d)
∈ rccxr×n cxr
(1)
unpaired cross-modal interaction learning for covid-19 segmentation
607
where cct and ccxr represent the channels of ct and x-ray feature sequence.
1(a), we design a knowledge condensation
(kc) module by introducing a momentum-updated prototype learning strategy
to condensate valuable knowledge in each modality from the learned features.
then we introduce a momentum learning function to
update the prototypes with ccxr
i
, which means that the updates at each iter-
ation not only depend on the current ccxr
i
but also consider the direction and
magnitude of the previous updates, deﬁned as
pcxr
i
← λpcxr
i
+ (1 − λ)
1
ccxr
i

m∈ccxr
i
m,
(3)
where λ is the momentum factor, which controls the inﬂuence of the previous
update on the current update.
the momentum term allows prototypes to move more smoothly and
consistently towards the optimal position, even in the presence of noise or other
factors that might cause the prototypes to ﬂuctuate.
inspired
by the knowledge prototypes, ki modules boost the interaction between the two
modalities and allow for the learning of strong representations for covid-19
segmentation and x-ray classiﬁcation tasks.
2.3
task-speciﬁc networks
the outputs of the ki module are fed into two multi-task heads - one decoder
for segmentation and one prediction head for classiﬁcation respectively.
the seg-
mentation decoder has a symmetric structure with the encoder, consisting of
three stages.
in each stage, the input feature map is ﬁrst up-sampled by the
3d patch embedding layer, and then reﬁned by the stacked transformer layers.
the decoder includes a segmen-
tation head for ﬁnal prediction.
we use the deep supervision strategy by adding auxiliary segmentation losses
(i.e., the sum of the dice loss and cross-entropy loss) to the decoder at diﬀerent
scales.
3
experiment
3.1
materials
we used the public covid-19 segmentation benchmark
it is collected from two public resources [5,8] on chest ct images
available on the cancer imaging archive (tcia)
all ct images were
acquired without intravenous contrast enhancement from patients with posi-
tive reverse transcription polymerase chain reaction (rt-pcr) for sars-
cov-2.
in total, we used 199 ct images including 149 training images and 50
test images.
the chestx-
ray14 dataset comprises 112,120 x-ray images showing positive cases from 30,805
patients, encompassing 14 disease image labels pertaining to thoracic and lung
ailments.
an image may contain multiple or no labels.
unpaired cross-modal interaction learning for covid-19 segmentation
609
3.2
implementation details
for ct data, we ﬁrst truncated the hu values of each scan using the range
of [−958, 327] to ﬁlter irrelevant regions, and then normalized truncated voxel
values by subtracting 82.92 and dividing by 136.97.
we randomly cropped sub-
volumes of size 32 × 256 × 256 as the input and employed the online data aug-
mentation like [10] to diversify the ct training set.
we employ the online data argumen-
tation, including random cropping and zooming, random rotation, and horizon-
tal/vertical ﬂip, to enlarge the x-ray training dataset.
we follow the extension of
[20] for weight initialization and use the adamw optimizer [11] and empirically
set the initial learning rate to 0.0001, batch size to 2 and 32 for segmentation
and classiﬁcation, maximum iterations to 25w, momentum factor λ to 0.99, and
the number of prototypes k to 256.
to evaluate the covid-19 segmentation performance, we utilized six met-
rics, including the dice similarity coeﬃcient (dsc), intersection over union
(iou), sensitivity (sen), speciﬁcity (spe), hausdorﬀ distance (hd), and aver-
age surface distance (asd).
these metrics provide a comprehensive assessment
of the segmentation quality.
the overlap-based metrics, namely dsc, iou, sen,
and spe, range from 0 to 1, with a higher score indicating better performance.
on the other hand, hd and asd are shape distance-based metrics that mea-
sure the dissimilarity between the surfaces or boundaries of the segmentation
output and the ground truth.
for hd and asd, a lower value indicates better
segmentation results.
3.3
compared with advanced segmentation approaches
table 1 gives the performance of our models and four advanced competing
ones, including nnunet [10], cotr
[24], and swin unetr [9]
in covid-19 lesion segmentation.
this suggests that the
segmentation outcomes generated by our models are in good agreement with
the ground truth.
notably, despite chestxr being more focused on covid-19
recognition, the uci model aided by the chestx-ray14 dataset containing 80k
images performs better than the uci model using the chestxr dataset with only
16k images.
this suggests that having a larger auxiliary dataset can improve the
segmentation performance even if it is not directly related to the target task.
the results also further prove the eﬀectiveness of using a wealth of chest x-rays
to assist the covid-19 segmentation under limited cts.
this
reduction demonstrates that our segmentation results provide highly accurate
boundaries that closely match the ground-truth boundaries.
quantitative results of advanced segmentation approaches on the test set.
2. we set the maximum iterations to 8w and
use chestx-ray14 as auxiliary data for all ablation experiments.
we compare ﬁve
variants of our uci: (1) baseline: trained solely on densely annotated ct images;
(2) w/o shared encoder: replacing the multi-modal encoder with two indepen-
dent encoders, each designed to learn features from a separate modality; (3) w/o
kc: removing the prototype and using the features before kc for interaction;
(4) w/o kc & ki: only with encoder to share multi-modal information; and (5)
w/o warm-up: removing the prototype warm-up in ki.
firstly, our uci model, which jointly uses chest x-rays,
outperforms the baseline segmentation results by up to 1.69%, highlighting the
eﬀectiveness of using cheap large-scale auxiliary images.
secondly, using only a
shared encoder for multi-modal learning (uci w/o kc & ki) can still bring a
segmentation gain of 0.96%, and the multi-modal encoder outperforms building
independent modality-speciﬁc networks (uci w/o shared encoder), underscoring
the importance of shared networks.
to evaluate the impact of hyper-parameter set-
tings on covid-19 segmentation, we conducted an investigation of the number
of prototypes (k) and the number of momentum factors (λ).
figure 3 illustrates
unpaired cross-modal interaction learning for covid-19 segmentation
611
fig.
3. dice scores of uci versus left: the number of prototypes k and right the
number of momentum factors λ.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_70.pdf:
however, com-
pared to the convolutional neural network (cnn) models, it has been
observed that the vit models struggle to capture high-frequency compo-
nents of images, which can limit their ability to detect local textures and
edge information.
as abnormalities in human tissue, such as tumors and
lesions, may greatly vary in structure, texture, and shape, high-frequency
information such as texture is crucial for eﬀective semantic segmentation
tasks.
furthermore, we introduce a novel
eﬃcient enhancement multi-scale bridge that eﬀectively transfers spatial
information from the encoder to the decoder while preserving the fun-
damental features.
we demonstrate the eﬃcacy of laplacian-former on
multi-organ and skin lesion segmentation tasks with +1.87% and +0.76%
dice scores compared to sota approaches, respectively.
our implemen-
tation is publically available at github.
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43898-1 70.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
https://doi.org/10.1007/978-3-031-43898-1_70
laplacian-former
737
keywords: deep learning · texture · segmentation · laplacian
transformer
1
introduction
the recent advancements in transformer-based models have revolutionized the
ﬁeld of natural language processing and have also shown great promise in a wide
range of computer vision tasks [5].
as a notable example, the vision trans-
former (vit) model utilizes multi-head self-attention (msa) blocks to globally
model the interactions between semantic tokens created by treating local image
patches as individual elements [7].
this approach stands in contrast to cnns,
which hierarchically increase their receptive ﬁeld from local to global to capture
a global semantic representation.
nevertheless, recent studies [3,20] have shown
that vit models struggle to capture high-frequency components of images, which
can limit their ability to detect local textures and it is vital for many diagnostic
and prognostic tasks.
this weakness in local representation can be attributed
to the way in which vit models process images.
vit models split an image
into a sequence of patches and model their dependencies using a self-attention
mechanism, which may not be as eﬀective as the convolution operation used in
cnn models in extracting local features within receptive ﬁelds.
this diﬀerence
in how vit and cnn process images may explain the superior performance of
cnn models in local feature extraction [1,8]. innovative approaches have been
proposed in recent years to address the insuﬃcient local texture representation
within transformer models.
one such approach is the integration of cnn and
vit features through complementary methods, aimed at seamlessly blending the
strengths of both in order to compensate for any shortcomings in local represen-
tation [5].
transformers as a complement to cnns: transunet
[11] proposed a novel solution called hiformer, which
leverages a swin transformer module and a cnn-based encoder to generate two
multi-scale feature representations, which are then integrated via a double-level
fusion module.
[10] used a transformer to create a powerful encoder
with a cnn decoder for 3d medical image segmentation.
[22],
the segmentation performance in low-resolution stages was improved.
despite
these advances, there remain some limitations in these methods such as com-
putationally ineﬃciency (e.g., transunet model), the requirement of a heavy
cnn backbone (e.g., hiformer), and the lack of consideration for multi-scale
information.
these limitations have resulted in less eﬀective network learning
results in the ﬁeld of medical image segmentation.
new attention models: the redesign of the self-attention mechanism within
pure transformer models is another method aiming to augment feature repre-
738
r. azad et al.
sentation to enhance the local feature representation ultimately.
former [24] is a pure transformer-based pipeline that comprises a double atten-
tion module to capture locally ﬁne-grained attention and interaction with dif-
ferent units in a dilated manner through its mechanism.
drawbacks of transformers: recent research has revealed that traditional
self-attention mechanisms, while eﬀective in addressing local feature discrepan-
cies, have a tendency to overlook important high-frequency information such
as texture and edge details [21].
this is especially problematic for tasks like
tumor detection, cancer-type identiﬁcation through radiomics analysis, as well
as treatment response assessment, where abnormalities often manifest in texture.
➋ we also introduce a novel eﬃcient
enhancement multi-scale bridge that eﬀectively transfers spatial information
from the encoder to the decoder while preserving the fundamental features.
➌ our method not only alleviates the problem of the traditional self-attention
mechanism mentioned above, but also it surpasses all its counterparts in terms
of diﬀerent evaluation metrics for the tasks of medical image segmentation.
1, taking an input image x ∈
rh×w ×c with spatial dimensions h and w, and c channels, it is ﬁrst passed
through a patch embedding module to obtain overlapping patch tokens of size
4 × 4 from the input image.
the proposed model comprises four encoder blocks,
each containing two eﬃcient enhancement transformer layers and a patch merg-
ing layer that downsamples the features by merging 2 × 2 patch tokens and
increasing the channel dimension.
the decoder is composed of three eﬃcient
enhancement transformer blocks and four patch-expanding blocks, followed by
a segmentation head to retrieve the ﬁnal segmentation map.
laplacian-former
then employs a novel eﬃcient enhancement multi-scale bridge to capture local
laplacian-former
739
fig.
2.1
eﬃcient enhancement transformer block
in medical imaging, it is important to distinguish diﬀerent structures and tis-
sues, especially when tissue boundaries are ill-deﬁned.
this is often the case for
accurate segmentation of small abnormalities, where high-frequency information
plays a critical role in deﬁning boundaries by capturing both textures and edges.
inspired by this, we propose an eﬃcient enhancement transformer block that
incorporates an eﬃcient frequency attention (ef-att) mechanism to capture
contextual information of an image while recalibrating the representation space
within an attention mechanism and recovering high-frequency details.
our eﬃcient enhancement transformer block ﬁrst takes a layernorm (ln)
from the input x.
it is proved in [19]
that as transformers become deeper, their features become less varied, which
restrains their representation capacity and prevents them from attaining optimal
performance.
to address this issue, we have implemented an augmented short-
740
r. azad et al.
fig.
the structure of our frequency enhancement transformer block.
cut method from [9], a diversity-enhanced shortcut (des), employing a kro-
necker decomposition-based projection.
this approach involves inserting addi-
tional paths with trainable parameters alongside the original shortcut x, which
enhances feature diversity and improves performance while requiring minimal
hardware resources.
this ﬁnal step completes
our eﬃcient enhancement transformer block, as illustrated in fig.
2.
2.2
eﬃcient frequency attention (ef-att)
the traditional self-attention block computes the attention score s using query
(q) and key (k) values, normalizes the result using softmax, and then multiplies
the normalized attention map with value (v):
s(q, k, v) = softmax

qkt
√dk

v,
(1)
where dk is the embedding dimension.
2.3
eﬃcient enhancement multi-scale bridge
it is widely known that eﬀectively integrating multi-scale information can lead
to improved performance
thus, we introduce the eﬃcient enhancement
multi-scale bridge as an alternative to simply concatenating the features from
the encoder and decoder layers.
1, deliv-
ers spatial information to each decoder layer, enabling the recovery of intricate
details while generating output segmentation masks.
in this approach, we aim
742
r. azad et al.
to calculate the eﬃcient attention mechanism for each level and fuse the multi-
scale information in their context; thus, it is important that all levels’ embedding
dimension is of the same size.
we then use a summation module to
aggregate the global context of all levels and reshape the query for matrix mul-
tiplication with the augmented global context.
taking the second level with the
dimension of h
8 × w
8 ×2c, the key and value are mapped to ( h
8
w
8 )×c, and the
query to (2 h
8
w
8 ) ×
c. the augmented global context with the shape of c × c
is then multiplied by the query, resulting in an enriched feature map with the
shape of (2 h
8
w
8 ) × c. we reshape the obtained feature map into h
8 × w
8 × 2c
and feed it through an ln and mix-ffn module with a skip connection to
empower the feature representations.
a batch size of 24 and a stochastic gradient descent
algorithm with a base learning rate of 0.05, a momentum of 0.9, and a weight
decay of 0.0001 was utilized during the training process, which was carried out
for 400 epochs.
we also followed [2] experiments to
evaluate our method on the isic 2018 skin lesion dataset
[6] with 2,694 images.
3. segmentation results of the proposed method on the synapse dataset.
synapse multi-organ segmentation: table 1 presents a comparison of our
proposal with previous sota methods using the dsc and hd metrics across
eight abdominal organs.
figure 3 illustrates a qualitative result
of our method for diﬀerent organ segmentation, speciﬁcally we can observe that
the lalacianformer produces a precise boundary segmentation on gallbladder,
liver, and stomach organs.
it is noteworthy to mention that our pipeline, as
a pure transformer-based architecture trained from scratch without pretraining
weights, outperforms all previously presented network architectures.
skin lesion segmentation: table 2a shows the comparison results of our pro-
posed method, laplacian-former, against leading methods on the skin lesion seg-
mentation benchmark.
our method achieves superior performance by utilizing the frequency attention
in a pyramid scale to model local textures.
speciﬁcally, our frequency attention
emphasizes the ﬁne details and texture characteristics that are indicative of skin
lesion structures and ampliﬁes regions with signiﬁcant intensity variations, thus
accentuating the texture patterns present in the image and resulting in better
performance.
it is evident standard
design frequency response in deep layers of structure attenuates more than the
laplacianformer, which is a visual endorsement of the capability of laplacian-
744
r. azad et al.
table
2.
(a) performance comparison of laplacian-former against the sota
approaches on isic 2018 skin lesion datset.
the supplementary
provides more visualization results.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_65.pdf:
pulmonary nodules and masses are crucial imaging features
in lung cancer screening that require careful management in clinical diag-
nosis.
despite the success of deep learning-based medical image segmen-
tation, the robust performance on various sizes of lesions of nodule and
mass is still challenging.
speciﬁcally, we introduce an adaptive scale-aware test-time click adap-
tation method based on eﬀortlessly obtainable lesion clicks as test-time
cues to enhance segmentation performance, particularly for large lesions.
extensive experiments on both open-source and in-house datasets
consistently demonstrate the eﬀectiveness of the proposed method over
some cnn and transformer-based segmentation methods.
keywords: pulmonary lesion segmentation · pulmonary mass
segmentation · test-time adaptation · multi-scale
1
introduction
lung cancer is the main cause of cancer death worldwide [18]. pulmonary nod-
ules and masses are both features present in computed tomography images that
aid in the diagnosis of lung cancer.
1. (a): visualization on results of four large-scale mass segmentation given by nnu-
net baseline
compared with the ground-truth segmentation, the recall rate for these
four samples is 46.29%, 58.34%, 79.51%, and 68.51%, respectively.
existing methods have low
recall rates for the segmentation of large scale nodules and masses.
[27] and determining follow-up treatment.
lesion
segmentation can be utilized to evaluate two important factors: the volume of
the lesion and its growth rate [5,6,8,12].
[14,17].
segmenting nodules is a tedious task that requires signiﬁcant human labor.
however, the accuracy of the 3d nodule segmentation model is prone
to signiﬁcantly decline in the application, regardless of whether its structure is
based on cnn or transformer [2].
1(a–c), the recall rate of the
large-scale nodule and mass is usually lower than the average level.
this makes
the pulmonary nodule and mass segmentation task resemble a long-tail problem
rather than a mere large scale span problem.
this leads to unsatisfactory results
when segmenting large lesions that require more accurate delineation [26].
[4], where the input images are resized to diﬀerent resolu-
test-time click adaptation for pulmonary lesion segmentation
683
tion ratios.
some other methods leverage multi-scale feature maps to capture
information from diﬀerent scales, such as cross-scale feature fusion [19] or using
multi-scale convolutional ﬁlters
furthermore, the attention mechanisms [23]
has also been utilized to emphasize the features that are more relevant for seg-
mentation.
though these methods have achieved impressive performance, they
still struggle to accurately segment the extremely imbalanced multi-scale lesions.
recently, some click-based lesion segmentation methods [19–21] introduce the
click at the input or feature level and modify the network accordingly, result-
ing in higher accuracy results.
this helps to improve the segmentation
performance of large-scale nodules and masses.
experimental results on two public datasets and one in-house dataset
demonstrate that the proposed method outperforms existing methods with dif-
ferent backbones.
2
method
2.1
restatement of image segmentation based on click
for pulmonary nodule and mass segmentation, existing methods mostly rely on
regions of interest (roi) obtained by lesion detection networks.
a set of 3d roi
inputs i can be represented as i ∈ rd×h×w with size (d, h, w), along with
its corresponding segmentation ground truth of nodules and masses represented
by s ∈ (0, 1)d×h×w .
for each roi input, the center point c of the lesion, which is represented
as pc = ( d
2 , h
2 , w
2 ) in cartesian coordinate system, can be used as a reference
point to assist the network in improving segmentation performance.
we ﬁrst get the predicted segmentation ˆsi and compute its minimum
3d bounding box b from the trained model.
we also adopt a multi-
scale input encoder to further improve the segmentation performance of nodules and
masses with diﬀerent scales.
to achieve this, we employ a clipping strategy to adjust the proportion of fore-
ground and background in the input image, producing a group of input images
with dimensions of 64×96×96, 32×48×48, and 16×24×24.
these images are
then passed through three convolution paths.
due to diﬀerences in the statistical distribution of pulmonary
nodule scale in image data from diﬀerent medical centers, the segmentation
results of some images, especially for large nodules, are worse than expected.
for such scenarios, we propose the scale-aware test-time click adaptation
method, which can improve the performance of segmentation results for large-
scale nodules and masses by adjusting some of the network parameters during
testing.
first, we use
the pre-trained network to pre-segment the input ct from the test set, get-
ting ˆsi = θ (ii) (i = 1, 2, · · · , n) where n is the number of samples in the test
set.
then we make a projection on the main connected region of ˆsi along three
test-time click adaptation for pulmonary lesion segmentation
685
coordinate axes to obtain the size of the bounding box bi = (d, w, h) of the
pre-segmentation result, and generate an ellipsoid mi with three axes length
proportional to the corresponding side length of the bounding box bi.
more
formally, the coordinates of any foreground voxel point v : (x, y, z) in mi meets
the following requirement:
(x − d
2 )2
r (d)2
+ (y − h
2 )2
r (h)2
+ (z − w
2 )2
r (w)2
= 1,
(1)
where r represents the mapping function between the axis length of the ellipsoid
and the side length of the bounding box bi.
ing adaptive click adjustment
2.4
training objective of sattca
we use the foreground range of adaptively adjusted ellipsoid mi to mask ˆsi to
obtain a masked segmentation ˆsm
i .
formally, ltt is given by:
ltt = lbce + σldice + γlent,
(3)
where σ and γ are hyper-parameters set to 0.5 and 1 in all experiments,
respectively.
3
experiments
3.1
datasets and evaluation protocols
we experiment on two public datasets and one in-house dataset.
686
z. li et al.
table 1. performance of diﬀerent backbones with or without the proposed sattca
and other click-based methods.
experiments are conducted with various pulmonary
nodule segmentation datasets using 3d nnunet [7], transbts [25] and nnunet with
multi-scale input encoder (ms) as the backbone.
comparative experiments are carried
out with the click-based methods in [19,20] and simple test-time click adaptation on
ms-unet.
[1]: the lidc dataset is a publicly available lung ct image database
containing 1018 scans, developed by the lung image database consortium
(lidc).
to generate the ground truth for each nodule and mass,
we combined the segmentation annotations from diﬀerent raters.
overall, we
selected a total of 1625 nodules and masses that were annotated by more than
three raters from the lidc dataset for the experiment.
each ct scan in the dataset has been segmented
by at least one radiologist.
we exclude nodules and masses with diameters larger
than 64 mm or smaller than 2 mm, as the diameter of the largest mass in the
public dataset is no more than 64 mm.
evaluation metrics: the performance of the nodule segmentation is evaluated
by three metrics: volume-based dice similarity coeﬃcient (dsc), surface-based
normalized surface dice (nsd)
[13], and recall rate, which calculates the shape
similarity between predictions and ground truth.
test-time click adaptation for pulmonary lesion segmentation
687
table 2.
the performance variation (%) of using test-time click adaptation (ttca)
and scale-aware ttca (sattca) on the nnunet
δ nsd δ recall
w/test-time click adaptation
micro
–0.034
–0.072
0.440
–0.078
–0.017
–0.089
–0.063
–0.042
0.334
small
0.042
–0.087
0.824
0.038
0.066
0.218
–0.016
–0.046
0.399
medium 0.147
0.114
0.795
0.136
0.195
0.217
0.040
0.023
0.431
mass
0.530
0.465
0.994
–0.007
–0.006
–0.002
0.107
0.099
0.378
w/scale-aware test-time click adaptation
micro
0.593
0.642
1.256
0.783
0.277
0.253
0.900
0.892
0.518
small
0.831
0.944
1.547
0.740
0.296
0.278
1.109
1.045
0.881
medium 1.337
1.892
1.693
0.930
1.014
0.964
1.324
1.219
1.646
mass
2.963
3.182
2.701
2.590
3.558
3.093
1.710
1.656
2.676
3.2
implementation details
the roi of the lesion is a patch cropped around nodules or masses from the
original ct scans with shape 64 × 96 × 96.
all experiments are
conducted on 4 nvidia rtx 3090 gpus with pytorch 1.11.0
[7] and transbts [25] as the backbone to evaluate the pro-
posed method on pulmonary nodule and mass segmentation.
its adaptive framework makes it well-
suited for pulmonary nodule segmentation.
transbts is a 3d medical image seg-
mentation network with a hybrid architecture of transformer and cnn.
the experimental results presented in table 1, consistently
demonstrate that the cnn-based network can achieve better results in multi-
scale pulmonary nodule and mass segmentation tasks across all three datasets.
this is mainly due to the fact that large receptive ﬁelds may involve background
features that are not conducive to segmentation inference for micro or small
nodules.
in datasets such as lidc and in-house, where the number imbalance
of multi-scale lesion phenomena is more notable, the multi-input method consis-
tently outperforms the other two baselines.
we also implemented comparative experiments with other click methods [20]
and [19].
as depicted in table 1, the experimental results show that using a point
688
z. li et al.
fig.
3. (a): visualization of some segmentation results predicted by the baseline with-
out test-time adaptation (tta), with test-time click adaptation (ttca), and scale-
aware test-time click adaptation (sattca).
the recall rates show that sattca sig-
niﬁcantly improves the segmentation performance for large nodules and masses.
and a ﬁxed range of gaussian intensity expansion in the case of large ﬂuctua-
tions in the size of the nodules does not take eﬀect in improving the segmentation
performance.
the inferior segmentation results of [19] can be attributed to the
fact that when it fuses features of diﬀerent depths and scales, the number of
channels in the feature map remains the same, and some of the up-sampling or
down-sampling strides are too large, leading to redundancy in shallow features
and a lack of deep features.
moreover, the sattca improves the dice coeﬃ-
cient and surface-based normalized surface dice of segmentation results in both
networks.
3, the recall rate of large nodule
segmentation is signiﬁcantly improved.
we further analyze the performance of sattca.
firstly, we present the
quantitative comparison in table 2, where we group the nodules and masses in
each dataset at 10 mm intervals and calculate the average segmentation perfor-
mance diﬀerences of the nodules in each scale group.
the statistical results show
that the proposed sattca signiﬁcantly improves the recall rate of the segmen-
tation on large nodules and masses.
3(a), for nodules smaller
than 20 mm, both ttca and sattca eﬀectively increase the recall rate of
predicted segmentation.
for the medium nodule and mass, our sattca proves
to be more eﬀective in improving segmentation performance.
the diﬀerence between the two scatter
diagrams indicates that the proposed sattca eﬀectively alleviates the issue of
test-time click adaptation for pulmonary lesion segmentation
689
extremely imbalanced lesion scales, and improves the segmentation performance
for large lesions.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_72.pdf:
limited by expensive pixel-level labels, polyp segmentation
models are plagued by data shortage and suﬀer from impaired general-
ization.
thus, to reduce labeling cost, we propose to learn
a weakly supervised polyp segmentation model (i.e.,weakpolyp) com-
pletely based on bounding box annotations.
by explicitly aligning predictions across the same image
at diﬀerent scales, the sc loss largely reduces the variation of predic-
tions.
extensive experiments demonstrate the eﬀectiveness of our
proposed weakpolyp, which surprisingly achieves a comparable perfor-
mance with a fully supervised model, requiring no mask annotations at
all.
keywords: polyp segmentation · weak supervision · colorectal
cancer
1
introduction
colorectal cancer (crc) has become a major threat to health worldwide.
given its signiﬁcance, automatic polyp segmentation models [5,8,16,18]
j. wei and y. hu—equal contributions.
[3] introduce the transformer [4]
backbone to extract global contexts, achieving a signiﬁcant performance gain.
however, pixel-by-pixel labeling is time-consuming and expensive, which
hampers practical clinical usage.
besides, many polyps do not have well-
deﬁned boundaries.
pixel-level labeling inevitably introduces subjective noise.
to address the above limitations, a generalized polyp segmentation model is
urgently needed.
in this paper, we achieve this goal by a weakly supervised
polyp segmentation model (named weakpolyp) that only uses coarse bound-
ing box annotations.
more meaningfully, weakpolyp can take existing large-scale polyp
detection datasets to assist the polyp segmentation task.
all these advantages make weakpolyp more clinically practical.
sur-
prisingly, just by redesigning the supervision loss without any changes to the
weakpolyp: you only look bounding box for polyp segmentation
759
model structure, weakpolyp achieves comparable performance to its fully super-
vised counterpart.
this indirect supervision avoids the misleading of box-shape bias of
annotations.
however, many regions in the predicted mask are lost in the projec-
tion and therefore get no supervision.
speciﬁcally, the sc loss explicitly reduces the distance between predictions
of the same image at diﬀerent scales.
by forcing feature alignment, it inhibits
the excessive diversity of predictions, thus improving the model generalization.
in summary, our contributions are three-fold: (1) we build the weakpolyp
model completely based on bounding box annotations, which largely reduces the
labeling cost and achieves a comparable performance to full supervision.
(3) our proposed weakpolyp is
a plug-and-play option, which can boost the performances of polyp segmentation
models under diﬀerent backbones.
2
method
model components.
2 depicts the components of weakpolyp, including
the segmentation phase and the supervision phase.
for the segmentation phase,
we adopt res2net
for input image
instead of the segmentation phase,
our contributions primarily lie in the supervision phase, including mask-to-box
(m2b) transformation and scale consistency (sc) loss.
for each input image i, we ﬁrst resize it into two diﬀerent
scales: i1 ∈ rs1×s1 and i2 ∈ rs2×s2.
then, i1 and i2 are sent to the segmentation
model and get two predicted masks p1 and p2, both of which have been resized
to the same size.
2. the framework of our proposed weakpolyp model, which consists of the seg-
mentation phase and the supervision phase.
the segmentation phase predicts the polyp
mask for each input ﬁrstly, and the supervision phase uses the coarse box annotation to
guide previous predicted mask.
2.1
mask-to-box (m2b) transformation
one naive method to achieve the weakly supervised polyp segmentation is to
use the bounding box annotation b to supervise the predicted mask p1/p2.
because
there is a strong box-shape bias in b. training with this bias, the model is forced
to predict the box-shape mask, unable to maintain the polyp’s contours.
this indirect supervision separates p1/p2
from b so that p1/p2 is not aﬀected by the shape bias of b while obtaining
the position and extent of polyps.
but how to implement the transformation
from p1/p2 to t1/t2?
[0, 1]h×1
(1)
weakpolyp: you only look bounding box for polyp segmentation
761
fig.
then, we element-wisely take the
minimum of p
′
w and p
′
h to achieve the bounding box mask t. as shown in
fig.
because both t1/t2 and b are box-like masks, we directly calculate the super-
vision loss between them without worrying about the misguidance of box-shape
bias.
notably, m2b is diﬀerentiable, which can be easily implemented with
pytorch and plugged into the model to participate in gradient backpropagation.
2.2
scale consistency (sc) loss
in m2b, most pixels in p are ignored in the projection, thus only a few pix-
els with high response values are involved in the supervision loss.
the gt row is the performance upper bound.
the box row
is the performance lower bound.
.734 .611 .824 .705
ours
.853
.781
.854
.777
.907 .839 .792 .707 .922 .859
p1 and p2 come from the same image i1.
lt otal = lsum + lsc
(5)
3
experiments
datasets.
two large polyp datasets are adopted to evaluate the model per-
formance, including sun-seg [9] and polyp-seg.
sun-seg originates
weakpolyp: you only look bounding box for polyp segmentation
763
fig.
modules
res2net-50
pvtv2-b2
easy testing hard testing easy testing hard testing
dice iou
dice iou
dice iou
dice iou
base
.715
.601
.718
.599
.769
.652
.770
.648
base+m2b
.748
.654
.768
.673
.822
.738
.822
.735
base+m2b+sc .792
.715
.807
.727
.853
.781
.854
.777
from [7,10], which consists of 19,544 training images, 17,070 easy tesing images,
and 12,522 hard testing images.
polyp-seg is our private polyp segmenta-
tion dataset, which contains 15,916 training images and 4,040 testing images.
weakpolyp is implemented using pytorch.
all input images
are uniformly resized to 352×352.
for data augmentation, random ﬂip, random
rotation, and multi-scale training are adopted.
1 compares the model performance under
diﬀerent supervisions, backbones, and datasets.
the overall performance order
is gt>weakpolyp>box>grabcut.
[13] masks
performs the worst, because the foreground and background of polyp images
are similar.
our weakpolyp predictably outperforms the model supervised by box
masks because it is not aﬀected by the box-shape bias of the annotations.
performance comparison with previous fully supervised models on sun-seg.
combining all these modules, our model achieves the highest
performance.
3 shows our weakpolyp
is even superior to many previous fully supervised methods: pranet

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_57.pdf:
computed tomography (ct) based precise prostate segmen-
tation for treatment planning is challenging due to (1) the unclear bound-
ary of the prostate derived from ct’s poor soft tissue contrast and (2)
the limitation of convolutional neural network-based models in capturing
long-range global context.
here we propose a novel focal transformer-based
image segmentation architecture to eﬀectively and eﬃciently extract local
visual features and global context from ct images.
additionally, we design
an auxiliary boundary-induced label regression task coupled with the main
prostate segmentation task to address the unclear boundary issue in ct
images.
we demonstrate that this design signiﬁcantly improves the quality
of the ct-based prostate segmentation task over other competing meth-
ods, resulting in substantially improved performance, i.e., higher dice sim-
ilarity coeﬃcient, lower hausdorﬀ distance, and average symmetric sur-
face distance, on both private and public ct image datasets.
keywords: focal transformer · prostate segmentation · computed
tomography · boundary-aware
1
introduction
prostate cancer is a leading cause of cancer-related deaths in adult males, as
reported in studies, such as [17].
a common treatment option for prostate can-
cer is external beam radiation therapy (ebrt) [4], where ct scanning is a
cost-eﬀective tool for the treatment planning process compared with the more
expensive magnetic resonance imaging (mri).
as a result, precise prostate seg-
mentation in ct images becomes a crucial step, as it helps to ensure that the
radiation doses are delivered eﬀectively to the tumor tissues while minimizing
harm to the surrounding healthy tissues.
due to the relatively low spatial resolution and soft tissue contrast in ct
images compared to mri images, manual prostate segmentation in ct images
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43898-1 57.
several automated segmentation methods have been proposed to alleviate
these issues, especially the fully convolutional networks (fcn) based u-net
despite good progress,
these methods often have limitations in capturing long-range relationships and
global context information [2] due to the inherent bias of convolutional opera-
tions.
[2] adapts vit to medical image segmentation
tasks by connecting several layers of the transformer module (multi-head sa)
to the fcn-based encoder for better capturing the global context information
from the high-level feature maps.
in spite of the improved performance for the aforementioned vit-based net-
works, these methods utilize the standard or shifted-window-based sa, which
is the ﬁne-grained local sa and may overlook the local and global interactions
as reported by [20], even pre-trained with a massive amount of medical
data using self-supervised learning, the performance of prostate segmentation
task using high-resolution and better soft tissue contrast mri images has not
been completely satisfactory, not to mention the lower-quality ct images.
addi-
tionally, the unclear boundary of the prostate in ct images derived from the
low soft tissue contrast is not properly addressed [7,22].
recently, focal transformer [24] is proposed for general computer vision
tasks, in which focal self-attention is leveraged to incorporate both ﬁne-grained
local and coarse-grained global interactions.
each token attends its closest sur-
rounding tokens with ﬁne granularity, and the tokens far away with coarse granu-
larity; thus, focal sa can capture both short- and long-range visual dependencies
eﬃciently and eﬀectively. inspired by this work, we propose the focalunetr
(focal u-net transformers), a novel focal transformer architecture for ct-
based medical image segmentation (fig. 1a).
[15] incorporates additional decoders to enhance boundary detection
and distance map estimation, they either lack the capacity for eﬀective global
context capture through fcn-based techniques or overlook the signiﬁcance of
considering the randomness of the boundary, particularly in poor soft tissue con-
trast ct images for prostate segmentation.
in contrast, our approach utilizes a
multi-task learning strategy that leverages a gaussian kernel over the boundary
of the ground truth segmentation mask
this serves as a regularization term for the main
task of generating the segmentation mask.
and the auxiliary task enhances the
model’s generalizability by addressing the challenge of unclear boundaries in
low-contrast ct images.
the architecture of focalunetr as (a) the main task for prostate segmenta-
tion and (b) a boundary-aware regression auxiliary task.
first, we develop a novel
focal transformer model (focalunetr) for ct-based prostate segmentation,
which makes use of focal sa to hierarchically learn the feature maps accounting
for both short- and long-range visual dependencies eﬃciently and eﬀectively.
second, we also address the challenge of unclear boundaries speciﬁc to ct images
by incorporating an auxiliary task of contour regression.
third, our methodology
advances state-of-the-art performance via extensive experiments on both real-
world and benchmark datasets.
2
methods
2.1
focalunetr
our focalunetr architecture (fig. 1) follows a multi-scale design similar to
[6,20], enabling us to obtain hierarchical feature maps at diﬀerent stages.
the
input medical image x ∈ rc×h×w is ﬁrst split into a sequence of tokens with
dimension ⌈ h
h′ ⌉×⌈ w
w ′ ⌉, where h, w represent spatial height and width, respec-
tively, and c represents the number of channels.
these tokens are then projected
into an embedding space of dimension d using a patch of resolution (h′, w ′).
2. (a) the focal sa mechanism, and (b) an example of perfect boundary matching
using focal sa for ct-based prostate segmentation task (lower panel), in which focal
sa performs query-key interactions and query-value aggregations in both ﬁne- and
coarse-grained levels (upper panel).
of two focal levels (ﬁne and coarse) for capturing the interaction of local and
global context for optimal boundary-matching between the prediction and the
ground truth for prostate segmentation.
finally, a relative position
bias is added to compute the focal sa for qi by
attention(qi, ki, vi) = softmax(qikt
i
√
d
+ b)vi,
where b = {bl}l
1 is the learnable relative position bias [24].
the encoder utilizes a patch size of 2×2 with a feature dimension of 2×2×1 =
4 (i.e., a single input channel ct) and a d-dimensional embedding space.
the
596
c. li et al.
overall architecture of the encoder comprises four stages of focal transformer
blocks, with a patch merging layer applied between each stage to reduce the
resolution by a factor of 2.
we utilize an fcn-based decoder (fig. 1a) with
skip connections to connect to the encoder at each resolution to construct a “u-
shaped” architecture for our ct-based prostate segmentation task.
the objective function
for the segmentation head is given by: lseg = ldice(ˆpi, g) + lce(ˆpi, g), where ˆpi
represents the predicted probabilities from the main task and g represents the
ground truth mask, both given an input image i. the predicted probabilities,
ˆpi, are derived from the main task through the application of the focalunetr
model to the input ct image.
to address the challenge of unclear boundaries in ct-based prostate segmen-
tation, an auxiliary task is introduced for the purpose of predicting boundary-
aware contours to assist the main prostate segmentation task.
this auxiliary
task is achieved by attaching another convolution head after the extracted fea-
ture maps at the ﬁnal stage (see fig.
we pre-
dict this heatmap with a regression task trained by minimizing mean-squared
error instead of treating it as a single-pixel boundary segmentation problem.
given the ground truth of contour gc
i , induced from the segmentation mask
for input image i, and the reconstructed output probability ˆpc
i , we use the fol-
lowing loss function: lreg =
1
n

i ||ˆpc
i − gc
i ||2 where n is the total number of
images for each batch.
this auxiliary task is trained concurrently with the main
segmentation task.
a multi-task learning approach is adopted to regularize the main segmen-
tation task through the auxiliary boundary prediction task.
focalunetr
597
3
experiments and results
3.1
datasets and implementation details
to evaluate our method, we use a large private dataset with 400 ct scans
and a large public dataset with 300 ct scans (amos
although the amos dataset includes the prostate class,
it mixes the prostate (in males) and the uterus (in females) into one single class
labeled pro/ute.
we ﬁlter out ct scans missing the pro/ute ground-truth
segmentation.
[4, 8, 16, 32] for each of the four stages.
for the implementation, we utilize a server equipped with 8 nvidia a100
gpus, each with 40 gb of memory.
all experiments are conducted in pytorch,
and each model is trained on a single gpu.
for 2d models, we ﬁrst slice each voxel
patch in the axial direction into 64 slices of 128 × 128 images for training and
stack them back for evaluation.
we
use random ﬂip, rotation, and intensity scaling as augmentation transforms with
probabilities of 0.1, 0.1, and 0.2, respectively.
however, we did not get
improved performance compared with directly applying the training parameters
learned from tuning the private dataset.
we report the dice similarity coef-
ﬁcient (dsc, %), 95% percentile hausdorﬀ distance (hd, mm), and average
symmetric surface distance (assd, mm) metrics.
3.2
experiments
comparison with state-of-the-art methods.
to demonstrate the eﬀec-
tiveness of focalunetr, we compare the ct-based prostate segmentation per-
formance with three 2d u-net-based methods: u-net
quantitative performance comparison on the private and amos datasets
with a mean (standard deviation) for 3 runs with diﬀerent seeds.
[16], two 2d transformer-based segmentation meth-
ods: transunet
transunet and swin-unet are the
only methods that are pre-trained on imagenet.
detailed information regarding
the number of parameters, flops, and average inference time can be found in
the supplementary materials.
the
amos dataset mixes the prostate(males)/uterus(females, a relatively small por-
tion).
thus, the overall performance of focalunetr is
overshadowed by this challenge, resulting in only moderate improvement over
the baselines on the amos dataset.
however, the performance margin signiﬁ-
cantly improves when using the real-world (private) dataset.
when co-trained
with the auxiliary contour regression task using the multi-task training strat-
egy, the performance of focalunetrs is further improved.
3. qualitative results on sample test ct images from the private (ﬁrst two rows)
and amos (last two rows)
datasets
with an auxiliary contour regression task can improve the challenging ct-based
prostate segmentation performance.
3.
the ﬁgure shows that our focalunetr-b and focalunetr-b* generate more
accurate segmentation results that are more consistent with the ground truth
than the results of the baseline models.
additionally, the focalunetrs are less likely to produce false posi-
tives (see more in supplementary materials) for ct images without a foreground
ground truth, due to the focal sa mechanism that enables the model to cap-
ture global context and helps to identify the correct boundary and shape of the
prostate.
overall, the focalunetrs demonstrate improved segmentation capa-
bilities while preserving shapes more precisely, making them promising tools for
clinical applications.
the results (table 2) indicate that as the
value of λ2 is gradually increased and that of λ1 is correspondingly decreased
(thereby increasing the relative importance of the auxiliary contour regression
task), segmentation performance initially improves.
however, as the ratio of con-
tour information to segmentation mask information becomes too unbalanced,
performance begins to decline.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_68.pdf:
this paper presents a new robust loss function, the t-loss,
for medical image segmentation.
our experiments show that the t-
loss outperforms traditional loss functions in terms of dice scores on two
public medical datasets for skin lesion and lung segmentation.
we also
demonstrate the ability of t-loss to handle diﬀerent types of simulated
label noise, resembling human error.
our results provide strong evidence
that the t-loss is a promising alternative for medical image segmenta-
tion where high levels of noise or outliers in the dataset are a typical
phenomenon in practice.
keywords: robust loss · medical image segmentation ·
noisy labels
1
introduction
convolutional neural networks (cnns) and visual transformers (vits) have
become the standard in semantic segmentation, achieving state-of-the-art results
in many applications [1,16,24].
however, supervised training of cnns and vits
requires large amounts of annotated data, where each pixel in the image is labeled
with the category it belongs to.
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43898-1 68.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
https://doi.org/10.1007/978-3-031-43898-1_68
robust t-loss for medical image segmentation
715
knowledge that is often scarcely available [6].
in addition, medical image anno-
tations can be aﬀected by human bias and poor inter-annotator agreement [23],
further complicating the process.
for instance, the fitzpatrick 17k dataset, commonly used in dermatology
research, contains non-skin images and noisy annotations.
in a random sample
of 504 images, 5.4% were labeled incorrectly or as other classes [10].
previous literature has explored many methods to mitigate the problem of
noisy labels in deep learning.
despite these advances, semantic segmenta-
tion with noisy labels is relatively understudied.
existing research in this area
has focused on the development of noise-resistant network architectures [15], the
incorporation of domain-speciﬁc prior knowledge [29], or more recent strategies
that update the noisy masks before memorization
although previous methods have shown robustness in semantic segmentation,
they often have limitations, such as more hyper-parameters, modiﬁcations to the
network architecture, or complex training procedures.
the t-loss, whose simplest formulation
features a single parameter, can adaptively learn an optimal tolerance level to
label noise directly during backpropagation, eliminating the need for additional
computations such as the expectation maximization (em) steps.
to evaluate the eﬀectiveness of the t-loss as a robust loss function for medi-
cal semantic segmentation, we conducted experiments on two widely-used bench-
mark datasets in the ﬁeld: one for skin lesion segmentation and the other for lung
segmentation.
we injected diﬀerent levels of noise into these datasets that simu-
late typical human labeling errors and trained deep learning models using various
robust loss functions.
our experiments demonstrate that the t-loss outperforms
716
a. gonzalez-jimenez et al.
these robust state-of-the-art loss functions in terms of segmentation accuracy and
robustness, particularly under conditions of high noise contamination.
section 3 covers the
datasets used in our experiments, the implementation and training details of
t-loss, and the metrics used for comparison.
2
methodology
let xi ∈ rc×w×h be an input image and yi ∈ {0, 1}w×h be its noisy annotated
binary segmentation mask, where c represents the number of channels, w the
image’s width, and h its height.
given a set of images {x1, . . .
, yn}, our goal is to train a model fw with parameters
w such that fw(x) approximates the accurate binary segmentation mask for any
given image x.
to this end we note that, heuristically, assuming error terms to follow a
student-t distribution (as suggested e.g. in [19]) allows for signiﬁcantly larger
noise tolerance with respect to the usual gaussian form.
recall that the student-t
distribution for a d-dimensional variable y is deﬁned by the probability density
function (pdf)
p(y|µ, σ; ν) = γ
 ν+d
2

γ
 ν
2

|σ|−1/2
(πν)d/2

1
(2)
robust t-loss for medical image segmentation
717
the functional form of our loss function for one image is then obtained with the
identiﬁcation y = yi and the approximation µ = fw(xi), and aggregated with
lt = 1
n
n

i=1
− log p(yi|fw(xi), σ; ν).
in the case of images, this can easily be in the
order of 104 or larger, which makes a general computation highly non-trivial and
may deteriorate the generalization capabilities of the model.
for these reasons,
we take σ to be the identity matrix id, despite knowing that pixel annotations
in an image are not independent.
the loss term for one image simpliﬁes to
− log p(y|µ, id; ν)
loss functions with similar dynamic tolerance parameters were also studied in
[2] in the context of regression, where using the student-t distribution is only
mentioned in passing.
3
experiments
in this section, we demonstrate the robustness of the t-loss for segmentation
tasks on two public image collections from diﬀerent medical modalities, namely
isic [5] and shenzhen
[15].
3.1
datasets
the isic 2017 dataset [5] is a well-known public benchmark of dermoscopy
images for skin cancer detection.
it contains 2000 training and 600 test images
with corresponding lesion boundary masks.
the images are annotated with lesion
type, diagnosis, and anatomical location metadata.
we resized the images to
256 × 256 pixels for our experiments.
718
a. gonzalez-jimenez et al.
shenzhen [4,13,25] is a public dataset containing 566 frontal chest radio-
graphs with corresponding lung segmentation masks for tuberculosis detection.
since there is not a predeﬁned split for shenzhen as in isic, to ensure represen-
tative training and testing sets, we stratiﬁed the images by their tuberculosis and
normal lung labels, with 70% of the data for training and the remaining 30% for
testing.
resulting in 296 training images and 170 test images.
all images were
resized to 256 × 256 pixels.
without a public benchmark with real noisy and clean segmentation masks,
we artiﬁcially inject additional mask noise in these two datasets to test the
model’s robustness to low annotation quality.
this simulates the real risk of
errors due to factors like annotator fatigue and diﬃculty in annotating certain
images.
the morphological transfor-
mations included erosion, dilation, and aﬃne transformations, which respectively
reduced, enlarged, and displaced the annotated area.
3.2
setup
we train a nnu–net [12] as a segmentation network from scratch.
to increase
variations in the training data, we augment them with random mirroring, ﬂip-
ping, and gamma transformations.
in addition, if the diﬀerence is signiﬁcant, we
1 https://github.com/gaozhitong/sp guided noisy label seg.
robust t-loss for medical image segmentation
719
perform the tukey post-hoc test [14] to determine which means are diﬀerent.
we
assume statistical signiﬁcance for p-values of less than p = 0.05 and denote this
with a ⋆.
4
results
4.1
results on the isic dataset
we present experimental results for the skin lesion segmentation task on the isic
dataset in table 1.
our results show that conventional losses perform well with
no noise or under low noise levels, but their performance decreases signiﬁcantly
with increasing noise levels due to the memorization of noisy labels.
1, where traditional robust
losses overﬁt data in later stages of learning while metrics for the t-loss do not
deteriorate.
examples of the obtained masks can be seen
in the supplementary material.
0.761(6)⋆
720
a. gonzalez-jimenez et al.
fig.
2. the behavior of ˜ν in the skin lesion segmentation task.
4.2
results on the shenzhen dataset
the results of lung segmentation for the shenzhen test set are reported in table
2. similar to the isic dataset, all considered robust losses perform well at low
noise levels.
4.3
dynamic tolerance to noise
the value of ˜ν is crucial for the model’s performance, as it controls the sensitivity
to label noise.
as seen in fig. 2, ˜ν dynamically adjusts annotation noise tolerance in the early
stages of training, independently of its initial value.
the plots demonstrate that
robust t-loss for medical image segmentation
721
table 2. dice score on the shenzhen dataset with diﬀerent noise ratios.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_45.pdf:
accurately segmenting the liver into anatomical segments
is crucial for surgical planning and lesion monitoring in ct imaging.
however, this is a challenging task as it is deﬁned based on vessel
structures, and there is no intensity contrast between adjacent seg-
ments in ct images.
speciﬁcally, we ﬁrst seg-
ment the liver and vessels from the ct image, and generate 3d
liver point clouds and voxel grids embedded with vessel structure
prior.
then, we design a multi-scale point-voxel fusion network to cap-
ture the anatomical structure and semantic information of the liver
and vessels, respectively, while also increasing important data access
through vessel structure prior.
finally, the network outputs the clas-
siﬁcation of couinaud segments in the continuous liver space, producing
a more accurate and smooth 3d couinaud segmentation mask.
our pro-
posed method outperforms several state-of-the-art methods, both point-
based and voxel-based, as demonstrated by our experimental results on
two public liver datasets.
code, datasets, and models are released at
https://github.com/xukun-zhang/couinaud-segmentation.
keywords: couinaud segmentation · point-voxel network · liver ct
x. zhang and y. liu—contributed equally.
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43898-1 45.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
https://doi.org/10.1007/978-3-031-43898-1_45
466
x. zhang et al.
1
introduction
primary liver cancer is one of the most common and deadly cancer diseases
in the world, and liver resection is a highly eﬀective treatment
the
couinaud segmentation [7] based on ct images divides the liver into eight func-
tionally independent regions, which intuitively display the positional relation-
ship between couinaud segments and intrahepatic lesions, and helps surgeons
for make surgical planning [3,13].
in clinics, couinaud segments obtained from
manual annotation are tedious and time-consuming, based on the vasculature
used as rough guide (fig. 1).
thus, designing an automatic method to accu-
rately segment couinaud segments from ct images is greatly demanded and
has attracted tremendous research attention.
however, automatic and accurate couinaud segmentation from ct images
is a challenging task.
since it is deﬁned based on the anatomical structure of live
vessels, even no intensity contrast (fig. 1.(b)) can be observed between diﬀerent
couinaud segments, and the uncertainty of boundary (fig. 1.(d)) often greatly
aﬀect the segmentation performance.
[4,8,15,19] mainly rely
on handcrafted features or atlas-based models, and often fail to robustly han-
dle those regions with limited features, such as the boundary between adjacent
couinaud segments.
recently, with the advancement of deep learning [5,10,18],
many cnn-based algorithms perform supervised training through pixel-level
couinaud annotations to automatically obtain segmentation results [1,9,21].
unfortunately, the cnn models treat all voxel-wise features in the ct image
equally, cannot eﬀectively capture key anatomical regions useful for couinaud
segmentation.
in addition, all these methods deal with the 3d voxels of the liver
directly without considering the spatial relationship of the diﬀerent couinaud
segments, even if this relationship is very important in couinaud segmentation.
it can supplement the cnn-based method and improve the segmentation per-
formance in regions without intensity contrast.
in this paper, to tackle the aforementioned challenges, we propose a point-
voxel fusion framework that represents the liver ct in continuous points to
better learn the spatial structure, while performing the convolutions in voxels
to obtain the complementary semantic information of the couinaud segments.
fig.
1. couinaud segments (denoted as roman numbers) in relation to the liver vessel
structure.
(a) and (b) brieﬂy several couinaud segments separated by the hepatic vein.
(c) and (d) show several segments surrounded by the portal vein, which are divided by
the course of the hepatic vein.
anatomical-aware point-voxel network
467
speciﬁcally, the liver mask and vessel attention maps are ﬁrst extracted from the
ct images, which allows us to randomly sample points embedded with vessel
structure prior in the liver space and voxelize them into a voxel grid.
the voxel-based branch is composed of
a series of convolutions to learn semantic features, followed by de-voxelization
to convert them back to points.
through the operation of voxelization and de-
voxelization at diﬀerent resolutions, the features extracted by these two branches
can achieve multi-scale fusion on point-based representation, and ﬁnally output
the couinaud segment category of each point.
extensive experiments on two
publicly available datasets named 3dircadb [20] and lits
[2] demonstrate that
our proposed framework achieves state-of-the-art (sota) performance, outper-
forming cutting-edge methods quantitatively and qualitatively.
fig.
2. overall framework of our proposed method for couinaud segmentation.
2
method
the overview of our framework to segment couinaud segments from ct images
is shown in fig.
2, including the liver segmentation, vessel attention map gener-
ation, point data sampling and multi-scale point-voxel fusion network.
2.1
liver mask and vessel attention map generation
liver segmentation is a fundamental step in couinaud segmentation task.
con-
sidering that the liver is large and easy to identify in the abdominal organs, we
468
x. zhang et al.
extracted the liver mask through a trained 3d unet [6]. diﬀerent from liver seg-
mentation, since we aim to use the vessel structure as a rough guide to improving
the performance of the couinaud segmentation, we employ another 3d unet
speciﬁcally, given a 3d ct
image containing only the area covered by the liver mask (l), the 3d unet
[6] output a binary vessel mask (m).
2.2
couinaud segmentation
based on the above work, we ﬁrst use the m ′ and the l to sample get point data,
which can convert into a voxel grid through re-voxelization.
the converted voxel
grid embeds the vessel prior and also dilutes the liver parenchyma information.
inspired by [12], a novel multi-scale point-voxel fusion network then is proposed
to simultaneously process point and voxel data through point-based branch and
voxel-based branch, respectively, aiming to accurately perform couinaud seg-
mentation.
in order to obtain the topological relationship between couinaud seg-
ments, a direct strategy is to sample the coordinate point data with 3d spatial
information from liver ct and perform point-wise classiﬁcation.
hence, we ﬁrst
convert the image coordinate points i =

i1, i2, ..., it, it ∈ r3
in liver ct into
the world coordinate points p =

p1, p2, ..., pt, pt ∈ r3
:
p = i ∗ spacing ∗ direction + origin,
(1)
where spacing represents the voxel spacing in the ct images, direction repre-
sents the direction of the scan, and origin represents the world coordinates
of the image origin.
however,
directly feeding the transformed point data as input into the point-based branch
undoubtedly ignores the vessel structure, which is crucial for couinaud segmen-
tation.
in the training stage of the
network, the label of the point pt = (xt + δx, yt + δy, zt + δz) is generated by:
ot = ot(r(xt + δx), r(yt + δy), r(zt + δz))
it is not enough to extract the topological information and
ﬁne-grained information of independent points only by point-based branch for
accurate couinaud segmentation.
to this end, we transform the point data
{(pt, ft)} into voxel grid {vu,v,w} by re-voxelization, where ft ∈ rc is the feature
corresponding to point pt, aiming to voxel-based convolution to extract comple-
mentary semantic information in the grid.
moreover, due to the previously mentioned point sampling
strategy, the converted voxel grid also inherits the vessel structure from the point
data and dilutes the unimportant information in the ct images.
intuitively, due to the image
intensity between diﬀerent couinaud segments being similar, the voxel-based
cnn model is diﬃcult to achieve good segmentation performance.
we propose
a multi-scale point-voxel fusion network for accurate couinaud segmentation,
take advantage of the topological relationship of coordinate points in 3d space,
and leverage the semantic information of voxel grids.
the features extracted
by these two branches on multiple scales are fused to provide more accurate
and robust couinaud segmentation performance.
at the
same time, the voxel grid {vu,v,w} passes the voxel branch based on convolution,
denoted as ev, which can aggregate the features of surrounding points and learn
the semantic information in the liver 3d space.
we re-transform the features
extracted from the voxel-based branch to point representation through trilinear
interpolation, to combine them with ﬁne-grained features extracted from the
point-based branch, which provide complementary information:
(pt, f 1
t ) = ep(p(pt, ft))
after three rounds of point-
voxel operations, we concatenate the original point feature ft and the features

f 1
t , f 2
t , f 3
t

with multiple scales, then send them into a point-wise decoder d,
parameterized by a fully connected network, to predict the corresponding couin-
aud segment category:
ˆ
ot = d(cat

ft, f 1
t , f 2
t , f 3
t

) ∈ {0, 1, ..., 7},
(5)
where {0, 1, ..., 7} denotes the couinaud segmentation category predicted by our
model for the point pt.
more method details are shown in the supplementary materials.
3
experiments
3.1
datasets and evaluation metrics
we evaluated the proposed framework on two publicly available datasets,
3dircadb
the 3dircadb dataset [20] contains 20 ct images
with spacing ranging from 0.56 mm to 0.87 mm, and slice thickness ranging from
1 mm to 4 mm with liver and liver vessel segmentation labels.
the lits dataset
[2] consists of 200 ct images, with a spacing of 0.56 mm to 1.0 mm and slice
thickness of 0.45 mm to 6.0 mm, and has liver and liver tumour labels, but with-
out vessels.
we annotated the 20 subjects of the 3dircadb dataset [20] with the
couinaud segments and randomly divided 10 subjects for training and another
10 subjects for testing.
[2], we observed the vessel structure
on ct images, annotated the couinaud segments of 131 subjects, and randomly
selected 66 subjects for training and 65 for testing.
we have used three widely used metrics, i.e., accuracy (acc, in %), dice
similarity metric (dice, in %), and average surface distance (asd, in mm) to
evaluate the performance of the couinaud segmentation.
3.2
implementation details
the proposed framework was implemented on an rtx8000 gpu using pytorch.
we perform scaling within the range of
0.9 to 1.1, arbitrary axis ﬂipping, and rotation in the range of 0 to 5 ◦c on the
input point data as an augmentation strategy.
all our experiments were trained 400
epochs, with a random seed was 2023, and then we used the model with the best
performance on the training set to testing.
quantitative comparison with diﬀerent segmentation methods.
acc and
dice are the averages of all testing subjects, while asd is the average of the average
performance of all segments in all testing subjects.
[17] with dual attention to focus on
the boundary of the couinaud segments and is speciﬁcally used for the couinaud
segmentation task.
we use pytorch to implement this model and maintain the
same implementation details as other methods.
[6] have achieved close performance in the lits
dataset
[2], which demonstrates the potential of the point-based methods in
the couinaud segmentation task.
finally, our pro-
posed point-voxel fusion segmentation framework achieves the best performance.
the ﬁrst two rows show
that the vessel structure is used as the boundary guidance for couinaud segmen-
tation, but voxel-based 3d unet
[6] fails to accurately capture this key structural
relationship, resulting in inaccurate boundary segmentation.
besides, compared with the 3d view, it is obvious that the voxel-based cnn
methods are easy to pay attention to the local area and produce a large area
of error segmentation, so the reconstructed surface is uneven.
3. comparison of segmentation results of diﬀerent methods.
diﬀerent
colours represent diﬀerent couinaud segments.
in the ﬁrst column, we show the vessel attention map in 2d and 3d views
as an additional reference for the segmentation results.
fig.
diﬀerent table 1, here acc
and dice are the average performance of each couinaud segment.
segmentation blur in boundary areas with high uncertainty.
our method com-
bines the advantages of point-based and voxel-based methods, and remedies their
respective defects, resulting in smooth and accurate couinaud segmentation.
3.4
ablation study
to further study the eﬀectiveness of our proposed framework, we compared two
ablation experiments: 1) random sampling of t points in the liver space, with-
anatomical-aware point-voxel network
473
out considering the guidance of vascular structure, and 2) considering only the
voxel-based branch, where the couinaud segments mask is output by a cnn
decoder.
figure 4 shows the ablation experimental results obtained on all the
couinaud segments of two datasets, under the dice and the asd metrics.
it
can be seen that our full method is signiﬁcantly better than the cnn branch
joint decoder method on both metrics of two datasets, which demonstrates the
performance gain by the combined point-based branch.
in addition, compared
with the strategy of random sampling, our full-method reduces the average asd
by more than 2mm on eight couinaud segments.
this is because to the ves-
sel structure-guided sampling strategy can increase the important data access
between the boundaries of the couinaud segments.
besides, perturbations are
applied to the points in the coverage area of the vessel attention map, so that our
full method performs arbitrary point sampling in the continuous space near the
vessel, and is encouraged to implicitly learn the couinaud boundary in countless
points.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_51.pdf:
objects with complex structures pose signiﬁcant challenges
to existing instance segmentation methods that rely on boundary or aﬃn-
ity maps, which are vulnerable to small errors around contacting pixels
that cause noticeable connectivity change.
while the distance transform
(dt) makes instance interiors and boundaries more distinguishable, it
tends to overlook the intra-object connectivity for instances with varying
width and result in over-segmentation.
to address these challenges, we
propose a skeleton-aware distance transform (sdt) that combines the
merits of object skeleton in preserving connectivity and dt in model-
ing geometric arrangement to represent instances with arbitrary struc-
tures.
comprehensive experiments on histopathology image segmenta-
tion demonstrate that sdt achieves state-of-the-art performance.
1
introduction
instances with complex shapes arise in many biomedical domains, and their
morphology carries critical information.
for example, the structure of gland
tissues in microscopy images is essential in accessing the pathological stages for
cancer diagnosis and treatment.
these instances, however, are usually closely
in touch with each other and have non-convex structures with parts of varying
widths (fig. 1a), posing signiﬁcant challenges for existing segmentation methods.
in the biomedical domain, most methods [3,4,13,14,22] ﬁrst learns interme-
diate representations and then convert them into masks with standard segmen-
tation algorithms like connected-component labeling and watershed transform.
however, for objects with non-convex morphology,
the boundary-based distance transform produces multiple local optima in the
energy landscape (fig. 1c), which tends to break the intra-instance connectivity
when applying thresholding and results in over-segmentation.
in quantitative evaluations,
we show that our proposed sdt achieves leading performance on histopathology
image segmentation for instances with various sizes and complex structures.
1.1
related work
instance segmentation.
bottom-up instance segmentation approaches have
become de facto for many biomedical applications due to the advantage in seg-
menting objects with arbitrary geometry.
however, for com-
plex structure with parts of varying width, the boundary-based dt tends to
produce relatively low values for thin connections and consequently causes over-
segmentation.
the vision community has been working on direct
object skeletonization from images [7,9,16,19].
[16] shows the application of the skeleton on segmenting single-object images.
we instead focus on the more challenging instance segmentation task with multi-
ple objects closely touching each other.
object skeletons are also used to correct
errors in pre-computed segmentation masks
our sdt framework instead
use the skeleton in the direct segmentation from images.
2
skeleton-aware distance transform
2.1
sdt energy function
given an image, we aim to design a new representation e for a model to learn,
which is later decoded into instances with simple post-processing.
the boundary
(or aﬃnity) map is a binary representation where e|γb = 0 and e|ω\γb = 1.
taking the merits of dt in modeling the geometric arrangement and skeleton
in preserving connectivity, we propose a new representation e that satisﬁes:
0
for the realization of e, let x be a pixel in the input image, and d be the
metric, e.g., euclidean distance.
in the regression mode, the output is a single-channel image.
we test both learning strategies
in the experiments to illustrate the optimal setting for sdt.
a fcn maps the
image into the energy space to minimize the loss.
(b) inference phase: we threshold the
sdt to generate skeleton segments, which is processed into seeds with the connected
component labeling.
speciﬁcally, in all
the experiments, we use a deeplabv3 model
we also add a coordconv [10] layer before the 3rd stage in the backbone
network to introduce spatial information into the segmentation model.
some objects may touch the image border
due to either a restricted ﬁeld of view (fov) of the imaging devices or spatial data
augmentation like the random crop.
if pre-computing the skeleton, we will get
local skeleton (fig. 4c) for objects with missing masks due to imaging restrictions,
and partial skeleton (fig. 4b) due to spatial data augmentation, which causes
ambiguity.
in
534
z. lin et al.
inference, we always run predictions on the whole images to avoid inconsistent
predictions.
in the sdt energy map, all boundary pix-
els share the same energy value and can be processed into segments by direct
thresholding and connected component labeling, similar to dwt [1].
we ﬁrst perform
connected component labeling of the skeleton pixels to generate seeds and run the
watershed algorithm on the reversed energy map using the seeds as basins (local
optima) to generate the ﬁnal segmentation.
we also follow previous works [4,22]
and reﬁne the segmentation by hole-ﬁlling and removing small spurious objects.
3
experiments
3.1
histopathology instance segmentation
accurate instance segmentation of gland tissues in histopathology images is
essential for clinical analysis, especially cancer diagnosis.
we use the gland segmentation challenge
dataset
[17] that contains colored light microscopy images of tissues with a
wide range of histological levels from benign to malignant.
there are 85 and
80 images in the training and test set, respectively, with ground truth annota-
tions provided by pathologists.
according to the challenge protocol, the test set
is further divided into two splits with 60 images of normal and 20 images of
abnormal tissues for evaluation.
three evaluation criteria used in the challenge
include instance-level f1 score, dice index, and hausdorﬀ distance, which mea-
sure the performance of object detection, segmentation, and shape similarity,
respectively.
we compare sdt with previous state-of-the-art
segmentation methods, including dcan
with the same training settings as our sdt, we also report the performance of
skeleton with scales (ss) and traditional distance transform (dt).
5. visual comparison on histopathology image segmentation.
since the training data is relatively limited due to
the challenges in collecting medical images, we apply pixel-level and spatial-level
augmentations, including random brightness, contrast, rotation, crop, and elastic
transformation, to alleviate overﬁtting.
2.
we use the classiﬁcation learning strategy and optimize a model with 11 output
channels (10 channels for energy quantized into ten bins and one channel for
536
z. lin et al.
table 1. comparison with existing
methods on the gland segmentation.
the results suggest that the
model trained with cross-entropy loss
with α = 0.8 and local skeleton gener-
ation achieves the best performance.
we train the model for 20k iterations with an initial learning rate
of 5 × 10−4 and a momentum of 0.9.
our sdt framework achieves state-of-the-art performance on 5 out of
6 evaluation metrics on the gland segmentation dataset (table 1).
besides, under
the hausdorﬀ distance for evaluating shape-similarity between ground-truth and
predicted masks, our sdt reports an average score of 44.82 across two test splits,
which improves the previous state-of-the-art approach (i.e., fullnet with an aver-
age score of 50.15) by 10.6%.
we set
b = 0.1 for the experiments.
table 2 shows that α = 0.8 achieves the best overall
performance, which is slightly better than α = 1.0.
the results show that
pre-computed sdt signiﬁcantly degrades performance (table 2).
we argue this
is because pre-computed energy not only introduces inconsistency for instances
touching the image border but also restricts the diversity of sdt energy maps.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_1.pdf:
deep learning-based medical image enhancement methods
(e.g., denoising and super-resolution) mainly rely on paired data and
correspondingly the well-trained models can only handle one type of
task.
in this paper, we address the limitation with a diﬀusion model-
based framework that mitigates the requirement of paired data and can
simultaneously handle multiple enhancement tasks by one pre-trained
diﬀusion model without ﬁne-tuning.
experiments on low-dose ct and
heart mr datasets demonstrate that the proposed method is versatile
and robust for image denoising and super-resolution.
we believe our work
constitutes a practical and versatile solution to scalable and generalizable
image enhancement.
1
introduction
computed tomography (ct) and magnetic resonance (mr) are two widely
used imaging techniques in clinical practice.
ct imaging uses x-rays to pro-
duce detailed, cross-sectional images of the body, which is particularly useful for
imaging bones and detecting certain types of cancers with fast imaging speed.
low-dose ct techniques have been developed to
address this concern by using lower doses of radiation, but the image quality is
degraded with increased noise, which may compromise diagnostic accuracy
[9].
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43898-1_1.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. (eds.): miccai 2023, lncs 14222, pp.
mr imaging, on the other hand, uses a strong magnetic ﬁeld and radio waves
to create detailed images of the body’s internal structures, which can produce
high-contrast images for soft tissues and does not involve ionizing radiation.
motivated by the aforementioned, there is a pressing need to improve the
quality of low-dose ct images and low-resolution mr images to ensure that
they provide the necessary diagnostic information.
numerous algorithms have
been developed for ct and mr image enhancement, with deep learning-based
methods emerging as a prominent trend [5,14], such as using the conditional
generative adversarial network for ct image denoising [32] and convolutional
neural network for mr image super-resolution (sr)
these algorithms are capable of improving image quality, but they have
two signiﬁcant limitations.
first, paired images are required for training,
e.g., low-dose and full-dose ct images; low-resolution and high-resolution mr
images).
although it is possible to simulate low-quality images from high-quality
images, the models derived from such data may have limited generalization abil-
ity when applied to real data [9,14].
recently, pre-trained diﬀusion models [8,11,21] have shown great promise
in the context of unsupervised natural image reconstruction [6,7,12,28].
how-
ever, their applicability to medical images has not been fully explored due to the
absence of publicly available pre-trained diﬀusion models tailored for the med-
ical imaging community.
the training of diﬀusion models requires a signiﬁcant
amount of computational resources and training images.
for example, openai’s
improved diﬀusion models [21] took 1600–16000 a100 hours to be trained on
the imagenet dataset with one million images, which is prohibitively expensive.
several studies have used diﬀusion models for low-dose ct denoising [30] and
mr image reconstruction
[22,31], but they still rely on paired images.
in this paper, we aim at addressing the limitations of existing image enhance-
ment methods and the scarcity of pre-trained diﬀusion models for medical
images.
speciﬁcally, we provide two well-trained diﬀusion models on full-dose ct
images and high-resolution heart mr images, suitable for a range of applications
including image generation, denoising, and super-resolution.
motivated by the
existing plug-and-play image restoration methods [26,34,35] and denoising dif-
fusion restoration and null-space models (ddnm)
[12,28], we further introduce
a paradigm for plug-and-play ct and mr image denoising and super-resolution
as shown in fig.
notably, it eliminates the need for paired data, enabling
greater scalability and wider applicability than existing paired-image dependent
pre-trained diﬀusion models
5
fig.
1. comparison of (a) the common paired-image dependent paradigm and (b) the
plug-and-play paradigm for medical image enhancement.
the former needs to build
customized models for diﬀerent tasks based on paired low/high-quality images, while
the latter can share one pre-trained diﬀusion model for all tasks and only high-quality
images are required as training data.
the pre-trained model can handle unseen images
as demonstrated in experiments.
methods.
our method does not need additional training on speciﬁc tasks
and can directly use the single pre-trained diﬀusion model on multiple medical
image enhancement tasks.
2
method
this section begins with a brief overview of diﬀusion models for image generation
and the mathematical model and algorithm for general image enhancement.
we
then introduce a plug-and-play framework that harnesses the strengths of both
approaches to enable unsupervised medical image enhancement.
2.1
denoising diﬀusion probabilistic models (ddpm)
for unconditional image generation
image generation models aim to capture the intrinsic data distribution from
a set of training images and generate new images from the model itself.
[11] for unconditional medical image generation, which contains a
diﬀusion (or forward) process and a sampling (or reverse) process.
the diﬀusion
process gradually adds random gaussian noise to the input image x0, following
a markov chain with transition kernel q(xt|xt−1)
= n(xt; √1 − βtxt−1, βti),
6
j. ma et al.
where t ∈ {1, · · · , t} represents the current timestep, xt and xt−1 are adjacent
image status, and βt ∈ {β1, · · · , βt } is a predeﬁned noise schedule.
this property enables simple
model training where the input is the noisy image xt and the timestep t and the
output is the predicted noise
the
sampling process aims to generate a clean image from gaussian noise xt ∼
n(0, i), and each reverse step is deﬁned by:
xt−1 =
1
√αt

xt − 1 − αt
√1 − ¯αt
ϵθ(xt, t)

+ βtz; z ∼ n(0, i).
(2)
2.2
image enhancement with denoising algorithm
in general, image enhancement tasks can be formulated by:
y = hx + n,
(3)
where y is the degraded image, h is a degradation matrix, x is the unknown
original image, and n is the independent random noise.
this model can represent
various image restoration tasks.
for instance, in the image denoising task, h is
the identity matrix, and in the image super-resolution task, h is the downsam-
pling operator.
(6)
intuitively, idbp iteratively estimates the original image from the current
degraded image and makes a projection by constraining it with prior knowledge.
although idbp oﬀers a ﬂexible way to solve image enhancement problems, it
still requires paired images to train the denoising operator [26].
pre-trained diﬀusion models
7
2.3
pre-trained diﬀusion models for plug-and-play medical image
enhancement
we introduce a plug-and-play framework by leveraging the beneﬁts of the diﬀu-
sion model and idbp algorithm.
here we highlight two beneﬁts: (1) it removes
the need for paired images; and (2) it can simply apply the single pre-trained
diﬀusion model across multiple medical image enhancement tasks.
the ﬁrst step estimates the
denoised image x0|t based on the current noisy image xt and the trained denoising
network ϵθ(xt, t).
the second step generates a rectiﬁed image xt−1 by taking a
weighted sum of x0|t and xt and adding a gaussian noise perturbation.
as mentioned in eq.
(3), our goal is to restore an unknown original image
x0 from a degraded image y. thus, the degraded image y needs to be involved
in the sampling process.
(8), we have:
xt−1 =
√¯αt−1βt
1 − ¯αt
ˆx0|t +
√αt(1 − ¯αt−1)
1 − αt
xt + βtz.
(10)
algorithm 1 shows the complete steps for image enhancement, which inherit the
denoising operator from ddpm and the projection operator from idbp.
the
former employs the strong denoising capability in the diﬀusion model and the
latter can make sure that the generated results match the input image.
8
j. ma et al.
algorithm 1. pre-trained ddpm for plug-and-play medical image enhancement
require: pre-trained ddpm ϵθ, low-quality image y, degradation operator h
1: initialize xt ∼ n(0, i).
project x0|t on the hyperplane y = hx
5:
xt−1 =
√
¯αt−1βt
1−¯αt
ˆx0|t +
√αt(1−¯αt−1)
1−¯αt
xt + βtz, z ∼ n(0, i) // sampling
6: end for
7: return enhanced image x0
3
experiments
dataset.
we conducted experiments on two common image enhancement tasks:
denoising and sr.
to mimic the real-world setting, the diﬀusion models were
trained on a diverse dataset, including images from diﬀerent centers and scan-
ners.
the testing set (e.g., mr images) is from a new medical center that has
not appeared in the training set.
experiments show that our model can gen-
eralize to these unseen images.
notably, the presented framework elimi-
nates the requirement of paired data.
for the ct image enhancement task, we
trained a diﬀusion model
[21] based on the full-dose dataset that contains 5351
images, and the hold-out quarter-dose images were used for testing.
for the mr
enhancement task, we used the whole acdc
the testing
images were downsampled by operator h with factors of 4× and 8× to produce
low-resolution images, and the original images served as the ground truth.
evaluation metrics.
the image quality was quantitatively evaluated by the
peak signal-to-noise ratio (psnr), structural similarity index (ssim)
[29],
and visual information fidelity (vif) [24], which are widely used measures in
medical image enhancement tasks [9,17].
implementation details.
the input image size was normalized to 256 × 256 and the 2d
u-net
[13] with a batch size of 16 and a learning
rate of 10−4, and an exponential moving average (ema) over model parame-
ters with a rate of 0.9999.
all the models were trained on a100 gpu and the
total training time was 16 d. the implementation was based on ddnm
speciﬁcally, we used the
identity matrix i as the degradation operator for the denoising task and scaled
pre-trained diﬀusion models
9
the projection diﬀerence h†(hx0|t−y) with coeﬃcient σ to balance the informa-
tion from measurement y and denoising output x0|t.
the downsampling opera-
tor implemented with torch.nn.adaptiveavgpool2d for the super-resolution task.
we also compared the present
method with one commonly used image enhancement method dip [10] and two
recent diﬀusion model-based methods: ivlr
[6], which adopted low-frequency
information from measurement y to guide the generation process towards a nar-
row data manifold, and dps
notably, dps used 1000 sampling
steps while we only used 100 sampling steps.
table 1. performance (mean±standard deviation) on ct denoising task.
the arrows
indicate directions of better performance.
methods psnr ↑
ssim ↑
vif ↑
baseline
24.9 ± 2.4
0.778 ± 0.07
0.451 ± 0.07
[7]
26.5 ± 2.3
0.791 ± 0.08
0.475 ± 0.09
ours
28.3 ± 2.8 0.803 ± 0.11 0.510 ± 0.10
4
results and discussion
low-dose ct image enhancement.
the presented method outperformed
all other methods on the denoising task in all metrics, as shown in table 1, with
average psnr, ssim, and vif of 28.3, 0.803, and 0.510, respectively.
supple-
mentary fig.
our method still achieves
better performance across all metrics as shown in table 2.
by visually comparing
the enhancement results in fig. 1 (b) and (c), our results can reconstruct more
anatomical details even in the challenging noisy 8× sr task.
10
j. ma et al.
table 2. evaluation results of joint denoising and super-resolution for ct images.
[7]
25.1±2.8
0.172±0.13
0.416±0.12
24.7±3.1
0.705±0.14
0.398±0.12
ours
26.2±2.6 0.743±0.10 0.446±0.10 25.9±3.1 0.731±0.13 0.431±0.12
table 3. evaluation results of heart mr image super-resolution tasks.
[7]
25.6±0.8
0.741±0.03
0.335±0.03
21.2±0.8
0.635±0.04
0.177±0.02
ours
26.9±0.8
0.805±0.025 0.416±0.03 22.4±0.8
0.700±0.03
0.226±0.02
mr image enhancement.
to demonstrate the generality of the presented
method, we also applied it for the heart mr image 4× and 8× sr tasks, and the
quantitative results are presented in table 3.
dip obtains slightly better scores in psnr for the 4× sr
task and psnr and ssim for the 8× sr tasks, but visualized image quality is
signiﬁcantly worse than our results as shown in supplementary fig.
2, e.g., many
anatomical structures are smoothed.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_53.pdf:
pulmonary vessel segmentation in computerized tomogra-
phy (ct) images is essential for pulmonary vascular disease and surgical
navigation.
however, the existing methods were generally designed for
contrast-enhanced images, their performance is limited by the low con-
trast and the non-uniformity of hounsﬁeld unit (hu) in non-contrast
ct images, meanwhile, the varying size of the vessel structures are
not well considered in current pulmonary vessel segmentation meth-
ods.
to address this issue, we propose a hierarchical enhancement net-
work (henet) for better image- and feature-level vascular representa-
tion learning in the pulmonary vessel segmentation task.
speciﬁcally, we
ﬁrst design an auto contrast enhancement (ace) module to adjust
the vessel contrast dynamically.
then, we propose a cross-scale non-
local block (csnb) to eﬀectively fuse multi-scale features by utilizing
both local and global semantic information.
experimental results show
that our approach achieves better pulmonary vessel segmentation out-
comes compared to other state-of-the-art methods, demonstrating the
eﬃcacy of the proposed ace and csnb module.
keywords: pulmonary vessel segmentation · non-contrast ct ·
hierarchical enhancement
1
introduction
segmentation of the pulmonary vessels is the foundation for the clinical diagnosis
of pulmonary vascular diseases such as pulmonary embolism (pe), pulmonary
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. (eds.): miccai 2023, lncs 14222, pp.
accurate vascular quantitative analysis is
crucial for physicians to study and apply in treatment planning, as well as mak-
ing surgical plans.
although contrast-enhanced ct images have better contrast
for pulmonary vessels compared to non-contrast ct images, the acquisition of
contrast-enhanced ct images needs to inject a certain amount of contrast agent
to the patients.
in the literature, several conventional methods [5,16] have been proposed for
the segmentation of pulmonary vessels in contrast-enhanced ct images.
most of
these methods employed manual features to segment peripheral intrapulmonary
vessels.
in recent years, deep learning-based methods have emerged as promising
approaches to solving challenging medical image analysis problems and have
demonstrated exciting performance in segmenting various biological structures
however, for vessel segmentation, the widely used models, such as
u-net and its variants, limit their segmentation accuracy on low-contrast small
vessels due to the loss of detailed information caused by the multiple down-
sampling operations.
[17] proposed a nested structure
unet++ to redesign the skip connections for aggregating multi-scale features
and improve the segmentation quality of varying-size objects.
[1] also proposed an orthogonal
fused u-net++ for pulmonary peripheral vessel segmentation.
however, all these
methods ignored the signiﬁcant variability in hu values of pulmonary vessels at
diﬀerent regions.
to summarize, there exist several challenges for pulmonary vessel segmen-
tation in non-contrast ct images: (1) the contrast between pulmonary vessels
and background voxels is extremely low (fig. 1(c)); (2) pulmonary vessels have
a complex structure and signiﬁcant variability in vessel appearance, with diﬀer-
ent scales in diﬀerent areas.
to address the above challenges, we propose a h ierarchical enhancement
n etwork (henet) for pulmonary vessel segmentation in non-contrast ct images
by enhancing the representation of vessels at both image- and feature-level.
for
the input ct images, we propose an auto contrast enhancement (ace) module
to automatically adjust the range of hu values in diﬀerent areas of ct images.
it mimics the radiologist in setting the window level (wl) and window width
hierarchical enhancement network
553
fig.
the challenges of accurate pulmonary vessel segmentation.
(c) hard to distinguish vessels in non-contrast ct images.
2. our proposed hier-
archical enhancement network (henet) consists of two main modules: (1) auto
contrast enhancement (ace) module, and (2) cross-scale non-local block
(csnb) as the skip connection bridge between encoders and decoders.
first, the ace module
is developed to enhance the contrast of vessels in the original ct images for the
following vessel segmentation network.
2.1
auto contrast enhancement
in non-contrast ct images, the contrast between pulmonary vessels and the
surrounding voxels is pretty low.
normally, radiol-
ogists have to manually set the suitable window level (wl) and window width
(ww) for diﬀerent regions in images to enhance vessels according to the hu
value range of surrounding voxels, just as diﬀerent settings to better visualize
the extrapulmonary and intrapulmonary vessels (fig. 1(d) and (e)).
the ace module leverages convolution operations to generate dynamic wl
and ww for the input ct images according to the hu values covered by the
kernel.
it consists of two components: (1) auto
contrast enhancement module (bottom).
normalization to linearly transform the hu values of the original image x to the
range (−1, 1).
here, the learned shift map and scale map act as the window
level and window width settings of the “width/level” scaling in ct images.
it
matches the requirement of the positive integer for ww.
the upsampled shift map and scale map are denoted
as mshift and mscale, respectively, and then the contrast enhancement image
xace can be generated through:
(1)
hierarchical enhancement network
555
it can be observed that the intensity values of input x are re-centered and
re-scaled by mshift and mscale (fig. 3(c)).
the clip operation (clip(·)) truncates
the ﬁnal output into the range [−1, 1], which sets the intensity value above 1
to 1, and below –1 to –1. in our experiments, we ﬁnd that a large kernel size
for learning of mshift and mscale could deliver better performance, which can
capture more information on hu values from the ct images.
2.2
cross-scale non-local block
there are studies
[14,18] showing that non-local operations could capture long-
range dependency to improve network performance.
to segment pulmonary ves-
sels with signiﬁcant variability in scale and shape, we design a cross-scale non-
local block (csnb) to fuse the local features extracted by cnn backbone from
diﬀerent scales, and to accentuate the cross-scale dependency to address the
complex scale variations of pulmonary vessels.
inspired by [18], our csnb incorporates 6 modiﬁed asymmetric non-local
blocks (anbs), which integrate pyramid sampling modules into the non-local
blocks to largely reduce the computation and memory consumption.
2, the csnb works as the information bridge between encoders
and decoders while also ensuring the feasibility of experiments involving large
3d data.
the speciﬁc computation of anb proceeds
as follows: first, three 1×1×1 convolutions (denoted as conv(·)) are applied
to transform fh and fl into diﬀerent embeddings q, k, and v ; then, spatial
pyramid pooling operations (denoted as p(·)) are implemented on k and v .
et al.
where the ﬁnal convolution is used as a weighting parameter to adjust the impor-
tance of this non-local operation and recover the channel dimension to ch. anb-
p has the same structure as anb-h, but the inputs fh and fl here are the same,
which is the output of anb-h at the same level.
thereby, the response of multi-scale vessels can be enhanced.
3
experiments and results
dataset and evaluation metrics.
we use a total of 160 non-contrast ct
images with the inplane size of 512 × 512, where the slice number varies from
217 to 622.
the annotations of pulmonary
vessels are semi-automatically segmented in 3d by two radiologists using the
3d slicer software.
the quantitative
results are reported by dice similarity coeﬃcient (dice), mean intersection over
union (miou), false positive rate (fpr), average surface distance (asd), and
hausdorﬀ distance (hd).
implementation details.
our experiments are implemented using pytorch
framework and trained using a single nvidia-a100 gpu.
in the training stage, we randomly crop sub-volumes with
the size of 192 × 192 × 64 near the lung ﬁeld, and then the cropped sub-volumes
are augmented by random horizontal and vertical ﬂipping with a probability of
0.5.
in the testing phase, we perform the sliding window average prediction with
strides of (64, 64, 32) to cover the entire ct images.
for a fair comparison,
we use the same hyper-parameter settings and dice similarity coeﬃcient loss
across all experiments.
in particular, we use the same data augmentation, no
post-processing scheme, adam optimizer with an initial learning rate of 10−4,
and train for 800 epochs with a batch size of 4.
in our experiments, we use a
two-step optimization strategy: 1) ﬁrst, train the ace module with the basic
u-net; 2) integrate the trained ace module and a new csnb module into the
u-net, and ﬁx the parameters of ace module when training this network.
hierarchical enhancement network
557
fig.
3. the eﬀectiveness of the proposed components, with the images in red circles
generated from our method.
(a–c) the contrast of vessels is signiﬁcantly enhanced in
the ct images processed by ace module.
compared to the baseline,
both ace module and csnb lead to better performance.
with the two compo-
nents, our henet has signiﬁcant improvements over baseline on all the metrics.
, the ace module eﬀectively enhances the contrast
of pulmonary vessels at the image-level.
∗ denotes signiﬁcant improvement
compared to the baseline u-net (p<0.05).
we also compare our method with state-of-the-art
deep learning-based vessel segmentation methods, including cldice [12], cs2-
net [7], and of-net [1].
4. qualitative segmentation results.
(color ﬁgure online)
but our method has better dice and miou than cs2-net (increasing 0.56% and
0.43%, respectively), indicating the under-segmentation of cs2-net and more
vessels being correctly segmented by our method.
in the ﬁrst row of qualitative
results (fig. 4), the competing methods can produce satisfactory results for the
overall structure but generate many false positives.
furthermore, due to low con-
trast between small intrapulmonary vessels and the surrounding voxels, results
of competing methods exist many discontinuities (the second row), while our
method obtains more connective segmentation for these small vessels.
also, for
the segmentation of large extrapulmonary vessels (the last row), our method can
produce more accurate results.
this implies
that cldice tends to over-segment vessels, and it cannot obtain precise segmen-
tation for the large extrapulmonary vessels.
hierarchical enhancement network
559
4

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_37.pdf:
despite its improved comprehensive diagnostic
performance, skin disease diagnosis should aim for personalized diag-
nosis rather than centralized and generalized diagnosis, due to personal
diversities and variability, such as skin color, wrinkles, and aging.
to this
end, we propose a novel deep learning network for personalized diagno-
sis in an adaptive manner, utilizing personal characteristics in diagnos-
ing dermatitis in a mobile- and fl-based environment.
apd-net incorporates a novel architectural design that lever-
ages personalized and centralized parameters, along with a ﬁne-tuning
method based on a modiﬁed ga to identify personal characteristics.
we validated apd-net on clinical datasets and demonstrated its supe-
rior performance, compared with state-of-the-art approaches.
our exper-
imental results showed that apd-net markedly improved personalized
diagnostic accuracy by 9.9% in dermatitis diagnosis, making it a promis-
ing tool for clinical practice.
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43898-1_37.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
https://doi.org/10.1007/978-3-031-43898-1_37
fine-tuning network in federated learning for personalized skin diagnosis
379
1
introduction
for the past several years, in skin disease diagnosis, deep learning (dl) tech-
niques have been extensively studied, due to its eﬀectiveness and outstanding
diagnostic performance [8,16,19].
for example, wu et al. used a custom dataset
to develop an eﬃcientnet-b4-based dl model and successfully diagnosed skin
diseases [19]. srinivasu et al. designed an advanced dl model, by combining
long short-term memory (lstm) with mobilenet and achieved improved per-
formance as well as fast prediction time in skin disease diagnosis
for the
development of dl models, a large number of datasets are needed for accurate
model ﬁtting at the training stage.
as such, the performance of dl
models is often limited, due to a small number of datasets [2,16,20].
fig.
1. pipeline of adp-net framework for a personalized diagnosis.
to overcome the limitations mentioned above, federated learning (fl) can be
a viable solution in the context of digital healthcare, especially in the covid-
19 pandemic era [15].
moreover, fl allows to acquire many heterogeneous
images from edge devices at multiple medical sites [1,6,7,10,15].
for example,
b. mcmahan et al. developed a protocol to average the gradients from decen-
tralized clients, without data sharing [10].
k. bonawitz et al. built high-level fl
systems and architectures in a mobile environment
however, dl models in
the fl environment were optimized to deal with datasets from multiple clients;
therefore, while the dl models yielded a generalized prediction capability across
all of the domains involved, the dl models cannot eﬃciently perform personal-
ized diagnosis, which is deemed a weakness of fl.
[4], human against machine (ham)
380
k. lee et al.
experimental results demonstrated that the apd-net yielded outstanding per-
formance, compared with other comparison methods, and achieved adaptively
personalized diagnosis.
the contributions of this paper are three-fold:
• we developed a mobile- and fl-based learning (apd-net) for skin disease
diagnosis and achieved superior performance on skin disease diagnosis for
public and custom datasets.
• we introduce a customized ga for apd-net, combined with a corresponding
network architecture, resulting in improved personalized diagnostic perfor-
mance as well as faster prediction time.
• we provide a new ﬂuorescence dataset for skin disease diagnosis containing
2,490 images for four classes, including eczema, dermatitis, rosacea, and
normal.
2.1
dual-pipeline (dp) architecture for apd-net
since the proposed fl system is implemented in a mobile-based environment,
mobilenetv3 is employed as a baseline network of apd-net.
2.2
customized genetic algorithm
in the fl environment, data privacy is achieved by transferring gradients with-
out sharing data.
the ga is the optimal solution for adaptively personalized
diagnosis in the fl environment, since it heuristically searches for another local
minimum point regardless of the domain gaps.
here, p (i)
f |k is the average value of pg|k and p ∗
p |k if g(i)
k
= 0.
in addition, since we experimentally demonstrated that lk ≥ 0.15 provides
a much longer time to ﬁne-tune apd-net, we constrained lk ≤ 0.15, and the
ﬁxed values of lk are randomly determined for each experiment.
while training the dl model, we experimentally veriﬁed that the convo-
lution weights are changed within the range of the maximum 0.2%.
let c(i; p)
be the output of part i using the parameter p with an input image (i).
therefore, the apd-net with our ga oﬀers
high accuracy in both the conventional diagnosis for overall patients and the
personalized diagnosis for each patient at a speciﬁc client.
fine-tuning network in federated learning for personalized skin diagnosis
383
2.3
training and fine-tuning apd-net
to summarize, (1) apd-net is initially trained in the fl server using gradi-
ents from many clients.
conﬁgurations of datasets (left) and experiment i (right).
datasets
nevus
melanoma
others
total
7pt
575
268
168
1011
isic
5193
284
27349
32826
ham
6705
1113
2197
10015
testset (25%)
nevus
melanoma
others
total
7pt
144
67
42
253
isic
1299
71
6838
8207
ham
1677
279
550
2504
# images
(# for testset)
group
nevus
melanoma
others
7pt
g-00
575 (144)
268 (67)
168 (42)
isic
g-01
180 (45)
80 (20)
17070 (4268)
g-02
685 (172)
363 (91)
7915 (1979)
g-03
4328 (1082)
(left) the number of skin images in the customized dataset for experiment
ii.
[14]
✓
ours
fl + da
✓
✓
✓
✓
3
experimental results
3.1
experimental setup
dataset.
to evaluate the performance and feasibility of apd-net, we used
three public datasets, including 7pt, isic, and ham, and detailed descriptions
for datasets are illustrated in table 1.
furthermore, in this work, we collected
skin images through the tertiary referral hospital under the approval of the
institutional review board (irb no. 1908-161-1059) and obtained images with
the consent of the subjects according to the principles of the declaration of
helsinki from 51 patients and subjects.
3. re-sampling the distribution of images in each client for experiment i
table 3.
4. classiﬁcation accuracy of apd-nets in each fl client
including eczema, dermatitis, rosacea, and normal skin, with 258, 294, 738, and
1,200 images, respectively, as illustrated in table 2(left).
to compensate for the limited number of images in the test set, a 4-fold
cross-validation approach was employed.
to assess the performance of the pro-
posed network as well as compared networks, two distinct fl environments were
considered: (1) an fl simulation environment to evaluate the performance of
apd-net and (2) a realistic fl environment to analyze the feasibility of apd-
net.
for the fl simulation environment in experiment i, public datasets were
employed, and the distribution of samples was re-sampled using t-distributed
stochastic neighbor embedding (t-sne)
the images in all skin datasets
were subsequently re-grouped, as illustrated in fig.
3. in contrast, for experi-
ment ii, we utilized a custom dataset for a realistic fl environment.
six dl
models that have shown exceptional performances in da, fl, and skin disease
diagnosis were used as comparison methods to evaluate the fl and dl perfor-
mance of apd-net.
fine-tuning network in federated learning for personalized skin diagnosis
385
3.2
experiment i.
an ablation study was conducted to evaluate the impact of
ga and dp on diagnostic performance.
as illus-
trated in table 3(left), the apd-net with ga and dp yielded the best perfor-
mance.
here, it is important to note that the dp architecture also improved the
performance of the models for adaptively personalized diagnosis, similar to ga,
by jointly using personalized and generalized parameters in the dp architecture.
performance of apd-net was compared against those
of the other dl models for adaptively personalized diagnosis.
table 3(right)
shows the performances of the dl models in every client (group).
furthermore, fig. 4 demonstrates that the
prediction with images from other clients yielded lower accuracy.
fig.
(right) comparison analysis of apd-net and the other dl models in a
desirable fl environment.
386
k. lee et al.
3.3
experiment ii.
fl environment
to verify the feasibility of apd-net, the performance of apd-net was evaluated
using the customized datasets acquired from our devices for adaptively person-
alized diagnosis.
the performance of apd-nets was compared against the other
dl models.
since the prediction time is critical in the mobile-based environ-
ment, the prediction times of the dl models were compared in addition to the
accuracy.
, apd-net (ours) yielded an outstanding
performance as well as a shorter prediction time compared with the other dl
models for adaptively personalized diagnosis.
5(right) shows
the performances of apd-nets for adaptively personalized diagnosis in every
client.
6(left) illustrates
the performance of apd-net.
since the number of images is relatively small, the
images were divided into three clients.
the results showed that our apd-net has the
potential to be used in the fl environment with an accuracy of 88.51%.
the ﬁtness score
and accuracy were calculated corresponding to many input images and various
versions of the ﬁne-tuned parameters.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_18.pdf:
we investigate performance disparities in deep classiﬁers.
we
ﬁnd that the ability of classiﬁers to separate individuals into subgroups
varies substantially across medical imaging modalities and protected char-
acteristics; crucially, we show that this property is predictive of algorith-
mic bias.
through theoretical analysis and extensive empirical evalua-
tion (code is available at https://github.com/biomedia-mira/subgroup-
separability), we ﬁnd a relationship between subgroup separability, sub-
group disparities, and performance degradation when models are trained
on data with systematic bias such as underdiagnosis.
our ﬁndings shed new
light on the question of how models become biased, providing important
insights for the development of fair medical imaging ai.
1
introduction
medical image computing has seen great progress with the development of deep
image classiﬁers, which can be trained to perform diagnostic tasks to the level of
skilled professionals [19].
recently, it was shown that these models might rely on
sensitive information when making their predictions [7,8] and that they exhibit
performance disparities across protected population subgroups [20].
although
many methods exist for mitigating bias in image classiﬁers, they often fail unex-
pectedly and may even be harmful in some situations [26].
today, no bias miti-
gation methods consistently outperform the baseline approach of empirical risk
minimisation (erm)
[22,27], and none are suitable for real-world deployment.
if we wish to deploy appropriate and fair automated systems, we must ﬁrst
understand the underlying mechanisms causing erm models to become biased.
some medical
images encode sensitive information that models may leverage to classify indi-
viduals into subgroups [7].
we may expect
groups with intrinsic physiological diﬀerences to be highly separable for deep
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43898-1 18.
https://doi.org/10.1007/978-3-031-43898-1_18
180
c. jones et al.
image classiﬁers (e.g. biological sex from chest x-ray can be predicted with
> 0.98 auc).
this is especially relevant in
medical imaging, where attributes such as age, biological sex, self-reported race,
socioeconomic status, and geographic location are often considered sensitive for
various clinical, ethical, and societal reasons.
we show that the ability of
models to detect which group an individual belongs to varies across modalities
and groups in medical imaging and that this property has profound consequences
for the performance and fairness of deep classiﬁers.
to the best of our knowledge,
ours is the ﬁrst work which analyses group-fair image classiﬁcation through the
lens of subgroup separability.
– we show theoretically that such diﬀerences in subgroup separability aﬀect
model bias in learned classiﬁers and that group fairness metrics may be inap-
propriate for datasets with low subgroup separability.
– we corroborate our analysis with extensive testing on real-world medical
datasets, ﬁnding that performance degradation and subgroup disparities are
functions of subgroup separability when data is biased.
2
related work
group-fair image analysis seeks to mitigate performance disparities caused by
models exploiting sensitive information.
follow-up work has addi-
tionally shown that these models may use sensitive information to bias their
predictions [7,8].
unfortunately, standard bias mitigation methods from com-
puter vision, such as adversarial training [1,14] and domain-independent training
[24], are unlikely to be suitable solutions.
on
natural images, zietlow et al.
[26] showed that bias mitigation methods worsen
performance for all groups compared to erm, giving a stark warning that blindly
applying methods and metrics leads to a dangerous ‘levelling down’ eﬀect [16].
one step towards overcoming these challenges and developing fair and perfor-
mant methods is understanding the circumstances under which deep classiﬁers
learn to exploit sensitive information inappropriately.
closely related to our work is oakden-rayner et al., who
consider how ‘hidden stratiﬁcation’ may aﬀect learned classiﬁers [18]; similarly,
jabbour et al. use preprocessing ﬁlters to inject spurious correlations into chest
x-ray data, ﬁnding that erm-trained models are more biased when the corre-
lations are easier to learn [12]. outside of fairness, our work may have broader
impact in the ﬁelds of distribution shift and shortcut learning [6,25], where many
examples exist of models learning to exploit inappropriate spurious correlations
[3,5,17], yet tools for detecting and mitigating the problem remain immature.
subgroup separability in medical image classiﬁcation
181
3
the role of subgroup separability
consider a binary disease classiﬁcation problem where, for each image x ∈ x, we
wish to predict a class label
→ [0, 1] the
underlying mapping between images and class labels.
suppose we have access to a
(biased) training dataset, where ptr is the conditional distribution between train-
ing images and training labels; we say that such a dataset is biased if ptr ̸= p.
we focus on group fairness, where each individual belongs to a subgroup a ∈ a
and aim to learn a fair model that maximises performance for all groups when
deployed on an unbiased test dataset drawn from p. we assume that the groups
are consistent across both datasets.
the bias we consider in this work is under-
diagnosis, a form of label noise [4] where some truly positive individuals x+ are
mislabeled as negative.
we are particularly concerned with cases where under-
diagnosis manifests in speciﬁc subgroups due to historic disparities in healthcare
provision or discriminatory diagnosis policy.
≤ p(y|x+, a∗) and ∀a ̸= a∗, ptr(y|x+, a) = p(y|x+, a)
(1)
we may now use the law of total probability to express the overall mapping
from image to label in terms of the subgroup-wise mappings in eq.
(3) – the probability of a truly positive individual
being assigned a positive label is lower in the biased training dataset than for
the unbiased test set.
ptr(y|x) =

a∈a
ptr(y|x, a)ptr(a|x)
(2)
ptr(y|x+) ≤ p(y|x+)
(3)
at training time, supervised learning with empirical risk minimisation aims
to obtain a model ˆp, mapping images to predicted labels ˆy = argmaxy∈y ˆp(y|x)
such that ˆp(y|x)
since this model approximates the biased
training distribution, we may expect underdiagnosis from the training data to be
reﬂected by the learned model when evaluated on the unbiased test set.
this model underdiagnoses group
a = a∗ whilst recovering the unbiased mapping for other groups.
(5) show that, at test-time, our model will demonstrate
worse performance for the underdiagnosed subgroup than the other subgroups.
indeed, consider true positive rate (tpr) as a performance metric.
the group-
wise tpr of an unbiased model, tpr(u)
a , is expressed in eq.
remember, in practice, we must train our model on the biased train-
ing distribution ptr.
(8) demonstrate that
tpr of the underdiagnosed group is directly aﬀected by bias from the training
set while other groups are mainly unaﬀected.
given this diﬀerence across groups,
an appropriately selected group fairness metric may be able to identify the bias,
in some cases even without access to an unbiased test set [23].
ˆp(y|x+, a) ≈ ptr(y|x+), ∀a ∈ a
(9)
equations (3) and (9) imply that the performance of the trained model
degrades for all groups.
(10) represents
performance degradation for all groups when separability is poor.
in such sit-
uations, we expect performance degradation to be uniform across groups and
thus not be detected by group fairness metrics.
tpr(b)
a
≈ |ptr(y|x+, a) > 0.5|
n+,a
≤ |p(y|x+, a) > 0.5|
n+,a
≈ tpr(u)
a , ∀a ∈ a
(10)
we have derived the eﬀect of underdiagnosis bias on classiﬁer performance
for the two extreme cases of high and low subgroup separability.
4, we empirically investigate (i) how subgroup separa-
bility varies in the wild, (ii) how separability impacts performance for each group
when underdiagnosis bias is added to the datasets, (iii) how models encode sen-
sitive information in their representations.
subgroup separability in medical image classiﬁcation
183
4
experiments and results
we support our analysis with experiments on ﬁve datasets adapted from a subset
of the medfair benchmark [27].
[9,10,21], fundus images [15], and chest x-ray
we record
summary statistics for the datasets used in the supplementary material (table
a1), where we also provide access links (table a2).
our architecture and hyper-
parameters are listed in table a3, adapted from the experiments in medfair.
mean and standard deviation are
reported over ten random seeds, with results sorted by ascending mean auc.
dataset-attribute
modality
subgroups
auc
group 0 group 1
μ
σ
papila-sex
fundus image
male
female
0.642 0.057
ham10000-sex
skin dermatology male
female
0.723 0.015
ham10000-age
skin dermatology <60
≥60
0.803 0.020
papila-age
fundus image
<60
≥60
0.812 0.046
fitzpatrick17k-skin skin dermatology i–iii
iv–vi
0.891 0.010
chexpert-age
chest x-ray
<60
≥60
0.920 0.003
mimic-age
chest x-ray
<60
≥60
0.930 0.002
chexpert-race
chest x-ray
white
non-white 0.936 0.005
mimic-race
chest x-ray
white
non-white 0.951 0.004
chexpert-sex
chest x-ray
male
female
0.980 0.020
mimic-sex
chest x-ray
male
female
0.986 0.008
age is consistently
well predicted across all modalities, whereas separability of biological sex varies,
184
c. jones et al.
with prediction of sex from fundus images being especially weak.
performance degradation under label bias
we now test our theoretical ﬁnding: models are aﬀected by underdiagnosis dif-
ferently depending on subgroup separability.
we inject underdiagnosis bias into
each training dataset by randomly mislabelling 25% of positive individuals in
group 1 (see table 1) as negative.
for each dataset-attribute combination, we
train ten disease classiﬁcation models with the biased training data and ten mod-
els with the original clean labels; we test all models on clean data.
we assess
how the test-time performance of the models trained on biased data degrades
relative to models trained on clean data.
we illustrate the mean percentage point
accuracy degradation for each group in fig.
and use the mann-whitney u test
(with the holm-bonferroni adjustment for multiple hypothesis testing) to deter-
mine if the performance degradation is statistically signiﬁcant at pcritical = 0.05.
we include an ablation experiment over varying label noise intensity in fig.
1. percentage-point degradation in accuracy for disease classiﬁers trained on
biased data, compared to training on clean data.
lower values indicate worse per-
formance for the biased model when tested on a clean dataset.
3. we report
no statistically signiﬁcant performance degradation for dataset-attribute combi-
nations with low subgroup separability (<0.9 auc).
in these experiments, the
proportion of mislabelled images is small relative to the total population; thus,
the underdiagnosed subgroups mostly recover from label bias by sharing the
subgroup separability in medical image classiﬁcation
185
correct mapping with the uncorrupted group.
while we see surprising improve-
ments in performance for papila, note that this is the smallest dataset, and
these improvements are not signiﬁcant at pcritical = 0.05.
as subgroup separabil-
ity increases, performance degrades more for the underdiagnosed group (group
1), whilst performance for the uncorrupted group (group 0) remains somewhat
unharmed.
we see a statistically signiﬁcant performance drop for group 0 in the
mimic-sex experiment – we believe this is because the model learns separate
group-wise mappings, shrinking the eﬀective size of the dataset for group 0.
use of sensitive information in biased models
finally, we investigate how biased models use sensitive information.
[7,8] to all
models trained for the previous experiment, involving freezing the trained back-
bone and re-training the ﬁnal layer to predict the sensitive attribute.
we ﬁnd that models trained on biased data
learn to encode sensitive information in their representations and see a statisti-
cally signiﬁcant association between the amount of information available and the
amount encoded in the representations.
models trained on unbiased data have
no signiﬁcant association, so do not appear to exploit sensitive information.
fig.
along the maximum sensitive information
line, models trained for predicting the disease encode as much sensitive information in
their representations as the images do themselves.
186
c. jones et al.
5
discussion
we investigated how subgroup separability aﬀects the performance of deep neural
networks for disease classiﬁcation.
in fairness liter-
ature, data is often assumed to contain suﬃcient information to identify indi-
viduals as subgroup members.
our results are not exhaustive – there are
many modalities and sensitive attributes we did not consider – however, by
demonstrating a wide range of separability results across diﬀerent attributes and
modalities, we highlight a rarely considered property of medical image datasets.
performance degradation is a function of subgroup separability.
we showed,
theoretically and empirically, that the performance and fairness of models trained
on biased data depends on subgroup separability.
when separability is high,
models learn to exploit the sensitive information and the bias is reﬂected by stark
subgroup diﬀerences.
this indicates that group
fairness metrics may be insuﬃcient for detecting bias when separability is low.
our analysis centred on bias in classiﬁers trained with the standard approach
of empirical risk minimisation – future work may wish to investigate whether
subgroup separability is a factor in the failure of bias mitigation methods and
whether it remains relevant in further image analysis tasks (e.g. segmentation).
sources of bias matter.
in our experiments, we injected underdiagnosis bias
into the training set and treated the uncorrupted test set as an unbiased ground
truth.
however, this is not an endorsement of the quality of the data.
at least
some of the datasets may already contain an unknown amount of underdiagnosis
bias (among other sources of bias)
this pre-existing bias will likely have
a smaller eﬀect size than our artiﬁcial bias, so it should not play a signiﬁcant
role in our results.
still, the unmeasured bias may explain some variation in
results across datasets.
future work should investigate how subgroup separability
interacts with other sources of bias.
we renew the call for future datasets to be
released with patient metadata and multiple annotations to enable analysis of
diﬀerent sources and causes of bias.
reproducibility and impact.
we provide a complete implementation
of our preprocessing, experimentation, and analysis of results at https://github.
com/biomedia-mira/subgroup-separability.
subgroup separability in medical image classiﬁcation
187
acknowledgements.
references
1. alvi, m., zisserman, a., nell˚aker, c.: turning a blind eye: explicit removal of biases
and variation from deep neural network embeddings.
potential sources of dataset bias complicate
investigation of underdiagnosis by machine learning algorithms.
https://doi.org/10.1038/s42256-020-00257-z
7. gichoya, j.w., et al.: ai recognition of patient race in medical imaging: a mod-
elling study.
https://doi.org/10.1016/j.ebiom.2023.104467
9. groh, m., harris, c., daneshjou, r., badri, o., koochek, a.: towards transparency
in dermatology image datasets with skin tone annotations by experts, crowds,
and an algorithm.
groh, m., et al.: evaluating deep neural networks trained on clinical images in
dermatology with the ﬁtzpatrick 17k dataset.
kim, b., kim, h., kim, k., kim, s., kim, j.: learning not to learn: training deep
neural networks with biased data.
kovalyk, o., morales-s´anchez, j., verd´u-monedero, r., sell´es-navarro, i., palaz´on-
cabanes, a., sancho-g´omez, j.l.: papila: dataset with fundus images and clini-
cal data of both eyes of the same patient for glaucoma assessment.
https://doi.org/10.1038/s41597-022-01388-1
16. mittelstadt, b., wachter, s., russell, c.: the unfairness of fair machine learning:
levelling down and strict egalitarianism by default, january 2023
17.
https://doi.org/10.1145/3368555.3384468
19. rajpurkar, p., et al.: chexnet: radiologist-level pneumonia detection on chest x-
rays with deep learning, november 2017
20. seyyed-kalantari, l., zhang, h., mcdermott, m.b., chen, i.y., ghassemi, m.:
underdiagnosis bias of artiﬁcial intelligence algorithms applied to chest radiographs
in under-served patient populations.
tschandl, p., rosendahl, c., kittler, h.: the ham10000 dataset, a large collection
of multi-source dermatoscopic images of common pigmented skin lesions.
wachter, s., mittelstadt, b., russell, c.: bias preservation in machine learning:
the legality of fairness metrics under eu non-discrimination law.
wang, z., et al.: towards fairness in visual recognition: eﬀective strategies for bias
mitigation.
zong, y., yang, y., hospedales, t.: medfair: benchmarking fairness for medical
imaging.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_15.pdf:
since mhsi
is a 3d hypercube, building a 3d segmentation network is the most
intuitive way for mhsi segmentation.
but, high spatiospectral dimen-
sions make it diﬃcult to perform eﬃcient and eﬀective segmentation.
by plugging our dual-stream strategy into 2d backbones,
we can achieve state-of-the-art mhsi segmentation performances with 3–
13 times faster compared with existing 3d networks in terms of inference
speed.
experiments show our strategy leads to remarkable performance
gains in diﬀerent 2d architectures, reporting an improvement up to 7.7%
compared with its 2d counterpart in terms of dsc on a public multi-
dimensional choledoch dataset.
keywords: medical hyperspectral images · mhsi segmentation
1
introduction
medical hyperspectral imaging (mhsi) is an emerging imaging modality which
acquires two-dimensional medical images across a wide range of electromag-
netic spectrum.
typically, an mhsi is presented as a hypercube, with hundreds
of narrow and contiguous spectral bands in spectral dimension, and thousands
of pixels in spatial dimension (fig. 1(a)).
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43901-8_15.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
https://doi.org/10.1007/978-3-031-43901-8_15
factor space and spectrum
153
(b)
posive
negave
spatial dimension 
spatial dimension 
(a)
2d-encode
2d-decoder
pca
mask
(d) pca-2d-net
2d-encoder
2d-decoder
mask
(c) 2d-net
3d-encoder
3d-decoder
mask
spectral-
pooling
(e) 3d-net
2d-encoder
2d-decoder
1d-encoder
spectral
mask
(f) dual-stream
550      700  
850    1000  
spectral correlation
550      700  
850    1000  
0   10  20
30   40   50   60  
spectral bands
0.5  0.6
0.7   0.8   0.9 
values
spatial correlation
fig.
(c)–(f): hsi classiﬁcation and segmentation backbones.
due to the success of 2-dimensional (2d) deep neural network in natural
images, the simplest way to classify/segment an mhsi is to treat its two spa-
tial dimensions as input spatial dimension, and treat its spectral dimension as
input channel dimension [25] (fig. 1(c)).
dimensionality reduction [12] and recur-
rent approaches
these methods are not suitable
for high spatial resolution mhsi, and they may bring noises in spatial features
while reducing spectral dimension.
high spatiospectral dimensions make it diﬃcult to perform a thorough anal-
ysis of mhsi.
exploring mhsi’s
low-rank prior can promote the segmentation performance.
in this paper, we consider treating spatiospectral dimensions separately and
propose an eﬀective and eﬃcient dual-stream strategy to “factor” the archi-
tecture, by exploiting the correlation information of mhsis.
for
the spatial feature extraction stream, inspired from spatial redundancy between
adjacent bands, we group adjacent bands into a spectral agent.
diﬀerent spec-
tral agents are fed into a 2d cnn backbone as a batch.
we also show that with our proposed strategy,
u-net model using resnet-34 can achieve state-of-the-art mhsi segmentation
with 3–13 faster than other 3d architectures.
the goal of mhsi segmentation is to predict the per-pixel annotation mask
ˆy ∈ {0, 1}h×w .
it represents a spatial stream, which focuses on extracting spa-
tial features from spectral agents (sect. 2.1).
the lightweight spectral stream
learns multi-granular spectral features, and it consists of three key modules:
depthwise convolution (dwconv), spectral matrix decomposition (smd) and
feed forward network, where smd module eﬀectively leverages low-rank prior
from spectral features (sect. 2.1).
the
input mhsi z is decomposed into a spatial input zspa ∈ rg×(s/g)×h×w and
a spectral input zspe ∈ rs×cspe
0
×h×w , where g indicates evenly dividing spec-
tral bands into g groups, i.e., spectral agents.
s/g and cspe
0
= 1 are the input
feature dimensions for two streams respectively.
2.1
dual-stream architecture with spatiospectral representation
as mentioned above, for the spatial stream, we ﬁrst reshape mhsi z ∈ rs×h×w
into zspa ∈ rg×(s/g)×h×w , which has g spectral agents.
each spectral agent is
factor space and spectrum
155
spectral encoder block
encoder block
decoder block
low-rank decomposition
g
h
w
s × c0
spe
an mhsi goes through a spectral
stream with the proposed spectral encoder block which consists of three key mod-
ules, i.e, dwconv, spectral matrix decomposition (smd) and feed forward network
(ffn), and it goes through a spatial stream after dividing into g spectral agents.
our
spectral encoder block can be formulated by:
x = dwconv(zin), x′ = smd(norm(x))+ x, zout = ffn(norm(x′))+x′,
(1)
where zin ∈ rs×cspe×h×w indicates the input spectral token tensor, and cspe
is the spectral feature dimension.
2, spectral information is integrated from
channel dimensions by performing concatenation, after the second and fourth
encoder blocks, to aggregate the spatiospectral features.
the reason for this
design is that spectral features are simpler and lack hierarchical structures com-
pared to spatial features, we will discuss more in the experimental section.
finally, we aggregate all rank-1 tensors (from a1 to ar) into the attention
map along the channel dimension, followed by a linear layer used to reduce the
feature dimension to obtain the low-rank feature ulow:
ulow = u ⊙ linear(concate(a1, a2, ..., ar)),
(3)
where ⊙ is the element-wise product, and ulow ∈ rc′×h′×w ′. we employ a
straightforward non-parametric ensemble approach for grouping spectral agents.
this approach involves multiple agents combining their features by averaging
the vote.
the encoders in the spatial stream produce 2d feature maps with g
spectral agents, deﬁned as fi ∈ rg×ci×h/2i×w/2i for the ith encoder, where g,
ci, h/2i, and w/2i represent the spectral, channel, and two spatial dimensions,
respectively.
f g
i ), where
f g
i
∈ rci×h/2i×w/2i represents the 2d feature map of the gth agent.
the
ensemble operation aggregates spectral agents to produce a 2d feature map
factor space and spectrum
157
table
sa denote the spectral agent.
✓
✓
✓
✓
✓
×
55.01 (15.54)
69.58 (14.10)
86.25 (35.40)
✓
×
✓
×
✓
✓
56.90 (17.38) 70.88 (15.05) 82.72 (31.77)
✓
×
conv ×
conv ✓
55.19 (18.58)
69.15 (16.63)
83.13 (31.08)
✓
×
tr
×
tr
✓
55.72 (17.16)
69.89 (15.26)
84.48 (32.13)
with enhanced information interactions learned from the multi-spectral agents.
the feature maps obtained from the ensemble can be decoded using lightweight
2d decoders to generate segmentation masks.
3
experimental results
3.1
experimental setup
we conducted experiments on the public multi-dimensional choledoch (mdc)
dataset
[31] with 538 scenes and hyperspectral gastric carcinoma (hgc)
dataset [33] (data provided by the author) with 414 scenes, both with high-
quality labels for binary mhsi segmentation tasks.
the size of a single band image in mdc and
hgc are both resized to 256 × 320.
we use data augmentation techniques such as rotation and ﬂipping, and train
with an adam optimizer using a combination of dice loss and cross-entropy
loss for 8 batch size and 100 epochs.
the segmentation performance is evalu-
ated using dice-sørensen coeﬃcient (dsc), intersection of union (iou), and
hausdorﬀ distance (hd) metrics, and throughput (images/s) is reported for
158
b. yun et al.
fig.
left ﬁgures plot each
feature embedding in ascending order of the number of times they are dominant in the
population (y-axis) and feature dimension (x-axis) on the statistical results of the test
set.
pytorch framework and four nvidia geforce rtx 3090 are used
for implementation.
3.2
evaluation of the proposed dual-stream strategy
ablation study.
as shown in table 1,
our ablation study shows that spectral agent strategy improves segmentation
performance by more than 2.5% (63.11 vs. 66.05).
if we utilize spectral infor-
mation from the spectral stream to assist in the spatial stream, we ﬁnd that
inserting spectral information at l2 and l4 yields a signiﬁcant improvement of
3.7% (69.73 vs. 66.05), while inserting at l4 alone also results in a signiﬁcant
increase of 1.9% in dsc (67.95 vs. 66.05).
a slight improvement is observed when
inserting at l2, possibly due to the coarse features of shallow spectral informa-
tion.
our proposed ld module is crucial, resulting in a 1.12% performance drop in
terms of dsc without it.
performance comparison in “mean (std)” in mdc and hcg dataset.
+ours
fpn
73.01 (13.84) 78.37 (30.26) 81.88 (14.75) 68.60 (45.63)
table 3. performance comparison with sota methods with throughput(images/s)
on mdc dataset and hgc dataset.
[28]
73.66
76.92 1.40
84.74
66.90
2.44
smd and ld modules can reduce feature redundancy and lead to an increase
in segmentation performance.
comparisons between w/ and w/o dual-stream strategy on diﬀerent
backbones.
to show the eﬀectiveness of our dual-stream strategy in improv-
ing mhsi segmentation performance in various architectures, we plug it into
diﬀerent segmentation methods, i.e., u-net
the results
obtained with the proposed dual-stream strategy can consistently boost the seg-
mentation performance by a large margin.
160
b. yun et al.
comparisons with state-of-the-art mhsi segmentation methods.
experimental
results show that 2d methods are generally faster than 3d methods in inference
speed, but 3d methods have an advantage in segmentation performance (dsc
& hd).
however, our approach outperforms other methods in both inference
speed and segmentation accuracy.
it is also plug-and-play, with the potential
to achieve better segmentation performance by selecting more powerful back-
bones.
the complete table (including iou and variance) and qualitative results
are shown in the supplementary material.
4
conclusion
in this paper, we present to factor space and spectrum for accurate and fast
medical hyperspectral image segmentation.
experiments show signiﬁcant performance improvements on
diﬀerent evaluation metrics, e.g., with our proposed strategy, we can obtain over
7.7% improvement in dsc compared with its 2d counterpart.
after plugging
our strategy into resnet-34 backbone, we can achieve state-of-the-art mhsi
segmentation accuracy with 3–13 times faster in terms of inference speed than
existing 3d networks.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_10.pdf:
recent researches on cancer segmentation in dynamic con-
trast enhanced magnetic resonance imaging (dce-mri) usually resort to
the combination of temporal kinetic characteristics and deep learning to
improve segmentation performance.
however, the diﬃculty in accessing
complete temporal sequences, especially post-contrast images, hinders
segmentation performance, generalization ability and clinical application
of existing methods.
in this work, we propose a diﬀusion kinetic model
(dkm) that implicitly exploits hemodynamic priors in dce-mri and
eﬀectively generates high-quality segmentation maps only requiring pre-
contrast images.
we speciﬁcally consider the underlying relation between
hemodynamic response function (hrf) and denoising diﬀusion process
(ddp), which displays remarkable results for realistic image generation.
our proposed dkm consists of a diﬀusion module (dm) and segmen-
tation module (sm) so that dkm is able to learn cancer hemodynamic
information and provide a latent kinetic code to facilitate segmenta-
tion performance.
once the dm is pretrained, the latent code estimated
from the dm is simply incorporated into the sm, which enables dkm to
automatically and accurately annotate cancers with pre-contrast images.
to our best knowledge, this is the ﬁrst work exploring the relationship
between hrf and ddp for dynamic mri segmentation.
we evaluate
the proposed method for tumor segmentation on public breast cancer
dce-mri dataset.
compared to the existing state-of-the-art approaches
with complete sequences, our method yields higher segmentation perfor-
mance even with pre-contrast images.
keywords: deep learning · kinetic representation · dce-mri ·
cancer segmentation · denoising diﬀusion model
1
introduction
dynamic contrast-enhanced magnetic resonance imaging (dce-mri) reveal-
ing tumor hemodynamics information is often applied to early diagnosis and
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
https://doi.org/10.1007/978-3-031-43901-8_10
diﬀusion kinetic model for cancer segmentation
101
fig.
x0 and xk
represent pre-contrast images and post-contrast images in dce-mri, respectively.
treatment of breast cancer
in particular, automatically and accurately seg-
menting tumor regions in dce-mri is vital for computer-aided diagnosis (cad)
and various clinical tasks such as surgical planning.
for the sake of promoting
segmentation performance, recent methods utilize the dynamic mr sequence and
exploit its temporal correlations to acquire powerful representations [2–4].
more
recently, a handful of approaches take advantage of hemodynamic knowledge
and time intensity curve (tic) to improve segmentation accuracy
[5,6]. how-
ever, the aforementioned methods require the complete dce-mri sequences and
overlook the diﬃculty in assessing complete temporal sequences and the missing
time point problem, especially post-contrast phase, due to the privacy protection
and patient conditions.
hence, these breast cancer segmentation models cannot
be deployed directly in clinical practice.
[7,8] has produced
a tremendous impact on image generation ﬁeld due to its impressive performance.
diﬀusion model is composed of a forward diﬀusion process that add noise to
images, along with a reverse generation process that generates realistic images
from the noisy input [8].
based on this, several methods investigate the potential
of ddpm for natural image segmentation [9] and medical image segmentation
[9] explores the intermediate activations
from the networks that perform the markov step of the reverse diﬀusion process
and ﬁnd these activations can capture semantic information for segmentation.
however, the applicability of ddpm to medical image segmentation are still
limited.
in addition, existing ddpm-based segmentation networks are generic
and are not optimized for speciﬁc applications.
in particular, a core question for
dce-mri segmentation is how to optimally exploit hemodynamic priors.
by designing a net-
work architecture to eﬀectively transmute pre-contrast images into post-contrast
images, the network should acquire hemodynamic inherent in hrf that can be
used to improve segmentation performance.
inspired by the fact that ddpm
generates images from noisy input provided by the parameterized gaussian pro-
cess, this work aims to exploit implicit hemodynamic information by a diﬀusion
process that predict post-contrast images from noisy pre-contrast images.
specif-
ically, given the pre-contrast and post-contrast images, the latent kinetic code is
learned using a score function of ddpm, which contains suﬃcient hemodynamic
characteristics to facilitate segmentation performance.
once the diﬀusion module is pretrained, the latent kinetic code can be easily
generated with only pre-contrast images, which is fed into a segmentation module
to annotate cancers.
to verify the eﬀectiveness of the latent kinetic code, the
sm adopts a simple u-net-like structure, with an encoder to simultaneously
conduct semantic feature encoding and kinetic code fusion, along with a decoder
to obtain voxel-level classiﬁcation.
in this manner, our latent kinetic code can
be interpreted to provide tic information and hemodynamic characteristics for
accurate cancer segmentation.
we verify the eﬀectiveness of our proposed diﬀusion kinetic model (dkm)
on dce-mri-based breast cancer segmentation using breast-mri-nact-pilot
dataset
compared to the existing state-of-the-art approaches with complete
sequences, our method yields higher segmentation performance even with pre-
contrast images.
in summary, the main contributions of this work are listed as
follows:
• we propose a diﬀusion kinetic model that implicitly exploits hemodynamic
priors in dce-mri and eﬀectively generates high-quality segmentation maps
only requiring pre-contrast images.
• compared to the existing approaches with complete sequences, the proposed
method yields higher cancer segmentation performance even with pre-contrast
images.
2. it can be observed that the devised model consists of a diﬀusion module
(dm) and a segmentation module (sm).
let {xk, k = 0, 1, ..., k} be a sequence
of images representing the dce-mri protocol, in which x0 represents the pre-
contrast image and xk represents the late post-contrast image.
the dm takes
a noisy pre-contrast image xt
as input and generates post-contrast images to
estimate the latent kinetic code.
once the dm is trained, the learned kinetic code
diﬀusion kinetic model for cancer segmentation
103
fig.
2. illustration of our method for implicitly exploiting hemodynamic information
from pre-contrast images.
the combination of learned kinetic code is an example.
is incorporated into the sm as hemodynamic priors to guide the segmentation
process.
= n(xt;

1 − βtxt−1, βti)
(1)
particularly, a noisy image xt can be directly obtained from the data x0:
q(xt|x0) := n(xt; √¯αtx0, (1 − ¯αt)i)
(2)
where αt := 1 − βt and ¯αt := t
s=1 αs.
104
t. lv et al.
inspired by the property of ddpm [8], we devise the diﬀusion module by
considering the pre-contrast images x0 as source and regarding the post-contrast
images xk as target.
as thus, the
dm gradually exploits the latent kinetic code by comparing the pre-contrast and
post-contrast images, which contains hemodynamic knowledge for segmentation.
2.2
segmentation module
once pretrained, the dm outputs multi-scale latent kinetic code fdm from inter-
mediate layers, which is fed into the sm to guide cancer segmentation.
in this way, the hemodynamic knowledge can be
incorporated into the sm to capture more expressive representations to improve
segmentation performance.
in the ﬁrst step, the dm is trained to transform
pre-contrast images into post-contrast images for a latent space where hemo-
dynamic priors are exploited.
in particular, the diﬀusion loss for the reverse
diﬀusion process can be formulated as follows:
ldm = et,ϵ,x||ϵθ(xt, t; x0, xk) − ϵ||2
(6)
where ϵθ represents the denoising model that employs an u-net structure, x0
and xk are the pre-contrast and post-contrast images, respectively, ϵ is gaussian
distribution data ∼ n(0, i), and t is a timestep.
for a second step, we train the sm that integrates the previously learned
latent kinetic code to provide tumor hemodynamic information for voxel-level
diﬀusion kinetic model for cancer segmentation
105
prediction.
considering the varying sizes, shapes and appearances of tumors
that results from intratumor heterogeneity and results in diﬃculties of accurate
cancer annotation, we design the segmentation loss as follows:
(7)
where lssim is used to evaluate tumor structural characteristics, s and g rep-
resents segmentation map and ground truth, respectively; μs is the mean of s
and μg is the mean of g; ϕs represents the variance of s and ϕg represents
the variance of g; c1 and c2 denote the constant to hold training stable
3
experiments
dataset: to demonstrate the eﬀectiveness of our proposed dkm, we evaluate
our method on 4d dce-mri breast cancer segmentation using the breast-mri-
nact-pilot dataset
ground truth segmentations of the data are provided in the dataset for tumor
annotation.
no data augmentation techniques are used to ensure fairness.
competing methods and evaluation metrics: to comprehensively evalu-
ate the proposed method, we compare it with 3d segmentation methods, includ-
ing dual attention net (danet)
[19], and 4d segmentation methods, including lnet
implementation details:
we implement our proposed framework with
pytorch using two nvidia rtx 2080ti gpus to accelerate model training.
[8], we set 128, 256, 256, 256 channels for each stage in the dm
and set the noise level from 10−4 to 10−2 using a linear schedule with t = 1000.
once the dm is trained, we extract intermediate feature maps from four res-
olutions for further segmentation task.
table 1. cancer segmentation comparison between our method and previous models
(mean ± std).
[5]
complete sequence 64.4 ± 2.4
51.5 ± 2.6
dkm (ours)
pre-contrast
71.5 ± 2.5 58.5 ± 2.6
512, 1024 channels for each stage in the sm to capture expressive and suﬃ-
cient semantic information.
no data augmentation techniques are used to ensure fairness.
imental results demonstrate that the proposed method comprehensively other
models with less scans (i.e., pre-contrast) in testing.
we attribute it to the abil-
ity of diﬀusion module to exploit hemodynamic priors to guide the segmentation
task.
speciﬁcally, in comparison with 3d segmentation models (e.g. mtln),
our method yields higher segmentation scores.
besides, we can observe that our method
achieves improvements when compared to 4d segmentation models using com-
plete sequence.
it probably due to two aspects: 1) the hemodynamic
knowledge is implicitly exploited by diﬀusion module from pre-contrast images,
which is useful for cancer segmentation.
2) the intermediate activations from
diﬀusion kinetic model for cancer segmentation
107
fig.
4. visual comparison of segmentation performance.
the baseline is implemented
without the incorporation of kinetic code.
ji (%) ↑
✓
67.9 ± 2.4
54.4 ± 2.4
✓
68.6 ± 2.3
55.0 ± 2.2
✓
68.3 ± 2.4
54.8 ± 2.5
✓
69.3 ± 2.0
55.5 ± 2.1
✓
✓
67.6 ± 2.3
53.2 ± 2.4
✓
✓
70.1 ± 2.1
56.2 ± 2.3
✓
✓
71.5 ± 2.5 58.5 ± 2.6
✓
✓
✓
70.2 ± 2.3
56.4 ± 2.3
✓
✓
✓
✓
69.5 ± 2.1
55.9 ± 2.4
diﬀusion models eﬀectively capture the semantic information and are excellent
pixel-level representations for the segmentation problem [9].
thus, combining
the intermediate features can further promote the segmentation performance.
in a word, the proposed framework can produce accurate prediction masks only
requiring pre-contrast images.
we denote the intermedi-
ate features extracted from each stage in the dm as f1, f2, f3, and f4, respec-
tively, where fi represents the feature map of i-th stage.
table 2 reports the
segmentation performance with diﬀerent incorporations of intermediate kinetic
codes.
it can be observed that the latent kinetic code is able to guide the network
training for better segmentation results.
figure 4 shows visual comparison of segmen-
tation performance.
4
conclusion
we propose a diﬀusion kinetic model by exploiting hemodynamic priors in dce-
mri to eﬀectively generate high-quality segmentation results only requiring pre-
contrast images.
our models learns the hemodynamic response function based on
the denoising diﬀusion process and estimates the latent kinetic code to guide the
segmentation task.
experiments demonstrate that our proposed framework has
the potential to be a promising tool in clinical applications to annotate cancers.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_71.pdf:
radiotherapy (rt) is a standard treatment modality for
head and neck (han) cancer that requires accurate segmentation of tar-
get volumes and nearby healthy organs-at-risk (oars) to optimize radi-
ation dose distribution.
however, computed tomography (ct) imaging
has low image contrast for soft tissues, making accurate segmentation
of soft tissue oars challenging.
therefore, magnetic resonance (mr)
imaging has been recommended to enhance the segmentation of soft tis-
sue oars in the han region.
based on our two empirical observations
that deformable registration of ct and mr images of the same patient
is inherently imperfect and that concatenating such images at the input
layer of a deep learning network cannot optimally exploit the information
provided by the mr modality, we propose a novel modality fusion mod-
ule (mfm) that learns to spatially align mr-based feature maps before
fusing them with ct-based feature maps.
the proposed mfm can be
easily implemented into any existing multimodal backbone network.
our
implementation within the nnu-net framework shows promising results
on a dataset of ct and mr image pairs from the same patients.
keywords: multimodal segmentation · head and neck ·
organs-at-risk · computed tomography · magnetic resonance ·
nnu-net
1
introduction
head and neck (han) cancer is a prevalent type of cancer [3] with a yearly inci-
dence of above 1 million cases and prevalence of above 4 million cases worldwide,
accounting for around 5% of all cancer sites [17]. radiotherapy (rt) is a standard
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43901-8 71.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
https://doi.org/10.1007/978-3-031-43901-8_71
746
g. podobnik et al.
treatment modality for han cancer, which aims to deliver high doses of radiation
to cancerous cells while sparing nearby healthy organs-at-risk (oars)
[21]. to
optimize radiation dose distribution, accurate three-dimensional (3d) segmen-
tation of target volumes and oars is required.
computed tomography (ct) is
the primary imaging modality used for rt planning due to its ability to pro-
vide information about electron density, however, its low image contrast for soft
tissues, including tumors, makes accurate segmentation of soft tissue oars chal-
lenging.
therefore, the integration of complementary imaging modalities, such as
magnetic resonance (mr), has been strongly recommended in clinical practice to
enhance the segmentation of several soft tissue oars in the han region
this
naturally poses a question of whether automatic oar segmentation can beneﬁt
from the mr image modality.
our study therefore aims to evaluate the impact of
mr integration on the quality and robustness of automatic oar segmentation
in the han region, therefore contributing to the growing body of research on
multimodal methods for medical image analysis.
[24] divides deep learning
(dl)-based multimodal segmentation methods into three fusion strategy groups:
early, late and hybrid (also named layer) fusion.
the ﬁrst two groups of meth-
ods are most commonly applied; early fusion comprises simple concatenation
of modalities along the channel dimension before feeding them into the deep
neural network.
[23]
proposed an attention mechanism to fuse fms from two separate u-nets that
accepted contrast-enhanced arterial and venous phase ct images.
the third
group, hybrid fusion, aims to combine the strengths of early and late fusion [24]
by employing two or more separate encoders (i.e. one for each modality) and a
single decoder, where features from diﬀerent resolution levels of the encoder are
fused and fed into the decoder that produces the ﬁnal full-resolution segmenta-
tion.
similar conclusions were reached in a review of multimodal seg-
mentation methods in the medical imaging community by zhou et al.
most
methods implement either early or late fusion, however, the layer fusion strat-
egy was identiﬁed as a better choice, since dense connections among layers can
exploit more complex and complementary information to enhance training.
[4]
that employs dense connections between two convolutional paths, and achieves
multimodal ct and mr segmentation of han oars
747
improvements compared to other fusion strategies and single modality variants.
relevant to the ﬁeld of multimodal segmentation are
also developments on unpaired multimodal segmentation, where cross-modality
learning is employed to take advantage of diﬀerent image modalities covering
the same anatomy, but without the constraint to collect images from the same
patients [5,10,19].
although the methodologies comprising cyclegans and/or
multiple segmentation networks [10,19] seem promising, they can be excessively
complex for the task of han oar segmentation where both ct and mr image
modalities from the same patient are often available.
consequently, our primary
focus is the paired multimodal segmentation problem, including the missing
modality scenario.
motivation.
when segmenting oars in the han region for the purpose of rt
planning, a multimodal segmentation model that can leverage the information
from ct and mr images of the same patient might be beneﬁcial compared to
separate single-modal models.
firstly, as intuition suggests, such a model would
rely on the ct image for bone structures and on the mr image for soft tissues,
and therefore improve the overall segmentation quality by exploiting the comple-
mentary information from both modalities.
secondly, a multimodal model would
facilitate cross-modality learning by extracting knowledge from one and applying
that knowledge to the other modality, potentially improving the segmentation
accuracy.
several studies indicated that such an approach is feasible, for exam-
ple, for improving video classiﬁcation by training a model on an auxiliary audio
reconstruction task [12], or for audio-based detection by using the multimodal
knowledge distillation concept, where teacher networks trained on rgb, depth
and thermal images improve a student network trained only on audio data [20].
finally, from the dl infrastructure maintenance perspective, it is easier to main-
tain a single model that can handle both modalities than two separate models
for each modality.
firstly,
although mr image acquisition is recommended, it is not always feasible due to
time constraints, scanner occupancy and ﬁnancial aspects.
consequently, auto-
matic oar multimodal segmentation is required to handle the missing modality
scenario, and provide a similar segmentation quality as a single-modality system.
secondly, because ct and mr images are not acquired simultaneously and with
the same acquisition parameters (e.g. resolution), there is an inherent misalign-
ment between both modalities.
this can be mitigated with image registration,
but not completely, mainly due to diﬀerent patient positioning that especially
aﬀects the deformation of soft tissues, and various modality-speciﬁc artifacts
(e.g. motion, implants, partial volume eﬀect, etc.).
to tackle these considerations, we propose a mechanism named
modality fusion module (mfm) that can generally be applied to any network
architecture that learns features from multiple modalities, and shows promising
performance also in the missing modality scenario.
the advantages of the pro-
posed mfm are the following: 1) it enables the spatial alignment of fms from
one with fms from the other modality to further reduce errors that persist after
deformable registration of input images, and enrich the fms to improve the ﬁnal
oar segmentation, 2) it signiﬁcantly improves the performance of the missing
modality scenario compared to other baseline fusion approaches, and 3) it per-
forms well also on single modality out-of-distribution data, therefore facilitating
cross-modality learning and contributing to better model generalizability.
our chosen backbone network is based on nnu-net,
a publicly available framework for dl-based segmentation [8] that builds on
the u-net architecture [16], adds self-conﬁgurable pre-processing, augmentation
and post-processing, and employs eﬃcient training strategies.
however, nnu-net,
which uses an early fusion strategy by concatenating input images or patches
before feeding them to the ﬁrst network layer, may not be the optimal strategy
for multimodal segmentation.
this is particularly problematic when fusing
ct and mr images, which diﬀer in several aspects, such as the type and loca-
tion of artifacts, acquisition parameters, and visibility of soft tissues and bone
structures.
while mr images can help to improve the delineations of oars that
are poorly visible in ct images, the primary delineation is always performed
on ct images with the help of registered mr images.
an important repercus-
sion is that image registration errors propagate into oar delineations, which
is particularly salient in the han region.
[9], who introduced a spatial transformer network (stn)
that learns to infer transformation parameters in a single forward pass, and then
uses them to transform images and/or fms.
the fundamental idea is that stn
can learn meaningful features that are spatially invariant to characteristics of
the input data, without the need for extra supervision, thereby enhancing task
multimodal ct and mr segmentation of han oars
749
fig.
the proposed backbone architecture is based on nnu-net but with separate
encoders for the computed tomography (ct) and magnetic resonance (mr) image,
and with the proposed modality fusion module.
performance.
while it was demonstrated that complete spatial invariance can-
not be achieved with stns [6], the work of jaderberg et al. is crucial in showing
that stns can be implemented as diﬀerentiable modules, enabling the loss to be
propagated through the sampling (interpolation) mechanism.
the same under-
lying principle of stns has also been leveraged in optical ﬂow and its derivative
work semantic ﬂow, where the ﬂow alignment module was proposed to resample
low-resolution fms and align them with high-resolution fms
we capital-
ize on the same principle to register fms from mr images to those from ct
images.
notably, mfm is diﬀerent from semantic ﬂow, as it takes two fms of
the same resolution but from diﬀerent modalities, and aligns fms from the aux-
iliary modality to fms of the primary modality.
the localization network is a regressor network that accepts concate-
nated fms from both encoders and applies four blocks of strided convolutions
followed by the relu activation to reduce their spatial dimensions.
both
the grid generator and the sampler and readily implemented in the pytorch
library [9], and because they are both diﬀerentiable, no special optimization is
needed for the localization network, allowing localization parameters to be opti-
mized with the main (segmentation) loss function.
the purpose of this architecture is to align fms from both
modalities and improve their fusion, leading to better segmentation results.
we evaluate the performance of the proposed mfm
nnu-net against three baseline networks: 1) a single modality nnu-net trained
only on ct images, 2) a nnu-net trained on concatenated ct and mr image
pairs, and 3) a model with separate encoders for both modalities, but with a
simple concatenation along the channel axis instead of the proposed mfm.
[23].
3
experiments and results
image datasets.
the han-seg dataset comprises ct and t1-weighted mr images of
56 patients, which were deformably registered with the simpleelastix registra-
tion tool, and corresponding curated manual delineations of 30 oars (for details,
please refer to [14]).
although only a subset of images is publicly available1 due
to the ongoing han-seg challenge2, both the publicly available training as well
as the privately withheld test images were used in our 4-fold cross-validation
experiments.
on the other hand, to evaluate the generalization ability of our
method, we also conducted experiments on the ct-only pddca dataset (for
details, please refer to [15]), from which we collected 15 images from the oﬀ-
and on-site test sets of the corresponding challenge for our evaluation.
as this
dataset is widely used for evaluating the performance of automatic han oar
segmentation methods, it serves as a valuable benchmark for comparison with
other state-of-the-art methods.
note that none of the images from the ct-only
pddca dataset were used for training, and as our model expects two inputs,
we substituted the missing mr modality with an empty matrix (i.e. zeros).
implementation details.
all models were trained for all oars using the
3d fullres conﬁguration of nnu-net, with the only modiﬁcation that we
reduced rotation around the axial axis and disabled image ﬂipping along
the sagittal plane, which eliminated segmentation errors that were previously
observed for the paired (left and right) oars.
multimodal ct and mr segmentation of han oars
751
fig.
to address the challenge of a rela-
tively small dataset, we adopted a 4-fold cross-validation strategy without using
any external training images.
the quality of the obtained oar segmentation masks was evaluated
by computing the dice similarity coeﬃcient (dsc) and the 95th-percentile haus-
dorﬀ distance (hd95) against reference manual delineations, and the results for
all oars are presented in figs.
since not all images con-
tain all 30 oars (due to a diﬀerent ﬁeld-of-view), we ﬁrst calculated the mean
metric for each oar and then the overall mean across all oars to ensure that
752
g. podobnik et al.
the contributions were equally weighted.
4
discussion
in this study, we evaluated the impact on the quality and robustness of auto-
matic oar segmentation in the han region caused by the incorporation of the
mr modality into the segmentation framework.
we devised a mechanism named
mfm and combined it with nnu-net as our backbone segmentation network.
segmentation results.
however,
dsc has been identiﬁed not to be the most appropriate metric for evaluating the
clinical adequacy of segmentations, especially when the results are close to the
interrater variability
on the other hand, distance-based metrics, such as hd95 (fig. 3),
are preferred as they better measure the shape consistency between the refer-
ence and predicted segmentations.
although maml achieved the best results
in terms of hd95, indicating that late fusion can eﬃciently merge the informa-
tion from both modalities, it should be noted that maml has a considerate
advantage due to having two decoders and an additional attention fusion block
compared to the baseline nnu-net with separate encoders and a single decoder.
an
approximate 10% improvement in hd95 suggests that mfm allows the network
to learn more informative fms that lead to a better overall performance.
the overall good performance on the han-seg
dataset suggests that all models are close to the maximal performance, which is
bounded by the quality of reference segmentations.
however, the performance
on the pddca dataset that consists only of ct images allows us to test how
the models handle the missing modality scenario and perform on an out-of-
distribution dataset, as images from this dataset were not used for training.
as
multimodal ct and mr segmentation of han oars
753
expected, the ct-only model performed best in its regular operating scenario,
with a mean dsc of 74.7% (fig. 2) and hd95 of 6.02 mm (fig. 3).
it should be noted that we did not
employ any training strategies to improve handling of missing modalities, such
as swapping input images or intensity augmentations.
a possible explanation is
that the proposed mfm facilitates cross-modality learning, enabling nnu-net
to extract better fms from ct images even in such extreme scenarios.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_66.pdf:
recently, deep learning methods have been widely used
for tumor segmentation of multimodal medical images with promising
results.
in this paper, we propose a hybrid densely connected net-
work for tumor segmentation, named h-denseformer, which combines
the representational power of the convolutional neural network (cnn)
and the transformer structures.
we con-
duct extensive experiments on two public multimodal datasets, heck-
tor21 and pi-cai22.
the experimental results show that our proposed
method outperforms the existing state-of-the-art methods while having
lower computational complexity.
keywords: tumor segmentation · multimodal medical image ·
transformer · deep learning
1
introduction
accurate tumor segmentation from medical images is essential for quantita-
tive assessment of cancer progression and preoperative treatment planning [3].
this study was supported by the fundamental
research funds for the central universities (no. yd2150002001).
in clinical practice, multimodal registered images, such as
pet-ct images and magnetic resonance (mr) images with diﬀerent sequences,
are often utilized to delineate tumors to improve accuracy.
however, manual
delineation is time-consuming and error-prone, with a low inter-professional
agreement [12].
these have prompted the demand for intelligent applications
that can automatically segment tumors from multimodal images to optimize
clinical procedures.
recently, multimodal tumor segmentation has attracted the interest of many
researchers.
with the emergence of multimodal datasets (e.g., brats [25] and
hecktor [1]), various deep-learning-based multimodal image segmentation
methods have been proposed [3,10,13,27,29,31]. overall, large eﬀorts have been
made on eﬀectively fusing image features of diﬀerent modalities to improve seg-
mentation accuracy.
as a typical approach, input-level fusion
[8,20,26,31,34] refers to concatenating multimodal images in the channel dimen-
sion as network input during the data processing or augmentation stage.
however, the shallow fusion entangles the low-level fea-
tures from diﬀerent modalities, preventing the eﬀective extraction of high-level
semantics and resulting in limited performance gains.
the core idea is to train an
independent segmentation network for each data modality and fuse the results
in a speciﬁc way.
in addition to the progress on the fusion of multimodal features, improving
the model representation ability is also an eﬀective way to boost segmentation
performance.
in the past few years, transformer structure [11,24,30], centered on
the multi-head attention mechanism, has been introduced to multimodal image
segmentation tasks.
extensive studies [2,4,14,16] have shown that the trans-
former can eﬀectively model global context to enhance semantic representations
and facilitate pixel-level prediction.
[31] proposed transbts, a form
of input-level fusion with a u-like structure, to segment brain tumors from mul-
timodal mr images.
[29] adopted a similar structure in which the
transformer serves as the encoder rather than a wrapper, also achieving promis-
ing performance.
[33], which combine the transformer
with the multimodal feature fusion approaches mentioned above, further demon-
strate the potential of this idea for multimodal tumor segmentation.
694
j. shi et al.
although remarkable performance has been accomplished with these eﬀorts,
there still exist several challenges to be resolved.
to this end, we propose an eﬃcient multimodal tumor segmentation solu-
tion named hybrid densely connected network (h-denseformer).
first, our
method leverages transformer to enhance the global contextual information
of diﬀerent modalities.
second, h-denseformer integrates a transformer-based
multi-path parallel embedding (mpe) module, which can extract and fuse mul-
timodal image features as a complement to naive input-level fusion structure.
speciﬁcally, mpe assigns an independent encoding path to each modality, then
merges the semantic features of all paths and feeds them to the encoder of the
segmentation network.
finally, we design a lightweight, densely connected transformer (dct)
module to replace the standard transformer to ensure performance and com-
putational eﬃciency.
extensive experimental results on two publicly available
datasets demonstrate the eﬀectiveness of our proposed method.
h-denseformer com-
prises a multi-path parallel embedding (mpe) module and a u-shaped seg-
mentation backbone network in form of input-level fusion.
speciﬁcally, given a multimodal image input
x3d ∈ rc×h×w ×d or x2d ∈ rc×h×w with a spatial resolution of h ×w, the
depth dimension of d (number of slices) and c channels (number of modalities),
we ﬁrst utilize mpe to extract and fuse multimodal image features.
then, the
obtained features are progressively upsampled and delivered to the encoder of
the segmentation network to enhance the semantic representation.
finally, the
segmentation network generates multi-scale outputs, which are used to calculate
deep supervision loss as the optimization target.
2.2
multi-path parallel embedding
many methods [5,10,15] have proved that decoupling the feature representa-
tion of diﬀerent modalities facilitates the extraction of high-quality multimodal
features.
then, interpolation upsampling is performed to obtain
the multimodal fusion feature fout ∈ rk× h
8 × w
8 × d
8 , where k = 128 refers to the
channel dimension.
finally, fout is progressively upsampled to multiple scales
and delivered to diﬀerent encoder stages to enhance the learned representation.
[11] typically consist of dense linear layers with
a computational complexity proportional to the feature dimension.
therefore,
integrating the transformer could lead to a mass of additional computation
and memory requirements.
each transformer layer has a lin-
ear projection layer that reduces the input feature dimension to g = 32 to save
computation.
diﬀerent transformer layers are connected densely to preserve rep-
resentational power with lower feature dimensions.
feature dimension resolution transformer
stacked dct (×3)
gflops ↓ params ↓ gflops ↓ params ↓
256
(512,512)
6.837
6.382m
2.671
1.435m
512
(512,512)
26.256
25.347m
3.544
2.290m
2.4
segmentation backbone network
the h-denseformer adopts a u-shaped encoder-decoder structure as its back-
bone.
the multi-level multimodal features from mpe are fused in a bit-
wise addition way to enrich the semantic information.
during training, the decoder has four outputs; for
example, the i-th output of 2d h-denseformer is oi ∈ rc× h
2i × w
2i , where
i ∈ [0, 1, 2, 3], and c = 2 (tumor and background) represents the number of
segmentation classes.
this approach can improve the convergence speed
and performance of the network.
3
experiments
3.1
dataset and metrics
to validate the eﬀectiveness of our proposed method, we performed extensive
experiments on hecktor21
[1] and pi-cai221. hecktor21 is a dual-
modality dataset for head and neck tumor segmentation, containing 224 pet-
ct image pairs.
pi-cai22 provides multimodal mr images of 220 patients
with prostate cancer, including t2-weighted imaging (t2w), high b-value
diﬀusion-weighted imaging (dwi), and apparent diﬀusion coeﬃcient (adc)
maps.
after standard resampling and center cropping, all images have a size of
(24,384,384).
for quantitative analysis, we use the dice
similarity coeﬃcient (dsc), the jaccard index (ji), and the 95% hausdorﬀ
distance (hd95) as evaluation metrics for segmentation performance.
a better
segmentation will have a smaller hd95 and larger values for dsc and ji.
we
also conduct holistic t-tests of the overall performance for our method and all
baseline models with the two-tailed p < 0.05.
3.2
implementation details
we use pytorch to implement our proposed method and the baselines.
for a
fair comparison, all models are trained from scratch using two nvidia a100
gpus and all comparison methods are implemented with open-source codes,
following their original conﬁgurations.
online data augmentation, including random rotation and ﬂipping, is performed
to alleviate the overﬁtting problem.
1 https://pi-cai.grand-challenge.org/.
698
j. shi et al.
3.3
overall performance
table 2. comparison with existing methods on independent test set.
we show the
mean ± std (standard deviation) scores of averaged over the 5 folds.
methods (year)
params↓ gflops↓ dsc(%) ↑
hd95(mm) ↓ ji(%) ↑
hecktor21, two modalities (ct and pet)
3d u-net (2016)
[4]
93.23m
72.62
44.8 ± 3.0
59.3 ± 14.8
33.2 ± 2.5
2d h-denseformer
4.25m
31.46
49.9 ± 1.2 35.9 ± 8.2
37.1 ± 1.2
table 2 compares the performance and computational complexity of our pro-
posed method with the existing state-of-the-art methods on the independent
test sets.
for hecktor21, 3d h-denseformer achieves a dsc of 73.9%, hd95
of 8.1mm, and ji of 62.5%, which is a signiﬁcant improvement (p < 0.01) over
3d u-net [7], unetr [16], and transbts [31].
it is worth noting that the per-
formance of hybrid models such as unetr is not as good as expected, even
worse than 3d u-net, perhaps due to the small size of the dataset.
overall, h-denseformer
reaches an eﬀective balance of performance and computational cost compared to
existing cnns and hybrid structures.
2 that our approach
can describe tumor contours more accurately while providing better segmenta-
tion accuracy for small-volume targets.
these results further demonstrate the
eﬀectiveness of our proposed method in multimodal tumor segmentation tasks.
as illustrated in table 3, the network performance
varies with the change in dct depth.
h-denseformer achieves the best perfor-
mance at the dct depth of 6.
ji (%) ↑
3
3.25m
242.38
73.5 ± 1.4
8.4 ± 0.7
62.2 ± 1.6
6
3.64m
242.96
73.9 ± 0.5 8.1 ± 0.6
62.5 ± 0.5
9
4.03m
243.55
72.7 ± 1.2
8.7 ± 0.6
61.2 ± 1.3
of the dct has increased from 3 to 9, the performance does not improve or even
worsen.
the above results demonstrate the superiority
of our method, but it is unclear which module plays a more critical role in perfor-
mance improvement.
therefore, we perform ablation experiments on mpe, dct
and ds loss.
as shown in table 4, the performance
decreases with varying degrees when removing them separately, which means all
the modules are critical for h-denseformer.
we can observe that dct has a
greater impact on overall performance than the others, further demonstrating its
eﬀectiveness.
hd95 (mm) ↓ ji (%) ↑
3d h-denseformer w/o mpe
72.1 ± 0.8
10.8 ± 1.1
60.4 ± 0.8
3d h-denseformer w/o dct
70.7 ± 1.8
11.9 ± 1.9
58.6 ± 2.1
3d h-denseformer w/o ds loss 72.2 ± 0.9
10.2 ± 1.0
60.1 ± 1.2
3d h-denseformer
73.9 ± 0.5 8.1 ± 0.6
62.5 ± 0.5
700
j. shi et al.
ﬁrms that decoupling the feature expression of diﬀerent modalities helps obtain
higher-quality multimodal features and improve segmentation performance.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_43.pdf:
despite its clinical utility, medical image segmentation (mis)
remains a daunting task due to images’ inherent complexity and variabil-
ity.
to overcome this obstacle, data-eﬃcient
vits were proposed, but they are typically trained using a single source
of data, which overlooks the valuable knowledge that could be leveraged
from other available datasets.
na¨ıvly combining datasets from diﬀerent
domains can result in negative knowledge transfer (nkt), i.e., a decrease
in model performance on some domains with non-negligible inter-domain
heterogeneity.
experiments on 4 skin lesion seg-
mentation datasets show that mdvit outperforms state-of-the-art algo-
rithms, with superior segmentation performance and a ﬁxed model size, at
inference time, even as more domains are added.
keywords: vision transformer · data-eﬃciency · multi-domain
learning · medical image segmentation · dermatology
1
introduction
medical image segmentation (mis) is a crucial component in medical image
analysis, which aims to partition an image into distinct regions (or segments)
that are semantically related and/or visually similar.
this process is essential
for clinicians to, among others, perform qualitative and quantitative assessments
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43901-8 43.
https://doi.org/10.1007/978-3-031-43901-8_43
mdvit
449
of various anatomical structures or pathological conditions and perform image-
guided treatments or treatment planning
they process images as sequences
of patches, with each patch having a global view of the entire image.
this enables
a vit to achieve improved segmentation performance compared to traditional
convolutional neural networks (cnns) on plenty of segmentation tasks [16].
how-
ever, due to the lack of inductive biases, such as weight sharing and locality, vits
are more data-hungry than cnns, i.e., require more data to train [31].
mean-
while, it is common to have access to multiple, diverse, yet small-sized datasets
(100 s to 1000 ss of images per dataset) for the same mis task, e.g., ph2
as each dataset alone is too small
to properly train a vit, the challenge becomes how to eﬀectively leverage the
diﬀerent datasets.
method
vit mitigate vits’ data-hunger
u f
[7,22,39] √
√ by adding inductive bias
× –
[32]
√
×
√ ×
mdvit
√
√ by multi-domain learning
√ √
various strategies have been proposed to address vits’ data-hunger
(table 1), mainly: adding inductive bias by constructing a hybrid network that
fuses a cnn with a vit
sharing knowl-
edge by transferring knowledge from a cnn [31] or pertaining vits on multiple
related tasks and then ﬁne-tuning on a down-stream task [37]; increasing data via
augmentation [34]; and non-supervised pre-training [8].
multi-domain learning, which trains
a single universal model to tackle all the datasets simultaneously, has been found
450
s. du et al.
promising for reducing computational demands while still leveraging information
from multiple domains
given the inter-domain heterogeneity resulting from variations in imaging
protocols, scanner manufacturers, etc.
[4,21], directly mixing all the datasets
for training, i.e., joint training, may improve a model’s performance on one
dataset while degrading performance on other datasets with non-negligible unre-
lated domain-speciﬁc information, a phenomenon referred to as negative knowl-
edge transfer (nkt)
to address vits’ data-hunger, in this work, we propose mdvit, a novel ﬁxed-
size multi-domain vit trained to adaptively aggregate valuable knowledge from
multiple datasets (domains) for improved segmentation.
(2) we propose a multi-domain vit,
mdvit, for medical image segmentation with a novel domain adapter to coun-
teract negative knowledge transfer and with mutual knowledge distillation to
enhance representation learning.
(3) the experiments on 4 skin lesion segmenta-
tion datasets show that our multi-domain adaptive training outperforms separate
and joint training (st and jt), especially a 10.16% improvement in iou on the
skin cancer detection dataset compared to st and that mdvit outperforms
state-of-the-art data-eﬃcient vits and multi-domain learning strategies.
2
methodology
let x ∈ rh×w ×3 be an input rgb image and y ∈ {0, 1}h×w be its ground-
truth segmentation mask.
i ≤ 4 is the number
of patches and ci is the channel dimension.
the h parallel heads of mhsa mimic
how humans examine the same object from diﬀerent perspectives [10].
rather than manually designate each
head to one of the domains, guided by a domain label, mdvit learns to focus on
the corresponding features from diﬀerent heads when encountering a domain.
k is the channel dimension of features from
the heads.
[uh
1, uh
2, ..., uh
k] ∈ rn×k from the hth head, we utilize
ah to calibrate the information along the channel dimension: ˜uh
k = ah
k · uh
k.
mutual knowledge distillation (mkd): distilling knowledge from domain-
speciﬁc networks has been found beneﬁcial for universal networks to learn more
robust representations [21,40].
1-a) go through an mlp layer and an up-sample oper-
ation to unify the channel dimension and resolution to h
4 × w
4 , which are then
concatenated with the feature involving domain-shared information from the
mdvit
453
table 2. segmentation results comparing base, mdvit, and sota methods.
(millions) (m) t
segmentation results in test sets (%)
dice ↑
iou ↑
isic
dmf
scd
ph2
avg ± std
isic
dmf
scd
ph2
avg ± std
(a) base
base
27.8×
st
90.18
90.68
86.82
93.41
90.27 ± 1.16
82.82
83.22
77.64
87.84
82.88 ± 1.67
base
27.8
jt
89.42
89.89
92.96
94.24
91.63 ± 0.42
81.68
82.07
87.03
89.36
85.04 ± 0.64
(b) our method
mdvit
28.5
mat 90.29
90.78
93.22 95.53 92.45 ± 0.65 82.99
83.41
87.80 91.57 86.44 ± 0.94
(c) other data-eﬃcient mis vits
swinunet
41.4×
st
89.25
90.69
88.58
94.13
90.66 ± 0.87
81.51
83.25
80.40
89.00
83.54 ± 1.27
swinunet
41.4
jt
89.64
90.40
92.98
94.86
91.97 ± 0.30
81.98
82.80
87.08
90.33
85.55 ± 0.50
utnet
10.0×
st
89.74
90.01
88.13
93.23
90.28 ± 0.62
82.16
82.13
79.87
87.60
82.94 ± 0.82
utnet
10.0
jt
90.24
89.85
92.06
94.75
91.72 ± 0.63
82.92
82.00
85.66
90.17
85.19 ± 0.96
bat
32.2×
st
90.45 90.56
90.78
94.72
91.63 ± 0.68
83.04
82.97
83.66
90.03
84.92 ± 1.01
bat
32.2
jt
90.06
90.06
92.66
93.53
91.58 ± 0.33
82.44
82.18
86.48
88.11
84.80 ± 0.53
transfuse
26.3×
st
90.43
91.04 91.37
94.93
91.94 ± 0.67
83.18 83.86 84.91
90.44
85.60 ± 0.95
transfuse
26.3
jt
90.03
90.48
92.54
95.14
92.05 ± 0.36
82.56
82.97
86.50
90.85
85.72 ± 0.56
swin unetr 25.1×
st
90.29
90.95
91.10
94.45
91.70 ± 0.51
82.93
83.69
84.16
89.59
85.09 ± 0.79
swin unetr 25.1
jt
89.81
90.87
92.29
94.73
91.93 ± 0.29
82.21
83.58
86.10
90.11
85.50 ± 0.44
(d) other multi-domain learning methods
rundo et al.
28.2
mat 89.43
89.46
92.62
94.68
91.55 ± 0.64
81.73
81.40
86.71
90.12
84.99 ± 0.90
wang et al.
28.1
mat 89.46
89.62
92.62
94.47
91.55 ± 0.54
81.79
81.59
86.71
89.76
84.96 ± 0.74
base†
27.8(.02×)
mat 90.22
90.61
93.69 95.55
92.52 ± 0.45
82.91
83.14
88.28 91.58
86.48 ± 0.74
mdvit†
28.6(.02×)
mat 90.24 90.71 93.38
95.90 92.56 ± 0.52 82.97 83.31 88.06
92.19 86.64 ± 0.76
universal network’s last transformer block.
finally, we pass the fused feature to
an mlp layer and do an up-sample to obtain a segmentation map.
2.2
objective function
similar to combo loss [29], base’s segmentation loss combines dice and binary
cross entropy loss: lseg = ldice+lbce.
in mdvit, we use the same segmentation
loss for the universal network and auxiliary peers, denoted as lu
seg and la
seg,
respectively.
after training, we discard the auxiliary peers
and only utilize the universal network for inference.
3
experiments
datasets and evaluation metrics: we study 4 skin lesion segmentation data-
sets collected from varied sources: isic 2018 (isic)
[11], dermoﬁt image library
(dmf)
the green and red contours present
the ground truth and segmentation results, respectively.
to facilitate a fairer performance
comparison across datasets, as in [4], we only use the 1212 images from dmf
that exhibited similar lesion conditions as those in other datasets.
implementation details: we conduct 3 training paradigms: separate (st),
joint (jt), and multi-domain adaptive training (mat), described in sect.
images are resized
to 256 × 256 and then augmented through random scaling, shifting, rotation,
ﬂipping, gaussian noise, and brightness and contrast changes.
the encoding
transformer blocks’ channel dimensions are [64, 128, 320, 512] (fig.
the hidden dimensions of the cnn bridge and auxiliary peers
are 1024 and 512.
we deploy models on a single titan v gpu and train them
for 200 epochs with the adamw [23] optimizer, a batch size of 16, ensuring 4
samples from each dataset, and an initial learning rate of 1×10−4, which changes
through a linear decay scheduler whose step size is 50 and decay factor γ = 0.5.
comparing against base: in table 2-a,b, compared with base in st,
base in jt improves the segmentation performance on small datasets (ph2
and scd)
but at the expense of diminished performance on larger datasets (isic
and dmf).
the above results
demonstrate that shared knowledge in related domains facilitates training a vit
on small datasets while, without a well-designed multi-domain algorithm, caus-
ing negative knowledge transfer (nkt) due to inter-domain heterogeneity, i.e.,
the model’s performance decreases on other datasets.
additionally, mdvit outperforms base in jt across all the domains,
with average improvements of 0.82% on dice and 1.4% on iou.
mdvit
455
table 3. ablation studies of mdvit and experiments of da’s plug-in capability.
: we conduct
experiments on sota data-eﬃcient mis vits and multi-domain learning meth-
ods.
previous mis vits mitigated the data-hunger in one dataset by adding
inductive bias, e.g., swinunet
we implement resnet-34 as the backbone of bat for fair
comparison (similar model size).
this is expected since they are designed
to reduce data requirements.
finally, mdvit achieves the best segmentation
performance in average dice and iou without nkt and has the best results
on scd and ph2.
figure 2 shows mdvit’s excellent performance on isic and
dmf and that it achieves the closest results to ground truth on scd and ph2.
more segmentation results are presented in the supplementary material.
though
bat and transfuse in st have better results on some datasets like isic, they
require extra compute resources to train m models as well as an m-fold increase
in memory requirements.
in table 2-d, base† confronts nkt, which
lowers the performance on dmf compared with base in st, whereas mdvit†
not only addresses nkt but also outperforms base† on average dice and iou.
456
s. du et al.
ablation studies and plug-in capability of da: we conduct ablation
studies to demonstrate the eﬃcacy of da, mkd, and auxiliary peers.
as shown
in table 3-a,b, da can be used in various vits but is more advantageous in
mdvit with more transformer blocks in the encoding and decoding process.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_40.pdf:
transformers for medical image segmentation have attracted broad
interest.
unlike convolutional networks (cnns), transformers use self-attentions
that do not have a strong inductive bias.
although they,
e.g. swinunetr, achieve state-of-the-art (sota) results on some benchmarks,
the lack of inductive bias makes transformers harder to train, requires much
more training data, and are sensitive to training recipes.
in many clinical scenar-
ios and challenges, transformers can still have inferior performances than sota
cnns like nnunet.
a transformer backbone and corresponding training recipe,
which can achieve top performances under different medical image segmenta-
tion scenarios, still needs to be developed.
in this paper, we enhance the swi-
nunetr with convolutions, which results in a surprisingly stronger backbone,
the swinunetr-v2, for 3d medical image segmentation.
it achieves top per-
formance on a variety of benchmarks of different sizes and modalities, including
the whole abdominal organ dataset (word), miccai flare2021 dataset,
msd pancreas dataset, msd prostate dataset, and msd lung cancer dataset,
all using the same training recipe (https://github.com/project-monai/research-
contributions/tree/main/swinunetr/btcv, our training recipe is the same as
that by swinunetr) with minimum changes across tasks.
keywords: swin transformer · convolution · hybrid model · medical image
segmentation
1
introduction
medical image segmentation is a core step for quantitative and precision medicine.
in
the past decade, convolutional neural networks (cnns) became the sota method to
achieve accurate and fast medical image segmentation
[21], has achieved top performances on over 20 medical segmenta-
tion challenges.
parallel to manually created networks such as nn-unet, dints [10],
a cnn designed by automated neural network search, also achieved top performances
in medical segmentation decathlon (msd)
the convolution operation
in cnn provides a strong inductive bias which is translational equivalent and efﬁ-
cient in capturing local features like boundary and texture.
however, this inductive
bias limits the representation power of cnn models which means a potentially lower
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
https://doi.org/10.1007/978-3-031-43901-8_40
swinunetr-v2: stronger swin backbone
417
performance ceiling on more challenging tasks [7].
recently, vision transformers have been proposed, which adopt the transformers
in natural language processing by splitting images into patches (tokens)
this is intriguing and numerous works
have been proposed to incorporate transformer attentions into medical image segmen-
tation
[23] has achieved the new
top performance in the msd challenge and beyond the cranial vault (btcv) segmen-
tation challenge by pretraining on large datasets.
although transformers have achieved certain success in medical imaging, the lack
of inductive bias makes them harder to be trained and requires much more training
data to avoid overﬁtting.
unlike natural image segmentation benchmarks, e.g. ade20k
[34],
where the challenge is in learning complex relationships and scene understanding from
a large amount of labeled training images, many medical image segmentation networks
need to be extremely focused on local boundary details while less in need of high-
level relationships.
besides lacking
inductive bias and enough training data, one extra reason could be that transformers are
computationally much expensive and harder to tune.
more improvements and empirical
evidence are needed before we say transformers are ready to replace cnns for medical
image segmentation.
in this paper, we try to develop a new “to-go” transformer for 3d medical image
segmentation, which is expected to exhibit strong performance under different data sit-
uations and does not require extensive hyperparameter tuning.
swinunetr reaches
top performances on several large benchmarks, making itself the current sota, but
without effective pretraining and excessive tuning, its performance on new datasets and
challenges is not as high-performing as expected.
many methods have been proposed and most
of them fall into two directions: 1) a new self-attention scheme to have convolution-
like properties [5,7,16,25,26,29].
it uses a local window instead of the whole image to perform self-attention.
[7] uses
gated positional self-attention which is equipped with a soft convolutional inductive
bias.
our swinunetr-v2 adds a convolu-
tion block at the beginning of each resolution stage.
although those works showed strong performances, which works best and can
be the “to go” transformer for 3d medical image segmentation is still unknown.
for
this purpose, we design the swinunetr-v2, which improves the current sota swi-
nunetr by introducing stage-wise convolutions into the backbone.
differently, our work only adds a resconv block at the beginning of each stage, which
is a macro-network level design.
although simple, we found it surprisingly effective for 3d medical image
segmentation.
the network is evaluated extensively on a variety of benchmarks and
achieved top performances on the word [17], flare2021
we also experimented with four design varia-
tions inspired by existing works to justify the swinunetr-v2 design.
four stages of swin transformer block
followed by patch merging are used to encode the input patches.
a patch merging layer is applied after every swin transformer block to
reduce each spatial dimension by half.
420
y. he et al.
stage-wise convolution.
although swin-transformer uses local window attention to
introduce inductive bias like convolutions, self-attentions can still mess up with the
local details.
we experimented with multiple designs as in fig.
and found that inter-
leaved stage-wise convolution is the most effective for swin: convolution followed by
swin blocks, then convolution goes on.
at the beginning of each resolution level (stage),
the input tokens are reshaped back to the original 3d volumes.
there are in total 4 resconv blocks at 4 stages.
we also tried
inverted convolution blocks with depth-wise convolution like moat [31] or with orig-
inal 3d convolution, they improve the performance but are worse than the resconv
block.
decoder.
a ﬁnal convolution with 1×1×1 kernel
is used to map features to segmentation maps.
3
experiments
we use extensive experiments to show its effectiveness and justify its design for 3d
medical image segmentation.
we use this split for our experiments.
2) the miccai flare 2021 dataset [18].
it provides 361 training scans with man-
ual labels from 11 medical centers.
each scan is an abdominal 3d ct image with 4
organ annotations.
the medi-
cal segmentation decathlon (msd)
the lung tumor dataset contains 63 lung ct images with tumor annotations.
the challenge comes from segmenting small tumors from large full 3d ct images.
implementation details
the training pipeline is based on the publicly available swinunetr codebase
(https://github.com/project-monai/research-contributions/tree/main/swinunetr/bt
cv, our training recipe is the same as that by swinunetr).
random gaussian smooth, gaussian noise, and ran-
dom gamma correction are also added as additional data augmentation.
msd data are resampled to 1 × 1x1 mm
resolution and normalized to zero mean and standard deviation (ct images are ﬁrstly
clipped by .5% and 99.5% foreground intensity percentile).
swin-var-bot changes the top 2 stages of the encoder with resconv
blocks and only keeps transformer blocks in the higher stages.
to make a fair com-
parison, we didn’t use any test-time augmentation or model ensemble.
flare 2021 5-fold cross-validation average test dice scores (on held-out test scans)
and standard deviation in brackets.
baseline results from 3d ux-net paper [14].
3d u-net segresnet rap-net nn-unet transbts unetr nnformer swinunetr 3d ux-net swinunetr-v2
spleen
0.911
0.963
0.946
0.971
0.964
0.927
0.973
0.979
0.981
0.980 (0.018)
kidney
0.962
0.934
0.967
0.966
0.959
0.947
0.960
0.965
0.969
0.973 (0.013)
liver
0.905
0.965
0.940
0.976
0.974
0.960
0.975
0.980
0.982
0.983 (0.008)
pancreas 0.789
0.745
0.799
0.792
0.711
0.710
0.717
0.788
0.801
0.851 (0.037)
mean
0.892
0.902
0.913
0.926
0.902
0.886
0.906
0.929
0.934
0.947 (0.019)
swinunetr-v2: stronger swin backbone
423
table 4. msd prostate, lung, and pancreas 5-fold cross-validation average test dice scores and
standard deviation in brackets.
following [14], the ﬁve trained models are evaluated on 20 held-out test
scans, and the average dice scores (not model ensemble) are shown in table 3.
for msd datasets, we perform 5-fold cross-validation and ran the base-
line experiments with our codebase using exactly the same hyperparameters as men-
tioned.
nnunet2d/3d baseline experiments are performed using nnunet’s original code-
base2 since it has its own automatic hyperparameter selection.
the test dice score and
standard deviation (averaged over 5 fold) are shown in table 4.
we didn’t compare with leaderboard results because
the purpose of the experiments is to make fair comparisons, while not resorting to addi-
tional training data/pretraining, postprocessing, or model ensembling.
variations of swinunetr-v2 in this section, we investigate other variations of adding
convolutions into swin transformer.
as for the (2.a) of parallel
branches, it increases the gpu memory usage for 3d medical image too much and
2 https://github.com/mic-dkfz/nnunet.
424
y. he et al.
3, we investigate 1) swin-var-
bot (2.b scheme): replacing the top 2 stages of swin transformer with resconv block,
and keeping the bottom two stages using swin blocks.
3) swin-
var-res (2.c scheme): instead of only adding resconv blocks at the beginning of each
stage, we create a new swin transformer block which all starts with this resconv block,
like the moat [31] work.
we can see that adding
convolution at different places does affect the performances, and the swinunetr-v2
design is the optimal on word test set.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_54.pdf:
breast
dynamic
contrast-enhanced
magnetic
resonance
imaging (dce-mri) plays an important role in the screening and prog-
nosis assessment of high-risk breast cancer.
the segmentation of can-
cerous regions is essential useful for the subsequent analysis of breast
mri.
to alleviate the annotation eﬀort to train the segmentation net-
works, we propose a weakly-supervised strategy using extreme points as
annotations for breast cancer segmentation.
the network ﬁrst utilizes the pseudo-masks generated using the extreme
points to train itself, by minimizing a contrastive loss, which encourages
the network to learn more representative features for cancerous voxels.
then the trained network ﬁne-tunes itself by using a similarity-aware
propagation learning (simple) strategy, which leverages feature similar-
ity between unlabeled and positive voxels to propagate labels.
experimental results demonstrate our method eﬀectively
ﬁne-tunes the network by using the simple strategy, and achieves a
mean dice value of 81%.
keywords: breast cancer · weakly-supervised learning · medical
image segmentation · contrastive learning · dce-mri
1
introduction
breast cancer is the most common cause of cancer-related deaths among women
all around the world [8].
early diagnosis and treatment is beneﬁcial to improve
the survival rate and prognosis of breast cancer patients.
1. breast mri and diﬀerent annotations: (a) t1-weighted images, (b) correspond-
ing contrast-enhanced images, (c) the cancer annotation with full segmentation masks,
and (d) the cancer annotation using extreme points (note that to facilitate the visual-
ization, here we show the extreme points in 2d images, our method is based on 3d).
t1-weighted acquisition depicts enhancing
abnormalities after contrast material administration, that is, the cancer screen-
ing is performed by using the post-contrast images.
radiologists will analyze
features such as texture, morphology, and then make the treatment plan or
prognosis assessment.
computer-aided feature quantiﬁcation and diagnosis algo-
rithms have recently been exploited to facilitate radiologists analyze breast dce-
mri [12,22], in which automatic cancer segmentation is the very ﬁrst and impor-
tant step.
to better support the radiologists with breast cancer diagnosis, various seg-
mentation algorithms have been developed [20].
early studies focused on image
processing based approaches by conducting graph-cut segmentation [29] or ana-
lyzing low-level hand-crafted features [1,11,19].
these methods may encounter
the issue of high computational complexity when analyzing volumetric data,
and most of them require manual interactions.
[28] proposed a
mask-guided hierarchical learning framework for breast tumor segmentation via
convolutional neural networks (cnns), in which breast masks were also required
to train one of cnns.
[30]
employed a 3d aﬃnity learning based multi-branch ensemble network for the
simple for weakly-supervised segmentation
569
segmentation reﬁnement and generated 78% dice on 90 testing subjects.
[25] proposed
a tumor-sensitive synthesis module to reduce false segmentation and obtained
78% dice value.
to reduce the huge annotation burden for the segmentation task,
zeng et al.
[27] presented a semi-supervised strategy to segment the manually
cropped dce-mri scans, and attained a dice value of 78%.
although [27] has been proposed to alleviate the annotation eﬀort, to acquire
the voxel-level segmentation masks is still time-consuming and laborious, see
fig. 1(c).
[21]
utilized extreme points to generate scribbles to supervise the training of the
segmentation network.
[5] introduced a regular-
ized loss [4] derived from a conditional random field (crf) formulation to
encourage the prediction consistency over homogeneous regions.
[6]
employed bounding boxes to train the segmentation network for organs.
how-
ever, the geometric prior used in [6] can not be an appropriate strategy for the
segmentation of lesions with various shapes.
to our knowledge, currently only
one weakly-supervised work [18] has been proposed for breast mass segmentation
in dce-mri.
this method employed three partial annotation methods including
single-slice, orthogonal-slice (i.e., 3 slices) and interval-slice (∼6 slices) to allevi-
ate the annotation cost, and then constrained segmentation by estimated volume
using the partial annotation.
in this study, we propose a simple yet eﬀective weakly-supervised strategy,
by using extreme points as annotations (see fig. 1(d)) to segment breast cancer.
speciﬁcally, we attempt to optimize the segmentation network via the conven-
tional train - ﬁne-tune - retrain process.
experimental
results show our method achieves competitive performance compared with fully
supervision, demonstrating the eﬃcacy of the proposed simple strategy.
the segmentation network is ﬁrstly trained based on
the initial pseudo-masks.
2. the schematic illustration of the proposed similarity-aware propagation learn-
ing (simple) and the train - ﬁne-tune - retrain procedure for the breast cancer seg-
mentation in dce-mri.
2.1
generate initial pseudo-masks
we use the extreme points to generate pseudo-masks based on random walker
algorithm
[9]. to improve the performance of random walker, according to [21],
we ﬁrst generate scribbles by searching the shortest path on gradient magnitude
map between each extreme point pair via the dijkstra algorithm
a simple training approach is to minimize the partial
cross entropy loss lpce, which is formulated as:
simple for weakly-supervised segmentation
571
lpce = −

yinit(k)=0
log(1 − f(x; θ)(k))
(1)
moreover, supervised contrastive learning is employed to encourage voxels
of the same label to gather around in feature space.
(3)
2.3
simple-based fine-tune and retrain
the performance of the network trained by the incomplete initial pseudo-masks
is still limited.
then the network is ﬁne-tuned using the partial cross
entropy loss same as in the initial train stage.
finally the network is retrained from random initial-
ization by minimizing the cross entropy loss with the binary pseudo-masks.
3
experiments
dataset.
the dce-mri sequences
(tr/te = 4.43 ms/1.50 ms, and ﬂip angle = 10◦) using gadolinium-based con-
trast agent were performed with the t1-weighted gradient echo technique,
and injected 0.2 ml/kg intravenously at 2.0 ml/s followed by 20 ml saline.
all cancerous regions and extreme points were manually annotated by an
experienced radiologist via itk-snap [26] and further conﬁrmed by another
radiologist.
implementation details.
the framework was implemented in pytorch, using
a nvidia geforce gtx 1080 ti with 11gb of memory.
the batch size was 2,
consisting of a random foreground patch and a random background patch
located via initial segmentation yinit.
for the simple strategy, we set n = 100, λ = 0.96, α = 0.96, w = 0.1.
1 we have tried diﬀerent amount of training data to investigate the segmentation
performance of the fully-supervised network.
simple for weakly-supervised segmentation
573
fig.
the pseudo-masks (shown as boundaries) at diﬀerent training stages, including
the initial pseudo-mask generated by the random walker (purple), the trained network’s
output (green), the ﬁne-tuned pseudo-mask using simple (blue) and the ground-truth
(red).
note that all images here are the training images.
the segmentation visualization in transversal slices.
the blue and red contours
are the segmented boundaries and the ground-truths, respectively.
• retrain: the training strategy was the same as the initial train stage.
we ﬁrst veriﬁed the eﬃcacy of our
simple in the training stage.
figure 3 illustrates the pseudo-masks at diﬀerent
training stages.
therefore, such fune-tuned
pseudo-masks could be used to retrain the network for better performance.
5. three cases of 3d visualization of the surface distance between segmented
surface and ground-truth.
the proposed simple consistently enhances the segmentation.
the numerical results of diﬀerent methods for breast cancer segmentation
methods
dice [%]
jaccard
lctr+ simple
81.20 ± 13.28 70.01 ± 15.02 0.69 ± 0.44 2.40 ± 1.69
fully supervision
81.52 ± 19.40 72.10 ± 20.45 0.68 ± 0.63 2.40 ± 2.76
figure 4 visualizes our cancer segmentation results on the testing data.
table 1 reports the quantitative dice, jaccard, average surface distance (asd),
and hausdorﬀ distance (95hd) results of diﬀerent methods.
in contrast, the proposed
simple largely boosted the performance of the basically trained networks, by
+14.74% dice and +15.16% jaccard (v.s. lcrf), +11.81% dice and +12.65%
jaccard (v.s. lctr).
note that the average anno-
tation time for extreme points and full masks were 31 s and 95 s per scan, respec-
tively.
figure 5 visualizes the 3d distance map between the segmented surface
and ground-truth.
it can be observed that our simple consistently enhanced
the segmentation.
simple for weakly-supervised segmentation
575
4
conclusion
we introduce a simple yet eﬀective weakly-supervised learning method for breast
cancer segmentation in dce-mri.
the primary attribute is to fully exploit the
simple train - ﬁne-tune - retrain process to optimize the segmentation network
via only extreme point annotations.
experimental results demonstrate the eﬃcacy of the proposed simple strat-
egy for weakly-supervised segmentation.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_4.pdf:
classiﬁcation and segmentation are crucial in medical image
analysis as they enable accurate diagnosis and disease monitoring.
how-
ever, current methods often prioritize the mutual learning features and
shared model parameters, while neglecting the reliability of features and
performances.
in this paper, we propose a novel uncertainty-informed
mutual learning (uml) framework for reliable and interpretable medi-
cal image analysis.
our uml introduces reliability to joint classiﬁcation
and segmentation tasks, leveraging mutual learning with uncertainty to
improve performance.
to achieve this, we ﬁrst use evidential deep learn-
ing to provide image-level and pixel-wise conﬁdences.
then, an uncer-
tainty navigator is constructed for better using mutual features and gen-
erating segmentation results.
overall, uml could
produce conﬁdence estimation in features and performance for each link
(classiﬁcation and segmentation).
the experiments on the public datasets
demonstrate that our uml outperforms existing methods in terms of
both accuracy and robustness.
our uml has the potential to explore
the development of more reliable and explainable medical image analysis
models.
keywords: mutual learning · medical image classiﬁcation and
segmentation · uncertainty estimation
k. ren and k. zou—denotes equal contribution.
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43901-8 4.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
https://doi.org/10.1007/978-3-031-43901-8_4
36
k. ren et al.
1
introduction
accurate and robust classiﬁcation and segmentation of the medical image are
powerful tools to inform diagnostic schemes.
in clinical practice, the image-level
classiﬁcation and pixel-wise segmentation tasks are not independent [8,27].
joint
classiﬁcation and segmentation can not only provide clinicians with results for
both tasks simultaneously, but also extract valuable information and improve
performance.
however, improving the reliability and interpretability of medical
image analysis is still reaching.
considering the close correlation between the classiﬁcation and segmenta-
tion, many researchers [6,8,20,22,24,27,28] proposed to collaboratively analyze
the two tasks with the help of sharing model parameters or task interacting.
most of the methods are based on sharing model parameters, which improves
the performance by fully utilizing the supervision from multiple tasks [8,27].
[20] combined whole image classiﬁcation and segmenta-
tion of skin cancer using a shared encoder.
however, there has been relatively
little research on introducing reliability into joint classiﬁcation and segmenta-
tion.
the reliability and interpretability of the model are particularly important
for clinical tasks, a single result of the most likely hypothesis without any clues
about how to make the decision might lead to misdiagnoses and sub-optimal
treatment [10,22].
one potential way of improving reliability is to introduce
uncertainty for the medical image analysis model.
all of these methods are widely utilized in
classiﬁcation and segmentation applications for medical image analysis.
[1] employed three uncertainty quantiﬁcation methods (monte carlo dropout,
ensemble mc dropout, and deep ensemble) simultaneously to deal with uncer-
tainty estimation during skin cancer image classiﬁcation.
[31] pro-
posed tbrats based on evidential deep learning to generate robust segmentation
results for brain tumor and reliable uncertainty estimations.
unlike the afore-
mentioned methods, which only focus on uncertainty in either medical image
classiﬁcation or segmentation.
furthermore, none of the existing methods have
considered how pixel-wise and image-level uncertainty can help improve perfor-
mance and reliability in mutual learning.
based on the analysis presented above, we design a novel uncertainty-
informed mutual learning (uml) network for medical image analysis in this
study.
our uml not only enhances the image-level and pixel-wise reliability of
medical image classiﬁcation and segmentation, but also leverages mutual learning
under uncertainty to improve performance.
[16,31] to simultaneously estimate the uncertainty of both to estimate
image-level and pixel-wise uncertainty.
we introduce an uncertainty navigator
for segmentation (un) to generate preliminary segmentation results, taking into
account the uncertainty of mutual learning features.
we also propose an uncer-
uncertainty-informed mutual learning
37
             uncertainty navigator (un)
             uncertainty instructor (ui)
lcls
c
c
label
cls 
feature
encoder
un
feature
mixer
seg
feature
encoder
f
c
cls 
evidence
lseg
u s
m
gt
s
(a)
(b)
mutual
feature
decoder
0
1
ui
gt
lmut
u c
b1
b0
mutual
evidence
mutual
dirichlet
cls 
dirichlet 
flow of 
feature
softplus
global average 
pooling
flow of 
uncertainty
c
concat
deep 
supervision
conv
reliable
mask
fig.
tainty instructor for classiﬁcation (ui) to screen reliable masks for classiﬁcation
based on the preliminary segmentation results.
our uml represents pioneering
work in introducing reliability and interpretability to joint classiﬁcation and seg-
mentation, which has the potential to the development of more trusted medical
analysis tools1.
2
method
the overall architecture of the proposed uml, which leverages mutual learn-
ing under uncertainty, is illustrated in fig.
firstly, uncertainty estimation
for classiﬁcation and segmentation adapts evidential deep learning to provide
image-level and pixel-wise uncertainty.
then, trusted mutual learning not only
utilizes the proposed un to fully exploit pixel-wise uncertainty as the guidance
for segmentation but also introduces the ui to ﬁlter the feature ﬂow between
task interaction.
given an input medical image
i, i ∈ rh,w , where h, w are the height and
width of the image, separately.
to maximize the extraction of speciﬁc infor-
mation required for two diﬀerent tasks while adequately mingling the common
feature which is helpful for both classiﬁcation and segmentation, i is ﬁrstly fed
into the dual backbone network that outputs the classiﬁcation feature maps
f c
i , i ∈ 1, ..., 4 and segmentation feature maps f s
i , i ∈ 1, ..., 4, where i denotes the
ith layer of the backbone.
2.1
uncertainty estimation for classiﬁcation and segmentation
classiﬁcation uncertainty estimation.
for the k classiﬁcation problems,
we utilize subjective logic [7] to produce the belief mass of each class and the
uncertainty mass of the whole image based on evidence.
, αc
k], which associated with the cls evidence ec
k, i.e. αc
k =
ec
k + 1. in the end, the image-level belief mass and the uncertainty mass of the
classiﬁcation can be calculated by
bc
k = ec
k
t c
ally, eq. 2 describes such a phenomenon that the higher the probability assigned
to the kth class, the more evidence observed for kth category should be.
segmentation uncertainty estimation.
essentially, segmentation is the
classiﬁcation for each pixel of a medical image.
given a pixel-wise segmenta-
tion result, following [31] the seg dirichlet distribution can be parameterized
by αs(h,w) =
we can compute the belief
mass and uncertainty mass of the input image by
bs(h,w)
q
= es(h,w)
q
t s(h,w) = αs(h,w)
q
− 1
t s(h,w)
,
and us(h,w) =
q
t s(h,w) ,
(3)
where bs(h,w)
q
≥ 0 and us(h,w) ≥ 0 denote the probability of the pixel at coordi-
nate (h, w) for the qth class and the overall uncertainty value respectively.
we
also deﬁne u s = {us(h,w), (h, w) ∈ (h, w)} as the pixel-wise uncertainty of the
segmentation result.
2.2
uncertainty-informed mutual learning
uncertainty navigator for segmentation.
actually, we have already
obtained an initial segmentation mask m = αs, m ∈ (q, h, w) through esti-
mating segmentation uncertainty, and achieved lots of valuable features such as
uncertainty-informed mutual learning
39
fig.
in our method, appropriate uncertainty guided decoding on the
feature list can obtain more reliable information and improve the performance
of segmentation [3,9,26].
so we introduce uncertainty navigator for segmen-
tation(un) as a feature decoder, which incorporates the pixel-wise uncertainty
in u sand lesion location information in m with the segmentation feature maps
to generate the segmentation result and reliable features.
having a unet-like
architecture [15], un computes segmentation si, i ∈ 1, .., 4 at each layer, as well
as introduces the uncertainty in the bottom and top layer by the same way.
, un calculates the reliable
mask m r by:
m r = (s1 ⊕ m) ⊗ e−u s,
(4)
then, the reliable segmentation feature rs, which combines the trusted informa-
tion in m r with the original features, is generated by:
rs = cat(conv(m r), cat(f s
1, f b
2)),
(5)
where f s
1 derives from jump connecting and f b
2 is the feature of the s2 with one
up-sample operation.
the rs is calculated from the segmentation result
s1 and contains uncertainty navigated information not found in s1.
in order to mine the comple-
mentary knowledge of segmentation as the instruction for the classiﬁcation and
eliminate intrusive features, we devise an uncertainty instructor for classiﬁcation
(ui) following [22]. figure 2(b) shows the architecture of ui.
⊗ f c
4),
(6)
where dn(·) denotes that the frequency of down-sampling operations is n. then
the produced features are transformed into a semantic feature vector by the
40
k. ren et al.
global average pooling.
2.3
mutual learning process
in a word, to obtain the ﬁnal results of classiﬁcation and segmentation, we con-
struct an end-to-end mutual learning process, which is supervised by a joint loss
function.
to obtain an initial segmentation result m and a pixel-wise uncertainty
estimation u s, following [31], a mutual loss is used as:
lm(αs, ys) = lice(αs, ys) + λm
1 lkl(αs)
+ λm
2 ldice(αs, ys),
(7)
where ys is the ground truth (gt) of the segmentation.
similarly, in order to estimate the image-
level uncertainty and classiﬁcation results.
8)
where yc is the true class of the input image.
the hyperparameter λc serves as
a crucial hyperparameter governing the kl, aligning with previous work [5]. to
obtain reliable segmentation results, we also adopt deep supervision for the ﬁnal
segmentation result s = {si, i = 1, ..., 4}, which can be denoted as:
ls =
4
i=1 ldice(υi−1(si), ys)
4
,
(9)
where υn indicates the number of up-sampling is 2n.
+ wclc(αc, yc) + wsls,
(10)
where wm, wc, ws denote the weights and are set 0.1, 0.5, 0.4, separately.
3
experiments
dataset and implementation.
[13]. refuge contains two tasks, classiﬁ-
cation of glaucoma and segmentation of optic disc/cup in fundus images.
the
overall 1200 images were equally divided for training, validation, and testing.
all images are uniformly adjusted to 256 × 256 px.
the tasks of ispy-1 are
the pcr prediction and the breast tumor segmentation.
for each case, we cut out the slices in the 3d image and totally got 1,570 2d
images, which are randomly divided into the train, validation, and test datasets
with 1,230, 170, and 170 slices, respectively.
uncertainty-informed mutual learning
41
table 1. evaluation of the classiﬁcation and segmentation performance.
3. the visual result of segmentation and classiﬁcation in refuge and ispy-1.
top is the original image, and bottom is the input with gaussian noise (σ = 0.05).
from left to right, input (with gt), the result of classiﬁcation (belief and image-level
uncertainty), the result of segmentation, pixel-wise uncertainty.
we implement the proposed method via pytorch and train it on nvidia
geforce rtx 2080ti.
we choose vgg-16 and res2net as the encoders
for classiﬁcation and segmentation, separately.
tbrats then extended ec to medical image seg-
mentation.
meriting both transformers and u-net, transunet is a strong model
for medical image segmentation.
the baseline of the joint classiﬁcation and segmentation framework
(bcs) is a simple but useful way to share model parameters, which utilize two
diﬀerent encoders and decoders for learning respectively.
the deep synergistic
interaction network (dsi) has demonstrated superior performance in joint task.
dice score (di) and average symmetric surface
distance (assd) are chosen for the segmentation task.
as shown in table 1,
we report the performance on the two datasets of the proposed uml and other
methods.
by comparison, we can observe the fact that the accuracy of the model
results is low if either classiﬁcation or segmentation is done in isolation, the
acc has only just broken 0.5 in ec.
but joint classiﬁcation and segmentation
changes this situation, the performance of bcs and dsi improves considerably,
especially the acc and the dice score of optic cup.
excitingly, our uml not only
achieves the best classiﬁcation performance in acc (85.3%) and f1 (0.875) with
signiﬁcant increments of 1.8%, 4.9%, but also obtains the superior segmentation
performance with increments of 6.6% in didisc and 3.2% in dicup.
a similar
improvement can be observed in the experimental results in ispy-1.
comparison under noisy data.
to further valid the reliability of our model,
we introduce gaussian noise with various levels of standard deviations (σ) to
the input medical images.
as
can be observed that, the accuracy of classiﬁcation and segmentation signiﬁ-
cantly decreases after adding noise to the raw data.
it is obvious that both
the image-level uncertainty and the pixel-wise uncertainty respond reasonably
well to noise.
these experimental results can verify the reliability and interpre of
the uncertainty guided interaction between the classiﬁcation and segmentation
in the proposed uml.
the results of more qualitative comparisons can be found
in the supplementary material.
ablation study.
it is clear that the performance
of classiﬁcation and segmentation is signiﬁcantly improved when we introduce
supervision of mutual features.
4
conclusion
in this paper, we propose a novel deep learning approach, uml, for joint classiﬁ-
cation and segmentation of medical images.
our approach is designed to improve
the reliability and interpretability of medical image classiﬁcation and segmenta-
tion, by enhancing image-level and pixel-wise reliability estimated by evidential
deep learning, and by leveraging mutual learning with the proposed un and
ui modules.
our extensive experiments demonstrate that uml outperforms
baselines and introduces signiﬁcant improvements in both classiﬁcation and seg-
mentation.
overall, our results highlight the potential of uml for enhancing the
performance and interpretability of medical image analysis.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_41.pdf:
skin lesion segmentation in dermoscopy images has seen
recent success due to advancements in multi-scale boundary attention
and feature-enhanced modules.
however, existing methods that rely on
end-to-end learning paradigms, which directly input images and output
segmentation maps, often struggle with extremely hard boundaries, such
as those found in lesions of particularly small or large sizes.
motivated by the impressive advances of diﬀusion models that
regard image synthesis as a parameterized chain process, we introduce
a novel approach that formulates skin lesion segmentation as a bound-
ary evolution process to thoroughly investigate the boundary knowledge.
speciﬁcally, we propose the medical boundary diﬀusion model (mb-
diﬀ), which starts with a randomly sampled gaussian noise, and the
boundary evolves within ﬁnite times to obtain a clear segmentation map.
first, we propose an eﬃcient multi-scale image guidance module to con-
strain the boundary evolution, which makes the evolution direction suit
our desired lesions.
we evaluate the performance of our model on two popular
skin lesion segmentation datasets and compare our model to the latest
cnn and transformer models.
our results demonstrate that our model
outperforms existing methods in all metrics and achieves superior per-
formance on extremely challenging skin lesions.
the proposed approach
has the potential to signiﬁcantly enhance the accuracy and reliability
of skin lesion segmentation, providing critical information for diagnosis
and treatment.
all resources will be publicly available at https://github.
com/jcwang123/mbdiﬀ.
keywords: skin lesion segmentation · diﬀusion model
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43901-8_41.
https://doi.org/10.1007/978-3-031-43901-8_41
428
j. wang et al.
1
introduction
segmentation of skin lesions from dermoscopy images is a critical task in
disease diagnosis and treatment planning of skin cancers [17].
manual lesion
segmentation is time-consuming and prone to inter- and intra-observer vari-
ability.
to improve the eﬃciency and accuracy of clinical workﬂows, numer-
ous automated skin lesion segmentation models have been developed over the
years
[2,18,19], resulting in signiﬁcant improvements in skin lesion segmenta-
tion performance.
despite these advances, the segmentation of skin lesions with
ambiguous boundaries, particularly at extremely challenging scales, remains a
bottleneck issue that needs to be addressed.
in such cases, even state-of-the-art
segmentation models struggle to achieve accurate and consistent results.
boundary evolution
image
small
large
fig.
it could be seen that various lesions can be
accurately segmented by splitting the segmentation into sequential timesteps (t), named
as boundary evolution in this work.
the small one covers
1.03% in the image space and the large one covers 72.96%.
as studied prior,
solving the segmentation problems of such two types of lesions have diﬀerent
strategies.
the latest transformer,
xbound-former, comprehensively addresses the multi-scale boundary problem
through cross-scale boundary learning and exactly reaches higher performance
on whatever small or large lesions.
, current models for skin lesion segmentation are still struggling with
extremely challenging cases, which are often encountered in clinical practice.
while some approaches aim to optimize the model architecture by incorporating
local and global contexts and multi-task supervision, and others seek to improve
performance by collecting more labeled data and building larger models, both
strategies are costly and can be limited by the inherent complexity of skin lesion
boundaries.
therefore, we propose a novel approach that shifts the focus from
merely segmenting lesion boundaries to predicting their evolution.
our approach
is inspired by recent advances in image synthesis achieved by diﬀusion proba-
bilistic models [6,9,14,15], which generate synthetic samples from a randomly
sampled gaussian distribution in a series of ﬁnite steps.
we adapt this process
to model the evolution of skin lesion boundaries as a parameterized chain pro-
cess, starting from gaussian noise and progressing through a series of denoising
steps to yield a clear segmentation map with well-deﬁned lesion boundaries.
by
predicting the next step in the chain process rather than the ﬁnal segmenta-
tion map, our approach enables the more accurate segmentation of challenging
lesions than previous models.
1, where each row corresponds to a diﬀerent step in the evolution process,
culminating in a clear segmentation map with well-deﬁned boundaries.
in this paper, we propose a medical boundary diﬀusion model (mb-diﬀ)
to improve the skin lesion segmentation, particularly in cases where the lesion
boundaries are ambiguous and have extremely large or small sizes.
however, it also includes two
key innovations: firstly, we have developed an eﬃcient multi-scale image guid-
ance module, which uses a pretrained transformer encoder to extract multi-scale
features from prior images.
secondly, we have implemented an
evolution uncertainty-based fusion strategy, which takes into account the uncer-
tainty of diﬀerent initializations to reﬁne the evolution results and obtain more
precise lesion boundaries.
we evaluate our model on two popular skin lesion
segmentation datasets, isic-2016 and ph2 datasets, and ﬁnd that it performs
signiﬁcantly better than existing models.
speciﬁcally, given the image and boundary mask distributions as (x, y), assum-
ing that the evolution consists of t steps in total, the boundary at t-th step
(yt ) is the randomly initialized noise and the boundary at 0-th (y0) step denotes
the accurate result.
(2)
note that the prediction function takes the input image as a condition, enabling
the evolving boundary to ﬁt the corresponding lesion accurately.
by modeling
boundary evolution as a step-by-step denoising process, mb-diﬀ can eﬀectively
capture the complex structures of skin lesions with ambiguous boundaries, lead-
ing to superior performance in lesion segmentation.
to optimize the model parameters θ, we use the evolution target as an approx-
imation of the posterior at each evolution step.
given the segmentation label y
as y0, the label is gradually added by a gaussian noise as:
q(y1:t |y0) := t
t=1 q(yt|yt−1) :
2.2
paramterized architecture with image prior
the proposed model is a parameterized chain process that predicts the μ∗
t−1 and
∗
t−1 at each evolution step t under the prior conditions of the image x
to capture the deep semantics of these conditions and
perform eﬃcient fusion, we adopt a basic u-net
[16] architecture inspired by the
plain dpm and introduce novel designs for condition fusion, that is the eﬃcient
multi-scale image guidance module.
at the bottleneck
layer, we fuse the evolution features with the image guidance to constrain the
evolution and ensure that the ﬁnal boundary suits the conditional image.
to achieve this, priors train a segmentation model concurrently with the
evolution model and use an attention-based parser to translate the image features
medical boundary diﬀusion model
431
in the segmentation branch into the evolution branch [22].
since the segmentation
model is trained much faster than the evolution model, we adopt a pretrained
pyramid vision transformer (pvt)
[20] as the image feature extractor to obtain
the multi-scale image features.
after that, the four features are concatenated and fed into a full-
connection layer to map the image feature space into the evolution space.
we
then perform a simple yet eﬀective addition of the mapped image feature and
the encoded prior evolution feature, similar to the fusion of time embeddings, to
avoid redundant computation.
2.3
evolution uncertainty
similar to typical evolutionary algorithms, the ﬁnal results of boundary evolu-
tion are heavily inﬂuenced by the initialized population.
the
reason is that the image features in such ambiguous regions may not provide
discriminative guidance for the evolution, resulting in signiﬁcant variations in
diﬀerent evolution times.
instead of reducing the diﬀerences, we surprisingly
ﬁnd that these diﬀerences can represent segmentation uncertainty.
based on the
evolution-based uncertainty estimation, the segmentation results become more
accurate and trustworthy in practice [4,5,12].
unlike traditional segmentation
models that typically scale the prediction into the range of 0 to 1, the evolved
maps generated by mb-diﬀ have unﬁxed distributions due to random sampling.
therefore,
432
j. wang et al.
table 1. comparison of skin lesion segmentation with diﬀerent approaches on the isic-
2016 and ph2 datasets.
the averaged scores of both sets are presented respectively.
[22]
83.39
89.85
12.38
31.23
82.21
89.73
13.53
36.59
mb-diﬀ (ours)
88.87 93.78 7.19
18.90
87.12 92.85 9.16
22.95
we employ the max vote algorithm to obtain the ﬁnal segmentation map.
finally, the segmentation map is
generated as y∗ = (n
i=1 y∗,i) ≥ τ.
3
experiment
3.1
datasets and evaluation metrics
datasets: we use two publicly available skin lesion segmentation datasets from
diﬀerent institutions in our experiments: the isic-2016 dataset and the ph2
dataset.
[13],
which contains 200 labeled samples and is used to evaluate the generalization
performance of our methods.
evaluation metrics: to comprehensively compare the segmentation results,
particularly the boundary delineations, we employ four commonly used metrics
to quantitatively evaluate the performance of our segmentation methods.
these
metrics include the dice score, the iou score, average symmetric surface dis-
tance (assd), and hausdorﬀ distance of boundaries (95−th percentile; hd95).
to ensure fair comparison, all labels and predictions are resized to (512×512)
before computing these scores, following the approach of a previous study [18].
3.2
implementation details
for the diﬀusion model hyper-parameters, we use the default settings of the plain
diﬀusion model, which can be found in the supplementary materials.
regarding
medical boundary diﬀusion model
433
false negatives
true positves
false postives
image
u-net++
ca-net
transunet
mb-diff
(ours)
gt
transfuse
xbound-
former
medseg
diff
uncertainty
(ours)
fig.
the training parameters, we resize all images to (256 × 256) for eﬃcient memory
utilization and computation.
we use a set of random augmentations, including
vertical ﬂipping, horizontal ﬂipping, and random scale change (limited to 0.9 ∼
1.1), to augment the training data.
3.3
comparison with state-of-the-arts
we majorly compare our method to the latest skin lesion segmentation models,
including the cnn-based and transformer-based models, i.e., u-net++
though the parameters of cnns and transformers are
selected with the best performance on isic-2016 validation set and the param-
eters of our method are selected by completing the 200,000 iterations, mb-diﬀ
still achieves the 1.18% iou improvement and 0.7% dice improvement.
moreover, our method shows a larger improvement in generalization
performance on the ph2 dataset, indicating its better ability to handle new data.
our visual comparison reveals several key ﬁndings: (1) mb-diﬀ consistently
achieves better segmentation performance on small and large lesions due to its
thorough learning of boundary evolution, as seen in rows 3, 5, and 6.
(2) mb-
diﬀ is able to produce correct boundaries even in cases where they are nearly
indistinguishable in human perception, eliminating the need for further manual
adjustments and demonstrating signiﬁcant practical value.
(3) mb-diﬀ generates
fewer false positive segmentation, resulting in cleaner predictions that enhance
the user experience.
this information can be
used to guide human reﬁnement of the segmentation in practical applications,
ultimately increasing the ai’s trustworthiness.
3.4
detailed analysis of the evolution
in this subsection, we make a comprehensive analysis to investigate the perfor-
mance of each component in our method and compare it to the diﬀusion-based
model, medsegdiﬀ. the results of our ablation study are presented in fig.
3(a),
where “w/o evo” refers to using image features to directly train a segmentation
model with fpn [11] architecture and “w/o fusion” means no evolution fusion is
used.
to ensure a fair comparison, we average the scores of multiple evolutions
to represent the performance of “w/o fusion”.
the results demonstrate that our
evolutionary approach can signiﬁcantly improve performance, and the evolution
uncertainty-based fusion strategy further enhances performance.
3(b) shows that our method
converges faster and achieves smaller losses, indicating that our multi-scale image
guidance is more eﬀective than that of medsegdiﬀ. furthermore, we evaluate our
medical boundary diﬀusion model
435
method’s performance using parameters saved at diﬀerent iterations, as shown
in fig.
our results demonstrate that our method has competitive perfor-
mance at 50k iterations versus medsegdiﬀ at 200k iterations and our method
at 100k iterations has already outperformed well-trained medsegdiﬀ.
4

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_51.pdf:
the use of functional imaging such as pet in radiother-
apy (rt) is rapidly expanding with new cancer treatment techniques.
a fundamental step in rt planning is the accurate segmentation of
tumours based on clinical diagnosis.
furthermore, recent tumour con-
trol techniques such as intensity modulated radiation therapy (imrt)
dose painting requires the accurate calculation of multiple nested con-
tours of intensity values to optimise dose distribution across the tumour.
recently, convolutional neural networks (cnns) have achieved tremen-
dous success in image segmentation tasks, most of which present the
output map at a pixel-wise level.
in addition, for the dose painting
strategy, there is a need to develop image segmentation approaches that
reproducibly and accurately identify the high recurrent-risk contours.
to
address these issues, we propose a novel hybrid-cnn that integrates a
kernel smoothing-based probability contour approach (kspc) to produce
contour-based segmentation maps, which mimic expert behaviours and
provide accurate probability contours designed to optimise dose paint-
ing/imrt strategies.
instead of user-supplied tuning parameters, our
ﬁnal model, named kspc-net, applies a cnn backbone to automatically
learn the parameters and leverages the advantage of kspc to simultane-
ously identify object boundaries and provide probability contour accord-
ingly.
the proposed model demonstrated promising performance in com-
parison to state-of-the-art models on the miccai 2021 challenge dataset
(hecktor).
keywords: image segmentation · pet imaging · probability
contour · dose painting · deep learning
1
introduction
fluorodeoxyglucose positron emission tomography (pet) is widely recognized
as an essential tool in oncology
[10], playing an important role in the stag-
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43901-8 51.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
it enables the extrac-
tion of semi-quantitative metrics such as standardized uptake values (suvs),
which normalize pixel intensities based on patient weight and radiotracer dose
[20].
manual delineation is a time-consuming and laborious task that is prone to
poor reproducibility in medical imaging, and this is particularly true for pet,
due to its low signal-to-noise ratio and limited spatial resolution [10].
in addition,
manual delineation depends heavily on the expert’s prior knowledge, which often
leads to large inter-observer and intra-observer variations [8].
therefore, there
is an urgent need for developing accurate automatic segmentation algorithms in
pet images which will reduce expert workload, speed up rt planning while
reducing intra-observer variability.
in the last decade, cnns have demonstrated remarkable achievements in
medical image segmentation tasks.
despite the
headway made in using cnns, their applications have been restricted to the
generation of pixel-wise segmentation maps instead of smooth contour.
although
cnns may yield satisfactory segmentation results, low values of the loss function
may not always indicate a meaningful segmentation.
the kspc provides a surface
over images that naturally produces contour-based results rather than pixel-wise
results, thus mimicking experts’ hand segmentation.
however, the performance
of kspc depends heavily on the tuning parameters of bandwidth and threshold
in the model, and it lacks information from other patients.
beyond tumour delineation, another important use of functional images, such
as pet images is their use for designing imrt dose painting (dp).
in partic-
ular, dose painting uses functional images to paint optimised dose prescriptions
based on the spatially varying radiation sensitivities of tumours, thus enhanc-
ing the eﬃcacy of tumour control
however, there is an urgent need to
develop image segmentation approaches that reproducibly and accurately iden-
tify the high recurrent-risk contours [18].
our previously proposed kspc pro-
vides a clear framework to calculate the probability contours of the suv values
and can readily be used to deﬁne an objective strategy for segmenting tumours
into subregions based on metabolic activities, which in turn can be used to design
the imrt dp strategy.
in the proposed
kspc-net, a cnn is employed to learn directly from the data to produce the
pixel-wise bandwidth feature map and initial segmentation map, which are used
to deﬁne the tuning parameters in the kspc module.
more speciﬁcally, we use the classic unet
[17] as the cnn backbone and evaluate our kspc-net on the publicly avail-
able miccai hecktor (head and neck tumor segmentation) challenge
2021 dataset.
moreover, it can produce contour-based segmentation results which provide a
more accurate delineation of object edges and provide probability contours as a
byproduct, which can readily be used for dp planning.
2
methods
2.1
kernel smoothing based probability contour
kernel-based method and follow up approach of modal clustering [13,16] have
been used to cluster high-dimensional random variables and natural-scene image
segmentation.
in particular, let y = (y1, y2, ..., yn)
denote n pixel’s suv in a 2d pet image sequentially, and xi = (xi1, xi2), i =
1, ..., n denote position vector with xi1 and xi2 being the position in 2d respec-
tively.
by placing a threshold plane, a contour-based segmen-
tation map can naturally be obtained.
note that one can obtain a pixel-based
segmentation map, by thresholding the surface at the observed grid points.
a visualization example of how kspc works: (a) an example of a pet image
(b) grid-level intensity values as observations (c) the resulting smoothed surface built
by kspc with a threshold plane.
the estimated probability contour level fω can be computed as the ω-th
quantile of ˆfω of ˆf(x1; h), ..., ˆf(xn; h) (proof in supplementary materials).
the primary advantage of utilizing probability contours is their ability to
assign a clear probabilistic interpretation on the deﬁned contours, which are
scale-invariant [5].
4.2.
2.2
the kspc-net architecture
in the kspc module, the model performance heavily depends on the bandwidth
matrix h and it is often assumed that each kernel shares the same scalar band-
width parameter.
additionally, we obtain the optimal thresh-
old for constructing the kspc contour from the initial segmentation map.
2 the proposed kspc-net integrates the kspc approach with a
cnn backbone (unet) in an end-to-end diﬀerentiable manner.
first, the ini-
tial segmentation map and pixel-level bandwidth parameter map h(xi1, xi2) of
kspc are learned from data by the cnn backbone.
then the kspc module
obtains the quantile threshold value for each image by identifying the quantile
corresponding to the minimum suv of the tumour class in the initial segmen-
tation map.
the next step involves transmitting the bandwidth map, quantile
threshold, and raw image to kspc module to generate the segmentation map
and its corresponding probability contours.
additionally, the initial unet segmentation can produce another
loss function, called cnn loss, which serves as an auxiliary supervision for the
cnn backbone.
deep probability contour framework
539
2.3
loss function
the dice similarity coeﬃcient is widely employed to evaluate segmentation mod-
els.
we utilize the dice loss function to optimize the model performance during
training, which is deﬁned as:
ldice(y, ˆy) = 1 −
2 n
i yiˆyi
n
i yi + n
i ˆyi + ϵ
,
where yi is the label from experts and ˆyi is the predicted label of i-th pixel.
3
experiments
3.1
dataset
the dataset is from the hecktor challenge in miccai 2021 (head and neck
tumor segmentation challenge).
for each patient, fdg-
pet input images and corresponding labels in binary description (0 s and 1 s)
for the primary gross tumour volume are provided and co-registered to a size
of 144 × 144 × 144 using bounding box information encompassing the tumour.
five-fold cross-validation is used to generalize the performance of models.
3.2
implementation details
we used python and a trained network on a nvidia dual quadro rtx machine
with 64 gb ram using the pytorch package.
4
results
4.1
results on hecktor 2021 dataset
to evaluate the performance of our kspc-net, we compared it with the results
of 5-fold cross-validation against three widely-used models namely, the standard
2d unet, the 2d residual unet and the 3d unet.
additionally, we compare our
540
w. zhang and s. ray
performance against newly developed approaches msa-net
to quantify the
performance, we report several metrics including dice similarity scores, preci-
sion, recall, and hausdorﬀ distance.
it is worth mentioning that since
our kspc-net is in a 2d unet structure, the hausdorﬀ distance here was calcu-
lated on slice averages to use a uniform metric across all 2d and 3d segmentation
models.
mean segmentation results of diﬀerent models and our proposed model.
the
model with best performance for each metric is indicated in bold*.
method
dice score hausdorﬀ dist precision recall
2d-unet
0.740
0.561
0.797
0.873
res-unet
0.680
0.611
0.740
0.841
3d-unet
0.764
0.546
0.839*
0.797
msa-net
0.757
-
0.788
0.785
ccut-net
0.750
-
0.776
0.804
kspc-net(ours) 0.768*
0.521*
0.793
0.911*
the results clearly demonstrate that the proposed kspc-net is eﬀective in
segmenting h&n tumours, achieving a mean dice score of 0.768.
this repre-
sents a substantial improvement over alternative approaches, including 2d-unet
(0.740), 3d u-net (0.764), residual-unet (0.680), msa-net (0.757) and ccut-
net (0.750).
while we acknowledge that there was no statistically signiﬁcant
improvement compared to other sota models, it is important to note that our
main goal is to showcase the ability to obtain probability contours as a nat-
ural byproduct while preserving state-of-the-art accuracy levels.
on the other
hand, in comparison to the baseline 2d-unet model, kspc-net yields a higher
recall (0.911) with a signiﬁcant improvement (4.35%), indicating that kspc-
net generates fewer false negatives (fn).
in addition, the proposed kspc-net achieves the best
performance on hausdorﬀ distance among the three commonly used unet mod-
els (2d-unet, res-unet and 3d-unet), which indicates that kspc-net exhibits
a stronger capacity for accurately localizing the boundaries of objects.
this is
consistent with the mechanisms of kspc, which leverages neighbouring weights
to yield outputs with enhanced smoothness.
for exam-
ple, fig. 3 provides two examples of pet image segmentation maps by kspc-
net and their corresponding probability contours in the last column.
3. illustrations of the segmentation results and probability contours on two exam-
ples.
the four columns are original pet images, ground truth provided by experts,
segmentation maps from kspc-net and its probability contours (in 10%, 30%, 50%,
70%, 90% respectively).

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_1.pdf:
segmenting prostate from mri is crucial for diagnosis and
treatment planning of prostate cancer.
however, if the local center has limited image collection capability,
there may also not be enough unlabeled data for semi-supervised learn-
ing to be eﬀective.
to overcome this issue, other partner centers can be
consulted to help enrich the pool of unlabeled images, but this can result
in data heterogeneity, which could hinder ssl that functions under the
assumption of consistent data distribution.
tailoring for this important
yet under-explored scenario, this work presents a novel category-level
regularized unlabeled-to-labeled (cu2l) learning framework for semi-
supervised prostate segmentation with multi-site unlabeled mri data.
our method is evaluated
on prostate mri data from six diﬀerent clinical centers and shows supe-
rior performance compared to other semi-supervised methods.
keywords: prostate segmentation · semi-supervised · heterogeneity
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
https://doi.org/10.1007/978-3-031-43901-8_1
4
z. xu et al.
40
45
50
55
60
65
70
75
80
suponly
mt
ua-mt
ict
cpcl
cct
cps
ssnet
local labeled 
local unlabeled
external multi-site unlabeled support data 
…
,
,
,
typical semi-supervised learning (ssl)
effectively work under the assumption that the 
centralized local i.i.d. unlabeled data is abundant

limited performance gain or even fail when the local
unlabeled data is also limited (e.g., due to restricted 
image collection capabilities or a scarcity of patients)

multi-site semi-supervised learning (ms-ssl)
the unlabeled image pool can be quickly enriched
via the support from partner clinical centers with low 
barriers of entry (only unlabeled images are required)

data heterogeneity due to different scanners, 
scanning protocols and subject groups, which violate 
the typical ssl assumption of i.i.d. data
local: c1.
each center supplied t2-weighted mr images of the prostate.
center source
#scans field strength (t) resolution (in-plane/through-plane in mm) coil
scanner
c1
runmc [1]
30
3
0.6–0.625/3.6–4
surface
siemens
c2
bmc
[1]
30
1.5
0.4/3
endorectal philips
c3
hcrudb [4] 19
3
0.67–0.79/1.25
–
siemens
c4
ucl
[5]
13
1.5 and 3
0.325–0.625/3–3.6
–
siemens
c5
bidmc [5]
12
3
0.25/2.2–3
endorectal ge
c6
hk
[5]
12
1.5
0.625/3.6
endorectal siemens
1
introduction
prostate segmentation from magnetic resonance imaging (mri) is a crucial step
for diagnosis and treatment planning of prostate cancer.
recently, deep learning-
based approaches have greatly improved the accuracy and eﬃciency of automatic
prostate mri segmentation [7,8].
yet, their success usually requires a large
amount of labeled medical data, which is expensive and expertise-demanding
in practice.
in this regard, semi-supervised learning (ssl) has emerged as an
attractive option as it can leverage both limited labeled data and abundant
unlabeled data [3,9–11,15,16,21–26,28].
unfor-
tunately, such “abundance” may be unobtainable in practice, i.e., the local unla-
beled pool is also limited due to restricted image collection capabilities or scarce
patient samples.
taking c1 as a case study, if the amount of
local unlabeled data is limited, existing ssl methods may still suﬀer from inferior
performance when generalizing to unseen test data (fig. 1).
yet, due to diﬀerences in imaging protocols and variations in
patient demographics, this solution usually introduces data heterogeneity, lead-
category-level regularized unlabeled-to-labeled learning
5
ing to a quality problem.
such heterogeneity may impede the performance of
ssl which typically assumes that the distributions of labeled data and unla-
beled data are independent and identically distributed (i.i.d.)
here, we deﬁne this new ssl scenario as multi-site semi-supervised learn-
ing (ms-ssl), allowing to enrich the unlabeled pool with multi-site heteroge-
neous images.
thus, it intuitively utilizes image-level mapping to minimize
dual-distribution discrepancy.
yet, their adversarial min-max optimization often
leads to instability and it is diﬃcult to align multiple external sources with the
local source using a single image mapping network.
2,
to achieve robust ms-ssl for prostate mri segmentation.
, the local unlabeled data is involved into pseudo-
label supervised-like learning to reinforce ﬁtting of the local data distribution;
(ii) considering that intra-class variance hinders eﬀective ms-ssl, we introduce
a non-parametric unlabeled-to-labeled learning scheme, which takes advantage of
the scarce expert labels to explicitly constrain the prototype-propagated predic-
tions, to help the model exploit discriminative and domain-insensitive features
from heterogeneous multi-site data to support the local center.
yet, observing
that such scheme is challenging when signiﬁcant shifts and various distributions
are present, we further propose category-level regularization, which advocates
prototype alignment, to regularize the distribution of intra-class features from
arbitrary external data to be closer to the local distribution; (iii) based on the
fact that perturbations (e.g., gaussian noises [15]) can be regarded as a simu-
lation of heterogeneity, perturbed stability learning is incorporated to enhance
the robustness of the model.
our method is evaluated on prostate mri data
from six diﬀerent clinical centers and shows promising performance on tackling
ms-ssl compared to other semi-supervised methods.
ema: exponential moving average.
xl
local(i), xu
local(i) ∈ rh×w ×d denote the scans with height h, width w and
depth d, and y l
local(i) ∈ {0, 1}h×w ×d denotes the label of xl
local(i) (we focus
on binary segmentation).
considering the
large variance on slice thickness among diﬀerent centers [7,8], our experiments
are performed in 2d.
speciﬁcally, the student f s
θ is an in-training model optimized by loss
back-propagation as usual while the teacher model f t
˜θ is slowly updated with
a momentum term that averages previous weights with the current weights,
where θ denotes the student’s weights and ˜θ the teacher’s weights.
˜θ is updated
by ˜θt = α˜θt−1 + (1 − α)θt at iteration t, where α is the exponential moving
average (ema) coeﬃcient and empirically set to 0.99
thus, the teacher model is
suitable for handling the heterogeneous external images and producing relatively
stable pseudo labels (will be used later).
category-level regularized unlabeled-to-labeled learning
7
2.2
pseudo labeling for local distribution fitting
as mentioned above, supervised-like learning is advocated for local unlabeled
data to help the model ﬁt local distribution better.
the dice loss is calculated for each
of the k equally-sized regions of the image, and the ﬁnal loss is obtained by
taking their mean.
inherently, the challenge of ms-ssl stems
from intra-class variation, which results from diﬀerent imaging protocols, disease
progress and patient demographics. inspired by prototypical networks [13,19,25]
that compare class prototypes with pixel features to perform segmentation,
here, we introduce a non-parametric unlabeled-to-labeled (u2l) learning scheme
that utilizes expert labels to explicitly constrain the prototype-propagated pre-
dictions.
such design is based on two considerations: (i) a good prototype-
propagated prediction requires both compact feature and discriminative pro-
totypes, thus enhancing this prediction can encourage the model to learn in a
variation-insensitive manner and focus on the most informative clues; (ii) using
expert labels as ﬁnal guidance can prevent error propagation from pseudo labels.
speciﬁcally, we denote the feature map of the external unlabeled image xu
e before
the penultimate convolution in the teacher model as f u,t
e
.
with the argmax pseudo label ˆy u,t
e
and the predicted probability map
p u,t
e
, the object prototype from the external unlabeled data can be computed via
conﬁdence-weighted masked average pooling: cu(obj)
e
=

v

ˆy u,t,obj
e(v)
·p u,t,obj
e(v)
·f u,t
e(v)


v

ˆy u,t,obj
e(v)
·p u,t,obj
e(v)

.
2, given the feature map f l
local of the local labeled image xl
local
from the in-training student model, we can compare {cu(obj)
e
, cu(bg)
speciﬁcally, we introduce category-level regularization, which
advocates class prototype alignment between local and external data, to regu-
larize the distribution of intra-class features from arbitrary external data to be
closer to the local one, thus reducing the diﬃculty of u2l learning.
the weight
of background prototype alignment is smaller due to less relevant contexts.
speciﬁcally, for the same unlabeled input xu ∈ {du
local ∪ du
e }
with diﬀerent perturbations ξ and ξ′ (using the same gaussian noises as in [26]),
we encourage consistent pre-softmax predictions between the teacher and stu-
dent models, formulated as lu
sta = d

f t
˜θ(xu + ξ), f s
θ (xu + ξ′)

, where mean
squared error is also adopted as the distance function d(·, ·).
(4)
3
experiments and results
materials.
we utilize prostate t2-weighted mr images from six diﬀerent clini-
cal centers (c1–6)
cu2l (ours)
8
10
86
86.46 (6.72)
76.74 (9.97)
82.30 (9.93)
70.71 (12.94)
supervised (upper bound) 18
0
0
89.19 (4.33)
80.76 (6.71)
85.01 (4.35)
74.15 (6.44)
rizes the characteristics of the six data sources, following [7,8], where [7,8] also
reveal the severity of inter-center heterogeneity here through extensive experi-
ments.
compared to c1 and
c2, scans from c3 to c6 are taken from patients with prostate cancer, either for
detection or staging purposes, which can cause inherent semantic diﬀerences in
the prostate region to further aggravate heterogeneity.
we take c1 or c2 as the local target center and randomly divide their 30 scans
into 18, 3, and 9 samples as training, validation, and test sets, respectively.
implementation and evaluation metrics.
the framework is implemented
on pytorch using an nvidia geforce rtx 3090 gpu.
cu2l-1
cu2l-2
cu2l-3
cu2l-4
cu2l
c1
80.24
82.75
83.31
84.77
85.93
c2
77.24
77.52
79.43
80.01
80.29
71
73
75
77
79
81
83
85
87
dsc (%)
c1
c2
image
cu2l (ours)
mt
ssnet
ahdc
local: c1
local: c2
93.61%
86.41%
89.95%
88.81%
82.76%
61.02%
59.12%
58.99%
(a)
(b)
fig.
data augmentation is applied, including random ﬂip and rotation.
we adopt the
dice similarity coeﬃcient (dsc) and jaccard as the evaluation metrics and the
results are the average over three runs with diﬀerent seeds.
all methods are imple-
mented with the same backbone and training protocols to ensure fairness.
as
observed, compared to the supervised-only baselines, our cu2l with {6, 8} local
labeled scans achieves {19.15%, 17.42%} and {9.1%, 6.44%} dsc improvements
in {c1, c2}, showing its eﬀectiveness in leveraging multi-site unlabeled data.
however, due to the lack of proper
mechanisms for learning from heterogeneous data, limited improvement can be
achieved by them, especially for cps [3] and fixmatch
[2] is mediocre in ms-ssl, mainly due to the instabil-
ity of adversarial training and the diﬃculty of aligning multiple distributions
to the local distribution via a single image-mapping network.
in contrast, with
specialized mechanisms for simultaneously learning informative representations
category-level regularized unlabeled-to-labeled learning
11
from multi-site data and handling heterogeneity, our cu2l obtains the best
performance over the recent ssl methods.
firstly, when we remove lu
p l (cu2l-1), the performance drops by
{5.69% (c1), 3.05%(c2)} in dsc, showing that reinforcing conﬁrmation on local
distribution is critical.
if we
remove lcr which accompanies with lu2l (cu2l-3), the performance degrades,
which justiﬁes the necessity of this regularization to reduce the diﬃculty of
unlabeled-to-labeled learning process.
as
observed, such a typical stability loss [15] can further improve the performance
by introducing hand-crafted noises to enhance the robustness to real-world het-
erogeneity.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_46.pdf:
transformer and its variants have been widely used for
medical image segmentation.
we incorporate a group multi-axis hadamard product attention module
(ghpa) and a group aggregation bridge module (gab) in a lightweight
manner.
the gab eﬀectively fuses multi-
scale information by grouping low-level features, high-level features, and
a mask generated by the decoder at each stage.
comprehensive experi-
ments on the isic2017 and isic2018 datasets demonstrate that ege-
unet outperforms existing state-of-the-art methods.
in short, compared
to the transfuse, our model achieves superior segmentation performance
while reducing parameter and computation costs by 494x and 160x,
respectively.
keywords: medical image segmentation · light-weight model ·
mobile health
1
introduction
malignant melanoma is one of the most rapidly growing cancers in the world.
thus, an automated skin lesion
segmentation system is imperative, as it can assist medical professionals in
swiftly identifying lesion areas and facilitating subsequent treatment processes.
to enhance the segmentation performance, recent studies tend to employ mod-
ules with larger parameter and computational complexity, such as incorporat-
ing self-attention mechanisms of vision transformer (vit)
[4], based on the swin transformer [11], leverages the feature extrac-
tion ability of self-attention mechanisms to improve segmentation performance.
1. (a) and (b) respectively show the visualization of comparative experimental
results on the isic2017 and isic2018 datasets.
[5] has pioneered a serial fusion of cnn and vit for medical image
segmentation.
[8] utilizes
a hybrid hierarchical architecture, eﬃcient bidirectional attention, and semantic
maps to achieve global multi-scale feature fusion, combining the strengths of
cnn and vit.
transbts [23] introduces self-attention into brain tumor seg-
mentation tasks and uses it to aggregate high-level information.
prior works have enhanced performance by introducing intricate modules,
but neglected the constraint of computational resources in real medical settings.
hence, there is an urgent need to design a low-parameter and low-computational
load model for segmentation tasks in mobile healthcare.
[21] to develop a lightweight model that
attains superior performance, while diminishing parameter and computation.
furthermore, malunet [19] has reduced the model size by declining the num-
ber of model channels and introducing multiple attention modules, resulting in
better performance for skin lesion segmentation than unext.
however, while
malunet greatly reduces the number of parameter and computation, its seg-
mentation performance is still lower than some large models, such as trans-
fuse.
therefore, in this study, we propose ege-unet, a lightweight skin lesion
segmentation model that achieves state-of-the-art while signiﬁcantly reducing
parameter and computation costs.
to be speciﬁc, ege-unet leverages two key modules: the group multi-axis
hadamard product attention module (ghpa) and group aggregation bridge
ege-unet: an eﬃcient group enhanced unet
483
module (gab).
mhsa
divides the input into multiple heads and calculates self-attention in each head,
which allows the model to obtain information from diverse perspectives, integrate
diﬀerent knowledge, and improve performance.
on the other hand, for gab, since the size
and shape of segmentation targets in medical images are inconsistent, it is essen-
tial to obtain multi-scale information [19].
via combining the
above two modules with unet, we propose ege-unet, which achieves excel-
lent segmentation performance with extremely low parameter and computation.
unlike previous approaches that focus solely on improving performance, our
model also prioritizes usability in real-world environments.
(2) we propose ege-
unet, an extremely lightweight model designed for skin lesion segmentation.
(3) we conduct extensive experiments, which demonstrate the eﬀectiveness of
our methods in achieving state-of-the-art performance with signiﬁcantly lower
resource requirements.
the encoder is composed of six stages,
each with channel numbers of {8, 16, 24, 32, 48, 64}.
while the ﬁrst three
stages employ plain convolutions with a kernel size of 3, the last three stages
utilize the proposed ghpa to extract representation information from diverse
perspectives.
in contrast to the simple skip connections in unet, ege-unet
incorporates gab for each stage between the encoder and decoder.
furthermore,
our model leverages deep supervision
via the integration of these advanced modules, ege-unet signiﬁcantly
reduces the parameter and computational load while enhancing the segmentation
performance compared to prior approaches.
we divide the input into four groups equally along the channel dimension
and perform hpa on the height-width, channel-height, and channel-width axes
for the ﬁrst three groups, respectively.
finally, we concatenate the four groups along the channel
dimension and apply another dw to integrate the information from diﬀerent
perspectives.
the acquisition of multi-scale informa-
tion is deemed pivotal for dense prediction tasks, such as medical image segmen-
tation.
secondly, we
partition both feature maps into four groups along the channel dimension, and
concatenate one group from the low-level features with one from the high-level
features to obtain four groups of fused features.
finally, the four groups are con-
catenated along the channel dimension, followed by the application of a plain
convolution with the kernel size of 1 to enable interaction among features at
diﬀerent scales.
[27] is employed to calculate the loss function
for diﬀerent stages, in order to generate more accurate mask information.
λi is the weight
for diﬀerent stage.
3
experiments
datasets and implementation details.
to assess the eﬃcacy of our model,
we select two public skin lesion segmentation datasets, namely isic2017
[2,6], containing 2150 and 2694 dermoscopy images, respectively.
all experiments are per-
formed on a single nvidia rtx a6000 gpu.
the images are normalized and
resized to 256 × 256.
we apply various data augmentation, including horizontal
ﬂipping, vertical ﬂipping, and random rotation.
the comparative experimental results presented in
table 1 reveal that our ege-unet exhibits a comprehensive state-of-the-art per-
formance on the isic2017 dataset.
speciﬁcally, in contrast to larger models,
such as transfuse, our model not only demonstrates superior performance, but
also signiﬁcantly curtails the number of parameter and computation by 494x
and 160x, respectively.
in comparison to other lightweight models, ege-unet
surpasses unext-s with a miou improvement of 1.55% and a dsc improvement
of 0.97%, while exhibiting parameter and computation reductions of 17% and
72% of unext-s. furthermore, ege-unet outperforms malunet with a miou
improvement of 1.03% and a dsc improvement of 0.64%, while reducing param-
eter and computation to 30% and 85% of malunet.
for the isic2018 dataset,
the performance of our model also outperforms that of the best-performing
model.
comparative experimental results on the isic2017 and isic2018 dataset.
(c) the micro ablation on gab.
type model
params(m)↓ gflops↓ miou(%)↑ dsc(%)↑
(a)
baseline
0.107
0.076
76.30
86.56
baseline + ghpa
0.034
0.058
78.82
88.16
baseline + gab
0.126
0.086
78.78
88.13
(b)
w/o multi-axis grouping
0.074
0.074
79.13
88.35
w/o dw for initialized tensor 0.050
0.072
79.03
88.29
(c)
w/o mask information
0.052
0.070
78.97
88.25
w/o dilation rate of conv2d
0.053
0.072
79.11
88.34
reducing parameter to about 50kb with excellent segmentation performance.
figure 1 presents a more clear visualization of the experimental ﬁndings and
fig.
4 shows some segmentation results.
we conduct extensive ablation experiments to demonstrate
the eﬀectiveness of our proposed modules.
the baseline utilized in our work is
referenced from malunet [19], which employs a six-stage u-shaped architecture
488
j. ruan et al.
fig.
each stage includes a plain
convolution operation with a kernel size of 3, and the number of channels at
each stage is set to {8, 16, 24, 32, 48, 64}.
secondly, we substitute the
skip-connection operation in baseline with gab, resulting in further improved
performance.
furthermore, we substitute the dilated convolutions in gab with plain
convolutions, which also leads to a reduction in performance.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_53.pdf:
modern deep learning methods for semantic segmentation
require labor-intensive labeling for large-scale datasets with dense pixel-
level annotations.
recent data augmentation methods such as dropping,
mixing image patches, and adding random noises suggest eﬀective ways to
address the labeling issues for natural images.
however, they can only be
restrictively applied to medical image segmentation as they carry risks of
distorting or ignoring the underlying clinical information of local regions
of interest in an image.
in this paper, we propose a novel data augmen-
tation method for medical image segmentation without losing the seman-
tics of the key objects (e.g., polyps).
our
proposed method signiﬁcantly outperforms various existing methods with
high sensitivity and dice scores and extensive experiment results with mul-
tiple backbones on two datasets validate its eﬀectiveness.
keywords: adversarial attack and defense · data augmentation ·
semantic segmentation
1
introduction
semantic segmentation aims to segment objects in an image by classifying each
pixel into an object class.
training a deep neural network (dnn) for such a task
is known to be data-hungry, as labeling dense pixel-level annotations requires
laborious and expensive human eﬀorts in practice [23,32].
furthermore, semantic
segmentation in medical imaging suﬀers from privacy and data sharing issues [13,
35] and a lack of experts to secure accurate and clinically meaningful regions of
interest (rois).
this data shortage problem causes overﬁtting for training dnns,
resulting in the networks being biased by outliers and ignorant of unseen data.
to alleviate the sample size and overﬁtting issues, diverse data augmentations
have been recently developed.
for example, cutmix [31] and cutout [4] aug-
ment images by dropping random-sized image patches or replacing the removed
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. (eds.): miccai 2023, lncs 14223, pp. 555–566, 2023.
https://doi.org/10.1007/978-3-031-43901-8_53
556
h. cho et al.
regions with a patch from another image.
geometric transforma-
tions such as elastic transformation [26] warp images and deform the original
shape of objects.
alternatively, feature perturbation methods augment data by
perturbing data in feature space [7,22] and logit space [9].
although these augmentation approaches have been successful for natural
images, their usage for medical image semantic segmentation is quite restricted
as objects in medical images contain non-rigid morphological characteristics that
should be sensitively preserved.
for example, basalioma (e.g., pigmented basal
cell carcinoma) may look similar to malignant melanoma or mole in terms of color
and texture [6,20], and early-stage colon polyps are mostly small and indistin-
guishable from background entrail surfaces
in these cases, the underlying
clinical features of target rois (e.g., polyp, tumor and cancer) can be distorted if
regional colors and textures are modiﬁed with blur-based augmentations or geo-
metric transformations.
considering the rois are usually small and under-
represented compared to the backgrounds, the loss of information may cause a
fatal class imbalance problem in semantic segmentation tasks.
in these regards, we tackle these issues with a novel augmentation method
without distorting the semantics of objects in image space.
we ﬁrst augment hard samples with adversarial attacks [18]
that deceive a network and defend against such attacks with anti-adversaries.
from active learning
perspective [12,19], as vague samples near the decision boundary are augmented
and trained, improvement on a downstream prediction task is highly expected.
we summarize our main contributions as follows: 1) we propose a novel
online data augmentation method for semantic segmentation by imposing object-
speciﬁc consistency regularization between anti-adversarial and adversarial data.
2) our method provides a ﬂexible regularization between diﬀerently perturbed
data such that a vulnerable network is eﬀectively trained on challenging sam-
ples considering their ambiguities.
3) our method preserves underlying mor-
phological characteristics of medical images by augmenting data with quasi-
imperceptible perturbation.
as a result, our method signiﬁcantly improves sen-
sitivity and dice scores over existing augmentation methods on kvasir-seg [11]
and etis-larib polyp db [25] benchmarks for medical image segmentation.
(color ﬁgure online)
2
preliminary: adversarial attack and anti-adversary
adversarial
attack
is
an
input
perturbation
method
that
adds
quasi-
imperceptible noises into images to deceive a dnn.
given an image x, let μ
be a noise bounded by l∞-norm.
while the diﬀerence between x and the per-
turbed sample x′ = x + μ is hardly noticeable to human perception, a network
fθ(·) can be easily fooled (i.e., fθ(x)
3
method
let {xi}n
i=1 be an image set with n samples each paired with correspond-
ing ground truth pixel-level annotations yi.
our proposed method aims to 1)
generate realistic images with adversarial attacks and 2) train a segmentation
model fθ(xi)
= yi for robust semantic segmentation with anti-adversarial con-
sistency regularization (aac).
figure 2 shows the overall training scheme with
three phases: 1) online data augmentation, 2) computing adaptive aac between
558
h. cho et al.
fig.
adversarial and anti-adversarial perturba-
tions are iteratively performed for the objects of a given image xi. adversarial noise
µ−
i,k moves xi across the decision boundary, whereas anti-adversarial noise µ+
i,k pushes
xi away from the boundary.
downstream consistency regularization loss rcon mini-
mizes the gap between adversaries {x−
i,k}k
k=1 and anti-adversary x+
i,k.
diﬀerently perturbed samples, and 3) updating the segmentation model using
the loss from the augmented and original data.
first, we generate plausible
images with iterative adversarial and anti-adversarial perturbations.
data augmentation with object-targeted adversarial attack.
in many
medical applications, false negatives (i.e., failing to diagnose a critical disease)
are much more fatal than false positives.
to do so, we ﬁrst exclude the
background and perturb only the objects in the given image.
given ˆxi,k as a perturbed sample at k-th step (k = 1, ..., k), the adversarial and
anti-adversarial perturbations use the same initial image as x−
i,0 = ˆ
xi and x+
(2)
in contrast to the adversarial attack in eq. 1, the anti-adversarial noise μ+
i,k =
argminμl(fθ(x+
i,k+1), yi) manipulates samples to increase the classiﬁcation
score.
note that, generating noises and images are online and training-free as the
loss derivatives are calculated with freezed network parameters.
speciﬁcally, if
x′
i is a harder sample to predict than x+
i,k, i.e., l(p ′
i, yi)>l(p +
i,k, yi), the weight
gets larger, and thus consistency regularization is intensiﬁed between the images.
training a segmentation network.
let ˆy +
i,k be a segmentation outcome, i.e.,
one-hot encoded pseudo-label from the network output p +
i,k of anti-adversary
x+
i,k. given xi and {x−
i,k}k
k=1 as training data, the supervised segmentation
loss lsup and the consistency regularization rcon are deﬁned as
lsup = 1
n
n

i=1
l(pi, yi) +
1
nk
n

i=1
k

k=1
l(p −
i,k, yi)
and
(4)
rcon = 1
n
n

i=1
w(xi, x+
i,k)l(pi, ˆy +
i,k) +
1
nk
n

i=1
k

k=1
w(x−
i,k, x+
i,k)l(p −
i,k, ˆy +
i,k).
with a hyperparame-
ter α, the whole training loss l = lsup+αrcon is minimized via backpropagation
to optimize the network parameters for semantic segmentation.
4
experiments
4.1
experimental setup
dataset.
we conducted experiments on two representative public polyp segmen-
tation datasets: kvasir-seg [11] and etis-larib polyp db [25] (etis).
the images of kvasir-seg were
resized to 512 × 608 (h × w) and that of etis was set with 966 × 1255 resolu-
tion.
implementation.
we implemented our method on pytorch framework with 4
nvidia rtx a6000 gpus.
along with conventional augmentation methods (i.e., random hori-
zontal and vertical ﬂipping denoted as ‘basic’ in table 1), recent methods such
as cutmix [31], cutout [4], elastic transform [26], random erase [33], drop-
block [7], gaussian noise training (gnt)
the basic augmentation was used in
all methods including ours by default.
for the training, we used k augmented
images with the given images for all baselines as in ours for a fair comparison.
evaluation.
as the evaluation metric, mean intersection
over union (miou) and mean dice coeﬃcient (mdice) are used for all experi-
ments on test sets.
additionally, we provide recall and precision scores to oﬀer
a detailed analysis of class-speciﬁc misclassiﬁcation performance.
note
that, all baselines showed improvements in most cases.
performance comparison with existing data augmentation methods.
3. comparisons of precision and recall on the test set of kvasir-seg with u-net.
performed better even compared with the tumorcp which uses seven diﬀer-
ent augmentations methods together for tumor segmentation.
this is because
our method preserves the semantics of the key rois with small but eﬀective
noises unlike geometric transformations [26,30], drop and cut-and-paste-based
methods [4,7,30,31,33].
also, as we augment uncertain samples that deliber-
ately deceive a network as in active learning [12,16], our method is able to
sensitively include the challenging (but roi-relevant) features into prediction,
unlike existing noise-based methods that extract noises from known distributions
[9,22,30].
562
h. cho et al.
fig.
(color ﬁgure
online)
4.3
analysis on anti-adversaries and adversaries
in fig. 4, we visualize data augmentation results with (anti-) adversarial pertur-
bations on kvasir-seg dataset.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_20.pdf:
nuclei appear small in size, yet, in real clinical practice, the
global spatial information and correlation of the color or brightness con-
trast between nuclei and background, have been considered a crucial com-
ponent for accurate nuclei segmentation.
however, the ﬁeld of automatic
nuclei segmentation is dominated by convolutional neural networks
(cnns), meanwhile, the potential of the recently prevalent transformers
has not been fully explored, which is powerful in capturing local-global
correlations.
to this end, we make the ﬁrst attempt at a pure trans-
former framework for nuclei segmentation, called transnuseg.
diﬀerent
from prior work, we decouple the challenging nuclei segmentation task
into an intrinsic multi-task learning task, where a tri-decoder structure
is employed for nuclei instance, nuclei edge, and clustered edge segmen-
tation respectively.
experi-
ments on two datasets of diﬀerent modalities, including monuseg have
shown that our methods can outperform state-of-the-art counterparts
such as ca2.5-net by 2–3% dice with 30% fewer parameters.
in con-
clusion, transnuseg conﬁrms the strength of transformer in the context
of nuclei segmentation, which thus can serve as an eﬃcient solution for
real clinical practice.
keywords: lightweight multi-task framework · shared attention
heads · nuclei · edge and clustered edge segmentation
1
introduction
accurate cancer diagnosis, grading, and treatment decisions from medical images
heavily rely on the analysis of underlying complex nuclei structures [7].
1. semantic illustrations of the nuclei segmentation networks with diﬀerent num-
bers of decoders.
(a) sole-decoder to perform a single task of nuclei segmentation.
(b)
bi-decoder to segment nuclei and locate nuclei edges simultaneously.
to the numerous nuclei contained in a digitized whole-slide image (wsi), or even
in an image patch of deep learning input, dense annotation of nuclei contour-
ing is extremely time-consuming and labor-expensive [11].
consequently, auto-
mated nuclei segmentation approaches have emerged to satisfy a broad range
of computer-aided diagnostic systems, where the deep learning methods, partic-
ularly the convolutional neural networks
[3] with bi-decoder structure achieves improved instance segmentation
performance by adopting multi-task learning, in which one decoder learns to
segment the nuclei and the other recognizes edges as described in fig.
[20] extends dcan with an extra information aggregator to fuse
the features from two decoders for more precise segmentation.
much recently,
ca2.5-net [6] shows identifying the clustered edges in a multiple-task learning
manner can achieve higher performance, and thereby proposes an extra out-
put path to learn the segmentation of clustered edges explicitly.
a signiﬁcant
drawback of the aforementioned multi-decoder networks is the ignorance of the
prediction consistency between branches, resulting in sub-optimal performance
and missing correlations between the learned branches.
speciﬁcally, a prediction
mismatch between the nuclei and edge branches is observed in previous work [8],
implying a direction for performance improvement.
he et al.
segmentation tasks.
inspired by the capability in long-range global context cap-
turing by transformers [17], we make the ﬁrst attempt to construct a tri-decoder
based transformer model to segment nuclei.
in short, our major contributions
are three-fold: (1) we propose a novel multi-task framework for nuclei segmenta-
tion, namely transnuseg, as the ﬁrst attempt at a fully swin-transformer driven
architecture for nuclei segmentation.
furthermore, the incorporation of a light-weighted mlp bottleneck
leads to a sharp reduction of parameters at no cost of performance decline.
fig.
[13] as
the building blocks to capture the long-range feature correlations in the nuclei
segmentation context.
our network consists of three individual output decoder
paths for nuclei segmentation, normal edges segmentation, and clustered edges
segmentation.
to capture the strong correlation between nuclei
segmentation and contour segmentation between multiple decoders [15], we intro-
duce a novel attention sharing scheme that is designed as an enhancement to
the multi-headed self-attention (msa) module in the plain transformer [17].
[·] writes for the concatenation, sa(·) denotes the self-attention head whose
output dimension is dh, and uu
msa ∈ r(m+n)·dh×d is a learnable matrix.
to reduce the complexity of the model, we leverage a
token mlp bottleneck as a light-weight alternative for the swin transformer bot-
tleneck.
to alleviate the inconsistency between the con-
tour generated from the nuclei segmentation prediction and the predicted edge,
210
z. he et al.
we propose a novel consistency self distillation loss, denoted as lsd.
we employ a multi-task learning paradigm
to train the tri-decoder network, aiming to improve model performance by
leveraging the additional supervision signal from edges.
particularly, the nuclei
semantic segmentation is considered the primary task, while the normal edge
and clustered edge semantic segmentation are viewed as auxiliary tasks.
subsequently, the overall loss l is calculated as a
weighted summation of semantic nuclei mask loss (ln), normal edge loss (le),
and clustered edge loss (lc), and the self distillation loss (lsd) i. e.
l = γn · ln + γe · le + γc · lc + γsd · lsd, where coeﬃcients γn, γe and γc are
set to 0.30, 0.35, 0.35 respectively, and γsd is initially set to 1 with a 0.3 decrease
for every 10 epochs until it reaches 0.4.
3
experiments
dataset.
(1) fluores-
cence microscopy image dataset: this set combines three diﬀerent data sources
to simulate the heterogeneous nature of medical images [9].
it consists of 524
ﬂuorescence images, each with a resolution of 512 × 512 pixels.
(2) histology
image dataset: this set is the combination of the open dataset monuseg [10]
and another private histology dataset
[8] of 462 images.
we crop each image
in the monuseg dataset into four partially overlapping 512 × 512 images.
fig.
the best performance with
respect to each metric is highlighted in boldface.
the private dataset contains 300 images sized at 512 × 512 tessellated from 50
wsis scanned at 20×, and meticulously labeled by ﬁve pathologists according
to the labeling guidelines of the monuseg [10].
table 2. comparison of the model complexity in terms of the number of parameters,
flops, as well as the training cost in the form of the averaged training time per epoch.
the average training time is computed using the same batch size for both datasets, with
the ﬁrst number indicating the averaged time on the fluorescence microscopy image
dataset and the second on the histology image dataset.
4. exemplary samples and their segmentation results using diﬀerent methods.
transnuseg demonstrates superior segmentation performance compared to its counter-
parts, which can successfully distinguish severely clustered nuclei from normal edges.
implementations.
all experiments are performed on one nvidia rtx 3090
gpu with 24 gb memory.
table 1 shows the quantitative comparisons for the nuclei segmen-
tation.
the large margin between the swinunet and the other cnn-based or
hybrid networks also conﬁrms the superiority of the transformer in ﬁne-grained
nuclei segmentation.
for example, in the histology image
dataset, transnuseg improves the dice score, f1 score, accuracy, and iou by
2.08%, 3.41%, 1.25%, and 2.70% respectively, over the second-best models.
simi-
larly, in the ﬂuorescence microscopy image dataset, our proposed model improves
dsc by 0.96%, while also leading to 1.65%, 1.03% and 1.91% increment in
f1 score, accuracy, and iou to the second-best performance.
for better visu-
alization, representative samples and their segmentation results using diﬀerent
methods are demonstrated in fig.
furthermore, table 2 compares the model
complexity in terms of the number of parameters, ﬂoating point operations per
second (flops), and the training computational cost, where our approach can
signiﬁcantly reduce around 28% of the training time compared to the state-of-
the-art cnn multi-task method ca2.5-net, while also boosting performance.
our ablation study yields that token mlp bottleneck and attention
sharing schemes can complementarily reduce the training cost while increasing
eﬃciency, as shown in table 2 (the last 4 rows).
as described in table 3, each component
proportionally contributes to the improvement to reach the overall performance
fig.
(a) raw input image.
segmentation results by transnuseg trained (b) w/o
self distillation, and (c) w/ self distillation.
accordingly, the numbers below images indicate the proportion of the
pixels belonging to the three parts.
compared to the results without self distillation,
the outputs with self distillation exhibit reduced mismatches, resulting in improved
segmentation performance.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_36.pdf:
one task in diagnosing
lyme disease is lesion segmentation, i.e., separating benign skin from
lesions, which can not only help clinicians to focus on lesions but also
improve downstream tasks such as disease classiﬁcation.
however, it is
challenging to segment lyme disease lesions due to the lack of well-
segmented, labeled lyme datasets and the nature of lyme, e.g., the
typical bull’s eye lesion and its closeness to normal skin.
in this paper,
we design a simple yet novel data preprocessing and alteration method,
called edgemixup, to help segment lyme lesions on imbalanced training
datasets.
the key insight is to deploy a linear combination of lesion edge,
either detected or computed, and the source image highlights the aﬀected
lesion area so that a learning model focuses more on the preserved lesion
structure instead of skin tone, thus iteratively improving segmentation
performance.
additionally, the improved edge from lesion segmentation
can be further used for lyme disease classiﬁcation—e.g., in diﬀerentiat-
ing lyme from other similar lesions including tinea corporis and herpes
zoster—with improved model fairness on diﬀerent subpopulations.
1
introduction
medical image analysis has greatly beneﬁted from advances in ai
[1] yet some
improvements still remain to be addressed, importantly in areas that allow both
algorithmic performance and fairness [2], and in certain medical applications that
promise to signiﬁcantly lessen morbidity and mortality.
early detection of skin
lesions is such an endeavor as it can aid in identifying infectious diseases with
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43901-8_36.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
https://doi.org/10.1007/978-3-031-43901-8_36
edgemixup
375
cutaneous manifestations.
the earliest
and most treatable phase of lyme disease is manifested via a red concentric
lesion at the site of a tick bite, called erythema migrans (em) [5].
[7–9].
one important diagnosis task is to segment lyme lesion, particularly the
em pattern, from benign skins.
such dl-assisted segmentation not only helps
clinicians in pre-screening patients but also improves downstream tasks such as
lesion classiﬁcation.
however, while lyme disease lesion segmentation is intu-
itively simple, it is challenging due to the following reasons.
first, there lacks
of a well-segmented dataset with manual labels on lyme disease.
on one hand,
some datasets—such as ham10000 [10] and isbi challenges [11]—have manual
annotated segmentations for diseases like melanoma, but they do not have lyme
disease lesions.
[12]—have
lyme disease and skin tone and classiﬁcation labels, but not segmentation.
second, the segmentation of lyme lesion is itself challenging due to the nature
of em pattern.
therefore, existing skin disease
segmentation
[13] as well as existing general segmentation works, such as u-
net [14], polar training
[17], usually suﬀer
from relatively low performance and reduced fairness [2,18,19].
in this paper, we present the ﬁrst lyme disease dataset that contains labeled
segmentation and skin tones.
our lyme disease dataset contains two parts: (i)
a classiﬁcation dataset, composed of more than 3,000 diseased skin images that
are either obtained from public resources or clinicians with patient-informed con-
sent, and (ii) a segmentation dataset containing 185 samples that are manually
annotated for three regions—i.e., background, skin (light vs. dark), and lesion—
conducted under clinician supervision and institutional review boards (irb)
approval.
our dataset with manual labels is available at this url
secondly, we design a simple yet novel data preprocessing and alternation
method, called edgemixup, to improve lyme disease segmentation and diagno-
sis fairness on samples with diﬀerent skin-tones.
the key insight is to alter a skin
image with a linear combination of the source image and a detected lesion bound-
ary so that the lesion structure is preserved while minimizing skin tone informa-
tion.
such an improvement is an iterative process that gradually improves lesion
edge detection and segmentation fairness until convergence.
a motivating example to illustrate why edgemixup improves model perfor-
mance and reduces biases via mixing up lesion boundary with original image (heatmap
is generated via grad-cam).
(color ﬁgure online)
converged edge in the ﬁrst step also helps classiﬁcation of lyme diseases via mixup
with improved fairness.
we evaluate edgemixup for skin disease segmentation and classiﬁcation
tasks.
our results show that edgemixup is able to increase segmentation utility
and improve fairness.
we also show that the improved segmentation further
improves classiﬁcation fairness as well as joint fairness-utility metrics compared
to existing debiasing methods, e.g., ad
[21] and st-debias
note that not all skin disease datasets are
carefully processed either due to the large amount of work required or the scarcity
of data samples collected, e.g., sd-198 [23] contains samples that are taken under
variant environments.
we
keep all hyper-parameters exactly the same for two models, and only augment the
same image with and without mixing lesion boundary up with the original image.
figure 1 shows the original image (fig. 1a) as well as two models’
attention as heat-maps where red color represents the highest attention, yellow a
higher attention, and purple the least attention.
the reason is that a legacy
diagnosis has no information about lesion and does not know where to locate its
focus, thus easily gets distracted by ﬁngers instead of the lesion pattern.
3
method
in this section, we ﬁrst give the deﬁnition for model fairness, and we then
describe the design of edgemixup for the purpose of de-biasing in fig.
we consider any model f, either a classiﬁcation model fclass or
a segmentation model fseg, to be biased against certain skin-tone st2 if given
metrics m and samples xst1 and xst2 from class y, where st1 and st2 are diﬀer-
ent skin-tones according to their ita scores, m(f(xst1), y) > m(f(xst2), y).
edgemixup improves model fairness on light and dark skin samples in both
segmentation and classiﬁcation tasks, and it has two major components: (i) edge
detection using mixup, and (ii) data preprocessing and alteration for downstream
tasks.
more speciﬁcally, our proposed edge detection has two parts: initial edge
detection and iterative improvement.
initial edge detection: the purpose of initial detection, which is documented
in the initial_edge_detection function of algorithm 1, is to provide a start-
ing point, i.e., a rough boundary, for the next step of iterative improvement.
first, edgemixup trains
a classiﬁcation model based on a mixup of the ground-truth segmentation under
clinician supervision and the original image (line 7).
second, edgemixup gen-
erates many edge candidates.
note that the initial
edge detection is irrelevant to the sample size of a particular subpopulation, thus
improving the fairness.
iterative edge improvement: edgemixup includes iterative edge improve-
ment in the training phase of our segmentation model to further improve model
utility.
pseudo-code of edgemixup
require: a labelled sample (x, y) ∈ d, mixup weights α, ground-truth edged training set dtrain
edge_gt
ensure: dataset dfinal_edge in which each sample has it lesion edge highlighted (xedge, y)
1: function main( )
2:
dinitial_edge = initial_edge_detection(d, α)
3:
dfinal_edge = iterative_edge_improvement(dinitial_edge, α)
4:
return dfinal_edge
5: end function
6: function initial_edge_detection(d, α)
7:
train classiﬁcation model mclass using dtrain
edge_gt
8:
for each sample x ∈ d do
9:
get all edge candidates {edge1, edge2, .., edgen} for each sample x
10:
mixup each edge candidate with x
11:
query mclass using all mixed-up {xedge1, ...xedgen} and choose the optimal edge edgeopt
12:
generate edged sample xedge
= mixup(x, edgeopt, α)
13:
end for
14:
return dedge
15: end function
16: function iterative_edge_improvement(dedge, α)
17:
train the ﬁrst model miter using edged dataset dtrain
edge
18:
evaluate miter using dtest
edge and get current_jaccard
19:
best_jaccard
3. illustration of iterative edge improvement on diﬀerent iterations with train loss
mixup of detected edge and original image, given the lesion boundary feature
detected in the previous iteration, the next-iteration segmentation model can
converge better and the lesion boundary predicted by it is ﬁne-grained.
speciﬁ-
cally, edgemixup iteratively trains segmentation models from scratch, and we
let the model trained in the previous iteration to predict lesion edge, which is
then mixed-up with original training samples as the new training set for next-
iteration model.
annotated segmentation and classiﬁcation dataset characteristics, broken
down by ita-based skin tones (light skin/ dark skin) and disease types.
besides, edgemixup calculates a linear combination of original image and lesion
boundary, i.e., by assigning the weight of original image as α and lesion bound-
ary as 1 − α. figure 3 shows the edge-mixed-up images for diﬀerent iterations.
first,
we collect and annotate a dataset with 3,027 images containing three types of
disease/lesions, i.e., tinea corporis (tc), herpes zoster (hz), and erythema
migrans (em).
all skin images are either collected from publicly available sources
or from clinicians with patient informed consent.
then, a medical technician
and a clinician in our team manually annotate each image.
for the segmentation
task, we annotate skin images into three classes: background, skin, and lesion;
then, for the classiﬁcation task, we annotate skin images by classifying them
into four classes: no disease (no), tc, hz, and em.
[23], a benchmark
dataset for skin disease classiﬁcation, as another dataset for both segmentation
and classiﬁcation tasks.
note that due to the amount of manual work involved
in annotation, we select those classes based on the number of samples in each
class.
we
choose 30 samples in each class for segmentation task, and we split them into
0.7, 0.1, and 0.2 ratio for training, validation, and testing, respectively.
table 1 show the characteristics of these two datasets for both classiﬁcation
and segmentation tasks broken down by the disease type and skin tone, as cal-
culated by the individual typology angle (ita)
one prominent observation is
that ls images are more abundant than ds images due to a disparity in the avail-
ability of ds imagery found from either public sources or from clinicians with
patient consent.
segmentation: performance and fairness (margin of error reported in paren-
thesis)
method
unet
polar
mfsnet
vit-adapter
edgemixup
skin
jaccard 0.7053(0.0035) 0.7126(0.0033) 0.5877(0.0080) 0.7027(0.0057) 0.7807(0.0031)
jgap
0.0809(0.0001) 0.0813(0.0001) 0.1291(0.0076) 0.2346(0.0035) 0.0379(0.0001)
sd-seg jaccard 0.7134(0.0031) 0.6527(0.0036) 0.6170(0.0052) 0.5088(0.0042) 0.7799(0.0031)
jgap
0.0753(0.0001) 0.1210(0.0003) 0.0636(0.0033) 0.2530(0.0021) 0.0528(0.0001)
5
evaluation
we implement edgemixup using python 3.8 and pytorch, and all experiments
are performed using one geforce rtx 3090 graphics card (nvidia).
segmentation evaluation.
our segmentation evaluation adopts four base-
lines, (i) a u-net trained to segment skin lesions, (ii) a polar training [15]
transforming images from cartesian coordinates to polar coordinates, (iii) vit-
adapter
[16], a state-of-the-art semantic segmentation using a ﬁne-tuned vit
model, (iv) mfsnet [17], a segmentation model with diﬀerently scaled feature
maps to compute the ﬁnal segmentation mask.
our evaluation metrics include (i) jaccard index (iou
score), which measures the similarity between a predicted mask and the manu-
ally annotated ground truth, and (ii) the gap between jaccard values (jgap) to
measure fairness.
table 2 shows the performance and fairness of edgemixup and diﬀerent
baselines.
we compare predicted masks with the manually-annotated ground
truth by calculating the jaccard index, and computing the gap for subpopula-
tions with ls and ds (based on ita).
edgemixup, a data preprocessing method,
improves the utility of lesion segmentation in terms of jaccard index compared
with all existing baselines.
one reason is that edgemixup preserves skin lesion
information, thus improving the segmentation quality, while attenuating markers
for protected factors.
note that edgemixup iteratively improves the segmen-
tation results.
our classiﬁcation evaluation involves: (i) adversar-
ial debiasing (ad)
[21], (ii) dexined-avg, the average version of dexined [26]
as an boundary detector used by edgemixup, and (iii) st-debias
[22], a debi-
asing method augmenting data with conﬂicting shape and texture information.
table 3 shows utility performance (acc and auc) and fairness results (gaps
of acc and auc between ls and ds subpopulations).
skin disease classiﬁcation and associated bias.
(margin of error reported in parentheses, subpopulation reported
in brackets)
metrics
resnet34
baselines
edgemixup (ours)
ad
dexined-avg st-debias
u-net
mask-based
skin
acc
88.08(3.66) 81.79(4.35)
69.87(5.17)
76.52(5.23)
86.75(3.82)
[ds]
70.59[ls]
72.11[ls]
75.00[ls]
cai0.5
–
0.525
−4.780
2.795
4.090
6.995
cai0.75
–
1.822
−0.934
6.127
6.850
10.06
auc
0.922(0.10)
0.962(0.06)
0.824(0.13)
0.941(0.08)
0.953(0.11)
0.970(0.06)
aucgap
0.429(0.35)
0.319(0.36)
0.175(0.29)
0.255(0.31)
0.178(0.29)
0.170(0.29)
aucmin
0.500[ds]
0.650[ds]
0.650[ds]
0.711[ds]
0.784[ds]
0.800[ds]
cauci0.5
–
0.075
0.078
0.097
0.140
0.153
cauci0.75 –
0.092
0.166
0.135
0.196
0.206
the baseline unet model while “mask-based” implements deep-learning model
involved methodology introduced in sect.
3. by adding the “unet” variant, we
demonstrate here that simply applying lesion edge predicetd by the baseline unet
model, while not optimal, eﬃciently reduces model bias on diﬀerent skin-tone
samples.
edgemixup outperforms sota approaches in balancing the model’s
performance and fairness, i.e., the caiα and cauciα values of edgemixup are
the highest compared with the vanilla resnet34 and other baselines.
6
related work
skin disease classiﬁcation and segmentation: previous researches mainly
work on improving model utility for both medical image
as for skin lesion segmentation tasks, few works has been proposed
due to the lack of datasets with ground-truth segmentation masks.
[11] to encourage researches studying lesion seg-
mentation, feature detection, and image classiﬁcation.
[10] only contains melanoma samples and all of the
samples are with light skins according to our inspection using ita scores.
382
h. yuan et al.
bias mitigation: researchers have addressed bias and heterogeneity in deep
learning models [18,29].
first, masking sensitive factors in imagery is shown
to improve fairness in object detection and action recognition
second,
adversarial debiasing operates on the principle of simultaneously training two
networks with diﬀerent objectives [31].
as a comparison,
edgemixup is an eﬀective preprocessing approach to debiasing when applied to
skin disease particularly for lyme-focused classiﬁcation and segmentation tasks.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_32.pdf:
accurate segmentation of brain tumors in mri images
requires precise detection of the edges.
in this paper, we introduce the
edge-oriented transformer (eoformer) which speciﬁcally captures and
enhances edge information for brain tumor segmentation.
the cnn structure captures low-
level local features in the image, while the transformer structure estab-
lishes long-range dependencies between features to generate high-level
global features.
experimental results on the brats 2020 dataset
and a private medulloblastoma dataset demonstrate the superiority of
our approach compared with existing state-of-the-art methods.
keywords: brain tumor segmentation · edge-oriented module ·
transformer
1
introduction
accurate segmentation of brain tumors from mri images is of great signiﬁcance
as it enables more accurate assessment of tumor morphology, size, location, and
distribution range, thereby providing clinicians with a reliable basis for diagnosis
and treatment [16].
physicians manually delineate the tumor regions based on
the varying signal intensities between diseased and normal tissues.
this signal
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43901-8 32.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
al.
disparity constitutes the edge information in the images, making it essential for
accurate tumor segmentation.
[8], have
made signiﬁcant progress in the ﬁeld of medical image segmentation, including
brain tumor segmentation.
with the emergence of transformer [19], which is
capable of modeling long-range dependencies that cnns struggle with, a num-
ber of cnn-transformer hybrid networks have been proposed, such as transbts
[21], unetr [7], swin-unetr [6] and nestedformer [23], leading to further
improvements in brain tumor segmentation.
however, the performance of exist-
ing brain tumor segmentation methods are still unsatisfactory, especially for the
segmentation of edges between tumor lesion and normal tissues.
considerable advancement has been achieved in the ﬁeld of natural image
segmentation by focusing on the edge information [3,11,18,25], and this idea
is also being applied to medical image segmentation.
other methods [1,12,20,22] involve post-processing uncertain regions
to more accurately segment pixels near edges.
[22] use the conﬁdence map to evaluate the uncertainty of each pixel to enhance
the segmentation of the ambiguous edges of ultrasound images.
however, the
methods mentioned above are not suitable for brain tumor segmentation for two
main reasons.
[9] require the calculation
of the hd at each iteration, which is both time-consuming and computationally
demanding.
moreover, processing every slice of large volumes of mri images at
the pixel-level is impractical.
unlike many other medical
image segmentation tasks that involve the segmentation of a single roi, brain
tumor segmentation requires the simultaneous segmentation of three regions:
the whole tumor (wt), the tumor core (tc), and the enhancing tumor (et)
regions.
therefore, in addition to focusing on the edge between the tumor lesion
and normal tissue to segment the wt, it is also necessary to consider the edges
within the tumor in order to segment the tc and et regions.
in this paper, we propose an edge-oriented transformer (eoformer), for
eﬃcient and accurate brain tumor segmentation.
speciﬁcally, the input image is ﬁrst processed by the cnn
blocks to extract low-level local features.
then, the extracted features are fed into
the transformer blocks to create long-range dependencies, resulting in the forma-
tion of high-level semantic features.
in order to reduce the computational and
eoformer: edge-oriented transformer for brain tumor segmentation
335
fig.
our model has been evaluated on both the publicly brats 2020 dataset
and a private medulloblastoma segmentation dataset.
the results demonstrate
that eoformer clearly outperforms the state-of-the-art methods with limited
model parameters and lower flops (see more in supplementary material).
, comprises four stages, each of which consists
of a feature extraction module and a downsampling module.
in the ﬁrst two stages of ehe, we use depth-wise convolution
(dwconv) to instantiate the token mixer, called the convformer block.
in the
third stage and bottleneck, we use the multi-head self-attention (msa) to instan-
tiate the token mixer, which is the typical transformer block.
for each stage i,
given an input feature map x, the output of the ith block x
′′ is computed as
follows:
x
′ = tokenmixeri(norm(x))
assuming the size of the input feature is n and the dimensionality is d, the
input feature x ∈ rn×d pass through three linear layers to generate the queries
q ∈ rn×dk, keys k ∈ rn×dk and values v ∈ rn×dv.
moreover, to boost the segmentation performance without sacriﬁcing eﬃciency,
we incorporate the re-parameterization technique in the decoder.
× 1 × 1 × 1 convolution to enhance the interaction between channel
eoformer: edge-oriented transformer for brain tumor segmentation
337
features of x, then utilizes a learnable scaled sobel ﬁlter to extract the 1st-
order diﬀerentiation edge information from x. this ﬁlter is capable of detecting
edges in three directions (i.e. horizontal, vertical, and orthogonal directions), so
it comprises three ﬁlters mx, my, and mz, each of which is represented by a 3
× 3 × 3 array.
we then apply a learnable scaling matrix s ∈ rc×1×1×1 to mx, which allows for
dynamic adjustment of the scaling factor in each channel.
we introduce the
re-parameterization [4,5] into the edge-oriented modules to boost the segmenta-
tion performance while maintaining high eﬃciency.
+ bconvlap
(7)
where ‘∗’ represents the convolution operation, wconv means the weights of the
convolution and bconv denotes the bias, and up(·) is the spatial broadcasting
operation ,which upgrades the bias b ∈ r1×c×1×1×1 into up(b) ∈ r1×c×3×3×3.
in the inference stage, the output feature f is produced by a normal 3 × 3 × 3
convolution as follows:
[15]
76.76
55.60
66.18
7.810
9.411
8.611
transbts [21]
72.35
55.56
63.96
11.09
11.19
11.14
unetr [7]
73.38
56.02
64.70
9.112
12.70
10.90
swin-unetr [6]
70.10
60.79 65.44
9.766
9.339
9.552
nestedformer [23] 79.89 55.76
67.83
7.099
12.08
9.587
eoformer
79.74
59.10
69.42 6.978 7.104 7.041
3
experiment
3.1
dataset and evaluation metric
in order to validate the performance of eoformer, we conduct extensive experi-
ments on both the publicly available brats 2020 dataset and a private medul-
loblastoma segmentation dataset (medseg).
the brats 2020 dataset [14] consists of mri image data from 369 patients,
with each patient having four modalities (t1, t1ce, t2 and t2-flair) of
skull-striped mri, which are aligned to a standard brain template.
the medseg dataset includes mri images of t1, t1ce, t2, and t2 flair
modalities from 255 patients with medulloblastoma.
the dataset includes manual
annotations of the wt and et regions.
the
images are registered to the size of 24 × 256 × 256.
eoformer: edge-oriented transformer for brain tumor segmentation
339
fig.
2. qualitative comparison of segmentation results on brats and medseg.
3.2
implementation details
we implement eoformer in pytorch 1.11.
for data
augmentation, we apply image croping, ﬂipping, identity scaling and shifting.
table 1 displays the performance comparison of eoformer against other
methods on the brats 2020 dataset.
eoformer achieves the highest dice scores
on tc, et, and the average.
in addition, eoformer attains the best hd95
scores on tc, et, and the average.
table 2 illus-
trates the performance of eoformer and other methods on medseg.
eoformer
outperforms the second-ranked nestedformer by an average of 1.59% on dice
and achieves the top performance for both wt and et on hd95.
index encoder/decoder dice (%) ↑
hd95 (mm) ↓
wt
tc
et
ave
wt
tc
et
ave
enc1
unet encoder
83.29
83.43
79.01
84.00
6.232
8.018
6.697
6.983
enc2
cf×4
88.51
83.42
82.56
84.83
7.131
8.772
5.641
7.181
enc3
cf×2+tf×2
90.92 86.63 81.05
86.20
3.906 5.227
5.637
4.923
enc4
cf×2+etf×2
90.84
86.38
83.22
86.81 3.974
4.500 3.432
3.968
enc5
etf×4
90.24
84.15
85.40 86.59
3.951
5.651
3.405 4.336
dec1
unet decoder
89.75
84.09
80.12
84.66
7.562
7.332
6.427
7.107
dec2
if×3
90.27
86.07
80.09
85.48
5.448
6.069
4.929
5.482
dec3
cf×3
90.63
85.63
81.61
85.96
4.098
4.467 3.023 4.842
dec4
eof×3
90.84 86.38 83.22 86.81 3.974 4.500
3.432
3.968
average hd95 improvement of 1.57 mm, highlighting its superior performance
in tumor boundary prediction.
it is worth mentioning that eoformer has the
lowest flops and limited model size.
2 represents the segmentation results
on the brats 2020 and medseg datasets.
the visualisation demonstrates that
the eoformer achieves the closest segmentation results to the ground truth.
speciﬁcally, eoformer accurately segments both tc and et region boundaries.
3.4
ablation
we evaluate the eﬀectiveness of our proposed eoformer framework by con-
ducting ablation experiments on the brats 2020.
our results show that ehe outperforms other methods, with high average
dice and low average hd95.
our proposed ehe balances the
strengths of both and achieves the best segmentation performance.
addition-
ally, our extended eﬃcient attention achieves better performance compared with
enc3 because it has a better ability to capture the periphery of objects [17].
decoder design.
we compare the performance of various decoders in table 3.
dec1 - 4 share the same ehe encoder, but employ diﬀerent decoders: dec1
uses the unet decoder, dec2 has three identityformer blocks, dec3 replaces
eoformer: edge-oriented transformer for brain tumor segmentation
341
the identityformer blocks with convformer blocks, and dec4 is our proposed
eoformer decoder.
our results show that the eoformer decoder achieves the
highest dice scores, and achieves the lowest average hd95 score due to the
incorporation of the eos and eol modules within the eoformer block.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_33.pdf:
capturing global contextual information plays a critical
role in breast ultrasound (bus) image classiﬁcation.
although con-
volutional neural networks (cnns) have demonstrated reliable perfor-
mance in tumor classiﬁcation, they have inherent limitations for mod-
eling global and long-range dependencies due to the localized nature
of convolution operations.
vision transformers have an improved capa-
bility of capturing global contextual information but may distort the
local image patterns due to the tokenization operations.
in this study,
we proposed a hybrid multitask deep neural network called hybrid-mt-
estan, designed to perform bus tumor classiﬁcation and segmentation
using a hybrid architecture composed of cnns and swin transformer
components.
the proposed approach was compared to nine bus clas-
siﬁcation methods and evaluated using seven quantitative metrics on
a dataset of 3,320 bus images.
multitask learning ·
hybrid cnn-transformer
1
introduction
breast cancer is the leading cause of cancer-related fatalities among women.
currently, it holds the highest incidence rate of cancer among women in the u.s.,
and in 2022 it accounted for 31% of all newly diagnosed cancer cases [1].
due
to the high incidence rate, early breast cancer detection is essential for reducing
mortality rates and expanding treatment options.
however, bus image analysis is also challenging due to the large variations in
tumor shape and appearance, speckle noise, low contrast, weak boundaries, and
occurrence of artifacts.
in the past decade, deep learning-based approaches achieved remarkable
advancements in bus tumor classiﬁcation [2,3].
the progress has been driven
by the capability of cnn-based models to learn hierarchies of structured image
representations as semantics.
nevertheless, one disadvantage of such architectural
choice is that the feature representations in the deeper layers become increasingly
abstract, leading to a loss of spatial and contextual information.
the intrinsic
locality of convolutional operations hinders the ability of cnns to model long-
range dependencies while preserving spatial information in images eﬀectively.
[5] and its variants recently demonstrated supe-
rior performance in image classiﬁcation tasks.
these models convert input images
into smaller patches and utilize the self-attention mechanism to model the rela-
tionships between the patches.
self-attention enables vits to capture long-range
dependencies and model complex relationships between diﬀerent regions of the
image.
however, the eﬀectiveness of vit-based approaches heavily relies on
access to large datasets for learning meaningful representations of input images.
this is primarily because the architectural design of vits does not rely on the
same inductive biases in feature extraction which allow cnns to learn spatially
invariant features.
accordingly, numerous prior studies introduced modiﬁcations to the origi-
nal vit network speciﬁcally designed for bus image classiﬁcation [13,14,23].
[4] designed
two hybrid cnn-transformer networks intended either for classiﬁcation or seg-
mentation of multi-modal breast cancer images.
despite the promising results
of such hybrid approaches, eﬀectively capturing the local patterns and global
long-range dependencies in bus images remains challenging [4,5,24].
multitask learning leverages shared information across related tasks by
jointly training the model.
moreover,
multitask learning acts as a regularizer by introducing inductive bias and pre-
vents overﬁtting [25] (particularly with vits), and with that, can mitigate the
challenges posed by small bus dataset sizes.
in this study, we introduce a hybrid multitask approach, hybrid-mt-estan,
which encompasses tumor classiﬁcation as a primary task and tumor segmenta-
tion as a secondary task.
hybrid-mt-estan combines the advantages of cnns
and transformers in a framework incorporating anatomical tissue information in
bus images.
the anatomy of the human breast
is categorized into four primary layers: the skin, premammary (subcutaneous
fat), mammary, and retromammary layers, where each layer has a distinct tex-
ture and generates diﬀerent echo patterns.
the primary layers in bus images
are arranged in a vertical stack, with similar echo patterns appearing horizon-
tally across the images.
[4], in which the authors used hybrid single-task cnn-transformer
networks for either classiﬁcation or segmentation of bus images.
the main contributions of this work are summarized as:
• the proposed architecture eﬀectively integrates the advantages of cnns for
extracting hierarchical and local patterns in bus images and swin trans-
formers for leveraging long-range dependencies.
• the multitask learning approach leverages the shared representations across
the classiﬁcation and segmentation tasks to improve the model performance.
fig.
1. hybrid-mt-estan consists of mt-estan and aaa encoders, a segmentation
branch, and a classiﬁcation branch.
[3], and a swin transformer-based encoder with anatomy-
aware attention (aaa) blocks, (2) a decoder branch for the segmentation task,
and (3) a branch with fully-connected layers for the classiﬁcation task.
mt-
estan [3] is a cnn-based multitask learning network that simultaneously per-
forms bus classiﬁcation and segmentation.
[17], which employs row-column-wise kernels to learn and
fuse context information in bus images at diﬀerent context scales (see fig.
these specialized convolutional kernels eﬀectively extract contextual
information of small tumors in bus images.
refer to [17,22], and [3] for the
implementation details of estan and mt-estan.
swin transformer parti-
tions an input image into non-overlapping patches of size 4×4, where each patch
is treated as a “token”.
a linear layer receives the patches and projects them into
an arbitrary dimension.
(7)
concretely, we ﬁrst reconstruct the i-th feature map (yi) by merging (m) all
patches, and afterward, we applied average pooling (avg-p) and max pooling
348
b. shareef et al.
(max-p) layers with size (2, 2).
3. anatomy-aware attention (aaa) block.
2.3
segmentation and classiﬁcation branches/tasks
the segmentation branch in fig.
2.4
loss function
we applied a multitask loss function (lmt) that aggregates two terms: a focal loss
lf ocal for the classiﬁcation task and dice loss ldice for the segmentation task.
since in medical image diagnosis achieving high sensitivity
places emphasis on the detection of malignant lesions, we employed the focal loss
for the classiﬁcation task to trade oﬀ between sensitivity and speciﬁcity.
because
malignant tumors are more challenging to detect due to greater diﬀerences in
margin, shape, and appearance in bus images, focal loss forces the model to
focus more on diﬃcult predictions.
in the formulation, α
bus tumor classiﬁcation using multitask cnn-transformer network
349
is a weighting coeﬃcient, n denotes the number of image samples, ti is the target
label of the ith training sample, and pi denotes the prediction.
the segmentation
loss is calculated using the commonly-employed dice loss (ldice) function.
3
experimental results
3.1
datasets
we evaluated the performance of hybrid-mt-estan using four public datasets,
hmss
we combined all four
datasets to build a large and diverse dataset with a total of 3,320 b-mode bus
images, of which 1,664 contain benign tumors and 1,656 have malignant tumors.
hmss dataset does not
provide the segmentation ground-truth masks, and for this study we arranged
with a group of experienced radiologists to prepare the masks for hmss.
bus dataset no. of images distribution
source
hmss
1,948
b:812, m:1136
netherlands
busi
647
b:437, m:210
egypt
busis
562
b:306, m:256
china
dataset b
163
b:109, m:54
spain
total
3,320
b: 1,664, m: 1,656
3.2
evaluation metrics
for performance evaluation of the classiﬁcation task, we used the following met-
rics: accuracy (acc), sensitivity (sens), speciﬁcity (spec), f1 score, area under
the curve of receiver operating characteristic (auc), false positive rate (fpr),
and false negative rate (fnr).
to evaluate the segmentation performance, we
used dice similarity coeﬃcient (dsc) and jaccard index (ji).
3.3
implementation details
the proposed approach was implemented with keras and tensorflow libraries.
all experiments were performed on a machine with nvidia quadro rtx 8000
gpus and two intel xeon silver 4210r cpus (2.40ghz) with 512 gb of ram.
all bus images in the dataset were zero-padded and reshaped to form square
images.
to avoid data leakage and bias, we selected the train, test, and vali-
dation sets based on the cases, i.e., the images from one case (patient) were
350
b. shareef et al.
table 2. performance metrics of the compared methods for bus image classiﬁcation
and segmentation.
classiﬁcation
segmentation
methods
acc↑ sens.↑ spec.↑ f1↑
auc↑ fnr↓ fpr↓ dsc↑
ji↑
sha-mtl [8]
69.6
48.1
90.8
0.58
69.5
51.9
9.2
72.2
60.7
mobilenet
[5]
72.1
74.1
69.3
0.73
71.7
25.9
30.7
-
-
chowdery [10]
77.4
77.3
77.3
0.77
77.3
22.7
22.7
77.0
67.9
swin transformer 77.4
72.6
82.5
0.74
77.6
27.4
17.5
-
-
mt-estan
78.6
83.7
72.6
0.83
78.2
16.3
27.4
78.2
69.3
ours
82.8
86.4
79.2
0.86 82.8
13.6
20.8
84.1
75.7
note: a dash ‘-’ in the segmentation column indicates that the model uses single-task
learning.
assigned to only one of the training, validation, and test sets.
furthermore,
we employed horizontal ﬂip, height shift (20%), width shift (20%), and rota-
tion (20◦c) for data augmentation.
the proposed approach utilizes the building
blocks of resnet50 and swin-transformer-v2, pretrained on imagenet dataset.
namely, mt-estan uses pretrained resnet50 as a base model for the ﬁve
encoder blocks (the implementation details of mt-estan can be found in
[3]).
for model
training we utilized adam optimizer with a learning rate of 10−5 and mini batch
size of 4 images.
3.4
performance evaluation and comparative analysis
we compared the performance of hybrid-mt-estan for bus classiﬁcation to
nine deep learning approaches commonly used for medical image analysis.
the values of the performance metrics are shown in table 2, indicating that the
proposed hybrid-mt-estan outperformed all nine approaches by achieving
the best accuracy, sensitivity, f1 score, and auc with 82.8%, 86.4%, 86.0%,
and 82.8%, respectively.
classiﬁcation
segmentation
methods
acc↑ sens.↑ spec.↑ f1↑
auc↑ fnr↓ fpr↓ dsc↑
ji↑
mt-estan [10]
78.6
83.7
72.6
0.83
78.2
16.3
27.4
78.2
69.3
swin trans.
77.4
72.6
82.5
0.74
77.6
27.4
17.5
-
-
mt-estan + swin trans.
80.3
84.2
76.3
0.83
80.2
15.8
23.7
82.3
73.6
ours
82.8
86.4
79.2
0.86 82.8
13.6
20.8
84.1
75.7
the preferred trade-oﬀ in medical image analysis typically is high sensitivity
without signiﬁcant degradation in speciﬁcity.
we evaluated the segmentation performance of hybrid mt-estan and com-
pared the results to ﬁve multitask approaches, including sha-mtl
as shown in
table 2,the proposed hybrid mt-estan achieved the highest performance and
increased dsc and ji by 5.9% and 6.4%, respectively compared to mt-estan.
note that results of single-task models in table 2 are not provided.
3.5
eﬀectiveness of the anatomy-aware attention (aaa) block
to verify the eﬀectiveness of the anatomy-aware attention (aaa) block, we
conducted an ablation study that quantiﬁed the impact of the diﬀerent com-
ponents in hybrid-mt-estan on the classiﬁcation and segmentation perfor-
mance.
table 3 presents the values of the performance metrics for mt-estan
(pure cnn-based approach), swin transformer (pure transformer network), a
hybrid architecture of mt-estan and swin transformer, and our proposed
hybrid-mt-estan with aaa block.
the hybrid architectures of mt-estan with swin
transformer improved the classiﬁcation performance and has higher accuracy,
sensitivity, f1 score, and auc with 80.3%, 84.2%, 83%, and 80.2%, compared
to mt-estan and swin transformer individually.
the proposed approach,
hybrid-mt-estan with aaa block, further improved accuracy, sensitivity, f1
score, and auc by 2.5%, 2.2%, 3%, and 2.6%, respectively, relative to the hybrid
model without the aaa block.
to evaluate the segmentation performance, we compared the proposed app-
roach with and without the aaa block and swin transformer.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_27.pdf:
designing deep learning algorithms for gland segmentation
is crucial for automatic cancer diagnosis and prognosis.
however, the
expensive annotation cost hinders the development and application of
this technology.
in this paper, we make a ﬁrst attempt to explore a deep
learning method for unsupervised gland segmentation, where no man-
ual annotations are required.
existing unsupervised semantic segmenta-
tion methods encounter a huge challenge on gland images.
they either
over-segment a gland into many fractions or under-segment the
gland regions by confusing many of them with the background.
to overcome this challenge, our key insight is to introduce an empirical
cue about gland morphology as extra knowledge to guide the segmen-
tation process.
to this end, we propose a novel morphology-inspired
method via selective semantic grouping.
we ﬁrst leverage the empirical
cue to selectively mine out proposals for gland sub-regions with variant
appearances.
then, a morphology-aware semantic grouping module is
employed to summarize the overall information about glands by explic-
itly grouping the semantics of their sub-region proposals.
in this way,
the ﬁnal segmentation network could learn comprehensive knowledge
about glands and produce well-delineated and complete predictions.
we
conduct experiments on the glas dataset and the crag dataset.
keywords: whole slide image · unsupervised gland segmentation ·
morphology-inspired learning · semantic grouping
1
introduction
accurate gland segmentation from whole slide images (wsis) plays a crucial role
in the diagnosis and prognosis of cancer, as the morphological features of glands
can provide valuable information regarding tumor aggressiveness
with the
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43901-8 27.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
(b)
prior uss methods in medical image research [2] and natural image research
(color ﬁgure online)
emergence of deep learning (dl), there has been a growing interest in developing
dl-based methods for semantic-level [9,12,36] and instance-level [5,13,25,27,
32,35] gland segmentation.
however, such methods typically rely on large-scale
annotated image datasets, which usually require signiﬁcant eﬀort and expertise
from pathologists and can be prohibitively expensive [28].
to reduce the annotation cost, developing annotation-eﬃcient methods for
semantic-level gland segmentation has attracted much attention [10,18,23,37].
recently, some researchers have explored weakly supervised semantic segmen-
tation methods which use weak annotations (e.g., bound box
[18]) instead of pixel-level annotations to train a gland segmentation net-
work.
[10,29] to design annotation-free
methods for gland segmentation.
however, the performance of these methods
can vary widely, especially in cases of malignancy.
this paper focuses on unsu-
pervised gland segmentation, where no annotations are required during
training and inference.
one potential solution is to adopt unsupervised semantic segmentation (uss)
methods which have been successfully applied to medical image research and nat-
ural image research.
on the one hand, existing uss methods have shown promis-
ing results in various medical modalities, e.g., magnetic resonance images [19],
x-ray images [1,15] and dermoscopic images [2].
however, directly utilizing these
methods to segment glands could lead to over-segment results where a gland is
segmented into many fractions rather than being considered as one target (see
fig.
this is because these methods are usually designed to be extremely
sensitive to color [2], while gland images present a unique challenge due to
their highly dense and complex tissues with intricate color distribution [18].
on the other hand, prior uss methods for natural images can be broadly cate-
gorized into coarse-to-ﬁne-grained
[4,14,16,21,31] and end-to-end (e2e) cluster-
morphology-inspired ugs via selective semantic grouping
[31], and self-attention maps [4,14,21])
as prior, which is not always feasible on gland images.
the e2e clustering meth-
ods, however, produce under-segment results on gland images by confusing many
gland regions with the background; see fig. 1(b).
as such, the e2e clustering
methods tend to blindly cluster pixels with similar properties and confuse many
gland regions with the background, leading to under-segment results.
to tackle the above challenges, our solution is to incorporate an empir-
ical cue about gland morphology as additional knowledge to guide
gland segmentation.
to this end, we propose a novel morphology-inspired method via selective
semantic grouping, abbreviated as mssg.
to begin, we leverage the empirical
cue to selectively mine out proposals for the two gland sub-regions with vari-
ant appearances.
then, considering that our segmentation target is the gland,
we employ a morphology-aware semantic grouping module to summarize the
semantic information about glands by explicitly grouping the semantics of the
sub-region proposals.
in this way, we not only prioritize and dedicate extra atten-
tion to the target gland regions, thus avoiding under-segmentation; but also
exploit the valuable morphology information hidden in the empirical cue, and
force the segmentation network to recognize entire glands despite the excessive
variance among the sub-regions, thus preventing over-segmentation.
our contributions are as follows: (1) we identify the major challenge encoun-
tered by prior unsupervised semantic segmentation (uss) methods when dealing
with gland images, and propose a novel mssg for unsupervised gland segmen-
tation.
(2) we propose to leverage an empirical cue to select gland sub-regions
and explicitly group their semantics into a complete gland region, thus avoid-
ing over-segmentation and under-segmentation in the segmentation results.
[13]), and the experiment results
demonstrate the eﬀectiveness of our mssg in unsupervised gland segmentation.
2. overview of our morphology-inspired unsupervised gland segmentation via
selective semantic grouping.
we leverage an
empirical cue to select proposals for gland sub-regions from the prediction of a shal-
low encoder f(·) which emphasizes low-level appearance features rather than high-
level semantic features.
(b) morphology-aware semantic grouping (msg) pipeline.
a segmentation network.
meantime, a morphology-aware semantic grouping
(msg) module is used to summarize the overall information about glands from
their sub-region proposals.
2.1
selective proposal mining
instead of generating pseudo-labels for the gland region directly from all the
pixels of the gland images as previous works typically do, which could lead to
over-segmentation and under-segmentation results, we propose using the empir-
ical cue as extra hints to guide the proposal generation process.
speciﬁcally, let the ith input image be denoted as xi ∈ rc×h×w , where h,
w, and c refer to the height, width, and number of channels respectively.
we train the
encoder in a self-supervised manner, and the loss function l consists of a typical
self-supervised loss lss, which is the cross-entropy loss between the feature map
fi and the one-hot cluster label ci = arg max (fi), and a spatial continuity loss
lsc, which regularizes the vertical and horizontal variance among pixels within
a certain area s to assure the continuity and completeness of the gland border
regions (see fig.
1 in the supplementary material).
the expressions for
lss and lsc are given below:
lss(fi[:, h, w], ci[:, h, w]) = −
d

d
ci[d, h, w] · ln fi[d, h, w]
(1)
morphology-inspired ugs via selective semantic grouping
285
lsc (fi) =
s,h−s,w −s

s,h,w
(fi[:, h + s, w] − fi
+ n5 equals the total number of pixels in the input image (h × w).
the aforemen-
tioned empirical cue is used to select proposals for the gland border and interior
epithelial tissues from the candidate regions yi.
particularly, we select the region
with the highest average gray level as the proposal for the gland border.
then,
we ﬁll the areas surrounded by the gland border proposal and consider them as
the proposal for the interior epithelial tissues, while the rest areas of the gland
image are regarded as the background (i.e., non-glandular region).
finally, we
obtain the proposal map pi ∈ r3×h×w , which contains the two proposals for
two gland sub-regions and one background proposal.
2.2
morphology-aware semantic grouping
a direct merge of the two sub-region proposals to train a fully-supervised seg-
mentation network may not be optimal for our case.
firstly, the two gland sub-
regions exhibit signiﬁcant variation in appearance, which can impede the seg-
mentation network’s ability to recognize them as integral parts of the same
object.
secondly, the spm module may produce proposals with inadequate
highlighting of many gland regions, particularly the interior epithelial tissues,
as shown in fig.
consequently,
applying pixel-level cross-entropy loss between the gland image and the merged
proposal map could introduce undesired noise into the segmentation network,
thus leading to under-segment predictions with confusion between the glands and
the background.
as such, we propose two types of morphology-aware semantic
grouping (msg) modules (i.e., msg for variation and msg for omission) to
respectively reduce the confusion caused by the two challenges mentioned above
and improve the overall accuracy and comprehensiveness of the segmentation
results.
here, we ﬁrst slice the gland image and its proposal map into patches as
inputs.
we can obtain the feature embedding map
ˆf which is derived as ˆf = ffeat( ˆx) and the prediction map 
x as 
x = fcls( ˆf),
where ffeat and fcls refers to the feature extractor and pixel-wise classiﬁer of
the segmentation network respectively.
msg for variation is designed to mitigate the adverse impact of appear-
ance variation between the gland sub-regions.
then,
we use the average of the pixel embeddings in gland border set g as the align-
ment anchor and pull all pixels of i towards the anchor:
lmsgv = 1
i

i∈i

i − 1
g

g∈g
g
2
.
it identiﬁes and relabels the overlooked gland regions in
the proposal map and groups them back into the gland semantic category.
finally, we impose a pixel-level cross-entropy loss
on the prediction and reﬁned proposal rp to train the segmentation network:
lmsgo = −
ˆ
h, ˆ
w

ˆh, ˆ
w
rp[:, ˆh, ˆw] · ln 
x[:, ˆh, ˆw],
(5)
the total objective function l for training the segmentation network can be
summarized as follows:
3
experiments
3.1
datasets
we evaluate our mssg on the gland segmentation challenge (glas) dataset
[27] and the colorectal adenocarcinoma gland (crag) dataset
the crag dataset has more irregular malignant
glands, which makes it more diﬃcult than glas, and we would like to empha-
size that the results on crag are from the model trained on glas without
retraining.
morphology-inspired ugs via selective semantic grouping
287
fig.
black
denotes glandular tissues and white denotes non-glandular tissues (more in the sup-
plementary material).
[2]
pspnet
none
69.29%
67.88%
55.31%
mssg
pspnet
none
77.43% 77.26% 65.89%
3.2
implementation details
the experiments are conducted on four rtx 3090 gpus.
for the msg, mmsegmentation
[7] is used
to construct a pspnet [38] with a resnet-50 backbone as the segmentation
network.
without msg, the performance is not good
enough, due to signiﬁcant sub-region variation and gland omission.
with msg mod-
ules, the performance of the network is progressively improved (more in the sup-
plementary material).
table 2. performance gains with msg modules.
the segmentation performance is
progressively improved as the involvement of msg for variation & msg for omission.
msg for variation
msg for omission
miou
improvement(δ)
×
×
48.42%
-
√
×
56.12%
on the glas dataset, the end-to-end clustering methods
(denoted by “∗”) end up with limited improvement over a randomly initialized
network.
addi-
tionally, we visualize the segmentation results of mssg and its counterpart (i.e.,
sgscn [2]) in fig.
morphology-inspired ugs via selective semantic grouping
289
3.4
ablation study
table 2 presents the ablation test results of the two msg modules.
it can be
observed that the segmentation performance without the msg modules is not
satisfactory due to the signiﬁcant sub-region variation and
moreover, with both msg modules
incorporated, the performance signiﬁcantly improves to 62.72% (+14.30%).
more ablation tests on the spm (tab. 1 & 2) and
hyper-parameters (tab. 3) are in the supplementary material.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_15.pdf:
gastric cancer is the third leading cause of cancer-related
mortality worldwide, but no guideline-recommended screening test
exists.
existing methods can be invasive, expensive, and lack sensitiv-
ity to identify early-stage gastric cancer.
we propose a novel cluster-induced mask
transformer that jointly segments the tumor and classiﬁes abnormal-
ity in a multi-task manner.
in
our experiments, the proposed method achieves a sensitivity of 85.0%
and speciﬁcity of 92.6% for detecting gastric tumors on a hold-out test
set consisting of 100 patients with cancer and
in compari-
son, two radiologists have an average sensitivity of 73.5% and speciﬁcity
of 84.3%.
our approach performs comparably to estab-
lished state-of-the-art gastric cancer screening tools like blood testing
and endoscopy, while also being more sensitive in detecting early-stage
cancer.
[16], which is
mainly attributed to patients being diagnosed with advanced-stage disease har-
boring unresectable tumors.
this is often due to the latent and nonspeciﬁc signs
and symptoms of early-stage gc.
however, patients with early-stage disease
have a substantially higher ﬁve-year survival rate of around 72% [16].
unfortunately, current guidelines do not recommend any screening tests for gc
[22].
this is because early-stage gastric tumors may only invade
the mucosal and muscularis layers, which are diﬃcult to identify without the
help of stomach preparation and contrast injection.
unlike the conventional “segmentation for classiﬁcation”
methods that directly employ segmentation networks, we developed a cluster-
induced mask transformer that performs segmentation and global classiﬁcation
simultaneously.
by incorporating self-attention layers for global context modeling,
our model can leverage both local and global cues for accurate detection.
in
our experiments, the proposed approach outperforms nnunet
researchers have explored automated tumor
detection techniques on endoscopic [13,14], pathological images [20], and the
prediction of cancer prognosis [12].
recent developments in deep learning have
signiﬁcantly improved the segmentation of gastric tumors [11], which is critical
for their detection.
while previous
studies have successfully detected pancreatic [25] and esophageal [26] cancers on
non-contrast ct, identifying gastric cancer presents a unique challenge due to
its subtle texture changes, various shape alterations, and complex background,
e.g., irregular gastric wall; liquid and contents in the stomach.
recent studies have used transformers for natural and
medical image segmentation [21].
[1] as memory-encoded queries for segmentation.
mask transformers are locally sen-
sitive to image textures for precise segmentation and globally aware of organ-
tumor morphology for recognition.
there-
fore, mask transformers are an ideal choice for an end-to-end joint segmentation
and classiﬁcation system for detecting gastric cancer.
pi ∈ l is the class label of the image, conﬁrmed by pathology, radiology, or
clinical records.
any misaligned
ones are revised manually.
in this manner (fig.
, a relatively coarse yet highly
reliable tumor mask can be obtained for the non-contrast ct image.
(a) the non-contrast ct image is ﬁrst forwarded into a u-
net [8,18] to extract a feature map.
the cluster assignment (a.k.a. mask prediction) is further used to generate the ﬁnal
segmentation output and the classiﬁcation probability.
segmentation
for classiﬁcation is widely used in tumor detection [25,26,32].
we ﬁrst train a
unet [8,18] to segment the stomach and tumor regions using the masks from the
previous step.
speciﬁcally, given image x ∈ rh×w ×d, annotation y ∈ rk×hw d, and
patient class p ∈ l, our model consists of three components: 1) a cnn back-
bone to extract its pixel-wise features f ∈ rc×hw d (fig. 1a), 2) a transformer
module (fig. 1b), and 3) a multi-task cluster inference module (fig. 1c).
the assignment of clusters (a.k.a. mask
prediction) m
(2)
the ﬁnal segmentation logits
the aggregation of pixels is achieved by
z = ckm, where the cluster-wise classiﬁcation ck is represented by an mlp
that projects the cluster centers c to k channels (the number of segmentation
classes).
the learned cluster centers possess high-level semantics with both inter-
cluster discrepancy and intra-cluster similarity for eﬀective classiﬁcation.
rather
than directly classifying the ﬁnal feature map, we ﬁrst generate the cluster-
path feature vector by taking the channel-wise average of cluster centers ¯c =
1
n

i=1 ci ∈ rc.
additionally, to enhance the consistency between the seg-
mentation and classiﬁcation outputs, we apply global max pooling to cluster
assignments r to obtain the pixel-path feature vector ¯r ∈ rn.
this establishes
a direct connection between classiﬁcation features and segmentation predictions.
the overall training objective is formulated as,
l = lseg(z, y) + lcls(ˆp, p),
(3)
where the segmentation loss lseg(·, ·) is a combination of dice and cross entropy
losses, and the classiﬁcation loss lcls(·, ·) is cross entropy loss.
4
experiments
4.1
experimental setup
dataset and ground truth.
all patients underwent multi-phase cts with a median spac-
ing of 0.75 × 0.75 × 5.0 mm and an average size of (512, 512, 108) voxel.
this
early-stage gc case is miss-detected by both radiologists and nnunet
[23], while the stomach was automatically
annotated using a self-learning model [31].
implementation details.
we set the num-
ber of object queries n to 8, with each having a dimension of 128, and included
an eight-head self-attention layer in each block.
we followed [8] to augment data.
to enhance performance, we added
deep supervision by aligning the cross-attention map with the ﬁnal segmentation
map, as per kmax-deeplab [27].
the hidden layer dimension in the two-layer
mlp is 128.
[8,18] to localize the stomach
region in the entire image in the testing phase.
evaluation metrics and reader study.
for the binary classiﬁcation, model
performance is evaluated using area under roc curve (auc), sensitivity (sens.),
and speciﬁcity (spec.).
and successful localization of the tumors is considered
when the overlap between the segmentation mask generated by the model and
152
m. yuan et al.
table 1.
(78.1, 91.1) (88.0, 96.5) (96.7, 98.7)
table 2. patient-level detection and tumor-level localization results (%) over gas-
tric cancer across diﬀerent t-stages.
tumor-level localization evaluates how segmented
masks overlap with the ground-truth cancer (dice > 0.01 for correct detection).
miss-t:
missing of t stage information.
[10],
ugis and endoscopy screening performance in large population [4], and early stage
gastric cancer detection rate of senior radiologists on narrow-band imaging with mag-
nifying endoscopy (me-nbi)
†: we also merely consider early-stage gastric
cancer cases, including tumor in situ, t1, and t2 stages, among whom we successfully
detect 17 of 19 cases.)
[10]
99.5
66.7
69.4∗
upper-gastrointestinal series [4] 96.1
36.7
85.0
endoscopy screening [4]
96.0
69.0
85.0
me-nbi (early-stage)
the ﬁrst two approaches belong to “segmentation
for classiﬁcation” (s4c)
a case
is classiﬁed as positive if the segmented tumor volume exceeds a threshold that
maximizes the sum of sensitivity and speciﬁcity on the validation set.
the advantage
of our approach is that it captures the local and global information simultane-
ously in virtue of the unique architecture of mask transformer.
it also extracts
high-level semantics from cluster representations, making it suitable for classiﬁ-
cation and facilitating a holistic decision-making process.
moreover, our method
reaches a considerable speciﬁcity of 97.7% on the external test set, which is cru-
cial in opportunistic screening for less false positives and unnecessary human
workload.
the model achieves a sensitivity of 85.0% in
detecting gastric cancer, which signiﬁcantly exceeds the mean performance of
doctors (73.5%) and also surpasses the best performing doctor (r2: 75.0%),
while maintaining a high speciﬁcity.
this early-stage cancer (t1) is miss-detected by both radiologists, whereas clas-
siﬁed and localized precisely by our model.
154
m. yuan et
in table 2, we report the performance of patient-level
detection and tumor-level localization stratiﬁed by tumor (t) stage.
we compare
our model’s performance with that of both radiologists.
the results show that
our model performs better in detecting early stage tumors (t1, t2) and provides
more precise tumor localization.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_28.pdf:
the success of large-scale pre-trained vision-language models
(vlm) has provided a promising direction of transferring natural image
representations to the medical domain by providing a well-designed
prompt with medical expert-level knowledge.
besides, the models pre-trained with natu-
ral images fail to detect lesions precisely.
to solve this problem, fusing
multiple prompts is vital to assist the vlm in learning a more compre-
hensive alignment between textual and visual modalities.
in this paper,
we propose an ensemble guided fusion approach to leverage multiple
statements when tackling the phrase grounding task for zero-shot lesion
detection.
extensive experiments are conducted on three public medical
image datasets across diﬀerent modalities and the detection accuracy
improvement demonstrates the superiority of our method.
keywords: vision-language models · lesion detection · multiple
prompts · prompt fusion · ensemble learning
1
introduction
medical lesion detection plays an important role in assisting doctors with the
interpretation of medical images for disease diagnosing, cancer staging, etc.,
which can improve eﬃciency and reduce human errors [9,19].
current object
detection approaches are mainly based on supervised learning with abundant
well-paired image-level annotations, which heavily rely on expert-level knowl-
edge.
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43904-9_28.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
https://doi.org/10.1007/978-3-031-43904-9_28
284
m. guo et al.
recently, large-scale pre-trained vision-language models (vlms), by learning
the visual concepts in the images through the weak labels from text, have pre-
vailed in natural object detection or visual grounding and shown extraordinary
performance.
however, current existing vlms are mostly based on a single prompt to
establish textual and visual alignment.
apparently, even a well-designed
prompt is not always able to combine all expressive attributes into one sentence
without semantic and syntactic ambiguity, e.g., the prompt design for melanoma
detection should include numerous kinds of information describing attributes
complementing each other, such as shape, color, size, etc
in this work, instead of striving to design a single satisfying prompt, we
aim to take advantage of pre-trained vlms in a more ﬂexible way with the
form of multiple prompts, where each prompt can elicit respective knowledge
from the model which can then be fused for better lesion detection performance.
in addition, we
also examine the language syntax based prompt fusion approach as a compar-
ison, and explore several fusion strategies by ﬁrst grouping the prompts either
with described attributes or categories and then repeating the fusion process.
we evaluate the proposed approach on a broad range of public medical
datasets across diﬀerent modalities including photography images for skin lesion
detection isic 2016
[2], endoscopy images for polyp detection cvc-300
[21],
and cytology images for blood cell detection bccd.
2
related work
object detection and vision-language models.
in the vision-language
ﬁeld, phrase grounding can be regarded as another solution for object detec-
tion apart from conventional r-cnns
recently, vision-language models
multiple prompt fusion for zero-shot lesion detection
285
have achieved exciting performance in the zero-shot and few-shot visual recog-
nition [4,16].
in addition, vild [7] is proposed
for open-vocabulary object detection taking advantage of the rich knowledge
learned from clip [4] and text input.
as pointed out by a review [3], ensemble learning meth-
ods achieve better performance by producing predictions based on extracted fea-
tures and fusing via various voting mechanisms.
for example, a selective ensem-
ble of classiﬁer chains [13] is proposed to reduce the computational cost and the
storage cost arose in multi-label learning [12] by decreasing the ensemble size.
3
method
in this section, we ﬁrst brieﬂy introduce the vision-language model for unifying
object detection as phrase grounding, e.g., glip
then we present
a simple language syntax based prompt fusion approach in sect.
3.3 to improve the zero-shot lesion detection.
3.1
preliminaries
phrase grounding is the task of identifying the ﬁne-grained correspondence
between phrases in a sentence and objects in an image.
the glip model takes
as input an image i and a text prompt p that describes all the m candidate
categories for the target objects.
then, glip uses a ground-
ing module to align image boxes with corresponding phrases in the text prompt.
the whole process can be formulated as follows:
o = enci(i), p = enct(p), sground = op ⊤, lcls = loss(sground; t),
(1)
where o ∈ rn×d, p ∈ rm×d denote the image and text features respectively for
n candidate region proposals and m target objects, sground ∈ rn×m represents
the cross-modal alignment scores, and t ∈ {0, 1}n×m is the target matrix.
3.2
language syntax based prompt fusion
as mentioned above, it is diﬃcult for a single prompt input structure such as
glip to cover all necessary descriptions even through careful designation of
the prompt.
however, it is challenging to
combine the grounding results from multiple prompts since manual integration is
subjective, ineﬀective, and lacks uniform standards.
we achieve this by extracting and
fusing the preﬁxes and suﬃxes of each prompt based on language conventions
and grammar rules.
moreover, the fused prompts are normally too long that the
model could lose proper attention to the key information, resulting in extremely
unstable performance (results shown in sect.
more speciﬁcally, the vlm outputs a set of candidate
region proposals ci for each prompt pi, and these candidates carry more multi-
dimensional information than prompts.
we ﬁnd in our preliminary experiments
that direct concatenation of the candidates is not satisfactory and eﬀective, since
simply integration hardly screens out the bad predictions.
therefore, we consider step-wise clustering mechanisms
using the above information to screen out the implausible candidates based on
clustering ensemble learning [3].
another observation in our preliminary experiments is that most of the can-
didates distribute near the target if the prompt description matches better with
the object.
4
experiments and results
4.1
experimental settings
we collect three public medical image datasets across various modalities includ-
ing skin lesion detection dataset isic 2016
for the experiments, we use the
glip-t variant [11] as our base pre-trained model and adopt two metrics for
the grounding evaluation, including average precision (ap) and ap50.
more
details on the dataset and implementation are described in the appendix.
[20]
1.16
5.37
3.27
9.40
1.22
4.75
concatenation
16.9
27.4
21.5
27.8
11.6
20.6
syntax based fusion 13.9
24.1
10.0
16.4
12.8
25.4
ours
19.8 30.9
36.1 47.9
15.8 32.6
4.2
results
this section demonstrates that our proposed ensemble guided fusion approach
can eﬀectively beneﬁt the model’s performance.
the proposed approach achieves the best performance in zero-
shot lesion detection compared to baselines.
to conﬁrm the validity of
our method, we conduct extensive experiments under the zero-shot setting and
include a series of fusion baselines: concatenation, non-maximum suppression
(nms)
these
prompts give comparable performance to our single prompt and can be still be
improved by our fusion method.
here we present part of the single prompts used in the experiments
for illustration.
fine-tuned models can further improve the detection performance.
we conduct 10-shot ﬁne-tuning experiments as a complement, and ﬁnd the per-
formance greatly improved.
therefore, we can conclude that the pre-trained glip
model has the ability to learn a reasonable alignment between textual and visual
modalities in medical domains.
visualizations.
syntax based fusion sometimes fails to ﬁlter out unreasonable
predictions because these regions are generated directly by the vlm without
further processing and eventually resulting in unstable detection performance.
the step-wise clustering mechanism based on
ensemble learning enables our method to exploit multi-dimensional information
besides visual features.
the results show that when combining the above three components,
the proposed approach gives the best lesion detection performance, suggesting
that all components are necessary and eﬀective in the proposed approach.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_16.pdf:
however, it is not clear how sensitive mil is to
clinically realistic domain shifts, i.e., diﬀerences in data distribution that
could negatively aﬀect performance, and if already existing metrics for
detecting domain shifts work well with these algorithms.
we trained an
attention-based mil algorithm to classify whether a whole-slide image
of a lymph node contains breast tumour metastases.
our
contributions include showing that mil for digital pathology is aﬀected
by clinically realistic diﬀerences in data, evaluating which features from
a mil model are most suitable for detecting changes in performance,
and proposing an unsupervised metric named fr´echet domain distance
(fdd) for quantiﬁcation of domain shifts.
shift measure performance
was evaluated through the mean pearson correlation to change in classiﬁ-
cation performance, where fdd achieved 0.70 on 10-fold cross-validation
models.
fdd could be a valuable tool for care
providers and vendors who need to verify if a mil system is likely to
perform reliably when implemented at a new site, without requiring any
additional annotations from pathologists.
supported by swedish e-science research center, vinnova, the ceniit career
development program at link¨oping university, and wallenberg ai, wasp funded by
the knut and alice wallenberg foundation.
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43904-9 16.
et al.
keywords: domain shift detection · attention-based mil · digital
pathology
1
introduction
the spreading digitalisation of pathology labs has enabled the development of
deep learning (dl) tools that can assist pathologists in their daily tasks.
how-
ever, supervised dl methods require detailed annotations in whole-slide images
(wsis) which is time-consuming, expensive and prone to inter-observer disagree-
ments
this remains a signiﬁcant obstacle to the deployment of dl
applications in clinical practice [7].
hence, it is important
to provide indications of the expected performance on a target dataset without
requiring annotations [5,25].
[33] which aims to detect individual samples that are ood, in contrast
to our objective of estimating a diﬀerence of expected performances between
some datasets.
alternatively,
a drop in performance can be estimated by comparing the model’s softmax out-
puts [8] or some hidden features [24,28] acquired on in-domain and domain shift
datasets.
we show that our proposed unsupervised metric for quantifying
domain shift correlates best with the changes in performance, in comparison to
multiple baselines.
the novel contributions of our work can be summarised as:
1. proposing an unsupervised metric named fr´echet domain distance (fdd)
for quantifying the eﬀects of domain shift in attention-based mil;
2. showing how fdd can help to identify subsets of patient cases for which mil
performance is worse than reported on the in-domain test data;
3.
comparing the eﬀectiveness of using uncertainty estimation versus learnt rep-
resentations for domain shift detection in mil.
2
methods
our experiments center on an mil algorithm with attention developed for clas-
siﬁcation in digital pathology.
[17] because it well represents an architecture of mil
with attention, meaning that our approach can equally be applied to many other
such methods.
2.2
mil features
we explored several diﬀerent feature sets that can be extracted from the
attention-based mil framework: learnt embedding of the instances (referred to
as patch features), and penultimate layer features (penultimate features).
inspired by fid, we propose a
metric named fr´echet domain distance (fdd) for evaluating if a model is expe-
riencing a drop in performance on some new dataset.
141 wsis from axillary nodes dissection cases (57 wsis with metastases):
potentially large shift as some patients have already started neoadjuvant
treatment as well as the tissue may be aﬀected from the procedure of sentinel
lymph node removal.
2a.
the sentinel/axillary division
is motivated by the diﬀering dl prediction performance on such subsets, as
detecting domain shift in mil
161
observed by jarkman et al.
moreover, discussions with pathologists led to
the conclusion that it is clinically relevant to evaluate the performance diﬀerence
between ductal and lobular carcinoma.
4
experiments
the goal of the study is to evaluate how well fddk and the baseline methods
correlate with the drop in classiﬁcation performance of attention-based mil
caused by several potential sources of domain shifts.
in this section, we describe
the experiments we conducted.
the classiﬁcation perfor-
mance is evaluated using the area under receiver operating characteristic curve
(roc-auc) and matthews correlation coeﬃcient (mcc) [2].
following the con-
clusions of [2] that mcc well represents the full confusion matrix and the fact
that in clinical practice a threshold needs to be set for a classiﬁcation decision,
mcc is used as a primary metric of performance for domain shift analysis while
roc-auc is reported for completeness.
a large diﬀerence
indicates a potential drop in performance.
however, it is not trivial
which hidden features of mil that are most suitable for this task, and we
evaluate several options (see sect. 2.2) with both methods.
162
m. poceviˇci¯ut˙e et al.
for all possible pairs of camelyon and the other test datasets, and for the
10 cv models, we compute the domain shift measures and compare them to
the observed drop in performance.
all results
are reported as mean and standard deviation over the 10-fold cv.
5
results
the ﬁrst part of our results is the performance of the wsi classiﬁcation task
across the subsets, summarized in table 1.
overall, the performance is in
line with previously published work [17,29].
table
1.
classiﬁcation
performance
reported in mean (standard deviation) of
mcc and roc-auc metrics, computed
over the 10-fold cv models.
clam models achieved
better performance on ductal carcinoma compared to the in-domain camelyon
test data.
table 2 summarises the pearson correlation between the change in perfor-
mance, i.e., the mcc diﬀerence between camelyon and other test datasets, and
the domain shift measures for the same pairs.
figure 1 shows how
individual drop in performance of model-dataset combinations are related to the
fdd64 metric.
for most models detecting larger drop in performance (> 0.05)
is easier on axillary lymph nodes data than on any other analysed dataset.
pearson correlations between domain shift measure and diﬀerence in per-
formance (mcc metric) of camelyon test set and the other datasets.
however, the results
were among the worst with any other number of k. both combined and positive
evidence achieved peak performance of 0.68 (0.17) and 0.70 (0.13), respectively,
when fd and k = 64 were used.
we conclude that in our setup the best and
most reliable performance of domain shift quantiﬁcation is achieved by positive
evidence with fd and k = 64, i.e. fdd64.
164
m. poceviˇci¯ut˙e et al.
6
discussion and conclusion
mil is aﬀected by domain shift.
however, as clinically relevant subsets of brln
data are analysed, stark diﬀerences in performance and reliability (indicated
by the standard deviation) are revealed.
the evaluated baselines that use uncer-
tainty and conﬁdence aggregation for domain shift detection, i.e., de and doc,
showed poor ability to estimate the experienced drop in performance (see table
2).
this could be a potential cause for the observed poor results by de and doc
in our experiments.
the highest
pearson correlation between change in performance and a distance metric is
achieved by fr´echet distance with 64 positive evidence features, fdd64 (see
table 2).
thus, it seems a critical component in domain
shift measurement in attention-based mil is to correctly make use of the atten-
tion scores.
from fig. 1 we can see that if we further investigated all model-
dataset combinations that resulted in fdd64 above 0.5, we would detect many
cases with a drop in performance larger than 0.05.
an interesting
aspect is that the performance was better for the out-of-domain ductal subset
compared to in-domain camelyon wsis.
in practical applications, it may be a
problem when the domain shift quantiﬁcation cannot separate between shifts
having positive or negative eﬀect on performance.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_38.pdf:
in this
paper, we present a new method, prime+, for breast cancer risk pre-
diction that leverages prior mammograms using a transformer decoder,
outperforming a state-of-the-art risk prediction method that only uses
mammograms from a single time point.
experimental results show that our model achieves
a statistically signiﬁcant improvement in performance over the state-of-
the-art based model, with a c-index increase from 0.68 to 0.73 (p < 0.05)
on held-out test sets.
keywords: breast cancer · mammogram · risk prediction
1
introduction
breast cancer impacts women globally [15] and mammographic screening for
women over a certain age has been shown to reduce mortality
to
mitigate this, supplemental screening like mri or a tailored screening inter-
val have been explored to add to the screening protocol
recently, several studies [8,32,33] revealed the potential of artiﬁcial intelligence
(ai) to develop a better risk assessment model to identify women who may ben-
eﬁt from supplemental screening or a personalized screening interval and these
may lead to improved screening outcomes.
in clinical practice, breast density and traditional statistical methods for pre-
dicting breast cancer risks such as the gail
several
studies have shown that utilizing past mammograms can improve the classi-
ﬁcation performance of radiologists in the classiﬁcation of benign and malig-
nant masses
[11,25,26,29], especially for the detection of subtle abnormalities
[25]. more recently, deep learning models trained on both prior and current
mammograms have shown improved performance in breast cancer classiﬁcation
tasks [24].
the data comprises three main elements: features x, time of the event
t, and the occurrence of the event e
for medical applications, x typically
represents patient information like age, family history, genetic makeup, and diag-
nostic test results (e.g., a mammogram).
δt
(1)
enhanced risk prediction with prior images
391
fig.
a common backbone network extracts features from
the prior and current images, resulting in xprior and xcurr.
we use an imagenet pre-
trained resnet-34
[12] as the image feature backbone.
the goal of training the
model is to minimize this loss function, which encourages the model to make
accurate predictions of the risk of developing breast cancer over time.
2.3
incorporating prior mammograms
to improve the performance of the breast cancer risk prediction model, we incor-
porate information from prior mammograms taken with the same view, using a
transformer decoder structure
= xcp c ⊕ xcurr, which is
then used by the base hazard network and time-dependent hazard network to
predict the cumulative hazard function ˆh.
enhanced risk prediction with prior images
393
3
experiments
3.1
dataset
we compiled an in-house mammography dataset comprising 16,113 exams
(64,452 images) from 9,113 patients across institutions from the united states,
gathered between 2010 and 2021.
mammograms were captured
using hologic (72.3%) and siemens (27.7%) devices.
the c-
index measures the performance of a model by evaluating how well it correctly
predicts the relative order of survival times for pairs of individuals in the dataset.
time-dependent roc
analysis generates an roc curve and the area under the curve (auc) for each
speciﬁc time point in the follow-up period, enabling evaluation of the model’s
performance over time.
we evaluate the eﬀectiveness of prime+ by comparing it with two other
models: (1) baseline based on mirai, a state-of-the art risk prediction method
from [33], and (2) prime, a model that uses prior images by simply summing
xcurr and xprior without the use of the transformer decoder.
3.3
implementation details
our model is implemented in pytorch and trained on four v100 gpus.
we
trained the model using stochastic gradient descent (sgd) for 20k iterations
with a learning rate of 0.005, weight decay of 0.0001, and momentum of 0.9.
we resize the images to 960 × 640 pixels and use a batch size of 96.
to
augment the training data, we apply geometric transformations such as vertical
ﬂipping, rotation and photometric transformations such as brightness/contrast
adjustment, gaussian noise, sharpen, clahe, and solarize.
empirically, we ﬁnd
that strong photometric augmentations improved the risk prediction model’s
394
h. lee et al.
table 1. ablation analysis on the eﬀectiveness of prior information and transformer
decoder.
all cases
prior decoder c-index
time-dependent auc
1-year
2-year
3-year
4-year
✗
✗
0.68±0.03 0.70±0.05 0.71±0.04 0.70±0.04 0.71±0.09
✓
✗
0.70±0.03 0.72±0.05 0.73±0.05 0.74±0.04 0.75±0.07
✓
✓
0.73±0.03 0.75±0.05 0.75±0.04 0.77±0.04 0.76±0.08
excluding cancer cases with event time < 180 days
prior decoder c-index
time-dependent auc
1-year
2-year
3-year
4-year
✗
✗
0.63±0.04 0.64±0.10 0.66±0.08 0.64±0.06 0.64±0.11
✓
✗
0.68±0.05 0.64±0.14 0.73±0.08 0.70±0.05 0.71±0.09
✓
✓
0.70±0.04 0.68±0.13 0.76±0.07 0.73±0.05 0.71±0.10
performance, while strong geometric transformations had a negative impact.
by using the transformer decoder to jointly model prior images,
we observed improved c-index from 0.70 (0.67 to 0.73) to 0.73 (0.70 to 0.76).
we observe similar performance improvements when evaluating cases with
at least 180 days to cancer diagnosis.
the model must learn patterns of risk, not
enhanced risk prediction with prior images
395
table 2. to better understand why the addition of prior images works, we split our
test set into two groups based on the mammographic density: change and no change.
the ﬁrst and second row corresponds to performance of the baseline and prime+
model, respectively.
our results support this intuition as the performance improvements over the
baseline are much more pronounced for longer term risk (3, 4-year auc) than
short term risk (1 year).
the prime and prime+ models, which incorporate
prior mammograms, show high performance for long-term risk prediction (3,
4-year auc), indicating that considering changes in breast over time contain
useful information for breast cancer risk prediction.
lastly, we empirically conﬁrm that a transformer decoder eﬀectively models
spatial relations between prior and current mammograms by demonstrating con-
sistent performance improvements of prime+ across both short-term and long-
term risk prediction settings.
our results suggest that incorporating changes
in patients using prior mammograms and a transformer decoder improves the
performance of breast cancer risk prediction models.
analysis based on density.
to better understand why adding prior
images improves performance, we divided our test set into subgroups to examine
the performance of the baseline model and the prime+ model on each of these
groups.
women with dense breasts have a four-to six-fold
higher risk of breast cancer
the addition of mammographic breast density
has improved the performance traditional breast cancer risk models
[4] and can
therefore help us understand why the addition of prior images works.
density change was deﬁned according to whether
the bi-rads category changed in the current image as compared to the prior
396
h. lee et al.
table 3.
in order to assess the performance of the models on varying levels of breast
density, a critical risk factor, we divided our test set into two groups based on mam-
mographic density: fatty and dense.
0.78±0.05 0.76±0.08
dense
0.68±0.06 0.66±0.09 0.68±0.09 0.71±0.08 0.65±0.21
0.71±0.05 0.72±0.08 0.73±0.08 0.72±0.08 0.72±0.25
image.
we suspect this is because deep neural networks generally work better on
low density images given that visual cues of cancer in images with lower breast
density are more clearly visible.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_11.pdf:
experimental results on
lung (83 cts from 19 patients) and liver (77 cects from 18 patients) datasets
with more than two scans per patient yielded an individual lesion change class
accuracy of 98% and 85%, and identiﬁcation of patterns of lesion change with
an accuracy of 96% and 76%, respectively.
keywords: longitudinal follow-up · lesion matching · lesion change analysis
1
introduction
the periodic acquisition and analysis of volumetric ct and mri scans of oncology
patients is essential for the evaluation of the disease status, the selection of the treat-
ment, and the response to treatment.
currently, scans are acquired every 2–12 months
according to the patient’s characteristics, disease stage, and treatment regime.
the scan
interpretation consists of identifying lesions (primary tumors, metastases) in the affected
supplementary information the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43904-9_11.
as
treatments improve and patients live longer, the number of scans in longitudinal studies
increases and their interpretation is more challenging and time-consuming.
the recist 1.1 guidelines call for ﬁnding new lesions (if any), identify-
ing up to the ﬁve largest lesions in each scan in the ct slice where they appear largest,
manually measuring their diameters, and comparing their difference [1].
while volu-
metric measures of individual lesions and of all lesions (tumor burden) have long been
established as more accurate and reliable than partial linear measurements, they are not
used clinically because they require manual lesion delineation and lesion matching in
unregistered scans, which is usually time-consuming and subject to variability [2].
in a previous paper, we presented an automatic pipeline for the detection and quantiﬁ-
cation of lesion changes in pairs of ct liver scans
complex lesion changes include merged lesions, which occurs when at least two lesions
grow and merge into one (possible disease progression), split lesions, which occurs
when a lesion shrinks and cleaves into several parts (possible response to treatment) and
conglomeration of lesions, which occurs when clusters of lesions coalesce.
com-
prehensive quantitative analysis of lesion changes and patterns is of clinical importance,
since response to treatment may vary among lesions, so the analysis of a few lesions
may not be representative.
experimental results on lung (83 cts, 19 patients) and liver (77 cects, 18 patients)
datasets show that our method yields high classiﬁcation accuracy.
although many methods exist for object tracking in optical images
and videos [15–17], they are unsuited for analyzing lesion changes since they assume
many consecutive 2d images where objects have very similar appearance and undergo
small changes between images.
overlap-based methods pair two lesions in registered
scans when their segmentations overlap, with a reported accuracy of 66–98% [3, 5–11,
108
b. di veroli et al.
18].
they are limited to 2d images, assume registration between images, and do not handle
conglomerate changes.
lesion matchings are computed with an overlap-based
lesion pairing method after establishing a common reference frame by deformable regis-
tration of the scans and organ segmentations.
graph-theoretic automatic lesion tracking and detection
109
the method inputs the scans and the organ and lesion segmentations in each scan.
the method is a pipeline of four steps: 1) pairwise
deformable registration of each prior scan, organ and lesion segmentations, with the
most recent (current) scan as in [3]; 2) overlap-based lesion matching; 3) construction of
the lesion change graph from the individual lesion segmentations and lesion matches; 4)
detection of changes in individual lesions and patterns of lesion changes from the graph
properties and from analysis of its connected components.
2.1
problem formalization
let s =

s1, . . .
, vi
ni

is a set of vertices vi
j corresponding to the lesions associated
with the lesion segmentation masks li =

li
1, li
2, . .
the lesion matching rule is lesion voxel overlap: when
the lesion segmentation voxels li
j, lk
l of vertices vi
j, vk
l overlap, 1 ≤
consecutivelesionmatchingonscans(si, si+1)isperformedwithaniterativegreedy
strategy whose aim is to compensate for registration errors: 1) the lesion segmentations
in li and li+1 are isotropically dilated in 3d by d millimeters; 2) for all pairs of lesions

vi
j, vi+1
l

, compute the intersection % of their corresponding lesion a segmentations
(li
j, li+1
l
) as max(



li
j ∩ li+1
l



/



li
j



,



li
j ∩ li+1
l



/



li+1
l



); ; 3) if the % intersection is ≥ p, then
edge ei,i+1
j,l
=

vi
j, vi+1
l

is added to ec; ; 4) remove the lesion segmentations li
j, li+1
l
from li, li+1, respectively.
3
experimental results
we evaluated our method with two studies on retrospectively collected patient datasets
that were manually annotated by an expert radiologist.
we ran our method on the dlungs and dliver lesion segmentations.
the
settings of the parameters were: dilation distance d = 1 mm, overlap percentage p =
10%, number of iterations r = 5 and 7, and centroid maximum distance δ = 17 and
23 mm for the lungs and liver lesions, respectively.
these patterns are hard
to detect manually but their correct classiﬁcation and tracking are crucial for the proper
application of the recist 1.1 follow-up protocol
see the supplemental
material for examples of these scenarios.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_62.pdf:
experimen-
however, recognizing important diagnostic features from ceus videos
to diagnose lesions as benign or malignant is non-trivial and requires lots of
experience.
to improve diagnostic eﬃciency and accuracy, many computational methods
were proposed to analyze renal us images and could assist radiologists in making
clinical decisions [6].
however, most of these methods only focused on conven-
tional b-mode images.
in recent years, there has been increasing interest in
multi-modal medical image fusion [1].
weight-based fusion
methods generally used an importance prediction module to learn the weight
of each modality and then performed sum, replacement, or exchange based on
the weights [7,16,17,19].
nevertheless, we prove in our experiments that these attention-
based methods may have the potential risks of entangling features of diﬀerent
modalities.
previous researches
have proved that temporal information is eﬀective in improving the performance
of deep learning models.
lin et al.[11] proposed a network for breast lesion
detection in us videos by aggregating temporal features, which outperformed
other image-based methods.
[2] showed that ceus videos can provide
more detailed blood supply information of tumors allowing a more accurate
breast lesion diagnosis than static us images.
experimental results
show that the proposed framework outperforms single-modal, single-frame, and
other state-of-the-art methods in renal tumor diagnosis.
it can be divided
into two stages: single-frame detection stage and video-based diagnosis stage.
(1)
in the single-frame detection stage, the network predicts the tumor bounding box
and category on each frame in the multi-modal ceus video clips.
during the diagnostic process, experi-
enced radiologists usually take the global features of us images into considera-
tion [20].
(2) in the video-based diagnosis stage, the network
automatically chooses high-conﬁdence region features of each frame according to
the single-frame detection results and performs temporal aggregation to output
a more accurate diagnosis.
the above two stages are trained successively.
we
ﬁrst perform a strong data augmentation to train the network for tumor detec-
tion and classiﬁcation on individual frames.
after that, the ﬁrst stage model is
switched to the evaluation mode and predicts the label of each frame in the video
clip.
2.2
dual-attention strategy for multimodal fusion
using complementary information between multi-modal data can greatly
improve the precision of detection.
the process
mentioned above can be formulated as follows:
finvar = softmax(qbkt
c
√
d
)vc + softmax(qckt
b
√
d
)
therefore, we design an ota module that
aggregates single-frame renal tumor detection results in temporal dimension for
diagnosing tumors as benign and malignant.
after feature selection, we aggregate the features in the temporal
dimension by time attention.
3
experimental results
3.1
materials and implementations
we collect a renal tumor us dataset of 179 cases from two medical centers,
which is split into the training and validation sets.
we further collect 36 cases
from the two medical centers mentioned above (14 benign cases) and another
center (fujian provincial hospital, 22 malignant cases) to form the test set.
some
examples of the images are shown in fig.
2. there is an obvious visual diﬀerence
between the images from the fujian provincial hospital (last column in fig.
more than two radiologists with ten
years of experience manually annotate the tumor bounding box and class label at
the frame level using the pair annotation software package (https://www.aipair.
the number of cases and annotated frames is summarized in table 1.
weights pre-trained from imagenet are used to initialize the swin-
transformer backbone.
data augmentation strategies are applied synchronously
to b-mode and ceus-mode images for all experiments, including random rota-
tion, mosaic, mixup, and so on.
the
weight decay is set to 0.0005 and the momentum is set to 0.9.
all
experiments are implemented in pytorch with an nvidia rtx a6000 gpu.
ap50 and ap75 are used to assess the performance of single-frame detection.
2. examples of the annotated b-mode and ceus-mode us images.
3.2
ablation study
single-frame detection.
as shown in table 2, using
swin-transformer as the backbone in yolox achieves better performance than
the original backbone while reducing half of the parameters.
the improvement
may stem from the fact that swin-transformer has a better ability to characterize
global features, which is critical in us image diagnosis.
however, “ca+sa” (row 6 in table 2) obtains inferior
performance than “ca” (row 5 in table 2).
we conjecture that connecting the two
attention modules in series leads to the entanglement of modality-speciﬁc and
modality-invariant information, which would disrupt the model training.
therefore, the proposed method
achieves the best performance.
we investigate the performance of the ota module
for renal tumor diagnosis in multi-modal videos.
this
suggests that the multi-frame model can provide a more comprehensive char-
acterization of the tumor and thus achieves better performance.
meanwhile,
increasing the sampling interval tends to decrease the performance (row 4 and
row 5 in table 3).
this proves that complementary informa-
tion exists among diﬀerent modalities.
for a fair comparison with other fusion
methods, we embed their fusion modules into our framework so that diﬀerent
approaches can be validated in the same environment.
this may be because
the generated weights are biased to make similar decisions to the source domain,
thereby reducing model generalization in the external data.
tmm focuses on both modality-speciﬁc and
modality-invariant information, but the chaotic confusion of the two types of
information deteriorates the model performance.
on the contrary, our amf
module prevents information entanglement by conducting cross-attention and
self-attention blocks in parallel.
meanwhile,
the improvement of the detection performance is beneﬁcial to our ota mod-
ule to obtain lesion features from more precise locations, thereby improving the
accuracy of benign and malignant renal tumor diagnosis.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_61.pdf:
accurate segmentation of polyps is a crucial step in the
eﬃcient diagnosis of colorectal cancer during screening procedures.
to address these
limitations, we propose a novel feature enhancement network that lever-
ages feature propagation enhancement and feature aggregation enhance-
ment modules for more eﬃcient feature fusion and multi-scale feature
propagation.
speciﬁcally, the feature propagation enhancement module
transmits all encoder-extracted feature maps from the encoder to the
decoder, while the feature aggregation enhancement module performs
feature fusion with gate mechanisms, allowing for more eﬀective infor-
mation ﬁltering.
the multi-scale feature aggregation module provides
rich multi-scale semantic information to the decoder, further enhanc-
ing the network’s performance.
keywords: polyp segmentation · feature propagation · feature
aggregation
1
introduction
colorectal cancer is a life-threatening disease that results in the loss of millions
of lives each year.
hence, regular bowel screenings are recommended, where
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
https://doi.org/10.1007/978-3-031-43904-9_61
revisiting feature propagation and aggregation in polyp segmentation
633
endoscopy is the gold standard.
to reduce
the workload on physicians and enhance diagnostic accuracy, computer vision
technologies, such as deep neural networks, are involved to assist in the pre-
segmentation of endoscopic images.
the unet-like model uses skip
connections that transmit only single-stage features.
in contrast, our approach utilizes
fpe to propagate features from all stages, incorporating a gate mechanism to regulate
the ﬂow of valuable information.
deep learning-based image segmentation methods have gained popularity in
recent years, dominated by unet
[11] in the ﬁeld of medical image segmenta-
tion.
unet’s success has led to the development of several other methods that
use a similar encoder-decoder architecture to tackle polyp segmentation, includ-
ing resunet++
however, these
methods are prone to ineﬃcient feature fusion at the decoder due to the trans-
mission of multi-stage features without ﬁltering out irrelevant information.
to address these limitations, we propose a novel feature enhancement net-
work for polyp segmentation that employs feature propagation enhancement
(fpe) modules to transmit multi-scale features from all stages to the decoder.
figure 1 illustrates a semantic comparison of our feature propagation scheme
with the unet-like model.
while the existing unet-like models use skip connec-
tions to propagate a single-scale feature, our method utilizes fpe to propagate
multi-scale features from all stages in encoder.
more importantly, this research
highlights the usage of fpe can eﬀectively replace skip connections by providing
more comprehensive multi-scale characteristics from full stages in encoder.
to
further address the issue of high-level semantics being overwhelmed in the pro-
gressive feature fusion process, we also integrate a feature aggregation enhance-
ment (fae) module that aggregates the outputs of fpe from previous stages at
634
y. su et al.
decoder.
finally, we propose a multi-scale aggregation (msa) module appended
to the output of the encoder to capture multi-scale features and provide the
decoder with rich multi-scale semantic information.
the msa incorporates a
cross-stage multi-scale feature aggregation scheme to facilitate the aggregation
of multi-scale features.
overall, our proposed method improves upon existing
unet-like encoder-decoder architectures by addressing the limitations in feature
propagation and feature aggregation, leading to improved polyp segmentation
performance.
our major contributions to accurate polyp segmentation are summarized as
follows.
(1) the method addresses the limitations of the unet-like encoder-decoder
architecture by introducing three modules: feature propagation enhance-
ment (fpe), feature aggregation enhancement (fae), and multi-scale
aggregation (msa).
(2) fpe transmits all encoder-extracted feature maps to the decoder, and fae
combines the output of the last stage at the decoder and multiple outputs
from fpe.
(3) the proposed method achieves state-of-the-art results in ﬁve polyp segmen-
tation datasets and outperforms the previous cutting-edge approach by a
large margin (3%) on cvc-colondb and etis datasets.
our proposed feature enhancement network illustrated in fig.
[16] pretrained on imagenet as the encoder.
the decoder consists of
three feature aggregation enhancement modules (fae) and a multi-scale aggre-
gation module (msa).
given an input image i, we ﬁrst extract the pyramidal
features using the encoder, which is deﬁned as follows,
p1, p2, p3, p4 = pvt(i)
(1)
where, {p1, p2, p3, p4} is the set of pyramidal features from four stages with the
spatial size of 1/4, 1/8, 1/16, 1/32 of the input respectively.
features with lower
spatial resolution usually contain richer high-level semantics.
then, these fea-
tures are transmitted by the feature propagation enhancement module (fpe) to
yield the feature set {c1, c2, c3, c4}, which provides multi-scale information from
all the stages.
this is diﬀerent from the skip connection which only transmits the
single-scale features at the present stage.
afterwards, feature fusion is performed by
revisiting feature propagation and aggregation in polyp segmentation
635
(a) overall architecture
(b) fpe
(c) fae
(d) msa
fig.
(a) overall architecture; (b) fpe:
feature propagation enhancement module; (c) fae: feature aggregation enhancement
module; (d) msa: multi-scale aggregation module
fae in the decoder, whereby it progressively integrates the outputs from fpe
and previous stages.
the higher-level semantic features of the fpe output are
capable of eﬀectively compensating for the semantics that may have been over-
whelmed during the upsampling process.
feature propagation enhancement module.
in contrast to the traditional
encoder-decoder architecture with skip connections, the fpe aims to transmit
multi-scale information from full stage at the encoder to the decoder, rather than
single-scale features at the current stage.
the input of the fpe includes the features from the other three stages,
in addition to the feature of the current stage, which delivers richer spatial and
semantic information to the decoder.
however, these multi-stage inputs need to be downsampled or upsampled to
match the spatial resolution of the features at the present stage.
the features from the other three stages, denoted as p1, p2, and p3, are
downsampled or upsampled to generate p
′
1, p
′
2, and p
′
3.
fpe
leverages such gate mechanism to obtain informative features in p and passes
them through a cu respectively.
feature aggregation enhancement module.
the fae is a novel approach
that integrates the outputs of the last stages at the decoder with the fpe’s
outputs at both the current and deeper stages to compensate for the high-level
semantics that may be lost in the process of progressive feature fusion.
in contrast
to the traditional encoder-decoder architecture with skip connections, the fae
assimilates the output of the present and higher-stage fpes, delivering richer
spatial and semantic information to the decoder.
the fae, depicted in fig. 2(c), integrates the outputs of the current and
deeper fpe stages with high-level semantics.
as an example, the last fae takes
as inputs o2 (output of the penultimate fae), c1 (output of the current fpe
stage), and c2 and c3 (outputs of fpe from deeper stages).
multiple outputs
from deeper fpe stages are introduced to compensate for high-level seman-
tics.
furthermore, gate mechanisms are utilized to ﬁlter out valueless features
revisiting feature propagation and aggregation in polyp segmentation
637
for fusion, and the resulting enhanced feature is generated by a cu after con-
catenating the ﬁltered features.
finally, o2 is merged with the output feature
through element-wise summation, followed by a cu to produce the ﬁnal output
feature o1.
[8], and
etc. by aiding in forming a coarse location of the polyp and contributing to
improved accuracy and performance.
3
experiments
datasets.
we conduct extensive experiments on ﬁve polyp segmentation
datasets, including kvasir [6], cvc-clinicdb
following the setting in [2,3,5,10,10,17,19], the model
is trained using a fraction of the images from cvc-clinicdb and kvasir, and its
performance is evaluated by the remaining images as well as those from cvc-t,
cvc-colondb, and etis.
in particular, there are 1450 images in the training
set, of which 900 are from kvasir and 550 from cvc-clinicdb.
the test set
contains all of the images from cvc-t, cvc-colondb, and etis, which have
60, 380, and 196 images, respectively, along with the remaining 100 images from
kvasir and the remaining 62 images from cvc-clinicdb.
implementations.
we utilize pytorch 1.10 to run experiments on an nvidia
rtx3090 gpu.
we adopt the same data augmentation techniques as uacanet [8],
including random ﬂip, random rotation, and color jittering.
in evaluation phase,
we mainly focus on mdice, miou, the two most common metrics in medical image
638
y. su et al.
segmentation, to evaluate the performance of the model.
according to the experimental
settings, the results on cvc-clinicdb and kvasir demonstrate the learning abil-
ity of the proposed model, while the results on cvc-t, cvc-colondb, and etis
demonstrate the model’s ability for cross-dataset generalization.
the experimen-
tal results are listed in table.1.
furthermore, our proposed method demonstrates strong cross-dataset gener-
alization capability on cvc-t, cvc-colondb, and etis datasets, with partic-
ularly good performance on the latter two due to their larger and more represen-
tative datasets.
these results validate the eﬀectiveness of feature-level enhancement and high-
light the superior performance of our method.
we carried out ablation experiments to verify the eﬀective-
ness of the proposed fpe, fae, and msa.
for our baseline, we use the sim-
ple encoder-decoder structure with skip connections for feature fusion and per-
form element-wise summation at the decoder.
table 2 presents the results of our
ablation experiments.
following the ablation study conducted on our proposed
revisiting feature propagation and aggregation in polyp segmentation
639
approach, it is with conﬁdence that we assert the signiﬁcant contribution of
each module to the overall performance enhancement compared to the baseline.
our results indicate that the impact of each module on the ﬁnal performance
is considerable, and their combination yields the optimal overall performance.
for
miou, the improvements are 1.5% and 3.5% on the corresponding datasets.
3. exemplary images and results that are segmented by diﬀerent approaches.
4
conclusion
we introduce a new approach to polyp segmentation that addresses ineﬃcient
feature propagation in existing unet-like encoder-decoder networks.
speciﬁcally,
640
y. su et al.
a feature propagation enhancement module is introduced to propagate multi-
scale information over full stages in the encoder, while a feature aggregation
enhancement module is attended at the decoder side to prevent the loss of
high-level semantics during progressive feature fusion.
experimental results on ﬁve popular polyp
datasets demonstrate the eﬀectiveness and superiority of our proposed method.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_75.pdf:
to reduce false posi-
tives, we identify three challenges: (1) unlike natural images, a malignant
mammogram typically contains only one malignant ﬁnding; (2) mam-
mography exams contain two views of each breast, and both views ought
to be considered to make a correct assessment; (3) most mammograms
are negative and do not contain any ﬁndings.
in this work, we tackle the
three aforementioned challenges by: (1) leveraging sparse r-cnn and
showing that sparse detectors are more appropriate than dense detectors
for mammography; (2) including a multi-view cross-attention module
to synthesize information from diﬀerent views; (3) incorporating multi-
instance learning (mil) to train with unannotated images and perform
breast-level classiﬁcation.
we validate m&m’s detec-
tion and classiﬁcation performance using ﬁve mammography datasets.
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43904-9_75.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
(b) quantitative detection evaluation with
and without negative images on optimam.
retina
net
fcos
faster
r-cnn
cascade
r-cnn
m&m
30
40
50
60
-26.9
-24.3
-25.4
-23.0
-3.5
average precision (ap)
without negative
with negative
fig.
(a) few
works report detailed performance in the clinically relevant region of less than 1
fp/image.
across all dense mod-
els, there is a large performance drop in the clinically representative setting that includes
negative images.
this means that the dense models are producing too many fps on neg-
ative images.
our model, m&m, successfully tackles this performance gap.
rate of false positive (fp) predictions of cad can cause a signiﬁcant reduc-
tion in radiologists’ speciﬁcity
1a, most works focus on reporting
recalls outside the clinically relevant region of less than 1 fp/image.
to tackle the high rate of false positives in mammography, we identify three
challenges: (1) a malignant mammogram typically contains only one malignant
ﬁnding.
this is diﬀerent from natural images: for example, an image in coco
contains on average 7.7 objects [11].
this calls into question the usage of dense
detectors for mammography; (2) a standard screening exam consists of two views
per breast.
however, excluding negative
images from training and evaluation leads to a distribution shift since negative
images are abundant in clinical practice.
to achieve these goals,
m&m leverages three components: (1) sparse r-cnn to replace dense anchors
with a set of sparse proposals; (2) multi-view cross-attention to synthesize
780
y. n. truong vu et al.
information from two views and iteratively reﬁne the predictions, and (3) multi-
instance learning (mil) to include negative images during training.
with sparse r-cnn,
m&m generalizes better to clinically-representative data, where the majority
of images are negative, i.e., have no ﬁndings (table 2);
with multi-view reasoning, m&m
improves the recall at 0.1 fp/image by 8.6%, as shown in fig.
we leverage mil to include images without bounding boxes during training
(sec. 2.3).
accordingly, m&m sees seven times more images during training.
with mil, m&m improves the recall at 0.1 fp/image by 12.6% (fig. 4).
1b, dense detectors generalize poorly to negative images as they
produce too many false positives.
m&m tackles false positives through (1, blue, dotted arrows) leveraging the
sparse r-cnn cascade architecture to iteratively reﬁne sparse learnable proposals into
predictions, (2, red, solid arrows) incorporating a cross-attention module to reason
about relations between objects across two views, and (3, green, dashed arrows) uti-
lizing image and breast mil pooling to train with images that do not have lesion
annotations.
yet, a model
generalizes poorly if these negative images are dropped during training (fig. 1b).
since image- and breast-level labels are available, we adopt an mil module
to include images without bounding boxes during training.
to compute image-
and breast-level scores, we leverage the proposal malignancy logits mi (eq.
since an image is malignant if it contains a malignant lesion, we obtain image-
level scores by applying the noisyor function f(x)
next, as cc and mlo views
oﬀer complimentary information on a breast, we obtain breast-level malignancy
score by averaging the image-level scores across these views.
we apply cross-entropy losses limage and lbreast at the image and breast
level for all training samples.
we thus obtain the following total training loss for m&m:
l = 1annotated lesionllesion + 0.5limage + 0.5lbreast.
(5)
3
experiments
implementation details.
we resize the images’ shorter edges to 2560 with the larger edges no longer than
3328.
each batch contains 16 breasts (32 images).
we employ a 1:1
sampling ratio between unannotated and annotated images.
we utilize three 2d digital mammography datasets: (1) optimam :
a development dataset derived from the optimam database
δ denotes the ap gap
between evaluating with and without negative images.
we report average precision with intersection over union from 0.25
to 0.75.
apmb denotes average precision on the set of annotated malignant and
benign images.
ap denotes average precision when all data is included.
we
report free response operating characteristic (froc) curves and recalls at var-
ious fp/image (r@t).
[31]
0.735
phresnet50 [14]
0.739
cross-view transformer [27]
0.803∗
m&m (ours)
0.883
23 points (pt) between excluding and including negative images.
large δ means
the models are producing too many fps on negative images.
with this performance gap closed, m&m is
able to achieve a high recall of 87.7% at just 0.1 fp/image.
for all
models, the breast-level score is the average of the cc score and mlo score,
while the exam-level score is the max of the left breast score and right breast
score.
4. eﬀect of m&m’s components on classiﬁcation and detection performance.
0.08–0.12 exam auc when evaluated on inhouse-a and inhouse-b. in compar-
ison, m&m has smaller performance gaps of 0.02 on inhouse-a and 0.04 on
inhouse-b. similar observations for other classiﬁers, such as eﬃcientnet, are
reported in the appendix.
on the left, we demonstrate how each component of m&m con-
tributes to closing the gap δ between evaluating with and without negative
images.
mil allows the model to train with
signiﬁcantly more negative images, reducing δ to −3.6pt (row 4).
on the right
of fig. 4, the froc curves show how each component of m&m improves recall
signiﬁcantly at low fp/image.
in particular, m&m’s recall at 0.1fp/image is
86.3%, +21.2% over vanilla sparse r-cnn.
further studies.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_8.pdf:
liver tumor segmentation and classiﬁcation are important
tasks in computer aided diagnosis.
it uses a mask transformer to jointly segment
and classify each lesion with improved anchor queries and a foreground-
enhanced sampling loss.
it also has an image-wise classiﬁer to eﬀectively
aggregate global information and predict patient-level diagnosis.
on contrast-
enhanced ct, our lesion-level detection precision, recall, and classiﬁca-
tion accuracy are 92%, 89%, and 86%, outperforming widely used cnn
and transformers for lesion segmentation.
plan is on par with a senior human radi-
ologist, showing the clinical signiﬁcance of our results.
keywords: liver tumor · lesion segmentation and classiﬁcation · ct
1
introduction
liver cancer is the third leading cause of cancer death world-wide in 2020
early detection and accurate diagnosis of liver tumors may improve overall
partially supported by the national natural science foundation of china (grant
82071885), basic research projects of liaoning provincial department of education
(ljkmz20221160), the national youth talent support program of china, and science
and technology innovation talent project in shenyang (rc210265).
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43904-9 8.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
[18] and
esophagus [20] have shown that latest deep learning techniques can detect subtle
texture and shape changes in nc ct that even human eyes may miss.
thus,
we aim to investigate the performance of liver tumor segmentation and classiﬁ-
cation in nc cts.
after an incidental
tumor is found, the patient may undergo further imaging examination such as
a multi-phase dce ct for diﬀerential diagnosis [11], which can provide useful
discriminative information such as the vascularity of lesions and the pattern of
contrast agent enhancement [19].
liver is largest solid organ in body and is the
site of many tumor types [11].
therefore, accurate tumor type classiﬁcation is
important for the decision of treatment plans and prognosis.
many researchers have developed algorithms to automatically segment [1,9,
13,15,23] or classify [19,21,25] liver tumors in ct to help radiologists improve
their accuracy and eﬃciency.
for example, public datasets such as the liver
tumor segmentation benchmark (lits)
[1] fostered a series of works aiming to
segment liver tumors with improved convolutional neural network (cnn) back-
bones
besides
lesion segmentation, cnn-based lesion classiﬁcation algorithms have been stud-
ied to distinguish common lesion types [19,21,25].
in this paper, we build a comprehensive framework to address both tumor
screening and diagnosis.
most existing works in tumor
segmentation and detection did not explicitly consider it since their training and
testing images are all tumor patients.
such models may generate false positives
in real-world screening scenario when facing diverse tumor-free images.
(2) most works studied liver tumor segmentation alone without
diﬀerentiating tumor types, while a few works classify liver tumors on cropped
tumor patches
meanwhile, we learn tumor segmentation and classiﬁ-
cation with one network using an instance segmentation framework [3].
(3) for evaluation,
previous segmentation works typically use pixel-level metrics such as dice coef-
ﬁcient.
such metrics cannot reﬂect the lesion-level accuracy (how many lesion
instances are correctly detected and classiﬁed) and may bias to large lesions when
a patient has multiple tumors.
patient-level metrics (e.g. classifying whether a
subject has malignant tumors) are also useful for treatment recommendation
in clinical practice [18,20].
algorithms for liver tumor segmentation have focused on improving the fea-
ture extraction backbone of a fully-convolutional cnn [9,13,15,23].
the pixel-
wise segmentation architectures may not be optimal for lesion and patient-level
evaluation metrics since they cannot consider a lesion or an image holistically.
[3,4,17] have emerged in the
computer vision community and achieved the state-of-the-art performance in
instance segmentation tasks.
in brief, they use object queries to interact with
image feature maps and with each other to produce mask and class predictions
for each instance.
inspired by them, we propose a novel end-to-end framework
named pixel-lesion-patient network (plan) for lesion segmentation and classi-
ﬁcation, as well as patient classiﬁcation.
it contains three branches with bottom-
up cooperation: the segmentation map from the pixel branch helps to initialize
the lesion branch, which is an improved mask transformer aiming to segment and
classify each lesion; the patient branch aggregates information from the whole
image and predicts image-level labels of each lesion type, with regularization
terms to encourage consistency with the lesion branch.
on the non-contrast tumor screening
and diagnosis task, plan achieves 95.0%, 96.4%, and 0.965 in patient-level sen-
sitivity, speciﬁcity, and average auc for malignant and benign patients, in con-
trast to 94.4%, 93.7%, and 0.889 for the widely-used nnu-net [8].
2
method
2.1
preliminary on mask transformer
mask transformers are a series of latest works achieving superior accuracy
on various segmentation tasks
[3,4,17,22]. diﬀerent from traditional fully-
convolutional segmentators [8] that predict a class label for each pixel, mask
transformers predict a class label and a binary mask for each object.
the image, where m is the embedding dimension, d × h × w is the shape
of the 3d image.
they are processed by a transformer
decoder to interact with multi-scale image features and each other using cross
and self-attention operations.
multiplying qi with p gives the
binary mask mi ∈ rd×h×w of object i. during inference, the class and mask
predictions of all queries can be merged by matrix multiplication to obtain the
ﬁnal semantic segmentation result ˆy ∈ rc×d×h×w .
mask transformers have various advantages when applied to our task.
therefore, we pioneer mask transformers’ adaptation
for lesion segmentation and classiﬁcation in 3d medical images.
given a ground-
truth or a predicted lesion mask image, we perform connected component (cc)
analysis and treat each cc as a lesion instance for training and evaluation.
76
k. yan et al.
2.2
pixel-lesion-patient network (plan)
our goal is to segment the mask and classify the type of each tumor in a liver ct.
[3] with three key improvements: (1) a pixel branch is added to
provide anchor queries to the lesion branch.
(2) the lesion branch is composed
of the transformer decoder in mask2former, and we improve its segmentation
loss to enhance recall of small lesions.
(3) a patient branch is attached to make
dedicated image-level predictions with a proposed lesion-patient consistency loss.
the pixel branch is a convolutional
layer after the pixel decoder and learns to predict pixel-wise segmentation maps
similar to traditional segmentators.
we do cc analysis to the predicted mask
to extract lesion instances, and then average the pixel embeddings inside each
predicted lesion to obtain a feature vector.
compared to the random queries in the original mask2former, the
anchor queries contain prior information of the lesions to be segmented, helping
the lesion branch to match with the lesion targets more easily
1. mask2former calculates its segmentation loss on k sam-
pled pixels instead of on the whole image, which is shown to both improve
accuracy and reduce gpu memory usage [3].
however, in lesion segmentation,
some tumors are very small compared to the whole 3d image.
the importance
sampling strategy [3] can hardly select any foreground pixels in such cases, so
the loss only contains background pixels, degrading the segmentation recall of
small lesions.
a patient-level diagnosis is useful for triage.
for example,
diagnosing the subject as normal, benign, or malignant will result in completely
diﬀerent treatments [24].
intuitively, we can also infer patient-level labels from
segmentation results by checking if there is any lesion in the predicted mask.
since one patient can have
multiple liver tumors of diﬀerent types, in our problem, we give each image
several hierarchical binary labels.
the ﬁrst label classiﬁes normal and tumor
subjects (whether the image contains any tumor); the second and third labels
indicate the existence of respectively benign and malignant tumors; the rest
c
we employ
the dual-path transformer block [17] to fuse multi-scale features from the pixel
encoder and decoder to generate a feature map, followed by global average pool-
ing and a linear classiﬁcation layer to predict the c + 3 labels.
liver tumor screening and diagnosis in ct
77
a lesion-patient consistency loss is further proposed to encourage coher-
ence of the lesion and patient-level predictions.
the overall loss of plan is listed in eq. 1, where lpixel is the combined cross-
entropy (ce) and dice loss for the pixel branch as in nnu-net [8]; llesion-class
is the ce loss [3] for lesion classiﬁcation in the lesion branch; llesion-mask is
the combined ce and dice loss [3] for binary lesion segmentation in the lesion
branch with the foreground-enhanced sampling strategy; lpatient is the binary
ce loss for the multi-label classiﬁcation task in the patient branch.
(1)
3
experiments
data.
eight tumor types are considered in our study: hepa-
tocellular carcinoma (hcc), intrahepatic cholangiocarcinoma (icc), metastasis
(meta), hepatoblastoma (hepato), hemangioma (heman), focal nodular hyper-
plasia (fnh), cyst, and others (all other tumor types).
if a lesion’s type cannot
be determined according to image signs [11] and pathology, it will be marked
as “unknown” and ignored in training and evaluation.
detailed
statistics and examples of the lesions are shown in the supplementary material.
implementation details.
we ﬁrst train an nnu-net on public datasets to segment liver and surround-
ing organs (gallbladder, hepatic vein, spleen, stomach, and pancreas), and then
crop the liver region to train plan.
to help plan diﬀerentiate liver tumors
and other organs, we train the network to segment both tumors and organs
patient-level performance on the test set of 500 cases.
2
malignant
benign
8-class average
nnu-net [8]
94.4
95.1
91.0
0.948
0.829
0.863
mask2former
the number of random queries is
q = 20; the embedding dimension is m = 64; the number of sampled pixels is
k = 12544
for dce ct, the three phases form a 3-channel image as
the network input.
extensive data augmentation is applied including random
cropping, scaling, ﬂipping, elastic deformation, and brightness adjustment
among the
8 tumor types, hcc, icc, meta, and hepato are malignant; heman, fnh, and
cyst are benign.
the nc test set contains 198 tumor cases, 202
completely normal cases, and 100 “hard” non-tumor cases which may have larger
image noise, artifact, ascites, diﬀuse liver diseases such as hepatitis and steatosis.
these cases are used to test the robustness of the model in real-world screening
scenario with diverse tumor-free images.
as displayed in
table 1, plan achieves the best accuracy on all tasks, especially in nc pre-
liminary diagnosis tasks, which demonstrates the eﬀectiveness of its dedicated
patient branch that can explicitly aggregate features from the whole image.
lesion and pixel-level results.
liver tumor screening and diagnosis in ct
79
table 2. lesion-level performance (precision, recall, recall of lesions with diﬀerent
radius, classiﬁcation accuracy of 8 tumor types), and pixel-level performance (dice per
case).
2. roc curve of our method versus
2 radiologists’ performance.
lesions smaller than 3 mm in radius are ignored.
although nc images have low contrast, they
can still be used to segment and classify lesions with ∼ 80% precision, recall, and
classiﬁcation accuracy.
it can be seen that
our proposed anchor queries produced by the pixel branch, fes loss, and lesion-
patient consistency loss are useful for the ﬁnal performance.
due to space limit, we will show the accuracy for each
tumor type and more qualitative examples in the supplementary material.
comparison with literature.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_9.pdf:
generic detectors perform below expectations on gld
tasks for 2 reasons: 1) the scale of labeled data of gld datasets is far
smaller than that of natural-image object detection datasets.
2) gastro-
scopic images exhibit distinct diﬀerences from natural images, which are
usually of high similarity in global but high diversity in local.
such char-
acteristic of gastroscopic images also degrades the performance of using
generic self-supervised or semi-supervised methods to solve the labeled
data shortage problem using massive unlabeled data.
in this paper, we
propose self- and semi-supervised learning (ssl) for gld tailored for
using massive unlabeled gastroscopic images to enhance gld tasks per-
formance, which consists of a hybrid self-supervised learning (hsl)
method for backbone pre-training and a prototype-based pseudo-label
generation (ppg) method for semi-supervised detector training.
the
hsl combines patch reconstruction with dense contrastive learning to
boost their advantages in feature learning from massive unlabeled data.
moreover, we contribute the ﬁrst large-scale
gld datasets (lgldd), which contains 10,083 gastroscopic images
with 12,292 well-annotated boxes for four-category lesions.
experiments
on lgldd demonstrate that ssl can bring signiﬁcant improvement.
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43904-9_9.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
although deep neural network-based object detectors
achieve tremendous success within the domain of natural images, directly train-
ing generic object detectors on gld datasets performs below expectations for
two reasons: 1) the scale of labeled data in gld datasets is limited in compar-
ison to natural images due to the annotation costs.
though gastroscopic images
are abundant, those containing lesions are rare, which necessitates extensive
image review for lesion annotation.
2) the characteristic of gastroscopic images
exhibits distinct diﬀerences from the natural images [18,19,21] and is often of
high similarity in global but high diversity in local.
speciﬁcally, each type of
lesion may have diverse appearances though gastroscopic images look quite sim-
ilar.
generic self-supervised backbone pre-training or semi-supervised
detector training methods can solve the ﬁrst challenge for natural images but its
eﬀectiveness is undermined for gastroscopic images due to the second challenge.
self-supervised backbone pre-training methods enhance object detection
performance by learning high-quality feature representations from massive unla-
belled data for the backbone.
self- and semi-supervised learning for gastroscopic lesion detection
85
image modeling
on the other hand, masked image modeling is expert in
extracting local detailed information but is weak in preserving the discriminabil-
ity of feature representation.
semi-supervised object detection methods [12,14,16,17,20,22,23] ﬁrst use
detectors trained with labeled data to generate pseudo-labels for unlabeled data
and then enhance object detection performance by regarding these unlabeled
data with pseudo-labels as labeled data to train the detector.
the motivation of this paper is to explore how to enhance gld perfor-
mance using massive unlabeled gastroscopic images to overcome the labeled
data shortage problem.
enlightened by this, we propose the self- and semi-supervised
learning (ssl) framework tailored to address challenges in daily clinical prac-
tice and use massive unlabeled data to enhance gld performance.
ssl over-
comes the challenges of gld by leveraging a large volume of unlabeled gastro-
scopic images using self-supervised learning for improved feature representations
and semi-supervised learning to discover and utilize potential lesions to enhance
performance.
[10] with the patch recon-
struction to inherit the advantages of discriminative feature learning and grasp
the detailed information that is important for gld tasks.
more-
over, we propose the ﬁrst large-scale gld datasets (lgldd), which contains
10,083 gastroscopic images with 12,292 well-annotated lesion bounding boxes
of four categories of lesions (polyp, ulcer, cancer, and sub-mucosal tumor).
we
evaluate ssl with multiple detectors on lgldd and ssl brings signiﬁcant
improvement compared with baseline methods (centernet [6]: +2.7ap, faster
rcnn
in summary, our contributions include:
– a self- and semi-supervise learning (ssl) framework to leverage massive
unlabeled data to enhance gld performance.
– a large-scale gastroscopic lesion detection datasets (lgldd)
86
x. zhang et al.
– experiments on lgldd demonstrate that ssl can bring signiﬁcant enhance-
ment compared with baseline methods.
1.
2.1
hybrid self-supervised learning
the motivation of hybrid self-supervised learning (hsl) is to learn the local
feature representations of high discriminability meanwhile contain detailed infor-
mation for the backbone from massive unlabeled gastroscopic images.
among
existing backbone pre-training methods, dense contrastive learning can preserve
local discriminability and masked image modeling can grasp local detailed infor-
mation.
therefore, to leverage the advantages of both types of methods, we
propose hybrid self-supervised learning (hsl), which combines patch recon-
struction with dense contrastive learning to achieve the goal.
structure.
hsl heritages the structure of the densecl
the global projection head
and the dense projection head heritages from the densecl
[10], and the pro-
posed reconstruction projection head is inspired by the masked image modeling.
like other self-supervised contrastive learning methods,
hsl randomly generates 2 diﬀerent “views” of the input image, uses the backbone
to extract the dense feature maps f1, f2 ∈ rh×w ×c, and then feeds them to
the following projection heads.
the global contrastive learning uses the global feature vector fg
of an image as query q and feature vectors from the alternate view of the query
image and the other images within the batch as keys k = {k1, k2, ..., }.
for each
self- and semi-supervised learning for gastroscopic lesion detection
87
query q, the only positive key k+ is the diﬀerent views of the same images and
the others are all negative keys (k−) like moco [9].
the negative keys t− here are the feature vectors of
diﬀerent images while the positive key t+ is the correspondence feature vector
of r in another view of the images.
lh = lg + λdld + λrlr
where λd and λr are the weights of ld and lr and are set to 1 and 2.
2.2
prototype-based pseudo-label generation method
we propose the prototype-based pseudo-label generation method (ppg) to
discover potential lesions from unlabeled gastroscopic data meanwhile avoid
introducing much noise to further enhance gld performance.
ssl can actually enhance the
gld performance for some challenging cases.
lgmdd collects about 1m+ gastroscopic images from 2 hospi-
tals of about 500 patients and their diagnosis reports.
we invite 10 senior doctors to annotate them from the unlabeled
endoscopic images.
finally, they annotates 12,292 lesion boxes in 10,083 images after going
through about 120,000 images.
evaluation metrics : we use standard object detection metrics to evaluate the
gld performance, which computes the average precision (ap) under multiple
intersection-of-union (iou) thresholds and then evaluate the performance using
the mean of aps (map) and the ap of some speciﬁc iou threshold.
we also report ap under some speciﬁc iou threshold (ap50 for .5, ap75 for .75)
and ap of diﬀerent scale lesions (aps, apm, apl) like coco [11].
4
experiments
please kindly refer to the supplemental materials for implementation details
and training setups.
both components of ssl (hsl
& ppg) can bring signiﬁcant performance enhancement for gld tasks.
parameters analysis experiment results.
(d)
extension experiment on endo21.
when compared
with the supervised pre-training (imagenet [5] weights) baseline, ssl can boost
more ap enhancement (centernet: +5.3ap, fasterrcnn: +3.2ap).
2. it can be noticed, ssl can actually enhance the gld
performance for both types of detectors, especially for some challenging cases.
ablation studies.
hsl can bring 1.8 ap and 1.1 ap enhancement for centernet and fasterrcnn
respectively compared with densecl.
ppg can bring extra 0.9ap and 0.9ap
enhancement for centernet and fasterrcnn respectively.
we conduct extra experiments based on faster rcnn
to further analyze the eﬀect of diﬀerent parameter settings on lgldd.
1) reconstruction loss weight λr is designed to balance the losses of con-
trastive learning and the reconstruction, which is to balance the discriminability
and the detailed information volume of local feature representations.
as illus-
trated in table 2.a, only suitable λr can fully boost the detection performance.
2) objectiveness score threshold τu: we compare ppg with objective-
ness score-based pseudo-label generation methods with diﬀerent τu (table 2.b).
a) a
self- and semi-supervised learning for gastroscopic lesion detection
91
low threshold generates noisy pseudo-labels, leading to reduced performance (-
0.6/-0.2 ap at thresholds 0.5/0.6).
b) a high threshold produces high-quality
pseudo-labels but may miss potential lesions, resulting in only slight performance
improvement (+0.3 ap at threshold 0.7).
c) ppg approach uses a low threshold
(0.5) to identify potential lesions, which are then ﬁltered using prototype feature
vectors, resulting in the most signiﬁcant performance enhancement (+0.9 ap).
3) memory update strategy inﬂuences the representativeness of memory
and the prototype feature vectors.
exper-
iment results (table 2.c) show our memory update strategy performs better.
4) endo21: to further evaluate the eﬀectiveness of ssl, we conduct experi-
ments on endo21
experi-
mental results in table 2.d show that ssl can bring signiﬁcant improvements to
publicly available datasets.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_73.pdf:
there is a growing body of
evidence that about 30% of colorectal cancer patients do not respond to
radiotherapy and will need alternative treatment.
by jointly predict-
ing a patient’s response to radiotherapy, the presence of cms4, and the
epithelial tissue map from morphological features extracted from stan-
dard h&e slides we provide a comprehensive clinically relevant assess-
ment of a biopsy.
a graph neural network is trained to achieve this joint
prediction task, which subsequently provides novel interpretability maps
to aid clinicians in their cancer treatment decision making process.
interpretability
supported by cancer research uk.
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43904-9_73.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. (eds.): miccai 2023, lncs 14224, pp.
a common form of treatment for such patients is neoadjuvant therapy,
including chemotherapy and radiotherapy, which can be given to patients with
locally advanced rectal cancer to shrink the tumour prior to surgery.
recent
evidence suggests that 10–20% of patients will have a complete pathological
response to neoadjuvant therapy and can therefore avoid surgery altogether [2,5].
however, one third of patients do not beneﬁt from radiotherapy treatment prior
to surgery [8], hence it is important to determine how a patient will respond to
radiotherapy with a personalized approach in order to avoid overtreatment.
histology-based digital biomarkers enable the possibility to predict a
patient’s response to therapy.
it has been shown that
these four cms classes can be predicted directly from the standard haematoxylin
and eosin (h&e) stained slide images using deep learning
interactions between the epithelial tissue
(cellular tissue lining) and other prevalent tissue types in the tumour microen-
vironment are also indicators of prognosis [15], since progression of colorectal
cancer is dependent on both the epithelial and stromal tissues [20].
other work
has looked at predicting chemoradiotherapy response in rectal cancer patients
from h&e images using diﬀerent approaches, but without providing contextual
interpretations [19,22].
input to our model is a standard h&e whole slide
image (wsi) which is split into smaller patches to overcome the memory limita-
tions of existing gpus.
pathologists and
oncologists can use this information to inspect the validity of the prediction result
and interrogate key aspects of the spatial biology that is critical for patient man-
agement.
2
methods
in this section we present the patch-level feature extraction, provide the detail
of the superpixel segmentation of the wsi, and illustrate the resulting graph
presentation.
for computational reasons, all images are split into patches of size
256 × 256 pixels.
the dino framework [4] uses a self-distillation
training approach, using data augmentation to locally crop the patches and train
with a local-global student-teacher approach.
we use the dino framework to
train a vit in a self-supervised manner on our h&e slides
we use only the training set to train this model, and
use the image patches at 20x magniﬁcation.
the slic superpixel algorithm seg-
ments the entire slide into smaller regions [1].
[1] on the wsis at 5x magniﬁcation to segment the tissue to capture cellular
neighbourhoods that are roughly between 80–100 µm2/pixels in size.
it can be
seen that the superpixel boundaries consistently align with the boundaries of
tissue compartments.
the pre-treatment biopsy slides were all sectioned and
stained in the same laboratory, and scanned at 20x magniﬁcation (0.5 µm2/pixel)
on an aperio scanner.
pathological complete response, which we use as a tar-
get outcome here, was derived from histopathological assessment from post-
treatment resections.
these epithelial segmentation masks were generated at 10x mag-
niﬁcation (1 µm2/pixel) with a u-net [17] which was trained and validated on
666 full tissue sections belonging to 362 patients from the focus cohort [18].
we use these masks in our analysis to ﬁlter out background and irrelevant
tissue from the images.
we address this imbalance
in the supplementary materials.
3
experiments
implementation.
[1] with compactness of
20, setting the number of segments for each wsi as half the mean size of the
wsi.
[21] with dimensions 64, 32 and 16 respectively.
we apply
dropout of 0.5 in-between graph layers, use minimum aggregation for message
passing between nodes and use maximum pooling for concatenating the node
activations.
despite the noise in our reference data used for training, our model
achieves good performance in terms of mean auc scores on all three prediction
branches of our model, predicting complete response to radiotherapy (rt) with
0.819 auc, cms4 with 0.819 auc and epithelial tissue at the node level with
0.760 auc across folds.
the prediction
performance of the model could be improved by utilising a larger training dataset
and performing more exhaustive parameter searches, however the current per-
formance of the model is suﬃcient to demonstrate the impact of this approach.
2. additional samples are presented in the
supplementary materials.
2, with further slides in the supplementary materials.
changing the dropout, loss weights, loss func-
tion, and message passing aggregation methods only changes prediction auc
scores by absolute values up to 0.03.
the results can be found in the supplementary materials.
these small changes indicate that the noise in our data does not degrade the
performance of our classiﬁer, reinforcing it as a robust and accurate model.
4
conclusion
by setting the prediction of response to therapy in context with disease biol-
ogy and spatial organisation of the tissue we are providing a novel approach
for enhancing the interpretablity of complex prediction tasks.
extending the amount of training data and improving model training will
improve model performance, which is already impressive.
the prediction branches only diverge
at the ﬁnal stage of translating these graph features into outcome predictions
for our three clinically relevant outcomes.
importantly, this level of visualisation
is not only accessible to pathologists, this joint prediction model also enhances
the communication between pathologists and oncologists which is critical for
patient management.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_72.pdf:
curricu-
lum learning is inspired by the way humans learn, starting with simple
examples and gradually progressing to more challenging ones.
this new approach has been evaluated on
two medical image datasets, and the results show that it outperforms
other curriculum learning methods.
keywords: medical image classiﬁcation · curriculum learning ·
uncertainty estimation
1
introduction
curriculum learning methods in deep learning are inspired by human educa-
tion and involve structuring the training data from easy to hard to teach net-
works progressively.
the ﬁrst is that human experts
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43904-9 72.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. (eds.): miccai 2023, lncs 14224, pp.
these metrics not only require suﬃcient domain knowledge, but they also run
the risk that metrics of diﬃculty from a human perspective may not be applica-
ble to a learning model due to diﬀerent decision boundaries between the model
and the human [25].
another popular approach is diﬃculty measurers based on the network,
including transfer learning
however, this type of approach suﬀers from the
uncertainty of quantifying the diﬃculty of data due to insuﬃcient training in
the early stages.
in addition, while deep learning models have achieved impres-
sive performance in the medical image analysis ﬁeld, there remain challenges in
measuring and developing a model with low in-domain uncertainty.
our approach is motivated by two key observations: 1) sample diﬃ-
culty is inﬂuenced by both the complexity of the data and the model’s inability
dclu for medical image classiﬁcation
749
to explain data, related to in-domain uncertainty, and 2) reducing in-domain
uncertainty by improving the learning process can boost model performance.
in particular, our dynamic diﬃculty measurer (ddm) generates uncertain-
ties and predictions for each image simultaneously.
uncertainties reﬂect the dif-
ﬁculty and we use these as the criteria for data rearrangement.
we evaluate our method on two medical
image datasets isic 2018 task 3 and chest-xray8 (covid-19).
as our proposal exploits in-domain uncer-
tainties from the current network at every iteration, we want to avoid using
additional modules and metrics to obtain similar uncertainty estimates from
the above methods and thereby reduce computational resource requirements.
dclu for medical image classiﬁcation
751
additionally, the dirichlet distribution of ddm can be deﬁned with parameters
α
≤ p1, ..., pk ≤ 1. b(α) is a k-dimensional multinomial
beta function [13].
[6]. inspired by this phenomenon, we employ in-domain uncertainty to measure
the diﬃculty of data at each iteration.
however, some existing pacing functions attempt to partition the
dataset into multiple subsets and gradually feed them into the network during
training - this fails to satisfy the requirement of our diﬃculty measurer.
in our work, we implement uas through two approaches.
firstly, uas (expo-
nential) incorporates both the reorder and sampling modules, prompting the net-
work to prioritize learning easier examples during the initial stages of training and
then gradually learn more diﬃcult examples.
4
experiment
4.1
dataset and experimental setup
dataset.
we evaluated our method on two public medical image datasets includ-
ing isic 2018 task 3
[18]. isic 2018 task
3 dataset has 10,015 training images and 194 images for validation.
chest-xray 8 (covid-19) contains 1125 x-ray images of the chest of the
individuals studied, including 125 images labeled covid-19 taken from [3], 500
images labeled pneumonia and 500 images labeled no ﬁndings were randomly
dclu for medical image classiﬁcation
753
taken from the chestx-ray8
chest-xray 8 (covid-
19) dataset is randomly divided into two parts, with 80% of the images being
the training set and 20% of the images being the test set.
evaluation metrics.
for both datasets, the performance of diagnosis is eval-
uated with both accuracy and f1 score.
moreover, for the assessment of uncer-
tainty estimation, we apply expected calibration error (ece) as the metric.
implementation details.
in our experiments, we used the tensorflow framework and trained on
an nvidia 3090 gpu with 32g of ram.
the total epoch is 50.
4.2
experimental results
comparison with state-of-the-art.
our method with uas (exponential) performs better than other
methods on chest-xray 8 (covid-19) and has the second best performance
on isic 2018 task 3.
the performance gap between our two methods may be
table 1. comparison with state-of-the-art curriculum learning methods on isic 2018
task 3 and chest-xray 8 (covid-19)
method
isic 2018 task 3
chest-xray8 (covid-19)
accuracy f1 score accuracy f1 score
vanilla
81.73
38.8
71.88
57.51
fcl
when the
samples are selected by uas (exponential) at the early training stage, samples
from smaller numbered classes may not be chosen which leads to the optimiza-
tion may tend to fall into local extrema.
compared with loss
function-based curriculum learning (spl, spcl and adaptive cl), our meth-
ods obtain better performance, showing the importance of using uncertainty
estimates as the criterion for diﬃculty measurement of data.
furthermore, our
method is more eﬀective than spl at an early training stage.
first of all,
dclu for medical image classiﬁcation
755
our method performs 8.93% better than fcl using the same exponential pacing
functions.
moreover, in order to demonstrate
that the order generated by ddm is robust, we have conducted experiments
on diﬀerent backbones, details of which can be found in the appendix.
finally, we compare the eﬀect of using the loss function with and without l2
regular term on the model performance, ﬁnding that using the loss function with
l2 can eliminate the eﬀect of overﬁtting.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_57.pdf:
yet, many downstream tasks including polyp
characterization (cadx), quality metrics, automatic reporting, require
aggregating polyp data from multiple frames.
our solution uses an attention-based self-supervised
ml model, speciﬁcally designed to leverage the temporal nature of video
input.
we quantitatively evaluate method’s performance and demon-
strate its value for the cadx task.
it is well known
that many polyps go unnoticed during colonoscopy [22].
the success of polyp detector sparkled the
development of new cad tools for colonoscopy, including polyp characterization
(cadx, or optical biopsy), extraction of various quality metrics, and automatic
reporting.
many of those new cad applications require aggregation of all avail-
able data on a polyp into a single uniﬁed entity.
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43904-9_57.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
while this is
true, there are a few factors speciﬁc to the colonoscopy setup: (a) due to abrupt
endoscope camera movements, targets (polyps) often go out of the ﬁeld of view,
(b) because of heavy imaging conditions (liquids, debris, low illumination) and
non-rigid nature of the colon, targets may change their appearance signiﬁcantly,
(c) many targets (polyps) are quite similar in appearance.
those factors limit the
scope and accuracy of existing frame-by-frame spatio-temporal tracking meth-
ods, which typically yield an over-fragmented result.
that is, the track is often
lost, resulting in relatively short tracklets (temporal sequences of same target
detections in multiple near-consecutive frames), see supplementary fig.1.
a recently published method
the tracklets are built incrementally, by adding a single frame detection to the
matched tracklet, one-by-one.
to avoid manual data annotation, which is extremely ineﬀective in our case,
we turn to self-supervision and adapt the widely used contrastive learning app-
roach
as tracklet re-identiﬁcation is a sequence-to-sequence matching problem, the
standard solution is comparing sequences element-wise and then aggregating
the per-element comparisons, e.g. by averaging or max/min pooling [21] - the
so-called late fusion technique.
[23] to leverage the attention paradigm for non-
uniform weighing and “knowledge exchange” between tracklet frames.
– the application of polyp reid to boost the polyp cadx performance.
as brieﬂy mentioned above, the proposed approach starts with an initial
grouping of polyp detections using an oﬀ-the-shelf multiple object tracking algo-
rithm.
2.1
single-frame representation for reid
to generate a single frame representation we train an embedding model that
maps a polyp image into a latent space, s.t.
a straightforward approach to train such model is supervised learning, which
requires forming a large collection of polyp image pairs, manually labeled as
same/not same polyp [1].
in addition, ﬁnding hard negative pairs is especially challenging, as
images of two randomly sampled polyps are usually very dissimilar.
moreover,
self-supervised techniques using extensive unannotated datasets has exhibited
substantial advantages within the medical domain [12].
hence, we turn to simclr [5], a contrastive self-supervised learning tech-
nique, which requires no manual labeling.
in simclr the loss is calculated
over the whole batch where all input samples serve as negatives of each other
and positive samples are generated via image augmentations.
combined with
the temperature mechanism this allows for hard negative mining by prioritizing
hard-to-distinguish pairs, resulting in a more eﬀective loss weighting scheme.
one caveat of simclr is the diﬃculty to generate augmentations beneﬁcial
for the learning process [5].
speciﬁcally for colonoscopy, the standard image
augmentations do not capture the diversity of polyp appearances in diﬀerent
views (see fig. 1(c)).
identiﬁcation in colonoscopy
593
instead of customizing the augmentations to ﬁt the colonoscopy setup, we
leverage the temporal nature of videos, and take diﬀerent polyp views from the
same tracklet as positive samples (see fig. 1(b)).
fig.
1. (a) a polyp image, (b) two additional views of the polyp in (a) taken from the
same tracklet, (c) two typical augmentations of the polyp in (a).
images in (b) oﬀer
more realistic variations, such as diﬀerent texture, tools, etc.
formally, a batch is formed by sampling one tracklet from n diﬀerent pro-
cedures to ensure the tracklets belong to diﬀerent polyps.
an example of similarities between frames
can be seen in supplementary fig.
we postulate
that learning a joint embedding of multiple views in an end-to-end manner will
produce a better representation of the visual properties of a polyp, by allowing
“knowledge exchange” between the tracklet frames.
we artiﬁcially split a
tracklet into 3 disjoint segments, where the middle segment is discarded, and the
ﬁrst and the last segments are used as a positive pair, thus providing suﬃciently
diﬀerent appearances of the same polyp as would happen in real procedures.
in addition, this type of sampling approach, which eﬀectively discards highly
correlated samples from training, has been shown to improve model performance
in [17].
3
experiments
this section includes two parts.
the
average length of the recorded procedures is 15 min, with a median duration of
self-supervised polyp re-identiﬁcation in colonoscopy
595
13 min. for training, we automatically generated polyp tracklets using automatic
polyp detection and tracking as described in sect.
for evaluation, the test set polyp tracklets were
manually annotated (timestamps and bounding boxes) by certiﬁed physicians.
in addition, tracklet pairs from the same procedure were manually labeled as
either belonging to the same polyp or not.
[9] as the single frame encoder, with an mlp
head projecting the representation into a 128-dimensional embedding vector.
we initialize the model using pre-trained imagenet
due to memory limitations, we
use 8 views per tracklet during training, resulting in 1024 ∗ 8 = 8192 images per
training step.
we evaluate the
performance using auc of the roc and precision-recall curve (prc) for track-
let similarity scores over the test set (see table 1 and supplementary fig. 3).
in addition, we evaluate the eﬀectiveness of reid by measuring the aver-
age polyp fragmentation rate (fr), deﬁned as the average number of tracklets
polyps are split into.
obviously, lower fragmentation rate means better result
(with the best fragmentation of 1), but it may come at the expense of wrong
tracklet matching (false positive).
we measure the fragmentation rate at the
operating point of 5% false positive rate.
the number of polyp fragments is
determined by matching tracklets to manually annotated polyps and counting
596
y. intrator et al.
table 1.
results presented in table 2 demonstrate that reid can reduce
the fragmentation rate by over 50%, compared to a tracking only solution [27].
table 2. fragmentation rate (fr) statistics before and after the reid.
fragmented polyps ratio is the percentage of polyps divided
into more than one tracklet.
fr
fr std fragmented polyps ratio
tracking
3.3
3.3
0.64
tracking+reid 1.86 1.49
0.45
3.2
reid for cadx
in this section, we investigate the potential beneﬁts of using polyp reid as part
of a cadx system.
the tracklets are then manually
grouped together to build a single sequence for every polyp.
we trained a simple image classiﬁcation cnn, composed of a mobilenet
[20] backbone, followed by an mlp layer with a sigmoid activation, to predict the
non-adenoma/adenoma score in [0, 1], for each frame.
the 3 evaluated
methods are: (1) manual annotation (2) grouping by tracking, and (3) grouping
by reid.
the manually annotated tracklets - the ground truth (gt) - are the
longest sequences, containing all frames of each polyp in the test set.
by construction, tracklets generated by methods (2) and
(3) are subsets of the corresponding manually annotated gt tracklet, and are
assigned its polyp classiﬁcation label.
a visualization of the resulting tracklets
using diﬀerent grouping methods is provided in supplementary fig.
cadx test data distribution and fragmentation rate (fr).
the result on the manually annotated data is the accu-
racy upper-bound and is brought as a reference point.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_43.pdf:
breast ultrasound videos contain richer information than
ultrasound images, therefore it is more meaningful to develop video
models for this diagnosis task.
in this paper, we explore the feasibil-
ity of enhancing the performance of ultrasound video classiﬁcation using
the static image dataset.
the kga-net adopts both video clips and static images to
train the network.
the coherence loss uses the feature centers generated
by the static images to guide the frame attention in the video model.
our kga-net boosts the performance on the public busv dataset by
a large margin.
[15] and static images from
busi
we use a 2d resnet trained on ultrasound images to get the features.
while ultrasound videos oﬀer more information, prior studies have primarily
focused on static image classiﬁcation [2,11,27]. obtaining ultrasound video data
with pathology gold standard results poses a major challenge.
sonographers typ-
ically record keyframe images during general ultrasound examinations, not entire
videos.
consequently, while there are many breast ultrasound image
datasets [1,28], breast ultrasound video datasets remain scarce, with only one
relatively small dataset [15] containing 188 videos available currently.
given the diﬃculties in collecting ultrasound video data, we investigate
the feasibility of enhancing the performance of ultrasound video classiﬁcation
using a static image dataset.
to achieve this, we ﬁrst analyze the relationship
between ultrasound videos and images.
the images in the ultrasound dataset
are keyframes of a lesion that exhibit the clearest appearance and most typical
symptoms, making them more discriminative for diagnosis.
1, the feature points of static images are more
concentrated, while the feature of video frames sometimes are away from the
class centers.
therefore, it is a
promising approach to guide the video model to pay more attention to important
frames close to the class center with the assistance of static keyframe images.
our approach leverages both
image (keyframes) and video datasets to train the network.
during training, we construct category fea-
ture centers for malignant and benign examples respectively using center loss [26]
on static image inputs and use the centers to guide the training of video frame
attention.
due to the feature centers being generated by the
larger scale image dataset, it provides more accurate and discriminative feature
centers which can guide the video frame attention to focus on important frames,
and ﬁnally leads to better video classiﬁcation.
our experimental results on the public busv dataset [15] show that our
kga-net signiﬁcantly outperforms other video classiﬁcation models by using an
external ultrasound image dataset.
this phenomenon makes our method more
explainable and provides a new perspective for selecting keyframes from video.
we analyze the relationship between ultrasound video data and image data,
and propose the coherence loss to use image feature centers to guide the
training of frame attention.
2. we propose kga-net, which adopts a static image dataset to boost the
performance of ultrasound video classiﬁcation.
[19,23,27] utilize multi-task learn-
ing to improve the model performance.
however, all of them are based on image
datasets, such as busi
2. overview of our proposed keyframe-guided attention network.
performance.
our proposed
kga-net is a simple framework that leverages the frame attention module to
aggregate multi-frame features eﬃciently.
2, our kga-net takes the video inputs and static image inputs
simultaneously to train the network.
the coherence loss is proposed to guide the
frame attention by using the feature centers generated by the images.
we will
then elaborate on each component in the following sections.
3.1
video and image classiﬁcation network
the video classiﬁcation network is illustrated in fig.
the image classiﬁcation network is used to assist in training the video
model.
[26] to the image model besides the cross-entropy loss.
(4)
446
a. sun et al.
lv
ce and li
ce denote the cross-entropy for video classiﬁcation and image and
frame classiﬁcation.
empirically, we set λ = 1 in our experiments.
4
experiments
4.1
implementation details
datasets.
[1] as the image dataset.
busi contains 445 images of benign lesions and 210 images
of malignant lesions.
all images of the busi dataset are adopted to train our kga-net.
[12] pretrained on imagenet [4] is used as backbone.
for
each batch, the video clips and static images are both sampled and sent to the
network.
we use a total batchsize of 16 and the sample probability of video clips
and images is 1:1.
we implement the model based on pytorch and train it with
nvidia titan rtx gpu cards.
in order
to satisfy the ﬁxed video length requirement of mvit
[7], we sample up to 128
frames of each video to form a video clip and predict its classiﬁcation result
using all the models in experiments.
therefore, we compare our method with strong
video baselines on natural images.
for fairness comparison, we train these models using both video
and image data, treating images as static videos.
evaluation metrics are reported
on the busv test set for performance assessment.
as shown in table 1, by leveraging the guidance of the image dataset, our
kga-net signiﬁcantly surpasses all other models on all of the metrics.
therefore, the success of our
kga-net lies in the correct usage of the image guidance.
[7]
90.53
82.05
80.77
84.62
kga-net (our) 94.67
89.74
88.46
92.31
formed by the image dataset with larger data size and clear appearance eﬀec-
tively improve the accuracy of frame attention hence boosting the video classi-
ﬁcation performance.
we use the same training schedule for all of the experiments.
image guidance is the main purpose of our method.
to portray the eﬀect of
using the image dataset, we train the kga-net using busv dataset alone in the
ﬁrst row of table 2.
without the image dataset, we generate the feature centers
from the video frames.
as a result, the performance signiﬁcantly drops due to the
decrease in dataset scale.
it also shows that the feature centers generated by the
image dataset are more discriminative than that of the video dataset.
it is not
only because the lesion number of busi is larger than busv, but also because
the images in busi are all the keyframes that contain typical characteristics of
lesions.
it can be seen that both of these two modules
contribute to the overall performance according to auc and acc.
this phenomenon certiﬁes the eﬀective-
ness of our kga-net to prevent false negatives in diagnosis.
w/o image guidance
85.21
76.92
73.08
84.62
w/o coherence loss & attention 88.17
74.36
61.54
100.0
w/o coherence loss
92.90
87.18
80.77
100.0
kga-net
94.67
89.74
88.46
92.31
4.4
visual analysis
fig.
overall speaking, the frames with high atten-
tion weights do have clear image appearances for diagnosis.
3(b) clearly demonstrate the edge micro-lobulation and
irregular shapes, which lead to malignant judgment.
the qualitative analysis proves the interpretability of our method, which will
beneﬁt clinical usage.
our kga-net takes as input both the video data and image data to
train the network.
we propose the coherence loss to guide the training of the
video model by the guidance of feature centers of the images.
our method signif-
icantly exceeds the performance of other competitive video baselines.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_7.pdf:
in current pathology image classiﬁcation, methods mostly
rely on patch-based multi-instance learning (mil), which only consid-
ers the relationship between patches and slides.
however, in clinical
medicine, doctors use slide-level labels to summarize patient-level labels
as a diagnostic result, indicating the involvement of three levels of patch,
slide, and patient in actual pathology image analysis, which we refer to
as the multi-level multi-instance learning (ml-mil) problem.
this allows for interaction between patient-level and slide-level
information and the correction of their respective features to achieve bet-
ter classiﬁcation performance.
the results show
that our method improves the performance of the baselines on both slide
and patient levels.
keywords: multiple instance learning · multi-level labels ·
pathology images · transformer
f. li, m. wang and b. huang—contribute equally to this work.
https://doi.org/10.1007/978-3-031-43904-9_7
64
f. li et al.
1
introduction
pathological image analysis is a vital area of research within medical image
analysis, focused on utilizing computer technology to aid doctors in diagnosing
and treating diseases by analyzing pathological tissue slide images [5]. advance-
ments in pathological image analysis have been made in early cancer diagnosis,
tumor localization, and grading, and treatment planning [3,10].
speciﬁcally, in clinical problems of pathological image analysis, doctors usu-
ally summarize patient-level labels based on slide labels as the diagnostic results
[1,6].
similar situations exist in other tasks, such as the classiﬁcation of breast cancer
metastases in lymph nodes, where slide categories may have diﬀerent classiﬁca-
tions, and the corresponding diagnosis of the same patient is whether the cancer
has spread to the regional lymph nodes (n-stage)
1, actual pathological image analysis involves the relationships of patches,
slides, and patients, which is called a multi-level multi-instance learning (ml-
mil) problem.
the ﬁrst
method is to directly average the prediction values of slides or take the maxi-
mum prediction value [9].
this
simple yet eﬀective method allows for interaction between patient-level and slide-
level information to correct their respective features and improve classiﬁcation
performance.
our framework consists of two steps: ﬁrst, at the patch-slide level,
a common mil framework is used to train a mil neural network and obtain
p&sre: a ml-mil framework for pathological image analysis
65
fig.
our method can eﬀectively solve the problem of diﬃcult train-
ing due to the scarcity of samples at the highest level in ml-mil, and can be
integrated into two state-of-the-art methods to further improve performance.
we
conducted rigorous experiments on two datasets and demonstrated the eﬀective-
ness of our method.
before this, no other frame-
work had directly tackled this speciﬁc problem, making our proposal a
ground-breaking step in the application of ml-mil in healthcare;
2) proposing a simple yet highly eﬀective method that leverages self-attention
mechanisms and transformer models to enhance the interaction between slide
and patient information.
this innovative approach not only improves the clas-
siﬁcation performance at the patient level but also at the slide level, show-
casing its eﬀectiveness and versatility;
3) conducting extensive experiments on two separate datasets.
the experiments resulted in improved per-
formance, indicating that our method enhances the eﬃcacy of these existing
approaches.
[9] for the slide-patch stage.
these matrices are element-
wise multiplied and then passed through an fc layer to obtain the weight of each
patch, ωk.
for dsmil, the attention of each patch is based on the cosine distance
between instances and key instances.
therefore, the slide feature output by both methods can
be generalized as:
p&sre: a ml-mil framework for pathological image analysis
67
hj =
mj

k=1
ωk ∗ pk/
mj

k=1
ωk
(1)
finally, we obtain the feature vector set hi={hj|j=1 to ni} for all slides {sj}
of patientxi through patch-slide mil.
then, we perform a
weighted average of the vectors based on this weight to obtain the patient fea-
ture vi:
αj = fc({hj|j
speciﬁcally, we merge the slide feature set {hj} and
the patient feature vi into the input tokens t in
i
= {h1, h2, ..., hni, vi} = {t},
68
f. li et al.
and then input them into a multi-layer transformer through self-attention and
feed-forward neural network layers to obtain the interaction information between
slides and output tokens t out
i
:
βk,l = softmax(w qtk
t (w ktl)/
√
d)
(4)
tk =
ni+1

l=1
βk,lw v tl
(5)
t′
k = relu(tkw r + b1)w o + b2
(6)
where d is the dimension of the token, and tk and tl come from t in
i .
b1
and b2 are bias vectors.
[7] loss function.
3
experiments and results
3.1
dataset and evaluation
cd-itb dataset.
on average, there were 5 slides per patient.
the dataset comprises an average of 2.3k instances per
bag, with the largest bag containing over 16k instances.
camelyon17 dataset.
there
were 5 slides per patient on average.
the patients are divided into two groups
p&sre: a ml-mil framework for pathological image analysis
69
based on their pn stage, namely lymph node positive and lymph node negative,
in proportions of 24:76, respectively.
the average number of instances per bag is approximately
6.1k, and the largest bag contains over 23k instances.
metrics.
to avoid randomness, we run all experiments ﬁve times and
report the averaged metrics.
3.2
implementation details
we utilized resnet50, which was pre-trained on imagenet1k, to extract features
from patches.
all networks are
implemented using pytorch and trained on a nvidia rtx titan gpu with
24 gb memory.
3.3
comparisons and results
we compared our strategy with two state-of-the-art mil methods to evaluate
its performance.
to investigate the impact of self-attention and transformers on
slide-level and case-level results, we conducted ablation experiments: “abmil
+ p&sre (with/without psfi)” and “dsmil + p&sre (with/without psfi)”,
respectively.
therefore, the ablation experiments demonstrate
the eﬀectiveness of p&sre in enhancing the classiﬁcation performance at both
the slide and patient levels.
in the future, we plan to leverage clustering and active learning methods
to reduce the number of patches and enable the interaction of all three levels
with the transformer, which would further enhance the accuracy and eﬃciency
of our proposed method.
based on existing
state-of-the-art mil methods, we then extend the framework to p&sre, which
p&sre: a ml-mil framework for pathological image analysis
71
conducts feature extraction and interaction at the slide-patient level.
by intro-
ducing a transformer, the framework enables iterative interaction and correction
of information between patients and slides, resulting in better performance at
both the patient level and slide level compared to existing state-of-the-art algo-
rithms on two validation datasets.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_5.pdf:
colonoscopy videos contain richer information than
still images, making them a valuable resource for deep learning methods.
for the background,
yona conducts background dynamic alignment guided by inter-frame
diﬀerence to eliminate the invalid features produced by drastic spatial
jitters.
quantitative and quali-
tative experiments on three public challenging benchmarks demonstrate
that our proposed yona outperforms previous state-of-the-art competi-
tors by a large margin in both accuracy and speed.
keywords: video polyp detection · colonoscopy · feature alignment ·
contrastive learning
y. jiang and z. zhang—equal contribution.
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43904-9_5.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
1(a), we show the target motion speed [26]1 on imagenetvid
the motion speed in
imagenetvid evenly distributes in three intervals.
thus we conjecture that collaborating too many frames for polyp video detec-
tion will increase the misalignment between adjacent frames and leads to poor
detection performance.
figure 1(b) shows the performance of fgfa
(b) the performance of
fgfa [26] using multiple reference frames increases on imagenetvid while decreasing
on ldpolypvideo.
(color ﬁgure online)
1 averaged intersection-over-union scores of target in the nearby frames (±10 frames).
speciﬁcally, we propose the foreground
temporal alignment (fta) module to explicitly align the foreground channel
activation patterns between adjacent features according to their foreground simi-
larity.
in addition, we design the background dynamic alignment (bda) module
after fta that further learns the inter-frame background spatial dynamics to
better eliminate the inﬂuence of motion speed and increase the training robust-
ness.
in summary, our contributions are in three-folds: (1) to the best of our knowl-
edge, we are the ﬁrst to investigate the obstacles to the development of existing
video polyp detectors and conclude that two-frame collaboration is enough for
video polyp detection.
it composes the foreground and background alignment modules
to align the features under the fast-moving condition.
(3) extensive experiments demonstrate that
our yona achieves new state-of-the-art performance on three large-scale public
video polyp detection datasets.
2. we leverage the centernet
then, multi-scale features are fused and up-sampled to the resolution of the
ﬁrst stage as the intermediate features f a, f r. then, we conduct foreground
temporal alignment (fig. 2(a)) on intermediate features to align their channel
activation pattern.
next, the enhanced anchor feature ˜f is further reﬁned by
the background dynamic alignment module (fig.
yona for accurate and fast video polyp detection
47
overall, the whole network is optimized with the combination loss function
in an end-to-end manner.
the
ﬁnal output of (b) is used to predict the bounding box of the current frame.
2.1
foreground temporal alignment
since the camera moves at a high speed, the changes in the frame are very
drastic for both foreground and background targets.
thus we propose to conduct tem-
poral alignment between adjacent features by leveraging the foreground context
of only one adjacent reference frame.
speciﬁ-
cally, given the intermediate features f a, f r and reference binary map m r, we
ﬁrst pooling f r to 1d channel pattern f r by the binary map on the spatial
dimension (rn×c×h×w → rn×c×1) and normalize it to [0, 1]:
f r = norm
[f r(x, y)]/sum[m r(x, y)]
if m r(x, y) = 1
(1)
then, the foreground temporal alignment is implemented by channel attention
mechanism, where the attention maps are computed by weighted dot-product.
48
y. jiang et al.
at the training stage, the ground truth boxes of the reference frame are used
to generate the binary map m r. during the inference stage, we conduct fta
only if the validated bounding box of the reference frame exists, where “validated”
denotes the conﬁdence scores of detected boxes are greater than 0.6.
for eﬃciency, we use the cosine similar-
ity metric [8] to measure the similarity, where f a is the 1d channel pattern of
f a computed with eq. 1:
α = exp
 f r · f a
|f r||f a|

(3)
2.2
background dynamic alignment
the traditional convolutional-based object detector can detect objects well when
the background is stable.
however, once it receives obvious interference, such as
light or shadow, the background changes may cause the degradation of spa-
tial correlation and lead to many false-positive predictions.
in practice, given the enhanced anchor feature ˜f from fta and reference
feature f r, the inter-frame diﬀerence is deﬁned as the element-wise subtraction
of enhanced anchor and reference feature.
lcontrast =
1
nt
nt

j=1
lnce
j
(8)
3
experiments
we evaluate the proposed method on three public video polyp detection bench-
marks: sun colonoscopy video database [7,10] (train set: 19,544 frames, test
set: 12,522 frames), ldpolypvideo
for the fairness of the experiments, we keep the same dataset settings
for yona and all other methods.
performance comparison with other image/video-based detection models.
detailed results are listed in the supple-
ment.
we randomly crop and resize the images to 512×512 and normalize them
using imagenet settings.
random rotation and ﬂip with probability p = 0.5 are
used for data augmentation.
besides,
yona achieves the best trade-oﬀ between accuracy and speed compared with
all other image-based sotas across all datasets.
thanks to this one-adjacent-frame framework,
our yona can not only prevent the false positive caused by part occlusion (1st
and 2nd clips) but also capture useful information under severe image quality
(2nd clip).
moreover, our yona shows robust performance even for challenging
scenarios like concealed polyps (3rd clip).
due to the large
variance of colonoscopy image content, the f1 score slightly decreases if directly
adding fta without the adaptive re-weighting strategy.
overall, by combining all the proposed
methods, our model can achieve new state-of-the-art performance.
fig.
to address the problem of fast-moving polyps, we introduced
the foreground temporal alignment module, which explicitly aligns the channel
patterns of two frames according to their foreground similarity.
for the complex
background content, we designed the background dynamic alignment module to
mitigate the large variances by exploiting the inter-frame diﬀerence.
extensive
experiment results conﬁrmed the eﬀectiveness of our method, demonstrating the
potential for practical use in real clinical applications.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_55.pdf:
yet, there are many healthcare domains for which ssl has not been
extensively explored.
these strong image representations serve
as a foundation for secondary training with limited annotated datasets,
resulting in state-of-the-art performance in endoscopic benchmarks like
surgical phase recognition during laparoscopy and colonoscopic polyp
characterization.
additionally, we achieve a 50% reduction in annotated
data size without sacriﬁcing performance.
over 250 million endoscopic
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43904-9_55.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
a cardinal challenge in performing endoscopy is the limited ﬁeld of view which
hinders navigation and proper visual assessment, potentially leading to high
detection miss-rate, incorrect diagnosis or insuﬃcient treatment.
these limita-
tions have fostered the development of computer-aided systems based on artiﬁcial
intelligence (ai), resulting in unprecedented performance over a broad range of
clinical applications
in the last few years, self-supervised
learning (ssl [5–8]) has been shown to be a revolutionary strategy for unsu-
pervised representation learning, eliminating the need to manually annotate vast
quantities of data.
we ﬁrst experiment solely on public datasets, cholec80
demonstrating performance on-par with the top results reported
in the literature.
through extensive experiments, we
ﬁnd that scaling the data size necessitates scaling the model architecture, lead-
ing to state-of-the-art performance in surgical phase recognition of laparoscopic
procedures, as well as in polyp characterization of colonoscopic videos.
, the proposed approach exhibits robust generalization, yielding better
performance with only 50% of the annotated data, compared with standard
supervised learning using the complete labeled dataset.
cholecystectomy is the surgical
removal of the gallbladder using small incisions and specialized instruments.
apart
from measuring quality and monitoring adverse event, this task also serves in
facilitating education, statistical analysis, and evaluating surgical performance.
self-supervised learning for endoscopy
571
furthermore, the ability to recognize phases allows real-time monitoring and
decision-making assistance during surgery, thus improving patient safety and
outcomes.
ai solutions have shown remarkable performance in recognizing sur-
gical phases of cholecystectomy procedures
as an alternative, ssl methods have
been developed [12,28,30], however, these are early-days methods that based
on heuristic, often require external information and leads to sub-optimal perfor-
mance.
this limitation has motivated the
development of ai systems for automatic optical biopsy, allowing non-experts to
also eﬀectively perform optical biopsy during polyp management.
through extensive experiments in sect.
[2] have set a new state-of-the-art among ssl methods on the imagenet
benchmark [29], with a particular focus on the low data regime.
during pretraining, on each image xi ∈ rn of a mini-batch of b ≥ 1 samples
(e.g. laparoscopic images) we apply two sets of random augmentations to gen-
erate anchor and target views, denoted by xa
i and xt
i respectively.
the resultant anchor and target sequences are used
as inputs to their respective image encoders fθa and fθt.
both encoders share
the same vision transformer (vit [16]) architecture where the parameters θt of
the target encoder are updated via an exponential moving average of the anchor
encoder parameters θa.
furthermore, to prevent representation collapse and
encourage the model to fully exploit the prototypes, a mean entropy maximiza-
tion (me-max) regularizer [2,22] is added, aiming to maximize the entropy h(¯pa)
of the average prediction across all the anchor views ¯pa ≜
1
mb
b
i=1
m
m=1 pa
i,m.
thus, the overall training objective to be minimized for both θa and q is where
λ > 0 is an hyperparameter and the gradients are computed only with respect
to the anchor predictions pa
i,m (not the target predictions pt
i).
we compiled a dataset of laparoscopic procedures videos exclu-
sively performed on patients aged 18 years or older.
the recorded procedures have an average duration of 47 min, with
a median duration of 40 min. each video recording was sampled at a rate of 1
frame per second (fps), resulting in an extensive dataset containing 23.3 million
images.
further details are given in the supplementary materials.
colonoscopy.
we have curated a dataset comprising 13,979 colonoscopy videos
of patients aged 18 years or older.
the average duration of the recorded procedures is 15 min,
with a median duration of 13 min. to identify and extract polyps from the videos,
we employed a pretrained polyp detection model
for each frame,
we cropped the bounding boxes to generate individual images of the polyps.
this
process resulted in a comprehensive collection of 2.2 million polyp images.
fig.
bottom: colonoscopy.
4
experiments
in this section, we empirically demonstrate the power of ssl in the context
of endoscopy.
our experimental protocol is the following: (i) ﬁrst, we perform
ssl pretraining with msns over our unlabeled private dataset to learn infor-
mative and generic representations, (ii) second we probe these representations
by utilizing them for diﬀerent public downstream tasks.
we speciﬁcally
use multi-stage temporal convolution networks (ms-tcn) as used in [13,27].
we perform an extensive
hyperparameter grid search for all downstream experiments and report the test
results for the models that exceed the best validation results.
implementation details.
for ssl we re-implemented msns in jax using
scenic library [15].
as our image encoders we train vision transformer (vit [16])
of diﬀerent sizes, abbreviated as vit-s/b/l, using 16 tpus.
downstream exper-
iments are implemented in tensorflow where training is performed on 4 nvidia
tesla v100 gpus.
see the supplementary for further implementation details.1
4.1
results and discussion
scaling laws of ssl.
we pretrain the models with msn and then
report their downstream performances.
we ﬁnd that replacing resnet50 with vit-s, despite
comparable number of parameters, yields sub-optimal performance.
the performance in per-frame
phase recognition is comparable with the baseline.
importantly, we see that the performance gap
becomes prominent when using the large scale private datasets for ssl pretrain-
ing.
here, per-frame and per-video phase recognition performances improve by
6.7% and 8.2%, respectively.
notice
that the performance improves with scaling both model and private data sizes,
demonstrating that both factors are crucial to achieve optimal performance.
next, we examine the beneﬁts of using msns to improve
downstream performance in a low-shot regime with few annotated samples.
1 for reproducibility purposes, code and model checkpoints are available at https://
github.com/royhirsch/endossl.
self-supervised learning for endoscopy
575
table 1.
comparing the downstream f1 performances of: (i) models trained on the
private (pri) and public (pub) datasets using ssl.
(ii) fully supervised baselines pre-
trained on imagenet-1k (in1k).
figure 3 shows the low-shot
performance for the two endoscopic tasks.
each experiment is repeated three times with a random sample of train
videos, and we report the mean and standard deviation (shaded area).
when examining the cholecystectomy phase recognition task, it is evident
that we can achieve comparable frame-level performance by using only 12% of
the annotated videos.
we see that 50% random masking (i.e. we keep 98 tokens out of
196 for the global view) and using 4 local views gives the best of performance.
we study the eﬀect of data augmentation.
ssl augmentation pipelines
have been developed on imagenet-1k
[7], hence, it is important to re-evaluate
these choices for medical images.
surprisingly, we see that augmentations primar-
ily found to work well on imagenet-1k are also eﬀective on laparoscopic videos
(e.g. color jiterring and horizontal ﬂips).
in table2e), we look at the eﬀect of
the training length when starting from scratch or from a good ssl pretrained
checkpoint on imagenet-1k.
we observe that excellent performance is achieved
with only 10 epochs of ﬁnetuning on medical data when starting from a strong
dino checkpoint [6]. table 2g) shows that imagenet-1k dino is a solid starting
point compared to other alternatives
a) number of prototypes
d) data augmentation
f) avoiding collapse.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_1.pdf:
to solve
this issue, we constructively introduce the segmentation of gv into the
classiﬁcation framework and propose the region-constraint module and
cross-region attention module for better feature localization and to learn
the correlation of context information.
we also collect a gv bleeding
risks rating dataset (gvbleed) with 1678 gastroscopy images from 411
patients that are jointly annotated in three levels of risks by senior clin-
ical endoscopists.
the experiments on our collected dataset show that
our method can improve the rating accuracy by nearly 5% compared to
the baseline.
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43904-9_1.
https://doi.org/10.1007/978-3-031-43904-9_1
4
y. jiang et al.
keywords: gastric varices · bleeding risk rating · cross-region
attention
1
introduction
esophagogastric varices are one of the common manifestations in patients with
liver cirrhosis and portal hypertension and occur in about 50 percent of patients
with liver cirrhosis [3,6].
it is crucial to iden-
tify high-risk patients and oﬀer prophylactic treatment at the appropriate time.
regular endoscopy examinations have been proven an eﬀective clinical approach
to promptly detect esophagogastric varices with a high risk of bleeding [7]. dif-
ferent from the grading of esophageal varices (ev) that is relatively complete
[1], the bleeding risk grading of gastric varices (gv) involves complex variables
including the diameter, shapes, colors, and locations.
although the existing rating systems tried to identify
the risk from diﬀerent perspectives, they still lack clear quantiﬁcation standard
and heavily rely on the endoscopists’ subjective judgment.
intuitively we may regard the gv bleeding risk rating as an image
classiﬁcation task and apply typical classiﬁcation architectures (e.g., resnet
however, they may
raise poor performance due to the large intra-class variation between gv with
the same bleeding risk and small inter-class variation between gv and normal
tissue or gv with diﬀerent bleeding risks.
also, since the gv images are taken from diﬀerent
distances and angles, the number of pixels of the gv area may not reﬂect its
actual size.
3. to encourage the model to learn more
robust representations, we constructively introduce segmentation into the clas-
siﬁcation framework.
with the segmentation information, we further propose a
region-constraint module (rcm) and a cross-region attention module (cram)
for better feature localization and utilization.
the segmentation results to constrain the cam heatmaps of the feature maps
extracted by the classiﬁcation backbone, avoiding the model making predictions
based on incorrect areas.
in cram, the varices features are extracted using the
segmentation results and combined with an attention mechanism to learn the
intra-class correlation and cross-region correlation between the target area and
the context.
while most works and public datasets focus on colonoscopy
[13,15] and esophagus [5,9], with a lack of study on gastroscopy images.
in the
public dataset of endocv challenge [2], the majority are colonoscopies while only
few are gastroscopy images.
in this work, we collect a gv bleeding risks rating
dataset (gvbleed) that contains 1678 gastroscopy images from 411 patients
with diﬀerent levels of gv bleeding risks.
three senior clinical endoscopists are
invited to grade the bleeding risk of the retrospective data in three levels and
annotated the corresponding segmentation masks of gv areas.
in sum, the contributions of this paper are: 1) a novel gv bleeding risk rating
framework that constructively introduces segmentation to enhance the robust-
ness of representation learning; 2) a region-constraint module for better feature
localization and a cross-region attention module to learn the correlation of tar-
get gv with its context; 3) a gv bleeding risk rating dataset (gvbleed) with
high-quality annotation from multiple experienced endoscopists.
experimental
results demonstrate the eﬀectiveness of our proposed framework and modules,
where we improve the accuracy by nearly 5% compared to the baseline model.
6
y. jiang et al.
fig.
the framework consists of a segmentation module, a cross-region attention module,
and a region constraint module.
2, which consists
of a segmentation module (sm), a region constraint module (rcm), and a cross-
region attention module (cram).
given a gastroscopy image, the sm is ﬁrst
applied to generate the varices mask of the image.
then, the image together
with the mask are fed into the cram to extract the cross-region attentive
feature map, and a class activation map (cam) is calculated to represent the
concentrated regions through rcm.
finally, a simple classiﬁer is used to predict
the bleeding risk using the extracted feature map.
2.1
segmentation module
due to the large intra-class variation between gv with the same bleeding risk
and small inter-class variation between gv and normal tissue or gv with diﬀer-
ent bleeding risks, existing classiﬁcation models exhibit poor perform and tend
to lose focus on the gv areas.
to solve this issue, we ﬁrst embed a segmentation
network into the classiﬁcation framework.
[11] as the segmentation network, considering its great per-
formance, and calculate the diceloss between the segmentaion result mp and
ground truth mask of vaices region mgt for optimizing the network:
lse = 1 −
2σmp ∗ mgt
σm 2p
automatic bleeding risk rating system of gastric varices
7
a straightforward strategy to utilize the segmentation mask is directly using
it as an input of the classiﬁcation model, such as concatenating the image with
the mask as the input.
although such strategy can improve the classiﬁcation
performance, it may still lose focus in some hard cases where the gv area can
hardly be distinguished.
to further regularize the attention and fully utilize the
context information around the gv area, on top of the segmentation framework
we proposed the cross-region attention module and the region-constraint module.
the cram
consists of an image encoder fim, a varices local encoder fvl and a varices global
encoder fve.
given the image i and the predicted varices mask mp, a feature
extraction step is ﬁrst performed to generate the image feature vm, the local
varices feature vvl and global varices feature vvg:
vm = fim(i),
vvl = fvl(i ∗ mp),
vvg = fvg(concat[i, mp]),
(2)
then, through similarity measuring, we can compute the attention with
a = (vvl)t vvg,
wij =
exp(aij)
σp(exp(apj)),
(3)
which composes of two correlations: self-attention over varices regions and cross-
region attention between varices and background regions.
the training process of the proposed network consists of three steps: 1) the
segmentation network is trained ﬁrst; 2) the ground-truth segmentation masks
and images are used as the inputs of the cram, the classiﬁcation network,
including cram and rcm, are jointly trained; 3) the whole framework is
jointly ﬁne-tuned.
3
gvbleed dataset
data collection and annotation.
the gvbleed dataset contains 1678 endo-
scopic images with gastric varices from 527 cases.
in the current version, images from patients with ages elder than 18 are
retained1.
the images are selected from the raw endoscopic videos and frames.
to maximize the variations, non-consecutive frames with larger angle diﬀerences
are selected.
2) moderate: moderate risk of
bleeding, and endoscopic treatment is necessary, with relatively low endoscopic
treatment diﬃculty (usually with a diameter between 5 mm and 10 mm).
3)
severe: high risk of bleeding and endoscopic treatment is necessary, with high
endoscopic treatment diﬃculty.
note that
the diameter is only one reference for the ﬁnal risk rating since the gv is with
1 please refer to the supplementary material for more detailed information about our
dataset.
the gvbleed dataset is partitioned into training and testing sets for evalu-
ation, where the training set contains 1337 images and the testing set has 341
images.
4
experiments
4.1
implementation details
in experiments, the weights ωs, ωco, and ωcl of the segmentation loss, region
constraint loss, and classiﬁcation loss are set to 0.2, 1, and 1, respectively.
the
details of the three-step training are as follows: 1) segmentation module: we
trained the segmentation network for 600 epochs, using adam as the optimizer,
and the learning rate is initialized as 1e−3 and drops to 1e−4 after 300 epochs.
2) cross-region attention module and region constraint module: we
used the ground-truth varices masks and images as the inputs of the cram,
and jointly trained the cram and rcm for 100 epochs.
in addition, common data augmentation techniques
such as rotation and ﬂipping were adopted here.
however, the
transformer-based models achieves much worse performances since they always
require more training data, which is not available in our task.
the simple cnn models as baselines since they achieve better performances.
by introducing the segmentation of gv into the frame-
work, concatenating the image with its segmentation mask as the inputs of the
classiﬁer can improve the classiﬁcation accuracy by 1.2%.
with the help of cram,
the performance of the model can be further improved.
although the model can
extract more important context information at the varices regions, the perfor-
mance improvement is not very large since the focus ability is not the best and
the model may still make predictions based on the incorrect regions for some
hard images.
by adding the rcm to the cram, the focus ability of the model
can be further improved, and thus the model has a signiﬁcant improvement in
performance by 5% compared to the baseline model, this proves the eﬀective-
ness of our proposed modules.
note that, the baseline model tends to predict
the images as severe, thus the f1-score of severe is high but the f1-scores of mild
and moderate are signiﬁcantly lower than other models.
more quantitative and
visualization results are shown in supplementary material.
in addition, given the
input image with resolution 512 × 512, the parameters and computational cost
of our framework are 40.2m, and 52.4g macs, and 29 ms inference time for a
single image on gpu rtx2080.
due to the large intra-class variation between gv with the same bleed-
ing risk and small inter-class variation between gv and normal tissue or gv with
diﬀerent bleeding risks, existing classiﬁcation models cannot correctly focus on
the varices regions and always raise poor performance.
to solve this issue, we
constructively introduce segmentation to enhance the robustness of representa-
tion learning.
the experiments on
our dataset demonstrated the eﬀectiveness and superiority of our framework.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_50.pdf:
bias in healthcare negatively impacts marginalized popula-
tionswithlowersocioeconomicstatusandcontributestohealthcareinequal-
ities.
eliminating bias in ai models is crucial for fair and precise medical
implementation.
the development of a holistic approach to reducing bias
aggregationinmultimodalmedicaldataandpromotingequityinhealthcare
ishighlydemanded.racialdisparitiesexistinthepresentationanddevelop-
mentofalgorithmsforpulmonaryembolism(pe),anddeepsurvivalpredic-
tionmodelcanbede-biasedwithmultimodaldata.inthispaper,wepresent
a novel survival prediction (sp) framework with demographic bias disen-
tanglement for pe.
the ctpa images and clinical reports are encoded by
the state-of-the-art backbones pretrained with large-scale medical-related
tasks.
the proposed de-biased sp modules eﬀectively disentangle latent
race-intrinsic attributes from the survival features, which provides a fair
survival outcome through the survival prediction head.
we evaluate our
method using a multimodal pe dataset with time-to-event labels and race
identiﬁcations.
the comprehensive results show an eﬀective de-biased per-
formance of our framework on outcome predictions.
keywords: pulmonary embolism · deep survival prediction ·
de-bias learning · multi-modal learning
1
introduction
bias in medicine has demonstrated a notable challenge for providing comprehen-
sive and equitable care.
implicit biases can negatively aﬀect patient care, particu-
larly for marginalized populations with lower socioeconomic status [30]. evidence
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43904-9_50.
https://doi.org/10.1007/978-3-031-43904-9_50
516
z. zhong et al.
has demonstrated that implicit biases in healthcare providers could contribute
to exacerbating these healthcare inequalities and create a more unfair system for
people of lower socioeconomic status
[30]. based on the data with racial bias, the
unfairness presents in developing evaluative algorithms.
using biased data for ai models
reinforces racial inequities, worsening disparities among minorities in healthcare
decision-making [22].
within the radiology arm of ai research, there have been signiﬁcant advances
in diagnostics and decision making [19].
along these advancements, bias in
healthcare and ai are exposing poignant gaps in the ﬁeld’s understanding of
model implementation and their utility [25,26].
ai model quality relies on input
data and addressing bias is a crucial research area.
systemic bias poses a greater
threat to ai model’s applications, as these biases can be baked right into the
model’s decision process
[22].
pulmonary embolism (pe) is an example of health disparities related to race.
black patients exhibit a 50% higher age-standardized pe fatality rate and a
twofold risk for pe hospitalization than white patients [18,24].
the pulmonary embolism severity index (pesi) is a
well-validated clinical tool based on 11 clinical variables and used for outcome pre-
diction measurement [2].
[7,12,14].
however, one issue with traditional survival analysis is bias from single modal
data that gets compounded when curating multimodal datasets, as diﬀerent
combinations of modes and datasets create with a uniﬁed structure.
multimodal
data sets are useful for fair ai model development as the bias complementary
from diﬀerent sources can make de-biased decisions and assessments.
in that
process, the biases of each individual data set will get pooled together, creating
a multimodal data set that inherits multiple biases, such as racial bias [1,15,23].
in addition, it has been found that creating multimodal datasets without any de-
biasing techniques does not improve performance signiﬁcantly and does increase
bias and reduce fairness [5]. overall, a holistic approach to model development
would be beneﬁcial in reducing bias aggregation in multimodal datasets.
[4] for bias disentanglement
improves model generalization for fairness [3,6,27].
we developed a pe outcome model that predicted mortality and detected
bias in the output.
we then implemented methods to remove racial bias in our
dataset and model and output unbiased pe outcomes as a result.
our contri-
butions are as follows: (1) we identiﬁed bias diversity in multimodal informa-
tion using a survival prediction fusion framework.
(2) we proposed a de-biased
survival prediction framework with demographic bias disentanglement.
(3) the
multimodal cph learning models improve fairness with unbiased features.
de-biased outcome prediction model
517
fig.
1. overview of the survival prediction (sp) framework and the proposed de-
biased sp module (lower right).
id branch (ei;ci) and survival branch (ec;cc) are
trained to disentangle race-intrinsic attributes and survival attributes with the feature
swapping augmentation, respectively.
the survival head predicts the outcomes based
on the de-biased survival attributes.
2
bias in survival prediction
this section describes the detail of how we identify the varying degrees of bias
in multimodal information and illustrates bias using the relative diﬀerence in
survival outcomes.
we will ﬁrst introduce our pulmonary embolism multimodal
datasets, including survival and race labels.
then, we evaluate the baseline sur-
vival learning framework without de-biasing in the various racial groups.
dataset.
the pulmonary embolism dataset used in this study from 918 patients
(163 deceased, median age 64 years, range 13–99 years, 52% female), including
3978 ctpa images and 918 clinical reports, which were identiﬁed via retro-
spective review across three institutions.
for
each patient, the race labels, survival time-to-event labels and pesi variables
are collected from clinical data, and the 11 pesi variables are used to calcu-
late the pesi scores, which include age, sex, comorbid illnesses (cancer, heart
failure, chronic lung disease), pulse, systolic blood pressure, respiratory rate,
temperature, altered mental status, and arterial oxygen saturation at the time
of diagnosis
[2].
diverse bias of multimodal survival prediction model.
the
frameworks without de-basing are evaluated for risk prediction in the test set
by performing survival prediction on ctpa images, clinical reports, and clini-
cal variables, respectively.
first, we use two large-scale data-trained models as
backbones to respectively extract features from preprocessed images and cleaned
clinical reports.
to encode survival features zm
sur from image,
text and pesi variables, these modules are trained to distinguish critical disease
from non-critical disease with cox partial log-likelihood loss (coxphloss)
we evaluate the performance of each module with
concordance probability (c-index), which measures the accuracy of prediction
in terms of ranking the order of survival times
when debiasing is not performed, signif-
icant diﬀerences exist among the diﬀerent modalities, with the image modality
exhibiting the most pronounced deviation, followed by text and pesi variables.
the biased performance of the imaging-based module is likely caused by the rich-
ness of redundant information in images, which includes implicit features such as
body structure and posture that reﬂect the distribution of diﬀerent races.
this
redundancy leads to model overﬁtting on race, compromising the fairness of risk
prediction across diﬀerent races.
besides, clinical data in the form of text reports
and pesi variables objectively reﬂect the patient’s physiological information and
the physician’s diagnosis, exhibiting smaller race biases in correlation with sur-
vival across diﬀerent races.
3
de-biased survival prediction model
based on our sp baseline framework and multimodal ﬁndings from sect.
2, we
present a feature-level de-biased sp module that enhances fairness in survival
de-biased outcome prediction model
519
outcomes by decoupling race attributes, as shown in the lower right of fig.
1.
in the de-biased sp module, ﬁrstly, two separate encoders em
i
and em
c are for-
mulated to embed features f m into disentangled latent vectors for race-intrinsic
attributes zid or race-conﬂicting attributes zsur implied survival information [16].
then, the linear classiﬁers cm
i
and cm
c constructed to predict the race label yid
with concatenated vector z =
to disentangle survival features from
the race identiﬁcation, we use the generalized cross-entropy (gce) loss
[31] to
train em
c and cm
c to overﬁt to race label while training em
i
and cm
i
with cross-
entropy (ce) loss.
the relative diﬃculty scores w as deﬁned in eq. 1 reweight
and enhance the learning of the race-intrinsic attributes [20].
the objective func-
tion for disentanglement shown in eq. 2, but the parameters of id or survival
branch are only updated by their respective losses:
w(z) =
ce (cc(z), yid)
+ gce (cc(z), yid)
(2)
to promote race-intrinsic learning in em
i
and cm
i , we apply diversify with
latent vectors swapping.
as the random combination are
generated from diﬀerent samples, the swapping decreases the correlation of these
feature vectors, thereby enhancing the race-intrinsic attributes.
the loss func-
tions of swapping augmentation added to train two neural networks is deﬁned
as:
lsw = w(z)ce (ci(zsw), yid)
the weights λsw and λsur are assigned as 0.5 and 0.8, respectively,
to balance the feature disentanglement and survival prediction.
4
experiment
we validate the proposed de-biased survival prediction frameworks on the col-
lected multi-modality pe data.
performance comparison of the proposed de-biased sp framework and base-
line using c-index values on multiple modal outcomes.
the larger c-index value is
better and the lower bias is fairer.
method
baseline
de-biased sp model
dataset
overall white color bias
overall white color bias
imaging
0.662
0.736
0.422
0.314 0.646
0.656
0.622
0.035
text
0.657
0.642
0.714
0.071 0.719
0.689
0.746
0.057
variable
0.668
0.669
0.741
0.072 0.698
0.683
0.778
0.095
multimodal 0.709
0.692
we apply race-balanced resam-
pling to the training and validation sets to eliminate training bias caused by
minority groups.
the lung region of cpta images is extracted with a slice thickness of 1.25 mm
and scaled to n × 512 × 512 pixels [10].
the penet is pre-trained on large-scale
ctpa studies and shows excellent pe detection performance with an auroc
of 0.85 on our entire dataset.
the 2048 dimensional features from the last convo-
lution with the highest probability of pe, are designated as the imaging features.
we build the encoders of the baseline sp modules and de-biased sp modules
with multi-layer perceptron (mlp) neural networks and relu activation.
the
mlps with 3 hidden layers are used to encode image and text features, and
another mlps with 2 layers encodes the features of pesi variables.
for training the biased and de-biased sp modules, we collect data from one
modality as a batch with synchronized batch normalization.
the sp modules
are optimized using the adamw [17] optimizer with a momentum of 0.9, a
weight decay of 0.0005, and a learning rate of 0.001.
experiments are conducted
on an nvidia gv100 gpu.
de-biased outcome prediction model
521
fig.
based on the com-
parison between the id features and others, it is observed that the clusters containing
race obtained from the same class are more compact.
3. kaplan-meier survival curves of our 3 de-biased sp modules and the multi-
modal coxph model.
(color ﬁgure online)
4.1
results
table 1 shows the quantitative comparisons of the baseline and de-biased frame-
works with the c-indexes of the multimodal survival predictions.
in general, our
framework including de-biased sp modules shows signiﬁcantly better predictions
in testing set than the pesi-based outcome estimation with c-indexes of 0.669,
0.654, 0.697, 0.043 for the overall testset, white testset, color testset and race
bias.
the de-biased results outperform the baseline in overall survival c-index
and show a lower race bias, especially in imaging- and fusion-based predictions.
the results indicate the eﬀectiveness of the proposed de-biasing in mitigating
race inequity.
the results also prove the observations for the diﬀerent biases
present in diﬀerent modalities, especially in the ctpa images containing more
abundant race-related information.
it also explains the limited eﬀectiveness of
de-biasing the clinical results, which contain less racial identiﬁcation.
every 2 columns (overall performance of testing
and bias) represent a training setting.
swapping
×
✓
×
✓
resampling
×
×
✓
✓
dataset
testing bias
testing bias
testing bias
testing bias
imaging
0.666
0.062 0.641
0.014 0.649
0.050 0.622
0.035
text
0.684
0.090 0.711
0.123 0.698
0.102 0.709
0.057
variable
0.702
0.095 0.701
0.052 0.697
0.082 0.699
0.095
multimodal 0.716
0.025 0.737
0.041 0.741
0.011 0.743
0.012
diction performance based on multiply modalities is signiﬁcantly better than the
pesi-based outcome estimation.
the disentangled representations, transformed
from latent space to a 2d plane via tsne and color-coded by race [9], are shown
in fig.
2. we observe the disentanglement in the visualization of the id features
zid, while the survival features zsur eliminate the race bias.
the lack of appar-
ent race bias observed in both the original features and those encoded in the
baseline can be attributed to the subordinate role that id features play in the
multimodal information.
in addition,
the predictions of the de-biased framework show favorable performance, and our
multimodal fusion demonstrates a more pronounced discriminative ability in the
k-m survival analysis compared to the single-modal results.
we conducted ablation studies to examine the eﬀect of the two key compo-
nents, including swapping feature augmentation and race-balance resampling.
as shown in table 2, the diﬀerent training settings show signiﬁcant diﬀerences
in survival prediction performance across modalities.
the swapping augmenta-
tion provides a strong bias correction eﬀect for image data with obvious bias.
for clinical data, the resampling generally improves performance in most cases.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_52.pdf:
we propose a novel text-guided cross-position attention
module which aims at applying a multi-modality of text and image to
position attention in medical image segmentation.
to match the dimen-
sion of the text feature to that of the image feature map, we multi-
ply learnable parameters by text features and combine the multi-modal
semantics via cross-attention.
it allows a model to learn the dependency
between various characteristics of text and image.
our proposed model
demonstrates superior performance compared to other medical models
using image-only data or image-text data.
the rois obtained from the model contribute
to improve the performance of classiﬁcation models.
keywords: image segmentation · multi modal learning · cross
position attention · text-guided attention · medical image
1
introduction
advances in deep learning have been witnessed in many research areas over
the past decade.
in medical ﬁeld, automatic analysis of medical image data has
actively been studied.
in particular, segmentation which identify region of inter-
est (roi) in an automatic way is an essential medical imaging process.
thus,
deep learning-based segmentation has been utilized in various medical domains
such as brain, breast cancers, and colon polyps.
among the popular architec-
tures, variants of u-net have been widely adopted due to their eﬀective encoder-
decoder structure, proﬁcient at capturing the characteristics of cells in images.
recently, it has been demonstrated that the attention modules [4,17,20] enable
deep learning networks to better extract robust features, which can be applied in
medical image segmentation to learn subtle medical features and achieve higher
performance [14,16,18,21].
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. (eds.): miccai 2023, lncs 14224, pp.
lee et al.
however, as image-only training trains a model with pixels that constitute
an image, there is a limit in extracting ﬁne-grained information about a target
object even if transfer learning is applied through a pre-trained model.
recently,
to overcome this limitation, multi-modality studies have been conducted, aiming
to enhance the expressive power of both text and image features.
for instance,
clip [12] used contrastive learning based on image-text pairs to learn the sim-
ilarity between the image of an object and the text describing it, achieving
signiﬁcant performance gains in a variety of computer vision problems.
the trend of text-image multi-modality-based research on image processing
has extended to the medical ﬁeld.
[19] proposed a semantic matching loss that
learns medical knowledge to supplement the disadvantages of clip that cannot
capture uncertain medical semantic meaning.
in [2], they trained to increase
the similarity between the image and text by calculating their inﬂuence on each
other as a weighted feature.
for the segmentation task, lvit
furthermore,
it proposed a double u-shaped structure consisting of a u-shaped vit that
combines image and text information and a u-shaped cnn that produces a
segmentation mask.
however, when combining medical images with non-ﬁne-
grained text information, noise can aﬀect the outcome.
in this paper, we propose a new text-guided cross-position attention mod-
ule (cpam t g) that combines text and image.
in a medical image, a position
attention module (pam) eﬀectively learns subtle diﬀerences among pixels.
we
utilized pam which calculates the inﬂuence among pixels of an image to capture
the association between text and image.
to this end, we converted the global
text representation generated from the text encoder into a form, such as an
image feature map, to create keys and values.
the image feature map generated
from an image encoder was used as a query.
learning the association between
text and image enables us to learn positional information of targets in an image
more eﬀectively than existing models that learned multi-modality from medical
images.
cpam t g showed an excellent segmentation performance in our com-
prehensive experiments on various medical images, such as cell, chest x-ray, and
magnetic resonance image (mri).
our main contributions are as follows:
– we devised a text-guided cross-position attention module (cpam t g) that
eﬃciently combines text information with image feature maps.
– we demonstrated the eﬀect of cpam t g on segmentation for various types
of medical images.
– for a practical computer-aided diagnosis system, we conﬁrm the eﬀectiveness
of the proposed method in a deep learning-based sacroiliac arthritis diagnosis
system.
text-guided cross-position attention for segmentation
539
fig.
1. overview of our proposed segmentation model.
2
methods
in this section, we propose text-guided segmentation model that can eﬀectively
learn the multi-modality of text and images.
figure 1 shows the overall architec-
ture of the proposed model, which consists of an image encoder for generating a
feature map from an input image, a text encoder for embedding a text describing
the image, and a cross-attention module.
the cross-attention module allows the
text to serve as a guide for image segmentation by using the correlation between
the global text representation and the image feature map.
to achieve robust text
encoding, we adopt a transformer [17] structure which performs well in natural
language processing (nlp).
for image encoding and decoding, we employed
u-net, widely used as a backbone in medical image segmentation.
to train our
proposed model, we utilize a dataset consisting of image and text pairs.
2.1
conﬁguration of text-image encoder and decoder
as transformer has demonstrated its eﬀectiveness in handling the long-range
dependency in sequential data through self-attention [1], it performs well in
various ﬁelds requiring nlp or contextual information analysis of data.
we
used a transformer (encodert ) to encode the semantic information of the text
describing a medical image into a global text representation vt ∈ r1×2c as
vt = encodert (t).
here, the text semantics (t) can be a sentence indicating
the location or characteristics of an interested region in an image such as a lesion
shown in fig.
1.
to create a segmentation mask from medical images (i), we used u-net
[13]
which has a relatively simple yet eﬀective structure for biomedical image segmen-
540
g.-e.
[15] as the encoder (encoderi) to obtain the image feature
fi ∈
rc×h×w as fi = encoderi(i) and the decoder (decoderi) that will
generate the segmented image from the enhanced encoding vector obtained by
the cross-position attention which will be described in the following subsection.
the weights of text and image encoders were initialized by the weights of
clip’s pre-trained transformer and vgg16 pre-trained on imagenet, respec-
tively, and ﬁne-tuned by a loss function for segmentation which will be described
in sect.
[5] to
combine the semantic information of text and image.
this module utilizes not
only the image feature map from the image encoder but also the global text
representation from the text encoder to learn the dependency between various
characteristics of text and image.
in particu-
lar, this correlation analysis among pixels can eﬀectively analyze medical images
in which objects are relatively ambiguous compared to other types of natural
images.
by the global
text representation (vt ) to match the dimension of the text feature with that of
the image feature map as ft = r(g(vt )⊺ × l), where g(·) is a fully connected
layer that adjusts the 2c channel of the global text representation vt to the
image feature map channel c. r(·) is a reshape operator to c × h × w.
text-guided cross-position attention for segmentation
541
the text feature map ft is used as key and value, and the image feature
map fi is used as a query to perform self-attention as
q = hq(fi),
k = hk(ft ),
v = hv (ft ),
(1)
where hq, hk, and hv are convolution layers with a kernel size of 1, and q,
k, and v are queries, keys, and values for self-attention.
attention = softmax(q⊺k)
(2)
cpam t g = attention⊺v + fi
(3)
finally, by upsampling the low-dimensional cpam t g obtained through cross-
attention of text and image together with skip-connection, more accurate seg-
mentation prediction can express the detailed information of an object.
3
experiments
3.1
setup
medical datasets.
[8] contains
30 digital microscopic tissue images of several patients and qata-cov19 are
covid-19 chest x-ray images.
among all mri slices, we selected the gadolinium-
enhanced fat-suppressed t1-weighted oblique coronal images, excluding the ﬁrst
and last several slices in which the pelvic bones did not appear, and added the
text annotations for the slices.
training and metrics.
for a better training, data augmentation was used.
we
randomly rotated images by −20◦ ∼ +20◦ and conducted a horizontal ﬂip with
0.5 probability for only the monuseg and qata-cov19 datasets.
the mdice and miou metrics, widely
used to measure the performance of segmentation models, were used to evaluate
the performance of object segmentation.
for experiments, pytorch (v1.7.0) were
used on a computer with nvidia-v100 32 gb gpu.
3.2
segmentation performance
table 1 presents the comparison of image segmentation performance among
the proposed model and the u-net
analyzing the results in table 1, unlike nat-
ural image segmentation, the attention module-based method (attention u-net)
and transformer-based method (medt) did not achieve signiﬁcant performance
gains compared to u-net based methods (u-net and u-net++).
by contrast, lvit and cpam t g, which utilize both text and image informa-
tion, signiﬁcantly improved image segmentation performance because of multi-
modal complementarity, even for medical images with complex and ambiguous
object boundaries.
furthermore, cpam t g achieves a better performance by 1
to 3% than lvit
this means that the proposed cpam t g
helps to improve segmentation performance by allowing text information to serve
as a guide for feature extraction for segmentation.
figure 3 shows the examples of segmentation masks obtained using each
method.
3 shows that cpam t g
and lvit, which use text information together for image segmentation, create a
segmentation mask with more distinctive borders than other methods.
figure 3 also shows that even on the qata-cov19 and
monuseg datasets, cpam t g predicted the most accurate segmentation masks
(see the red box areas).
from these results, we conjecture that the reasons for the
performance improvement of cpam t g are as follows.
cpam t g independently
encodes the input text and image and then combines semantic information via
a cross-attention module.
consequently, the two types of information (text and
image) do not act as noise from each other, and cpam t g achieves an improved
performance compared to lvit.
in addition, we investigated whether text
information about images serves as a guide in the position attention process for
image segmentation by comparing it with cpam t g. table 2 summarizes the
result of each case.
as can be observed in table 2, the performance of pam
was higher than that of the backbone.
this indicates that pam improves per-
formance by learning associations between pixels for ambiguous targets, as in
medical images.
in addition, the best performance results of cpam t g show
that text information provided helpful information in an image segmentation
process using the proposed model.
text-guided cross-position attention for segmentation
543
table 1.
performance comparison of medical segmentation models with three datasets
qata-cov19
monuseg
sij
mdice
miou
mdice
miou
mdice
miou
u-net
0.7902
0.6946
0.7645
0.6286
0.7395
0.6082
u-net++ 0.7962
0.7025
0.7701
0.6304
0.7481
0.6124
attunet
0.7931
3. qualitative results of segmentation models.
3.4
application: deep-learning based disease diagnosis
in this section, we conﬁrm the eﬀectiveness of the proposed segmentation method
through a practical bio-medical application as a deep learning-based active
sacroiliitis diagnosis system.
we segmented the pelvic bones in mri slices using the proposed method
to construct a fully automatic deep learning-based active sacroiliitis diagnosis
system, including roi settings from mri input images.
“pam” means
we used it instead of cpam t g for image-only training.
as presented in table 3, compared to the case of using
the original mri image without the roi setting, using the hand-crafted roi
patch [9] showed an average of 7% higher performance in recall, precision, and
f1.
it is noticeable that the automatically set roi patch showed similar or better
performance than the manual roi patch for each measurement.
4
conclusion
in
this
study,
we
developed
a
new
text-guided
cross-attention
module
(cpam t g) that learns text and image information together.
the proposed
model has a composite structure of position attention and cross-attention in
that the key and value are from text data, and the query is created from the
image.
we use a learnable parameter to convert text features into a tensor of
the same dimension as the image feature map to combine text and image infor-
mation eﬀectively.
by calculating the association between the reshaped global
text representation and each component of the image feature map, the proposed
method outperformed image segmentation performance compared to previous
studies using both text and image or image-only training method.
we also con-
ﬁrmed that it could be utilized for a deep-learning-based sacroiliac arthritis
text-guided cross-position attention for segmentation
545
diagnosis system, one of the use cases for practical medical applications.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_3.pdf:
the rapid identiﬁcation and accurate diagnosis of breast can-
cer, known as the killer of women, have become greatly signiﬁcant for
those patients.
numerous breast cancer histopathological image classiﬁ-
cation methods have been proposed.
(1) these methods can only hand high-resolution (hr) images.
however, the low-resolution (lr) images are often collected by the dig-
ital slide scanner with limited hardware conditions.
compared with hr
images, lr images often lose some key features like texture, which
deeply aﬀects the accuracy of diagnosis.
(2) the existing methods have
ﬁxed receptive ﬁelds, so they can not extract and fuse multi-scale fea-
tures well for images with diﬀerent magniﬁcation factors.
to ﬁll these
gaps, we present a single histopathological image super-resolution
classiﬁcation network (shisrcnet), which consists of two modules:
super-resolution (sr) and classiﬁcation (cf) modules.
sr module
reconstructs lr images into sr ones.
cf module extracts and fuses
the multi-scale features of sr images for classiﬁcation.
in the training
stage, we introduce hr images into the cf module to enhance shis-
rcnet’s performance.
finally, through the joint training of these two
modules, super-resolution and classiﬁed of lr images are integrated into
our model.
the experimental results demonstrate that the eﬀects of our
method are close to the sota methods with taking hr images as inputs.
keywords: breast cancer · histopathological image · super-resolution ·
classiﬁcation · joint training
1
introduction
breast cancer is one of the high-mortality cancers among women in the 21st
century.
every year, 1.2 million women around the world suﬀer from breast
cancer and about 0.5 million die of it
https://doi.org/10.1007/978-3-031-43904-9_3
24
l. xie et al.
will make a correct assessment of the patient’s risk and improve the chances
of survival.
due to being collected by various devices, the resolution of histopathologi-
cal images extracted may not always be high.
low-resolution (lr) images lack
of lots of details, which will have an important impact on doctors’ diagnosis.
considering the improvement of histopathological images’ acquisition equipment
will cost lots of money while signiﬁcantly increasing patients’ expense of detec-
tion.
the super-resolution (sr) algorithms that improve the resolution of lr
images at a small cost can be a practical solution to assist doctors in diagno-
sis.
[9] and multi-scale reﬁned context to improve the eﬀect of
reconstructing histopathological images.
this limits its performance in the scenarios
with various magniﬁcation factors.
therefore, designing an appropriate feature
extraction block for sr of the histopathological images is still a challenging task.
in recent years, a series of deep learning methods have been proposed to
solve the breast cancer histopathological image classiﬁcation issue by the high-
resolution (hr) histopathological images.
[12,21,22] improved the speciﬁc model
structure to classify breast histopathology images, which showed a signiﬁcant
improvement in recognition accuracy compared with the previous works [1,20].
so it is still worth to explore the potential of extraction and fusion of multi-scale
features for breast images classiﬁcation.
to tackle the problem of lr breast cancer histopathological images recon-
struction and diagnosis, we propose the single histopathological image super-
resolution classiﬁcation network (shisrcnet) integrating super-resolution
(sr) and classiﬁcation (cf) modules.
these make mfeblock reconstruct lr images into sr images well.
(2) the cf module completes the task of image classiﬁcation by utilizing the
sr images.
(3) through the joint training of these two designed modules, the super-
resolution and classiﬁcation of low-resolution histopathological images are inte-
grated into our model.
for improving the performance of cf module and reduc-
ing the error caused by the reconstructed sr images, we introduce hr images to
cf module in the training stage.
the experimental results demonstrate that the
eﬀects of our method are close to those of sota methods that take hr breast
cancer histopathological images as inputs.
the sr module reconstructs the lr image into the sr image.
the
cf module utilize the reconstructed sr images to diagnose histopathological
images.
in the training stage, we introduce hr images to improve the perfor-
mance of cf module and alleviate the error caused by sr images.
[11], srmfenet takes
a single low-resolution image as input and uses the pixelshuﬄe layer to get the
restructured image.
the diﬀerence between srmfenet and srresnet is that
a multi-features extraction block (mfeblock) is proposed to extract and fuse
multi-scale histopathological images’ features.
i <= n
where n is the number of atrous convolutions and is set to 4 by the experiments.
it is beneﬁcial for the network to extract shallow local tex-
ture information and global semantic information.
firstly, we conduct global average pooling (gap)
[14] on the multi-scale features to obtain their average channel-wise weights.
next soft-
max operation normalizes the same position of the obtained multi-scale average
channel-wise weights.
mfeblock is very applicable to process histopatho-
logical images of diﬀerent magniﬁcation factors, as it employs convolution and
attention operations to capture local and global image context information and
fuse them well.
2.2
classiﬁcation module
the task of the cf module is to classify the reconstructed sr images.
in csfblock, the upsampling operations are performed on
shisrcnet
27
the low-resolution features xl to realize consistency with xh dimension.
xh and
restructured xl are fused via an element-wise summation:
u = xh + up(xl)
then, using gap along the channel dimension to get the global information
s. a fc layer generates a compact feature vector z which guides the feature
selection procedure.
and z is reconstructed into two weight vectors a, b of the
same dimension as s through two fc layers, which can be deﬁned as:
z = δ(wcs),
a = waz,
b = wbz
where δ denotes relu and wa, wb, wc, means the weight of the fc layers.
in the cf module,
we introduce hr images to cf module in the training stage for improving the
performance of cf module and reducing the error caused by the reconstructed
sr images.
[16] to alleviate the class imbalanced data problem
of the hr and sr images’ classiﬁcation.
[5], the hr and sr of the same images are similar to two
diﬀerent views.
in the inference stage, only sr images are taken as inputs
by cf module.
in our experiment, λ1, λ2 and λ3 are set to 0.6, 0.3 and 0.1,
respectively.
and the temperature parameter in nt − xent loss is set to 0.5.
3
experiment
dataset: this work uses the breast cancer histopathological image database
(breakhis)1 [20].
the images in the dataset have four magniﬁcation factors
1 https://web.inf.ufpr.br/vri/databases/breast-cancer-histopathological-database-
breakhis/.
28
l. xie et al.
(40x, 100x, 200x, 400x) and eight breast cancer classes.
implementation details: for all experiments, we conduct 5-fold cross valida-
tion, and report the mean.
we use lr histopathological images with size 48 × 48,
96 × 96, 192 × 192 as input for diﬀerent single image sr tasks (x8, x4, x2) and
set batch size to 8.
for the corresponding lr and hr images in the training
dataset, the same data augmentation is adopted, such as rotation, color jitter.
[6] to evalu-
ate the performance of the sr model.
and the joint train-
ing of srmfenet and cf module improves the performance of super-resolution.
we compare our introduced cf module with ﬁve state-of-the-art breast can-
cer histopathological image models and diagnosis network with mrc-net
average psnr/ssim for x8, x4, x2 sr.
2. qualitative comparison with sr methods on breast cancer histopathological
images x8 and x4.
table 2. compare results with state-of-the-art on image level (* means that inputs are
hr images, # means that inputs are down sample to half resolution from hr images.).
[6] 2021
94.43
94.45
94.73
93.92
shisrcnet (ours)#
-
97.49 96.19 97.60 97.04
performance in four diﬀerent magniﬁcation factors.
shisrcnet, which uses down sample to half resolution (x2) from hr images,
outperforms the ssca at 40x, 200x and 400x.
meanwhile, compared with the diagnosis
network that also uses lr images as input, shiscnet has remarked performance
advantages.
table 3 compares our results with the cf module using diﬀerent res-
olution images.
the performance of the cf module decreases signiﬁcantly with
the reduction of resolution.
in contrast, shisrcnet greatly improves the cf
module performance of diﬀerent scale low-resolution images.
30
l. xie et al.
table 3. comparison of accuracy under diﬀerent scales on the image level.
4.2
ablation study of the shisrcnet
to verify the eﬀectiveness of the proposed components in shisrcnet, a com-
parison between shisrcnet and its ﬁve components on x2 images is given in
table 3.
(3) w/o csfblock, w/o hr images and w/o nt-
xent loss remove the corresponding operation, respectively.
as shown in table 3,
ﬁrstly, the performance of super-resolution in the shisrcnet is signiﬁcantly
reduced when we remove msf.
thirdly, compared with using fpn alone, the
performance of shisrcnet is further improved by adding csfblock to fpn.
finally, the introduction of hr images further promotes the performance of
shisrcnet.
because the training method of hr and sr images proposed by us
helps to improve the generalization of the shisrcnet (table 4).
table 4. ablation study of shisrcnet on x2 images.
w/o fpn+csfblock 34.41
0.9609
93.98
92.11
91.35
92.15
w/o csfblock
34.57
0.9619
94.37
93.21
93.99
94.71
w/o hr images
34.43
0.9611
93.13
94.28
93.86
94.17
w/o nt-xent loss
34.54
0.9623
95.53
95.20
95.01
95.36
shisrcnet
31
5
conclusion
this paper proposes shisrcnet for the low-resolution breast cancer histopatho-
logical images’ super-resolution and classiﬁcation problem.
the sr module
employs mfeblock to extract and fuse multi-scale features for reconstruct-
ing low-resolution histopathological images into high-resolution ones.
we introduce high-resolution images
into the cf module in the training stage to improve shisrcnet’s robustness.
through the joint training of the two modules, the super-resolution and classiﬁ-
cation of the low-resolution histopathological images are integrated in one model.
our method’s results are close to the sota methods, which require using high-
resolution breast cancer histopathological images instead of low-resolution ones.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_20.pdf:
in clinical practice,
the contextual structure of nodules and the accumulated experience of
radiologists are the two core elements related to the accuracy of identi-
ﬁcation of benign and malignant nodules.
the context parsing module ﬁrst
segments the context structure of nodules and then aggregates contex-
tual information for a more comprehensive understanding of the nodule.
the prototype recalling module utilizes prototype-based learning to con-
dense previously learned cases as prototypes for comparative analysis,
which is updated online in a momentum way during training.
building
on the two modules, our method leverages both the intrinsic characteris-
tics of the nodules and the external knowledge accumulated from other
nodules to achieve a sound diagnosis.
experiments on several
datasets demonstrate that our method achieves advanced screening per-
formance on both low-dose and noncontrast scenarios.
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43904-9 20.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
particularly, the evaluation of nodule (i.e., 8–30mm)
malignancy is recommended in the guidelines [13].
fig.
it is char-
acterized by a lack of standard-of-
truth of labels for malignancy [16,
27], and due to this limitation, many
studies use radiologists’ subjective
judgment on ct as labels, such as
lidc-idri
despite their advantages in rep-
resentation learning, these methods do not take into account expert diagnostic
knowledge and experience, which may lead to a bad consequence of poor general-
ization.
motivated by this, we ﬁrst segment the context
structure, i.e., nodule and its surroundings, and then aggregate the context
information to the nodule representation via the attention-based dependency
modeling, allowing for a more comprehensive understanding of the nodule itself.
experimental results on sev-
eral datasets demonstrate that our method achieves outstanding performance on
both ldct and ncct screening scenarios.
(4) our method achieves advanced
malignancy prediction performance in both screening scenarios (0.931 auc), and
exhibits strong generalization in external validation, setting a new state of the
art on lungx (0.801 auc).
202
j. zhang et al.
2
method
figure 2 illustrates the overall architecture of pare, which consists of three
stages: context segmentation, intra context parsing, and inter prototype recall-
ing.
we now delve into diﬀerent stages in detail in the following subsections.
2.1
context segmentation
the nodule context information has an important eﬀect on the benign and malig-
nant diagnosis.
therefore, we use a u-like net-
work (unet) to parse the semantic mask m for the input image patch x, thus
allowing subsequent context modeling of both the nodule and its surrounding
structures.
this segmentation process allows pare to gather
comprehensive context information that is crucial for an accurate diagnosis.
for
the diagnosis purpose, we extract the global feature from the bottleneck of unet
as the nodule embedding q, which will be used in later diagnostic stages.
2.2
intra context parse
in this stage, we attempt to enhance the discriminative representations of
nodules by aggregating contextual information produced by the segmentation
model.
the input image is also split into patches and
then embedded into the context tokens to keep the original image information.
besides, positional encoding is added in a learnable manner to retain location
information.
here
g is the number of context tokens, and d represents the embedding dimension.
we believe that explicitly
modeling the dependency between nodule embedding and its contextual struc-
ture can lead to the evolution of more discriminative representations, thereby
improving discrimination between benign and malignant nodules.
2.3
inter prototype recall
deﬁnition of the prototype: to retain previously acquired knowledge, a
more eﬃcient approach is needed instead of storing all learned nodules in mem-
ory, which leads to a waste of storage and computing resources.
cross prototype attention: in addition to parsing intra context, we also
encourage the model to capture inter-level dependencies between nodules and
external prototypes.
updating prototype online: the prototypes are updated in an online man-
ner, thereby allowing them to adjust quickly to changes in the nodule represen-
tations.
as for the nodule embedding q of the data (x, y), its nearest prototype
is singled out and then updated by the following momentum rules,
 p b
arg minjd(q,p b
j ) = λ · p b
arg minjd(q,p b
j ) + (1 − λ) · q
if y = 0
p m
arg minjd(q,p m
j
)
+ (1 − λ) · q otherwise
(1)
where λ is the momentum factor, set to 0.95 by default.
the momentum updating
can help accelerate the convergence and improve the generalization ability.
2.4
training process of pare
the algorithm 1 outlines the training process of our pare model which is
based on two objectives: segmentation and classiﬁcation.
the dice and cross-
entropy loss are combined for segmentation, while cross-entropy loss is used for
classiﬁcation.
additionally, deep classiﬁcation supervision is utilized to enhance
the representation of nodule embedding in shallow layers like the output of the
unet and sca modules.
3
experiment
3.1
datasets and implementation details
data collection and curation: nlst is the ﬁrst large-scale ldct dataset
for low-dose ct lung cancer screening purpose
segmentation annotation: we provide the segmentation mask for our
in-house data, but not for the nlst data considering its high cost of pixel-level
labeling.
the nodule mask of each in-house data was manually annotated with
the assistance of ct labeler
[20] by our radiologists, while other contextual masks
such as lung, vessel, and trachea were generated using the totalsegmentator
segmentation: we
also evaluate the segmentation performance of our method on the public nod-
ule segmentation dataset lidc-idri
[3], which has 2,630 nodules with nodule
segmentation mask.
evaluation metrics: the area under the receiver operat-
ing characteristic curve (auc) is used to evaluate the malignancy prediction
performance.
towards accurate lung nodule malignancy prediction like radiologists
205
implementation: all experiments in this work were implemented based on the
nnunet framework [8], with the input size of 32 × 48 × 48, batch size of 64, and
total training iterations of 10k.
the hyper-parameters of pare
are empirically set based on the ablation experiments on the validation set.
due to the lack of manual annotation of nodule masks for the
nlst dataset, we can only optimize the segmentation task using our in-house
dataset, which has manual nodule masks.
3.2
experiment results
ablation study: in table 1, we investigate the impact of diﬀerent conﬁgura-
tions on the performance of pare on the validation set, including transformer
layers, number of prototypes, embedding dimension, and deep supervision.
we
observe that a higher auc score can be obtained by increasing the number of
transformer layers, increasing the number of prototypes, doubling the channel
dimension of token embeddings, or using deep classiﬁcation supervision.
based
on the highest auc score of 0.931, we empirically set l=4, n=40, d=256, and
ds=true in the following experiments.
in table 2, we investigate the ablation
study of diﬀerent methods/modules on the validation set and observe the follow-
ing results: (1) the pure segmentation method performs better than the pure
classiﬁcation method, primarily because it enables greater supervision at the
pixel level, (2) the joint segmentation and classiﬁcation is superior to any sin-
gle method, indicating the complementary eﬀect of both tasks, (3) both context
parsing and prototype comparing contribute to improved performance on the
strong baseline, demonstrating the eﬀectiveness of both modules, and (4) seg-
menting more contextual structures such as vessels, lungs, and trachea provide
a slight improvement, compared to solely segmenting nodules.
table 1. ablation comparison of hyper-
parameters (transformer layers (l), num-
ber of prototypes (n), embedding dimen-
sion (d), and deep supervision (ds))
*: only nodule mask
was used in the segmentation task.
method
auc
pure classiﬁcation
0.907
pure segmentation
0.915
mt
0.916
mt+context*
0.921
mt+context
0.924
mt+context+prototype 0.931
206
j. zhang et al.
table 3. comparison of diﬀerent methods on both nlst and in-house test sets.
†:
pure classiﬁcation; ‡: pure segmentation; ⋄: multi-task learning; *: ensemble of deep
supervision heads.
note that we add the segmentation task in ca-net.
[25]†
0.821
0.755
0.810
0.908
0.858
0.784
0.751
0.904
nnunet [8]‡
0.815
0.736
0.815
0.910
0.863
0.804
0.750
0.911
ca-net [12]⋄ 0.833
0.759
0.807
0.916
0.878
0.786
0.779
0.918
pare⋄
0.882
0.770
0.826
0.928
0.892
0.817
0.783
0.927
pare⋄*
0.890
0.781
0.827
0.931 0.899
0.821
0.780
0.931
comparison to other methods on both screening scenarios: table 3
presents a comparison of pare with other advanced methods, including pure
classiﬁcation-based, pure segmentation-based, and multi-task-based methods.
stratiﬁcation assessments were made in both test sets based on the nodule size
distribution.
the results indicate that the segmentation-based method outper-
forms pure classiﬁcation methods, mainly due to its superior ability to segment
contextual structures.
3 reveal that our method achieves
performance comparable to that of radiologists.
generalization on ldct and ncct: our model is trained on a mix of
ldct and ncct datasets, which can perform robustly across low-dose and
regular-dose applications.
we compare the generalization performance of the
models obtained under three training data conﬁgurations (ldct, ncct, and
a combination of them).
however, our mixed training approach performs best on both
ldct and ncct with almost no performance degradation.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_35.pdf:
accurately classifying the histological subtype of non-small cell lung
cancer (nsclc) using computed tomography (ct) images is critical for clinicians
in determining the best treatment options for patients.
although recent advances
in multi-view approaches have shown promising results, discrepancies between
ct images from different views introduce various representations in the feature
space, hindering the effective integration of multiple views and thus impeding
classiﬁcation performance.
speciﬁcally, we introduce a cross-view representation alignment
learning network which learns effective view-invariant representations in a com-
mon subspace to reduce multi-view discrepancies in a discriminability-enforcing
way.
additionally, carl learns view-speciﬁc representations as a complement
to provide a holistic and disentangled perspective of the multi-view ct images.
experimentalresultsdemonstratethatcarlcaneffectivelyreducethemulti-view
discrepancies and outperform other state-of-the-art nsclc histological subtype
classiﬁcation methods.
keywords: cross-view alignment · representation learning · multi-view ·
histologic subtype classiﬁcation · non-small cell lung cancer
1
introduction
lung cancer is currently the foremost cause of cancer-related mortalities globally, with
non-small cell lung cancer (nsclc) being responsible for 85% of reported cases [25].
since scc and adc differ in the
effectiveness of chemotherapy and the risk of complications, accurate identiﬁcation of
different subtypes is crucial for clinical treatment options [15].
therefore, non-invasive meth-
ods utilizing computed tomography (ct) images have garnered signiﬁcant attention over
the last decade [15, 16].
recently, several deep-learning methods have been put forward to differentiate
between the nsclc histological subtypes using ct images [4, 11, 13, 22]. chaun-
zwa et al.
[13] both employ a convolutional neural network
(cnn) model with axial view ct images to classify the tumor histology into scc and
adc.
albeit the good performance, the above 2d cnn-based models only take ct
images from a single view as the input, limiting their ability to describe rich spatial
properties of ct volumes
[22] aggregate features from axial, coronal, and sagittal view
ct images via a multi-view fusion model.
additionally,
images from certain views may inevitably contain some unique background information,
e.g., the spine in the sagittal view [17].
consequently, the discrepancies
of distinct views will hamper the fusion of multi-view information, limiting further
improvements in the classiﬁcation performance.
to overcome the challenge mentioned above, we propose a novel cross-aligned repre-
sentation learning (carl) method for the multi-view histologic subtype classiﬁcation of
nsclc.
carl offers a holistic and disentangled perspective of multi-view ct images
by generating both view-invariant and -speciﬁc representations.
speciﬁcally, carl
incorporates a cross-view representation alignment learning network which targets the
reduction of multi-view discrepancies by obtaining discriminative view-invariant repre-
sentations.asharedencoderwithanoveldiscriminability-enforcingsimilarityconstraint
is utilized to map all representations learned from multi-view ct images to a common
subspace, enabling cross-view representation alignment.
such aligned projections help
to capture view-invariant features of cross-view ct images and meanwhile make full
use of the discriminative information obtained from each view.
additionally, carl
learns view-speciﬁc representations as well which complement the view-invariant ones,
providing a comprehensive picture of the ct volume data for histological subtype pre-
diction.
detailed experimental results demonstrate the
effectiveness of carl in reducing multi-view discrepancies and improving nsclc
360
y. luo et al.
histological subtype classiﬁcation performance.
to reduce the discrepancies of multi-
view ct images, carl incorporates a cross-view representation alignment learning
network for discriminative view-invariant representations.
– we employ a view-speciﬁc representation learning network to learn view-speciﬁc
representations as a complement to the view-invariant representations.
– we conduct experiments on a publicly available dataset and achieve superior
performance compared to the most advanced methods currently available.
fig.
the cross-view representation align-
mentlearningnetworkincludesasharedencoderwhichprojectspatchesofaxial,coronal,
and sagittal views into a common subspace with a discriminability-enforcing similarity
constraint to obtain discriminative view-invariant representations for multi-view dis-
crepancy reduction.
in addition, carl introduces a view-speciﬁc representation learn-
ing network consisting of three unique encoders which focus on learning view-speciﬁc
carl: cross-aligned representation learning
361
representations in respective private subspaces to yield complementary information to
view-invariant representations.
2.2
cross-view representation alignment learning
since the discrepancies of different views may result in divergent statistical properties in
feature space, e.g., huge distributional disparities, aligning representations of different
views is essential for multi-view fusion.
with the aim to reduce multi-view discrepancies,
carl introduces a cross-view representation alignment learning network for mapping
the representations from distinct views into a common subspace, where view-invariant
representations can be obtained by cross-view alignment.
technically speaking, given the axial view image iav, coronal view image icv, and
sagittal view image isv, the cross-view representation alignment learning network tries to
generate view-invariant representations hc
v, v ∈ {av, cv, sv} via a shared encoder based
on a residual neural network
however, the distributions of hc
av, hc
cv and hc
sv are very complex due to the signif-
icant variations between different views, which puts a burden on obtaining well-aligned
view-invariant representations with merely an encoder.
to address this issue, we design a discriminability-enforcing similarity loss ldsim to
further enhance the alignment of cross-view representations in the common subspace.
mathematically, we introduce a cross-view similarity loss lsim which
calculates the central moment discrepancy (cmd) metric
despite the fact that minimizing the lsim can efﬁciently mitigate
the issue of distributional disparities, it may not guarantee that the alignment network
will learn informative and discriminative representations.
by minimizing ldsim, the
cross-view representation alignment learning network pushes the representations of each
sub-view to align with those of the main view in a discriminability-enforcing manner.
notably, the beneﬁts of such cross-alignment are twofold.
secondly, since the alignment between distinct views compels
the representation distribution of the sub-views to match that of the discriminative main
view, it can also enhance the discriminative power of the sub-view representations.
in other words, the cross-alignment procedure spontaneously promotes the transfer of
discriminative information learned by the representations of the main view to those of
the sub-views.
as a result, the introduced cross-view representation alignment learning
network is able to generate consistent and discriminative view-invariant representations
cross all views to effectively narrow the multi-view discrepancies.
2.3
view-speciﬁc representation learning
on the basis of learning view-invariant representations, carl additionally learns
view-speciﬁc representations in respective private subspaces, which provides supple-
mentary information for the view-invariant representations and contribute to subtype
classiﬁcation as well.
a
reconstruction module is also employed to calculate a reconstruction loss lrec between
original image iv and reconstructed image ir
v using the l1-norm, which ensures the
hidden representations to capture details of the respective view.
throughout the experiments, we set the values of α, β and γ to 0.6s, 0.4 and 0.6,
respectively.
3
experiments and results
3.1
dataset
our dataset nsclc-tcia for lung cancer histological subtype classiﬁcation is sourced
from two online resources of the cancer imaging archive (tcia)
we evaluate the
performance of nsclc classiﬁcation in ﬁve-fold cross validation on the nsclc-tcia
dataset, and measure accuracy (acc), sensitivity (sen), speciﬁcity (spe), and the area
under the receiver operating characteristic (roc) curve (auc) as evaluation metrics.
we
also conduct analysis including standard deviations and 95% ci, and delong statistical
test for further auc comparison.
for preprocessing, given that the ct data from nsclc-tcia has an in-plane reso-
lution of 1 mm × 1 mm and a slice thickness of 0.7–3.0 mm, we resample the ct images
using trilinear interpolation to a common resolution of 1mm × 1mm × 1mm.
finally following [7], we clip the intensities of the input patches to the interval
(−1000, 400 hounsﬁeld unit) and normalize them to the range of [0, 1].
3.2
implementation details
the implementation of carl is carried out using pytorch and run on a worksta-
tion equipped with nvidia geforce rtx 2080ti gpus and intel xeon cpu 4110 @
2.10ghz.
we use pub-
licly available codes of these comparison methods and implement models for methods
without code.
the experimental results are reported in table 1.
2(a) is also closer to the upper-left corner, further indicating
its superior performance.
besides, carl-b3 and carl-b4 show better performance than
carl-b0, illustrating view-speciﬁc representations as a complement which can also
contribute to subtype classiﬁcation.
though single loss already contributes to perfor-
mance improvement, carl-b5 to carl-b7 demonstrate that the combinations of
different losses can further enhance classiﬁcation results.
more importantly, carl with
all losses achieves the best performance among all methods, demonstrating that our pro-
posed method effectively reduces multi-view discrepancies and signiﬁcantly improves
the performance of histological subtype classiﬁcation by providing a holistic and disen-
tangled perspective of the multi-view ct images.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_27.pdf:
the recent surge of foundation models in computer vision
and natural language processing opens up perspectives in utilizing multi-
modal clinical data to train large models with strong generalizability.
yet pathological image datasets often lack biomedical text annotation
and enrichment.
guiding data-eﬃcient image diagnosis from the use of
biomedical text knowledge becomes a substantial interest.
in this paper,
we propose to connect image and text embeddings (cite) to enhance
pathological image classiﬁcation.
cite injects text insights gained from
language models pre-trained with a broad range of biomedical texts,
leading to adapt foundation models towards pathological image under-
standing.
through extensive experiments on the patchgastric stomach
tumor pathological image dataset, we demonstrate that cite achieves
leading performance compared with various baselines especially when
training data is scarce.
cite oﬀers insights into leveraging in-domain
text knowledge to reinforce data-eﬃcient pathological image classiﬁca-
tion.
keywords: foundation models · multi-modality · model adaptation ·
pathological image classiﬁcation
1
introduction
deep learning for medical imaging has achieved remarkable progress, leading to
a growing body of parameter-tuning strategies [1–3].
in parallel, foundation models [4] have surged in computer vision [5,6]
and natural language processing [7,8] with growing model capacity and data
size, opening up perspectives in utilizing foundation models and large-scale clin-
ical data for diagnostic tasks.
given
the complex tissue characteristics of pathological whole slide images (wsi), it
is crucial to develop adaptation strategies allowing (1) training data eﬃciency,
and (2) data fusion ﬂexibility for pathological image analysis.
https://doi.org/10.1007/978-3-031-43904-9_27
text-guided foundation model adaptation for medical image classiﬁcation
273
fig.
1. connecting image and text embeddings.
an image with the visual prompt is processed through a
vision encoder and a projection layer.
classiﬁcation prediction is made
by the similarity between image and text embeddings.
although foundation models promise a strong generalization ability [4], there
is an inherent domain shift between medical and natural concepts in both vision
and language modalities.
pre-trained biomedical language models are increas-
ingly applied to medical context understanding [9–11].
language models prove
to be eﬀective in capturing semantic characteristics with a lower data acquisition
and annotation cost in medical areas [12].
in addition, vision-language models demonstrate the importance of joining
multi-modal information for learning strong encoders
thus, connecting
visual representations with text information from biomedical language models
becomes increasingly critical to adapting foundation models for medical image
classiﬁcation, particularly in the challenging setting of data deﬁciency.
in this study, we propose cite, a data-eﬃcient adaptation framework that
connects image and text embeddings from foundation models to perform
pathological image classiﬁcation with limited training samples (see fig. 1).
to
enable language comprehension, cite makes use of large language models pre-
trained on biomedical text datasets [10,11] with rich and professional biomedical
knowledge.
we demonstrate the usefulness of injecting biomedical text knowledge into
foundation model adaptation for improved pathological image classiﬁcation.
3. cite is simple yet eﬀective that outperforms supervised learning, visual
prompt tuning, and few-shot baselines by a remarkable margin, especially
under the data deﬁciency with limited amounts of training image samples
(e.g., using only 1 to 16 slides per class).
2
related work
medical image classiﬁcation.
deep learning for medical image classiﬁcation
has long relied on training large models from scratch [1,15].
also, ﬁne-tuning
or linear-probing the pre-trained models obtained from natural images [16–18]
is reasonable.
in addition, task-speciﬁc models do
not generalize well with diﬀerent image modalities [2].
to tackle this issue, we
emphasize the adaptation of foundation models in a data-eﬃcient manner.
vision-language pre-training.
recent work has made eﬀorts in pre-training
vision-language models.
clip [5] collects 400 million image-text pairs from the
internet and trains aligned vision and text encoders from scratch.
however, those methods establish vision-language alignment by pre-training on
large-scale image-text pairs.
prompt tuning proves to be an
eﬃcient adaptation method for both vision and language models [22,23]. orig-
inating from natural language processing, “prompting” refers to adding (man-
ual) text instructions to model inputs, whose goal is to help the pre-trained
model better understand the current task.
for instance, coop [22] introduces
learnable prompt parameters to the text branch of vision-language models.
to address this challenge,
we leverage large language models pre-trained with biomedical text to inject
medical domain knowledge.
biomedical language model utilization.
leveraging language
models pre-trained with biomedical text for medical language tasks is a common
application.
[9] pre-train a clinical text model with
biobert [10] initialization and show a signiﬁcant improvement on ﬁve clinical
text-guided foundation model adaptation for medical image classiﬁcation
(a) the pathological images are cut into patches.
(b)
the class token, image tokens, and learnable prompt tokens are concatenated.
(c) the
tokens are processed by a pre-trained vision transformer to generate image embeddings.
(d) the image is recognized as
the class with maximum cosine similarity between image and text embeddings.
(e) the
class names are processed by a biomedical language model to generate text embeddings.
language tasks.
in our eﬀorts, we
emphasize the importance of utilizing biomedical language models for adapting
foundational vision models into cancer pathological analysis.
3
methodology
figure 2 depicts an overview of our approach cite for data-eﬃcient pathological
image classiﬁcation.
cite jointly understands the image features extracted by
vision encoders pre-trained with natural imaging, and text insights encoded in
large language models pre-trained with biomedical text (e.g., biolinkbert [11]
which captures rich text insights spanning across biomedical papers via cita-
tions).
we connect text and imaging by a projection and classify the images by
comparing the cosine similarity between image and text embeddings.
they are (1) prompt tokens in the input space to model task-speciﬁc
information, and (2) a projection layer in the latent space to align image and text
embeddings.
276
y. zhang et al.
3.1
connecting text and imaging
an image i to be classiﬁed is processed through a pre-trained vision encoder to
generate the image embedding xv with dimension dv, where v stands for “vision”:
xv = visionencoder(i)
xv ∈ rdv.
[1, c]) with a
pre-trained biomedical language model instead of training a classiﬁcation head
(see fig.
we tokenize and process tc through the language encoder to
generate the text embedding xc
l with dimension dl, where l stands for “language”:
xc
l = languageencoder(tokenizer(tc))
(2)
vision-language models like clip [5] contain both a vision encoder and a
language encoder, which provide well-aligned embeddings in the same feature
space.
in this case, prediction ˆy is obtained by applying softmax on scaled cosine
similarities between the image and text embeddings (see fig.
):
p(ˆy = c|i) =
exp(sim(xc
l , xv)/τ)
c
c′=1 exp(sim(xc′
l , xv)/τ)
,
(3)
where sim(·, ·) refers to cosine similarity and τ is the temperature parameter.
for irrelevant vision and language encoders, we introduce an extra projection
layer to the end of the vision encoder to map the image embeddings to the same
latent space as the text embeddings.
3.2
learning visual prompt
medical concepts exhibit a great visual distribution shift from natural images,
which becomes impractical for a ﬁxed vision encoder to capture task-speciﬁc
information in few-shot scenarios.
visual prompt tuning (vpt [23]) is a
lightweight adaptation method that can alleviate such an inherent diﬀerence
by only tuning prompt tokens added to the visual inputs of a ﬁxed vision trans-
former [24], showing impressive performance especially under data deﬁciency.
a vision transformer ﬁrst cuts the image into a sequence of n patches and
projects them to patch embeddings e0 ∈ rn×dv, where dv represents the visual
embedding dimension.
cls embedding
of the last layer output is the image feature xv.
, pp] ∈ rp×dv,
text-guided foundation model adaptation for medical image classiﬁcation
277
where p is the prompt length, with cls token c0 and patch embeddings e0 before
they are processed through the ﬁrst transformer layer:
, k
xv = ck
xv ∈ rdv,
(5)
where [·, ·] refers to concatenation along the sequence length dimension, and
zk ∈ rp×dv represents the output embeddings of the k-th transformer layer
at the position of the prompts (see fig.
3.1.
4
experimental settings
dataset.
we adopt the patchgastric [25] dataset, which includes histopatho-
logical image patches extracted from h&e stained whole slide images (wsi)
of stomach adenocarcinoma endoscopic biopsy specimens.
we randomly split the wsis into train (20%) and validation (80%) sub-
sets for measuring the model performance.
the evaluation metric is patient-wise accuracy, where the prediction of a wsi
is obtained by a soft vote over the patches, and accuracy is averaged class-wise.
implementation.
[5] as the visual backbone, with
input image size 224 × 224, patch size 16 × 16, and embedding dimension dv =
512.
we adopt biolinkbert-large [11] as the biomedical language model, with
embedding dimension dl
= 1, 024. to show the extensibility of our approach, we
additionally test on vision encoders including imagenet-21k vit-b/16
[6], and biomedical language model biobert-large [10].
our implementation is based on clip1, huggingface2 and mmclassiﬁcation3.
we resize the images to 224×224
to ﬁt the model and follow the original data pipeline in patchgastric [25].
a
class-balanced sampling strategy is adopted by choosing one image from each
class in turn.
all our experiment results
are averaged on 3 random seeds unless otherwise speciﬁed.
[27] backbone pre-trained on imagenet-21k
[5] backbone. averaged results and standard deviation (error
bars) of 3 runs are displayed.
our cite consistently outperforms all baselines under
all data fractions, showing a remarkable improvement under data deﬁciency.
[27] backbone pre-trained on imagenet-21k
[18]: apply an attention network on image features to predict pseudo
labels and cluster the images.
(5) zero-shot [5]: classify images to the nearest text
embeddings obtained by class names, without training.
(6) few-shot [28]: cluster
image features of the training data and classify images to the nearest class cen-
ter.
our
cite outperforms all baselines that require training classiﬁcation heads, as well
as image feature clustering methods, demonstrating the key beneﬁt of leveraging
additional biomedical text information for pathological image classiﬁcation.
cite shows a favorable improvement when data is scarce.
when only
one training slide per class is available, cite achieves a remarkable performance,
outperforming all baselines by a signiﬁcant margin (from 51.4% to 60.2%).
together, our ﬁndings
text-guided foundation model adaptation for medical image classiﬁcation
279
table 1. ablation study of cite with and without prompt and text.
we
report the average accuracy and standard deviation.
each component improves the performance.
[5], imagenet-21k vit-b/16
[11] language models.
the highest performance of each visual encoder is bolded.
60.2±1.2 59.1±1.2 60.3±0.8
66.4±0.7 67.9±0.4 69.7±0.1
in-21k vit-b/16
linear
-
46.7±0.7
45.8±1.6
53.4±1.2
59.5±0.5
60.6±0.6
66.5±0.8
fine-tune -
48.0±0.3
49.6±0.1
50.8±0.1
59.3±0.3
62.2±0.4
66.3±0.2
cite
bb
51.4±1.4
51.8±1.3
56.6±1.9
62.7±1.0
64.0±0.5
67.2±1.4
cite
blb
52.4±1.5 52.7±0.8 57.0±0.9 62.8±1.2 64.5±1.1 67.4±0.7
intern vit-b/16 linear
-
47.3±0.2
47.2±0.2
52.4±0.5
59.7±0.3
63.1±0.2
66.8±0.7
fine-tune -
42.0±0.3
46.0±0.3
51.0±0.9
60.4±0.1
62.7±0.5
68.2±0.4
cite
bb
51.7±0.1 55.4±1.8 59.6±0.3 66.4±0.8 68.1±0.8 69.7±0.7
cite
blb
48.4±5.2
49.1±5.5
57.9±0.8
65.3±0.4
67.9±0.8
69.4±0.9
demonstrate that adding domain-speciﬁc text information provides an eﬃcient
means to guide foundation model adaptation for pathological image diagnosis.
we evaluate our approach with additional
backbones and biomedical language models to assess its potential extensibility.
the text information encoded in biomedical language models allows vision
280
y. zhang et al.
models pre-trained with natural imaging to bridge the domain gap without task-
speciﬁc pre-training on medical imaging.
importantly, when using both the vision
and language encoders of clip vit-b/16, our approach still outperforms the
baselines by a remarkable margin (47.7% to 60.1%), demonstrating the impor-
tance of multi-modal information.
while clip gains such modality matching
through pre-training, our cite shows an appealing trait that irrelevant vision
and language models can be combined to exhibit similar multi-modal insights
on pathological tasks without a need of joint pre-training.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_25.pdf:
such a mil-based
wvad method leverages video-level annotations to detect frame-level
diseases and shows promising results.
the former is used to
learn atoms for representing normal features, and the latter is used to
encourage our model to gain robust disease detection.
we demonstrate
that our cfd network is achieving new sota performance on the exist-
ing polyp dataset and the introduced panda-mil dataset.
with the recent success of deep learning, researchers are
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43904-9 25.
as a result, the
diﬃculty of data collection restricts the development of the supervised cad.
due to the diﬃculty of acquiring the abundant annotated training data, the
current sota method, i.e., csm [14], proposes a mil-based wvad manner to
speciﬁcally tackle one speciﬁc disease detection task, i.e., colorectal cancer diag-
nosis via colonoscopy.
with the decoupled snippet-level feature ingredi-
ents, our cfd employs both the normal and abnormal feature ingredients via a
contrastive learning paradigm to concurrently optimize video-level and snippet-
level disease scores for pursuing more accurate detection.
to assess the proposed contrastive feature decoupling network, we conduct
experiments on two datasets, i.e., polyp and panda-mil.
the ablation study shows the decoupled
diseased ingredients enable accurate disease detection, and the accompanied
contrastive learning paradigm provides further improvement.
– we demonstrate the generalization of cfd by achieving new sota perfor-
mance on two biomedical imaging datasets concerning diﬀerent pathological
modalities, i.e., the colonoscopy videos in polyp and prostate tissue biopsies
in panda-mil.
254
j.-c.
[8] established a large-scale attention-
based database and designed a specialized model using retinal fundus images for
detecting glaucoma.
one popular branch is contrastive learning which shows a remarkable ability to
obtain the desired semantic representation from various perspectives.
our model contains an oﬄine trained memory
bank to store feature atoms before the cfd training procedure, which associates
a contrastive loss to boost the model performance using decoupled features per
instance.
, t
is the number of instances, and c represents the instance-level feature dimension.
,
(4)
where φd, g, and ψ are linear projections, global average pooling, and the multi-
scale temporal network [13], respectively.
a complete objec-
tive should consider the symmetric form by switching d1 and d0 in (8).
4
experiments
4.1
dataset and metric
we evaluate our model against sotas on the existing polyp [14] dataset and
the panda-mil dataset introduced in this work.
please refer to the
supplementary material for the statistics of the two datasets.
the prostate cancer grade assessment (panda) challenge
[2] comprises over 10k whole-slide images (wsis) of digitized hematoxylin
and eosin-stained biopsies originating from radboud university medical cen-
ter and karolinska institute.
we follow the previous methods [4,13,14] to employ the instant-level
area under curve (auc) and the average precision (ap) for a fair comparison.
the larger values of both metrics mean better disease detection performance.
[14]
miccai’22 98.41
86.63
76.52
73.12
cfd
99.51 88.13 87.28 80.78
4.2
implementation details
all the evaluated methods in the experiment used the same feature encoder,
i.e., i3d
precisely, our model achieves the new sota by 1.1%
auc and 1.5% ap improvements on the polyp dataset and 1.09% auc and
2.45% ap improvements on the panda-mil dataset.
please refer to the sup-
plementary material for the completed results, including more wvad methods
[12,16,20,24].
figure 2 visualizes one disease detection result of our cfd model on the
panda-mil dataset.
3.3, each loss function shows its improvement
in our model performance.
the contrastive loss contributes the most to auc
improvement, enabling our model to achieve the sota performance.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_24.pdf:
pancreatic ductal adenocarcinoma (pdac) is a highly lethal
cancer in which the tumor-vascular involvement greatly aﬀects the
resectability and, thus, overall survival of patients.
this paper proposes a novel learnable neural distance that describes the
precise relationship between the tumor and vessels in ct images of dif-
ferent patients, adopting it as a major feature for prognosis prediction.
besides, diﬀerent from existing models that used cnns or lstms to
exploit tumor enhancement patterns on dynamic contrast-enhanced ct
imaging, we improved the extraction of dynamic tumor-related texture
features in multi-phase contrast-enhanced ct by fusing local and global
features using cnn and transformer modules, further enhancing the fea-
tures extracted across multi-phase ct images.
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43904-9 24.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
keywords: pancreatic ductal adenocarcinoma (pdac) · survival
prediction · texture-aware transformer · cross-attention · nerual
distance
1
introduction
pancreatic ductal adenocarcinoma (pdac) is one of the deadliest forms of
human cancer, with a 5-year survival rate of only 9% [16].
how-
ever, current clinical markers such as larger tumor size and high carbohydrate
antigen (ca) 19-9 level may not be suﬃcient to accurately tailor neoadjuvant
treatment for patients [19].
therefore, multi-phase contrast-enhanced ct has a
great potential to enable personalized prognostic prediction for pdac, lever-
aging its ability to provide a wealth of texture information that can aid in the
development of accurate and eﬀective prognostic models [2,10].
previous studies have utilized image texture analysis with hand-crafted fea-
tures to predict the survival of patients with pdacs [1], but the representational
fig.
we deﬁne the surface-to-surface distance based on point-to-surface dis-
tance (weighted-average of red lines from ♦ to △) instead of point-to-point distance (blue
lines) to better capture the relationship between the tumor and the perivascular tissue.
it is necessary to incor-
porate tumor-vascular involvement into the feature extraction process of the
prognostic model.
[21,22], these methods may not be suﬃciently capable of capturing the
complex dynamics between the tumor and its environment.
furthermore, to capture the tumor enhancement patterns across multi-phase ct
images, we are the ﬁrst to combine convolutional neural networks (cnn) and
transformer [4] modules for extracting the dynamic texture patterns of pdac
and its surroundings.
this approach takes advantage of the visual transformer’s
adeptness in capturing long-distance information compared to the cnn-only-
based framework in the original approach.
our proposed model has the
potential to be used in combination with clinical factors for risk stratiﬁcation
and treatment decisions for patients with pdac.
the second component proposes a neural dis-
tance metric between pdac and important vessels to assess their involvements.
these
blocks encode the input feature of an image fi
the 3 × 3 × 3 convolution captures local spatial
information, while the 1 × 1 × 1 convolution maps the input tensor to a higher-
dimensional space (i.e., cl
the cross-modality output fcross
and in-modality output ft
o are then concatenated and passed through an average
pooling layer to obtain the ﬁnal output feature of the texture branch, denoted
as ft ∈ rct.
2.2
neural distance: positional and structural information
between pdac and vessels
the vascular involvement in patients with pdac aﬀects the resectability and
treatment planning [5].
we used a semi-supervised
nnunet model to segment pdac and the surrounding vessels, following recent
work [11,21].
we deﬁne a general distance between the surface boundaries of
pdac (p) and the aforementioned four types of vessels (v) as d(v, p), which
can be derived as follows:
d(v, p) = dss(v, p)
(5)
neural distance allows for the ﬂexible assignment of weights to diﬀerent points
and is able to ﬁnd positional information that is more suitable for pdac prog-
nosis prediction.
finally, we concatenate the features extracted from the two components and
apply a fully-connected layer to predict the survival outcome, denoted as oos,
which is a value between 0 and 1. to optimize the proposed model, we use the
negative log partial likelihood as the survival loss [9].
3
experiments
dataset.
pdac masks for 340 patients were manually labeled by a radiol-
ogist from shengjing hospital with 18 years of experience in pancreatic cancer,
while the rest were predicted using self-learning models [11,24] and checked by
the same annotator.
other vessel masks were generated using the same semi-
supervised segmentation models.
implementation details: we used nested 5-fold cross-validation and aug-
mented the training data by rotating volumetric tumors in the axial direction
and randomly selecting cropped regions with random shifts.
we also set the out-
put feature dimensions to ct = 64 for the texture-aware transformer, cs = 64
for the structure extraction and k = 32 for the neural distance.
the batch size
was 16 and the maximum iteration was set to 1000 epochs, and we selected
the model with the best performance on the validation set during training for
testing.
we implemented our experiments using pytorch 1.11 and trained the
models on a single nvidia 32g-v100 gpu.
ablation study.
we ﬁrst evaluated the performance of our proposed texture-
aware transformer (tat) by comparing it with the resnet18 cnn backbone
and vit transformer backbone, as shown in table 1.
our model leverages the
strengths of both local and global information in the pancreas and achieved the
best result.
next, we compared diﬀerent methods for multi-phase stages, includ-
ing lstm, early fusion (fusion), and cross-attention (cross) in our method.
to further evaluate the performance of our proposed model, we
compared it with recent deep prediction methods [17,21] and report the results
in table 2.
our proposed method, which
uses the transformer and structure-aware blocks to capture tumor enhancement
248
h. dong et al.
table 1. ablation tests with diﬀerent network backbones including resnet18 (res),
vit and texture-aware transformer (tat) and methods for multi-phases including
lstm, early fusion (fusion) and cross-attention (cross).
nested 5-fold cv (n = 892)
independent test (n = 178)
c-index
auc
c-index auc
3dcnn-p [12]
0.630 ± 0.009
0.668 ± 0.019
0.674
0.740
early fusion [17]
0.635 ± 0.011
0.670 ± 0.024
0.696
0.779
deepct-pdac [21] 0.640 ± 0.018
0.680 ± 0.036
0.697
0.773
ours
0.656 ± 0.017 0.695 ± 0.023 0.710
0.792
patterns and tumor-vascular involvement, demonstrated its eﬀectiveness with
better performance in both nested 5-fold cross-validation and the multi-center
independent test set.
in table 3, we used univariate and multivariate cox proportional-hazards
models to evaluate our signature and other clinicopathologic factors in the inde-
pendent test set.
the proposed risk stratiﬁcation was a signiﬁcant prognostic
factor, along with other factors like pathological tnm stages.
to demonstrate the added value of our
signature as a tool to select patients for neoadjuvant treatment before surgery,
we plotted kaplan-meier survival curves in fig.
3. we further stratify patients by
our signature after grouping them by tumor size and ca19-9, two clinically used
preoperative criteria for selection, and also age.
our signature could signiﬁcantly
stratify patients in all cases and those in the high-risk group had worse outcomes
and might be considered as potential neoadjuvant treatment candidates (e.g. 33
high-risk patients with larger tumor size and high ca19-9).
independent test set (n = 178)
univariate analysis
multivariate analysis
hr (95% ci)
p-value
hr (95% ci)
p-value
proposed (high vs low risk)
2.42(1.64-3.58)
<0.0001
1.85(1.08-3.17)
0.027
age (> 60 vs = 60)
1.49(1.01-2.20)
0.043
1.01(0.65-1.58)
0.888
sex (male vs female)
1.28(0.86-1.90)
0.221
-
-
pt (pt3-pt4 vs pt1-pt2)
3.17(2.10-4.77)
<0.0001
2.44(1.54-3.86)
0.00015
pn (positive ve negative)
1.47(0.98-2.20)
0.008
1.34(0.85-2.12)
0.210
resection margin (r1 vs r0)
2.84(1.64-4.93)
<0.0001
1.68(0.92-3.07)
0.091
ca19-9 (> 210 vs ≤ 210 u/ml)
0.94(0.64-1.39)
0.759
-
-
tumor size (> 25 vs ≤ 25 mm)
2.36(1.59-3.52)
<0.0001
0.99(0.52-1.85)
0.963
tumor location (head vs tail)
1.06(0.63-1.79)
0.819
-
-
fig.
high risk group indicated by the proposed method is the potential patient group
that could beneﬁt from neoadjuvant treatment before surgery.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_16.pdf:
our
exhaustive study of state-of-the-art (sota) cnn image classiﬁcation
models for this problem reveals that they often fail to learn the gouty
mskus features, including the double contour sign, tophus, and snow-
storm, which are essential for sonographers’ decisions.
to address this
issue, we establish a framework to adjust cnns to “think like sonog-
raphers” for gout diagnosis, which consists of three novel components:
(1) where to adjust: modeling sonographers’ gaze map to emphasize
the region that needs adjust; (2) what to adjust: classifying instances
to systematically detect predictions made based on unreasonable/biased
reasoning and adjust; (3) how to adjust: developing a training mecha-
nism to balance gout prediction accuracy and attention reasonability for
improved cnns.
the experimental results on clinical mskus datasets
demonstrate the superiority of our method over several sota cnns.
keywords: musculoskeletal ultrasound · gout diagnosis · gaze
tracking · reasonability
1
introduction
gout is the most common inﬂammatory arthritis and musculoskeletal ultrasound
(mskus) scanning is recommended to diagnose gout due to the non-ionizing
radiation, fast imaging speed, and non-invasive characteristics of mskus
although
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43987-2 16.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. (eds.): miccai 2023, lncs 14225, pp. 159–168, 2023.
https://doi.org/10.1007/978-3-031-43987-2_16
160
z. cao et al.
convolutional neural networks (cnns) based ultrasound classiﬁcation models
have been successfully used for diseases such as thyroid nodules and breast can-
cer, conspicuously absent from these successful applications is the use of cnns
for gout diagnosis from mskus images.
fig.
1. (a) mskus images.
in medical image analysis, recent works have attempted to inject the recorded
gaze information of clinicians into deep cnn models for helping the models to
predict correctly based on lesion area.
[9,10] modeled the visual search
behavior of radiologists for breast cancer using cnn and injected human visual
attention into cnn to detect missing cancer in mammography.
[15]
demonstrated that the eye movement of radiologists can be a new supervision
form to train the cnn model.
[11] proposed the use
of a teacher-student knowledge transfer framework for us image analysis, which
combines doctor’s eye-gaze data with us images as input to a large teacher
model, whose outputs and intermediate feature maps are used to condition a
student model.
although these methods have led to promising results, they can
be diﬃcult to implement due to the need to collect doctors’ eye movement data
for each image, along with certain restrictions on the network structure.
(1)
where to adjust: modeling sonographers’ gaze map to emphasize the region that
needs adjust; (2) what to adjust: classify the instances to systemically detect
predictions made based on unreasonable/biased reasoning and adjust; (3) how
to adjust: developing a training mechanism to strike the balance between gout
prediction accuracy and attention reasonability.
2
method
fig.
the mskus image i0 ∈ rh×w ×3 is ﬁrst input into cnn encoder that con-
tains ﬁve convolution blocks.
considering that ﬂatten operation leads to losing the
spatial information, the absolute position encoding [14] is combined with the ﬂat-
ten feature map via element-wise addition to form the input of the transformer
layer.
in the cnn decoder part, a pure cnn architecture progressively up-samples
the feature maps into the original image resolution and implements pixel-wise
prediction for modeling sonographers’ gaze map.
in addition, the transformer’s out-
put is fused with the feature map from the decoding process by an element-wise
product operation to further enhance the long-range and multi-scale visual infor-
mation.
up: unreasonable precise: although the
gout diagnosis is precise, amount of attention
is given to irrelevant feature of mskus image.
our target of adjustment is to reduce
imprecise and unreasonable predictions.
in
this way, cnns not only ﬁnish correct gout
diagnosis, but also acquire the attention
region that agreements with the sonographers’
gaze map.
3
experiments
mskus dataset collection.
informed written
consent was obtained at the time of recruitment.
dataset totally contains 1127
us images from diﬀerent patients including 509 gout images and 618 healthy
images.
the resolution of the mskus images were resized to 224 × 224.
during
experiments, we randomly divided 10% of the dataset into testing sets, then
the remaining data was divided equally into two parts for the diﬀerent phases
of the training.
we collected the eye movement data with the tobii 4c
eye-tracker operating at 90 hz.
the mskus images were displayed on a 1920 ×
1080 27-inch lcd screen.
binary maps
of the same size as the corresponding mskus images were generated using the
gaze data, with the pixel corresponding to the point of gaze marked with a’1’
and the other pixels marked with a’0’.
a sonographer gaze map s was generated
for each binary map by convolving it with a truncated gaussian kernel g(σx,y),
where g has 299 pixels along x dimension, and 119 pixels along y dimension.
five metrics were used to evaluate model performance:
accuracy (acc), area under curve (auc), correlation coeﬃcient (cc), sim-
ilarity (sim) and kullback-leibler divergence (kld)
acc and auc were
implemented to assess the gout classiﬁcation performance of each model, while
cc, sim, and kld were used to evaluate the similarity of the areas that the
model and sonographers focus on during diagnoses.
the results, shown in table 1,
revealed that using our tls mechanism led to a signiﬁcant improvement in all
metrics.
resnet34 with tls acquired
the highest improvement in acc with a 4.41% increase, and resnet18 with
tls had a 0.027 boost in auc.
our tls mechanism consistently performed
well in improving the gout classiﬁcation performance of the cnn models.
the performances of models training wi/wo our mechanism in mskus.
consequently, it was possible to
use predicted gaze maps for both the training and testing phases of the classiﬁca-
tion models without any notable performance decrease.
this removed the need to
collect eye movement maps during the training and testing phases, signiﬁcantly
lightening the workload of data collection.
therefore, our tls mechanism, which
involved predicting the gaze maps, could potentially be used in clinical environ-
ments.
this would allow us to bypass the need to collect the real gaze maps of
the doctors while classifying newly acquired us images, and thus improved the
clinical implications of our mechanism, “thinking like sonographers”.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_17.pdf:
dynamic contrast-enhanced ultrasound (ceus) video with
microbubble contrast agents reﬂects the microvessel distribution and
dynamic microvessel perfusion, and may provide more discriminative
information than conventional gray ultrasound (us).
in this paper, we propose a novel framework to diagnose
thyroid nodules based on dynamic ceus video by considering microves-
sel inﬁltration and via segmented conﬁdence mapping assists diagnosis.
speciﬁcally, the temporal projection attention (tpa) is proposed to
complement and interact with the semantic information of microvessel
perfusion from the time dimension of dynamic ceus.
the experimental results on clinical
ceus video data indicate that our approach can attain an diagnostic
accuracy of 88.79% for thyroid nodule and perform better than conven-
tional methods.
in addition, we also achieve an optimal dice of 85.54%
compared to other classical segmentation methods.
therefore, consid-
eration of dynamic microvessel perfusion and inﬁltrative expansion is
helpful for ceus-based diagnosis and segmentation of thyroid nodules.
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43987-2 17.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
in practice, ceus video allows the
dynamic observation of microvascular perfusion through intravenous injection of
contrast agents.
many microvessels around nodules are constantly inﬁltrating and growing into
the surrounding tissue.
ˆpn} represents the inﬁltration pro-
cess of microvessels from gray us to ceus, and ˆpn is the ﬁnal segmentation result.
used the spatial feature enhancement for disease diagnosis based on dynamic
ceus.
even the sota segmentation methods cannot accurately identify the lesion
area for blurred lesion boundaries, thus, the existing automatic diagnosis network
using ceus still requires manual labeling of pixel-level labels which will lose key
information around the tissues
whether the awareness of inﬁltrative area information can
be helpful in the improvement of diagnostic accuracy is still unexplored.
first, the temporal projection
attention (tpa) is proposed to complement and interact with the semantic
information of microvessel perfusion from the time dimension.
the tasks of
lesion area recognition and diﬀerential diagnosis are pixel-level and image-level
classiﬁcations, and some low-level features of these two tasks can be shared inter-
actively [8].
after that,
in the temporal-based lesions area recognition (tlar) module, an enhanced
v-net with the tpa is implemented to identify the relatively clear lesion area
which are visible on both gray us and ceus video.
2.1
temporal-based lesions area recognition (tlar)
the great challenge of automatic recognition of lesion area from ceus video is
that the semantic information of the lesion area is diﬀerent in the ceus video of
the diﬀerent microvessel perfusion periods.
especially in the perfusion period and
the regression period, the semantic information of lesions cannot be fully depicted
in an isolated ceus frame.
thus, the interactive fusion of semantic information
of the whole microvessel perfusion period will promote the identiﬁcation of the
lesion area, and we design the temporal projection attention (tpa) to realize
this idea.
given
a
feature
f4th
∈
rc×t × h
16 × w
16 after four down-sampling operations in encoder, its original 3d fea-
ture map is projected [11] to 2d plane to get keys and queries: k, q ∈ rc× h
16 × w
16 ,
and we use global average pooling (gap) and global maximum pooling (gmp)
as temporal projection operations.
then, we concatenate l with q to fur-
ther obtain the global attention g ∈ rc×1× h
16 × w
16 by two consecutive 1 × 1 2d
convolutions and dimension expend.
then, we use
parallel average pooling and full connection operation to reweight the channel
information of f ′′
4th to obtain the reweighted feature f ′
4th ∈ rc×t × h
16 × w
16 .
based on the fact that
inceptext [15] has experimentally demonstrated that asymmetric convolution
can eﬀectively solve the problem of highly variable size and aspect ratio, we use
asymmetric convolution in the ipo unit.
thus, we
design a task focus loss lta to generate conﬁdence maps p, as follows:
lmse =
n

i=1
1
ω

p∈ω
∥gi(pi), ˆpi(pi)∥2
(8)
thyroid nodule diagnosis in dynamic ceus via mia
175
lta =
1
2σ2
n

i=1
∥pi − pi∥2
2 + log σ
(9)
where gi is the label of ˆpi, which is generated by the operation of saf(d(i,j), αi);
pi denotes pixel in the image domain ω, σ is a learnable parameter to eliminate
the hidden uncertainty information.
as
the weight parameter, we set λ1, λ2, λ3 are 0.5,0.2,0.3 in the experiments.
3
experiments
dataset.
on the one hand, the percutaneous biopsy based pathological
examination was implemented to determine the ground-truth of malignant and
benign.
on the other hand, a sonographer with more than 10 years of experience
manually annotated the nodule lesion mask to obtain the pixel-level ground-
truth of thyroid nodules segmentation.
all data were approved by the institu-
tional review board of nanjing drum tower hospital, and all patients signed
the informed consent before enrollment into the study.
implementation details.
our network was implemented using pytorch frame-
work with the single 12 gb gpu of nvidia rtx 3060.
in addition, we carried out data augmentation,
including random rotation and cropping, and we resize the resolution of input
table 1.
quantitative lesion recognition results are compared with sota methods
and ablation experiments.
three indexes including dice, recall, and iou, were used to evaluate
the lesion recognition task, while ﬁve indexes, namely average accuracy (acc),
sensitivity (se), speciﬁcity (sp), f1-score (f1), and auc, were used to evaluate
the diagnosis task.
experimental results.
table 1 revealed that the modules (tpa, saf, and ipo)
used in the network greatly improved the segmentation performance compared
to baseline, increasing dice and recall scores by 7.60% and 7.23%, respectively.
quantitative diagnostic results are compared with sota methods and abla-
tion experiments.
for fair comparison, all methods used the manually
annotated lesion mask to assist the diagnosis.
experimental results in table 2
revealed that our baseline network could be useful for the diagnosis.
figure 3 (c) showed that the diagnosis accuracy
increased along with the increment of α and then tended to become stable when
α was close to 9.
therefore, for balancing the eﬃciency and performance, the
number of ipo was set as n = 3 and α was set as α = {1, 5, 9} to generate a
group of conﬁdence maps that can simulate the process of microvessel inﬁltration.
a4 of the supplementary
material.)

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_39.pdf:
survival prediction is crucial for cancer patients as it provides early
prognostic information for treatment planning.
recently, deep survival models
based on deep learning and medical images have shown promising performance
for survival prediction.
however, existing deep survival models are not well devel-
oped in utilizing multi-modality images (e.g., pet-ct) and in extracting region-
speciﬁc information (e.g., the prognostic information in primary tumor (pt) and
metastatic lymph node (mln) regions).
in view of this, we propose a merging-
diverging learning framework for survival prediction from multi-modality images.
our framework
is demonstrated on survival prediction from pet-ct images in head and neck
(h&n) cancer, by designing an x-shape merging-diverging hybrid transformer
network (named xsurv).
our xsurv combines the complementary information
in pet and ct images and extracts the region-speciﬁc prognostic information
in pt and mln regions.
extensive experiments on the public dataset of head
and neck tumor segmentation and outcome prediction challenge (hecktor
2022) demonstrate that our xsurv outperforms state-of-the-art survival prediction
methods.
keywords: survival prediction · transformer · head and neck cancer
supplementary information the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43987-2_39.
https://doi.org/10.1007/978-3-031-43987-2_39
merging-diverging hybrid transformer networks
401
1
introduction
head and neck (h&n) cancer refers to malignant tumors in h&n regions, which is
among the most common cancers worldwide [1]. survival prediction, a regression task
that models the survival outcomes of patients, is crucial for h&n cancer patients: it pro-
vides early prognostic information to guide treatment planning and potentially improves
the overall survival outcomes of patients [2].
therefore, survival prediction from pet-ct images in h&n cancer has
attracted wide attention and serves as a key research area.
for instance, head and neck
tumor segmentation and outcome prediction challenges (hecktor) have been held
for the last three years to facilitate the development of new algorithms for survival
prediction from pet-ct images in h&n cancer
[5–7].
traditional survival prediction methods are usually based on radiomics [8], where
handcrafted radiomics features are extracted from pre-segmented tumor regions and
then are modeled by statistical survival models, such as the cox proportional hazard
(coxph) model
in addition, deep survival models based on deep learning have
been proposed to perform end-to-end survival prediction from medical images, where
pre-segmented tumor masks are often unrequired [10].
deep survival models usually
adopt convolutional neural networks (cnns) to extract image features, and recently
visual transformers (vit) have been adopted for its capabilities to capture long-range
dependency within images [11, 12].
for survival prediction in
h&n cancer, deep survival models have achieved top performance in the hecktor
2021/2022 and are regarded as state-of-the-art
firstly, existing deep survival models are underdeveloped in utilizing complemen-
tary multi-modality information, such as the metabolic and anatomical information in
pet and ct images.
[17, 18] or rely on early fusion (i.e., concatenating
multi-modality images as multi-channel inputs) to combine multi-modality informa-
tion [11, 14–16, 19].
however, early fusion has difﬁculties in extracting intra-modality information due to
entangled (concatenated) images for feature extraction, while late fusion has difﬁcul-
ties in extracting inter-modality information due to fully independent feature extraction.
however, the performance of this method heavily relies on using tumor
segmentation masks as inputs, which limits its generalizability.
secondly, although deep survival models have advantages in performing end-to-end
survival prediction without requiring tumor masks, this also incurs difﬁculties in extract-
ing region-speciﬁc information, such as the prognostic information in primary tumor
402
m. meng et al.
(pt) and metastatic lymph node (mln) regions.
to address this limitation, recent deep
survival models adopted multi-task learning for joint tumor segmentation and survival
prediction, to implicitly guide the model to extract features related to tumor regions
however, most of them only considered pt segmentation and ignored the
prognostic information in mln regions [11, 24–26].
meng et al.
[16] performed survival
prediction with joint pt-mln segmentation and achieved one of the top performances
in hecktor 2022.
our xsurv has a merg-
ing encoder to fuse complementary anatomical and metabolic information in pet and
ct images and has a diverging decoder to extract region-speciﬁc prognostic informa-
tion in pt and mln regions.
this
framework is specialized in leveraging multi-modality images and extracting region-
speciﬁc information, which potentially could be applied to many survival prediction
tasks with multi-modality imaging.
extensive experiments on the public dataset of hecktor 2022
our xsurv performs joint survival prediction and segmentation,
where the two decoder branches are trained to perform pt/mln segmentation and
provide pt-/mln-related deep features for survival prediction (refer to sect.
our
xsurv also can be enhanced by leveraging the radiomics features extracted from the
xsurv-segmented pt/mln regions (refer to sect.
our implementation is provided
at https://github.com/mungomeng/survival-xsurv.
2.1
pet-ct merging encoder
assuming nconv, nself , and ncross are three architecture parameters, each encoder branch
consists of nconv conv blocks, nself hybrid parallel self-attention (hpsa) blocks, and
ncross hpca blocks.
in this study, we set nconv,
nself , and ncross as 1, 1, and 3, as this setting achieved the best validation results (refer
to the supplementary materials).
other architecture details are also presented in the
supplementary materials.
the idea of adopting convolutions and transformers in parallel has been explored
for segmentation [28], which suggests that parallelly aggregating global and local infor-
mation is beneﬁcial for feature learning.
404
m. meng et al.
fig.
different from the vanilla attention gate (ag) block
[29], rag blocks leverage the gating signals from two decoder branches and generate
mutually exclusive (softmax-activated) attention maps.
the output of the last conv block in the pt/mln branch is fed into a segmentation
head, which generates pt/mln segmentation masks using a sigmoid-activated 1 × 1
× 1 convolutional layer.
2.3
multi-task learning
following existing multi-task deep survival models [11, 16, 24–26], our xsurv is end-
to-end trained for survival prediction and pt-mln segmentation using a combined loss:
l = lsurv+λ(lpt +lmln),wheretheλisaparametertobalancethesurvivalprediction
term lsurv and the pt/mln segmentation terms lpt/mln.
the loss functions are detailed in the supplementary
materials.
the λ is set as 1 in the experiments as default.
merging-diverging hybrid transformer networks
405
2.4
radiomics enhancement
our xsurv also can be enhanced by leveraging radiomics features (denoted as radio-
xsurv).
following [16], radiomics features are extracted from the xsurv-segmented
pt/mln regions via pyradiomics
[33] and selected by least absolute shrinkage and
selection operator (lasso) regression.
the process of radiomics feature extraction is
providedinthesupplementarymaterials.then,acoxphmodel[9]isadoptedtointegrate
the selected radiomics features and the xsurv-predicted survival score to make the ﬁnal
prediction.
in addition, clinical indicators (e.g., age, gender) also can be integrated by
the coxph model.
3
experimental setup
3.1
dataset and preprocessing
we adopted the training dataset of hecktor 2022 (refer to https://hecktor.grand-cha
llenge.org/), including 488 h&n cancer patients acquired from seven medical centers
each patient underwent pretreatment pet/ct and has clinical indicators.
we present
the distributions of all clinical indicators in the supplementary materials.
recurrence-
free survival (rfs), including time-to-event in days and censored-or-not status, was
provided as ground truth for survival prediction, while pt and mln annotations were
provided for segmentation.
we resampled pet-ct images into isotropic voxels where 1 voxel corresponds to
1 mm3.
each image was cropped to 160 × 160 × 160 voxels with the tumor located
in the center.
pet images were standardized using z-score normalization, while ct
images were clipped to [−1024, 1024] and then mapped to [−1, 1].
3.2
implementation details
we implemented our xsurv using pytorch on a 12 gb geforce gtx titan x gpu.
data augmentation was applied in real-time
during training to minimize overﬁtting, including random afﬁne transformations and
random cropping to 112 × 112 × 112 voxels.
in our experiments, one training iteration (including data augmentation) took roughly
4.2 s, and one inference iteration took roughly 0.61 s.
406
m. meng et al.
3.3
experimental settings
we compared our xsurv to six state-of-the-art survival prediction methods, includ-
ing two traditional radiomics-based methods and four deep survival models.
[14], transformer-based multimodal
networks for segmentation and survival prediction (tmss)
deepmtlr-coxph, icare, and radio-deepmts achieved top performance in
hecktor 2021 and 2022.
for a fair comparison, all methods took the same pre-
processed images and clinical indicators as inputs.
survival prediction and segmenta-
tion were evaluated using concordance index (c-index) and dice similarity coefﬁcient
(dsc), which are the standard evaluation metrics in the challenges [6, 7, 35].
we also performed two ablation studies on the encoder and decoder separately: (i)
we replaced hpca/hpsa blocks with conv blocks and compared different strategies to
combinepet-ctimages.(ii)weremovedragblocksandcompareddifferentstrategies
to extract pt/mln-related information.
methods
survival
prediction
(c-index)
pt segmentation
(dsc)
mln
segmentation
(dsc)
coxph
our xsurv achieved a higher c-index than all compared methods, which demon-
strates that our xsurv has achieved state-of-the-art performance in survival prediction
of h&n cancer.
when radiomics enhancement was adopted in xsurv and deepmts,
our radio-xsurv also outperformed the radio-deepmts and achieved the highest c-
index.
moreover, the segmentation results of multi-task deep survival models (tmss,
deepmts, and xsurv) are also reported in table 1.
we attribute
these performance improvements to the use of our proposed merging-diverging learning
framework, hpca block, and rag block, which can be evidenced by ablation studies.
[19]’s study, which suggests that
early and late fusion cannot effectively leverage the complementary information in pet-
ct images.
as we have mentioned, early and late fusion have difﬁculties in extracting
intra- and inter-modality information, respectively.
our encoder ﬁrst adopts conv/hpsa
blocks to extract intra-modality information and then leverages hpca blocks to discover
their interactions, which achieved the highest c-index.
for pt and mln segmentation,
our encoder also achieved the highest dscs, which indicates that our encoder also can
improve segmentation.
[22] were compared and showed poor
performance.
this is likely attributed to the fact that leveraging non-local attention at
multiple scales has corrupted local spatial information, which degraded the segmentation
performanceanddistractedthemodelfromptandmlnregions.torelievethisproblem,
in tang et al.’s study
[22], tumor segmentation masks were fed into the model as explicit
guidance to tumor regions.
however, it is intractable to have segmentation masks at the
inference stage in clinical practice.
we
found that, even without adopting ag, using a dual-branch decoder for pt and mln
segmentation resulted in a higher c-index than using a single-branch decoder, which
demonstratestheeffectivenessofourdivergingdecoderdesign.adoptingvanillaag[29]
orraginthedual-branchdecoderfurtherimprovedsurvivalprediction.comparedtothe
vanillaag,ourragcontributedtoalargerimprovement,andthisenabledourdecoderto
achieve the highest c-index.
in the supplementary materials, we visualized the attention
maps produced by rag blocks, where the attention maps can precisely locate pt/mln
regions and screen out pt-/mln-related features.
for pt and mln segmentation, using
a single-branch decoder for pt- or mln-only segmentation achieved the highest dscs.
this is expected as the model can leverage all its capabilities to segment only one target.
nevertheless, our decoder still achieved the second-best dscs in both pt and mln
segmentation with a small gap.
408
m. meng et al.
table 2. ablation study on the pet-ct merging encoder.
methods
survival
prediction
(c-index)
pt segmentation
(dsc)
mln
segmentation
(dsc)
sbe with ce =
ce: the channel numbers or embedding dimensions used in the encoder.
methods
survival
prediction
(c-index)
pt segmentation
(dsc)
mln
segmentation
(dsc)
sbd with cd =
[256, 128, 64, 32,
16]
only pt
0.751
0.803
/
only mln
0.746
/
0.758
pt and mln
0.765
0.790
0.734
dbd with cd =
[128, 64, 32, 16, 8]
no ag
0.770
0.792
0.740
vanilla ag

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_76.pdf:
in recent years, computational pathology has seen tremen-
dous progress driven by deep learning methods in segmentation and
classiﬁcation tasks aiding prognostic and diagnostic settings.
nuclei seg-
mentation, for instance, is an important task for diagnosing diﬀerent
cancers.
however, training deep learning models for nuclei segmentation
requires large amounts of annotated data, which is expensive to col-
lect and label.
this necessitates explorations into generative modeling of
histopathological images.
in this work, we use recent advances in con-
ditional diﬀusion modeling to formulate a ﬁrst-of-its-kind nuclei-aware
semantic tissue generation framework (nasdm) which can synthesize
realistic tissue samples given a semantic instance mask of up to six dif-
ferent nuclei types, enabling pixel-perfect nuclei localization in generated
samples.
these synthetic images are useful in applications in pathology
pedagogy, validation of models, and supplementation of existing nuclei
segmentation datasets.
we demonstrate that nasdm is able to syn-
thesize high-quality histopathology images of the colon with superior
quality and semantic controllability over existing generative methods.
implementation: https://github.com/4m4n5/nasdm.
segmentation models ﬁnd
applications in spatial identiﬁcation of diﬀerent nuclei types
https://doi.org/10.1007/978-3-031-43987-2_76
nasdm: nuclei-aware histopathology image generation
787
be used to generate histopathology images with speciﬁc characteristics, such as
visual patterns identifying rare cancer subtypes [4].
as such, generative models
can be sampled to emphasize each disease subtype equally and generate more
balanced datasets, thus preventing dataset biases getting ampliﬁed by the mod-
els [7].
generative models have the potential to improve the pedagogy, trustwor-
thiness, generalization, and coverage of disease diagnosis in the ﬁeld of histology
by aiding both deep learning models and human pathologists.
addition-
ally, conditional generation of annotated data adds even further value to the
proposition as labeling medical images involves tremendous time, labor, and
training costs.
[8]
have achieved tremendous success in conditional and unconditional generation
of real-world images [3].
further, the semantic diﬀusion model (sdm) demon-
strated the use of ddpms for generating images given semantic layout [27].
in
this work, (1) we leverage recently discovered capabilities of ddpms to design a
ﬁrst-of-its-kind nuclei-aware semantic diﬀusion model (nasdm) that can gener-
ate realistic tissue patches given a semantic mask comprising of multiple nuclei
types, (2) we train our framework on the lizard dataset
[5] consisting of colon
histology images and achieve state-of-the-art generation capabilities, and (3) we
perform extensive ablative, qualitative, and quantitative analyses to establish
the proﬁciency of our framework on this tissue generation task.
2
related work
deep learning based generative models for histopathology images have seen
tremendous progress in recent years due to advances in digital pathology, com-
pute power, and neural network architectures.
more recently,
denoising diﬀusion models have been shown to generate highly compelling images
by incrementally adding information to noise [8].
success of diﬀusion models in
generating realistic images led to various conditional [12,21,22] and uncondi-
tional [3,9,19] diﬀusion models that generate realistic samples with high ﬁdelity.
semantic image synthesis is a task
involving generating diverse realistic images from semantic layouts.
gan-based
semantic image synthesis works [20,24,25] generally struggled at generating high
quality and enforcing semantic correspondence at the same time.
to this end,
a semantic diﬀusion model has been proposed that uses conditional denoising
diﬀusion probabilistic model and achieves both better ﬁdelity and diversity [27].
we use this progress in the ﬁeld of conditional diﬀusion models and semantic
image synthesis to formulate our nasdm framework.
788
a. shrivastava and p. t. fletcher
3
method
in this paper, we describe our framework for generating tissue patches con-
ditioned on semantic layouts of nuclei.
given a nuclei segmentation mask, we
intend to generate realistic synthetic patches.
in this section, we (1) describe our
data preparation, (2) detail our stain-normalization strategy, (3) review condi-
tional denoising diﬀusion probabilistic models, (4) outline the network architec-
ture used to condition on semantic label map, and (5) highlight the classiﬁer-free
guidance mechanism that we employ at sampling time.
3.1
data processing
we use the lizard dataset
this dataset con-
sists of histology image regions of colon tissue from six diﬀerent data sources
at 20× objective magniﬁcation.
the images are accompanied by full segmenta-
tion annotation for diﬀerent types of nuclei, namely, epithelial cells, connective
tissue cells, lymphocytes, plasma cells, neutrophils, and eosinophils.
a gener-
ative model trained on this dataset can be used to eﬀectively synthesize the
colonic tumor micro-environments.
the dataset contains 238 image regions, with
an average size of 1055 × 934 pixels.
as there are substantial visual variations
across images, we construct a representative test set by randomly sampling a
7.5% area from each image and its corresponding mask to be held-out for test-
ing.
the test and train image regions are further divided into smaller image
patches of 128 × 128 pixels at two diﬀerent objective magniﬁcations: (1) at 20×,
the images are directly split into 128 × 128 pixels patches, whereas (2) at 10×,
we generate 256 × 256 patches and resize them to 128 × 128 for training.
as such, at (1) 20× we extract a total of 54,735 patches for training
and 4,991 patches as a held-out set, while at (2) 20× magniﬁcation we generate
12,409 training patches and 655 patches are held out.
3.2
stain normalization
a common issue in deep learning with h&e stained histopathology slides is the
visual bias introduced by variations in the staining protocol and the raw mate-
rials of chemicals leading to diﬀerent colors across slides prepared at diﬀerent
labs [1].
a conditional
nasdm: nuclei-aware histopathology image generation
789
fig.
1. nasdm training framework: given a real image x0 and semantic mask y,
we construct the conditioning signal by expanding the mask and adding an instance
edge map.
the corrupted image xt, timestep t, and semantic condition y are
then fed into the denoising model which predicts ˆϵ as the amount of noise added to the
model.
the denoising neural network can be
parameterized in several ways, however, it has been observed that using a noise-
prediction based formulation results in the best image quality [8].
overall, our
nasdm denoising model is trained to predicting the noise added to the input
image given the semantic layout y and the timestep t using the loss described
as follows:
lsimple = et,x,ϵ
therefore, following the strategy in improved ddpms [8], we train a network to
directly predict an interpolation coeﬃcient v per dimension, which is turned into
variances and optimized directly using the kl divergence between the estimated
distribution pθ(xt−1 | xt, y) and the diﬀusion posterior q(xt−1 | xt, x0) as lvlb =
dkl(pθ(xt−1 | xt, y) ∥ q(xt−1 | xt, x0)).
(5)
3.4
conditioning on semantic mask
nasdm requires our neural network noise-predictor ϵθ(xt, y, t) to eﬀectively
process the information from the nuclei semantic map.
for this purpose, we
leverage a modiﬁed u-net architecture described in wang et al.
[27], where
semantic information is injected into the decoder of the denoising network using
multi-layer, spatially-adaptive normalization operators.
1, we
construct the semantic mask such that each channel of the mask corresponds to
a unique nuclei type.
3.5
classiﬁer-free guidance
to improve the sample quality and agreement with the conditioning signal, we
employ classiﬁer-free guidance [10], which essentially ampliﬁes the conditional
distribution using unconditional outputs while sampling.
during training, the
conditioning signal, i.e., the semantic label map, is randomly replaced with a
null mask for a certain percentage of samples.
− ∇xt log p(xt),
∝ ∇xt log p(y | xt),
(6)
nasdm: nuclei-aware histopathology image generation
791
fig.
2. guidance scale ablation: for a given mask, we generate images using diﬀer-
ent values of the guidance scale, s. the fid and is metrics are computed by generating
images for all masks in the test set at 20× magniﬁcation.
where ∅ denotes an empty semantic mask.
(7)
4
experiments
in this section, we ﬁrst describe our implementation details and training proce-
dure.
we then per-
form quantitative and qualitative assessments to demonstrate the eﬃcacy of our
nuclei-aware semantic histopathology generation model.
in all following exper-
iments, we synthesize images using the semantic masks of the held-out dataset
at the concerned objective magniﬁcation.
we then compute fr´echet inception
distance (fid) and inception score (is) metrics between the synthetic and real
images in the held-out set.
4.1
implementation details
our diﬀusion model is implemented using a semantic unet architecture
(sect. 3.4), trained using the objective in (5).
additionally, we adopt an exponential moving average (ema)
of the denoising network weights with 0.999 decay.
the whole framework is implemented using pytorch and
trained on 4 nvidia tesla a100 gpus with a batch-size of 40 per gpu. code
will be made public on publication or request.
quantitative assessment: we report the performance of our method using
fr´echet inception distance (fid) and inception score (is) with the metrics reported in
existing works.
∗note that performance reported for best competing method on the colon data
is from our own implementation, performances for both this and our method should
improve with better tuning.
[18] colon
morphology
18.8
2.2
nasdm (ours)
colon
semantic mask 14.1
2.7
4.2
ablation over guidance scale (s)
in this study, we test the eﬀectiveness of the classiﬁer-free guidance strategy.
2, increase
in guidance scale initially results in better image quality as more detail is added
to visual structures of nuclei.
however, with further increase, the image quality
degrades as the model overemphasizes the nuclei and staining textures.
3.1, we generate patches at two dif-
ferent objective magniﬁcations of 10× and 20×. in this
section, we contrast the generative performance of the
models trained on these magniﬁcation levels respectively.
from the table on right, we observe that the model trained at 20× objective
magniﬁcation produces better generative metrics.
4.4
quantitative analysis
to the best of our knowledge, ours is the only work that is able to synthesize
histology images given a semantic mask, making a direct quantitative compari-
son tricky.
however, the standard generative metric fr´echet inception distance
(fid) measures the distance between distributions of generated and real images
in the inception-v3
[14] latent space, where a lower fid indicates that the model
is able to generate images that are very similar to real data.
nasdm: nuclei-aware histopathology image generation
793
fig.
3. qualitative results: we generate synthetic images given masks with each
type of nuclei in diﬀerent environments to demonstrate the proﬁciency of the model to
generate realistic nuclei arrangements.
we now qualitatively discuss the proﬁciency
of our model in generating realistic visual patterns in synthetic histopathology
images (refer fig. 3).
in the synthetic images, we can see
that the lymphocytes are accurately circular, while neutrophils and eosinophils
have a more lobed structure.
epithelial cells are most diﬃcult to generate in a convincing
manner, however, we can see that model is able to capture the nuances well and
generates accurate chromatin distributions.
5
conclusion and future works
in this work, we present nasdm, a nuclei-aware semantic tissue generation
framework.
additionally, this framework can be extended to also generate semantic
masks enabling an end-to-end tissue generation framework that ﬁrst generates a
mask and then synthesizes the corresponding patch.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_62.pdf:
despite the fact that the appearance of the tumor-associated stroma
contributes to diagnostic impressions, its assessment has not been stan-
dardized.
given the crucial role of the tumor microenvironment in tumor
progression, it is hypothesized that the morphological analysis of stroma
could have diagnostic and prognostic value.
the model achieved an average testing auroc of 86.53% on a large
curated dataset with over 1.1 million stroma patches.
our experimental
results indicate that stromal alterations are detectable in the presence of
prostate cancer and highlight the potential for tumor-associated stroma
to serve as a diagnostic biomarker in prostate cancer.
understanding ﬁeld eﬀect is essential for cancer research
as it provides insights into the mechanisms underlying tumor development and
progression.
tumor-associated stroma, which consists of various cell types, such
as ﬁbroblasts, smooth muscle cells, and nerve cells, is an integral component
of the tumor microenvironment that plays a critical role in tumor development
and progression.
reactive stroma, a distinct phenotype of stromal cells, arises
in response to signaling pathways from cancerous cells and is characterized by
altered stromal cells and increased extracellular matrix components [7,8].
altered stroma can create a
pro-tumorigenic environment by producing a multitude of chemokines, growth
factors, and releasing reactive oxygen species [9,10], which can lead to tumor
development and aggressiveness
manual review for tumor-associated stroma is time-consuming and lacks
quantitative metrics
machine learning algorithms have been used to quantify
the percentage of tumor to stroma in bladder cancer patients, but required
dichotomizing patients based on a threshold
software has been used to seg-
ment tumor and stroma tissue in breast cancer patient samples, but the method
required constant supervision by a pathologist [15].
combining the information from both modalities can provide a more
644
z. wang et al.
accurate understanding of the tumor microenvironment.
in this work, we explore
the ﬁeld eﬀect in prostate cancer by analyzing tumor-associated stroma in multi-
modal histopathological images.
our main contributions can be summarized as
follows:
– to the best of our knowledge, we present the ﬁrst deep-learning approach
to characterize prostate tumor-associated stroma by integrating histological
image analysis from both whole-mount and biopsy slides.
– we proposed a novel approach for stroma classiﬁcation with spatial graphs
modeling, which enable more accurate and eﬃcient analysis of tumor microen-
vironment in prostate cancer pathology.
given the spatial nature of cancer
ﬁeld eﬀect and tumor microenvironment, our graph-based method oﬀers valu-
able insights into stroma region analysis.
– we developed a comprehensive pipeline for constructing tumor-associated
stroma datasets across multiple data sources, and employed adversarial train-
ing and neighborhood consistency regularization techniques to learn robust
multimodal-invariant image representations.
2
method
2.1
stroma tissue segmentation
accurately analyzing tumor-associated stroma requires a critical pre-processing
step of segmenting stromal tissue from the background, including epithelial tis-
sue.
this segmentation task is challenging due to the complex and heterogeneous
appearance of the stroma.
to address this, we propose utilizing the pointrend
model [17], which can handle complex shapes and appearances and produce
smooth and accurate segmentations through iterative object boundary reﬁne-
ment.
moreover, the model’s eﬃciency and ability to process large images quickly
make it suitable for analyzing whole-mount slides.
by leveraging the pointrend
model, we can generate stromal segmentation masks for more precise down-
stream analysis.
the spatial
relationship can reveal valuable information about the tumor microenvironment,
and neighboring stroma cells can undergo similar phenotypic changes in response
to cancer.
the stroma segmentation module generates a stroma mask to
isolate the stromal tissue, which is then used to construct spatial patch graphs for the
proposed deep-learning model.
fig.
the use of neighbor
sampling enables eﬃcient processing of large images and allows for stochastic
training of the model.
to predict tumor-associated binary labels of stroma patches, we employ a
message-passing approach that propagates patch features in the spatial graph.
the gat uses an attention mechanism
on node features to construct a weighting kernel that determines the impor-
tance of nodes in the message-passing process.
the
gat layer is deﬁned as
ge (vi) =

vj∈n e
vi∪{vi}
αvi,vjw⃗hvj
(1)
where w ∈ rm×n is a learnable matrix transforming n-dimensional features to
m-dimensional features.
based on this assumption, ncr introduces a neighbor consistency loss
deep learning for prostate tumor-associated stroma identiﬁcation
647
to encourage similar predictions of stroma patches that are similar in feature
space.
2.4
adversarial multi-modal learning
biopsy and whole-mount slides provide complementary multi-modal informa-
tion on the tumor microenvironment, and combining them can provide a more
comprehensive understanding of tumor-associated stroma.
however, using data
from multiple modalities can introduce systematic shifts, which can impact the
performance of a deep learning model.
speciﬁcally, whole-mount slides typically
contain larger tissue sections and are processed using diﬀerent protocols than
biopsy slides, which can result in diﬀerences in image quality, brightness, and
contrast.
these technical diﬀerences can aﬀect the pixel intensity distributions
of the images, leading to systematic shifts in the features that the deep learning
model learns to associate with tumor-associated stroma.
for instance, a model
trained on whole-mount slides only may not generalize well to biopsy slides due
to systematic shifts, hindering model performance in the clinical application
scenario.
to address the above issues, we propose an adversarial multi-modal learning
(aml) module to force the feature extractor to produce multimodal-invariant
representations on multiple source images.
the module takes
the stroma embedding as an input and predicts the source of the image (biopsy
or whole-mount) using multilayer perceptron (mlp) with cross-entropy loss
function laml.
all mod-
ules were concurrently optimized in an end-to-end manner.
3
experiment
3.1
dataset
in our study, we utilized three datasets for tumor-associated stroma analysis.
(1) dataset a comprises 513 tiles extracted from the whole mount slides of 40
patients, sourced from the archives of the pathology department at cedars-
sinai medical center (irb# pro00029960).
it combines two sets of tiles: 224
images from 20 patients featuring stroma, normal glands, low-grade and high-
grade cancer
[22], along with 289 images from 20 patients with dense high-grade
cancer (gleason grades 4 and 5) and cribriform/non-cribriform glands [23].
each
tile measures 1200×1200 pixels and is extracted from whole slide images cap-
tured at 20x magniﬁcation (0.5 microns per pixel).
the tiles were annotated
at the pixel-level by expert pathologists to generate stroma tissue segmentation
masks and were cross-evaluated and normalized to account for stain variabil-
ity.
(2) dataset b included 97 whole mount slides with an average size of over
174,000×142,000 pixels at 40x magniﬁcation.
the prostate tissue within these
slides had an average tumor area proportion of 9%, with an average tumor area of
77 square mm.
dataset a was utilized for training the stroma segmentation model.
extensive
data augmentation techniques, such as image scaling and staining perturbation,
were employed during the training process.
the model achieved an average test
dice score of 95.57 ± 0.29 through 5-fold cross-validation.
this model was then
applied to generate stroma masks for all slides in datasets b and c. to precisely
isolate stroma tissues and avoid data bleeding from epithelial tissues, we only
extracted patches where over 99.5% of the regions were identiﬁed as stroma at
40x magniﬁcation to construct the stroma classiﬁcation dataset.
for positive tumor-associated stroma patches, we sampled patches near
tumor glands within annotated tumor region boundaries, as we presumed that
tumor regions represent zones in which the greatest amount of damage has pro-
gressed.
to incorporate multi-modal information, we
randomly sampled negative stroma patches from all biopsy slides in dataset c.
overall, we selected over 1.1 million stroma patches of size 256×256 pixels at 40x
magniﬁcation for experiments.
during model training and testing, we performed
stain normalization and standard image augmentation methods.
3.2
model training and evaluation
for constructing knn-based patch graphs, we limited the graph size by setting
k = 4 and layer number l = 3.
all models were
implemented using pytorch on a single tesla v100 gpu. to evaluate the model
performance, we perform 5-fold cross-validation, where all slides are stratiﬁed by
source origin and divided into 5 subsets.
we measure the prediction performance using the area under the receiver
operating characteristic (auroc), f1 score, precision, and recall.
4
results and discussions
table 1.
performance comparison with model variants.
results are averaged over 5
folds and shown in terms of mean value ± standard deviation.
to evaluate the eﬀectiveness of our proposed method, we conducted an abla-
tion study by comparing the performance of diﬀerent model variants presented
in table 1.
we systematically add
one or more modules to the base model to evaluate their performance contri-
bution.
the results show that the full model outperforms the base model by a
large margin with 10.04% in auroc and 10.97% in f1 score, and each module
contributes to the overall performance.
compared to the base model, the addi-
tion of the gat module resulted in a signiﬁcant improvement in all metrics,
suggesting spatial information captured by the patch graph was valuable for
stroma classiﬁcation.
the most notable performance improvement was achieved
by the aml module, with a 5.72% increase in auroc and 5.55% increase in
650
z. wang et al.
recall.
this improvement indicates that aml helps the model better capture the
multimodal-invariant features that are associated with tumor-associated stroma
while reducing the false negative prediction by eliminating the inﬂuence of sys-
tematic shift cross modalities.
finally, the addition of the ncr module further
increased the average model performance and improved the model robustness
across 5 folds.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_63.pdf:
automated detection of cervical abnormal cells from thin-
prep cytologic test (tct) images is essential for eﬃcient cervical abnor-
mal screening by computer-aided diagnosis system.
however, the detec-
tion performance is inﬂuenced by noise samples in the training dataset,
mainly due to the subjective diﬀerences among cytologists in annotating
the training samples.
in this paper, we propose a cervical
abnormal cell detection method optimized by a novel distillation strat-
egy based on local-scale consistency reﬁnement.
then, a pre-trained patch correction network (pcn)
is leveraged to obtain local-scale features and conduct further reﬁne-
ment for these suspicious cell patches.
our experiments demon-
strate that our distillation method can greatly optimize the performance
of cervical abnormal cell detection without changing the detector’s net-
work structure in the inference.
the code is publicly available at https://
github.com/feimanman/cervical-abnormal-cell-detection.
keywords: cervical abnormal cell detection · consistency learning ·
cervical cytologic images
1
introduction
cervical cancer is the second most common cancer among adult women.
nevertheless, delayed
diagnosis of cervical cancer until an advanced stage will have a negative impact
on patient prognosis and consume medical resources.
currently, early screening
of cervical cancer is recommended worldwide as an eﬀective method to prevent
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
therefore, there is a high demand
for automated cervical abnormality screening to facilitate eﬃcient and accurate
identiﬁcation of cervical abnormalities.
with the development of deep learning
[2] developed an attention feature pyramid network
(attfpn) for automatic abnormal cervical cell detection in cervical cytopatho-
logical images to assist pathologists in making more accurate diagnoses.
[11] proposed to explore contextual
relationships to boost the performance of cervical abnormal cell detection.
it is worth mentioning that all of the aforementioned
detection methods inevitably produce false positive results, which should be fur-
ther reﬁned by pathologists for manual checking or classiﬁcation models estab-
lished for automatic screening.
[23] proposed a
three-stage method including cell-level detection, image-level classiﬁcation, and
case-level diagnosis obtained by an svm classiﬁer.
although the above-mentioned attempts can improve the screening perfor-
mance signiﬁcantly, there are several issues that need to be addressed: 1) object
detection methods often require accurate annotated data to guarantee perfor-
mance with robustness and generalization.
however, due to legal limitations, the
scarcity of positive samples, and especially the subjectivity diﬀerences between
cytopathologists for manual annotations [20], it is likely to generate noisy sam-
ples that aﬀect the performance of the detection model.
to address these issues, we propose a novel method for cervical abnormal
cell detection using distillation from local-scale consistency reﬁnement.
in
addition, we propose an roi-correlation consistency (rcc) loss between roi
features and local-scale features from the pcn, which encourages the detector
to explore the feature correlations of the suspicious cells.
our proposed method
achieves improved performance during inference without changing the detector
structure.
concerning the huge size of the whole slide image
(wsi) and the infeasibility to handle a wsi scan for detection, we crop the
wsi into images with the size of 1024 × 1024 as input to the detection.
we implement the detection to locate
the suspicious lesion cervical cells and extract the top k patches from the orig-
inal image.
our
framework leverages a local-scale classiﬁcation reﬁnement mechanism to guide
the training of the detection model.
the pcn is employed to
reﬁne and enhance the retinanet proposal classiﬁer, which is trained from a
large number of patches collected in advance with more excellent classiﬁcation
performance.
more speciﬁcally, the input image is processed by the base detector fd(·)
ﬁrstly to obtain the primary proposal information.
the proposed pcn fc(·) takes
the top-k patches as inputs, which are cropped from original images according
to the proposal location, denoted as ip = cr(i, p), where cr(·) denotes the crop
function, i and p denote input image and proposal boxes predicted by fd(·),
respectively.
(1)
the key idea is to augment the base detector fd(·) with the pcn fc(·) in parallel
to enhance the proposal classiﬁcation capability.
speciﬁcally, the ranking loss is given by:
lrank(sd, sc) = max {0, sc − sd + margin} ,
(2)
where sc is the classiﬁcation reﬁnement score and sd is the detection score,
which enforces sd > sc + margin in training.
based on the consistency strategy [14], which enhances the consistency of
the intrinsic relation among diﬀerent models, we propose roi-correlation consis-
tency, which regularizes the network to maintain the consistency of the seman-
tic relation between patches under roi features and local-scale features, and
thereby encourage the detector to explore the feature interaction between cells
from the extracted patches to improve the network performance.
and each sample undergoes the roi align layer to obtain the top
k rois, we denote the activation map of rois as f r ∈ rb×k×h×w ×c, where
h and w are the spatial dimension of the feature map, and c is the channel
number.
we average pooling the
feature map f r along the spatial dimension and reshape it into ar ∈ rbk×c,
and then the case-wise gram matrix gr ∈ rbk×bk is computed as:
gr = ar · (ar)t ,
(3)
where gij is the inner product between the vectorized activation map ar
i and
ar
j , whose intuitive meaning is the similarity between the activations of ith roi
and jth roi within the input mini-batch.
we perform average pooling on the feature map f c across
the spatial dimension and then reshape it into ac ∈ rbk×hw c, the case-wise
robust cervical abnormal cell detection
657
gram matrix gc ∈ rbk×bk and the ﬁnal relation matrix rc are computed
as:
gc = ac · (ac)t ,
(5)
rc =

gc
1
gc
1

2
, · · · ,
gc
bk
gc
bk

2
t
.
(6)
the rcc requires the correlation matrix to be stable under roi features
and local-scale features to preserve the semantic relation between patches.
by minimizing
lrcc during the training process, the network could be enhanced to capture the
intrinsic relation between patches, thus helping to extract additional semantic
information from cells.
2.4
optimization
to better optimize the retinanet detector in a reinforced way, we take the fol-
lowing training strategy, which consists of three major stages.
in the ﬁrst stage,
we collect images with doctors’ labels for training and initialized the detection
net.
in the second stage, we train pcn with cross-entropy loss until convergence.
in the last stage, we freeze the pcn and optimize the detector.
3
experimental results
3.1
dataset and experimental setup
dataset.
for cervical cell detection, our dataset includes 3761 images of 1024×
1024 pixels cropped from wsis.
performance comparison with state-of-the-art methods.
[11]
44.6
77.5
47.7
60.0
retinanet [12]
45.7
81.3
46.2
58.8
proposed method
51.1
86.6
54.3
62.5
images, while pathologists b and c had 10 years of experience each.
initially, the
images were randomly assigned to pathologist b or c for initial labeling.
any discrepancies found were checked and re-labeled by pathologist
a. these images were divided into the training set and the testing set according
to the ratio of 9:1.
implementation details.
the model is implemented by pytorch on 2 nvidia tesla p100 gpus.
we conduct a quantitative evaluation using two metrics: the coco-style [13]
average precision (ap) and average recall (ar).
we calculate the average ap
over multiple iou thresholds from 0.5 to 0.95 with a step size of 0.05, and indi-
vidually evaluated ap at the iou thresholds of 0.5 and 0.75 (denoted as ap.5
and ap.75), respectively.
3.2
evaluation of cervical abnormal cell detection
comparison with sota methods.
we compare the performance of our pro-
posed method against known methods for cervical lesion detection as well as rep-
resentative methods for object detection.
(2) based on retinanet, our
method improves the detection performance signiﬁcantly, especially ap.5 shows
great performance improvement.
(a) shows input images
with ground-truth annotations.
table 2. performance of ablation study for our local-scale consistency reﬁnement.
+ranking loss and rcc loss 51.1 86.6 54.3
62.5
better performance, especially in ap.75, with an improvement of 2.8.
(3) with both ranking
loss and rcc loss, our method has the best performance, which surpasses the
baseline model by a large margin, validating the eﬀectiveness of our method.
1. those feature
maps are from the conv3 stages of the class-subnet backbone.
speciﬁcally, we
sum and average the features in the channel dimension, and upsample them to
the original image size.
, our method can really learn better
feature representations for abnormal cells, with the help of our proposed classi-
ﬁcation ranking reﬁnement and roi-correlation consistency learning.
our work can achieve better performance without adding new modules
during inference.
experiments demonstrate the eﬀectiveness and robustness of
our method on the task of cervical abnormal cell detection.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_77.pdf:
quantitative immunoﬂuorescence (qif) enables identifying
immune cell subtypes across histopathology images.
the triaangil’s eﬃcacy for
microenvironment characterization from qif images is demonstrated in
problems of predicting (1) response to immunotherapy (n = 122) and
(2) overall survival (n = 135) in patients with lung cancer in comparison
with four hand-crafted approaches namely dentil, gg, ccg, spatil,
and deep learning with gnn.
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43987-2 77.
https://doi.org/10.1007/978-3-031-43987-2_77
798
s. arabyarmohammadi et al.
1
introduction
the tumor microenvironment (tme) is comprised of cancer, immune (e.g. b
lymphocytes, and t lymphocytes), stromal, and other cells together with non-
cellular tissue components
it is well acknowledged that tumors
evolve in close interaction with their microenvironment.
quantitatively charac-
terizing tme has the potential to predict tumor aggressiveness and treatment
response [3,23,24,30].
immunotherapy
(io) is the standard treatment for patients with advanced non-small cell lung
cancer (nsclc)
[19] but only 27–45% of patients respond to this treatment [21].
therefore, better algorithms and improved biomarkers are essential for identify-
ing which cancer patients are most likely to respond to io in advance of treat-
ment.
2
previous related work and novel contributions
many studies have only looked at the density of a single biomarker (e.g. tils),
to show that a high density of tils is associated with improved patient sur-
vival and treatment response in nsclc
[3,24]. other works have attempted to
characterize the spatial arrangement of cells in tme using computational graph-
based approaches.
[9] which attempted to characterize the interplay between
immune and cancer cells and has proven to be helpful in predicting the recur-
rence in early-stage nsclc.
this in turn allows for development of
machine classiﬁers to predict outcome and response in lung cancer patients
treated with io.
(2) triangil includes a set of quantitative metrics that capture the interplay
within and between nuclei corresponding to diﬀerent types/families.
[2,5,9,14]
while triangil measurements are able to consider inter- and intra-family
relationships.
(3) although deep learning (dl) models (e.g., graph neural networks(gnn))
have shown great capabilities in solving complex problems in the biomedical
ﬁeld, these tend to be black-box in nature.
these complex interactions enhance our understanding
of the tme and will help pave the way for new therapeutic strategies that
leverage these insights.
next, we
extract a series of features including clustering coeﬃcient, average degree from
the resulting subgraph.
in this manner, a total of 126 features (sup-
plemental table 1) are extracted (42 features for absence of one family ×3).
2) triangulation-based
connections:
a delaunay triangulation is con-
structed by the nodes of α, β, γ (fig.
next, a series of features
were extracted from the remaining subgraph (e.g. number of edges between the
nodes of α and β, β and γ, α and γ; complete list of features in supplemental
table 1).
next, we call gettrianglefeatures() function to quantify
triangular relationships by extracting features from the resulting subgraphs
(e.g. perimeter and area of triangles; complete list of features in supplemental
table 1).
4
experimental results and discussion
4.1
dataset
the cohort employed in this study was composed of pre-treatment tumor biopsy
specimens from patients with nsclc from ﬁve centers (two centers for training
triangular analysis of geographical interplay of lymphocytes (triangil)
801
algorithm 1: finding triangles
input: a jagged array del : delaunay graph with three vertices of every
triangle in each row, a hashmap φ : maps nodes to their type
output: triangle features trifeatset
let triindex ← ∅ be the list for triangle indices
for i = 1 to i = length(del) do
let marker ← ∅ be a auxiliary list to keep the viewed markers
for j = 1 to 3 do
if φ(del(i, j))
the entire analysis was
carried out using 122 patients in experiment 1 (73 in st, and 49 in sv) and
135 patients in experiment 2 (81 in st, and 54 in sv).
specimens were ana-
lyzed with a multiplexed quantitative immunoﬂuorescence (qif) panel using
the method described in [22].
from each whole slide image, 7 representative
tiles were obtained and used to train the software inform to deﬁne background,
tumor and stromal compartments.
then, individual cells were segmented based
on nuclear dapi staining and the segmentation performance was controlled by
direct visualization of samples by a trained observer.
[3,24]
(supplemental table 2).
(a) representative qif image.
after selecting every two cell types, features are extracted from their
convex hulls (e.g. the number of clusters of each cell type, area intersected
between clusters [9]; complete list of combinations in supplemental table 3).
gnn: a recent study [31] demonstrated that transformer-based [29] gnns are
able to learn the arrangement of tiles across pathology images for survival analysis.
here, for each tile in the slide, a delaunay graph was constructed regardless of cell
subtypes, and tile-level feature representations (e.g.side length minimum, maxi-
mum, mean, and standard deviation, triangle area minimum, maximum, mean,
and standard deviation) were aggregated by a transformer according to their spa-
tial arrangement [31].
our approach utilized the weisfeiler-lehman (wl) test
well-known approaches, such
as graphsage [10], are considered as continuous approximations to the wl test.
4.3
experiment 1: immunotherapy response prediction in lung
cancer
design: triangil was also trained to diﬀerentiate between patients who
responded to io and those who did not.
therefore, triangil approach is not only predic-
tive of treatment response but more critically it enables biological interpretations
that a dl model might not be able to provide.
the leftmost column shows a part of a qif image.
(color ﬁgure online)
804
s. arabyarmohammadi et al.
4.4
experiment 2: predicting survival in lung cancer patients
treated with immunotherapy
design: st was used to construct a least absolute shrinkage and selection oper-
ator (lasso)
lasso features are
listed in supplemental table 4.
kaplan-meier (km) survival curves [26] were plotted and the model perfor-
mance was summarized by hazard ratio (hr), with corresponding (95% conﬁ-
dence intervals (ci)) using the log-rank test, and harrell’s concordance index
(c-index) on sv.
5
concluding remarks
we presented a new approach, triangular analysis of geographical interplay of
lymphocytes (triangil), to quantitatively chartacterize the spatial arrange-
ment and relative geographical interplay of multiple cell families across patho-
logical images.
compared to previous spatial graph-based methods, triangil
quantiﬁes the spatial interplay between multiple cell families, providing a more
comprehensive portrait of the tumor microenvironment.
although ﬁve cell families were studies in this work, triangil
is ﬂexible and could include other cell types (e.g., macrophages).
future work
will entail larger validation studies and also evaluation on other use cases.
acknowledgements.
research reported in this publication was supported by the
national cancer institute under award numbers r01ca268287a1, u01 ca269181,
r01 ca26820701a1, r01ca249992- 01a1, r01ca202752- 01a1, r01ca208236- 01a1,
r01ca216579- 01a1, r01ca220581-01a1, r01ca257612- 01a1, 1u01ca239055- 01,
1u01ca248226- 01, 1u54ca254566- 01, national heart, lung and blood institute
1r01hl15127701a1, r01hl15807101a1, national institute of biomedical imaging
and bioengineering 1r43eb028736- 01, va merit review award ibx004121a from
the united states department of veterans aﬀairs biomedical laboratory research and
development service the oﬃce of the assistant secretary of defense for health aﬀairs,
through the breast cancer research program (w81xwh- 19- 1-0668), the prostate
cancer research program (w81xwh- 20-1- 0851), the lung cancer research program
(w81xwh-18-1-0440, w81xwh-20-1-0595), the peer reviewed cancer research pro-
gram (w81xwh- 18-1-0404, w81xwh- 21-1-0345, w81xwh- 21-1-0160), the kidney
precision medicine project (kpmp) glue grant and sponsored research agreements
from bristol myers-squibb, boehringer-ingelheim, eli-lilly and astrazeneca.
the con-
tent is solely the responsibility of the authors and does not necessarily represent the
oﬃcial views of the national institutes of health, the u.s. department of veterans
aﬀairs, the department of defense, or the united states government.
auﬀarth, b., l´opez, m., cerquides, j.: comparison of redundancy and relevance
measures for feature selection in tissue classiﬁcation of ct images.
computerized image-based detection and grading of
lymphocytic inﬁltration in her2+ breast cancer histopathology.
rev. 63(5), 277 (1956)
5. corredor, g., et al.: spatial architecture and arrangement of tumor-inﬁltrating
lymphocytes for predicting likelihood of recurrence in early-stage non-small cell
lung cancer.
ding, r., et al.: image analysis reveals molecularly distinct patterns of tils in
nsclc associated with treatment outcome.
human pathol.
leman, a., weisfeiler, b.: a reduction of a graph to a canonical form and an
algebra arising during this reduction.
20. newman, m.:
sato, j., et al.: cd20+ tumor-inﬁltrating immune cells and cd204+ m2
macrophages are associated with prognosis in thymic carcinoma.
schalper, k.a., et al.: objective measurement and clinical signiﬁcance of tils in
non-small cell lung cancer 107(3).
simon, r.m., subramanian, j., li, m.c., menezes, s.: using cross-validation to
evaluate predictive accuracy of survival risk classiﬁers based on high-dimensional
data.
https://doi.org/10.48550/
arxiv.1706.03762
30. whiteside, t.: the tumor microenvironment and its role in promoting tumor
growth.
transformer as a spatially aware multi-instance learning frame-
work to predict the risk of death for early-stage non-small cell lung cancer.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_61.pdf:
however, com-
pared to h&e staining, ihc staining can be much more expensive in
terms of both labor and the laboratory equipment required.
to remedy this situation, we present a new loss
function, adaptive supervised patchnce (asp), to directly deal with
the input to target inconsistencies in a proposed h&e-to-ihc image-
to-image translation framework.
the asp loss is built upon a patch-
based contrastive learning criterion, named supervised patchnce (sp),
and augments it further with weight scheduling to mitigate the negative
impact of noisy supervision.
in our exper-
iment, we demonstrate that our proposed method outperforms existing
image-to-image translation methods for stain translation to multiple ihc
stains.
keywords: generative adversarial network · contrastive learning ·
h&e-to-ihc stain translation
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43987-2 61.
for instance, the her2 (human epidermal growth factor
receptor 2) biomarker is associated with aggressive breast tumor development
and is essential in forming a precise treatment plan.
despite its capability to
provide highly valuable diagnostic information, the process of ihc staining is
very labor-intensive, time-consuming and requires specialized histotechnologists
and laboratory equipments [2].
in routine diagnostics, on account of its much lower cost, an
h&e-stained slide is prepared by pathologists in order to determine whether or
not to also apply the ihc stains for a more precise assessment of the disease.
to that end, researchers have recently proposed to use gan-based image-to-
image translation (i2it) algorithms for transforming h&e-stained slides into
ihc.
despite the progress, the outstanding challenge in training such i2it frame-
works is the lack of aligned h&e-ihc image pairs, or in other words, the incon-
sistencies in the h&e-ihc groundtruth pairs.
this inevitably prevents pixel-perfect image correspondences due to
the slice-to-slice changes in cell morphology, staining-induced degradation (e.g.
tissue-tearing), imaging artifacts that may vary among slices (e.g. camera out-of-
focus) and multi-slice registration errors.
in the latter, comparing the groundtruth ihc image to the input h&e image,
one can clearly see the inconsistencies – nearly the entire left half of the tissue
present in the h&e image is missing.
moreover, existing approaches have also exploited using expert
annotations such as per-cell labels [9], semantic masks
however,
the robustness of such approaches that punish absolute errors in the generated
image to dealing with gt inconsistencies remains unclear.
the
634
f. li et al.
work we present in this paper is based on the important realization that even
when pairs of consecutive tissue slices do not yield images that are pixel-perfect
aligned, it is highly likely that the corresponding patches in the two stains share
the same diagnostic label.
therefore, we set our goal to
meaningfully leverage such correlations to beneﬁt the h&e-to-ihc i2it while
being resilient to any inconsistencies.
furthermore, based on the observation that
any dissimilarity between the patch embeddings at corresponding locations in the
generated and groundtruth ihc images is indicative to the level of inconsistency
of the gt at that location, we employ an adaptive weighting scheme in asp.
we evaluated existing
i2it methods and ours for multiple ihc stains and demonstrate the superior
performance achieved by our method both qualitatively and quantitatively.
fig.
the patch embeddings z are extracted by a shared network f.
adaptive supervised patchnce loss
635
2
method description
2.1
the supervised patchnce (sp) loss
before getting to our asp loss, we need to ﬁrst introduce the sp loss as a robust
means to learning from inconsistent gt image pairs.
it takes the
same form as the patchnce loss as introduced in [11], except that it is applied
on the generated-gt image pair (instead of the input-generated pair).
it does so by minimizing a patch-based infonce loss [10],
which encourages the network to associate the corresponding patches with each
other in the learned embedding space, while disassociating them from the non-
corresponding ones.
with infonce, the patchnce loss is set up as follows:
given the anchor embedding ˆzy of a patch in the output image, the positive zx
is the embedding of the corresponding patch from the input image, while the
negatives zx are embeddings of the non-corresponding ones, i.e. lpatchnce =
linfonce(ˆzy , zx, zx).
as for the sp loss, given the embedding of an output patch ˆzy as anchor,
we now designate the embedding of the corresponding patch in the groundtruth
image zy as the positive and the embeddings of the non-corresponding ones
zy as the negatives.
it is worth noting that, despite the fact
that a similar patchwise constrastive loss was proposed in [1] for supervised i2it,
it is one of our contributions in this paper to explicitly exploit the robustness
of this contrastive loss in the context of h&e-to-ihc translation where the gt
pairs can be highly inconsistent for reasons mentioned previously.
2.2
the adaptive supervised patchnce (asp) loss
to learn selectively from more consistent groundtruth locations, we further pro-
pose to augment the supervised patchnce loss in an adaptive manner.
2, we show an example pair of generated vs
gt ihc images that contain signiﬁcant inconsistencies and their anchor-positive
similarity heat map.
to that end, we further augment the weight so that it is also a function of the
training iterations.
such scheduling of the weights is done so that in the beginning
of the training, the weights are uniform in order not to wrongly bias the network
when the embeddings are still indiscriminative.
(2)
we refer to the new augmented supervised patchnce loss as the adaptive
supervised patchnce (asp) loss, which can be expressed as:
lasp(g, h, x, y, t) = e(x,y)∼(x,y )
l

l=1
sl

s=1
wt(ˆzl,s
y , zl,s
y )
w l
t
·linfonce(ˆzl,s
y , zl,s
y , zl,s
y ),
(3)
adaptive supervised patchnce loss
637
fig.
2. (a) input h&e image x, (b) generated ihc image ˆy, (c) groundtruth ihc
image y, and (d) heat map of the anchor-positive cosine similarities produced by a
trained network at corresponding locations:
4. left to right: (a) input h&e image; (b) groundtruth ihc image; (c) generated
image without lsp; (d) with lsp; (e) with l(lambda,linear)
asp
.
cut is from [11]
3
experiments
datasets.
the following datasets are used in our experiments: the breast cancer
immunohistochemical (bci) challenge dataset
note
that we have additionally normalized the brightness levels of all bci images to
the same level.
due to the page limit, from the mist dataset, here we only
present detailed results on her2 and er.
additional results on mistki67 and mistpr are provided in
the supplementary materials.
implementation details.
to compare a pair of images, generated and groundtruth, we
use the standard ssim (structural similarity index measure) and phv (per-
ceptual hash value) as described in [8].
4, we compare visually the generated ihc
images by our framework.
it can be observed that by using either lsp or lasp,
the pathological representations in the generated images are signiﬁcantly more
accurate.
for those methods, fig. 5 visually illustrates the extent of
hallucinations which we believe is the reason for their poor quantitative perfor-
mance.
with lsp already being a
strong baseline, using diﬀerent adaptive strategies can provide further gains in
performance.
false morphological
alterations cause the tissue structures in the translated images to no longer match
those in the input h&e image, especially the nuclei.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_75.pdf:
weakly supervised classiﬁcation of whole slide images (wsis) in dig-
ital pathology typically involves making slide-level predictions by aggregating
predictions from embeddings extracted from multiple individual tiles.
we
validated the method using four hematoxylin and eosin stained wsi classiﬁca-
tion tasks: human epidermal growth factor receptor 2 status and estrogen receptor
status in primary breast cancer, breast cancer metastasis in lymph node tissue,
and cell of origin classiﬁcation in diffuse large b-cell lymphoma.
using the publicly available herohe challenge data
set, the method achieved a state-of-the-art performance of 90% area under the
receiver operating characteristic curve.
additionally, we present a novel model
explainability method that could identify cells associated with different classiﬁca-
tion groups, thus providing supplementary validation of the classiﬁcation model.
keywords: deep learning · whole slide images · hematoxylin and eosin
1
introduction
accurate diagnosis plays an important role in achieving the best treatment outcomes
for people with cancer [1]. identiﬁcation of cancer biomarkers permits more granular
classiﬁcation of tumors, leading to better diagnosis, prognosis, and treatment decisions
for many cancers, clinically reliable genomic, molecular, or imaging biomarkers
supplementary information the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43987-2_75.
https://doi.org/10.1007/978-3-031-43987-2_75
deep cellular embeddings: an explainable plug and play improvement
777
havenotbeenidentiﬁedandbiomarkeridentiﬁcationtechniques(e.g.,ﬂuorescenceinsitu
hybridization) have limitations that can restrict their clinical use.
however, visual examination of h&e-stained
slides is insufﬁcient for classiﬁcation of some tumors because identifying morphological
differencesbetweenmolecularlydeﬁnedsubtypesisbeyondthelimitofhumandetection.
the introduction of digital pathology (dp) has enabled application of machine learn-
ing approaches to extract otherwise inaccessible diagnostic and prognostic information
from h&e-stained whole slide images (wsis)
however, many of these models use embeddings
derived from tiles extracted using pretrained networks, and these often fail to capture
useful information from individual cells.
our
new method achieved better performance on wsi classiﬁcation tasks and had a greater
level of explainability than models that used only tile-level embeddings.
2
embedding extraction scheme
transfer learning using backbones pretrained on natural images is a common method
that addresses the challenge of using data sets that largely lack annotation.
however,
using backbones pretrained on natural images is not optimal for classiﬁcation of clinical
images [11].
the backbone
was trained with the bootstrap your own latent (byol) method [13] using four publicly
available data sets from the cancer genome atlas (tcga) and three data sets from
private vendors that included healthy and malignant tissue from a range of organs [14].
2.1
tile-level embeddings
following standard practice, we extracted tiles with dimensions of 256 × 256 pixels
from wsis (digitized at 40 × magniﬁcation) on a spatial grid without overlap.
we extracted deep cell-level embeddings by ﬁrst detecting individual cellular bound-
aries using stardist [18] and extracting 32 × 32-pixel image crops centered around each
segmented nucleus to create cell-patch images.
we then used the pre-trained resnet50
model to extract cell-level embeddings in a similar manner to the extraction of the tile-
level embeddings.
since resnet50 has a spatial reduction factor of 32 in the output of the
cnn, the 32 × 32-pixel image had a 1:1 spatial resolution in the output.
to ensure the
cell-level embeddings contained features relevant to the cells, prior to the mean pooling
in resnet50 we increased the spatial image resolution to 16 × 16 pixels in the output
from the cnn by enlarging the 32 × 32-pixel cell-patch images to 128 × 128 pixels
and skipping the last 4-layers in the network.
because of heterogeneity in the size of cells detected, each 32 × 32-pixel cell-
patch image contained different proportions of cellular and noncellular features.
higher
proportions of noncellular features in an image may cause the resultant embeddings to
be dominated by noncellular tissue features or other background features.
therefore, to
limit the information used to create the cell-level embeddings to only cellular features, we
removed portions of the cell-patch images that were outside of the segmented nuclei by
setting their pixel values to black (rgb 0, 0, 0).
finally, to prevent the size of individual
nuclei or amount of background in each cell-patch image from dominating over the cell-
level features, we modiﬁed the resnet50 global average pooling layer to only average
deep cellular embeddings: an explainable plug and play improvement
779
the features inside the boundary of the segmented nuclei, rather than averaging across
the whole output tensor from the cnn.
2.3
combined embeddings
to create a combined representation of the tile-level and cell-level embeddings, we
ﬁrst applied a nuclei segmentation network to each tile.
in addition to the wsi classiﬁcation results presented in the next sections, we also
performed experiments to compare the ability of combined embeddings and tile-level
embeddings to predict nuclei-related features that were manually extracted from the
images and to identify tiles where nuclei had been ablated.
the details and results of these
experiments are available in supplementary materials and provide further evidence of
the improved ability to capture cell-level information when using combined embeddings
compared with tile-level embeddings alone.
[19] (the code was
adapted from a publicly available implementation [20]).
when comparing the combined
embedding extraction method with the tile-level only embeddings, parameters were ﬁxed
to demonstrate differences in performance without additional parameter tuning.
[22] since it consumes less memory
(the code was adapted from a publicly available implementation [23]).
for breast cancer human epidermal growth factor receptor 2 (her2) prediction,
we used data from the herohe challenge data set [26].
for
prediction of estrogen receptor (er) status, we used images from the tcga-breast
invasive carcinoma (tcga-brca) data set [28] for which the er status was known.
deep cellular embeddings: an explainable plug and play improvement
781
for these two tasks we used artifact-free tiles from tumor regions detected with an
in-house tumor detection model.
for breast cancer metastasis detection in lymph node tissue, we used wsis of h&e-
stained healthy lymph node tissue and lymph node tissue with breast cancer metastases
from the publicly available camelyon16 challenge data set [16, 29].
4
model classiﬁcation performance
for the her2 prediction, er prediction, and metastasis detection classiﬁcation tasks,
combined embeddings outperformed tile-level only embeddings irrespective of the
downstream classiﬁer architecture used (fig. 4).
fig.
4. model performance using the xformer and a-mil architectures for the breast cancer
her2 status, breast cancer er status, and breast cancer metastasis detection in lymph node tissue
classiﬁcation tasks.
error bars represent 95% conﬁdence intervals computed by a 5000-sample
bias-corrected and accelerated bootstrap.
in fact, for the her2 classiﬁcation task, combined embeddings obtained using the
xformer architecture achieved, to our knowledge, the best performance yet reported on
the herohe challenge data set (area under the receiver operating characteristic curve
[auc], 90%; f1 score, 82%).
for coo classiﬁcation in dlbcl, not only did the combined embeddings achieve
better performance than the tile-level only embeddings with both the xformer and a-
mil architectures (fig. 5) on the ct1 test set and ct2 holdout data set, but they also
782
j. gildenblat et al.
had a signiﬁcant advantage versus tile-only level embeddings in respect of the additional
insights they provided through cell-level model explainability (sect. 4.1).
fig.
5. model performance using the xformer and a-mil architectures for the coo in dlbcl
classiﬁcation task.
error bars represent 95% conﬁdence intervals computed by a 5000-sample
bias-corrected and accelerated bootstrap.
4.1
model explainability
tile-based approaches in dp often use explainability methods such as gradient-weighted
classactivationmapping[30]tohighlightpartsoftheimagethatcorrespondwithcertain
category outputs.
to
gain insights into cell-level patterns that were very difﬁcult or impossible to obtain from
tile-level only embeddings, we applied an explainability method that assigned attention
weights to the cellular average part of the embedding.
the cellular average embedding is
1
n
n−1

i=0
eij
where eij ∈ r256 is the cellular embedding extracted from every detected cell in the tile
j

i ∈

1, 2, . .
, nj

where nj is the number of cells in the tile j. this can be rewritten
as a weighted average of the cellular embeddings
n−1

i=0
eijsigmoid(wi)/
n−1

i=0
sigmoid(wi)
where wi ∈ r256 are the per cell attention weights that if initialized to 0 result in the
original cellular average embedding.
6. cells with positive attention gradients shifted the output towards
deep cellular embeddings: an explainable plug and play improvement
783
a classiﬁcation of tumor and are labeled green.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_49.pdf:
histology analysis of the tumor micro-environment inte-
grated with genomic assays is the gold standard for most cancers in
modern medicine.
this paper proposes a gene-induced multimodal pre-
training (gimp) framework, which jointly incorporates genomics and
whole slide images (wsis) for classiﬁcation tasks.
our work aims at
dealing with the main challenges of multi-modality image-omic classi-
ﬁcation w.r.t.
experimental results
on the tcga dataset show the superiority of our network architectures
and our pre-training framework, achieving 99.47% in accuracy for image-
omic classiﬁcation.
keywords: multimodal learning · whole slide image classiﬁcation
1
introduction
pathological image-omic analysis is the cornerstone of modern medicine and
demonstrates promise in a variety of diﬀerent tasks such as cancer diagnosis
and prognosis [12].
with the recent advance of digital pathology and sequencing
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43987-2_49.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
https://doi.org/10.1007/978-3-031-43987-2_49
gene-induced multimodal pre-training for image-omic classiﬁcation
509
technologies, modern cancer screening has jointly incorporated genomics and
histology analysis of whole slide images (wsis).
though deep learning techniques have revolutionized medical imaging, design-
ing a task-speciﬁc algorithm for image-omic multi-modality analysis is challeng-
ing.
(1) the gigapixel wsis, which generally yield 15,000 foreground patches
during pre-processing, make attention-based backbones [6] hard to extract pre-
cise image (wsi)-level representations.
(3) image-omic
feature fusion [2,3] may fail to model high-order relevance and the inherent struc-
tural characteristics of each modality, making the fusion less eﬀective.
[22] via global feature, local feature
or multi-granularity alignment.
besides, vision-language models in the computer
vision community stand out for their remarkable versatility [13,14]. neverthe-
less, constrained by computing resources, the most commonly used multimodal
representation learning strategy, contrastive learning, which relies on a large
number of negative samples to avoid model collapse
a big domain gap also hampers their usage in leverag-
ing the structural characteristic of tumor micro-environment and genomic assay.
recently, the literature corpus has proposed some methods for accomplishing
speciﬁc image-omic tasks via kronecker product fusion [2] or co-attention map-
ping between wsis and genomics data [3].
in this paper, we propose a task-speciﬁc framework dubbed gene-induced
multimodal pre-training (gimp) for image-omic classiﬁcation.
furthermore, to model the high-order
relevance of the two modalities, we combine cls tokens of paired image and
genomic data to form uniﬁed representations and propose a triplet learning mod-
ule to diﬀerentiate patient-level positive and negative samples in a mini-batch.
it is worth mentioning that although our uniﬁed representation fuses features
from the whole gene expression cohort and partial wsis in a mini-batch, we
510
t. jin et al.
fig.
given a batch of image-omic pairs, we ran-
domly select a ﬁxed-length patch cohort and mask parts of the patch embeddings.
experimental results demonstrate that our gimp achieves signiﬁ-
cant improvement in accuracy than other image-omic competitors, and our mul-
timodal framework shows competitive performance even without pre-training.
2
method
given a multimodal dataset d consisting of pairs of wsi pathological images and
genomic data (xi, xg), our gimp learns feature representations via accomplish-
ing masked patch modeling and triplets learning.
-training for image-omic classiﬁcation
511
2.1
group multi-head self attention
in this section, we propose group multi-head self attention (groupmsa), a
specialized gene encoder to capture structured features in genomic data cohorts.
speciﬁcally, inspired by tokenisation techniques in natural language process-
ing
∈ rnge is partitioned into nf non-
overlapping fragments, and we then use a linear projection head to acquire
fragment features
hf ∈ rnf ×d, where d is the hidden dimension.
firstly, the fragment features are divided into groups and there
are ngr learnable group tokens linked to each group resulting in (nf/ngr
after that, we
model cross-group interactions by another msa layer on the global scale with
the locally learned group tokens and a ﬁnal classiﬁcation token clsge ∈ rd.
finally, groupmsa could learn dense semantics from the genomic data cohort.
2.2
patch aggregator with eﬃcient attention operation
let’s denote the whole slide pathological image with h×w spatial resolution and
c channels by xi ∈
we follow the preprocessing strategy of clam
[11] to acquire patch-level embedding sequence, i.e., each foreground patch with
256×256 pixels is fed into an imagenet-pretrained resnet50 and the background
region is discarded.
∈ r1024np
j=1 denote the sequence of patch
embeddings corresponding to wsi xi and note that the total patch number np
is image-speciﬁc.
[20] to aggre-
gate patch embeddings and yield image-level predictions.
speciﬁcally, the input
sequence hp is ﬁrst embedded into a d-dimensional feature space and combined
with a classiﬁcation token clsimg, yielding h0
p ∈ r(np+1)×d.
in addition, in order to construct the mini-batch, the sub-
sequences we intercept in the mpm pre-training phase may not be suﬃciently
representative of the image-level characteristics.
to overcome these issues, we
further propose a gene-induced triplet learning module, which uses pathological
images and genomic data as input and extracts high-order and discriminative
features via cls tokens.
the loss function for optimizing triplet learning is computed
by:
ltri = max(


x − x+

2
2 + δ −


x − x−

2
2 , 0),
(4)
gene-induced multimodal pre-training for image-omic classiﬁcation
513
δ indicates a threshold, e.g., δ = 0.8.
applying the pre-trained backbone to image-omic
classiﬁcation task is straightforward, since gimp pre-training allows it to learn
representative patient-level features.
3
experiments
3.1
experimental setup
datasets.
among 946 image-omic pairs, 470 of them belong to luad
and 476 cases are lusc.
implementation details.
the pre-training process of all algorithms is con-
ducted on the training set, without any extra data augmentation.
at last, we measure the performance on the test set.
all
experiments are conducted on a single nvidia geforce rtx 3090.
3.2
comparison between gimp and other methods
we conduct comparisons between gimp and three competitors under diﬀer-
ent settings.
(a)
image-omic gimp pre-trained, (b) gimp pre-trained without gene inducing, (c) biovil
[1] pre-trained, (d) mgca
[23],
three popular multimodal pre-training algorithms in medical text-image classiﬁ-
cation task.
even without pre-training stage, gimp
shows competitive performance compared to porpoise [4], pathomic fusion
[2], and mcat
[3], three inﬂuential image-omic classiﬁcation architectures.
compari-
gene-induced multimodal pre-training for image-omic classiﬁcation
515
table 2. ablation study on tcga lung cancer dataset.
moreover, compared to the mentioned self-supervised methods biovil
in the ﬁrst two rows, groupmsa achieves
0.53% improvement compared to snn [7], a popular genetic encoders used
in porpoise [4] and pathomic fusion [2].
“aggregator
+ triplet” indicates using unimodal image features to build triplets.
we can
likewise ﬁnd that the lack of precise global representation leads to worse per-
formance.
we can observe a
performance drop without mpm module, e.g., from 99.47% to 95.26%, which
demonstrates that local pathological information is equally critical as high-order
relevance.
516
t. jin et al.
4
conclusion
in this paper, we propose a novel multimodal pre-training method to exploit
the complementary relationship of genomic data and pathological images.
experimental
results demonstrate the superior performance of the proposed gimp compared
to other state-of-the-art methods.
the contribution of each proposed component
of gimp is also demonstrated in the experiments.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_74.pdf:
the utility of machine learning models in histopathology
image analysis for disease diagnosis has been extensively studied.
while
most current techniques utilize small ﬁelds of view (so-called local fea-
tures) to link histopathology images to patient outcome, in this work we
investigate the combination of global (i.e., contextual) and local features
in a graph-based neural network for patient risk stratiﬁcation.
we compared the performance of our proposed model against the state-
of-the-art (sota) techniques in histopathology risk stratiﬁcation in two
cancer datasets.
keywords: histopathology · risk assessment · graph processing
1
introduction
the examination of tissue and cells using microscope (referred to as histology)
has been a key component of cancer diagnosis and prognostication since more
than a hundred years ago.
the great rise of deep learning in the past decade and our ability to digitize
histopathology slides using high-throughput slide scanners have fueled inter-
ests in the applications of deep learning in histopathology image analysis.
the
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43987-2 74.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
et al.
majority of the eﬀorts, so far, focus on the deployment of these models for diag-
nosis and classiﬁcation [27].
while prognostication and survival analysis oﬀer invaluable insights
for patient management, biological studies and drug development eﬀorts, they
require careful tracking of patients for a lengthy period of time; rendering this
as a task that requires a signiﬁcant amount of eﬀort and funding.
in the machine learning domain, patient prognostication can be treated as a
weakly supervised problem, which a model would predict the outcome (e.g., time
to cancer recurrence) based on the histopathology images.
diﬀerent mil variations have shown supe-
rior performances in grading or subtype classiﬁcation in comparison to outcome
prediction [10].
[17] by utilizing mes-
sage passing mechanism via edges connecting the nodes (i.e., small patches in our
case).
while local contexts mainly capture cell-cell
interactions, global patterns such as immune cell inﬁltration patterns and tumor
invasion in normal tissue structures (e.g., depth of invasion through myometrium
in endometrial cancer [1]) could capture critical information about outcome [10].
hence, locally focused methods are unable to beneﬁt from the coarse properties
of slides due to their high dimensions which may lead to poor performance.
the code and graph
embeddings are publicly available at https://github.com/pazadimo/all-in
2
related works
2.1
weakly supervised learning in histopathology
utilizing weakly supervised learning for modeling histopathology problems has
been getting popular due to the high resolution of slides and substantial time
all-in
767
and ﬁnancial costs associated with annotating them as well as the development
of powerful deep discriminative models in the recent years
[24].
such models are used to perform nuclei segmentation
however, current gnn-based risk
assessment variants are only focused on short-range interactions [16,17] or con-
sider local contexts
we hypothesize that graph-based models’ performance
in survival prediction improves by leveraging both ﬁne and coarse properties.
below, we have provided
details of each module.
3.1
problem formulation
for pn, which is the n-th patient, a set of patches {patchj}m
j=1 is extracted
from the related whole slide images.
it
utilizes global and local augmentations of the input patchj and passes them
to the student (sθ1,v it ) and teacher (tθ2,v it ) models to ﬁnd their respective
768
p. azadi et al.
fig.
c) a graph is constructed and the new local instance-level embeddings
are obtained through the message-passing process.
by exploiting the message
passing mechanism, this module iteratively aggregates features from neighbors
of each vertex and generates the new node representations.
intuitively, the number of super-nodes
k should not be very large or small, as the former encourages them to only
represent local clusters and the latter leads to larger clusters and loses subtle
all-in
769
details.
also, tr(.) represents the trace
of matrix and an,norm is the normalized adjacency matrix.
overall, utilizing these two terms encourages the model to extract super-
nodes by leaning more towards the strongly associated vertexes and keeping
them against weakly connected ones
[5], while the main survival loss still controls
the global extraction process.
3.5
fine-coarse distillation
we propose our ﬁne-coarse morphological feature distillation module to leverage
all-scale interactions in the ﬁnal prediction by ﬁnding a local and a global patient-
level representations (ˆhl,n, ˆhg,n).
4
experiments and results
4.1
dataset
we utilize two prostate cancer (pca) datasets to evaluate the performance of
our proposed model.
the ﬁrst set (pca-as) includes 179 pca patients who
were managed with active surveillance (as).
radical therapy is considered
overtreatment in these patients, so they are instead monitored with regular
serum prostate-speciﬁc antigen (psa) measurements, physical examinations,
sequential biopsies, and magnetic resonance imaging [23].
this treatment involves placing a
radioactive material inside the body to safely deliver larger dose of radiation at
all-in
771
table 1. comparison of our method against baselines and ablation study on policies.
we also utilized the prostate cancer grade assessment (panda) challenge
dataset [7] that includes more than 10,000 pca needle biopsy slides (no outcome
data) as an external dataset for training the encoder of our model.
4.2
experiments
we evaluate the models’ performance in two scenarios utilizing several objective
metrics.
implementation details are available in supplementary material.
patch-gcn [10]) models that were
utilized recently for histopathology risk assessment.
superior performance of our mca policy implies that balanced exploitation of
ﬁne and coarse features with shared weights may provide more robust contex-
tual information compared to using mixed guided information or utilizing them
independently.
we evaluate model performances via kaplan-
meier curve [15] (cut-oﬀ set as the ratio of patients with recurrence within 3
772
p. azadi et al.
fig.
while
none of the baselines are capable of assigning patients into risk groups with
statistical signiﬁcance, our distillation policies achieve signiﬁcant separation in
both pca-as and pca-bt datasets; suggesting that global histo-morphological
properties improve patient stratiﬁcation performance.
this group should be managed diﬀerently from
the rest of the low-risk prostate cancer patients in the clinic.
while a prognostic biomarker provides information
about a patient’s outcome (without speciﬁc recommendation on the next course
of action), a predictive biomarker gives insights about the eﬀect of a therapeutic
intervention and potential actions that can be taken.
ablation study.
we also assess the impact of our vit on
the baselines (full-results in appendix), showing that it can, on average, improve
their performance by an increase of ∼ 0.03 in c-index for pca-as.
however, the
best baseline with vit still has poorer performance compared to our model in
both datasets, while the number of parameters (reported for vit embeddings’
size in table 1) in our full-model is about half of this baseline.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_60.pdf:
survival outcome assessment is challenging and inherently
associated with multiple clinical factors (e.g., imaging and genomics
biomarkers) in cancer.
we
emphasize the unsupervised pretraining to capture the intrinsic interac-
tion between tissue microenvironments in gigapixel whole slide images
(wsis) and a wide range of genomics data (e.g., mrna-sequence,
copy number variant, and methylation).
after the multimodal knowl-
edge aggregation in pretraining, our task-speciﬁc model ﬁnetuning could
expand the scope of data utility applicable to both multi- and single-
modal data (e.g., image- or genomics-only).
keywords: histopathological image analysis · multimodal learning ·
cancer diagnosis · survival prediction
1
introduction
cancers are a group of heterogeneous diseases reﬂecting deep interactions
between pathological and genomics variants in tumor tissue environments
high-resolution pathological images have
proven their unique beneﬁts for improving prognostic biomarkers prediction via
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43987-2 60.
https://doi.org/10.1007/978-3-031-43987-2_60
pathology-and-genomics multimodal transformer for survival prediction
623
exploring the tissue microenvironmental features
despite their impor-
tance, seldom eﬀorts jointly exploit the multimodal value between cancer image
morphology and molecular biomarkers.
the major goal of multimodal data learning is to extract complementary con-
textual information across modalities
supervised studies [5–7] have allowed
multimodal data fusion among image and non-image biomarkers.
to alle-
viate label requirement, unsupervised learning evaluates the intrinsic similar-
ity among multimodal representations for data fusion.
for example, integrating
image, genomics, and clinical information can be achieved via a predeﬁned unsu-
pervised similarity evaluation [4]. to broaden the data utility, the study
[28]
leverages the pathology and genomic knowledge from the teacher model to guide
the pathology-only student model for glioma grading.
meanwhile, the size of multimodal medical
datasets is not as large as natural vision-language datasets, which necessitates
the need for data-eﬃcient analytics to address the training diﬃculty.
to tackle above challenges, we propose a pathology-and-genomics multimodal
framework (i.e., pathomics) for survival prediction (fig. 1).
to overcome the gap of modality
heterogeneity between images and genomics, we project the multimodal embed-
dings into the same latent space by evaluating the similarity among them.
as a result, the task-speciﬁc ﬁnetuning broadens the dataset
usage (fig 1b and c), which is not limited by data modality (e.g., both single-
and multi-modal data).
our
approach could achieve comparable performance even with fewer ﬁnetuned data
(e.g., only use 50% of the ﬁnetuned data) when compared with using the entire
ﬁnetuning dataset.
1a, in the pretraining, our unsu-
pervised data fusion aims to capture the interaction pattern of image and
genomics features.
overall, we formulate the objective of multimodal feature
learning by converting image patches and tabular genomics data into group-
wise embeddings, and then extracting multimodal patient-wise embeddings.
more speciﬁcally, we construct group-wise representations for both image and
genomics modalities.
for image feature representation, we randomly divide image
patches into groups; meanwhile, for each type of genomics data, we construct
groups of genes depending on their clinical relevance
1b
and c, our approach enables three types of ﬁnetuning modal modes (i.e., multi-
modal, image-only, and genomics-only) towards prognostic prediction, expanding
the downstream data utility from the pretrained model.
fig.
in (a), we show the pipeline of extracting image
and genomics feature embedding via an unsupervised pretraining towards multimodal
data fusion.
group-wise image and genomics embedding.
the group-wise genomics representation is
deﬁned as gn ∈ r1×dg, where n ∈ n, dg is the attribute dimension in each group
which could be various.
to better extract high-dimensional group-wise genomics
representation, we use a self-normalizing network (snn) together with scaled
exponential linear units (selu) and alpha dropout for feature extraction to
generate the group-wise embedding gn ∈ r1×256 for each group.
for group-wise wsis representation, we ﬁrst cropped all tissue-region image
tiles from the entire wsi and extracted cnn-based (e.g., resnet50)
di-
dimensional features for each image tile k as hk ∈ r1×di, where di = 1, 024,
k ∈ k and k is the number of image patches.
we construct the group-wise wsis
representation by randomly splitting image tile features into n groups (i.e., the
same number as genomics categories).
therefore, group-wise image representa-
tion could be deﬁned as in ∈ rkn×1024, where n ∈ n and kn represents tile
k in group
[17], which is
able to weight the feature embeddings in the group, together with a dimension
deduction (e.g., fully-connected layers) to achieve the group-wise embedding.
1a, we propose a pathology-and-genomics multimodal model containing
two model streams, including a pathological image and a genomics data stream.
in the pathological image stream,
the patient-wise image representation is aggregated by n group representations
as ip ∈ rn×256, where p ∈ p and p is the number of patients.
due to the domain
gap between image and molecular feature heterogeneity, a proper design of
multimodal fusion is crucial to advance integrative analysis.
in the pretrain-
ing stage, we develop an unsupervised data fusion strategy by decreasing the
mean square error (mse) loss to map images and genomics embeddings into
the same space.
ideally, the image and genomics embeddings belonging to the
same patient should have a higher relevance between each other.
mse measures
the average squared diﬀerence between multimodal embeddings.
in this way, the
pretrained model is trained to map the paired image and genomics embeddings
to be closer in the latent space, leading to strengthen the interaction between
diﬀerent modalities.
lfusion = argmin 1
p
p

p=1
((ip
embedding − gp
embedding)2)
(4)
in the single modality ﬁnetuning, even if we use image-only data, the model is
able to produce genomic-related image feature embedding due to the multimodal
knowledge aggregation already obtained from the model pretraining.
as a result,
our cross-modal information aggregation relaxes the modality requirement in the
ﬁnetuning stage.
1b, for multimodal ﬁnetuning, we deploy a
concatenation layer to obtain the fused multimodal feature representation and
implement a risk classiﬁer (fc layer) to achieve the ﬁnal survival stratiﬁcation
(see appendix 2).
3
experiments and results
datasets.
all image and genomics data are publicly available.
experimental settings and implementations.
we implement two types of
settings that involve internal and external datasets for model pretraining and
ﬁnetuning.
then, we implement four-fold cross-validation on the
pathology-and-genomics multimodal transformer for survival prediction
627
fig.
2. dataset usage.
for the external setting, we implement pretraining and ﬁnetuning on the
diﬀerent datasets, as shown in fig 2b; we use tcga-coad for pretraining;
then, we only use tcga-read for ﬁnetuning and ﬁnal evaluation.
we imple-
ment a ﬁve-fold cross-validation for pretraining, and the best pretrained models
are used for ﬁnetuning.
for all experiments, we calculate the average
performance on the evaluation set across the best models.
the concordance index (c-index) is used to measure the survival prediction
performance.
for each experiment, we reported the average c-index among
three-times repeated experiments.
in table 1, our approach shows improved survival prediction per-
formance on both tcga-coad and tcga-read datasets.
we recognize that the combination of image and mrna
sequencing data leads to reﬂecting distinguishing survival outcomes.
in the meantime,
on the tcga-read, our single-modality ﬁnetuned model achieves a better
performance than multimodal ﬁnetuned baseline models (e.g., with model pre-
training via image and methylation data, we have only used the image data for
ﬁnetuning and achieved a c-index of 74.85%, which is about 4% higher than the
best baseline models).
we show that with a single-modal ﬁnetuning strategy, the
model could generate meaningful embedding to combine image- and genomic-
related patterns.
in table 1, our method could
yield better performance compared with baselines on the small dataset across
the combination of images and multiple types of genomics data.
table 1.
the comparison of c-index performance on tcga-coad and tcga-read
dataset.
[30]
image+mrna
-
58.70 ± 1.10
image+mrna
70.19 ± 1.45
image+cna
–
51.50 ± 2.60
image+cna
62.50 ± 2.52
image+methy
–
65.61 ± 1.86
image+methy
55.78 ± 1.22
ab-mil
[17]
image+mrna
–
54.12 ± 2.88
image+mrna
68.79 ± 1.44
image+cna
–
54.68 ± 2.44
image+cna
66.72 ± 0.81
image+methy
–
49.66 ± 1.58
image+methy
55.78 ± 1.22
transmil
[26]
image+mrna
–
54.15 ± 1.02
image+mrna
67.91 ± 2.35
image+cna
–
59.80 ± 0.98
image+cna
62.75 ± 1.92
image+methy
–
53.35 ± 1.78
image+methy
53.09 ± 1.46
mcat
[6]
image+mrna
-
65.02 ± 3.10
image+mrna
70.27 ± 2.75
image+cna
–
64.66 ± 2.31
image+cna
60.50 ± 1.25
image+methy
–
60.98 ± 2.43
image+methy
59.78 ± 1.20
porpoi-se [7] image+mrna
–
65.31 ± 1.26
image+mrna
68.18 ± 1.62
image+cna
–
57.32 ± 1.78
image+cna
60.19 ± 1.48
image+methy
–
61.84 ± 1.10
image+methy
68.80 ± 0.92
ours
image+mrna
image+mrna
67.32 ± 1.69 image+mrna
74.35 ± 1.15
image
63.78 ± 1.22
image
74.85 ± 0.37
mrna
60.76 ± 0.88
mrna
59.61 ± 1.37
image+cna
image+cna
61.19 ± 1.03
image+cna
73.95 ± 1.05
image
58.06 ± 1.54
image
71.18 ± 1.39
cna
56.43 ± 1.02
cna
63.95 ± 0.55
image+methy
image+methy
67.22 ± 1.67
image+methy
71.80 ± 2.03
image
60.43 ± 0.72
image
64.42 ± 0.72
methy
61.06 ± 1.34
methy
65.42 ± 0.91
pathology-and-genomics multimodal transformer for survival prediction
629
finetuning data ratio
our proposed method
average of baselines
image and mrna data
image and cna data
image and methylation 
data
line color
marker on the line
a. data efficiency evaluation on tcga-coad
b. data efficiency evaluation on tcga-read
20
40
60
80
100
40
60
80
100
finetuning data ratio
70
65
60
55
50
45
40
c-index
75
70
65
60
55
c-index
fig.
we show the average
c-index of baselines, the detailed results are shown in the appendix 3.2.
ablation analysis.
3a, by using 50% of tcga-coad ﬁnetuning data, our approach
achieves the c-index of 64.80%, which is higher than the average performance
of baselines in several modalities.
3b, our model retains a good
performance by using 50% or 75% of tcga-read ﬁnetuning data compared
with the average of c-index across baselines (e.g., 72.32% versus 64.23%).
for
evaluating the eﬀect of cross-modality information extraction in the pretraining,
we kept supervised model training (i.e., the ﬁnetuning stage) while removing
the unsupervised pretraining.
the performance is lower 2%-10% than ours on
multi- and single-modality data.
for evaluating the genomics data usage, we
designed two settings: (1) combining all types of genomics data and categorizing
them by groups; (2) removing category information while keeping using diﬀerent
types of genomics data separately.
4
conclusion
developing data-eﬃcient multimodal learning is crucial to advance the survival
assessment of cancer patients in a variety of clinical data scenarios.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_58.pdf:
whole slide image (wsi) classiﬁcation is an essential task in com-
putational pathology.
the experimental results on two public wsi datasets demonstrate
that the pro-posed mspt outperforms all the compared algorithms, suggesting its
potential applications.
keywords: whole slide image · multiple instance learning · multi-scale
feature · prototypical transformer
1
introduction
histopathological images are regarded as the ‘gold standard’ in the diagnosis of cancers.
with the advent of the whole slide image (wsi) scanner, deep learning has gained its
reputation in the ﬁeld of computational pathology [1–3].
in this
context, a wsi is considered as a bag, and the cropped patches within the slide are the
supplementary information the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43987-2_58.
however, since the average bag size of
a wsi is more than 8000 at 20 × magniﬁcation, it is computationally infeasible to use
the conventional transformer and other stacked self-attention network architectures in
mil-related tasks.
these clustering-based mil algorithms can signiﬁcantly
reduce the redundant instances, and thereby improving the training efﬁciency for wsi
classiﬁcation.however,itisdifferentfork-meanstospecifytheclusternumberaswellas
the initial cluster centers, and different initial values may lead to different cluster results,
thus affecting the performance of mil.
therefore, it is necessary to develop a method that can
fully exploit the potential complementary information between critical instances and
prototypes to improve representation learning of prototypes.
on the other hand, when pathologists analysis the wsis, they always observe the
tissues at various resolutions [17]. inspired by this diagnostic manner, some works use
multi-scaleinformationofwsistoimprovediagnosticaccuracy.forexample,lietal.[9]
adopted a pyramidal concatenation mechanism to fuse the multi-scale features of wsis,
in which the feature vectors of low-resolution patches are replicated and concatenated
with the those of their corresponding high-resolution patches; hou et al.
the mlp-mixer adopts two types of mlp layers to
allow information communication in different dimensions of data.
it can effec-
tively capture multi-scale information in wsi to improve the performance of wsi
classiﬁcation.
it is a time consuming and tedious task for pathologists to annotate
the patch-level labels in gigapixel wsis, thus, a common practice is to use a pre-trained
encoder network to extract instance-level features, such as an imagenet pre-trained
encoder or a self-supervised pre-trained encoder.
1, the optimization
process can be divided into two steps: 1) the initial cluster prototype bag pbag is obtained
in the pre-processing stage by using the k-means clustering on xbag; ; 2) pt uses xbag
to optimize pbag via the self-attention mechanism in transformer.
these attention scores are
then weighted to xbag to update the pk ∈ r1×dk for completing the calibration of the
clustering prototypes ˆp ∈ rk×dk.
606
s. ding et al.
as mentioned above, existing clustering-based mil methods use the k-means clus-
tering to identify instances prototypes in the bag, where the most important instances
that contain the key semantic information may be ignored.
speciﬁcally, the procedure of mffm is described as follows:
we ﬁrst perform the feature concatenation operation on the multi-scale output
clustering prototypes

ˆp20×, ˆp10×, ˆp5×

to construct a feature pyramid
⌣p:
concat

ˆp20×, ˆp10×, ˆp5×

→
⌣p ∈ rk×3dk
(3)
where dk is the feature vector dimension of the prototypes.
= h t
1 + w4σ

w3ln

h t
1

(4)
where ln denotes the layer normalization, σ denotes the activation function imple-
mented by gelu, w1 ∈ rk×c, w2 ∈ rc×k, w3 ∈ r3dk×dsandw4 ∈ rds×3dk are the
weight matrices of mlp layers.c and ds are tunable hidden widths in the token-mixing
and channel-mixing mlp, respectively.
finally, the h is fed to the gated attention pooling (gap)
3
experiments and results
3.1
datasets
to evaluate the effectiveness of mspt, we conducted experiments on two public dataset,
namely camelyon16 [24] and tcga-nsclc.
after pre-processing, a total of 2.4 million patches at
×20 magniﬁcation, 0.56 million patches at ×10 magniﬁcation, and 0.16 million patches
at ×5 magniﬁcation, with an average of about 5900, 1400, and 400 patches per bag.
the dataset yields 4.3 million patches at 20× magniﬁcation, 1.1
million patches at 10× magniﬁcation, and 0.30 million patches at 5× magniﬁcation with
an average of about 5000, 1200, and 350 patches per bag.
3.2
experiment setup and evaluation metrics
in wsi pre-processing, each slide is cropped into non-overlapping 256 × 256 patches
at different magniﬁcations, and a threshold is set to ﬁlter out background ones.
after
patching, we use a pre-trained resnet18 model to convert each 256 × 256 patch into
a 512- dimensional feature vector.
608
s. ding et al.
3.3
implementation details
for the feature extractor, we employed the simclr encoder trained by lee et al.
all models were implemented by python 3.8 with pytorch toolkit 1.11.0
on a platform equipped with an nvidia geforce rtx 3090 gpu.
3.4
comparisons experiment
comparison algorithms.
[11]
0.9225
0.9734
0.9095 ± 0.014
0.9432 ± 0.016
remix [16]
0.9458
0.9740
0.9167 ± 0.013
0.9509 ± 0.016
pt (ours)
0.9458
0.9809
0.9257 ± 0.011
0.9567 ± 0.013
mspt (ours)
0.9536
0.9869
0.9289 ± 0.011
0.9622 ± 0.015
experimental results.
it achieves the best classiﬁcation performance of 0.9289
multi-scale prototypical transformer
609
± 0.011 and 0.9622 ± 0.015 on the acc and auc.
in the camelyon16 dataset, the performance
of both pt and prototype-bag increases with the increase of k value, and achieves the
best results with k = 16.
these experimental results demonstrate that pt can effectively
re-calibrate the clustering prototypes to achieve superior results.
compared
with other multi-scale variants, the proposed mspt improves acc by at least 0.78%
and 0.85% on camelyon16 and tcga-nsclc, respectively, which proves that the
mlp-mixer in mffm can effectively enhance the information communication among
phenotypes and their features, thus improving the performance of feature aggregation.
we provide more empirical studies, i.e., the effect of the multi-
resolution scheme, the visualization results, and the training budgets, in supplementary
materials to better understand mspt.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_64.pdf:
in pathological image analysis, determination of gland mor-
phology in histology images of the colon is essential to determine
the grade of colon cancer.
however, manual segmentation of glands is
extremely challenging and there is a need to develop automatic methods
for segmenting gland instances.
recently, due to the powerful noise-to-
image denoising pipeline, the diﬀusion model has become one of the hot
spots in computer vision research and has been explored in the ﬁeld of
image segmentation.
in this paper, we propose an instance segmenta-
tion method based on the diﬀusion model that can perform automatic
gland instance segmentation.
firstly, we model the instance segmenta-
tion process for colon histology images as a denoising process based on
the diﬀusion model.
thirdly, to improve
the distinction between the object and the background, we apply con-
ditional encoding to enhance the intermediate features with the original
image encoding.
to objectively validate the proposed method, we com-
pared state-of-the-art deep learning model on the 2015 miccai gland
segmentation challenge (glas) dataset and the colorectal adenocarci-
noma gland (crag) dataset.
the experimental results show that our
method improves the accuracy of segmentation and proves the eﬃcacy
of the method.
gland segmentation · diﬀusion model · colon histology
images
1
introduction
colorectal cancer is a prevalent form of cancer characterized by colorectal ade-
nocarcinoma, which develops in the colon or rectum’s inner lining and exhibits
glandular structures
https://doi.org/10.1007/978-3-031-43987-2_64
instance-aware diﬀusion model for gland segmentation
663
gland formation is a crucial factor in determining tumor grade and diﬀeren-
tiation.
accurate segmentation of glandular instances on histological images is
essential for evaluating glandular morphology and assessing colorectal adeno-
carcinoma malignancy.
however, manual annotation of glandular instances is a
time-consuming and expertise-demanding process.
hence, automated methods
for glandular instance segmentation hold signiﬁcant value in clinical practice.
fig.
1. (a–b) example images from the crag dataset.
(c–d) example images from
the glas dataset.
automated segmentation has been explored using deep learning techniques
[21,33], including u-net [17], fcn
[10,11] and their vari-
ations for semantic segmentation
there are also methods that combine
information bottleneck for detection and segmentation [23].
additionally, two-
stage instance segmentation methods like mask r-cnn
[3]
have been utilized, combining object detection and segmentation sub-networks.
limitations arise from
image scaling and cropping, leading to information loss or distortion, resulting
in ineﬀective boundary recognition and over-/under-segmentation.
to overcome
these limitations, we aim to perform gland instance segmentation to accurately
identify the target location and prevent misclassiﬁcation of background tissue.
in the task of image synthesis, diﬀusion model has evolved to achieve
state-of-the-art performance in terms of quality and mode coverage compared
with gan [32].
[4] treats the object detection task as a generative task on
the bounding box space in images to handle projection detection.
several studies
have explored the feasibility of using diﬀusion model in image segmentation [26].
these methods generate segmentation maps from noisy images and demonstrate
better representation of segmentation details compared to previous deep learning
methods.
in this paper, we propose a new method for gland instance segmentation
based on the diﬀusion model.
(1) our method utilizes a diﬀusion model to per-
form denoising and tackle the task of gland instance segmentation in histology
images.
the noise boxes are generated from gaussian noise, and the predicted
ground truth (gt) boxes and segmentation masks are performed during the dif-
fusion process.
(2) to improve segmentation, we use instance-aware techniques
664
m. sun et al.
to recover lost details during denoising.
this includes employing a ﬁlter and a
multi-scale mask branch to create a global mask and reﬁne ﬁner segmentation
details.
(3) to enhance object-background diﬀerentiation, we utilize conditional
encoding to augment intermediate features with the original image encoding.
this method eﬀectively integrates the abundant information from the original
image, thereby enhancing the distinction between the objects and the surround-
ing background.
our proposed method was trained and tested on the 2015 mic-
cai gland segmentation (glas) challenge dataset [20] and colorectal adeno-
carcinoma gland (crag) dataset
[6] (as shown in fig. 1), and the experiment
results demonstrate the eﬃcacy of the method.
the image encoder consists of
a backbone that extracts multi-scale features from the input image.
the image decoder
based on a diﬀusion model incorporates the original image features as conditions to
enhance the intermediate features.
2
method
in this section, we present the architecture of our proposed method, which
includes an image encoder, an image decoder, and a mask branch.
2.
2.1
image encoder
we propose to perform subsequent operations on the features of the original
image, so we use an image encoder for advanced feature extraction.
the image
encoder takes the original image as input and we use a convolutional neural
network such as resnet [8] for feature extraction and a feaure pyramid network
instance-aware diﬀusion model for gland segmentation
665
(fpn)
the input image is x and the output is a high-level feature fr.
the image encoder operates only once and uses the
fr as condition to progressively reﬁne and generate predictions from the noisy
boxes.
2.2
image decoder
we designed our model based on the diﬀusion model
∈ (0, 1), t ∈ {1, ..., t} determines the amount of noise
that is introduced at each stage.
our image decoder is based on diﬀusion model, which can be viewed as a
noise-to-gt denoising process.
the neural network fθ(zt, t) is trained to predict z0 from the zt based on
the corresponding image x. in addition, to achieve complementary information
by integrating the segmentation information from zt into the original image
encoding, we introduce conditional encoding, which uses the encoding features
of the current step to enhance its intermediate features.
in this stage, we use the mask branch to fuse the diﬀerent scale information of
the fpn and output the mask feature fmask.
the diﬀusion process decodes roi
features into local masks, and multi-scale features can be supplemented with
more detailed information for predicting global masks to compensate for the
detail lost in the diﬀusion process, and we believe that instance masks require
a larger perceptual domain because of the higher demands on instance edges.
the optimal value for the parameter γ is usually determined based
on achieving the best overall performance on the validation set.
3
experiments and results
we presented the segmentation results of our model compared to the ground
truth in fig. 3, and provided both qualitative and quantitative evaluations that
validate the eﬀectiveness of our proposed network for gland instance segmenta-
tion.
data and evaluation metrics: we evaluated the eﬀectiveness of the pro-
posed model on two datasets: the glas dataset and the crag dataset.
the
glas dataset comprises 85 training and 80 testing images, divided into 60 images
in test a and 20 images in test b.
the crag dataset consists of 173 training
and 40 testing images.
furthermore, to enhance the training dataset and mitigate the risk
of overﬁtting, we employed random combinations of image ﬂipping, translation,
gaussian blur, brightness variation, and other augmentation techniques.
we assessed the segmentation results using three metrics from the glas chal-
lenge: (1) object f1, which measures the accuracy of detecting individual glands,
instance-aware diﬀusion model for gland segmentation
667
fig.
the instance segmentation results on the glas dataset and crag dataset.
from top to bottom: the original images, the ground truth, and the segmentation
results produced by our method.
(2) object dice, which evaluates the volume-based accuracy of gland segmen-
tation, and (3) object hausdorﬀ, which assesses the shape similarity between
the segmentation result and the ground truth.
we assigned each method three
ranking numbers based on these metrics and computed their sum to determine
the ﬁnal ranking for each method’s overall performance.
implementation details: in our experiments, we choose the resnet-50 with
fpn as the backbone in the proposed method.
the backbone is pretrained on
imagenet.
image decoder, mask branch and mask fcn head are trained end-to-
end.
we trained on the glas and crag datasets in a python 3.8.3 environment
on ubuntu 18.04, using pytorch 1.10 and cuda 11.4.
= 0.02. training was performed on a100 gpu with a
batch size of 2.
results on the glas challenge dataset: we conducted experiments to
evaluate the performance of our proposed model by comparing it with the dse
model
[6], the gcsba-net [25], and the mpcnn [19]. table 1
provides an overview of the average performance of these models.
our proposed model demonstrated a enhancement in performance, surpass-
ing the second-best method on both test a and test b datasets.
speciﬁcally,
on test a, we observed an improvement of 0.006, 0.01, and 1.793 in object
f1, object dice, and object hausdorf.
similarly, on test b, resulting in an
improvement of 0.022, 0.014 and 3.694 in object f1, object dice, and object
hausdorf, respectively.
although test b presented a more challenging task due to
the presence of complex morphology in the images, our proposed model demon-
strated accurate segmentation in all cases.
the experimental results highlighted
the eﬀectiveness of our approach in improving the accuracy of gland instance
segmentation.
668
m. sun et al.
results on the crag dataset: the proposed model was additionally evalu-
ated on the crag dataset by comparing it against the gcsba-net, doubleu-
net, dse model, mild-net, and dcan.
the average performance of these
models is shown in table 2.
our experimental results demonstrate that our pro-
posed method achieves superior performance, with improvements of 0.017, 0.012,
and 4.026 for object f1, object dice, and object hausdorﬀ, respectively, com-
pared to the second-best method.
these results demonstrate the eﬀectiveness of
our method in segmenting diﬀerent datasets.
ablation studies: our network utilizes the mask branch and conditional
encoding to enhance performance and segmentation quality.
the mask branch is responsible for multi-scale feature extraction and
fusion with the backbone network, as well as reﬁning the image decoder’s out-
put.
without the mask branch, direct usage of original image features lacks
multi-scale information and results in less accurate segmentation.
conditional
encoding is employed to establish a connection between input image features
table 1.
the experimental results on glas challenge dataset.
the experimental results on the crag dataset.
[2]
0.736 5
0.794 6
218.76
6
17
instance-aware diﬀusion model for gland segmentation
669
and the diﬀusion model.
when employing mask branch, our approach resulted in
an improvement of 0.082, 0.09, 0.07 in object f1, and 0.07, 0.078, 0.07 in object
dice, while object hausdorﬀ decreased by 10.29, 11.11, 24.47 on glas test a,
glas test b, and crag, respectively.
similarly, by utilizing conditional encod-
ing, we observed an improvement of 0.048, 0.034, 0.052 in object f1, and 0.026,
0.042, 0.057 in object dice, while object hausdorﬀ decreased by 6.771, 8.115,
12.141 on glas test a, glas test b, and crag, respectively.
table 3.
the ablation study results on the crag and glas datasets demonstrate the
impact of diﬀerent modules on performance.
the mask branch module contributes to
multi-scale feature extraction, while the conditional encoding module establishes the
connection between input image features and the diﬀusion model.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_65.pdf:
in computational pathology, nuclei segmentation from his-
tology images is a fundamental task.
while deep learning based nuclei
segmentation methods yield excellent results, they rely on a large amount
of annotated images; however, annotating nuclei from histology images
is tedious and time-consuming.
to get rid of labeling burden completely,
we propose a label-free approach for nuclei segmentation, motivated
from one pronounced yet omitted property that characterizes histology
images and nuclei: intra-image self similarity (iiss), that is, within
an image, nuclei are similar in their shapes and appearances.
first, we
leverage traditional machine learning and image processing techniques
to generate a pseudo segmentation map, whose connected components
form candidate nuclei, both positive or negative.
finally, we apply the learned u-net to produce ﬁnal nuclei segmenta-
tion.
experimental results demonstrate the eﬀectiveness of our design and,
to the best of our knowledge, it achieves the state-of-the-art per-
formances of label-free segmentation on the benchmark monuseg
dataset with a mean dice score of 79.2%.
keywords: label-free · nuclei segmentation · pseudo label
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
(color ﬁgure
online)
1
introduction
nuclei segmentation is a fundamental step in histology image analysis.
however, accurate pixel-level
annotation of nuclei is not always accessible for segmentation labeling is a labor-
intensive and time-consuming procedure.
unsupervised learning (ul) methods achieved great success in the data
dependency problem for nuclei segmentation, which learns from the structural
properties in the data without any manual annotations.
traditional ul nuclei segmenta-
tion methods include watershed [6], contour detection
these methods focus on either pixel value or shape informa-
tion but fail to take advantage of both of them.
therefore, some researchers [11–15] resort to deep ul segmentation models to
better utilize both pixel value and shape information and develop a robust app-
roach.
the common and eﬀective way is to employ image clustering by maximiz-
ing mutual information between image and predicted labels to distinguish fore-
ground and background regions.
many image-clustering-based deep ul meth-
ods for natural tasks still achieve strong performances in nuclei segmentation.
[11] constrain a convolutional neural network (cnn) with super-
pixel level segmentation results.
while reasonable results are obtained, these deep clustering-based
label-free nuclei segmentation using intra-image self similarity
675
methods still suﬀer diﬃculties: (i) poor segmentation of the regions between
adjacent nuclei.
deep clustering models succeed in transferring images to high-
dimensional feature space and obtaining image segmentation results by means
of clustering pixels’ features.
(ii) underutilization of intra-image
self similarity (iiss) information.
1, in terms of value, shape
and texture, nuclei show a similar appearance within the same image but vary
greatly among diﬀerent images1.
this phenomenon oﬀers valuable information
for networks to use but the current clustering models do not take this into
account.
to address the above issues and motivated by the iiss property, we hereby
propose a novel self-similarity-driven segmentation network (ssimnet) for
unsupervised nuclei segmentation.
2, instead of designing com-
plex discriminative network architectures, our framework derives knowledge from
the iiss property to aid the segmentation.
speciﬁcally, we obtain candidate
nuclei with some unsupervised image processing.
for the obtained candidates,
it is common that adjacent nuclei merged into one candidate due to imperfect
staining and low image quality, which violate the iiss property.
finally, we apply
the learned ssimnet to produce the ﬁnal nuclei segmentation.
to validate the eﬀectiveness of our method, we conduct extensive experiments
on the monuseg dataset [16,17] based on ten existing unsupervised segmentation
methods [9,11–15,18–20].
our method outperforms all comparison methods with
an average dice score of 0.792 and aggregated jaccard index of 0.498 on the
monuseg dataset which is close to the supervised method.
2
method
as shown in fig. 2, our ssimnet aims at unsupervised segmentation of nuclei
from histology images.
speciﬁcally, by using a matrix factorization on hema-
toxylin and eosin (h&e) stained histology images, we get the hematoxylin chan-
nel image for clustering, active contour reﬁning and softening to generate the
ﬁnal soft candidate label.
last, while testing
on the test image, to adapt the network to learn nucleus similarity within the
same image, we ﬁne tune the network with soft pseudo labels of some patches
in current test images.
1 note that in our experiments, we use an image of size 10002 or 5002.
suppose that we are given a training set is =
{is
i }n
i=1 of histopathology images without any manual annotation.
for each
image, stained tissue colors are results from light attenuation, which depends
on the type and amount of dyes that the tissues have absorbed.
this property
is prescribed by the beer-lambert law:
v = log(i0/i) = wh,
(1)
where i ∈ r3×n represents the histology image with three color channels and
n pixels, i0 is the illuminating light intensity of sample with i0
= 255 for 8-bit
images in our cases,
note that usually histopathology images are
stained with h&e and nuclei mainly absorb hematoxylin
to reduce the noise in clustering results, we use
active contour method as a smoothing operation to get hard candidate labels:
p = {pi}n
i=1 = {activecontour(fcm(it
i ))}n
i=1
(3)
label-free nuclei segmentation using intra-image self similarity
677
label smoothing.
since hard label is overconﬁdent at the border of nuclei,
which is detrimental to the training of the network, we soften the hard label one
by one for each connected component in pi using the following formulation:
(4), we obtain our soft
candidate labels ˜p from p.
2.2
data puriﬁcation and ssimnet learning
so far, soft candidate labels ˜pi have been acquired for each image is
i .
we sample k patches with overlap from original image
is
i .
by denoting our segmentation network
as f, our ﬁnal loss function to supervise the network training can be formulated
as:
loss =

˜x∈ ˜
x,˜y∈ ˜y,˜z∈ ˜
z
λlbce(f(˜x), ˜y) +
also, we can obtain tissue patches and corresponding pseudo labels for each
image in the test set termed as setk = ( ˜
x t
k , ˜yt
k , ˜zt
k ).
right: illustration of average con-
vex hull area and the average of connected component area based on usmi.
(color ﬁgure online)
3
experiments
3.1
datasets and settings
monuseg.
multi-organ nuclei segmentation [16,17] (monuseg) is used to
evaluated our ssimnet.
the monuseg dataset consists of 44 h&e stained
histopathology images with 28,846 manually annotated nuclei.
with 1000 × 1000
pixel resolution, these images were extracted from whole slide images from the
the cancer genome atlas (tcga) repository, representing 9 diﬀerent organs
from 44 individuals.
the
training and test set each consisted of 32 images tiles selected and extracted
from a set of non-small cell lung cancer (nsclc), head and neck squamous
cell carcinoma (hnscc), glioblastoma multiforme (gbm) and lower grade
glioma (lgg) tissue images.
we compare our ssimnet with several current unsupervised segmen-
tation methods.
we follow the dcgn [15] to conduct comparison experiments.
we crop the image indataset into patches of 256 × 256 pixels for training.
all the
methods were trained for 150 epochs on monuseg and 200 epochs on cpm17
each time and experimented with an initial learning rate of 5e−5 and a decay
of 0.98 per epoch.
our experiment repeated ten times on monuseg dataset and
only once on cpm17 dataset for an augmented convenience.
moreover, we ﬁne tune the network with only ﬁve epochs for each
image on test set with optimizer parameter saved in checkpoint.
label-free nuclei segmentation using intra-image self similarity
679
table 1.
performance of the nuclei segmentation on monuseg dataset.
51.0± 0.9(52.4)
3.2
experimental results
to evaluate the eﬀectiveness of ssimnet, we compare it with several deep learn-
ing based and conventional unsupervised segmentation methods on the men-
tioned datasets, including minibatch k-means (termed as mkmeans), gaus-
sian mixture model [9] (termed as gmm), invariant information clustering
[12] (termed as iic), double dip
[19] (termed as dcagmm), deep image clustering
besides, we conduct an additional comparison experiment based on cpm17
dataset to demonstrate the generalization of our method.
as shown in table 2,
our method again achieves the top performances.
moreover, as the image size of
cpm17 is smaller than that of monuseg, the performance gain is not as big as
on the monuseg dataset.
4. comparison of unsupervised nuclei segmentation results on monuseg.
(color ﬁgure online)
table 2. performance of the nuclei segmentation on cpm17 dataset.
as shown in table 3, each component in our
ssimnet can bring diﬀerent degrees of improvement, which shows that all of the
label softening, data puriﬁcation and ﬁnetuning process are signiﬁcant parts of
our ssimnet and play an indispensable role in achieving superior performance.
labelsoftening
datapuriﬁcation
finetune
precision%↑
recall%↑
dice%↑
aji%↑
✓
✓
78.8
78.7
78.3
45.6
✓
✓
79.6
77.3
77.9
47.6
✓
✓
80.8
76.1
76.7
44.1
✓
✓
✓
82.0
77.2
79.2
49.8
label-free nuclei segmentation using intra-image self similarity
681
4

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_59.pdf:
whole-slide histopathology image (wsi) is regarded as the
gold standard for survival prediction of breast cancer (bc) across dif-
ferent subtypes.
then,
besides aligning the embeddings of diﬀerent types of nodes across the
source and target domains, we proposed a novel tumor-tils interaction
alignment (ttia) module to ensure that the distribution of interaction
weights are similar in both domains.
we evaluated the performance of
our method on the brca cohort derived from the cancer genome atlas
(tcga), and the experimental results indicated that t2uda outper-
formed other domain adaption methods for predicting patients’ clinical
outcomes.
https://doi.org/10.1007/978-3-031-43987-2_59
t2uda
613
keywords: tumor-inﬁltrating lymphocytes · unsupervised domain
adaption · prognosis prediction · graph attention network · breast
cancer
1
introduction
breast cancer (bc) is the most common cancer diagnosed among females and
the second leading cause of cancer death among women after lung cancer [1].
thus, eﬀective and accurate prognosis of bc
as well as stratifying cancer patients into diﬀerent subgroups for personalized
cancer management has attracted more attention than ever before.
among diﬀerent types of imaging biomarkers, histopathological images are
generally considered the golden standard for bc prognosis since they can confer
important cell-level information that can reﬂect the aggressiveness of bc [4].
recently, with the availability of digitalized whole-slide pathological images
(wsis), many computational models have been employed for the prognosis pre-
diction of various subtypes of bc.
[5] presented a novel
approach for predicting the prognosis of er-positive bc patients by quantifying
nuclear shape and orientation from histopathological images.
[9] developed graph neural networks for unsupervised domain adaptation
in histopathological image analysis, based on a backbone for embedding input
images into a feature space, and a graph neural layer for propagating the super-
vision signals of images with labels.
although much progress has been achieved, most of the existing studies
applied the feature alignment strategy to reduce the distribution diﬀerence
between source and target domains.
it can be expected that better
prognosis performance can be achieved if we leveraged the tils-tumor interac-
tion information to resolve the survival analysis task on the target domain.
in order to pre-
serve the node-level and interaction-level similarities across diﬀerent domains,
we not only aligned the embedding for diﬀerent types of nodes but also designed
a novel tumor-tils interaction alignment (ttia) module to ensure that the
distribution of the interaction weights are similar in both domains.
we evalu-
ated the performance of our method on the breast invasive carcinoma (brca)
cohort derived from the cancer genome atlas (tcga), and the experimental
results indicated that t2uda outperforms other domain adaption methods for
predicting patients’ clinical outcomes.
2
method
we summarized the proposed t2uda network in fig. 1, which consists of three
parts, i.e., graph attention network-based framework, feature alignment(fa),
and tils-tumor interaction alignment(ttia).
source graph 
construction
target graph 
construction
gat
sagpool
gat
sagpool
gat
sagpool
cox
gat
sagpoo
l
gat
sagpool
cox
 
 tumor patch
tils patch
 ℒ
gat
sagpool
fa
fa
target sample
source sample
ttia
fa: feature alignment
ttia:  tumor-tils interaction alignment 
fig.
we obtained valid patches of 512 × 512 pixels from
pathological images and segment the tils and tumor tissues using a pre-trained
u-net++ model.
then, the principal
component analysis (pca) is implemented to reduce dimensionalities of the node
features to 128.
calculating
tils-tumor
interaction
via
graph
attention
net-
works(gats).
feature alignment.
in the proposed gat-based transfer learning framework,
the feature alignment component was employed on its ﬁrst two layers.
[15].
here, we adopted mmd for feature alignment due to its ability to measure the
distance between two distributions without explicit assumptions on the data
distribution, we showed the objective function of mmd in our method as follows:
lf a =

r=1,2

k∈l,t

1
n
n

i=1
(fi,k)r − 1
m
m

i=1

f ′
i,k
r

2
h
(4)
where h is a hilbert space, f represents the features from the source, f ′ rep-
resents the feature from the target, r represents the layer number, k ∈ {l, t}
referred to tils or tumor node.
the illustration of the proposed interaction weight alignment module.
tils-tumor interaction alignment.
to achieve domain-adaptive prognosis prediction, the ﬁnal
loss function included the cox loss, fa loss, and ttia loss as the following
formula:
lt = lcox + αlf a + βlt t ia,
(7)
where α and β represent the weights assigned to the importance of fa component
and ttia component respectively.
3
experiments and results
datasets.
we conducted our experiments on the breast invasive carcinoma
(brca) dataset from the cancer genome atlas (tcga).
we hope to investigate if
the proposed t2uda could be used to help improve the prognosis performance
of (er+) or (er−) with the aid of the survival information on its counterpart.
3.1
implementation details and evaluation metrics
the dimension of intermediate layers in gat was 256.
during training,
the model was trained for 150 epochs for both the main experiment and all
comparative experiments.
we evaluated the performance of our model using
the concordance index (ci) and area under the curve (auc) as performance
metrics.
both ci and auc range from 0 to 1, with larger values indicating better
prediction performance and
quantitative performance comparison between diﬀerent unsupervised
domain adaptation methods and our method.
3.2
result and discussion
in this study, we compared the performance of our proposed model with sev-
eral existing domain adaptation methods, including 1) ddc
the experimental results were
presented in table 1.
first, our
proposed method outperformed feature alignment-based methods such as ddc
and deepjdot in terms of both ci and auc values.
we also evaluated the contributions of the key components of our framework
and found that t2uda performed better than source only and t2uda-v1,
which shows the advantage of minimizing diﬀerences in tils-tumor interaction
weights.
in addition, we also evaluated the patient stratiﬁcation performance of dif-
ferent methods.
3, our proposed t2uda outperformed feature
alignment-based methods (such as ddc and deepjdot), adversarial-based
methods (such as dann and mdd), and t2uda-v1 in stratiﬁcation perfor-
mance, proving that considering the interaction between tils and tumors as
migration knowledge leads to better prognostic results.
4
conclusion
in this paper, we presented an unsupervised domain adaptation algorithm that
leverages tils-tumor interactions to predict patients’ survival in a target bc
subtype(t2uda).

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_73.pdf:
in computation pathology, the pyramid structure of gigapixel
whole slide images (wsis) has recently been studied for capturing vari-
ous information from individual cell interactions to tissue microenviron-
ments.
considering that the information from diﬀerent resolutions is
complementary and can beneﬁt each other during the learning process,
we further design a novel bidirectional interaction block to establish com-
munication between diﬀerent levels within the wsi pyramids.
we evaluate our methods
on two public wsi datasets from tcga projects, i.e., kidney carcinoma
(kica) and esophageal carcinoma (esca).
experimental results show
that our higt outperforms both hierarchical and non-hierarchical state-
of-the-art methods on both tumor subtyping and staging tasks.
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43987-2 73.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
https://doi.org/10.1007/978-3-031-43987-2_73
756
z. guo et al.
1
introduction
histopathology is considered the gold standard for diagnosing and treating many
cancers [19].
the tissue slices are usually scanned into whole slide images
(wsis) and serve as important references for pathologists.
unlike natural images,
wsis typically contain billions of pixels and also have a pyramid structure, as
shown in fig.
typically, mil-based wsi
analysis methods have three steps: (1) crop the huge wsi into numerous image
patches; (2) extract instance features from the cropped patches; and (3) aggre-
gate instance features together to obtain slide-level prediction results.
many
advanced mil models emerged in the past few years.
such graph-
transformer architecture has also been introduced into wsi analysis [15,20] to
mine the thorough global and local correlations between diﬀerent image patches.
diﬀerent resolution levels in the wsi pyramids contain diﬀerent and comple-
mentary information [3].
the images at a high-resolution level contain cellular-
level information, such as the nucleus and chromatin morphology features
[10].
at a low-resolution level, tissue-related information like the extent of tumor-
immune localization can be found [1], while the whole wsi describes the entire
tissue microenvironment, such as intra-tumoral heterogeneity and tumor inva-
sion
speciﬁ-
cally, we abstract the multi-resolution wsi pyramid as a heterogeneous hierar-
chical graph and devise a hierarchical interaction graph-transformer architec-
ture to learn both short-range and long-range correlations among diﬀerent image
patches within diﬀerent resolutions.
considering that the information from dif-
ferent resolutions is complementary and can beneﬁt each other, we specially
design a bidirectional interaction block in our hierarchical interaction vit mod-
hierarchical interaction graph-transformer
757
fig.
to reduce the tremendous computation and memory
cost, we further adopt the eﬃcient pooling operation after the hierarchical gnn
part to reduce the number of tokens and introduce the separable self-attention
mechanism in hierarchical interaction vit modules to reduce the computation
burden.
the extensive experiments with promising results on two public wsi
datasets from tcga projects, i.e., kidney carcinoma (kica) and esophageal
carcinoma (esca), validate the eﬀectiveness and eﬃciency of our framework
on both tumor subtyping and staging tasks.
1, a wsi is cropped into numerous non-overlapping 512 ×
512 image patches under diﬀerent magniﬁcations (i.e., ×5, ×10) by using a
sliding window strategy, where the otsu algorithm
[16] to
extract the feature embedding of each image patch.
n is the total number of the
region nodes and m is the number of patch nodes belonging to a certain region
node, and c denotes the dimension of feature embedding (1,024 in our experi-
ments).
2.2
hierarchical graph neural network
to learn the short-range relationship among diﬀerent patches within the wsi
pyramid, we propose a new hierarchical graph message propagation operation,
called raconv+.
therefore, the layer-wise
graph message propagation can be represented as:
h(l+1) = σ

a · h(l) · w (l)
,
(3)
where a represents the attention score matrix, and the attention score for the
j-th row and j′-th column of the matrix is given by eq.
note that here we introduced ssa into the pl block to reduce the
computation complexity of attention calculation from quadratic to linear while
maintaining the performance [13].
2.4
slide-level prediction
in the ﬁnal stage of our framework, we design a fusion block to combine the
coarse-grained and ﬁne-grained features learned from the wsi pyramids.
specif-
ically, we use an element-wise summation operation to fuse the coarse-grained
thumbnail feature and patch-level features from the hierarchical interaction
gnn part, and then further fuse the ﬁne-grained patch-level features from the
hivit part with a concatenation operation.
3
experiments
datasets and evaluation metrics.
the kica dataset consists of 371 cases of kidney carcinoma,
of which 279 are classiﬁed as early-stage and 92 as late-stage.
the esca dataset comprises 161 cases of esophageal car-
cinoma, with 96 cases classiﬁed as early-stage and 65 as late-stage.
hierarchical interaction graph-transformer
761
experimental
setup.
the
proposed
framework
was
implemented
by
pytorch
all experiments were conducted on a
workstation with eight nvidia geforce rtx 3090 (24 gb) gpus.
even for the non-hierarchical graph-transformer baseline
la-mil and hierarchical transformer model hipt, our model approaches at
least around 3% and 2% improvement on auc and acc in the classiﬁcation of
the staging of the kica dataset.
we analyze the computation cost during the
experiments to compare the eﬃciency between our methods and existing state-of-
the-art approaches.
besides we visualized the model size (mb) and the training
memory allocation of gpu (gb) v.s. performance in kica’s typing and staging
task plots in fig.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_66.pdf:
contrastive learning has gained popularity due to its robust-
ness with good feature representation performance.
however, cosine
distance, the commonly used similarity metric in contrastive learning,
is not well suited to represent the distance between two data points,
especially on a nonlinear feature manifold.
inspired by manifold learn-
ing, we propose a novel extension of contrastive learning that leverages
geodesic distance between features as a similarity metric for histopathol-
ogy whole slide image classiﬁcation.
to reduce the computational over-
head in manifold learning, we propose geodesic-distance-based feature
clustering for eﬃcient contrastive loss evaluation using prototypes with-
out time-consuming pairwise feature similarity comparison.
the eﬃcacy
of the proposed method is evaluated on two real-world histopathology
image datasets.
keywords: contrastive learning · manifold learning · geodesic
distance · histopathology image classiﬁcation · multiple instance
learning
1
introduction
whole slide image (wsi) classiﬁcation is a crucial process to diagnose diseases
in digital pathology.
although mil does
not require perfect per-patch label assignment, it is important to construct good
feature vectors that are easily separated into diﬀerent classes to make the clas-
siﬁcation more accurate.
1. comparison of geodesic and cosine distance in n-dimensional space.
[3] introduced the utilization of
data augmentation and a learnable nonlinear transformation between the feature
embedding and the contrastive loss to generally improve the quality of feature
embedding.
[6] employed a dynamic dictionary along with a momentum
encoder in the contrastive learning model to serve as an alternative to the super-
vised pre-trained imagenet model in various computer vision tasks.
[5] integrated the k-means clustering and contrastive learning model by
introducing prototypes as latent variables and assigning each sample to multi-
ple prototypes to learn the hierarchical semantic structure of the dataset.
these
prior works used cosine distance as their distance measurement, which computes
the angle between two feature vectors as shown in fig.
although cosine dis-
tance is a commonly used distance metric in contrastive learning, we observed
that the cosine distance approximates the diﬀerence between local neighbors and
is insuﬃcient to represent the distance between far-away points on a complicated,
nonlinear manifold.
the main motivation of this work is to extend the current contrastive learn-
ing to represent the nonlinear feature manifold inspired by manifold learning.
owing to the manifold distribution hypothesis [8], the relative distance between
high-dimensional data is preserved on a low-dimensional manifold.
isomap [12]
is a well-known manifold learning approach that represents the manifold struc-
ture by using geodesic distance (i.e., the shortest path length between points on
the manifold).
there are several previous works that use manifold learning for
image classiﬁcation and reconstruction tasks, such as lu et al.
however, the use of geodesic distance on the feature manifold for image
classiﬁcation is a recent development.
[2] applied the random walk
algorithm on the nearest neighbor graph to compute the pairwise geodesic dis-
tance and proposed the n-pair loss to maximize the similarity between samples
from the same class for image retrieval and clustering applications.
[4]
employed the geodesic distance computed using the dijkstra algorithm on the k-
nearest neighbor graph to measure the correlation between the original samples
deep manifold contrastive learning
685
and then further divided each class into sub-classes to deal with the problems of
high spectral dimension and channel redundancy in the hyperspectral images.
however, this method captured the nonlinear data manifold structure on the
original data (not on the feature vectors) only once at the beginning stage,
which is not updated in the further training process.
in this study, we propose a hybrid method that combines manifold learn-
ing and contrastive learning to generate a good feature extractor (encoder) for
histopathology image classiﬁcation.
our method uses the sub-classes and proto-
types as in conventional contrastive learning, but we propose the use of geodesic
distance in generating the sub-classes to represent the non-linear feature mani-
fold more accurately.
by doing this, we achieve better separation between fea-
tures with large margins, resulting in improved mil classiﬁcation performance.
the main contributions of our work can be summarized as follows:
– we introduce a novel integration of manifold geodesic distance in contrastive
learning, which results in better feature representation for the non-linear fea-
ture manifold.
– we propose a geodesic-distance-based feature clustering for eﬃcient con-
trastive loss evaluation using prototypes without brute-force pairwise fea-
ture similarity comparison while approximating the overall manifold geometry
well, which results in reduced computation.
– we demonstrate that the proposed method outperforms other state-of-the-
art (sota) methods with a much smaller number of sub-classes without
complicated prototype assignment (e.g., hierarchical clustering).
to the best of our knowledge, this work is the ﬁrst attempt to leverage manifold
geodesic distance in contrastive learning for histopathology wsi classiﬁcation.
2. it is composed of two
stages: (1) train the feature extractor using deep manifold embedding learning
and (2) train the wsi classiﬁer using the deep manifold embedding extracted
from the ﬁrst stage.
the input wsis are pre-processed to extract 256 × 256 × 3
dimensional patches from the tumor area at a 10× magniﬁcation level.
patches
with less than 50% tissue coverage are excluded from the experiment.
2.1
deep manifold embedding learning
as illustrated in fig.
the output is then passed through two diﬀerent paths, namely, deep manifold
and softmax paths.
2. overview of our proposed method, which is composed of two stages: (a) deep
manifold embedding learning and (b) mil classiﬁcation.
deep manifold.
in this stage, the patches from each class are further grouped
into sub-classes based on manifold geodesic distance.
each node (patch feature) is connecting to its k-nearest
neighbors (knn) based on the weighted edges computed with euclidean dis-
tance, given that the neighbor samples on the manifold should have a higher
potential to be in the same sub-class.
the geodesic distance matrix m on the
manifold is then computed between each sample pair by using dijkstra’s algo-
rithm based on the gc.
for the deep manifold training, we adopted two losses: (1)
intra-subclass loss lintra and (2) inter-subclass loss linter.
lintra is formulated as follows:
lintra =
1
j · i
j

j=1
i

i=1
(f(xi
j) − p+)t (f(xi
j) − p+)
(1)
deep manifold contrastive learning
687
where xi
j is the i-th patch in the j-th batch, j represents the total number of
batches, i represents the total number of patches per batch, f(·) is the feature
extractor, and p+ indicates the positive prototype of the patch (i.e., the proto-
type of the subclass containing xi
j).
linter = 1
j
j

j=1
(△ − d(f(qa
j ), p b))
(2)
d(y, z) = max{sup
y∈y
d(y, z), sup
z∈z
d(z, y )}
(3)
where f(qa
j ) is a set of patch features in batch j from class a, p b is a set of
prototypes from the sub-classes of class b, and △ is a positive margin between
classes on data manifold.
then, the manifold loss
is formulated as
lmanifold = lintra + linter
(4)
another path via softmax is simply trained on outputs from the feature
extractor with the ground truth slide-level labels y by the cross-entropy loss
lce, which is deﬁned as follows:
finally, the total loss for the ﬁrst stage is deﬁned as follows:
ltotal = lmanifold + lce
(6)
2.2
mil classiﬁcation
as illustrated in fig.
2(b), in the second stage, the pre-trained feature extractor
from the previous stage is then deployed to extract features for bag generation.
a total of 50 bags are generated for each wsi, in which each bag is composed
of the concatenation of the features from 100 patches in 512 dimensions.
we collected 121
wsis for the training set, and the remaining wsis were used as the testing set.
3.2
implementation detail
we used a pre-trained vgg16 with imagenet as the initial encoder, which was
further modiﬁed via deep manifold model training using the proposed manifold
and cross-entropy loss functions.
in the deep manifold
embedding learning model, the learning rates were set to 1e-4 with a decay rate
of 1e-6 for the ihccs subtype classiﬁcation and to 1e-5 with a decay rate of
1e-8 for the liver cancer type classiﬁcation.
we used
batch sizes 64 and 4 for training the deep manifold embedding learning model and
the mil classiﬁcation model, respectively.
the number of epochs for the deep
manifold embedding learning model was 50, while 50 and 200 epochs for the
ihccs subtype classiﬁcation and liver cancer type classiﬁcation, respectively.
as for the optimizer, we used stochastic gradient decay for both stages.
the
result shown in the tables is the average result from 10 iterations of the mil
classiﬁcation model.
3.3
experimental results
the performance of diﬀerent models from two diﬀerent datasets is reported in
this section.
the mil classiﬁcation result of the ihccs subtype
deep manifold contrastive learning
689
classiﬁcation is shown in table 1.
our proposed method outperformed the base-
line cnn by about 4% increment in accuracy, precision, recall, and f1 score.
table 1. classiﬁcation performance on ihccs subtype and liver cancer type dataset.
f1
cnn
na
0.7315
0.7372
0.7315
0.7270
0.7710
0.7781
0.7719
0.7657
pcl
500-800-1000
0.7386
0.7478
0.7394
0.7354
0.8146
0.7898
0.8146
0.7979
hcsc
2-10-100
0.7230
0.7265
0.7230
0.7231
0.7995
0.8524 0.7995
0.7825
ours
20
0.7703 0.7710 0.7678 0.7668 0.8239 0.8351
0.8239 0.8227
table 2. ablation study of prototype assignment strategies.
our
method achieved about 5% improvement in accuracy against the baseline and
1% to 2% improvement in accuracy against the sota methods.
moreover, it
outperformed the sota methods with far fewer prototypes and without com-
plicated hierarchical prototype assignments.
to further evaluate the eﬀect of
prototypes, we conducted an ablation study for diﬀerent prototype assignment
strategies as shown in table 2.
when both are used together, it implies a hierarchi-
cal prototype assignment where local prototypes interact with the corresponding
global prototype.
meanwhile, the combination of both prototypes achieved a similar performance
to that of the model with local prototypes only.
since the hierarchical (global +
local) assignment did not show a signiﬁcant improvement but instead increased
computation, we used only local sub-class prototypes in our ﬁnal experiment
setting.
jeong
table 3. classiﬁcation performance of geodesic distance and cosine distance.
since one of our contributions is the use of geodesic distance, we assessed the
eﬃcacy of the method by comparing it with the performance using cosine dis-
tance, as shown in table 3. to measure the performance of the cosine-distance-
based method, we simply replaced our proposed manifold loss with nt-xent
loss [3], which uses cosine distance in their feature similarity measurement.
two
cosine distance experiments were conducted as follows: (1) use only their ground-
truth class without further dividing the samples into sub-classes (i.e., global pro-
totypes) and (2) divide the samples from each class into 10 sub-classes by using
k-means clustering (i.e., local prototypes).
as shown in table 3, using multiple
local prototypes shows slightly better performance compared to using global pro-
totypes.
by switching the nt-xent loss with our geodesic-based manifold loss,
the overall performance is increased by about 2%.
red dots represent sdt samples and
blue dots represent ldt samples from the ihccs dataset (corresponding histol-
ogy thumbnail images are shown on the right).
it is clearly shown
that geodesic distance can correctly measure the feature distance (similarity) on
the manifold so that sdt and ldt groups are located far away in the t-sne
plot, whereas cosine distance failed to separate these groups and they are located
nearby in the plot.
deep manifold contrastive learning
691
3.4
conclusion and future work
in this paper, we proposed a novel geodesic-distance-based contrastive learn-
ing for histopathology image classiﬁcation.
unlike conventional cosine-distance-
based contrastive learning methods, our method can represent nonlinear feature
manifold better and generate better discriminative features.
in the future, we
plan to optimize the algorithm and apply our method to other datasets and
tasks, such as multi-class classiﬁcation problems and natural image datasets.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_72.pdf:
the major limitation of previous learning frame-
works for whole slide image (wsi) based survival prediction is that the
contextual interactions of pathological components (e.g., tumor, stroma,
lymphocyte, etc.)
speciﬁcally, we ﬁrst uti-
lize a multi-scope analysis strategy, which leverages an in-slide superpixel
and a cross-slide clustering, to mine the spatial and semantic priors of
wsis.
furthermore, based on the extracted spatial prior, a hierarchical
graph convolutional network is proposed to progressively learn the topo-
logical features of the variant microenvironments ranging from patch-
level to tissue-level.
in addition, guided by the identiﬁed semantic prior,
tissue-level features are further aggregated to represent the meaningful
pathological components, whose contextual interactions are established
and quantiﬁed by the designed transformer-based prediction head.
experimental results demonstrate that our proposed
method yields superior performance and richer interpretability compared
to the state-of-the-art approaches.
keywords: whole slide image · survival prediction · contextual
interaction · graph neural network · transformer
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. (eds.): miccai 2023, lncs 14225, pp.
https://doi.org/10.1007/978-3-031-43987-2_72
746
w. hou et al.
1
introduction
the ability to predict the future risk of patients with cancer can signiﬁcantly
assist clinical management decisions, such as treatment and monitoring [21].
generally, pathologists need to manually assess the pathological images obtained
by whole-slide scanning systems for clinical decision-making, e.g., cancer diagno-
sis and prognosis
however, due to the complex morphology and structure
of human tissues and the continuum of histologic features phenotyped across
the diagnostic spectrum, it is a tedious and time-consuming task to manually
assess the whole slide image (wsi) [12].
therefore, automated wsi analysis method for survival prediction
task is highly demanded yet challenging in clinical practice.
over the years, deep learning has greatly promoted the development of
computational pathology, including wsi analysis [9,17,24].
due to the huge
size, wsis are generally cropped to numerous patches with a ﬁxed size and
encoded to patch features by a cnn encoder (e.g., imagenet pretrained
resnet50
first, to mine more comprehensive and in-depth
attribute priors of wsi, we propose a multi-scope analysis strategy consisting
of in-slide superpixels and cross-slide clustering, which can not only extract the
spatial prior but also identify the semantic prior of wsis.
speciﬁcally, based on the extracted
spatial topology, the hierarchical graph convolution layer in hgt progressively
aggregate the patch-level features to the tissue-level features, so as to learn
the topological features of variant microenvironments ranging from ﬁne-grained
(e.g., cell) to coarse-grained (e.g., necrosis, epithelium, etc.).
then, under the
guidance of the identiﬁed semantic prior, the tissue-level features are further
sorted and assigned to form the feature embedding of pathological components.
extensive experiments on three cancer
cohorts (i.e., crc, tcga-lihc and tcga-kirc) demonstrates the eﬀective-
ness and interpretability of our framework.
due to the huge size,
wsis are generally cropped to numerous patches with a ﬁxed size (i.e., 256×256)
and encoded to patch features vpatch ∈ rn×d in the embedding space d by a
cnn encoder (i.e., imagenet pretrained resnet50
[11]) for further analysis,
where n is the number of patches, d = 1024 is the feature dimension.
the goal
of wsi-based cancer survival prediction is to learn the feature embedding of v
in a supervised manner and output the survival risk o ∈ r1.
however, conventional patch-level analysis cannot model complex pathologi-
cal patterns (e.g., tumor lymphocyte inﬁltration, immune cell composition, etc.),
resulting in limited cancer survival prediction performance.
to this end, we pro-
posed a novel learning network, i.e., hgt, which utilized the spatial and seman-
tic priors mined by a multi-scope analysis strategy (i.e., in-slide superpixel and
cross-slide clustering) to represent and capture the contextual interaction of
pathological components.
however, the conventional patch-level anal-
ysis is diﬃcult to meet this requirement.
intuitively, the
cropped patches and segmented tissues in a wsi can be considered as hierar-
chical entities ranging from ﬁne-grained level (e.g., cell) to coarse-grained level
(e.g., necrosis, epithelium, etc.).
then, the patches in each superpixel are further connected in an
8-adjacent manner, thus generating patch adjacency matrix epatch ∈ rn×n.
the
spatial assignment matrix between cropped patches and segmented tissues is
denoted as aspa ∈ rn×m.
based on the spatial topology extracted
by in-slide superpixel, the patch graph convolutional layer (patch gcl) is
designed to learn the feature of the ﬁne-grained microenvironment (e.g., cell)
through the message passing between adjacent patches, which can be represented
as:
v
′
patch = σ(graphconv(vpatch, epatch)),
(1)
where σ(·) denotes the activation function, such as relu.
graphconv denotes
the graph convolutional operation, e.g., gcnconv [15], graphsage
third, based on the spatial assignment
matrix aspa, the learned patch-level features can be aggregated to the tissue-level
hierarchical graph transformer
749
features which contain the information of necrosis, epithelium, etc.
the tissue graph convolutional
layer (tissue gcl) is further designed to learn the feature of this coarse-grained
microenvironment, which can be represented as:
v
′
tissue = σ(graphconv(vtissue, etissue)).
however, existing analysis frameworks for
wsi often ignore the capture of contextual interactions of pathological compo-
nents (e.g., tumor, stroma, lymphocyte, etc.), resulting in limited performance
and interpretability.
the semantic assignment matrix between segmented tissues
and pathological components is denoted as asem ∈ rm×k.
under the guidance of the semantic prior iden-
tiﬁed by cross-slide clustering, the learned tissue features v
′
tissue can be further
aggregated, forming a series meaningful component embeddings p ′ speciﬁc to
the cancer type.
⎞
⎠ ,
(6)
where δi denote the censorship of i-th patient, o(i) and o(j) denote the survival
output of i-th and j-th patient in a batch, respectively.
3
experiments
3.1
experimental settings
dataset.
the average patch number of each wsi is
18727, 3680, 3742 for crc, tcga-lihc, tcga-kirc, respectively.
implementation details.
our graph convolutional model is implemented by pytorch geometric [7].
the number of transformer heads is 8, and the attention scores of all heads
are averaged to produce the heatmap of contextual interactions.
ci
ranges from 0 to 1, where a larger ci indicates better performance.
in this study, we conduct a 5-fold evaluation procedure with 5 runs
to evaluate the survival prediction performance for each method.
for fair comparison, same cnn
extractor (i.e. imagenet pretrained resnet50
generally, most mil methods, i.e., deepsets, abmil, dsmil,
transmil mainly focus on a few key instances for prediction, but they do
not have signiﬁcant advantages in cancer prognosis.
deepattnmisl has a certain semantic
perception ability for patch, which achieves better performance in lihc cohort.
patchgcn is capable to capture the local contextual interactions between patch,
which also achieves satisﬁed performance in kirc cohort.
experimental results of ci. results not signiﬁcantly worse than the best
(p-value > 0.05, two-sample t-test) are shown in bold.
the trained classiﬁcation model can be used to determine the biological
semantics of the pathological components extracted by our model with a major
voting rule.
figure 3 shows the original image, spatial topology, proportion and
fig.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_57.pdf:
nucleus segmentation is usually the ﬁrst step in pathologi-
cal image analysis tasks.
generalizable nucleus segmentation refers to the
problem of training a segmentation model that is robust to domain gaps
between the source and target domains.
the domain gaps are usually
believed to be caused by the varied image acquisition conditions, e.g.,
diﬀerent scanners, tissues, or staining protocols.
first, we introduce a re-coloring method that relieves dra-
matic image color variations between diﬀerent domains.
we evaluate the proposed methods on
two h&e stained image datasets, named consep and cpm17, and two
ihc stained image datasets, called deepliif and bc-deepliif.
exten-
sive experimental results justify the eﬀectiveness of our proposed darc
model.
keywords: domain generalization · nucleus segmentation · instance
normalization
1
introduction
automatic nucleus segmentation has captured wide research interests in recent
years due to its importance in pathological image analysis [1–4].
however, as
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43987-2 57.
1. example image patches from diﬀerent datasets.
their appearance diﬀers sig-
niﬁcantly from each other due to variations in image modalities, staining protocols,
scanner types, and tissues.
shown in fig.
1, the variations in image modalities, staining protocols, scanner
types, and tissues signiﬁcantly aﬀect the appearance of nucleus images, result-
ing in notable gap between source and target domains [5–7].
therefore, it is highly desirable to train a robust
nucleus segmentation model that is generalizable to diﬀerent domains.
[12,13] and they can be roughly grouped into data augmentation-, representation
learning-, and optimization-based methods.
it is a consensus that a generalizable nucleus segmentation model should be
robust to image appearance variation caused by the change in staining proto-
cols, scanner types, and tissues, as illustrated in fig.
it performs both semantic segmentation and contour detection for nucleus
darc: distribution-aware re-coloring model
593
fig.
the whole model is trained in an end-to-end
manner.
darc ﬁrst re-colors each image to relieve the impact caused by diﬀerent
image acquisition conditions.
the re-colored image is then fed into the u-net encoder
and the ratio prediction head.
then, the re-colored image is fed into darc again with ρ for ﬁnal prediction.
the area of each nucleus instance is obtained via subtraction between
the segmentation and contour prediction maps [1].
details of the baseline model
is provided in the supplementary material.
darc ﬁrst
re-colors each image to relieve the inﬂuence caused by image acquisition condi-
tions.
the re-colored image is then fed into the u-net encoder and the ratio pre-
diction head.
with the predicted ratio, the dain layers can estimate feature
statistics more robustly and facilitate more accurate nucleus segmentation.
2.2
nucleus image re-coloring
we propose the re-coloring (rc) method to overcome the color change in dif-
ferent domains.
speciﬁcally, given a rgb image i, e.g., an h&e or ihc stained
image, we ﬁrst obtain its grayscale image ig.
in this way, we obtain an initial re-colored
image ir.
however, de-colorization results in the loss of ﬁne-grained textures and may
harm the segmentation accuracy.
to handle this problem, we compensate ir
with the original semantic information contained in i. recent works
[40] show
that semantic information can be reﬂected via the order of pixels according
to their gray value.
[41] to
combine the semantic information in i with the color values in ir.
594
s. chen et al.
algorithm 1. re-coloring
input:
the input rgb image
rh×w ×3;
the module t whose input and output channel numbers are 1 and 3, respectively;
output:
the re-colored image io ∈
rh×w ×3;
1: de-colorizing i to obtain ig;
2: ir ← t(ig)
3: reshaping i and ir to rhw ×3
4: sortindex ← argsort(argsort(i))
5: sortv alue ← sort(ir)
6: io ← assignv alue(sortindex, sortv alue)
7: return io
table 1. evaluation on the impact of foreground-background ratio to model perfor-
mance.
details of the module t are included in the supplementary material.
in this way, the re-colored image is advantageous in two aspects.
first,
the appearance diﬀerence between pathological images caused by the change in
scanners and staining protocols is eliminated.
second, the re-colored image pre-
serves ﬁne-grained structure information, enabling precise instance segmentation
to be possible.
however, for
dense-prediction tasks like semantic segmentation or contour detection, adopt-
ing in alone cannot fully address the feature statistics variation problem.
speciﬁcally, an image with more nucleus instances
produces more responses in feature maps and thus higher feature statistic val-
ues, and vice versa.
the diﬀerence in this ratio causes interference to nucleus
segmentation.
the c-dimensional feature vector on its
pixel (i, j) is denoted as xij;
the modules eμ and eδ that re-estimate feature statistics;
δsr a ∈ r1×1×c that is obtained via running mean of δs in the training stage;
the momentum factor α used to update δsr a ;
(optional) δs = f(ρ);
output:
normalized feature maps y ∈ rh×w ×c;
1: μ ←
1
hw
h

i=1
w

j=1
xij
2: δ2 ←
1
hw
h

i=1
w

j=1
(xij − μ)2
3: if δs is given then
4:
// using δs to re-estimate feature statistics for ﬁnal segmentation
5:
μ′, δ′ ← eμ (μ, δ, δs), eδ (μ, δ, δs)
6:
if training then
7:
// updating δsr a during training
8:
δsr a ← (1 − α)δsr a + αδs
9:
end if
10: else
11:
// using δsr a to re-estimate feature statistics for ratio prediction
12:
μ′, δ′ ← eμ (μ, δ, δsr a ), eδ (μ, δ, δsr a )
13: end if
14: y ← (x − μ′)/δ′
15: return y
to verify the above viewpoint, we evaluate the baseline model under diﬀerent
foreground-background ratios.
speciﬁcally, we ﬁrst remove the foreground pixels
via in-painting [27], and then pad the original testing images with the obtained
background patches.
we adopt b to denote the ratio between the size of the
obtained new image and the original image size.
compared with the original
images, the new images have the same foreground regions but more background
pixels, and thus have diﬀerent foreground-background ratios.
finally, we evaluate
the performance of the baseline model with diﬀerent b values.
experimental
results are presented in table 1.
it is shown that the value of b aﬀects the model
performance signiﬁcantly.
the above problem is common in nucleus segmentation because pathologi-
cal images from diﬀerent organs or tissues tend to have signiﬁcantly diﬀerent
foreground-background ratios.
however, this phenomenon is often ignored in
existing research.
the structures of eμ and eδ are included in the
supplemental materials.
596
s. chen et al.
as shown in fig.
2, to obtain the foreground-background ratio ρ of one input
image, we ﬁrst feed it to the model encoder with δsra as the additional input.
δsra acts as pseudo residuals of feature statistics and is obtained in the training
stage via averaging δs in a momentum fashion.
here, f is a 1 × 1 convolutional layer that transforms ρ
to a feature vector whose dimension is the same as the target layer’s channel
number.
after that, the input image is fed into the model again with δs as
additional input and ﬁnally makes more accurate predictions.
the training of rph requires an extra loss term lrph, which is formulated
as bellow:
lrph = lbce(ρ, ρg) + lmse(f(ρ), f(ρg)),
(1)
where ρg denotes the ground truth foreground-background ratio, and lbce
and lmse denote the binary cross entropy loss and the mean squared error,
respectively.
3
experiments
3.1
datasets
the proposed method is evaluated on four datasets, including two h&e stained
image datasets
consep [3] contains 28 training and
14 validation images, whose sizes are 1000×1000 pixels.
the images are extracted
from 16 colorectal adenocarcinoma wsis, each of which belongs to an individual
patient, and scanned with an omnyx vl120 scanner within the department of
pathology at university hospitals coventry and warwickshire, uk. cpm17
[28] contains 32 training and 32 validation images, whose sizes are 500 ×
the images are selected from a set of glioblastoma multiforme, lower
grade glioma, head and neck squamous cell carcinoma, and non-small cell
lung cancer whole slide tissue images.
[29] contains 575 training and
91 validation images, whose sizes are 512 × 512 pixels.
the images are extracted
from the slides of lung and bladder tissues.
[29,32] contains 385
training and 66 validation ki67 stained images of breast carcinoma, whose sizes
are 512 × 512 pixels.
3.2
implementation details
in the training stage, patches of size 224×224 pixels are randomly cropped from
the original samples.
we adopt the standard augmentation, like image color jittering and
gaussian blurring.
in all experiments, the segmentation and contour detection
predictions are penalized using the binary cross entropy loss.
darc: distribution-aware re-coloring model
597
table 2. comparisons in generalization performance on nucleus segmentation datasets.
results in each column are related to models trained on one domain and evaluated on
the other three unseen domains.
results are in percentages.
methods
consep
cpm17
deepliif
bc-deepliif average
aji
dice
aji
dice
aji
dice
aji
dice
aji
dice
baseline (bn)
16.67 24.10 33.30 61.18 08.42 38.17 21.27 39.92
19.92
40.84
baseline (in)
32.13 48.67 33.94 65.83 41.48 67.17 21.52 37.49
32.27
54.79
bin
models
#parameters (m)
inference time (s/image)
baseline (in)
5.03
0.0164
darcenc
5.47
0.0253
3.3
experimental results and analyses
in this paper, the models are compared using the aji
in
the experiments, models trained on one of the datasets will be evaluated on the
three unseen ones.
to avoid the inﬂuence of the diﬀerent sample numbers of the
datasets, we calculate the average scores within each unseen domain respectively
and then average them across domains.
in this paper, we re-implement some existing popular domain generalization
algorithms for comparisons under the same training conditions.
speciﬁcally, we
re-implement the tent [34], bin [19], dsu [20], frequency amplitude nor-
malization (ampnorm)
[38] and stain mix-up [39] methods that are popular
in pathological image analysis.
their performances are presented in table 2.
the normalization layers in the encoder with dain and uses bn in its decoder.
as shown in table 2, darcenc achieves the best average performance among all
methods.
speciﬁcally, darcenc improves the baseline model’s average aji and
dice scores by 4.81% and 7.04%.
compared with the other domain generalization
methods, dain, dain w/o ratio, darcall and darcenc achieve impressive
performances on bc-deepliif, which justify that re-estimating the instance-
wise statistics is important for improving the domain generalization ability of
models trained on bc-deepliif.
compared
with the baseline model, rc improves the average aji and dice scores by 1.41%
and 2.59%, and dain improves the average aji and dice scores by 1.13% and
4.08%.
compared with the variant model without foreground-background ratio
prediction, dain improves the average aji and dice scores by 0.90% and 3.74%.
finally, the combinations of rc and dain, i.e., darcall and darcenc, achieve
the best average scores.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_56.pdf:
however,
two major challenges hinder accurate nuclei segmentation from thyroid
cytology.
firstly, unbalanced distribution of nuclei morphology across
diﬀerent tbsrtc categories can lead to a biased model.
secondly,
the insuﬃciency of densely annotated images results in a less gener-
alized model.
in contrast, image-wise tbsrtc labels, while contain-
ing lightweight information, can be deeply explored for segmentation
guidance.
to this end, we propose a tbsrtc-category aware nuclei
segmentation framework (tcsegnet).
to top up the small amount of
pixel-wise annotations and eliminate the category preference, a larger
amount of image-wise labels are taken in as the complementary supervi-
sion signal in tcsegnet.
this integration of data can eﬀectively guide
the pixel-wise nuclei segmentation task with a latent global context.
we also propose a semi-supervised extension of tcsegnet that lever-
ages images with only tbsrtc-category labels.
our tcsegnet outperforms state-of-the-art segmentation
approaches with an improvement of 2.0% dice and 2.7% iou; besides,
the semi-supervised extension can further boost this margin.
in conclu-
sion, our study explores the weak annotations by constructing an image-
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
https://doi.org/10.1007/978-3-031-43987-2_56
tcseqnet
581
wise-label-guided nuclei segmentation framework, which has the poten-
tial medical importance to assist thyroid abnormality examination.
keywords: unbalanced nuclei segmentation · semi-supervised
learning · thyroid cytology · tbsrtc diagnostic category
1
introduction
thyroid cancer is the most common cancer of the endocrine system, account-
ing for 2.1% of all malignant cancers [1].
the emergence of computational pathology allows automatic diagno-
sis of thyroid cancer, and nuclei segmentation becomes one of the most critical
diagnostic tasks [4,5], as the shapes of nuclei, whether round, oval, or elongated,
can provide valuable information for further analysis [6].
for example, small and
scattered thyroid cells with a light hue and relatively low cell density are usually
low-grade and indicative of early-stage cancer; whereas large and dark cells with
extreme-dense agglomeration are usually middle- or late-grade [3]. correspond-
ingly, accurate location of cell boundaries is essential for both pathologists and
computer-aided diagnosis (cad) systems to assist decision [7].
however, nuclei segmentation in thyroid cytopathology is still challenged
by the varying cellularity of images from diﬀerent tbsrtc categories [3,8].
for example, benign cells (i & ii) present high sparsity and are diﬃcult to be
distinguished from background tissues, thus may account for a relatively small
proportion when equal images are involved in a training set [3].
in this way, an unbalanced distribution
across diﬀerent categories resulted, correspondingly, the training leads to biased
models with lower accuracy
such distinct morphological diﬀerences can
be characterized by the tbsrtc category, which thus inspires us to utilize
the handy image-wise grading labels to guide the nuclei segmentation model
learning from unbalanced datasets.
moreover, amongst multiple annotation paradigms [12], pixel-
level labeling is the most time-consuming and laborious, whereas the image-wise
diagnostic labels, i.e. tbsrtc categories, are comparatively simpler.
despite
the labeling intensity, prevalent nuclei segmentation methods, e.g., cia-net
[15], are limited to pixel-wise annotations, where
the potential beneﬁts of integrating accessible image-wise labels are unaware.
to narrow the gap discussed, we propose a novel tbsrtc-category-aware
nuclei segmentation framework.
(1) we pro-
pose a cytopathology nuclei segmentation network named tcsegnet, to pro-
vide supplementary guidance to facilitate the learning of nuclei boundaries.
582
j. zhu et al.
innovatively, our approach can help reduce bias in the learning process of the
segmentation model with the routine unbalanced training set.
(2) we expand
tcsegnet to semi-tcsegnet to leverage image-wise labels in a semi-supervised
learning manner, which signiﬁcantly reduces the reliance on annotation-intensive
pixel-wise labels.
additionally, an hsv-intensity noise is designed speciﬁcally
for cytopathology images to boost the generalization ability.
(3) we establish a
dataset of thyroid cytopathology image patches of 224 × 224, where 4,965 image
labels are provided following tbsrtc, and 1,473 of them are densely anno-
tated [3] (to be on github upon acceptance).
to the best of our knowledge,
it is the ﬁrst publicized thyroid cytopathology dataset of both image-wise and
pixel-wise labels.
tcsegnet uti-
lizes annotation-lightweight image-wise tbsrtc-category labels to aid in the learning
of unbalanced nuclei morphology in segmentation.
we propose a novel tbsrtc-category aware segmentation net-
work (tcsegnet) to segment nuclei boundaries in cytopathology images, which
tcseqnet
583
is guided by tbsrtc-category label to learn from unbalanced data.
considering
the spatial distributions of thyroid cells in cytopathology images, our design
provides extended global information for more accurate segmentation.
formally, the overall segmentation loss lseg to train
our model is a combination of the binary cross-entropy loss (bce), i.e.
lseg = γni · bce(ˆycnn
ni , yni)
this block consists of two learnable fully connected
layers that process the feature extracted by the cnn and transformer branches
separately, which obtains image-wise tbsrtc-category prediction denoted as
ˆycnn
cls and ˆytrans
cls
.
= ce(ˆycnn
cls , ycls) + γcls · ce(ˆytrans
cls
, ycls),
(2)
where ycls is the image-wise tbsrtc-category label, and the balancing coeﬃ-
cient γcls is set to 3, as the global feature captured by the transformer branch
is tightly correlated with the image-level classiﬁcation tag.
to leverage images that only have
image-wise labels, we extend to a semi-supervised mean teacher [18] framework
called semi-tcsegnet.
in this framework, both the student and teacher share the
same full-supervised nuclei segmentation architecture of tcsegnet.
the weights
of the teacher θt are updated with the exponential moving average (ema) of the
weights of student θs, and smoothing coeﬃcient α = 0.99, following the previous
work [19].
formally, the weights of the teacher at e-th epoch are updated by
θe
t = αθe−1
t
+ (1 − α)θe
s.
(4)
584
j. zhu et al.
during the training stage, the teacher model assigns pixel-wise soft labels to
the images with exclusive image-wise labels, thus expanding the scale of labeled
data to the student model.
(3) computed on fully-annotated
data, and the semi-supervised loss lss computed on data with only image-wise
labels.
it follows that the overall training objective in semi-tcsegnet is to min-
imize the loss function l = ls + λss · lss, where lss measures the segmentation
consistency between the teacher and student models via l2-norm.
the traditional method of integrating gaussian noise
in the mean teacher [18] may be problematic when working with cytopathology
images that have an imbalanced color distribution.
speciﬁcally, xv is the pixel value
of the image’s v channel in hsv space, and hyper-parameter λv is set as 0.5
to control the amplitude of the intensity-based noise.
finally, the value of the
obtained noise is clamped to [−0.2, 0.2] before being added to the images.
3
experiments
image dataset.
we construct a clinical thyroid cytopathology dataset with
images of both image-wise and pixel-wise labels as a benchmark (appear in
github upon acceptance)
some representative images are presented in fig.
the dataset comprises 4,965 h&e stained
image patches and labels of tbsrtc, where a subset of 1,473 images was
densely annotated for nuclei boundaries by three experienced cytopathologists
and reached a total number of 31,064 elaborately annotated nuclei.
patient-level
images were partitioned ﬁrst for training and test images, and patch-level cura-
tion was performed.
we divided the dataset with image-wise labels into 80%
training samples and the remaining 20% testing samples.
our collection of thy-
roid cytopathology images was granted with an ethics approval document.
quantitative comparisons in both fully-supervised and semi-supervised man-
ners.
the best performance is highlighted in bold, where we can observe that both
tcsegnet and its semi-supervised extension outperform state-of-the-art.
[15]
0.866
0.775
mtmt-net [27]
0.878
0.789
semi-tcsegnet (ours) 0.889 0.805
implementations.
the proposed method and compared methods are imple-
mented on a single nvidia geforce rtx 3090 gpu card.
both tcsegnet and semi-tcsegnet use sgd optimizer with a
momentum of 0.9 and a weight decay of 10−4.
we set the batch size for tcsegnet to 8, and for semi-tcsegnet
to 10, i.e. 8 fully-annotated images and 2 partially-annotated images per batch.
we compared tcsegnet
with the fully-supervised counterparts, including method speciﬁc for segmenta-
tion in general image [20,22], medical image
intersection over union (iou) and dice score were applied as
the evaluation metrics, where a higher value indicated a better semantic seg-
mentation performance.
3. examples of segmented nuclei in thyroid cytopathology images by tcsegnet
and prevent fully-supervised models methods.
experimental results.
the results in table 1 indicated that tcsegnet can
achieve the highest performance by a dice score of 87.7% and an iou of 78.8%.
the performance values in the challenging regions are highlighted with red boxes
in fig.
our approach is capable
to address the current issue in the recognition and segmentation of small iso-
lated cells graded in the i category, which is always ignored by the unbalanced
pixel-wise cell morphology with other approaches.
also, it yields that the incor-
poration of tbsrtc-category can contribute to a partial alleviation of a biased
model, resulting in more satisfying segmentation performance experimentally.
furthermore, the fact that the tbsrtc-category label is easy to obtain endows
tcseqnet
587
the applicability of our model to various circumstances that nuclei in various
sizes, shapes, and dyeing styles can be accurately recognized and segmented.
moreover, with the semi-supervised
learning, semi-tcsegnet can further boost the performance to an 88.9% dice
score, and 80.5% iou, by leveraging additional data with image-wise tbsrtc-
category labels solely.
the performance improvement of 1.2% dice, 1.7% iou,
together with the general improvement is shown in the boxplot in fig.
4 (c, d), as
a demonstration of the advantage using full data resources with semi-tcsegnet.
fig.
c and d are the distribution of the dice score and iou of all mentioned models
respectively.
our models presented a general improvement across the metrics.
ablation study.
the results indicate that performance improvement is
accumulated with increasing data size.
besides, training with a classiﬁcation-
learning block alone can increase the nuclei segmentation performance by 1.7%
and 2.6% in the dice score and iou, respectively.
meanwhile, trained with
specially designed hsv-intensity noise can also increase the performance by
0.9% dice and 1.4% iou, showing its potential for generation ability improve-
ment.
importantly, the beneﬁts from the two blocks are orthonormal, where
semi-tcsegnet achieves the optimal performance with the utilization of both.
588
j. zhu et al.
table 2. ablation study for our semi-tcsegnet and functional blocks.
classiﬁcation
learning
hsv-intensity
noise
dice
iou
0.867
0.771

0.876
0.785

0.884
0.797


0.889 0.805
w. image-wise data
+1k data
0.879
0.790
+2k data
0.882
0.795
4

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_54.pdf:
due to the huge gigapixel-level size and diverse nature
of whole-slide images (wsis), analyzing them through multiple instance
learning (mil) has become a widely-used scheme, which, however, faces
the challenges that come with the weakly supervised nature of mil. con-
ventional mil methods mostly either utilized instance-level or bag-level
supervision to learn informative representations from wsis for down-
stream tasks.
in this work, we propose a novel mil method for patholog-
ical image analysis with integrated instance-level and bag-level supervi-
sion (termed iib-mil).
extensive experiments demonstrate
that iib-mil outperforms state-of-the-art approaches in both bench-
marking datasets and addressing the challenging practical clinical task.
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43987-2 54.
as the demand for intelligently pathological image analysis continues to grow,
an increasing number of researchers have paid attention to this ﬁeld [12,14,
25].
however, pathological image analysis remains a challenging task due to the
complex and heterogeneous nature
[19] of obtained whole slide images (wsis),
as well as their huge gigapixel-level size [20]. to address this issue, multiple
instance learning (mil)
[1] is usually applied to formulate pathological image
analysis tasks into weakly supervised learning problems.
although mil-based methods have shown impressive
potential in solving a wide range of pathological image analysis tasks including
cancer grading and subtype diagnosis [23], prognosis prediction [18], genotype-
related tasks such as gene mutation prediction [4], etc., it is still an open question
regarding learning an informative and eﬀective representation of the entire wsi
for down-streaming task based on mil architecture.
[9,17], also known as embedding-
based mil, involves converting patches (instances) into low-dimensional embed-
dings, which are then aggregated into wsi (bag)-level representations to conduct
the analysis tasks [22].
the instance-level mil, however, faces the problem of noisy labels,
which is caused by the common strategy of assigning the wsi labels to patches
and the fact that there are lots of patches irrelevant to the wsi labels [3,6].
considering these conventional mil methods usually utilize either bag-level
or instance-level supervision, leading to suboptimal performance.
then we propose to combine bag-level and instance-level supervision
to improve the performance of mil.
the detailed contributions can
be summarized as follows:
1) we propose a novel mil method for pathological image analysis that leverages
a specially-designed residual transformer backbone and organically integrates
both transformer-based bag-level and label-disambiguation-based instance-
level supervision for performance enhancement.
2) we develop a label-disambiguation module that leverages prototypes and con-
ﬁdence bank to tackle the weakly supervised nature of instance-level super-
vision and reduce the impact of assigned noisy labels.
3) the proposed framework outperforms state-of-the-art (sota) methods on
public datasets and in a practical clinical task, demonstrating its superiors
in wsi analysis.
since
bag-level supervision channel is trained to globally summarise information of all
patches for prediction, the bag-level outputs are used as the ﬁnal predictions
during the test stage.
2.2
problem formulation
assume there is a set of n wsis denoted by s = {s1, s2, ..., sn}.
to reduce the
computational cost, we used a frozen pre-trained encoder to transform patches
into k dimensional embeddings {ei,j|ei,j ∈ rk, i ∈
t(·) maps
patch embeddings {ei,j, ...} to a lower-dimensional feature space, denoted as
{xi,j, ...}, where xi,j = t(ei,j), xi,j ∈ rd is the calibrated embedding, t(·) is
composed of transformer layers and skip connections (details are given in the
supplementary.).
the prototypes, denoted as p ∈ rc×d, are initialized with all-zero vectors
and employ momentum-based updates using selected instance features x with
564
q. ren et al.
the highest probability probinst of belonging to their corresponding categories.
conﬁdence b ∈ rn×m×c is initialized with all wsi labels and
uses momentum-based updates with z. detailed steps are summarized as follows:
step 1: obtain the instance classiﬁer output.
then, we use a
momentum-based update rule to obtain pc,t+1:
pc,t+1 = α · pc,t
xi,j, if xi,j ∈ setc,t,
(3)
where α is the momentum coeﬃcient that automatically decreases from α = 0.95
to α = 0.8 across epochs.
zi,j,
(5)
where β is the momentum update parameter with a default value of β = 0.99.
rm×d → rd and a wsi classiﬁer fbag(·) :
rd → rc in turn (architecture details are given in the supplementary.).
3
experiments
3.1
dataset
we evaluate our model with three datasets.
(1) luad-gm dataset: the objec-
tive is to predict the epidermal growth factor receptor (egfr) gene mutations
in patients with lung adenocarcinoma (luad) using 723 whole slide image
(wsi) slices, where 47% of cases have egfr mutations.
3.2
experiment settings
the dataset was randomly split into three parts: training, validation, and testing,
with 60%, 20%, and 20% of the samples, respectively.
the proposed
model was implemented in pytorch, trained on a 32gb tesla v100 gpu,
using adamw
the performance of iib-mil compared with other sota methods.
= 1
83.19
71.23
98.05
91.39
99.05
93.12
λ = 5
85.62
78.77
98.11
90.91
99.57
95.24
λ = 10
85.60
76.03
96.51
89.47
99.23
89.95
4
results and discussion
4.1
comparison with state-of-the art methods
table 1 presents a performance comparative analysis of iib-mil in relation to
other sota methods, including abmil
we can also ﬁnd iib-mil outperformed other sota methods, in the
three tasks with at least 1.78%, 0.74%, and 0.56% performance enhancement
(auc), respectively.
4.2
ablation studies
we conducted ablation studies to assess the eﬃcacy of each component in iib-
mil.
we also investigated the
impact of the warm-up epoch number and found that selecting an appropriate
value, such as warmup = 10, can lead to better model performance.
further-
more, we examined the impact of the weighting factor λ, and the outcomes indi-
cated that assigning greater importance to instance-level supervision (λ = 5)
helps iib-mil enhance its performance, thus demonstrating the eﬀectiveness of
the designed label-disambiguation-based instance-level supervision.
4.3
model interpretation
figure 2(a) shows the t-sne plot of the obtained patch features from the back-
bone of the iib-mil.
the numbers displayed within
each group represent the average likelihood of the egfr mutation predicted
by the patches.
the
resulting heatmap shows the decision mechanism of iib-mil in the accurate
distinguishment between egfr mutation-positive and negative samples.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_68.pdf:
we introduce a new ai-ready computational pathology
dataset containing restained and co-registered digitized images from
eight head-and-neck squamous cell carcinoma patients.
as opposed to subjec-
tive and error-prone immune cell annotations from individual pathol-
ogists (disagreement > 50%) to drive sota deep learning approaches,
this dataset provides objective immune and tumor cell annotations via
mif/mihc restaining for more reproducible and accurate characteri-
zation of tumor immune microenvironment (e.g. for immunotherapy).
we demonstrate the eﬀectiveness of this dataset in three use cases: (1)
ihc quantiﬁcation of cd3/cd8 tumor-inﬁltrating lymphocytes via style
transfer, (2) virtual translation of cheap mihc stains to more expensive
mif stains, and (3) virtual tumor/immune cellular phenotyping on stan-
dard hematoxylin images.
keywords: multiplex immuoﬂuorescence · multiplex
immunohistochemistry · tumor microenvironment · virtual
stain-to-stain translation
p. ghahremani, j. marino, c. h. chung, and s. nadeem—equal contribution.
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43987-2 68.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
https://doi.org/10.1007/978-3-031-43987-2_68
an ai-ready multiplex staining dataset
705
1
introduction
accurate spatial characterization of tumor immune microenvironment is critical
for precise therapeutic stratiﬁcation of cancer patients (e.g. via immunother-
apy).
currently, this characterization is done manually by individual pathologists
on standard hematoxylin-and-eosin (h&e) or singleplex immunohistochemistry
(ihc) stained images.
however, this results in high interobserver variability
among pathologists, primarily due to the large (> 50%) disagreement among
pathologists for immune cell phenotyping [10].
this is also a big cause of con-
cern for publicly available h&e/ihc cell segmentation datasets with immune
cell annotations from single pathologists.
in contrast, current brightﬁeld mihc staining
protocols relying on dab (3,3’-diaminobenzidine) alcohol-insoluble chromogen,
even though easily implementable with current clinical staining protocols, suf-
fer from occlusion of signal from sequential staining of additional markers.
this
706
p. ghahremani et al.
requires only aﬃne registration to align the digitized restained images to obtain
non-occluded signal intensity proﬁles for all the markers, similar to mif stain-
ing/scanning.
in this paper, we introduce a new dataset that can be readily used out-of-
the-box with any artiﬁcial intelligence (ai)/deep learning algorithms for spatial
characterization of tumor immune microenvironment and several other use cases.
to date, only two denovo stained datasets have been released publicly: bci h&e
and singleplex ihc her2 dataset [7] and deepliif singleplex ihc ki67 and
mif dataset
in contrast, we
release the ﬁrst denovo mif/mihc stained dataset with tumor and immune
markers for more accurate characterization of tumor immune microenvironment.
we also demonstrate several interesting use cases: (1) ihc quantiﬁcation of
cd3/cd8 tumor-inﬁltrating lymphocytes (tils) via style transfer, (2) virtual
translation of cheap mihc stains to more expensive mif stains, and (3) virtual
tumor/immune cellular phenotyping on standard hematoxylin images.
demographics and other relevant details of the eight anonymized head-and-
neck squamous cell carcinoma patients, including ecog performance score, pack-year,
and surgical pathology stage (ajcc8).
id
age gender race
ecog smoking py
pstage cancer site
cancer subsite
case1 49
male
white 3
current
21
1
oral cavity ventral tongue
case2 64
male
white 3
former
20
4
larynx
vocal cord
case3 60
male
black
2
current
45
4
larynx
false vocal cord
case4
53
male
white 1
current
68
4
larynx
supraglottic
case5 38
male
white 0
never
0
4
oral cavity lateral tongue
case6 76
female
white 1
former
30
2
oral cavity lateral tongue
case7 73
male
white 1
former
100
3
larynx
glottis
case8 56
male
white 0
never
0
2
oral cavity tongue
2
dataset
the complete staining protocols for this dataset are given in the accompany-
ing supplementary material.
images were acquired at 20× magniﬁcation at
moﬃtt cancer center.
the demographics and other relevant information for all
eight head-and-neck squamous cell carcinoma patients is given in table 1.
2.1
region-of-interest selection and image registration
after scanning the full images at low resolution, nine regions of interest (rois)
from each slide were chosen by an experienced pathologist on both mif and
mihc images: three in the tumor core (tc), three at the tumor margin (tm),
and three outside in the adjacent stroma (s) area.
hematoxylin-stained rois were ﬁrst used to align all
an ai-ready multiplex staining dataset
707
the mihc marker images in the open source fiji software using aﬃne registra-
tion.
after that, hematoxylin- and dapi-stained rois were used as references
to align mihc and mif rois again using fiji and subdivided into 512×512
patches, resulting in total of 268 co-registered mihc and mif patches (∼33
co-registered mif/mihc images per patient).
second column shows stains extracted from ﬁrst column
mihc-aec images using otsu thresholding.
third column shows the corresponding
perfectly co-registered original mif images.
using the mif image, we separated fore-
ground of the mihc-aec image from its background and calculated the mean value
of the foreground pixels as well as the background pixels.
each square represents an image in the dataset and
the top half of each square shows the mean color value of the positive cells, extracted
from mihc-aec using its corresponding mif image and the bottom half of it shows
the mean color value of its background.
the last column shows the rmse and
ssim diagrams of all four stains calculated using the extracted stain from ihc-aec
images (second column) and the mif images (third column).
the low error rate of
rmse and high structural similarity seen in these diagrams show high concordance
among mihc-aec and mif images.
708
p. ghahremani
3. examples of synthesized ihc images and corresponding input images.
style ihc
images were taken from the public lyon19 challenge dataset [14].
we used grayscale
hematoxylin images because they performed better with style transfer.
fig.
4. examples of hematoxylin, mif dapi, mif cd3 and classiﬁed segmentation
mask for this marker.
the dapi images were segmented using cellpose [13] and man-
ually corrected by a trained technician and approved by a pathologist.
the segmented
masks were classiﬁed using the cd3 channel intensities.
[15] nuclick
0.49 ± 0.30 0.37 ± 0.25 0.63 ± 0.37 2.75 ± 5.29
our dataset 0.53 ± 0.30 0.41 ± 0.26 0.70 ± 0.36 2.19 ± 2.89
3.1
ihc cd3/cd8 scoring using mif style transfer
we generate a stylized ihc image (fig. 3) using three input images: (1) hema-
toxylin image (used for generating the underlying structure of cells in the stylized
image), (2) its corresponding mif cd3/cd8 marker image (used for staining
positive cells as brown), and (3) sample ihc style image (used for transferring
its style to the ﬁnal image).
the complete architecture diagram is given in the
supplementary material.
speciﬁcally, the model consists of two sub-networks:
(a) marker generation: this sub-network is used for generating mif marker
data from the generated stylized image.
[4] for generating the marker images.
the cgan network
consists of a generator, responsible for generating mif marker images given an
ihc image, and a discriminator, responsible for distinguishing the output of the
generator from ground truth data.
we ﬁrst extract the brown (dab channel)
from the given style ihc image, using stain deconvolution.
then, we use pairs
of the style images and their extracted brown dab marker images to train this
sub-network.
this sub-network improves staining of the positive cells in the ﬁnal
stylized image by comparing the extracted dab marker image from the stylized
image and the input mif marker image at each iteration.
(b) style transfer: this sub-network creates the stylized ihc image using an
attention module, given (1) the input hematoxylin and the mif marker images
and (2) the style and its corresponding marker images.
for synthetically gen-
erating stylized ihc images, we follow the approach outlined in adaattn [8].
this sub-network is used to create a stylized image using
the structure of the given hematoxylin image while transferring the overall color
distribution of the style image to the ﬁnal stylized image.
the generated marker
image from the ﬁrst sub-network is used for a more accurate colorization of the
710
p. ghahremani et al.
positive cells against the blue hematoxylin counterstain/background; not deﬁn-
ing loss functions based on the markers generated by the ﬁrst sub-network leads
to discrepancy in the ﬁnal brown dab channel synthesis.
for the stylized ihc images with ground truth cd3/cd8 marker images, we
also segmented corresponding dapi images using our interactive deep learning
impartial
[9] tool https://github.com/nadeemlab/impartial and then classiﬁed
the segmented masks using the corresponding cd3/cd8 channel intensities, as
shown in fig.
4. we extracted 268 tiles of size 512×512 from this ﬁnal segmented
and co-registered dataset.
for the purpose of training and testing all the models,
we extract four images of size 256 × 256 from each tile due to the size of the
external ihc images, resulting in a total of 1072 images.
[14] to use as style ihc images.
using
these images, we created a dataset of synthetically generated ihc images from
the hematoxylin and its marker image as shown in fig.
3.
we evaluated the eﬀectiveness of our synthetically generated dataset (styl-
ized ihc images and corresponding segmented/classiﬁed masks) using our gener-
ated dataset with the nuclick training dataset (containing manually segmented
cd3/cd8 cells)
lyon19 ihc cd3/cd8 images
are taken from breast, colon, and prostate cancer patients.
we split their training
set into training and validation sets, containing 553 and 118 images, respectively,
and use their validation set for testing our trained models.
5. examples of ground-truth and generated mif data from mihc-aec images.
we also tested the trained models on 1,500 randomly selected images from
the training set of the lymphocyte assessment hackathon (lysto)
[1], con-
taining image patches of size 299 × 299 obtained at a magniﬁcation of 40× from
breast, prostate, and colon cancer whole slide images stained with cd3 and cd8
markers.
only the total number of lymphocytes in each image patch are reported
in this dataset.
to evaluate the performance of trained models on this dataset,
we counted the total number of marked lymphocytes in a predicted mask and
calculated the diﬀerence between the reported number of lymphocytes in each
image with the total number of lymphocytes in the predicted mask by the model.
in table 2, the average diﬀerence value (diﬀcount) of lymphocyte number for
the whole dataset is reported for each model.
3.2
virtual translation of cheap mihc to expensive mif stains
unlike clinical dab staining, as shown in style ihc images in fig.
3, where
brown marker channel has a blue hematoxylin nuclear counterstain to stain
for all the cells, our mihc aec-stained marker images (fig. 5) do not stain
for all the cells including nuclei.
in this use case, we show that mihc marker
images can be translated to higher quality mif dapi and marker images which
fig.
6. examples of ground-truth and generated mif immune (cd3) and tumor
(panck) markers from standard hematoxylin images.
712
p. ghahremani et al.
stain eﬀectively for all the cells.
we trained deepliif on mihc cd3 aec-
stained images to infer mif dapi and cd3 marker.
some examples of testing
the trained model on cd3 images are shown in fig.
3.3
virtual cellular phenotyping on standard hematoxylin images
there are several public h&e/ihc cell segmentation datasets with manual
immune cell annotations from single pathologists.
these are highly problem-
atic given the large (> 50%) disagreement among pathologists on immune cell
phenotyping [10].
in this last use case, we infer immune and tumor markers
from the standard hematoxylin images using again the public deepliif vir-
tual translation module [2,3].
sample
images/results taken from the testing dataset are shown in fig.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_69.pdf:
transformer-based multiple instance learning (mil) frame-
work has been proven advanced for whole slide image (wsi) analysis.
moreover, the current mil
cannot take advantage of a large number of unlabeled wsis for training.
in this paper, we propose a novel self-supervised whole slide image rep-
resentation learning framework named position-aware masked autoen-
coder (pama), which can make full use of abundant unlabeled wsis
to improve the discrimination of slide features.
moreover, we propose
a position-aware cross-attention (paca) module with a kernel reorien-
tation (kro) strategy, which makes pama able to maintain spatial
integrity and semantic enrichment during the training.
the results of experiments show our pama
is superior to sota mil methods and ssl methods.
keywords: wsi representation learning · self-supervised learning
1
introduction
in the past few years, the development of histopathological whole slide image
(wsi) analysis methods has dramatically contributed to the intelligent cancer
diagnosis [4,10,15].
however, due to the limitation of hardware resources, it is
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43987-2 69.
recent
studies usually divide the wsi analysis into multiple stages.
these transformer-based models achieved state-of-the-art performance in
sub-type classiﬁcation, survival prediction, gene mutant prediction, etc.
[5] explored and posed a new challenge
referred to as slide-level self-learning and proposed hipt, which leveraged the
hierarchical structure inherent in wsis and constructed multiple levels of the
self-supervised learning framework to learn high-resolution image representa-
tions.
this approach enables mil-based frameworks to take advantage of abun-
dant unlabeled wsis, further improving the accuracy and robustness of tumor
recognition.
the bias and error generated in each level of the representation
model will accumulate in the ﬁnal decision model.
moreover, the vit [6] back-
bone used in hipt is originally designed for nature sense images in ﬁxed sizes
whose positional information is consistent.
but these masks are manually deﬁned which is
not trainable and lacked orientation information.
in this paper, we propose a novel whole slide image representation learning
framework named position-aware masked autoencoder (pama), which achieves
slide-level representation learning by reconstructing the local representations of
the wsi in the patch feature space.
(1)
we propose a novel whole slide image representation learning framework named
position-aware masked autoencoder (pama).
(2) we pro-
pose a position-aware cross-attention (paca) module with a kernel reorienta-
tion (kro) strategy, which makes the framework able to maintain the spa-
tial integrity and semantic enrichment of slide representation during the self-
supervised training.
(3) the experiments on two datasets show our pama can
achieve competitive performance compared with sota mil methods and ssl
methods.
fig.
1. the overview of the proposed whole slide image representation with position-
aware masked autoencoder (pama), where (i) shows the data preprocessing including
the patch embedding and anchors clustering, (ii) describes the workﬂow of wsi rep-
resentation self-supervised learning with pama, (iii) is the structure of the position-
aware cross-attention (paca) module which is the core of the encoder and decoder,
and (iv) shows the kernel reorientation (kro) strategy and the detailed process is
described in algorithm 1.
position-aware masked autoencoder
717
2
methods
2.1
problem formulation and data preparation
mae
[7] is a successful ssl framework that learns image presentations by recon-
structing the masked image in the original pixel space.
first, we divided wsis into non-overlapping image
patches and meanwhile removed the background without tissue regions based on
a threshold (as shown in fig. 1(i)).
afterward, the
features for a wsi are represented as x ∈ rnp×df , where df is the dimension of
the feature and np is the number of patches in the wsi. inspired by kat
[7], we random mask patch tokens with a high masking ratio (i.e.
75% in our experiments).
718
k. wu et al.
the message passing between the anchors and patches is achieved by a bi-
directional cross-attention between the patches and anchors.
= σ(k(n)w(n)
q
· (x(n)w(n)
k )
t
√de
+ϕd(d(n))+ϕp(p(n)))·(x(n)w(n)
v ), (1)
where wl ∈ rdf ×de, l = q, k, v are learnable parameters with de denoting the
dimension of the head output, σ represents the softmax function, and ϕd and ϕp
are the embedding functions that respectively take the distance and polar angle
as input and output the corresponding trainable embedding values.
the embedding of relative distance and polar angle information helps the model
maintain the semantic and structural integrity of the wsi and meanwhile pre-
vents the wsi representation from collapsing to the local area throughout the
training process.
in natural scene images, there is
natural directional conspicuousness of semantics.
but histopathology images have no absolute deﬁnition of
direction.
the semantics of wsi will not change with rotation and ﬂip.
based on the updated polar
axis, we can then amend p(n) to p(n+1).
the detailed algorithm is described in
algorithm 1.
position-aware masked autoencoder
719
3
experiments and results
3.1
datasets
we evaluated the proposed method on two datasets, the public tcga-lung and
the in-house endometrial dataset, which are introduced as follows.
we con-
ducted wsi multi-type classiﬁcation experiments on the two datasets.
the results of the test set were
reported for comparison.
3.2
implementation details
the wsi representation pre-training stage uses all training data and does not
involve any supervised information.
2. semi-supervised experiments with 10%, 35%, 60% and 85% of labelled data
on the endometrial dataset.
the usage of [cls] token refers to the mae
we imple-
mented all the models in python 3.8 with pytorch 1.7 and cuda 10.2 and run
the experiments on a computer with 4 gpus of nvidia geforce 2080ti.
position-aware masked autoencoder
721
3.3
eﬀectiveness of the wsi representation learning
we ﬁrst conducted experiments on the endometrial dataset to verify the eﬀec-
tiveness of self-supervised learning for wsi analysis under label-limited condi-
tions.
2, where the performance obtained with dif-
ferent ratios of labeled training wsis are compared.
[7] based on the patch
features is implemented as the baseline.
[7]
0.965
83.90
0.970
87.50
0.801
38.87
0.832
41.95
mae+
0.969
85.07
0.981
88.25
0.811
37.91
0.845
42.85
pama
0.982 90.84 0.988 92.48 0.829 43.38 0.851 43.64
overall, pama consistently achieves signiﬁcantly better performance across
all the label ratios than mae
[5] is a two-stage self-learning frame-
work, which ﬁrst leverages dino
the multi-stage framework accumulated the training bias and noise,
which caused an auc gap of hipt
we also observed a signiﬁcant improvement when
comparing mae+ with mae
please refer to the
supplementary materials for more detailed results.
722
k. wu et al.
3.4
ablation study
then, we conducted ablation experiments to verify the necessity of the proposed
structural embedding strategy.
the results are shown in table 2. overall, pama consistently achieves the best
performance.
[19] are state-of-
the-art methods for histopathological image classiﬁcation.
the experiments on two large-scale datasets
have demonstrated the eﬀectiveness of pama in the condition of limited-label.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_55.pdf:
segmentation of pathological images is a crucial step for
accurate cancer diagnosis.
however, acquiring dense annotations of such
images for training is labor-intensive and time-consuming.
to address
this issue, semi-supervised learning (ssl) has the potential for reducing
the annotation cost, but it is challenged by a large number of unlabeled
training images.
in this paper, we propose a novel ssl method based on
cross distillation of multiple attentions (cdma) to eﬀectively leverage
unlabeled images.
additionally, uncertainty minimization is applied to the average
prediction of the three branches, which further regularizes predictions
on unlabeled images and encourages inter-branch consistency.
our pro-
posed cdma was compared with eight state-of-the-art ssl methods on
the public digestpath dataset, and the experimental results showed that
our method outperforms the other approaches under diﬀerent annotation
ratios.
keywords: semi-supervised learning · knowledge distillation ·
attention · uncertainty
1
introduction
automatic segmentation of tumor lesions from pathological images plays an
important role in accurate diagnosis and quantitative evaluation of cancers.
recently, deep learning has achieved remarkable performance in pathological
image segmentation when trained with a large and well-annotated dataset [6,13,
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. (eds.): miccai 2023, lncs 14225, pp.
https://doi.org/10.1007/978-3-031-43987-2_55
semi-supervised segmentation via cross distillation of multiple attentions
571
20].
however, obtaining dense annotations for pathological images is challenging
and time-consuming, due to the extremely large image size (e.g., 10000 × 10000
pixels), scattered spatial distribution, and complex shape of lesions.
the
consistency-based methods impose consistency constraints on the predictions
of an unlabeled image under some perturbations.
for example, mean teacher
(mt)-based methods [14,23] encourage consistent predictions between a teacher
and a student model with noises added to the input.
[21] introduced
a pairwise relation network to exploit semantic consistency between each pair
of images in the feature space.
[7] proposed
to encourage the predictions of auxiliary decoders and a main decoder to be
consistent under perturbed hierarchical features.
pseudo label-based methods
typically generate pseudo labels for labeled images to supervise the network [4].
since using a model’s prediction to supervise itself may over-ﬁt its bias, chen
et al.
however, the pseudo
labels are not accurate and contain a lot of noise, using argmax or sharpening
operation will lead to over-conﬁdence of potentially wrong predictions, which lim-
its the performance of the models.
[17] directly
applied entropy minimization to the segmentation results.
in this work, we propose a novel and eﬃcient method based on cross distilla-
tion with multiple attentions (cdma) for semi-supervised pathological image
segmentation.
unlike mc-net+ [19]
that is based on diﬀerent upsampling strategies, our mtnet uses diﬀerent atten-
tion mechanisms in three decoder branches that calibrate features in diﬀerent
aspects to obtain diverse and complementary outputs.
secondly, inspired by the
observation that smoothed labels are more eﬀective for noise-robust learning
found in recent studies [10,22], we propose a cross decoder knowledge distil-
lation (cdkd) strategy to better leverage the diverse predictions of unlabeled
images.
in addition, we apply an
uncertainty minimization-based regularization to the average probability pre-
diction across the decoders, which not only increases the network’s conﬁdence,
but also improves the inter-decoder consistency for leveraging labeled images.
the contribution of this work is three-fold: 1) a novel framework named
cdma based on mtnet is introduced for semi-supervised pathological image
segmentation, which leverages diﬀerent attention mechanisms for generating
diverse and complementary predictions for unlabeled images; 2) a cross decoder
knowledge distillation method is proposed for robust and eﬃcient learning from
noisy pseudo labels, which is combined with an average prediction-based uncer-
tainty minimization to improve the model’s performance; 3) experimental results
show that the proposed cdma outperforms eight state-of-the-art ssl methods
on the public digestpath dataset [3].
fig.
1. our cdma for semi-supervised segmentation.
cross decoder knowledge distillation
(cdkd) is proposed to better deal with noisy pseudo labels, and an uncertainty min-
imization is applied to the average probability prediction of the three branches.
lsup
is only for labeled images.
1, the proposed cross distillation of multiple attentions
(cdma) framework for semi-supervised pathological image segmentation con-
sists of three core modules: 1) a tri-branch network mtnet that uses three
diﬀerent attention mechanisms to obtain diverse outputs, 2) a cross decoder
knowledge distillation (cdkd) module to reduce the eﬀect of noisy pseudo
labels based on soft supervision, and 3) an average prediction-based uncertainty
minimization loss to further regularize the predictions on unlabeled images.
semi-supervised segmentation via cross distillation of multiple attentions
573
2.1
multi-attention tri-branch network (mtnet)
attention is an eﬀective network structure design in fully supervised image seg-
mentation
it can calibrate the feature maps for better performance by
paying more attention to the important spatial positions or channels with only a
few extra parameters.
however, it has been rarely investigated in semi-supervised
segmentation tasks.
to more eﬀectively exploit attention mechanisms for semi-
supervised pathological image segmentation, our proposed mtnet consists of a
shared encoder and three decoder branches that are based on channel attention
(ca), spatial attention (sa) and simultaneous channel and spatial attention
(csa), respectively.
pools
avg and pools
max represent average
pooling and max-pooling across the spatial dimension, respectively.
sa branch leverages spatial attention to highlight the most relevant spatial
positions and suppress the irrelevant regions in a feature map.
poolc
avg and poolc
max are average and
max-pooling across the channel dimension, respectively.
a csa block consists of a ca block followed by an sa block,
taking advantage of channel and spatial attention simultaneously.
for an input image, the logit predictions obtained by the three branches are
denoted as zca, zsa and zcsa, respectively.
after using a standard softmax
operation, their corresponding probability prediction maps are denoted as pca,
psa and pcsa, respectively.
574
l. zhong et al.
2.2
cross decoder knowledge distillation (cdkd)
since the three branches have diﬀerent decision boundaries, using the predictions
from one branch as pseudo labels to supervise the others would avoid each branch
over-ﬁtting its bias.
however, as the predictions for unlabeled training images
are noisy and inaccurate, using hard or sharpened pseudo labels [2,19] would
strengthen the conﬁdence on incorrect predictions, leading the model to overﬁt
the noise [10,22].
to address this problem, we introduce cdkd to enhance the
ability of our mtnet to leverage unlabeled images and eliminate the negative
impact of noisy pseudo labels.
(5)
2.3
average prediction-based uncertainty minimization
minimizing the uncertainty (e.g., entropy)
[15] has been shown to be an eﬀective
regularization for predictions on unlabeled images, which increases the model’s
conﬁdence on its predictions.
to avoid this problem
and further encourage inter-decoder consistency for regularization, we propose
an average prediction-based uncertainty minimization:
(6)
semi-supervised segmentation via cross distillation of multiple attentions
575
where ¯p = (pcsa + pca + psa)/3 is the average probability map.
i is the average probability
for class c at pixel i. note that when lum for a pixel is close to zero, the average
probability for class c of that pixel is close to 0.0 (1.0), which drives all the
decoders to predict it as 0.0 (1.0) and encourages inter-decoder consistency.
finally, the overall loss function for our cdma is:
l = lsup + λ1lcdkd + λ2lum
(7)
where lsup = (lcsa
sup
+ lca
sup + lsa
sup)/3 is the average supervised learning loss
for the three branches on the labeled training images, and the supervised loss
for each branch calculates the dice loss and cross entropy loss between the
probability prediction (pcsa, pca and psa) and the ground truth label.
note that lcdkd and lum are
applied on both labeled and unlabeled training images.
fig.
2. visual comparison between our proposed cdma with state-of-the-art methods
for semi-supervised semantic segmentation of wsis.
3
experiments and results
dataset and implementation details.
we used the public digestpath data-
set [3] for binary segmentation of colonoscopy tumor lesions from whole slide
images (wsi) in the experiment.
the wsis were collected from four medi-
cal institutions of ×20 magniﬁcation (0.475 μm/pixel) with an average size of
5000 × 5000.
for com-
putational feasibility, we cropped the wsis into patches with a size of 256 × 256.
576
l. zhong et al.
at inference time for segmenting a wsi, we used a sliding window of size 256×256
with a stride of 192 × 192.
the cdma framework was implemented in pytorch, and all experiments
were performed on one nvidia 2080ti gpu. mtnet was implemented by
extending deeplabv3+
the encoder used
a backbone of resnet50 pre-trained on imagenet.
the kernel size of conv in
the sa block is 7 × 7. sgd optimizer was used for training, with weight decay
5 × 10−4, momentum 0.9 and epoch number 150.
for data augmentation,
we adopted random ﬂipping, random rotation, and random gaussian noise.
for
inference, only the csa branch was used due to the similar performance of the
three branches after converge and the increased inference time of their ensemble,
and no post-processing was used.
[2]; 6) hierarchical consistency enforcement (hce)
they were also compared with the lower bound
of supervised learning (sl) that only learns from the labeled images.
semi-supervised segmentation via cross distillation of multiple attentions
577
quantitative evaluation of these methods is shown in table 1.
[2] showed the best performance for both of
the two annotation ratios.
our proposed cdma achieved a better performance
than all the existing methods, with a dsc score of 69.72% and 72.24% when
the annotation ratio was 5% and 10%, respectively.
it can be observed that our cdma yields
less mis-segmentation compared with cps
for ablation study, we set the baseline as using the proposed
mtnet with three diﬀerent decoders for supervised learning from labeled images
only.
it obtained an average dsc of 65.02% and 68.61% under the two annotation
ratios respectively.
table 2 shows
that our lcdkd obtained an average dsc of 68.84% and 71.49% under the two
annotation ratios respectively, and it outperformed lcdkd (argmax) and lcdkd
(t=1), demonstrating that our cdkd based on softened probability prediction
is more eﬀective in dealing with noisy pseudo labels.
by introducing our average
prediction-based uncertainty minimization lum, the dsc was further improved
to 69.72% and 72.24% under the two annotation ratios respectively.
the results in the second section of table 2 show that using the
same structures for diﬀerent branches, i.e., mtnet(-atten) and mtnet(csa×3),
had a lower performance than using diﬀerent attention blocks, and using three
578
l. zhong et al.
attention branches outperformed just using two attention branches.
it can also
be found that using csa branch for inference had a very close performance to
mtnet(ensb), and it is more eﬃcient than the later.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_45.pdf:
whole slide image (wsi) classiﬁcation remains a challenge
due to their extremely high resolution and the absence of ﬁne-grained
labels.
mil methods involve a patch embedding module and a bag-level
classiﬁcation module, but they are prohibitively expensive to be trained
in an end-to-end manner.
such schemes
hinder the patch embedder’s access to slide-level semantic labels, result-
ing in inconsistency within the entire mil pipeline.
we tested our framework on two datasets
using three diﬀerent backbones, and our experimental results demonstrate
consistent performance improvements over state-of-the-art mil methods.
keywords: multiple instance learning · whole slide image ·
deep
learning
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43987-2 45.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
compared to traditional microscope-
based observation, whole slide scanning converts glass slides into gigapixel digital
images that can be conveniently stored and analyzed.
this approach allows for the direct application of existing image classiﬁcation
models, but requires additional patch-level labeling.
therefore,
many weakly-supervised [8,24] and semi-supervised [3,5] methods have been pro-
posed to generate patch-level pseudo labels at a lower cost.
however, the lack
of reliable supervision directly hinders the performance of these methods, and
serious class-imbalance problems could arise, as tumor patches may only account
for a small portion of the entire wsi [12].
in contrast, mil-based methods have become increasingly preferred due to
their only demand for slide-level labels [18].
the aim is to predict whether there are positive instances,
such as tumor patches, in a bag, and if so, the bag is considered positive as well.
in practice, a ﬁxed imagenet pre-trained feature extractor g(·) is usually used to
convert the tiled patches in a wsi into feature maps due to limited gpu mem-
ory.
as a result, many
methods focus solely on improving a(·) or f(·), leaving g(·) untrained on the wsi
dataset (as shown in fig.
however, the domain shift between wsi and nat-
ural images may lead to sub-optimal representations, so recently there have been
methods proposed to ﬁne-tune g(·) using self-supervised techniques [4,12,21] or
weakly-supervised techniques [10,13,23] (as shown in fig. 2(c)).
(d) our pro-
posed icmil which can bridge the loss back-propagation process from f(·) to g(·) by
iteratively coupling them during training.
to address the challenges mentioned above, we propose a novel mil frame-
work called icmil, which can iteratively couple the patch feature embedding pro-
cess with the bag-level classiﬁcation process to enhance the eﬀectiveness of mil
training (as illustrated in fig. 2(d)).
(3) we conduct extensive
experiments on two datasets using three diﬀerent backbones and demonstrate
the eﬀectiveness of our proposed framework.
[6] pre-trained on imagenet [19] (step 1⃝ in fig. 3).
3), of which the detailed implementation is pre-
sented in sect.
after this, g(·) is ﬁne-tuned for the speciﬁc wsi dataset,
which allows it to generate improved representations for each instance, thereby
enhancing the performance of f(·).
moreover, with a better f(·), we can use the
iterative coupling technique again, resulting in further performance gains and
mitigation to the distribution inconsistencies between instance- and bag-level
embeddings.
otherwise, a(·) may
lead to larger diﬀerence between the decision boundaries of bag-level classifer f(·)
and instance-level classiﬁer f ′(·), which may cause icmil taking more time to
converge.
therefore, in our experiments, we choose to use the attention-based instance
aggregation method [9] which has been widely used in many of the existing
iteratively coupled multiple instance learning
471
fig.
moreover, incorpo-
rating augmented inputs in the training process allows for the better utilization
of supervision signals, resulting in a more robust g(·).
for a given patch input x, the teacher generates the
corresponding pseudo label, while the student receives an augmented image x′
and attempts to generate a similar prediction to that of the teacher through a
consistency loss lc.
the overall
loss function for this step is lc + αlw, with α set to 0.5 in our experiments.
3
experiments
3.1
datasets
our experiments utilized two datasets, with the ﬁrst being the publicly available
breast cancer dataset, camelyon16
although patch-level labels are oﬃcially provided in
camelyon16, they were not used in our experiments.
the ground truth labels are binary classes of low risk and high
risk, which were provided by experienced pathologists.
3.2
implementation details
for camelyon16, we tiled the wsis into 256×256 patches on 20× magniﬁcation
using the oﬃcial code of [25], while for the hcc dataset the patches are 384×384
on 40× magniﬁcation following the pathologists’ advice.
for both datasets, we
used an imagenet pre-trained resnet50 to initialize g(·).
[25]
✓
93.2
84.9
89.0
83.0
85.5
78.1
ours
(w/ max pooling)
✓
✓
✓
85.2
(+5.7)
74.7
(+4.1)
81.9
(+1.6)
86.6
(+6.5)
87.3
(+3.0)
82.0
(+5.2)
ours
(w/ ab−mil)
✓
✓
✓
90.0
(+4.6)
80.5
(+2.5)
86.6
(+2.1)
87.1
(+5.9)
88.3
(+2.3)
83.3
(+5.2)
ours
(w/ dtfd−mil)
✓
✓
✓
93.7
(+0. 5)
87.0
(+2. 1)
90.6
(+1. 6)
87.7
(+4. 7)
89.1
(+3. 6)
83.5
(+5. 4)
ﬁrstly embedded into a 1024-dimension vector, and then be projected to a 512-
dimension hidden space for further bag-level training.
experiments were all conducted on a nvidia tesla
m40 (12gb).
3.3
experimental results
ablation study.
from
table 1(a), we can learn that as the number of icmil iteration increases, the
performance will also go up until reaching a stable point.
since the number of
instances is very large in wsi datasets, we empirically recommend to choose
to run icmil one iteration for ﬁne-tuning g(·) to achieve the balance between
performance gain and time consumption.
experimental results are presented in
table 2.
as shown, our icmil framework consistently improves the performance
of three diﬀerent mil baselines (i.e., max pool, ab-mil, and dtfd-mil),
demonstrating the eﬀectiveness of bridging the loss back-propagation from bag
calssiﬁer to embedder.
when used with the
state-of-the-art mil method dtfd-mil, icmil further increases its perfor-
mance on camelyon16 by 0.5% auc, 2.1% f1, and 1.6% acc.
results on the hcc dataset also proves the eﬀectiveness of icmil, despite
the minor diﬀerence on the relative performance of baseline methods.
mean pool-
ing performs better on this dataset due to the large area of tumor in the wsis
(about 60% patches are tumor patches), which mitigates the impact of average
pooling on instances.
also, the performance diﬀerences among diﬀerent vanilla
mil methods tends to be smaller on this dataset since risk grading is a harder
task than camelyon16.
as a
result, after applying icmil on the mil baselines, these methods all gain great
performance boost on the hcc dataset.
4
conclusion
in this work, we propose icmil, a novel framework that iteratively couples
the feature extraction and bag classiﬁcation stages to improve the accuracy of
mil models.
icmil leverages the category knowledge in the bag classiﬁer as
pseudo supervision for embedder ﬁne-tuning, bridging the loss propagation from
classiﬁer to embedder.
the
experimental results show that our method brings consistent improvement to
existing mil backbones.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_1.pdf:
the proposed ultradet demonstrates signiﬁcant improvement
over previous state-of-the-arts and achieves real-time inference speed.
while previous works focused on lesion detection in still images [25]
and oﬄine videos [9,11,22], this paper explores real-time ultrasound video lesion
detection.
real-time lesion prompts can assist radiologists during scanning, thus
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43987-2_1.
(b) the ntca
module leverages temporal contexts to suppress the fp.
4.4, the inability to utilize ntc is
a key issue leading to the fps reported by general-purpose detectors.
to address this issue, we propose a novel ultradet model to leverage ntc.
the ntca module
leverages roi-level ntc which are crucial for radiologists but ignored in previous
works, thereby eﬀectively improving the detection performance in a reliable and
interpretable way.
experiments on cva-bus dataset [9] demonstrate that ultra-
det, with real-time inference speed, signiﬁcantly outperforms previous works,
reducing about 50% fps at a recall rate of 0.90.
(2) we propose a novel ultradet model, incorpo-
mining negative temporal contexts to suppress fps
5
rating an ntca module that eﬀectively leverages ntc for fp suppression.
(3)
we conduct extensive experiments to demonstrate the proposed ultradet sig-
niﬁcantly outperforms the previous state-of-the-arts.
one-stage
detectors
previous works have explored lesion detection in still images [25] and oﬄine
videos
thus
their performances are far from satisfactory.
optical flow [3] is used to guide ultrasound segmentation [12], motion estima-
tion
the rpn generates proposals consisting of boxes bτ and
proposal features qτ using roi align and average pooling:
qτ = avgpool (roialign(fτ, bτ))
we conduct iof align and average pooling to extract ct,τ:
ct,τ = avgpool (iofalign(fτ, bt, ot→τ))
4
experiments
4.1
dateset
cva-bus dateset.
visual comparisons
of two versions of labels are available in supplementary materials.
quantitative results of real-time lesion detection on cva-bus [9].
model
type
pr80
pr90
fp80
fp90
ap50
r@16 fps
one-stage detectors
yolox
[5]
image 69.73.7 43.47.7 23.84.8 87.624.5 80.41.6 97.50.5
[8]
image 75.72.5
[16]
image 87.22.2 72.25.1 11.02.4 23.03.7 89.51.4 98.80.3 56.1
defcn
[21]
image 81.51.8 67.52.3 21.13.2 33.44.3 86.41.3 99.30.3 51.2
track-yolo
[27]
image 90.13.2 72.710.6 5.62.2 37.820.9
[14]
image 91.30.9
[7]
image 91.41.3 79.22.9 6.22.0 24.45.6 87.61.7 92.40.9 42.7
fgfa
to evaluate the highest achievable sensitivity, we report the frame-level
average recall rates of top-16 proposals, denoted as r@16.
mining negative temporal contexts to suppress fps
9
4.3
implementation details
ultradet settings.
other hyper-parameters are listed in supplementary materials.
we set the feature dimensions of
detection heads to 256 and baselines are re-implemented to utilize only previous
frames.
we compare performances of real-time detectors with
the ultradet in table 1.
especially, the pr90 of ultradet achieves 90.8%, representing a 5.4% absolute
improvement over the best competitor, ptseformer [20].
the determination of whether fps can be inhibited
10
h. yu et al.
by ntc is based on manual judgments of experienced radiologists.
the ultradet achieves an infer-
ence speed of 30.4 fps and already meets the 30 fps requirement.
the basicdet reports fps at t = 30 and 40 as it fails to leverage ntc
speciﬁcally, we replace the iof align
with an roi align and the temporal aggregation with a simple average pool-
ing in the temporal dimension.
the results demonstrate that both iof align
and temporal aggregation are crucial, as removing either of them leads to a
noticeable drop in performance.
level aggregation provides no performance gains.
the ntca
module leverages negative temporal contexts that are essential for fp suppres-
sion but ignored in previous works, thereby being more eﬀective in suppressing
fps.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_50.pdf:
histological whole slide images (wsis) can be usually com-
promised by artifacts, such as tissue folding and bubbles, which will
increase the examination diﬃculty for both pathologists and computer-
aided diagnosis (cad) systems.
existing approaches to restoring arti-
fact images are conﬁned to generative adversarial networks (gans),
where the restoration process is formulated as an image-to-image trans-
fer.
those methods are prone to suﬀer from mode collapse and unex-
pected mistransfer in the stain style, leading to unsatisﬁed and unre-
alistic restored images.
speciﬁcally, artifusion formulates the arti-
fact region restoration as a gradual denoising process, and its training
relies solely on artifact-free images to simplify the training complexity.
however, the complex scanning procedure for histological whole-
slide images (wsis) digitization may result in the alteration of tissue structures,
due to improper removal, ﬁxation, tissue processing, embedding, and storage [11].
https://doi.org/10.1007/978-3-031-43987-2_50
artifact restoration in histology images with diﬀusion probabilistic models
519
for pathologists but also increases the risk of misdiagnosis for computer-aided
diagnosis (cad) systems
(a) cyclegan [19] formulates
the artifact restoration as an image-to-image transfer problem.
it leverages two pairs of
the generator and discriminator to learn the transfer between the artifact and artifact-
free image domains.
for example, cyclegan [19] formulates the
artifact restoration as an image-to-image transfer problem by learning the trans-
fer between the artifact and artifact-free image domains from unpaired images,
as depicted in fig.
further-
more, our approach is trained solely with artifact-free images, which reduces the
diﬃculty in data collection.
this approach diﬀers from gan-based methods that require either paired or
unpaired artifacts and artifact-free images, as our artifusion relies solely on
artifact-free images, resulting in a simpliﬁed training process.
extensive evaluations on real-world histology datasets and down-
stream tasks demonstrate the superiority of our framework in artifact removal
performance, which can generate reliable restored images while preserving the
stain style.
fig.
the semantic illustration of inference stage in artifusion for local regional
artifact restoration.
the proposed histology artifact restoration diﬀusion model
artifusion, comprises two stages, namely the training, and inference.
during
the training stage, artifusion learns to generate regional histology tissue struc-
tures based on the contextual information from artifact-free images.
in the infer-
ence stage, artifusion formulates the artifact restoration as a gradual denoising
process.
speciﬁcally, it ﬁrst replaces the artifact regions with gaussian noise, and
then gradually restores them to artifact-free images using the contextual infor-
mation from nearby regions.
diﬀusion training stage.
the proposed artifusion learns the capability
of generating local tissue representation from contextual information during
the training stage.
[5],
which involve a forward process that gradually injects gaussian noise into an
artifact restoration in histology images with diﬀusion probabilistic models
521
artifact-free image and a reverse process that aims to reconstruct images from
noise.
522
z. he et al.
artifact restoration in inference stage.
during the inference stage, we
ﬁrst use a threshold method to detect the artifact region in the input image
x0.
then, unlike the conventional diﬀusion models [5] that aim to generate the
entire image, artifusion selectively performs denoising resampling only in the
artifact region to maximally preserve the original morphology and stain style in
the artifact-free region, as shown in fig.
speciﬁcally, we represent the artifact-
free region and the artifact region in the input image as x0⊙(1−m) and x0⊙m,
respectively [10], where m is a boolean mask indicating the artifact region and ⊙
is the pixel-wise multiplication operator.
to perform the denoising resampling,
we write the input image xin
t
at each reverse step from t to t − 1 as the sum of
the diﬀused artifact-free region and the denoised artifact region, i.e.,
xin
t
= xsample
t
⊙ (1 − m)
consequently, the ﬁnal restored image is obtained as
x0 ⊙ (1 − m)
the resulting tokens are then processed by the attention layers,
and the auxiliary time token is discarded to retain the original feature dimension
to ﬁt the swin-transformer block design after the attention layers.
3
experiments
dataset.
to evaluate the performance of artifact restoration, a training set is
curated from a subset of camelyon17 [8]1.
it comprises a total number of 2445
artifact-free images and another 2547 images with artifacts, where all histological
images are scaled to the resolution of 256 × 256 pixels at the magnitude of 20×.
the test set uses another public histology image dataset [6] with 462 artifact-free
1 available at https://camelyon17.grand-challenge.org.
artifact restoration in histology images with diﬀusion probabilistic models
523
images2, where we obtain the paired artifact images by the manually-synthesized
artifacts [18].
fig.
4. artifact restoration on ﬁve real-world artifact images.
it highlights the ability of artifusion to
progressively remove artifacts from the histology image, resulting in a ﬁnal restored
image that is both visually pleasing and scientiﬁcally accurate.
implementations.
we implement the proposed artifusion and its counter-
part in python 3.8.10 and pytorch 1.10.0.
all experiments are carried out in
parallel on two nvidia rtx a4000 gpu cards with 16 gib memory.
consequently, we leverage the prevalent cycle-
gan [19] as the baseline for comparison, because of its excellent performance
2 available at https://github.com/lu-yizhou/clusterseg.
524
z. he et al.
in the image transfer, and also its nature that requires no paired data can ﬁt
our circumstance.
unlike cyclegan which requires both artifact-free images
and artifact images, artifusion only relies on artifact-free images, leading to
a size of the training set that is half that of cyclegan.
we use the following metrics: l2
distance (l2) with respect to the artifact region, the mean-squared error (mse)
over the whole image, structural similarity index (ssim)
quantitative comparison of artifusion with cyclegan on artifact restora-
tion performance.
the ↓ indicates the smaller value, the better performance; and vice
versa.
methods
l2 (×104) ↓
mse ↓
ssim ↑
psnr ↑
fsim ↑
sre ↑
cyclegan (#1)
[19]
1.119
0.5583
0.9656
42.37
0.7188
51.42
cyclegan (#2) [19]
1.893
0.5936
0.9622
42.12
0.7162
50.21
artifusion (u-net)
0.5027
0.2508
0.9850
47.61
0.8173
54.59
artifusion (add)
0.5007
0.2499
0.9850
47.79
0.8184
54.76
artifusion (full settings)
0.4940
0.2465
0.9860
48.08
0.8216
55.43
table 2. comparison of the model complexity and eﬃciency in terms of the number
of parameters, flops, and averaged inference time.
the quantitative comparison with
cyclegan and artifusion are shown in table 1, where some exemplary images
are illustrated in fig.
for instance, artifusion can reduce the l2 and mse by
artifact restoration in histology images with diﬀusion probabilistic models
525
more than 50%, namely from 1 × 104 to 0.5 × 104 and from 0.55 to 0.25 respec-
tively.
finally, the concate-
nating time token with feature tokens can bring an improvement in terms of all
evaluation matrices, making it a better ﬁt for the transformer architecture than
the direct summation scheme in u-net [5].
in table 2, we compare the model com-
plexity in terms of the number of parameters, floating point operations per sec-
ond (flops), and averaged inference time on one image.
however, a smaller model
size can facilitate easier deployment in real clinical practice.
evaluations by downstream classiﬁcation task.
we consider the performance on the original
unprocessed data, denoted as ‘clean’, as the upper bound.
then, we manually
synthesize the artifact (denoted as ‘artifact’) and evaluate the classiﬁcation per-
formance with restoration approaches cyclegan and artifusion.
in table 3,
comparisons show that the presence of artifacts can result in a signiﬁcant per-
formance decline of over 5% across all ﬁve network architectures.
importantly,
the classiﬁcation accuracy on images restored with artifusion is consistently
526
z.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_44.pdf:
breast cancer (bc) is one of the most common cancers iden-
tiﬁed globally among women, which has become the leading cause of
death.
multi-modal pathological images contain diﬀerent information for
bc diagnosis.
hematoxylin and eosin (h&e) staining images could reveal
a considerable amount of microscopic anatomy.
immunohistochemical
(ihc) staining images provide the evaluation of the expression of various
biomarkers, such as the human epidermal growth factor receptor (her2)
hybridization.
in this paper, we propose a multi-modal pre-training model
via pathological images for bc diagnosis.
the experiments on two datasets (herohe chal-
lenge and bci challenge) show state-of-the-art results.
keywords: breast cancer · hematoxylin and eosin staining ·
immunohistochemical staining · multi-modal pre-training
1
introduction
breast cancer (bc) is one of the most common malignant tumors in women
worldwide and it causes nearly 0.7 million deaths in 2020
the patholog-
ical process is usually the golden standard approach for bc diagnosis, which
relies on leveraging diverse complementary information from multi-modal data.
in addition to obtaining the histological characteristics of tumors from hema-
toxylin and eosin (h&e) staining images, immunohistochemical (ihc) staining
images are also widely used for pathological diagnoses, such as the human epi-
dermal growth factor receptor 2 (her2), the estrogen receptor (er), and the
progesterone receptor (pr) [22].
with the development of deep learning, there
are a lot of multi-modal fusion methods for cancer diagnosis [6,7,20,21].
recently, with the development of transformer, multi-modal pre-training has
achieved great success in the ﬁelds of computer vision (cv) and natural language
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
1. one is based on isomorphic data,
such as vision-language pre-training [5] and vision-speech-text pre-training [3].
bachmann et al.
[2] proposed multi-
mae to pre-train models with intensity images, depth images, and segmentation
maps.
in the ﬁeld of medical image analysis, it is widely recognized that using
multi-modal data can produce more accurate diagnoses than using single-modal
data.
however, the development of multi-modal pre-training methods has been
limited due to the scarcity of paired multi-modal data.
most methods focus on
chest x-ray vision-language pre-training [8,11].
we choose paired h&e and ihc (only her2) staining images, which
are cropped into non-overlapped patches as the input of our model.
finally,
we use modal-speciﬁc decoders to reconstruct the original h&e and ihc staining
images respectively.
 we evaluate the proposed method on two public datasets as herohe chal-
lenge and bci challenge, which shows that our method achieves state-of-the-
art performance.
a pair of images x and y (h&e and
ihc) are cropped into n non-overlapped patches, which are randomly masked by ratio
λ1 and λ2.
2.
a pair of h&e and her2 images are cropped into regular non-overlapping
patches.
then we use the mixed attention module to extract intra-modal and inter-modal
complementary information.
finally, the modal-speciﬁc tokens are fed into the
modal-speciﬁc decoders to reconstruct the original h&e and her2 images.
the
pre-trained modal-fusion encoder could be used for downstream tasks (e.g., her2
status prediction and her2 image generation based on h&e images).
an image is cropped into several non-
overlapping patches, and these patches are mapped to d dimension tokens with
the linear projection and added position embeddings to retain positional informa-
tion.
we
replace the mhsa with mhca in the inter-modal attention to learn complementary
information.
transformer processing ﬂow.
input: a set of patches from one image x = {xi}n
i=1, x ∈ rn×(r×r×c)
1: transfer patches into linear embeddings
2: for i = 1 to n do
3:
fi ← lp(xi), where f = {fi}n
i=1, f ∈ rn×d
4: end for
5: position encoding concatenation
6: f0 ← concat (fp, f), where fp ∈ r1×d
7: for l = 1 to l do
8:
f
′
l ← mhsa(ln(fl−1))
in the her2 staining image generation
task, we remain the structure of gan and replace the generator with our pre-trained
model.
in the her2 status prediction task, we replace the feature extractor with our
pre-trained model to obtain representations with her2 semantics.
we use mhca to leverage diverse complementary information
between two modalities.
ax(f)
diﬀerent from the transformer encoder, the target of the
transformer decoder is used to reconstruct the original image.
given a pair of h&e image x and her2 image y ,
which is cut into 16 × 16 non-overlapping patches {xi}n
i=1 and {yi}n
i=1.
after the process
of the mixed attention module, h&e and her2 patch tokens are fed into the
modal-speciﬁc decoders respectively to reconstruct the original h&e image x′
and her2 image y ′. the reconstruction loss is computed by the mean squared
error between the original images x, y and the generative images x′, y ′, which
is computed as
lh&e = 1
t1
t1

i=1
| pi − p′
i |2, lher2 = 1
t2
t2

i=1
| qi − q′
i |2 .
4.
we choose two relevant tasks: her2 image generation based on h&e images and
her2 status prediction.
we use pairs of h&e and ihc images for gan training.
3
experimental results
3.1
datasets
acrobat challenge.
in the ﬁnal phase of nonrigid
registration, we use the optimized transformation to get the initial displacement
ﬁeld, which is optimized across diﬀerent levels of wsis by gradient update.
we resize the displacement ﬁeld and apply it to the original moving wsi.
after
all the wsi pairs are well registered, we convert the padded h&e image to
grayscale and apply median blur to it.
next, the otsu threshold is applied to
extract the foreground area, which is cropped into non-overlapping 256 × 256
images.
finally, all the chosen images (around 0.35 million) from wsi in the
same pair are saved for mmp-mae pre-training.
breast cancer immunohistochemical image generation chal-
lenge [16] consists of 3896 pairs of images for training and 977 pairs for testing,
which are used to generate her2 images based on h&e images.
performance comparison on bci challenge.
3.2
experimental setup
experiments are implemented in pytorch
in the her2 staining image generation task, we use 2 gpus with a batch size
of 4.
peak signal to noise ratio (psnr) and
structural similarity (ssim) are used as the evaluation indicators for the quality
of the her2 generated images.
3.3
method comparison
her2 staining image generation.
three methods on bci datasets are
compared in our experiments, as shown in table 1.
cyclegan is a representa-
tive unsupervised method, which doesn’t need paired images for training.
so
table 2. performance comparison on herohe challenge.
method/team auc precision recall f1-score
macaroon
0.71
0.57
0.83
0.68
mitel
0.74
0.58
0.78
0.67
piaz
0.84
0.77
0.55
0.64
dratur
0.75
0.57
0.70
0.63
irisai
0.67
0.58
0.67
0.62
proposed
0.84
0.72
0.82
0.74
464
m. lu et al.
fig.
the
region in the red box shows our mmp-mae could learn the semantic information from
the adjacent area.
our mmp-mae further improves
the performance, which achieves higher psnr by 1.60, and ssim by 0.007.
team piaz and dratur both use a multi-network ensem-
ble strategy to improve their performances.
team irisai ﬁrst segment the tumor
area and then predict the her2 status.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_52.pdf:
accurate segmentation and analysis of membranes from immuno-
histochemical (ihc) images are crucial for cancer diagnosis and prognosis.
although several fully-supervised deep learning methods for membrane seg-
mentation from ihc images have been proposed recently, the high demand for
pixel-level annotations makes this process time-consuming and labor-intensive.
to overcome this issue, we propose a novel deep framework for membrane seg-
mentation that utilizes nuclei point-level supervision.
our framework consists of
two networks: a seg-net that generates segmentation results for membranes and
nuclei, and a tran-net that transforms the segmentation into semantic points.
in
this way, the accuracy of the semantic points is closely related to the segmenta-
tion quality.
thus, the inconsistency between the semantic points and the point
annotations can be used as effective supervision for cell segmentation.
keywords: membrane segmentation · point-based supervision ·
immunohistochemical image
1
introduction
accurate quantiﬁcation of immunohistochemistry (ihc) membrane staining images is
a crucial aspect of disease assessment [14,15].
in clinical diagnosis, pathologists typ-
ically grade diseases by manually estimating the proportion of stained membrane area
however, this manual
l. cui, j. feng, w. yang and l. yang–equally contribution.
supplementary information the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43987-2_52.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
1. illustration of the full supervision (blue line) and point supervision (ours, red line) for
membranes and nuclei segmentation.
(color ﬁgure online)
despite numerous deep learning methods have been proposed for detecting cell
nuclei [11,20] from hematoxylin-eosin (h&e) staining images, little attention has
focused on analyzing cell membranes from ihc images.
currently, only a few fully
supervised ihc membrane segmentation methods have been proposed [9,19], demon-
strating the superiority of deep learning-based membrane segmentation.
in contrast, as annotating the centers of nuclei requires much fewer
efforts, weakly supervised learning has been studied for nuclei segmentation
nevertheless, how to utilize point annotations to supervise cell membrane segmentation
is still under investigation.
this study proposes a novel point-based cell membrane segmentation method,
which can signiﬁcantly reduce the cost of pixel-level annotation required in conven-
tional methods, as shown in fig.
we employ a network named seg-net to segment
the nuclei and membranes separately, followed by a trans-net to convert the segmen-
tation results into semantic points.
since the accuracy of semantic points is directly
related to the segmentation results, the segmentation quality can be implicitly super-
vised by the loss between the semantic points and the point annotations, as shown in
fig.
2
to the best of our knowledge, this is the ﬁrst study that using point-level supervision
for membrane segmentation from ihc images, which could signiﬁcantly advance future
segment membranes and nuclei via point-level supervision
541
fig.
the illustration of how point annotations are used to supervise the cell segmentation.
additionally, our method is the ﬁrst to employ point annotations
to simultaneously supervise the segmentation of two objects.
extensive experiments
conﬁrm the efﬁcacy of the proposed method, attaining performance that is comparable
to models trained with fully supervised data.
2
related works
deep learning-based segmentation methods have been widely developed for cell nuclei
segmentation from h&e images in recent years, ranging from convolutional neural
networks [5,12,24] to transformer-based architectures [8], resulting in continuously
improved accuracy in nuclei segmentation.
for the task of analyzing ihc membrane-stained images, due to the challenge of
pixel-level annotation, existing methods mostly adopt traditional unsupervised algo-
rithms, such as watershed [17,26], active contour [3,25], and color deconvolution
in recent years, a few
fully supervised cell membrane segmentation methods also have emerged [9,19], but
the high cost of data annotation limits their applicability to various membrane staining
image analysis tasks.
to reduce the annotation cost of nuclei segmentation in histopathological images,
weakly supervised segmentation training methods have received attention, including:
1) unsupervised cell nuclei segmentation methods represented by adversarial-based
methods [6,7].
however, unsupervised methods are challenged by the difﬁculty of
constraining the search space of model parameters, making it hard for the model to
handle visually complex pathology images, such as h&e or ihc; 2) weakly super-
vised cell nucleus segmentation algorithms with point annotation [16,21].
because the
cell nucleus shape in h&e images is almost elliptical, point annotation combined with
542
h. li et al.
voronoi diagram [1] were used to generate pseudo-annotations for iterative model train-
ing and reﬁnement.
although these methods can perform weakly supervised segmenta-
tion of cell nuclei from ihc membrane-stained images, they are usually ineffective in
segmenting messy cell membranes.
therefore, this paper proposes a novel point-supervised cell membrane segmenta-
tion method, addressing a major challenge in the ﬁeld.
the paper also explores the
feasibility of point supervision for the segmentation of two types of objects (cell nuclei
and cell membranes) for the ﬁrst time.
3
method
this study aims to explore how to perform membrane and nucleus segmentation in ihc
membrane-stained images using only point-level supervision.
nuclei segmentation is
performed for cell localization and counting, while membrane quantiﬁcation provides
clinical evidence for diagnosis.
fig.
the training stage employs two networks,
namely a segmentation network (seg-net) and a transition network (tran-net).
during infer-
ence, we only adopt the seg-net for segmentation.
3.1
formulation of the point-level supervised segmentation problem
given an input cell image set {ii}n
i=1, where n is the number of images in this
set, ii ∈ rh×w ×3 with h, w representating the height and width of the image,
respectively, and 3 being the number of channels of the image.
our goal is to obtain
the mask of membranes (
mi ∈ rh×w ×1) and nuclei (si ∈ rh×w ×1), that is

mi, si

= σ(fθ (ii)), where fθ is a segmentation network (seg-net) and with train-
able parameters θ, and σ is the sigmoid activation function.
we have point annotations
pi ∈ rh×w ×(c+1) for image ii, in which c is the number of semantic categories used
to describe the states of membrane staining.
in order to train fθ to segment membranes 
mi and nuclei si using point annotations
pi in image ii, we need to establish the relationship from input to segmentation, and
segment membranes and nuclei via point-level supervision
543
then to point annotation, as shown in eq.
gω transforms


mi, si

to semantic points pi ∈ rh×w ×(c+1), it should be noted that 
mi and si
respectively provide the semantic and spatial information to gω for semantic points
prediction, so that the segmentation performance is crucial for gω.
so that, by ﬁtting pi
to pi ( pi ∼= pi), the segmentation can be supervised.
this is because
gω is utilized to predict the category of semantic points, which are the center points of
cells and related to the membrane.
3.3
decouple the membranes and nuclei segmentation
our goal is to use seg-net to generate masks for both membranes (
mi ∈ rh×w ×1)
and nuclei (si
however, the two seg-net
channels are interdependent, which can result in nuclei and membranes being insepa-
rably segmented.
to overcome this issue, we enforce one channel to output the nuclei
segmentation using a supervised mask si ∈ rh×w ×1.
thus, to provide seman-
tic information to tran-net for predicting semantic points, the other channel must con-
tain information describing the staining status of the membrane, which in turn decouples
membrane segmentation.
because both si and si are single-channel, we employ the naive l1 loss to supervise
the segmentation of the nuclei, as shown in eq.
(2)
3.4
constraints for membranes segmentation
as there are no annotations available for pixel-level membrane segmentation, the
network could result in unwanted over-segmentation of membranes.
this over-
segmentation can take two forms: (1) segmentation of stained impurities, which can
restrict the network’s generalization performance by learning simple color features, and
(2) segmentation of nuclei.
the purpose of this
544
h. li et al.
loss term is to encourage the network to learn a smoother membrane segmentation that
does not capture small stained regions or nuclei.
(3)
however, relying solely on the ℓnorm
i
normalization term might lead to a trivial solu-
tion, as it only minimizes the average value of the prediction result, potentially resulting
in a minimal maximum conﬁdence for the cell membrane segmentation (e.g., 0.03).
(4), to
constrain the distribution of membrane segmentation results.
(4)
3.5
point-level supervision
using gω to detect the central point of the cells is a typical semantic points detection
task.
the difference is that the input of gω is the output of fθ rather than the image.
3.6
total loss
by leveraging the advantages of the above items, we can obtain the total loss as follows:
li = ℓnuclei
i
+ ℓnorm
i
+ ℓhinge
i
+ ℓpoints
i
,
(6)
where ℓnorm
i
and ℓhinge
i
are antagonistic, and their values are close to 0.5 in the ideal
optimal state, which can be achieved at τ
4
experiments
in order to comprehensively verify the proposed method, we conduct extensive exper-
iments on two ihc membrane-stained data sets, namely the pdl1 (programmed cell
depth 1 light 1) and her2 (human epidermal growth factor receiver 2) datasets.
the
her2 experiment is dedicated to validate segmentation performance, while the pdl1
experiment is utilized to verify the effectiveness of converting segmentation results into
clinically relevant indicators.
segment membranes and nuclei via point-level supervision
545
4.1
dataset
we collected 648 her2 and 1076 pdl1 images at 40x magniﬁcation with a resolution
of 1024 × 1024 from wsis.
pixel-level annotations are used for
testing and fully supervised experiments only.
4.2
implementation details and evaluation metric
we totally train the networks 50 epochs and employ adam optimizer
[10] with the
initial learning rate of 5×10−4 and the momentum of 0.9.
images are randomly cropped
to 512 × 512, and data augmentations such as random rotation and ﬂip were employed
during model training.
we employ the intersection over union (iou) segmentation metric and pixel-level
f1 score to validate the performance of the proposed method.
however, only point-level
annotations are equipped for the pdl1 dataset, we evaluate the segmentation perfor-
mance at the point-level by converting the segmentation into key point predictions, and
the conversion process details are available in the supplementary materials.
table 1 shows the cell segmentation results of the proposed method and
six comparison methods on the dataset her2.
our proposed method can segment both
cell nuclei and membranes simultaneously, outperforming both unsupervised methods
and other point-level methods, with an iou score of 0.5774 and an f1 score of 0.6899
for membranes, and an iou score of 0.5242 and an f1 score of 0.6795 for nuclei.
furthermore, our ablation study shows that the hinge loss and normalization loss play
important roles in improving the segmentation performance.
notably, other point-level
methods not only fail to segment the cell membranes but also have limited performance
in segmenting cell nuclei due to over-segmentation and under-segmentation errors, as
shown in fig.
we chose to compare our method with unsupervised segmentation
methods because existing point-supervised segmentation methods are unable to seg-
ment cell membranes.
among the unsupervised meth-
ods, color deconvolution [4] shows the best performance with f1 scores of 0.5984 and
0.6136 for negative and positive cells, respectively.
besides, qualitative experimental results can be found in the
supplementary materials.
546
h. li et al.
table 1. comparison of the cell segmentation results on her2 test data.
w/o: without.
supervised settings
methods
membranes
nuclei
iou
f1 score
iou
f1 score
unspervised
usar (our implementation)
, we present a novel method for precise segmentation of cell membranes and
nuclei in immunohistochemical (ihc) membrane staining images using only point-level
segment membranes and nuclei via point-level supervision
547
supervision.
our method achieves comparable performance to fully supervised pixel-
level annotation methods while signiﬁcantly reducing annotation costs, only requir-
ing one-tenth of the cost of pixel-level annotation.
this approach effectively reduces
the expenses involved in developing, deploying, and adapting ihc membrane-stained
image analysis algorithms.
in the future, we plan to further optimize the segmentation
results of cell nuclei to further boost the performance of the proposed method, and
extend it to the whole slide images (wsis).

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_46.pdf:
multiple instance learning is a powerful approach for whole slide
image-based diagnosis in the absence of pixel- or patch-level annotations.
in spite
of the huge size of whole slide images, the number of individual slides is often
rather small, leading to a small number of labeled samples.
to improve train-
ing, we propose and investigate novel data augmentation strategies for multiple
instance learning based on the idea of linear and multilinear interpolation of fea-
ture vectors within and between individual whole slide images.
based on state-
of-the-art multiple instance learning architectures and two thyroid cancer data
sets, an exhaustive study was conducted considering a range of common data
augmentation strategies.
keywords: histopathology · data augmentation · mixup · multiple instance
learning
1
motivation
whole slide imaging is capable of effectively digitizing specimen slides, showing both
the microscopic detail and the larger context, without any signiﬁcant manual effort.
due
to the enormous resolution of the whole slide images (wsis), a classiﬁcation based
on straight-forward convolutional neural network architectures is not feasible.
multi-
ple instance learning [8,10,13,18,20] (mil) represents a methodology (with a high
momentum indicated by a large number of recent publications) to deal with these huge
images corresponding to single (global) labels.
mil
approaches typically consist of a feature extraction stage, a mil pooling stage and a
following downstream classiﬁcation.
for training the feature extraction stage, clas-
sical supervised and self-supervised learning is employed
while the majority
of methods rely on separate learning stages, also end-to-end approaches have been pro-
posed [3,14].
general data augmentation strategies, such as rotations, ﬂipping, stain
augmentation and normalization and afﬁne transformations, are applicable to increase
the amount of data [15].
all of these methods are performed in the image domain.
here, we consider feature-level data augmentation directly applied to the representation
extracted using a convolutional neural network.
these methods can be easily combined
with image-based augmentation and show the advantage of a high computational efﬁ-
ciency (since operations are efﬁcient and pre-computed features can be used)
[11] proposed an augmentation strategy based on sampling the
patch-descriptors to generate several bags for an individual wsi.
this method was originally proposed as data agnostic approach
which also shows good results if applied to image data [2,4,16].
due to the structure of mil training data, we identiﬁed several options to perform
interpolation-based data augmentation.
the main contribution of this work is a set of novel data augmentation strategies for
mil, based on the interpolation of patch descriptors.
for evaluation, a large experimental study was conducted, including 2 histolog-
ical data sets, 5 deep learning conﬁgurations for mil, 3 common data augmentation
strategies and 4 mixup settings.
to obtain an improved understanding of reasons behind the
experimental results, we also investigate the feature distributions.
2
methods
in this paper, we consider mil approaches relying on separately trained feature
extraction and classiﬁcation stages [9,10,12].
the proposed augmentation methods are
applied to the patch descriptors obtained after the feature extraction stage.
this strategy
is highly efﬁcient during training since the features are only computed once (per patch)
and for augmentation only simple arithmetic operations are applied to the (smaller)
feature vectors.
image-based data augmentation strategies (such as stain-augmentation,
rotations or deformations) can be combined easily with the feature-based approaches
but require individual feature extraction during training.
however, to avoid the curse of
meta-parameters and thereby experiments these methods are not considered here.
mixup-mil: novel data augmentation for multiple instance learning
479
in the original mixup formulation of zhang et al.
the weight α is drawn from a uniform distribution between 0 and 1.
a single input (corresponding to a wsi) of a mil approach with a separate feature
extraction stage [10] can be expressed as a p-tupel x = (x1, ..., xp ) with xi being the
feature vector of an individual patch and p being the number of patches per wsi.
1. overview of the proposed feature-based data augmentation approaches.
before applying the mixup operation, the vector
tupel is randomly shufﬂed (as performed in all experiments).
this number was kept stable (1024) during all experiments.
the thereby
obtained vector tupel (x1′, ..., xp ′) ﬁnally represents the synthetic wsi-based image
descriptor.
while the intra-mixup method described before represents a linear interpola-
tion method, we also investigated a multilinear approach by computing xk ′ such that
xk ′ = α◦xi +(1−α)◦xj with α being a random vector and ◦ being the element-wise
product.
this element-wise linear (multilinear) approach enables even higher variability
in the generated samples.
2.2
experimental setting
as experimental architecture, use the dual-stream mil approach proposed by li et
al
the model makes use of an individual feature extraction stage.
due to the limited num-
ber of wsis, we did not train the feature extraction stage
speciﬁcally, we applied a resnet18 pre-trained on the image-net chal-
lenge data, due to the high performance in previous work on similar data [5]. resnet18
was assessed as particularly appropriate due to the rather low dimensional output (512
mixup-mil: novel data augmentation for multiple instance learning
481
dimensions).
we actively decided not to use a self-supervised contrastive learning app-
roach [10] as feature extraction stage since invariant features could interfere with the
effect of data augmentation.
as comparison, several other augmentation methods on feature level are investi-
gated including random sampling, selective random sampling and random noise.
in the experiments, we adjust the sample ratio
q between the patch-based features for training and testing.
the elements of r
are randomly sampled (individually for each xi) from a normal distribution n(0, σ′).
to incorporate for the fact that the feature dimensions show different magnitudes, σ′
is computed as the product of the meta parameter σ and the standard deviation of the
respective feature dimension.
this differentiation is crucial, due to the different treatment options, in particular with
respect to the extent of surgical resection of the thyroid gland [19].
the data set utilized
in the experiments consists of 80 wsis overall.
all images were acquired during clinical routine at the kardinal schwarzenberg
hospital.
procedures were approved by the ethics committee of the county of salzburg
(no. 1088/2021).
the mean and median age of patients at the date of dissection was
47 and 50 years, respectively.
the data set comprised 13 male and 27 female patients,
corresponding to a slight gender imbalance.
the images were digitized with an olympus vs120-ld100
slide loader system.
overviews at a 2x magniﬁcation were generated to manually deﬁne
scan areas, focus points were automatically deﬁned and adapted if needed.
the
image ﬁles were stored in the oympus vsi format based on lossless compression.
482
m. gadermayr et al.
inst
3/1
2/2
1/3
emb
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0.49 0.72
0.7
0.68 0.69
inst
3/1
2/2
1/3
emb
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0.41 0.79 0.81 0.79 0.78
100 %
75 %
50 %
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0.7
0.71
0.69
100 %
75 %
50 %
0.69
0.7
0.7
100 %
75 %
50 %
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0.81
0.82
0.8
100 %
75 %
50 %
0.78
0.82
0.8
100 %
75 %
50 %
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0.7
0.72
0.73
100 %
75 %
50 %
0.69
0.71
0.71
100 %
75 %
50 %
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0.81
0.81
0.82
100 %
75 %
50 %
0.78
0.82
0.83
0
25 % 50 % 75 % 100 %
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0.7
0.7
0.72 0.75 0.77
0
25 % 50 % 75 % 100 %
0.69
0.7
0.71 0.74 0.78
0
25 % 50 % 75 % 100 %
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0.81 0.77
0.8
0.79 0.79
0
25 % 50 % 75 % 100 %
0.78 0.81 0.82 0.82 0.84
intra-mixup linear
inter-mixup
selective sampling
(c) mixup feature interpolation
(b) baseline data augmentation
(a) no augmentation
random sampling
instance & embedding-
based mil
random noise
0%
50%
100%
0.69
0.7
0.71 0.68 0.67
v1
v2
v1
v2
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0.81 0.76 0.77 0.76 0.75
0%
50%
100%
v1
v2
v1
v2
0.78 0.76 0.79 0.77 0.77
0%
50%
100%
v1
v2
v1
v2
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0.7
0.7
0.68 0.68 0.68
0%
50%
100%
v1
v2
v1
v2
q
q
q
q
q
q
q
q
0
25 % 50 % 75 % 100 %
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0
25 % 50 % 75 % 100 %
0
25 % 50 % 75 % 100 %
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0
25 % 50 % 75 % 100 %
intra-mixup multilinear
dual stream (2/2)
embedding-based
dual stream (2/2)
embedding-based
0.7
0.7
0.72 0.71 0.73
0.69
0.7
0.68 0.72 0.73
0.81 0.78 0.79 0.77 0.79
0.78
0.8
0.8
0.82 0.82
frozen
section
dataset
paraﬃn
section
dataset
0
0.001
0.01
0.1
0.78
0.79
0.8
0.77
0
0.001
0.01
0.1
0.69
0.69
0.7
0.71
0
0.001
0.01
0.1
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0.7
0.71
0.7
0.71
0
0.001
0.01
0.1
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0.81
0.8
0.8
0.8
fig.
subﬁgure
(b) shows the scores obtained with baseline data augmentation for embedding-based and dual-
stream mil. subﬁgure (c) shows the scores obtained with interpolation between (inter-mixup)
mixup-mil: novel data augmentation for multiple instance learning
483
the data set was randomly separated into training (80 %) and test data (20 %).
random shufﬂing
of the vector tupels (shufﬂing within the wsis) was applied for all experiments.
for feature extraction, a resnet18 net-
work, pretrained on the image-net challenge was deployed
we use the refer-
ence implementation of the dual-stream mil approach [10]. to obtain further insight
into the feature distribution, we randomly selected patch descriptor pairs and computed
the euclidean distances.
subﬁgure (b) show the scores obtained with
baseline data augmentation for embedding-based and dual-stream mil. subﬁgure (c)
shows the scores obtained with interpolation between patches between (inter-mixup)
and within wsis (intra-mixup).
without data augmentation, scores between 0.49 and
0.72 were obtained for frozen and scores between 0.41 and 0.81 for the parafﬁn data
set.
with baseline data augmentation, scores between 0.69 and 0.73 were
achieved for the frozen and between 0.78 and 0.83 for the parafﬁn data set.
intra-
mixup showed average accuracy up to 0.78 for the frozen and up to 0.84 for the parafﬁn
data set.
4
discussion
in this work, we proposed and examined novel data augmentation strategies based on
the idea of interpolations of feature vectors in the mil setting.
the considered dual-stream approach, including an embedding
and instance-based stream, exhibited slightly improved average scores, compared to
embedding-based mil only.
with the baseline
data augmentation approaches, the maximum improvements were 0.03, and 0.02 for the
frozen, and 0.01, and 0.05 for the parafﬁn data set.
the inter-mixup approach did not
show any systematic improvements.
also a clear trend with increasing scores in the case of an increasing
ratio of augmented data (β) is visible.
in addition, the results showed that the variability between
mixup-mil: novel data augmentation for multiple instance learning
485
classes is, on patch-level, not clearly larger than the variability within a class.
we expect that stain nor-
malization methods (but not stain augmentation) could be utilized to align the different
wsis to provide a more appropriate basis for inter-wsi interpolation.
with the proposed intra-mixup augmentation strategy, this effect
diminishes, since the amount and quality of training data is increased.
to conclude, we proposed novel data augmentation strategies based on the idea
of interpolations of image descriptors in the mil setting.
based on the experimental
results, the multilinear intra-mixup setting proved to be highly effective, while the
inter-mixup method showed inferior scores compared to a state-of-the-art baseline.
in the future, additional experiments will be conducted including stain normalization
methods and larger benchmark data sets to provide further insights.
acknowledgement.
this work was partially funded by the county of salzburg (no. fhs2019-
10-kiamed)
references
1. buddhavarapu, v.g., jothi, a.a.: an experimental study on classiﬁcation of thyroid
histopathology images using transfer learning.
in: proceedings of the international con-
ference on medical image computing and computer assisted intervention (miccai), pp.
519–528 (2020)
4. dabouei, a., soleymani, s., taherkhani, f., nasrabadi, n.m.: supermix: supervising the
mixing data augmentation.
6. galdran, a., carneiro, g., ballester, m.a.g.: balanced-mixup for highly imbalanced med-
ical image classiﬁcation.
in: proceedings of the conference on medical image computing
and computer assisted intervention (miccai), pp. 323–333 (2021)
486
m. gadermayr et al.
7.
hou, l., samaras, d., kurc, t.m., gao, y., davis, j.e., saltz, j.h.: patch-based convolutional
neural network for whole slide tissue image classiﬁcation.
lerousseau, m., et al.: weakly supervised multiple instance learning histopathological tumor
segmentation.
li, b., li, y., eliceiri, k.w.: dual-stream multiple instance learning network for whole slide
image classiﬁcation with self-supervised contrastive learning.
li, z., et al.: a novel multiple instance learning framework for covid-19 severity assessment
via data augmentation and self-supervised learning.
image anal.
rymarczyk, d., borowa, a., tabor, j., zielinski, b.: kernel self-attention for weakly-
supervised image classiﬁcation using deep multiple instance learning.
shao, z., et al.: transmil: transformer based correlated multiple instance learning for
whole slide image classiﬁcation.
sharma, y., shrivastava, a., ehsan, l., moskaluk, c.a., syed, s., brown, d.: cluster-to-
conquer: a framework for end-to-end multi-instance learning for whole slide image classi-
ﬁcation.
tellez, d., et al.: quantifying the effects of data augmentation and stain color normalization
in convolutional neural networks for computational pathology.
image anal.
verma, v., et al.: manifold mixup: better representations by interpolating hidden states.
transpath: transformer-based self-supervised learning for histopathological
image classiﬁcation.
in: proceedings of the conference on medical image computing and
computer assisted intervention (miccai), pp.
zhang, h., et al.: dtfd-mil: double-tier feature distillation multiple instance learning for
histopathology whole slide image classiﬁcation.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_3.pdf:
existing deep learning models have achieved promising per-
formance in recognizing skin diseases from dermoscopic images.
therefore, it is crucial to automatically discover and
identify new semantic categories from new data.
in this paper, we propose
a new novel class discovery framework for automatically discovering new
semantic classes from dermoscopy image datasets based on the knowledge
of known classes.
speciﬁcally, we ﬁrst use contrastive learning to learn a
robust and unbiased feature representation based on all data from known
and unknown categories.
finally, we further reﬁne the pseudo label by aggregating neighborhood
information through local sample similarity to improve the clustering per-
formance of the model for unknown categories.
we conducted extensive
experiments on the dermatology dataset isic 2019, and the experimen-
tal results show that our approach can eﬀectively leverage knowledge from
known categories to discover new semantic categories.
we also further vali-
dated the eﬀectiveness of the diﬀerent modules through extensive ablation
experiments.
keywords: novel class discovery · skin lesion recognition · deep
learning
1
introduction
automatic identiﬁcation of lesions from dermoscopic images is of great impor-
tance for the diagnosis of skin cancer [16,22].
currently, deep learning mod-
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43987-2 3.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
to allevi-
ate the labeling burden, semi-supervised learning has been proposed to exploit
a large amount of unlabeled data to improve performance in the case of limited
labeled data [10,15,19].
[7,9,24], which aims to transfer knowledge from known classes to discover new
semantic classes.
most ncd methods follow a two-stage scheme: 1) a stage of
fully supervised training on known category data and 2) a stage of clustering
on unknown categories [7,9,24].
[9] further introduced
self-supervised learning in the ﬁrst stage to learn general feature representations.
[21] to further
exploit the information from known classes to improve the performance of unsu-
pervised clustering.
in
addition, they only consider the global alignment of samples to the category
center, ignoring the local inter-sample alignment thus leading to poor clustering
performance.
speciﬁcally, we ﬁrst use contrastive
learning to pretrain the model based on all data from known and unknown cat-
egories to learn a robust and general semantic feature representation.
the cross-pseudo-supervision strategy is then used to force the model to
maintain consistent prediction outputs for diﬀerent views of unlabeled images.
finally, to encourage local neighborhood alignment and further reﬁne the pseudo
26
w. feng et al.
fig.
labels, we propose a local information aggregation module to aggregate the infor-
mation of the neighborhood samples to boost the clustering performance.
we
conducted extensive experiments on the dermoscopy dataset isic 2019, and the
experimental results show that our method outperforms other state-of-the-art
comparison algorithms by a large margin.
in addition, we also validated the
eﬀectiveness of diﬀerent components through extensive ablation experiments.
2
methodology
given an unlabeled dataset {xu
i }n u
i=1 with n u images, where xu
i is the ith unla-
beled image.
in addition, we also have access to a labeled dataset {xl
i, yl
i}n l
i=1 with
n l images, where xl
i is the ith labeled image and yl
specif-
ically, we use xi and x′
i to represent diﬀerent augmented versions of the same
image in a mini-batch.
the unsupervised contrastive loss can be formulated as:
lucl
i
= − log
exp (zi · z′
i/τ)

n 1[n̸=i] exp (zi · zn/τ)
(1)
where zi = e(xi) is the deep feature representation of the image xi, e is the
feature extractor network, and τ is the temperature value.
in addition, to help the feature extractor learn semantically meaningful fea-
ture representations, we introduce supervised contrastive learning
for an original image xi, we generate two augmented
versions of xi, xv1
i
and xv2
i .
we then feed these two augmented images into m1
and m2 to obtain the predictions for xv1
i
and xv2
i :
pv1
i,1
for an input image
xi, if xi is from the known category, we construct the training target as one
hot vector, where the ﬁrst cl elements are ground truth labels and the last cu
elements are 0.
if xi is from the unknown category, we set the ﬁrst cl elements
to 0 and use pseudo labels for the remaining cu elements.
; yu
bu
	
∈ rbu×cu will assign bu unknown category samples to
cu category prototypes uniformly, i.e., each category prototype will be selected
bu/cu times on average.
speciﬁcally,
we ﬁrst compute the variance of the predicted outputs of the models for the
diﬀerent augmented images via kl-divergence:
if the variance of the model’s predictions
for diﬀerent augmented images is large, the pseudo label may be of low quality,
and vice versa.
however, it ignores the alignment between local neighborhood
samples, i.e., the samples are susceptible to interference from some irrelevant
semantic factors such as background and color.
here, we propose a local infor-
mation aggregation to enhance the alignment of local samples.
by aggregating
the information of the neighborhood samples, we are able to ensure consistency
between local samples, which further improves the clustering performance.
3
experiments
dataset.
to validate the eﬀectiveness of the proposed algorithm, we con-
duct experiments on the widely used public dermoscopy challenge dataset isic
2019
the dataset contains a total of 25,331 dermoscopic images from
eight categories: melanoma (mel), melanocytic nevus (nv), basal cell carci-
noma (bcc), actinic keratosis (ak), benign keratosis (bkl), dermatoﬁbroma
(df), vascular lesion (vasc), and squamous cell carcinoma (scc).
for task 1 and task 2, we
report the average performance of 5 runs.
implementation details.
for data augmentation, we use ran-
dom horizontal/vertical ﬂipping, color jitter, and gaussian blurring following
[7].
the batch
size in all experiments is 512.
following [9,23,24], we
report the clustering performance on the unlabeled unknown category dataset.
clustering performance of diﬀerent comparison algorithms on diﬀerent tasks.
following [2,9], we use the average clustering accuracy (acc), normalized
mutual information (nmi) and adjusted rand index (ari) to evaluate the clus-
tering performance of diﬀerent algorithms.
speciﬁcally, we ﬁrst match the clus-
tering assignment and ground truth labels by the hungarian algorithm
after
the optimal assignment is determined, we then compute each metric.
we imple-
ment all algorithms based on the pytorch framework and conduct experiments
on 8 rtx 3090 gpus.
comparison with state-of-the-art methods.
we compare our algorithms
with some state-of-the-art ncd methods, including rankstats [9], rankstats+
(rankstats with incremental learning)
table 1 shows the clustering performance of each comparison algorithm on
diﬀerent ncd tasks.
it can be seen that the clustering performance of the bench-
mark method is poor, which indicates that the model pre-trained using only the
known category data does not provide a good clustering of the unknown category.
moreover, the state-of-the-art ncd methods can improve the clustering perfor-
mance, which demonstrates the eﬀectiveness of the currently popular two-stage
solution.
compared with the best comparison algorithm uno, our method yields 5.23%
acc improvement, 3.56% nmi improvement, and 2.55% ari improvement on
task1, and 3.24% acc improvement, 1.34% nmi improvement, and 2.37% ari
improvement on task2, which shows that our method is able to provide more
reliable pseudo labels for ncd.
we performed ablation exper-
iments to verify the eﬀectiveness of each component.
it can be
observed that cl brings a signiﬁcant performance gain, which indicates that
towards novel class discovery: a study in novel skin lesions clustering
31
table 2. ablation study of each key component.
in addition, umcps also improves the clustering performance of the
model, which indicates that uniﬁed training helps to the category information
interaction.
lia further improves the clustering performance, which indicates
that local information aggregation helps to provide better pseudo labels.
finally,
our algorithm incorporates each component to achieve the best performance.
as shown in
table 3, it can be observed that both components improve the clustering perfor-
mance of the model, which indicates that scl helps the model to learn seman-
tically meaningful feature representations, while ucl makes the model learn
robust unbiased feature representations and avoid its overﬁtting to known cate-
gories.
as shown in table 3, it can
be seen that cps outperforms w/o cps, which indicates that cps encourages
the model to maintain consistent predictions for diﬀerent augmented versions
32
w. feng et al.
of the input images, and enhances the generalization performance of the model.
umcps achieves the best clustering performance, which shows its ability to use
uncertainty to alleviate the eﬀect of noisy pseudo labels and avoid causing error
accumulation.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_47.pdf:
current solutions mostly need to localize
suspicious cells and classify abnormality based on local patches, con-
cerning the fact that whole slide images of tct are extremely large.
it
thus requires many annotations of normal and abnormal cervical cells, to
supervise the training of the patch-level classiﬁer for promising perfor-
mance.
in this paper, we propose cellgan to synthesize cytopatholog-
ical images of various cervical cell types for augmenting patch-level cell
classiﬁcation.
built upon a lightweight backbone, cellgan is equipped
with a non-linear class mapping network to eﬀectively incorporate cell
type information into image generation.
we also propose the skip-layer
global context module to model the complex spatial relationship of the
cells, and attain high ﬁdelity of the synthesized images through adver-
sarial learning.
our experiments demonstrate that cellgan can produce
visually plausible tct cytopathological images for diﬀerent cell types.
we also validate the eﬀectiveness of using cellgan to greatly augment
patch-level cell classiﬁcation performance.
keywords: conditional image synthesis · generative adversarial
network · cytopathological image classiﬁcation · data augmentation
1
introduction
cervical cancer accounts for 6.6% of the total cancer deaths in females world-
wide, making it a global threat to healthcare
early cytology screening is
highly eﬀective for the prevention and timely treatment of cervical cancer
[23].
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43987-2_47.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
the nilm cells have no cyto-
logical abnormalities while the others are manifestations of cervical abnormal-
ity to a diﬀerent extent.
by observing cellular features (e.g., nucleus-cytoplasm
ratio) and judging cell types, pathologists can provide a diagnosis that is critical
to the clinical management of cervical abnormality.
after scanning whole-slide images (wsis) from tct samples, automatic
tct screening is highly desired due to the large population versus the lim-
ited number of pathologists.
as the wsi data per sample has a huge size, the
idea of identifying abnormal cells in a hierarchical manner has been proposed
and investigated by several studies using deep learning
the promising performance of cell classiﬁca-
tion at the patch level is critical, which contributes to sample-level diagnosis
after integrating outcomes from many patches in a wsi.
and
the eﬀorts in collecting reliably annotated data can hardly be negligible, which
requires high expertise due to the intrinsic diﬃculty of visually reading wsis.
to alleviate the shortage of suﬃcient data to supervise classiﬁcation, one
may adopt traditional data augmentation techniques, which yet may bring little
improvement due to scarcely expanded data diversity [26].
thus, synthesizing
cytopathological images for cervical cells is highly desired to eﬀectively augment
training data.
existing literature on pathological image synthesis has explored
the generation of histopathological images [10,28].
in cytopathological images,
on the contrary, cervical cells can be spatially isolated from each other, or are
highly squeezed and even overlapped.
the spatial relationship of individual cells
is complex, adding diversity to the image appearance of color, morphology, tex-
ture, etc. in addition, the diﬀerences between cell types are mainly related to
nuanced cellular attributes, thus requiring ﬁne granularity in modulating syn-
thesized images toward the expected cell types.
therefore, the task to synthesize
realistic cytopathological images becomes very challenging.
aiming at augmenting the performance of cervical abnormality screening, we
develop a novel conditional generative adversarial network in this paper, namely
cellgan, to synthesize cytopathological images for various cell types.
we lever-
age fastgan [16] as the backbone for the sake of training stability and compu-
tational eﬃciency.
to the best of our knowledge, our proposed cellgan is
the ﬁrst generative model with the capability to synthesize realistic cytopatho-
logical images for various cervical cell types.
the experimental results validate
the visual plausibility of cellgan synthesized images, as well as demonstrate
their data augmentation eﬀectiveness on patch-level cell classiﬁcation.
2
method
the dilemma of medical image synthesis lies in the conﬂict between the lim-
ited availability of medical image data and the high demand for data amount
to train reliable generative models.
to ensure the synthesized image quality
given relatively limited training samples, the proposed cellgan is built upon
fastgan [16] towards stabilized and fast training for few-shot image synthesis.
by working in a class-conditional manner, cellgan can explicitly control the
cervical squamous cell types in the synthesized cytopathological images, which
is critical to augment the downstream classiﬁcation task.
1, and more detailed structures of the key
components are displayed in supplementary materials.
the ﬁrst input of the class
label y, which adopts one-hot encoding, provides class-conditional information to
indicate the expected cervical cell type in the synthesized image isyn.
the second
input of the 128-dimensional latent vector z represents the remaining image
information, from which isyn is gradually expanded.
speciﬁcally, the class label y is ﬁrst projected to a class embed-
ding c via a non-linear mapping network, which is implemented using four groups
of fully connected layers and leakyrelu activations.
we set the dimensions of
class embedding c to the same as the latent vector z.
then, we pass c through
learnable aﬃne transformations, such that the class embedding is specialized
to the scaling and bias parameters controlling adaptive instance normalization
(adain)
the motivation for the design above comes from
our hypothesis that the class-conditional information mainly encodes cellular
attributes related to cell types, rather than common image appearance.
2 in supplementary materials), to better handle the diver-
sity of the spatial relationship of the cells.
in this way, the proposed
sgc module learns a global understanding of the cell-to-cell spatial relationship
and injects it into image generation via computationally eﬃcient modeling of
long-range dependency.
2.2
discriminator and adversarial training
in an adversarial training setting, the discriminator forces the generator to
faithfully match the conditional data distribution of real cervical cytopatho-
logical images, thus prompting the generator to produce visually and seman-
tically realistic images.
in particular, ﬁve resnet-like [7] down-
blocks are employed to convert the input image into an 8×8×512 feature map.
two simple decoders reconstruct downscaled and randomly cropped versions of
input images i′
crop and i′
resize from 82 and 162 feature maps, respectively.
− t (x)∥ℓ1

,
(1)
where t denotes the image processing (i.e., 1
2 downsampling and 1
4 random crop-
ping) on real image ireal, f is the processed intermediate feature map from the
cellgan: conditional cervical cell synthesis
491
discriminator dis, and dec stands for the reconstruction decoder.
this simple
self-supervised technique provides a strong regularization in forcing the discrim-
inator to extract a good image representation.
to provide more detailed feedback from the discriminator, patchgan [12]
architecture is adopted to output an 8×8 logit map by using a 1×1 convolution
on the last feature map.
by penalizing image content at the scale of patches, the
color ﬁdelity of synthesized images is guaranteed as illustrated in our ablation
study (see fig. 3).
to align the class-conditional fake and real data distributions
in the adversarial setting, the discriminator directly incorporates class labels
as additional inputs in the manner of projection discriminator [20].
the class
label is projected to a learned 512-dimensional class embedding and takes inner-
product at every spatial position of the 8 × 8 × 512 feature map.
the resulting
8×8 feature map is then added to the aforementioned 8×8 logit map, composing
the ﬁnal output of the discriminator.
combining all the loss functions above,
the total objective ltotal to train the proposed cellgan in an adversarial manner
can be expressed as:
ltotal = ladv + lrecon + λreglreg,
(2)
where λreg is empirically set to 0.01 in our experiments.
3
experimental results
3.1
dataset and experimental setup
dataset.
in this study, we collect 14,477 images with 256 × 256 pixels from
three collaborative clinical centers.
all the images are manually inspected to
contain diﬀerent cervical squamous cell types.
in total, there are 7,662 nilm,
2,275 asc-us, 2,480 lsil, 1,638 asc-h, and 422 hsil images.
all the 256×256
images with their class labels are selected as the training data.
implementation details.
[19], diﬀerentiable augmentation
[30]
and exponential-moving-average optimization
[8] is used to measure the overall
semantic realism of the synthesized images.
all the experiments are conducted
using an nvidia geforce rtx 3090 gpu with pytorch
[22].
3.2
evaluation of image synthesis quality
we compare cellgan with the state-of-the-art generative models for class-
conditional image synthesis, i.e., biggan
the quantitative comparison by fid in table 1 also
demonstrates the superiority of cellgan in synthesized image quality.
to verify the eﬀects of key components in the proposed cellgan, we conduct
an ablation study on four model settings in table 2 and fig.
the visual results of model i suﬀer from severe color distortions while the other
models do not, indicating that the patchgan-based discriminator can guarantee
color ﬁdelity by patch-level image content penalty.
3. generated images from ablation study of the following key components: (a)
patchgan architecture, (b) class mapping network, (c) sgc module.
this phenomenon
suggests that the implementation of the class mapping network facilitates more
distinguishable feature representations for diﬀerent cell types.
by comparing the
synthesized images from model iii with cellgan, it is observed that adopting
sgc modules can yield more clear cell boundaries, which demonstrates the capa-
bility of sgc module in modeling complicated cell-to-cell relationships in image
space.
3.3
evaluation of augmentation eﬀectiveness
to validate the data augmentation capacity of the proposed cellgan, we con-
duct 5-fold cross-validations on the cell classiﬁcation performances of two classi-
494
z. shen et al.
table 3.
data augmentation comparison between the proposed cellgan and other
synthesis-based methods (↑: higher is better).
[11]) using four training data settings for
comparison: (1) real data only (the baseline); (2) baseline + biggan synthe-
sized images; (3) baseline + ldm synthesized images; (4) baseline + cellgan
synthesized images.
for each cell type, we randomly select 400 real images and
divide them into 5 groups.
for diﬀerent data settings, we synthe-
size 2,000 images for each cell type using the corresponding generative method,
and add them to the training data of each fold.
random ﬂip is applied to all data settings since it is reasonable
to use traditional data augmentation techniques simultaneously in practice.
the experimental accuracy, precision, recall, and f1 score are listed in
table 3.
meanwhile, the scores of other metrics are all
improved by more than 4%, indicating that our synthesized data can signif-
icantly enhance the overall classiﬁcation performance.
thanks to the visually
plausible and semantically realistic synthesized data, cellgan is conducive to
the improvement of cell classiﬁcation, thus serving as an eﬃcient tool for aug-
menting automatic abnormal cervical cell screening.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_53.pdf:
the commonly presented histology stain variation may mod-
erately obstruct the diagnosis of human experts, but can considerably
downgrade the reliability of deep learning models in various diagnostic
tasks.
many stain style transfer methods have been proposed to elimi-
nate the variance of stain styles across diﬀerent medical institutions or
even diﬀerent batches.
in this paper, we make the ﬁrst attempt at a dif-
fusion probabilistic model to cope with the indispensable stain style
transfer in histology image context, called staindiff.
speciﬁcally, our
diﬀusion framework enables learning from unpaired images by proposing
a novel cycle-consistent constraint, whereas existing diﬀusion models are
restricted to image generation or fully supervised pixel-to-pixel transla-
tion.
moreover, given the stochastic nature of staindiff that multiple
transferred results can be generated from one input histology image,
we further boost and stabilize the performance by the proposal of a
novel self-ensemble scheme.
our model can avoid the challenging issues
in mainstream networks, such as the mode collapses in gans or align-
ment between posterior distributions in aes.
in conclusion, staindiff
suﬃces to increase the stain style transfer quality, where the training is
straightforward and the model is simpliﬁed for real-world clinical deploy-
ment.
speciﬁcally, with dyes such as hematoxylin and eosin, transparent tissue ele-
ments can be transformed into distinguishable features [1].
yet, the stain variations
can cause inconsistencies between human domain experts [11]; and also hinder
the performance of computer-aided diagnostic (cad) systems
moreover,
experiments have shown that stain variations can lead to a signiﬁcant decrease in
the accuracy and reproducibility of deep learning algorithms in histology analy-
sis.
while the conventional color matching [22] and stain
separation methods [19] used to be popular; learning-based approaches have
become increasingly dominant, because they eliminate the need for challenging
manual selection of the template images.
another approach, called staingan [26], improves on stst by
tailoring a cyclegan [34] to get rid of the dependence on learning from paired
histology images and enable an unsupervised learning manner.
however, gan approaches and ae
suﬀer from the training of extra discriminators and challenging alignment of the
posterior distributions, respectively [27].
[9], have emerged
as an alternative approach that can achieve competitive performance in various
staindiﬀ: stain style transfer with diﬀusion
551
image-related tasks, such as image generation, inpainting, super-resolution, and
etc
importantly, diﬀusion models oﬀer several advantages over gans and
aes, including tractable probabilistic parameterization, stable training proce-
dures, and theoretical guarantees [3].
additionally, they can avoid some of the
challenges encountered by gans and aes, such as the alignment of posterior
distributions or training extra discriminators, leading to a simpler model and
training process.
while the current diﬀusion models focus on
image synthesis [9] or supervised image-to-image transaction
therefore, we design an
innovative cycle-consistent diﬀusion model that allows the transfer of representa-
tions between latent spaces at diﬀerent time steps with the same morphological
structure preserved in an unsupervised manner, as shown in fig. 1(c).
more innovatively, unlike existing diﬀusion
models, staindiff is capable of learning from unpaired histology images, mak-
ing it a more ﬂexible and practical solution.
the model is superior to gan-based
methods as the training of additional discriminators is free, and also spares for
the diﬃculty in the alignment of posterior probabilities in ae-based approaches.
(2) we also propose a self-ensemble scheme to further improve and stabilize the
style transfer performance in staindiff.
this scheme utilizes the stochastic
property of the diﬀusion model to generate multiple slightly diﬀerent outputs
from one input at the inference stage.
(3) a broad range of histology tasks, such
as stain normalization between multiple clients, can be conveniently achieved
with minor adjustment to the loss in staindiff.
[9] to transfer the
stain style between two domains, i.e., x a, x b. however, the traditional training
paradigm of conditional ddpms with paired images (xa
0 , xb
0 )
to overcome this
limitation, we design an innovative diﬀusion framework for stain style transfer,
named staindiff, which leverages the success of cyclegan [34] and style-
gan [26] and thus can be trained in an unsupervised manner with a novel
cycle-consistency constraint.
speciﬁcally, staindiff comprises two forward pro-
cesses that perturb the histology image of two stain style domains to noise respec-
tively, and two corresponding reverse processes that attempt to reconstruct noise
back to original images from the perturbed ones.
it comprises two diﬀusion paths, that each learns the histology image gen-
eration with respect to one stain style domain.
parameterized by the markov chain, the forward process
in staindiff follows the vanilla ddpm by perturbing the histology images
gradually with gaussian noise, until all structures and morphological context
information are lost.
formally, given a histology image xa
0 with respect to the
stain style domain a, a transition kernel q progressively generates a sequence of
t latent variables xa
1 , xa
2 , · · · , xa
t thorough the following equation:
q(xa
t |xa
t−1) = n(xa
t ;

1 − βtxa
t−1, βti),
(1)
where n(·) denotes the gaussian distribution, i is the identity matrix.
identically, we can progress the
latent variables xb
1 , xb
2 , · · · , xb
t for the histology image xb
0 from the stain style
domain b in the same fashion as eq.
(1) and generate images characterized by stain style a and b respec-
tively, by gradually removing the noise initialized from gaussian prior.
[4] is leveraged to train
the denoising networks in the transition kernels.
due to the absence of pixel-to-
pixel paired histology of both stain styles, it is infeasible to learn the interplay
between them in a supervised manner as in most previous works.
the directed graphical model for the inference stage with self-ensemble scheme
of the staindiff.
we describe the inference stage of
staindiff by transferring the histology images from stain style a to b; while
the inverse, namely from b to a, is similar.
given a histology image input xa
0
characterized by stain style a, we begin by perturbing it s steps with eq.
next, we use the p-sample [9] iteratively to denoise
the ˜xb
s and obtain the transferred image ˜xb
0 .
meanwhile, in some clinical settings, multiple institu-
tions or hospitals are involved, where stain normalization is usually employed for
multiple stain styles to one style alignment.
this modiﬁcation allows us
to use staindiff for stain normalization without any other adjustments to the
inference process.
3
experiments
datasets.
this dataset aims to measure the
style transfer performance on 284 histology frames.
meanwhile, 500 paired patches are generated from the remaining 100 slides as the
test set, where we use pearson correlation coeﬃcient (pc), structural similarity
index (ssim) [31] and feature similarity index for image quality assessment
(fsim)
this dataset evaluates the performance of stain normalization quanti-
ﬁed by the downstream nine-category tissue structure classiﬁcation accuracy
[14].
implementations.
all experiments are implemented in python 3.8.13 with
pytorch 1.12.1 on two nvidia geforce rtx 3090 gpu cards with 24gib of
1 https://mitos-atypia-14.grand-challenge.org.
staindiﬀ: stain style transfer with diﬀusion
555
table 1. comparison of stain style transfer performance on dataset-a. to show the
statistical signiﬁcance, the p-values in terms of ssim and fsim are computed with
respect to staindiff (full setting).
the ‘w/o se’ denotes the exclusion of the self-
ensemble scheme from the inference stage.
we leverage the adam optimizer with a learning rate of
2 × 10−4, and a batch size of 4.
the learning scheme follows previous work [18],
where the training process continues for 100 epochs if the overall loss did not
decrease to the average loss of the previous 20 epochs.
all experiments are repeated for 7 runs with diﬀerent ﬁxed random seeds
i.e., {0, 1, 2, 3, 4, 5, 6}; and metrics are reported in the form of mean±standard
deviation.
‘w/o
se’ denotes excluding the self-ensemble scheme from the inference stage.
moreover, the statistical signiﬁcance of our
performance boost is validated by the p-values that are consistently smaller than
0.005, as computed from the wilcoxon signed-rank test.
table 2 presents the comparison
results of the downstream classiﬁcation task, where the histology images in
dataset-b are normalized using diﬀerent methods.
table 1 and 2 show that incorporating a self-ensemble scheme
can both boost the performance of staindiff, and bring down the variations,
demonstrating its eﬀectiveness in stabilizing the stain transfer and normaliza-
tion.
to further investigate the eﬀect of ensemble number m, we conduct ablation
on dataset-a. experimentally, the fsim when m = 1, 5, 10, 15, 20, 50 are 0.742,
0.749, 0.753, 0.756, 0.759, 0.759 respectively.
while a slight performance gain
can be achieved with higher m values than 10, the ensemble becomes more time-
consuming, as the cost time is linear to m. it implies an optimal m should be
selected as a trade-oﬀ between the performance and computational time, such
as 10 in this work.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_23.pdf:
vulvovaginal candidiasis (vvc) is the most prevalent human
candidal infection, estimated to aﬄict approximately 75% of all women
at least once in their lifetime.
automatic whole slide image
(wsi) classiﬁcation is highly demanded, for the huge burden of dis-
ease control and prevention.
our experimental results
demonstrate that our framework achieves state-of-the-art performance.
keywords: whole slide image · vulvovaginal candidiasis ·
attention-guided
1
introduction
vulvovaginal candidiasis (vvc) is a type of fungal infection caused by candida,
which results in discomforting symptoms, including itching and burning in the
genital area [4,18].
it is the most prevalent human candidal infection, estimated
to aﬄict approximately 75% of all women at least once in their lifetime [1,20],
resulting in huge consumption of medical resources.
manual reading upon whole slide image (wsi) of tct is time-consuming and
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
1. examples of wsis (usually about 20000 × 20000 pixels), a cropped image of
1024 × 1024 pixels from the wsi, and zoom-in views of candida and its position (indi-
cated by the red arrows and annotation).
for example, momenzadeh et al.
[11] implemented
automatic diagnosis based on machine learning.
many systems of automatic computer-aided wsi screening have been designed
for cytopathology [22,23], and histopathology
(2) in addition to occupying only
a small image space for each candida, the overall candida quantity in wsis is
also low compared to the number of other cells.
all
of the above issues make it diﬃcult for diagnostic models to focus on candida,
thus resulting in poor classiﬁcation performance and generalization capability.
in this paper, we ﬁnd that the attention for a deep network to focus on can-
dida is the key to the high performance of the screening task.
our
contributions are summarized into three parts: (1) we use a detection task to
pre-train the encoder of the classiﬁcation model, moving the network’s attention
away from individual cells and onto candida-like objects; (2) we propose skip
self-attention (ssa) to take into account multi-scale semantic and texture fea-
progressive attention guidance
235
fig.
2. given a wsi, we
ﬁrst crop it into multiple images, each of which is sized 1024×1024.
for each
cropped image, we conduct image-level classiﬁcation to ﬁnd out whether it suﬀers
from suspicious candida infection.
the image-level classiﬁer produces a score and
feature representation of the image under consideration.
then scores and features
from all cropped images are reorganized and aggregated by a transformer for ﬁnal
classiﬁcation by a fully connected (fc) layer.
2.1
detection task for pre-training
we use a pre-trained detection model as prior to initialize the classiﬁcation
model.
in experimental exploration, we ﬁnd that, if we train the detection net-
work directly, the bounding-box annotation indicates the location of candida
and can rapidly establish a rough understanding of the morphology of candida.
meanwhile, directly training a classiﬁcation model is usually easier to converge.
however, in such a task, as candida occupies only a few pixels in an image, it is
diﬃcult for the classiﬁer to focus on the target.
that, the attention of the clas-
siﬁer may spread across the entire image, leading to overﬁtted training quickly.
therefore, we argue that the detection and classiﬁcation tasks are comple-
mentary to solve our problem.
particularly, we pre-train a detector and inherit
its advantages in the classiﬁer.
3. attention guided image-level classiﬁcation (corresponding to the classiﬁcation
model in fig. 2).
note that pre-training not only discards the complex posi-
tioning task but also makes it easier for the classiﬁcation network to converge
especially in the early stage of training.
2.2
transformer with skip self-attention (ssa)
we design a novel skip self-attention (ssa) module to fuse discriminative fea-
tures of candida from diﬀerent scales.
at a coarse-grained level, there is the phenomenon that a candida
usually links multiple host cells and yields a string of them.
cnn-based methods have achieved excellent performance in computer-aided
diagnosis including cervical cancer
in recent years, vision transformer (vit)
has been widely used in visual tasks for its global attention mechanism [14],
sensitivity to shape information in images [19], and robustness to occlusion [12].
nevertheless, such a transformer can be hard to train for our task, due to the
large image size, huge network parameters, and huge demand for training data.
progressive attention guidance
237
speciﬁcally, we use the pre-trained cnn-based encoder to extract features
for each cropped image.
on the contrary,
the feature maps extracted from the last layer are high-level, which represents
semantics regarding candida.
2.3
contrastive learning
as mentioned in sect.
our approach has two key goals: (1) to ensure that the features from the origi-
nal image remain consistent after undergoing various image augmentations, and
(2) to construct an image without the region of candida, resulting in highly
dissimilar features compared to the original.
inspired by a weakly supervised learning segmentation method
3.
to achieve this, we use augmentation and the attention map generated during
the training process to construct three types of images and apply contrastive
learning to the features extracted from them.
for a given image i, we use image
augmentation to generate iaug and use the encoder attached with ssa to extract
feature, f c
aug.
we get the masked image imasked by subtracting m from i.
m
3, the features f, faug and fmasked from the three types
of images i, iaug and imasked by the shared classiﬁer.
in our task, we hope
that the style gap does not aﬀect the feature extraction of the image, so the
distance between faug and f should be attracted.
if our network has eﬀective attention, the masked image should not contain
any candida, so the score of the candida category after the mask s(imasked)
should be minimized.
otherwise, attention
maps that cover the whole image can also result in low ltri.
we take the average
grayscale of attention map ¯
m as a restriction, as shown in the last part of eq. 2.
ltri,lam, and lfocus are combined as lcl to constrain each other and take full
advantage of contrastive learning, as shown in eq.
(3)
2.4
aggregated classiﬁcation for wsi
with the strategies above, we have built the classiﬁer for all cropped images in a
wsi.
speciﬁcally, for each cropped image, we conduct image-level
classiﬁcation to ﬁnd out whether it suﬀers from suspicious candida infection.
the image-level classiﬁcation also produces a score, as well as the feature repre-
sentation of the image under consideration.
then, we reorganize features from
all images by ranking their scores and preserving that with top-k scores.
3
experimental results
datasets and experimental setup.
each sample is scanned into a wsi following
standard cytology protocol, which can be further cropped to 500 images sized
1024×1024.
for pre-training the detector, we prepare 1467 images with the size of
1024×1024 pixels, all of which have bounding-box annotations.
the ratio of
training and validation is 4:1.
for training of the image-level classiﬁcation model, we use 1940 positive
images (1467 of which are used in detector pre-training) and 2093 negative
images.
all images used to pre-train the detector are categorized as training
data here.
the rest 473 images are split in 5-fold cross-validation, from which
we collect experimental results and report later.
progressive attention guidance
239
table 1. image-level classiﬁcation results and ablation study on the three contributions
of our method.
for implementation details, the models are implemented by pytorch and
trained on 4 nvidia tesla v100s gpus.
the batch sizes of the
detection task, image-level classiﬁcation, and wsi-level classiﬁcation are 8, 8, 16,
respectively.
to aggregate wsi classiﬁcation, we use top-10 cropped images and
their features.
we report the performance using ﬁve common metrics: area under
the receiver operating characteristic curve (auc), accuracy (acc), sensitivity
(sen), speciﬁcity (spe), and f1-score.
comparisons for image-level classiﬁcation.
we conduct an ablation
study to evaluate the contribution of pre-training (pt), skip self-attention (ssa),
and contrastive learning (cl) for the image-level classiﬁcation, as shown in
table 1.
pt
shows improvement in all situations, as a reasonable initial focus provides a solid
foundation.
ssa and cl can bring 2.89% and 6.38% improvement respectively
compared to the method without each of them.
it shows that ssa and cl can
perform better when the model already has the basic ability to localize candida,
i.e., after pt.
to verify whether our model focuses on important regions of the input image
for accurate classiﬁcation, we visualize the model’s attention using grad-cam
[16].
4. (a) the original image, where the green box indicates candida (enlarged in
the left) and the red box shows the prediction of the detection model.
to save
computation, we did not verify the performance of the methods that performed
too poorly on dataset-small.
the detection-based method [23] uses a detection
network to get suspicious candida and classify wsis with average conﬁdence.
resnet trained without our method is the same as the baseline in table 1.
we both considered the original trans-
mil with pre-trained resnet-50 and the modiﬁed version with our image-level
encoder.
method
dataset-small
dataset-large
image-level wsi-level
auc
acc
sen
auc
acc
sen
detection
threshold
88.57 ± 9.56
80.00 ± 10.0
79.03 ± 14.4
\
\
\
resnet
threshold
88.75 ± 6.58
77.50 ± 9.35
82.17 ± 13.0
\
\
\
resnet
transmil
93.85 ± 3.71
89.50 ± 3.67
87.99 ± 7.55 80.46
86.33
67.85
ours
threshold
92.50 ± 5.36
86.00 ± 6.44
83.04 ± 8.71
82.59
88.14 64.29
ours
mlp
94.36 ± 3.50
91.00 ± 6.44
85.11 ± 13.16
83.40
87.32
67.86
ours
transmil
95.35 ± 1.48
91.00 ± 3.21
85.74 ± 4.25
81.19
86.78
62.86
ours
transformer 95.78 ± 2.25 91.64 ± 3.17 85.55 ± 5.91
84.18 87.67
68.57
progressive attention guidance
241
and is the most stable.
our attention-based method brings 6% improvement of
accuracy on data-small compared to other methods with the same wsi-level
method ’threshold’.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_37.pdf:
colorectal cancer is a prevalent form of cancer, and many
patients develop colorectal cancer liver metastasis (crlm) as a result.
these scans form unique ﬁve-
dimensional data (time, phase, and axial, sagittal, and coronal planes
in 3d ct).
most of the existing deep learning models can readily han-
dle four-dimensional data (e.g., time-series 3d ct images) and it is not
clear how well they can be extended to handle the additional dimension
of phase.
our
experimental results show that a multi-plane architecture based on 3d
bi-directional lstm, which we call mpbd-lstm, works best, achieving
an area under curve (auc) of 0.79.
on the other hand, analysis of the
results shows that there is still great room for further improvement.
keywords: colorectal cancer liver metastasis · liver cancer
prediction · contrast-enhanced ct scan · bi-directional lstm
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43987-2 37.
in addition to the axial, sagit-
tal, and coronal planes in 3d ct scans, the data comprises contrast-enhanced
multiple phases as its 4th dimension, along with diﬀerent timestamps as its 5th
dimension.
radiologists heavily rely on this data to detect the crlm in the
very early stage [15].
extensive existing works have demonstrated the power of deep learning on
various spatial-temporal data, and can potentially be applied towards the prob-
lem of crlm.
however, all these methods have only demonstrated their eﬀectiveness
towards 3d/4d data (i.e., time-series 2d/3d images), and it is not clear how
to best extend them to work with the 5d cect data.
[12]
shows uni-directional lstm works well on natural videos while several other
works show bi-directional lstm is needed in certain medical image segmenta-
tion tasks [2,7].
1. representative slices from 3d ct images of diﬀerent patients in our dataset,
at a/v phases and timestamps t0, t1, t2 (cropped to 256 × 256 for better view).
382
x. li et al.
– patients were previously diagnosed with colorectal cancer tnm stage i to
stage iii, and recovered from colorectal radical surgery.
– we already determined whether or not the patients had liver metastases
within 2 years after the surgery, and manually labeled the dataset based
on this.
– no potential focal infection in the liver before the colorectal radical surgery.
– no metastases in other organs before the liver metastases.
– no other malignant tumors.
ct images are collected with the following
acquisition parameters: window width 150, window level 50, radiation dose 120
kv, slice thickness 1 mm, and slice gap 0.8 mm.
all images underwent manual
quality control to exclude any scans with noticeable artifacts or blurriness and
to verify the completeness of all slices.
additional statistics on our dataset are
presented in table 1 and examples of representative images are shown in fig.
μ is the average function.
[13,14], uses spatiotemporal lstm
(st-lstm) by stacking multiple convlstm units and connecting them in
a zigzag pattern to handle spatiotemporal data of 4 dimensions.
4) simvp
additionally, inspired by the bi-directional lstm used in medical
image segmentation task
after the computation of the 3d-lstm
modules in each plane, we use an average function to combine the output hidden
states from both planes.
an alternative approach is to additionally connect two planes by combining
the hidden states of 3d-lstm modules and taking their average if a module
receives two inputs.
however, we found that such design actually resulted in a
worse performance.
3
experiments
3.1
data augmentation and selection
we selected 170 patients who underwent three or more cect scans from our
original dataset, and cropped the images to only include the liver area, as shown
in fig.
for data augmentation, we randomly rotated the images
from −30◦ to 30◦ and employed mixup [17].
we applied the same augmentation
technique consistently to all phases and timestamps of each patient’s data.
for
each slice, the dimension was 256 × 256 after cropping.
the dimension of our ﬁnal input is (3 × 2 × 64 × 64 × 64), representing
mpbd-lstm
385
(t × p × d
h × w), where t is the number of timestamps, p is the number
of diﬀerent phases, d is the slice depth, h is the height, and w is the width.
3.2
experiment setup
as the data size is limited, 10-fold cross-validation is adopted, and the ratio of
training and testing dataset is 0.9 and 0.1, respectively.
as this is a classiﬁcation
task, we evaluate all models’ performance by their auc scores.
additional
data on accuracy, sensitivity speciﬁcity, etc. can be found in the supplementary
material.
furthermore, predrnn-v2 [14], which passes memory ﬂow
in a zigzag manner of bi-directional hierarchies, outperforms the uni-directional
lstm-based saconvlstm
this produced a one-dimensional
bi-directional lstm (fig. 2(a), without the gray plane) with an input dimension
of 3×128×64×64, which is the same as we used on other models.
by replacing the bi-directional connection with
a uni-directional connection, the mpbd-lstm model’s performance decreased
to 0.768 on the original dataset.
this result indicates that the bi-directional
connection is crucial for computing temporal information eﬀectively, and its
inclusion is essential for achieving high performance in mpbd-lstm.
also, as mentioned previously, we initially connected the 3d-lstm mod-
ules in two planes with their hidden states.
therefore, we removed the inter-plane connections in the early stage, since their
hidden states are still added together and averaged after they are processed by
the lstm layers.
we conducted ablation stud-
ies using ct images from diﬀerent timestamps and phases to evaluate the
eﬀectiveness of time-series data and multi-phase data.
the results, as shown
in table 4, indicate that mpbd-lstm achieves auc scores of 0.660, 0.676, and
0.709 if only images from timestamps t0, t1, and t2 are used, respectively.
these scores suggest that predicting crlm at earlier stages is more challenging
since the features about potential metastases in ct images get more signiﬁcant
over time.
however, all of these scores are signiﬁcantly lower than the result
using ct images from all timestamps.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_26.pdf:
basal cell carcinoma (bcc) is a prevalent and increasingly
diagnosed form of skin cancer that can beneﬁt from automated whole
slide image (wsi) analysis.
however, traditional methods that utilize
popular network structures designed for natural images, such as the
imagenet dataset, may result in reduced accuracy due to the signiﬁ-
cant diﬀerences between natural and pathology images.
in this paper,
we analyze skin cancer images using the optimal network obtained by
neural architecture search (nas) on the skin cancer dataset.
furthermore, unlike traditional unilaterally aug-
mented (ua) methods, the proposed supernet skin-cancer net (sc-net)
considers the fairness of training and alleviates the eﬀects of evalua-
tion bias.
we use the sc-net to fairly treat all the architectures in the
search space and leveraged evolutionary search to obtain the optimal
architecture for a skin cancer dataset.
our experiments involve 277,000
patches split from 194 slides.
keywords: basal cell carcinoma (bcc) · whole-slide pathological
images · deep learning · neural architecture search (nas)
1
introduction
skin cancer, the most prevalent cancer globally, has seen increasing incidences
over recent decades
timely bcc diagnosis is crucial to avoid complex treatments.
although his-
tological evaluation remains the gold standard for detection [3], deep learning
and computer vision advancements can streamline this process.
scanned tradi-
tional histology slides result in whole slide images (wsis) that can be analyzed
by deep learning models, signiﬁcantly easing the histological evaluation burden.
recent advancements underscore the promise of this approach [4–6].
fig.
[7–9] typically employs models like
inception net and resnet, designed for natural images like those in the imagenet
dataset.
the signiﬁcant variance in pathology and natural images can compro-
mise these models’ accuracy.
however, current nas methods often overlook fairness in architecture
ranking, impeding the discovery of top-performing models.
we observed that conventional nas methods
often overlook fairness ranking during the search, hindering the search for opti-
mal solutions.
the eﬃcacy of sc-net was conﬁrmed by our experimental
results, with our resnet50 achieving 96.2% top-1 accuracy and 96.5% auc,
outperforming baseline methods by 4.8% and 4.7% respectively.
mod-
ule (a) extracts the region of interest (roi) from wsi and generates patches,
detection of basal cell carcinoma in whole slide images
265
while module (b) uses optimal model architecture from nas to analyze features
from patches and generate classiﬁcations.
fig.
a balanced evolutionary algorithm
is then used to select the optimal structure from the search space, with the
candidate structures’ performance evaluated using mini-batch patch data.
the search
leverages a supernet s with weights w, with each path γ inheriting weights from
w. this makes one-shot nas a two-step optimization process: supernet training
and architecture search.
et al.
network width d∗ corresponds to the network width with the best performance
(e.g. classiﬁcation accuracy) on the validation dataset, i.e.,
d∗ = arg max
d∈d
acc(d, w∗
d; w∗, s, dv), s.t. flops(d) ≤ fp,
(2)
where fp is the resource budget of flops.
the search for eq. 2 can be eﬃciently
performed by various algorithms, such as random or evolutionary search [18].
afterward, the performance of the searched optimal width d∗ is analyzed by
training from scratch.
fig.
in the
ua principle, some channels are trained twice while others are trained only once or
not at all, leading to channel training unfairness and evaluation bias.
in contrast, our
proposed method ensures that all channels are trained evenly (twice) by training both
the width d and its complementary width.
[19–21] often employ a uni-
laterally augmented (ua) principle to evaluate each width, resulting in unfair
training of channels in the supernet.
3(a), to search for a
dimension d at a layer with a maximum of n channels, the ua principle assigns
the left d channels in the supernet to indicate the corresponding architecture as
γa (d) =
the unfairness can be quantiﬁed
detection of basal cell carcinoma in whole slide images
267
fig.
this introduces evaluation bias and leads to sub-optimal
results.
to mitigate evaluation bias on width, we propose a new sc-net that pro-
motes the fairness of channels during training.
+ 1) : (n − d)],
(6)
where ⊎ represents the union of two lists with repeatable elements.
+ 1
(8)
therefore, the training degree t for each channel will always be equal to the
same constant value of the width, independent of the channel index, ensuring
fairness in terms of channel (ﬁlter) levels.
however, the search
space involved in nas is large, with more than 1020 possible architectures,
requiring an evolutionary search using the multi-objective nsga-ii algorithm
to improve the search performance.
during the evolutionary search, the width d
of each network is represented by the average precision of its corresponding left
and right paths in the supernet s, as shown in eq.
the optimal width (not
subnetwork) is determined as the one that achieves the best performance when
trained from scratch.
acc(w, d, dv) = 1
2 (acc(sl, d; dv) + acc(sr, d; dv))
(9)
3
experiments
3.1
experiment settings
table 1.
generated dataset split
bcc-positive bcc-negative
training wsi patch
wsi patch
118
132,981 37
90,291
testing
wsi patch
wsi patch
30
31,651
9
22,838
the dataset, comprised of 194 skin slides acquired from the southern sun pathol-
ogy laboratory, includes 148 bcc cases and 46 other types (common nevus,
detection of basal cell carcinoma in whole slide images
269
scc), all manually annotated by a dermatopathologist.
the experimental setup involved training models
on two nvidia rtx a6000 gpus using pytorch.
training used the adam optimizer with a
dynamic learning rate reduction strategy, starting with a learning rate of 5e-5
following a cosine schedule.
table 2. performance comparison on skin cancer dataset
type
model
flops
parameters
acc
se
sp
f1
auc
wsi analysis-related
tian et al.
[24]
4.1g
25.5m
93.8
92.9
92.2
93.0
93.0
ours
ori resnet50
4.1g
25.5m
91.4
90.2
90.4
90.5
91.8
s resnet50
4.1g
27.2m
96.2
94.7
95.8
95.2
96.5
ori mobilenetv2
300m
3.5m
86.7
86.2
85.8
86.2
87.3
s mobilenetv2
300m
4.0m
91.9
90.4
91.7
90.7
92.4
3.2
performance evaluation
we validated our algorithm using the curated skin cancer dataset and sc-net
as a supernet, testing both heavy and light models.
to ensure a fair comparison on our
dataset, we selected several papers in the ﬁeld of pathological image analysis,
such as [9,22,23], as well as others using the ua principle, such as [18,24].
evaluation metrics.
our model was evaluated on: (1) accuracy (acc): per-
centage of correct classiﬁcations.
(4)
f1 score (f1): precision and recall’s harmonic mean, indicating label alignment.
(5) auc: roc curve area, reﬂecting the false/true positive rate trade-oﬀ.
as shown in table 2, the s resnet50 model outperformed in all metrics,
showing 4.8%, 4.5%, 5.4%, 4.7% and 4.7% improvements in accuracy, sensitivity,
speciﬁcity, f1 score, and auc, respectively, over ori resnet50, and surpassing
270
h. xu et al.
table 3. performance of searched models with diﬀerent searching methods.
table 3 presents our experiments testing the
sc-net on resnet50 and mobilenetv2 using various supernets and search meth-
ods.
these results high-
light sc-net’s eﬀectiveness as a supernet in bolstering evaluation and search
performance.
as shown in table 4, our s resnet50 surpassed the original
resnet50 on all datasets, gaining 2.3% and 1.8% more auc on chestmnist
and dermamnist respectively, proving the model’s robust generalization.
detection of basal cell carcinoma in whole slide images
271
table 4. performance comparison on chestmnist and dermamnist datasets
model
chestmnist dermamnist
auc acc
auc acc
resnet18
76.8
94.7
91.7
73.5
resnet50
76.9
94.7
91.3
73.5
auto-sklearn
64.9
77.9
90.2
71.9
autokeras
74.2
93.7
91.5
74.9
google automl 77.8
94.8
91.4
76.8
s resnet50
79.2
95.5
93.1
77.8
4

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_19.pdf:
however, existing methods suffer from the over-smoothing problem for their
commonly used l1 or l2 loss with posterior average calculations.
to ensure the accuracy of the
prediction, we further design a structure encoder to extract anatomical information
from patient anatomy images and enable the noise predictor to be aware of the
dose constraints within several essential organs, i.e., the planning target volume
and organs at risk.
extensive experiments on an in-house dataset with 130 rectum
cancer patients demonstrate the superiority of our method.
keywords: radiotherapy treatment · dose prediction · diffusion model ·
deep
learning
1
introduction
radiotherapy, one of the mainstream treatments for cancer patients, has gained notable
advancements in past decades.
for promising curative effect, a high-quality radiotherapy
plan is demanded to distribute sufﬁcient dose of radiation to the planning target volume
(ptv) while minimizing the radiation hazard to organs at risk (oars).
to achieve this,
radiotherapy plans need to be manually adjusted by the dosimetrists in a trial-and-error
manner, which is extremely labor-intensive and time-consuming [1, 2].
additionally, the
quality of treatment plans might be variable among radiologists due to their different
expertise and experience [3].
https://doi.org/10.1007/978-3-031-43987-2_19
192
z. feng et al.
recently, the blossom of deep learning (dl) has promoted the automatic medi-
cal image processing tasks [4–6], especially for dose prediction [7–14].
[10] utilized a progressive reﬁnement unet (prunet) to
reﬁne the predictions from low resolution to high resolution.
[16] to excavate con-
textual information from different scales, thus obtaining accuracy improvements in the
dose prediction of rectum cancer.
[13] designed a multi-organ constraint loss to enforce the deep
model to better consider the dose requirements of different organs.
[8] utilized isodose line and gradient information to
promote the performance of dose prediction of rectum cancer.
[17] constructed an additional segmentation task
to provide the dose prediction task with essential anatomical knowledge.
although the above methods have achieved good performance in predicting dose
distribution, they suffer from the over-smoothing problem.
[17, 18], leading to the over-smoothed predicted images without important
high-frequency details [19].
consequently, exploring an automatic
method to generate high-quality predictions with rich high-frequency information is
important to improve the performance of dose prediction.
fig.
[20] has veriﬁed its remarkable potential in modeling
complex image distributions in some vision tasks [21–23].
unlike other dl models, the
diffusion model is trained without any extra assumption about target data distribution,
thus evading the average effect and alleviating the over-smoothing problem [24]. figure 1
(4) provides an example in which the diffusion-based model predicts a dose map with
shaper and clearer boundaries of ray-penetrated areas.
to further ensure the accuracy of the predicted dose distribution
for both the ptv and oars, we design a dl-based structure encoder to extract the
anatomical information from the ct image and the segmentation masks of the ptv and
oars.
(2) we introduce a structure encoder to extract the anatomical information
available in the ct images and organ segmentation masks, and exploit the anatomical
information to guide the noise predictor in the diffusion model towards generating more
precisepredictions.(3)theproposeddiffdpisextensivelyevaluatedonaclinicaldataset
consisting of 130 rectum cancer patients, and the results demonstrate that our approach
outperforms other state-of-the-art methods.
an image set of cancer patient
is deﬁned as {x, y}, where x ∈ rh×w×(2+o) represents the structure images, “2” signiﬁes
the ct image and the segmentation mask of the ptv, and o denotes the total number
of segmentation mask of oars.
meanwhile, y ∈ rh×w×1 is the corresponding dose
distribution map for x. concretely, the forward process produces a sequence of noisy
images {y0, y1, . .
to obtain the anatomic information, a structure encoder
g is designed to extract the crucial feature representations from the structure images.
in the forward process, the diffdp model employs the markov chain
to progressively add noise to the initial dose distribution map y0 ∼ q(y0) until the
ﬁnal disturbed image yt becomes completely gaussian noise which is represented as
yt ∼ n(yt | 0, i).
based on this, we can directly obtain
the distribution of yt at any step t from y0 through the following formula:
q(yt | y0) = n

yt; √γty0, (1 − γt)i

,
(3)
where the disturbed image yt is sampled using:
the reverse process also harnesses the markov chain to progressively
convert the latent variable distribution pθ(yt) into distribution pθ(y0) parameterized by
θ. corresponding to the forward process, the reverse one is a denoising transformation
under the guidance of structure images x that begins with a standard gaussian distribution
yt ∼ n(yt | 0, i).
to address this, we design a structure encoder g
that effectively extracts the anatomical information from the structure images guiding
the noise predictor to generate more accurate dose maps by incorporating extracted
structural knowledge.
the residual connections are reserved for preventing gradient vanishment in the training.
it takes structure image x
as input, which includes the ct image and segmentation masks of ptv and oars, and
evacuates the compact feature representation in different levels to improve the accuracy
of dose prediction.
196
z. feng et al.
2.3
noise predictor
the purpose of the noise predictor f (xe, yt, γt) is to predict the noise added on the
distribution map yt with the guidance of the feature representation xe extracted from the
structure images x and current noise intensity γt in each step t. inspired by the great
achievements of unet
in the encoding procedure, to guide the noise predictor with essential anatomical
structure, the feature representations respectively extracted from the structure images x
and noisy image yt are simultaneously fed into the noise predictor.
then, these two feature maps are fused
by element-wise addition, allowing the structure information in x to be transferred to the
noise predictor.
algorithm 1: training procedure
1: input: input image pairs
where 
is the structure image and 
is the cor-
responding dose distribution map, the total number of diffusion steps 
2: initialize: randomly initialize the noise predictor 
and pre-trained structure encoder
3: repeat
4:      sample 
5:      sample 
6:      perform the gradient step on equation (9)
7: until converged
diffdp: radiotherapy dose prediction via a diffusion model
197
2.5
training details
we accomplish the proposed network in the pytorch framework.
all of our experiments
are conducted through one nvidia rtx 3090 gpu with 24 gb memory and a batch
size of 16 with an adaptive moment estimation (adam) optimizer.
additionally,
the noise intensity is initialized to 1e−2 and decayed to 1e−4 linearly along with the
increase of steps.
3
experiments and results
dataset and evaluations.
we measure the performance of our model on an in-house
rectum cancer dataset which contains 130 patients who underwent volumetric modulated
arc therapy (vmat) treatment at west china hospital.
concretely, for every patient, the
ct images, ptv segmentation, oars segmentations, and the clinically planned dose
distribution are included.
the thickness of the cts is 3 mm and all the images are resized to the resolution
of 256 × 256 before the training procedure.
we measure the performance of our proposed model with multiple metrics.
con-
sidering dm represents the minimal absorbed dose covering m% percentage volume of
ptv, we involve d98, d2, maximum dose (dmax), and mean dose (dmean) as metrics.
to
quantify performance more directly, we calculate the difference ) of these metrics
between the ground truth and the predicted results.
[27] as another essential metric of dose prediction perfor-
mance.
as ford2 anddmax, our method gains overwhelming performance with
0.0008 and 0.0005, respectively.
the p-values between the proposed and other sotas are
almost all less than 0.05, indicating that the enhancement of performance is statistically
meaningful.
diffdp: radiotherapy dose prediction via a diffusion model
199
our method and the ground truth is the smallest, demonstrating the superior performance
of the proposed.
to study the contributions of key components of the proposed method,
we conduct the ablation experiments by 1) removing the structure encoder from the
proposed method and concatenating the anatomical images x and noisy image yt together
as the original input for diffusion model (denoted as baseline); 2) the proposed diffdp
model.
we can clearly see the performance
for all metrics is enhanced with the structure encoder, demonstrating its effectiveness in
the proposed model.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_24.pdf:
cervical cancer is a signiﬁcant health burden worldwide, and
computer-aided diagnosis (cad) pipelines have the potential to improve
diagnosis eﬃciency and treatment outcomes.
however, traditional cad
pipelines have limitations due to the requirement of a detection model
trained on a large annotated dataset, which can be expensive and time-
consuming.
they also have a clear performance limit and low data uti-
lization eﬃciency.
to address these issues, we introduce a two-stage
detection-free pipeline, incorporating pooling transformer and moco pre-
training strategies, that optimizes data utilization for whole slide images
(wsis) while relying solely on sample-level diagnosis labels for training.
the experimental results demonstrate the eﬀectiveness of our approach,
with performance scaling up as the amount of data increases.
overall, our
novel pipeline has the potential to fully utilize massive data in wsi classi-
ﬁcation and can signiﬁcantly improve cancer diagnosis and treatment.
by
reducing the reliance on expensive data labeling and detection models,
our approach could enable more widespread and cost-eﬀective implemen-
tation of cad pipelines in clinical settings.
keywords: detection-free · contrastive learning · pathology image
classiﬁcation · cervical cancer
1
introduction
cervical cancer is a common and severe disease that aﬀects millions of women
globally, particularly in developing countries [9].
early diagnosis is vital for suc-
cessful treatment, which can signiﬁcantly increase the cure rate [17].
several computer-aided cervical cancer screening methods have been pro-
posed for whole slide images (wsis) in the literature.
in the second step, the patches
centered on these detected cells are processed through a classiﬁcation model, to
reﬁne the judgment of whether they are positive or negative.
some methods improve the ﬁnal classiﬁcation performance by improving the
detection model to identify positive cells more reliably.
[1] improved
the detection performance by incorporating clinical knowledge and attention
mechanism into their cell detection model of attfpn.
other methods improve the classiﬁcation per-
formance by changing the post-processing modules behind the detection model.
[5] proposed a progressive identiﬁcation method that leveraged
multi-scale visual cues to identify abnormal cells and then an rnn [27] for
sample-level classiﬁcation.
these methods have achieved good results through continuous improvement
on the detection-based pipeline, but there are some common drawbacks.
a lot of data would be wasted if only a small part
of annotated images (e.g., corresponding to positive cells and bounding boxes)
was used as training data.
finally, many existing methods focus on detecting
and classifying individual cells.
the tendency to neglect eﬀective integration of
the overall information across the entire wsi results in poor performance in
sample-level classiﬁcation.
to address the aforementioned issues, we propose a detection-free pipeline
in this paper, which does not rely on any detection model.
instead, our pipeline
requires only sample-level diagnosis labels, which are naturally available in clin-
ical scenarios and thus get rid of additional image labeling.
to attain this goal,
we have designed a two-stage pipeline as in fig.
1. in the coarse-grained stage,
we crop and downsample a wsi into multiple images, and conduct sample-level
classiﬁcation roughly based on all resized images.
then, in the ﬁne-grained
stage, we use these key patches for ﬁne prediction of the sample.
the two stages
in our pipeline adopt the same network design (i.e., encoder + pooling trans-
detection-free pipeline
245
former), which makes our solution friendly to develop and to use.
as a summary, our pipeline surpasses pre-
vious detection-based methods and achieves state-of-the-art performance with
large-scale training.
our experiments show that our method becomes more eﬀec-
tive when increasing the data size for training.
moreover, while many patholog-
ical images are also based on wsis, our pipeline has a high potential to extend
to other pathological tasks.
for feasibility of computation, we crop
a wsi into mutiple images.
the cropped images are passed through the coarse-grained
and ﬁne-grained stages, where only sample-level diagnosis labels of wsis, instead of
any additional manual labeling, are required for training.
two-stage pipeline with attention guided selection.
the overview of
our two-stage pipeline is shown in fig.
[18], so we crop each
wsi into local images sized 1024 × 1024 pixels.
the images are then processed
through the coarse-grained and ﬁne-grained stages in order to obtain the wsi-
level classiﬁcation results, respectively.
in general, the purpose of the coarse-
grained stage is to replace the detection model and identify local images that
may contain abnormal positive cells.
the ﬁne-grained stage then integrates these
key regions, producing reﬁned classiﬁcation for the sample.
to complete sample-level classiﬁcation, both stages share basically the same
network architecture.
the input images are ﬁrst processed by a cnn encoder to
246
m. cao et al.
extract features.
additionally, the input images for both stages are 256 × 256.
in the coarse-grained stage, in order to allow the model to examine as many
local images as possible, we resize the cropped local images from 1024 × 1024 to
256 × 256.
in the ﬁne-grained stage, we enlarge suspicious local abnormality and
thus crop input images to 256 × 256 from 1024 × 1024.
for the coarse-grained stage, after passing the resized local images through
encoder and pooling transformer, we obtain a rough prediction result at the
sample level.
in addition, we calculate
the attention score to identify the local image inputs that are most likely to yield
positive reading.
f t

dx0
)f,
(1)
where x0 represents classiﬁcation token (which is a commonly used setting in
transformer [7,22]), and dx0 is 512 in our implementation
, f represents the fea-
ture vector of a certain input local image.
after calculating attention scores, we
preserve top-8 (resized) local images with the highest scores from the entire wsi
for subsequent ﬁne-grained classiﬁcation.
next, in the ﬁne-grained stage, each local image that has passed attention
guided selection is cropped into 16 patches of the size 256 × 256.
the network of the ﬁne-grained stage is the same as that of
the coarse-grained stage, but the weights of the encoder is pre-trained in an
unsupervised manner (sect. 2).
the same ce loss supervised by sample-level
ground truth is used for the ﬁne-grained stage here.
for inference, the output of
the ﬁne-grained stage will be treated as the ﬁnal result of the test wsi.
fig.
we use a transformer network to aggregate features of
multiple inputs and to derive the sample-level outcome in both coarse-grained
and ﬁne-grained stages.
we have observed that diﬀerent local images of the same
sample often have patterns of grouped similarity (such as the ﬁrst two images
in the upper-right of fig.
for negative samples, most of the local images are
similar with each other.
for positive samples, the images of abnormal cells are
inclined to be grouped into several clusters.
therefore, inspired by [2,14], we propose pooling transformer that is eﬀec-
tive to reduce the redundancy and distortion from the input images.
within each clustered class, we average the features and aggregate a sin-
gle token.
[25,26] pre-training on imagenet
[6], we also perform pre-
training for ﬁne-grained encoder on a large scale of pathology images.
for data, wsi naturally has the advantage of having a large amount
of training data.
therefore, we only need 2,000–3,000 wsi samples to obtain
a dataset that can even be compared to imagenet in quantity.
in our task, since the structural features of cells are relatively
weak compared to natural images, it is not suitable to model the loss function
using masks.
speciﬁcally, in the same training batch, a patch (256 × 256, the same to the
input size of the ﬁne-grained stage) and its augmented patch are treated as a
positive pair (note that here “positive/negative” is deﬁned in the context of
contrastive learning), and their features are required to be as similar as possible.
using this method, we can pre-train a feature encoder in an
unsupervised manner and initialize it into our encoder for the ﬁne-grained stage.
248
m. cao et al.
3
experiment and results
dataset and experimental setup.
we conduct the
experiment with initial learning rate of 1.0 × 10−4, batch size of 4, and sgd
optimizer [21] for 30 epochs each stage.
in this section, we experiment to compare
our method with popular state-of-the-art (sota) methods, which are all fully
supervised and detection-based.
the detection dataset has 3761 images and 7623 cell-level annotations.
method
accuracy↑
precision↑
recall↑
f1-score↑
attfpn+average [1]
78.33 ± 1.51
71.23 ± 2.10
79.39 ± 1.56
74.10 ± 2.33
while our method has a large margin
with most methods in the table, the improvement against [28] (top-ranked in
current detection-based methods) is relatively limited.
in this section, we experiment to demonstrate the eﬀective-
ness of all the proposed parts in our pipeline.
here, cg means the classiﬁcation passes only the coarse-grained stage.
as can be seen, its performance is low, in that the resized images sacriﬁces the
resolution and thus perform poorly for image-based classiﬁcation.
fg refers to
classifying in the ﬁne-grained stage.
it is worth noting that without the atten-
tion scores provided by the coarse-grained stage, we have no way of knowing
which local images might contain suspicious positive cells.
thus, we use random
selection to experiment for fg only, as exhaustively checking all local images is
computationally forbidden.
as can be seen, the classiﬁcation result is the lowest
because it lacks enough access to the key image content in wsis.
conﬁguration
metric (%)
cg fg pt cl acc
precision
recall
f1 score
✔
–
–
–
74.96 ± 1.23 71.39 ± 1.21 82.49 ± 1.39 75.34 ± 1.66
–
✔
–
–
73.11 ± 2.10 70.48 ± 1.45 81.59 ± 1.45 74.09 ± 1.49
✔
✔
–
–
79.72 ± 1.30 74.64 ± 1.34 84.45 ± 1.95 78.48 ± 1.23
✔
✔
✔
–
81.34 ± 1.56 77.36 ± 1.23 84.79 ± 0.98 80.72 ± 0.93
✔
✔
✔
✔
83.84 ± 1.56 78.36 ± 1.23 85.22 ± 0.98 82.12 ± 0.93
by combining the two stages for attention guided selection, it is eﬀective
to improve the classiﬁcation performance compared to the two previous exper-
iments.
ultimately, we combine them together
to achieve the best performance.
for the experiment of sample numbers, we compare the best
fully supervised detection-based method (retinanet+gat [28]) with ours under
the sample numbers of 500, 1000, 2000, and 5384.
although our method
initially has poorer performance, it has shown an impressive growth trend.
and
at our current maximum data number (5384), the proposed pipeline has already
exceeded the performance of the detection-based method.
the above results also
demonstrate that our new pipeline method has greater potential, even though it
requires no cell-level image annotation.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_30.pdf:
in medical image analysis, imbalanced noisy dataset classiﬁ-
cation poses a long-standing and critical problem since clinical large-
scale datasets often attain noisy labels and imbalanced distributions
through annotation and collection.
additionally, the factor of class hardness hinder-
ing label noise removal remains undiscovered, causing a critical neces-
sity for an approach to enhance the classiﬁcation performance of noisy
imbalanced medical datasets with various class hardness.
to address this
paradox, we propose a robust classiﬁer that trains on a multi-stage noise
removal framework, which jointly rectiﬁes the adverse eﬀects of label
noise, imbalanced distribution, and class hardness.
multi-environment risk
minimization (mer) strategy captures data-to-label causal features for
noise identiﬁcation, and the rescaling class-aware gaussian mixture
modeling (rcgm) learns class-invariant detection mappings for noise
removal.
extensive experiments on two imbalanced noisy clinical datasets
demonstrate the capability and potential of our method for boosting the
performance of medical image classiﬁcation.
keywords: imbalanced data · noisy labels · medical image analysis
1
introduction
image classiﬁcation is a signiﬁcant challenge in medical image analysis.
although
some classiﬁcation methods achieve promising performance on balanced and
clean medical datasets, balanced datasets with high-accuracy annotations are
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43987-2 30.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
https://doi.org/10.1007/978-3-031-43987-2_30
learning robust medical image classiﬁer by minimizing invariant risk
307
fig.
existing approaches for non-ideal medical image classiﬁcation can be sum-
marized into noisy classiﬁcation, imbalanced recognition, and noisy imbalanced
identiﬁcation.
imbalanced recognition approaches [9,15,21] utilize
augmented embeddings and imbalance-invariant training loss to re-balance the
long-tailed medical data artiﬁcially, but the disturbance from noisy labels leads
to uncasual feature learning, impeding the recognition of tail classes.
in this work, we propose a multi-stage noise removal framework to address
these concerns jointly.
the main contributions of our work include: 1) we decom-
pose the negative eﬀects in practical medical image classiﬁcation, 2) we minimize
308
j. li et al.
the invariant risk to tackle noise identiﬁcation inﬂuenced by multiple factors,
enabling the classiﬁer to learn causal features and be distribution-invariant, 3)
a re-scaling class-aware gaussian mixture modeling (cgmm) approach is pro-
posed to distinguish noise labels under various class hardness, 4) we evaluate our
method on two medical image datasets, and conduct thorough ablation studies
to demonstrate our approach’s eﬀectiveness.
further, we denote the backbone
as h(·; θ), x → z mapping data manifold to the latent manifold, the classiﬁer
head as g(·; γ), z → c linking latent space to the category logit space, and the
identiﬁer as f(·; φ), z → c. we aim to train a robust medical image classiﬁca-
tion model composed of a representation backbone and a classiﬁer head on label
noise and imbalance distribution, resulting in a minimized loss on the testing
dataset:
given that backbone mapping is independent
of noisy imbalanced eﬀects, we conduct further disentanglement by deﬁning e as
the negative eﬀects and p as constant for ﬁxed probability mappings:
p(y = c|x, e) = ph(z|x) · pg(y = c|z, e)
= ph(z|x) · {pg(y = c|z) ·
by bayes theorem, we decompose the eﬀect into imbalance,
learning robust medical image classiﬁer by minimizing invariant risk
309
fig.
noise identiﬁer f is optimized across three constructed environments with h ﬁxed
during mer.
furthermore, the
impact of hardness eﬀects has not been considered in previous studies, which
adds an extra dimension to noise removal.
in essence, the fundamental idea
of noisy classiﬁcation involves utilizing clean data for classiﬁer training, which
determines the importance of noise identiﬁcation and removal.
however, they fail to consider the inﬂuence of imbalanced distri-
butions, which might cause a biased gradient direction on the optimization sub-
space.
following [25], we minimize the invariant risk [2] across multi-environment
for independent detector learning.
by assuming that the robust classiﬁer per-
forms well on every data distribution, we solve the optimizing object by ﬁnding
the optima to reduce the averaged distance for gradient shift:
310
j. li et al.
min
hθ:x→z
fφ:z→y

ε∈etr
l(fφ ◦ hθ)
s.t.
∈ etr,
(3)
where ε represents an environment (distribution) for classiﬁer fφ and backbone
hθ; and l denotes the empirical loss for classiﬁcation.
in practice, all environments
are established from the training set with the same class categories.
from the perspective of clustering, deﬁnite and immense
gaps between two congregate groups contribute to more accurate decisions.
how-
ever, in medical image analysis, an overlooked mismatch exists between class
hardness and diﬃculty in noise identiﬁcation.
=

k∈{c,n}
αikpk
i

qij | μk
i , σk
i

,
(5)
which produces more accurate and independent measurements of label quality.
learning robust medical image classiﬁer by minimizing invariant risk
311
instead of assigning a hard label to the potential noisy data as [8] which also
employs a class-speciﬁc gmm to cluster the uncertainty, we further re-scale the
conﬁdence score of class-wise noisy data.
τ
(7)
2.5
overall learning framework for imbalanced and noisy data
in contrast to two-stage noise removal and imbalance classiﬁcation techniques,
our approach applies a multi-stage protocol: warm-up phases, noise removal
phases, and ﬁne-tuning phases as shown in fig.
2. in the warm-up stage, we train
backbone h and classiﬁer g
a few epochs by assuming that g only remembers
clean images with less empirical loss.
sqrt sampler is applied to re-balance the
data, and cross-stage kl
3
experiment
3.1
dataset and evaluation metric
we evaluated our approach on two medical image datasets with imbalanced class
distributions and noisy labels.
[22], is a dermato-
scopic image dataset for skin-lesion classiﬁcation with 10,015 images divided into
seven categories.
it contains a training set with 7,007 images, a validation set
with 1,003 images, and a testing set with 2,005 images.
[29], is a histopathology image
dataset manually annotated into four cancer categories by three pathological
312
j. li et al.
table 1.
the second-best performances are underlined.
consequently, chaoyang dataset
consists of a training set with 2,181 images, a validation set with 713 images,
and a testing set with 1,426 images, where the validation and testing sets have
clean labels.
the evaluation metrics are macro-f1, b-acc, and mcc.
3.2
implementation details
we mainly follow the training settings of fcd
resnet-18 pretrained on the
imagenet is the backbone.
learning rates are 0.06, 0.001,
0.06 and 0.006 with the cosine schedule for four stages, respectively.
the size of input image is 224 × 224.
the scale and threshold in rcgm
are 0.6 and 0.1, respectively.
learning robust medical image classiﬁer by minimizing invariant risk
313
3.3
comparison with state-of-the-art methods
we compare our model with state-of-the-art methods which contain noisy meth-
ods (including dividemix [13], nl
we train all approaches under the same data augmenta-
tions and network architecture.
in noisy methods, nl and gce also suﬀer great
performance declines.
we mix these weakly-performed approaches with meth-
ods from the other category, observing the accuracy improvement.
our framework achieves improvements in all metrics on both datasets,
demonstrating the rationality of the assumption and the eﬀectiveness of our
framework.
fig.
(a) and (b) quantitative performance comparison of diﬀerent
components of our method on ham10000 and chaoyang datasets, respectively.
3, we evaluate the eﬀectiveness of the components in our
method by decomposing them on extensive experiments.
we choose the ﬁrst
stage of fcd
figure 3a and 3b show that only using mer
or rcgm achieves better performance than our strong baseline on both datasets.
for example, mer achieves 5.37% and 1.15% improvements on ham10000 and
chaoyang, respectively, demonstrating the eﬀectiveness of our noise removal
314
j. li et al.
techniques.
further, our multi-stage noise removal technique outperforms single
mer and rcgm, revealing that the decomposition for noise eﬀect and hard-
ness eﬀect works on noisy imbalanced datasets.
furthermore, similar performance trends reveal the robustness of scale s.
4

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_15.pdf:
however, existing classiﬁcation approaches rely on
generic radiomic features that may not be optimal for the task, whilst
deep learning methods tend to over-ﬁt to the high-dimensional vol-
ume inputs.
in this work, we propose a novel radiomics-informed deep-
learning method, ridl, that combines the advantages of deep learning
and radiomic approaches to improve af sub-type classiﬁcation.
unlike
existing hybrid techniques that mostly rely on na¨ıve feature concatena-
tion, we observe that radiomic feature selection methods can serve as
an information prior, and propose supplementing low-level deep neural
network (dnn) features with locally computed radiomic features.
furthermore, we ensure complementary
information is learned by deep and radiomic features by designing a
novel feature de-correlation loss.
the disease can lead to stroke and heart failure, and has a mortal-
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43990-2 15.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
[8]. accurate knowledge of the disease type is there-
fore highly valuable for treatment planning and has high prognostic value [22].
[7] showed that eat volume,
approximated from left-atrium ct images, can be used as a predictor for af
recurrence.
[22] trained a random forest model to classify af sub-
type based on radiomic features and volume measurements, achieving 85.3%
auc.
although these methods demonstrate the usefulness of radiomic features
for af sub-type classiﬁcation, such features are generic and not speciﬁc to the
task, which can limit model performance [12].
na¨ıvely using deep neural networks (dnns) to predict
af sub-types from ct volumes yields poor results however due to over-ﬁtting
on high-dimensional volume inputs (see results for dnn in table 1).
although these methods propose diﬀerent ways
of using both approaches, they do not explicitly address the limitations of either
approach or explore ways to combine their complementary advantages.
we note that textural radiomic features identiﬁed by feature selection methods
can serve as an information prior to supplement low-level features from dnns,
since they are designed to capture low-level context and have predictive power
[23].
furthermore, we encourage the dnn to learn features
complementary to radiomic features to obtain more comprehensive signals and
design a novel feature de-correlation loss.
1. unlike exist-
ing works, our method is designed to directly addresses the limitations of both
deep learning and radiomic approaches and achieves state-of-the-art performance
on af sub-type classiﬁcation.
our method also
fuses locally computed radiomic features with low-level dnn features and encourages
complementary deep and radiomic features to be learned.
– furthermore, we enforce feature de-correlation using a novel feature-bank
design to ensure complementary deep and radiomic features are extracted.
2
methodology
we combine radiomic and deep learning approaches using two novel components:
1) feature fusion of local radiomic features and low-level dnn features to improve
local context, 2) encouraging complementary deep and radiomic features through
feature de-correlation.
(b) feature bank implementation for feature de-
correlation.
local features can be calculated for multiple texture features and patch
size p, which are then concatenated to obtain rl
i ∈ rl×h×w ×d, where l is the
total number of features used and h, w, and d are original input dimensions.
we note that only texture radiomic features are used for local calculation since
they are speciﬁcally intended to capture local context.
radiomics-informed deep learning (ridl)
157
rl
i is then concatenated with low-level dnn features, zi ∈ rc×h×w ×d,
to supplement the dnn with local radiomic features.
z′
i = a(rl
i ⊕ zi) ⊗ (rl
i ⊕ zi)
(3)
where z′
i is the fused feature, ⊕ is channel concatenation, and ⊗ is element-wise
multiplication.
the learned attention tensor a(rl
i ⊕zi) has dimensions (c +l)×
1 × 1 × 1 and is broadcasted along the volume dimension, such that attention is
applied channel-wise and spatial feature distributions are preserved.
2.2
encouraging complementary deep and radiomic features
through feature de-correlation
global radiomic features are also included in our model by concatenation
with high-level dnn features before the classiﬁcation layer.
unlike existing
approaches however, we encourage our dnn to learn features complementary
to radiomic features by enforcing de-correlation between the two.
we
instead propose a novel feature-bank implementation with exponential weighting
to estimate sample statistics.
(4)
the ﬁrst b samples, where b is the batch size, belong to the training sample
of the current iteration, and their losses are back-propagated to encourage deep
features to have zero correlation with radiomic features.
to provide
further regularization and prevent over-ﬁtting, we perform an additional self-
reconstruction task, using loss lrec, which we describe in more detail in the
supplementary materials.
(6)
3
experiments
3.1
implementation details
dataset.
volumes are resized to the same aspect ratio to ensure consistent dimensions
across samples.
we use ﬁve-fold cross-validation and report average
test performance across folds.
cross-validation is implemented by splitting the
dataset into ﬁve equal subsets and using three subsets for training, one subset
for validation, and one subset for testing.
data
acquisition procedures and statistics are given in the supplementary materials.
setup.
we use the pyradiomic package
bottle-neck features are averaged
across spatial dimensions for classiﬁcation, whilst decoder outputs are used for
self-reconstruction regularization.
additional experiments and details are included in the supplementary materials.
radiomics-informed deep learning (ridl)
159
3.2
comparison with state-of-the-art methodologies
we compare our method with alternative state-of-the-art approaches based on
radiomics, deep learning, and hybrid techniques.
dnn⋆ is a na¨ıve implementation using the m3dunet model.
baseline† is a na¨ıve hybrid
implementation using simple feature concatenation.
we perform ablation experiments to demonstrate
improvements from using local radiomic features, global radiomic features, and
feature de-correlation loss.
using feature
de-correlation further boosts performance and leads to the best overall results.
dnn⋆ is a na¨ıve implementation using the m3dunet model.
baseline† is a na¨ıve hybrid
implementation using simple feature concatenation.
feature used for local calculation
selected
auc (%)
map (%)
gldm dependencenonuniformitynormalized
✗
86.1 ± 0.8
85.5 ± 0.9
glrlm longrunemphasis
✗
86.4 ± 0.8
85.4 ± 0.8
gldm largedependenceemphasis
✗
85.6 ± 0.7
84.6 ± 0.9
glcm idn (ours)
✓
86.9 ± 0.6
86.3 ± 0.6
we can see that using discarded features leads to worse performance in gen-
eral.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_38.pdf:
drawing on the capabil-
ity of the contrastive language-image pre-training (clip) model to learn
generalized visual representations from text annotations, in this paper,
we propose clip-lung, a textual knowledge-guided framework for lung
nodule malignancy prediction.
third, we align image features with both class and attribute
features via contrastive learning, rectifying false positives and false nega-
tives in latent space.
experimental results on the benchmark lidc-idri
dataset demonstrate the superiority of clip-lung, in both classiﬁcation
performance and interpretability of attention maps.
keywords: lung nodule classiﬁcation · vision-language model ·
usually, the malignancy predic-
tion is often formulated as benign-malignant binary classiﬁcation [9,10,19], and
the higher classiﬁcation performance and explainable attention maps are impres-
sive.
indeed, the ordinal regression-
based methods are able to learn ordered manifolds and to further enhance the
prediction accuracy.
however, the aforementioned methods still face challenges in distinguishing
visually similar samples with adjacent rank labels.
1,
this text information is beneﬁcial for distinguishing visually similar pairs, while we
conduct this behavior by applying contrastive learning that pulls semantic-closer
samples and pushes away semantic-farther ones.
to integrate text annotations into the image-domain learning process, an
eﬀective text encoder providing accurate textual features is required.
fortu-
nately, recent advances in vision-language models, such as contrastive language-
image pre-training (clip)
2. illustration of the proposed clip-lung.
pose clip-lung, a framework to classify lung nodules using image-text pairs.
then,
we design a textual knowledge-guided contrastive learning based on obtained
image features and textual features involving classes and attributes.
experimen-
1) we propose clip-lung for lung nodule malignancy prediction, which lever-
ages clinical textual knowledge to enhance the image encoder and classiﬁer.
2) we design a channel-wise conditional prompt module to establish consistent
relationships among the correlated text tokens and feature maps.
3) we simultaneously align the image features with class and attribute fea-
tures through contrastive learning while generating more explainable atten-
tion maps.
in this paper, we arrange the lung nodule classiﬁcation
dataset as {i, y, c, a}, where i = {ii}n
i=1 is an image set containing n lung nod-
ule images.
finally, a = {am}m
m=1 is the set of attribute embeddings, where each element
am ∈ rd×1 is a vector representing the embedding of an attribute word such as
“spiculation”.
, the training framework contains an image encoder fθ
and a text encoder gφ.
first, the input image ii is fed into fθ and then gener-
ates the feature maps.
note that
the vectors f t,:, lt,:, l′
t,:, and ck,: are with the same dimension
consequently, we have image feature f ∈ rt ×d, class feature c
hence, the element-
wise multiplication wi · ai is unique to ii.
2.3
channel-wise conditional prompt
cocoop [20] ﬁrstly proposed to learn language contexts for vision-language mod-
els conditioned on visual features.
hence,
the conditional prompt for the t-th token is lt+l′
t. in addition, ccp also outputs
the f t,: for image-class and image-attribute contrastive learning.
in this paper, we conduct such image-text contrastive learning
by utilizing pre-trained clip text encoder
image-class alignment.
first, the same to clip, we align the image and class
information by minimizing the cross-entropy (ce) loss for the sample {ii, yi}:
lic = −
t

t=1
k

k=1
yilog
exp(σ(f t,:, ck,:)/τ)
k
k′=1 exp(σ(f t,:, ck′,:)/τ)
,
(2)
where ck,: = gφ(ck
(l1+l′
1, l2+l′
2, . .
therefore, lic implements
the contrastive learning between channel-wise features and corresponding class
features, i.e., the ensemble of grouped image-class alignment results.
image-attribute alignment.
in addition to image-class alignment, we further
expect the image features to correlate with speciﬁc attributes.
so we conduct
image-attribute alignment by minimizing the infonce loss [5,16]:
lia = −
t

t=1
m

m=1
log
exp(σ(f t,:, wm,: · am,:)/τ)
m
m′=1 exp(σ(f t,:, wm′,: · am′,:)/τ)
.
class-attribute alignment.
although the image features have been aligned
with classes and attributes, the class embeddings obtained by the pre-trained
clip encoder may shift in the latent space, which may result in inconsistent
class space and attribute space, i.e., annotated attributes do not match the
corresponding classes, which is contradictory to the actual clinical diagnosis.
to
avoid this weakness, we further align the class and attribute features:
lca = −
k

k=1
m

m=1
log
exp(σ(ck,:, wm,: · am,:)/τ)
m
m′=1 exp(σ(ck,:, wm′,: · am′,:)/τ)
,
(4)
and this loss implies semantic consistency between classes and attributes.
408
y. lei et al.
finally, the total loss function is deﬁned as follows:
l = ei i∈i

lce +
note that during the
inference phase, test images are only fed into the trained image encoder and clas-
siﬁer.
3
experiments
3.1
dataset and implementation details
dataset.
following [9,11], we modiﬁed the ﬁrst layer of
the image encoder to be with 32 channels.
according to existing works [11,18], we
regard a nodule with an average score between 2.5 and 3.5 as unsure nodules, benign
and malignant categories are those with scores lower than 2.5 and larger than 3.5,
respectively.
experimental settings.
in this paper, we apply the clip pre-trained vit-
b/16 as the text encoder for clip-lung, and the image encoder we used is
resnet-18
the image
encoder is initialized randomly.
the learning rate is 0.001 following the cosine decay, while the optimizer
is stochastic gradient descent with momentum 0.9 and weight decay 0.00005.
all of
our experiments are implemented with pytorch [15] and trained with nvidia
a100 gpus.
the experimental results are reported with average values through
ﬁve-fold cross-validation.
3.2
experimental results and analysis
performance comparisons.
in table 1, we compare the classiﬁcation perfor-
mances on the lidc-a dataset, where we regard the benign-unsure-malignant
clip-lung for lung nodule malignancy prediction
409
table 1.
table 2 presents a performance comparison of clip-lung on the lidc-b and
lidc-c datasets.
consequently, aligning these distinct feature types becomes challenging, resulting
in a bias towards the text features associated with malignant nodules.
410
y. lei et al.
fig.
3, we
can see that clip yields a non-compact latent space for two kinds of nodules.
cocoop and clip-lung alleviate this phenomenon, which demonstrates that
the learnable prompts guided by nodule classes are more eﬀective than ﬁxed
prompt engineering.
based on lic, lia and lca improve the
performances on lidc-a, indicating the eﬀectiveness of capturing ﬁne-grained
features of ordinal ranks using class and attribute texts.
that is to
say, lia is more important in latent space rectiﬁcation, i.e., image-attribute con-
sistency.
in addition, we observe that lic+lia performs better than lia+lca,
which is attributed to that lca regularizes the image features indirectly.
clip-lung for lung nodule malignancy prediction
411
4
conclusion
in this paper, we proposed a textual knowledge-guided framework for pulmonary
nodule classiﬁcation, named clip-lung.
clip-
lung aligned the diﬀerent modalities of features generated from nodule classes,
attributes, and images through contrastive learning.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_48.pdf:
esophageal cancer is a signiﬁcant global health concern, and
radiotherapy (rt) is a common treatment option.
accurate delineation
of the gross tumor volume (gtv) is essential for optimal treatment out-
comes.
in clinical practice, patients may undergo a second round of rt to
achieve complete tumor control when the ﬁrst course of treatment fails
to eradicate cancer completely.
however, manual delineation is labor-
intensive, and automatic segmentation of esophageal gtv is diﬃcult due
to the ambiguous boundary of the tumor.
detailed tumor information
naturally exists in the previous stage, however the correlation between
the ﬁrst and second course rt is rarely explored.
we propose a novel
prior anatomy and rt information enhanced second-course esophageal
gtv segmentation network (artseg).
a region-preserving attention
module (ram) is designed to understand the long-range prior knowl-
edge of the esophageal structure, while preserving the regional patterns.
sparsely labeled medical images for various isolated tasks necessitate
eﬃcient utilization of knowledge from relevant datasets and tasks.
to
achieve this, we train our network in an information-querying manner.
artseg incorporates various prior knowledge, including: 1) tumor vol-
ume variation between ﬁrst and second rt courses, 2) cancer cell pro-
liferation, and 3) reliance of gtv on esophageal anatomy.
extensive
quantitative and qualitative experiments validate our designs.
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43990-2_48.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
https://doi.org/10.1007/978-3-031-43990-2_48
512
y. sun et al.
keywords: second course radiotherapy · esophageal gross tumor
volume · data eﬃcient learning · prior anatomical information ·
attention
1
introduction
esophageal cancer is a signiﬁcant contributor to cancer-related deaths glob-
ally [3,15].
one eﬀective treatment option is radiotherapy (rt), which utilizes
high-energy radiation to target cancerous cells [4].
to ensure optimal treatment
outcomes, both the cancerous region and the adjacent organ-at-risk (oar) must
be accurately delineated, to focus the high-energy radiation solely on the cancer-
ous area while protecting the oars from any harm.
in the clinical setting, patients may undergo a second round of rt treatment
to achieve complete tumor control when initial treatment fails to completely
eradicate cancer
moreover, the automatic delineation of the gtv in the esophagus
poses a signiﬁcant diﬃculty, primarily attributable to the low contrast between
the esophageal gtv and the neighboring tissue, as well as the limited datasets.
recently, advances in deep learning [21] have promoted research in auto-
matic esophageal gtv segmentation from computed tomography (ct)
[9,10] improve the segmentation accuracy
by incorporating additional information from paired positron emission tomog-
raphy (pet).
nevertheless, such approaches require several imaging modalities,
which can be both costly and time-consuming, while disregarding any knowledge
from previous treatment or anatomical understanding.
we proposed a novel prior anatomy and rt
information enhanced second-course esophageal gtv segmentation network
(artseg).
a region-preserving attention module (ram) is designed to eﬀec-
tively capture the long-range prior knowledge in the esophageal structure, while
preserving regional tumor patterns.
to the best of our knowledge, we are the
ﬁrst to reveal the domain gap between the ﬁrst and second courses for gtv
segmentation, and explicitly leverage prior information from the ﬁrst course to
improve gtv segmentation performance in the second course.
the medical images are labeled sparsely, which are isolated by diﬀerent
tasks [20].
meanwhile, an ideal method for automatic esophageal gtv segmenta-
tion in the second course of rt should consider three key aspects: 1) changes in
tumor volume after the ﬁrst course of rt, 2) the proliferation of cancerous cells
from a tumor to neighboring healthy cells, and 3) the anatomical-dependent
second-course esophageal gtv segmentation
513
fig.
our training approach leverages multi-center datasets containing relevant anno-
tations, that challenges the network to retrieve information from e1 using the features
from e2.
our training strategy leverages three datasets that introduce
prior knowledge to the network of the following three key aspects: 1) tumor volume
variation, 2) cancer cell proliferation, and 3) reliance of gtv on esophageal anatomy.
nature of gtv on esophageal locations.
to achieve this, we eﬃciently exploit
knowledge from multi-center datasets that are not tailored for second-course
gtv segmentation.
our training strategy does not speciﬁc to any tasks but
challenges the network to retrieve information from another encoder with aug-
mented inputs, which enables the network to learn from the above three aspects.
extensive quantitative and qualitative experiments validate our designs.
2
network architecture
in the ﬁrst course of rt, a ct image denoted as i1 is utilized to manually
delineate the esophageal gtv, g1.
during the second course of rt, a ct image
i2 of the same patient is acquired.
however, i2 is not aligned with i1 due to soft
tissue movement and changes in tumor volume that occurred during the ﬁrst
course of treatment.
both images i1/2 have the spatial shape of h × w ×
d.
our objective is to predict the esophageal gtv g2 of the second course.
it would be advantageous to leverage insights from the ﬁrst course, as it com-
prises comprehensive information pertaining to the tumor in its preceding phase.
the features f d
1/2 are reshaped to hw d
23d
× c before passed to the mha, where
c is the channel dimension.
the attentive features f d
a can be formulated as:
f d
a = mha(q, k, v ) = mha(f d
2 , f d
1 , f d
1 ), d = 3, 4.
(2)
since mha perturbs the positional information, we preserve the tumor local
patterns by concatenating original features to the attentive features at the chan-
nel dimension, followed by a 1 × 1 × 1 bottleneck convolution ξ1×1×1 to squeeze
the channel features (named as ram), as shown in the following equations,
f d =
concat(f d
1 , f d
2 ),
d
3
training strategy
the network should learn from three aspects: 1) tumor volume variation: the
structural changes of the tumor from the ﬁrst to the second course; 2) cancer
cell proliferation: the tumor in esophageal cancer tends to inﬁltrate into the
adjacent tissue; 3) reliance of gtv on esophageal anatomy: the anatom-
ical dependency between esophageal gtv and the position of the esophagus.
medical images are sparsely labeled which are isolated by diﬀerent tasks [20],
and are often inadequate.
in order to fully leverage both public and private datasets, the training objec-
tive should not be speciﬁc to any tasks.
1, our strategy is to challenge the network to retrieve information from
augmented inputs in e1 using the features from e2, which can incorporate a wide
range of datasets that are not tailored for second-course gtv segmentation.
3.1
tumor volume variation
the diﬀerences in tumor volume between the ﬁrst and second courses following
an rt treatment can have a negative impact on the state-of-the-art (sota)
learning-based techniques, which will be discussed in sect.
in sp, i1
p and i2
p are the ﬁrst and second
course ct images, while g1
p and g2
p are the corresponding gtv annotations.
second-course esophageal gtv segmentation
515
3.2
cancer cell proliferation
the paired dataset sp for the ﬁrst and second courses is limited, whereas an
unpaired gtv dataset sv = {iv; gv} can be easily obtained in a standard clini-
cal workﬂow with a substantial amount.
sv lacks its counterpart for the second
course, in which iv/gv are the ct image and the corresponding annotation for
gtv. to address this, we apply two distinct randomized augmentations, p1, p2,
to mimic the unregistered issue of the ﬁrst and second course ct.
(4)
the esophageal tumor can proliferate with varying morphologies into the sur-
rounding tissues.
3.3
reliance of gtv on esophageal anatomy
to make full use of the datasets of relevant tasks, we incorporate a public esoph-
agus segmentation dataset, denoted as se = {ie; ge}, where ie/ge represent the
ct images and corresponding annotations of the esophagus structure.
by aug-
menting the data as described in eq.
similarly, data from
the paired sp is also augmented by p1/2 to increase the network’s robustness.
in summary, our training strategy is not dataset-speciﬁc or target-speciﬁc,
thus allowing the integration of prior knowledge from multi-center esophageal
gtv-related datasets, which eﬀectively improves the network’s ability to retrieve
information for the second course from the three key aspects stated in sect.
3.
4
experiments
4.1
experimental setup
datasets.
the paired ﬁrst-second course dataset, sp, is collected from sun yat-
sen university cancer center (ethics approval number: b2023-107-01), com-
prising paired ct scans of 69 distinct patients from south china.
for both sp and sv, physicians annotated the esophageal
cancer gtv in each ct.
the results suggest a domain gap between the ﬁrst and second courses, which
indicates increased diﬃculty in gtv segmentation for the second course.
asterisks
indicate p-value < 0.05 for the performance gap between the ﬁrst and second course.
(color ﬁgure online)
have esophageal cancer.
implementation details.
the augmentations p1/2 involve a combination of ran-
dom 3d resized cropping, ﬂipping, rotation in the transverse plane, and gaussian
noise.
the network is implemented using pytorch
[1], and detailed conﬁgurations are in the supplementary material.
experiments are performed on an nvidia rtx 3090 gpu with 24gb memory.
performance metrics.
dice score (dsc), averaged surface distance (asd)
and hausdorﬀ distance (hsd) are used as metrics for evaluation.
the wilcoxon
signed-rank test is used to compare the performance of diﬀerent methods.
second-course esophageal gtv segmentation
517
4.2
domain gap between the first and second course
as previously mentioned, the volume of the tumors changes after the ﬁrst course
of rt.
the
results presented in table 1 indicate a performance gap between gtv segmen-
tation in the ﬁrst and second courses, with the latter being more challenging.
notably, the paired ﬁrst-second course dataset stest
p
pertains to the same group
of patients, thereby ensuring that any performance drop can be attributed solely
to diﬀerences in courses of rt, rather than variations across diﬀerent patients.
therefore, for accurate second-course gtv segmentation,
we need to explicitly propagate prior information from the ﬁrst course using dual
encoders in artseg, and incorporate learning about tumor changes.
4.3
evaluations of second-course gtv segmentation performance
combination of various datasets.
table 2 presents the information gain
derived from multi-center datasets using quantiﬁed metrics for segmentation
performance.
when prior information from the ﬁrst course is explicitly introduced
using sp, artseg outperforms other baselines for gtv segmentation in the
second course, which reaches a dsc of 66.73%.
however, in fig. 3, it can be
observed that the model failed to accurately track the gtv area along the
esophagus (orange arrows) due to the soft and elongated nature of the esophageal
tissue, which deforms easily during ct scans performed at diﬀerent times.
meanwhile, the esophageal tumor comprises two
primary regions, the original part located in the esophagus and the extended
part that has invaded the surrounding tissue.
quantitative comparison of gtv segmentation performance in the second
course.
our proposed artseg+ram achieved better overall performance, where
asterisks indicate artseg+ram outperforms other methods with p-value < 0.05.
methods
strain
p
sv se dsc (%) ↑
asd(mm) ↓
hsd(mm) ↓
inference
mean ± std.
med.
mean ± std.
med.
the impact of diﬀerent prior knowledge on esophageal tumor detection.
our proposed approach, encompassing comprehensive prior knowledge, shows
superior performance.
(color
ﬁgure online)
further improve the dsc to 74.54% by utilizing comprehensive knowledge of
both the tumor morphology and esophageal structures.
although
introducing
the
esophageal structural prior knowledge using se can improve the performance
in dsc and asd (table 2), the increase in hsd (38.22 to 47.89 mm; 21.71 to
27.00 mm) indicates that there are outliers far from the ground truth bound-
aries.
second-course esophageal gtv segmentation
519
however, there is no performance gain with mha as shown in table 2, and the
hsd further increased to 27.33 mm.
to tackle the aforementioned problem, we propose ram which involves the
concatenation of the original features with attention outputs, allowing for the
preservation of convolution-generated regional tumor patterns while eﬀectively
comprehending long-range prior knowledge speciﬁc to the esophagus.
for the method’s generalizability, analysis of diverse imaging pro-
tocols and segmentation backbones are inadequate.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_8.pdf:
how-
ever, the use of gadolinium-based contrast agents (gbca) to obtain
ce-mri may be associated with nephrogenic systemic ﬁbrosis and may
lead to bioaccumulation in the brain, posing a potential risk to human
health.
moreover, and likely more important, the use of gadolinium-based
contrast agents requires the cannulation of a vein, and the injection of
the contrast media which is cumbersome and places a burden on the
patient.
to reduce the use of contrast agents, diﬀusion-weighted imaging
(dwi) is emerging as a key imaging technique, although currently usu-
ally complementing breast ce-mri.
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43990-2 8.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
https://doi.org/10.1007/978-3-031-43990-2_8
80
t. zhang et al.
reﬁned feature maps, and leverage hierarchical representation informa-
tion fused at diﬀerent scales while utilizing the contributions from diﬀer-
ent sequences from a model-driven approach by introducing the weighted
diﬀerence module.
keywords: contrast-enhanced mri · diﬀusion-weighted imaging ·
deep learning · multi-sequence fusion · breast cancer
1
introduction
breast cancer is the most common cancer and the leading cause of cancer death
in women
early detection of breast cancer allows patients to receive timely
treatment, which may have less burden and a higher probability of survival [6].
among current clinical imaging modalities, magnetic resonance imaging (mri)
has the highest sensitivity for breast cancer detection
however, the use of
gadolinium-based contrast agents (gbca) requires iv-cannulation, which is a
burden to patients, time consuming and cumbersome in a screening situation.
moreover, contrast administration can lead to allergic reactions and ﬁnaly ce-
mri may be associated with nephrogenic systemic ﬁbrosis and lead to bioaccu-
mulation in the brain, posing a potential risk to human health
in
2017, the european medicines agency concluded its review of gbca, conﬁrm-
ing recommendations to restrict the use of certain linear gbca used in mri
body scans and to suspend the authorization of other contrast agents, albeit
macrocyclic agents can still be freely used [10].
with the development of computer technology, artiﬁcial intelligence-based
methods have shown potential in image generation and have received extensive
attention.
among them, synthe-
sis of ce-mri is very important as mentioned above, but few studies have been
done by researchers in this area due to its challenging nature.
therefore, it is necessary to
focus on the most promising sequences to synthesize ce-mri.
diﬀusion-weighted imaging (dwi) is emerging as a key imaging technique
to complement breast ce-mri [3]
ii we invented hierarchical fusion module, weighted diﬀerence module and
multi-sequence attention module to enhance the fusion at diﬀerent scale, to
control the contribution of diﬀerent sequence and maximising the usage of
the information within and across sequences.
all mris were resampled to 1 mm isotropic voxels and uniformly sized, resulting
in volumes of 352 × 352 pixel images with 176 slices per mri, and subsequent
registration was performed based on advanced normalization tools (ants)
the l1-norm is used as a reconstruction loss to measure
the diﬀerence between the reconstructed image and the ground truth.
inspired by adc, a weighted diﬀerence module is
designed, in which the neural network is used to simulate the dynamic analysis
of the ln function, and the element-wise subtraction algorithm is used to extract
the diﬀerentiation features between dwis with diﬀerent b-values, and ﬁnally the
features are weighted to obtain weighted feature maps (fdwi, eq. 2).
[fθl(sl) − fθh(sh)]/(bh − bl)
(2)
where sl and sh represent the image signals obtained from lower b value bl and
higher bh, fθl and fθh represent the corresponding neural networks for dwi with
a lower and higher b value.
the input feature maps (fconcat) go through the maximum pooling layer
and the average pooling layer respectively, and then are added element-wise
after passing through the shared fully connected neural network, and ﬁnally the
weight map as is generated after passing through the activation function, as
shown in eq.
(4)
where ⊗ represents element-wise multiplication, ⊕ represents element-wise
summation, σ represents the sigmoid function, θfc represents the corresponding
network parameters of the shared fully-connected neural network, and avgpool
and maxpool represent average pooling and maximum pooling operations,
respectively.
in the synthesis process, the generator g tries to generate an image according
to the input multi-sequence mri (d1, d2, d3, d4, t1), and the discriminator d
tries to distinguish the generated image g(d1, d2, d3, d4, t1) from the real image
y, and at the same time, the generator tries to generate a realistic image to
mislead the discriminator.
[log (1 − d(g(d1, d2, d3, d4, t1)))]
(6)
where prodata(d1, d2, d3, d4, t1) represents the empirical joint distribution of
inputs d1 (dwib0), d2 (dwib150), d3 (dwib800), d4 (dwib1500) and t1 (t1-
weighted mri), λ1 is a non-negative trade-oﬀ parameter, and l1-norm is used
to measure the diﬀerence between the generated image and the corresponding
ground truth.
the numbers of ﬁlters
are 32, 64, 128, 256 and 512, respectively.
2.3
visualization
the t1-weighted images and the contrast-enhanced images were subtracted to
obtain a diﬀerence mri to clearly reveal the enhanced regions in the ce-mri.
2. detailed structure of the hierarchical fusion module.
2.4
experiment settings
based on the ratio of 8:2, the training set and independent test set of the in-house
dataset have 612 and 153 cases, respectively.
structural similarity index mea-
surement (ssim), peak signal-to-noise ratio (psnr) and normalized mean
synthesis of contrast-enhanced breast mri
85
squared error (nmse) were used as metrics, all formulas as follows:
ssim =
(2μy(x)μg(x) + c1)(2σy(x)g(x) + c2)
(μ2
y(x) + μ2
g(x) +
[5]
t1+dwis
87.58 ± 2.68
27.80 ± 1.56
0.0692 ± 0.035
proposed
t1+dwis
89.93 ± 2.91
28.92 ± 1.63
0.0585 ± 0.026
where g(x) represents a generated image, y(x) represents a ground-truth
image, μy(x) and μg(x) represent the mean of y(x) and g(x), respectively, σy(x)
and σg(x) represent the variance of y(x) and g(x), respectively, σy(x)g(x) repre-
sents the covariance of y(x) and g(x), and c1 and c2 represent positive constants
used to avoid null denominators.
3
results
first, we compare the performance of diﬀerent existing methods on synthetic
ce-mri using our source data, the quantitative indicators used include psnr,
ssim and nmse.
it may
be because their model can only combine bi-modality and cannot integrate the
features of all sequences, so it cannot mine the diﬀerence features between mul-
tiple b-values, which limits the performance of the model.
[5] used full-sequence mri to synthesize ce-mri, it
would be advantageous to obtain synthetic ce-mri images using as little data
as possible, taking advantage of the most contributing sequences.
they did not
take advantage of multi-b-value dwi, nor did they use the hierarchical fusion
module to fully fuse the hierarchical features of multi-sequence mri.
86
t. zhang et al.
fig.
finally, the addition of the multi-sequence attention mod-
ule further improved the performance of the model, with ssim of 89.93 ± 2.91,
psnr of 28.92 ± 1.63, and nmse of 0.0585 ± 0.026.
3. it can be
seen from the visualization results that after the diﬀerence between the generated
ce-mri and the original t1-weighted mri, the lesion position of the breast is
highlighted, the red circle represents the highlighted area, which proves that our
method can eﬀectively synthesize contrast-enhanced images, highlighting the
same parts as the real enhanced position.
see supplementary material for more
visualization results, including visualizations of breast ce-mri synthesized in
axial, coronal, and sagittal planes.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_9.pdf:
early detection and diagnosis of breast cancer using ultrasound images
are crucial for timely diagnostic decision and treatment in clinical application.
however, the similarity between tumors and background and also severe shadow
noises in ultrasound images make accurate segmentation of breast tumor chal-
lenging.
in this paper, we propose a large pre-trained model for breast tumor seg-
mentation, with robust performance when applied to new datasets.
speciﬁcally,
our model is built upon unet backbone with deep supervision for each stage of
the decoder.
besides using dice score, we also design discriminator-based loss
on each stage of the decoder to penalize the distribution dissimilarity from multi-
scales.
our proposed model is validated on a large clinical dataset with more than
10000 cases, and shows signiﬁcant improvement than other representative models.
keywords: large-scale clinical dataset · deep-supervision · multi-scale
segmentation · breast ultrasound images
registration number: 4319
1
introduction
breast cancer is a serious health problem with high incidence and wide prevalence for
women throughout the world
regular screening and early detection are crucial for
effective diagnosis and treatment, and hence for improved prognosis and survival rate [3,
4].
manual delineation always depends on the
experience of radiologists, which tends to be subjective and time-consuming [9, 10].
89–96, 2023.
https://doi.org/10.1007/978-3-031-43990-2_9
90
m. li et al.
therefore, there is a high demand for automatic and robust methods to achieve accurate
breast tumor segmentation.
however, due to speckle noise and shadows in ultrasound
images, breast tumor boundaries tend to be blurry and are difﬁcult to be distinguished
from background.
these issues pose challenges and difﬁculties for accurate
breast tumor segmentation in ultrasound images.
various approaches based on deep learning have been developed for tumor seg-
mentation with promising results [13–19].
[13] designed a multi-scale u-net
to extract more semantic and diverse features for medical image segmentation, using
multiple convolution sequences and convolution kernels with different receptive ﬁelds.
[14] raised a deeply-supervised encoder-decoder network, which is connected
through a series of nested and dense skip pathways to reduce semantic gap between fea-
ture maps.
in [15], a multi-scale selection and multi-channel fusion segmentation model
was built, which gathers global information from multiple receptive ﬁelds and integrates
multi-level features from different network positions for accurate pancreas segmenta-
tion.
[17]
introduced a unet 3+ for medical image segmentation, which incorporates low-level
and high-level feature maps in different scales and learns full-scale aggregated feature
representations.
[18] established a convolution neural network optimized by
super-pixel and support vector machine, segmenting multiple organs from ct scans
to assist physicians diagnosis.
[19] introduced channel and position attention
module into deep learning neural network to obtain contextual information for colorec-
tal tumors segmentation in ct scans.
however, although these proposed models have
achieved satisfactory results in different medical segmentation tasks, their performances
are limited for breast tumor segmentation in ultrasound images due to the low image
contrast and blurry tissue boundary.
to address these challenges, we present, to the best of our knowledge, the ﬁrst
work to adopt multi-scale features collected from large set of clinical ultrasound images
for breast tumor segmentation.
the main contributions of our work are as follows: (1)
we propose a well-pruned simple but effective network for breast tumor segmentation,
which shows remarkable and solid performance on large clinical dataset; (2) our large
pretrained model is evaluated on two additional public datasets without ﬁne-tuning and
shows extremely stabilized improvement, indicating that our model has outstanding
generalizability and good robustness against multi-site data data.
in particular, we apply
similarity constraint for each stage of the unet decoder to obtain consistent and sta-
ble segmentation maps.
instead of using dice score in the ﬁnal layer of unet, we also
use dice loss on each of the decoder stages.
besides, we integrate an adversarial loss
as additional constraint to penalize the distribution dissimilarity between the predicted
developing large pre-trained model for breast tumor segmentation
91
segmentation map and the ground truth.
in the framework of gan, we take our segmen-
tation network as the generator and a convolutional neural network as the discriminator.
therefore, we formulate the overall loss for the generator,
namely the segmentation network, as
loverall =
4
n=1 αn · (1 − 2
p(n) ∩ g
 ·
p(n)
(1)
where sn represents the segmentation network and cn represents the involved convo-
lutional network.
θsn and θcn refer to the parameters in the segmentation and convo-
lutional network, respectively.
p(n) represents the segmentation maps obtained from the
n-th stage in the segmentation network, and g refers to the corresponding ground truth.
psn(proba) denotes the distribution of probability maps.
it should be noted that, in unet, there are 4 stages and hence
we employ 4 cnns for each of them without sharing their weights.
the ﬁrst constraint is used to enhance prediction
similarity to the standard ones, for preliminary segmentation.
the second constraint is used to
capture data distribution to maintain consistency in high-dimensional space for map reﬁnement.
cnθcn(g) represents the
probability for the input of cn coming from the original dataset.
intheimplementation,weupdatethesegmentationnetworkandallthediscriminators
alternatingly in each iteration until both the generator and discriminators are converged.
92
m. li et al.
3
experiments
3.1
dataset and implementation details
we collected 10927 cases for this research from yunnan cancer hospital.
five-fold cross validation is performed on
the dataset in all experiments to verify our proposed network.
in order to com-
prehensively evaluate segmentation efﬁciency of our model, dice similarity coefﬁcient
(dsc), precision, recall, jaccard, and root mean squared error (rmse) are used as
evaluation metrics in this work.
quantitative comparison with state-of-the-art segmentation methods on large-scale
clinical breast ultrasound dataset.
our proposed algorithm is conducted on pytorch, and all experiments are performed
on nvidia tesla a100 gpu.
the epochs to train all models are 100 and the batch size
in training process is set as 4.
3.2
comparison with state-of-the-art methods
to verify the advantages of our proposed model for breast tumor segmentation in ultra-
sound images, we compare our deep-supervised convolutional network with the state-of-
the-art tumor segmentation methods, including deepres
[13], unet++
developing large pre-trained model for breast tumor segmentation
93
[14], segnet [25], attu-net [16], u2-net
the comparison exper-
iments are carried on a large-scale clinical breast ultrasound dataset, and the quantitative
results are reported in table 1.
it is obvious that our proposed model achieves the opti-
mal performance compared with other segmentation models.
these results indicate the effectiveness of
the proposed model in delineating breast tumors in ultrasound images.
representative segmentation results using different methods are provided in fig.
2. segmentation results of ﬁve subjects obtained from different models.
red boxes are used
to highlight the boundaries which are difﬁcult to segment.
fig.
3. five evaluation criteria for total cases from different frameworks: stage i, stage ii, stage
iii, and stage iv (from left to right in each index).
four
groups of frameworks (stage i, stage ii, stage iii and stage iv) are designed, with the
94
m. li et al.
numerals denoting the level of deep supervision counting from the last deconvolutional
layer.
we test these four frameworks on the in-house breast ultrasound dataset, and verify
their segmentation performance using the same ﬁve evaluation criteria.
the obtained quan-
titative results are shown in table 2, where stage iv model achieves the optimal dsc,
precision, recall, and jaccard.
that is, the segmentation ability of the
proposed stage iv is ameliorated from every possible perspective.
moreover, stage
iv obtains minimal rmse compared with other three models (0.68 mm vs 0.84 mm,
0.82 mm, 0.75 mm), which means better matching of the predicted maps from stage iv
with the corresponding ground truth.
all these comparison results verify the superiority
of deep supervision for breast tumor segmentation in ultrasound images.
quantitative results among four groups of segmentation frameworks stage i, stage ii,
stage iii, and stage iv.
method
dsc (%)↑
precision (%)↑
recall (%)↑
jaccard (%)↑
rmse (mm)↓
stage i
70.52 ± 1.25
70.01 ± 1.77
76.16 ± 2.04
55.72 ± 1.53
0.84 ± 0.09
stage ii
73.43 ± 1.07
76.10 ± 1.30
74.19 ± 2.30
58.99 ± 1.37
0.82 ± 0.08
stage iii
77.67 ± 1.06
79.63 ± 1.43
78.17 ± 1.98
64.42 ± 1.47
0.75 ± 0.07
stage iv
80.97 ± 0.88
81.64 ± 1.30
82.19 ± 1.61
68.83 ± 1.38
0.68 ± 0.06
3.4
performance on two external public datasets
in order to evaluate the generalizability of the proposed model, we introduce external
dataset 1 and dataset 2 for external validation experiments.
speciﬁcally, dataset 1 and
dataset 2 are used as testing data to evaluate the generalization performance of the
models trained on our own dataset without ﬁne tuning, and the corresponding results
are shown in table 3.
promising performance demonstrates outstanding generalization
ability of our large pre-trained model, with a dsc score of 81.35% and a recall of
80.96% on dataset 1, and a dsc score of 77.16% and a recall of 93.22% on dataset 2.
table 3.
external validation performance on two independent publicly-available dataset 1 and
dataset 2.
dataset
dsc (%)
precision (%)
recall (%)
jaccard (%)
rmse (mm)
dataset 1
81.35 ± 2.04
85.75 ± 2.97
80.96 ± 2.09
70.65 ± 3.07
0.19 ± 0.01
dataset 2
77.16 ± 2.51
68.34 ± 4.03
93.22 ± 0.65
65.26 ± 3.67
0.42 ± 0.01
developing large pre-trained model for breast tumor segmentation
95
4

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_7.pdf:
however, the eﬀectiveness of deep neural net-
works is often limited by the lack of interpretability and the need for
signiﬁcant amount of manual annotations.
moreover, considering many stud-
ies overlooking interactive information relevant to diagnosis, we accord-
ingly utilize transformer-based attention in our network to mutualize
multi-view pathological information, and further employ a bidirectional
fusion learning (bfl) to more eﬀectively fuse multi-view information.
experimental results demonstrate that our proposed model signiﬁcantly
improves both mammogram classiﬁcation performance and interpretabil-
ity through incorporation of gaze data and cross-view interactive infor-
mation.
keywords: mammogram classiﬁcation · gaze · multi-view
interaction · bidirectional fusion learning
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43990-2 7.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
https://doi.org/10.1007/978-3-031-43990-2_7
mammo-net for multi-view mammogram classiﬁcation
69
1
introduction
breast cancer is the most prevalent form of cancer among women and can have
serious physical and mental health consequences if left unchecked [5].
early detec-
tion through mammography is critical for early treatment and prevention
[19].
mammograms provide images of breast tissue, which are taken from two views:
the cranio-caudal (cc) view, and the medio-lateral oblique (mlo) view [4].
by
identifying breast cancer early, patients can receive targeted treatment before
the disease progresses.
however, these models often require a
large number of manual annotations and lack interpretability, which can prevent
their broader applications in breast cancer diagnosis.
by using gaze data to guide model training,
we can improve model interpretability and performance [24].
radiologists’ eye movements can be automatically and unobtrusively
recorded during the process of reading mammograms, providing a valuable source
of data without the need for manual labeling.
leveraging gaze from radiol-
ogists to aid in model training not only increases eﬃciency and minimizes the risk
of errors linked to manual annotation, but also can be seamlessly implemented
without aﬀecting radiologists’ normal clinical interpretation of mammograms.
mammography primarily detects two types of breast lesions: masses and
microcalciﬁcations [16].
in this work, we propose a novel diagnostic model, namely mammo-net,
which integrates radiologists’ gaze data and interactive information between
cc-view and mlo-view to enhance diagnostic performance.
• we demonstrate the eﬀectiveness of our approach through experiments using
mammography datasets, which show the superiority of mammo-net.
we use several resnet blocks pre-trained on imagenet
then, we use global average pooling (gap) and fully connected
layers to compute the feature vectors produced by the model.
this allows the model to
mimic the attention of radiologists and enhance diagnostic performance.
+ μlbf l.
(8)
3
experiments and results
3.1
datasets
mammogram dataset.
our experiments were conducted on cbis-ddsm
[12] and inbreast [16].
it is worth noting that the oﬃcial inbreast dataset does not provide
image-level labels, so we obtained these labels following shen et al.
eye movement data was collected by reviewing all cases in
inbreast using a tobii pro nano eye tracker.
the scenario is shown in appendix
and can be accessed at https://github.com/jamesqfreeman/miceye.
74
c. ji et al.
3.2
implementation details
we trained our model using the adam optimizer [10] with a learning rate of
10−4 (partly implemented by mindspore).
to overcome the problem of limited
data, we employed various data augmentation techniques, including translation,
rotation, and ﬂipping.
[26]
0.859
0.791
mlo-view
0.663
0.716
cc-view
0.650
0.704
cross-view
0.762
0.755
cross-view+bfl
0.786
0.812
cross-view+ra
0.864
0.830
cross-view+bfl+ra (mammo-net) 0.889 0.849
mammo-net for multi-view mammogram classiﬁcation
75
performance comparison.
we also compare our model with other methods that use eye movement
supervision as shown in table 1.
we believe that one possible reason for the inferior
performance of ga-net compared to mammo-net might be the use of a simple
mse loss by ga-net, which neglects the coarse nature of the gaze data.
[8] proposed a double-model that fuses gaze maps with original images
before training.
this model requires gaze input during both the training
and inference stages, which limits its practical use in hospitals without eye-
trackers.
in contrast, our method does not rely on gaze input during inference
stage.
for each exam, we present gaze heat
maps generated from eye movement data.
the results of the visualization demonstrate that the model’s capability in
localizing lesions becomes more precise when radiologist attention is incorpo-
rated in the training stage.
table 1
suggests that each part of the proposed framework contributes to the increased
performance.
our
experimental results on mammography datasets demonstrate the superiority of
our proposed model.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_6.pdf:
the question of “what the symmet-
rical bi-mg would look like when the asymmetrical abnormalities have
been removed ?” has not yet received strong attention in the develop-
ment of algorithms on mammograms.
we conduct experiments on three pub-
lic and one in-house dataset, and demonstrate that our method outper-
forms existing methods in abnormality classiﬁcation, segmentation, and
localization tasks.
keywords: bilateral mammogram · asymmetric transformer ·
disentanglement · self-adversarial learning · synthesis
1
introduction
breast cancer (bc) is the most common cancer in women and incidence is
increasing [14].
besides using the data-driven manner, to
achieve accurate diagnosis and interpretation of the ai-assisted system output,
it is essential to consider mammogram domain knowledge in a model-driven
fashion.
it can
provide valuable diagnostic information and guide the model in learning the
diagnostic process akin to that of a human radiologist.
recently, two studies
explored generating healthy latent features of target mammograms by referenc-
ing contralateral mammograms, achieving state-of-the-art (sota) classiﬁcation
performance
image generation techniques for gen-
erating symmetric bi-mg have not yet been investigated.
a more interpretable and pristine strategy is disentangle-
ment learning [9,17] which utilizes synthetic images to supervise the model in
separating asymmetric anomalies from normal regions at the image level.
in this work, we present a novel end-to-end framework, disasymnet, which
consists of an asymmetric transformer-based classiﬁcation (asyc) module and an
asymmetric abnormality disentanglement (asyd) module.
additionally, we leverage a
self-adversarial learning scheme to reinforce two modules’ capacity, where the
feedback from the asyc is used to guide the asyd’s disentangling, and the
asyd’s output is used to reﬁne the asyc in detecting subtle abnormalities.
to
disentanglement of asymmetrical abnormality on bilateral mammograms
59
fig.
the schematic overview of the proposed disasymnet.
facilitate the learning of semantic symmetry, we also introduce synthesis, com-
bining randomly created synthetic asymmetrical bi-mg with real mammograms
to supervise the learning process.
(3) we
demonstrate the robustness of our approach on four mammogram datasets for
classiﬁcation, segmentation, and localization tasks.
we employ an online class activation mapping (cam) module
[10,11] to generate heatmaps for segmentation and localization.
the sa and ca modules use multi-head attention (mha),
ψh=8
mha(fq, fk, fv ) with the number of heads h = 8, which is a standard compo-
nent in transformers and has already gained popularity in medical image ﬁelds
[1,16,26].
then, the starting feature f, and the attention fea-
tures fsa and fca are concatenated in the channel dimension and fed into the
ffn layers to fuse the information and maintain the same size as f. the trans-
former block is repeated n = 12 times to iteratively integrate information from
bi-mg, resulting in the output feature f r
out, f l
out = ψn=12
asyt (f r, f l).
unlike previous studies [18,19], which only generated normal features
in the latent space, our asyd module use weights shared u-net-like decoders ψg,
to generate both abnormal (xab) and normal (xn) images for each side through
a two-channel separation, as xn, xab = ψg(fout).
we constrain the model to
reconstruct images realistically using l1 loss (ll1) with the guidance of cams
(m), as follows, lrec = ll1((1−m)x, (1−m)xn)+ll1(mx, xab).
however, it is
disentanglement of asymmetrical abnormality on bilateral mammograms
61
diﬃcult to train the generator in a supervised manner due to the lack of annota-
tions of the location for asymmetrical pairs.
2.3
asymmetric synthesis for supervised reconstruction
to alleviate the lack of annotation pixel-wise asymmetry annotations, in this
study, we propose a random synthesis method to supervise disentanglement.
training with synthetic artifacts is a low-cost but eﬃcient way to supervise the
model to better reconstruct images [15,17].
the alpha weights αk is a 2d gaussian distribution map, in which the co-variance
is determined by the size of k-th tumor t, representing the transparency of the
pixels of the tumor.
speciﬁcally, to maintain the rule of weakly-
supervised learning of segmentation and localization tasks, we collect the tumors
from the ddsm dataset as t and train the model on the inbreast dataset.
thus, the supervised reconstruction loss is lsyn =
ll1(x|real, xn|fake), where x|real is the real image before synthesis and xn|fake
is the disentangled normal image from the synthesised image x|fake.
2.4
loss function
for each training step, there are two objectives, training asyc and asyd mod-
ule, and then is the reﬁnement of asyc.
the values of weight terms
λ1, λ2, λ3, and λ4 are experimentally set to be 1, 0.1, 1, and 0.5, respectively.
the loss of the second objective is lrefine as aforementioned.
3
experimental
3.1
datasets
this study reports experiments on four mammography datasets.
the inbreast
dataset [7] consists of 115 exams with bi-rads labels and pixel-wise anno-
62
x. wang et al.
tations, comprising a total of 87 normal (bi-rads = 1) and 342 abnormal
(bi-rads ̸= 1) images.
the ddsm dataset [3] consists of 2,620 cases, encom-
passing 6,406 normal and 4,042 (benign and malignant) images with outlines
generated by an experienced mammographer.
the vindr-mammo dataset [8]
includes 5,000 cases with bi-rads assessments and bounding box annotations,
consisting of 13,404 normal (bi-rads = 1) and 6,580 abnormal (bi-rads
̸= 1) images.
the in-house dataset comprises 43,258 mammography exams from
10,670 women between 2004–2020, collected from a hospital with irb approvals.
in this study, we randomly select 20% women of the full dataset, comprising 6,000
normal (bi-rads = 1) and 28,732 abnormal (bi-rads ̸= 1) images.
2. abnormality classiﬁcation performance of disasymnet in terms of auc trained
on diﬀerent sizes of training sets.
disentanglement of asymmetrical abnormality on bilateral mammograms
63
3.2
experimental settings
the mammogram pre-processing is conducted following the pipeline proposed by
[5].
then we standardize the image size to 1024 × 512 pixels.
for training models,
we employ random zooming and random cropping for data augmentation.
we
employ the resnet-18 [2] with on imagenet pre-trained weights as the common
backbone for all methods.
all experiments are implemented in
the pytorch framework and an nvidia rtx a6000 gpu (48 gb).
the training
takes 3–24 h (related to the size of the dataset) on each dataset.
to assess the performance of diﬀerent models in classiﬁcation tasks, we
calculate the area under the receiver operating characteristic curve (auc) met-
ric.
for the segmentation task, we utilize intersec-
tion over union (iou), intersection over reference (ior), and dice coeﬃcients.
3.3
experimental results
we compare our proposed disasymnet with single view-based baseline
resnet18,
[16], and
attention-based mv methods proposed by wang et al., [20] on classiﬁcation, seg-
mentation, and localization tasks.
the features from the bi-mg are simply concatenated and passed to
the classiﬁer.
comparison of performance in diﬀerent tasks: for the classiﬁcation
task, the auc results of abnormal classiﬁcation are shown in table 1.
additionally,
our “asyd” only method improves the performance compared to the late-fusion
method, demonstrating that our disentanglement-based self-adversarial learn-
ing strategy can reﬁne classiﬁers and enhance the model’s ability to classify
anomalies and asymmetries.
the proposed “synthesis” method further enhances
64
x. wang et al.
table 2. comparison of weakly supervised abnormalities segmentation and localization
tasks on public datasets.
segmentation task
localization task
inbreast
inbreast
ddsm
vindr-mammo
methods
iou
ior
dice
mean tiou mean tior mean tiou mean tior mean tiou mean tior
resnet18
(color ﬁgure online)
the performance of our proposed method.
moreover, we investigate the abil-
ity of diﬀerent methods to classify abnormalities under various percentages
of ddsm, vindr, and in-house datasets.
the inbreast dataset was excluded
from this experiment due to its small size.
figure 2 illustrates the robustness of
our method’s advantage and our approach consistently outperformed the other
methods, regardless of the size of the training data used and data sources.
for
the weakly supervised segmentation and localization tasks, results are shown in
table 2.
the results demonstrate that our proposed framework achieves superior
segmentation and localization performance compared to other existing methods
across all evaluation metrics.
the results of the ablation experiment also reveal
that all modules incorporated in our framework oﬀer improvements for the tasks.
without using pixel-level asymmetry ground
disentanglement of asymmetrical abnormality on bilateral mammograms
65
truth from the “synthesis” method, our generator tends to excessively remove
asymmetric abnormalities at the cost of leading to the formation of black holes or
areas that are visibly darker than the surrounding tissue because of the limitation
of our discriminator and lack of pixel-level supervision.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_40.pdf:
despite recent developments in ct planning that enabled
automation in patient positioning, time-consuming scout scans are still
needed to compute dose proﬁle and ensure the patient is properly posi-
tioned.
in this paper, we present a novel method which eliminates the
need for scout scans in ct lung cancer screening by estimating patient
scan range, isocenter, and water equivalent diameter (wed) from 3d
camera images.
we demonstrate the eﬀec-
tiveness of our method on a testing set of 110 pairs of depth data and ct
scan, resulting in an average error of 5 mm in estimating the isocenter,
13 mm in determining the scan range, 10 mm and 16 mm in estimating
the ap and lateral wed respectively.
ct lung cancer screening is a low-
dose ct (ldct) scan of the chest that can detect lung cancer at an early stage,
when it is most treatable.
however, the current workﬂow for performing ct lung
scans still requires an experienced technician to manually perform pre-scanning
steps, which greatly decreases the throughput of this high volume procedure.
while recent advances in human body modeling [4,5,12,13,15] have allowed for
automation of patient positioning, scout scans are still required as they are used
by automatic exposure control system in the ct scanners to compute the dose
to be delivered in order to maintain constant image quality [3].
any patient movement during the time between the two scans may cause mis-
alignment and incorrect dose proﬁle, which could ultimately result in a repeat
of the entire process.
we introduce a novel method for estimating patient scanning parameters
from non-ionizing 3d camera images to eliminate the need for scout scans dur-
ing pre-scanning.
addi-
tionally, we introduce a novel approach for updating the estimated wed in
real-time, which allows for reﬁnement of the scan parameters during acquisition,
thus increasing accuracy.
the contributions of this work can be summarized as follows:
– a novel workﬂow for automated ct lung cancer screening without the need
for scout scan
– a clinically relevant method meeting iec 62985:2019 requirements on wed
estimation.
– a generative model of patient wed trained on over 60, 000 patients.
– a novel method for real-time reﬁnement of wed, which can be used for dose
modulation
2
method
water equivalent diameter (wed) is a robust patient-size descriptor [17] used
for ct dose planning.
it represents the diameter of a cylinder of water having
the same averaged absorbed dose as the material contained in an axial plane at a
given craniocaudal position z
while wed can be derived from ct images, paired ct scans and camera
images are rarely available, making direct regression through supervised learning
challenging.
we propose a semi-supervised approach to estimate wed from
depth images.
we then train an encoder network to map the patient depth image to
the wed manifold.
426
b. teixeira et al.
2.2
depth encoder training
after training our generative model on a large collection of unpaired ct scans,
we train our encoder network on a smaller collection of paired depth images
and ct scans.
[1] taking as input
the depth image and outputting a latent vector in the previously learned latent
space.
2.3
real-time wed reﬁnement
while the depth image provides critical information on the patient anatomy,
it may not always be suﬃcient to accurately predict the wed proﬁles.
for
example, some patients may have implants or other medical devices that cannot
be guessed solely from the depth image.
additionally, since the encoder is trained
on a smaller data collection, it may not be able to perfectly project the depth
image to the wed manifold.
first, we use our encoder network to initialize
the latent vector to a point in the manifold that is close to the current patient.
after the latent vector
has been optimized to ﬁt the previously scanned data, a large deviation between
the optimized prediction and the ground truth proﬁles may indicate that our
approach is not able to ﬁnd a point in the manifold that is close to the data.
(color ﬁgure online)
of depth image and ct scan from 2, 742 patients from 6 diﬀerent sites across
north america and europe acquired using a ceiling-mounted kinect 2 camera.
our evaluation set consists of 110 pairs of depth image and ct scan from 110
patients from a separate site in europe.
3.2
patient preparation
patient positioning is the ﬁrst step in lung cancer screening workﬂow.
[7] taking the camera depth
image as input and outputting a gaussian heatmap centered at the patient’s lung
top location.
to ensure the lung is fully visible in the ct image, we
added a 2 cm oﬀset on our prediction towards the outside of the lung.
we then
deﬁned the accuracy as whether the lung is fully visible in the ct image when
using the oﬀset prediction.
wed proﬁle regression with and without real-
time reﬁnement.
second column shows the performance of our model without real-time reﬁnement.
third and fourth columns show the performance of our model with real-time reﬁnement
every 5 cm and 2 cm respectively.
while the original prediction was oﬀ towards the center of the lung,
the real-time reﬁnement was able to correct the error.
isocenter.
[1] taking the camera depth image as input and
outputting the patient isocenter.
the encoder was trained on our paired
ct scan and depth image dataset of 2, 742 patients.
[7]
taking the camera depth image as input and outputting the water equivalent
diameter proﬁle.
method (lateral)
mean error 90th perc error max error
direct regression
45.07
76.70
101.50
proposed (initial)
27.06
52.88
79.27
proposed (reﬁned, w = 5) 19.18
42.44
73.69
proposed (reﬁned, w = 2) 15.93
35.93
61.68
method (ap)
direct regression
45.71
71.85
82.84
proposed (initial)
16.52
31.00
40.89
proposed (reﬁned, w = 5) 12.19
25.73
37.36
proposed (reﬁned, w = 2) 10.40
22.44
33.85
then measured the performance of our model before and after diﬀerent degrees
of real-time reﬁnement, using the same optimizer and learning rate.
bringing in real-time reﬁnement greatly improves the results with a
mean lateral error over 40% and a 90th percentile lateral error over 20% lower
than before reﬁnement.
ap proﬁles show similar results with a mean ap error
improvement of nearly 40% and a 90th percentile ap error improvement close
to 30%.
when using our proposed method with a 20 mm window reﬁnement,
our proposed approach outperforms the direct regression baseline by over 60%
for lateral proﬁle and nearly 80% for ap.
figures 3 highlights the beneﬁts of using real-time reﬁnement.
4. qualitative analysis of the proposed method with 2 cm reﬁnement on patient
with diﬀerent morphology.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_54.pdf:
colorectal polyps detected during colonoscopy are strongly
associated with colorectal cancer, making polyp segmentation a critical
clinical decision-making tool for diagnosis and treatment planning.
how-
ever, accurate polyp segmentation remains a challenging task, particu-
larly in cases involving diminutive polyps and other intestinal substances
that produce a high false-positive rate.
previous polyp segmentation net-
works based on supervised binary masks may have lacked global seman-
tic perception of polyps, resulting in a loss of capture and discrimination
capability for polyps in complex scenarios.
to address this issue, we
propose a novel gaussian-probabilistic guided semantic fusion method
that progressively fuses the probability information of polyp positions
with the decoder supervised by binary masks.
extensive experiments on ﬁve widely adopted datasets
show that petnet outperforms existing methods in identifying polyp
t. ling and c. wu—equal contributions.
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43990-2_54.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
https://doi.org/10.1007/978-3-031-43990-2_54
petnet improves complex polyp segmentation
573
camouﬂage, appearance changes, and small polyp scenes, and achieves a
speed about 27fps in edge computing devices.
keywords: colonoscopy · polyp segmentation · vision transformer
1
introduction
colorectal cancer (crc) remains a major health burden with elevated mortal-
ity worldwide [1].
polyp segmentation is a fundamental task
in the computer-aided detection (cade) of polyps during colonoscopy, which is
of great signiﬁcance in the clinical prevention of crc.
traditional machine learning approaches in polyp segmentation primarily
focus on learning low-level features, such as texture, shape, or color distribu-
tion
furthermore, transformer [3,17,19,20] models have also been proposed for
polyp segmentation, and achieve the state-of-the-art(sota) performance.
however, supervised segmentation learning solely based on binary
masks may not be eﬀective in discriminating polyps in complex clinical scenarios.
endoscopic images often contain pseudo-polyp objects with strong boundaries,
such as colon folds, blood vessels, and air bubbles, which can result in false
positives.
however, this method has limitations in accurately segmenting
polyp boundaries, which are crucial for clinical decision-making.
therefore, the primary challenge lies in enhancing polyp segmentation per-
formance in complex scenarios by precisely preserving the polyp segmentation
boundaries, while simultaneously maximizing the decoder’s attention on the
overall pattern of the polyps.
in this paper, we propose a novel transformer-based polyp segmentation
framework, petnet, which addresses the aforementioned challenges and achieves
sota performance in locating polyps with high precision.
our contributions are
threefold:
• we propose a novel gaussian-probabilistic guided semantic fusion method for
polyp segmentation, which improves the decoder’s global perception of polyp
locations and discrimination capability for polyps in complex scenarios.
574
t. ling et al.
• we evaluate the performance of petnet on ﬁve widely adopted datasets,
demonstrating its superior ability to identify polyp camouﬂage and small
polyp scenes, achieving state-of-the-art performance in locating polyps with
high precision.
• we design several polyp instance-level evaluation metrics, considering that
conventional pixel-level calculation methods cannot explicitly and compre-
hensively evaluate the overall performance of polyp segmentation algorithms.
1, petnet is an end-to-end polyp segmentation framework con-
sists of three core module groups.
(b) depicts the stage encoder.
petnet improves complex polyp segmentation
575
fig.
we add a
mta layer to encode the last level features, enhancing the model’s semantic
representation and accelerating the training process [16].
moreover, the encoder
output features are presented as {xe
i }4
i=1 with channels of [2c, 4c, 8c, 16c].
2.3
gaussian-probabilistic modeling group
to incorporate both polyp location probability and surface pattern information
in a progressive manner, we propose the gaussian probabilistic-induced tran-
sition (git) method.
+ (y − yo)2
(1)
where (xo, yo) is the mass of each polyp in the binary image f(x, y).
finally, we determine the ﬁnal gaussian
probabilistic mask pg for all polyps within an image mask by computing the
element-wise maximum.
∈ r(ci+cg)×hi×wi in an multi-layer sandwiches manner.
we further introduce residual learn-
ing in a parallel manner at diﬀerent group-aware scales.
building on this manner, we pro-
pose the ensemble method that integrates multiple simple decoders to enhance
petnet improves complex polyp segmentation
577
the detection and discrimination of diﬃcult polyp samples.
the output mask p is obtained by element-wise
summation of pi, where i represents the binary decoder index.
in our evaluation, we also examine the decoder cfm utilized in [3],
which shares the same input features (excluding the ﬁrst level) as the fus.
3
experiments
3.1
datasets settings
to evaluate models fairly, we completely follow pranet
[4] and use ﬁve public
datasets, including 548 and 900 images from clinicdb
[5]
as training sets, and the remaining images as validation sets.
we also test the
generalization capability of all models on three unseen datasets (etis [13] with
196 images, cvc-colondb
[8] with 380 images, and endoscene
[15] with 60
images).
further-
more, we employ intermediate decoder outputs to calculate auxiliary losses for
convergence acceleration.
578
t. ling et al.
3.3
evaluation metrics
conventional evaluation metrics for polyp segmentation are typically limited to
pixel-level calculations.
false positives (fps) occur when a wrong detection
output is provided for a negative region, and false negatives (fns) occur
when a polyp is missed in a positive image.
table s1 displays the results of our model’s
training and learning performance.
our model achieves comparable performance
to the sota model on the kvasir-seg and clinicdb datasets.
results show that
petnet achieves excellent generalization performance compared with previous
models.
we also observe a
performance mismatch phenomenon in pixel-level evaluation and instance-level
evaluation.
we selected images from two unseen datasets with 0∼2% polyp labeled area
to perform the test.
as shown, petnet demonstrates great strength in both
datasets, which indicates that one of the major advantages of our model lies in
detecting small polyps with lower false-positive rates.
table 3 presents the results of our ablation study, where we
investigate the contribution of the two key components of our model, namely the
gaussian-probabilistic guided semantic fusion method and ensemble decoders.
we observe that while the impact of each binary decoder varies, all sub binary
decoders contribute to the overall performance.
furthermore, the git method
signiﬁcantly enhances instance-level evaluation without incurring performance
penalty in pixel-level evaluation, especially in unseen datasets.
petnet improves complex polyp segmentation
579
table 1.
small polyps are deﬁned as the polyp area accounts for 0∼2% of the
entire image.
, we deployed petnet on the edge computing device nvidia jetson
orin and optimized its performance using tensorrt.
our results demonstrate
that petnet achieves real-time denoising and segmentation of polyps with high
accuracy, achieving a speed of 27 frames per second on the device(video s1).

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_68.pdf:
age-related macular degeneration (amd) is the leading
cause of blindness in the elderly.
current grading systems based on imag-
ing biomarkers only coarsely group disease stages into broad categories
that lack prognostic value for future disease progression.
in quantitative experi-
ments we found our method yields temporal biomarkers that are predic-
tive of conversion to late amd.
furthermore, these clusters were highly
interpretable to ophthalmologists who conﬁrmed that many of the clus-
ters represent dynamics that have previously been linked to the progres-
sion of amd, even though they are currently not included in any clinical
grading system.
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43990-2_68.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
https://doi.org/10.1007/978-3-031-43990-2_68
clustering disease trajectories for temporal biomarker proposal in amd
725
keywords: contrastive learning · biomarker discovery · clustering ·
disease trajectories · age-related macular degeneration
1
introduction
age-related macular degeneration (amd) is the leading cause of blindness in the
elderly, aﬀecting nearly 200 million people worldwide [24].
patients with early
stages of the disease exhibit few symptoms until suddenly converting to the late
stage, at which point their central vision rapidly deteriorates
[12]. clinicians
currently diagnose amd, and stratify patients, using biomarkers derived from
optical coherence tomography (oct), which provides high-resolution images of
fig.
1. our method ﬁnds common patterns of disease progression in datasets of lon-
gitudinal images.
others have proposed deep-learning-
based methods to discover new biomarkers at scale by clustering oct images
or detecting anomalous features [17,18,23].
however, these approaches neglect
temporal relationships between images and the obtained biomarkers are by def-
inition static and cannot capture the dynamic nature of the disease.
in experiments
involving 160,558 retinal scans, four ophthalmologists veriﬁed that our method
identiﬁed several candidates for temporal biomarkers of amd.
moreover, our
clusters demonstrated greater prognostic value for late-stage amd when com-
pared to the widely adopted amd grading system.
the established
amd grading system stratiﬁes early and intermediate stages solely by the size
of drusen in a single oct image [1,6,7,10].
the degree of atrophy can be staged using crora (complete retinal pigment
epithelium and outer retinal atrophy), which measures the width in μm of focal
atrophy in oct
however, this only works if biomarkers are known
a priori and requires manual annotation of entire time series.
automated discovery of unknown biomarkers: prior work for automated
biomarker discovery in amd explores the latent feature space of encoders trained
for image reconstruction [18,23], segmentation
however, these neural networks are prone to overﬁt to their speciﬁc
task and lose semantic information regarding the disease.
contrastive methods
[3,8,26] encode invariance to a set of image transformations, which are uncorre-
lated with disease features, resulting in a more expressive feature space.
however, all aforementioned methods group single images acquired at one point
in time, and in doing so neglect temporal dynamics.
[5].
3
materials and methods
3.1
oct image datasets
we use two retinal oct datasets curated in the scope of the pinnacle study
[20].
we ﬁrst design and test our method on a development dataset, which was
collected from the southampton eye unit.
all images were acquired using topcon 3d oct devices (topcon cor-
poration, tokyo, japan).
after strict quality control, the development dataset
consists of 46,496 scans of 6,236 eyes from 3,456 patients.
eyes were scanned 7.7
times over 1.9 years on average at irregular time intervals.
eyes were
scanned 16.6 times over 3.5 years on average.
[8] in eq. 1 to train a resnet50 (4x) model f
over each batch of twice transformed images x
l(x)
= 2 − 2
⟨f(x), f ′(x)⟩
||f(x)||2 · ||f ′(x)||2
(1)
where the output of the momentum updated ‘teacher’ network f ′ is passed
through a stop-gradient, so that only the student network f is updated.
as
several of the contrastive transformations designed for natural images are inap-
plicable to medical images, such as solarisation, colour shift and greyscale, we
use the set tailored for retinal oct images by holland et al.
[9]. models were
trained on the entire dataset for 120,000 steps using the adam optimiser with a
momentum of 0.9, weight decay of 1.5·10−6 and a learning rate of 5·10−4.
after
training f, we ﬁrst remove the ﬁnal linear layer before projecting all labelled
images to the feature space of 2048 dimensions.
fig.
we illustrate clusters assignments, denoted by colour,
resulting from three combinations of φ and λ.
3.3
extracting sub-trajectories via partitioning
naively clustering whole time series of patients ignores two characteristics of
longitudinal data.
firstly, individual time series are not directly comparable as
patients enter and leave the study at diﬀerent stages of their overall progression.
secondly, longer time series can record multiple successive transitions in disease
stage.
for each eye, we ﬁrst form piecewise-linear trajectories by linking points in fea-
ture space that were derived from consecutively acquired oct images.
we then
extract sub-trajectories by ﬁnding all sequences of images spanning 1.0 ± 0.5
years of elapsed time within each trajectory.
next, to avoid oversampling trajec-
tories with a shorter time interval between images, we randomly sample at most
one sub-trajectory in every 0.5-year time interval.
however, by ignoring intermediary images, this metric does not respect the
disease pathway along which patients progress.
dtw ﬁnds the optimal temporal alignment between
two time series before computing their distance.
this re-alignment allows us to
match sub-trajectories that traverse the same disease states in the same order,
irrespective of the rate of change between states.
3. we show four clusters from the development dataset (left half) and the equiv-
alent clusters in the unseen dataset (right half).
clusters show two representative sub-trajectories originating from
diﬀerent patients, each containing ﬁve longitudinal images with the time and location
of greatest progression marked by arrows.
3.5
qualitative and quantitative evaluation of clusters
initially, we tune the hyperparameters, λ, φ and k, on the development dataset
by heuristically selecting values that result in higher uniformity between sub-
trajectories within each cluster.
the oph-
thalmologists then review these clusters and conﬁrm whether they capture the
clustering disease trajectories for temporal biomarker proposal in amd
731
same temporal biomarkers observed in the development dataset.
we also
include a demographic baseline using age and sex.
finally, to demonstrate
the performance gap between our interpretable approach and black-box super-
vised learning algorithms, we include a fully supervised deep learning baseline
by ﬁtting an svr directly to the feature space.
each experiment uses 10-fold
cross validation on random 80/20 partitions, while ensuring a patient-wise split.
finally, we repeat the entire method, starting from sub-trajectory extraction,
followed by clustering and then regression experiments, using 7 random seeds
and report the means and standard deviations.
development dataset
time to late amd ↓ time to cnv ↓ time to crora ↓ current visual acuity ↓
demographic
0.756±0.01
0.822±0.012
0.703±0.028
0.381±0.007
current grading system
0.757±0.01
0.819±0.012
0.685±0.035
0.367±0.008
single timepoint clusters
0.747±0.013
0.776±0.015
0.630±0.05
0.230±0.005
sub-trajectory clusters
0.739±0.01
0.748±0.011
0.636±0.031
0.375±0.007
fully supervised
0.709±0.015
0.726±0.012
0.609±0.033
0.199±0.004
unseen dataset
demographic
1.343±0.027
1.241±0.017
1.216±0.062
0.188±0.007
current grading system
1.308±0.018
1.244±0.022
1.286±0.053
0.177±0.008
single timepoint clusters
1.325±0.049
1.341±0.080
1.297±0.096
0.136±0.005
sub-trajectory clusters
1.322±0.029
1.235±0.027
1.257±0.056
0.188±0.006
fully supervised
1.301±0.044
1.298±0.08
1.255±0.097
0.135±0.006
4
experiments and results
sub-trajectory clusters are candidate temporal biomarkers: by ﬁrst
applying our method to the development dataset we found that using λ = 0.75,
φ = 0.75 and k = 30 resulted in the most uniform and homogeneous clusters
while still limiting the total number of clusters to a reasonable amount.
achiev-
ing the same cluster quality with smaller values of φ required many more clusters
in order to encode all combinations of possible start and end disease states.
the
expert ophthalmologists remarked that many of the identiﬁed clusters capture
732
r. holland et al.
dynamics that have already been linked to the progression of amd, even though
they are not currently included in any clinical grading system.
they named these as ‘rapid growth of drusen pig-
ment epithelial detachments (ped)’, ‘regression of drusen ped’, ‘development
of subretinal ﬂuid’, ‘development of intraretinal ﬂuid’, ‘development of hyper-
transmission’ and ‘stable state’ (no signs of progression at each disease state).
in all tasks the standard biomarkers are only marginally more indicative of risk
than the patient’s age and sex.
this experiment conﬁrms that our clusters are
related to disease progression.
5
discussion and conclusion
motivated to improve inadequate grading systems for amd that do not incor-
porate temporal dynamics we developed a method to automatically propose
biomarkers that are time-dependent, interpretable, and predictive of conversion
to late-stage amd.
furthermore, we experimentally demonstrated that the found clusters predict
conversion to late-stage amd on par with the established grading system.
in the future, biomarkers identiﬁed by our method can be further reﬁned
by clinicians.
we will also use the full volumetric image to model progression
dynamics outside the macular.
as late stage patients were overrepresented in our
datasets, we also intend to apply our method to datasets with greater numbers
of patients progressing from earlier disease stages.
ultimately, we envision that
proposals from our method may inform the next generation of grading systems
for amd that incorporate the temporal dimension intrinsic to this dynamic dis-
ease.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_4.pdf:
multi-organ segmentation in abdominal computed tomog-
raphy (ct) images is of great importance for diagnosis of abdominal
lesions and subsequent treatment planning.
though deep learning based
methods have attained high performance, they rely heavily on large-
scale pixel-level annotations that are time-consuming and labor-intensive
to obtain.
due to its low dependency on annotation, weakly supervised
segmentation has attracted great attention.
however, there is still a large
performance gap between current weakly-supervised methods and fully
supervised learning, leaving room for exploration.
in this work, we pro-
pose a novel 3d framework with two consistency constraints for scribble-
supervised multiple abdominal organ segmentation from ct. speciﬁ-
cally, we employ a triple-branch multi-dilated network (tdnet) with
one encoder and three decoders using diﬀerent dilation rates to cap-
ture features from diﬀerent receptive ﬁelds that are complementary to
each other to generate high-quality soft pseudo labels.
experiments on
the public word dataset show that our method outperforms ﬁve exist-
ing scribble-supervised methods.
keywords: weakly-supervised learning · scribble annotation ·
uncertainty · consistency
1
introduction
abdominal organ segmentation from medical images is an essential work in clin-
ical diagnosis and treatment planning of abdominal lesions [17].
https://doi.org/10.1007/978-3-031-43990-2_4
34
m. han et al.
learning methods based on convolution neural network (cnn) have achieved
impressive performance in medical image segmentation tasks [2,24].
[5], and image-
level tags
compared with the other weak annotations, scribbles can
provide more location information about the segmentation targets, especially
for objects with irregular shapes [1].
therefore, this work focuses on exploring
high-performance models for multiple abdominal organ segmentation based on
scribble annotations.
training cnns for segmentation with scribble annotations has been increas-
ingly studied recently.
pseudo label learning methods deal with unannotated pixels by generating fake
semantic labels for learning.
[11] proposed to leverage minimum span-
ning trees to generate low-level and high-level aﬃnity matrices based on color
information and semantic features to reﬁne the pseudo labels.
[22] introduced the condi-
tional random field (crf) regularization loss for image segmentation directly.
recently,
consistency strategies that encourage consistent outputs of the network for the
same input under diﬀerent perturbations have achieved increasing attentions.
[26] proposed a framework composed
of mix augmentation and cycle consistency.
although these scribble-supervised
methods have achieved promising results, their performance is still much lower
than that of fully-supervised training, leaving room for improvement.
diﬀerently from most existing weakly supervised methods that are designed
for 2d slice segmentation with a single or few organs, we propose a highly opti-
mized 3d triple-branch network with one encoder and three diﬀerent decoders,
named tdnet, to learn from scribble annotations for segmentation of multiple
abdominal organs.
particularly, the decoders are assigned with diﬀerent dilation
rates [25] to learn features from diﬀerent receptive ﬁelds that are complementary
to each other for segmentation, which also improves the robustness of dealing
with organs at diﬀerent scales as well as the feature learning ability of the shared
encoder.
in addition, we extend the consistency to the class-related information
scribble-based 3d multiple abdominal organ segmentation
35
ℒ
ℒ
uncertainty 
rectified
ℒ
ℒ
image
scribble
1
2
encoder
decoder 
1, 
= 1
decoder 
, 
= 6
decoder 
2, 
= 3
1
2
2
1
ℒ
ℒ
ℒ
−
−
×
∗ ×
×
× (∗)
project in sagittal view
reshape matrix
×
multiply operation
transpose matrix 
(b) class affinity calculation
(a) the proposed tdnet
project in coronal view
project in axial view
class affinity calculation
fig.
speciﬁ-
cally, we generate the class aﬃnity matrices in diﬀerent decoders and encourage
them to be consistent after projection in diﬀerent views.
the contributions of this paper are summarized as follows: 1) we propose
a novel 3d triple-branch multi-dilated network called tdnet for scribble-
supervised segmentation.
by equipping with varying dilation rates, the network
can better leverage multi-scale context for dealing with organs at diﬀerent scales.
2) we propose two novel consistency loss functions, i.e., uncertainty-weighted
soft pseudo label consistency (uspc) loss and multi-view projection-based
class-similarity consistency (mpcc) loss, to regularize the prediction from the
pixel-wise and class-wise perspectives respectively, which helps the segmenta-
tion network obtain reliable predictions on unannotated pixels.
3) experiments
results show our proposed method outperforms ﬁve existing scribble-supervised
methods on the public dataset word [17] for multiple abdominal organ seg-
mentation.
2
method
figure 1 shows the proposed framework for scribble-supervised medical image
segmentation.
the decoders’ outputs are
averaged to generate a soft pseudo label that is rectiﬁed by uncertainty and then
used to supervise each branch.
to better deal with multi-class segmentation, a
class similarity consistency loss is also used for regularization.
let x, s be a training image and the corresponding scribble
annotation, respectively.
let c denote the number of classes for segmentation,
and ω = ωs ∪ ωu denote the whole set of voxels in x, where ωs is the set of
labeled pixels annotated in s, and ωu is the unlabeled pixel set.
decoders using con-
volution with large dilation rates can better leverage the global information but
may lose some details for accurate segmentation.
in this work, our tdnet is
implemented by introducing two auxiliary decoders into a 3d unet
as the
three decoders capture features at diﬀerent scales that are complementary to
each other, an ensemble of them would be more robust than a single branch.
therefore, we take an average of p1, p2, p3 to get a better soft pseudo label
¯p = (p1 + p2 + p3)/3 that is used to supervise each branch during training.
kl() is the kullback-
scribble-based 3d multiple abdominal organ segmentation
37
leibler divergence.
multi-view projection-based class-similarity consistency (mpcc).
for multi-class segmentation tasks, it is important to learn inter-class relation-
ship for better distinguishing them.
1. in order to save computing
resources, we project the soft pseudo labels along each dimension and then cal-
culate the aﬃnity matrices, which also strengthens the class relationship infor-
mation learning.
here, the aﬃnity matrices
represents the relationship between any pair of classes along the dimensions.
then we constraint the consistency among the corresponding aﬃnity matrices
by multi-view projection-based class-similarity consistency (mpcc) loss:
lmp cc =
1
3 × 3

v

n=1,2,3
kl(qv
n∥ ¯qv)
(3)
where v ∈ {axial, sagittal, coronal} is the view index, and ¯qv is the average class
aﬃnity matrix in a certain view obtained by the three decoders.
in this way, the model can learn
accurate information from scribble annotations, which also avoids getting stuck
in a degenerate solution due to low-quality pseudo labels at an early stage.
3
experiments and results
3.1
dataset and implementation details
we used the publicly available abdomen ct dataset word [17] for experiments,
which consists of 150 abdominal ct volumes from patients with rectal cancer,
prostate cancer or cervical cancer before radiotherapy.
we aimed to segment seven organs: the liver,
spleen, left kidney, right kidney, stomach, gallbladder and pancreas.
we used the commonly-
adopted dice similarity coeﬃcient (dsc), 95% hausdorﬀ distance (hd95) and
the average surface distance (asd) for quantitative evaluation.
our framework was implemented in pytorch
[3] as the backbone network for
all experiments, and extended it with three decoders by embedding two auxil-
iary decoders with diﬀerent dilation rates, as detailed in sect.
the stochastic gradient descent (sgd) optimizer with
momentum of 0.9 and weight decay of 10−4 was used to minimize the overall
loss function formulated in eq. 5, where α=10.0 and β=1.0 based on the best
performance on the validation set.
the ﬁnal segmentation
results were obtained by using a sliding window strategy.
for a fair comparison,
we used the primary decoder’s outputs as the ﬁnal results during the inference
stage and did not use any post-processing methods.
note that all experiments
were conducted in the same experimental setting.
the existing methods are
implemented with the help of open source codebase from [14].
scribble-based 3d multiple abdominal organ segmentation
39
table 1.
best view in color.
3.2
comparison with other methods
we compared our method with ﬁve weakly supervised segmentation meth-
ods with the same set of scribbles, including pce only [12], total variation
loss (tv)
[15], the average dsc was
increased by 2.67 percent points, and the average asd and hd95 were decreased
by 5.44 mm and 16.16 mm, respectively.
[9] obtained a
worse performance than pce, which is mainly because that method classiﬁes pix-
els by minimizing the intra-class intensity variance, making it diﬃcult to achieve
good segmentation due to the low contrast.
3.
visualization
of
the
improvement obtained by using
diﬀerent dilation rates and uncer-
tainty rectifying.
it can be obviously seen that the results obtained by
our method are closer to the ground truth, with less mis-segmentation in both
slice level and volume level.
3.3
ablation experiment
we then performed ablation experiments to investigate the contribution of
each part of our method, and the quantitative results on the validation set
are shown in table 2, where lusp c(−ω) means using lusp c without pixel-
wise uncertainty rectifying.
it can be observed that by using lusp c(−ω) with muti-
ple decoders, the model segmentation performance is greatly enhanced with
average dsc increasing by 7.70%, asd and hd95 decreasing by 16.11 mm and
48.87 mm, respectively.
by equipping each decoders with diﬀerent dilation rates,
the model’s performance is further improved, especially in terms of asd and
hd95, which proves our hypothesis that learning features from diﬀerent scales
can improve the segmentation accuracy.
3 demonstrates that over-segmentation can be mitigated by using diﬀerent
dilation rates in the three decoders, and using the uncertainty-weighted pseudo
labels can further improve the segmentation accuracy with small false positive
regions removing.
additionally, table 2 shows that combining lusp c and lmp cc obtained the
best performance, where the average dsc, asd and hd95 were 84.75%, 2.64 mm
and 7.91 mm, respectively, which demonstrates the eﬀectiveness of the proposed
class similarity consistency.
scribble-based 3d multiple abdominal organ segmentation
41
4
conclusion
in this paper, we proposed a scribble-supervised multiple abdominal organ seg-
mentation method consisting of a 3d triple-branch multi-dilated network with
two-level consistency constraints.
by equipping each decoder with diﬀerent dila-
tion rates, the model leverages features at diﬀerent scales to obtain high-quality
soft pseudo labels.
experiments on a public abdominal ct dataset word
demonstrated the eﬀectiveness of the proposed method, which outperforms
ﬁve existing scribble-based methods and narrows the performance gap between
weakly-supervised and fully-supervised segmentation methods.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_51.pdf:
two board-certiﬁed oncologists were invited
for evaluating the vce-mri in two aspects: image quality and effectiveness in
primary tumor delineation.
image quality of vce-mri evaluation includes dis-
tinguishability between real contrast-enhanced mri (ce-mri) and vce-mri,
clarity of tumor-to-normal tissue interface, veracity of contrast enhancement in
tumorinvasionriskareas,andefﬁcacyinprimarytumorstaging.forprimarytumor
delineation, the gtv was manually delineated by oncologists.
results showed the
mean accuracy to distinguish vce-mri from ce-mri was 53.33%; no signiﬁ-
cant difference was observed in clarity of tumor-to-normal tissue interface between
vce-mri and ce-mri; for the veracity of contrast enhancement in tumor inva-
sion risk areas and efﬁcacy in primary tumor staging, a jaccard index of 76.04%
and accuracy of 86.67% were obtained, respectively.
the image quality evalu-
ation suggests that the quality of vce-mri is approximated to real ce-mri.
in tumor delineation evaluation, the dice similarity coefﬁcient and hausdorff
distance of the gtvs that delineated from vce-mri and ce-mri were 0.762
(0.673–0.859) and 1.932 mm (0.763 mm–2.974 mm) respectively, which were
clinically acceptable according to the experience of the radiation oncologists.
in china,
npc accounts for up to 50% of all head and neck cancers, while in southeast asia,
npc accounts for more than 70% of all head and neck cancers [3]. radiotherapy (rt) is
currently the main treatment remedy, which needs precise tumor delineation to ensure a
satisfactory rt outcome.
however, accurately delineating the npc tumor is challenging
due to the highly inﬁltrative nature of npc and its complex location, which is surrounded
by critical organs such as brainstem, spinal cord, temporal lobes, etc. to improve the
visibility of npc tumor for precise gross-tumor-volume (gtv) delineation, contrast-
enhanced mri (ce-mri) is administrated through injection of gadolinium-based con-
trast agents (gbcas) during mri scanning.
nsf can cause severe physical impairment, such as joint contractures
of ﬁngers, elbows, and knees, and can progress to involve critical organs such as the
heart, diaphragm, pleura, pericardium, kidney, liver, and lung
currently, there
is no effective treatment for nsf, making it crucial to ﬁnd a ce-mri alternative for
patients at risk of nsf.
in recent years, artiﬁcial intelligence (ai), especially deep learning, plays a game-
changingroleinmedicalimaging[7,8],whichshowedgreatpotentialtoeliminatetheuse
of the toxic gbcas through synthesizing virtual contrast-enhanced mri (vce-mri)
from gadolinium-free sequences, such as t1-weighted (t1w) and t2-weighted (t2w)
mri
in addition to the advantage
of eliminating the use of gbca, vce-mri synthesis can also speed up the clinical
workﬂow by eliminating the need for acquiring ce-mri scan, which saves time for
both clinical staff and patients.
however, current studies mostly focus on algorithms
development while lack comprehensive clinical evaluations to demonstrate the efﬁcacy
of the synthetic vce-mri in clinical settings.
rigorous clinical evaluations can establish the safety and efﬁcacy of ai-based tech-
niques, identify potential biases and limitations, and facilitate the integration of clinical
expertise to ensure accurate and meaningful results [13].
furthermore, the clinical evalu-
ation of ai-based techniques can help identify areas for improvement and optimization,
leading to development of more effective algorithms.
clinical evaluation of ai-assisted virtual contrast enhanced mri
543
to bridge this bench-to-bedside research gap, in this study, we conducted a series of
clinicalevaluationstoassesstheeffectivenessofsyntheticvce-mriinnpcdelineation,
with a particular focus on assessment in vce-mri image quality and primary gtv
delineation.
this dataset included 303 biopsy-proven (stage i-ivb) npc patients who received radi-
ation treatment during 2012–2016.
mri images were automatically registered as mri images for each
patient were scanned in the same position.
the use of this dataset was approved by the
institutional review board of the university of hong kong/hospital authority hong
kong west cluster (hku/ha hkw irb) with reference number uw21-412, and the
research ethics committee (kowloon central/kowloon east) with reference number
kc/ke-18-0085/er-1.
for model development, 288 patients were used for model development and
15 patients were used to synthesize vce-mri for clinical evaluation.
prior to model training, mri images were resampled to 256*224
by bilinear interpolation [14] due to the inconsistent matrix sizes of the three datasets.
in this work, we obtained
12806 image pairs for model training.
different from the original study, which used
single institutional data for model development and utilized min-max value of the whole
dataset for data normalization, in this work, we used mean and standard deviation of
each individual patient to normalize mri intensities due to the heterogeneity of the mri
intensities across institutions [15].
544
w. li et al.
table 1. details of the multi-institutional patient characteristics.
fs: ﬁeld strength; tr: repetition
time; te: echo time; no.: number; avg: average.
(train/test)
avg. age
modality
tr (ms)
te (ms)
institution-1
(siemens-1.5t)
110 (105/5)
56 ± 11
t1w
562–739
13–17
t2w
7640
97
ce-mri
562–739
13–17
institution-2
(philips-3t)
58 (53/5)
49 ± 15
t1w
4.8–9.4
2.4–8.0
t2w
3500–4900
50–80
ce-mri
4.8–9.4
2.4–8.0
institution-3
(siemens-3t)
135 (130/5)
57 ± 12
t1w
620
9.8
t2w
2500
74
ce-mri
3.42
1.11
2.3
clinical evaluations
the evaluation methods used in this study included image quality assessment of vce-
mri and primary gtv delineation.
two board-certiﬁed radiation oncologists (with
8 years’ and 6 years’ clinical experience, respectively) were invited to perform the
vce-mri quality assessment and gtv delineation according to their clinical expe-
rience.
image quality assessment of vce-mri.
to evaluate the image quality of synthetic
vce-mri against the real ce-mri, we conducted four rt-related evaluations: (i) dis-
tinguishability between ce-mri and vce-mri; (ii) clarity of tumor-to-normal tissue
interface; (iii) veracity of contrast enhancement in tumor invasion risk areas; and (iv)
efﬁcacy in primary tumor staging.
the mri volumes were shown in axial view, sagittal view and coronal
view, and the oncologists can scroll through the slices to view adjacent image slices.
(i) distinguishability between ce-mri and vce-mri. to evaluate the reality of vce-
mri, oncologists were invited to differentiate the synthetic patients (i.e., image
volumes that generated from synthetic vce-mri) from real patients (i.e., image
volumes that generated from real ce-mri).
different from the previous studies
that utilized limited number (20-50 slices, axial view) of 2d image slices for reality
evaluation [9, 10], we used 3d volumes in this study to help oncologists visualize
the inter-slice adjacent information.
the judgement results were recorded, and the
accuracy of each institution and the overall accuracy were calculated.
(iii) veracity of contrast enhancement in tumor invasion risk areas.
to better evaluate the veracity of contrast
enhancement in vce-mri, we selected 25 tumor invasion risk areas according to
[16], including 13 high-risk areas and 12 medium-risk areas, and asked oncologists
to determine whether these areas were at risk of being invaded according to the
contrast-enhanced tumor regions.
the 13 high-risk areas include: retropharyngeal
space, parapharyngeal space, levator veli palatine muscle, prestyloid compartment,
tensor veli palatine muscle, poststyloid compartment, nasal cavity, pterygoid pro-
cess, basis of sphenoid bone, petrous apex, prevertebral muscle, clivus, and foramen
lacerum.
the 12 medium-risk areas include foramen ovale, great wing of sphenoid
bone, medial pterygoid muscle, oropharynx, cavernous sinus, sphenoidal sinus,
pterygopalatine fossa, lateral pterygoid muscle, hypoglossal canal, foramen rotun-
dum, ethmoid sinus, and jugular foramen.
a critical rt-related application of ce-mri is
tumor staging, which plays a critical role in treatment planning and prognosis pre-
diction
to assess the efﬁcacy of vce-mri in npc tumor staging, oncologists
were asked to determine the stage of the primary tumor shown in ce-mri and
vce-mri.
gtv delineation is the foremost prerequisite for a success-
ful rt treatment of npc tumor, which demands excellent precision [19].
for
comparison, ce-mri was also imported to eclipse for tumor delineation but assigned
as a different patient, which were shown to oncologists in a random and blind manner.
dsc is a broadly used metric to compare the agree-
ment between two segmentations [23].
it measures the spatial overlap between two
segmentations, which ranges from 0 (no spatial overlap) to 1 (complete overlap).
even though dsc is a well-accepted segmentation compari-
son metric, it is easily inﬂuenced by the size of contours.
small contours typically receive
lower dsc than larger contours [24].therefore, hd was applied as a supplementary to
make a more thorough comparison.
(3)
where d(x, cvce) and d(y, cce) represent the distance from point x in contour cce to
contour cvce and the distance from point y in contour cvce to contour cce.
3
results and discussion
3.1
image quality of vce-mri
table 2 summarizes the results of the four vce-mri quality evaluation metrics, includ-
ing: (i) distinguishability between ce-mri and vce-mri; (ii) clarity of tumor-to-
normal tissue interface; (iii) veracity of contrast enhancement in tumor invasion risk
areas; and (iv) efﬁcacy in primary tumor staging.
the overall judgement
accuracy for the mri volumes was 53.33%, which is close to a random guess
accuracy (i.e., 50%).
the average scores for real and synthetic
patients were 3.6 and 3, 3.6 and 3.8, 3.8 and 3.6 for institution-1, institution-2, and
institution-3, respectively.
(iii) veracity of contrast enhancement in tumor invasion risk areas.
the average ji obtained from institution-1, institution-2, and
institution-3 dataset were similar with a result of 71.54%, 74.78% and 75.85%,
respectively.
for the
institution-2 data, all synthetic patients observed the same stages as real patients.
for the two t-stage disagreement patients, one synthetic patient was staged as phase
iv while the corresponding real patient was staged as phase iii, the other synthetic
patient was staged as i while corresponding real patient was staged as phase iii.
table 2. image quality evaluation results of vce-mri: (a) distinguishability between ce-mri
and vce-mri; (b) clarity of tumor-to-normal tissue interface; (c) veracity of contrast enhance-
ment in risk areas; and (d) t-staging.
abbreviations: inst: institution; c.a.: center-based average;
o.a.: overall average; syn: synthetic.
(a)
(b)
inst-1
inst-2
inst-3
inst-1
inst-2
inst-3
/
/
/
real
syn
real
syn
real
syn
c.a.
70%
40%
50%
3.6
3
3.6
3.8
3.8
3.6
o.a.
53.33%
real: 3.67
syn: 3.47
(c)
(d)
inst-1
inst-2
inst-3
inst-1
inst-2
inst-3
c.a.
71.54%
74.78%
75.85%
80%
100%
80%
o.a.
74.06%
86.67%
548
w. li et al.
figure 1 illustrates an example of the synthetic vce-mri.
the deep learning model integrated the complementary information of
t1w mri and t2w mri, and successfully synthesized vce-mri with similar contrast
and tumor volume as ce-mri, with no obvious contrast differences in tumor regions,
as shown in difference map between ce-mri and vce-mri.
fig.
1. illustration of the synthetic vce-mri.
3.2
primary gtv delineation
the average dsc and hd between the cce and cvce was 0.762 (0.673–0.859) with a
median of 0.774, and 1.932 mm (0.763 mm–2.974 mm) with a median of 1.913 mm,
respectively.
for institution-1, institution-2, and institution-3, the average dsc were
0.741, 0.794 and 0.751 respectively, while the average hd were 2.303 mm, 1.456 mm,
and 2.037 mm respectively.
figure 2 illustrated the delineated primary gtv contours
from an average patient with the dsc of 0.765 and hd of 1.938 mm.
2. illustration of the primary gtvs from a typical patient with an average dsc and hd.
clinical evaluation of ai-assisted virtual contrast enhanced mri
549
4

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_50.pdf:
data augmentation (da) is a key factor in medical image
analysis, such as in prostate cancer (pca) detection on magnetic reso-
nance images.
however, such augmentations do not substantially
increase the organ as well as tumor shape variability in the training set,
limiting the model’s ability to generalize to unseen cases with more diverse
localized soft-tissue deformations.
we propose a new anatomy-informed
transformation that leverages information from adjacent organs to sim-
ulate typical physiological deformations of the prostate and generates
unique lesion shapes without altering their label.
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43990-2_50.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
https://doi.org/10.1007/978-3-031-43990-2_50
532
b. kovacs et al.
computational requirements, it can be easily integrated into common da
frameworks.
we demonstrate the eﬀectiveness of our augmentation on a
dataset of 774 biopsy-conﬁrmed examinations, by evaluating a state-of-
the-art method for pca detection with diﬀerent augmentation settings.
keywords: data augmentation · soft-tissue deformation · prostate
cancer detection
1
introduction
data augmentation (da) is a key factor in the success of deep neural networks
(dnn) as it artiﬁcially enlarges the training set to increase their generaliza-
tion ability as well as robustness [22].
it plays a crucial role in medical image
analysis [8] where annotated datasets are only available with limited size.
dnns
have already successfully supported radiologists in the interpretation of mag-
netic resonance images (mri) for prostate cancer (pca) diagnosis [3].
however,
the da scheme received less attention, despite its potential to leverage the data
characteristic and address overﬁtting as the root of generalization problems.
state-of-the-art approaches still rely on simplistic spatial transformations,
like translation, rotation, cropping, and scaling by globally augmenting the mri
sequences [12,20].
however, soft tissue deformations, which
are currently missing from the da schemes, are known to signiﬁcantly aﬀect the
image morphology and therefore play a critical role in accurate diagnosis
both lesion and prostate shape geometrical appearance inﬂuence the clinical
assessment of prostate imaging-reporting and data system (pi-rads)
ignoring
these deformations in the da scheme can potentially limit model performance.
model-driven transformations attempting to simulate organ functions - like
respiration, urinary excretion, cardiovascular- and digestion mechanics - oﬀer a
high degree of diversity while also providing realistic transformations.
currently,
the ﬁnite element method (fem) is the standard for modeling biomechanics [13].
however, their computation is overly complex
motion mod-
els have not been integrated into any deep learning framework as an online data
augmentation yet, thereby leaving the high potential of inducing application-
speciﬁc knowledge into the training procedure unexploited.
anatomy-informed data augmentation
533
in this work we propose an anatomy-informed spatial augmentation, which
leverages information from adjacent organs to mimic typical deformations of the
prostate.
due to its lightweight computational requirements, it can be easily
integrated into common da frameworks.
inducing this kind of soft tissue deforma-
tion ultimately led to improved model performance in patient- and lesion-level
pca detection on an independent test set.
fig.
the proposed anatomy-informed prostate augmentation.
due to its lightweight computational requirements, it can
be easily integrated into online network training.
we
make it publicly available in batchgenerators [9] and integrate it into a nnu-net
trainer https://github.com/mic-dkfz/anatomy_informed_da.
2.2
experimental setting
we evaluate our anatomy-informed da qualitatively as well as quantitatively.
we derive
the diagnosis through semantic segmentation of the malignant lesions following
previous studies [5,11,12,20,21].
semantic segmentation provides interpretable
predictions that are sensitive to spatial transformations, making it appropriate
for testing spatial das.
to compare the performance of the trained models to
radiologists, we calculate their performance using the clinical pi-rads scores
and histopathological ground truths.
afterward, we evaluate
model performances on object-level using the free-response receiver operating
characteristic (froc) and the number of detections at the radiologists’ lesion
level performance for pi-rads ≥ 4, at 0.32 average number of false positives
per scan.
objects were derived by applying a threshold of 0.5 to the softmax
outputs followed by connected component analysis to identify connected regions
in the segmentation maps.
[8], which is an extensive augmentation pipeline
containing simple spatial transformations, namely translation, rotation and
scaling.
2. random deformable transformations as implemented in the nnu-net [8]
da pipeline extending the basic da scheme (1) to test its presence in the
medical domain.
our hypothesis is that it will produce counterproductive
examples, resulting in inferior performance compared to our proposed da.
anatomy-informed data augmentation
535
3.
the ethics
committee of the medical faculty heidelberg approved the study (s-164/2019)
and waived informed consent to enable analysis of a consecutive cohort.
all
experiments were performed in accordance with the declaration of helsinki
[2] and relevant data privacy regulations.
malignancy of the segmented lesions was determined from
a systematic-enhanced lesion ground-truth histopathological assessment, which
has demonstrated reliable ground-truth assessment with sensitivity comparable
to radical prostatectomy [17].
based on the biopsy results, every cspca
lesion was segmented on the t2-weighted sequences retrospectively by multiple
in-house investigators under the supervision of a board-certiﬁed radiologist.
in
addition to the lesions, the rectum and the bladder segmentations were auto-
matically predicted by a model built upon nnu-net [8] trained iteratively on
an in-house cohort initially containing a small portion of our cohort.
multiple
radiologists conﬁrmed the quality of the predicted segmentations.
the mri
sequences were registered using b-spline transformation based on mutual infor-
mation to match the ground-truth segmentations across all modalities [12,14].
as the limited number of exams with cspca and the small lesion size com-
pared to the whole image can cause instability during training, we adapted
the cropping strategy from [21] by keeping the organ segmentations to use the
anatomy-informed da (oﬀsets of ±9 mm axial to the prostate and ±11.25 mm
in the axial plane to the rectum and the bladder).
the images are preprocessed
by the automated algorithm of nnu-net [8].
compared
to the standard nnu-net settings, we implemented balanced sampling regard-
ing the prevalence of cspca and reduced the number of epochs to 350 to avoid
overﬁtting.
the middle images show the original
mri sequence, the left images simulate rectal space evacuation, while the right images
rectal distension.
anatomy-informed data augmentation
537
in table 1
we summarize the patient-level pauroc and f1-scores; and
lesion-level froc results on the independent test set showing the advantage
of using anatomy-informed da.
to further highlight the practical advantage of
the proposed augmentation, we compare the performance of the trained models
to the radiologists’ diagnostic performance for pi-rads ≥ 4, which locate the
most informative performance point clinically on the roc diagram, see fig.
the radiologists’ perfor-
mance with pi-rads ≥ 4 is marked to locate the most informative performance point
clinically.
both variants of the proposed anatomy-informed da (3.a and 3.b) increased
the sensitivity value around the clinical pi-rads ≥ 4 performance point compared to
the simple (1) and random elastic (3) da schemes, approaching it closely.
extending the basic da scheme with the proposed anatomy-informed defor-
mation not only increased the sensitivity closely matching the radiologists’
patient-level diagnostic performance but also improved the detection of pca
538
b. kovacs et al.
on a lesion level.
interestingly, while the use of random deformable transforma-
tion also improved lesion-level performance, it did not approach the diagnostic
performance of the radiologists, unlike the anatomy-informed da.
at the selected patient- and object-level working points, the model with the
proposed rectum- and bladder-informed da scheme reached the best results with
signiﬁcant improvements (p < 0.05) compared to the model with the basic da
setting by increasing the f1-score with 5.11% and identifying 4 more lesions
(5.3%) from the 76 lesions in our test set.
the time overhead introduced by anatomy-informed augmentation caused no
increase in the training time, the gpu remained the main bottleneck.
towards radiologists’ performance.
inducing lesion shape variability via
anatomy-informed augmentation to the training process improved the lesion
detection performance and increased the sensitivity value towards radiologist-
level performance in pca diagnosis in contrast to the training with the basic
da setting.
these soft tissue deformations are part of physiology, but only one
snapshot is captured from the many possible functional states within each indi-
vidual mr examination.
we got additional, but slight improvements
by extending the da scheme with bladder distensions.
a possible explanation for
this result is that less than 30% of the lesions are located close to the bladder, and
our dataset did not contain enough training examples for more improvements.
our proposed anatomy-
informed transformation was designed to mimic real-world deformations in order
to preserve essential image features.
to support the importance of realism in da quantitatively, we
compared the performance of the basic and our anatomy-informed da scheme
with that of the random deformable transformation.
the random deformable da
scheme generated high lesion shape variability, but it resulted in lower perfor-
mance values.
this could be due to the fact that it can also cause implausible or
anatomy-informed data augmentation
539
even harmful image warping, distorting important features, and producing coun-
terproductive training examples.
in comparison, our proposed anatomy-informed
da outperformed the basic and random deformable da, demonstrating the sig-
niﬁcance of realistic transformations for achieving superior model performance.
its limitation is the need for additional organ segmenta-
tions, which requires additional eﬀort from the annotator.
however, pre-trained
networks for segmenting anatomical structures like nnu-net
additionally, our
transformation computation allows certain errors in the organ segmentations
compared to applications where fully accurate segmentations are needed.
the
success of anatomy-informed da opens the research question of whether it
enhances performance across diverse datasets and model backbones.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_52.pdf:
this paper presents a ﬁrst attempt to jointly predict
molecular markers and histology features and model their interactions
for classifying diﬀuse glioma bases on whole slide images.
our
experiments show that our method outperforms other state-of-the-art
methods in classifying diﬀuse glioma, as well as related histology and
molecular markers on a multi-institutional dataset.
hence, automatic algorithms
based on histology whole slide images (wsis)
[15], namely digital pathology,
promise to oﬀer rapid diagnosis and aid precise treatment.
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43990-2_52.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
the role of key molecular
markers, i.e., isocitrate dehydrogenas (idh) mutations, co-deletion of chromo-
some 1p/19q and homozygous deletion (homdel) of cyclin-dependent kinase
inhibitor 2a/b (cdkn), have been highlighted as major diagnostic markers
for glioma, while histology features that are traditionally emphasized are now
considered as reference, although still relevant in many cases.
for instance, in
the new pathology scheme, glioblastoma is increasingly diagnosed according to
idh mutations, while previously its diagnosis mostly relies on histology features,
including necrosis and microvascular proliferation (nmp).1
however, the primary approaches to assess molecular markers include gene
sequencing and immuno-staining, which are time-consuming and expensive than
histology assessment.
[25] devised a dual-pool contrastive
learning for classifying fundus and x-ray images.
note for wsis with patch number< n,
we adopt a biological repeat strategy for dimension alignment.
4
experiments and results
4.1
implementation details
the proposed deepmo-glioma is trained on the training set for 70 epochs,
with batch size of 8 and initial learning rate of 0.003 with adam optimizer
[11] together with weight decay.
key hyper-parameters are listed in table 1 of
supplementary material.
all hyper-parameters are tuned to achieve the best per-
formance over the validation set.
all experiments are conducted on a computer
with an intel(r) xeon(r) e5-2698 cpu @2.20 ghz, 256 gb ram and 4 nvidia
tesla v100 gpus.
additionally, our method is implemented on pytorch with
python environment.
performance of classifying glioma based on who 2021 criteria
refer to supplementary fig.
4.2
performance evaluation
1) glioma classiﬁcation.
[16] are mil framework, while
others are commonly-used image classiﬁcation methods, set as our baselines.
the left panel of table 1 shows that deepmo-glioma performs the best, achiev-
ing at least 6.1%, 13.1%,
3.1% and 11.0% improvement over other models in
accuracy, sensitivity, speciﬁcity and auc, respectively, indicating that our model
558
x. wang et al.
table 2. performance in predicting genomic markers, histology and ablation studies.
figure 3 plots the rocs of all models, demonstrating the
superior performance of our model over other comparison models.
3) network interpretability.
an additional visualization experiment is con-
ducted based on patch decision scores to test the interpretability of our method.
due to the page limit, the results are presented in supplementary fig.
1.
4.3
results of ablation experiments
1) cplc-graph network.
the right panels of table 1 shows that the performance after remov-
ing lc loss decreases in all metrics, causing a reduction of 6.1%, 15.0%, 4.3%
and 9.8%, in accuracy, sensitivity, speciﬁcity and f1-score, respectively.
from table 1, we observe that the proposed dcc loss improves
the performance in terms of accuracy by 9.1%.
such performance is also
found in comparing the rocs in fig.
our experiments demonstrate that our model has achieved superior
performance over other state-of-the-art methods, serving as a potentially useful
tool for digital pathology based on wsis in the era of molecular pathology.
acknowledgments.
not necessarily those of the nhs, the nihr or the department of health and social
care.
32(4), e13060 (2022)
2. campanella, g., et al.: clinical-grade computational pathology using weakly super-
vised deep learning on whole slide images.
imaging 41(4), 757–770 (2020)
4. dosovitskiy, a., et al.: an image is worth 16x16 words: transformers for image
recognition at scale.
he, k., zhang, x., ren, s., sun, j.: deep residual learning for image recognition.
imboden, s., et al.: implementation of the 2021 molecular esgo/estro/esp
risk groups in endometrial cancer.
jiang, s., zanazzi, g.j., hassanpour, s.: predicting prognosis and idh mutation
status for patients with lower-grade gliomas using whole slide images.
liang, s., et al.: clinical practice guidelines for the diagnosis and treatment of
adult diﬀuse glioma-related epilepsy.
lu, m.y., williamson, d.f., chen, t.y., chen, r.j., barbieri, m., mahmood,
f.: data-eﬃcient and weakly supervised computational pathology on whole-slide
images.
shao, z., bian, h., chen, y., wang, y., zhang, j., ji, x., et al.: transmil: trans-
former based correlated multiple instance learning for whole slide image classiﬁca-
tion.
syst. 34, 2136–2147 (2021)
17. simonyan, k., zisserman, a.: very deep convolutional networks for large-scale
image recognition.
trpkov, k., et al.: new developments in existing who entities and evolving molecu-
lar concepts: the genitourinary pathology society (gups) update on renal neoplasia.
deep learning-based six-type classiﬁer for lung cancer and mimics
from histopathological whole slide images: a retrospective study.
zhang, l., wei, y., fu, y., price, s., schönlieb, c.b., li, c.: mutual contrastive
low-rank learning to disentangle whole slide image representations for glioma grad-
ing.
zhang, y., luo, l., dou, q., heng, p.a.: triplet attention and dual-pool contrastive
learning for clinic-driven multi-label medical image classiﬁcation.
image anal.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_2.pdf:
many renal cancers are incidentally found on non-contrast
ct (ncct) images.
on contrast-enhanced ct (cect) images, most
kidney tumors, especially renal cancers, have diﬀerent intensity values
compared to normal tissues.
however, on ncct images, some tumors
called isodensity tumors, have similar intensity values to the surround-
ing normal tissues, and can only be detected through a change in organ
shape.
several deep learning methods which segment kidney tumors from
cect images have been proposed and showed promising results.
how-
ever, these methods fail to capture such changes in organ shape on ncct
images.
in this paper, we present a novel framework, which can explicitly
capture protruded regions in kidneys to enable a better segmentation of
kidney tumors.
we created a synthetic mask dataset that simulates a
protuberance, and trained a segmentation network to separate the pro-
truded regions from the normal kidney regions.
to achieve the segmen-
tation of whole tumors, our framework consists of three networks.
the
ﬁrst network is a conventional semantic segmentation network which
extracts a kidney region mask and an initial tumor region mask.
the proposed
method was evaluated on a publicly available kits19 dataset, which con-
tains 108 ncct images, and showed that our method achieved a higher
dice score of 0.615 (+0.097) and sensitivity of 0.721 (+0.103) compared
to 3d-unet.
to the best of our knowledge, this is the ﬁrst deep learning
method that is speciﬁcally designed for kidney tumor segmentation on
ncct images.
keywords: renal cancer · tumor segmentation · non-contrast ct
1
introduction
over 430,000 new cases of renal cancer were reported in 2020 in the world
https://doi.org/10.1007/978-3-031-43990-2_2
14
t. hatsutani et al.
than 7 cm) often the whole kidney is removed, however, when the tumor size
is small (less than 4 cm), partial nephrectomy is the preferred treatment
however, early-stage renal cancers are
usually asymptomatic, therefore they are often incidentally found during other
examinations [19], which includes non-contrast ct (ncct) scans.
segmentation of kidney tumors on ncct images adds challenges compared
to contrast-enhanced ct (cect) images, due to low contrast and lack of multi-
phase images.
on cect images, the kidney tumors have diﬀerent intensity val-
ues compared to the normal tissues.
there are several works that demonstrated
successful segmentation of kidney tumors with high precision [13,21].
however,
on ncct images, as shown in fig.
[3] is the go-to network for segmenting kidney tumors on cect
images.
however, convolutional neural networks (cnns) are biased towards tex-
ture features [5].
therefore, without any intervention, they may fail to capture
the protuberance caused by isodensity tumors on ncct images.
our goal is to segment kidney tumors includ-
ing isodensity types on ncct images.
to achieve this goal, we create a syn-
thetic dataset, which has separate annotations for normal kidneys and protruded
regions, and train a segmentation network to separate the protruded regions from
the normal kidney regions.
in order to segment whole tumors, our framework
consists of three networks.
this proposed framework
enables a better segmentation of isodensity tumors and boosts the performance
of segmentation of kidney tumors on ncct images.
present a pioneering work for segmentation of kidney tumors on ncct
images.
2. propose a novel framework that explicitly captures protuberances in a kid-
ney to enable a better segmentation of tumors including isodensity types on
ncct images.
segmentation of kidney tumors on ncct images
15
fig.
1. example cect and ncct images.
a) cect image.
b) ncct image.
2
related work
the release of two public ct image datasets with kidney and tumor masks from
the 2019/2021 kidney and kidney tumor segmentation challenge [8] (kits19,
kits21) attracted researchers to develop various methods for segmentation.
looking at the top 3 teams from each challenge
however, the paper notes that modifying
the architecture resulted in only slight improvement.
the other 5 teams took
a similar approach to nnu-net’s coarse-to-ﬁne cascaded network [12], where it
predicts from a low-resolution image in the ﬁrst stage and then predicts kidneys
and tumors from a high-resolution image in the second stage.
to overcome this issue, we
developed a framework that speciﬁcally incorporates protuberances in kidneys,
allowing for an eﬀective segmentation of tumors on ncct images.
[14] developed a computer-aided diagnosis system to detect exophytic
kidney tumors on ncct images using belief propagation and manifold diﬀusion
to search for protuberances.
in our work, we will not only segment protruded tumors but also
other tumors as well.
this enables us to extract a part of tumors that forms
protuberance, but our goal is segmenting all visible kidney tumors on ncct
images.
in detail,
we perform a summation of the initial tumor mask and the protruded region
mask, and then concatenate the result with the input image.
3.1
step1: training base network
in the ﬁrst step, we train the base network, which is a standard segmentation
network, to extract kidney and tumor masks from the images.
and as a loss function, we use the dice loss [16] and
the cross-entropy loss equally.
segmentation of kidney tumors on ncct images
17
3.2
step2: training protuberance detection network
in the second step, we train the protuberance detection network alone to separate
protruded regions from the normal kidney masks.
to enable a segmentation of protruded regions only, a sep-
arate annotation of each region is usually required.
the output of the protuberance detection network will likely have more false
positives than the base network since it has no access to the input image.
the ﬁrst channel is the input image, and the second
channel is the result of summation of the initial tumor mask and the protruded
region mask.
we concatenate the input image so that the last network can remove
false positives from the predicted masks as well as predicting the missing tumor
regions from the protuberance detection network.
by having multiple modules in this manner, the network can ﬁx the
initial mistakes in early modules and corrects in later modules.
4
experiments
no prior work exists that uses ncct images from kits19
thus, we ﬁrst
created our baseline model and compared the performance with existing methods
on cect images.
we then trained the model using ncct images and
compared with our proposed method.
4.1
datasets and preprocessing
we used a dataset from kits19
[8] which contains both cect and ncct
images.
for cect images, there are 210 images for training and validation
and, 90 images for testing.
for ncct images, there are 108 images, which are
diﬀerent series of the 210 images.
the ground truth masks are only available for
the 210 cect images.
thus, we transfer the masks to ncct images.
the images were ﬁrst clipped to the intensity value range of [−90, 210] and
normalized from −1 to 1.
during
the training, the images were randomly cropped to a patch size of 128×128×128
voxels.
we applied random rotation, random scaling and random noise addition
as data augmentation.
we applied some
augmentations during training to input masks to simulate the incoming inputs
from the base network.
therefore, we applied gaussian blurring, gaussian
noise addition and intensity value shifting.
segmentation of kidney tumors on ncct images
19
table 1.
dice performance of existing method and our baseline model.
evaluated using
cect images from kits19.
composite dice is an average dice between kidney and
tumor dice.
the results were obtained by submitting our predicted masks to the grand
challenge page.
[13] 0.9123
0.9737
0.8509
our baseline model
0.8832
0.9728
0.7935
4.2
training details and evaluation metrics
our model was trained using sgd with a 0.9 momentum and a weight decay of
1e−7.
we conducted our experiments using
jax (v.0.4.1)
we trained the model using a single
nvidia rtx a5000 gpu.
for the experiment on cect images, we used the dice score as our evalu-
ation metrics following the same formula from kits19.
for the experiment on
ncct images, we also evaluated the sensitivity and false positives per image
(fps/image).
5
results
5.1
performance on cect images
to show that our model is properly tuned, we compare our baseline model with
an existing method using cect images.
we used this
baseline model as our base network for the experiments on ncct images.
5.2
performance on ncct images
table 2 shows our experimental results and ablation studies on ncct images.
the ablation studies show that adding each component (cect
images and the protuberance detection network) resulted in an increase in the
performance.
while adding cect images contributed the most for the increase
in tumor dice and sensitivity, adding the protuberance detection network further
pushed the performance.
however, the false positives per image (fps/image)
20
t. hatsutani et al.
table 2.
result of our proposed method on ncct images from kits19.
the values
are average values of a ﬁve-fold cross-validation.
protuberance detection network with cect images tumor dice sensitivity fps/image
✗
✗
0.518
0.618
0.283
✗
✓
0.585
0.686
0.340
✓
✓
0.615
0.721
0.421
fig.
the protuberance detection network cannot dis-
tinguish the protrusions that were caused by tumors or cysts, so the output
from this network has many fps at this stage.
thus, the fusion network has to
eliminate cysts by looking again the input image, however, it may have failed to
eliminate some cysts (fig.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_47.pdf:
we propose a method (named mskdex) to estimate
ﬁne-grained muscle properties from a plain x-ray image, a low-cost, low-
radiation, and highly accessible imaging modality, through musculoskele-
tal decomposition leveraging ﬁne-grained segmentation in ct.
we train
a multi-channel quantitative image translation model to decompose an
x-ray image into projections of ct of individual muscles to infer the lean
muscle mass and muscle volume.
we propose the object-wise intensity-
sum loss, a simple yet surprisingly eﬀective metric invariant to muscle
deformation and projection direction, utilizing information in ct and
x-ray images collected from the same patient.
while our method is basi-
cally an unpaired image-to-image translation, we also exploit the nature
of the bone’s rigidity, which provides the paired data through 2d-3d
rigid registration, adding strong pixel-wise supervision in unpaired train-
ing.
the average pearson correlation coeﬃcient between the predicted and
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43990-2_47.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
we
believe our method opened up a new musculoskeletal diagnosis method
and has the potential to be extended to broader applications in multi-
channel quantitative image translation tasks.
keywords: muscles · radiography · generative adversarial networks
(gan) · sarcopenia · image-to-image translation
1
introduction
sarcopenia is a prevalent musculoskeletal disease characterized by the inevitable
loss of skeletal muscle, causing increased risks of all-cause mortality and dis-
ability that result in heavy healthcare costs [1–6].
however, dxa and ct
require special equipment that is much less accessible in a small clinic.
further-
more, ct requires high radiation exposure, and dxa allows the measurement
of only overall body composition, which lacks details in individual muscles such
as the iliacus muscle, which overlays with the gluteus maximus muscle in dxa
images.
although several recent works used x-ray images for bone mineral den-
sity (bmd) estimation and osteoporosis diagnosis [12–15], only a few works esti-
mated muscle metrics and sarcopenia diagnosis [16,17], and the deep learning
technology used is old.
recently, bmd-gan [15] was proposed for estimating
bmd through x-ray image decomposition using x-ray and ct images aligned by
2d-3d registration.
[17]
proposed an x-ray image decomposition for individual muscles.
in this study, we propose mskdex: musculoskeletal (msk) decomposition
from a plain x-ray image for the ﬁne-grained estimation of lean muscle mass
and volume of each individual muscle, which are useful metrics for evaluating
muscle diseases including sarcopenia.
the contribution of this paper
is three-fold: 1) proposal of the object-wise intensity-sum (owis) loss, a simple
yet eﬀective metric invariant to muscle deformation and projection direction, for
quantitative learning of the absolute volume and lean mass of the muscles, 2)
proposal of partially aligned training utilizing the aligned (paired) dataset for
the rigid object for the pixel-wise supervision in an unpaired image translation
task, 3) extensive evaluation of the performance using a 539-patient dataset.
patient #1 (young, male) and patient #2 (old, female) had similar
bmi and almost the same gluteus maximus volume, while the lean muscle mass was
signiﬁcantly diﬀerent, likely due to the fatty degeneration in patient #2, which was
clearly observable in the projections of the lean muscle mass volume.
fig.
three types of object-wise drrs (of seg-
mented individual muscle/bone regions) were obtained from ct through segmentation
[18], intensity conversion [19,20], 2d-3d registration for bones
a decomposition model was trained using
gan loss and proposed gc loss chain, owis loss, and bone loss to decompose an
x-ray image into drrs whose intensity sum derives the metric of volume and mass.
2
method
2.1
dataset preparation
figure 2 illustrates the overview of the proposed mskdex.
we collected a dataset
of 552 patients subject to the total hip arthroplasty surgery (455 females and
500
y. gu et al.
97 males, height 156.9 ± 8.3 cm, weight 57.5 ± 11.9 kg, bmi 23.294 ± 3.951
we acquired a pair of pre-operative x-ray and ct images from
each patient, assuming consistency in bone shape, lean muscle mass, and muscle
volume.
automated segmentation of individual bones and muscles was obtained
from ct
three diﬀerent intensity conversions were applied to the segmented
ct; 1) the original intensity, 2) intensity of 1.0 for voxels inside the structure
and 0.0 for voxels outside to estimate muscle volume, 3) intensity correspond-
ing to the lean muscle mass density based on a conversion function from the
hounsﬁeld unit (hu) to the mass density [19,20] to estimate lean muscle mass.
then, object-wise drrs
for the three conversions were generated for each segmented individual object
(bone/muscle) region.
the summation of all the objects of wvdrrs becomes an image
with a contrast similar to the real x-ray image used to calculate the reconstruc-
tion gradient correlation (gc) loss [17,22].
a 2d-3d registration [21] of each
bone between ct and x-ray image of the same patient was performed to obtain
its drr aligned with the x-ray image, which is used in the proposed partially
aligned training.
instead, we exploited the invariant property of muscles using the newly proposed
intensity-sum loss.
2.2
model training
we train a decomposition model g to decompose an x-ray image into the
drrs = {v drr, mdrr, wv drr} to infer the lean muscle mass and muscle
volume, adopting cyclegan
the gan loss lup
gan we use is formulated in supplemental materials.
we call the summation of a drr over all the chan-
nels (objects) the virtual x-ray image deﬁned as iv x = v (idrr) = 
i idrr
i
,
where idrr
i
is the i-th object image of a drr.
= eix − gc(ix, v (g(ix)w v drr))
(1)
mskdex
501
to maintain the structure consistency between an x-ray image and decomposed
drr, where g(ix)w v drr is the decomposed wvdrr.
+gc(st(g(ix)w v drr
i
), g(ix)mdrr
i
)

(2)
to chain the structural constraints from wvdrr to vdrr and mdrr, where
the g(ix)w v drr
i
, g(ix)v drr
i
, and g(ix)mdrr
i
are i-th object image of the
decomposed wvdrr, vdrr, and mdrr, respectively.
unlike general images, our drrs embedded
speciﬁc information so that the intensity sum represents physical metrics (mass
and volume).
furthermore, the conventional method did not utilize the paired
information of an x-ray image and drr (obtained from the same patient).
we
took advantage of the paired information, proposing the object-wise intensity-
sum loss, a simple yet eﬀective metric invariant to patient pose and projection
direction, for quantitative learning.
− s(idrr
i
)
,
(3)
where idrr
i
and s(·) are the i-th object image of drr and the intensity sum-
mation operator (sum over the intensity of an image), respectively.
the h and
w are the image height and weight, respectively, served as temperatures for
numeric stabilizability.
[21] to align the pelvis and femur drrs
with the paired x-ray images for partially aligned training to improve overall
performance, including muscle metrics estimation.
3
experiments and results
the automatic segmentation results of 552 cts were visually veriﬁed, and 13
cases with severe segmentation failures were omitted from our analysis, resulting
in 539 cts.
the baseline of our experiment was the
vanilla cyclegan with the reconstruction gc loss proposed in [17].
from 3d ct images with three metrics, pearson correlation coeﬃcient (pcc),
intra-class correlation coeﬃcient (icc), and mean absolute error (mae).
addi-
tionally, we evaluated the image quality of predicted drrs of the bones by com-
paring them with the aligned drrs using peak-signal-noise-ratio (psnr) and
structural similarity index measure (ssim).
implementation details are described
in supplemental materials.
signiﬁcant improvements by the proposed method were
observed, achieving high pccs of 0.877 and 0.901 of the lean muscle mass and
muscle volume estimations, respectively, for the gluteus medius, and 0.865 and
0.873, respectively, for the iliacus.
the overall intensity of the conventional method was clearly diﬀerent
from the reference, while the proposed method decomposed the x-ray image
considering the structural faithfulness and quantitative accuracy, outperform-
ing the conventional method signiﬁcantly.
more detailed results and a visualiza-
tion video can be found in supplemental materials.
we observed signiﬁcant improvements from the baseline by both
proposed features, owis loss and partially aligned training lb.
the average pcc for the muscles was improved from 0.457 to 0.826 by adding
the owis loss (λis = 100) and to 0.796 by adding the bone loss, while their
combination achieved the best average pcc of 0.855, demonstrating the supe-
rior ability of quantitative learning of the proposed mskdex.
the results also
suggested that the weight balance for loss terms needs to be made to achieve the
best performance.
more detailed results are shown in supplemental materials.
performance comparison between the conventional and proposed methods in
a cross-validation study using 539 data.
sac.
0
(false)
.415
.419
.469
.368
.473
.600
.542
.265
0
(true)
.734
.799
.788
.855
.784
.813
.954 .798
100
(false)
.799
.815
.829
.854
.815
.842
.925
.774
100
(true)
.854
.857
.837
.883 .854
.846 .947
.898
1000 (false)
.798
.765
.770
.839
.704
.840
.870
.767
1000 (true)
.795
.776
.767
.828
.748
.820
.929
.745
mskdex
505
4
summary
we proposed mskdex, a method for ﬁne-grained estimation of the lean muscle
mass and volume from a plain x-ray image (2d) through the musculoskele-
tal decomposition, which, in fact, recovers ct (3d) information.
our method
decomposes an x-ray image into drrs of objects to infer the lean muscle mass
and volume considering the structural faithfulness (by the gradient correlation
loss chain) and quantitative accuracy (by the object-wise intensity-sum loss and
aligned bones training), outperforming the conventional method by a large mar-
gin as shown in sect.
the prediction of muscles overlapped
with the pelvis in the x-ray image can leverage the strong pixel-wise supervision
by the aligned pelvis’s drr, which can be considered as a type of calibration.
our future works are the validation with a large-scale dataset and extension to
the decomposition into a larger number of objects.
acknowledgement.
https://doi.org/10.1007/s40520-016-0704-
5
4. petermann-rocha, f., balntze, v., gray, s.r., et al.: global prevalence of sar-
copenia and severe sarcopenia: a systematic review and meta-analysis.
https://doi.org/10.1002/jcsm.12890
6. edwards, m.h., dennision, e.m., sayer, a.a., et al.: osteoporosis and sarcopenia
in older age.
nana, a., slater, g.j., stewart, a.d., burke, l.m.: methodology review: using
dual-energy x-ray absorptiometry (dxa) for the assessment of body composition
in athletes and active people.
https://doi.org/10.1123/ijsnem.2013-0228
9. feliciano, e.m.c., et al.: evaluation of automated computed tomography segmen-
tation to assess body composition and mortality associations in cancer patients.
hsieh, c.-i., zheng, k., lin, c., mei, l., et al.: automated bone mineral density
prediction and fracture risk assessment using plain radiographs via deep learning.
wang, f., zheng, k., lu, le, et al.: lumbar bone mineral density estimation from
chest x-ray images: anatomy-aware attentive multi-roi modeling.
gu, y., et al.: bmd-gan: bone mineral density estimation using x-ray image
decomposition into projections of bone-segmented quantitative computed tomog-
raphy using hierarchical learning.
medical image computing and computer assisted intervention
– miccai 2022.
https://doi.org/10.1038/s41598-023-35075-x
18. hiasa, y., otake, y., et al.: automated muscle segmentation from clinical ct using
bayesian u-net for personalized musculoskeletal modeling.
aubrey, j., esfandiari, n., baracos, v.e., buteau, f.a., et al.: measurement of
skeletal muscle radiation attenuation and basis of its biological variation.
otake, y., et al.: intraoperative image-based multiview 2d/3d registration for
image-guided orthopaedic surgery: incorporation of ﬁducial-based c-arm tracking
and gpu-acceleration.
hiasa, y., et al.: cross-modality image synthesis from unpaired data using cycle-
gan.
zhu, j.-y., park, t., isola, p., efros, a.a.: unpaired image-to-image translation
using cycle-consistent adversarial networks.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_53.pdf:
in this paper for the ﬁrst time, we propose
evidential graph transformer network, a combination of attention map-
ping and uncertainty estimation to increase the performance and inter-
pretability of surgical margin assessment.
the performance of the model was
compared with diﬀerent baselines in an ex-vivo cross-validation scheme,
with extensive ablation study.
results: the purposed model outperformed all baselines, statistically
signiﬁcantly, with average balanced accuracy of 91.6%.
conclusion: deployment of ex-vivo models is challenging
due to the tissue heterogeneity of intra-operative data.
the proposed
evidential graph transformer is a powerful tool that while providing
the attention distribution of biochemical subbands, improve the surgical
deployment power by providing decision conﬁdence.
keywords: intra-operative deployment · uncertainty estimation ·
interpretation · graph transformer network · breast cancer margin
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. (eds.): miccai 2023, lncs 14226, pp.
in bcs the surgeon removes breast cancer while attempting to
preserve as much healthy tissue as possible to prevent permanent deformation
and to enhance cosmesis.
the success of clinical deployment of learning models heavily relies on
approaches that are not only accurate but also interpretable.
particularly, graph transformer networks (gtn) has have shown
to further enhance the transparency of underlying relation between the graph
nodes and decision making via attention mechanism [11].
biological data, specially those acquired intra-opertively, are heterogeneous
by nature.
while the use of ex-vivo data collected under speciﬁc protocols are
beneﬁcial to develop baseline models, intra-operative deployment of these models
is challenging.
for iknife, the ex-vivo data is usually collected from homogeneous
regions of resected specimens under the guidance of a trained pathologist, ver-
uncertainty-aware models in computer-assisted interventions can provide
clinicians with feedback on prediction conﬁdence to increase their reliability dur-
ing deployment.
since the evidential approach jointly gen-
erates the network prediction and uncertainty estimation, it seems more suitable
for computationally eﬃcient intra-operative deployment.
in this paper, we propose evidential graph transformer (egt), a combina-
tion of graph-based feature-level attention mechanism with sample-level uncer-
tainty estimation, to increase the performance and interpretability of surgical
margin assessment.
this is done by implementing the evidential loss and pre-
diction functions within a graph transformer model to output the uncertainty,
intermediate attention, and model prediction.
to demonstrate the state-of-the-
art performance of the proposed approach on mass spectrometry data, the model
is compared with diﬀerent baselines in both cross-validation and prospective
schemes on ex-vivo data.
furthermore, the performance of model is also inves-
tigated intraoperatively.
the study
is approved by the institutional research ethics board and patients consent to be
included.
each spectrum is then labeled based both on
surgeons comments during the operation and post-operative pathology report.
in the context of surgical margin assessment, the attentions reveal the rele-
vant metabolic ranges to cancerous tissue, while uncertainty helps identify and
ﬁlter data with unseen pathology.
to
ﬁt the dirichlet distribution to the output layer of our network, we use a loss
function consisting of the prediction error lp
i and the evidence adjustment le
i
li(θ) = lp
i (θ) + λle
i(θ)
(2)
where λ is the annealing coeﬃcient to balance the two terms.
is kl diver-
gence to the uniform dirichlet distribution [18].
2.3
experiments
network/graph ablation: we explore the hyper-parameters of the proposed
model in an extensive ablation study.
for the evidential loss, we evaluate the choice of loss function
(the 3 previously mentioned), and the annealing coeﬃcient (5–50 with step size
evidential graph transformer
567
of 5).
ex-vivo evaluation: the performance of the proposed network is compared
with 3 baseline models including gtn, graph convolution network
four-fold cross validation is used for compar-
ison of the diﬀerent approaches, to increase the generalizability (3 folds for
train/validation, test on remaining unseen fold, report average test performance).
all experiments are implemented using pytorch
with adam optimizer, learning rate of 10−4, batch size of 32, and early stopping
based on validation loss.
to demonstrate the robustness of the model and ensure
it is not overﬁtting, we also report the performance of the ensemble model from
the 4-fold cross validation study on the 5th unseen prospective test fold.
clinical relevance: hormone receptor status plays an important role in deter-
mining breast cancer prognosis and tailoring treatment plans for patients [6].
these
hormones are involved in diﬀerent types of signaling that the cell depends on [5].
intra-operative deployment:
to explore the intra-operative capability of
the models, we deploy the ensemble models of the proposed method as well as
the baselines from the cross-validation study to the bcs iknife stream.
3
results and discussion
ablation study and ex-vivo evaluation: according to our ablation study,
hyper parameters of 11 attention heads, 11 hidden features per attention head,
the cross entropy loss function, and annealing coeﬃcient of 30, result in higher
performances when compared to other conﬁgurations (370k learnable parame-
ters).
the performance of egt in comparison with the mentioned baselines are
summarized in table 1.
as can be seen, the proposed egt model with aver-
age accuracy of 94.1% outperformed all the baselines statistically signiﬁcantly
(maximum p-values of 0.02 in one-tail paired wilcoxon signed-rank test).
lastly, when compared to other state-of-
the-art baselines with uncertainty estimation mechanisms, the proposed evi-
dential graph transformer network (average balanced accuracy of 91.6 ± 4.3%
in table 1) outperforms mc dropout
average(standard deviation) of accuracy (acc), balanced accuracy (bac)
sensitivity (sen), speciﬁcity (spc), and the area under the curve (auc) for the
proposed evidential graph transformer in comparison with graph transformer (gtn),
graph convolution (gcn), and non-graph convolution (cnn) baselines.
right eﬀect of uncertain data exclusion on accuracy and auc during model deploy-
ment.
this information can be provided during deployment to further augment
surgical decision making for uncertain data instances.
the result of our graph structure ablation shows the drop of average acc
to 85.6% by randomizing the edges in the graph (p-value 0.004).
although
the model still trained due to node aggregation, random graph structure acts
as noise and aﬀects the performance.
we have also found that there’s
more attention in this range for pr negative breast cancer in comparison pr
positive, which is in concordance with previous literature demonstrating that
these subtypes have higher glutamine metabolic activity [4,5].
intra-operative deployment:
the raw intra-operative iknife data (y-axis
is m/z spectral range and x-axis is the surgery timeline) along with the tem-
poral reference labels extracted from surgeon’s call-outs and pathology report
are shown in fig.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_24.pdf:
however, recent research has revealed that deep
neural networks for skin lesion recognition may overly depend on disease-
irrelevant image artifacts (i.e. dark corners, dense hairs), leading to poor
generalization in unseen environments.
concretely, epvt leverages a set of
domain prompts, each of which plays as a domain expert, to capture
domain-speciﬁc knowledge; and a shared prompt for general knowledge
over the entire dataset.
experiments on four out-of-distribution datasets and six
diﬀerent biased isic datasets demonstrate the superior generalization
ability of epvt in skin lesion recognition across various environments.
keywords: skin lesions · prompt · domain generalization · debiasing
1
introduction
skin cancer is a serious and widespread form of cancer that requires early detec-
tion for successful treatment.
computer-aided diagnosis systems (cad) using
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43990-2_24.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
however, recent research has revealed that the success of these models
may be a result of overly relying on “spurious cues” in dermoscopic images, such
as rulers, gel bubbles, dark corners, and hairs
when a deep learning model overﬁts speciﬁc artifacts instead of
learning the correct dermoscopic patterns, it may fail to identify skin lesions in
real-world environments where the artifacts are absent or inconsistent.
to alleviate the artifact bias and enhance the model’s generalization ability,
we rethink the problem from the domain generalization (dg) perspective, where
a model trained within multiple diﬀerent but related domains are expected to
perform well in unseen test domains.
1, we deﬁne the domain
labels based on the types of artifacts present in the training images, which can
provide environment-aware prior knowledge reﬂecting a range of noisy contexts.
previous dg algorithms learning domain-invariant features from source
domains have succeeded in natural image tasks [2,17,19], but cannot directly
apply to medical images, in particular skin images, due to the vast cross-domain
diversity of skin lesions in terms of shapes, colors, textures, etc.
to overcome the above problems, we propose an environment-aware prompt
vision transformer (epvt) for domain generalization of skin lesion recognition.
epvt: environment-aware prompt vision transformer
251
fig.
2. the overview of our environment-aware prompt vision transformer (epvt).
[8] is adopted to fully model the
relationship between image tokens and prompt vectors.
on the other hand, to
encourage cross-domain information sharing while preserving the domain-speciﬁc
knowledge of each domain prompt, we propose a domain prompt generator based
on low-rank weights updating.
additionally, we devise a domain mixup strategy to resolve
the problem of co-occurring artifacts in dermoscopic images and mitigate the
resulting noisy domain label assignments.
our contributions can be summarized as: (1) we resolve an artifacts-derived
biasing problem in skin cancer diagnosis using a novel environment-aware prompt
learning-based dg algorithm, epvt; (2) epvt takes advantage of a vit-
based domain-aware prompt learning and a novel domain prompt generator to
improve domain-speciﬁc and cross-domain knowledge learning simultaneously;
(3) a domain mixup strategy is devised to reduce the co-artifacts speciﬁc to
dermoscopic images; (4) extensive experiments on four out-of-distribution skin
datasets and six biased isic datasets demonstrate the outperforming general-
ization ability and robustness of epvt under heterogeneous distribution shifts.
(1)
where f is the feature encoder of the vit, x0 denotes the class token, e0 is
the image patch embedding, fm is the feature extracted by vit with the m-th
prompt, and 0 is the index of the ﬁrst layer.
domain prompts pd are a set of
learnable tokens, with each prompt p m being fed into the vision transformer
along with the image and corresponding class tokens from a speciﬁc domain.
through optimizing, each prompt becomes a domain expert only responsible
for the images from its own domain.
our approach is inspired by model adaptation
and multi-task learning techniques used in natural language processing [13,26].
aghajanyan et al.
to this end, we decompose each p m into a hadamard
product between a randomly initialized shared prompt p ∗ and a rank-one matrix
pk obtained from two randomly initialized learnable vectors uk and vk, which
is:
p m = p ∗ ⊙ pk
where
pk = uk · vt
k
(2)
epvt: environment-aware prompt vision transformer
253
where p m represents the domain-speciﬁc prompt, computed by hadamard prod-
uct of p ∗ and pk.
here, p ∗ ∈ rs×d is utilized to learn general knowledge, with s
and d representing the dimensions of the prompt vector and feature embedding
respectively.
by using the hadamard product, the model can eﬃciently leverage cross-domain
knowledge for target domain prediction.
2.3
mitigating the co-artifacts issue
the artifacts-based domain labels can provide domain information for der-
moscopic images.
instead of assign-
ing a hard prediction label (“0” or “1”) to each image, in each batch, we mix
every image using two randomly selected images from two diﬀerent domains.
we then
apply the cross-entropy loss to the corresponding labels of bot images, as shown
in fig.
this strategy can overcome
the challenge of ambiguous domain labels in dermoscopic images and improve
the performance of our model.
2.4
optimization
so far, we have introduced lmixup in eq. 3 for optimizing our model.
however,
since our goal is to generalize the model to unseen environments, we also need
to take advantage of each domain prompt.
[30] that learns the linear correlation
between the domain prompts and the target image prediction.
=
m

m=1
wm · p m,
s.t.
m

m=1
wm = 1
(4)
where a represents an adapter containing a two-layer mlp with a softmax layer,
and wm denotes the learned weights.
to train the adapter a, we simulate the inference process for each image in
the source domain by treating it as an image from the pseudo-target domain.
then
we calculated the adapted prompt padapted for the pseudo-target environment
image x using the adapter a: padapted = a( ˆ
fm(x)).
to ensure that the adapter learns the correct linear correlation between the
domain prompts and the target image, we use the domain label from source
domains to directly supervise the weights wm.
we also use the cross-entropy loss
to maintain the model performance with the adapted prompt:
ladapted = lce(h( ˆfm(x)), y)
3
experiments
experimental setup: we consider two challenging melanoma-benign classi-
ﬁcation settings that can eﬀectively evaluate the generalization ability of our
model in diﬀerent environments and closely mimic real-world scenarios.
we use
the artifacts annotations from [3] and divide the training set of isic2019 into ﬁve
groups: dark corner, hair, gel bubble, ruler, and clean, with 2351, 4884, 1640, 672,
and 2796 images, respectively.
it’s worth noting that isic2019, derm7pt-
dermoscopic, and ph2 are dermoscopic images, while derm7pt-clinical and
pad are clinical images.
(2) trap set debiasing: we train and test our epvt
with its baseline on six trap sets [3] with increasing bias levels, ranging from 0
(randomly split training and testing sets from the isic2019 dataset) to 1 (the
highest bias level where the correlation between artifacts and class label is in
the opposite direction in the dataset splits).
more details about these datasets
and splits are provided in the complementary material.
implementation details: for a fair comparison, we train all models using vit-
base/16
[8] backbone pre-trained on imagenet and report the roc-auc with
ﬁve random seeds.
we conduct a grid search over learning rate
(from 3e−4 to 5e−6), weight decay (from 1e−2 to 1e−5), and the length of the
epvt: environment-aware prompt vision transformer
255
table 1.
the comparison on out-of-distribution datasets
method
derm7pt_d
derm7pt_c
pad
ph2
average
erm
81.24 ± 1.6
71.61 ± 1.9
82.62 ± 1.6
83.06 ± 1.9
79.63 ± 1.5
dro
[30] 82.38 ± 1.0
71.61 ± 1.7
83.81 ± 1.4
91.33 ± 1.8
82.06 ± 1.6
selfreg [15]
81.83 ± 1.9
73.29 ± 1.4
85.27 ± 1.3
85.16 ± 3.3
81.12 ± 1.0
epvt (ours)
83.69 ± 1.4 73.96 ± 1.6 86.67 ± 1.5
91.91 ± 1.5 84.11 ± 1.4
prompt (from 4 to 16, when available) and report the best performance of all
models.
we resize the input image to a size of 224 × 224 and adopt
the standard data augmentation like random ﬂip, crop, rotation, and color jitter.
an early stopping with the patience of 22 is set and with a total of 60 epochs
for ood evaluation and 100 epochs for trap set debiasing.
all experiments are
conducted on a single nvidia rtx 3090 gpu.
out-of-distribution evaluation: table 1 presents a comprehensive compar-
ison of our epvt algorithm with existing domain generalization methods.
the
results clearly demonstrate the superiority of our approach, with the best perfor-
mance on three out of four ood datasets and remarkable improvements over the
erm algorithm, especially achieving 4.1% and 8.9% improvement on the pad
and ph2 datasets, respectively.
although some algorithms may perform simi-
larly to our model on one of the four datasets, none can consistently match the
performance of our method across all four datasets.
particularly, our approach
showcases the highest average performance, with a 2.05% improvement over the
second-best algorithm across all four datasets.
these ﬁndings highlight the eﬀec-
tiveness of our algorithm in learning robust features and its strong generalization
abilities across diverse environments.
firstly, we observe
that the baseline model with prompt only improves the average performance by
256
s. yan et al.
table 2. ablation study on out-of-distribution datasets
method
derm7pt_d
derm7pt_c
pad
ph2
average
baseline
81.24 ± 1.6
71.61 ± 1.9
82.62 ± 1.6
83.06 ± 1.9
79.63 ± 1.5
+p
82.13 ± 1.1
71.41 ± 1.3
82.15 ± 1.6
84.21 ± 1.4
79.73 ± 1.3
+p+a
82.55 ± 1.6
72.86 ± 1.1
81.02 ± 1.5
84.97 ± 1.8
81.10 ± 1.6
+p+a+m
81.43 ± 1.4
73.18 ± 1.5
85.78 ± 1.9
89.28 ± 1.3
82.42 ± 1.7
3. (a) deibiasing evaluation (b) domain distance (c) domain weights
0.1%, showing that simply combining prompt does not very helpful for domain
generalization.
when we combine the adapter, the model’s average performance
improves by 1.37%, but it performs worse than erm on pad dataset.
subse-
quently, we added domain mixup and domain prompt generator to the model,
resulting in signiﬁcant further improvements in the model’s average performance
by 1.32% and 1.69%, respectively.
the consistently better performance than the
baseline on all four datasets also highlights the importance of addressing co-
artifacts and cross-domain learning for dg in skin lesion recognition.
trap set debiasing: in fig.
3a, we present the performance of the erm base-
line and our epvt on six biased isic2019 datasets.
each point on the graph
represents an algorithm that is trained and tested on a speciﬁc bias degree split.
the graph shows that the erm baseline performs better than our epvt when
the bias is low (0 and 0.3).
as the bias degree increases, the correlation between artifacts and
class labels decreases, and overﬁtting the train set causes the performance of
erm to drop dramatically on the test set with a signiﬁcant distribution diﬀer-
ence.
in contrast, our epvt exhibits greater robustness to diﬀerent bias levels.
notably, our epvt outperforms the erm baseline by 9.4% on the bias 1 dataset.
[9] between each domain and the target
dataset using the extracted feature, representing the domain distance between
epvt: environment-aware prompt vision transformer
257
them.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_48.pdf:
both cnn-based and transformer-based object detection
with bounding box representation have been extensively studied in com-
puter vision and medical image analysis, but circular object detection in
medical images is still underexplored.
speciﬁcally, queries with circle representation
in transformer decoder iteratively reﬁne the circular object detection
results, and a circle cross attention module is introduced to compute the
similarity between circular queries and image features.
moreover, our approach is easy to generalize to
the segmentation task by adding a simple segmentation branch to cir-
cleformer.
we evaluate our method in circular nuclei detection and seg-
mentation on the public monuseg dataset, and the experimental results
show that our method achieves promising performance compared with
the state-of-the-art approaches.
our code is released at: https://
github.com/zhanghx-iim-ahu/circleformer.
keywords: circular object analysis · circular queries · transformer
1
introduction
nuclei detection is a highly challenging task and plays an important role in many
biological applications such as cancer diagnosis and drug discovery.
1.
transformer-based
circle
detection and segmentation.
however,
recent studies on transformer-based detection methods are designed for rectan-
gle object detection in computer vision, which are not speciﬁcally designed for
circular objects in medical images.
circleformer: circular nuclei detection
495
in this paper, we introduce circleformer, a transformer-based circular object
detection for medical image analysis.
we propose a novel circle cross
attention module which enables us to apply circle center (x, y) to extract image
features around a circle and make use of circle radius to modulate the cross
attention map.
we evaluate our circleformer on
the public monuseg dataset for nuclei detection in whole slide images.
experi-
mental results show that our method outperforms both cnn-based methods for
box detection and circular object detection.
to further study the generalization ability of our approach, we add a
simple segmentation branch to circleformer following the recent query based
instance segmentation models [2,17] and verify its performance on monuseg as
well.
kx,y = concat(fx,y, pe(x, y)), vx,y = fx.y,
(3)
496
h. zhang et al.
where fx,y ∈ rd denote the image feature at position (x, y) and an mlp(csq) :
rd → rd is used to obtain a scaled vector conditioned on content information
for a query.
by representing a circle query as (x, y, r), we can reﬁne the circle query
layer-by-layer in the transformer decoder.
in this way, the circle query
representation is suitable for circular object detection and is able to accelerate
the learning convergence via layer-by-layer reﬁnement scheme.
2.3
circle cross attention
we propose circle-modulated attention and deformable circle cross attention to
consider size information of circular object detection in cross attention module.
the circle radius modulated positional atten-
tion map provides beneﬁts to extract image features of objects with diﬀerent
scales.
ma((x, y), (xref, yref))
given an input feature map f ∈ rc×h×w , let i index a query element
with content feature zi and a reference point pi, the deformable circle cross
attention feature is calculated by:
cda(zi, pi, f) =
m

m=1
wm
k

k=1
attnmik ˙w ′
mf ((pix + δrmik
×ri,ref ×
experiments show that cda-c initialization of reference
points outperforms others.
2.5
circle instance segmentation
a mask is predicted from a decoder embedding by ˆmi = ffn(ffn(fi)
we
use dice and bce as the segmentation loss: lseg = λdiceldice(mi, ˆmi) +
λbcelbce(mi, ˆmi) between prediction ˆmi and the groundtruth mi.
2.6
generalized circle iou
circlenet extends intersection over union (iou) of bounding boxes to circle iou
(ciou) and shows that the ciou is a valid overlap metric for detection of circular
objects in medical images.
we show that gciou can bring consistent improvement
on circular object detection.
figure 4 shows the diﬀerent measurements between
two rectangles and circles.
following detr, i-th each element of the groundtruth
set is yi = (li, ci), where li is the target class label (which may be ∅) and
ci = (x, y, r).
+ i{li̸=∅}lcircle(ci, ˆcσ(i)),
(6)
where σ ∈ sn is a permutation of all prediction elements, ˆyσ(i) = (ˆlσ(i), ˆcσ(i)) is
the prediction, λfocal ∈ r are hyperparameters, and lfocal is focal loss [8].
mi is the ground truth obtained by roi align [5]
corresponding to ˆmi.
498
h. zhang et al.
3
experiment
3.1
dataset and evaluation
monuseg dataset.
monuseg dataset is a public dataset from the 2018 multi-
organ nuclei segmentation challenge [6].
it contains 30 training/validataion
tissue images sampled from a separate whole slide image of h&e stained tissue
and 14 testing images of lung and brain tissue images.
following [16], we ran-
domly sample 10 patches with size 512 ×512 from each image and create 200
training images, 100 validation images and 140 testing images.
[16], and ap m for the instance segmentation evaluation metrics.
s
and m are used to measure the performance of small scale with area less than
322 and median scale with area between 322 and 962.
3.2
implementation details
two variants of our proposed method for nuclei detection, circleformer and
circleformer-d are built with a circle cross attention module and a deformable
circle cross attention module, respectively.
circleformer-d-joint (ours) extends
circleformer-d to include instance segmentation as additional output.
since the maximum number of objects per image in the dataset is
close to 1000, we set the number of queries to 1000.
we use λiou = 2.0, λc = 5.0,
λdice = 8.0 and λbce = 2.0 in the experiments.
in summary, our circleformer designed for circular object detection
achieves superior performance compared to both cnn-based box detection and
cnn-based circle detection approaches.
our circleformer with detection head also yields better performance than
transformer-based methods.
[10]
box
✓
resnet50
49.6
89.5
51.5
50.1
31.9
circleformer
circle
resnet50
49.7
88.8
50.9
51.1
35.4
circleformer-d
circle
✓
resnet50
52.9
89.6
58.7
54.1
31.7
circleformer-d-joint
circle
✓
resnet50
53.0
90.0
59.0
53.9
32.8
table 2. results of nuclei joint detection and segmentation on monuseg dataset.
detection
segmentation
methods
ap
ap(50)
ap(75)
[17]
44.8
82.6
45.2
45.5
27.5
41.3
80.6
38.8
41.3
40.7
deformable-detr-joint
45.7
86.7
43.6
46.2
28.2
43.5
84.8
40.5
43.5
42.3
circleformer-d-joint
53.0
90.0
59.0
53.8
32.8
44.4
84.5
43.5
44.4
45.3
jointly outputs detection and segmentation results additionally boosts the detec-
tion results of circleformer-d.
experiments of joint nuclei detection and segmentation are listed in table 2.
our method outperforms queryinst
[2], a cnn-based instance segmentation
method and soit
[17], an transformer-based instance segmentation approach.
we extend transformer-based box detection method to provide additional seg-
mentation output inside the detection region, denoted as deformable-detr-
joint.
our method with circular query representation largely improves both
detection and segmentation results.
to summarize, our method with only detection head outperforms both cnn-
based methods and transformer based approaches in most evaluation metrics
for circular nuclei detection task.
our circleformer-d-joint provides superior
results compared to cnn-based and transformer-based instance segmentation
methods.
also, our method with joint detection and segmentation outputs also
improves the detection-only setting.
in circleformer, the proposed circle-modulated attention (c-ma) improves
the performance of box ap from 45.7% to 48.6% box ap (row 1 and row 2
in p1).
we replaced circle iou (ciou) loss with generalized circle iou (gciou)
loss, the performance is further boosted by 2.2% (row 2 and row 3 in p1).
we obtain similar observations of circleformer-d. when using standard
deformable attention (sda), learning ciou loss gives a 1.2% improvement on
box ap compared to using box iou (row 1 and row 2 in p2).
replacing ciou
with gciou, the performances of sda (row 2 and row 5 in p2), cda-r (row
3 and row 6 in p2) and cda-c (row 4 and row 7 in p2) are boostd by 0.3%
box ap, 0.7% box ap and 1.8% box ap, respectively.
we
vary the number of heads for multi-head attention and the performance of the
model is shown in the table 4.
we ﬁnd that the performance increases gradually
as the number of heads increases up to 8.
however, the performance drops when
the number of head is 16.
we assume increasing the number of heads brings too
many parameters and makes the model diﬃcult to converge.
we
ﬁnd that 4 reference points give the best performance.
therefore, we choose to
use 8 attention heads of decoder and use 4 reference points in the cross attention
module through all the experiments.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_60.pdf:
whole slide image (wsi) classiﬁcation is a critical task
in computational pathology, requiring the processing of gigapixel-sized
images, which is challenging for current deep-learning methods.
due to the lack of task-speciﬁc annotated data, these features
are either obtained from well-established backbones on natural images,
or, more recently from self-supervised models pretrained on histopathol-
ogy.
however, both approaches yield task-agnostic features, resulting in
performance loss compared to the appropriate task-related supervision, if
available.
extensive experiments on three wsi
datasets, tcga-brca, tcga-crc, and bright, demonstrate the
superiority of prompt-mil over conventional mil methods, achieving a
relative improvement of 1.49%–4.03% in accuracy and 0.25%–8.97% in
auroc while using fewer than 0.3% additional parameters.
compared
to conventional full ﬁne-tuning approaches, we ﬁne-tune less than 1.3%
of the parameters, yet achieve a relative improvement of 1.29%–13.61%
in accuracy and 3.22%–27.18% in auroc and reduce gpu memory
consumption by 38%–45% while training 21%–27% faster.
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43993-3_60.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
https://doi.org/10.1007/978-3-031-43993-3_60
prompt-mil: boosting mil schemes via prompt tuning
625
keywords: whole slide image classiﬁcation · multiple instance
learning · prompt tuning
1
introduction
whole slide image (wsi) classiﬁcation is a critical task in computational pathol-
ogy enabling disease diagnosis and subtyping using automatic tools.
imagenet pretrained networks have been widely used as mil feature extractors.
more recently, self-supervised learning (ssl), using a large amount of unlabeled
histopathology data, has become quite popular for wsi classiﬁcation [5,13] as
it outperforms imagenet feature encoders.
most existing mil methods do not ﬁne-tune their feature extractor together
with their classiﬁcation task; this stems from the requirement for far larger
gpu memory than is available currently due to the gigapixel nature of wsis,
e.g. training a wsi at 10x magniﬁcation may require more than 300 gb of
gpu memory.
these methods show better performance compared to
conventional mil; they suﬀer, however, from two limitations.
first, they are
imagenet-pretrained and do not leverage the powerful learning capabilities of
histology-trained ssl models.
however, we ﬁnd that conven-
tional ﬁne-tuning approaches, where the entire network is ﬁne-tuned, achieve low
performance.
the poor performance
is probably caused by the large network over-ﬁtted to the limited downstream
training data, leading to suboptimal feature representation.
indeed, especially
for weakly supervised wsi classiﬁcation, where annotated data for downstream
tasks is signiﬁcantly less compared to natural image datasets, conventional ﬁne-
tuning schemes can prove to be quite challenging.
to address the subpar performance of ssl-pretrained vision transformers,
we utilize the prompt tuning techniques.
initially proposed in natural language
processing, a prompt is a trainable or a pre-deﬁned natural language statement
that is provided as additional input to a transformer to guide the neural net-
work towards learning a speciﬁc task or objective [3,12].
recently, prompts have also been adopted in computer vision and
demonstrated superior performance compared to conventional ﬁne-tuning meth-
ods [10].
our con-
tributions are:
– fine-tuning: unlike existing works in histopathology image analysis,
prompt-mil is ﬁne-tuned using prompts rather than conventional full ﬁne-
tuning methods.
extensive experiments on three public wsi datasets, tcga-brca, tcga-
crc, and bright demonstrate the superiority of prompt-mil over conven-
tional mil methods, achieving a relative improvement of 1.49%–4.03% in accu-
racy and 0.25%–8.97% in auroc by using only less than 0.3% additional
parameters.
compared to the conventional full ﬁne-tuning approach, we ﬁne-
tune less than 1.3% of the parameters, yet achieve a relative improvement of
1.29%–13.61% in accuracy and 3.22%–27.18% in auroc.
given a wsi and its label y, the image is tiled into n tissue
prompt-mil: boosting mil schemes via prompt tuning
627
patches/instances {x1, x2, . . .
the vit ﬁrst divides an input image xi into w smaller patches [z1, z2, . . .
an input image xi is cropped into w small
patches z1, . .
, f(xn, p)]), y),
(7)
where only the parameters of the g(·) and the prompt p are optimized, while
the feature extractor model f(·) is frozen.
training the entire pipeline in an end-to-end fashion on gigapixel images is
infeasible using the current hardware.
in this step, we just conduct a forward pass like the inference stage, with-
out storing the memory-intensive computational graph for back-propagation.
3
experiments and discussion
3.1
datasets
we assessed prompt-mil using three histopathological wsi datasets: tcga-
brca
reported metrics
(in %age) are the average across 3 runs.
“num. of parameters” represents the number
of optimized parameters
dataset
tcga-brca
tcga-crc
bright
num. of
metric
accuracy auroc accuracy auroc accuracy auroc parameters
conventional mil
92.10
96.65
73.02
69.24
62.08
80.96
70k
full ﬁne-tuning
88.14
93.78
74.53
56.63
56.13
75.87
5.6m
prompt-mil (ours) 93.47
96.89
75.47
75.45
64.58
81.31
70k + 192
630
j. zhang et al.
3.2
implementation details
we cropped non-overlapping 224 × 224 sized patches in all our experiments
and used vit-tiny (vit-t/16)
for ssl pretraining,
we leveraged the dino framework [4] with the default hyperparameters, but
adjusted the batch size to 256 and employed the global average pooling for token
aggregation.
we
applied a cosine annealing learning rate decay policy in all our experiments.
for all full
ﬁne-tuning experiments, we used the learning rate in the corresponding prompt
experiment as the base learning rate.
all model implementations were in pytorch
evaluation of prompt tuning performance: we compared the proposed
prompt-mil with two baselines: 1) a conventional mil model with a frozen
feature extractor [13], 2) ﬁne-tuning all parameters in the feature model (full
ﬁne-tuning).
compared to the conventional mil method, prompt-mil added negli-
gible parameters (192, less than 0.3% of the total parameters), achieving a
relative improvement of 1.49% in accuracy and 0.25% in auroc on tcga-
brca, 3.36% in accuracy and 8.97% in auroc on tcga-crc, and 4.03% in
accuracy and 0.43% in auroc on bright.
the observed improvement can
be attributed to a more optimal alignment between the feature representation
learned during the ssl pretraining and the downstream task, i.e., the prompt
explicitly calibrated the features toward the downstream task.
compared to the full ﬁne-tuning method, our
method achieved a relative improvement of 1.29% to 13.61% in accuracy and
3.22% to 27.18% in auroc on the three datasets.
wsi size
44k × 21k 26k × 21k 22k × 17k 11k × 16k
#tissue patches
9212
4765
2307
1108
gpu mem.
full ﬁne-tuning
21.81g
18.22g
16.37g
12.71g
prompt (ours)
12.04g
10.66g
10.00g
7.90g
reduction percentage 44.79%
41.50%
38.92%
37.84%
time per slide full ﬁne-tuning
17.73 s
8.92 s
4.37 s
2.15 s
prompt (ours)
13.92 s
7.09 s
3.35s
1.56 s
reduction percentage 21.49%
20.51%
23.32%
27.27%
table 3. comparison of accuracy and auroc on three datasets for a pathological
foundation model.
evaluation on the pathological foundation models: we demonstrated
our prompt-mil also had a better performance when used with the pathologi-
cal foundation model.
in table 3, we showed
that our method robustly boosted the performance on both tcga (the same
632
j. zhang et al.
domain as the foundation model trained on) and bright (a diﬀerent domain).
the improvement is more prominent in bright, which further conﬁrmed that
prompt-mil aligns the feature extractor to be more task-speciﬁc.
table 4. performance with a diﬀerent number of prompt tokens.
for two diﬀerent wsi
classiﬁcation tasks, one token was enough to boost the performance of the conventional
mil schemes.
on the tcga-
brca dataset, our prompt-mil model with 1 to 3 prompt tokens reported
similar performance.
on the bright dataset, the performance of our model
dropped with the increased number of prompt tokens.
empirically, this ablation
study shows that for classiﬁcation tasks, one prompt token is suﬃcient to boost
the performance of conventional mil methods.
4
conclusion
in this work, we introduced a new framework, prompt-mil, which combines the
use of multiple instance learning (mil) with prompts to improve the perfor-
mance of wsi classiﬁcation.
in such a scheme, only a small fraction of parameters calibrates the pretrained
representations to encode task-speciﬁc information, so the entire training can be
performed in an end-to-end manner.
extensive experiments demonstrated the superiority
of prompt-mil over the conventional mil as well as the conventional fully ﬁne-
tuning methods.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_64.pdf:
consideration of subgroups or domains within medical image
datasets is crucial for the development and evaluation of robust and gen-
eralizable machine learning systems.
the variational deep embed-
ding (vade) model is trained to learn lower-dimensional representations
of images based on a mixture-of-gaussians latent space prior distribu-
tion while optimizing cluster assignments.
our experimental results demonstrate that
the considered models are capable of separating digital pathology images
into meaningful subgroups.
we provide a general-purpose implementa-
tion of all considered deep clustering methods as part of the open source
python package domid (https://github.com/didsr/domid).
keywords: domain identiﬁcation · deep clustering · subgroup
identiﬁcation · variational autoencoder · generative model
1
introduction
machine learning (ml), speciﬁcally deep learning (dl), algorithms have shown
exceptional performance on numerous medical image analysis tasks [2].
never-
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43993-3 64.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
for a generalizabil-
ity assessment, reporting only aggregate performance measures is not suﬃcient.
due to model complexity and limited training data, ml performance often varies
across data subgroups or domains, such as diﬀerent patient subpopulations or
varied data acquisition scenarios.
aggregate performance measures (e.g., sensi-
tivity, speciﬁcity, roc auc) can be dominated by the larger subgroups, masking
the poor ml model performance on smaller but clinically important subgroups
[11].
thus, achieving (through training) and demonstrating (as part of testing)
satisfactory ml model performance across relevant subgroups is crucial before
the real-world clinical deployment of a medical ml system [13].
however, a challenging situation arises when relevant subgroups are unrec-
ognized.
however, due to the complexity and high dimensionality of the medical
imaging data and the resulting diﬃculty in establishing a concrete notion of
similarity, extracting low-dimensional characteristics becomes the key to estab-
lishing the best criteria for grouping.
unsupervised generative clustering aims
to simultaneously address both domain identiﬁcation and dimensionality reduc-
tion.
deep unsupervised clustering algorithms could map the medical imaging
data back to their causal factors or underlying domains, such as image acqui-
sition equipment, patient subpopulations, or other meaningful data subgroups.
the resulting algorithmic cluster assignments could then be
used to improve ml algorithm training, or for generalizability and robustness
evaluation.
2
methods
we provide a pytorch-based implementation of all deep clustering algorithms
described below (vade, cdvade, and dec) in the open source python package
domid that is publicly available under https://github.com/didsr/domid.
2.1
variational deep embedding (vade)
variational deep embedding (vade)
the encoder learns to
668
m. sidulova et al.
compress the high-dimensional input images x into lower-dimensional latent rep-
resentations z. using a mixture-of-gaussians (mog) prior distribution for the
latent representations z, we examine subgroups or domains within the dataset,
revealed by the individual gaussians within the learned latent space, and how z
aﬀects the generation of x. the model can be used to perform inference, where
observed images x are mapped to corresponding latent variables z and their
cluster/domain assignments c. we denote the latent space dimensionality by d
(i.e., z ∈ rd), and the number of clusters by d (i.e., c ∈ {1, 2, . . .
the
trained decoder cnn can also be used to generate synthetic images from the
algorithmically identiﬁed subgroups.
finally, the cluster assignments can be determined via
q(c|x)
we
refer to [6] for details.
in all our experiments, we apply vade with cnn architectures for the
encoder and decoder.
speciﬁcally, the generative process of cdvade takes the form
p(c) = cat(π)
(4)
p(z|c) = n

z; μc, diag

σ2
c

,
(5)

μxy, log σ2
xy

= f(z, y; φ),
(6)
p(x|z, y) = n

x; μxy, diag

σ2
xy

(7)
since our goal is to ﬁnd clusters c that are unassociated with the available
variables y of choice and to learn latent representations z that do not contain
information about y, the generative process of cdvade assumes that z, c are
jointly independent of y.
the changes compared to the generative process of vade can also be
regarded as imposing a structure on the model, where the encoder learns hidden
representations of the image x conditioned to the additional variables y (i.e.,
q(z|x, y)), but acts as an identity function with respect to y (i.e., y can be
regarded as being simply concatenated to the latent space representations z).
in all our experiments, we use the same cnn
architectures for the encoder and decoder as in vade (see sect. 2.1).
in our dec experiments,
we use the same autoencoder architecture and the same initialization as for the
vade.
typically, clustering is performed on top of
features extracted with the use of an encoder neural network, and the cluster
assignments are determined by using conventional clustering algorithms, such
as k-means, on top of the learned latent representations [1,5,7,12].
3
experiments
3.1
colored mnist
the colored mnist is an extension to the classic mnist dataset [3], which
contains binary images of handwritten digits.
the colored mnist includes col-
ored images of the same digits, where each number and background have a color
assignment.
we present results of the experiments with ﬁve distinct colors and
ﬁve digits of mnist (0–4).
to enhance computational eﬃciency and expedite
experiments, we utilized only 1% of the mnist images, which were sampled
at random.
this simple dataset can be used to investigate whether a given
clustering algorithm will categorize the images by color or by the digit label
conditionally decoded variational deep embedding (cdvade)
we use latent space
dimensionality d = 20 for all models.
fig.
2 a summary of the results for the experiments on the colored mnist
dataset is presented.
notably,
both vade and dec end up clustering the data by color, as it is the most
striking distinguishing characteristic of these images.
on the other hand, the
predicted domains of cdvade have no association with color, and the data are
separated by the shapes in the images, distinguishing some of the digit labels
(albeit imperfectly).
human epidermal growth factor receptor 2 (her2 or
her2/neu) is a protein involved in normal cell growth, which plays an impor-
tant role in the diagnosis and treatment of breast cancer
we use a subset of this dataset consisting of 672 images (the remainder is
held out for future research).
the dimensions of the
images vary from 600 to 826 pixels, and we scale all data to a uniform size of
128 × 128 pixels before further processing.
672
m. sidulova et al.
this retrospective human subject dataset has been made available to us by
the authors of the prior studies [4,8], who are not associated with this paper.
we evaluate
the performance and behavior of the dec, vade, and cdvade models on
the her2 dataset.
the dimensionality of the latent
embedding space was set to d = 500 for all three models.
3. results summary for vade, cdvade, and dec, all with d = 3. example
images from the identiﬁed clusters are visualized for each method.
figure 3 demonstrates that even without scrutinizing, one can observe a
strong visual separation between the algorithmically identiﬁed image domains
for both vade and dec experiments.
3 images tend to have slightly visible boundaries but a com-
paratively uniform light appearance overall.
in the second predicted domain,
images have less visible boundaries and more pail staining.
in the third pre-
dicted domain, images have more visible staining and sharper edges compared
to the other domains.
the pearson’s correlation coeﬃcient between the
clustering assignments c of vade and the her2/neu scores is 0.46.
4. boxplots of her2/neu scores per predicted domain for all experiments.
the correlation coeﬃcient between the cdvade cluster assignments and the
her2/neu scores is 0.39.
moreover, the clusters identiﬁed by
cdvade are distinctly diﬀerent from those of vade, with a 0.43 proportion of
agreement between the two algorithms (after matching the two sets of cluster
assignments using the hungarian algorithm).

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_59.pdf:
accurate 3d mitochondria instance segmentation in electron
microscopy (em) is a challenging problem and serves as a prerequisite
to empirically analyze their distributions and morphology.
further,
we introduce a semantic foreground-background adversarial loss during
training that aids in delineating the region of mitochondria instances
from the background clutter.
our extensive experiments on three bench-
marks, lucchi, mitoem-r and mitoem-h, reveal the beneﬁts of the
proposed contributions achieving state-of-the-art results on all three
datasets.
keywords: electron microscopy · mitochondria instance
segmentation · spatio-temporal transformer · hybrid
cnn-transformers
1
introduction
mitochondria are membrane-bound organelles that generate the primary energy
required to power the cell activities, thereby crucial for metabolism.
mitochon-
drial dysfunction, which occurs when mitochondria are not functioning properly
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43993-3_59.
[23,25]. elec-
tron microscopy (em) images are typically utilized to reveal the corresponding
3d geometry and size of mitochondria at a nanometer scale, thereby facilitating
basic biological research at ﬁner scales.
therefore, automatic instance segmenta-
tion of mitochondria is desired, since manually segmenting from a large amount
of data is particularly laborious and demanding.
however, automatic 3d mito-
chondria instance segmentation is a challenging task, since complete shape of
mitochondria can be sophisticated and multiple instances can also experience
entanglement with each other resulting in unclear boundaries.
here, we look
into the problem of accurate 3d mitochondria instance segmentation.
earlier works on mitochondria segmentation employ standard image process-
ing and machine learning methods
[20,21,33].
in
case of bottom-up mitochondria instance segmentation approaches, a binary
segmentation mask, an aﬃnity map or a binary mask with boundary instances
is computed typically using a 3d u-net
on the other hand, top-down methods typ-
ically rely on techniques such as mask r-cnn [7] for segmentation.
when designing a attention-based framework for 3d mitochondria instance
segmentation, a straightforward way is to compute joint spatio-temporal self-
attention where all pairwise interactions are modelled between all spatio-
temporal tokens.
the focus of our design is the introduction of a split
spatio-temporal attention (sst) module that captures long-range dependen-
cies within the cubic volume of human and rat mitochondria samples.
– to accurately delineate the region of mitochondria instances from the clut-
tered background, we further introduce a semantic foreground-background
(fg-bg) adversarial loss during the training that aids in learning improved
instance-level features.
– we conduct experiments on three commonly used benchmarks: lucchi
our stt-unet achieves state-of-the-art
3d mitochondria instance segmentation with spatio-temporal transformers
615
fig.
1. qualitative 3d instance segmentation comparison between the recent res-
unet [16] and our proposed stt-unet approach on the example input regions
from mitoem-h and mitoem-r validation sets.
here, we present the corresponding
segmentation predictions of the baseline and our approach along with the ground truth.
our stt-unet approach achieves superior segmentation performance by accurately
segmenting 16% more cell instances in these examples, compared to res-unet-r.
segmentation performance on all three datasets.
2
related work
most recent approaches for 3d mitochondria instance segmentation utilize con-
volution based designs within the “u-shaped” 3d encoder-decoder architecture.
in such an architecture, the encoder aims to generate a low-dimensional rep-
resentation of the 3d data by gradually performing the downsampling of the
extracted features.
on the other hand, the decoder performs upsampling of
these extracted feature representations to the input resolution for segmenta-
tion prediction.
although such a cnn-based designs [11,16,34] has achieved
promising segmentation results compared to traditional methods, they struggle
to eﬀectively capture long-range dependencies due to their limited local recep-
tive ﬁeld.
inspired from success in natural language processing [32], recently
vision transformers (vits)
[6,13,19,30,31] have been successfully utilized in dif-
ferent computer vision problems due to their capabilities at modelling long-range
dependencies and enabling the model to attend to all the elements in the input
616
o. thawakar et al.
sequence.
the core component in vits is the self-attention mechanism that that
learns the relationships between sequence elements by performing relevance esti-
mation of one item to other items.
the other attention such as [1,8,10,29,35]
have demonstrated remarkable eﬃcacy in eﬀectively managing volumetric data.
inspired by vits
[10,19] and based on the observation that attention-based vision
transformers architectures are an intuitive design choice for modelling long-range
global contextual relationships in volume data, we investigate designing a cnn-
transformers based framework for the task of 3d mitochondria instance segmen-
tation.
the input volume is denoised
using an interpolation network adapted for medical images [9].
the decoder outputs semantic mask and instance
boundary, which are then post-processed using connected component labelling
to generate ﬁnal instance masks.
while
self-attention has been shown to be beneﬁcial when combined with convolutional
layers for diﬀerent medical imaging tasks, to the best of our knowledge, no pre-
vious attempt to design spatio-temporal self-attention as an exclusive building
block for the problem of 3d mitochondria instance segmentation exists in liter-
ature.
next, we present our approach that eﬀectively utilizes an eﬃcient spatio-
temporal attention mechanism for 3d mitochondria instance segmentation.
3.2
spatio-temporal transformer res-unet (stt-unet)
figure 2(a) presents the overall architecture of the proposed hybrid transformers-
cnn based 3d mitochondria instance segmentation approach, named stt-
unet.
it comprises a denoising module, transformer based encoder-decoder
3d mitochondria instance segmentation with spatio-temporal transformers
617
fig.
2. (a) overall architecture of our stt-unet framework for 3d mitochondria
instance segmentation.
the resulting reconstructed volume is then fed to our split
spatio-temporal attention based encoder-decoder to generate the semantic-level mito-
chondria segmentation masks.
consequently, the semantic masks from the decoder
are then input to the instance segmentation module to generate the ﬁnal instance
masks.
the entire framework is trained using the standard bce loss (lbce) and our
semantic foreground-background (fg-bg) adversarial loss (lfg−bg).
with split spatio-temporal attention and an instance segmentation block.
the
denoising module alleviates the segmentation faults caused by anomalies in the
em images, as in the baseline.
the resulting denoised
output is then processed by our transformer based encoder-decoder with split
spatio-temporal attention to generate the semantic masks.
consequently, these
semantic masks are post-processed by an instance segmentation module using
a connected component labelling scheme, thereby generating the ﬁnal instance-
level segmentation output prediction.
to further enhance the semantic segmen-
tation quality with cluttered background we introduced semantic adversarial loss
which leads to improved semantic segmentation in noisy background.
2(b), that strives to capture long-range dependencies within the cubic
volume of human and rat samples.
the spatial attention reﬁnes the instance
level features from input features along the spatial dimensions, whereas the tem-
poral attention eﬀectively learns the inter-dependencies between the input vol-
ume.
as shown in fig 2(b), the normalized 3d input volume of denoised features x
of size (t ×h ×w ×c) where t is volume size, (h ×w) is spatial dimension of
volume and c is number of channels.
the spatial and temporal attention is deﬁned as,
xs = softmax(qskt
s
√dk
)vs
(1)
xt = softmax(qtpkt
tp
√dk
)vtp
(2)
where, xs is spatial attention map, xt is temporal attention map and dk is
dimension of qs and ks.
we empirically
observe that fusing spatial and temporal features through a deformable convolu-
tion, instead of concatenation through a conv. layer or addition, leads to better
performance.
the resulting spatio-temporal features of decoder are then input
to instance segmentation block to generate ﬁnal instance masks, as in baseline.
semantic fg-bg adversarial loss: as discussed earlier, a common chal-
lenge in mitochondria instance segmentation is to accurately delineate the region
of mitochondria instances from the cluttered background.
to address this, we
introduce a semantic foreground-background (fg-bg) adversarial loss during
the training to enhance the fg-bg separability.
the discriminator takes the input volume i
3d mitochondria instance segmentation with spatio-temporal transformers
619
along with the corresponding mask as an input.
while the discriminator d attempts to
distinguish between ground truth and predicted masks (mgt and mpred, respec-
tively), the model ψ learns to output semantic mask such that the predicted
masks mpred are close to ground truth mgt.
+ λ1ψ[d(fgt) − d(fpr)]
(4)
consequently, the overall loss for training is: l = lbce + λ · lfg−bg, where,
lbce is bce loss, λ = 0.5 and lfg−bg is semantic adversarial loss.
table 1.
[16]
0.895
0.945
res-unet-r + mrda
[4]
0.897
0.946
stt-unet (ours)
0.913
0.962
4
experiments
dataset: we evaluate our approach on three datasets: mitoem-r [36], mitoem-
h
[36] is a dense mitochondria instance seg-
mentation dataset from isbi 2021 challenge.
the dataset consists of 2 em image
volumes (30 μm3) of resolution of 8 × 8 × 30 nm, from rat tissues (mitoem-r)
and human tissue (mitoem-h) samples, respectively.
each volume has 1000
grayscale images of resolution (4096 × 4096) of mitochondria, out of which train
set has 400, validation set contains 100 and test set has 500 images.
[22]
is a sparse mitochondria semantic segmentation dataset with training and test
volume size of 165 × 1024 × 768.
implementation details: we implement our approach using pytorch1.9
during training
of mitoem, for the fair comparison, we adopt same data augmentation technique
from [36].
[16], we do not follow multi-scale training and
620
o. thawakar et al.
perform single stage training for 200k iterations.
for lucchi, we follow training
details of [16,36] for semantic segmentation.
our stt-unet achieves state-of-the-art per-
formance on both sets.
note that [16] employs two decoders for mitoem-h. in contrast, we
utilize only a single decoder for both mitoem-h and mitoem-r sets, while still
achieving improved segmentation performance.
fig 3 presents the segmentation
predictions of our approach on example input regions from the validation set.
our approach achieves promising segmentation results despite the noise in the
input samples.
3. qualitative 3d instance segmentation results of our stt-unet on the exam-
ple input regions from mitoem-h and mitoem-r val sets.
baseline per-
formance comparison.
methods mitoem-r mitoem-h
baseline
0.921
0.823
deisgn choice
mitoem-r mitoem-h
spatial
0.914
0.812
spatial-temporal
0.922
0.817
temporal-spatial
0.937
0.832
spatial|temporal
0.958
0.849
3d mitochondria instance segmentation with spatio-temporal transformers
621
ablation study: table 3 shows a baseline comparison when progressively inte-
grating our contributions: sst module and semantic foreground-background
adversarial loss.
the introduction of sst module improves performance from
0.921 to 0.941 with a gain of 2.7%.
the performance is further improved by
1%, when introducing our semantic foreground-background adversarial loss.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_57.pdf:
nuclei segmentation is a fundamental but challenging task
in the quantitative analysis of histopathology images.
although fully-
supervised deep learning-based methods have made signiﬁcant progress,
a large number of labeled images are required to achieve great segmenta-
tion performance.
considering that manually labeling all nuclei instances
for a dataset is ineﬃcient, obtaining a large-scale human-annotated
dataset is time-consuming and labor-intensive.
therefore, augmenting a
dataset with only a few labeled images to improve the segmentation per-
formance is of signiﬁcant research and application value.
in this paper,
we introduce the ﬁrst diﬀusion-based augmentation method for nuclei
segmentation.
the idea is to synthesize a large number of labeled images
to facilitate training the segmentation model.
in the ﬁrst step, we train an unconditional diﬀusion
model to synthesize the nuclei structure that is deﬁned as the repre-
sentation of pixel-level semantic and distance transform.
each synthetic
nuclei structure will serve as a constraint on histopathology image syn-
thesis and is further post-processed to be an instance map.
in the second
step, we train a conditioned diﬀusion model to synthesize histopathology
images based on nuclei structures.
the synthetic histopathology images
paired with synthetic instance maps will be added to the real dataset for
this work is supported by chinese key-area research and development program of
guangdong province (2020b0101350001), and the guangdong basic and applied basic
research foundation (2023a1515011464, 2020b1515020048), and the national natural
science foundation of china (no. 62102267, no. 61976250), and the shenzhen science
and technology program (jcyj20220818103001002, jcyj20220530141211024), and
the guangdong provincial key laboratory of big data computing, the chinese uni-
versity of hong kong, shenzhen.
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43993-3 57.
https://doi.org/10.1007/978-3-031-43993-3_57
diﬀusion-based data augmentation for nuclei image segmentation
593
training the segmentation model.
the experimental results show that by
augmenting 10% labeled real dataset with synthetic samples, one can
achieve comparable segmentation results with the fully-supervised base-
line.
keywords: data augmentation · nuclei segmentation · diﬀusion
models
fig.
1
introduction
nuclei segmentation is a fundamental step in medical image analysis.
accu-
rately segmenting nuclei helps analyze histopathology images to facilitate clin-
ical diagnosis and prognosis.
in recent years, many deep learning based nuclei
segmentation methods have been proposed [5,18,19,23].
most of these methods
are fully-supervised so the great segmentation performance usually relies on a
large number of labeled images.
however, manually labeling the pixels belonging
to all nucleus boundaries in an image is time-consuming and requires domain
knowledge.
in practice, it is hard to obtain an amount of histopathology images
with dense pixel-wise annotations but feasible to collect a few labeled images.
a question is raised naturally: can we expand the training dataset with a small
proportion of images labeled to reach or even exceed the segmentation perfor-
mance of the fully-supervised baseline?
intuitively, since the labeled images are
samples from the population of histopathology images, if the underlying distri-
bution of histopathology images is learned, one can generate inﬁnite images and
their pixel-level labels to augment the original dataset.
therefore, it is demanded
to develop a tool that is capable of learning distributions and generating new
paired samples for segmentation.
[2,4,12,16,20] have been widely used
in data augmentation [11,22,27,31].
specially, a newly proposed gan-based
method can synthesize labeled histopathology image for nuclei segmentation
[21].
while gans are able to generate high quality images, they are known
for unstable training and lack of diversity in generation due to the adversarial
training strategy.
due to the theo-
retical basis and impressive performance of diﬀusion models, they were soon
applied to a variety of vision tasks, such as inpainting, superresolution [30],
text-to-image translation, anomaly detection and segmentation
as
likelihood-based models, diﬀusion models do not require adversarial training and
outperform gans on the diversity of generated images [3], which are naturally
more suitable for data augmentation.
in this paper, we propose a novel diﬀusion-
based augmentation framework for nuclei segmentation.
the proposed method
consists of two steps: unconditional nuclei structure synthesis and conditional
histopathology image synthesis.
on the training stage, we train the unconditional diﬀusion
model using nuclei structures calculated from instance maps and the conditional
diﬀusion model using paired images and nuclei structures.
on the testing stage,
the nuclei structures and the corresponding images are generated successively
by the two models.
as far as our knowledge, we are the ﬁrst to apply diﬀusion
models on histopathology image augmentation for nuclei segmentation.
our contributions are: (1) a diﬀusion-based data augmentation framework
that can generate histopathology images and their segmentation labels from
scratch; (2) an unconditional nuclei structure synthesis model and a condi-
tional histopathology image synthesis model; (3) experiments show that with
our method, by augmenting only 10% labeled training data, one can obtain
segmentation results comparable to the fully-supervised baseline.
2
method
our goal is to augment a dataset containing a limited number of labeled images
with more samples to improve the segmentation performance.
to increase the
diversity of labeled images, it is preferred to synthesize both images and their
corresponding instance maps.
we propose a two-step strategy for generating new
labeled images.
since it is not viable
to directly generate an instance map, we instead choose to generate its surrogate
– nuclei structure, which is deﬁned as the concatenation of pixel-level semantic
and distance transform.
pixel-level semantic is a binary map where 1 or 0 indi-
cates whether a pixel belongs to a nucleus or not.
clearly, the nuclei struc-
ture is a 3-channel map with the same size as the image.
as nuclei instances
diﬀusion-based data augmentation for nuclei image segmentation
595
can be identiﬁed from the nuclei structure, we can easily construct the corre-
sponding instance map by performance marker-controlled watershed algorithm
on the nuclei structure
2.2
conditional histopathology image synthesis
in the second step, we synthesize histopathology images conditioned on nuclei
structures.
there are usually two ways to synthesize images constrained
596
x. yu et al.
fig.
2. the proposed diﬀusion-based data augmentation framework.
we ﬁrst generate
a nuclei structure with an unconditional diﬀusion model, and then generate images
conditioned on the nuclei structure.
the instance map from the nuclei structure is
paired with the synthetic image to forms a new sample.
by certain conditions: classiﬁer-guided diﬀusion
unlike the network of unconditional nuclei structure synthesis which inputs
the noisy nuclei structure yt and outputs the prediction of ϵt(yt, t), the network
of conditional nuclei image synthesis takes the noisy nuclei image xt and the
corresponding nuclei structure y as inputs and the prediction of ϵt(xt, t, y) as
output.
therefore, the conditional network should be equipped with the ability
to well align the paired histopathology image and nuclei structure.
since nuclei
structures and histopathology images have diﬀerent feature spaces, simply con-
catenating or passing them through a cross-attention module [7,15,17] before
entering the u-net will degrade image ﬁdelity and yield unclear correspondence
between synthetic nuclei image and its nuclei structure.
inspired by [28], we
embed information of the nuclei structure into feature maps of nuclei image
by the spatially-adaptive normalization (spade) module [25].
in other words,
the spatial and morphological information of nuclei modulates the normalized
diﬀusion-based data augmentation for nuclei image segmentation
597
feature maps such that the nuclei are generated in the right places while the
background is left to be created freely.
the network of conditional nuclei image synthesis also applies a u-net archi-
tecture.
3
experiments and results
3.1
implementation details
datasets.
we conduct experiments on two datasets: monuseg
the monuseg dataset has 44 labeled images of size 1000 × 1000, 30 for
training and 14 for testing.
the kumar dataset consists of 30 1000×1000 labeled
images from seven organs of the cancer genome atlas (tcga) database.
the
dataset is splited into 16 training images and 14 testing images.
to validate the eﬀectiveness of the proposed aug-
mentation method, we create 4 subsets of each training dataset with 10%, 20%,
50% and 100% nuclei instance labels.
precisely, we ﬁrst crop all images of each
dataset into 256 × 256 patches with stride 128, then obtain the features of all
patches with pretrained resnet50
for the conditional histopathology image synthesis network,
each layer of the encoder and the decoder has 2 resblocks and 2 condresblocks
respectively, and last 3 layers contain attnblocks.
we use adamw optimizer with learning rates of 10−4
and 2 × 10−5 for the two training stages, respectively.
the syn-
thetic nuclei structures are generate by the nuclei structure synthesis network
and the corresponding images are generated by the histopathology image synthe-
sis network with the classiﬁer-free guidance scale w = 2.
we then obtain the augmented subsets
by adding the synthetic paired images to the corresponding labeled subsets.
598
x. yu et al.
nuclei segmentation.
the eﬀectiveness of the proposed augmentation method
can be evaluated by comparing the segmentation performance of using the four
labeled subsets and using the corresponding augmented subsets to train a seg-
mentation model.
we choose to train two nuclei segmentation models – hover-
net [5] and pff-net [18].
to quantify the segmentation performance, we use
two metrics: dice coeﬃcient and aggregated jaccard index (aji)
the third and fourth row show selected synthetic images and corresponding
nuclei with similar style as the real one in the same column.
3.2
eﬀectiveness of the proposed data augmentation method
fig.
3 shows the synthetic samples from the models trained on the subset with
10% labeled images.
first, the synthetic
samples look realistic: the patterns of synthetic nuclei structures and textures
of synthetic images are close to the real samples.
second, due to the conditional
mechanism of the image synthesis network and the classiﬁer-guidance sampling,
the synthetic images are well aligned with the corresponding nuclei structures,
which is the prerequisite to be additional segmentation training samples.
third,
the synthetic nuclei structures and images show great diversity: the synthetic
samples resemble diﬀerent styles of the real ones but with apparent diﬀerences.
we then train segmentation models on the four labeled subsets of monuseg
and kumar dataset and corresponding augmented subsets with both real and
synthetic labeled images.
with a speciﬁc labeling proportion, say 10%, we name
diﬀusion-based data augmentation for nuclei image segmentation
599
the original subset as 10% labeled subset and the augmented on as 10% aug-
mented subset.
table 1 show the segmentation performances with hover-net.
for monuseg
dataset, it is clear that the segmentation metrics drop with fewer labeled images.
for example, with only 10% labeled images, dice and aji reduce by 2.4% and
3.1%, respectively.
however, by augmenting the 10% labeled subset, dice and
aji exceed the fully-supervised baseline by 0.9% and 1.3%.
for the 20% and
50% case, the two metrics obtained by augmented subset are of the same level
as using all labeled images.
note that the metrics of 10% augmented subset
are higher than those of 20% augmented subset, which might be attributed to
the indetermination of the diﬀusion model training and sampling.
interestingly,
augmenting the full dataset also helps: dice increases by 1.3% and aji increases
by 1.6% compared with the original full dataset.
therefore, the proposed aug-
mentation method consistently improves segmentation performance of diﬀerent
labeling proportion.
for kumar dataset, by augmenting 10% labeled subset,
aji increases to a level comparable with that using 100% labeled images; by
augmenting 20% and 50% labeled subset, ajis exceed the fully-supervised base-
line.
these results demonstrate the eﬀectiveness of the proposed augmentation
method that we can achieve the same or higher level segmentation performance
of the fully-supervised baseline by augmenting a dataset with a small amount of
labeled images.
generalization of the proposed data augmentation.
moreover, we have
similar observations when using pff-net as the segmentation model.
table 2
shows the segmentation results with pff-net.
this indicates the generalization of our proposed augmentation method.
eﬀectiveness of the proposed data augmentation method with hover-net.
training data
monuseg
kumar
dice
aji
dice
aji
10% labeled
0.7969 0.6344 0.8040 0.5939
10% augmented
0.8291 0.6785 0.8049 0.6161
20% labeled
0.8118 0.6501 0.8078 0.6098
20% augmented
0.8219 0.6657 0.8192 0.6255
50% labeled
0.8182 0.6603 0.8175 0.6201
50% augmented
0.8291 0.6764 0.8158 0.6307
100% labeled
0.8206 0.6652 0.8150 0.6183
100% augmented 0.8336 0.6810 0.8210 0.6301
600
x. yu et al.
table 2. generalization of the proposed data augmentation method with pff-net.
training data
monuseg
kumar
dice
aji
dice
aji
10% labeled
0.7489 0.5290 0.7685 0.5965
10% augmented
0.7764 0.5618 0.8051 0.6458
20% labeled
0.7691 0.5629 0.7786 0.6087
20% augmented
0.7891 0.5927 0.8019 0.6400
50% labeled
0.7663 0.5661 0.7797 0.6175
50% augmented
0.7902 0.5998 0.8104 0.6524
100% labeled
0.7809 0.5708 0.8032 0.6461
100% augmented 0.7872 0.5860 0.8125 0.6550
4

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_55.pdf:
most existing methods classify
nuclei independently or do not make full use of the semantic similarity
betweennucleiandtheirgroupingfeatures.inthispaper,weproposeanovel
end-to-end nuclei detection and classiﬁcation framework based on a group-
ingtransformer-basedclassiﬁer.thenucleiclassiﬁerlearnsandupdatesthe
representations of nuclei groups and categories via hierarchically grouping
the nucleus embeddings.
experimental results show that the proposed method signiﬁ-
cantly outperforms the existing models on three datasets.
keywords: nuclei classiﬁcation · prompt tuning · clustering ·
transformer
1
introduction
nucleus classiﬁcation is to identify the cell types from digital pathology image,
assisting pathologists in cancer diagnosis and prognosis
for example, the
this work was supported in part by the chinese key-area research and development
program of guangdong province (2020b0101350001), in part by the national natural
science foundation of china (no. 62102267, no. 61976250), in part by the guangdong
basic and applied basic research foundation (2023a1515011464, 2020b1515020048),
in part by the shenzhen science and technology program (jcyj20220818103001002,
jcyj20220530141211024), and the guangdong provincial key laboratory of big data
computing, the chinese university of hong kong, shenzhen.
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43993-3_55.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
https://doi.org/10.1007/978-3-031-43993-3_55
570
j. huang et al.
involvement of tumor-inﬁltrating lymphocytes (tils) is a critical prognostic
variable for the evaluation of breast/lung cancer
thus, we aim to automatically classify cell nuclei in pathological images.
a number of methods [7,10,14,23–25,33,34] have been proposed for auto-
matic nuclei segmentation and classiﬁcation.
diﬀerently, in
pathological images, experts often identify nuclear communities via their rela-
tionships and spatial distribution.
however, the semantics similarity and dissimilar-
ity between nucleus instances as well as the category representations have not
been fully exploited.
based on these observations, we develop a learnable grouping transformer
based classiﬁer (gtc) that leverages the similarity between nuclei and their clus-
ter representations to infer their types.
on the other hand, there exist domain gaps in the patho-
logical images of diﬀerent organs, staining, and institutions, which makes it nec-
essary to ﬁne-tune models to new applications.
inspired by the prompt tuning methods [13,16,20] which train continuous
prompts with frozen pretrained models for natural language processing tasks,
we propose a grouping prompt based learning strategy for eﬃcient tuning.
our contributions are: (1) a prompt-based grouping transformer framework for
end-to-end detection and classiﬁcation of nuclei; (2) a novel grouping prompt
learning mechanism that exploits nucleus clusters to guide feature learning with
low tuning costs; (3) experimental results show that our method achieves the
state-of-the-art on three public benchmarks.
1, we propose a novel framework, prompt-based grouping
transformer (pgt), which directly outputs the coordinates of nuclei centroids
and leverages grouping prompts for cell-type prediction.
the pixel-level feature maps output from stage 2 to stage 4 of the
backbone are extracted.
then the stage-4 feature map is downsampled with a
3 × 3 convolution of stride 2 to yield another lower-resolution feature map.
then we utilize the hard assignment strategy
for a typical swin-transformer backbone, an input pathological image i ∈
rh×w ×3 is divided into hw
e2 image patches of size e × e. we ﬁrst embed each
574
j. huang et al.
image patch into a d-dimensional latent space via a linear projection.
in the pre-tuning phase, we adopt the swin-b backbone
pre-trained on imagenet, replace the gtc head in our model (fig. 1) with 2
fc layers, and train the overall framework without prompts and gtc.
3
experiments and results
3.1
datasets and implementation details
consep1
[10] is a colorectal nuclear dataset with three types, consisting
of 41 h&e stained image tiles from 16 colorectal adenocarcinoma whole-slide
images (wsis).
[1] is a breast cancer dataset with three types and consists of
120 image tiles from 113 patients.
[9] has 291 histology images of colon tissue from six datasets, containing
nearly half a million labeled nuclei in h&e stained colon tissue.
the wsis are
at 20× magniﬁcation with an average size of 1,016 × 917 pixels.
our implementation and the setting of hyper-parameters are based on
mmdetection
random crop,
ﬂipping, and scaling are used for data augmentation.
more details are listed in the supplementary material.
3.2
comparison with the state-of-the-art
the proposed method is compared with the state-of-the-art models: the exist-
ing methods for detecting and classifying cells in pathological images, i.e., hover-
net
[7], and the sate-of-the-art methods for object
detection in natural images, i.e., ddod
the details are listed in the supplementary material.
with a frozen backbone, the performances of ‘w/o pt’ and
‘w/o gtc’ are both dropping, which veriﬁes the strength of the prompt tuning
and the gtc module, respectively.
table 3 shows the eﬀect of diﬀerent numbers of grouping prompts on
consep dataset.
fd denotes the mean of detection f-scores of all testing images.
as shown in table 4, we calculate fd of each testing
image as sample data and conduct t-test to obtain p-values on the consep
dataset.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_50.pdf:
however, attention
maps of vits are often fragmented, leading to unsatisfactory explana-
tions.
it replaces
all linear transformations with the b-cos transform to promote weight-
input alignment.
they are often still
considered black boxes, limiting their application in safety-critical domains such
e. klaiman—equal contribution.
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43993-3_50.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
however, with the
rise of transformers [31] in computational pathology, and their increasing appli-
cation to cancer classiﬁcation, segmentation, survival prediction, and mutation
detection tasks [26,32,33], the old tools need to be reconsidered.
but these often lead to fragmented and unsat-
isfactory explanations [10].
recent research on understanding vision models has mostly focused on attri-
bution methods [13,20], which aim to identify important parts of an image and
highlight them in a saliency map.
perturbation-based techniques, such
as shap [22], are another way to extract salient features from images.
the image shows an eosinophil,
which is characterized by its split, but connected nucleus, large speciﬁc granules (pink
structures in the cytoplasm), and dense chromatin (dark spots inside the nuclei)
[11] combines multi-scale images and dino [8]
pre-training to learn hierarchical visual concepts in a self-supervised fashion.
6, we show that the b-cos transformer produces superior fea-
ture maps over various post-hoc approaches – suggesting that our architecture
does indeed learn human-plausible features that are independent of the speciﬁc
visualization technique used.
3
methods
we focus on the original vision transformer [12]: the input image is divided
into non-overlapping patches, ﬂattened, and projected into a latent space of
dimension d. class tokens
this ensures that only weight vectors with higher cosine similarity to the inputs
are selected, which further increases the alignment pressure during optimization.
4. we compute the central kernel alignment (cka), which measures the repre-
sentation similarity between each hidden layer.
when trained with the binary cross-entropy loss
(bce) instead of the categorical cross-entropy loss (cce), the alignment is higher.
query, key, and value thus capture more patterns in an image – which the
attention mechanism can then attend to.
this can be shown visually by plotting
the centered kernel alignment (cka).
4
implementation and evaluation details
task-based evaluation: cancer classiﬁcation and segmentation is an
important ﬁrst step for many downstream tasks such as grading or staging.
we classify image patches from
the public colorectal cancer dataset nct-crc-he-100k
we then performed the conover post-hoc test
after friedman with adjusted p-values according to the two-stage benjamini-hochberg
procedure.
520
m. tran et al.
table 2. results of swin and bwin (ours) experiments on the test set of nct-crc-he-
100k and munich-aml-morphology.
to assess this, we propose a blinded study with four steps: (i)
randomly selecting images from the test set of tcga-coad-20x (32 samples)
and munich-aml-morphology (56 samples), (ii) plotting the last-layer attention
and transformer attributions for each image, (iii) anonymizing and randomly
shuﬄing the outputs, (iv) submitting them to two domain experts in histology
and cytology for evaluation.
most importantly, we show them all the available
saliency maps without pre-selecting them to get their unbiased opinion.
implementation details: in our experiments, we compare diﬀerent variants of
the b-cos vision transformer and the vision transformer.
speciﬁcally, we imple-
ment two versions of vit: vit-t/8 and vit-s/8.
we explore
whether this is also true for transformers in our experiments.
we believe
this is due to the simultaneous optimization of two objectives: classiﬁcation loss
and weight-input alignment.
in many visualization techniques, we see that bvt,
unlike vit, focuses exclusively on these structures (fig. 3).
a third expert points out that vit might overﬁt certain patterns in
this dataset, which could aid the model in improving its performance.
[21] is a
popular alternative to vit (e.g., it is currently the sota feature extractor for
histopathological images [33]).
in our experiments (table 2), we observe that bwin outperforms swin by
up to 2.7% and 4.8% in f1-score on nct-crc-he-100k and munich-aml-
morphology, respectively.
5: when bvt is trained from scratch, the model faces a trade-oﬀ between
522
m. tran et al.
learning the weight and input alignment and ﬁnding the appropriate inductive
bias to solve the classiﬁcation task.
by reintroducing many of the inductive biases
of cnns through the window attention in the case of swin or transfer learning
in the case of bvt, the model likely overcomes this initial problem.
moreover, we would like to emphasize that the modiﬁed models have no
negative impact on the model’s performance.
we have also shown that bvt is competitive with vit in terms
of quantitative performance.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_52.pdf:
state-of-the-art object detection and segmentation meth-
ods for microscopy images rely on supervised machine learning, which
requires laborious manual annotation of training data.
here we present
a self-supervised method based on time arrow prediction pre-training
that learns dense image representations from raw, unlabeled live-cell
microscopy videos.
our method builds upon the task of predicting the
correct order of time-ﬂipped image regions via a single-image feature
extractor followed by a time arrow prediction head that operates on the
fused features.
we furthermore demonstrate the utility of these represen-
tations on several live-cell microscopy datasets for detection and segmen-
tation of dividing cells, as well as for cell state classiﬁcation.
keywords: self-supervised learning · live-cell microscopy
1
introduction
live-cell microscopy is a fundamental tool to study the spatio-temporal dynam-
ics of biological systems
the resulting datasets can consist of terabytes
of raw videos that require automatic methods for downstream tasks such as clas-
siﬁcation, segmentation, and tracking of objects (e.g. cells or nuclei).
the
manual creation of these annotations, however, is laborious and often constitutes
a practical bottleneck in the analysis of microscopy experiments [6].
in ssl one ﬁrst deﬁnes a pretext task
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43993-3_52.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
d) the learned tap
representations z are used as input to a downstream model d.
which can be formulated solely based on unlabeled images (e.g. inpainting [8], or
rotation prediction [5]) and tasks a neural network to solve it, with the aim of
generating latent representations that capture high-level image semantics.
in a
second step, these representations can then be either ﬁnetuned or used directly
(e.g. via linear probing) for a downstream task (e.g. image classiﬁcation) with
available ground truth [7,10,18].
in this paper we investigate whether time arrow prediction, i.e. the prediction
of the correct order of temporally shuﬄed image frames extracted from live-cell
microscopy videos, can serve as a suitable pretext task to generate meaningful
representations of microscopy images.
we are motivated by the observation that
for most biological systems the temporal dynamics of local image features are
closely related to their semantic content: whereas static background regions are
time-symmetric, processes such as cell divisions or cell death are inherently time-
asymmetric (cf. fig.
importantly, we are interested in dense representations
of individual images as they are useful for both image-level (e.g. classiﬁcation)
or pixel-level (e.g. segmentation) downstream tasks.
to that end, we propose
a time arrow prediction pre-training scheme, which we call tap, that uses a
feature extractor operating on single images followed by a time arrow predic-
tion head operating on the fused representations of consecutive time points.
[19] and has since then seen numerous
self-supervised dense representation learning
539
applications for image-level tasks, such as action recognition, video retrieval,
and motion classiﬁcation
concretely our contributions are: i) we introduce the time
arrow prediction pretext task to the domain of live-cell microscopy and propose
the tap pre-training scheme, which learns dense representations (in contrast to
only image-level representations) from raw, unlabeled live-cell microscopy videos,
ii) we propose a custom (permutation-equivariant) time arrow prediction head
that enables robust training, iii) we show via attribution maps that the repre-
sentations learned by tap capture biologically relevant processes such as cell
divisions, and ﬁnally iv) we demonstrate that tap representations are beneﬁcial
for common image-level and pixel-level downstream tasks in live-cell microscopy,
especially in the low training data regime.
2
method
our proposed tap pre-training takes as input a set {i} of live-cell microscopy
image sequences
×h×w with the goal to produce a feature extractor f
that generates c-dimensional dense representations
z = f(x) ∈ rc×h×w from
single images x ∈ rh×w (cf. fig.
=
e˜zt
i ·˜zj/τ
c
j=1 e˜zt
i ·˜zj/τ
(2)
here ˜z ∈ rc×2hw denotes the stacked features z ﬂattened across the non-channel
dimensions, and τ is a temperature parameter.
throughout the experiments
we use λ = 0.01 and τ = 0.2.
note that instead of creating image pairs from
consecutive video frames we can as well choose a custom time step δt ∈ n and
sample x1 ⊂
in contrast to common models (e.g.
resnet [9]) that lack this symmetry, we here directly incorporate this induc-
tive bias via a permutation-equivariant head h that is a generalization of the set
permutation-equivariant layer proposed in [32] to dense inputs.
the last layer hl includes an additional global average pooling
along the spatial dimensions to yield the ﬁnal logits ˆy ∈ r2.
augmentations: to avoid overﬁtting on artiﬁcial image cues that could be
discriminative of the temporal order (such as a globally consistent cell drift,
or decay of image intensity due to photo-bleaching) we apply the following aug-
mentations (with probability 0.5) to each image patch pair x1, x2: ﬂips, arbitrary
rotations and elastic transformations (jointly for x1 and x2), translations for x1
and x2 (independently), spatial scaling, additive gaussian noise, and intensity
shifting and scaling (jointly+independently).
3
experiments
3.1
datasets
to demonstrate the utility of tap for a diverse set of specimen and microscopy
modalities we use the following four diﬀerent datasets:
hela.
human cervical cancer cells expressing histone 2b-gfp imaged by ﬂuo-
rescence microscopy every 30 min
3b), imaged by ﬂuorescence microscopy every 4 min [27,28].
3a), imaged by spinning disk confocal microscopy every 5 min [4,20].
3c) imaged by phase-contrast microscopy
every 3 min
2. a) tap validation accuracy for diﬀerent image augmentations on crops of back-
ground, interphase (non-dividing), and mitotic (dividing) cells (from hela dataset).
we
show results of three runs per model.
3.2
implementation details:
for the feature extractor f we use a 2d u-net [21] with depth 3 and c = 32
output features, batch normalization and leaky relu activation (approx.
the time arrow prediction head h consists of two permutation-
equivariant layers with batch normalization and leaky relu activation, followed
by global average pooling and a ﬁnal permutation-equivariant layer (approx.
total training time for a single tap model is roughly 8h
on a single gpu. tap is implemented in pytorch.
3.3
time arrow prediction pretraining
we ﬁrst study how well the time arrow prediction pretext task can be solved
depending on diﬀerent image structures and used data augmentations.
to
that end, we train tap networks with an increasing number of augmentations
on hela and compute the tap classiﬁcation accuracy for consecutive image
patches x1, x2 that contain either background, interphase (non-dividing) cells,
or mitotic (dividing) cells.
50% irrespective of the used augmentations, suggesting the
absence of predictive cues in the background for this dataset.
in contrast, on
regions with cell divisions the accuracy reaches almost 100%, conﬁrming that
542
b. gallusser et al.
tap is able to pick up on strong time-asymmetric image features.
when
using more data augmentations the accuracy decreases by roughly 12% points,
suggesting that data augmentation is key to avoid overﬁtting on confounding
cues.
strikingly, the attribution maps highlight only a few distributed, yet
highly localized image regions.
when inspecting the top six most discriminative
regions and their temporal context for a single image frame, we ﬁnd that virtually
all of them contain cell divisions (cf. fig.
moreover, when examining the
attribution maps for full videos, we ﬁnd that indeed most highlighted regions
correspond to mitotic cells, underlining the strong potential of tap to reveal
time-asymmetric biological phenomena from raw microscopy videos alone (cf.
supplementary video 1).
3. a single image frame overlayed with tap attribution maps (computed with
grad-cam [23]) for a) flywing, b) mdck, and c) yeast.
first we test the learned representations on two
image-level classiﬁcation tasks, and later on two dense segmentation tasks.
fig.
in fig. 4a we show average precision (ap)
on a held-out test set while varying the amount of available training data.
as
expected, the performance of the supervised baseline drops substantially for low
amounts of training data and surprisingly is already outperformed by a linear
classiﬁer (100 params) on top of tap representations (e.g. 0.90 vs. 0.77 for 76
labeled crops).
notably, already at
30% training data it reaches the same performance (0.97) as the baseline model
trained on the full training set.
fig.
5. a) mitosis segmentation in flywing for two consecutive timepoints with
ﬁxed/ﬁnetuned tap representations vs. a supervised u-net baseline (green).
mitosis segmentation on flywing: we now apply tap on a pixel-level down-
stream task to fully exploit that the learned tap representations are dense.
to evaluate performance, we
match a predicted/ground truth object if their intersection over union (iou) is
greater than 0.5, and report the f1 score after matching.
training a u-net on ﬁxed tap representa-
tions always outperforms the baseline, and when only using 3% of the train-
ing data it reaches similar performance as the baseline trained on all available
labels (0.67 vs. 0.68, fig. 5a).
emerging bud detection on yeast: finally, we test tap on the challenging
task of segmenting emerging buds in phase contrast images of yeast colonies.
we train tap networks on yeast and generate a dataset of 1205 crops of size
5 × 192 × 192 where we densely label yeast buds in the central frame (deﬁned
self-supervised dense representation learning
545
as buds that appeared less than 13 frames ago) based on available segmenta-
tion data [17].
we evaluate all methods on held out test videos by interpreting
the resulting 2d+time segmentations as 3d objects and computing the f1 score
using an iou threshold of 0.25.
we show
that tap uncovers sparse time-asymmetric biological processes and events in raw
unlabeled recordings without any human supervision.
although in this
work we focus on 2d+t image sequences, the principle of tap should generalize
to 3d+t datasets, for which dense ground truth creation is often prohibitively
expensive and therefore the beneﬁts of modern deep learning are not fully tapped
into.
we leave this to future work, together with the application of tap to cell
tracking algorithms, in which accurate mitosis detection is a crucial component.
acknowledgements.
we thank albert dominguez (epfl) and uwe schmidt for
helpful comments, natalie dye (pol dresden) and franz gruber for providing the
flywing dataset, benedikt mairhörmann and kurt schmoller (helmholtz munich)
for providing additional yeast training data, and alan lowe (ucl) for providing the
mdck dataset.
elife 4, e07090 (2015)
5. gidaris, s., singh, p., komodakis, n.: unsupervised representation learning by
predicting image rotations.
in: iclr, openreview.net (2018)
546
b. gallusser et al.
6. greenwald, n.f., miller, g., moen, e., kong, a., kagel, a., et al.: whole-cell
segmentation of tissue images with human-level performance using large-scale data
annotation and deep learning.
he, k., zhang, x., ren, s., sun, j.: deep residual learning for image recognition.
hsu, j., gu, j., wu, g., chiu, w., yeung, s.: capturing implicit hierarchical
structure in 3d biomedical images with self-supervised hyperbolic representations.
padovani, f., mairhörmann, b., falter-braun, p., lengefeld, j., schmoller, k.m.:
segmentation, tracking and cell cycle analysis of live-cell imaging data with cell-
acdc.
padovani, f., mairhörmann, b., lengefeld, j., falter-braun, p., schmoller, k.:
cell-acdc: segmentation, tracking, annotation and quantiﬁcation of microscopy
imaging data (dataset).
ronneberger, o., fischer, p., brox, t.: u-net: convolutional networks for biomed-
ical image segmentation.
25. stringer, c., wang, t., michaelos, m., pachitariu, m.: cellpose: a generalist algo-
rithm for cellular segmentation.
tomer, r., khairy, k., keller, p.j.: shedding light on the system: studying embry-
onic development with light sheet microscopy.
ulicna, k., vallardi, g., charras, g., lowe, a.r.: automated deep lineage tree
analysis using a bayesian single cell tracking approach.
ulman, v., maška, m., magnusson, k.e.g., ronneberger, o., haubold, c., et al.:
an objective comparison of cell-tracking algorithms.
wei, d., lim, j., zisserman, a., freeman, w.t.: learning and using the arrow of
time.
weigert, m., schmidt, u., haase, r., sugawara, k., myers, g.: star-convex poly-
hedra for 3d object detection and segmentation in microscopy.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_46.pdf:
brachial plexopathy is a form of peripheral neuropathy, which occurs
when there is damage to the brachial plexus (bp).
this paper
proposes a texture pattern based convolutional neural network, called tppnet,
to carry out abnormal prediction of bp from multiple routine magnetic reso-
nance image (mri) pulse sequences, i.e. t2, t1, and t1 post-gadolinium contrast
administration.
different from classic cnns, the input of the proposed tppnet is
multiple texture patterns instead of images.
it has several special characteristics including 1) avoidance of
image augmentation, 2) huge number of channels, 3) simple end-to-end architec-
ture, 4) free from the interference of multi-texture-pattern arrangements.
ablation
study and comparisons demonstrate that the proposed tppnet yields outstand-
ing performances with the accuracies of 96.1%, 93.5% and 93.6% over t2, t1
and post-gadolinium sequences which exceed at least 1.3%, 5.3% and 3.4% over
state-of-the-art methods for classiﬁcation of normal vs. abnormal brachial plexus.
it occurs when there is
damage to the brachial plexus (bp) which is a complex nerve network under the skin
of the shoulder.
supplementary information the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43993-3_46.
it is due to compression or direct invasion
of the nerves by tumor which will bring many serious symptoms [3].
[4]. automatic identiﬁ-
cation of the bp in mri and ultrasound images has become a hot topic.
many radiomics studies have experimentally demonstrated that image texture has
great potential for differentiation of different tissue types and pathologies
in the
past several decades, many state-of-the-art methods have been proposed to extract texture
patterns
however, how
to arrange these glcms to form the 3d volume to optimize the performance is a major
challenge.
finally, we analyze the model’s performance in the experimental section.
the major
contributions of this study include 1) directed triangle construction idea for tpp, 2) huge
number of tpp matrices as the heterogeneity representations of bp, 3) tppnet with 15
layers and huge number of channels, 4) the bp dataset containing mr images and their
corresponding roi masks.
the range of the age are varying from 15 to 85 years old.
the female patient
number and male patient number are almost even.
there-
fore, each case underwent several essential image adjustments such as multi-series
splitting,two-seriesmerging,sliceswapping,artifactcheckingandboundarycorrections.
table 1.
image 
total
normal
abnormal
t2
t1
pg
t2
t1
pg
mri
462
123
123
123
31
31
31
mask
462
123
123
123
31
31
31
to yield the roi, ﬁrstly, we randomly sampled −40% of the sequences including
both normal and abnormal ones that were manually segmented with itk- snap by two
skilled trainees [14, 15].
then, the manual segmentations were utilized to train a 3d
nnunet model which was utilized to train the model which was used to predict rois
for the rest series [16].
the predicted segmentations were manually divided into three
groups, i.e. good, fair and poor.
this process
was repeated until no improvements in the predictions for the remaining sequences was
seen.
only patients that had all three sequences segmented (t2, t1
and post-gadolinium) were included in the dataset.
in general, image textures extracted by these methods contain both
local texture properties and global texture information.
in summary, as the requirement of the image texture and deep learning, an excellent
image texture pattern should have some essential features including 1) local proper-
ties to characterize the micro-unit of the image texture, 2) global properties to represent
a texture neural network to predict the abnormal brachial plexus
473
the macro-structure of the image texture, 3) uniform shapes under nonuniform-shape
images, 4) invariant or robustness under some common geometric transforms such as
rotation, scaling and so on.
according above requirements, we developed a method to produce a serial of novel
texture patterns by introducing a directed triangle idea with an adjacent triple pixel as a
ternary group, called triple point pattern (tpp), to extract the local texture information.
then, a statistical method like histogram is employed to count the number of the same
type of pixel-triplets within the roi or throughout the whole image.
finally, a three-
dimensional (3d) tpp matrix is formed to characterize the image texture globally as the
following:
two-dimensional image:
tpp(pi,pc,pj)(x, y, z) =
m −1

m=0
n−1

j=0
⎧
⎪⎪⎨
⎪⎪⎩
1
i((m, n) + pi) = x&
i((m, n) + pc) = y&
i

(m, n) + pj

=
z
0
others
(1)
where i is a mxn image, x, y, and z is the pixel triplet, x,y,
1. directed triangle idea for tpp construction in 2d images where p0 is the concerned pixel,
p1 …p8 are its adjacent pixels.
three-dimensional image:
tpp(pi,pc,pj)(x, y, z) =
m −1

m=0
n−1

n=0
k−1

k=0
⎧
⎪⎪⎨
⎪⎪⎩
1
i((m, n, k) + pi)
= x&
i((m, n, k) + pc) = y&
i

(m, n, k) + pj

= z
0
others
(2)
where i is a three dimensional image with the shape of mxnxk, pc = (0,0,0), other
parameters are similar to the two-dimensional image.
1, the tpp is formed by the concerned pixel and its two adjacent pixels in
two-dimensional(2d) images.
similarly, the tpp in 3d images is constructed by one
474
w. cao et al.
concerned voxel and its two neighboring voxels.
more details could be found in the
supplementary material.
as the construction idea of tpp, there are four independent
modes categorized by the concerned angle, i.e. 45°, 90°, 135° and 180° in 2d images,
which produce 8 tpps, 8 tpps, 8 tpps and 8 tpps respectively.
analogously, the 3d
image has twelve independent angle modes, i.e. 35.26°, 45°, 54.74°, 60°, 70.53°, 90°,
109.47°, 120°, 125.26°, 135°, 144.74°, and 180°.
totally there are 32 tpps in 2d images and
325 tpps in 3d images and every tpp could produce one corresponding tpp matrix.
2. the pipeline of the proposed tppnet over 3d images where nml denote normal, abn
denotes abnormal, i in (2) is the block id, r is the adaptive argument to control the ﬁlter number.
in our study, these isomorphic tpp matrices are not dropped from the tpp matrix
set because they are equivalent to image rotations and re-scaling.
image scaling can
result in the image pixels increasing.
therefore, data augmentation could be
omitted when we combine tpp with deep learning for this study.
as its deﬁnition, the
tpp matrix should be a cubic array with the shape of lxlxl where l is the gray level
of the image.
based
on the construction idea of tpp, the size of the tpp matrix depends on the gray level of
the image.
for the same image or roi, the larger the gray level, the sparser the matrix
will be.
therefore,
the image requires a re-scaling step to lower its gray level to avoid the sparsity of
the tpp matrix.
it has four particular features as follows:
1) avoidance of image augmentation.
due to the stability of tpp matrix under rota-
tion, scale and afﬁne transformations, image augmentation could be omitted in the
preprocessing step which can lead to image deformation.
for
2d images, there are at least 32 channels if more displacements of tpp is considered.
similarly, we could generate no less than 325 tpps in 3d images.
3) simple end-to-end architecture.
4) free from the interference of multi-texture-pattern arrangements.
since each channel
is corresponding with one tpp, it can solve the pattern arrangement issue occurred
in glcm-cnn.
table 2. test performances for different gray levels over t2, t1 and post-gadolinium where acc
denotes accuracy and pg denotes post-gadolinium.
gray 
level 
t2
t1
pg 
acc 
loss
acc 
loss
acc 
loss
8 
0.942±0.032
0.319±0.028
0.902±0.022
0.395±0.207
0.929±0.023
0.292±0.087
12
0.961±0.025
0.277±0.107
0.935±0.021
0.307±0.164
0.936±0.027
0.279±0.046
16
0.948±0.034
0.350±0.333
0.922±0.028
0.306±0.107
0.921±0.041
0.313±0.053
20
0.948±0.026
0.342±0.126
0.922±0.035
0.363±0.098
0.942±0.034
0.302±0.124
24
0.948±0.034
0.348±0.162
0.929±0.023
0.309±0.067
0.922±0.046
0.322±0.176
3
experiments
3.1
preparations
some important speciﬁcities of our computing platform contain: one amd epyc 7352
24-core processor, 1 tb memory and four nivida a100-sxm gpus with 320 gb
gpu memory.
3.2
ablation studies
since all images in our dataset are 3d images, therefore, the initial channel is set 325
which is equal to the tpp number.
the loss functions in the following experiments
shared categorical_crossentropy.
all performances listed in this section
are the average of performances with 5-fold cross-validation.
3.2.1
impact of gray level
the image gray level determines the shape of the tpp matrix.
while
rescaling the image intensity, an arc tangent approach is utilized to yield the new image.
their performances evaluated by
accuracies are listed in table2 which tells us that t2 sequence yields the highest accuracy
of 96.1% when the gray level is 12.
other performances could be read
in supplementary materials.
table 3. test performances for three intensity rescaling approaches over t2, t1 and post-
gadolinium where the gray level is set 12, acc denotes accuracy, pg denotes post -gadolinium,
and artan is arc tangent function.
0.936±0.027 0.279±0.046
rescaling
approach
t2
t1
pg
table 4. test performances comparison between multi-channel and solo-channel over t2, t1 and
post-gadolinium where the intensity rescaling function is arc tangent, the gray level is set 20, acc
denotes accuracy and pg denotes post -gadolinium.
acc
loss
acc
loss
acc
loss
325
0.948±0.026 0.342±0.126 0.922±0.035 0.363±0.098 0.934±0.034 0.302±0.124
1
0.863±0.027 0.274±0.091 0.810±0.145 0.458±0.105 0.811±0.047 0.590±0.107
input
channel
t2
t1
pg
a texture neural network to predict the abnormal brachial plexus
477
3.2.2
impact of intensity rescaling approaches
rescaling approaches of image intensity could also bring impacts on the bp’s differ-
entiation while producing the tpp matrix.
to test the performances fairly, we test above rescaling methods at the same gray
level 12.
the yielded performances evaluated by accuracies are shown in table 3 where
arc tangent method achieves the highest accuracy of 96.1% over the t2 sequence.
other
performances are shown in supplementary materials.
3.2.3
multi-channels vs solo-channel
we carry out experiments to train the tppnet model and make tests with arc tangent
rescaling approach under gray level 16.
performances with
accuracies and loss are listed in table 4.
other performances are listed in supplementary
materials.
all approaches shared the same image shape of 128 × 128 × 64 with 1
channel.
their performances are
478
w. cao et al.
table 5. performance comparison between tppnet t1 sequence where acc is accuracy, auc
denotes spe denotes speciﬁcity.
3. t2 and post-
gadolinium’s performances could be found in supplementary materials.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_28.pdf:
this paper addresses the need for improved ct-guidance
during needle-based liver procedures (i.e., tumor ablation), while reduces
the need for contrast agent injection during such interventions.
to
achieve this objective, we augment the intraoperative ct with the pre-
operative vascular network deformed to match the current acquisition.
first, a neural network learns local image features in a non-contrasted
ct image by leveraging the known preoperative vessel tree geometry
and topology extracted from a matching contrasted ct image.
then,
the augmented ct is generated by fusing the labeled vascular tree and
the non-contrasted intraoperative ct.
our method is trained and val-
idated on porcine data, achieving an average dice score of 0.81 on the
predicted vessel tree instead of 0.51 when a medical expert segments the
non-contrasted ct.
source code of this work is publicly avail-
able at https://github.com/sidaty1/intraoperative ct augmentation.
keywords: liver tumor ablation · needle-based procedures ·
patient-speciﬁc interventions · ct-guidance · medical image
augmentation
1
introduction
needle-based liver tumor ablation techniques (e.g., radiofrequency, microwave,
laser, cryoablation) have a great potential for local curative tumor control
[1],
with comparable results to surgery in the early stages for both primary and
secondary cancers.
ct-guidance is a widely used imaging modality for placing
the needles, monitoring the treatment, and following up patients.
https://doi.org/10.1007/978-3-031-43996-4_28
292
s. el hadramy et al.
injection of contrast agents to visualize the intrahepatic vessels and the target
tumor(s).
in standard clinical settings, the insertion of each needle requires multiple
check points during its progression, ﬁne-tune maneuvers, and eventual reposi-
tioning.
however, intrahepatic vessels (and some tumors) are only visible after
contrast-enhancement, which has a short lifespan and dose-related deleterious
kidney eﬀects.
a workaround to shortcut these limitations is to
perform an image fusion between previous contrasted and intraoperative non-
contrasted images.
in this work, we propose a method for visualizing intrahepatic structures
after organ motion and needle-induced deformations, in non-injected images, by
exploiting image features that are generally not perceivable by the human eye
in common clinical workﬂows.
to address this challenge, two main strategies could be considered: image
fusion and image processing techniques.
image fusion typically relies on the
estimation of rigid or non-rigid transformations between 2 images, to bring into
the intraoperative image structures of interest only visible in the preoperative
data.
recent deep learning approaches
[11,12,14] have proved to be a successful alternative to solve image fusion prob-
lems, even when a large non-linear mapping is required.
when ground-truth
displacement ﬁelds are not known, state-of-the-art methods use unsupervised
techniques, usually an encoder-decoder architecture [7,13], to learn the unknown
displacement ﬁeld between the 2 images.
however, such unsupervised methods
fail at solving our problem due to lack of similar image features between the
contrasted (cct) and non-contrasted (ncct) image in the vascular tree region
(see sect. 3.3).
on the other hand, deep learning techniques have proven to be very eﬃcient
at solving image processing challenges [15].
for instance, image segmentation
[16], image style transfer
[17], or contrast-enhancement to cite a few.
yet, seg-
menting vessels from non-contrasted images remains a challenge for the medical
imaging community
[16]. style transfer aims to transfer the style of one image
to another while preserving its content
contrast-enhancement methods could be an
alternative.
[20], a deep neural network
synthesizes contrast-enhanced ct from non contrast-enhanced ct. neverthe-
less, results obtained by this method are not suﬃciently robust and accurate to
provide an augmented intraoperative ct on which needle-based procedures can
be guided.
intraoperative ct augmentation for needle-based liver interventions
293
in this paper we propose an alternative approach, where a neural network
learns local image features in a ncct image by leveraging the known preopera-
tive vessel tree geometry and topology extracted from a matching (undeformed)
cct.
then, the augmented ct is generated by fusing the deformed vascular
tree with the non-contrasted intraoperative ct. section 2 presents the method
and its integration in the medical workﬂow.
a few days or a week before the intervention, a preoperative
diagnostic multiphase contrast-enhanced image (mpcect) is acquired (fig. 1,
yellow box).
the day of the intervention, a second mpcect image is acquired
before starting the needle insertion, followed by a series of standard, non-injected
acquisitions to guide the needle insertion (fig. 1, blue box).
using such a non-
contrasted intraoperative image as input, our method performs a combined
non-rigid registration and augmentation of the intraoperative ct by
adding anatomical features (mainly intrahepatic vessels and tumors) from the
preoperative image to the current image.
to achieve this result, our method only
requires to process and train on the baseline mpcect image (fig. 1, red box).
the neural network trained
on preoperative mpcect avoids contrast agent injections during the intervention.
fig.
finally, the augmented ct is created by fusing the
segmented image and labels with the intraoperative ncct.
since vascular structures are not visible in non-contrasted images,
the extraction of this map is done by segmenting the cct and then using this
segmentation as a mask in the ncct.
mathematical morphology operators, in
particular a dilation operation [23], are performed on the segmented region of
interest to slightly increase its dimensions.
this is needed to compensate for
segmentation errors and the slight anatomical motion that may exist between
the contrasted and non-contrasted image acquisitions.
in practice, the acquisition
protocols limit the shift between the ncct and cct acquisitions, and only a
few sequential dilation operations are needed to ensure we capture the true vessel
ﬁngerprint in the ncct image.
note that the resulting vessel map is not a binary
mask, but a subset of the image limited to the volume covered by the vessels.
2.2
data augmentation
the preoperative mpcect provides a couple of registered ncct and cct
images.
therefore, we augment the data set by applying multiple random deformations
to the original images.
random deformations are created by considering a pre-
deﬁned set of control points for which we deﬁne a displacement ﬁeld with a
random normal distribution.
the displacement ﬁeld of the full volume is then
obtained by linearly interpolating the control points’ displacement ﬁeld to the
rest of the volume.
our network learns to ﬁnd the image features (or vessel ﬁngerprint)
present in the vessel map, in a given ncct assuming the knowledge of its geom-
etry, topology, and the distribution of contrast from the preoperative mpcect.
3. it consists of a four lay-
ers analysis (left side) and synthesis (right side) paths that provide a non-linear
mapping between low resolution input and output images.
in the last layer, a 1 × 1 × 1 convolution reduces the
number of output channels to one, yielding the vessel map in the intraoperative
intraoperative ct augmentation for needle-based liver interventions
295
image.
3. our neural network uses a four-path encoder-decoder architecture and takes
as input a two-channel image corresponding to the intraoperative ncct image con-
catenated with the preoperative vessel map.
the output is the intraoperative vessel
map.
2.4
augmented ct
once the network has been trained on the patient-speciﬁc preoperative data, the
next step is to augment and visualize the intraoperative ncct.
2.1 are not reversible (i.e. the
segmented vessel tree cannot be recovered from the vm by applying the same
number of erosion operations).
also, neighboring branches in the vessel tree
could end up being fused, thus changing the topology of the vessel map.
therefore, to retrieve the correct segmented (yet deformed) vascular tree, we
compute a displacement ﬁeld between the pre- and intraoperative vms.
the resulting displacement ﬁeld is
applied on the preoperative segmentation to retrieve the intraoperative vessel
tree segmentation.
4.
– the augmented image is obtained by fusing the predicted intraoperative seg-
mentation with the intraoperative ncct image.
the augmented vessels are
displayed in green to ensure the clinician is aware this is not a true cct
image (see fig. 5).
– it is also possible to add anatomical labels to the intraoperative augmented
ct to further assist the clinician.
to achieve this objective, we compute a
graph data structure from the preoperative segmentation.
we use the graph structure to associate each anatomical label
(manually deﬁned) with a strahler [6] graph ordering.
the same process is
applied to the predicted intraoperative segmentation.
this makes it possible
to correctly map the preoperative anatomical labels (e.g. vessel name) and
display them on the augmented image.
fig.
4. this ﬁgure illustrates the diﬀerent stages of the pipeline adopted to generate the
vm and show how the vessel tree topology is retrieved from the predicted intraoperative
vm by computing a displacement ﬁeld between the preoperative vm and the predicted
vm.
this ﬁeld is applied to the preoperative segmentation to get the intraoperative
one.
3
results and discussion
3.1
dataset and implementation details
to validate our approach, 4 couples of mpcect abdominal porcine images
were acquired from 4 diﬀerent subjects.
we recall that an
mpcect contains a set of registered ncct and cct images.
these images are
then cropped and down-sampled to 256 × 256 × 256, and the voxels intensities
are scaled between 0 and 255.
finally, we extract the vm from each mpcect
sample and apply 3 dilation operations, which demonstrated the best perfor-
mance in terms of prediction accuracy and robustness on our data.
[25] and others do not ﬁt
our problem since they do not include the ncct images.
for a given subject, we
generate 100 displacement ﬁelds using the data augmentation strategy explained
above with 50 voxels for the control points spacing in the three spatial directions
intraoperative ct augmentation for needle-based liver interventions
297
and a standard deviation of 5 voxels for the normal distributions.
our
method is implemented in tensorﬂow 2.4, on a geforce rtx 3090.
the
training process converges in about 1,000 epochs with a batch size of 1 and 200
steps per epoch.
3.2
results
to assess our method, we use a dice score to measure the overlap between our
predicted segmentation and the ground truth.
being a commonly used metric
for segmentation problems, dice aligns the nature of our problem as well as the
clinical impact of our solution.
an example of a subject intraoperative augmented ct is illustrated in
fig.
5, where the three images correspond respectively to the initial non injected
ct, the augmented ct without and with labels.
the green vessels correspond to the ground truth
intraoperative segmentation, the orange ones to the predicted intraoperative
segmentation and ﬁnally the gray vessel tree corresponds to the preoperative
cct vessel tree.
the middle image shows
the augmented ct with the predicted vessel tree (in green).
the rightmost image shows
the augmented image with anatomical labels transferred from the preoperative image
segmentation and labelling.
6. assessment of our method for subject 1.
(color ﬁgure online)
qualitative assessment: to further demonstrate the value of our method,
we have asked two clinicians to manually segment the ncct images in the
intraoperative mpcect data.
our method outperforms the results of both clinicians, with
an average dice score of 0.81 against 0.51 as a mean for the clinical experts.
using the data of the subject 1, a u-net was trained
to segment the vessel tree of the intraoperative ncct image.
the network only
managed to segment a small portion of the main portal vein branch.
we also studied the inﬂuence of the diﬀusion kernel applied to
the initial segmentation.
we have seen, on our experimental data, that 3 dilation
operations were suﬃcient to compensate for the possible motion between ncct
and cct acquisitions.
comparison with voxelmorph: the problem that we address can be seen
from diﬀerent angles.
in particular, we could attempt to solve it by register-
ing the preoperative ncct to the intraoperative one and then applying the
intraoperative ct augmentation for needle-based liver interventions
299
resulting displacement ﬁeld to the known preoperative segmentation.
however,
state-of-the-art registration methods such as voxelmorph [7] and others do not
necessarily guarantee a diﬀeomorphic [8] displacement ﬁeld that ensures the
continuity of the displacement ﬁeld inside the parenchyma where the intensity
is quite homogeneous on the ncct.
while the voxelmorph
network accurately registers the liver shape, the displacement ﬁeld is almost null
in the region of vessels inside the parenchyma.
therefore, the preoperative vessel
segmentation is not correctly transferred into the intraoperative image.
fig.
7. illustration of voxelmorph registration between ncct preoperative and intra-
operative images.
df stands for
the displacement ﬁelds on x and y predicted by voxelmorph method.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_10.pdf:
first, we build a video
transformer, which captures both local and global long-range dependen-
cies across spatial and temporal dimensions.
second, we pre-train our
transformer model using global and local views via a self-supervised
manner, aiming to make it robust to spatial-temporal variations and
discriminative across diﬀerent scenes.
with experiments on 3 diﬀerent types of down-
stream tasks, including classiﬁcation, segmentation, and detection, our
endo-fm surpasses the current state-of-the-art (sota) self-supervised
pre-training and adapter-based transfer learning methods by a signiﬁcant
margin, such as vcl (3.1% f1, 4.8% dice, and 5.5% f1 for classiﬁcation,
segmentation, and detection) and st-adapter (5.9% f1, 9.6% dice, and
9.9% f1 for classiﬁcation, segmentation, and detection).
keywords: foundation model · endoscopy video · pre-train
1
introduction
foundation models pre-trained on large-scale data have recently showed suc-
cess in various downstream tasks on medical images including classiﬁcation [9],
z. wang and c. liu—equal contributions.
[33], and segmentation
it is arguable that a
speciﬁc foundation model trained on some certain type of data is useful at the
moment.
[20,21], involves pre-training on large-scale
image-text pairs and relies on large language models to learn cross-modality
features.
however, since clinical routines for endoscopy videos typically do not
involve text data, a pure image-based foundation model is currently more fea-
sible.
this would indicate that our video transformer
could have suﬃcient capacity to model the rich spatial-temporal information of
endoscopy videos.
to learn rich spatial-temporal information from endoscopy video data [12],
our endo-fm is pre-trained via a self-supervised manner by narrowing the gap
between feature representations from diﬀerent spatial-temporal views of the same
video.
experimental results on 3 diﬀerent types of
downstream tasks demonstrate the eﬀectiveness of endo-fm, surpassing the
current state-of-the-art self-supervised pre-training and adapter-based transfer
learning methods by a signiﬁcant margin, such as vcl (3.1% f1, 4.8% dice, and
5.5% f1 for classiﬁcation, segmentation, and detection) and st-adapter (5.9%
f1, 9.6% dice, and 9.9% f1 for classiﬁcation, segmentation, and detection).
the spatial and
temporal attention mechanisms in our model capture long-range dependencies
across both spatial and temporal dimensions, with a larger receptive ﬁeld than
conventional convolutional kernels
our model also includes a learnable class
token, representing the global features learned by the model along the spatial
and temporal dimensions.
speciﬁ-
cally, we ﬁx the spatial and temporal positional encoding vectors to the highest
resolution of the input view for each dimension, making it easy to interpolate
for views with smaller spatial size or lower temporal frame rate.
diﬀerent from image-based pre-training [33], our
video-oriented pre-training is designed to capture the relationships between dif-
ferent spatial-temporal variations.
moreover, by predicting the nuanced diﬀerences of tissue and lesions in
a view with a high frame rate from another with a low frame rate, the model is
encouraged to learn more comprehensive motion-related contextual information.
to prevent
the problem of the teacher and student models constantly outputting the same
value during pre-training, we update the student model θ through backpropaga-
tion, while the teacher model φ is updated through exponential moving average
(ema) using the student’s weights.
this is achieved by updating the teacher’s
weights as φt ← αφt−1 + (1 − α)θt at each training iteration t. here, α is a
momentum hyper-parameter that determines the updating rate.
these videos are captured using diﬀerent sur-
gical systems and in a wide range of environmental conditions [10].
to address
this variability, we apply temporally consistent spatial augmentations
our augmentation approach includes random hori-
zontal ﬂips, color jitter, gaussian blur, solarization, and so on, which enhances
the robustness and generalizability of endo-fm.
for endo-fm, we set the patch size p as 16 and embedding dimension d as
768.
the mlp head projects the dimension
of class token to 65536.
the ema update momentum α is 0.996.
the
pre-training is ﬁnished with 30 epochs with a cosine schedule [16].
3
experiment
3.1
datasets and downstream setup
we collect all possible public endoscope video datasets and a new one from
baoshan branch of renji hospital for pre-training.
method
venue
pre-training
polypdiag
cvc-12k
kumc
time (h)
(classiﬁcation) (segmentation) (detection)
scratch (rand. init.)
n/a
83.5±1.3
53.2±3.2
73.5±4.3
timesformer
[24]
neurips’22
8.1
84.8±0.7
64.3±1.9
74.9±2.9
endo-fm (ours)
20.4
90.7±0.4
73.9±1.2
84.1±1.3
of 5 s on average.
we evaluate our pre-trained endo-fm on three downstream
tasks: disease diagnosis (polypdiag [32]), polyp segmentation (cvc-12k [2]), and
detection (kumc [15]).
2) cvc-12k: a transunet equipped with endo-fm as the
backbone is implemented.
3) kumc: we implement a stft
the same
experimental setup is applied to all the experiments for fair comparisons.
we can observe that
the scratch model shows low performance on all 3 downstream tasks, especially
for segmentation.
compared with training from scratch, our endo-fm achieves
+7.2% f1, +20.7% dice, and +10.6% f1 improvements for classiﬁcation, seg-
108
z. wang et al.
mentation, and detection tasks, respectively, indicating the high eﬀectiveness of
our proposed pre-training approach.
such signiﬁcant improvements are
beneﬁted from our speciﬁc spatial-temporal pre-training designed for endoscopy
videos to tackle the complex context information and dynamic scenes.
we can
learn that both spatial and temporal sampling for local views can help improve
the performance and their combination produces a plus, yielding +4.3% f1
improvement.
furthermore, our proposed dynamic matching scheme boosts the
performance to 89.7%, demonstrating the importance of capturing the motion
related context information from dynamic scenes.
additionally, the performance
is further improved with video augmentations from 89.7% to 90.7%.
it indicates that joint prediction scenarios, where we
predict vg from both vl (cross-view matching) and vg (dynamic motion match-
ing), result in optimal performance.
we ﬁnd that incorporating more views
and increasing the length variations of local views yields better performance.
these improvements stem from the spatial-temporal change invariant and
cross-video discriminative features learned from the diverse endoscopy videos.
exten-
sive experimental results on 3 downstream tasks demonstrate the eﬀectiveness
of endo-fm, signiﬁcantly outperforming other state-of-the-art video-based pre-
training methods, and showcasing its potential for clinical application.
[14] model, which is developed for segmentation
task, we try to apply sam for our downstream task cvc-12k with the same
ﬁne-tuning scheme as endo-fm.
the experimental results show that sam can
achieve comparable performance with our endo-fm for the downstream seg-
mentation task.
moreover, besides
segmentation, endo-fm can also be easily applied to other types of tasks includ-
ing classiﬁcation and detection.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_58.pdf:
we propose a bevel tip oce needle probe for percutaneous inser-
tions, where biomechanical characterization of deep tissue could enable
precise needle placement, e.g., in prostate biopsy.
using a novel setup, we simulate deep tissue indentations where
frictional forces and bulk sample displacement can aﬀect biomechanical
characterization.
performing surface and deep tissue indentation experi-
ments, we compare our approach with external force and needle position
measurements at the needle shaft.
compared to surface
indentations, external force-position measurements are strongly aﬀected
by frictional forces and bulk displacement and show a relative error of
49.2% and 42.4% for soft and stiﬀ phantoms, respectively.
in contrast,
quantitative oce measurements show a reduced relative error of 26.4%
and 4.9% for deep indentations of soft and stiﬀ phantoms, respectively.
finally, we demonstrate that the oce measurements can be used to
eﬀectively discriminate the tissue mimicking phantoms.
[4]. diﬀerent imaging modalities
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43996-4 58.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
handheld oce systems for intraoperative assessment [2,23] have
also been proposed.
taking prostate cancer as an example, biomechanical characterization could
guide needle placement for improved cancer detection rates while reducing com-
plications associated with increased core counts, e.g. pain and erectile dysfunc-
tion [14,18].
however, the measurement of both the applied load and the local
sample compression is challenging.
furthermore, the prostate
is known to display large bulk displacement caused by patient movement and
needle insertions
tip force sensing for estimating elastic properties has been proposed [5] but bulk
tissue displacement of deep tissue was not considered.
we design an experimental setup
that can simulate friction forces and bulk displacement occurring during needle
biopsy (fig. 1).
we consider tissue-mimicking phantoms for surface and deep tis-
sue indentation experiments and compare our results with force-position curves
externally measured at the needle shaft.
we then present an experimental setup for simulating
friction and bulk displacement and describe the conducted surface and deep tis-
sue indentation experiments.
the forward viewing ﬁber (fiber 1) images sample
compression while the load sensing ﬁber (fiber 2) visualizes the displacement of
a reference epoxy layer that is deformed under load.
friction
forces (red) and tip forces (grey) are superimposed and the forward motion of the needle
(black) only partially results in sample compression (green) due to bulk displacement
(blue).
middle: experimental setup used for indentation experiments, with a linear
actuator (a), an axial force sensor (b), the oce needle probe (c) and the sample
layers (d).
friction can be added by puncturing multiple layers and bulk displacement is simulated
by placing the sample on springs.
(color ﬁgure online)
610
r. mieling et al.
2.2
oce measurement
in unconﬁned compression, the elasticity of the sample can be determined by
the relation between stress σ and bulk strain ϵ denoted by the young’s modulus
oct
mux
a-scan fiber 2
a-scan fiber 1
epoxy
2mm
trigger
fig.
we use optical ﬁber 1 to measure sample compression and ﬁber 2 for
the displacement of a reference epoxy layer (green) that is deformed under tip forces.
to obtain a single parameter for comparing
two measurements, we assume a linear relation
eoce(ft , ϵl)
the phase shift between two a-scans is proportional to the depth
dependent displacement δui(z, t)
δφi(z, t) = 4 π n δui(z, t)
λ0
,
(3)
assuming a refractive index n of 1.45 and 1.5 for tissue (fiber 1) and epoxy
(fiber 2), respectively.
for ﬁber 1, we employ a
moving average with a window size of 0.1 mm.
we estimate local strain based on
the ﬁnite diﬀerence along the spatial dimension over an axial depth δz of 1 mm.
ϵl(t)
(5)
2.3
experimental setup
we build an experimental setup for surface and deep tissue indentations with
simulated force and bulk displacement (fig. 1).
for deep tissue indentations, dif-
ferent tissue phantoms are stacked on a sample holder with springs in between.
for surface measurements, we position the tissue phantoms separately without
additional springs or tissue around the needle shaft.
we use a motorized linear
stage (zfs25b, thorlabs gmbh, ger) to drive the needle while simultaneously
logging motor positions.
reference elasticity is determined by unconﬁned compression exper-
iments of three cylindrical samples for each material according to eq. 1, using
force and position sensor data (see supplementary material).
the young’s modu-
lus is obtained by linear regression for the combined measurements of each mate-
rial.
acquisition window during pre-deformation phase (a) considered
for oce measurements is indicated by dashed white line.
local strain is calculated
based on the tracked deformation from the oct phase diﬀerence as visualized in red.
612
r. mieling et al.
then determine a linear ﬁt according to eq. 5 and obtain af = 174.4 mn mm−1
from external force sensor and motor position measurements (see supplementary
material).
2.4
indentation experiments
in total, we conduct ten oce indentation measurements for each material.
three
surface measurements with ﬁxed samples and seven deep tissue indentations with
simulated friction and bulk displacement.
as the beginning of the needle movement
might not directly correspond to the beginning of sample indentation, we eval-
uate oce measurements only if the estimated tip force is larger than 50 mn.
to further ensure that measurements occur within the pre-rupture deformation
phase [6,15], only samples below 20 % local strain are considered.
3. we evaluate external
needle shaft measurements of relative axial force and relative motor position
with the same endpoint obtained from local strain estimates.
as we can con-
sider surface measurements as equivalents to the known elasticity, we regard
the relative error (re) of the mean value obtained for deep indentations, with
respect to the average estimate during surface indentations.
4. (a) oce needle measurements for surface and deep tissue indentations based on
the estimated tip force (ﬁber 2) and the detected local strain (ﬁber 1).
friction increases the measured axial force,
while bulk displacement decreases the observed slope.
bulk displacement can occur
suddenly due to stick-slip, as seen in two cases of material b. (b) resulting elasticity
estimates show overlap between the two materials, hampering quantitative biomechan-
ical characterization.
for both oce and external measurements and material a and b, respectively.
3
results
the oce measurements for surface and deep tissue indentations are displayed
in fig.
it can be seen that oce measurements result in separation of both
materials while an overlap is visible for external sensors.
biomechanical characteriza-
tion based on the oce estimates allows complete separation between materials,
with auroc and auprc scores of 1.00 (see supplementary material).
exter-
nal measurements do not enable robust discrimination of materials and yielded
auroc and auprc scores of only 0.85 and 0.861, respectively.
b surf
14.92 ± 3.28 12.10 18.53 361.02 ± 6.94
353.37 366.92 3
deep 15.39 ± 4.48 10.02 21.75 201.31 ± 89.04 136.56 354.60 7
re
4.9 %
42.4 %
ducted indentation experiments demonstrate the feasibility of oce elasticity
estimates for deep tissue needle insertions.
oce estimates show better agree-
ment between surface and deep tissue indentations compared to external mea-
surements, as displayed by reduced relative errors of 26.4% and 4.9% for both
phantoms, respectively.
bulk displacement causes considerate underestimation
of elasticity estimates when only needle position and axial forces are considered,
shown by relative errors of 49.2% and 42.4% for material a and b, respec-
tively.
4b and the auroc and auprc
scores of 1. note that the high errors for external measurements at the needle
shaft are systematic, as friction and bulk displacement are unknown.
moreover, considering the
standard deviation for oce estimates, improved calibration of our dual-ﬁber
needle probe is expected to further improve performance.
weighted strain estimation
based on oct signal intensity [26] could address the underestimation of local
strain during segments of low signal-to-noise-ratio (see supplementary material).
compression oce theoretically
enables the analysis of non-linear elastic behavior [26] and future experiments
will consider non-linear models and unloading cycles better beﬁtting needle-
tissue-interaction
while the cylindrical tip is advantageous for calculating the
optical coherence elastography needle for biomechanical characterization
615
young’s modulus, it has been shown that the calculation of an equivalent young’s
modulus is rarely comparable across diﬀerent techniques and samples
further experiments need to include biological soft tissue to validate
the approach for clinical application, as our evaluation is currently limited to
homogeneous gelatin.
this needle probe could also be very useful when con-
sidering robotic needle insertions, e.g., to implement feedback control based on
elasticity estimates.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_64.pdf:
homologous anatomical landmarks between medical scans
are instrumental in quantitative assessment of image registration qual-
ity in various clinical applications, such as mri-ultrasound registra-
tion for tissue shift correction in ultrasound-guided brain tumor resec-
tion.
while manually identiﬁed landmark pairs between mri and ultra-
sound (us) have greatly facilitated the validation of diﬀerent registra-
tion algorithms for the task, the procedure requires signiﬁcant expertise,
labor, and time, and can be prone to inter- and intra-rater inconsis-
tency.
so far, many traditional and machine learning approaches have
been presented for anatomical landmark detection, but they primarily
focus on mono-modal applications.
speciﬁcally, two convolutional neural networks
were trained jointly to encode image features in mri and us scans to
help match the us image patch that contain the corresponding land-
marks in the mri.
early surgical treat-
ment to remove the maximum amount of cancerous tissues while preserving the
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
the phenomenon is
referred to as brain shift, and often invalidates the pre-surgical plan by displacing
surgical targets and other vital anatomies.
as the true underlying deformation from brain shift is impossible to obtain
and the diﬀerences of image features between mri and us are large, quantitative
validation of automatic mri-us registration algorithms often rely on homolo-
gous anatomical landmarks that are manually labeled between corresponding
mri and intra-operative us scans
however, manual landmark identiﬁcation
requires strong expertise in anatomy and is costly in labor and time.
these factors make quality assess-
ment of brain shift correction for us-guided brain tumor resection challenging.
previously, many groups have proposed algorithms to label landmarks in
anatomical scans [4–9].
in addition, unlike
other applications, where the full anatomy is visible in the scan and all land-
marks have consistent spatial arrangements across subjects, intra-operative us
of brain tumor resection only contains local regions of the pathology with non-
canonical orientations.
speciﬁcally, the technique
leverages two convolutional neural networks (cnns) to learn features between
mri and us that distinguish the inter-modal image patches which are cen-
tered at the matching landmarks from those that are not.
670
s. salari et al.
2
related work
contrastive learning has recently shown great results in a wide range of medical
image analysis tasks [12–18].
this
self-supervised learning set-up allows robust feature learning and embedding
without explicit guidance from ﬁne-grained image annotations, and the encoded
features can be adopted in various downstream tasks, such as segmentation.
a
few recent works [19–21] explored the potential of cl in anatomical landmark
annotation in head x-ray images for 2d skull landmarks.
[19,20]
attempted to leverage cl for more eﬃcient and robust learning.
[21]
used multiscale pixel-wise contrastive proxy tasks for skull landmark detection
in x-ray images.
these prior works with cl focus on single-modal 2d landmark
identiﬁcation with systematic landmark localization protocols and sharp image
contrast (i.e., skull in x-ray).
in cl,
many works have employed the infonce loss function [22,23] in attaining good
outcomes.
[10] (https://archive.sigma2.no/pages/
public/dataset detail.jsf?id=10.11582/2020.00025) to train and evaluate our
proposed method.
all images were resampled to a uniﬁed dimension of 256 × 256 ×
288 voxels, with an isotropic resolution of ∼0.5mm. between mri and the cor-
responding us images, matching anatomical landmarks were manually labeled
by experts and 15∼16 landmarks were available per case.
3.2
contrastive learning framework
we used two cnns with identical architectures in parallel to extract robust
image features from mri and us scans.
3.3
landmark matching with a 2.5d approach
working with 3d images is computationally expensive and can make the model
training unstable and prone to overﬁtting, especially when the size of the
database is limited.
therefore, instead of a full 3d processing, we decided to
implement a 2.5d approach [25] to leverage the eﬃciency of 2d cnn in the
672
s. salari et al.
cl framework for the task.
in this case, we extracted a series of three adjacent
2d image patches in one canonical direction (x-, y-, or z-direction), with the
middle slice centred at the true or candidate landmarks in a 3d scan to provide
slight spatial context for the middle slice of interest.
to construct the full 2.5d
formulation, we performed the same image patch series extraction in all x-, y-,
and z-directions for a landmark, and this 2.5d patch forms the basis to compute
the similarity between the queried us and reference mri patches.
note that during network
training, instead of 2.5d patches, we compared the 2d image patch series in
one canonical direction between mri and us, and 2d patch series in all three
directions were used.
during the inference stage, the similarity between mri
and us 2.5d patches was obtained by summing the similarities of correspond-
ing 2d image patch series in each direction, and a match was determined with
the highest similarity from all queried us patches.
the general overview of the utilized framework for 2d image patch
extraction is shown in fig.
[11] is a well-known tool for keypoint detection and image
registration.
it has been widely used in multi-modal medical registration, such
as landmark matching for brain shift correction in image-guided neurosurgery
[8,26].
towards multi-modal anatomical landmark detection
673
4
experimental setup
4.1
data preprocessing
for cl training, both positive and negative sample pairs need to be created.
these sample pairs were used to train two cnns to extract relevant
image features across mri and us leveraging the infonce loss.
the loss function has been
widely used and demonstrated great performance in many vision tasks.
i are the cropped image patches around the corresponding landmarks in
mr and us scans, respectively, and xn
i
is a mismatched patch in the us image
to that cropped around the mri reference landmark.
4.3
implementation details and evaluation
to train our dl model, we made subject-wise division of the entire dataset into
70%:15%:15% as the training, validation, and testing sets, respectively.
also,
to improve the robustness of the network, we used data augmentation for the
training data by random rotation, random horizontal ﬂip, and random vertical
ﬂip.
in order to evaluate the performance of our technique, we used the provided
ground truth landmarks from the database and calculated the euclidean distance
between the ground truths and predictions.
3: an overview of the framework for image feature learning.
when inspecting landmark identiﬁcation errors across all subjects between the
cl and sift techniques, we also noticed that our cl framework has signiﬁ-
cantly lower standard deviations (p <1e-4), implying that our technique has a
better performance consistency.
6
discussion
inter-modal anatomical landmark localization is still a diﬃcult task, espe-
cially for the described application, where landmarks have no consistent spa-
tial arrangement across diﬀerent cases and image features in us are rough.
first, while the 2.5d approach is memory eﬃcient and quick,
3d approaches may better capture the full corresponding image features.
finally, we only
employed us scans before resection since tissue removal can further complicate
feature matching between mri and us, and requires more elaborate strategies,
such as those involving segmentation of resected regions [27].
as a baseline comparison, we employed the
sift algorithm, which has demonstrated excellent performance in a large variety
of computer vision problems for keypoint matching.
this could be due to the coarse
image features and textures of intra-operative us and the diﬀerences in the
physical resolution between mri and us.
besides better landmark identiﬁcation accuracy, the tighter standard deviations
also imply that our dl approach serves a better role in grasping the local image
features within the image patches.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_66.pdf:
in brain tumor resection, accurate removal of cancerous tis-
sues while preserving eloquent regions is crucial to the safety and out-
comes of the treatment.
intra-operative ultrasound (ius) has been adopted
to provide real-time images to track brain shift, and inter-modal (i.e.,
mri-ius) registration is often required to update the pre-surgical plan.
quality control for the registration results during surgery is important
to avoid adverse outcomes, but manual veriﬁcation faces great challenges
due to diﬃcult 3d visualization and the low contrast of ius.
keywords: registration · inter-modal · error estimation · deep
learning
1
introduction
resection of early-stage brain tumors can greatly reduce the mortality rate of
patients.
however, as the true underlying tissue deformation is
unknown due to the 3d nature of the surgical data and the time constraint,
real-time manual inspection of mri-ius registration results is challenging and
error-prone, especially for precision-sensitive neurosurgery.
therefore, algorithms
that can detect and quantify unreliable inter-modal medical image registration
results are highly beneﬁcial.
recently, automatic quality assessment for medical image registration has
attracted increasing attention [4] from the domains of big medical data analy-
sis and surgical interventions.
with high eﬃciency, machine, and deep learning
techniques have been proposed to allow automatic grading and dense estima-
tion of medical image registration errors.
more recently, deep learning (dl) techniques that learn task-speciﬁc
features have also been adopted in automatic evaluation of medical image reg-
istration, with a primary focus on intra-contrast/modal applications, including
ct
unfortunately, so far, error grading and estimation in
inter-contrast/modal registration have rarely been explored, despite the partic-
ular demand in surgical applications.
although their algorithm performed well in simulated
cases, the results on real clinical scans still required improvements.
[13] from 2d
to 3d and employed the technique in registration error assessment for the ﬁrst
time.
2
methods and materials
2.1
dataset and preprocessing
for methodological development and assessment, we used the resect (retro-
spective evaluation of cerebral tumors) dataset
[16], which has pre-operative
focalerrornet for inter-modal registration error estimation
691
mri, and ius scans at diﬀerent surgical stages from 23 subjects who underwent
low-grade glioma resection surgeries.
1. we hypothesized that directly leveraging
clinical ius could help learn more realistic image features with potentially better
outcomes in clinical applications than with simulated contrasts [9,12].
however,
since the true brain shift model is impossible to obtain, we followed the strategy
of creating silver ground truths for image alignment [9,12], upon which simulated
misalignment is augmented in the ius to build and test our dl model.
1. left to right: demonstration of sample pre-operative mri, perfectly registered,
and deformed ius with a mean registration error of 1.4 mm.
to perform spatial misalignment augmentation, we continued to leverage 3d
b-spline transformation, similar to earlier reports on the same topic [10,12,17].
in short, b-spline transformation can be modeled by a grid of regularly spaced
control points and the associated parameters to allow various levels of nonlin-
ear deformation.
while the spacing of the control points determines the lev-
els of details in local deformation ﬁelds, the displacement parameters control
the magnitude of the deformation.
to ensure that simulated registration errors
are of diﬀerent varieties and sizes, we randomly selected the number of control
points and the associated displacements (in each 3d axis) with a maximum of
20 points and 30 mm, respectively.
after misalignment augmentation
on the previously co-registered ius, matching pairs of 3d image patches of size
33 × 33 × 33 voxels were taken from both the ius volume and the correspond-
ing mri.
since
b-spline transformation oﬀers a displacement vector at each voxel of the ius
volume, we directly considered the norm of the vector as the simulated registra-
tion error at the associated voxel.
in our design, we determined the registration
error of the image patch pair as the mean of all voxel-wise errors within the
ius patch.
finally, the image patch pairs, along with corresponding registration
errors were then fed to the proposed dl algorithm for training and validation.
with a
similar goal as the vision transformer (vit), the focal modulation network was
designed to model contextual information in images.
it incorporates three main
elements to achieve the goal: 1) focal contextualization that comprises a stack of
depth-wise convolutional layers to account for long- to short-range dependencies,
2) gated aggregation to collect contexts into a modulator for individual query
tokens, and 3) element-wise aﬃne transformation to inject the modulator into
the query.
we designed the focalerrornet as
a resnet-like variant of the focal modulation network to better encode rele-
vant features across the input image and ensure a better gradient ﬂow.
2.3
uncertainty quantiﬁcation
for registration error regression in surgical applications, knowledge regarding
the reliability of the automated results is instrumental for the safety and well-
being of the patients.
although the concept has
been widely applied in image segmentation and classiﬁcation, it has not been
employed for registration error estimation, especially in the case of multi-modal
situations, such as mri-ius alignment.
[9,12]
1.69 ± 1.37
0.61
focalerrornet 0.59 ± 0.57
0.82
2.4
experimental setup and implementation
from the transformation augmentation, we acquired 3380 samples of mri-ius
pairs.
for our experiments, we arbitrarily split the subjects into training, valida-
tion, and test sets with the proportion of 60%, 20%, and 20%, respectively.
to
prevent information leakage, we ensured that each patient was included in only
one of the split sets.
furthermore, in addition to
the transformation augmentation, we also included additional data augmenta-
tion, including random noise addition and random image ﬂipping on training sets
694
s. salari et al.
to mitigate overﬁtting and increase the model’s generalizability.
3)
that was employed for medical image registration error regression.
finally, to test the robustness of the focalerrornet, we acquired additional
mri-ius patch pairs from the test subjects, by introducing random linear shifts
(the max displacement from landmark locations is 10 voxels) from the selected
locations in the original set, and evaluated the dl model performance.
fig.
in addition, the correlations between the
predicted and ground truths errors are 0.82 (p < 1e-4) and 0.61 (p < 1e-3) for
focalerrornet and 3d cnn, respectively, further conﬁrming the advantage of
the proposed technique.
these metrics proved the validity of our
uncertainty measure and further conﬁrmed the performance of focalerrornet.
3.3
robustness of the proposed model
to examine the performance of our proposed method for image regions that
contain fewer potent anatomical features, we acquired additional image pairs
from test subjects, according to sect.
in this test, patches can contain large areas of zeros (image con-
tent out of the scanning fov of the ius).
the main reason for the observed
performance decline is due to the reduction in suﬃcient image features in ius.
however, despite these challenges, we saw an acceptable outcome from focaler-
rornet (absolute error = 1.28 mm or ∼1 voxel in clinical mris).
696
s. salari et al.
4
discussion
in image-guided interventions, there is an urgent need for automatic assessment
of image registration quality.
first, dissimilar contrasts between
images require more elaborate strategies to derive relevant features for error
assessment.
second, unlike segmentation or classiﬁcation, the ground truths of
registration errors are diﬃcult to obtain.
to
tackle these challenges, we employed 3d focal modulation with depth-wise con-
volution to encode contextual information for the image pair.
although we admit that residual errors still
remain after landmark-based b-spline nonlinear alignment, this approach has
been adopted in diﬀerent prior studies, considering the residual landmark reg-
istration error is fairly low (mtre of 0.0008 ± 0.0010mm).
although simulated
ultrasound has been used to provide a perfect alignment with mris, the ﬁdelity
of the simulated results is still suboptimal, and this may explain the under-
performance of the previous technique in real clinical data [12].
to ensure the
performance of our focalerrornet, we opted to regress the mean registration
error of image patches than simplistic error grades or voxel-wise error maps.
we believe that this design choice oﬀers a more stable performance, which is
supported by our validation.
we adopted uncertainty estimation in inter-modal
registration error assessment for the ﬁrst time.
furthermore, the use of standard deviation as an uncertainty measurement
maintains the same unit as the regressed errors, thus making the interpretation
more intuitive.
from quantitative and qualitative evaluations using correlation
coeﬃcients and scatter plots to assess the association of uncertainty measures
with the prediction errors and image entropy, we conﬁrmed the validity of the
proposed uncertainty estimation approach.
for our focalerrornet, we achieved
a prediction error of 0.59 ± 0.57 mm, which is on par with the image resolution
(0.5 mm).
these signify a robust performance of the focalerrornet.
therefore, we created random deformations for
patch-wise error estimation, and will further explore data-eﬃcient approaches
for registration error assessment.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_56.pdf:
mesoscopic ﬂuorescence lifetime imaging (flim) of tissue
ﬂuorophores (i.e., collagen and metabolic co-factors nadh and fad) emission
has demonstrated the potential to demarcate the extent of head and neck cancer
in patients undergoing surgical procedures of the oral cavity and the orophar-
ynx.
keywords: tors · positive surgical margin · flim · head and neck oncology
supplementary information the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43996-4_56.
achieving clear margins can be challenging in some
cases, particularly in tumors with involved deep margins [3, 4].
during transoral robotic surgery (tors), surgeons may assess the surgical margin
via visual inspection, palpation of the excised specimen and intraoperative frozen sec-
tions analysis (ifsa)
in the surgical cavity, surgeons visually inspect for residual
tumors and use specimen driven or defect-driven frozen section analysis to identify any
residual tumor [6, 7].
[9] to inspect psms in the excised specimen.
while promising, each modality presents certain limitations (e.g., time-consuming anal-
ysis, administration of a contrast agent, controlled lighting environment), which has
limited their clinical adoption
[10, 11].
label-free mesoscopic ﬂuorescence lifetime imaging (flim) has been demonstrated
as an intraoperative imaging guidance technique with high classiﬁcation performance
(auc = 0.94) in identifying in vivo tumor margins at the epithelial surface prior to tumor
excision [12].
flim can generate optical contrast using autoﬂuorescence derived from
tissue ﬂuorophores such as collagen, nadh, and fad.
due to the sensitivity of these
ﬂuorophores to their microenvironment, the presence of tumor changes their emission
properties (i.e., intensity and lifetime characteristics) relative to healthy tissue, thereby
enabling the optical detection of cancer
each annotated h&e section was registered with the ex vivo and in vivo
flim scan images.
we implemented
the image guidance by augmenting the classiﬁcation map to the surgical view using the
predictor output and point locations of the scan.
2.1
flim hardware and data acquisition
this study used a multispectral ﬂuorescence lifetime imaging (flim) device to acquire
data [14].
390/40 nm attributed to collagen autoﬂuorescence, (2) 470/28 nm to nadh, and (3)
542/50 nm to fad.
the resulting autoﬂuorescence waveform measurements for each
channel are averaged four times, thus with a 480 hz excitation rate, resulting in 120
averaged measurements per second
the flim device includes a 440 nm continuous wave laser that serves as an aiming
beam; this aiming beam enables real-time visualization of the locations where ﬂuores-
cence (point measurements) is collected by generating visible blue illumination at the
location where data is acquired.
segmentation of the ‘aiming beam’ allows for flim
data points to be localized as pixel coordinates within a surgical white light image (see
fig.
an ex vivo flim scan was then performed on the surgically excised specimen.
for each patient, the operating surgeon conducted an en bloc surgi-
cal tumor resection procedure (achieved by tors-electrocautery instruments), and the
resulting excised specimen was sent to a surgical pathology room for grossing.
the tissue
specimenwasseriallysectionedtogeneratetissueslices,whichwerethenformalin-ﬁxed,
parafﬁn-embedded, sectioned, and stained to create hematoxylin & eosin (h&e) slides
for pathologist interpretation (see fig. 1).
to
validate optical measurements to pathology labels (e.g., benign tissue vs. residual tumor),
pathology labels from the excision margins were digitally annotated by a pathologist on
each h&e section.
the aggregate of h&e sections was correspondingly labeled on
the ex vivo specimen at the cut lines where the tissue specimen was serially sectioned.
this
process enables the direct validation of flim measurements to the pathology status of
the electrocauterized surgical margins (see table 1).
2.3
flim preprocessing
the raw flim waveform contains background noise, instrument artifacts, and other
types of interference, which need to be carefully processed and analyzed to extract
meaningful information (i.e., the ﬂuorescence signal decay characteristics).
to retrieve the ﬂuorescence
function, we used a non-parametric model based on a laguerre expansion polynomi-
als and a constrained least-square deconvolution with the instrument impulse response
function as previously described [17].
due to its robust performance, we chose the generalized one-class
discriminative subspaces (gods) classiﬁcation model
the gods
is a pairwise complimentary classiﬁer deﬁned by two separating hyperplanes to min-
imize the distance between the two classiﬁers, limiting the healthy flim data within
the smallest volume and maximizing the margin between the hyperplanes and the data,
thereby avoiding overﬁtting while improving classiﬁcation robustness.
η − max

w t
2 xi + b2
2
∔
(1)
where w1, w2 are the orthonormal frames,
min
w∈sk
d ,b
is the stiefel manifold, η is the
sensitivity margin, and was set η = 0.4 for our experiments.
ν denote a penalty factor on
these soft constraints, and b is the biases.
[21].
592
m. a. hassan et al.
2.5
classiﬁer training and evaluation
the novelty detection model used for detecting residual cancer is evaluated at the point-
measurement level to assess the diagnostic capability of the method over an entire tissue
surface.
we used grid search to optimize the
hyper-parameters and features used in each model and are tabulated in the supplemen-
tary section table s1.
the sensitivity, speciﬁcity, and accuracy were used as evaluation
metrics to assess the performance of classiﬁcation models in the context of the study.
results of a binary classiﬁcation model using svm are also shown in the supplementary
section table s2.
2.6
classiﬁer augmented display
theclassiﬁer augmentationdepends onthreeindependent processingsteps: aimingbeam
localization, motion correction, and interpolation of the point measurements.
a detailed
description of implementing the augmentation process is discussed in [23].
the inter-
polation consists of ﬁtting a disk to the segmented aiming beam pixel location for each
point measurement and applying a color map (e.g., green: healthy and red: cancer) for
each point prediction.
individual pixels from overlapping disks are averaged to produce
the overall classiﬁcation map and augmented to the surgical ﬁeld as a transparent overlay.
3
results
table 2 tabulates the classiﬁcation performance comparison of novelty detection models
for classifying residual cancer vs. healthy on in vivo flim scans in the cavity.
the gods reported
the best classiﬁcation performance with an average sensitivity of 0.75 ± 0.02 (see fig. 2).
the oc-svm
and robust covariance reported a high standard deviation, indicating that the performance
of the classiﬁcation model is inconsistent across different patients.
we also observed that
changing the hyper-parameter, such as the anomaly factor, biased the model toward a
single class indicating overﬁtting (see supplementary section fig.
the gods uses two separating hyperplanes to minimize the distance between the
two classiﬁers by learning a low-dimensional subspace containing flim data properties
flim-based in vivo classiﬁcation of residual cancer
593
table 2. classiﬁcation performance comparison of novelty detection models for classifying resid-
ual cancer vs. healthy on in vivo flim scans in the cavity, mean (sd).
we observed that the gods with the flim decay
curves in the cdt space achieve the best classiﬁcation performance compared to other
novelty detection models with a mean accuracy of 0.76 ± 0.02.
this is mainly due to the
robustness of the model, the ability to handle high-dimensional data, and the contrast in
the flim decay curves.
the proposed model can resolve residual tumor at the point-measurement
level over a tissue surface.
this enhances surgical precision for tors procedures otherwise limited to
visual inspection of the cavity, palpation of the excised specimen, and ifsa.
combining the
proposed approach and ifsa could lead to an image-guided frozen section analysis to
help surgeons achieve negative margins in a more precise manner.
the columns represent each patient, and the rows depict the ground truth labels,
thepoint-predictionoverlay,andtheaugmentedsurgicalview.fpr-falsepositiverate,fnr-false
negative rate.
accounted for by the interpolation approach used for the classiﬁer augmentation (refer
to supplementary section fig.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_69.pdf:
automatic segmentation of colonoscopic intestinal lesions is
essential for early diagnosis and treatment of colorectal cancers.
cur-
rent deep learning-driven methods still get trapped in inaccurate colono-
scopic lesion segmentation due to diverse sizes and irregular shapes of
diﬀerent types of polyps and adenomas, noise and artifacts, and illumi-
nation variations in colonoscopic video images.
this work proposes a new
deep learning model called cascade transformer encoded boundary-aware
multibranch fusion networks for white-light and narrow-band colorectal
lesion segmentation.
it further introduces a boundary-aware multibranch fusion mech-
anism as a decoder that can enhance blurred lesion edges and extract
salient features, and simultaneously suppress image noise and artifacts
and illumination changes.
such a newly designed encoder-decoder archi-
tecture can preserve lesion appearance feature details while aggregating
the semantic global cues at several diﬀerent feature levels.
addition-
ally, a hybrid spatial-frequency loss function is explored to adaptively
concentrate on the loss of important frequency components due to the
inherent bias of neural networks.
we evaluated our method not only on
an in-house database with four types of colorectal lesions with diﬀerent
pathological features, but also on four public databases, with the exper-
imental results showing that our method outperforms state-of-the-art
network models.
in particular, it can improve the average dice similarity
coeﬃcient and intersection over union from (84.3%, 78.4%) to (87.0%,
80.5%).
intestinal lesions, particularly polyps
and adenomas, are usually developed to crc in many years.
therefore, diagnosis
and treatment of colorectal polyps and adenomas at their early stages are essen-
tial to reduce morbidity and mortality of crc.
however
these lesions in colonoscopic images are easily omitted and wrongly classiﬁed
due to limited knowledge and experiences of surgeons.
automatic and accurate
segmentation is a promising way to improve colorectal examination.
many researchers employ u-shaped network
[7,13,18] for colonoscopic polyp
segmentation.
unlike a family of u-net driven segmentation
methods, numerous papers have been worked on boundary constraints to seg-
ment colorectal polyps.
both polyp boundary-aware segmentation
methods work well but still introduce much false positive.
[6] removed the attention mechanism and replaced
res2net50 by hardnet to build hardnet-mseg that can achieve faster segmen-
tation.
[9] modiﬁed pranet to construct uacanet with
parallel axial attention and uncertainty augmented context attention to compute
uncertain boundary regions.
[10] introduced task-relevant feature replenishment networks for cross-
center polyp segmentation, while tian et al.
to
address these issues mentioned above, we explore a new deep learning archi-
tecture called cascade transformer encoded boundary-aware multibranch fusion
(ctbmf) networks with cascade transformers and multibranch fusion for polyp
and adenoma segmentation in colonoscopic white-light and narrow-band video
images.
first,
we construct cascade transformers that can extract global semantic and subtle
boundary features at diﬀerent resolutions and establish weighted links between
global semantic cues and local spatial ones for intermediate reasoning, providing
long-range dependencies and a global receptive ﬁeld for pixel-level segmentation.
, we built a new colonoscopic lesion image database and will make it publicly
available, while this work also conducts a thorough evaluation and comparison
on our new database and four publicly available ones (fig. 2).
fig.
let x0 and xi be input patches and the feature map at stage i, respec-
tively.
overlapping patch embedding (ope) separates an image into ﬁxed-size
patches and linearly embeds them into tokenized images while making adja-
cent windows overlap by half of a patch.
either key ki or value vi is the input
cascade transformer encoded boundary-aware multibranch fusion
721
sequence of linear spatial reduction (lsr) that implements layer normalization
(ln) and average pooling (ap) to reduce the input dimension:
lsr(ki) = ap(reshape(ln(ki ⊕ ω(ki), ri)wki)
(1)
where ω(·) denotes the output parameters of position embedding, ⊕ is the
element-wise addition, wki indicates the parameters that reduces the dimen-
sion of ki or vi, and ri is the reduction ratio of the attention layers at stage i.
as the output of lsr is fed into multihead attention, we can obtain attention
feature map aj
i from head j (j = 1, 2, · · · , n, n is the head number of the
attention layer) at stage i:
aj
i = attention(qw j
qi, lsr(ki)w j
ki, lsr(vi)w j
vi)
(2)
where attention(·) is calculated as the original transformer [14].
eventually, the output feature map xi of the pyramid
transformer at stage i can be represented by
xi = reshape(ψ ⊕ cff(qi, ki, vi))
(6)
2.2
boundary-aware multibranch fusion decoding
boundary-aware attention module.
given the feature map xi with semantic cues and rough appearance details,
we perform convolution (conv) on it and obtain ˜xi = conv(xi), which is further
augmented by channel and spatial attentions.
(7)
722
a. wang et al.
where ⊗ indicates the elementwise product.
we subtract the feature map ˜xi from
the enhanced map zi to obtain the augmented boundary attention map bi,
and also establish the correlation between the neighbor layers xi+1 and xi to
generate multilevel boundary map gi:
bi = zi ⊖ ˜xi, i = 1, 2, 3, 4
gi = ˜xi ⊖ us( ˜xi+1), i = 1, 2, 3
(9)
where ⊖ and us indicate subtraction and upsampling.
we obtain the fused feature representation map mi (i = 1, 2, 3, 4) from the
elementwise addition or summation of mi+1, di, and the residual feature ˜xi by
mi = conv( ˜xi ⊕ di ⊕ us(mi+1)), i = 1, 2, 3
m4 = conv(b4)
(11)
eventually, the output m1 of the boundary-aware multibranch fusion decoder
is represented by the following equation:
m1 = conv( ˜x1 ⊕ d1 ⊕ us(m2))
(12)
which precisely combines global semantic features with boundary or appearance
details of colorectal lesions.
2.3
hybrid spatial-frequency loss
this work proposes a hybrid spatial-frequency loss function hl to train our
network architecture for colorectal polyp and adenoma segmentation:
the frequency-domain loss fl can be computed by [8]
fl = λ
1
wh
w −1

u=0
h−1

v=0
γ(u, v)|g(u, v) − p(u, v)|2
(14)
cascade transformer encoded boundary-aware multibranch fusion
723
where w ×h is the image size, λ is the coeﬃcient of fl, g(u, v) and p(u, v) are
a frequency representation of ground truth g and prediction p using 2-d dis-
crete fourier transform.
3
experiments
our clinical in-house colonoscopic videos were acquired from various colonoscopic
procedures under a protocol approved by the research ethics committee of the
university.
these white-light and narrow-band colonoscopic images contain four
types of colorectal lesions with diﬀerent pathological features classiﬁed by sur-
geons: (1) 268 cases of hyperplastic polyp, (2) 815 cases of inﬂammatory polyp,
(3) 1363 cases of tubular adenoma, and (4) 143 cases of tubulovillous adenoma.
we implemented ctbmf on pytorch and trained it with a single nvidia
rtx3090 to accelerate the calculations for 100 epochs at mini-batch size 16.
factors λ (eq.
3. visual comparison of the segmentation results of using the four diﬀerent meth-
ods tested on those in-house and public datasets.green and blue show ground truth
and prediction.
(color ﬁgure online)
table 1. results and computational time of using ﬁve databases(our in-house and four
public databases)
average
dsc
iou
fβ
in-house
public
average
pranet
the momentum and weight decay were
set as 0.9 and 0.0005.
further, we resized input images to 352 × 352 for training
and testing and the training time was nearly 1.5 h to achieve the convergence.
we employ three metrics to evaluate the segmentation: dice similarity coeﬃcient
(dsc), intersection over union (iou), and weighted f-measure (fβ).
fig.
public data segmented results of our ablation study
modules dsc
iou
fβ
d1
0.681 0.593 0.634
d2
0.822 0.753 0.798
d3
0.820 0.748 0.793
residual 0.828 0.757 0.802
fl
0.834 0.764 0.804
4
results and discussion
figure 3 visually compares the segmentation results of the four methods tested on
our in-house and public databases.
our method can accurately segment polyps
in white-light and narrow-band colonoscopic images under various scenarios,
and ctbmf can successfully extract small, textureless and weak boundary and
colorectal lesions.
the segmented boundaries of our method are sharper and
clear than others especially in textureless lesions that resemble intestinal lining.
cascade transformer encoded boundary-aware multibranch fusion
725
figure 4 shows the dsc-boxplots to evaluate the quality of segmented polyps and
adenomas, which still demonstrate that our method works much better than the
others.
furthermore, we also summarizes the average three
metrics computed from all the ﬁve databases (the in-house dataset and four
public datasets).
our method attains much higher average dsc and iou of
(0.870, 0.805) than the others on the ﬁve databases.
fig.
each module
can improve the segmentation performance.
particularly, the boundary-aware
attention module critically improves the average dsc, iou, and fβ.
first, the cascade-transformer encoder can extract local
and global semantic features of colorectal lesions with diﬀerent pathological
characteristics due to its pyramid representation and linear spatial reduction
attention.
while the pyramid operation extracts multiscale local features, the
attention mechanism builds global semantic cues.
additionally, the hybrid spatial-frequency loss was also contributed to the
improvement of colorectal lesion segmentation.
5
conclusion
this work proposes a new deep learning model of cascade pyramid transformer
encoded boundary-aware multibranch fusion networks to automatically segment
diﬀerent colorectal lesions of polyps and adenomas in colonoscopic imaging.
while such an architecture employs simple and convolution-free cascade trans-
formers as an encoder to eﬀectively and accurately extract global semantic fea-
tures, it introduces a boundary-aware attention multibranch fusion module as a
decoder to preserve local and global features and enhance structural and bound-
ary information of polyps and adenomas, as well as it uses a hybrid spatial-
frequency loss function for training.
the thorough experimental results show
that our method outperforms the current segmentation models without any pre-
processing.
in particular, our method attains much higher accuracy on colono-
scopic images with small, illumination changes, weak-boundary, textureless, and
motion blurring lesions, improving the average dice similarity coeﬃcient and
intersection over union from (89.5%, 84.1%) to (90.3%, 84.4%) on our in-house
database, from (78.9%, 72.6%) to (83.4%, 76.5%) on the four public databases,
and from (84.3%, 78.4%) to (87.0%, 80.5%) on the ﬁve databases.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_50.pdf:
deep learning-based image segmentation for radiotherapy is
intended to speed up the planning process and yield consistent results.
however, most of these segmentation methods solely rely on distribution
and geometry-associated training objectives without considering tumor
control and the sparing of healthy tissues.
to incorporate dosimetric
eﬀects into segmentation models, we propose a new training loss func-
tion that extends current state-of-the-art segmentation model training
via a dose-based guidance method.
we hypothesized that adding such a
dose-guidance mechanism improves the robustness of the segmentation
with respect to the dose (i.e., resolves distant outliers and focuses on loca-
tions of high dose/dose gradient).
we demonstrate the eﬀectiveness of
the proposed method on gross tumor volume segmentation for glioblas-
toma treatment.
the obtained dosimetry-based results show reduced
dose errors relative to the ground truth dose map using the proposed
dosimetry-segmentation guidance, outperforming state-of-the-art distri-
bution and geometry-based segmentation losses.
keywords: segmentation · radiotherapy · dose guidance · deep
learning
1
introduction
radiotherapy (rt) has proven eﬀective and eﬃcient in treating cancer patients.
however, its application depends on treatment planning involving target lesion
and radiosensitive organs-at-risk (oar) segmentation.
hence, this manual segmentation step is very time-consuming and must
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43996-4 50.
studies
have shown that the manual segmentation task accounts for over 40% of the
treatment planning duration
hence, deep learning-based (dl) segmenta-
tion is essential for reducing time-to-treatment, yielding more consistent results,
and ensuring resource-eﬃcient clinical workﬂows.
nowadays, training of dl segmentation models is predominantly based on
loss functions deﬁned by geometry-based (e.g., softdice loss [15]), distribution-
based objectives (e.g., cross-entropy), or a combination thereof [13].
speciﬁcally, the
dice loss, allegedly the most popular segmentation loss function, has been shown
to have a tendency to yield overconﬁdent trained models and lack robustness
in out-of-distribution scenarios
in the ﬁeld of rt planning for brain tumor patients, the recent study
of [17] shows that current dl-based segmentation algorithms for target struc-
tures carry a signiﬁcant chance of producing false positive outliers, which can
have a considerable negative eﬀect on applied radiation dose, and ultimately,
they may impact treatment eﬀectiveness.
therefore, we postulate that training dl-based
segmentation models for rt planning should consider this clinical objective.
in this paper, we propose an end-to-end training loss function for dl-based
segmentation models that considers dosimetric eﬀects as a clinically-driven learn-
ing objective.
our contributions are: (i) a dosimetry-aware training loss function
for dl segmentation models, which (ii) yields improved model robustness, and
(iii) leads to improved and safer dosimetry maps.
in
addition, we report results comparing the proposed loss function, called dose-
segmentation loss (doselo), with models trained with a combination of
binary cross-entropy (bce) and softdice loss functions.
a segmentation
model (u-net
[20]) is trained to output target segmentation predictions for the
gross tumor volume (gtv) based on patient mri sequences.
predicted segmen-
tations and their corresponding ground-truth (gt) are fed into a dose predictor
model, which outputs corresponding dose predictions (denoted as dp and dp in
fig.
a pixel-wise mean squared error between both dose predictions is then
dose guidance for radiotherapy-oriented deep learning segmentation
527
fig.
a segmentation model (u-net
[20]) is trained to output target segmentation predic-
tions (st ) for the gross tumor volume (gtv) based on patient mri sequences imr.
predicted (st ) and ground-truth segmentations (st ) are fed into the dose predictor
model along with the ct-image (ict ), and oar segmentation (sor).
2.1
deep learning-based dose prediction
recent dl methods based on cascaded u-nets have demonstrated the feasibility
of generating accurate dose distribution predictions from segmentation masks,
approximating analytical dose maps generated by rt treatment planning sys-
tems
this good level of
performance, along with its ability to yield near-instant dose predictions, enables
us to create a training pipeline that guides learned features to be dose-aware.
following [12], the dose predictor model consists of a cascaded u-net (i.e., the
input to the second u-net is the output of the ﬁrst concatenated with the input
to the ﬁrst u-net) trained on segmentation masks, ct images, and reference
dose maps.
the model’s input is a normalized ct volume and segmentation
masks for target volume and oars.
as output, it predicts a continuous-valued
dose map of the same dimension as the input.
we refer the reader to [9,12] for further implementation
details.
we remark that the dose predictor model was also trained with data
augmentation, so imperfect segmentation masks and corresponding dose plans
are included.
this allows us in this study to use the dose predictor to model the
interplay between segmentation variability and dosimetric changes.
formally, the dose prediction model md receives as inputs: segmentations
masks for the gtv st ∈ zw ×h and the oars sor ∈ zw ×h, the ct image
(used for tissue attenuation calculation purposes in rt) ict ∈ rw ×h, and
outputs md(st , sor, ict ) →
2.2
dose segmentation loss (doselo)
during the training of the segmentation model, we used the dose predictor model
to generate pairs of dose predictions for the model-generated segmentations and
the gt segmentations.
the diﬀerence between these two predicted dose maps is
used to guide the segmentation model.
the intuition behind this is to guide the
segmentation model to yield segmentation results being dosimetrically consistent
with the dose maps generated via the corresponding gt segmentations.
formally, given a set of n pairs of labeled training images {(imr, sp )i : 1 ≤
i ≤ n}, imr ∈ rd (with d : {t1, t1c, t2, flair} mri clinical sequences),
and corresponding gt segmentations of the gtv st ∈ zh×w , a dl segmen-
tation model ms(imr) → st is commonly updated by minimizing a standard
loss term, such as the bce loss (lbce).
to guide the training process with dosimetry information stemming from seg-
mentation variations, we propose to use the mean squared error (mse) between
dose predictions for the gt segmentation (st ) and the predicted segmentation
(st ), and construct the following dose-segmentation loss,
ldsl =
1
h × w
h×w

i
(di
p − di
p )2
(1)
dp = md(st , sor, ict )
the ﬁnal loss is then,
ltotal = lbce + λldsl,
(4)
dose guidance for radiotherapy-oriented deep learning segmentation
529
where λ is a hyperparameter to weigh the contributions of each loss term.
we
remark that during training we use standard data augmentations including spa-
tial transformations, which are also subjected to dose predictions, so the model is
informed about relevant segmentation variations producing dosimetry changes.
3
experiments and results
3.1
data and model training
we divide the descriptions of the two separate datasets used for the dose pre-
diction and segmentation models.
this includes ct imaging data, segmentation masks of 13 oars, and the gtv.
gtvs were deﬁned according to the estro-acrop guidelines [16].
we refer the reader to [9] for further
details.
segmentation models:
to develop and test the proposed approach, we
employed a separate in-house dataset (i.e., diﬀerent cases than those used to
train the dose predictor model) of 50 cases from post-operative gmb patients
receiving standard rt treatment.
all cases comprise a planning ct
registered to the standard mri images (t1-post-contrast (gd), t1-weighted,
t2-weighted, flair), and gt segmentations containing oars as well as the
gtv.
we note that for this ﬁrst study, we decided to keep the dose prediction
model ﬁxed during the training of the segmentation model for a simpler presen-
tation of the concept and modular pipeline.
hence, only the parameters of the
segmentation model are updated.
baselines and implementation details: we employed the same u-net
[20]
architecture for all trained segmentation models, with the same training param-
eters but two diﬀerent loss functions, to allow for a fair comparison.
as a strong
comparison baseline, we used a combo-loss formed by bce plus softdice, which
is also used by nnunet and recommended by its authors [8].
our method1 was implemented in pytorch 1.13 using
adam optimizer [10] with β1 = 0.9, β2 = 0.999, batch normalization, dropout
set at 0.2, learning rate set at 10−4, 2 · 104 update iterations, and a batch size of
16.
the input image size is 256 × 256 pixels with an isotropic spacing
of 1 mm.
3.2
evaluation
to evaluate the proposed doselo, we computed dose maps for each test case
using a standardized clinical protocol with eclipse (varian medical systems inc.,
palo alto, usa).
we calculated dose maps for segmentations using the state-
of-the-art bce+softdice and the proposed doselo.
[12], which is the mean absolute error between
the reference dose map (dst ) and the dose map derived from the corresponding
segmentation result (dst , where st ∈ {bce+softdice, doselo}), and set it
relative to the reference dose map (dst ) (see eq. 5).
rmae =
1
h × w
h×w

i
|dst − d 
st |
dst
(5)
although it has been shown that geometric-based segmentation metrics
poorly correlate with the clinical end-goal in rt
[4,11,18,23], we report in sup-
plementary material dice and hausdorﬀ summary statistics as well (supplemen-
tary table 3).
this signiﬁcant dose error reduc-
tion shows the ability of the proposed approach to yield segmentation results in
better agreement with dose maps obtained using gt segmentations than those
obtained using the state-of-the-art bce+softdice combo-loss.
table 1 shows results for the ﬁrst and most signiﬁcant four cases from a
rt point of view (due to space limitations, all other cases are shown in sup-
plementary material).
dose guidance for radiotherapy-oriented deep learning segmentation
531
fig.
2. relative mean absolute dose errors/diﬀerences (rmae) between the reference
dose map and dose maps obtained using the predicted segmentations.
across all tested cases and folds we observe a large rmae reduction for dose maps
using the proposed doselo (average rmae reduction of 42.5%).
maps.
for case no. 3
the tumor presents a non-convex shape alongside the skull’s parietal lobe, which
was not adequately modeled by the training dataset used to train the segmen-
tation models.
indeed, we remark that both models failed to yield acceptable
segmentation quality in this area.
in case no. 4, both models failed to segment the
diﬀuse tumor area alongside the skull; however, as shown in fig.
case no. 5 (shown in supple-
mentary material) is an interesting case called butterﬂy gbm, which is a rare
type of gbm (around 2% of all gbm cases [3]), characterized by bihemispheric
involvement and invasion of the corpus callosum.
although we are aware that classical segmentation metrics poorly correlate
with dosimetric eﬀects [18], we report that the proposed method is more robust
than the baseline bce+softdice loss function, which yields outliers with haus-
dorﬀ distances: 64.06 ± 29.84 mm vs 28.68 ± 22.25 mm (–55.2% reduction) for
the proposed approach.
as pointed out by [17], segmentation outliers can have a
detrimental eﬀect on rt planning.
it can be seen that
doselo yields improved dose maps, which are in better agreement with the reference
dose maps (dose map color scale: 0 (blue) - 70gy (red)).
case
input image
dose simulation
reference
|ref.-(bce+sd)| |ref. -

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_47.pdf:
imitation learning has shown its eﬃcacy in
learning skills from expert demonstrations, but it faces challenges in pre-
dicting uncertain future movements and generalizing to various surgical
scenes.
experimental results demonstrate that our solution outperforms sota
imitation learning methods on our formulated task.
imitation learning has been widely studied in various domains [11,16,18]
with its good ability to learn complex skills, but it still needs adaptation and
improvement when being applied to learn dissection trajectory from surgical
data.
supervised learning such as behavior cloning (bc) [3] tends to average all pos-
sible prediction paths, which leads to inaccurate predictions.
in addition, the model performance can be sensitive to data distribution
and the noise in training data would result in unstable trajectory predictions.
to eﬀectively model the surgeon’s
behaviors and handle the large variation of surgical scenes, we leverage implicit
modeling to express expert dissection skills.
to address the limitations of inef-
ﬁcient training and unstable performance associated with ebm-based implicit
policies, we formulate the implicit policy using an unconditional diﬀusion model,
which demonstrates remarkable ability in representing complex high-dimensional
data distribution for videos.
for experi-
mental evaluation, we collected a surgical video dataset of esd procedures, and
preprocessed 1032 short clips with dissection trajectories labelled.
results show
that our method achieves superior performances in diﬀerent contexts of surgical
scenarios compared with representative popular imitation learning methods.
, it},
it ∈ rh×w ×3 and the output is an action distribution of a sequence of 2d
coordinates a = {yt+1, yt+2, ..., yt+n}, yt ∈ r2 indicating the future dissection
trajectory projected to the image space.
in order to obtain the demonstrated dissection trajectories from the expert
video data, we ﬁrst manually annotate the dissection trajectories on the video
frame according to the moving trend of the instruments observed from future
frames, then create a dataset d = {(s, a)i}m
i=0 containing m pairs of video clip
(state) and dissection trajectory (action).
by rep-
resenting the data using a continuous thermodynamics diﬀusion process, which
can be discretized into a series of gaussian transitions, the diﬀusion model is
498
j. li et al.
able to express complex high-dimensional distribution with simple parameter-
ized functions.
in addition, the diﬀusion process also serves as a form of data
augmentation by adding a range of levels of noise to the data, which guarantees
a better generalization in high-dimensional state space.
3
experiments
3.1
experimental dataset and evaluation metrics
dataset.
the input state is a
1.5-s length video clip containing 3 consecutive frames, and the expert dissection
trajectory is represented by a 6-point polyline indicating the tool’s movements
in future 3 s. we totally annotated 1032 video clips, which contain 3 frames for
each clip.
experiment setup.
to evaluate the performance of the proposed approach,
we adopt several metrics, including commonly used evaluation metrics for trajec-
tory prediction as used in [23,26], including average displacement error (ade),
the lower is the better.
method in-the-context
out-of-the-context
ade
fde
fd
ade
fde
fd
bc
10.43 (±5.52) 16.68
(±10.59) 24.92
(±14.59) 13.67
(±6.43) 17.03
(±12.50) 29.23
(±16.56)
ibc[5]
15.54 (±4.79) 22.66
(±8.06)
35.26
(±11.78) 15.81
(±4.66) 19.66
(±7.56)
31.66
(±9.14)
mid[8]
9.90
(±0.66) 15.26
(±1.35)
23.78
(±1.73)
12.42
(±1.89) 16.32
(±3.11)
27.04
(±4.79)
ours
9.47
(±1.66) 13.85 (±2.01)
21.43 (±3.89)
10.21 (±3.17) 14.14 (±3.63)
21.56 (±5.97)
which respectively reports the overall deviations between the predictions and the
ground truths, and final displacement error (fde) describing the diﬀerence
from the moving target by computing the l2 distance between the last trajec-
tory points.
pixel errors are used
as units for all metrics, while the input images are in 128 × 128 resolution.
3.2
comparison with state-of-the-art methods
to evaluate the proposed approach, we have selected popular baselines and state-
of-the-art methods for comparison.
we have chosen the fully supervised method,
behavior cloning, as the baseline, which is implemented using a cnn-mlp
network.
[5], the performance did
not meet our expectations and was even surpassed by the baseline.
according to the bar charts in fig. 3, the explicit diﬀusion policy
shows a performance drop for both evaluation sets on ade compared with the
implicit form.
3, the implicit diﬀusion policy
beneﬁts more from the forward-diﬀusion guidance in the “in-the-context” scenes,
achieving an improvement of 0.33 on ade.
when encountered with the unseen
scenarios in “out-of-the-context” data, the performance improvement of such
inference strategy is marginal.
value of synthetic data.
3 shows the synthetic data is useful as the augmented data for
downstream task learning.
3. left: ablation study of key method components; middle: visualization of
reverse processes of unconditional/conditional sampling from implicit policy; right:
performance of bc trained with synthetic data v.s. our method on ade.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_33.pdf:
image-guided surgery requires fast and accurate registration to align
preoperative imaging and surgical spaces.
rigid registration fails to account for nonrigid soft
tissue deformations, and biomechanical modeling approaches like ﬁnite element
simulations can be cumbersome in implementation and computation.
computing a displacement ﬁeld using this method does not
require mesh discretization or large matrix assembly and inversion conventionally
associated with ﬁnite element or mesh-free methods.
we solve for the optimal
superposition of regularized kelvinlet functions that achieves registration of the
medical image to simulated intraoperative geometric point data of the breast.
we
present registration performance results using a dataset of supine mr breast imag-
ing from healthy volunteers mimicking surgical deformations with 237 individual
targets from 11 breasts.
to demonstrate application, we perform
registration on a breast cancer patient case with a segmented tumor and compare
performance to other image-to-physical and image-to-image registration methods.
we show comparable accuracy to a previously proposed image-to-physical reg-
istration method with improved computation time, making regularized kelvinlet
functions an attractive approach for image-to-physical registration problems.
keywords: deformation · registration · elasticity · image-guidance · breast ·
ﬁnite element · kelvinlet
© the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. (eds.): miccai 2023, lncs 14228, pp.
https://doi.org/10.1007/978-3-031-43996-4_33
regularized kelvinlet functions to model linear elasticity
345
1
introduction
image-to-physical registration is a necessary process for computer-assisted surgery to
align preoperative imaging to the intraoperative physical space of the patient to in-form
surgical decision making.
most intraoperatively utilized image-to-physical regis-trations
are rigid transformations calculated using ﬁducial landmarks [1].
this has made image-guided surgery
more tractable for soft tissue organ systems like the liver, prostate, and breast
image-to-physical registration methods that accurately model an elastic
soft-tissue environment while also complying with intraoperative data constraints is an
active ﬁeld of research.
determining correspondences between imaging space and geo-
metric data is required for image-to-physical registration, but it is often an inexact and
ill-posed problem.
deep learning image registra-
tion methods like voxelmorph have also been used for this purpose [10].
other
non-learning image-to-physical registration strategies include [11] which utilized a coro-
tational linear-elastic ﬁnite element method (fem) combined with an iterative closest
point algorithm.
similarly, the registration method introduced in [12] iteratively updated
the image-to-physical correspondence between surface point clouds while solving for
an optimal deformation state.
both [11] and [12] leverage fem, which uses a 3d mesh to solve for
unique deformation solutions.
the element-free galerkin method is a mesh-free method that requires
only nodal point data and uses a moving least-squares approximation to solve for a
solution [13].
in this work, we propose an image-to-physical registration method that uses regu-
larized kelvinlet functions as a novel deformation basis for nonrigid registration.
finally, our approach is validated on an exem-
plar breast cancer case with a segmented tumor by comparing performance to previously
proposed registration methods.
(1), where e is young’s modulus, ν is poisson’s ratio, u(x)
is the displacement vector, and f(x) is the forcing function.
analytical displacement
solutions to eq.
the closed-form displacement solution for eq.
(3)
regularized kelvinlet functions to model linear elasticity
347
becomes numerically problematic in discretized problems because the displacement
and displacement gradient become indeﬁnite as x approaches x0.
= f

15ε4
8π
1
r7ε

(4)
uε,grab(r) =

a−b
rε i + b
r3ε rrt + a
2
ε2
r3ε i

f = kgrab(r)f
(5)
the second type of regularized kelvinlet functions represent “twist” deformations
which are derived by expanding the previous formulation to accommodate locally afﬁne
loads instead of displacement point sources.
the pure twist displacement ﬁeld
response uε,twist(r) to the forcing matrix in eq.
then, the
348
m. ringel et al.
f grab and f twist vectors are optimized to solve for a displacement ﬁeld that minimizes
distance error between geometric data inputs.
for a predetermined conﬁguration of regularized kelvinlet “grab” and “twist” func-
tions centered at different x0 control point locations, an elastically deformed state can
be represented as the summation of all regularized kelvinlet displacement ﬁelds where
∼u (x) is the superposed displacement vector and k = kgrab + ktwist in eq.
an objective function is formulated to minimize misalignment between the moving
space xmoving and ﬁxed space xﬁxed through geometric data constraints.
for the breast
imaging datasets in this work, we used simulated intraoperative data features that real-
istically could be collected in a surgical environment visualized in fig.
these data feature designations are consistent with
implementations in previous work [16, 17].
for a given deformation state, each data feature contributes to the total error measure.
ese is the average strain energy density within the breast geometry,
and it is computed for each β at every iteration.
the optimal
state β is iteratively solved using levenberg-marquardt optimization terminating at
|(β)|<10–12.
(β) =
1
npoint
npoint

i=1
(ei
point)2 +
1
nsurface
nsurface

i=1
(ei
surface)2 + wse(ese)2
(11)
3
experiments and results
in this section, two experiments are conducted.
the second
validates the registration method in a breast cancer patient and compares registration
accuracy and computation time to previously proposed methods.
3.1
hyperparameters sensitivity analysis
this dataset consists of supine breast mr images simulating surgical deformations of
11 breasts from 7 healthy volunteers.
volunteers (ages 23–57) were enrolled in a study
approved by the institutional review board at vanderbilt university.
mr images (0.391 × 0.391 ×
1 mm3 or 0.357 × 0.357 × 1 mm3) were acquired with the volunteers’ arms placed by
their sides.
this image was used as the xmoving space.
a second
mr image in the deformed state was acquired to create simulated intraoperative physical
data and to use for validation.
this second image was used as the xﬁxed space.
the breast in xmoving was segmented at the boundary between the chest wall and
breast parenchyma to create a 3d model.
the skin ﬁducials and intra-ﬁducial surface point clouds were
labeled in both images as data features.
subsurface
anatomical targets were labeled in both images and used to compute target error after
registration.
outliers
are noted as (x) and are 1.5•iqr.
3.2
registration methods comparison
this dataset consists of supine breast mr images simulating surgical deformations from
one breast cancer patient.
skin ﬁducial placement, image acquisition, arm placement, and
preprocessing steps followed the same protocol detailed in sect.
the tumor was
segmented in both images by a subject matter expert, and a 3d tumor model was created
to evaluate tumor overlap metrics after registration.
regularized kelvinlet function registration was compared to 3 other registration
methods: rigid registration, an fem-based image-to-physical registration method, and an
image-to-image registration method.
the
fem-based image-to-physical registration method, detailed in [12] and implemented
in breast in [16], utilizes the same optimization scheme as this method but with an
fem-generated basis.
the image-to-image registration method was a symmetric diffeomorphic method
with explicit b-spline regularization publicly available in the advanced normalization
toolkit (ants) repository [19, 20].
image-to-image registration would not be possi-
ble for intraoperative registration in most surgical settings.
the rigid and image-to-physical
registrations were performed on a single thread of a 3.6 ghz amd ryzen 7 3700x
cpu.
image-to-image registration was multithreaded on 2.3 ghz intel xeon (e5–4610
v2) cpus.
registration results for the 4 methods are shown in table 1.
the regularized kelvinlet
methodaccuracywascomparable(ifnotslightlyimproved)tothefem-basedmethodfor
thisexamplecase.runtimefortheregularizedkelvinletmethodwasimprovedcompared
to the fem-based method.
as expected, registration without deformable correction was
poor, and image-to-image registration had the best accuracy.
4.
table 1. registration performance for 4 methods.
rigid
image-to-physical
image-to-image
fem
r. kelvinlets
point
metrics
fiducial error (mm)
7.4 ± 2.0
0.7 ± 0.5
1.4 ± 0.6
2.0 ± 1.7
target error (mm)
6.1 ± 1.4
3.3 ± 1.1
3.0 ± 1.1
2.3 ± 1.5
tumor
overlap
metrics
dice coefﬁcient
2.3%
32.7%
49.5%
85.8%
centroid distance (mm)
7.3
4.4
3.5
1.3
modiﬁed hd (mm)
4.1
2.2
1.7
0.6
runtime (seconds)
< 1
188
14
15,942
fig.
orange – image-to-image registered xmoving tumor.
in this work, we demonstrated the use of regularized kelvinlet functions for image-
to-physical registration of the breast.
we believe that this approach is gen-
eralizable to other soft-tissue organ systems and is well-suited for improving navigation
during image-guided surgeries.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_25.pdf:
endoscopic radio-guided cancer detection and
resection has recently been evaluated whereby a novel tethered laparo-
scopic gamma detector is used to localize a preoperatively injected radio-
tracer.
this can both enhance the endoscopic imaging and complement
preoperative nuclear imaging data.
initial failed attempts used segmentation or geometric meth-
ods, but led to the discovery that it could be resolved by leveraging high-
dimensional image features and probe position information.
to demon-
strate the eﬀectiveness of this solution, we designed and implemented a
simple regression network that successfully addressed the problem.
through intensive experimentation, we demonstrated that
our method can successfully and eﬀectively detect the sensing area, estab-
lishing a new performance benchmark.
keywords: laparoscopic image-guided intervention · minimally
invasive surgery · detection of sensing area
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43996-4 25.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
surgery is
one of the main curative treatment options for cancer.
1. (a) hardware set-up for experiments, including a customized portable stereo
laparoscope system and the ‘sensei’ probe, a rotation stage, a laparoscopic lighting
source, and a phantom; (b) an example of the use of the ‘sensei’ probe in mis.
leverages the cancer-targeting ability of nuclear
agents typically used in nuclear imaging to more accurately identify cancer intra-
operatively from the emitted gamma signal (see fig.
geometrically, the sensing
area is deﬁned as the intersection point between the gamma probe axis and
the tissue surface in 3d space, but projected onto the 2d laparoscopic image.
in this study, in order to provide sensing area visual-
ization ground truth, we modiﬁed a non-functional ‘sensei’ probe by adding
a miniaturized laser module to clearly optically indicate the sensing area on
the laparoscopic images - i.e. the ‘probe axis-surface intersection’.
our system
consists of four main components: a customized stereo laparoscope system for
capturing stereo images, a rotation stage for automatic phantom movement, a
shutter for illumination control, and a daq-controlled switchable laser module
(see fig.
2
related work
laparoscopic images play an important role in computer-assisted surgery and
have been used in several problems such as object detection
[9], image segmenta-
tion [23], depth estimation
a 3d displacement module was explored in [21] and 3d geometric consistency
was utilized in [8] for self-supervised monocular depth estimation.
[19] presented a spatiotemporal vision transformer-based method and a self-
supervised generative adversarial network was introduced in [7] for depth esti-
mation of stereo laparoscopic images.
however, acquiring per-pixel ground
truth depth data is challenging, especially for laparoscopic images, which makes
it diﬃcult for large-scale supervised training [8].
laparoscopic segmentation is another important task in computer-assisted
surgery as it allows for accurate and eﬃcient identiﬁcation of instrument posi-
tion, anatomical structures, and pathological tissue.
for instance, a uniﬁed
framework for depth estimation and surgical tool segmentation in laparoscopic
images was proposed in [5], with simultaneous depth estimation and segmen-
tation map generation.
in [12], self-supervised depth estimation was utilized to
regularize the semantic segmentation in knee arthroscopy.
[16]
introduced a multi-task convolutional neural network for event detection and
semantic segmentation in laparoscopic surgery.
the dual swin transformer u-net
was proposed in [11] to enhance the medical image segmentation performance,
which leveraged the hierarchical swin transformer into both the encoder and the
decoder of the standard u-shaped architecture, beneﬁting from the self-attention
computation in swin transformer as well as the dual-scale encoding design.
[3] has been commonly used as the encoder to extract the
image features and geometric information of the scene.
in particular, in [21],
concatenated stereo image pairs were used as inputs to achieve better results,
and such stereo image types are also typical in robot-assisted minimally invasive
surgery with stereo laparoscopes.
hence, stereo image data was also adopted in
this paper.
we note that the standard illumination image from the laparoscopic probe
is also captured with the same setup when the laser module is on.
therefore, we
can establish a dataset with an image pair (rgb image and laser image) that
shares the same intersection point ground truth with the laser image (see fig.
the assumptions made are that the probe’s 3d pose when projected
into the two 2d images is the observed 2d pose, and that the intersection point
is located on its axis.
(a)
(b)
(c)
laser spot
laser spot
(d)
left
image
left
image
right
image
right
image
fig.
(a) standard illumination left rgb image; (b) left image with
laser on and laparoscopic light oﬀ; same for (c) and (d) but for right images.
the accompanying api
allowed for automatic image acquisition, exposure time adjustment, and white
balancing.
we acquired the dataset on a silicone tissue phantom which was 30 × 21 × 8
cm and was rendered with tissue color manually by hand to be visually realistic.
the phantom was placed on a rotation stage that stepped 10 times per revolution
to provide views separated by a 36-degree angle.
at each position, stereo rgb
images were captured i) under normal laparoscopic illumination with the laser
oﬀ; ii) with the laparoscopic light blocked and the laser on; and iii) with the
laparoscopic light blocked and the laser oﬀ. subtraction of the images with laser
on and oﬀ readily allowed segmentation of the laser area and calculation of its
central point, i.e. the ground truth probe axis-surface intersection.
all data acquisition and devices were controlled by python and labview
programs, and complete data sets of the above images were collected on visually
realistic phantoms for multiple probe and laparoscope positions.
therefore, our ﬁrst newly acquired dataset, named jerry, contains 1200 sets
of images.
– depth estimation: corresponding ground truth will be released.
– tool segmentation: corresponding ground truth will be released.
4
probe axis-surface intersection detection
4.1
overview
the problem of detecting the intersection point is trivial when the laser is on and
can be solved by training a deep segmentation network.
however, segmentation
requires images with a laser spot as input, while the real gamma probe produces
no visible mark and therefore this approach produces inferior results.
however, marker-based tracking and pose
estimation methods have sterilization implications for the instrument, and the
sfm method requires the surgeon to constantly move the laparoscope, reducing
the practicality of these methods for surgery.
furthermore, this sim-
ple methodology facilitated an average inference time of 50 frames per second,
enabling real-time sensing area map generation for intraoperative surgery.
(a) the input rgb image, (b) the estimated line using
pca for obtaining principal points, (c) the image with laser on that we used to detect
the intersection ground truth.
images
resblock1
....
resblock4
fc
concatenation
mlp1
mlp2
loss
feature1
point
feature
visual
feature
feature2
principal points
fig.
4. an overview of our approach using resnet and mlp.
4.2
intersection detection as segmentation
we utilized diﬀerent deep segmentation networks as a ﬁrst attempt to address
our problem [10,18].
please refer to the supplementary material for the imple-
mentation details of the networks.
we observed that when we do not use images
with the laser, the network was not able to make any good predictions.
(a) and (c) are standard illumination images and (b) and (d)
are
images with laser on and laparoscopic light oﬀ. the predicted intersection point
is shown in blue and the green point indicates the ground truth, which are further
indicated by arrows for clarity.
(color ﬁgure online)
understandable as the red laser spot provides the key information for the seg-
mentation.
therefore the network does not have any visual information to make
predictions from images of the gamma probe.
we note that to enable real-world
applications, we need to estimate the intersection point using the images when
the laser module is turned oﬀ.
4.3
intersection detection as regression
problem formulation.
formally, given a pair of stereo images il, ir, n points
{pl
1, pl
2, ..., pl
n} were sampled along the principal axis of the probe, pl
i ∈ r2
from the left image.
the same process was repeated for the right image.
unlike the segmentation approach, the intersection
point was directly predicted using a regression network.
the images fed to the
network were ‘laser oﬀ’ stereo rgb, but crucially, the intersection point for these
images was known a priori from the paired ‘laser on’ images.
the raw image
resolution was 4896×3680 but these were binned to 896×896.
[15] was used to extract the central axis of the probe and
50 points were sampled along this axis as an extra input dimension.
a network
was designed with two branches, one branch for extracting visual features from
the image and one branch for learning the features from the sequence of principal
points using resnet [3] and vision transformer (vit)
detecting the sensing area of a laparoscopic probe in cancer surgery
267
4.4
implementation
evaluation metrics.
implementation details.
the networks were implemented in pytorch [17],
with an input resolution of 896 × 896 and a batch size of 12.
we partitioned the
jerry dataset into three subsets, the training, validation, and test set, consisting
of 800, 200, and 200 images, respectively, and the same for the coﬀbee dataset.
6.9
8.2
16.7 21.3
7.0
3d median
6.0
5.9
7.1
5.3
6.2
5
results
quantitative results on the released datasets are shown in table 1 and table 2
with diﬀerent backbones for extracting image features, resnet and vit.
for
the 2d error on two datasets, among the diﬀerent settings, the combination of
resnet and mlp gave the best performance with a mean error of 70.5 pixels
268
b.
comparing
the table 1 and table 2, we found that the resnet backbone was better than the
vit backbone in the image processing task, while mlp was better than lstm in
probe pose representation.
resnet processed the input images as a whole, which
was better suited for utilizing the global context of a uniﬁed scene composed of
the tissue and the probe, compared to the vit scheme, which treated the whole
scene as several patches.
it is worth noting that the results from stereo inputs exceeded
those from mono inputs, which can be attributed to the essential 3d information
included in the stereo image pairs.
for the 3d error, the resnet backbone still gave generally better performance
than the vit backbone while under the resnet backbone, lstm and mlp gave
competitive results and they are all in sub-milimeter level.
this ﬁgure illustrates that our proposed method successfully detected the inter-
section point using solely standard rgb laparoscopic images as the input.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_29.pdf:
it is
based on a siamese architecture, including a recurrent neural network
that leverages the ultrasound image features and the optical ﬂow to
estimate the relative position of frames.
in addition, despite the
predominant non-linearity motion in our context, our method achieves
a good reconstruction with ﬁnal and average drift rates of 23.11% and
28.71% respectively.
keywords: intraoperative ultrasound · liver surgery · volume
reconstruction · recurrent neural networks
1
introduction
liver cancer is the most prevalent indication for liver surgery, and although there
have been notable advancements in oncologic therapies, surgery remains as the
only curative approach overall [20].
liver laparoscopic resection has demonstrated fewer complications compared
to open surgery [21], however, its adoption has been hindered by several reasons,
such as the risk of unintentional vessel damage, as well as oncologic concerns such
as tumor detection and margin assessment.
https://doi.org/10.1007/978-3-031-43999-5_29
304
s. el hadramy et al.
performing ious during laparoscopic liver surgery poses signiﬁcant chal-
lenges, as laparoscopy has poor ergonomics and narrow ﬁelds of view, and on the
other hand, ious demands skills to manipulate the probe and analyze images.
at the end, and despite its real-time capabilities, ious images are inter-
mittent and asynchronous to the surgery, requiring multiple iterations and
repetitive steps (probe-in −→ instruments-out −→ probe-out −→ instruments-in).
therefore, any method enabling a continuous and synchronous us assessment
throughout the surgery, with minimal iterations required would signiﬁcantly
improve the surgical workﬂow, as well as its eﬃciency and safety.
to overcome these limitations, the use of intravascular ultrasound (ivus)
images has been proposed, enabling continuous and synchronous inside-out
imaging during liver surgery [19].
with an intravascular approach, an overall
view and full-thickness view of the liver can quickly and easily be obtained
through mostly rotational movements of the catheter, while this is constrained
to the lumen of the inferior vena cava, and with no interaction with the tissue
(contactless, a.k.a. standoﬀ technique) as illustrated in fig.
1. left: ivus catheter positioned in the lumen of the inferior vena cava in the
posterior surface of the organ, and an example of the lateral ﬁring and longitudinal
beam-forming images; middle: anterior view of the liver and the rotational move-
ments of the catheter providing full-thickness images; right: inferior view showing the
rotational us acquisitions
however, to beneﬁt from such a technology in a computer-guided solution,
the diﬀerent us images would need to be tracked and possibly integrated into
a volume for further processing.
this information is then used to register the 3d ultrasound image with
the patient’s anatomy.
with the recent advances in deep learning, recent
trackerless volume reconstruction from intraoperative ultrasound images
305
works have proposed to learn a higher order nonlinear mapping between adjacent
frames and their relative spatial transformation.
[9] ﬁrst demon-
strated the eﬀectiveness of a convolution neural network to learn the relative
motion between a pair of us images.
[13] leverages past and future frames to esti-
mate the relative transformation between each pair of the sequence; they used
the consistency loss proposed in [14].
recent work [15,16] proposed to exploit the acceleration and
orientation of an inertial measurement unit (imu) to improve the reconstruc-
tion performance and reduce the drift error.
we use a
siamese architecture based on a sequence to vector(seq2vec) neural network
that leverages image and optical ﬂow features to learn relative transformation
between a pair of images.
to achieve this goal, we propose a siamese architecture that leverages
the optical ﬂow in the sequences in addition to the frames of interest in order to
provide a mapping with the relative frames spatial transformation.
then, gaussian heatmaps are used to describe the
motion of the m points in an image-like format(see sect.
we use the pyramidal
implementation of lucas-kanade method proposed in [4] to solve the equation.
+ 2}
we only keep the ﬁrst and last position of each point, which corresponds to the
trackerless volume reconstruction from intraoperative ultrasound images
307
(a) first frame in the sequence
(b) last frame in the sequence
fig.
∈ rh×w with
the same dimension as the ultrasound frames to encode these points, they are
more suitable as input for the convolutional networks.
these pairs
concatenated with the ultrasound ﬁrst and last frames form the recurrent neural
network sequential input of size (m + 1, h, w, 2), where m + 1 is the number
of channels (m heatmaps and one ultrasound frame), h and w are the height
and width of the frames and ﬁnally 2 represents the temporal dimension.
2.3
network architecture
the siamese architecture is based on a sequence to vector network.
our network
maps a sequence of two images having m + 1 channel each to a six degrees of
freedrom vector (three translations and three rotation angles).
the network
takes as input a sequence of two images with m + 1 channel each, m heatmaps and
an ultrasound frame.
||t(1,k+2)− ˆt(1,k+2)||2+||t(k+2,2k+3)− ˆt(k+2,2k+3)||2+||t(1,2k+3)− ˆt(1,2k+3)||2
(3)
3
results and discussion
3.1
dataset and implementation details
to validate our method, six tracked sequences were acquired from an ex vivo
swine liver.
a manually manipulated ivus catheter was used (8 fr lateral ﬁring
acunavtm 4–10 mhz) connected to an ultrasound system (acuson s3000
helx touch, siemens healthineers, germany), both commercially available.
frames were cropped to
remove the patient and probe characteristics, then down-sampled to a size of
128 × 128 with an image spacing of 0.22 mm per pixel.
first and end stages of the
sequences were removed from the six acquired sequences, as they were considered
to be largely stationary, and aiming to avoid training bias.
clips were created by
sliding a window of 7 frames (corresponding to a value of k = 2) with a stride of 1
trackerless volume reconstruction from intraoperative ultrasound images
309
over each continuous sequence, yielding a data set that contains a total of 13734
clips.
the number of
heatmaps m and the frame jump k were experimentally chosen among 0, 2, 4, 6.
our
method is implemented in pytorch1 1.8.2, trained and evaluated on a geforce
rtx 3090.
the model with the best
performance on the validation data was selected and used for the testing.
average drift rate (adr): the average cumulative drift of all frames
divided by the length from the frame to the starting point of the sequence.
(color ﬁgure online)
trackerless volume reconstruction from intraoperative ultrasound images
311
4

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_12.pdf:
imaging patients with inﬂammatory bowel disease (ibd) can
be especially problematic, owing to involuntary bowel movements and
diﬃculties with long breath-holds during acquisition.
therefore, this
paper proposes a deep adversarial super-resolution (sr) reconstruction
approach to address the problem of multi-task degradation by utilizing
cycle consistency in a staged reconstruction model.
we leverage a low-
resolution (lr) latent space for motion correction, followed by super-
resolution reconstruction, compensating for imaging artefacts caused by
respiratory motion and spontaneous bowel movements.
this alleviates
the need for semantic knowledge about the intestines and paired data.
learned image reconstruction approaches are believed to
occasionally hide disease signs.
we investigate this hypothesis by evalu-
ating a downstream task, automatically scoring ibd in the area of the
terminal ileum on the reconstructed images and show evidence that our
method does not suﬀer a synthetic domain bias.
keywords: abdominal mr · motion correction · super-resolution ·
deep learning
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43999-5 12.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
one of its manifestations, crohn’s
disease, often exhibits symptoms such as abdominal pain, diarrhoea, fatigue, and
cramping pain, which can be accompanied by severe complications [3].
although
crohn’s disease cannot be completely cured, early diagnosis can signiﬁcantly
reduce treatment costs and permanent physical damage [4].
as a result, many patients’ images are degraded by respiration, involuntary
movements and peristalsis.
furthermore, due to technical limitations, it is dif-
ﬁcult to acquire hr images in all scan orientations.
this limits the assessment
of the complete volume in 3d.
despite these challenges, mc and sr are crucial because corrupted mr
images can lead to inaccurate interpretation and diagnosis
manual correction
or enhancement of these volumes is not feasible.
contribution: our method (mocosr) alleviates the need for semantic knowl-
edge and manual paired-annotation of individual structures and the requirement
for acquiring multiple image stacks from diﬀerent orientations, e.g., [8].
there are several methodological contributions of our work: (1) first, to
account for non-isotropic voxel sizes of abdominal images, we reconstruct spa-
tial resolution from corrupted bowel mr images by enforcing cycle consistency.
the complementary spatial information from unpaired quality images
is exploited via cycle regularisation to provide an explicit constraint.
(3) experimental evaluation and analysis show that our mocosr is able to
generate high-quality mr images and performs favourably against other, alter-
mocosr
123
native methods.
furthermore, we explore conﬁdence in the generated data and
improvements to the diagnostic process.
(4) experiments with existing models
for predicting the degree of small bowel inﬂammation in crohn’s disease patients
show that mocosr can retain diagnostically relevant features and maintain the
original hr feature distribution for downstream image analysis tasks.
joint optimization of mc and sr remains challenging
because of the high-dimensionality of hr image space, and lr latent space has
been introduced in order to alleviate this issue.
recent studies on sr joint with
other tasks (e.g., reconstruction, denoising) have demonstrated improvements in
the lr space [11,19,20].
in the ﬁeld of machine learning and gastroin-
testinal disease, [21] used random forests to segment diseased intestines, which is
the ﬁrst time that image analysis support has been applied to bowel mri.
how-
ever, this technique requires radiologists to label and evaluate diseased bowel
segments, and patients’ scan times are long.
during inference, only the ﬁrst pair of lr encoder eclr and sr
decoder dsr will be utilized to generate high-quality, motion-free, and super-resolved
images.
a pair of task-speciﬁc
discriminators is used to improve the performance of each task-related encoder
and decoder.
loss functions: rather than aiming to reconstruct motion-free and hr images
in high dimensional space with paired data, we propose to regularize in low
dimensional latent space to obtain a high quality lr feature that can be used
for upscaling.
a lmc between ˜zq downsampled from qhr and zq cycled after
llr and clr, deﬁnes in an unpaired manner as follows:
+ ladv( ˜y )
(4)
the corresponding two task-speciﬁc discriminators ldmc and ldsr for dis-
criminating between corrupted and quality images followed are used for the
mocosr
125
purpose of staged reconstruction zq and ˆy of mc at latent space and sr at
spatial space, respectively.
the residual
output is then added to the input using a residual connection to obtain a staged
output.
the model implements the extraction of local features while integrating
all previous features through the connected blocks and compression layers.
3
experiments
data degradation: we use 64 × 64 × 64 patches.
for downsampling and the
degradation associated with mri scanning, (1) gaussian noise with a standard
deviation of 0.25 was added to the image.
the
simulated motion includes the inﬂuence of environmental factors that cause the
respiratory intensity and frequency to ﬂuctuate within a certain range.
this lead
to the presence of inter-slice misalignment in image domains.
(b) the staged reconstruction process involving application
of pmrm, mc result, and ground truth (gt) on tcga-lihc dataset.
we applied simulated motion to tcga mri-abdomen series to generate the
motion-corrupted dataset with respiration-induced shift.
ibd data set, inﬂammatory bowel disease:
mri sequences obtained
include axial t2 images, coronal t2 images and axial postcontrast mri data
on a philips achieva 1.5 t mr scanner.
abdominal 2d-acquired images exhibit
motion shifts between slices and ﬁbrillation artefacts due to the diﬃculty of
holding one’s breath/body movement and suppressing random organ motion for
extended periods.
the abnormal crohn’s disease sample cases, which
could contain more than one segment of terminal ileum and small bowel crohn’s
disease, were deﬁned based on the review of clinical endoscopic, histological, and
radiological images and by unanimous review by the same two radiologists (this
criterion has been used in the recent metric trial investigating imaging in
crohn’s disease [24]).
setting:
we compare with interpolation of bicubic and bilinear techniques,
rapid and accurate mr image sr (raisr-mr) with hashing-based learning [25],
which we use as the representative of the model-based methods, and mri
sr with various generative adversarial networks (mresr [17], cmrsr
various methods were included in the quantitative evaluation,
including single forward wgan (s-wgan) and cycle rdb (c-rdb) for abla-
tion experiments on the cycle consistency framework and the glrb setting.
there is a performance gap between interpolation and gan-
based methods, and the sr method based on learning has a signiﬁcant mapping
advantage for complex gastrointestinal images.
mocosr achieves the best per-
formance among all evaluation metrics.
cmrsr and mresr cannot guarantee
the output quality of mapping from hr back to lr, resulting in poor perfor-
mance on complex 3d bowel data.
our evaluation is based on the
average possibility of normal and abnormal small bowel inﬂammation on mri.
complete results including lr degraded image, sr image reconstructed by
mresr, cmrsr, and mocosr, are shown in table 3.
the ideal sr data can achieve classiﬁcation results as close as possible to
hr data with lower requirements.
the results of mresr
are volatile but present an unexpected improvement on healthy samples.
combining multi-
scale image information in the feature space of diﬀerent resolution image domains
yields better results than inter-domain integration.
this leads to a distribution shift in the samples, which makes
the disease prediction biased as shown in fig.
(b)
the comparison method results in the loss and concealment of discriminative features.
crohn’s disease classiﬁcation performance on diﬀerent 3d data.
mocosr has
a negligible eﬀect on the downstream classiﬁcation task as shown by high p-values in
contrast to lr, mresr, and cmrsr which produce signiﬁcantly lower performance.
the present sensitivity study is lim-
ited to the automatic classiﬁcation from single domain and down-stream task
framework, and future extensions will explore model-based and learning segmen-
tation tasks across data domains and acquisitions.
mocosr
is evaluated extensively and compared to the various image sr reconstruction
algorithms on a public abdominal dataset, simulating diﬀerent degrees of respira-
tory motion, and an ibd dataset with inherent motion.
mocosr demonstrated
superior performance.
to test if our learned reconstruction preserves clinically
relevant features, we tested on a downstream disease scoring method and found
no decrease in disease prediction performance with mocosr.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_10.pdf:
therefore, we propose an ldct image super-resolution
network consisting of a dual-guidance feature distillation backbone for
elaborate visual feature extraction, and a dual-path content communica-
tion head for artifacts-free and details-clear ct reconstruction.
the dgfm guides the network to concentrate the feature repre-
sentation of the 3d inter-slice information in the region of interest (roi)
by introducing the average ct image and segmentation mask as comple-
ments of the original ldct input.
furthermore, the heads
with the same function share the parameters so as to eﬃciently improve
the reconstruction performance by reducing the amount of parameters.
the experiments compared with 6 state-of-the-art methods on 2 public
datasets prove the superiority of our method.
keywords: low-dose computed tomography · image denoising ·
image super-resolution · deep learning
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. (eds.): miccai 2023, lncs 14229, pp.
https://doi.org/10.1007/978-3-031-43999-5_10
low-dose ct image super-resolution network
99
1
introduction
following the “as low as reasonably achievable” (alara) principle
[18] and cancer screening [28]. to
balance the high image quality and low radiation damage compared to normal-
dose ct (ndct), numerous algorithms have been proposed for ldct super-
resolution [3,4].
in the past decades, image post-processing techniques attracted much atten-
tion from researchers because they did not rely on the vendor-speciﬁc parameters
[2] like iterative reconstruction algorithms
image post-processing super-resolution (sr) methods
could be divided into 3 categories: interpolated-based methods [16,25], model-
based methods [13,14,24,26] and learning-based methods [7–9,17].
and model-based methods often involved time-consuming
optimization processes and degraded quickly when image statistics were biased
from the image prior
[6].
with the development of deep learning (dl), various learning-based meth-
ods have been proposed, such as edsr [20], rcan [31], and swinir [19].
those
methods optimized their trainable parameters by pre-degraded low-resolution
(lr) and high-resolution (hr) pairs to build a robust model with generaliza-
tion and ﬁnally reconstruct sr images.
[11] introduced a deep alternating network (dan) which
estimated the degradation kernels and corrected those kernels iteratively and
reconstructed results following the inverse process of the estimated degradation.
more recently, aiming at improving the quality of medical images further, huang
et al.
to
accurately reconstruct hr ct images from lr ct images, hou et al.
the aforementioned methods still have drawbacks: (1) they treated the
regions of interest (roi) and regions of uninterest equally, resulting in the extra
cost in computing source and ineﬃcient use for hierarchical features.
(2) most of
them extracted the features with a ﬁxed resolution, failing to eﬀectively lever-
age multi-scale features which are essential to image restoration task [27,32].
1(a), we propose an ldct image
sr network with dual-guidance feature distillation and dual-path content com-
100
j. chi et al.
fig.
avg ct is the average image among adjacent ct slices
of each patient.
munication.
(2) we propose a sampling attention block (sab)
which consists of sampling attention module (sam), channel attention module
(cam) and elaborate multi-depth residual connection aiming at the essential
multi-scale features by up-sampling and down-sampling to leverage the features
in ct images.
we ﬁrst calculate the
average ct image of adjacent ct slices of each patient to provide the 3d spatial
structure information of ct volume.
meanwhile, the roi mask is obtained by
a pre-trained segmentation network to guide the network to concentrate on the
focus area or tissue area.
then those guidance images and the input ldct image
low-dose ct image super-resolution network
101
are fed to the dual-guidance feature distillation backbone to extract the deep
features.
finally, the proposed dual-path architecture consisting of parameter-
shared sr heads and denoising heads leverages the deep visual features obtained
by our backbone to build the connection between the sr task and the denoising
task, resulting in noise-free and detail-clear reconstructed results.
to decrease the redundant
computation and make full use of the above-mentioned extra information, we
design a dual-guidance feature distillation backbone consisting of a dual-guidance
fusion module (dgfm) and sampling attention block(sab).
firstly, we use a 3 × 3 convolutional layer to extract the shallow features of
the three input images.
considering the indicative function of roi, we calculate the correlation
matrix between ldct and its mask and then acquire the response matrix
between the correlation matrix and the average ct image by multi-heads atten-
tion mechanism:
g represent the shallow
features of the input roi mask and the average ct image respectively.
furthermore, to take advantage of the multi-scale information which is essen-
tial for obtaining the response matrix containing the connections between dif-
ferent levels of features, as shown in fig.
3
experiments
3.1
datasets and experiment setup
datasets.
two widely-used public ct image datasets, 3d-ircadb
we choose 1663 ct images from 16 patients for training, 226
ct images from 2 patients for validation and 185 ct images from 2 patients for
testing.
similarly, the pancreas dataset is used for pancreas segmentation
which consists of 19328 512 × 512 ct ﬁles from 82 patients.
we choose 5638
ct images from 65 patients for training, 668 ct images from 8 patients for
validation and 753 ct images from 9 patients for testing.
and we use bicubic interpolation to degrade
the hr images to 256 × 256 lr images and 128 × 128 lr images.
low-dose ct image super-resolution network
103
table 1. ablation experiments on pancreas dataset with the scale factor of 2 and 4
(a) ablation experiments for dual-guidance on the pancreas dataset with the scale factor of
2 and 4
avg ct
mask
×2
×4
psnr
ssim
psnr
ssim
×
×
30.0282 ± 2.9426
0.8948 ± 0.0431
28.5120 ± 2.2875
0.8643 ± 0.0508
✓
×
29.9600 ± 3.2378
0.8950 ± 0.0419
28.1490 ± 2.3284
0.8592 ± 0.0543
×
✓
30.2991 ± 3.1391
0.8960 ± 0.0413
28.6589 ± 2.2497
0.8639 ± 0.0522
✓
✓
30.4047 ± 3.1558
0.8974 ± 0.0383
28.7542 ± 2.2728
0.8672 ± 0.0412
the best quantitative performance is shown in bold and the second-best in underlined.
(b) ablation experiments for shared heads mechanism on the pancreas dataset with the scale
factor of 2 and 4
heads
param (m) ×2
×4
×2/×4
psnr
ssim
psnr
ssim
sr only 5.748/5.896 30.2904 ± 3.0620
0.8948 ± 0.0431 28.4422 ± 2.3707
0.8628 ± 0.0523
unshared 6.009/6.304 30.3257 ± 3.2504
0.8940 ± 0.0442 28.5675 ± 2.2540
0.8645 ± 0.0529
shared
5.795/5.934 30.4047 ± 3.1558 0.8974 ± 0.0383 28.7542 ± 2.2728 0.8672 ± 0.0412
experiment setup.
all experiments are implemented on ubuntu 16.04.12
with an nvidia rtx 3090 24g gpu using pytorch 1.8.0 and cuda 11.1.74.
we augment the data by rotation and ﬂipping ﬁrst and then randomly crop
them to 128 × 128 patches.
peak signal-to-noise (psnr) and
structural similarity (ssim) are used as the quantitative indexes to evaluate the
performance.
3.2
ablation study
table 1a shows the experimental result of the dual-guidance ablation study.
introducing the average ct image guidance alone degrades performance com-
pared with the model without guidance for both the scale factor of 2 and 4.
and introducing mask guidance alone could improve the reconstruction eﬀect.
when the average ct image guidance and the mask guidance are both embed-
ded, the performance will be promoted further.
the experimental result proves that
introducing the proposed dual-path architecture could promote the reconstruc-
tion performance and the model with shared heads is superior than that without
them in both reconstruction ability and parameter amount.
(a)
is the hr image and its red rectangle region displays the liver and its lateral issues.
(color ﬁgure online)
3.3
comparison with state-of-the-art methods
we compare the performance of our proposed method with other state-of-the-
art methods, including bicubic interpolation
all methods enhance the image quality to diﬀer-
ent extents compared with bicubic interpolation.
similarly, our method outperforms the second-best methods
low-dose ct image super-resolution network
105
fig.
(a) is the
hr image and its red rectangle region shows the pancreas and kidney.
those quantitative superiorities conﬁrm our qualitative observa-
tions.
106
j. chi et al.
4
conclusion
in this paper, we propose an ldct image sr network with dual-guidance fea-
ture distillation and dual-path content communication.
especially, the dgfm could fuse the average
ct image to take the advantage of the 3d spatial information of ct volume and
the segmentation mask to focus on the roi, which provides pixel-wise shallow
information and deep semantic features for our backbone.
the sab leverages
the essential multi-scale features to enhance the ability for feature extraction.
the experiments compared with 6 state-of-
the-art methods on 2 public datasets demonstrate the superiority of our method.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_48.pdf:
we propose a new approach to 3d reconstruction from
sequences of images acquired by monocular endoscopes.
we demonstrate excellent accuracy on phantom
imagery.
remarkably, the watertight prior combined with illumination
decline, allows to complete the reconstruction of unseen portions of the
surface with acceptable accuracy, paving the way to automatic quality
assessment of cancer screening explorations, measuring the global per-
centage of observed mucosa.
unfortunately, we do
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43999-5 48.
this would usher exciting new developments, such as post-intervention
diagnosis, measuring polyps and stenosis, and automatically evaluating explo-
ration thoroughness in terms of the surface percentage that has been observed.
it has been shown that the colon
3d shape can be estimated from single images acquired during human colono-
scopies
however, to model large sections of it while increasing the recon-
struction accuracy, multiple images must be used.
as most endoscopes contain
a single camera, the natural way to do this is to use video sequences acquired
by these cameras in the manner of structure-from-motion algorithms.
an impor-
tant ﬁrst step in that direction is to register the images from the sequences.
and occlusions, specularities, varying albedos, and speciﬁcities of
endoscopic lighting make it a challenging one.
to overcome these diﬃculties, we rely on two properties of endoscopic images:
– endoluminal cavities such as the gastrointestinal tract, and in particular the
human colon, are watertight surfaces.
to take advantage of these speciﬁcities, we build on the success of neural implicit
surfaces (neus)
[25] that have been shown to be highly eﬀective at deriving sur-
face 3d models from sets of registered images.
[15] that inspired them, they were designed to operate on regular images
taken around a scene, sampling fairly regularly the set of possible viewing direc-
tions.
neus training selects a pixel from an image and samples points along its
projecting ray.
earlier methods [3] have reported similar accu-
racies but only on very few synthetic images and on short sections of the colon.
this makes us the ﬁrst to show
accurate results of extended 3d watertight surfaces from monocular endoscopy
images.
2
related works
3d reconstruction from endoscopic images.
unfortunately, many state-of the-
art slam techniques based on feature matching [5] or direct methods [6,7]
are impractical for dense endoscopic reconstruction due to the lack of texture
and the inconsistent lighting that moves along with the camera.
nevertheless,
sparse reconstructions by classical structure-from-motion (sfm) algorithms can
be good starting points for reﬁnement and densiﬁcation based on shape-from-
shading (sfs)
in contrast, for the same environments, we
propose the watertight prior coded by implicit sdf representations.
recent methods for dense reconstruction rely on neural networks to predict
per-pixel depth in the 2d space of each image and fuse the depth maps by
using multi-view stereo (mvs)
however, holes in
the reconstruction appear due to failures in triangulation and inaccurate depth
estimation or in areas not observed in any image.
[27] show the
potential of neural rendering in reconstruction from medical images, although
they use a binocular static camera with ﬁxed light source, which is not feasible
in endoluminal endoscopy.
lightneus: neural surface reconstruction in endoscopy
507
3.2
endoscope photometric model
apart from illumination decline, there are several signiﬁcant diﬀerences between
the images captured by endoscopes and those conventionally used to train nerfs
and neus: ﬁsh-eye lenses, strong vignetting, uneven scene illumination, and post-
processing.
hence, we also modiﬁed the original neus implementation to support these
models.
(3) is not
constant for all image pixels.
the post-processing software of medical endoscopes is designed to always
display well-exposed images, so that physicians can see details correctly.
an
adaptive gain factor g is applied by the endoscope’s internal logic and gamma
correction is also used to adapt to non-linear human vision, achieving better
contrast perception in mid tones and dark areas.
endoscope manufacturers know
the post-processing logic of their devices, but this information is proprietary and
not available to users.
again, gamma correction can be calibrated assuming it is
constant [3], and the gain change between successive images can be estimated,
for example, by sparse feature matching.
thus,
our photometric loss is computed using a normalized image:
i′ =
 iγ
leg
1/γ
(5)
4
experiments
we validate our method on the c3vd dataset [4], which covers all diﬀerent sec-
tions of the colon anatomy in 22 video sequences.
the images were recorded inside a phantom, a model of a human colon made of
silicone.
in the ﬁrst rows of table 1, we report median (medae), mean (mae), and
root mean square (rmse) values of these distances for all vertices seen in at
least one image.
(e) we managed to reconstruct a
curved section of the colon.
(f) our method plausibly estimates the wall of the colon
at the right of camera (b), although it was never seen in the images.
2 and additional ones in the supple-
mentary material.
3. this ability to “ﬁll in” observation gaps may be useful in providing
the endoscopist with an estimate of the percentage of unsurveyed area during a
procedure.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_73.pdf:
we evaluate the quantitative performance of structuregnet for head
and neck cancer between 3d ct scans and 2d histopathological slides,
enabling pixel-wise mapping of low-quality radiologic imaging to gold-
standard tumor extent and bringing biological insights toward homoge-
nized clinical guidelines.
additionally, our method can be used in radia-
tion therapy by mapping 3d planning ct into the 2d mr frame of the
treatment day for accurate positioning and dose delivery.
keywords: multimodal · registration · 2d-3d · histopathology ·
radiology
1
introduction
2d-3d registration refers to the highly challenging process of aligning an input 2d
image to its corresponding slice inside a given 3d volume [4].
it has received growing
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43999-5 73.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. (eds.): miccai 2023, lncs 14229, pp.
https://doi.org/10.1007/978-3-031-43999-5_73
772
a. leroy et al.
attention in medical imaging due to the various contexts where it applies, like image
fusion between 2d real-time acquisitions and either pre-operative 3d images for
guided interventions or reference planning volumes for patient positioning in radi-
ation therapy (rt).
indeed, mri or ct scans are the baseline source
ofinformationforcancertreatmentbutfailtoprovideanaccurateassessmentofdis-
ease proliferation, leading to high variability in tumor detection
on the
other hand, high-resolution digitized histopathology, called whole slide imaging
(wsi),providescell-levelinformationonthetumorenvironmentfromthesurgically
resected specimens.
however, the registration process is substantially diﬃcult due
to the visual characteristics, resolution scale, and dimensional diﬀerences between
thetwomodalities.inaddition,histologicalpreparationinvolvestissueﬁxationand
slicing,leadingtoseverecollapseandout-of-planedeformations.(semi-)automated
methodshavebeendevelopedtoavoidtime-consumingandbiasedmanualmapping,
including protocols with 3d mold or landmarks [10,22], volume reconstruction to
perform 3d registration [2,18,19,23], or optimization algorithms for direct multi-
modalcomparison[3,15].morerecently,deeplearning(dl)hasbeenintroducedbut
is limited to 2d/2d and requires prior plane selection
onepromisingsolutionistorelyonrigidstructuresthataresupposedlymorerobust
duringthepreparation.structuralinformationtoguideimageregistrationhasbeen
studiedwiththehelpofsegmentationsintothetrainingloop[11],orbylearningnew
image representations for reﬁned mapping [12].
in this paper, we propose to leverage the structural features of tissue and
more particularly the rigid areas to guide the registration process with two dis-
tinct contributions: (1) a cascaded rigid alignment driven by stiﬀ regions and
coupled with recursive plane selection, and (2) an improved 2d/3d deformable
motion model with distance ﬁeld regularization to handle out-of-plane deforma-
tion.
we also use the cyclegan for image translation
and direct monomodal signal comparison [25].
like [14,24], we combine regis-
tration with modality translation and integrate the two aforementioned compo-
nents.
the modality transfer is a 2d image-to-image translation
problem deﬁned as follows: given a sequence of n slices h = {h1, ..., hn} and a
volume considered as a full stack of m axial slices ct = {ct1, ..., ctm}, we build
a cyclegan with two generators and two discriminators gh→ct , gct →h, dh
and dct .
with a symmetric situation for gct →h, gh→ct outputs a synthetic
ct image, which is then processed by dct along with randomly sampled orig-
inal input slices with an associated adversarial loss ladv.
the cyclical pattern
lies in the similarity between the original images and the reconstructed samples
gct →h ◦ gh→ct (hi) through a pixel-wise cycle loss lcyc.
finally, we employ
two additional metrics: an identity loss lid to encourage modality-speciﬁc fea-
ture representation when considering hi being the input for gct →h with an
expected identity synthesis; and a structure consistency mind loss from [7] to
ensure style transfer without content alteration.
these losses are the classical
implementations for cyclegan and are detailed in the supplementary material.
2.2
recursive cascaded plane selection
we replace the volume reconstruction step with a recursive dual model.
for rigid initialization, the hypothesis is that the histological specimen is
cut with an unknown spacing and angle, but the latter is supposed constant
between wsis.
a rigid alignment is thus suﬃcient to reorient moving ct onto
ﬁxed h. based on a theoretical axial slice sequence z = (z1, ..., zm), we deﬁne
774
a. leroy et al.
fig.
2. cascaded alignment through rigid structure-aware warping followed by recur-
sive plane selection.
because soft tissues undergo too large out-of-plane deformations, we
leverage the rigid structures which are supposed not to be distorted or shrunk
during the histological process.
we extract their segmentation masks mct, mh
for both modalities (see preprocessing in sect.
we then introduce a sequence alignment
problem, the objective being to update the slice sequence z of sct by mapping
it to a corresponding sequence j of 2d images from ct.
[zi − 2, zi + 2]. based on
these rigid registration and plane selection blocks, we build a cascaded module
to iteratively reﬁne the alignment where the intermediate warping becomes the
new input.
this
dual model is crucial for initialization but does not take into account out-of-
plane deformations and a perfect alignment is not accessible yet.
the deformable
framework bridges this gap by focusing on irregular displacements caused by
tissue manipulation and reﬁning the rigid warping.
2.3
deformable 2d-3d registration
given one ﬁxed multi-slice sct ′ and a moving rigidly warped r(ct) from
the previous module, we adopt an architecture close to voxelmorph [1].
both latent representations are element-wise sub-
tracted.
a decoder is connected to both encoders and generates a displacement
ﬁeld φ the same size as input images but with (x, y, z)-channels corresponding
to the displacement in each spatial coordinate.
soft tissues away from bones
and cartilage are more subject to shrinkage or disruption, so we harness the
information from the cartilage segmentation mask of ct to generate a distance
transform map δ deﬁned as δ(v) = minm∈mct ||v−m||2.
we can then
control the displacement ﬁeld, with close tissue being more highly constrained
than isolated areas: φ′ = φ⊙(δ+ϵ), where ⊙ is the hadamard product and ϵ is
a hyperparameter matrix allowing small displacement even for cartilage areas for
which distance transform is null.
= 
v∈r3 ||∇φ′(v)||2 on the volume to constrain spatial gradients and
thus encourage smooth deformation, which is essential for empty slices which
are excluded from ldefo.
3. deformable 2d-3d registration pipeline, made of two encoders and a shared
decoder, with regularization applied on the displacement ﬁeld φ thanks to the distance
map from ct.
3
experiments
dataset and preprocessing.
two expert radiation oncologists on ct delineated both the thy-
roid and cricoid cartilages for structure awareness and the gross tumor volume
(gtv) for clinical validation, while two expert pathologists did the same on
wsis.
we ended up with images of size 256×256
(×64 for 3d ct) of 1 mm isotropic grid space.
to demon-
strate the performance of our model on another application, we also retrieved
the datasets from [14] for pelvis 3d ct/2d mr.
all masks were provided by
the authors and were originally segmented by internal experts.
hyperparameters.
we drew our code from cyclegan and voxelmorph imple-
mentations with modiﬁcations explained above, and we thank the authors of
msv-regsynnet for making their code and data available to us
a
detailed description of architectures and hyperparameters can be found in the
supplementary material.
we implemented our model with pytorch1.13 frame-
work and trained for 600 (800 for mr/ct) epochs with a batch size of 8 (4 for
mr/ct) patients parallelized over 4 nvidia gtx 1080 tis.
evaluation.
next, we implemented the modality translation-based msv-
structuregnet: 2d-3d multimodal registration
777
regsyn-net and modiﬁed it for our application to measure the importance of
joint structure-aware initialization and regularization.
1. from a qualitative per-
spective, the densities of the diﬀerent tissues are well reconstructed, with rigid
structures like cartilage being lighter than soft tissues or tumors.
the general
shape of the larynx also complies with the original radiologic images.
therefore, the cascaded rigid initialization is
crucial and helps the modality translation module in getting more similar pairs
of images for eased synthesis on the next pass.
4. the initialization enables an accurate plane
selection as proved by the similar shape of cartilages in (b).
(a) original ct, (b) warped ct after rigid ini-
tialization and plane selection, (c) warped ct after deformable registration, (d) origi-
nal histology with landmarks from pathologist (black) and warped projected landmarks
from radiologists (yellow), (e) overlaid cartilage masks after registration of histology
(ﬁlled blue) and radiology (red for our method, yellow for msv-regsynnet), (f) over-
laid contours between warped ct (gtv, red) and wsi (true tumor extent, blue).
dev. registration performance in terms of dice score (%),
hausdorﬀ distance (mm) and landmark error (mm).
method
dice
hausdorﬀ
landmark
runtime
voxelmorph 3d
68.4 ± 0.6
8.53 ± 0.32
6.71 ± 0.16
1.3
voxelmorph 2/3d 71.9 ± 1.7
7.19 ± 0.24
5.99 ± 0.22
1.4
msv-regsynnet
76.3 ± 1.4
6.88 ± 0.28
4.98 ± 0.15
2.1
ours (no init)
77.9 ± 1.9
6.91 ± 0.19
4.73 ± 0.31
2.1
ours (no regu)
85.1 ± 0.8
4.23 ± 0.27
3.71 ± 0.19
2.8
ours
86.9 ± 1.3 3.81 ± 0.20
3.28 ± 0.16 2.9
3d ct/2d mr
0.35t truefisp → 3d ct 1.5t t2 → 3d ct
dice
hausdorﬀ
dice
hausdorﬀ
msv-regsynnet
84.6 ± 0.9
7.25 ± 0.05
86.1 ± 1.0
5.84 ± 0.15
ours
84.8 ± 1.1 7.12 ± 0.08
87.9 ± 1.2
5.21 ± 0.09
diﬃculties inherent to the histological process like a cut larynx, the model suc-
cessfully maps both cartilage and soft tissue without completely tearing the ct
image thanks to regularization (c-d-e).
for quantitative assessment, we computed
the dsc as well as the hausdorﬀ distance between cartilages, and the average
distance between characteristic landmarks disposed before registration(table 1).
the superior performance of msv-regsynnet advocates for a modality
translation-based method compared to a direct multimodal similarity criterion.
we
also compared against msv-regsynnet on its own validation dataset for gener-
alization assessment: we yielded comparable results for the ﬁrst cohort and sig-
niﬁcantly better ones for the second, which proves that structuregnet behaves
well on other modalities and that the structure awareness is an essential asset for
better registration, as pelvis is a location where organs are moving.
visuals of
registration results are displayed in the supplementary material.
eventually, an
important clinical endpoint of our study is to compare the gtv delineated on
ct with gold-standard tumor extent after co-registration to highlight system-
atic errors and better understand the biological environment from the radiologic
signals.
the
typical error cases are the inclusion of cartilage or edema, which highlights the
structuregnet: 2d-3d multimodal registration
779
limitations and variability of radiology-based examinations, leading to increased
toxicity or untreated areas in rt.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_66.pdf:
we propose an unsupervised deep learning method to recon-
struct a 3d tomographic image from biplanar x-rays, to reduce the num-
ber of required projections, the patient dose, and the acquisition time.
we optimize the latent vectors of the generative model to recover a vol-
ume that both integrates this prior knowledge and ensures consistency
between the reconstructed image and input projections.
keywords: image reconstruction · inverse problem · sparse
sampling · deep generative model · ct
1
introduction
tomographic imaging estimates body density using hundreds of x-ray projec-
tions, but it’s slow and harmful to patients.
this can improve image-guided therapies and preoperative planning, espe-
cially for radiotherapy, which requires precise patient positioning with minimal
radiation exposure.
however, this task is an ill-posed inverse problem: x-ray measurements are
the result of attenuation integration across the body, which makes them very
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43999-5_66.
feed-forward methods do not manage to
predict a detailed and matching tomographic volume from a few projections.
in
other words, many 3d volumes may have generated such projections a priori.
these
non-learning methods show good results when the number of input projections
remains higher than a dozen but fail when very few projections are provided, as
our experiments in sect.
to do this, we
leverage the potential of generative models to learn a low-dimensional manifold
of the target body part.
3d ct reconstruction from biplanar x-rays with deep structure prior
701
compared to other 3d gans, it is proven to provide the best disentanglement
of the feature space related to semantic features [2].
compared to nerf-based methods, our method exploits prior
knowledge from many patients to require only two projections.
we perform several experiments to com-
pare our method with a feed-forward-based method [30] and a recent nerf-based
method [23], which are the previous state-of-the-art methods for the very few or
few projections cases, respectively.
to
summarize, our contributions are two-fold: (i) a new paradigm for 3d recon-
struction with biplanar x-rays: instead of learning to invert the measurements,
we leverage a 3d style-based generative model to learn deep image priors of
anatomic structures and optimize over the latent space to match the input pro-
jections; (ii) a novel unsupervised method, fast and robust to sampling ratio,
source energy, angles and geometry of projections, all of which making it general
for downstream applications and imaging systems.
we ﬁrst learn the low-
dimensional manifold of ct volumes of a target body region.
at inference, we
estimate the maximum a posteriori (map) volume on this manifold given very
few projections: we ﬁnd the latent vectors that minimize the error between the
synthetic projections from the corresponding volume on the manifold and the
real ones.
in this section, we formalize the problem, describe how we learn the
manifold, and detail how we optimize the latent vectors.
we ﬁrst learn the low-dimensional manifold of 3d structures
using a generative model.
2.2
manifold learning
to regularize the domain space of solutions, we leverage a style-based generative
model to learn deep priors of anatomic structures.
the map-
ping network learns to disentangle the initial latent space relatively to semantic
features which is crucial for the inverse problem.
we implement adaptive discriminator augmentation from stylegan-ada
[14]
to improve learning of the model’s manifold with limited medical imaging data.
(3)
note that by contrast with [18] for example, we optimize on the noise vectors
n as well: as we discovered in our early experiments, the n are also useful to
embed high-resolution details.
= − 
i,j log m(θi,j|0, κ) encourages the wi vectors to be
collinear so to keep the generation of coarse-to-ﬁne structures coherent.
3
experiments and results
3.1
dataset and preprocessing
manifold learning.
we focused
ct scans on the head and neck region above shoulders, with a resolution of
80 × 96 × 112, and centered on the mouth after automatic segmentation using a
pre-trained u-net [22].
planning ct scans were obtained for dose preparation, and cbct
scans were obtained at each treatment fraction for positioning with full gantry
acquisition.
3 and the supplementary material, all these
cases are challenging as there are large changes between the original ct scan
and the cbct scans.
2.3.
3.2
implementation details
manifold learning.
we used pytorch to implement our model, based on style-
gan2
we also
used 8 fully-convolutional layers with dimension 512 and an input latent vec-
tor of dimension 512, with tanh function as output activation.
[15] and style mixing [15], and added a 0.2
probability for generating images without gaussian noise to focus on embedding
the most information.
we augmented the discriminator with vertical and depth-
oriented ﬂips, rotation, scaling, motion blur and gaussian noise at a probability
of 0.2.
we
perform 100 optimization steps starting from the mean of the mapped latent
space, which takes 25 s, enabling clinical use.
3.3
results and discussion
manifold learning.
we tested our model’s ability to learn the low-dimensional
manifold.
this may be due to
a more complex architecture, discriminator augmentation, or simpler anatomy.
we compared our method against the main feed-forward method
x2ct-gan [30] and the neural radiance ﬁelds with prior image embedding
method nerp
method
1 projection
2 projections
psnr (db)↑ ssim↑
psnr (db)↑ ssim↑
nerp (w/o prior volume) 14.8 (±2.7)
0.12 (±0.10)
18.4 (±3.8)
0.17 (±0.10)
nerp (w/ prior volume)
22.5 (±3.2)
0.29 (±0.07)
23.5 (±3.5)
0.30 (±0.06)
x2ct-gan
20.7 (±2.4)
0.57 (±0.07)
21.8 (±2.5)
0.72 (±0.08)
ours
23.2 (±2.8)
0.79 (±0.09) 25.8 (±3.2)
0.85 (±0.10)
4 projections
8 projections
nerp (w/o prior volume) 19.9 (±2.6)
0.21 (±0.04)
20.0 (±2.5)
0.23 (±0.05)
nerp (w/ prior volume)
24.2 (±2.7)
0.32 (±0.05)
24.9 (±4.9)
0.34 (±0.08)
ours
28.2 (±3.5)
0.89 (±0.10) 30.1 (±3.9)
0.92 (±0.11)
improvements compared to x2ct-gan [30] and have similar constraints to
feed-forward methods.
additionally, no public implementation is available.
to evaluate our method’s performance with biplanar pro-
jections, we focused on positioning imaging for radiotherapy.
even when initialised with a previous
ct volume, nerp often fails to converge to the correct volume and introduces
many artifacts when few projections are used.
we used quantitative metrics (psnr and ssim) to evaluate reconstruction
error and human perception, respectively.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_72.pdf:
multimodal image registration is a challenging but essential
step for numerous image-guided procedures.
experiments
on three diﬀerent datasets demonstrate that our approach generalizes
well beyond the training data, yielding a broad capture range even on
unseen anatomies and modality pairs, without the need for specialized
retraining.
keywords: image registration · multimodal · metric learning ·
diﬀerentiable · deformable registration
1
introduction
multimodal imaging has become increasingly popular in healthcare due to its
ability to provide complementary anatomical and functional information.
how-
ever, to fully exploit its beneﬁts, it is crucial to perform accurate and robust
registration of images acquired from diﬀerent modalities.
multimodal image reg-
istration is a challenging task due to diﬀerences in image appearance, acquisition
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43999-5 72.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
while simple similarity measures directly based on the images’ intensities
such as sum of absolute (l1) or squared (l2) diﬀerences and normalized cross-
correlation (ncc)
essentially, it abstracts the problem to the
statistical concept of information theory and optimizes image-wide alignment
statistics.
as an alternative to directly assessing similarity on the original images, var-
ious groups have proposed to ﬁrst compute intermediate representations, and
then align these with conventional l1 or l2 metrics [5,20].
[5], which is
based on image self-similarity and has with minor adaptations (denoted mind-
ssc for self-similarity context) also been applied to us problems [7].
yet, such
feature descriptors are not expressive enough to cope with complex us artifacts
and exhibit many local optima, therefore requiring closer initialization.
some of these methods involve the utiliza-
tion of convolutional neural networks (cnn) to extract segmentation volumes
from the source data, transforming the problem into the registration of label
maps
it has furthermore been proposed in the past to utilize
cnns as a replacement for a similarly metric.
in [3,17], the two images being
registered are resampled into the same grid in each optimizer iteration, concate-
nated and fed into a network for similarity evaluation.
while such a measure
can directly be integrated into existing registration methods, it still suﬀers from
similar limitations in terms of runtime performance and modality dependance.
this approach combines ml and classical multimodal image
disa: universal multimodal registration
763
registration techniques in a novel way, avoiding the common limitations of ml
approaches: ground truth registration is not required, it is diﬀerentiable and
computationally eﬃcient, and generalizes well across anatomies and imaging
modalities.
2
approach
we formulate image registration as an optimization problem of a similarity met-
ric s between the moving image m and the ﬁxed image f with respect to the
parameters α of a spatial transformation tα :
tα the deformed image, the optimization target can be
expressed in the following way:
f(α) =

p∈ω
w(p) s(f[p], m ◦ tα[p]),
(1)
where w(p) is the weight assigned to the point p, s(·, ·) deﬁnes a local similarity
and the [·] operator extracts a patch (or a pixel) at a given spatial location.
the core idea of our method is to approximate the similarity metric s(p1, p2)
of two image patches with a dot product ⟨φ(p1), φ(p2)⟩ where φ(·) is a function
that extracts a feature vector, for instance in r16, from its input patch.
our experiments show that
this assumption (implicitly made also by other descriptors like mind) does not
present any practical impediment.
our method exhibits a large capture range
and can converge over a wide range of rotations and deformations.
advantages.
in contrast to many existing methods, our approach doesn’t
require any ground truth registration and can be trained using patches from
unregistered pairs of images.
each heatmap shows
the similarity of the marked point on the source image to every point in the target
image.
that the cnn has a negligible computational cost and can generalize well across
anatomies and modalities: a single network can be used for all types of images
and does not need to be retrained for a new task.
3
method
we train our model to approximate the three-dimensional lc2 similarity, as it
showed good performance on a number of tasks, including ultrasound [2,22].
in order to reduce the sensitivity on the scale, our target is actually the
average lc2 over diﬀerent radiuses of 3, 5, and 7.
in order to be consistent with
the original implementation of lc2 we use the same weighting function w based
on local patch variance.
our neural network is trained using patches from the “gold atlas
- male pelvis - gentle radiotherapy” [14] dataset, which is comprised of 18
patients each with a ct, mr t1, and mr t2 volumes.
as lc2 requires the usage of gradient magnitude in one of the
modalities, we randomly pick it from either ct or mr.
we would like to report that, initially, we also made use of a proprietary
dataset including us volumes.
we do not use any normalization layer, as this
resulted in a reduction in performance.
the architecture
consists of ten layers and a total of 90,752 parameters, making it notably smaller
than many commonly utilized neural networks.
augmentation on the training data is used to make the model as robust as
possible while leaving the target similarity unchanged.
the training converges to
an average patch-wise l2 error of 0.0076 on the training set and 0.0083 on the
validation set.
4
experiments and results
we present an evaluation of our approach across tasks involving diverse modali-
ties and anatomies.
notably, the experimental data utilized in our analysis diﬀers
signiﬁcantly from our model’s training data in terms of both anatomical struc-
tures and combination of modalities.
fre is the average of ﬁducial errors in millimeters across all cases, while fre25,
fre50, and fre75 refer to the 25th, 50th, and 75th percentiles.
method
mode
avg.
fre fre25 fre50 fre75
mind-ssc rigid
5.05
1.69
2.20
3.31
mind-ssc aﬃne 2.01
1.44
1.84
2.29
lc2
rigid
1.71
1.31
1.56
1.72
lc2
aﬃne 1.73
1.32
1.67
1.89
disa-lc2
rigid
1.82
1.37
1.65
1.80
disa-lc2
aﬃne 1.74
1.33
1.58
1.73
table 2. results on the abdomen mr-ct task of the learn2reg challenge 2021.
in all experiments, we use a wilcoxon signed-rank test with p-value
10−2 to establish the signiﬁcance of our results.
4.3)
our method obtains a signiﬁcantly larger capture range, opening new possibilities
for tackling this challenging problem.
4.1
aﬃne registration of brain us-mr
in this experiment, we evaluate the performance of diﬀerent methods for
estimating aﬃne registration of the retrospective evaluation of cerebral
tumors (resect) miccai challenge dataset
in conclusion, our experiments demonstrate that the proposed
disa-lc2, combined with a simple optimization strategy, is capable of achieving
equivalent performance to manually tuned lc2.
4.2
deformable registration of abdominal mr-ct
our second application is the abdomen mr-ct task of the learn2reg challenge
2021
we
estimate dense deformation ﬁelds using the methodology outlined in [6] (without
inverse consistency) which ﬁrst estimates a discrete displacement using explicit
search and then iteratively enforces global smoothness.
segmentation maps of
anatomical structures are used to measure the quality of the registration.
the hyperparameters of the reg-
istration algorithm have been manually optimized for each approach.
table 2
shows that our method obtains signiﬁcantly better results than mind-scc on
the dsc metrics while being not signiﬁcantly better on hd95.
4.3
deformable registration of abdominal us-ct and us-mr
as the most challenging experiment, we ﬁnally use our method to achieve
deformable registration of abdominal 3d freehand us to a ct or mr volume.
between
4 and 9 landmark pairs (vessel bifurcations, liver gland borders, gall bladder,
kidney) were manually annotated by an expert.
note that this registration problem is much more challenging than the prior
two due to diﬃcult ultrasonic visibility in the abdomen, strong deformations,
and ambiguous matches of liver vasculature.
therefore, to the best of our knowl-
edge, these results present a signiﬁcant leap towards reliable and fully automatic
fusion, doing away with cumbersome manual landmark placements.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_6.pdf:
medical image synthesis is a challenging task due to the
scarcity of paired data.
several methods have applied cyclegan to lever-
age unpaired data, but they often generate inaccurate mappings that shift
theanatomy.thisproblemisfurtherexacerbatedwhentheimagesfromthe
sourceandtargetmodalitiesareheavilymisaligned.recently,currentmeth-
ods have aimed to address this issue by incorporating a supplementary seg-
mentation network.
extensive experiments demonstrate that maskgan outper-
formsstate-of-the-artsynthesismethodsonachallengingpediatricdataset,
where mr and ct scans are heavily misaligned due to rapid growth in
children.speciﬁcally,maskganexcelsinpreservinganatomicalstructures
withouttheneedforexpertannotations.thecodeforthispapercanbefound
at https://github.com/hieuphan33/maskgan.
mri and ct pro-
duce diﬀerent tissue contrast and are often used in tandem to provide comple-
mentary information.
while mri is useful for visualizing soft tissues (e.g. muscle,
acknowledgement: this study was supported by channel 7 children’s research foun-
dation of south australia incorporated (crf).
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43999-5 6.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
unfortunately, ct imaging
exposes patients to ionizing radiation, which can damage dna and increase
cancer risk [9], especially in children and adolescents.
given these issues, there
are clear advantages for synthesizing anatomically accurate ct data from mri.
despite the superior perfor-
mance, supervised methods require a large amount of paired data, which is
prohibitively expensive to acquire.
several unsupervised mri-to-ct synthesis
methods [4,6,14], leverage cyclegan with cycle consistency supervision to elim-
inate the need for paired data.
unfortunately, the performance of unsupervised
ct synthesis methods [4,14,15] is inferior to supervised counterparts.
due to
the lack of direct constraints on the synthetic outputs, cyclegan [20] struggles
to preserve the anatomical structure when synthesizing ct images, as shown in
fig.
this problem is particularly relevant
in brain scanning, where both the pixel-wise correlation and noise statistics in
mr and ct images are diﬀerent, as a direct consequence of the signal acqui-
sition technique.
the alternative shape-wise consistency methods [3,4,19] aim
58
v. m. h. phan et al.
to preserve the shapes of major body parts in the synthetic image.
notably,
shape-cyclegan [4] segments synthesized ct and enforces consistency with the
ground-truth mri segmentation.
however, these methods rely on segmentation
annotations, which are time-consuming, labor-intensive, and require expert radi-
ological annotators.
a recent natural image synthesis approach, called attention-
gan [12], learns attention masks to identify discriminative structures.
atten-
tiongan implicitly learns prominent structures in the image without using the
ground-truth shape.
comparisons of diﬀerent shape-aware image synthesis.
method
mask
supervision
human
annotation
structural
consistency
shape-cyclegan [4]
precise mask
yes
yes
attentiongan [12]
not required
no
no
maskgan (ours) coarse mask
no
yes
in this paper, we propose maskgan, a novel unsupervised mri-to-ct
synthesis method, that preserves the anatomy under the explicit supervision of
coarse masks without using costly manual annotations.
unlike segmentation-
based methods [4,18], maskgan bypasses the need for precise annotations,
replacing them with standard (unsupervised) image processing techniques, which
can produce coarse anatomical masks.
such masks, although imperfect, provide
suﬃcient cues for maskgan to capture anatomical outlines and produce struc-
turally consistent images.
maskgan is the ﬁrst framework that maintains shape consis-
tency without relying on human-annotated segmentation.
3) extensive experiments show that our
method outperforms state-of-the-art methods by using automatically extracted
coarse masks to eﬀectively enhance structural consistency.
the cycle shape consistency (csc) loss lcsc minimizes the l1
distance between the masks learned by the mri and ct generators, promoting con-
sistent anatomy segmentation across domains.
intuitively, each channel ai in the mask tensor a focuses on diﬀerent anatom-
ical structures in the medical image.
2.2
cyclegan supervision
the two generators, gmr and gct , map images from mri domain (x) and
ct domain (y ), respectively.
two discriminators, dmr and dct , are used to
distinguish real from fake images in the mri and ct domains.
the adversarial
loss for training the generators to produce synthetic ct images is deﬁned as
lct(gmr, dct, x, y) = ey∼pdata(y)

log dct(y)

+ ex∼pdata(x)

log(1 − dct(gmr(x)))

.
(2)
the adversarial loss lmr for generating mri images is deﬁned in a similar
manner.
we extract the coarse mask b using
basic image processing operations.
speciﬁcally, we design a simple but robust
algorithm that works on both mri and ct scans, with a binarization stage
followed by a reﬁnement step.
in the binarization stage, we normalize the inten-
sity to the range
in the
post-processing stage, we reﬁne the binary image using morphological operations,
speciﬁcally employing a binary opening operation to remove small artifacts.
we introduce a novel mask supervision loss that penalizes the diﬀerence
between the background mask an learned from the input image and the ground-
truth background mask b in both mri and ct domains.
previous shape-aware methods [4,18] use a pre-trained u-net [10]
segmentation network to enforce shape consistency on the generator.
u-net is
pre-trained in a separate stage and frozen when the generator is trained.
hence,
structure-preserving synthesis: maskgan for unpaired mr-ct translation
61
any errors produced by the segmentation network cannot be corrected.
our loss penalizes the discrepancy between the background atten-
tion mask amr
n
learned from the input mri image and the mask ˜act
n
learned
from synthetic
3
experimental results
3.1
experimental settings
data collection.
we targeted the age group from 6–24 months
since pediatric patients are more susceptible to ionizing radiation and experience
a greater cancer risk (up to 24% increase) from radiation exposure [7]. further-
more, surgery for craniosynostosis, a birth defect in which the skull bones fuse
too early, typically occurs during this age [5,16].
following [13],
we conducted experiments on sagittal slices.
all models are trained using
1 ethics approval was granted by southern adelaide clinical human research ethics
committee.
to provide a quantitative evaluation of methods, we com-
pute the same standard performance metrics as in previous works
the scope of the
paper centers on theoretical development; clinical evaluations such as dose cal-
culation and treatment planning will be conducted in future work.
3.2
results and discussions
comparisons with state-of-the-art.
we compare the performance of our
proposed maskgan with existing state-of-the-art image synthesis methods,
including cyclegan [20], attentiongan [12], structure-constrained cyclegan
(sc-cyclegan)
[4]. shape-cyclegan requires anno-
tated segmentation to train a separate u-net.
for a fair comparison, we imple-
ment shape-cyclegan using our extracted coarse masks based on the authors’
oﬃcial code.
as better mri synthesis leads to improved
ct synthesis, we also report the model’s performance on mri synthesis.
the improvement of maskgan over all compared methods is statistically signiﬁcant.
unlike shape-cyclegan, which underperforms when trained with coarse
segmentations, our method obtains consistently higher results.
3. visual comparison of synthesized ct images by diﬀerent methods on two sam-
ples.
under this challenging setting, unpaired image syn-
thesis can have non-optimal visual results and ssim scores.
yet, our maskgan
achieves the highest quality, indicating its suitability for pediatric image synthe-
sis.
the combination of both mask and
cycle shape consistency losses results in the largest improvement, demonstrating
the complementary contributions of our two losses.
we compare the performance
of our approach with shape-cyclegan [4] using deformed masks that simulate
human errors during annotation.
to alter object shapes, we employ random
elastic deformation, a standard data augmentation technique [10] that applies
random displacement vectors to objects.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_5.pdf:
multi-sequence mri is valuable in clinical settings for reli-
able diagnosis and treatment prognosis, but some sequences may be unus-
able or missing for various reasons.
recent deep learning-based methods have achieved
good performance in combining multiple available sequences for missing
sequence synthesis.
despite their success, these methods lack the ability
to quantify the contributions of diﬀerent input sequences and estimate
region-speciﬁc quality in generated images, making it hard to be practical.
hence, we propose an explainable task-speciﬁc synthesis network, which
adapts weights automatically for speciﬁc sequence generation tasks and
provides interpretability and reliability from two sides: (1) visualize and
quantify the contribution of each input sequence in the fusion stage by
a trainable task-speciﬁc weighted average module; (2) highlight the area
the network tried to reﬁne during synthesizing by a task-speciﬁc atten-
tion module.
we conduct experiments on the brats2021 dataset of 1251
subjects, and results on arbitrary sequence synthesis indicate that the pro-
posed method achieves better performance than the state-of-the-art meth-
ods.
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43999-5 5.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
[16], lesion segmentation [17], treatment prognosis
however, some
acquired sequences are unusable or missing in clinical settings due to incorrect
machine settings, imaging artifacts, high scanning costs, time constraints, con-
trast agents allergies, and diﬀerent acquisition protocols between hospitals [5].
without rescanning or aﬀecting the downstream pipelines, the mri synthesis
technique can generate missing sequences by leveraging redundant shared infor-
mation between multiple sequences
many studies have demonstrated the potential of deep learning methods for
image-to-image synthesis in the ﬁeld of both nature images [8,11,12] and medical
images [2,13,19].
most of these works introduce an autoencoder-like architecture
for image-to-image translation and employ adversarial loss to generate more
realistic images.
unlike these one-to-one approaches, mri synthesis faces the
challenge of fusing complementary information from multiple input sequences.
recent studies about multi-sequence fusion can speciﬁcally be divided into two
groups: (1) image fusion and (2) feature fusion.
the image fusion approach is
to concatenate sequences as a multi-channel input.
image-level fusion is
simple and eﬃcient but unstable – zero-padding inputs for missing sequences
lead to training unstable and slight misalignment between images can easily
cause artifacts.
in contrast, eﬀorts have been made on feature fusion, which can
alleviate the discrepancy across multiple sequences, as high-level features focus
on the semantic regions and are less aﬀected by input misalignment compared
to images.
more importantly, recent studies only focus on
proposing end-to-end models, lacking quantifying the contributions for diﬀerent
sequences and estimating the qualities of generated images.
specially, this framework
can be easily extended to other tasks, such as segmentation.
our primary con-
tributions are as follows: (1) we propose a ﬂexible network to synthesize the
target mri sequence from an arbitrary combination of inputs; (2) the network
explainable task-speciﬁc fusion network
47
e
e
e
1
2
3
4
placeholder
flair
t2
t1
concat
task-specific code
task-specific attention
g
tsem
t1gd
task-specific 
weighted average
code
spatial 
attention
channel 
attention
fig.
finally, the fused features are decoded to the
target sequence by g. furthermore, to explain the mechanism of multi-sequence
fusion, our network can quantify the contributions of diﬀerent input sequences
with the task-speciﬁc weighted average module and visualize the tsem with
the task-speciﬁc attention module.
to leverage shared information between sequences, we use e and g from
seq2seq [10], which is a one-to-one synthetic model that integrates arbitrary
sequence synthesis into single e and g. they can reduce the distance between
diﬀerent sequences at the feature level to help more stable fusion.
to fuse mul-
tiple sequences at the feature level, we ﬁrst encode images and concatenate the
features as ⃗f = {e(xi)|i = 1, ..., n}.
the multi-sequence fusion module includes: (1)
a task-speciﬁc weighted average module for the linear combination of available
features; (2) a task-speciﬁc attention module to reﬁne the fused features.
task-speciﬁc weighted average.
the weighted average is an intuitive fusion
strategy that can quantify the contribution of diﬀerent sequences directly.
+ ϵ
(1)
where w and b are weights and bias for the fc layer, ϵ = 10−5 to avoid dividing
0 in the following equation.
ω = ω0 · csrc
⟨ω0, csrc⟩
(2)
where · refers to the element-wise product and ⟨·, ·⟩ indicates the inner product.
it
demonstrates that the designed ω can help the network excellently inherit the
synthesis performance of pre-trained e and g. in this work, we use ω to quantify
the contribution of diﬀerent input combinations.
task-speciﬁc attention.
1, channel attention and spatial attention can provide adap-
tive feature reﬁnement guided by the task-speciﬁc code c to generate residual
attentional fused features fa.
λr and
λp are weight terms and are experimentally set to be 10 and 0.01.
2.2
task-speciﬁc enhanced map
as fa is a task-speciﬁc contextual reﬁnement for fused features, analyzing it can
help us understand more what the network tried to do.
many studies focus on
visualizing the attention maps to interpret the principle of the network, especially
for the transformer modules [1,6].
thus, we proposed the
tsem by subtracting the reconstructed target sequences with and without fa,
which has the same resolution as the original images and clear interpretation for
speciﬁc tasks.
tsem = |x′
a − x′|
(6)
3
experiments
3.1
dataset and evaluation metrics
we use brain mri images of 1,251 subjects from brain tumor segmentation 2021
(brats2021)
all the images are intensity normalized to [−1, 1] and central
cropped to 128×192×192.
the synthesis performance is quantiﬁed using the metrics of peak signal noise
rate (psnr), structural similarity index measure (ssim), and learned perceptual
image patch similarity (lpips)
[21], which evaluate from intensity, structure,
and perceptual aspects.
3.2
implementation details
the models are implemented with pytorch and trained on the nvidia geforce
rtx 3090 ti gpu.
the initial convolutional layer is responsible for encoding intensities to
features, while the second and third convolutional layers downsample images by a
factor of four.
[12] (average)
26.2 ± 2.7
0.834 ± 0.054
15.84 ± 6.05
mm-gan [18]
28.0 ± 2.3
0.878 ± 0.037
10.33 ± 3.58
diamondgan
[10] (average) 28.2 ± 2.2
0.879 ± 0.035
11.11 ± 3.72
tsf-seq2seq (w/o fa) 28.0 ± 2.4
0.875 ± 0.039
9.89 ± 3.63
tsf-seq2seq
28.3 ± 2.4 0.882 ± 0.038 9.48 ± 3.58
3
pix2pix
[12] (average)
26.6 ± 2.5
0.842 ± 0.041
15.77 ± 5.08
mm-gan
[10] (average) 28.5 ± 2.3
0.880 ± 0.038
11.61 ± 3.87
tsf-seq2seq (w/o fa) 28.3 ± 2.6
0.876 ± 0.044
9.61 ± 4.00
tsf-seq2seq
28.8 ± 2.6 0.887 ± 0.042
the e
and g from seq2seq are pre-trained using the adam optimizer with an initial
learning rate of 2 × 10−4 and a batch size of 1 for 1,000,000 steps, taking about
60 h. then we ﬁnetune the tsf-seq2seq with the frozen e using the adam
optimizer with an initial learning rate of 10−4 and a batch size of 1 for another
300,000 steps, taking about 40 h.
3.3
quantitative results
we compare our method with one-to-one translation, image-level fusion,
and feature-level fusion methods.
[10]. image-level fusion methods consist of mm-
gan [18], diamondgan
table 1 reports the sequence synthesis performance for comparison meth-
ods organized by the diﬀerent numbers of input combinations.
note that, for
multiple inputs, one-to-one translation methods synthesize multiple outputs sep-
arately and average them as one.
as shown
in table 1, the proposed method achieves the best performance in diﬀerent input
combinations.
3.4
ablation study
we compare two components of our method, including (1) task-speciﬁc weighted
average and (2) task-speciﬁc attention, by conducting an ablation study between
seq2seq, tsf-seq2seq (w/o fa), and tsf-seq2seq.
as shown in
table 1, when only one sequence is available, our method can inherit the perfor-
mance of seq2seq and achieve slight improvements.
for multi-input situations,
the task-speciﬁc weighted average can decrease lpips to achieve better percep-
tual performance.
3.5
interpretability visualization
the proposed method not only achieves superior synthesis performance but also
has good interpretability.
3, both t1 and t1gd contribute greatly to
the sequence synthesis of each other, which is expected because t1gd are t1-
weighted scanning after contrast agent injection, and the enhancement between
these two sequences is indispensable for cancer detection and diagnosis.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_50.pdf:
the computed tomography (ct) for diagnosis of lesions in
human internal organs is one of the most fundamental topics in med-
ical imaging.
the proposed method can capture the local
coherence of adjacent images by optical ﬂow, which yields signiﬁcant
improvements in the precision and stability of the constructed images.
we evaluate our proposed method on real datasets and the experimental
results suggest that it can outperform existing state-of-the-art recon-
struction approaches signiﬁcantly.
keywords: ct reconstruction · low-dose · generative adversarial
networks · local coherence · optical ﬂow
1
introduction
computed tomography (ct) is one of the most widely used technologies in
medical imaging, which can assist doctors for diagnosing the lesions in human
internal organs.
however, when the
dose is low together with the issues like sparse-view or limited angles, it becomes
quite challenging to reconstruct high-quality ct images.
the high-quality ct
images are important to improve the performance of diagnosis in clinic [27].
+ δ,
(1)
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43999-5 50.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. (eds.): miccai 2023, lncs 14229, pp.
https://doi.org/10.1007/978-3-031-43999-5_50
solving low-dose ct reconstruction via gan with local coherence
525
where xr ∈ rd denotes the unknown ground-truth picture, y ∈ rm denotes
the received measurement, and δ is the noise.
[11] can produce serious detrimental artifact.
another popular line
for learning the regularizers comes from deep learning [13,17]; the advantage
of the deep learning methods is that they can achieve an end-to-end recov-
ery of the true image xr from the measurement y
recent researches
reveal that convolutional neural networks (cnns) are quite eﬀective for image
denoising, e.g., the cnn based algorithms
[10,34] can directly learn the recon-
structed mapping from initial measurement reconstructions (e.g., fbp) to the
ground-truth images.
the dual-domain network that combines the sinograms
with reconstructed low-dose ct images were also proposed to enhance the gen-
eralizability
a major drawback of the aforementioned reconstruction methods is that they
deal with the input ct 2d slices independently (note that the goal of ct recon-
struction is to build the 3d model of the organ).
namely, the neighborhood
correlations among the 2d slices are often ignored, which may aﬀect the recon-
struction performance in practice.
in the ﬁeld of computer vision, “optical ﬂow”
is a common technique for tracking the motion of object between consecutive
frames, which has been applied to many diﬀerent tasks like video generation [35],
prediction of next frames [22] and super resolution synthesis [5,31]. to estimate
the optical ﬂow ﬁeld, existing approaches include the traditional brightness gra-
dient methods [2] and the deep networks [7].
the idea of optical ﬂow has also
been used for tracking the organs movement in medical imaging [16,20,33].
when a patient is
located in a ct equipment, a set of consecutive cross-sectional images are gen-
erated.
so we apply optical ﬂow, though
there exist several technical issues waiting to solve for the design and imple-
mentation, to capture the local coherence of adjacent ct images for reducing
the artifacts in low-dose ct reconstruction.
we introduce the “local coherence” by characterizing the correlation of con-
secutive ct images, which plays a key role for suppressing the artifacts.
2. together with the local coherence, our proposed generative adversarial net-
works (gans) can yield signiﬁcant improvement for texture quality and sta-
bility of the reconstructed images.
3. to illustrate the eﬃciency of our proposed approach, we conduct rigorous
experiments on several real clinical datasets; the experimental results reveal
the advantages of our approach over several state-of-the-art ct reconstruc-
tion methods.
as mentioned in sect.
1, optical ﬂow can capture the tem-
poral coherence of object movements, which plays a crucial role in many video-
related tasks.
[9].
solving low-dose ct reconstruction via gan with local coherence
527
based on these assumptions, the brightness of optical ﬂow can be described by
the following equation:
∇iw · vw + ∇ih · vh + ∇it = 0,
(4)
where v = (vw, vh) represents the optical ﬂow of the position (w, h) in the image.
∇i = (∇iw, ∇ih) denotes spatial gradients of image brightness, and ∇it denotes
the temporal partial derivative of the corresponding region.
in practice, the brightness of
adjacent ct images often has very tiny diﬀerence, due to the inherent continu-
ity and structural integrity of human body.
therefore, we introduce the “local
coherence” that indicates the correlation between adjacent images of a tissue.
namely, adjacent ct images often exhibit signiﬁcant similarities within a certain
local range along the vertical axis of the human body.
the scanning window of x-
ray slides from the position of the left image to the position of the right image.
the left and
right images share the local coherence and thus the optical ﬂows are small.
(color
ﬁgure online)
3
gans with local coherence
in this section, we introduce our low-dose ct image generation framework with
local coherence in detail.
the proposed framework comprises three
components, including a generator g, a discriminator d and an optical ﬂow esti-
mator f. the generator is the core component, and the ﬂow estimator provides
auxiliary warping images for the generation process.
suppose we have a sequence of measurements y1, y2, · · · , yn; for each yi,
1 ≤
i ≤ n, we want to reconstruct its ground truth image xr
i as the eq.
before
performing the reconstruction in the generator g, we apply some prior knowledge
in physics and run ﬁlter backward projection on the measurement yi in eq.
then the network has two input components,
i.e., the initial backward projected image si that serves as an approximation of
the ground truth xr
i , and a set of neighbor ct slices n(si) = {si−1, si+1}1 for
preserving the local coherence.
the discriminator d assigns the label “1” to real standard-
dose ct images and “0” to generated images.
the goal of d is to maximize the
separation between the distributions of real images and generated images:
1 if i = 1, n(si) = {s2}; if i = n, n(si) = {sn−1}.
+ log(1 − d(xg
i ))),
(5)
where xg
i is the image generated by g (the formal deﬁnition for xg
i will be intro-
duced below).
we use the generator g to reconstruct the high-quality ct image
for the ground truth xr
i from the low-dose image si.
the generated image is
obtained by
xg
i = g(si, w(n(xg
i )));
n(xg
i ) = g(n(si)),
(6)
where w(·) is the warping operator.
sub-
sequently, according to the optical ﬂow f(n(si), si), we warp the reconstructed
images n(xg
i ) to align with the current slice by adjusting the brightness values.
(7)
in (7), “lpixel” is the loss measuring the pixel-wise mean square error of the
generated image xg
i with respect to the ground-truth xr
i .
“ladv” represents the
adversarial loss of the discriminator d, which is designed to minimize the dis-
tance between the generated standard-dose ct image distribution pxg and the
real standard-dose ct image distribution px.
through capturing the high frequency diﬀerences in ct images, lpercept can
enhance the sharpness for edges and increase the contrast for the reconstructed
images.
530
w. liu and h. ding
4
experiment
datasets.
first, our proposed approaches are evaluated on the “mayo-clinic
low-dose ct grand challenge” (mayo-clinic) dataset of lung ct images [19].
the dataset contains 2250 two dimensional slices from 9 patients for training,
and the remaining 128 slices from 1 patient are reserved for testing.
the low-
dose measurements are simulated by parallel-beam x-ray with 200 (or 150) uni-
in order to further verify the denoising ability of our
approaches, we add the gaussian noise with standard deviation σ = 2.0 to the
sinograms after x-ray projection in 50% of the experiments.
the proposed networks were implemented in the pytorch
framework and trained on nvidia 3090 gpu with 100 epochs.
baselines and evaluation metrics.
following most of the previous articles on
3d ct reconstruction, we evaluate the experimental performance by two met-
rics: the peak signal-to-noise ratio (psnr) and the structural similarity index
(ssim)
psnr measures the pixel diﬀerences of two images, which is nega-
tively correlated with mean square error.
ssim measures the structure similarity
between two images, which is related to the variances of the input images.
the
methods fbp and uar are very sensitive to noise; the performance of lpd
is relatively stable but with low reconstruction accuracy.
experimental results for mayo-clinic dataset.
experimental results for rider dataset.
due to the bias in the
datasets collected from diﬀerent facilities, the performances of all the models are
declined to some extents.
to illustrate the reconstruction performances more clearly, we also show the
reconstruction results for testing images in fig.
3. we can see that our network
can reconstruct the ct image with higher quality.
due to the space limit, the
experimental results of diﬀerent views nv and more visualized results are placed
in our supplementary material.
“ground truth” is the standard-dose ct
image.
by considering the inherent
continuity of human body, local coherence can be captured through optical ﬂow,
which is small deformations and structural diﬀerences between consecutive ct
slices.
the experimental results on real datasets demonstrate the advantages
of our proposed network over several popular approaches.
in future, we will
evaluate our network on real-world ct images from local hospital and use the
reconstructed images to support doctors for the diagnosis and recognition of lung
nodules.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_46.pdf:
respiratory correlated cone beam computed tomography
(4dcbct) is a technique used to address respiratory motion artifacts
that aﬀect reconstruction quality, especially for the thorax and upper-
abdomen.
4dcbct sorts the acquired projection images in multiple
respiratory correlated bins.
this technique results in the emergence of
aliasing artifacts caused by the low number of projection images per bin,
which severely impacts the image quality and limits downstream use.
using a funda-
mental property of the fdk reconstruction algorithm, and prior results
from the literature, we prove mathematically the ability of the method
to work and specify the underlying assumptions.
we apply the method to a public dataset and to an in-house dataset
and show that it matches the performance of a supervised approach and
outperforms it when measurement noise is present in the data.
image
guided rt (igrt) is a technique to capture the anatomy of the day using in
room imaging in order to align the treatment beam with the tumor location [1].
cone beam ct (cbct) is the most widely used imaging modality for igrt.
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43999-5 46.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
https://doi.org/10.1007/978-3-031-43999-5_46
482
s. papa et al.
a major challenge especially for cbct imaging of the thorax and upper-
abdomen is the respiratory motion that introduces blurring of the anatomy,
reducing the localization accuracy and the sharpness of the image.
however, only 20 to 60 respiratory peri-
ods are imaged.
additionally, the projections are aﬀected by stochastic mea-
surement noise caused by the ﬁnite imaging dose used, which further degrades
the quality of the reconstruction even when all projections are used.
however, the method cannot reduce measurement
noise because it is still present in the images used as targets during training.
a diﬀerent method, called noise2inverse, uses an unsupervised approach to
reduce measurement noise in the traditional ct setting [4].
in this case, the motion artifacts that blur the image will appear
again, as noise2inverse requires averaging the sub-reconstructions to obtain a
clean reconstruction.
we explore diﬀerent dataset sizes to understand their eﬀects on the
reconstructed images.
2
theoretical background
in this section, we will introduce the concepts and the notation necessary to
understand the method and the choices made during implementation.
given input-target pairs x, y ∈
r we can deﬁne the regression problem in the one-dimensional setting as ﬁnding
noise2aliasing
483
f ∗ : r → r which satisﬁes the following:
f ∗ = arg min
f
ex,y

∥f(x)
[5], input-target pairs are two samples of the same image that
only diﬀer because of some independent mean-zero noise (x + δ1, x + δ2) with
eδ2
[x + δ2|x + δ1] = x. then f ∗ will recover the input image without any noise:
f ∗(x + δ1) =
during a ct scan, a volume x
is imaged by acquiring projections y = ax using an x-ray source and a detector
placed on the opposite side of the volume.
− ˆxj∥2
2 + e∥ˆxj − ˜xj∥2
2,
(4)
where j is a random variable that picks subsets of projections at random and j′
is its complementary.
given eq. 2, we observe that function f ∗ which minimizes l is:
f ∗(˜xj′)
(5)
when using reconstructions from a subset of noisy projections as input and
reconstructions from their complementary as its output, a neural network will
learn to predict the expected reconstruction without the noise.
property of expectation over subsets of projections using fdk.
[2] that reconstructs a volume of dimensionality
dv from projections j each with dimensionality dd (geometrical details on the
exact setup are not relevant).
the fdk uses, as its fundamental step, the dual
484
s. papa et al.
radon transform
}, the fdk reconstruction
algorithm r, and the noisy projections ˜y = ax+ϵ with ϵ mean-zero element-wise
independent noise.
− ˜xj1∥2
2.
(8)
additionally, j1, j2 are disjoint, the noise is mean-zero element-wise, and we
are using the fdk reconstruction algorithm which deﬁnes a linear operator r.
these allow us to use eq. 5 to ﬁnd that the function f ∗ that minimizes l is the
following:
f ∗(˜xj) = ej1,j2(ˆxj1|˜xj2 = ˜xj).
(9)
this is suﬃcient to reduce stochastic noise but we need to further manipulate this
expression to address view aliasing.
the reconstructions will dis-
play organs in their average position and, therefore, have the same underlying
structure.
coincidentally, a previously proposed subset selection
method utilized for supervised aliasing reduction ﬁts all these requirements and
will, therefore, be used in this work [4].
4
experiments
first, we used the spare varian dataset to study whether noise2aliasing can
match the performance of the supervised baseline and if it can outperform it
when adding noise to the projections.
then, we use the internal dataset to explore
the requirements for the method to be applied to an existing clinical dataset.
the projections obtained during
a scan are sub-sampled according to the pseudo-average subset selection method
described in [6] and then used to obtain 3d reconstructions.
given two volumes
(x, y), the training pairs (xi(k), yi(k)) are the same i-th slice along the k-th dimen-
sion of each volume chosen to be the axial plane.
the spare varian dataset was used to provide performance results on pub-
licly available patient data.
to more closely resemble normal respiratory
motion per projection image, the 8 min scan has been used from each patient
(ﬁve such scans are available in the dataset).
the msd makes use of
dilated convolutions to process features at all scales of the image.
in the
supervised approach, the model is trained by using as input reconstructions
obtained from subsets deﬁned with pseudo-average subset selection while the
targets use all of the projections available.
inference speed with the nvidia a100 gpu averages 600ms
per volume made of 220 slices.
in the low-
noise setting, both supervised and noise2aliasing outperform fdk with very
similar results, often within a single standard deviation.
noise2aliasing successfully matches the performance of the supervised base-
line.
noise2aliasing and the supervised method produce very similar images
in the low-noise case.
from fig. 1 and table 1, the supervised approach repro-
duces the noise that was seen during training, while noise2aliasing manages to
remove it consistently, outperforming the supervised approach, especially in the
soft tissue area around the lungs, where the noise aﬀects attenuation coeﬃcients
the most.
when applied to a clinical dataset, noise2aliasing beneﬁts from more
patients being included in the dataset, however, qualitatively good performance
is already achieved with 5 patients.
we have empirically demonstrated its performance on
a publicly available dataset and on an internal clinical dataset.
noise2aliasing
noise2aliasing
489
outperforms a supervised approach when stochastic noise is present in the pro-
jections and matches its performance on a popular benchmark.
the method removes noise more reliably when the
dataset size is increased, however further analysis is required to establish a good
quantitative measurement of this phenomenon.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_3.pdf:
conventional ac techniques require additionally-acquired com-
puted tomography (ct) or magnetic resonance (mr) images to calculate
attenuation coeﬃcients, which increases imaging expenses, time costs, or
radiation hazards to patients, especially for whole-body scanners.
in this
paper, considering technological advances in acquiring more anatomi-
cal information in raw pet images, we propose to conduct attenuation
correction to pet by itself.
to achieve this, we design a deep learn-
ing based framework, namely anatomical skeleton-enhanced generation
(aseg), to generate pseudo ct images from non-attenuation corrected
pet images for attenuation correction.
experiments on four public pet/ct
datasets demonstrate that our aseg outperforms existing methods by
achieving better consistency of anatomical structures in generated ct
images, which are further employed to conduct pet attenuation correc-
tion with better similarity to real ones.
this work veriﬁes the feasibility of
generating pseudo ct from raw pet for attenuation correction without
acquising additional images.
the associated implementation is available
at https://github.com/yongshengpan/aseg-for-pet2ct.
keywords: pet · attenuation correction · ct · image generation
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43999-5_3.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al.
according to the pet imaging principle, radioactive tracers injected
into the body involve in the metabolism and produce γ decay signals externally.
however, due to photoelectric absorption and compton scattering, the decay
signals are attenuated when passing through human tissues to external receivers,
resulting in incorrect tracer distribution reasoning (see non-attenuation corrected
pet (nac-pet) in fig.
to obtain correct tracer distribution (see ac-
pet in fig.
traditional ac accompanies additional costs caused by the simultaneously
obtained mr or ct images which are commonly useless for diagnosis.
to reduce the costs,
including expense, time, and radiation hazards, some studies proposed to con-
duct ac by exploiting each pet image itself.
researchers have been motivated
to generate pseudo ct images from nac-pet images [2,7], or more directly,
26
y. pan et al.
to generate ac-pet images from nac-pet images [5,11].
since pseudo ct is
convenient to be integrated into conventional ac processes, generating pseudo
ct images is feasible in clinics for ac.
the pseudo ct images should satisfy two-fold requests.
firstly, the pseudo
ct images should be visually similar in anatomical structures to correspond-
ing actual ct images.
secondly, pet images corrected by pseudo ct images
should be consistent with that corrected by actual ct images.
however, cur-
rent techniques of image generation tend to produce statistical average values
and patterns, which easily erase signiﬁcant tissues (e.g., bones and lungs).
therefore, special techniques should be
investigated to guarantee the ﬁdelity of anatomical structures in these generated
pseudo ct images.
aseg focuses more on the ﬁdelity of tissue distribu-
tion, i.e., anatomical skeleton, in pseudo ct images.
g1 devotes to delineating the anatomical skele-
ton from a nac-pet image, thus producing a prior tissue distribution map
to g2, while g2 devotes to rendering the tissue details according to both the
skeleton and nac-pet image.
we regard g1 as a segmentation network that
is trained under the combination of cross-entropy loss and dice loss and out-
puts the anatomical skeleton.
experiments on four
publicly collected pet/ct datasets demonstrate that our aseg outperforms
existing methods by preserving better anatomical structures in generated pseudo
ct images and achieving better visual similarity in corrected pet images.
g2 then devotes to rendering the tissue details in the ct pattern exploit-
ing both the skeleton and nac-pet images.
specially, the general dice loss and cross-entropy loss [16]
revealing anatomical structures in pet
27
are employed to guarantee g1 for the ﬁdelity of tissue distributions while general
mean absolute error and feature matching losses are utilized to guarantee g2 for
potential coarse-to-ﬁne semantic constraint.
to improve the ﬁdelity of anatomi-
cal structures, we further propose the anatomical consistency loss to encourage
g2 to generate ct images that are consistent in tissue distributions with actual
ct images in particular.
let xnac and xac denote the nac-pet and ac-pet
images, and y be the actual ct image used for ac.
since ct image is highly
crucial in conventional ac algorithms, they generally have a relationship as
xac = f(xnac, y ),
(1)
under an ac algorithm f. to avoid scanning an additional ct image, we attempt
to predict y from xnac as an alternative in ac algorithm.
(2)
this results in a pioneering ac algorithm that requires only a commonly reusable
mapping function g for all pet images rather than a corresponding ct image
y for each pet image.
as veriﬁed in some previous studies [1,2,7], g can be assigned by some
image generation techniques, e.g. gans and cnns.
however, since these general
techniques tend to produce statistical average values, directly applying them may
lead to serious brightness deviation, for those tissues with large intensity ranges.
to avoid annotating the ground truth, yas can be derived from the actual ct
image by a segmentation algorithm (denoted as s : yas = s(y )).
herein, we ﬁrst smooth each non-normalized ct
image with a small recursive gaussian ﬁlter to suppress the impulse noise, and
then threshold this ct image to four binary masks according to the hounsﬁeld
scale of tissue density
as mentioned above, two generative modules {g1, g2}
work for two tasks, namely the skeleton prediction and tissue rendering, respec-
tively.
it is generally known that ct images can provide
anatomical observation because diﬀerent tissues have a distinctive appearance
in hounsﬁeld scale (linear related to attenuation coeﬃcients).
therefore, it is
crucial to ensure the consistency of tissue distribution in the pseudo ct images,
tracking which we propose to use the tissue distribution consistency to guide the
network learning.
based on the segmentation algorithm s, both the actual and
generated cts {y, ˆy } can be segmented to anatomical structure/tissue distribu-
tion masks {s(y ), s( ˆy )}, and their consistency can then be measured by dice
coeﬃcient.
(6)
during the inference phase, only the nac-pet image of each input subject
is required, where the pseudo ct image is derived by ˆy ≈ g2(g1(xnac), xnac).
revealing anatomical structures in pet
29
3
experiments
3.1
materials
the data used in our experiments are collected from the cancer image archive
(tcia)
in our experiments, we re-sampled all of them
to a voxel spacing of 2×2×2 and re-scaled the intensities of nac-pet/ac-pet
images to a range of [0, 1], of ct images by multiplying 0.001.
to achieve full-fov output, the consecutive outputs of each sample
are composed into a single volume where the overlapped regions are averaged.
3.2
comparison with other methods
we compared our aseg with three state-of-the-art methods, including (i) a
u-net based method
[3] that directly learns a mapping from nac-pet to
ct image with mae loss (denoted as u-net), (ii) a conventional gan-based
method [1,2] that uses the u-net as the backbone and employ the style-content
loss and adversarial loss as an extra constraint (denoted as cgan), and (iii)
an auxiliary gan-based method [10] that uses the ct-based segmentation (i.e.,
the simple thresholding s) as an auxiliary task for ct generation (denoted as
agan).
for a fair comparison, we implemented these methods by ourselves in a
tensorflow platform with an nvidia 3090 gpu.
and follow the same experimental settings.
as the most import application of ct that
is to display the anatomical information, we propose to measure the anatom-
ical consistency between the pseudo ct images and actual ct images, where
the dice coeﬃcients on multiple anatomical regions that extracted from the
pseudo/actual ct images are calculated.
to avoid excessive self-referencing in
evaluating anatomical consistency, instead of employing the simple threshold-
ing segmentation (i.e., s), we resort to the open-access totalsegmentator
[15]
to ﬁnely segment the actual and pseudo ct images to multiple anatomical
structures, and compose them to nine independent tissues for simplifying result
30
y. pan et al.
table 1. comparison of pseudo ct images generated by diﬀerent methods.
firstly, u-net and cgan generate ct images with
slightly better global intensity similarity but worse anatomical consistency in
some tissues than agan and aseg.
this indicates that the general constraints
(mae and perceptual feature matching) cannot preserve the tissue distribution
since they tend to produce statistical average values or patterns, particularly in
these regions with large intensity variants.
as the pseudo ct images gener-
ated from nac-pet are expected to be used in ac, it is necessary to further
evaluate the eﬀectiveness of pseudo ct images in pet ac.
to evaluate the pseudo ct images, we simply use
them to take place of the actual ct. four metrics, including the peak signal to
noise ratio (psnr), mean absolute error (mae), normalized cross correla-
tion (ncc), and ssim, are used to measure acgan with pseudo ct images
on test datasets (nsclc, tcga-hnsc, and tcga-luda).
the results are
reported in table 1(b), where the fourth column list the acgan results with
actual ct images.
meanwhile, we also report the results of direct mapping nac-
pet to ac-pet without ct images in the third column (“no ct”), which is
trained from scratch and independent from acgan.
it can be observed from table 1(b) that: (1) acgan with actual ct images
can predict images very close to the actual ac-pet images, thus is qualiﬁed to
simulate the ac process; (2) with actual or pseudo ct images, acgan can
predict images closer to the actual ac-pet images than without ct, demon-
strating the necessity of ct images in process of pet ac; (3) these pseudo cts
cannot compare to actual cts, reﬂecting that there exist some relative informa-
tion that can hardly be mined from nac-pet; (4) the pseudo cts generated
by aseg achieve the best in three metrics (mae, psnr, ncc) and second
in the other metric (ssim), demonstrating the advance of our aseg.
2. visualization of diﬀerent pseudo ct images (top) and their ac eﬀect (bottom).
4
conclusion
in this paper, we proposed the anatomical skeleton-enhance generation (aseg)
to generate pseudo ct images for pet attenuation correction (ac), with the
goal of avoiding acquiring extra ct or mr images.
experiments on a collection of public
datasets demonstrate that our aseg outperforms existing methods by achiev-
ing advanced performance in anatomical consistency.
our study support that
aseg could be a promising and lower-cost alternative of ct acquirement for
ac.
our future work will extend our study to multiple pet tracers.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_21.pdf:
contrast-enhanced ultra-sound (ceus) has become a
viable method for non-invasive, dynamic visualization in medical diag-
nostics, yet ultrasound localization microscopy (ulm) has enabled
a revolutionary breakthrough by oﬀering ten times higher resolution.
to date, delay-and-sum (das) beamformers are used to render ulm
frames, ultimately determining the image resolution capability.
to take
full advantage of ulm, this study questions whether beamforming is
the most eﬀective processing step for ulm, suggesting an alternative
approach that relies solely on time-diﬀerence-of-arrival (tdoa) infor-
mation.
keywords: ultrasound · microbubble · localization · microscopy ·
geometry · parallax · triangulation · trilateration · multilateration ·
time-of-arrival
1
introduction
ultrasound localization microscopy (ulm) has revolutionized medical imaging
by enabling sub-wavelength resolution from images acquired by piezo-electric
transducers and computational beamforming.
the discovery of ulm has recently surpassed the diﬀraction-limited spatial
resolution and enabled highly detailed visualization of the vascularity [8]. ulm
borrows concepts from super-resolution ﬂuorescence microscopy techniques to
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43999-5_21.
https://doi.org/10.1007/978-3-031-43999-5_21
218
c. hahne and r. sznitman
precisely locate individual particles with sub-pixel accuracy over multiple frames.
by the accumulation of all localizations over time, ulm can produce a super-
resolved image, providing researchers and clinicians with highly detailed repre-
sentation of the vascular structure.
[8] initially demonstrated the potential of
ulm by successfully localizing contrast agent particles (microbubbles) using a
2d point-spread-function model.
1. comparison of ulm processing pipelines: classical ulm (top) employs compu-
tational beamforming from n channels and image ﬁlters to localize microbubbles.
as
a reﬁnement step, ellipse intersections are fused via clustering (right).
for example, a recent
study has shown that ultrasound image segmentation can be learned from radio-
frequency data and thus without beamforming [13].
[3], optimization of
the point-spread function (psf) poses high demands on the transducer array,
data storage, and algorithm complexity.

--------------------------------------------------------------------------------

Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_24.pdf:
sparse-view computed tomography (ct) is a promising solu-
tion for expediting the scanning process and mitigating radiation expo-
sure to patients, the reconstructed images, however, contain severe streak
artifacts, compromising subsequent screening and diagnosis.
recently,
deep learning-based image post-processing methods along with their
dual-domain counterparts have shown promising results.
however, exist-
ing methods usually produce over-smoothed images with loss of details
due to i) the diﬃculty in accurately modeling the artifact patterns in
the image domain, and ii) the equal treatment of each pixel in the loss
function.
to address these issues, we concentrate on the image post-
processing and propose a simple yet eﬀective frequency-band-aware
and self-guided network, termed freeseed, which can eﬀectively remove
artifacts and recover missing details from the contaminated sparse-view
ct images.
speciﬁcally, we ﬁrst propose a frequency-band-aware arti-
fact modeling network (freenet), which learns artifact-related frequency-
band attention in the fourier domain for better modeling the globally
distributed streak artifact on the sparse-view ct images.
we then intro-
duce a self-guided artifact reﬁnement network (seednet), which lever-
ages the predicted artifact to assist freenet in continuing to reﬁne the
severely corrupted details.
extensive experiments demonstrate the supe-
rior performance of freeseed and its dual-domain counterpart over the
state-of-the-art sparse-view ct reconstruction methods.
source code is
made available at https://github.com/masaaki-75/freeseed.
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43999-5 24.
sparse-view ct is one of the eﬀective solutions, which reduces the
radiation by only sampling part of the projection data for image reconstruction.
nevertheless, images reconstructed by the conventional ﬁltered back-projection
(fbp) present severe artifacts, thereby compromising their clinical value.
existing learning-based approaches
mainly include image-domain methods [2,4,18] and dual-domain ones [7,13,16],
both involving image post-processing to restore a clean ct image from the
low-quality one with streak artifacts.
for the image post-processing, residual
learning [3] is often employed to encourage learning the artifacts hidden in
the residues, which has become a proven paradigm for enhancing the perfor-
mance [2,4,6,16].
unfortunately, existing image post-processing methods may
fail to model the globally distributed artifacts within the image domain.
they
can also produce over-smoothed images due to the lack of diﬀerentiated super-
vision for each pixel.
in this paper, we advance image post-processing to beneﬁt
both classical image-domain methods and the dominant dual-domain ones.
motivation.
we view the sparse-view ct image reconstruction as a two-step
task: artifact removal and detail recovery.
while fourier domain band-pass maps help capture the pattern of the arti-
facts, restoring the image detail contaminated by strong artifacts may still be
diﬃcult due to the entanglement of artifacts and details in the residues.
sequently, we propose a self-guided artifact reﬁnement network (seednet) that
provides supervision signals to aid freenet in reﬁning the image details con-
taminated by the artifacts.
freeseed achieves promising results with only image data and can be further
enhanced once the sinogram is available.
252
c. ma et al.
our contributions can be summarized as follows: 1) a novel frequency-band-
aware network is introduced to eﬃciently capture the pattern of global artifacts
in the fourier domain among diﬀerent sparse-view scenarios; 2) to promote the
restoration of heavily corrupted image detail, we propose a self-guided artifact
reﬁnement network that ensures targeted reﬁnement of the reconstructed image
and consistently improves the model performance across diﬀerent scenarios; and
3) quantitative and qualitative results demonstrate the superiority of freeseed
over the state-of-the-art sparse-view ct reconstruction methods.
first row: sparse-view ct images (left half) and the corresponding artifacts
(right half); second row: real fourier amplitude maps of artifacts (left half) and the
learned band-pass attention maps (right half, with inner radius and bandwidth respec-
tively denoted by d0 and w. values greater than 0.75 are bounded by red dotted line)
given diﬀerent number of views nv.
(color ﬁgure online)
2
methodology
2.1
overview
given a sparse-view sinogram with projection views nv, let is and if denote
the directly reconstructed sparse- and full- view images by fbp, respectively.
in
this paper, we aim to construct an image-domain model to eﬀectively recover is
with a level of quality close to if.
2, which mainly
consists of two designs: freenet that learns to remove the artifact and is built
with band-pass fourier convolution blocks that better capture the pattern of the
artifact in fourier domain; and seednet as a proxy module that enables freenet
to reﬁne the image detail under the guidance of the predicted artifact.
(4)
2.3
self-guided artifact reﬁnement network
areas heavily obscured by the artifact should be given more attention, which
is hard to achieve using only freenet.
concretely, given sparse-
view ct images is, freenet predicts the artifact 	a and restored image 	i =
is − 	a; the latter is fed into seednet to produce targeted reﬁned result 
i. to
guide the network on reﬁning the image detail obscured by heavy artifacts, we
design the transformation t that turns 	a into a mask m using its mean value
as threshold:
the pseudo-code for the training process and
the exploration on the selection of α can be found in our supplementary material.
to further enhance the image reconstruc-
tion quality, we extend freeseed to the dominant dual-domain framework by
adding the sinogram-domain sub-network from dudonet
the sinogram-
domain sub-network involves a mask u-net that takes in the linearly interpo-
lated sparse sinogram ss, where a binary sinogram mask m s that outlines the
unseen part of the sparse-view sinogram is concatenated to each stage of the
u-net encoder.
3. overview of dual-domain counterpart of freeseed.
3
experiments
3.1
experimental settings
we conduct experiments on the dataset of “the 2016 nih-aapm mayo clinic
low dose ct grand challenge”
[8], which contains 5,936 ct slices in 1 mm
image thickness from 10 anonymous patients, where a total of 5,410 slices from
9 patients, resized to 256 × 256 resolution, are randomly selected for training
and the 526 slices from the remaining one patient for testing without patient
overlap.
specifying the distance from the x-ray source to the
rotation center as 59.5 cm and the number of detectors as 672, we generate
sinograms from full-dose images with multiple sparse views nv ∈ {18, 36, 72, 144}
uniformly sampled from full 720 views covering [0, 2π].
the models are implemented in pytorch
experi-
ments are conducted on a single nvidia v100 gpu using the same setting.
[15].
256
c. ma et al.
3.2
overall performance
we compare our models (freeseed and freeseeddudo) with the following recon-
struction methods: direct fbp, ddnet
[13]. fbpconv and ddnet are image-domain methods, while
dudonet and dudotrans are state-of-the-art dual-domain methods eﬀective for
ct image reconstruction.
not surprisingly, we ﬁnd that the performance of conventional image-domain
methods is inferior to the state-of-the-art dual-domain method, mainly due to
the failure of removing the global artifacts.
note that when the sinogram data are available,
dual-domain counterpart freeseeddudo gains further improvements, showing the
great ﬂexibility of our model.
3.3
ablation study
table 2 presents the eﬀectiveness of each component in freeseed, where seven
variants of freeseed are: (1) fbpconv upon which freenet is built (baseline); (2)
freenet without band-pass attention maps nor seednet guidance lmask (baseline
+ fourier); (3) fbpconv trained with lmask (baseline + seednet); (4) freenet
frequency-band-aware and self-guided network for sparse-view ct
257
trained without lmask (freenet); (5) freenet trained with simple masked loss
l1+mask = ∥(af − 	a) ⊙ (1 + m)∥2 (freenet1+mask); (6) freenet trained with
lmask using ℓ1 norm (freeseedℓ1); and (7) freenet trained with lmask using ℓ2
norm, i.e., the full version of our model (freeseed).
by comparing the ﬁrst two rows of table 2, we ﬁnd that simply applying ffc
provides limited performance gains.
interestingly, we observe that the advantage
of band-pass attention becomes more pronounced given more views, which can be
seen in the last row of fig.
1 where the attention maps are visualized by averaging
all inner radii and bandwidths in diﬀerent stages of freenet and calculating the
map following eq.
= 36, 72, 144 where
artifacts are less entangled with the image content and present a banded shape
in the frequency domain.
visually, clinical details in the image that are obscured by
the heavy artifacts can be further reﬁned by freenet; please refer to fig.
s1 in
our supplementary material for more examples and ablation study.
we also ﬁnd
that freenet1+mask does not provide stable performance gains, probably because
directly applying a mask on the pixel-wise loss leads to the discontinuous gradient
that brings about sub-optimal results, which, however, can be circumvented with
the guidance of seednet.
we ﬁnd that ℓ1 norm does not ensure
stable performance gains when ffc is used.

--------------------------------------------------------------------------------

